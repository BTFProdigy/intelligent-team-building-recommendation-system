In: Proceedings Of CoNLL-2000 and LLL-2000, pages 43-48, Lisbon, Portugal, 2000. 
Memory-Based Learning for Article Generation 
Guido  Minnen*  Franc is  Bond t 
Cognitive and Computing Sciences MT Research Group 
University of Sussex NTT Communication Science Labs 
Palmer BN1 9QH, Brighton, UK 2-4 Hikari-dai, Kyoto 619-0237, JAPAN 
Guido. Minnen?cogs. susx. ac. uk bond?cslab, kecl. ntt. co. jp 
Ann Copestake  
CSLI 
Stanford University 
Stanford CA 94305-2150, USA 
aac?csli, stanford, edu 
Abst ract  
Article choice can pose difficult problems in ap- 
plications uch as machine translation and auto- 
mated summarization. In this paper, we investi- 
gate the use of corpus data to collect statistical 
generalizations about article use in English in 
order to be able to generate articles automati- 
cally to supplement a symbolic generator. We 
use data from the Penn Treebank as input to a 
memory-based learner (TiMBL 3.0; Daelemans 
et al, 2000) which predicts whether to gener- 
ate an article with respect o an English base 
noun phrase. We discuss competitive r sults ob- 
tained using a variety of lexical, syntactic and 
semantic features that play an important role in 
automated article generation. 
1 In t roduct ion  
Article choice can pose difficult problems in nat- 
ural language applications. Machine transla- 
tion (MT) is an example of such an applica- 
tion. When translating from a source language 
that lacks articles, such as Japanese or Rus- 
sian, to one that requires them, such as English 
or German, the system must somehow generate 
the source language articles (Bond and Ogura, 
1998). Similarly in automated summarization: 
when sentences or fragments are combined or 
reduced, it is possible that the form of a noun 
phrase (NP) is changed such that a change of 
the article associated with the NP's head be- 
comes necessary. For example, consider the sen- 
tences A talk will be given on Friday about NLP; 
The talk will last .for one hour which might get 
summarized as Friday's NLP talk will last one 
* Visiting CSLI, Stanford University (2000). 
t Visiting CSLI, Stanford University (1999-2000). 
hour. However, given the input sentences, it is 
not clear how to decide not to generate an arti- 
cle for the subject NP in the output sentence. 
Another important application is in the field 
known as augmentative and alternative com- 
munication (AAC). In particular, people who 
have lost the ability to speak sometimes use 
a text-to-speech generator as a prosthetic de- 
vice. But most disabilities which affect speech, 
such as stroke or amyotrophic lateral sclerosis 
(ALS or Lou Gehrig's disease), also cause some 
more general motor impairment, which means 
that prosthesis users cannot achieve a text in- 
put rate comparable to normal typing speeds 
even if they are able to use a keyboard. Many 
have to rely on a slower physical interface (head- 
stick, head-pointer, eye-tracker tc). We are at- 
tempting to use a range of NLP technology to 
improve text input speed for such users. Article 
choice is particularly important for this applica- 
tion: many AAC users drop articles and resort 
to a sort of telegraphese, but this causes degra- 
dation in comprehension f synthetic speech and 
contributes to its perception as unnatural and 
robot-like. Our particular goal is to be able to 
use an article generator in conjunction with a 
symbolic generator for AAC (Copestake, 1997; 
Carroll et al, 1999). 
In this paper we investigate the use of corpus 
data to collect statistical generalizations about 
article use in English so as to be able to gen- 
erate them automatically. We use data from 
the Penn Treebank as input to a memory-based 
learner (TiMBL 3.0; Daelemans et al, 2000) 
that is used to predict whether to generate the 
or alan or no article. 1 We discuss a variety 
of lexical, syntactic and semantic features that 
1We assume a postprocessor to determine whether to 
generate a or an as  described in Minnen et al (2000). 
43 
play an important role in automated article gen- 
eration, and compare our results with other re- 
searchers'. 
The paper is structured as follows. Section 2 
relates our work to that of others. Section 3 
introduces the features we use. Section 4 intro- 
duces the learning method we use. We discuss 
our results in Section 5 and suggest some di- 
rections for future research, then conclude with 
some final remarks in Section 6. 
2 Re la ted  Work  
There has been considerable research on gen- 
erating articles in machine translation sys- 
tems (Gawrofiska, 1990; Murata and Nagao, 
1993; Bond and Ogura, 1998; Heine, 1998). 
These systems use hand-written rules and lex- 
ical information to generate articles. The best 
cited results, 88% accuracy, are quoted by Heine 
(1998) which were obtained with respect o a 
very small corpus of 1,000 sentences in a re- 
stricted omain. 
Knight and Chander (1994) present an ap- 
proach that uses decision trees to determine 
whether to generate the or alan. They do not 
consider the possibility that no article should 
be generated. On the basis of a corpus of 400K 
NP instances derived from the Wall Street Jour- 
nal, they construct decision trees for the 1,600 
most frequent nouns by considering over 30,000 
lexical, syntactic and semantic features. They 
achieve an accuracy of 81% with respect to these 
nouns. By guessing the for the remainder of the 
nouns, they achieve an overall accuracy of 78%. 
3 Features  Determin ing  Automated  
Ar t i c le  Generat ion  
We have extracted 300K base noun phrases 
(NPs) from the Penn Treebank Wall Street 
Journal data (Bies et al, 1995) using the tgrep 
tool. The distribution of these NP instances 
with respect o articles is as follows: the 20.6%, 
a/an 9.4% and 70.0% with no article. 
We experimented with a range of features: 
1. Head of the NP: We consider as the head 
of the NP the rightmost noun in the NP. If an 
NP does not contain a noun, we take the last 
word in the NP as its head. 
2. Part-of-speech (PoS) tag of the head of 
the NP: PoS labels were taken from the Penn 
Treebank. We list the tags that occurred with 
(PP-DIR to/T0 
(NP the/DT problem/NN)) 
Figure 1: An example ofa  prepositional phrase 
annotated with a functionaltag 
the heads of theNPs in Table 1. 
PoS Tag the alan no 
NN 42,806 27,160 53,855 
NNS 10,705 446 58,118 
NNP 6,938 271 47,721 
NNPS 536 2 1,329 
CD 382 180 13,368 
DT 18 0 3,045 
PRP 0 0 21,214 
PRP$ 0 0 25 
EX 0 0 1,073 
IN 0 1 502 
JJ 388 143 931 
JJR 11 1 310 
JJS 184 0 282 
RB 15 41 498 
VBG 43 12 210 
VB 0 1 89 
WDT 2 0 4,812 
WP 0 0 2,759 
Misc. 40 8 269 
Total: 62,068 28,266 210,410 
Table 1: Distribution of NP instances in Wall 
Street Journal data (300,744 NPs in all) 
3. Functional tag of the head of the NP: In 
the Penn Treebank each syntactic ategory can 
be associated with up to four functional tags as 
listed in Table 2. We consider the sequence of 
functional tags associated with the category of 
the NP as a feature; if a constituent has no func- 
tional tag, we give the feature the value NONE. 
4. Category of the constituent embedding the 
NP: We looked at the category of the embedding 
constituent. See Figure 1: The category of the 
constituent embedding the NP the problem is 
PP. 
5. Functional tag of the constituent 
embedding the NP: If the category of the con- 
stituent embedding the NP is associated with 
one or more functional tags, they are used as 
features. The functional tag of the constituent 
embedding the problem in Figure 1 is DIR. 
6. Other determiners of the NP: We looked 
at the presence of a determiner in the NP. By 
definition, an NP in the Penn Treebank can only 
44 
Functional Marks: 
Tag (ft) Text categories 
HLN headlines and datelines 
LST list markers 
TTL titles 
Grammat ica l  functions 
CLF 
N0M 
ADV 
LGS 
PRD 
SUBJ 
TPC 
CLR 
BNF 
DTV 
true clefts 
non NPs that function as NPs 
clausal and NP adverbials 
logical subjects in passives 
nonVP predicates 
surface subject 
topicalized/fronted constituents 
closely related 
beneficiary of action 
dative object 
Semantic roles 
V0C vocatives 
DIR direction and trajectory 
L0C location 
MNR manner 
PRP purpose and reason 
TMP temporal phrases 
PUT locative complement of put 
EXT spatial extent of activity 
Table 2: Functional tags and their mean- 
ing (Santorini, 1990) 
have one determiner (Bies et al, 1995), so we 
expect it to be a good predictor of situations 
where we should not generate an article. 
7. Head countability preferences of the head 
of the NP: In case the head of an NP is a noun 
we also use its countability as a feature. We an- 
ticipate that this is a useful feature because sin- 
gular indefinite countable nouns normally take 
the article a/n, whereas singular indefinite un- 
countable nouns normally take no article: a dog 
vs water. We looked up the countability from 
the transfer lexicon used in the Japanese-to- 
English machine translation system ALT- J /E  
(Ikehara et al, 1991). We used six values for 
the countability feature: FC (fully countable) for 
nouns that have both singular and plural forms 
and can be directly modified by numerals and 
modifiers such as many; UC (uncountable) for 
nouns that have no plural form and can be mod- 
ified by much; SC (strongly countable) for nouns 
that are more often countable than uncount- 
able; WC (weakly countable) for nouns that are 
more often uncountable than countable; and PT 
(pluralia tantum) for nouns that only have plu- 
ral forms, such as for example, scissors (Bond 
et al, 1994). Finally, we used the value UNKNOWN 
if the lexicon did not provide countability infor- 
mation for a noun or if the head of the NP was 
not a noun. 41.4% of the NP instances received 
the value UNKNOWN for this feature. 
8. Semantic classes of the head of the NP: If 
the head of the NP is a noun we also take into 
account its semantic lassification in a large se- 
mantic hierarchy. The underlying idea is that 
the semantic lass of the noun can be used as a 
way to back off in case of unknown head nouns. 
The 2,710 node semantic hierarchy we used was 
also developed in the context of the ALT- J /E 
system (Ikehara et al, 1991). Edges in this hi- 
erarchy represent IS-A or HAS-A relationships. 
In case the semantic lasses associated with two 
nodes stand in the IS-A relation, the semantic 
class associated with the node highest in the hi- 
erarchy subsumes the semantic lass associated 
with the other node. 
Each of the nodes in this part of the hierarchy 
is represented by a boolean feature which is set 
to 1 if that node lies on the path from the root 
of the hierarchy to a particular semantic class. 
Thus, for example, the semantic features of a 
noun in the semantic class organization con- 
sists of a vector of 30 features where the features 
corresponding to the nodes noun, concrete ,  
agent and organization are set to I and all 
other features are set to 0. 2 
4 Memory-based  learn ing  
We used the Tilburg memory based learner 
TiMBL 3.0.1 (Daelemans et al, 2000) to learn 
from examples for generating articles using the 
features discussed above. Memory-based learn- 
ing reads all training instances into memory and 
classifies test instances by extrapolating a class 
from the most similar instance(s) in memory. 
Daelemans et al (1999) have shown that 
for typical natural language tasks, this ap- 
proach has the advantage that it also extrap- 
olates from exceptional and low-frequency in- 
stances. In addition, as a result of automat- 
ically weighing features in the similarity func- 
tion used to determine the class of a test in- 
stance, it allows the user to incorporate large 
2If a noun has multiple senses, we collapse them by 
taking the semantic lasses of a noun to be the union of 
the semantic lasses of all its senses. 
45 
numbers of features from heterogeneous sources: 
When data is sparse, feature weighing embod- 
ies a smoothing-by-similarity effect (Zavrel and 
Daelemans, 1997). 
5 Evaluat ion and Discuss ion 
We tested the features discussed in section 3 
with respect o a number of different memory- 
based learning methods as implemented in the 
TiMBL system (Daelemans et al, 2000). 
We considered two different learning algo- 
rithms. The first, IB1 is a k-nearest neighbour 
algorithm. 3 This can be used with two differ- 
ent metrics to judge the distance between the 
examples: overlap and modified value difference 
metric (MVDM). TiMBL automatically learns 
weights for the features, using one of five dif- 
ferent weighting methods: no weighting, gain 
ratio, information gain, chi-squared and shared 
variance. The second algorithm, IGTREE, stores 
examples in a tree which is pruned according 
to the weightings. This makes it much faster 
and of comparable accuracy. The results for 
these different methods, for k = 1, 4, 16 are dis- 
played in Table 3. IB1 is tested with leave-one- 
out cross-validation, IGTREE with ten-fold cross 
validation. 
The best results were (82.6%) for IB1 with 
the MVDM metric, and either no weighting or 
weighting by gain ratio. IGTREE did not per- 
form as well. We investigated more values of k, 
from 1 to 200, and found they had little influ- 
ence on the accuracy results with k = 4 or 5 
performing slightly better. 
We also tested each of the features described 
in Section 3 in isolation and then all together. 
We used the best performing algorithm from our 
earlier experiment: IB1 with MVDM, gain ratio 
and k = 4. The results of this are given in 
Table 4. 
When interpreting these results it is impor- 
tant to recall the figures provided in Table 1. 
The most common article, for any PoS, was no 
and for many PoS, including pronouns, gener- 
ating no article is always correct. There is more 
variation in NPs headed by common ouns and 
adjectives, and a little in NPs headed by proper 
nouns. Our baseline therefore consists of never 
3Strictly speaking, it is a k nearest distance algo- 
rithm, which looks at all examples in the nearest k dis- 
tances, the number of which may be greater than k. 
Feature Accuracy 
head 80.3% 
head's part-of-speech 70.0% 
NP's functional tag 70.5% 
embedding category 70.0% 
embedding functional tag 70.0% 
determiner present or not 70.0% 
head's countability 70.0% 
head's semantic lasses 72.9% 
hline 
Table 4: Accuracy results by feature 
generating an article: this will be right in 70.0% 
of all cases. 
Looking at the figures in Table 4, we see that 
many of the features investigated id not im- 
prove results above the baseline. Using the head 
of the NP itself to predict the article gave the 
best results of any single feature, raising the ac- 
curacy to 79.4%. The functional tag of the head 
of the NP itself improved results slightly. The 
use of the semantic lasses (72.1%) clearly im- 
proves the results over the baseline thereby indi- 
cating that they capture useful generalizations. 
The results from testing the features in com- 
bination are shown in Table 5. Interestingly, 
features which were not useful on their own, 
proved useful in combination with the head 
noun. The most useful features appear to be the 
category of the embedding constituent (81.1%) 
and the presence or absence of a determiner 
(80.9%). Combining all the features gave an 
accuracy of 82.9%. 
Feature Accuracy 
head+its part-of-speech 80.8% 
head+functional t g of NP 81.1% 
head+embedding category 80.8% 
head+embedding functional tag 81.4% 
head+determiner present or not 81.7% 
head+countability 80.8% 
head+semantic classes 80.8% 
hline all features 83.6% 
all features-semantic classes 83.6% 
Table 5: Accuracy with combined features 
Our best results (82.6%), which used all fea- 
tures are significantly better than the baseline 
of generating no articles (70.0%) or using only 
the head of the NP for training (79.4%). We 
46 
Algor i thm 
k 
Feature  Weight ing  
None Gain ratio Information gain X 2 Shared variance 
IB1 1 83.5% 83.5% 83.3% 83.2% 83.3% 
(MVDM) 4 83.5% 83.6% 83.3% 83.3% 83.3% 
16 83.6% 83.5% 83.2% 83.2% 83.2% 
IB1 1 83.1% 83.5% 83.3% 83.2% 83.3% 
(overlap) 4 82.9% 83.1% 83.1% 83.1% 83.1% 
16 82.9% 83.0% 82.9% 82.9% 82.9% 
IGTREE - -  - -  82.9% 82.5% 82.4% 82.6% 
Table 3: Accuracy results broken down with respect o memory-based learning methods used 
also improve significantly upon earlier esults of 
78% as reported by Knight and Chander (1994), 
which in any case is a simpler task since it only 
involved choice between the and alan. Further, 
our results are competitive with state of the art 
rule-based systems. Because different corpora 
are used to obtain the various results reported 
in the literature and the problem is often de- 
fined differently, detailed comparison is difficult. 
However, the accuracy achieved appears to ap- 
proach the accuracy results achieved with hand- 
written rules. 
In order to test the effect of the size of the 
training data, we tested used the best perform- 
ing algorithm from our earlier experiment (IB1 
with MVDM, gain ratio and k = 4) on various 
subsets of the corpus: the first 10%, the first 
20%, the first 30% and so on to the whole cor- 
pus. The results are given in Table 6. 
Size Accuracy  
10% 80.95% 
20% 81.67% 
30% 82.14% 
4O% 82.45% 
50% 82.69% 
60% 83.04% 
70% 83.17% 
80% 83.24% 
90% 83.45% 
100% 83.58% 
(100% is 300,744 NPs) 
Table 6: Accuracy versus Size of Training Data 
The accuracy is still improving even with 
300,744 NPs, an even larger corpus should give 
even better results. It is important to keep in 
mind that we, like most other researchers, have 
been training and testing on a relatively homo- 
geneous corpus. Furthermore, we took as given 
information about the number of the NP. In 
many applications we will have neither a large 
amount of homogeneous training data nor infor- 
mation about number. 
5.1 Future  Work  
In the near future we intend to further ex- 
tend our approach in various directions. First, 
we plan to investigate other lexical and syn- 
tactic features that might further improve our 
results, such as the existence of pre-modifiers 
like superlative and comparative adjectives, and 
post-modifiers like prepositional phrases, rela- 
tive clauses, and so on. We would also like to in- 
vestigate the effect of additional discourse-based 
features uch as one that incorporates informa- 
tion about whether the referent of a noun phrase 
has been mentioned before. 
Second, we intend to make sure that the fea- 
tures we are using in training and testing will 
be available in the applications we consider. For 
example, in machine translation, the input noun 
phrase may be all dogs, whereas the output 
could be either all dogs or all the dogs. At 
present, words such as all, both, half in our in- 
put are tagged as pre-determiners if there is a 
following determiner (it can only be the or a 
possessive), and determiners if there is no arti- 
cle. To train for a realistic application we need 
to collapse the determiner and pre-determiner 
inputs together in our training data. 
Furthermore, we are interested in training 
on corpora with less markup, like the British 
National Corpus (Burnard, 1995) or even no 
markup at all. By running a PoS tagger and 
then an NP chunker, we should be able to get 
a lot more training data, and thus significantly 
improve our coverage. If we can use plain text 
47 
to train on, then it will be easier to adapt our 
tool quickly to new domains, for which there are 
unlikely to be fully marked up corpora. 
6 Conc lud ing  remarks  
We described a memory-based approach to au- 
tomated article generation that uses a variety of 
lexical, syntactic and semantic features as pro- 
vided by the Penn Treebank Wall Street Jour- 
nal data and a large hand-encoded MT dictio- 
nary. With this approach we achieve an accu- 
racy of 82.6%. We believe that this approach 
is an encouraging first step towards a statistical 
device for automated article generation that can 
be used in a range of applications uch as speech 
prosthesis, machine translation and automated 
summarization. 
Acknowledgments  
The authors would like to thank the Stanford NLP 
reading group, the LinGO project at CSLI, Timo- 
thy Baldwin, Kevin Knight, Chris Manning, Wal- 
ter Daelemans and two anonymous reviewers for 
their helpful comments. This project is in part sup- 
ported by the National Science Foundation under 
grant number IRI-9612682. 
References  
Ann Bies, Mark Fergusona, Karen Katz, and Robert 
MacIntyre, 1995. Bracketing Guidelines for Tree- 
bank H Style. Penn Treebank Project, University 
of Pennsylvania. 
Francis Bond and Kentaro Ogura. 1998. Reference 
in Japanese-to-English machine translation. Ma- 
chine Translation, 13(2-3):107-134. 
Francis Bond, Kentaro Ogura, and Satoru Ikehara. 
1994. Countability and number in Japanese-to- 
English machine translation. In 15th Interna- 
tional Conference on Computational Linguistics: 
COLING-94, pages 32-38, Kyoto. (http: / /xxx.  
lanl. gov/abs/cmp- ig/951 i001). 
Lou Burnard. 1995. User reference guide for the 
British National Corpus. Technical report, Ox- 
ford University Computing Services. 
John Carroll, Ann Copestake, Dan Flickinger, and 
Victor Poznanski. 1999. An efficient chart gen- 
erator for (semi-)lexicalist grammars. In Proceed- 
ings o/ the 7th European Workshop on Natural 
Language Generation (EWNLG'99), pages 86-95, 
Toulouse, France. 
Ann Copestake. 1997. Augmented and alternative 
NLP techniques for augmentative and alternative 
communication. In Proceedings of the ACL work- 
shop on Natural Language Processing for Commu- 
nication Aids, pages 37-42, Madrid. 
Walter Daelemans, Antal van den Bosch, and Jakub 
Zavrel. 1999. Forgetting exceptions i harmful in 
language learning. Machine Learning, 34. 
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, 
and Antal van den Bosch. 2000. TiMBL: Tilburg 
memory based learner, version 3.0, reference 
guide. ILK Technical Report 00-01, ILK, Tilburg, 
The Netherlands. (ILK-0001; h t tp : / / i l k .kub .  
nl). 
Barbara Gawrofiska. 1990. "Translation Great 
Problem" on the problem of inserting articles 
when translating from Russian into Swedish. In 
13th International Conference on Computational 
Linguistics: COLING-90, Helsinki. 
Julia E. Heine. 1998. Definiteness predictions for 
Japanese noun phrases. In 36th Annual Meeting 
o\] the Association \]or Computational Linguistics 
and 17th International Conference on Computa- 
tional Linguistics: COLING/A CL-98, pages 519- 
525, Montreal, Canada. 
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hi- 
romi Nakaiwa. 1991. Toward an MT system with- 
out pre-editing - effects of new methods in ALT- 
J /E - .  In Third Machine Translation Summit: 
MT Summit III, pages 101-106, Washington DC. 
(http ://xxx. lanl. gov/abs/cmp- ig/9510008). 
Kevin Knight and Ishwar Chander. 1994. Au- 
tomated postediting of documents. In Proceed- 
ings of the 12th National Conference on Artificial 
Intelligence: AAAI-9~, pages 779-784, Seattle. 
(http ://xxx. lanl. gov/abs/cmp-ig/9407028). 
Guido Minnen, John Carroll, and Darren Pearce. 
2000. Robust, applied morphological generation. 
In Proceedings of the first International Natural 
Language Genration Conference, Mitzpe Ramon, 
Israel. 
Masaki Murata and Makoto Nagao. 1993. Deter- 
mination of referential property and number of 
nouns in Japanese sentences for machine transla- 
tion into English. In Fifth International Confer- 
ence on Theoretical and Methodological Issues in 
Machine Translation: TMI-93, pages 218-25, Ky- 
oto, July. (ht tp : / /xxx.  laa l .  gov/abs/cmp-lg/ 
9405019). 
Beatrice Santorini. 1990. Part-of-speech tagging 
guidelines for the Penn Treebank Project. Tech- 
nical Report MS-CIS-90-47, Department of Com- 
puter and Information Science, University of 
Pennsylvania. 
Jakub Zavrel and Walter Daelemans. 1997. 
Memory-based learning: Using similarity for 
smoothing. In Proceedings of the 35th Annual 
Meeting of the Association for Computational 
Linguistics, Madrid, Spain. 
48 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 649?656
Manchester, August 2008
Semantic Classification with Distributional Kernels
Diarmuid
?
O S
?
eaghdha
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
do242@cl.cam.ac.uk
Ann Copestake
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
aac10@cl.cam.ac.uk
Abstract
Distributional measures of lexical similar-
ity and kernel methods for classification
are well-known tools in Natural Language
Processing. We bring these two meth-
ods together by introducing distributional
kernels that compare co-occurrence prob-
ability distributions. We demonstrate the
effectiveness of these kernels by present-
ing state-of-the-art results on datasets for
three semantic classification: compound
noun interpretation, identification of se-
mantic relations between nominals and se-
mantic classification of verbs. Finally, we
consider explanations for the impressive
performance of distributional kernels and
sketch some promising generalisations.
1 Introduction
This paper draws a connection between two well-
known topics in statistical Natural Language Pro-
cessing: distributional measures of lexical simi-
larity and kernel methods for classification. Dis-
tributional similarity measures quantify the sim-
ilarity between pairs of words through their ob-
served co-occurrences with other words in corpus
data. The kernel functions used in support vec-
tor machine classifiers also allow an interpretation
as similarity measures; however, not all similar-
ity measures can be used as kernels. In particu-
lar, kernel functions must satisfy the mathemati-
cal property of positive semi-definiteness. In Sec-
tion 2 we consider kernel functions suitable for
comparing co-occurrence probability distributions
and show that these kernels are closely related to
measures known from the distributional similarity
literature. We apply these distributional kernels
?2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/
by-nc-sa/3.0/). Some rights reserved.
to three semantic classification tasks: compound
noun interpretation, identification of semantic re-
lations between nominals and semantic classifica-
tion of verbs. In all cases, the distributional ker-
nels outperform the linear and Gaussian kernels
standardly used for SVM classification and fur-
thermore achieve state-of-the-art results. In Sec-
tion 4 we provide a concrete explanation for the
superior performance of distributional kernels, and
in Section 5 we outline some promising directions
for future research.
2 Theory
2.1 Distributional Similarity Measures
Distributional approaches to lexical similarity as-
sume that words appearing in similar contexts are
likely to have similar or related meanings. To
measure distributional similarity, we use a repre-
sentation of words based on observation of their
relations with other words. Specifically, a target
word w is represented in terms of a set C of ad-
missible co-occurrence types c = (r, w
?
), where
the word w
?
belongs to a co-occurrence vocab-
ulary V
c
and r is a relation that holds between
w and w
?
. Co-occurrence relations may be syn-
tactic (e.g., verb-argument, conjunct-conjunct) or
may simply be one of proximity in text. Counts
f(w, c) of a target word w?s co-occurrences can
be estimated from language corpora, and these
counts can be weighted in a variety of ways to re-
flect prior knowledge or to reduce statistical noise.
A simple weighting method is to represent each
word w as a vector of co-occurrence probabilities
(P (c
1
|w), . . . , P (c
|C|
|w)). This vector defines the
parameters of a categorical or multinomial proba-
bility distribution, giving a useful probabilistic in-
terpretation of the distributional model. As the
vector for each target word must sum to 1, the
marginal distributions of target words have little
effect on the resulting similarity estimates. Many
649
similarity measures and weighting functions have
been proposed for distributional vectors; compara-
tive studies include Lee (1999), Curran (2003) and
Weeds and Weir (2005).
2.2 Kernel Methods for Computing
Similarity and Distance
In this section we describe two classes of func-
tions, positive semi-definite and negative semi-
definite kernels, and state some relationships be-
tween these classes. The mathematical treatment
follows Berg et al (1984). A good general intro-
duction to kernels and support vector machines is
the book by Cristianini and Shawe-Taylor (2000).
Let X be a set of items and let k : X ? X ?
R be a symmetric real-valued function on pairs of
items in X . Then k is a positive semi-definite (psd)
kernel if for all finite n-element sets X ? X , the
n? n Gram matrix K defined by K
ij
= k(x
i
, x
j
)
satisfies the property
v
?
Kv ? 0, ?v ? R
n
(1)
This is equivalent to requiring that k define an in-
ner product in a Hilbert space F which may be the
same as X or may differ in dimensionality or in
type (F is by definition a vector space, but X need
not be). An intuitive interpretation of psd kernels is
that they provide a similarity measure on members
ofX based on an embedding ? from input spaceX
into feature space F . It can be shown that a func-
tion is psd if and only if all Gram matrices K have
no negative eigenvalues.
Kernel functions have received significant atten-
tion in recent years through their applications in
machine learning, most notably support vector ma-
chines (SVMs, Cortes and Vapnik (1995)). SVM
classifiers learn a decision boundary between two
data classes that maximises the minimum distance
or margin from the training points in each class to
the boundary. The notion of distance used and the
feature space in which the boundary is set are de-
termined by the choice of kernel function. So long
as the kernel satisfies (1), the SVM optimisation
algorithm is guaranteed to converge to a global op-
timum that affords the geometric interpretation of
margin maximisation. Besides these desirable op-
timisation properties, kernel methods have the ad-
vantage that the choice of kernel can be based on
prior knowledge about the problem and on the na-
ture of the data.
A negative semi-definite (nsd) kernel is a sym-
metric function
?
k : X ? X ? R such that for all
finite n-element sets X ? X and for all vectors
v = (v
1
, . . . , v
n
) ? R
n
with
?
i
v
i
= 0
v
?
?
Kv ? 0 (2)
Whereas positive semi-definite kernels correspond
to inner products in a Hilbert space F , negative
semi-definite kernels correspond to squared dis-
tances. In particular, if
?
k(x, y) = 0 only when
x = y then
?
?
k is a metric. If a function k is
psd, then ?k is always nsd, but the converse does
not hold.
1
However, Berg et al (1984) describe
two simple methods for inducing a positive semi-
definite function k from negative semi-definite
?
k:
k(x, y) =
?
k(x, x
0
) +
?
k(y, x
0
)?
?
k(x, y)
?
?
k(x
0
, x
0
), ?x
0
? X (3a)
k(x, y) = exp(??
?
k(x, y)), ?? > 0 (3b)
The point x
0
in (3a) can be viewed as providing
an origin in F that is the image of some point in
the input space X ; the choice of x
0
does not have
an effect on SVM classification. A familiar exam-
ple of these transformations arises if we take
?
k to
be the squared Euclidean L
2
distance ?x ? y?
2
=
?
i
(x
i
? y
i
)
2
. Applying (3a) and setting x
0
to
be the zero vector, we obtain a quantity that is
twice the linear kernel k(x, y) =
?
i
x
i
y
i
. Apply-
ing (3b) we derive the Gaussian kernel k(x, y) =
exp(???x?y?
2
). In the next section we consider
kernels obtained by plugging alternative squared
metrics into equations (3a) and (3b).
2.3 Distributional Kernels
Given the effectiveness of distributional similarity
measures for numerous tasks in NLP and the in-
terpretation of kernels as similarity functions, it
seems natural to consider the use of kernels tai-
lored for co-occurrence distributions when per-
forming semantic classification. As shown in Sec-
tion 2.2 the standardly used linear and Gaussian
kernels derive from theL
2
distance, yet Lee (1999)
has shown that this distance measure is relatively
poor at comparing co-occurrence distributions. In-
formation theory provides a number of alterna-
tive distance functions on probability measures, of
which the L
1
distance (also called variational dis-
tance), Kullback-Leibler divergence and Jensen-
Shannon divergence are well-known in NLP and
1
Negated nsd functions are sometimes called condition-
ally psd; they constitute a superset of the psd functions.
650
Distance Definition Derived linear kernel
(L
2
distance)
2
?
c
(P (c|w
1
)? P (c|w
2
))
2
?
c
P (c|w
1
)P (c|w
2
)
L
1
distance
?
c
|P (c|w
1
)? P (c|w
2
)|
?
c
min(P (c|w
1
), P (c|w
2
))
Jensen-Shannon
?
c
P (c|w
1
) log
2
(
2P (c|w
1
)
P (c|w
1
)+P (c|w
2
)
) + ?
?
c
P (c|w
1
) log
2
(
P (c|w
1
)
P (c|w
1
)+P (c|w
2
)
) +
divergence P (c|w
2
) log
2
(
2P (c|w
2
)
P (c|w
1
)+P (c|w
2
)
) P (c|w
2
) log
2
(
P (c|w
2
)
P (c|w
1
)+P (c|w
2
)
)
Hellinger distance
?
c
(
?
P (c|w
1
)?
?
P (c|w
2
))
2
?
c
?
P (c|w
1
)P (c|w
2
)
Table 1: Squared metric distances on co-occurrence distributions and corresponding linear kernels
were shown by Lee to give better similarity esti-
mates than the L
2
distance.
In Section 2.2 we have seen how to derive psd
kernels (similarities) from nsd kernels (distances).
It seems likely that distance measures that are
known to work well for comparing co-occurrence
distributions will also give us suitable psd similar-
ity measures. Negative semi-definite kernels are
by definition symmetric, which rules the Kullback-
Leibler divergence and Lee?s (1999) ?-skew diver-
gence out of consideration. The nsd condition (2)
is met if the distance function is a squared metric in
a Hilbert space. In this paper we use a parametric
family of squared Hilbertian metrics on probability
distributions that has been discussed by Hein and
Bousquet (2005). This family contains many fa-
miliar distances including the L
1
distance, Jensen-
Shannon divergence (JSD) and the Hellinger dis-
tance used in statistics, though not the squared L
2
distance. Positive semi-definite distributional ker-
nels can be derived from these distances through
equations (3a) and (3b). We interpret the distribu-
tional kernels produced by (3a) and (3b) as ana-
logues of the linear and Gaussian kernels respec-
tively, given by a different norm or concept of dis-
tance in the feature space F . Hence the linear dis-
tributional kernels produced by (3a) correspond to
inner products in the input space X , and the rbf
distributional kernels produced by (3b) are radial
basis functions corresponding to inner products
in a high-dimensional Hilbert space of Gaussian-
like functions. In this paper we use the unmodi-
fied term ?linear kernel? in the standard sense of
the linear kernel derived from the L
2
distance and
make explicit the related distance when referring to
other linear kernels, e.g., the ?JSD linear kernel?.
Likewise, we use the standard term ?Gaussian? to
refer to the L
2
rbf kernel, and denote other rbf ker-
nels as, for example, the ?JSD rbf kernel?.
Table 1 lists relevant squared metric distances
and their derived linear kernels. The linear ker-
nel derived from the L
1
distance is the same as the
difference-weighted token-based similarity mea-
sure of Weeds and Weir (2005). The JSD linear
kernel can be rewritten as (2 - JSD), where JSD
is the value of the Jensen-Shannon divergence.
This formulation is used as a similarity measure
by Lin (1999). Dagan et al (1999) use a similar-
ity measure 10
??JSD
, though they acknowledge
that this transformation is heuristically motivated.
The rbf kernel exp(??JSD) provides a theoret-
ically sound alternative when the psd property is
required. It follows from the above discussion
that these previously known distributional similar-
ity measures are valid kernel functions and can be
used directly for SVM classification.
Finally, we consider the status of other popular
distributional measures. The familiar cosine sim-
ilarity measure is provably a valid psd kernel, as
it is the L
2
linear kernel calculated between L
2
-
normalised vectors. Distributional vectors are by
definition L
1
-normalised (they sum to 1), but there
is evidence that L
2
normalisation is optimal when
using L
2
kernels for tasks such as text categori-
sation (Leopold and Kindermann, 2002). Indeed,
in the experiments described below L
2
-normalised
feature vectors are used with the L
2
kernels, and
the L
2
linear kernel function then becomes identi-
cal to the cosine similarity. Other similarity mea-
sures, such as that of Lin (1998), can be shown to
be non-psd by calculating similarity matrices from
real or artificial data and showing that their non-
zero eigenvalues are not all positive, as is required
by psd functions.
651
3 Practice
3.1 General Methodology
All experiments were performed using the LIB-
SVM Support Vector Machine library (Chang and
Lin, 2001), modified to implement one-against-
all classification. The members of the distribu-
tional kernel family all performed similarly but the
Jensen-Shannon divergence kernels gave the most
consistently impressive results, and we restrict dis-
cussion to these kernels due to considerations of
space and clarity. In each experiment we com-
pare the standard linear and Gaussian kernels with
the linear and JSD rbf kernels. As a preprocess-
ing step for the L
2
kernels, each feature vector
was normalised to have unit L
2
norm. For the
Jensen-Shannon kernels, the feature vectors were
normalised to have unit L
1
norm, i.e., to define
a probability distribution. For all datasets and all
training-test splits the SVM cost parameter C was
optimised in the range (2
?6
, 2
?4
, . . . , 2
12
) through
cross-validation on the training set. In addition, the
width parameter ? was optimised in the same way
for the rbf kernels. The number of optimisation
folds differed according to the size of the dataset
and the number of training-test splits to be eval-
uated: we used 10 folds for the compound task,
leave-one-out cross-validation for SemEval Task 4
and 25 folds for the verb classification task.
3.2 Compound Noun Interpretation
The task of interpreting the semantics of noun
compounds is one which has recently received
considerable attention (Lauer, 1995; Girju et al,
2005; Turney, 2006). For a given noun-noun com-
pound, the problem is to identify the semantic re-
lation between the compound?s constituents ? that
a kitchen knife is a knife used in a kitchen but a
steel knife is a knife made of steel.
2
The difficulty
of the task is due to the fact that the knowledge re-
quired to interpret compounds is not made explicit
in the contexts where they appear, and hence stan-
dard context-based methods for classifying seman-
tic relations in text cannot be applied. Most previ-
ous work making use of lexical similarity has been
based on WordNet measures (Kim and Baldwin,
2005; Girju et al, 2005).
?
O S?eaghdha and Copes-
take (2007) were to our knowledge the first to ap-
ply a distributional model. Here we build on their
2
In the classification scheme considered here, kitchen
knife would have the label IN and steel knife would be la-
belled BE.
methodology by introducing a probabilistic feature
weighting scheme and applying the new distribu-
tional kernels.
For our experiments we used the dataset of
?
O S?eaghdha and Copestake (2007), which con-
sists of 1443 noun compounds annotated with
six semantic relations: BE, HAVE, IN, AGENT,
INSTRUMENT and ABOUT.
3
The classification
baseline associated with always choosing the
most frequent relation (IN) is 21.3%. For
each compound (N
1
, N
2
) in the dataset, we
associate the co-occurrence probability vector
(P (c
1
|N
1
), . . . , P (c
|C|
|N
1
)) with N
1
and the vec-
tor (P (c
1
|N
2
), . . . , P (c
|C|
|N
2
)) with N
2
. The
probability vector for the compound is created
by appending the two constituent vectors, each
scaled by 0.5 to weight both constituents equally
and ensure that the new vector sums to 1.
These probability vectors are used to compute
the Jensen-Shannon kernel values. The pre-
processing step for the L
2
kernels is analogous,
except that the co-occurrence frequency vector
(f(c
1
, N
i
), . . . , f(c
|C|
, N
i
)) for each constituent
N
i
is normalised to have unit L
2
norm (instead
of unit L
1
norm); the combined feature vector for
each data item is also L
2
-normalised.
4
The co-occurrence relation we counted to esti-
mate the probability vectors was the conjunction
relation. This relation gives sparse but high-quality
information, and was shown to be effective by
?
O
S?eaghdha and Copestake. We extracted two fea-
ture sets from two very different corpora. The
first is the 90 million word written component of
the British National Corpus (Burnard, 1995). This
corpus was parsed with the RASP parser (Briscoe
et al, 2006) and all instances of the conj gram-
matical relation were counted. The co-occurrence
vocabulary V
c
was set to the 10,000 words most
frequently entering into a conj relation across
the corpus. The second corpus we used was the
Web 1T 5-Gram Corpus (Brants and Franz, 2006),
which contains frequency counts for n-grams up
to length 5 extracted from Google?s index of ap-
proximately 1 trillion words of Web text. As the
nature of this corpus precludes parsing, we used a
simple pattern-based technique to extract conjunc-
tions. An n-gram was judged to contain a conjunc-
tion co-occurrence between N
i
and N
j
if it con-
3
This dataset is available from http://www.cl.cam.
ac.uk/
?
do242/resources.html.
4
The importance of performing both normalisation steps
was suggested to us by an anonymous reviewer?s comments.
652
BNC 5-Gram
Kernel Acc F Acc F
Linear 57.9 55.8 55.0 52.5
Gaussian 58.0 56.2 53.5 50.8
JSD (linear) 59.9 57.8 60.2 58.1
JSD (rbf) 59.8 57.9 61.0 58.8
Table 2: Results for compound interpretation
tained the patternN
i
and (?N)*N
j
(?N)*. A noun
dictionary automatically constructed from Word-
Net and an electronic version of Webster?s 1913
Unabridged Dictionary determined the sets of ad-
missible nouns {N} and non-nouns {?N}.
5
The
vocabulary V
c
was again set to the 10,000 most
frequent conjuncts, and the probability estimates
P (c|w) were based on the n-gram frequencies for
each n-gram matching the extraction pattern. A
third feature set extracted from the 5-Gram Corpus
by using a larger set of joining terms was also stud-
ied but the results were not significantly different
from the sparser conjunction feature sets and are
not presented here.
Performance was measured by splitting the data
into five folds and performing cross-validation.
Results for the two feature sets and four kernels
are presented in Table 2. The kernels derived from
the Jensen-Shannon divergence clearly outperform
the L
2
distance-based linear and Gaussian kernels
in both accuracy and macro-averaged F-score. The
best performing kernel-feature combination is the
Jensen-Shannon rbf kernel with the 5-Gram fea-
tures, which attains 61.0% accuracy and 58.8%
F-score. This surpasses the best previous result
of 57.1% accuracy, 55.3% F-score that was re-
ported by
?
O S?eaghdha and Copestake (2007) for
this dataset. That result was obtained by combin-
ing a distributional model with a relational simi-
larity model based on string kernels; incorporating
relational similarity into the system described here
improves performance even further (
?
O S?eaghdha,
2008).
3.3 SemEval Task 4
Task 4 at the 2007 SemEval competition (Girju et
al., 2007) focused on the identification of seman-
tic relations among nominals in text. Identifica-
tion of each of seven relations was designed as
a binary classification task with 140 training sen-
5
The electronic version of Webster?s is available from
http://msowww.anu.edu.au/
?
ralph/OPTED/.
BNC 5-Gram
Kernel Acc F Acc F
Linear 67.6 57.1 65.4 63.3
Gaussian 66.8 60.7 65.6 62.9
JSD (linear) 71.4 68.8 69.6 65.8
JSD (rbf) 69.9 66.7 70.7 67.5
Table 3: Results for SemEval Task 4
tences and around 70 test sentences.
6
To ensure
that the task be a challenging one, the negative
test examples were all ?near misses? in that they
were plausible candidates for the relation to hold
but failed to meet one of the criteria for that rela-
tion. This was achieved by selecting both positive
and negative examples from the results of the same
targeted Google queries. The majority-class base-
line for this task gives Accuracy = 57.0%, F-score
= 30.8%, while the all-true baseline (label every
test sentence positive) gives Accuracy = 48.5%, F-
score = 64.8%.
We used the same feature sets and kernels as in
Section 3.2. The results are presented in Table 3.
Again, the JSD kernels outperform the standard
L
2
kernels by a considerable margin. The best
performing feature-kernel combination achieves
71.4% Accuracy and 68.8% F-score, higher than
the best performance attained in the SemEval com-
petition without using WordNet similarity mea-
sures (Accuracy = 67.0%, F-score = 65.1%; Nakov
and Hearst (2007)). This is also higher than the
performance of all but three of the 14 SemEval en-
tries which did use WordNet. Davidov and Rap-
poport (2008) have recently described a WordNet-
free method that attains slightly lower accuracy
(70.1%) and slightly higher F-score (70.6%) than
our method. Taken together, Davidov and Rap-
poport?s results and ours define the current state
of the art on this task.
3.4 Verb Classification
To investigate the effectiveness of distributional
kernels on a different kind of semantic classifi-
cation task, we tested our methods on the verb
class data of Sun et al (2008). This dataset con-
sists of 204 verbs assigned to 17 of Levin?s (1993)
verb classes. Each verb is represented by a set
of features corresponding to the distribution of its
instances across subcategorisation frames (SCFs).
6
The relations are Cause-Effect, Instrument-Agency,
Product-Producer, Origin-Entity, Theme-Tool, Part-Whole
and Content-Container.
653
FS3 FS5
Kernel Acc F Acc F
Linear 67.1 65.5 67.6 65.9
Gaussian 60.8 58.6 62.7 60.2
JSD (linear) 70.6 67.3 69.6 66.4
JSD (rbf) 68.6 65.1 70.1 67.2
Sun et al (SVM) 57.8 58.2 57.3 57.4
Sun et al (GS) 59.3 57.1 64.2 62.5
Table 4: Results for leave-one-out verb classifica-
tion and comparison with Sun et al?s (2008) SVM
and Gaussian fitting methods
These frames include information about syntac-
tic constituents (NP, NP NP, NP SCOMP, . . . )
and some lexical information about subcategorised
prepositions (NP with, out, . . . ). The feature val-
ues are counts of SCFs extracted from a large cor-
pus. As the feature vector for each verb natu-
rally defines a probability distribution over SCFs,
it seems intuitive to apply distributional kernels to
the problem of predicting Levin classes for verbs.
Sun et al use multiple feature sets of varying
sparsity and noisiness. We report results on the
two feature sets for which they reported best per-
formance; for continuity we keep the names FS3
and FS5 for these feature sets. These were de-
rived from the least filtered and hence least sparse
subcategorisation lexicon (which they call VALEX
1) and differ in the granularity of prepositional
SCFs. The SCF representation in FS5 is richer
and hence potentially more discriminative, but it is
also sparser. Using an SVM with a Gaussian ker-
nel, Sun et al achieved their best results on FS3.
Perhaps surprisingly, their best results overall were
attained with FS5 by a simple method based on
fitting multivariate Gaussian distributions to each
class in the training data and assigning the maxi-
mum likelihood class to test points.
Following Sun et al, we use a leave-one-out
measure of verb classification performance. As
the examples are distributed equally across the 17
classes, the random baseline accuracy is 5.9%. Ta-
ble 4 presents our results with L
2
and JSD kernels,
as well as those of Sun et al The best overall
performance is attained by the JSD linear kernel,
which scores higher than the L
2
-derived kernels
on both feature sets. The L
2
linear kernel also per-
forms quite well and with consistency. The JSD
rbf kernel was less consistent over cross-validation
runs, seemingly due to uncertainty in selecting the
optimal ? parameter value; it clearly outperforms
the L
2
linear kernel on one feature set (FS5) but
on the other (FS3) it attains a slightly lower F-
score while maintaining a higher accuracy. The
Gaussian kernel seems particularly ill-suited to this
dataset, performing significantly worse than the
other kernels. The difference between Sun et al?s
results with the Gaussian kernel and ours with the
same kernel may be due to the use of one-against-
all classification here instead of one-against-one,
or it may be due to differences in preprocessing or
parameter optimisation.
4 The effect of marginal distributions
It is natural to ask why distributional kernels per-
form better than the standard linear and Gaussian
kernels. One answer might be that just as infor-
mation theory provides the ?correct? notion of in-
formation for many purposes, it also provides the
?correct? notion of distance between probability
distributions. Hein and Bousquet (2005) show that
their family of distributional kernels are invariant
to bijective transformations of the event space C
and suggest that this property is a valuable one for
image histogram classification where data may be
represented in a range of equivalent colour spaces.
However, it is not clear that this confers an advan-
tage when comparing lexical co-occurrence distri-
butions; when transformations are performed on
the space of co-occurrence types, they are gener-
ally not information-conserving, for example lem-
matisation or stemming.
A more practical explanation is that the distri-
butional kernels and distances are less sensitive
than the (squared) L
2
distance and its derived ker-
nels to the marginal frequencies of co-occurrence
types. When a type c has high frequency we expect
that it will have higher variance, i.e., the differ-
ences |P (c|w
1
)? P (c|w
2
)| will tend to be greater
even if c is not a more important signifier of simi-
larity.
7
These differences contribute quadratically
to the L
2
distance and hence also to the associ-
ated rbf kernel, i.e., the Gaussian kernel. It is also
easy to see that types c for which P (c|w
i
) tends
to be large will dominate the value of the linear
kernel. This explanation is also plausibly a fac-
tor in the relatively poor performance of L
2
dis-
tance as a lexical dissimilarity measure, as demon-
7
Chapelle et al (1999) give a similar explanation for the
performance of a related family of kernels on a histogram
classification task.
654
strated by Lee (1999). In contrast, the differences
|P (c|w
1
)?P (c|w
2
)| are not squared in the L
1
dis-
tance formula, and the minimum function in theL
1
linear kernel dampens the effect of high-variance
co-occurrence types. The Jensen-Shannon formula
is more difficult to interpret, as the difference terms
do not directly appear. While co-occurrence types
with large P (c|w
1
) and P (c|w
2
) do contribute
more to the distance and kernel values, it is the
proportional size of the difference that appears in
the log term rather than its magnitude. Finally, the
Hellinger distance and kernels squash the variance
associated with c through the square root function.
5 Discussion and Future Directions
Kernels on probability measures have been dis-
cussed in the machine learning literature (Kondor
and Jebara, 2003; Cuturi et al, 2005; Hein and
Bousquet, 2005), but they have previously been
applied only to standard image and text classifi-
cation benchmark tasks. We seem to be the first to
use distributional kernels for semantic classifica-
tion and to note their connection with familiar lex-
ical similarity measures. Indeed, the only research
we are aware of on kernels tailored for lexical sim-
ilarity is the small body of work on WordNet ker-
nels, e.g., Basili et al (2006). In contrast, Sup-
port Vector Machines have been widely adopted
for computational semantic tasks, from word sense
disambiguation (Gliozzo et al, 2005) to semantic
role labelling (Pradhan et al, 2004). The standard
feature sets for semantic role labelling and many
other tasks are collections of heterogeneous fea-
tures that do not correspond to probability distri-
butions. So long as the features are restricted to
positive values, distributional kernels can be ap-
plied; it will be interesting (and informative) to see
whether they retain their superiority in this setting.
One advantage of kernel methods is that kernels
can be defined for non-vectorial data structures
such as strings, trees, graphs and sets. A promis-
ing topic of future research is the design of distri-
butional kernels for comparing structured objects,
based on the feature space embedding associated
with convolution kernels (Haussler, 1999). These
kernels map structures in X into a space whose di-
mensions correspond to substructures of the ele-
ments of X . Thus strings are mapped onto vec-
tors of substring counts, and trees are mapped onto
vectors of subtree counts. We adopt the perspec-
tive that this mapping represents structures x
i
? X
as measures over substructures x?
1
, . . . , x?
d
. Prop-
erly normalised, this gives a distributional proba-
bility vector (P (x?
1
), . . . , P (x?
d
)) similar to those
used for computing lexical similarity. This per-
spective motivates the use of distributional inner
products instead of the dot products implicitly used
in standard convolution kernels. Several authors
have suggested applying distributional similarity
measures to sentences and phrases for tasks such as
question answering (Lin and Pantel, 2001; Weeds
et al, 2005). Distributional kernels on strings and
trees should provide a flexible implementation of
these suggestions that is compatible with SVM
classification and does not require manual feature
engineering. Furthermore, there is a ready gener-
alisation to kernels on sets of structures; if a set
is represented as the normalised sum of its mem-
ber embeddings in feature space F , distributional
methods can be applied directly.
6 Conclusion
In this paper we have introduced distributional ker-
nels for classification with co-occurrence proba-
bility distributions. The suitability of distribu-
tional kernels for semantic classification is intu-
itive, given their relation to proven distributional
methods for computing semantic similarity, and in
practice they work very well. As these kernels
give state-of-the-art results on the three datasets we
have tested, we expect that they will prove useful
for a wide range of semantic classification prob-
lems in future.
Acknowledgements
We are grateful to Andreas Vlachos and three
anonymous reviewers for their useful comments,
and to Anna Korhonen for providing the verb clas-
sification dataset. This work was supported in part
by EPSRC Grant EP/C010035/1.
References
Basili, Roberto, Marco Cammisa, and Alessandro Mos-
chitti. 2006. A semantic kernel to classify texts with
very few training examples. Informatica, 30(2):163?
172.
Berg, Christian, Jens P. R. Christensen, and Paul Ressel.
1984. Harmonic Analysis on Semigroups: Theory of
Positive Definite and Related Functions. Springer,
Berlin.
Brants, Thorsten and Alex Franz, 2006. Web 1T 5-gram
Corpus Version 1.1. Linguistic Data Consortium.
655
Briscoe, Ted, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the ACL Interactive Presentation Sessions.
Burnard, Lou, 1995. Users? Guide for the British Na-
tional Corpus. British National Corpus Consortium,
Oxford University Computing Service.
Chang, Chih-Chung and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines. Software
available at http://www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Chapelle, Olivier, Patrick Haffner, and Vladimir N.
Vapnik. 1999. Support vector machines for
histogram-based image classification. IEEE Trans-
actions on Neural Networks, 10(5):1055?1064.
Cortes, Corinna and Vladimir Vapnik. 1995. Support
vector networks. Machine Learning, 20(3):273?297.
Cristianini, Nello and John Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines. Cambridge
University Press, Cambridge.
Curran, James. 2003. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Cuturi, Marco, Kenji Fukumizu, and Jean-Philippe
Vert. 2005. Semigroup kernels on measures. Jour-
nal of Machine Learning Research, 6:1169?1198.
Dagan, Ido, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence prob-
abilities. Machine Learning, 34(1?4):43?69.
Davidov, Dmitry and Ari Rappoport. 2008. Classifica-
tion of semantic relationships between nominals us-
ing pattern clusters. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics.
Girju, Roxana, Dan Moldovan, Marta Tatu, and
Daniel Antohe. 2005. On the semantics of
noun compounds. Computer Speech and Language,
19(4):479?496.
Girju, Roxana, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of seman-
tic relations between nominals. In Proceedings of
the 4th International Workshop on Semantic Evalua-
tions.
Gliozzo, Alfio, Claudio Giuliano, and Carlo Strappar-
ava. 2005. Domain kernels for word sense disam-
biguation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics.
Haussler, David. 1999. Convolution kernels on dis-
crete structures. Technical Report UCSC-CRL-99-
10, Computer Science Department, University of
California at Santa Cruz.
Hein, Matthias and Olivier Bousquet. 2005. Hilbertian
metrics and positive definite kernels on probability
measures. In Proceedings of the 10th International
Workshop on Artificial Intelligence and Statistics.
Kim, Su Nam and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing.
Kondor, Risi and Tony Jebara. 2003. A kernel between
sets of vectors. In Proceedings of the 20th Interna-
tional Conference on Machine Learning.
Lauer, Mark. 1995. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University.
Lee, Lillian. 1999. Measures of distributional similar-
ity. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Leopold, Edda and J?org Kindermann. 2002. Text cat-
egorization with support vector machines. how to
represent texts in input space? Machine Learning,
46(1?3):423?444.
Levin, Beth. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
Lin, Dekang and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Lin, Dekang. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning.
Nakov, Preslav I. and Marti A. Hearst. 2007. UCB:
System description for SemEval Task #4. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations.
?
O S?eaghdha, Diarmuid and Ann Copestake. 2007. Co-
occurrence contexts for noun compound interpreta-
tion. In Proceedings of the ACL Workshop on a
Broader Perspective on Multiword Expressions.
?
O S?eaghdha, Diarmuid. 2008. Learning Compound
Noun Semantics. Ph.D. thesis, Computer Labora-
tory, University of Cambridge. In preparation.
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2004. Support vector learning for semantic ar-
gument classification. Machine Learning, 60(1):11?
39.
Sun, Lin, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the 9th International Conference
on Intelligent Text Processing and Computational
Linguistics.
Turney, Peter D. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Weeds, Julie and David Weir. 2005. Co-occurrence
retrieval: A flexible framework for lexical dis-
tributional similarity. Computational Linguistics,
31(4):439?476.
Weeds, Julie, David Weir, and Bill Keller. 2005. The
distributional similarity of sub-parses. In Proceed-
ings of the ACL Workshop on Empirical Modeling of
Semantic Equivalence and Entailment.
656
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 1?9,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Slacker semantics: why superficiality, dependency and avoidance of
commitment can be the right way to go
Ann Copestake
Computer Laboratory, University of Cambridge
15 JJ Thomson Avenue, Cambridge, UK
aac@cl.cam.ac.uk
Abstract
This paper discusses computational com-
positional semantics from the perspective
of grammar engineering, in the light of ex-
perience with the use of Minimal Recur-
sion Semantics in DELPH-IN grammars.
The relationship between argument index-
ation and semantic role labelling is ex-
plored and a semantic dependency nota-
tion (DMRS) is introduced.
1 Introduction
The aim of this paper is to discuss work on com-
positional semantics from the perspective of gram-
mar engineering, which I will take here as the de-
velopment of (explicitly) linguistically-motivated
computational grammars. The paper was written
to accompany an invited talk: it is intended to pro-
vide background and further details for those parts
of the talk which are not covered in previous pub-
lications. It consists of an brief introduction to our
approach to computational compositional seman-
tics, followed by details of two contrasting topics
which illustrate the grammar engineering perspec-
tive. The first of these is argument indexing and its
relationship to semantic role labelling, the second
is semantic dependency structure.
Standard linguistic approaches to compositional
semantics require adaptation for use in broad-
coverage computational processing. Although
some of the adaptations are relatively trivial, oth-
ers have involved considerable experimentation by
various groups of computational linguists. Per-
haps the most important principle is that semantic
representations should be a good match for syn-
tax, in the sense of capturing all and only the in-
formation available from syntax and productive
morphology, while nevertheless abstracting over
semantically-irrelevant idiosyncratic detail. Com-
pared to much of the linguistics literature, our
analyses are relatively superficial, but this is essen-
tially because the broad-coverage computational
approach prevents us from over-committing on the
basis of the information available from the syntax.
One reflection of this are the formal techniques
for scope underspecification which have been de-
veloped in computational linguistics. The im-
plementational perspective, especially when com-
bined with a requirement that grammars can be
used for generation as well as parsing, also forces
attention to details which are routinely ignored in
theoretical linguistic studies. This is particularly
true when there are interactions between phenom-
ena which are generally studied separately. Fi-
nally, our need to produce usable systems disal-
lows some appeals to pragmatics, especially those
where analyses are radically underspecified to al-
low for syntactic and morphological effects found
only in highly marked contexts.1
In a less high-minded vein, sometimes it is right
to be a slacker: life (or at least, project funding) is
too short to implement all ideas within a grammar
in their full theoretical glory. Often there is an easy
alternative which conveys the necessary informa-
tion to a consumer of the semantic representations.
Without this, grammars would never stabilise.
Here I will concentrate on discussing work
which has used Minimal Recursion Semantics
(MRS: Copestake et al (2005)) or Robust Min-
imal Recursion Semantics (RMRS: Copestake
(2003)). The (R)MRS approach has been adopted
as a common framework for the DELPH-IN ini-
tiative (Deep Linguistic Processing with HPSG:
http://www.delph-in.net) and the work dis-
cussed here has been done by and in collaboration
with researchers involved in DELPH-IN.
The programme of developing computational
compositional semantics has a large number of
aspects. It is important that the semantics
has a logically-sound interpretation (e.g., Koller
and Lascarides (2009), Thater (2007)), is cross-
1For instance, we cannot afford to underspecify number
on nouns because of examples such as The hash browns is
getting angry (from Pollard and Sag (1994) p.85).
1
linguistically adequate (e.g., Bender (2008)) and
is compatible with generation (e.g., Carroll et al
(1999), Carroll and Oepen (2005)). Ideally, we
want support for shallow as well as deep syn-
tactic analysis (which was the reason for devel-
oping RMRS), enrichment by deeper analysis (in-
cluding lexical semantics and anaphora resolution,
both the subject of ongoing work), and (robust) in-
ference. The motivation for the development of
dependency-style representations (including De-
pendency MRS (DMRS) discussed in ?4) has been
to improve ease of use for consumers of the repre-
sentation and human annotators, as well as use in
statistical ranking of analyses/realisations (Fujita
et al (2007), Oepen and L?nning (2006)). Inte-
gration with distributional semantic techniques is
also of interest.
The belated ?introduction? to MRS in Copestake
et al (2005) primarily covered formal represen-
tation of complete utterances. Copestake (2007a)
described uses of (R)MRS in applications. Copes-
take et al (2001) and Copestake (2007b) concern
the algebra for composition. What I want to do
here is to concentrate on less abstract issues in
the syntax-semantics interface. I will discuss two
cases where the grammar engineering perspective
is important and where there are some conclusions
about compositional semantics which are relevant
beyond DELPH-IN. The first, argument indexing
(?3), is a relatively clear case in which the con-
straints imposed by grammar engineering have a
significant effect on choice between plausible al-
ternatives. I have chosen to talk about this both
because of its relationship with the currently pop-
ular task of semantic role labelling and because
the DELPH-IN approach is now fairly stable af-
ter a quite considerable degree of experimentation.
What I am reporting is thus a perspective on work
done primarily by Flickinger within the English
Resource Grammar (ERG: Flickinger (2000)) and
by Bender in the context of the Grammar Matrix
(Bender et al, 2002), though I?ve been involved in
many of the discussions. The second main topic
(?4) is new work on a semantic dependency rep-
resentation which can be derived from MRS, ex-
tending the previous work by Oepen (Oepen and
L?nning, 2006). Here, the motivation came from
an engineering perspective, but the nature of the
representation, and indeed the fact that it is possi-
ble at all, reveals some interesting aspects of se-
mantic composition in the grammars.
2 The MRS and RMRS languages
This paper concerns only representations which
are output by deep grammars, which use MRS, but
it will be convenient to talk in terms of RMRS and
to describe the RMRSs that are constructed under
those assumptions. Such RMRSs are interconvert-
ible with MRSs.2 The description is necessarily
terse and contains the minimal detail necessary to
follow the remainder of the paper.
An RMRS is a description of a set of trees cor-
responding to scoped logical forms. Fig 1 shows
an example of an RMRS and its corresponding
scoped form (only one for this example). RMRS
is a ?flat? representation, consisting of a bag of el-
ementary predications (EP), a set of argument
relations, and a set of constraints on the possi-
ble linkages of the EPs when the RMRS is resolved
to scoped form. Each EP has a predicate, a la-
bel and a unique anchor and may have a distin-
guished (ARG0) argument (EPs are written here as
label:anchor:pred(arg0)). Label sharing between
EPs indicates conjunction (e.g., in Fig 1, big, an-
gry and dog share the label l2). Argument relations
relate non-arg0 arguments to the corresponding EP
via the anchor. Argument names are taken from a
fixed set (discussed in ?3). Argument values may
be variables (e.g., e8, x4: variables are the only
possibility for values of ARG0), constants (strings
such as ?London?), or holes (e.g. h5), which in-
dicate scopal relationships. Variables have sortal
properties, indicating tense, number and so on, but
these are not relevant for this paper. Variables cor-
responding to unfilled (syntactically optional) ar-
guments are unique in the RMRS, but otherwise
variables must correspond to the ARG0 of an EP
(since I am only considering RMRSs from deep
grammars here).
Constraints on possible scopal relationships be-
tween EPs may be explicitly specified in the gram-
mar via relationships between holes and labels. In
particular qeq constraints (the only type consid-
ered here) indicate that, in the scoped forms, a
label must either plug a hole directly or be con-
nected to it via a chain of quantifiers. Hole argu-
ments (other than the BODY of a quantifier) are al-
ways linked to a label via a qeq or other constraint
(in a deep grammar RMRS). Variables survive in
the models of RMRSs (i.e., the fully scoped trees)
whereas holes and labels do not.
2See Flickinger and Bender (2003) and Flickinger et al
(2003) for the use of MRS in DELPH-IN grammars.
2
l1:a1: some q, BV(a1,x4), RSTR(a1,h5), BODY(a1,h6), h5 qeq l2,
l2:a2: big a 1(e8), ARG1(a2,x4), l2:a3: angry a 1(e9), ARG1(a3,x4), l2:a4: dog n 1(x4),
l4:a5: bark v 1(e2), ARG1(a5,x4), l4:a6: loud a 1(e10), ARG1(a6,e2)
some q(x4, big a 1(e8,x4) ? angry a 1(e9, x4) ? dog n 1(x4), bark v 1(e2,x4) ? loud a 1(e10,e2))
Figure 1: RMRS and scoped form for ?Some big angry dogs bark loudly?. Tense and number are omitted.
The naming convention for predicates corre-
sponding to lexemes is: stem major sense tag,
optionally followed by and minor sense tag (e.g.,
loud a 1). Major sense tags correspond roughly
to traditional parts of speech. There are also non-
lexical predicates such as ?poss? (though none oc-
cur in Fig 1).3 MRS varies from RMRS in that the
arguments are all directly associated with the EP
and thus no anchors are necessary.
I have modified the definition of RMRS given
in Copestake (2007b) to make the ARG0 argument
optional. Here I want to add the additional con-
straint that the ARG0 of an EP is unique to it (i.e.,
not the ARG0 of any other EP). I will term this
the characteristic variable property. This means
that, for every variable, there is a unique EP which
has that variable as its ARG0. I will assume for this
paper that all EPs, apart from quantifier EPs, have
such an ARG0.4 The characteristic variable prop-
erty is one that has emerged from working with
large-scale constraint-based grammars.
A few concepts from the MRS algebra are also
necessary to the discussion. Composition can
be formalised as functor-argument combination
where the argument phrase?s hook fills a slot in
the functor phrase, thus instantiating an RMRS ar-
gument relation. The hook consists of an index
(a variable), an external argument (also a vari-
able) and an ltop (local top: the label correspond-
ing to the topmost node in the current partial tree,
ignoring quantifiers). The syntax-semantics inter-
face requires that the appropriate hook and slots be
set up (mostly lexically in a DELPH-IN grammar)
and that each application of a rule specifies the slot
to be used (e.g., MOD for modification). In a lex-
ical entry, the ARG0 of the EP provides the hook
3In fact, most of the choices about semantics made by
grammar writers concern the behaviour of constructions and
thus these non-lexical predicates, but this would require an-
other paper to discuss.
4I am simplifying for expository convenience. In current
DELPH-IN grammars, quantifiers have an ARG0 which corre-
sponds to the bound variable. This should not be the charac-
teristic variable of the quantifier (it is the characteristic vari-
able of a nominal EP), since its role in the scoped forms is as
a notational convenience to avoid lambda expressions. I will
call it the BV argument here.
index, and, apart from quantifiers, the hook ltop
is the EP?s label. In intersective combination, the
ltops of the hooks will be equated. In scopal com-
bination, a hole argument in a slot is specified to
be qeq to the ltop of the argument phrase and the
ltop of the functor phrase supplies the new hook?s
ltop.
By thinking of qeqs as links in an RMRS graph
(rather than in terms of their logical behaviour
as constraints on the possible scoped forms), an
RMRS can be treated as consisting of a set of trees
with nodes consisting of EPs grouped via intersec-
tive relationships: there will be a backbone tree
(headed by the overall ltop and including the main
verb if there is one), plus a separate tree for each
quantified NP. For instance, in Fig 1, the third
line contains the EPs corresponding to the (single
node) backbone tree and the first two lines show
the EPs comprising the tree for the quantified NP
(one node for the quantifier and one for the N?
which it connects to via the RSTR and its qeq).
3 Arguments and roles
I will now turn to the representation of arguments
in MRS and their relationship to semantic roles. I
want to discuss the approach to argument labelling
in some detail, because it is a reasonably clear
case where the desiderata for broad-coverage se-
mantics which were discussed in ?1 led us to a
syntactically-driven approach, as opposed to using
semantically richer roles such as AGENT, GOAL
and INSTRUMENT.
An MRS can, in fact, be written using a conven-
tional predicate-argument representation. A repre-
sentation which uses ordered argument labels can
be recovered from this in the obvious way. E.g.,
l:like v 1(e,x,y) is equivalent to l:a:like v 1(e),
ARG1(a,x), ARG2(a,y). A fairly large inventory of
argument labels is actually used in the DELPH-IN
grammars (e.g., RSTR, BODY). To recover these
from the conventional predicate-argument nota-
tion requires a look up in a semantic interface
component (the SEM-I, Flickinger et al (2005)).
But open-class predicates use the ARGn conven-
tion, where n is 0,1,2,3 or 4 and the discussion here
3
only concerns these.5
Arguably, the DELPH-IN approach is Davidso-
nian rather than neo-Davidsonian in that, even in
the RMRS form, the arguments are related to the
predicate via the anchor which plays no other role
in the semantics. Unlike the neo-Davidsonian use
of the event variable to attach arguments, this al-
lows the same style of representation to be used
uniformly, including quantifiers, for instance. Ar-
guments can omitted completely without syntactic
ill-formedness of the RMRS, but this is primarily
relevant to shallower grammars. A semantic pred-
icate, such as like v 1, is a logical predicate and as
such is expected to have the same arity wherever it
occurs in the DELPH-IN grammars. Thus models
for an MRS may be defined in a language with or
without argument labels.
The ordering of arguments for open class lex-
emes is lexically specified on the basis of the
syntactic obliqueness hierarchy (Pollard and Sag,
1994). ARG1 corresponds to the subject in the
base (non-passivised) form (?deep subject?). Ar-
gument numbering is consecutive in the base form,
so no predicate with an ARG3 is lexically missing
an ARG2, for instance. An ARG3 may occur with-
out an instantiated ARG2 when a syntactically op-
tional argument is missing (e.g., Kim gave to the
library), but this is explicit in the linearised form
(e.g., give v(e,x,u,y)).
The full statement of how the obliqueness hi-
erarchy (and thus the labelling) is determined for
lexemes has to be made carefully and takes us too
far into discussion of syntax to explain in detail
here. While the majority of cases are straightfor-
ward, a few are not (e.g., because they depend
on decisions about which form is taken as the
base in an alternation). However, all decisions are
made at the level of lexical types: adding an en-
try for a lexeme for a DELPH-IN grammar only
requires working out its lexical type(s) (from syn-
tactic behaviour and very constrained semantic no-
tions, e.g., control). The actual assignment of ar-
guments to an utterance is just a consequence of
parsing. Argument labelling is thus quite different
from PropBank (Palmer et al, 2005) role labelling
despite the unfortunate similarity of the PropBank
naming scheme.
It follows from the fixed arity of predicates
that lexemes with different numbers of argu-
5ARG4 occurs very rarely, at least in English (the verb bet
being perhaps the clearest case).
ments should be given different predicate symbols.
There is usually a clear sense distinction when this
occurs. For instance, we should distinguish be-
tween the ?depart? and ?bequeath? senses of leave
because the first takes an ARG1 and an ARG2 (op-
tional) and the second ARG1, ARG2 (optional),
ARG3. We do not draw sense distinctions where
there is no usage which the grammar could disam-
biguate.
Of course, there are obvious engineering rea-
sons for preferring a scheme that requires mini-
mal additional information in order to assign argu-
ment labels. Not only does this simplify the job of
the grammar writer, but it makes it easier to con-
struct lexical entries automatically and to integrate
RMRSs derived from shallower systems. However,
grammar engineers respond to consumers: if more
detailed role labelling had a clear utility and re-
quired an analysis at the syntax level, we would
want to do it in the grammar. The question is
whether it is practically possible.
Detailed discussion of the linguistics literature
would be out of place here. I will assume that
Dowty (1991) is right in the assertion that there
is no small (say, less than 10) set of role labels
which can also be used to link the predicate to its
arguments in compositionally constructed seman-
tics (i.e., argument-indexing in Dowty?s terminol-
ogy) such that each role label can be given a con-
sistent individual semantic interpretation. For our
purposes, a consistent semantic interpretation in-
volves entailment of one or more useful real world
propositions (allowing for exceptions to the entail-
ment for unusual individual sentences).
This is not a general argument against rich role
labels in semantics, just their use as the means
of argument-indexation. It leaves open uses for
grammar-internal purposes, e.g., for defining and
controlling alternations. The earliest versions of
the ERG experimented with a version of Davis?s
(2001) approach to roles for such reasons: this
was not continued, but for reasons irrelevant here.
Roles are still routinely used for argument index-
ation in linguistics papers (without semantic inter-
pretation). The case is sometimes made that more
mnemonic argument labelling helps human inter-
pretation of the notation. This may be true of se-
mantics papers in linguistics, which tend to con-
cern groups of similar lexemes. It is not true of a
collaborative computational linguistics project in
which broad coverage is being attempted: names
4
can only be mnemonic if they carry some meaning
and if the meaning cannot be consistently applied
this leads to endless trouble.
What I want to show here is how problems
arise even when very limited semantic generalisa-
tions are attempted about the nature of just one or
two argument labels, when used in broad-coverage
grammars. Take the quite reasonable idea that a
semantically consistent labelling for intransitives
and related causatives is possible (cf PropBank).
For instance, water might be associated with the
same argument label in the following examples:
(1) Kim boiled the water.
(2) The water boiled.
Using (simplified) RMRS representations, this
might amount to:
(3) l:a:boil v(e), a:ARG1(k), a:ARG2(x), water(x)
(4) l:a:boil v(e), a:ARG2(x), water(x)
Such an approach was used for a time in the ERG
with unaccusatives. However, it turns out to be im-
possible to carry through consistently for causative
alternations.
Consider the following examples of gallop: 6
(5) Michaela galloped the horse to the far end of
the meadow, . . .
(6) With that Michaela nudged the horse with her
heels and off the horse galloped.
(7) Michaela declared, ?I shall call him Lightning
because he runs as fast as lightning.? And with
that, off she galloped.
If only a single predicate is involved, e.g., gal-
lop v, and the causative has an ARG1 and an
ARG2, then what about the two intransitive cases?
If the causative is treated as obligatorily transi-
tive syntactically, then (6) and (7) presumably both
have an ARG2 subject. This leads to Michaela
having a different role label in (5) and (7), de-
spite the evident similarity of the real world situ-
ation. Furthermore, the role labels for intransitive
movement verbs could only be predicted by a con-
sumer of the semantics who knew whether or not
a causative form existed. The causative may be
rare, as with gallop, where the intransitive use is
clearly the base case. Alternatively, if (7) is treated
6http://www.thewestcoast.net/bobsnook/kid/horses.htm.
as a causative intransitive, and thus has a subject
labelled ARG1, there is a systematic unresolvable
ambiguity and the generalisation that the subjects
in both intransitive sentences are moving is lost.
Gallop is an not isolated case in having a vo-
litional intransitive use: it applies to most (if not
all) motion verbs which undergo the causative al-
ternation. To rescue this account, we would need
to apply it only to true lexical anti-causatives. It is
not clear whether this is doable (even the standard
example sink can be used intransitively of deliber-
ate movement) but from a slacker perspective, at
this point we should decide to look for an easier
approach.
The current ERG captures the causative relation-
ship by using systematic sense labelling:
(8) Kim boiled the water.
l:a:boil v cause(e), a:ARG1(k), a:ARG2(x),
water(x)
(9) The water boiled.
l:a:boil v 1(e), a:ARG1(x), water(x)
This is not perfect, but it has clear advantages.
It allows inferences to be made about ARG1 and
ARG2 of cause verbs. In general, inferences about
arguments may be made with respect to particular
verb classes. This lends itself to successive refine-
ment in the grammars: the decision to add a stan-
dardised sense label, such as cause, does not re-
quire changes to the type system, for instance. If
we decide that we can identify true anti-causatives,
we can easily make them a distinguished class via
this convention. Conversely, in the situation where
causation has not been recognised, and the verb
has been treated as a single lexeme having an op-
tional ARG2, the semantics is imperfect but at least
the imperfection is local.
In fact, determining argument labelling by the
obliqueness hierarchy still allows generalisations
to be made for all verbs. Dowty (1991) argues
for the notion of proto-agent (p-agt) and proto-
patient (p-pat) as cluster concepts. Proto-agent
properties include volitionality, sentience, causa-
tion of an event and movement relative to another
participant. Proto-patient properties include be-
ing causally affected and being stationary relative
to another participant. Dowty claims that gener-
alisations about which arguments are lexicalised
as subject, object and indirect object/oblique can
be expressed in terms of relative numbers of p-agt
and p-pat properties. If this is correct, then we can,
5
for example, predict that the ARG1 of any predi-
cate in a DELPH-IN grammar will not have fewer
p-agt properties than the ARG2 of that predicate.7
As an extreme alternative, we could use la-
bels which were individual to each predicate,
such as LIKER and LIKED (e.g., Pollard and Sag
(1994)). For such role labels to have a consistent
meaning, they would have to be lexeme-specific:
e.g., LEAVER1 (?departer?) versus LEAVER2 (?be-
queather?). However this does nothing for seman-
tic generalisation, blocks the use of argument la-
bels in syntactic generalisations and leads to an
extreme proliferation of lexical types when us-
ing typed feature structure formalisms (one type
would be required per lexeme). The labels add
no additional information and could trivially be
added automatically to an RMRS if this were use-
ful for human readers. Much more interesting is
the use of richer lexical semantic generalisations,
such as those employed in FrameNet (Baker et al,
1998). In principle, at least, we could (and should)
systematically link the ERG to FrameNet, but this
would be a form of semantic enrichment mediated
via the SEM-I (cf Roa et al (2008)), and not an
alternative technique for argument indexation.
4 Dependency MRS
The second main topic I want to address is a
form of semantic dependency structure (DMRS:
see wiki.delph-in.net for the evolving details).
There are good engineering reasons for producing
a dependency style representation with links be-
tween predicates and no variables: ease of read-
ability for consumers of the representation and for
human annotators, parser comparison and integra-
tion with distributional lexical semantics being the
immediate goals. Oepen has previously produced
elementary dependencies from MRSs but the pro-
cedure (partially sketched in Oepen and L?nning
(2006)) was not intended to produce complete rep-
resentations. It turns out that a DMRS can be con-
structed which can be demonstrated to be inter-
convertible with RMRS, has a simple graph struc-
ture and minimises redundancy in the representa-
tion. What is surprising is that this can be done
for a particular class of grammars without mak-
7Sanfilippo (1990) originally introduced Dowty?s ideas
into computational linguistics, but this relative behaviour
cannot be correctly expressed simply by using p-agt and p-
pat directly for argument indexation as he suggested. It is
incorrect for examples like (2) to be labelled as p-agt, since
they have no agentive properties.
ing use of the evident clues to syntax in the pred-
icate names. The characteristic variable property
discussed in ?2 is crucial: its availability allows
a partial replication of composition, with DMRS
links being relatable to functor-argument combi-
nations in the MRS algebra. I should emphasize
that, unlike MRS and RMRS, DMRS is not intended
to have a direct logical interpretation.
An example of a DMRS is given in Fig 2. Links
relate nodes corresponding to RMRS predicates.
Nodes have unique identifiers, not shown here. Di-
rected link labels are of the form ARG/H, ARG/EQ
or ARG/NEQ, where ARG corresponds to an RMRS
argument label. H indicates a qeq relationship,
EQ label equality and NEQ label inequality, as ex-
plained more fully below. Undirected /EQ arcs
also sometimes occur (see ?4.3). The ltop is in-
dicated with a *.
4.1 RMRS-to-DMRS
In order to transform an RMRS into a DMRS, we
will treat the RMRS as made up of three subgraphs:
Label equality graph. Each EP in an RMRS
has a label, which may be shared with any number
of other EPs. This can be captured in DMRS via
a graph linking EPs: if this is done exhaustively,
there would be n(n? 1)/2 binary non-directional
links. E.g., for the RMRS in Fig 1, we need to link
big a 1, angry a 1 and dog n 1 and this takes
3 links. Obviously the effect of equality could be
captured by a smaller number of links, assuming
transitivity: but to make the RMRS-to-DMRS con-
version deterministic, we need a method for se-
lecting canonical links.
Hole-to-label qeq graph. A qeq in RMRS links
a hole to a label which labels a set of EPs. There
is thus a 1 : 1 mapping between holes and la-
bels which can be converted to a 1 : n mapping
between holes and the EPs which share the la-
bel. By taking the EP with the hole as the origin,
we can construct an EP-to-EP graph, using the ar-
gument name as a label for the link: of course,
such links are asymmetric and thus the graph is
directed. e.g., some q has RSTR links to each of
big a 1, angry a 1 and dog n 1. Reducing this
to a 1 : 1 mapping between EPs, which we would
ideally like for DMRS, requires a canonical method
of selecting a head EP from the set of target EPs (as
does the selection of the ltop).
Variable graph. For the conversion to DMRS,
we will rely on the characteristic variable prop-
6
some q big a 1 angry a at dog n 1 bark v 1* loud a 1
-
ARG1/EQ
ff
ARG1/EQ
ff
ARG1/NEQ
-
ARG1/EQ
-
RSTR/H
Figure 2: DMRS for ?Some big angry dogs bark loudly.?
erty, that every variable has a unique EP associated
with it via its ARG0. Any non-hole argument of an
EP will have a value which is the ARG0 of some
other EP, or which is unbound (i.e., not found else-
where in the RMRS) in which case we ignore it.
Thus we can derive a graph between EPs, such
that each link is labelled with an argument posi-
tion and points to a unique EP. I will talk about an
EP?s ?argument EPs?, to refer to the set of EPs its
arguments point to in this graph.
The three EP graphs can be combined to form
a dependency structure. But this has an excessive
number of links due to the label equality and qeq
components. We need deterministic techniques for
removing the redundancy. These can utilise the
variable graph, since this is already minimal.
The first strategy is to combine the label equal-
ity and variable links when they connect the same
two EPs. For instance, we combine the ARG1
link between big a 1, and dog n 1 with the la-
bel equality link to give a link labelled ARG1/EQ.
We then test the connectivity of the ARG/EQ links
on the assumption of transitivity and remove any
redundant links from the label graph. This usually
removes all label equality links: one case where
it does not is discussed in ?4.3. Variable graph
links with no corresponding label equality are an-
notated ARG/NEQ, while links arising from the
qeq graph are labelled ARG/H. This retains suf-
ficient information to allow the reconstruction of
the three graphs in DMRS-to-RMRS conversion.
In order to reduce the number of links arising
from the qeq graph, we make use of the variable
graph to select a head from a set of EPs sharing
a label. It is not essential that there should be a
unique head, but it is desirable. The next section
outlines how head selection works: despite not us-
ing any directly syntactic properties, it generally
recovers the syntactic head.
4.2 Head selection in the qeq graph
Head selection uses one principle and one heuris-
tic, both of which are motivated by the composi-
tional properties of the grammar. The principle is
that qeq links from an EP should parallel any com-
parable variable links. If an EP has two arguments,
one of which is a variable argument which links
to EP? and the other a hole argument which has a
value corresponding to a set of EPs including EP?,
EP? is chosen as the head of that set.
This essentially follows from the composition
rules: in an algebra operation giving rise to a qeq,
the argument phrase supplies a hook consisting
of an index (normally, the ARG0 of the head EP)
and an ltop (normally, the label of the head EP).
Thus if a variable argument corresponds to EP?,
EP? will have been the head of the corresponding
phrase and is thus the choice of head in the DMRS.
This most frequently arises with quantifiers, which
have both a BV and a RSTR argument: the RSTR
argument can be taken as linking to the EP which
has an ARG0 equal to the BV (i.e., the head of the
N?). If this principle applies, it will select a unique
head. In fact, in this special case, we drop the BV
link from the final DMRS because it is entirely pre-
dictable from the RSTR link.
In the case where there is no variable argu-
ment, we use the heuristic which generally holds
in DELPH-IN grammars that the EPs which we
wish to distinguish as heads in the DMRS do not
share labels with their DMRS argument EPs (in
contrast to intersective modifiers, which always
share labels with their argument EPs). Heads may
share labels with PPs which are syntactically ar-
guments, but these have a semantics like PP mod-
ifiers, where the head is the preposition?s EP ar-
gument. NP arguments are generally quantified
and quantifiers scope freely. AP, VP and S syn-
tactic arguments are always scopal. PPs which are
not modifier-like are either scopal (small clauses)
or NP-like (case marking Ps) and free-scoping.
Thus, somewhat counter-intuitively, we can select
the head EP from the set of EPs which share a label
by looking for an EP which has no argument EPs
in that set.
4.3 Some properties of DMRS
The MRS-to-DMRS procedure deterministically
creates a unique DMRS. A converse DMRS-to-MRS
procedure recreates the MRS (up to label, anchor
7
the q dog n 1 def explicit q poss toy n 1 the q cat n 1 bite v 1 bark v 1*
ff
ARG2/EQ
ff
ARG1/NEQ
-
RSTR/H
-
RSTR/H
-
ARG1/NEQ
ff
ARG2/NEQ
-
RSTR/H
/EQ
ff
ARG1/NEQ
Figure 3: DMRS for ?The dog whose toy the cat bit barked.?
and variable renaming), though requiring the SEM-
I to add the uninstantiated optional arguments.
I claimed above that DMRSs are an idealisa-
tion of semantic composition. A pure functor-
argument application scheme would produce a tree
which could be transformed into a structure where
no dependent had more than one head. But in
DMRS the notion of functor/head is more complex
as determiners and modifiers provide slots in the
RMRS algebra but not the index of the result. Com-
position of a verb (or any other functor) with an
NP argument gives rise to a dependency between
the verb and the head noun in the N?. The head
noun provides the index of the NP?s hook in com-
position, though it does not provide the ltop, which
comes from the quantifier. However, because this
ltop is not equated with any label, there is no direct
link between the verb and the determiner. Thus the
noun will have a link from the determiner and from
the verb.
Similarly, if the constituents in composition
were continuous, the adjacency condition would
hold, but this does not apply because of the mech-
anisms for long-distance dependencies and the
availability of the external argument in the hook.8
DMRS indirectly preserves the information
about constituent structure which is essential for
semantic interpretation, unlike some syntactic de-
pendency schemes. In particular, it retains infor-
mation about a quantifier?s N?, since this forms the
restrictor of the generalised quantifier (for instance
Most white cats are deaf has different truth condi-
tions from Most deaf cats are white). An inter-
esting example of nominal modification is shown
in Fig 3. Notice that whose has a decomposed
semantics combining two non-lexeme predicates
def explicit q and poss. Unusually, the relative
clause has a gap which is not an argument of its
semantic head (it?s an argument of poss rather than
bite v 1). This means that when the relative clause
8Given that non-local effects are relatively circumscribed,
it is possible to require adjacency in some parts of the DMRS.
This leads to a technique for recording underspecification of
noun compound bracketing, for instance.
is combined with the gap filler, the label equality
and the argument instantiation correspond to dif-
ferent EPs. Thus there is a label equality which
cannot be combined with an argument link and has
to be represented by an undirected /EQ arc.
5 Related work and conclusion
Hobbs (1985) described a philosophy of computa-
tional compositional semantics that is in some re-
spects similar to that presented here. But, as far as
I am aware, the Core Language Engine book (Al-
shawi, 1992) provided the first detailed descrip-
tion of a truly computational approach to com-
positional semantics: in any case, Steve Pulman
provided my own introduction to the idea. Cur-
rently, the ParGram project also undertakes large-
scale multilingual grammar engineering work: see
Crouch and King (2006) and Crouch (2006) for an
account of the semantic composition techniques
now being used. I am not aware of any other
current grammar engineering activities on the Par-
Gram or DELPH-IN scale which build bidirectional
grammars for multiple languages.
Overall, what I have tried to do here is to give a
flavour of how compositional semantics and syn-
tax interact in computational grammars. Analy-
ses which look simple have often taken consider-
able experimentation to arrive at when working on
a large-scale, especially when attempting cross-
linguistic generalisations. The toy examples that
can be given in papers like this one do no justice to
this, and I would urge readers to try out the gram-
mars and software and, perhaps, to join in.
Acknowledgements
Particular thanks to Emily Bender, Dan Flickinger
and Alex Lascarides for detailed comments at
very short notice! I am also grateful to many
other colleagues, especially from DELPH-IN and
in the Cambridge NLIP research group. This
work was supported by the Engineering and Phys-
ical Sciences Research Council [grant numbers
EP/C010035/1, EP/F012950/1].
8
References
Hiyan Alshawi, editor. 1992. The Core Language En-
gine. MIT Press.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc.
ACL-98, pages 86?90, Montreal, Quebec, Canada.
Association for Computational Linguistics.
Emily Bender, Dan Flickinger, and Stephan Oepen.
2002. The Grammar Matrix: An open-source
starter-kit for the rapid development of cross-
linguistically consistent broad-coverage precision
grammars. In Proc. Workshop on Grammar Engi-
neering and Evaluation, Coling 2002, pages 8?14,
Taipei, Taiwan.
Emily Bender. 2008. Evaluating a crosslinguistic
grammar resource: A case study of Wambaya. In
Proc. ACL-08, pages 977?985, Columbus, Ohio,
USA.
John Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Proc. IJCNLP05, Springer Lecture
Notes in Artificial Intelligence, Volume 3651, pages
165?176, Jeju Island, Korea.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznanski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proc. 7th Eu-
ropean Workshop on Natural Language Generation
(EWNLG?99), pages 86?95, Toulouse.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An algebra for semantic construction in
constraint-based grammars. In Proc. ACL-01,
Toulouse.
Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal Recursion Semantics: an
introduction. Research on Language and Computa-
tion, 3(2-3):281?332.
Ann Copestake. 2003. Report on the design of RMRS.
DeepThought project deliverable.
Ann Copestake. 2007a. Applying robust semantics.
In Proc. PACLING 2007 ? 10th Conference of the
Pacific Association for Computational Linguistics,
pages 1?12, Melbourne.
Ann Copestake. 2007b. Semantic composition with
(Robust) Minimal Recursion Semantics. In Proc.
Workshop on Deep Linguistic Processing, ACL
2007, Prague.
Dick Crouch and Tracy Holloway King. 2006. Seman-
tics via F-structure rewriting. In Miriam Butt and
Tracy Holloway King, editors, Proc. LFG06 Con-
ference, Universitat Konstanz. CSLI Publications.
Dick Crouch. 2006. Packed rewriting for mapping se-
mantics and KR. In Intelligent Linguistic Architec-
tures Variations on Themes by Ronald M. Kaplan,
pages 389?416. CSLI Publications.
Anthony Davis. 2001. Linking by Types in the Hierar-
chical Lexicon. CSLI Publications.
David Dowty. 1991. Thematic proto-roles and argu-
ment selection. Language, 67(3):547?619.
Dan Flickinger and Emily Bender. 2003. Compo-
sitional semantics in a multilingual grammar re-
source. In Proc. Workshop on Ideas and Strate-
gies for Multilingual Grammar Development, ESS-
LLI 2003, pages 33?42, Vienna.
Dan Flickinger, Emily Bender, and Stephan Oepen.
2003. MRS in the LinGO Grammar Matrix: A prac-
tical user?s guide. http://tinyurl.com/crf5z7.
Dan Flickinger, Jan Tore L?nning, Helge Dyvik,
Stephan Oepen, and Francis Bond. 2005. SEM-I
rational MT ? enriching deep grammars with a se-
mantic interface for scalable machine translation. In
Proc. MT Summit X, Phuket, Thailand.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
Sanae Fujita, Francis Bond, Stephan Oepen, and
Takaaki Tanaka. 2007. Exploiting semantic infor-
mation for HPSG parse selection. In Proc. Work-
shop on Deep Linguistic Processing, ACL 2007,
Prague.
Jerry Hobbs. 1985. Ontological promiscuity. In Proc.
ACL-85, pages 61?69, Chicago, IL.
Alexander Koller and Alex Lascarides. 2009. A logic
of semantic representations for shallow parsing. In
Proc. EACL-2009, Athens.
Stephan Oepen and Jan Tore L?nning. 2006.
Discriminant-based MRS banking. In Proc. LREC-
2006, Genoa, Italy.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
Carl Pollard and Ivan Sag. 1994. Head-driven Phrase
Structure Grammar. University of Chicago Press,
Chicago.
Sergio Roa, Valia Kordoni, and Yi Zhang. 2008. Map-
ping between compositional semantic representa-
tions and lexical semantic resources: Towards accu-
rate deep semantic parsing. In Proc. ACL-08, pages
189?192, Columbus, Ohio. Association for Compu-
tational Linguistics.
Antonio Sanfilippo. 1990. Grammatical Relations,
Thematic Roles and Verb Semantics. Ph.D. thesis,
Centre for Cognitive Science, University of Edin-
burgh.
Stefan Thater. 2007. Minimal Recursion Semantics
as Dominance Constraints: Graph-Theoretic Foun-
dation and Application to Grammar Engineering.
Ph.D. thesis, Universita?t des Saarlandes.
9
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 621?629,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Using lexical and relational similarity to classify semantic relations
Diarmuid O? Se?aghdha
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
do242@cl.cam.ac.uk
Ann Copestake
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
aac10@cl.cam.ac.uk
Abstract
Many methods are available for comput-
ing semantic similarity between individ-
ual words, but certain NLP tasks require
the comparison of word pairs. This pa-
per presents a kernel-based framework for
application to relational reasoning tasks of
this kind. The model presented here com-
bines information about two distinct types
of word pair similarity: lexical similarity
and relational similarity. We present an
efficient and flexible technique for imple-
menting relational similarity and show the
effectiveness of combining lexical and re-
lational models by demonstrating state-of-
the-art results on a compound noun inter-
pretation task.
1 Introduction
The problem of modelling semantic similarity be-
tween words has long attracted the interest of re-
searchers in Natural Language Processing and has
been shown to be important for numerous applica-
tions. For some tasks, however, it is more appro-
priate to consider the problem of modelling sim-
ilarity between pairs of words. This is the case
when dealing with tasks involving relational or
analogical reasoning. In such tasks, the chal-
lenge is to compare pairs of words on the basis of
the semantic relation(s) holding between the mem-
bers of each pair. For example, the noun pairs
(steel,knife) and (paper,cup) are similar because
in both cases the relation N2 is made of N1 fre-
quently holds between their members. Analogi-
cal tasks are distinct from (but not unrelated to)
other kinds of ?relation extraction? tasks where
each data item is tied to a specific sentence con-
text (e.g., Girju et al (2007)).
One such relational reasoning task is the prob-
lem of compound noun interpretation, which
has received a great deal of attention in recent
years (Girju et al, 2005; Turney, 2006; But-
nariu and Veale, 2008). In English (and other
languages), the process of producing new lexical
items through compounding is very frequent and
very productive. Furthermore, the noun-noun re-
lation expressed by a given compound is not ex-
plicit in its surface form: a steel knife may be a
knife made from steel but a kitchen knife is most
likely to be a knife used in a kitchen, not a knife
made from a kitchen. The assumption made by
similarity-based interpretation methods is that the
likely meaning of a novel compound can be pre-
dicted by comparing it to previously seen com-
pounds whose meanings are known. This is a
natural framework for computational techniques;
there is also empirical evidence for similarity-
based interpretation in human compound process-
ing (Ryder, 1994; Devereux and Costello, 2007).
This paper presents an approach to relational
reasoning based on combining information about
two kinds of similarity between word pairs: lex-
ical similarity and relational similarity. The as-
sumptions underlying these two models of similar-
ity are sketched in Section 2. In Section 3 we de-
scribe how these models can be implemented for
statistical machine learning with kernel methods.
We present a new flexible and efficient kernel-
based framework for classification with relational
similarity. In Sections 4 and 5 we apply our
methods to a compound interpretation task and
demonstrate that combining models of lexical and
relational similarity can give state-of-the-art re-
sults on a compound noun interpretation task, sur-
passing the performance attained by either model
taken alone. We then discuss previous research
on relational similarity, and show that some previ-
ously proposed models can be implemented in our
framework as special cases. Given the good per-
formance achieved for compound interpretation, it
seems likely that the methods presented in this pa-
621
per can also be applied successfully to other rela-
tional reasoning tasks; we suggest some directions
for future research in Section 7.
2 Two models of word pair similarity
While there is a long tradition of NLP research
on methods for calculating semantic similarity be-
tween words, calculating similarity between pairs
(or n-tuples) of words is a less well-understood
problem. In fact, the problem has rarely been
stated explicitly, though it is implicitly addressed
by most work on compound noun interpretation
and semantic relation extraction. This section de-
scribes two complementary approaches for using
distributional information extracted from corpora
to calculate noun pair similarity.
The first model of pair similarity is based on
standard methods for computing semantic similar-
ity between individual words. According to this
lexical similarity model, word pairs (w1, w2) and
(w3, w4) are judged similar if w1 is similar to w3
and w2 is similar to w4. Given a measure wsim
of word-word similarity, a measure of pair simi-
larity psim can be derived as a linear combination
of pairwise lexical similarities:
psim((w1, w2), (w3, w4)) = (1)
?[wsim(w1, w3)] + ?[wsim(w2, w4)]
A great number of methods for lexical semantic
similarity have been proposed in the NLP liter-
ature. The most common paradigm for corpus-
based methods, and the one adopted here, is based
on the distributional hypothesis: that two words
are semantically similar if they have similar pat-
terns of co-occurrence with other words in some
set of contexts. Curran (2004) gives a comprehen-
sive overview of distributional methods.
The second model of pair similarity rests on the
assumption that when the members of a word pair
are mentioned in the same context, that context
is likely to yield information about the relations
holding between the words? referents. For exam-
ple, the members of the pair (bear, forest) may
tend to co-occur in contexts containing patterns
such as w1 lives in the w2 and in the w2,. . . a w1,
suggesting that a LOCATED IN or LIVES IN re-
lation frequently holds between bears and forests.
If the contexts in which fish and reef co-occur are
similar to those found for bear and forest, this is
evidence that the same semantic relation tends to
hold between the members of each pair. A re-
lational distributional hypothesis therefore states
that two word pairs are semantically similar if their
members appear together in similar contexts.
The distinction between lexical and relational
similarity for word pair comparison is recognised
by Turney (2006) (he calls the former attributional
similarity), though the methods he presents focus
on relational similarity. O? Se?aghdha and Copes-
take?s (2007) classification of information sources
for noun compound interpretation also includes a
description of lexical and relational similarity. Ap-
proaches to compound noun interpretation have
tended to use either lexical or relational similarity,
though rarely both (see Section 6 below).
3 Kernel methods for pair similarity
3.1 Kernel methods
The kernel framework for machine learning is a
natural choice for similarity-based classification
(Shawe-Taylor and Cristianini, 2004). The cen-
tral concept in this framework is the kernel func-
tion, which can be viewed as a measure of simi-
larity between data items. Valid kernels must sat-
isfy the mathematical condition of positive semi-
definiteness; this is equivalent to requiring that the
kernel function equate to an inner product in some
vector space. The kernel can be expressed in terms
of a mapping function ? from the input space X to
a feature space F :
k(xi,xj) = ??(xi), ?(xj)?F (2)
where ??, ??F is the inner product associated with
F . X and F need not have the same dimension-
ality or be of the same type. F is by definition an
inner product space, but the elements of X need
not even be vectorial, so long as a suitable map-
ping function ? can be found. Furthermore, it is
often possible to calculate kernel values without
explicitly representing the elements of F ; this al-
lows the use of implicit feature spaces with a very
high or even infinite dimensionality.
Kernel functions have received significant at-
tention in recent years, most notably due to the
successful application of Support VectorMachines
(Cortes and Vapnik, 1995) to many problems. The
SVM algorithm learns a decision boundary be-
tween two data classes that maximises the mini-
mum distance or margin from the training points
in each class to the boundary. The geometry of the
space in which this boundary is set depends on the
622
kernel function used to compare data items. By
tailoring the choice of kernel to the task at hand,
the user can use prior knowledge and intuition to
improve classification performance.
One useful property of kernels is that any sum
or linear combination of kernel functions is itself
a valid kernel. Theoretical analyses (Cristianini
et al, 2001; Joachims et al, 2001) and empiri-
cal investigations (e.g., Gliozzo et al (2005)) have
shown that combining kernels in this way can have
a beneficial effect when the component kernels
capture different ?views? of the data while indi-
vidually attaining similar levels of discriminative
performance. In the experiments described below,
we make use of this insight to integrate lexical and
relational information for semantic classification
of compound nouns.
3.2 Lexical kernels
O? Se?aghdha and Copestake (2008) demonstrate
how standard techniques for distributional similar-
ity can be implemented in a kernel framework. In
particular, kernels for comparing probability dis-
tributions can be derived from standard probabilis-
tic distance measures through simple transforma-
tions. These distributional kernels are suited to a
data representation where each word w is identi-
fied with the a vector of conditional probabilities
(P (c1|w), . . . , P (c|C||w)) that defines a distribu-
tion over other terms c co-occurring with w. For
example, the following positive semi-definite ker-
nel between words can be derived from the well-
known Jensen-Shannon divergence:
kjsd(w1, w2) =
?
?
c
[P (c|w1) log2(
P (c|w1)
P (c|w1) + P (c|w2)
)
+ P (c|w2) log2(
P (c|w2)
P (c|w1) + P (c|w2)
)] (3)
A straightforward method of extending this model
to word pairs is to represent each pair (w1, w2) as
the concatenation of the co-occurrence probability
vectors forw1 andw2. Taking kjsd as a measure of
word similarity and introducing parameters ? and
? to scale the contributions of w1 and w2 respec-
tively, we retrieve the lexical model of pair similar-
ity defined above in (1). Without prior knowledge
of the relative importance of each pair constituent,
it is natural to set both scaling parameters to 0.5,
and this is done in the experiments below.
3.3 String embedding functions
The necessary starting point for our implementa-
tion of relational similarity is a means of compar-
ing contexts. Contexts can be represented in a va-
riety of ways, from unordered bags of words to
rich syntactic structures. The context representa-
tion adopted here is based on strings, which pre-
serve useful information about the order of words
in the context yet can be processed and compared
quite efficiently. String kernels are a family of ker-
nels that compare strings s, t by mapping them
into feature vectors ?String(s), ?String(t) whose
non-zero elements index the subsequences con-
tained in each string.
A string is defined as a finite sequence s =
(s1, . . . , sl) of symbols belonging to an alphabet
?. ?l is the set of all strings of length l, and ?? is
set of all strings or the language. A subsequence
u of s is defined by a sequence of indices i =
(i1, . . . , i|u|) such that 1 ? i1 < ? ? ? < i|u| ? |s|,
where |s| is the length of s. len(i) = i|u| ? i1 + 1
is the length of the subsequence in s. An embed-
ding ?String : ?? ? R|?|
l
is a function that maps
a string s onto a vector of positive ?counts? that
correspond to subsequences contained in s.
One example of an embedding function is a
gap-weighted embedding, defined as
?gapl(s) = [
?
i:s[i]=u
?len(i)]u??l (4)
? is a decay parameter between 0 and 1; the
smaller its value, the more the influence of a dis-
continuous subsequence is reduced. When l = 1
this corresponds to a ?bag-of-words? embedding.
Gap-weighted string kernels implicitly compute
the similarity between two strings s, t as an inner
product ??(s), ?(t)?. Lodhi et al (2002) present
an efficient dynamic programming algorithm that
evaluates this kernel in O(l|s||t|) time without ex-
plicitly representing the feature vectors ?(s), ?(t).
An alternative embedding is that used by Turney
(2008) in his PairClass system (see Section 6). For
the PairClass embedding ?PC , an n-word context
[0?1 words]N1|2 [0?3 words]N1|2 [0?1 words]
containing target words N1, N2 is mapped onto
the 2n?2 patterns produced by substituting zero
or more of the context words with a wildcard ?.
Unlike the patterns used by the gap-weighted em-
bedding these are not truly discontinuous, as each
wildcard must match exactly one word.
623
3.4 Kernels on sets
String kernels afford a way of comparing individ-
ual contexts. In order to compute the relational
similarity of two pairs, however, we do not want to
associate each pair with a single context but rather
with the set of contexts in which they appear to-
gether. In this section, we use string embeddings
to define kernels on sets of strings.
One natural way of defining a kernel over sets
is to take the average of the pairwise basic kernel
values between members of the two sets A and B.
Let k0 be a kernel on a set X , and let A,B ? X
be sets of cardinality |A| and |B| respectively. The
averaged kernel is defined as
kave(A,B) =
1
|A||B|
?
a?A
?
b?B
k0(a, b) (5)
This kernel was introduced by Ga?rtner et
al. (2002) in the context of multiple instance learn-
ing. It was first used for computing relational sim-
ilarity by O? Se?aghdha and Copestake (2007). The
efficiency of the kernel computation is dominated
by the |A| ? |B| basic kernel calculations. When
each basic kernel calculation k0(a, b) has signifi-
cant complexity, as is the case with string kernels,
calculating kave can be slow.
A second perspective views each set as corre-
sponding to a probability distribution, and takes
the members of that set as observed samples from
that distribution. In this way a kernel on distribu-
tions can be cast as a kernel on sets. In the case of
sets whose members are strings, a string embed-
ding ?String can be used to estimate a probability
distribution over subsequences for each set by tak-
ing the normalised sum of the feature mappings of
its members:
?Set(A) =
1
Z
?
s?A
?String(s) (6)
where Z is a normalisation factor. Different
choices of ?String yield different relational simi-
larity models. In this paper we primarily use the
gap-weighted embedding ?gapl ; we also discuss
the PairClass embedding ?PC for comparison.
Once the embedding ?Set has been calculated,
any suitable inner product can be applied to the
resulting vectors, e.g. the linear kernel (dot prod-
uct) or the Jensen-Shannon kernel defined in (3).
In the latter case, which we term kjsd below, the
natural choice for normalisation is the sum of the
entries in
?
s?A ?String(s), ensuring that ?Set(A)
has unit L1 norm and defines a probability dis-
tribution. Furthermore, scaling ?Set(A) by 1|A| ,
applying L2 vector normalisation and applying
the linear kernel retrieves the averaged set kernel
kave(A,B) as a special case of the distributional
framework for sets of strings.
Instead of requiring |A||B| basic kernel evalua-
tions for each pair of sets, distributional set kernels
only require the embedding ?Set(A) to be com-
puted once for each set and then a single vector
inner product for each pair of sets. This is gen-
erally far more efficient than the kernel averaging
method. The significant drawback is that repre-
senting the feature vector for each set demands
a large amount of memory; for the gap-weighted
embedding with subsequence length l, each vec-
tor potentially contains up to |A|
(|smax|
l
)
entries,
where smax is the longest string in A. In practice,
however, the vector length will be lower due to
subsequences occurring more than once and many
strings being shorter than smax.
One way to reduce the memory load is to re-
duce the lengths of the strings used, either by re-
taining just the part of each string expected to be
informative or by discarding all strings longer than
an acceptable maximum. The PairClass embed-
ding function implicitly restricts the contexts con-
sidered by only applying to strings where no more
than three words occur between the targets, and by
ignoring all non-intervening words except single
ones adjacent to the targets. A further technique
is to trade off time efficiency for space efficiency
by computing the set kernel matrix in a blockwise
fashion. To do this, the input data is divided into
blocks of roughly equal size ? the size that is rele-
vant here is the sum of the cardinalities of the sets
in a given block. Larger block sizes b therefore
allow faster computation, but they require more
memory. In the experiments described below, b
was set to 5,000 for embeddings of length l = 1
and l = 2, and to 3,000 for l = 3.
4 Experimental setup for compound
noun interpretation
4.1 Dataset
The dataset used in our experiments is O? Se?aghdha
and Copestake?s (2007) set of 1,443 compound
nouns extracted from the British National Corpus
(BNC).1 Each compound is annotated with one of
1The data are available from http://www.cl.cam.
ac.uk/?do242/resources.html.
624
six semantic relations: BE,HAVE, IN, AGENT, IN-
STRUMENT and ABOUT. For example, air disas-
ter is labelled IN (a disaster in the air) and freight
train is labelled INSTRUMENT (a train that car-
ries freight). The best previous classification result
on this dataset was reported by O? Se?aghdha and
Copestake (2008), who achieved 61.0% accuracy
and 58.8% F-score with a purely lexical model of
compound similarity.
4.2 General Methodology
All experiments were run using the LIBSVM Sup-
port Vector Machine library.2 The one-versus-all
method was used to decompose the multiclass task
into six binary classification tasks. Performance
was evaluated using five-fold cross-validation. For
each fold the SVM cost parameter was optimised
in the range (2?6, 2?4, . . . , 212) through cross-
validation on the training set.
All kernel matrices were precomputed on near-
identical machines with 2.4 Ghz 64-bit processors
and 8Gb of memory. The kernel matrix compu-
tation is trivial to parallelise, as each cell is inde-
pendent. Spreading the computational load across
multiple processors is a simple way to reduce the
real time cost of the procedure.
4.3 Lexical features
Our implementation of the lexical similarity
model uses the same feature set as O? Se?aghdha
and Copestake (2008). Two corpora were used
to extract co-occurrence information: the writ-
ten component of the BNC (Burnard, 1995) and
the Google Web 1T 5-Gram Corpus (Brants and
Franz, 2006). For each noun appearing as a com-
pound constituent in the dataset, we estimate a co-
occurrence distribution based on the nouns in co-
ordinative constructions. Conjunctions are identi-
fied in the BNC by first parsing the corpus with
RASP (Briscoe et al, 2006) and extracting in-
stances of the conj grammatical relation. As the
5-Gram corpus does not contain full sentences it
cannot be parsed, so regular expressions were used
to extract coordinations. In each corpus, the set of
co-occurring terms is restricted to the 10,000 most
frequent conjuncts in that corpus so that each con-
stituent distribution is represented with a 10,000-
dimensional vector. The probability vector for the
compound is created by appending the two con-
stituent vectors, each scaled by 0.5 to weight both
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm
constituents equally and ensure that the new vec-
tor sums to 1. To perform classification with these
features we use the Jensen-Shannon kernel (3).3
4.4 Relational features
To extract data for computing relational similarity,
we searched a large corpus for sentences in which
both constituents of a compound co-occur. The
corpora used here are the written BNC, contain-
ing 90 million words of British English balanced
across genre and text type, and the English Giga-
word Corpus, 2nd Edition (Graff et al, 2005), con-
taining 2.3 billion words of newswire text. Extrac-
tion from the Gigaword Corpus was performed at
the paragraph level as the corpus is not annotated
for sentence boundaries, and a dictionary of plural
forms and American English variants was used to
expand the coverage of the corpus trawl.
The extracted contexts were split into sentences,
tagged and lemmatised with RASP. Duplicate sen-
tences were discarded, as were sentences in which
the compound head and modifier were more than
10 words apart. Punctuation and tokens containing
non-alphanumeric characters were removed. The
compound modifier and head were replaced with
placeholder tokens M:n and H:n in each sentence
to ensure that the classifier would learn from re-
lational information only and not from lexical in-
formation about the constituents. Finally, all to-
kens more than five words to the left of the left-
most constituent or more than five words to the
right of the rightmost constituent were discarded;
this has the effect of speeding up the kernel com-
putations and should also focus the classifier on
the most informative parts of the context sen-
tences. Examples of the context strings extracted
for the modifier-head pair (history,book) are the:a
1957:m pulitizer:n prize-winning:j H:n describe:v
event:n in:i american:j M:n when:c elect:v of-
ficial:n take:v principle:v and he:p read:v con-
stantly:r usually:r H:n about:i american:j M:n
or:c biography:n.
This extraction procedure resulted in a corpus
of 1,472,798 strings. There was significant varia-
tion in the number of context strings extracted for
each compound: 288 compounds were associated
with 1,000 or more sentences, while 191 were as-
3O? Se?aghdha and Copestake (2008) achieve their single
best result with a different kernel (the Jensen-Shannon RBF
kernel), but the kernel used here (the Jensen-Shannon lin-
ear kernel) generally achieves equivalent performance and
presents one fewer parameter to optimise.
625
kjsd kave
Length Acc F Acc F
1 47.9 45.8 43.6 40.4
2 51.7 49.5 49.7 48.3
3 50.7 48.4 50.1 48.6
?12 51.5 49.6 48.3 46.8
?23 52.1 49.9 50.9 49.5
?123 51.3 49.0 50.5 49.1
?PC 44.9 43.3 40.9 40.0
Table 1: Results for combinations of embedding
functions and set kernels
sociated with 10 or fewer and no sentences were
found for 45 constituent pairs. The largest context
sets were predominantly associated with political
or economic topics (e.g., government official, oil
price), reflecting the journalistic sources of the Gi-
gaword sentences.
Our implementation of relational similarity ap-
plies the two set kernels kave and kjsd defined in
Section 3.4 to these context sets. For each kernel
we tested gap-weighted embedding functions with
subsequence length values l in the range 1, 2, 3,
as well as summed kernels for all combinations
of values in this range. The decay parameter ?
for the subsequence feature embedding was set to
0.5 throughout, in line with previous recommen-
dations (e.g., Cancedda et al (2003)). To inves-
tigate the effects of varying set sizes, we ran ex-
periments with context sets of maximal cardinality
q ? {50, 250, 1000}. These sets were randomly
sampled for each compound; for compounds asso-
ciated with fewer strings than the maximal cardi-
nality, all associated strings were used. For q = 50
we average results over five runs in order to re-
duce sampling variation. We also report some
results with the PairClass embedding ?PC . The
restricted representative power of this embedding
brings greater efficiency and we were able to use
q = 5, 000; for all but 22 compounds, this allowed
the use of all contexts for which the ?PC embed-
ding was defined.
5 Results
Table 1 presents results for classification with re-
lational set kernels, using q = 1, 000 for the gap-
weighted embedding. In general, there is little dif-
ference between the performance of kjsd and kave
with ?gapl ; the only statistically significant differ-
ences (at p < 0.05, using paired t-tests) are be-
tween the kernels kl=1 with subsequence length
l = 1 and the summed kernels k?12 = kl=1+kl=2.
The best performance of 52.1% accuracy, 49.9%
F-score is obtained with the Jensen-Shannon ker-
nel kjsd computed on the summed feature embed-
dings of length 2 and 3. This is significantly lower
than the performance achieved by O? Se?aghdha
and Copestake (2008) with their lexical similar-
ity model, but it is well above the majority class
baseline (21.3%). Results for the PairClass em-
bedding are much lower than for the gap-weighted
embedding; the superiority of ?gapl is statistically
significant in all cases except l = 1.
Results for combinations of lexical co-
occurrence kernels and (gap-weighted) relational
set kernels are given in Table 2. With the excep-
tion of some combinations of the length-1 set
kernel, these results are clearly better than the
best results obtained with either the lexical or
the relational model taken alone. The best result
is obtained by the combining the lexical kernel
computed on BNC conjunction features with the
summed Jensen-Shannon set kernel k?23 ; this
combination achieves 63.1% accuracy and 61.6%
F-score, a statistically significant improvement (at
the p < 0.01 level) over the lexical kernel alone
and the best result yet reported for this dataset.
Also, the benefit of combining set kernels of
different subsequence lengths l is evident; of the
12 combinations presented Table 2 that include
summed set kernels, nine lead to statistically
significant improvements over the corresponding
lexical kernels taken alone (the remaining three
are also close to significance).
Our experiments also show that the distribu-
tional implementation of set kernels (6) is much
more efficient than the averaging implementation
(5). The time behaviour of the two methods
with increasing set cardinality q and subsequence
length l is illustrated in Figure 1. At the largest
tested values of q and l (1,000 and 3, respectively),
the averaging method takes over 33 days of CPU
time, while the distributional method takes just
over one day. In theory, kave scales quadratically
as q increases; this was not observed because for
many constituent pairs there are not enough con-
text strings available to keep adding as q grows
large, but the dependence is certainly superlinear.
The time taken by kjsd is theoretically linear in q,
but again scales less dramatically in practice. On
the other hand kave is linear in l, while kjsd scales
exponentially. This exponential dependence may
626
kjsd kave
BNC 5-Gram BNC 5-Gram
Length Acc F Acc F Acc F Acc F
1 60.6 58.6 60.3 58.1 59.5 57.6 59.1 56.5
2 61.9* 60.4* 62.6 60.8 62.0 60.5* 61.3 59.1
3 62.5* 60.8* 61.7 59.9 62.8* 61.2** 62.3** 60.8**
?12 62.6* 61.0** 62.3* 60.6* 62.0* 60.3* 61.5 59.2
?23 63.1** 61.6** 62.3* 60.5* 62.2* 60.7* 62.0 60.3
?123 62.9** 61.3** 62.6 60.8* 61.9* 60.4* 62.4* 60.6*
No Set 59.9 57.8 60.2 58.1 59.9 57.8 60.2 58.1
Table 2: Results for set kernel and lexical kernel combination. */** indicate significant improvement at
the 0.05/0.01 level over the corresponding lexical kernel alone, estimated by paired t-tests.
50 250 1000100
102
104
106
108
q
time/s
kave
kjsd
(a) l = 1
50 250 1000100
102
104
106
108
q
time/s
kave
kjsd
(b) l = 2
50 250 1000100
102
104
106
108
q
time/s
kavekjsd
(c) l = 3
Figure 1: Timing results (in seconds, log-scaled) for averaged and Jensen-Shannon set kernels
seem worrying, but in practice only short subse-
quence lengths are used with string kernels. In
situations where set sizes are small but long sub-
sequence features are desired, the averaging ap-
proach may be more appropriate. However, it
seems likely that many applications will be sim-
ilar to the task considered here, where short sub-
sequences are sufficient and it is desirable to use
as much data as possible to represent each set.
We note that calculating the PairClass embedding,
which counts far fewer patterns, took just 1h21m.
For optimal efficiency, it seems best to use a gap-
weighted embedding with small set cardinality;
averaged across five runs kjsd with q = 50 and
l = ?123 took 26m to calculate and still achieved
47.6% Accuracy, 45.1% F-score.
6 Related work
Turney et al (2003) suggest combining various in-
formation sources for solving SAT analogy prob-
lems. However, previous work on compound in-
terpretation has generally used either lexical simi-
larity or relational similarity but not both in com-
bination. Previously proposed lexical models in-
clude the WordNet-based methods of Kim and
Baldwin (2005) and Girju et al (2005), and the
distributional model of O? Se?aghdha and Copes-
take (2008). The idea of using relational similar-
ity to understand compounds goes back at least as
far as Lebowitz? (1988) RESEARCHER system,
which processed patent abstracts in an incremental
fashion and associated an unseen compound with
the relation expressed in a context where the con-
stituents previously occurred.
Turney (2006) describes a method (Latent Rela-
tional Analysis) that extracts subsequence patterns
for noun pairs from a large corpus, using query
expansion to increase the recall of the search and
feature selection and dimensionality reduction to
reduce the complexity of the feature space. LRA
performs well on analogical tasks including com-
pound interpretation, but has very substantial re-
source requirements. Turney (2008) has recently
proposed a simpler SVM-based algorithm for ana-
logical classification called PairClass. While it
does not adopt a set-based or distributional model
of relational similarity, we have noted above that
PairClass implicitly uses a feature representation
similar to the one presented above as (6) by ex-
tracting subsequence patterns from observed co-
occurrences of word pair members. Indeed, Pair-
Class can be viewed as a special case of our frame-
627
work; the differences from the model we have
used consist in the use of a different embedding
function ?PC and a more restricted notion of con-
text, a frequency cutoff to eliminate less common
subsequences and the Gaussian kernel to compare
vectors. While we cannot compare methods di-
rectly as we do not possess the large corpus of
5 ? 1010 words used by Turney, we have tested
the impact of each of these modifications on our
model.4 None improve performance with our set
kernels, but the only statistically significant effect
is that of changing the embedding model as re-
ported in section Section 5. Implementing the full
PairClass algorithm on our corpus yields 46.2%
accuracy, 44.9% F-score, which is again signifi-
cantly worse than all results for the gap-weighted
model with l > 1.
In NLP, there has not been widespread use of
set representations for data items, and hence set
classification techniques have received little at-
tention. Notable exceptions include Rosario and
Hearst (2005) and Bunescu and Mooney (2007),
who tackle relation classification and extraction
tasks by considering the set of contexts in which
the members of a candidate relation argument pair
co-occur. While this gives a set representation for
each pair, both sets of authors apply classifica-
tion methods at the level of individual set mem-
bers rather than directly comparing sets. There
is also a close connection between the multino-
mial probability model we have proposed and the
pervasive bag of words (or bag of n-grams) repre-
sentation. Distributional kernels based on a gap-
weighted feature embedding extend these models
by using bags of discontinuous n-grams and down-
weighting gappy subsequences.
A number of set kernels other than those dis-
cussed here have been proposed in the machine
learning literature, though none of these propos-
als have explicitly addressed the problem of com-
paring sets of strings or other structured objects,
and many are suitable only for comparing sets of
small cardinality. Kondor and Jebara (2003) take a
distributional approach similar to ours, fitting mul-
tivariate normal distributions to the feature space
mappings of setsA andB and comparing the map-
pings with the Bhattacharrya vector inner product.
The model described above in (6) implicitly fits
multinomial distributions in the feature space F ;
4Turney (p.c.) reports that the full PairClass model
achieves 50.0% accuracy, 49.3% F-score.
this seems more intuitive for string kernel embed-
dings that map strings onto vectors of positive-
valued ?counts?. Experiments with Kondor and
Jebara?s Bhattacharrya kernel indicate that it can
in fact come close to the performances reported
in Section 5 but has significantly greater compu-
tational requirements due to the need to perform
costly matrix manipulations.
7 Conclusion and future directions
In this paper we have presented a combined model
of lexical and relational similarity for relational
reasoning tasks. We have developed an efficient
and flexible kernel-based framework for compar-
ing sets of contexts using the feature embedding
associated with a string kernel.5 By choosing a
particular embedding function and a particular in-
ner product on subsequence vectors, the previ-
ously proposed set-averaging and PairClass algo-
rithms for relational similarity can be retrieved as
special cases. Applying our methods to the task
of compound noun interpretation, we have shown
that combining lexical and relational similarity is a
very effective approach that surpasses either simi-
larity model taken individually.
Turney (2008) argues that many NLP tasks can
be formulated in terms of analogical reasoning,
and he applies his PairClass algorithm to a number
of problems including SAT verbal analogy tests,
synonym/antonym classification and distinction
between semantically similar and semantically as-
sociated words. Our future research plans include
investigating the application of our combined sim-
ilarity model to analogical tasks other than com-
pound noun interpretation. A second promising
direction is to investigate relational models for un-
supervised semantic analysis of noun compounds.
The range of semantic relations that can be ex-
pressed by compounds is the subject of some con-
troversy (Ryder, 1994), and unsupervised learning
methods offer a data-driven means of discovering
relational classes.
Acknowledgements
We are grateful to Peter Turney, Andreas Vla-
chos and the anonymous EACL reviewers for their
helpful comments. This work was supported in
part by EPSRC grant EP/C010035/1.
5The treatment presented here has used a string represen-
tation of context, but the method could be extended to other
structural representations for which substructure embeddings
exist, such as syntactic trees (Collins and Duffy, 2001).
628
References
Thorsten Brants and Alex Franz, 2006. Web 1T 5-gram
Corpus Version 1.1. Linguistic Data Consortium.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the ACL-06 Interactive Presentation
Sessions.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the Web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL-07).
Lou Burnard, 1995. Users? Guide for the British Na-
tional Corpus. British National Corpus Consortium.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING-08).
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean-Michel Renders. 2003. Word-sequence ker-
nels. Journal of Machine Learning Research,
3:1059?1082.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of the
15th Conference on Neural Information Processing
Systems (NIPS-01).
Corinna Cortes and Vladimir Vapnik. 1995. Support
vector networks. Machine Learning, 20(3):273?
297.
Nello Cristianini, Jaz Kandola, Andre Elisseeff, and
John Shawe-Taylor. 2001. On kernel target algn-
ment. Technical Report NC-TR-01-087, Neuro-
COLT.
James Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, School of Informatics,
University of Edinburgh.
Barry Devereux and Fintan Costello. 2007. Learning
to interpret novel noun-noun compounds: Evidence
from a category learning experiment. In Proceed-
ings of the ACL-07 Workshop on Cognitive Aspects
of Computational Language Acquisition.
Thomas Ga?rtner, Peter A. Flach, Adam Kowalczyk,
and Alex J. Smola. 2002. Multi-instance kernels.
In Proceedings of the 19th International Conference
on Machine Learning (ICML-02).
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun
compounds. Computer Speech and Language,
19(4):479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of seman-
tic relations between nominals. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations (SemEval-07).
Alfio Gliozzo, Claudio Giuliano, and Carlo Strappar-
ava. 2005. Domain kernels for word sense disam-
biguation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05).
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda, 2005. English Gigaword Corpus, 2nd Edi-
tion. Linguistic Data Consortium.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cate-
gorisation. In Proceedings of the 18th International
Conference on Machine Learning (ICML-01).
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP-05).
Risi Kondor and Tony Jebara. 2003. A kernel between
sets of vectors. In Proceedings of the 20th Interna-
tional Conference on Machine Learning (ICML-03).
Michael Lebowitz. 1988. The use of memory in
text processing. Communications of the ACM,
31(12):1483?1502.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Christopher J. C. H. Watkins.
2002. Text classification using string kernels. Jour-
nal of Machine Learning Research, 2:419?444.
Diarmuid O? Se?aghdha and Ann Copestake. 2007. Co-
occurrence contexts for noun compound interpreta-
tion. In Proceedings of the ACL-07 Workshop on A
Broader Perspective on Multiword Expressions.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proceedings of the 22nd International Conference
on Computational Linguistics (COLING-08).
Barbara Rosario and Marti A. Hearst. 2005. Multi-
way relation classification: Application to protein-
protein interactions. In Proceedings of the 2005
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing (HLT-EMNLP-05).
Mary Ellen Ryder. 1994. Ordered Chaos: The Inter-
pretation of English Noun-Noun Compounds. Uni-
versity of California Press, Berkeley, CA.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press, Cambridge.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining indepen-
dent modules to solve multiple-choice synonym and
analogy problems. In Proceedings of the 2003 Inter-
national Conference on Recent Advances in Natural
Language Processing (RANLP-03).
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (COLING-08).
629
An Algebra for Semantic Construction in Constraint-based Grammars
Ann Copestake
Computer Laboratory
University of Cambridge
New Museums Site
Pembroke St, Cambridge, UK
aac@cl.cam.ac.uk
Alex Lascarides
Division of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, Scotland, UK
alex@cogsci.ed.ac.uk
Dan Flickinger
CSLI, Stanford University and
YY Software
Ventura Hall, 220 Panama St
Stanford, CA 94305, USA
danf@csli.stanford.edu
Abstract
We develop a framework for formaliz-
ing semantic construction within gram-
mars expressed in typed feature struc-
ture logics, including HPSG. The ap-
proach provides an alternative to the
lambda calculus; it maintains much of
the desirable flexibility of unification-
based approaches to composition, while
constraining the allowable operations in
order to capture basic generalizations
and improve maintainability.
1 Introduction
Some constraint-based grammar formalisms in-
corporate both syntactic and semantic representa-
tions within the same structure. For instance, Fig-
ure 1 shows representations of typed feature struc-
tures (TFSs) for Kim, sleeps and the phrase Kim
sleeps, in an HPSG-like representation, loosely
based on Sag and Wasow (1999). The semantic
representation expressed is intended to be equiv-
alent to r name(x,Kim) ? sleep(e, x).1 Note:
1. Variable equivalence is represented by coin-
dexation within a TFS.
2. The coindexation in Kim sleeps is achieved
as an effect of instantiating the SUBJ slot in
the sign for sleeps.
3. Structures representing individual predicate
applications (henceforth, elementary predi-
cations, or EPs) are accumulated by an ap-
pend operation. Conjunction of EPs is im-
plicit.
1The variables are free, we will discuss scopal relation-
ships and quantifiers below.
4. All signs have an index functioning some-
what like a ?-variable.
A similar approach has been used in a large
number of implemented grammars (see Shieber
(1986) for a fairly early example). It is in many
ways easier to work with than ?-calculus based
approaches (which we discuss further below) and
has the great advantage of allowing generaliza-
tions about the syntax-semantics interface to be
easily expressed. But there are problems. The
operations are only specified in terms of the TFS
logic: the interpretation relies on an intuitive cor-
respondence with a conventional logical represen-
tation, but this is not spelled out. Furthermore
the operations on the semantics are not tightly
specified or constrained. For instance, although
HPSG has the Semantics Principle (Pollard and
Sag, 1994) this does not stop the composition pro-
cess accessing arbitrary pieces of structure, so it
is often not easy to conceptually disentangle the
syntax and semantics in an HPSG. Nothing guar-
antees that the grammar is monotonic, by which
we mean that in each rule application the seman-
tic content of each daughter subsumes some por-
tion of the semantic content of the mother (i.e.,
no semantic information is dropped during com-
position): this makes it impossible to guarantee
that certain generation algorithms will work ef-
fectively. Finally, from a theoretical perspective,
it seems clear that substantive generalizations are
being missed.
Minimal Recursion Semantics (MRS: Copes-
take et al(1999), see also Egg (1998)) tight-
ens up the specification of composition a little.
It enforces monotonic accumulation of EPs by
making all rules append the EPs of their daugh-
ters (an approach which was followed by Sag
and Wasow (1999)) but it does not fully spec-
Kim
?
?
?
?
?
?
?
?
?
SYN
?
?
np
HEAD noun
SUBJ< >
COMPS< >
?
?
SEM
?
?
?
INDEX 5 ref-ind
RESTR<
[
RELN R NAME
INSTANCE 5
NAME KIM
]
>
?
?
?
?
?
?
?
?
?
?
?
?
sleeps
?
?
?
?
?
?
?
?
?
?
?
?
SYN
?
?
?
?
HEAD verb
SUBJ<
[
SYN np
SEM
[
INDEX 6
RESTR 7
]
]
>
COMPS< >
?
?
?
?
SEM
?
?
?
INDEX 15 event
RESTR<
[
RELN SLEEP
SIT 15
ACT 6
]
>
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Kim sleeps
?
?
?
?
?
?
?
?
?
?
SYN
[
HEAD 0 verb
]
SEM
?
?
?
INDEX 2 event
RESTR 10 <
[
RELN R NAME
INSTANCE 4
NAME KIM
]
> ? 11 <
[
RELN SLEEP
SIT 2 event
ACT 4
]
>
?
?
?
HEAD-DTR.SEM
[
INDEX 2
RESTR 10
]
NON-HD-DTR.SEM.RESTR 11
?
?
?
?
?
?
?
?
?
?
Figure 1: Expressing semantics in TFSs
ify compositional principles and does not for-
malize composition. We attempt to rectify these
problems, by developing an algebra which gives
a general way of expressing composition. The
semantic algebra lets us specify the allowable
operations in a less cumbersome notation than
TFSs and abstracts away from the specific fea-
ture architecture used in individual grammars, but
the essential features of the algebra can be en-
coded in the hierarchy of lexical and construc-
tional type constraints. Our work actually started
as an attempt at rational reconstruction of se-
mantic composition in the large grammar imple-
mented by the LinGO project at CSLI (available
via http://lingo.stanford.edu). Se-
mantics and the syntax/semantics interface have
accounted for approximately nine-tenths of the
development time of the English Resource Gram-
mar (ERG), largely because the account of seman-
tics within HPSG is so underdetermined.
In this paper, we begin by giving a formal ac-
count of a very simplified form of the algebra and
in ?3, we consider its interpretation. In ?4 to ?6,
we generalize to the full algebra needed to capture
the use of MRS in the LinGO English Resource
Grammar (ERG). Finally we conclude with some
comparisons to the ?-calculus and to other work
on unification based grammar.
2 A simple semantic algebra
The following shows the equivalents of the struc-
tures in Figure 1 in our algebra:
Kim: [x2]{[]subj , []comp}[r name(x2,Kim)]{}
sleeps: [e1]{[x1]subj , []comp}[sleep(e1, x1)]{}
Kim sleeps: [e1]{[]subj , []comp}[sleep(e1, x1),
r name(x2,Kim)]{x1 = x2}
The last structure is semantically equivalent to:
[sleep(e1, x1), r name(x1,Kim)].
In the structure for sleeps, the first part, [e1], is
a hook and the second part ([x1]subj and []comp)
is the holes. The third element (the lzt) is a bag
of elementary predications (EPs).2 Intuitively, the
hook is a record of the value in the semantic en-
tity that can be used to fill a hole in another entity
during composition. The holes record gaps in the
semantic form which occur because it represents
a syntactically unsaturated structure. Some struc-
tures have no holes, such as that for Kim. When
structures are composed, a hole in one structure
(the semantic head) is filled with the hook of the
other (by equating the variables) and their lzts are
appended. It should be intuitively obvious that
there is a straightforward relationship between
this algebra and the TFSs shown in Figure 1, al-
though there are other TFS architectures which
would share the same encoding.
We now give a formal description of the alge-
bra. In this section, we simplify by assuming that
each entity has only one hole, which is unlabelled,
and only consider two sorts of variables: events
and individuals. The set of semantic entities is
built from the following vocabulary:
2As usual in MRS, this is a bag rather than a set because
we do not want to have to check for/disallow repeated EPs;
e.g., big big car.
1. The absurdity symbol ?.
2. indices i1, i2, . . ., consisting of two subtypes
of indices: events e1, e2, . . . and individuals
x1, x2, . . ..
3. n-place predicates, which take indices as ar-
guments
4. =.
Equality can only be used to identify variables of
compatible sorts: e.g., x1 = x2 is well formed,
but e = x is not. Sort compatibility corresponds
to unifiability in the TFS logic.
Definition 1 Simple Elementary Predications
(SEP)
An SEP contains two components:
1. A relation symbol
2. A list of zero or more ordinary variable ar-
guments of the relation (i.e., indices)
This is written relation(arg1, . . . ,argn). For in-
stance, like(e, x, y) is a well-formed SEP.
Equality Conditions: Where i1 and i2 are in-
dices, i1 = i2 is an equality condition.
Definition 2 The Set ? of Simple semantic Enti-
ties (SSEMENT)
s ? ? if and only if s = ? or s = ?s1, s2, s3, s4?
such that:
? s1 = {[i]} is a hook;
? s2 = ? or {[i?]} is a hole;
? s3 is a bag of SEPs(the lzt)
? s4 is a set of equalities between variables
(the eqs).
We write a SSEMENT as: [i1][i2][SEPs]{EQs}.
Note for convenience we omit the set markers {}
from the hook and hole when there is no possible
confusion. The SEPs, and EQs are (partial) de-
scriptions of the fully specified formulae of first
order logic.
Definition 3 The Semantic Algebra
A Semantic Algebra defined on vocabulary V is
the algebra ??, op? where:
? ? is the set of SSEMENTs defined on the vo-
cabulary V , as given above;
? op : ? ? ? ?? ? is the operation of se-
mantic composition. It satisfies the follow-
ing conditions. If a1 = ? or a2 = ? or
hole(a2) = ?, then op(a1, a2) = ?. Other-
wise:
1. hook(op(a1, a2)) = hook(a2)
2. hole(op(a1, a2)) = hole(a1)
3. lzt(op(a1, a2)) = lzt(a1) ? lzt(a2)
4. eq(op(a1, a2)) = Tr(eq(a1)?eq(a2)?
hook(a1) = hole(a2)})
where Tr stands for transitive closure
(i.e., if S = {x = y, y = z}, then
Tr(S) = {x = y, y = z, x = z}).
This definition makes a2 the equivalent of a se-
mantic functor and a1 its argument.
Theorem 1 op is a function
If a1 = a3 and a2 = a4, then a5 = op(a1, a2) =
op(a3, a4) = a6. Thus op is a function. Further-
more, the range of op is within ?. So ??, op? is
an algebra.
We can assume that semantic composition al-
ways involves two arguments, since we can de-
fine composition in ternary rules etc as a sequence
of binary operations. Grammar rules (i.e., con-
structions) may contribute semantic information,
but we assume that this information obeys all the
same constraints as the semantics for a sign, so
in effect such a rule is semantically equivalent to
having null elements in the grammar. The corre-
spondence between the order of the arguments to
op and linear order is specified by syntax.
We use variables and equality statements to
achieve the same effect as coindexation in TFSs.
This raises one problem, which is the need to
avoid accidental variable equivalences (e.g., acci-
dentally using x in both the signs for cat and dog
when building the logical form of A dog chased
a cat). We avoid this by adopting a convention
that each instance of a lexical sign comes from
a set of basic sements that have pairwise distinct
variables. The equivalent of coindexation within
a lexical sign is represented by repeating the same
variable but the equivalent of coindexation that
occurs during semantic composition is an equality
condition which identifies two different variables.
Stating this formally is straightforward but a little
long-winded, so we omit it here.
3 Interpretation
The SEPs and EQs can be interpreted with respect
to a first order model ?E,A, F ? where:
1. E is a set of events
2. A is a set of individuals
3. F is an interpretation function, which as-
signs tuples of appropriate kinds to the pred-
icates of the language.
The truth definition of the SEPs and EQs
(which we group together under the term SMRS,
for simple MRS) is as follows:
1. For all events and individuals v, [v]?M,g? =
g(v).
2. For all n-predicates Pn,
[Pn]?M,g? = {?t1, . . . , tn? : ?t1, . . . , tn? ?
F (Pn)}.
3. [Pn(v1, . . . , vn)]?M,g? = 1 iff
?[v1]?M,g?, . . . , [vn]?M,g?? ? [Pn]?M,g?.
4. [? ? ?]?M,g? = 1 iff
[?]?M,g? = 1 and [?]?M,g? = 1.
Thus, with respect to a modelM , an SMRS can be
viewed as denoting an element of P(G), where
G is the set of variable assignment functions (i.e.,
elements ofG assign the variables e, . . . and x, . . .
their denotations):
[smrs]M = {g : g is a variable assignment
function and M |=g smrs}
We now consider the semantics of the algebra.
This must define the semantics of the operation op
in terms of a function f which is defined entirely
in terms of the denotations of op?s arguments. In
other words, [op(a1, a2)] = f([a1], [a2]) for
some function f . Intuitively, where the SMRS
of the SEMENT a1 denotes G1 and the SMRS of
the SEMENT a2 denotes G2, we want the seman-
tic value of the SMRS of op(a1, a2) to denote the
following:
G1 ?G2 ? [hook(a1) = hole(a2)]
But this cannot be constructed purely as a func-
tion of G1 and G2.
The solution is to add hooks and holes to the
denotations of SEMENTS (cf. Zeevat, 1989). We
define the denotation of a SEMENT to be an ele-
ment of I ? I ? P(G), where I = E ? A, as
follows:
Definition 4 Denotations of SEMENTs
If a 6= ? is a SEMENT, [[a]]M = ?[i], [i?], G?
where:
1. [i] = hook(a)
2. [i?] = hole(a)
3. G = {g : M |=g smrs(a)}
[[?]]M = ??, ?, ??
So, the meanings of SEMENTs are ordered three-
tuples, consisting of the hook and hole elements
(from I) and a set of variable assignment func-
tions that satisfy the SMRS.
We can now define the following operation f
over these denotations to create an algebra:
Definition 5 Semantics of the Semantic Con-
struction Algebra
?I ? I ? P(G), f? is an algebra, where:
f(??, ?, ??, ?[i2], [i?2], G2?) = ??, ?, ??
f(?[i1], [i?1], G1?, ??, ?, ??) = ??, ?, ??
f(?[i1], [i?1], G1?, ?[i2], ?, G2? = ??, ?, ??
f(?[i1], [i?1], G1?, ?[i2], [i
?
2], G2?) =
?[i2], [i?1], G1 ?G2 ?G
??
where G? = {g : g(i1) = g(i?2)}
And this operation demonstrates that semantic
construction is compositional:
Theorem 2 Semantics of Semantic Construction
is Compositional
The mapping [[]] : ??, op? ?? ??I, I,G?, f?
is a homomorphism (so [[op(a1, a2)]] =
f([[a1]], [[a2]])).
This follows from the definitions of [], op and f .
4 Labelling holes
We now start considering the elaborations neces-
sary for real grammars. As we suggested earlier,
it is necessary to have multiple labelled holes.
There will be a fixed inventory of labels for any
grammar framework, although there may be some
differences between variants.3 In HPSG, comple-
ments are represented using a list, but in general
there will be a fixed upper limit for the number
of complements so we can label holes COMP1,
COMP2, etc. The full inventory of labels for
3For instance, Sag and Wasow (1999) omit the distinction
between SPR and SUBJ that is often made in other HPSGs.
the ERG is: SUBJ, SPR, SPEC, COMP1, COMP2,
COMP3 and MOD (see Pollard and Sag, 1994).
To illustrate the way the formalization goes
with multiple slots, consider opsubj :
Definition 6 The definition of opsubj
opsubj(a1, a2) is the following: If a1 = ? or a2 =
? or holesubj(a2) = ?, then opsubj(a1, a2) = ?.
And if ?l 6= subj such that:
|holel(a1) ? holel(a2)| > 1
then opsubj(a1, a2) = ?. Otherwise:
1. hook(opsubj(a1, a2)) = hook(a2)
2. For all labels l 6= subj:
holel(opsubj(a1, a2)) = holel(a1) ?
holel(a2)
3. lzt(opsubj(a1, a2)) = lzt(a1) ? lzt(a2)
4. eq(opsubj(a1, a2)) = Tr(eq(a1) ? eq(a2)?
{hook(a1) = holesubj(a2)})
where Tr stands for transitive closure.
There will be similar operations opcomp1,
opcomp2 etc for each labelled hole. These
operations can be proved to form an algebra
??, opsubj , opcomp1, . . .? in a similar way to the
unlabelled case shown in Theorem 1. A lit-
tle more work is needed to prove that opl is
closed on ?. In particular, with respect to
clause 2 of the above definition, it is necessary
to prove that opl(a1, a2) = ? or for all labels l?,
|holel?(opl(a1, a2))| ? 1, but it is straightforward
to see this is the case.
These operations can be extended in a straight-
forward way to handle simple constituent coor-
dination of the kind that is currently dealt with
in the ERG (e.g., Kim sleeps and talks and Kim
and Sandy sleep); such cases involve daughters
with non-empty holes of the same label, and
the semantic operation equates these holes in the
mother SEMENT.
5 Scopal relationships
The algebra with labelled holes is sufficient to
deal with simple grammars, such as that in Sag
and Wasow (1999), but to deal with scope, more is
needed. It is now usual in constraint based gram-
mars to allow for underspecification of quantifier
scope by giving labels to pieces of semantic in-
formation and stating constraints between the la-
bels. In MRS, labels called handles are associ-
ated with each EP. Scopal relationships are rep-
resented by EPs with handle-taking arguments.
If all handle arguments are filled by handles la-
belling EPs, the structure is fully scoped, but in
general the relationship is not directly specified
in a logical form but is constrained by the gram-
mar via additional conditions (handle constraints
or hcons).4 A variety of different types of condi-
tion are possible, and the algebra developed here
is neutral between them, so we will simply use
relh to stand for such a constraint, intending it to
be neutral between, for instance, =q (qeq: equal-
ity modulo quantifiers) relationships used in MRS
and the more usual ? relationships from UDRT
(Reyle, 1993). The conditions in hcons are accu-
mulated by append.
To accommodate scoping in the algebra, we
will make hooks and holes pairs of indices and
handles. The handle in the hook corresponds to
the LTOP feature in MRS. The new vocabulary is:
1. The absurdity symbol ?.
2. handles h1, h2, . . .
3. indices i1, i2, . . ., as before
4. n-predicates which take handles and indices
as arguments
5. relh and =.
The revised definition of an EP is as in MRS:
Definition 7 Elementary Predications (EPs)
An EP contains exactly four components:
1. a handle, which is the label of the EP
2. a relation
3. a list of zero or more ordinary variable ar-
guments of the relation (i.e., indices)
4. a list of zero or more handles corresponding
to scopal arguments of the relation.
4The underspecified scoped forms which correspond to
sentences can be related to first order models of the fully
scoped forms (i.e., to models of WFFs without labels) via
supervaluation (e.g., Reyle, 1993). This corresponds to stip-
ulating that an underspecified logical form u entails a base,
fully specified form ? only if all possible ways of resolving
the underspecification in u entails ?. For reasons of space,
we do not give details here, but note that this is entirely con-
sistent with treating semantics in terms of a description of
a logical formula. The relationship between the SEMENTS
of non-sentential constituents and a more ?standard? formal
language such as ?-calculus will be explored in future work.
This is written h:r(a1, . . . ,an,sa1, . . . ,sam). For
instance, h:every(x, h1, h2) is an EP.5
We revise the definition of semantic entities to
add the hcons conditions and to make hooks and
holes pairs of handles and indices.
H-Cons Conditions: Where h1 and h2 are
handles, h1relhh2 is an H-Cons condition.
Definition 8 The Set ? of Semantic Entities
s ? ? if and only if s = ? or s =
?s1, s2, s3, s4, s5? such that:
? s1 = {[h, i]} is a hook;
? s2 = ? or {[h?, i?]} is a hole;
? s3 is a bag of EP conditions
? s4 is a bag of HCONS conditions
? s5 is a set of equalities between variables.
SEMENTs are: [h1, i1]{holes}[eps][hcons]{eqs}.
We will not repeat the full composition def-
inition, since it is unchanged from that in ?2
apart from the addition of the append operation
on hcons and a slight complication of eq to deal
with the handle/index pairs:
eq(op(a1, a2)) = Tr(eq(a1) ? eq(a2)?
{hdle(hook(a1)) = hdle(hole(a2)),
ind(hook(a1)) = ind(hole(a2))})
where Tr stands for transitive closure as before
and hdle and ind access the handle and index of
a pair. We can extend this to include (several) la-
belled holes and operations, as before. And these
revised operations still form an algebra.
The truth definition for SEMENTS is analogous
to before. We add to the model a set of la-
bels L (handles denote these via g) and a well-
founded partial order ? on L (this helps interpret
the hcons; cf. Fernando (1997)). A SEMENT then
denotes an element of H? . . .H?P(G), where
the Hs (= L? I) are the new hook and holes.
Note that the language ? is first order, and
we do not use ?-abstraction over higher or-
der elements.6 For example, in the standard
Montagovian view, a quantifier such as every
5Note every is a predicate rather than a quantifier in
this language, since MRSs are partial descriptions of logical
forms in a base language.
6Even though we do not use ?-calculus for composition,
we could make use of ?-abstraction as a representation de-
vice, for instance for dealing with adjectives such as former,
cf., Moore (1989).
is represented by the higher-order expression
?P?Q?x(P (x), Q(x)). In our framework, how-
ever, every is the following (using qeq conditions,
as in the LinGO ERG):
[hf , x]{[]subj , []comp1, [h?, x]spec, . . .}
[he : every(x, hr, hs)][hr =q h?]{}
and dog is:
[hd, y]{[]subj , []comp1, []spec, . . .}[hd : dog(y)][]{}
So these composes via opspec to yield every dog:
[hf , x]{[]subj , []comp1, []spec, . . .}
[he : every(x, hr, hs), hd : dog(y)]
[hr =q h?]{h? = hd, x = y}
This SEMENT is semantically equivalent to:
[hf , x]{[]subj , []comp1, []spec, . . .}
[he : every(x, hr, hs), hd : dog(x)][hr =q hd]{}
A slight complication is that the determiner is
also syntactically selected by the N? via the SPR
slot (following Pollard and Sag (1994)). How-
ever, from the standpoint of the compositional
semantics, the determiner is the semantic head,
and it is only its SPEC hole which is involved: the
N? must be treated as having an empty SPR hole.
In the ERG, the distinction between intersective
and scopal modification arises because of distinc-
tions in representation at the lexical level. The
repetition of variables in the SEMENT of a lexical
sign (corresponding to TFS coindexation) and the
choice of type on those variables determines the
type of modification.
Intersective modification: white dog:
dog: [hd, y]{[]subj , []comp1, . . . , []mod}
[hd : dog(y)][]{}
white: [hw, x]{[]subj , []comp1, .., [hw, x]mod}
[hw : white(x)][]{}
white dog: [hw, x]{[]subj , []comp1, . . . , []mod}
(opmod) [hd : dog(y), hw : white(x)][]
{hw = hd, x = y}
Scopal Modification: probably walks:
walks: [hw, e?]{[h?, x]subj , []comp1, . . . , []mod}
[hw : walks(e?, x)][]{}
probably: [hp, e]{[]subj , []comp1, . . . , [h, e]mod}
[hp : probably(hs)][hs =q h]{}
probably [hp, e]{[h?, x]subj , []comp1, . . . , []mod}
walks: [hp:probably(hs), hw:walks(e?, x)]
(opmod) [hs =q h]{hw = h, e = e?}
6 Control and external arguments
We need to make one further extension to allow
for control, which we do by adding an extra slot to
the hooks and holes corresponding to the external
argument (e.g., the external argument of a verb
always corresponds to its subject position). We
illustrate this by showing two uses of expect; note
the third slot in the hooks and holes for the exter-
nal argument of each entity. In both cases, x?e is
both the external argument of expect and its sub-
ject?s index, but in the first structure x?e is also the
external argument of the complement, thus giving
the control effect.
expect 1 (as in Kim expected to sleep)
[he, ee, x?e]{[hs, x
?
e, x
?
s]subj , [hc, ec, x
?
e]comp1, . . .}
[he : expect(ee, x?e, h?e)][h?e =q hc]{}
expect 2 (Kim expected that Sandy would sleep)
[he, ee, x?e]{[hs, x
?
e, x
?
s]subj , [hc, ec, x
?
c]comp1, . . .}
[h : expect(ee, x?e, h?e)][h?e =q hc]{}
Although these uses require different lexical en-
tries, the semantic predicate expect used in the
two examples is the same, in contrast to Montago-
vian approaches, which either relate two distinct
predicates via meaning postulates, or require an
additional semantic combinator. The HPSG ac-
count does not involve such additional machinery,
but its formal underpinnings have been unclear:
in this algebra, it can be seen that the desired re-
sult arises as a consequence of the restrictions on
variable assignments imposed by the equalities.
This completes our sketch of the algebra neces-
sary to encode semantic composition in the ERG.
We have constrained accessibility by enumerating
the possible labels for holes and by stipulating the
contents of the hooks. We believe that the han-
dle, index, external argument triple constitutes all
the semantic information that a sign should make
accessible to a functor. The fact that only these
pieces of information are visible means, for in-
stance, that it is impossible to define a verb that
controls the object of its complement.7 Although
obviously changes to the syntactic valence fea-
tures would necessitate modification of the hole
labels, we think it unlikely that we will need to in-
crease the inventory further. In combination with
7Readers familiar with MRS will notice that the KEY fea-
ture used for semantic selection violates these accessibility
conditions, but in the current framework, KEY can be re-
placed by KEYPRED which points to the predicate alone.
the principles defined in Copestake et al(1999)
for qeq conditions, the algebra presented here re-
sults in a much more tightly specified approach
to semantic composition than that in Pollard and
Sag (1994).
7 Comparison
Compared with ?-calculus, the approach to com-
position adopted in constraint-based grammars
and formalized here has considerable advantages
in terms of simplicity. The standard Montague
grammar approach requires that arguments be
presented in a fixed order, and that they be strictly
typed, which leads to unnecessary multiplication
of predicates which then have to be interrelated
by meaning postulates (e.g., the two uses of ex-
pect mentioned earlier). Type raising also adds
to the complexity. As standardly presented, ?-
calculus does not constrain grammars to be mono-
tonic, and does not control accessibility, since the
variable of the functor that is ?-abstracted over
may be arbitrarily deeply embedded inside a ?-
expression.
None of the previous work on unification-
based approaches to semantics has considered
constraints on composition in the way we have
presented. In fact, Nerbonne (1995) explicitly
advocates nonmonotonicity. Moore (1989) is
also concerned with formalizing existing prac-
tice in unification grammars (see also Alshawi,
1992), though he assumes Prolog-style unifica-
tion, rather than TFSs. Moore attempts to for-
malize his approach in the logic of unification,
but it is not clear this is entirely successful. He
has to divorce the interpretation of the expres-
sions from the notion of truth with respect to the
model, which is much like treating the semantics
as a description of a logic formula. Our strategy
for formalization is closest to that adopted in Uni-
fication Categorial Grammar (Zeevat et al 1987),
but rather than composing actual logical forms we
compose partial descriptions to handle semantic
underspecification.
8 Conclusions and future work
We have developed a framework for formally
specifying semantics within constraint-based rep-
resentations which allows semantic operations in
a grammar to be tightly specified and which al-
lows a representation of semantic content which
is largely independent of the feature structure ar-
chitecture of the syntactic representation. HPSGs
can be written which encode much of the algebra
described here as constraints on types in the gram-
mar, thus ensuring that the grammar is consistent
with the rules on composition. There are some as-
pects which cannot be encoded within currently
implemented TFS formalisms because they in-
volve negative conditions: for instance, we could
not write TFS constraints that absolutely prevent
a grammar writer sneaking in a disallowed coin-
dexation by specifying a path into the lzt. There is
the option of moving to a more general TFS logic
but this would require very considerable research
to develop reasonable tractability. Since the con-
straints need not be checked at runtime, it seems
better to regard them as metalevel conditions on
the description of the grammar, which can any-
way easily be checked by code which converts the
TFS into the algebraic representation.
Because the ERG is large and complex, we have
not yet fully completed the exercise of retrospec-
tively implementing the constraints throughout.
However, much of the work has been done and
the process revealed many bugs in the grammar,
which demonstrates the potential for enhanced
maintainability. We have modified the grammar
to be monotonic, which is important for the chart
generator described in Carroll et al(1999). A
chart generator must determine lexical entries di-
rectly from an input logical form: hence it will
only work if all instances of nonmonotonicity can
be identified in a grammar-specific preparatory
step. We have increased the generator?s reliability
by making the ERG monotonic and we expect fur-
ther improvements in practical performance once
we take full advantage of the restrictions in the
grammar to cut down the search space.
Acknowledgements
This research was partially supported by the Na-
tional Science Foundation, grant number IRI-
9612682. Alex Lascarides was supported by an
ESRC (UK) research fellowship. We are grateful
to Ted Briscoe, Alistair Knott and the anonymous
reviewers for their comments on this paper.
References
Alshawi, Hiyan [1992] (ed.) The Core Language
Engine, MIT Press.
Carroll, John, Ann Copestake, Dan Flickinger
and Victor Poznanski [1999] An Efficient Chart
Generator for Lexicalist Grammars, The 7th In-
ternational Workshop on Natural Language Gen-
eration, 86?95.
Copestake, Ann, Dan Flickinger, Ivan Sag
and Carl Pollard [1999] Minimal Recursion Se-
mantics: An Introduction, manuscript at www-
csli.stanford.edu/?aac/newmrs.ps
Egg, Marcus [1998] Wh-Questions in Under-
specified Minimal Recursion Semantics, Journal
of Semantics, 15.1:37?82.
Fernando, Tim [1997] Ambiguity in Changing
Contexts, Linguistics and Philosophy, 20.6: 575?
606.
Moore, Robert C. [1989] Unification-based Se-
mantic Interpretation, The 27th Annual Meeting
for the Association for Computational Linguistics
(ACL-89), 33?41.
Nerbonne, John [1995] Computational
Semantics?Linguistics and Processing, Shalom
Lappin (ed.) Handbook of Contemporary
Semantic Theory, 461?484, Blackwells.
Pollard, Carl and Ivan Sag [1994] Head-
Driven Phrase Structure Grammar, University of
Chicago Press.
Reyle, Uwe [1993] Dealing with Ambiguities
by Underspecification: Construction, Represen-
tation and Deduction, Journal of Semantics, 10.1:
123?179.
Sag, Ivan, and Tom Wasow [1999] Syntactic
Theory: An Introduction, CSLI Publications.
Shieber, Stuart [1986] An Introduction to
Unification-based Approaches to Grammar,
CSLI Publications.
Zeevat, Henk [1989] A Compositional Ap-
proach to Discourse Representation Theory, Lin-
guistics and Philosophy, 12.1: 95?131.
Zeevat, Henk, Ewan Klein and Jo Calder
[1987] An introduction to unification categorial
grammar, Nick Haddock, Ewan Klein and Glyn
Morrill (eds), Categorial grammar, unification
grammar, and parsing: working papers in cogni-
tive science, Volume 1, 195?222, Centre for Cog-
nitive Science, University of Edinburgh.
Generating Referring Expressions in Open Domains
Advaith Siddharthan Ann Copestake
Computer Science Department Computer Laboratory
Columbia University University of Cambridge
as372@cs.columbia.edu aac10@cl.cam.ac.uk
Abstract
We present an algorithm for generating referring
expressions in open domains. Existing algorithms
work at the semantic level and assume the avail-
ability of a classification for attributes, which is
only feasible for restricted domains. Our alterna-
tive works at the realisation level, relies on Word-
Net synonym and antonym sets, and gives equiva-
lent results on the examples cited in the literature
and improved results for examples that prior ap-
proaches cannot handle. We believe that ours is
also the first algorithm that allows for the incremen-
tal incorporation of relations. We present a novel
corpus-evaluation using referring expressions from
the Penn Wall Street Journal Treebank.
1 Introduction
Referring expression generation has historically
been treated as a part of the wider issue of gener-
ating text from an underlying semantic representa-
tion. The task has therefore traditionally been ap-
proached at the semantic level. Entities in the real
world are logically represented; for example (ignor-
ing quantifiers), a big brown dog might be repre-
sented as big1(x) ? brown1(x) ? dog1(x), where
the predicates big1, brown1 and dog1 represent dif-
ferent attributes of the variable (entity) x. The task
of referring expression generation has traditionally
been framed as the identification of the shortest log-
ical description for the referent entity that differen-
tiates it from all other entities in the discourse do-
main. For example, if there were a small brown dog
(small1(x) ? brown1(x) ? dog1(x)) in context, the
minimal description for the big brown dog would be
big1(x) ? dog1(x)1.
This semantic framework makes it difficult to ap-
ply existing referring expression generation algo-
rithms to the many regeneration tasks that are im-
portant today; for example, summarisation, open-
ended question answering and text simplification.
Unlike in traditional generation, the starting point in
1The predicate dog1 is selected because it has a distin-
guished status, referred to as type in Reiter and Dale (1992).
One such predicate has to to be present in the description.
these tasks is unrestricted text, rather than a seman-
tic representation of a small domain. It is difficult
to extract the required semantics from unrestricted
text (this task would require sense disambiguation,
among other issues) and even harder to construct
a classification for the extracted predicates in the
manner that existing approaches require (cf., ?2).
In this paper, we present an algorithm for generat-
ing referring expressions in open domains. We dis-
cuss the literature and detail the problems in apply-
ing existing approaches to reference generation to
open domains in ?2. We then present our approach
in ?3, contrasting it with existing approaches. We
extend our approach to handle relations in ?3.3 and
present a novel corpus-based evaluation on the Penn
WSJ Treebank in ?4.
2 Overview of Prior Approaches
The incremental algorithm (Reiter and Dale, 1992)
is the most widely discussed attribute selection
algorithm. It takes as input the intended refer-
ent and a contrast set of distractors (other enti-
ties that could be confused with the intended refer-
ent). Entities are represented as attribute value ma-
trices (AVMs). The algorithm also takes as input
a *preferred-attributes* list that contains, in order
of preference, the attributes that human writers use
to reference objects. For example, the preference
might be {colour, size, shape...}. The algorithm
then repeatedly selects attributes from *preferred-
attributes* that rule out at least one entity in the
contrast set until all distractors have been ruled out.
It is instructive to look at how the incremental al-
gorithm works. Consider an example where a large
brown dog needs to be referred to. The contrast set
contains a large black dog. These are represented
by the AVMs shown below.
?
?
type dog
size large
colour brown
?
?
?
?
type dog
size large
colour black
?
?
Assuming that the *preferred-attributes* list is
[size, colour, ...], the algorithm would first com-
pare the values of the size attribute (both large),
disregard that attribute as not being discriminating,
compare the values of the colour attribute and re-
turn the brown dog.
Subsequent work on referring expression genera-
tion has expanded the logical framework to allow
reference by negation (the dog that is not black)
and references to multiple entities (the brown or
black dogs) (van Deemter, 2002), explored different
search algorithms for finding the minimal descrip-
tion (e.g., Horacek (2003)) and offered different
representation frameworks like graph theory (Krah-
mer et al, 2003) as alternatives to AVMs. However,
all these approaches are based on very similar for-
malisations of the problem, and all make the follow-
ing assumptions:
1. A semantic representation exists.
2. A classification scheme for attributes exists.
3. The linguistic realisations are unambiguous.
4. Attributes cannot be reference modifying.
All these assumptions are violated when we move
from generation in a very restricted domain to re-
generation in an open domain. In regeneration
tasks such as summarisation, open-ended question
answering and text simplification, AVMs for enti-
ties are typically constructed from noun phrases,
with the head noun as the type and pre-modifiers
as attributes. Converting words into semantic la-
bels would involve sense disambiguation, adding
to the cost and complexity of the analysis module.
Also, attribute classification is a hard problem and
there is no existing classification scheme that can be
used for open domains like newswire; for example,
WordNet (Miller et al, 1993) organises adjectives
as concepts that are related by the non-hierarchical
relations of synonymy and antonymy (unlike nouns
that are related through hierarchical links such as
hyponymy, hypernymy and metonymy). In addi-
tion, selecting attributes at the semantic level is
risky because their linguistic realisation might be
ambiguous and many common adjectives are pol-
ysemous (cf., example 1 in ?3.1). Reference modi-
fication, which has not been considered in the refer-
ring expression generation literature, raises further
issues; for example, referring to an alleged mur-
derer as the murderer is potentially libellous.
In addition to the above, there is the issue of over-
lap between values of attributes. The case of sub-
sumption (for example, that the colour red sub-
sumes crimson and the type dog subsumes chi-
huahua) has received formal treatment in the liter-
ature; Dale and Reiter (1995) provide a find-best-
value function that evaluates tree-like hierarchies
of values. As mentioned earlier, such hierarchi-
cal knowledge bases do not exist for open domains.
Further, a treatment of subsumption is insufficient,
and degrees of intersection between attribute values
also require consideration. van Deemter (2000) dis-
cusses the generation of vague descriptions when
entities have gradable attributes like size; for ex-
ample, in a domain with four mice sized 2, 5, 7
and 10cm, it is possible to refer to the large mouse
(the mouse sized 10cm) or the two small mice (the
mice sized 2 and 5cm). However, when applying re-
ferring expression generation to regeneration tasks
where the representation of entities is derived from
text rather than a knowledge base, we have to con-
sider the case where the grading of attributes is not
explicit. For example, we might need to compare
the attribute dark with black, light or white.
In contrast to previous approaches, our algorithm
works at the level of words, not semantic labels, and
measures the relatedness of adjectives (lexicalised
attributes) using the lexical knowledge base Word-
Net rather than a semantic classification. Our ap-
proach also addresses the issue of comparing inter-
sective attributes that are not explicitly graded, by
making novel use of the synonymy and antonymy
links in WordNet. Further, it treats discriminating
power as only one criteria for selecting attributes
and allows for the easy incorporation of other con-
siderations such as reference modification (?5).
3 The Lexicalised Approach
3.1 Quantifying Discriminating Power
We define the following three quotients.
Similarity Quotient (SQ)
We define similarity as transitive synonymy. The
idea is that if X is a synonym of Y and Y is a syn-
onym of Z, then X is likely to be similar to Z. The
degree of similarity between two adjectives depends
on how many steps must be made through WordNet
synonymy lists to get from one to the other.
Suppose we need to find a referring expression
for e0. For each adjective aj describing e0, we cal-
culate a similarity quotient SQj by initialising it to
0, forming a set of WordNet synonyms S1 of aj ,
forming a synonymy set S2 containing all the Word-
Net synonyms of all the adjectives in S1 and form-
ing S3 from S2 similarly. Now for each adjective
describing any distractor, we increment SQj by 4 if
it is present in S1, by 2 if it is present in S2, and by 1
if it is present in S3. SQj now measures how similar
aj is to other adjectives describing distractors.
Contrastive Quotient (CQ)
Similarly, we define contrastive in terms of
antonymy relationships. We form the set C1 of
strict WordNet antonyms of aj . The set C2 con-
sists of strict WordNet antonyms of members of S1
and WordNet synonyms of members of C1. C3 is
similarly constructed from S2 and C2. We now ini-
tialise CQj to zero and for each adjective describing
each distractor, we add w =? {4, 2, 1} to CQj , de-
pending on whether it is a member of C1, C2 or C3.
CQj now measures how contrasting aj is to other
adjectives describing distractors.
Discriminating Quotient (DQ)
An attribute that has a high value of SQ has bad
discriminating power. An attribute that has a high
value of CQ has good discriminating power. We
can now define the Discriminating Quotient (DQ)
as DQ = CQ ? SQ. We now have an order (de-
creasing DQs) in which to incorporate attributes.
This constitutes our *preferred* list. We illustrate
the benefits of our approach with two examples.
Example 1: The Importance of Lexicalisation
Previous referring expression generation algorithms
ignore the issue of realising the logical description
for the referent. The semantic labels are chosen
such that they have a direct correspondence with
their linguistic realisation and the realisation is thus
considered trivial. Ambiguity and syntactically
optional arguments are ignored. To illustrate one
problem this causes, consider the two entities
below:
e1 e2
?
?
type president
age old
tenure current
?
?
?
?
type president
age young
tenure past
?
?
If we followed the strict typing system used
by previous algorithms, with *preferred*={age,
tenure}, to refer to e1 we would compare the
age attributes and rule out e2 and generate the
old president. This expression is ambiguous since
old can also mean previous. Models that select
attributes at the semantic level will run into trouble
when their linguistic realisations are ambiguous.
In contrast, our algorithm, given flattened attribute
lists:
e1 e2
[
head president
attrib old, current
][
head president
attrib young, past
]
successfully picks the current president as current
has a higher DQ (2) than old (0):
attribute distractor CQ SQ DQ
old e2{young, past} 4 4 0
current e2{young, past} 2 0 2
In this example, old is a WordNet antonym of young
and a WordNet synonym of past. Current is a
WordNet synonym of present, which is a WordNet
antonym of past. Note that WordNet synonym and
antonym links capture the implicit gradation in the
lexicalised values of the age and tenure attributes.
Example 2: Naive Incrementality
To illustrate another problem with the original in-
cremental algorithm, consider three dogs: e1(a big
black dog), e2(a small black dog) and e3(a tiny
white dog).
Consider using the original incremental algo-
rithm to refer to e1 with *preferred*={colour,
size}. The colour attribute black rules out e3.
We then we have to select the size attribute big as
well to rule out e2, thus generating the sub-optimal
expression the big black dog. Here, the use of a
predetermined *preferred* list fails to capture what
is obvious from the context: that e1 stands out not
because it is black, but because it is big.
In our approach, for each of e1?s attributes, we
calculate DQ with respect to e2 and e3:
attribute distractor CQ SQ DQ
big e2{small, black} 4 0 4
big e3{tiny, white} 2 0 2
black e2{small, black} 1 4 -3
black e3{tiny, white} 2 1 1
Overall, big has a higher discriminating power
(6) than black (-2) and rules out both e2 and e3.
We therefore generate the big dog. Our incremen-
tal approach thus manages to select the attribute that
stands out in context. This is because we construct
the *preferred* list after observing the context. We
discuss this issue further in the next section. Note
again that WordNet antonym and synonym links
capture the gradation in the lexicalised size and
colour attributes. However, this only works where
the gradation is along one axis; in particular, this
approach will not work for colours in general, and
cannot be used to deduce the relative similarity be-
tween yellow and orange as compared to, say, yel-
low and blue.
3.2 Justifying our Algorithm
The psycholinguistic justification for the incremen-
tal algorithm (IA) hinges on two premises:
1. Humans build referring expressions incrementally.
2. There is a preferred order in which humans select
attributes (e.g., colour>shape>size...).
Our algorithm is also incremental. However, it
departs significantly from premise 2. We assume
that speakers pick out attributes that are distinctive
in context (cf., example 2, previous section). Aver-
aged over contexts, some attributes have more dis-
criminating power than others (largely because of
the way we visualise entities) and premise 2 is an
approximation to our approach.
We now quantify the extra effort we are making
to identify attributes that ?stand out? in a given con-
text. Let N be the maximum number of entities in
the contrast set and n be the maximum number of
attributes per entity. The table below compares the
computational complexity of an optimal algorithm
(such as Reiter (1990)), our algorithm and the IA.
Incremental Algo Our Algorithm Optimal Algo
O(nN) O(n2N) O(n2N )
Both the IA and our algorithm are linear in the
number of entities N . This is because neither al-
gorithm allows backtracking; an attribute, once se-
lected, cannot be discarded. In contrast, an opti-
mal search requires O(2N ) comparisons. As our
algorithm compares each attribute of the discourse
referent to every attribute of every distractor, it is
quadratic in n. The IA compares each attribute of
the discourse referent to only one attribute per dis-
tractor and is linear in n. Note, however, that values
for n of over 4 are rare.
3.3 Relations
Semantically, attributes describe an entity (e.g., the
small grey dog) and relations relate an entity to
other entities (e.g., the dog in the bin). Relations
are troublesome because in relating an entity eo to
e1, we need to recursively generate a referring ex-
pression for e1. The IA does not consider relations
and the referring expression is constructed out of at-
tributes alone. The Dale and Haddock (1991) algo-
rithm allows for relational descriptions but involves
exponential global search, or a greedy search ap-
proximation. To incorporate relational descriptions
in the incremental framework would require a clas-
sification system which somehow takes into account
the relations themselves and the secondary entities
e1 etc. This again suggests that the existing algo-
rithms force the incrementality at the wrong stage
in the generation process. Our approach computes
the order in which attributes are incorporated after
observing the context, by quantifying their utility
through the quotient DQ. This makes it easy for
us to extend our algorithm to handle relations, be-
cause we can compute DQ for relations in much
the same way as we did for attributes.We illustrate
this for prepositions.
3.4 Calculating DQ for Relations
Suppose the referent entity eref contains a relation
[prepo eo] that we need to calculate the three quo-
tients for (cf., figure 1 for representation of rela-
tions in AVMs). We consider each entity ei in the
contrast set for eref in turn. If ei does not have a
prepo relation then the relation is useful and we in-
crement CQ by 4. If ei has a prepo relation then
two cases arise. If the object of ei?s prepo rela-
tion is eo then we increment SQ by 4. If it is not
eo, the relation is useful and we increment CQ by
4. This is an efficient non-recursive way of com-
puting the quotients CQ and SQ for relations. We
now discuss how to calculate DQ. For attributes,
we defined DQ = CQ ? SQ. However, as the lin-
guistic realisation of a relation is a phrase and not
a word, we would like to normalise the discriminat-
ing power of a relation with the length of its lin-
guistic realisation. Calculating the length involves
recursively generating referring expressions for the
object of the preposition, an expensive task that we
want to avoid unless we are actually using that rela-
tion in the final referring expression. We therefore
initially approximate the length as follows. The re-
alisation of a relation [prepo eo] consists of prepo,
a determiner and the referring expression for eo. If
none of eref?s distractors have a prepo relation then
we only require the head noun of eo in the refer-
ring expression and length = 3. In this case, the
relation is sufficient to identify both entities; for ex-
ample, even if there were multiple bins in figure 1,
as long as only one dog is in a bin, the reference
the dog in the bin succeeds in uniquely referencing
both the dog and the bin. If n distractors of eref
contain a prepo relation with a non-eo object that is
distractor for eo, we set length = 3 + n. This is an
estimate for the word length of the realised relation
that assumes one extra attribute for distinguishing
eo from each distractor. Normalisation by estimated
length is vital; if eo requires a long description, the
relations?s DQ should be small so that shorter pos-
sibilities are considered first in the incremental pro-
cess. The formula for DQ for relations is therefore
DQ = (CQ ? SQ)/length.
This approach can also be extended to allow for
relations such as comparatives which have syntac-
tically optional arguments (e.g., the earlier ight vs
the ight earlier than UA941) which are not allowed
for by approaches which ignore realisation.
3.5 The Lexicalised Context-Sensitive IA
Our lexicalised context-sensitive incremental algo-
rithm (below) generates a referring expression for
Entity. As it recurses, it keeps track of entities it has
used up in order to avoid entering loops like the dog
in the bin containing the dog in the bin.... To gener-
ate a referring expression for an entity, the algorithm
calculates the DQs for all its attributes and approxi-
mates the DQs for all its relations (2). It then forms
the *preferred* list (3) and constructs the referring
expression by adding elements of *preferred* till
the contrast set is empty (4). This is straightfor-
ward for attributes (5). For relations (6), it needs to
recursively generate the prepositional phrase first.
It checks that it hasn?t entered a loop (6a), gener-
ates a new contrast set for the object of the relation
(6(a)i), recursively generates a referring expression
for the object of the preposition (6(a)ii), recalculates
DQ (6(a)iii) and either incorporates the relation in
the referring expression or shifts the relation down
the *preferred* list (6(a)iv). This step ensures that
an initial mis-estimation in the word length of a re-
lation doesn?t force its inclusion at the expense of
shorter possibilities. If after incorporating all at-
tributes and relations, the contrast set is still non-
empty, the algorithm returns the best expression it
can find (7).
set generate-ref-exp(Entity, ContrastSet, UsedEntities)
1. IF ContrastSet = [] THEN RETURN {Entity.head}
2. Calculate CQ, SQ and DQ for each attribute and
relation of Entity (as in Sec 3.1 and 3.4)
3. Let *preferred* be the list of attributes/ relations
sorted in decreasing order of DQs. FOR each ele-
ment (Mod) of *preferred* DO steps 4, 5 and 6
4. IF ContrastSet = [] THEN RETURN RefExp ?
{Entity.head}
5. IF Mod is an Attribute THEN
(a) LET RefExp = {Mod} ? RefExp
(b) Remove from ContrastSet, any entities Mod
rules out
6. IF Mod is a Relation [prepi ei] THEN
(a) IF ei ? UsedEntities THEN
i. Set DQ = ??
ii. Move Mod to the end of *preferred*
ELSE
i. LET ContrastSet2 be the set of non-ei en-
tities that are the objects of prepi rela-
tions in members of ContrastSet
ii. LET RE = generate-referring-exp(ei,
ContrastSet2, {ei}?UsedEntities)
iii. recalculate DQ using length = 2 +
length(RE)
iv. IF position in *preferred* is lowered
THEN re-sort *preferred*
ELSE
(?) SET RefExp = RefExp ?
{[prepi|determiner|RE]}
(?) Remove from ContrastSet, any
entities that Mod rules out
7. RETURN RefExp ? {Entity.head}
An Example Trace:
We now trace the algorithm above as it generates a
referring expression for d1 in figure 1.
call generate-ref-exp(d1,[d2],[])
? step 1: ContrastSet is not empty
? step 2: DQsmall = ?4, DQgrey = ?4
DQ[in b1] = 4/3, DQ[near d2] = 4/4
? step 3: *preferred* = [[in b1], [near d2], small,
grey]
d2
d1
b1
d1
?
?
?
?
head dog
attrib [small,
grey]
in b1
near d2
?
?
?
?
d2
?
?
?
?
head dog
attrib [small,
grey]
outside b1
near d1
?
?
?
?
b1
?
?
?
head bin
attrib [large, steel]
containing d1
near d2
?
?
?
Figure 1: AVMs for two dogs and a bin
? Iteration 1 ? mod = [in b1]
? step 6(a)i: ContrastSet2 = []
? step 6(a)ii: call generate-ref-exp(b1,[],[d1])
? step 1: ContrastSet = []
return {bin}
? step 6(a)iii: DQ[in b1] = 4/3
? step 6(a)iv?: RefExp = {[in, the, {bin}]}
? step 6(a)iv?: ContrastSet = []
? Iteration 2 ? mod = [near d2]
? step 4: ContrastSet = []
return {[in the {bin}], dog}
The algorithm presented above is designed to re-
turn the shortest referring expression that uniquely
identifies an entity. If the scene in figure 1 were clut-
tered with bins, the algorithm would still refer to d1
as the dog in the bin as there is only one dog that is
in a bin. The user gets no help in locating the bin.
If helping the user locate entities is important to the
discourse plan, we need to change step 6(a)(ELSE)i
so that the contrast set includes all bins in context,
not just bins that are objects of in relations of dis-
tractors of d1.
3.6 Compound Nominals
Our analysis so far has assumed that attributes are
adjectives. However, many nominals introduced
through relations can also be introduced in com-
pound nominals, for example:
1. a church in Paris ? a Paris church
2. a novel by Archer ? an Archer novel
3. a company from London ? a London company
This is an important issue for regeneration appli-
cations, where the AVMs for entities are constructed
from text rather than a semantic knowledge base
(which could be constructed such that such cases
are stored in relational form, though possibly with
an underspecified relation). We need to augment our
algorithm so that it can compare AVMs like:
[
head church
in
[
head Paris ]
]
and
[
head church
attrib [Paris]
]
Formally, the algorithm for calculating SQ and
CQ for a nominal attribute anom of entity eo is:
FOR each distractor ei of eo DO
1. IF anom is similar to any nominal attribute of ei
THEN SQ = SQ + 4
2. IF anom is similar to the head noun of the object
of any relation of ei THEN
(a) SQ = SQ + 4
(b) flatten that relation for ei, i.e., add the at-
tributes of the object of the relation to the at-
tribute list for ei
In step 2, we compare a nominal attribute anom
of eo to the head noun of the object of a relation
of ei. If they are similar, it is likely that any at-
tributes of that object might help distinguish eo from
ei. We then add those attributes to the attribute list
of ei. Now, if SQ is non-zero, the nominal at-
tribute anom has bad discriminating power and we
set DQ = ?SQ. If SQ = 0, then anom has good
discriminating power and we set DQ = 4.
We also extend the algorithm for calculating DQ
for a relation [prepj ej ] of eo as follows:
1. IF any distractor ei has a nominal attribute anom
THEN
(a) IF anom is similar to the head of ej THEN
i. Add all attributes of eo to the attribute list
and calculate their DQs
2. calculate DQ for the relation as in section 3.4
We can demonstrate how this approach works us-
ing entities extracted from the following sentence
(from the Wall Street Journal):
Also contributing to the firmness in copper, the
analyst noted, was a report by Chicago pur-
chasing agents, which precedes the full pur-
chasing agents report that is due out today and
gives an indication of what the full report might
hold.
Consider generating a referring expression for eo
when the distractor is e1:
eo =
?
?
?
?
head report
by
?
?
head agents
attrib [Chicago,
purchasing]
?
?
?
?
?
?
e1 =
[
head report
attributes [full, purchasing, agents]
]
The distractor the full purchasing agents report
contains the nominal attribute agents. To compare
report by Chicago purchasing agents with full pur-
chasing agents report, our algorithm flattens the for-
mer to Chicago purchasing agents report. Our algo-
rithm now gives:
DQagents = ?4, DQpurchasing = ?4,
DQChicago = 4, DQby Chicago purchasing agents = 4/4
We thus generate the referring expression the
Chicago report. This approach takes advantage of
the flexibility of the relationships that can hold be-
tween nouns in a compound: although examples can
be devised where removing a nominal causes un-
grammaticality, it works well enough empirically.
To generate a referring expression for e1 (full
purchasing agents report) when the distractor is
eo(report by Chicago purchasing agents), our algo-
rithm again flattens eo to obtain:
DQagents = ?4, DQpurchasing = ?4
DQfull = 4
The generated referring expression is the full report.
This is identical to the referring expression used in
the original text.
4 Evaluation
As our algorithm works in open domains, we were
able to perform a corpus-based evaluation using the
Penn WSJ Treebank (Marcus et al, 1993). Our eval-
uation aimed to reproduce existing referring expres-
sions (NPs with a definite determiner) in the Penn
Treebank by providing our algorithm as input:
1. The first mention NP for that reference.
2. The contrast set of distractor NPs
For each referring expression (NP with a definite
determiner) in the Penn Treebank, we automatically
identified its first mention and all its distractors in a
four sentence window, as described in ?4.1. We then
used our program to generate a referring expres-
sion for the first mention NP, giving it a contrast-
set containing the distractor NPs. Our evaluation
compared this generated description with the orig-
inal WSJ reference that we had started out with.
Our algorithm was developed using toy examples
and counter-examples constructed by hand, and the
Penn Treebank was unseen data for this evaluation.
4.1 Identifying Antecedents and Distractors
For every definite noun phrase NPo in the Penn
Treebank, we shortlisted all the noun phrases NPi
in a discourse window of four sentences (the two
preceding sentences, current sentence and the fol-
lowing sentence) that had a head noun identical to
or a WordNet synonym of the head noun of NPo.
We compared the set of attributes and relations
for each shortlisted NPi that preceded NPo in the
discourse window with that of NPo. If the attributes
and relations set of NPi was a superset of that of
NPo, we assumed that NPo referred to NPi and
added NPi to an antecedent set. We added all other
NPi to the contrast set of distractors.
Similarly, we excluded any noun phrase NPi that
appeared in the discourse after NPo whose attributes
and relations set was a subset of NPo?s and added
the remaining NPi to the contrast set. We then se-
lected the longest noun phrase in the antecedent set
to be the antecedent that we would try and generate
a referring expression from.
The table below gives some examples of distrac-
tors that our program found using WordNet syn-
onyms to compare head nouns:
Entity Distractors
first half-free Soviet vote fair elections in the GDR
military construction bill fiscal measure
steep fall in currency drop in market stock
permanent insurance death benefit coverage
4.2 Results
There were 146 instances of definite descriptions in
the WSJ where the following conditions (that ensure
that the referring expression generation task is non-
trivial) were satisfied:
1. The definite NP (referring expression) contained at
least one attribute or relation.
2. An antecedent was found for the definite NP.
3. There was at least one distractor NP in the dis-
course window.
In 81.5% of these cases, our program returned a
referring expression that was identical to the one
used in the WSJ. This is a surprisingly high accu-
racy, considering that there is a fair amount of vari-
ability in the way human writers use referring ex-
pressions. For comparison, the baseline of repro-
ducing the antecedent NP performed at 48%2.
Some errors were due to non-recognition of mul-
tiword expessions in the antecedent (for example,
our program generated care product from personal
care product). In many of the remaining error cases,
it was difficult to decide whether what our pro-
gram generated was acceptable or wrong. For ex-
ample, the WSJ contained the referring expression
the one-day limit, where the automatically detected
antecedent was the maximum one-day limit for the
2We are only evaluating content selection (the nouns and
pre- and post-modifiers) and ignore determiner choice.
S&P 500 stock-index futures contract and the auto-
matically detected contrast set was:
{the five-point opening limit for the contract,
the 12-point limit, the 30-point limit, the in-
termediate limit of 20 points}
Our program generated the maximum limit, where
the WSJ writer preferred the one-day limit.
5 Further Issues
5.1 Reference Modifying Attributes
The analysis thus far has assumed that all at-
tributes modify the referent rather than the refer-
ence to the referent. However, for example, if e1
is an alleged murderer, the attribute alleged mod-
ifies the reference murderer rather than the refer-
ent e1 and referring to e1 as the murderer would
be factually incorrect. Logically e1 could be rep-
resented as (alleged1(murderer1))(x), rather than
alleged1(x) ? murderer1(x). This is no longer
first-order, and presents new difficulties for the tra-
ditional formalisation of the reference generation
problem. One (inelegant) solution would be to in-
troduce a new predicate allegedMurderer1(x).
A working approach in our framework would be
to add a large positive weight to the DQs of refer-
ence modifying attributes, thus forcing them to be
selected in the referring expression.
5.2 Discourse Context and Salience
The incremental algorithm assumes the availability
of a contrast set and does not provide an algorithm
for constructing and updating it. The contrast set, in
general, needs to take context into account. Krah-
mer and Theune (2002) propose an extension to the
IA which treats the context set as a combination of a
discourse domain and a salience function. The black
dog would then refer to the most salient entity in the
discourse domain that is both black and a dog.
Incorporating salience into our algorithm is
straightforward. As described earlier, we compute
the quotients SQ and CQ for each attribute or re-
lation by adding an amount w ? {4, 2, 1} to the
relevant quotient based on a comparison with the at-
tributes and relations of each distractor. We can in-
corporate salience by weighting w with the salience
of the distractor whose attribute or relation we are
considering. This will result in attributes and rela-
tions with high discriminating power with regard to
more salient distractors getting selected first in the
incremental process.
5.3 Discourse Plans
In many situations, attributes and relations serve dif-
ferent discourse functions. For example, attributes
might be used to help the hearer identify an entity
while relations might serve to help locate the en-
tity. This needs to be taken into account when gen-
erating a referring expression. If we were gener-
ating instructions for using a machine, we might
want to include both attributes and relations; so to
instruct the user to switch on the power, we might
say switch on the red button on the top-left corner.
This would help the user locate the switch (on the
top-left corner) and identify it (red). If we were
helping a chef find the salt in a kitchen, we might
want to use only relations because the chef knows
what salt looks like. The salt behind the corn akes
on the shelf above the fridge is in this context prefer-
able to the white powder. If the discourse plan that
controls generation requires our algorithm to pref-
erentially select relations or attributes, it can add a
positive amount ? to their DQs. Then, the resultant
formula is DQ = (CQ ? SQ)/length + ?, where
length = 1 for attributes and by default ? = 0 for
both relations and attributes.
6 Conclusions and Future Work
We have described an algorithm for generating re-
ferring expressions that can be used in any domain.
Our algorithm selects attributes and relations that
are distinctive in context. It does not rely on the
availability of an adjective classification scheme and
uses WordNet antonym and synonym lists instead.
It is also, as far as we know, the first algorithm that
allows for the incremental incorporation of relations
and the first that handles nominals. In a novel eval-
uation, our algorithm successfully generates identi-
cal referring expressions to those in the Penn WSJ
Treebank in over 80% of cases.
In future work, we plan to use this algorithm as
part of a system for generation from a database of
user opinions on products which has been automat-
ically extracted from newsgroups and similar text.
This is midway between regeneration and the clas-
sical task of generating from a knowledge base be-
cause, while the database itself provides structure,
many of the field values are strings corresponding
to phrases used in the original text. Thus, our lexi-
calised approach is directly applicable to this task.
7 Acknowledgements
Thanks are due to Kees van Deemter and three
anonymous ACL reviewers for useful feedback on
prior versions of this paper.
This document was generated partly in the con-
text of the Deep Thought project, funded under
the Thematic Programme User-friendly Information
Society of the 5th Framework Programme of the Eu-
ropean Community (Contract N IST-2001-37836)
References
Robert Dale and Nicholas Haddock. 1991. Gen-
erating referring expressions involving relations.
In Proceedings of the 5th Conference of the Eu-
ropean Chapter of the Association for Compu-
tational Linguistics (EACL?91), pages 161?166,
Berlin, Germany.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gen-
eration of referring expressions. Cognitive Sci-
ence, 19:233?263.
Helmut Horacek. 2003. A best-first search algo-
rithm for generating referring expressions. In
Proceedings of the 11th Conference of the Eu-
ropean Chapter of the Association for Compu-
tational Linguistics (EACL?03), pages 103?106,
Budapest, Hungary.
Emiel Krahmer and Marie?t Theune. 2002. Efficient
context-sensitive generation of referring expres-
sions. In Kees van Deemter and Rodger Kib-
ble, editors, Information Sharing: Givenness and
Newness in Language Processing, pages 223?
264. CSLI Publications, Stanford,California.
Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg. 2003. Graph-based generation of re-
ferring expressions. Computational Linguistics,
29(1):53?72.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large natural
language corpus of English: The Penn Treebank.
Computational Linguistics, 19:313?330.
George A. Miller, Richard Beckwith, Christiane D.
Fellbaum, Derek Gross, and Katherine Miller.
1993. Five Papers on WordNet. Technical report,
Princeton University, Princeton, N.J.
Ehud Reiter. 1990. The computational complex-
ity of avoiding conversational implicatures. In
Proceedings of the 28th Annual Meeting of Asso-
ciation for Computational Linguistics (ACL?90),
pages 97?104, Pittsburgh, Pennsylvania.
Ehud Reiter and Robert Dale. 1992. A fast al-
gorithm for the generation of referring expres-
sions. In Proceedings of the 14th International
Conference on Computational Linguistics (COL-
ING?92), pages 232?238, Nantes, France.
Kees van Deemter. 2000. Generating vague de-
scriptions. In Proceedings of the 1st Interna-
tional Conference on Natural Language Genera-
tion (INLG?00), pages 179?185, Mitzpe Ramon,
Israel.
Kees van Deemter. 2002. Generating referring ex-
pressions: Boolean extensions of the incremental
algorithm. Computational Linguistics, 28(1):37?
52.
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 5?8,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Generating research websites using summarisation techniques
Advaith Siddharthan & Ann Copestake
Natural Language and Information Processing Group
Computer Laboratory, University of Cambridge
{as372,aac10}@cl.cam.ac.uk
Abstract
We describe an application that generates web
pages for research institutions by summarising
terms extracted from individual researchers?
publication titles. Our online demo covers all
researchers and research groups in the Com-
puter Laboratory, University of Cambridge.
We also present a novel visualisation interface
for browsing collaborations.
1 Introduction
Many research organisations organise their websites
as a tree (e.g., department pages ? research group
pages ? researcher pages). Individual researchers
take responsibility for maintaining their own web
pages and, in addition, researchers are organised
into research groups that also maintain a web page.
In this framework, information easily gets outdated,
and publications lists generally stay more up-to-date
than research summaries. Also, as individuals main-
tain their own web pages, connections between re-
searchers in the organisation are often hard to find;
a surfer then needs to move up and down the tree
hierarchy to browse the profiles of different peo-
ple. Browsing is also diffcult because individual
web pages are organised differently, since standard-
ised stylesheets are often considered inappropriate
for diverse organisations.
Research summary pages using stylesheets can
offer alternative methods of information access and
browsing, aiding navigation and providing different
views for different user needs, but these are time-
consuming to create and maintain by hand. We are
exploring the idea of automatically generated and
updated web pages that accurately reflect the re-
search interests being pursued within a research in-
stitution. We take as input existing personal pages
from the Computer Laboratory, University of Cam-
bridge, that contain publication lists in html. In
our automatically generated pages, content (a re-
search summary) is extracted from publication ti-
tles, and hence stays up-to-date provided individ-
ual researchers maintain their publication lists. Note
that publication information is increasingly avail-
able through other sources, such as Google Scholar.
We aim to format information in a way that facil-
itates browsing; a screen shot is shown in Figure 1
for the researcher Frank Stajano, who is a member
of the Security and DTG research groups. The left
of the page contains links to researchers of the same
research groups and the middle contains a research
profile in the form of lists of key phrases presented
in five year intervals (by publication date). In addi-
tion, the right of the page contains a list of recom-
mendations: other researchers with similar research
interests. Web pages for research groups are created
by summarising the research profiles of individual
members. In addition, we present a novel interactive
visualisation that we have developed for displaying
collaborations with the rest of the world.
In this paper we describe our methodology for
identifying terms, clustering them and then creating
research summaries (?2) and a generative sum-
mariser of collaborations (?4) that plugs into a novel
visualisation (?3). An online demo is available at:
http://www.cl.cam.ac.uk/research/nl/webpage-demo/NLIP.html
2 Summarising research output
Our program starts with a list of publications ex-
tracted from researcher web pages; for example:
? S. Teufel. 2007. An Overview of evaluation meth-
ods in TREC Ad-hoc Information Retrieval and TREC
Question Answering. In Evaluation of Text and Speech
Systems. L. Dybkjaer, H. Hemsen, W. Minker (Eds.)
Springer, Dordrecht (The Netherlands).
5
From each publication entry such as that above,
the program extracts author names, title and year of
publication. This is the only information used. We
do not use the full paper, as pdfs are not available for
all papers in publication pages (due to copyright and
other issues). The titles are then parsed using the
RASP parser (Briscoe and Carroll, 2002) and key-
phrases are extracted by pattern matching. From the
publication entry above, the extracted title:
?An overview of evaluation methods in TREC ad-hoc
information retrieval and TREC question answering?
produces five key-phrases:
?evaluation methods?, ?evaluation methods in TREC
ad-hoc information retrieval?, ?TREC ad-hoc infor-
mation retrieval?, ?TREC question answering?, ?infor-
mation retrieval?
Figure 1: Screenshot: researcher web page.
http://www.cl.cam.ac.uk/research/nl/webpage-demo/Frank Stajano.html
Figure 2: Screenshot: research group web page.
http://www.cl.cam.ac.uk/research/nl/webpage-demo/DTG.html
2.1 Individual researcher summaries
To create a web page for an individual researcher,
the key-phrases extracted from all the paper titles
authored by that researcher are clustered together
based on similarity - an example cluster is shown
below (from Karen Sparck Jones? profile):
?automatic classification for information retrieval?,
?intelligent automatic information retrieval?, ?infor-
mation retrieval test collections?, ?information re-
trieval system?, ?automatic classification?, ?intelligent
retrieval?, ?information retrieval?, ?information sci-
ence?, ?test collections?, ?mail retrieval?, ?trec ad-hoc
information retrieval?
A representative phrase (most similar to others in
the cluster) is selected from each cluster (?informa-
tion retrieval? from the above) and this phrase is
linked with all the publication dates for papers the
terms in the cluster come from. These extracted key-
phrases are enumerated as lists in five year intervals;
for example (from Karen Sparck Jones? profile):
1990?1994: ?information retrieval?; ?document re-
trieval?; ?video mail retrieval?; ?automatic summari-
sation?; ?belief revision?; ?discourse structure?; ?cam-
bridge/olivetti retrieval system?; ?system architec-
ture?; ?agent interaction?; ?better NLP system evalua-
tion?; ?early classification work?; ?text retrieval?; ?dis-
course modelling?...;
2.2 Recommendations (related people)
Recommendations for related people are generated
by comparing the terms extracted between 2000 and
2008 for each researcher in the Computer Labora-
tory. The (at most) seven most similar researchers
are shown in tabular form along with a list of terms
from their profiles that are relevant to the researcher
being viewed. These term lists inform the user as to
why they might find the related people relevant.
2.3 Research Group Pages
Group pages are produced by summarising the pages
of members of the group. Terms from individual
research profiles are clustered according to who is
working on them (gleaned from the author lists of
the the associated paper title). The group page is pre-
sented as a list of clusters. This presentation shows
how group members collaborate, and for each term
shows the relevant researchers, making navigation
6
easier. Two clusters for the Graphics and Interaction
(Rainbow) Group are show below to illustrate:
?histogram warping?; ?non-uniform b-spline subdi-
vision?; ?stylised rendering?; ?multiresolution im-
age representation?; ?human behaviour?; ?subdivi-
sion schemes?; ?minimising gaussian curvature vari-
ation near extraordinary vertices?; ?sampled cp sur-
faces?; ?bounded curvature variants?: Neil Dodgson;
Thomas Cashman; Ursula Augsdorfer;
?text for multiprojector tiled displays?; ?tabletop in-
terface?; ?high-resolution tabletop applications?; ?dis-
tributed tabletops?; ?remote review meetings?; ?rapid
prototyping?: Peter Robinson; Philip Tuddenham;
3 Visualisation
Scalable Vector Graphics (SVG)1 is a language for
describing two-dimensional graphics and graphical
applications in XML. Interactive images such as
those in Figure 3 are produced by an XSLT script
that transforms an input XML data file containing
information about collaborations and latitudes and
longitudes of cities and countries into an SVG rep-
resentation2 . This can be viewed through an Adobe
Browser Plugin3. In the map, circles indicate the lo-
cations of co-authors of members of the NLIP re-
search group, their size being proportional to the
number of co-authors at that location. The map can
be zoomed into, and at sufficient zoom, place names
are made visible. Clicking on a location (circle) pro-
vides a summary of the collaboration (the summari-
sation is described in ?4), while clicking on a coun-
try (oval) provides a contrywise overview such as:
In the Netherlands, the NLIP Group has collabora-
tors in Philips Research (Eindhoven), University of
Twente (Enschede), Vrije Universiteit (VU) (Amster-
dam) and University of Nijmegen.
4 Summarising collaborations
Our summarisation module slots into the visualisa-
tion interface; an example is shown in Figure 4. The
aim is to summarise the topics that members of the
research group collaborate with the researchers in
1http://www.w3.org/Graphics/SVG/
2Author Affiliations and Latitudes/Longitudes are semi-
automatically extracted from the internet and hand corrected.
The visualisation is only available for some research groups.
3http://www.adobe.com/svg/viewer/install/main.html
Figure 3: Screenshot: Visualisation of Collaboration be-
tween the NLIP Group and the rest of the world
Figure 4: Screenshot: Visualisation of Collaborations of
ARG Group; zoomed into Europe and having clicked on
Catonia (Italy) for a popup summary
each location on. The space constraints are dic-
tated by the interface. To keep the visualisation
clean, we enforce a four sentence limit for the sum-
maries. There are four elements that each sentence
contains? names of researchers in research group,
names of researchers at location, terms that sum-
marise the collaboration, and years of collaboration.
Our summaries are produced by an iterative pro-
cess of clustering and summarising. In the first step,
terms (key phrases) are extracted from all the papers
that have co-authors in the location. Each term is
tagged with the year(s) of publication and the names
of researchers involved. These terms are then clus-
tered based on the similarity of words in the terms
and the similarity of their authors. Each such clus-
ter contributes one sentence to the summary. The
clustering process is pragmatic; the four sentence
per summary limit means that at most four clusters
should be formed. This means coarser clustering
(fewer and larger clusters) for locations with many
collaborations and finer-grained (more and smaller
clusters) for locations with fewer collaborations.
The next step is to generate a sentence from each
cluster. In this step, the terms in a sentence clus-
ter are reclustered according to their date tag. then
each time period is realised separately within the
sentence, for example:
7
Lawrence C Paulson collaborated with Cristiano
Longo and Giampaolo Bella from 1997 to 2003 on
?formal verification?, ?industrial payment and non-
repudiation protocol?, ?kerberos authentication sys-
tem? and ?secrecy goals? and in 2006 on ?cardholder
registration in Set? and ?accountability protocols?.
To make the summaries more readable, lists of
conjunctions are restricted to a maximum length of
four. Terms are incorporated into the list in decreas-
ing order of frequency of occurrence. Splitting the
sentence above into two time periods allows for the
inclusion of more terms, without violating the re-
striction on list length. This form of sentence split-
ting is also pragmatic and is performed more aggres-
sively in summaries with fewer sentences, having
the effect of making short summaries slightly longer.
Another method for increasing the number of terms
is by aggregating similar terms. In the example be-
low, three terms (video mail retrieval, information
retrieval and document retrieval) are aggregated into
one term. Thus six terms have made it to the clause,
while keeping to the four terms per list limit.
In the mid 1990s, K Sparck Jones, S J Young and
M G Brown collaborated with J T Foote on ?video
mail, information and document retrieval?, ?cam-
bridge/olivetti retrieval system?, ?multimedia docu-
ments? and ?broadcast news?.
The four word limit is also enforced on lists of
people. If there are too many people, the program
refers to them by affiliation; for example:
Joe Hurd collaborated with University of Utah on
?theorem proving?, ?encryption algorithms?, ?func-
tional correctness proofs? and ?Arm verification?.
5 Discussion and Conclusions
Our summarisation strategy mirrors the multi-
document summarisation strategy of Barzilay
(2003), where sentences in the input documents are
clustered according to their similarity. Larger clus-
ters represent information that is repeated more of-
ten; hence the size of a cluster is indicative of im-
portance. The novelty of our application is that this
strategy has been used at a sub-sentential level, to
summarise terms that are then used to generate sen-
tences. While there has been research on generative
summarisation, much of this has been focused on
sentence extraction followed by some rewrite oper-
ation (e.g., sentence shortening (Vanderwende et al,
2007; Zajic et al, 2006; Conroy et al, 2004), ag-
gregation (Barzilay, 2003) or reference regeneration
(Siddharthan et al, 2004; Nenkova and McKeown,
2003)). In contrast, our system does not extract sen-
tences at all; rather, it extracts terms from paper ti-
tles and our summaries are produced by clustering,
summarising, aggregating and generalising over sets
of terms and people. Our space constraints are dic-
tated by by our visualisation interface, and our pro-
gram employs pragmatic clustering and generalisa-
tion based on the amount of information it needs to
summarise.
Acknowledgements
This work was funded by the Computer Labora-
tory, University of Cambridge, and the EPSRC
(EP/C010035/1 and EP/F012950/1).
References
R. Barzilay. 2003. Information Fusion for Multidoc-
ument Summarization: Paraphrasing & Generation.
Ph.D. thesis, Columbia University.
E.J. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the
3rd International Conference on Language Resources
and Evaluation, pages 1499?1504, Las Palmas, Gran
Canaria.
J.M. Conroy, J.D. Schlesinger, J. Goldstein, and D.P.
O?Leary. 2004. Left-brain/right-brain multi-
document summarization. Proceedings of DUC 2004.
A. Nenkova and K. McKeown. 2003. References to
named entities: a corpus study. Companion pro-
ceedings of HLT-NAACL 2003?short papers-Volume 2,
pages 70?72.
A. Siddharthan, A. Nenkova, and K. McKeown. 2004.
Syntactic simplification for improving content selec-
tion in multi-document summarization. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics (COLING 2004), pages 896?902,
Geneva, Switzerland.
L. Vanderwende, H. Suzuki, C. Brockett, and
A. Nenkova. 2007. Beyond SumBasic: Task-
focused summarization with sentence simplification
and lexical expansion. Information Processing and
Management, 43(6):1606?1618.
D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2006.
Sentence Compression as a Component of a Multi-
Document Summarization System. Proceedings of the
2006 Document Understanding Workshop, New York.
8
Using an open-source unification-based system for CL/NLP teaching
Ann Copestake
Computer Laboratory
University of Cambridge
Cambridge, UK
aac@cl.cam.ac.uk
John Carroll
Cognitive and Computing Sciences
University of Sussex
Falmer, Brighton, UK
johnca@cogs.susx.ac.uk
Dan Flickinger
CSLI, Stanford University and
YY Software
Ventura Hall,
Stanford, USA
danf@csli.stanford.edu
Robert Malouf
Alfa Informatica,
University of Groningen,
Postbus 716, 9700 AS Groningen,
The Netherlands
malouf@let.rug.nl
Stephan Oepen
YY Software and
CSLI, Stanford University
110 Pioneer Way
Mountain View, USA
oe@yy.com
Abstract
We demonstrate the open-source LKB
system which has been used to teach the
fundamentals of constraint-based gram-
mar development to several groups of
students.
1 Overview of the LKB system
The LKB system is a grammar development
environment that is distributed as part of the
open source LinGO tools (http://www-
csli.stanford.edu/?aac/lkb.html
and http://lingo.stanford.edu, see
also Copestake and Flickinger, 2000). It is an
open-source grammar development environment
implemented in Common Lisp, distributed not
only as source but also as a standalone application
that can be run on Linux, Solaris and Windows
(see the website for specific requirements). It
will also run under Macintosh Common Lisp,
but for this a license is required. The LKB in-
cludes a parser, generator, support for large-scale
inheritance hierarchies (including the use of
defaults), various tools for manipulating semantic
representations, a rich set of graphical tools
for analyzing and debugging grammars, and
extensive on-line documentation. Grammars of
all sizes have been written using the LKB, for
several languages, mostly within the linguistic
frameworks of Categorial Grammar and Head-
Driven Phrase Structure Grammar. The LKB
system was initially developed in 1991, but has
gone through multiple versions since then. It
is in active use by a considerable number of
researchers worldwide. An introductory book
on implementing grammars in typed feature
structure formalisms using the LKB is near
completion (Copestake, in preparation).
2 Demo outline
Although the LKB has been successfully used for
large-scale grammar development, this demon-
stration will concentrate on its use with relatively
small scale teaching grammars, of a type which
can be developed by students in practical exer-
cises. We will show an English grammar frag-
ment which is linked to a textbook on formal syn-
tax (Sag and Wasow, 1999) to illustrate how the
system may be used in conjunction with more tra-
ditional materials in a relatively linguistically ori-
ented course. We will demonstrate the tools for
analyzing parses and for debugging and also dis-
cuss the way that parse selection mechanisms can
be incorporated in the system. If time permits, we
will show how semantic analyses produced with a
somewhat more complex grammar can be linked
up to a theorem prover and also exploited in se-
mantic transfer for Machine Translation. Exer-
cises where the grammar is part of a larger system
are generally appropriate for advanced courses or
for NLP application courses.
The screen dump in the figure is from a session
working with a grammar fragment for Esperanto.
This shares its basic types and rules with the
English textbook grammar fragment mentioned
above. The windows shown are:
1. The LKB Top interaction window: main
  
 
 
 
Figure 1: Screen dump of the LKB system
menus plus feedback and error messages
2. Type hierarchy window (fragment): the
more general types are on the left. Nodes in
the hierarchy have menus that provide more
information about the types, such as their as-
sociated constraints.
3. Type constraint for the type intrans-verb:
again nodes are clickable for further infor-
mation.
4. Parse tree for La knabo dormas (the boy
sleeps): a larger display for parse trees is
also available, but this scale is useful for
summary information. Menus associated
with trees allow for display of associated se-
mantic information if any is included in the
grammar and for generation. Here the dis-
play shows inflectional rules as well as nor-
mal syntactic rules: hence the VP node un-
der dormas, which corresponds to the stem.
5. In the middle is an emacs window displaying
the source file for the lexicon associated with
this grammar.1 It shows the entry for the lex-
1(We generally use emacs as an editor when teaching,
eme dorm, which, like most lexical entries in
this grammar, just specifies a spelling and a
type (here intrans-verb).
6. Part of the parse chart corresponding to the
tree is shown in the bottom window: nodes
which have knabo as a descendant are high-
lighted. Again, these nodes are active: one
very useful facility associated with them is a
unification checker which allows the gram-
mar writer to establish why a rule did not
apply to a phrase or phrases.
3 Use of the LKB in teaching
Teaching uses of the LKB have included under-
graduate and graduate courses on formal syntax
and on computational linguistics at several sites,
grammar engineering courses at two ESSLLI
summer schools, and numerous student projects
at undergraduate, masters and doctoral levels. An
advantage of the LKB is that students learn to use
a system which is sufficiently heavy duty for more
advanced work, up to the scale at least of research
although this causes some overhead, especially for students
who are only used to word processing programs.
prototypes. This provides them with a good plat-
form on which to build for further research. Feed-
back from the courses we have taught has mostly
been very positive, but we have found a ratio of
six students to one instructor (or teaching assis-
tant) to be the maximum that is workable. One
major reason is that debugging students? gram-
mars and teaching debugging techniques is time-
consuming.
When teaching an introductory course with the
LKB, we start the students off with a very sim-
ple grammar, which they are asked to expand
in specific ways. We introduce various addi-
tional techniques and formal devices (such as in-
flectional and lexical rules, defaults, difference
lists and gaps) gradually during a course. Mate-
rial from our ESSLLI courses, including starting
grammars, exercises and solutions is distributed
via the website. Several other small grammars
developed by students are also distributed as part
of the LKB system and we would welcome fur-
ther contributions. We are hoping to facilitate this
by making it easier for people outside the LinGO
group to add and modify grammars.
Several graduate students have used versions
of the LKB system as part of their thesis work,
for diverse projects including machine transla-
tion and grammar learning. It has been used
in the development of several large grammars,
especially the LinGO English Resource Gram-
mar (ERG), which is itself open-source. Re-
search applications for the ERG include spoken
language machine translation in Verbmobil, gen-
eration for a speech prosthesis, and automated
email response, under development for commer-
cial use. The LKB/ERG combination can be used
by researchers who require a grammar which pro-
vides a detailed semantic analysis and reason-
ably broad coverage, for instance for experiments
on dialogue. The LKB has also been used as
a grammar preprocessor to facilitate experiments
on efficiency using the ERG with other systems
(Flickinger et al 2000).
4 Comparison with other work
There is a long history of the use of fea-
ture structure based systems in teaching, dat-
ing back at least to PATR (Shieber, 1986:
see http://www.ling.gu.se/?li/). The
Alvey Natural Language Tools (Briscoe et al
1987) have been used for teaching at several uni-
versities: Briscoe and Grover developed an ex-
tensive set of teaching examples and exercises,
which is however unpublished. Versions of the
SRI Core Language Engine (Alshawi, 1992) and
of the XTAG grammar (XTAG group, 1995) and
parser have also been used for teaching. Besides
the LKB, typed feature structure environments
have been used at many universities, though un-
like the systems cited above, most have only been
used with small grammars and may not scale
up. Hands on courses using various systems have
been run at many recent summer schools includ-
ing ESSLLI 99 (using the Xerox XLE, see Butt
et al 1999) and ESSLLI 97 and the 1999 LSA
summer school (both using ConTroll, see Hin-
richs and Meurers, 1999). Very little seems to
have been formally published describing expe-
riences in teaching with grammar development
environments, though Bouma (1999) describes
material for teaching a computational linguistics
course that includes exercises using the Hdrug
unification-based enviroment to extend a gram-
mar.
Despite this rich variety of tools, we believe
that the LKB system has a combination of fea-
tures which make it distinctive and give it a useful
niche in teaching. The most important points are
that its availability as open source, combined with
scale and efficiency, allow advanced projects to be
supported as well as introductory courses. As far
as we are aware, it is the only system freely avail-
able with a broad coverage grammar that sup-
ports semantic interpretation and generation. Es-
pecially for more linguistically oriented courses,
the link to the Sag and Wasow textbook is also
important. Similar grammars could be developed
for other systems, but would be less directly com-
parable to the textbook since this assumes a de-
fault formalism which so far is only implemented
in the LKB.
On the other hand, the LKB is not a suitable ba-
sis for a course that involves the students learning
to implement a unifier, parser and so on. The sys-
tem is quite complex (about 120 files and 40,000
lines of Lisp code) and though the vast majority
of this is concerned with non-core functionality,
such as the graphical interfaces, it is still some-
what daunting. This seems an inevitable trade-
off of having a system powerful enough for real
applications (see Bouma (1999) for related dis-
cussion). It is questionable whether the LKB is
entirely satisfactory as a student?s first computa-
tional grammar system, although we have used it
with students who have no prior experience of this
sort: ideally we would suggest starting off with
brief exercises with a pure context-free grammar
to explain the concepts of well-formedness, re-
cursion and so on. We also wouldn?t necessar-
ily advocate using the LKB as a core component
of a first course on formal syntax for linguistic
students, since the specifics of dealing with an
implementation may interfere with understanding
of basic concepts, though it is suitable as a sup-
plement to an initial course or as the basis for a
slightly more advanced course.
We think there is considerable potential for
building materials for courses that allow students
to work with realistic but transparent applications
using the LKB and a large grammar as a compo-
nent. Developing such materials is clearly nec-
essary in order to give students useful practical
experience. It is however very time-consuming,
and most probably will have to be undertaken as
part of a cooperative, open-source development
involving people from several different institu-
tions.
Acknowledgements
This research was partially supported by the Na-
tional Science Foundation, grant number IRI-
9612682. The current versions of the English
grammars associated with the Sag and Wasow
textbook were largely developed by Christopher
Callison-Burch while he was an undergraduate at
Stanford.
References
Alshawi, Hiyan (ed). [1992] The Core Language
Engine, MIT Press, Cambridge, MA.
Bouma, Gosse. [1999] ?A modern computa-
tional linguistics course using Dutch.? In Frank
van Eynde and Ineke Schuurman, editors, CLIN
1998, Papers from the Ninth CLIN Meeting, Am-
sterdam. Rodopi Press.
Briscoe, Ted, Claire Grover, Bran Boguraev
and John Carroll. [1987] ?A formalism and en-
vironment for the development of a large gram-
mar of English?, Proceedings of the 10th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI-87), Milan, Italy, 703?708.
Butt, Miriam, Anette Frank and Jonas Kuhn.
[1999] ?Development of large scale LFG gram-
mars ? Linguistics, Engineering and Resources?,
http://www.xrce.xerox.com/people/
frank/esslli99-hp/index.html
Copestake, Ann. [in preparation] Implementing
typed feature structure grammars, CSLI Publica-
tions, Stanford.
Copestake, Ann and Dan Flickinger. [2000]
?An open-source grammar development environ-
ment and broad-coverage English grammar us-
ing HPSG?, Second conference on Language Re-
sources and Evaluation (LREC-2000), Athens,
Greece.
Flickinger, Daniel, Stephan Oepen, Hans
Uszkoreit and Jun?ichi Tsujii. [2000] Journal of
Natural Language Engineering. Special Issue on
Efficient Processing with HPSG: Methods, Sys-
tems, Evaluation, 6(1).
Hinrichs, Erhard and Detmar Meurers [1999]
?Grammar Development in Constraint-Based
Formalisms?,
http://www.ling.ohio-state.edu/
?dm/lehre/lsa99/material.html,
see also http://www.sfs.nphil.uni-
tuebingen.de/controll/
Sag, Ivan, and Tom Wasow [1999] Syntactic
Theory: An Introduction, CSLI Publications.
Shieber, Stuart [1986] An Introduction to
Unification-based Approaches to Grammar,
CSLI Publications.
The XTAG Research Group [1995]. ?A Lex-
icalized Tree Adjoining Grammar for English?
IRCS Report 95-03, University of Pennsylvania?
Lexical Encoding of MWEs
Aline Villavicencio
Department of Language and Linguistics
University of Essex
Wivenhoe Park
Colchester, CO4 3SQ, UK
and
Computer Laboratory, University of Cambridge
avill@essex.ac.uk
Ann Copestake, Benjamin Waldron, Fabre Lambeau
Computer Laboratory
University of Cambridge
William Gates Building, JJ Thomson Avenue
Cambridge, CB3 0FD, UK
 
aac10,bmw20,faml2  @cl.cam.ac.uk
Abstract
Multiword Expressions present a challenge for lan-
guage technology, given their flexible nature. Each
type of multiword expression has its own charac-
teristics, and providing a uniform lexical encoding
for them is a difficult task to undertake. Nonethe-
less, in this paper we present an architecture for the
lexical encoding of these expressions in a database,
that takes into account their flexibility. This encod-
ing extends in a straightforward manner the one re-
quired for simplex (single) words, and maximises
the information contained for them in the descrip-
tion of multiwords.
1 Introduction
Multiword Expressions (MWEs) can be defined as
idiosyncratic interpretations that cross word bound-
aries (or spaces) (from Sag et al (2002). They
comprise a wide-range of distinct but related phe-
nomena like idioms, phrasal verbs, noun-noun com-
pounds and many others, that due to their flexible
nature, are considered to be a challenge for many
areas of current language technology. Even though
some MWEs are fixed, and do not present inter-
nal variation, such as ad hoc, others are much more
flexible and allow different degrees of internal vari-
ability and modification, as, for instance, touch a
nerve (touch/find a nerve) and spill beans (spill sev-
eral/musical/mountains of beans). In terms of se-
mantics, some MWEs are opaque and their seman-
tics cannot be straightforwardly inferred from the
meanings of the component words (e.g. to kick the
bucket as to die). In other cases the meaning is more
transparent and can be inferred from the words in
the MWE (e.g. eat up, where the particle up adds a
completive sense to eat).
Given the flexibility and variation in form of
MWEs and the complex interrelations that may be
found between their components, an encoding that
treats them as invariant strings (a words with spaces
approach), will not be adequate to fully describe any
such expression appropriately with the exception of
the simplest fixed cases such as ad hoc ((Sag et al,
2002), (Calzolari et al, 2002)). Different strate-
gies for encoding MWEs have been employed by
different lexical resources with varying degrees of
success, depending on the type of MWE. One case
is the Alvey Tools Lexicon (Carroll and Grover,
1989), which has a good coverage of phrasal verbs,
providing extensive information about their syntac-
tic aspects (variation in word order, subcategorisa-
tion, etc), but it does not distinguish compositional
from non-compositional entries neither does it spec-
ify entries that can be productively formed. Word-
Net, on the other hand, covers a large number of
MWEs (Fellbaum, 1998), but does not provide in-
formation about their variability. Neither of these
resources covers idioms. The challenge in design-
ing adequate lexical resources for MWEs, is to en-
sure that the variability and the extra dimensions re-
quired by the different types of MWE can be cap-
tured. Such a move is called for by Calzolari et al
(2002) and Copestake et al (2002). Calzolari et al
(2002) discuss these problems while attempting to
establish the standards for MWE description in the
context of multilingual lexical resources. Their fo-
cus is on MWEs that are productive and that present
regularities that can be generalised and applied to
other classes of words that have similar properties.
Copestake et al (2002) present an initial schema for
MWE description and we build on these ideas here,
by proposing an architecture for a lexical encoding
of MWEs, which allows for a unified treatment of
different kinds of MWE.
In what follows, we start by laying out the min-
imal encoding needed for simplex (single) words.
Then, we analyse two different types of MWE (id-
ioms and verb-particle constructions), and discuss
their requirements for a lexical encoding. Given
these requirements, we present a possible encoding
for MWEs, that uniformly captures different types
of expressions. This database encoding minimises
the amount of information that needs to be specified
for MWE entries, by maximising the information
that can be obtained from simplex words, while re-
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 80-87
quiring only minimal modification to the encoding
used for simplex words. We finish with some dis-
cussion and conclusions.
2 Simplex Entries
Simplex entries, in this context, refer to simple stan-
dalone words that are defined independently of oth-
ers, and form the bulk of most lexical resources.
For these entries, it is necessary to define at least
their orthography, and syntactic and semantic char-
acteristics, but more information can also be spec-
ified, such as particular dialect, register, and so on,
and table 1 shows one such encoding. In this min-
imal encoding a lexical entry has an identifier (to
uniquely distinguish between the different entries
defining different combinations of parts-of-speech
and senses for a given word), the word?s orthog-
raphy, grammatical (syntactic and semantic) type
and predicate name.1 In the case of this example,
the identifier is like tv 1, which is an entry for the
verb like, with type trans-verb for transitive verbs,
and predicate name like v rel. A type like trans-
verb embodies the constraints defined for a given
construction (in this case transitive verbs), in a par-
ticular grammar, and these vary from grammar to
grammar. Thus, these words can be expanded into
full feature structures during processing according
to the constraints defined in a specific grammar.
Table 1: LINGO ERG lexical database encoding
identifier orthography type predicate
like tv 1 like trans-verb like v rel
This table shows a minimal encoding for simplex
words, but it can serve as basis for a more complete
one. That is the case of the LinGO ERG (Copes-
take and Flickinger, 2000) lexicon, which adopts for
its database version, a compatible but more com-
plex encoding which is successfully used to de-
scribe simplex words (Copestake et al, 2004). In
the next sections, we investigate what would be nec-
essary for extending this encoding for successfully
capturing MWEs.
3 Idioms
Idioms constitute a complex case of MWEs, al-
lowing a great deal of variation. Some idioms are
1The identifier and semantic relation names follow the stan-
dard adopted by the LinGO ERG (Copestake and Flickinger,
2000), while the grammatical type names are also compatible
with it.
very flexible and can be passivised, topicalised, in-
ternally modified, and/or have optional elements
(e.g. spill beans in those beans were spilt, users
spilt password beans and judges spill their musical
beans), while others are more inflexible and only ac-
cept morphological inflection (e.g. kick/kicks/kicked
the bucket).
In order to verify empirically the possible space
of variation that idioms allow, we analysed a sam-
ple of some of the most frequent idioms in English.
This sample was used for determining the require-
ments that an encoding needs in order to provide the
means of adequately capturing idioms.
The Collins Cobuild Dictionary of Idioms lists
approximately 4,400 idioms in English, and 750 of
them are marked as the most frequent listed.2 From
these, 100 idioms were randomly selected and anal-
ysed as described by Villavicencio and Copestake
(2002).
A great part of the idioms in this sample seems
to form natural classes that follow similar patterns
(e.g. the class of verb-object idioms, where an id-
iom consists of a specific verb that takes a specific
object such as rock boat and spill beans). The re-
maining idioms, on the other hand, cannot so eas-
ily be grouped together, forming a large tail of
classes often containing only one or two idioms (e.g.
thumbs up and quote, unquote).
Most of the idioms in this sample present a
large degree of variability, especially in terms of
their syntax, also allowing variable elements (throw
SOMEONE to the lions), and optional ones (in a
(tight) corner). The type of variation that these
MWEs allow seems to be linked to their decom-
posability (Nunberg et al, 1994) in the sense that
many idioms seem to be compositional if we con-
sider that some of their component words have non-
standard meanings. Then, using compositional pro-
cesses, the meaning of an idiom can be derived from
the meanings of its elements. Thus, in these idioms,
referred to as semantically decomposable idioms,
a meaning can be assigned to individual words (even
if some of them are non-standard meanings) from
where the meaning of the idiom can be composi-
tionally constructed. One example is spill the beans,
where if spill is paraphrased as reveal and beans as
secrets, the idiom can be interpreted as reveal se-
crets. On the other hand, an idiom like to kick the
bucket, meaning to die, according to this approach
is non decomposable.
When semantic decomposability is used as ba-
sis for the classification, the majority of the idioms
2These idioms have at least one occurrence in every 2 mil-
lion words of the corpus employed to build this dictionary.
in this sample is classified as decomposable, and a
few cases as non-decomposable. The decompos-
able cases correspond to the flexible idioms, and
the non-decomposable to the fixed ones, provid-
ing a clear cut division for their treatment. For the
non-decomposable idioms, a treatment of idioms as
words with space can be adopted similar to that of
simplex words, where in a single entry the orthog-
raphy of the component words is specified, along
with the syntactic and semantic type of the idiom,
and a corresponding predicate name. In addition,
for the cases that allow morphological inflection, it
is also important to define which of the elements
of the MWE can be inflected. In this case, an id-
iom like kick the bucket, is given the type of a nor-
mal intransitive verb, except that it is composed of
more than one word, and only the verb can be in-
flected (e.g. kick/kicked/kicks the bucket,...). Conse-
quently, an encoding for non-decomposable idioms
needs to allow the definition of several orthographic
elements for an entry, as well as the specification of
the entry?s orthographic element that allows inflec-
tion.
In order to capture the flexibility of decompos-
able idioms, a treatment using normal composi-
tional processes can be employed as discussed by
Copestake (1994). In this approach, each idiomatic
component of an idiom could be defined as a sep-
arate entry similar to that of a simplex word, ex-
cept that it would also be possible to specify a para-
phrase for its meaning. In the case of spill beans, it
would mean defining an entry for the idiomatic spill,
which can be paraphrased as reveal and another for
the idiomatic beans paraphrased as secrets. More-
over, as an idiomatic entry for a word may share
many of the properties of (one of) the word?s non-
idiomatic entries (sometimes differing from the lat-
ter only in terms of their semantics), it is important
to define also for each idiomatic element a corre-
sponding non-idiomatic one, from which many as-
pects will be inherited by default. For example, in
an idiom such as spill beans, the idiomatic entry for
spill shares with the non-idiomatic entry the mor-
phology (spilled or spilt) and the syntax (as a transi-
tive verb), and so does the idiomatic beans with the
non-idiomatic one. In addition, as there is a vari-
ability in the status of the words that form MWEs,
with some words having a more literal interpretation
and others a more idiomatic one, only the idiomatic
words need to have separate entries defined. For ex-
ample in the case of the idiom pull the plug, pull
can be interpreted as contributing one of its non-
idiomatic senses (that of removing), while plug has
an idiomatic interpretation (that can be understood
as meaning support). Thus, only an idiomatic en-
try (like that for plug) needs to be defined, while the
contribution of a non-idiomatic entry (like that for
pull) to the idiom comes from the standard entry for
that word.
Having idiomatic and non-idiomatic entries avail-
able for use in idioms is just the first step in being
able to capture this type of MWE. For a precise en-
coding of idioms, it is also necessary to define a very
specific context of use for the idiomatic entries, to
avoid the possibility of overgeneration. Thus, the
verb spill has its idiomatic meaning of reveal only in
the context of spilt the beans but not otherwise (e.g.
in spill the water). The definition of these idiomatic
contexts is important to ensure that idiomatic entries
are used only in the context of the idiom, and that
outside the idiom these entries are disallowed. Con-
versely, it is important to be able to define for each
idiom, all the elements that need to be present for
the idiomatic interpretation to be available. An id-
iom is only going to be understood as such if all of
its obligatory components are present. In addition, it
is necessary to ensure that the appropriate relation-
ship among the components of an idiom is found,
for the idiomatic meaning to be available, in order
to avoid the case of false positives, where all the
elements of an idiom are found, but not with the rel-
evant interrelations. Thus, a sentence like He threw
the cat among the pigeons has a possible idiomatic
interpretation available, but this interpretation is not
available in a sentence like He held the cat and she
threw the bread among the pigeons, even though it
has all the obligatory elements for the idiom (throw,
cat, among, pigeons), because cat did not occur as
a semantic argument (the agent) of throw. Many
idioms also present some slight variation in their
components, accepting any one of a restricted set of
words, as for example on home ground and on home
turf. Each of these possibilities corresponds to the
same idiom realised in a slightly different way, but
which nonetheless has the same meaning. Some id-
ioms have also optional elements (such as in a cor-
ner and in a tight corner), and for these it is nec-
essary to indicate which are the optional and which
are the obligatory elements.
Idioms also present variation in the number of
(obligatory) components they have, with some as
short as two words (e.g. pull strings) to others as
long as 10 words (e.g. six of one and half a dozen
of the other) or more, but with no lower and upper
bound, or standard size. Consequently, an adequate
treatment of idioms cannot assume that idioms will
have a specific pre-defined size, but instead it needs
to be able to deal with this variability.
4 Verb Particle Constructions
Verb Particle Constructions (VPCs) are combina-
tions of verbs and prepositional or adverbial par-
ticles, such as break down in The old truck broke
down. In syntactic terms, VPCs can be used in
several different subcategorisation frames (e.g. eat
up as intransitive or transitive VPC). In semantic
terms VPCs can range from idiosyncratic or semi-
idiosyncratic combinations, such as get alng mean-
ing to be in friendly terms, where the meaning of the
combination cannot be straightforwardly inferred
from the meaning of the verb and the particle, (in
e.g. He got along well with his colleagues), to more
regular ones, such as tear up (in e.g. In a rage she
tore up the letter Jack gave her). The latter is a case
where the particle compositionally adds a specific
meaning to the construction and follows a produc-
tive pattern (e.g. as in tear up, cut up and split up,
where the verbs are semantically related and up adds
a sense of completion to the action of these verbs).
In terms of inflectional morphology, the verb-
particle verb follows the same pattern as the simplex
verb (e.g. split up and split). Other characteristics,
like register and dialect are also shared between the
verb in a VPC and the simplex verb. If the VPC and
corresponding simplex verb are defined as indepen-
dent unrelated entries, these generalisations about
what is common between them would be lost. One
option to avoid this problem is to define the VPC
entry in a lexical encoding in terms of the corre-
sponding simplex verb entry.
As discussed earlier for many VPCs the particle
compositionally adds to the meaning of the verb
to form the meaning of the VPC, and this pro-
vides one more reason for keeping the link between
the VPC entry (e.g. wander up) and the simplex
verb entry (e.g. wander), which share the seman-
tics of the verb. Moreover, some of the compo-
sitional VPCs seem to follow productive patterns
(e.g. the resultative combinations walk/jump/run
up/down/out/in/away/around/... from joining these
verbs and the directional/locative particles up,
down, out, in, away, around, ...). This is dis-
cussed in Fraser (1976), who notes that the seman-
tic properties of verbs seem to affect their possi-
bility of combination with particles. For produc-
tive VPCs, one possibility is then to use the en-
tries of verbs already listed in a lexical resource
to productively generate VPC entries by combin-
ing them with particles according to their seman-
tic classes, as discussed by Villavicencio (2003).
However, there are also cases of semi-productivity,
since the possibilities of combinations are not fully
predictable from a particular verb and particle (e.g.
phone/ring/call/*telephone up). Thus, although
some classes of VPCs can be productively gener-
ated from verb entries, to avoid overgeneration we
adopt an approach where the remaining VPCs need
to be explicitly licensed by the specification of the
appropriate VPC entry.
To sum up, for VPC entries an appropriate en-
coding needs to maintain the link between a VPC
and the corresponding simplex form, from where
the VPC inherits many of its characteristics, includ-
ing inflectional morphology and for compositional
cases, the semantics of the verb. On the other hand,
for a non-compositional entry, like get alng, it is
necessary to specify the resulting semantics. In this
case, the semantics defined in the VPC entry over-
rides that inherited by default from its components.
5 A Possible Encoding for MWEs
Taking the encoding of simplex entries as basis for
an MWE encoding, we now discuss the necessary
extensions to the former, to be able to provide the
means of capturing the extra dimensions required
by the latter. While taking these requirements into
account, it is also desirable to define a very gen-
eral architecture, in which simplex and MWE en-
tries can be defined quite similarly, and in which
different types of MWE can be captured in a uni-
form encoding.
In the proposed encoding, simplex entries are still
defined in terms of orthography, grammatical type
and semantic predicate, in the Simplex table (ta-
ble 2). The same encoding can be used for fixed
MWEs, which are treated as words with space, ex-
cept that it also allows for the definition of the ele-
ment in the MWE that can be inflected. This is the
case of kick the bucket, which is defined as an in-
transitive construction whose first orthographic el-
ement (kick) is marked as allowing inflection, and
from where variations such as kicks the bucket can
be derived, table 2.
The encoding of flexible MWEs, on the other
hand, is done in 3 stages. In the first one, the id-
iomatic components of an MWE are defined in a
similar way to simplex words, in terms of an identi-
fier, grammatical type and semantic predicate, in the
MWE table (table 3). In addition, they also make
reference to a non-idiomatic simplex entry (base
form in table 3) from where they inherit by de-
fault many of their characteristics, including orthog-
raphy. This is done by means of the non-idiomatic
entry?s identifier. In the case of e.g. the idiomatic
spill (i spill tv 1), the corresponding non-idiomatic
entry is the transitive spill defined in the simplex ta-
ble, and whose identifier is spill tv 1. Moreover,
when appropriate, a non-idiomatic paraphrase for
the idiomatic element can also be defined. This is
achieved by specifying, in paraphrase the equiv-
alent non-idiomatic element?s semantic predicate.
The idiomatic spill, for example, is assigned as
corresponding paraphrase the non-idiomatic reveal
(reveal tv rel) defined in the simplex table. This
can be used to generate a non-idiomatic paraphrase
for the whole MWE (e.g. reveal secrets as para-
phrase of spill beans, as defined in table 3).
However, in order to be able to encode precisely
an MWE, in the second stage its context is speci-
fied, where all the elements that make that MWE
are listed. This ensures that only when all the core
elements defined for an MWE are present, is that the
MWE is recognised as such (e.g. spill and beans for
the MWE spill beans), preventing the case of false
positives (e.g. spill the milk) from being treated as
an instance of this MWE. Likewise, this prevents id-
iomatic entries from being used outside the context
of the MWE (e.g. the idiomatic spill being inter-
preted as reveal in spill some water). This is done
in the table known as MWE Components, table 4.
In this table each entry is defined in terms of an
identifier for the MWE (e.g. i spill beans 1), and
identifiers for each of the MWE components (e.g.
i spill tv 1 and i bean n 1), that provide the link to
the lexical specification of these components either
in the simplex table (table 2), or in the MWE table
(table 3). In order to allow MWEs with any number
of elements to be uniformly defined, (from shorter
ones like spill beans, rows 1 to 2 in table 4, to longer
ones like pull the curtain down on) we propose an
encoding where each element of the MWE is speci-
fied as a separate contextual entry (row). Thus, what
links all the components of an MWE together, spec-
ified each as an entry, is that they have the same
MWE identifier (e.g. i spill beans 1). Moreover,
to account for MWEs with optional elements, like
in a corner and in a tight corner where tight is op-
tional, each of the elements of the MWE needs to be
marked as obligatory or optional in this table.
For some MWEs, such as VPCs, one of the com-
ponents may be contributing a very specific mean-
ing in the context of that particular MWE, and often
the meaning is more specific than the one defined in
the corresponding base form entry for the compo-
nent, from when the meaning is obtained by default.
Thus, for non-compositional VPCs, such as look
up, the particles can be assumed to have a vacuous
semantic contribution, and the semantics of these
VPCs are contributed solely by the verbs. For look
up, the verbal component, look tv 1, defines the
meaning of the VPC as look-up tv rel while up is
assigned a vacuous relation (up-vacuous prt rel).
Similarly, up in a VPC such as wander up has either
a directional or locational/aspectual interpretation,
which in both cases can be regarded as qualifying
the event of wandering and can be compositionally
added to the meaning of the verb to generate the
meaning of the combination. For these cases, it is
important to allow the semantics of the component
in question to be further refined in its entry for that
MWE (e.g. up with semantics up-end-pt prt rel in
table 4). The approach taken means that the com-
monality in the directional interpretation between
wander up and walk up, where the semantics of the
particle is shared, is captured by means of the spe-
cific semantic type defined for the particle, which
means that generalizations can be made in an infer-
ence component or in semantic transfer for Machine
Translation. Similarly, by defining a VPC from the
base form of the corresponding verb, it is possible to
capture the fact that the semantics of verb is shared
between the verb wander and the VPC wander up.
Finally, in order to specify the appropriate rela-
tionships between the elements of the MWE, a set
of labels is used (PRED1, PRED2,...), which refer
to the position of the element in the logical form
for the MWE. This can be seen in the MWE Type
table (table 5). The basic idea behind the use of
these labels, defined in the column slot, is that they
can be employed as place holders in the semantic
predicate associated with that particular MWE. The
precise correspondences between these place hold-
ers and the predicates are specified in meta-types
defined for each different class of MWE. Thus the
particular meta-type verb-object-idiom is for idioms
with two obligatory elements, where PRED1 cor-
responds to pred1(X,Y) and PRED2 to pred2(Y),
and PRED1 (corresponding to the verb) is a pred-
icate whose second semantic argument (Y) is coin-
dexed with the second predicate (the object). When
this meta-type is instantiated with the entries for an
MWE like spill beans (i spill beans 1) the slots are
instantiated as i spill rel(X,Y), and i bean rel(Y).3
These meta-types act as interface between the
database and a specific grammar system. As men-
tioned before MWEs can be grouped together in
classes according to the patterns they follow (in
terms of syntactic and semantic characteristics).
Therefore, for each particular class of MWE, a
specific meta-type is defined, which contains the
precise interrelation between the components of
the MWE. This means that for a particular gram-
mar, for each meta-type there must be a (grammar-
3For reasons of clarity, in this paper we are using a simpli-
fied but equivalent notation for the meta-type description.
Table 5: MWE Type Table
mwe meta-type
i find nerve 1 verb-object-idiom
i spill beans 1 verb-object-idiom
walk up 1 verb-particle-np
wander up 1 verb-particle-np
look up 1 verb-particle-np
dependent) type that maps the semantic relations be-
tween the elements of the MWE into the appropri-
ate grammar dependent features. Thus, in the third
stage, it is necessary to specify the meta-types for
the MWEs encoded.
In order to test the generality of the meta-types
defined, a further sample of 25 idioms was ran-
domly selected, and an attempt was made to clas-
sify them according to the meta-types defined. The
majority of these idioms could be successfully de-
scribed by the available types, with only a few for
which further meta-types needed to be defined.
The same mechanisms are also used for defining
MWEs which have an element that can be realised
in different ways, but as one of a restricted set of
words like touch a nerve and find a nerve which
are instances of the same MWE. For these cases,
it is necessary to define each of the possible variants
and the position in the idiom in which they occur.
This is done in table 4, where find and touch, the
variants of the idiom find/touch a nerve are defined
as occurring in a particular slot, PRED1 (and nerve
as PRED2): i touch rel(X,Y) i nerve rel(Y) and
i find rel(X,Y) i nerve rel(Y). By using the same
identifier (i find nerve 1) and slot (PRED1) in both
cases, find and touch are specified as two possible
distinct realizations of the slot for that same idiom.
6 Discussion
Multiword Expressions present a challenge for lan-
guage technology, given their flexible nature. In this
paper we described a possible architecture for the
lexical encoding of these expressions. Even though
different types of MWEs have their own character-
istics, this proposal provides a uniform lexical en-
coding for defining them. This architecture takes
into account the flexibility of MWEs extending in a
straightforward manner the one required for simplex
words, and maximises the information contained for
them in the description of MWEs while minimising
the amount of information that needs to be defined
in the description of these expressions.
This encoding provides a clear way to capture
both fixed (and semi-fixed) MWEs and flexible
ones. The former are treated in the same manner
as simplex words, but with the possibility of speci-
fying the inflectional element of the MWE. For flex-
ible MWEs, on the other hand, the encoding is done
in three stages. The first one is the definition of
the idiomatic elements, in the MWE table, the sec-
ond the definition of an MWE?s components, in the
MWE Components table, and the third is the spec-
ification of a class (or meta-type) for the MWE, in
the MWE Type table. Different types of MWEs can
be straightforwardly described using this encoding,
as discussed in terms of idioms and VPCs.
A database employing this encoding can be in-
tegrated with a particular grammar, providing the
grammar system with a useful repertoire of MWEs.
This is the case of the MWE grammar (Villavicen-
cio, 2003) and of the wide-coverage LinGO ERG
(Flickinger, 2004), both implemented on the frame-
work of HPSG and successfully integrated with this
database. This encoding is also used as basis of the
architecture for a multilingual database of MWEs
defined by Villavicencio et al (2004), which has
the added complexity of having to record the cor-
respondences and differences in MWEs in different
languages: different word orders, different lexical
and syntactic constructions, etc. In terms of usage,
this encoding means that the search facilities pro-
vided by the database can help the user investigate
MWEs with particular properties. This in turn can
be used to aid the addition of new MWEs to the
database by analogy with existing MWEs with sim-
ilar characteristics.
7 Acknowledgements
This research was supported in part by the
NTT/Stanford Research Collaboration, research
project on multiword expressions and by the Noun
Phrase Agreement and Coordination AHRB Project
MRG-AN10939/APN17606. This document was
generated partly in the context of the DeepThought
project, funded under the Thematic Programme
User-friendly Information Society of the 5th Frame-
work Programme of the European Community
(Contract No IST-2001-37836).
References
Nicoletta Calzolari, Charles Fillmore, Ralph Gr-
ishman, Nancy Ide, Alessandro Lenci, Cather-
ine MacLeod, and Antonio Zampolli. 2002. To-
wards best practice for multiword expressions in
computational lexicons. In Proceedings of the
3rd International Conference on Language Re-
sources and Evaluation (LREC 2002), Las Pal-
mas, Canary Islands.
John Carroll and Claire Grover. 1989. The deriva-
tion of a large computational lexicon of English
from LDOCE. In B. Boguraev and E. Briscoe,
editors, Computational Lexicography for Natural
Language Processing. Longman.
Ann Copestake and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage English grammar using
HPSG. In Proceedings of the 2nd International
Conference on Language Resources and Evalua-
tion (LREC 2000).
Ann Copestake, Fabre Lambeau, Aline Villavicen-
cio, Francis Bond, Timothy Baldwin, Ivan Sag,
and Dan Flickinger. 2002. Multiword expres-
sions: Linguistic precision and reusability. In
Proceedings of the 3rd International Conference
on Language Resources and Evaluation (LREC
2002), Las Palmas, Canary Islands.
Ann Copestake, Fabre Lambeau, Benjamin Wal-
dron, Francis Bond, Dan Flickinger, and Stephan
Oepen. 2004. A lexicon module for a grammar
development environment. In To appear in Pro-
ceedings of the International Conference on Lan-
guage Resources and Evaluation (LREC 2004),
Lisbon, Portugal.
Ann Copestake. 1994. Representing idioms. Paper
presented at the HPSG Conference.
Christiane Fellbaum. 1998. Towards a representa-
tion of idioms in WordNet. In Proceedings of the
workshop on the use of WordNet in Natural Lan-
guage Processing Systems (Coling-ACL 1998),
Montreal.
Dan Flickinger. 2004. Personal Communication.
Bruce Fraser. 1976. The Verb-Particle Combina-
tion in English. Academic Press, New York,
USA.
Geoffrey Nunberg, Ivan A. Sag, and Tom Wasow.
1994. Idioms. Language, 70:491?538.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for NLP. In
Proceedings of the 3rd International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing-2002), pages 1?15, Mexico
City, Mexico.
Aline Villavicencio and Ann Copestake. 2002. As-
pectual on the nature of idioms. LinGO Working
Paper No. 2002-04.
Aline Villavicencio, Timothy Baldwin, and Ben-
jamin Waldron. 2004. A multilingual database of
idioms. In To appear in Proceedings of the Inter-
national Conference on Language Resources and
Evaluation (LREC 2004), Lisbon, Portugal.
Aline Villavicencio. 2003. Verb-particle construc-
tions and lexical resources. In Francis Bond,
Anna Korhonen, Diana McCarthy, and Aline
Villavicencio, editors, Proceedings of the ACL
2003 Workshop on Multiword Expressions: Anal-
ysis, Acquisition and Treatment, pages 57?64,
Sapporo, Japan.
Table 2: Simplex Table: Extended Encoding for Simplex Entries
identifier orthography type predicate inflectional
position
find tv 1 find trans-verb find tv rel
look tv 1 look trans-verb look tv rel
mention tv 1 mention trans-verb mention tv rel
pull tv 1 pull trans-verb pull tv rel
reveal tv 1 reveal trans-verb reveal tv rel
spill tv 1 spill trans-verb spill tv rel
touch tv 1 touch trans-verb touch tv rel
wander tv 1 wander trans-verb wander tv rel
up prt 1 up particle up prt rel
bean n 1 bean noun bean n rel
nerve n 1 nerve noun nerve n rel
secret n 1 secret noun secret n rel
unmentionable n 1 unmentionable noun unmentionable n rel
kick-the-bucket iv 1 kick, the, bucket intrans-verb kick-the-bucket iv rel 1
walk tv 1 walk intrans-verb walk iv rel
Table 3: MWE Table:Encoding for Idiomatic Entries
identifier base form type predicate paraphrase
i find tv 1 find tv 1 idiomatic-trans-verb i find tv rel mention tv rel
i spill tv 1 spill tv 1 idiomatic-trans-verb i spill tv rel reveal tv rel
i touch tv 1 touch tv 1 idiomatic-trans-verb i touch tv rel mention tv rel
i bean n 1 bean n 1 idiomatic noun i bean n rel secret n rel
i nerve n 1 nerve n 1 idiomatic noun i nerve n rel unmentionable n rel
Table 4: MWE Components
Phrase Component Predicate Slot Optional
i spill beans 1 i spill tv 1 PRED1 no
i spill beans 1 i bean n 1 PRED2 no
i find nerve 1 i find tv 1 PRED1 no
i find nerve 1 i touch tv 1 PRED1 no
i find nerve 1 i nerve n 1 PRED2 no
walk up 1 walk iv 1 PRED1 no
walk up 1 up prt 1 up-end-pt prt rel PRED2 no
wander up 1 wander tv 1 PRED1 no
wander up 1 up prt 1 up-end-pt prt rel PRED2 no
look up 1 look tv 1 look-up tv rel PRED1 no
look up 1 up prt 1 up vacous prt rel PRED2 no
A Standoff Annotation Interface between DELPH-IN Components
Benjamin Waldron and Ann Copestake
University of Cambridge Computer Laboratory
JJ Thomson Avenue
Cambridge CB3 0FD, UK
{bmw20,aac10}@cl.cam.ac.uk
Abstract
We present a standoff annotation frame-
work for the integration of NLP compo-
nents, currently implemented in the con-
text of the DELPH-IN tools1. This pro-
vides a flexible standoff pointer scheme
suitable for various types of data, a lat-
tice encodes structural ambiguity, intra-
annotation relationships are encoded, and
annotations are decorated with structured
content. We provide an XML serialization
for intercomponent communication.
1 Background
An NLP system aims to map linguistic data to
a description at some suitable level of represen-
tation. To achieve this various component pro-
cesses must perform complex tasks. Increasingly
these individual processes are performed by dis-
tinct software components in cooperation. The
expressiveness of communication between such
components hence becomes an issue. For exam-
ple: we may wish to preserve a linkage from a
final semantic analysis to the raw data input; we
may wish to pass ambiguity to some later stage
where it is more appropriately resolved; we may
wish to represent the dependence of certain analy-
ses on other analyses; and we require sufficient ex-
pressiveness for the content of individual analyses
(henceforth ?annotations?). This work addresses
these issues.
Annotations are often associated with a doc-
ument inline. This provides a convenient and
straightforward method of annotating many doc-
uments, but suffers from well-known drawbacks.
We adopt standoff annotation as an alternative.
1http://wiki.delph-in.net
Here annotations live in a separate standoff anno-
tation document, and are anchored in the raw data
via standoff pointers.
2 The DELPH-IN collaboration
DELPH-IN is a loose international collaboration
of researchers developing open-source software
components for language processing. These com-
ponents include deep parsers, deep grammars for
various natural languages, and tools for shallower
processing. The HOG system (Callmeier et al,
2004) for the integration of shallow and deep lin-
guistic processors (using a pipeline making use of
XML plus XSLT transformations to pass data be-
tween processors) was developed during the Deep
Thought project, as was a standard for the inte-
gration of semantic analyses produced by diverse
components: RMRS (Copestake, 2003) allows un-
derspecification of semantic analyses in such a
way that the analysis produced by a shallow com-
ponent may be considered an underspecification of
a fuller semantic analysis produced by a deeper
component. Other work (Waldron et al, 2006) has
provided a representation of partial analyses at the
level of tokenization/morphology ? using a mod-
ification of MAF (Clement and de la Clergerie,
2005). Current work within the SciBorg project2
is investigating more fine-grained integration of
shallow and deep processors.
3 Standoff Annotation Framework
(SAF)
Our standoff annotation framework borrows heav-
ily from the MAF proposal. The key components
of our framework are (i) grounding in primary lin-
guistic data via flexible standoff pointers, (ii) dec-
2http://www.sciborg.org.uk/
97
<?xml version=?1.0? encoding=?UTF8??>
<!DOCTYPE saf SYSTEM ?saf.dtd?>
<saf addressing=?char?>
<olac:olac xmlns:olac=?http://www.language-archives.org/OLAC/1.0/? xml
ns=?http://purl.org/dc/elements/1.1/? xmlns:xsi=?http://www.w3.org/2001
/XMLSchema-instance? xsi:schemaLocation=?http://www.language-archives.o
rg/OLAC/1.0/ http://www.language-archives.org/OLAC/1.0/olac.xsd?>
<creator>LKB x-preprocessor</creator>
<created>18:11:31 1/31/2006 (UTC)</created>
</olac:olac>
<fsm init=?v0? final=?v9?>
<state id=?v0?/>...<state id=?v9?/>
<annot type=?token? id=?t1? from=?0? to=?6? value=?Gutten? source=?
v0? target=?v1?/>
...
<annot type=?token? id=?t11? from=?30? to=?31? value=?.? source=?v8
? target=?v9?/>
</fsm>
</saf>
Figure 1: SAF XML (containing token lattice)
Figure 2: Parse tree (LKB)
oration of individual annotations with structured
content, (iii) representation of structural ambigu-
ity via a lattice of annotations and (iv) a structure
of intra-annotation dependencies. In each case we
have generalized heavily in order to apply the SAF
framework to a wide domain. The basic unit of the
SAF framework is the annotation. An annotation
possesses properties as outlined below.
Each annotation describes a given span in the
raw linguistic data. This span is specified by from
and to standoff pointers. In order to cope with dif-
ferent possible data formats (e.g. audio files, XML
<saf document=?/home/bmw20/b110865
b.xml? addressing=?xpoint?>
<annot type=?sentence? id=?s93? fr
om=?/1/3/54.3? to=?/1/3/58.89? val
ue=?The results of this study are
depicted in Table 2&lt;p/&gt;?/>
</saf>
Figure 3: A ?sentence? annotation
text, pdf files) we make the pointer scheme a prop-
erty of each individual SAF object. So annotations
with respect to an audio file may use frame off-
sets, whilst for an XML text file we may use char-
acter (or more sophisticated xpoint-based) point-
ers. When processing XML text files, we have
found it easiest to work with a hybrid approach to
the standoff pointer scheme. Existing non-XML-
aware processing components can often be easily
adapted to produce (Unicode) character pointers;
for XML-aware components it is easier to work
with XML-aware pointing schemes ? here we use
an extension of the xpoint scheme described in the
XPath specification3 . A mapping between these
two sets of points provides interconversion suffi-
cient for our needs.
3For example: /1/3.2 specifies the second point in the third
element of the first element of the root node, and an extension
allows text nodes in non-elements to be referenced also.
98
<fs type="ne-organisation">
<f name="OrgName">National Aerona
utics and Space Administration</f>
<f name="OrgType">institution</f>
</fs>
Figure 4: Named entity FSR content
Each annotation possesses a content, provid-
ing a (structured) description of the linguistic data
covered by the annotation. E.g. the content of an
annotation describing a token may be the text of
the token itself (see fig. 1); the content of an anno-
tation describing a named entity may be a feature
structure describing properties of the entity (see
fig. 4); the content of an annotation describing the
semantics of a sentence may be an RMRS descrip-
tion (see fig. 6). In most cases we describe this
content via a simple text string, or a feature struc-
ture following the TEI/ISO specification4 . But in
some cases other representations are more appro-
priate (such cases are signalled by the type prop-
erty on annotations). The content will generally
contain meta-information in addition to the pure
content itself. The precise specification for the
content of different annotation types is a current
thrust of development.
Each annotation lives in a global lattice. Use of
a lattice (consisting of a set of nodes ? including
a special start node and end node ? and a set of
edges each with a source node and a target node)
allows us to handle the ambiguity seen in linguistic
analyses of natural languages. E.g. an automatic
speech recognition system may output a word lat-
tice, and a lattice representation can be very useful
in other contexts where we do not wish to collapse
the space of alternative hypotheses too early.
Fig. 2 shows a Norwegian sentence5 for which
the token lattice is very useful. Here the posses-
sive s clitic may attach to any word, but unlike in
English no apostrophe is used. Hence it not fea-
sible for the tokenizer to resolve this ambiguity
in tokenisation. The token lattice (produced by a
regex-based SAF-aware preprocessor) provides an
elegant solution to this problem: between nodes 2
and 4 (and nodes 4 and 6) the lattice provides alter-
native paths.6 The parser is able to resolve the am-
4http://www.tei-c.org/release/doc/tei-p5-
doc/html/FS.html
5Translation: The boy who is sitting?s house is yellow.
6The sentence also exhibits the same phenomena for the
final period ? it could form part of an abbreviation.
0-1 [1] Gutten <0 c 6>
1-2 [2] som <7 c 10>
2-3 [3] sitter <11 c 17>
2-4 [5] sitters <11 c 18>
3-4 [4] s <16 c 18>
4-5 [6] hu <19 c 21>
4-6 [8] hus <19 c 22>
5-6 [7] s <20 c 22>
6-7 [9] er <23 c 25>
7-8 [10] gult <26 c 30>
7-9 [12] gult. <26 c 31>
8-9 [11] . <30 c 31>
Figure 5: Token lattice with character-point stand-
off pointers
biguity with lexical and syntactic knowledge un-
available to the preprocessor component. See fig.
5 for a simple representation of the token lattice,
and fig. 1 for the equivalent SAF XML.
Each annotation also lives in a hierarchy of an-
notation dependencies built over the lattice. E.g.
sentence splitting may be the lowest level; then
from each sentence we obtain a set (lattice) of to-
kens; for individual tokens (or each set of tokens
on a partial path through the lattice) we may ob-
tain an analysis from a named-entity component.
A parser may build on top of this, producing per-
haps a semantic analysis for certain paths in the
lattice. Each such level consists of a set of annota-
tions each of which may be said to build on a set
of lower annotations. This is encoded by means of
a depends on property on each annotation. The an-
notation in fig. 6 exhibits the use of the depends on
property to mark its dependency on the annotation
shown in fig. 3.
A number of well-formedness constrains apply
to SAF objects. For example, the ordering of
standoff pointers must be consistent with the or-
dering of annotation elements through all paths in
the lattice. Sets of annotations related (directly or
indirectly) via the depends on property must lie on
a single path through the lattice.
4 XML Serialization
Our SAF XML serialization is provided both for
inter-component communication and for persis-
tent storage. XML provides a clean standards-
based framework in which to serialize our SAF
objects. Our serialization was heavily influenced
by the MAF XML serialization.
99
<annot type=?rmrs? deps=?s93?>
<label vid=?1?/>
<ep cfrom=?18476? cto=?18526?>
<gpred>prpstn_m_rel</gpred>
<label vid=?1?/>
<var sort=?e? vid=?2?
tense=?present?/>
</ep>
...
<rarg>
<rargname>MARG</rargname>
<label vid=?1?/>
<var sort=?h? vid=?3?/>
</rarg>
</annot>
Figure 6: An annotation with RMRS content
The SAF XML serialization is contained within
the top saf XML element. Here the pointer ad-
dressing scheme used (e.g. char for charac-
ter point offsets, xpoint for our xpoint-based
scheme), and the location of the primary data are
specified as attributes. This element may contain
an optional olac element7 to specify metadata
(e.g. creator) and a single fsm element holds the
rest of the object (as shorthand we also allow a se-
quence of the annot elements defined below in
place of the fsm). The fsm element consists of
a number of state elements (with attribute id)
declaring the available lattice nodes, followed by
annot annotation definitions.
Each annotation (annot) element possesses
the following attributes: from and to give stand-
off pointers into the primary data, encoded accord-
ing to the scheme specified by the saf element?s
addressing attribute; source and target
each give a state id (absent if the annotations
are listed sequentially outside of an fsr element);
deps is a set of idrefs; value is shorthand for
a string-valued content; type is shorthand for a
particular type of annotation content. The annota-
tion content, if not a value string, is represented
using the TEI/ISO FSR XML format or the appro-
priate XML format corresponding to the annota-
tion type.
5 Summary
We are in the process of SAF-enabling a num-
ber of the DELPH-IN processing components.
7http://www.language-archives.org/OLAC/metadata.html
A SAF-aware sentence splitter produces SAF
XML describing the span of each sentence, from
which a SAF-aware (and XML-aware) preproces-
sor/tokeniser maps raw sentence text into a SAF
XML token lattice (with some additional annota-
tion to describe tokens such as digit sequences).
External preprocessor components (such as a mor-
phological analyser for Japanese) may also be ma-
nipulated in order to provide SAF input to the
parser. SAF is integrated into the parser of the
LKB grammar development environment (Copes-
take, 2002) and can also be used with the PET run-
time parser (Callmeier, 2000). The MAF XML
format (compatible with SAF) is also integrated
into the HOG system, and we hope to generalize
this to the full SAF framework.
6 Acknowledgements
The authors wish to thank Bernd Kiefer, Ul-
rich Schaefer, Dan Flickinger, Stephan Oepen and
other colleagues within the DELPH-IN collabo-
ration for many informative discussions. This
work was partly funded by a grant from Boeing
to Cambridge University, partly by the NorSource
Grammar Project8 at NTNU, and partly by EPSRC
project EP/C010035/1.
References
Ulrich Callmeier, Andreas Eisele, Ulrich Schaefer, and
Melanie Siegel. 2004. The DeepThought Core Ar-
chitecture Framework. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, LREC?04, Lisbon, Portugal.
Ulrich Callmeier. 2000. PET. A Platform for Exper-
imentation with Efficient HPSG Processing Tech-
niques. Journal of Natural Language Engineering,
6(1):99?108.
Lionel Clement and Eric Villemonte de la Clergerie.
2005. MAF: a morphosyntactic annotation frame-
work. In Proceedings of the 2nd Language and
Technology Conference, Poznan, Poland.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stanford.
Ann Copestake. 2003. Report on the Design of RMRS.
Technical Report D1.1a, University of Cambridge,
UK.
Benjamin Waldron, Ann Copestake, Ulrich Schaefer,
and Bernd Kiefer. 2006. Preprocessing and Tokeni-
sation Standards in DELPH-IN Tools. In Proceed-
ings of the 5th International Conference On Lan-
guage Resources and Evaluation, LREC?06, Genoa,
Italy.
8http://www.ling.hf.ntnu.no/forskning/norsource/
100
Errors in wikis: new challenges and new opportunities ? a discussion
document
Ann Copestake
Computer Laboratory
University of Cambridge
aac@cl.cam.ac.uk
Abstract
This discussion document concerns the
challenges to assessments of reliability
posed by wikis and the potential for lan-
guage processing techniques for aiding
readers to decide whether to trust partic-
ular text.
1 Wikis and the trust problem
Wikis, especially open wikis, pose new challenges
for readers in deciding whether information is
trustworthy. An article in a wikipedia may be
generally well-written and appear authoritative, so
that the reader is inclined to trust it, but have some
additions by other authors which are incorrect.
Corrections may eventually get made, but there
will be a time lag. In particular, many people are
now using Wikipedia (www.wikipedia.org)
as a major reference source, so the potential for
misinformation to be spread is increasing. It
has already become apparent that articles about
politicians are being edited by their staff to make
them more favourable and no doubt various inter-
est groups are manipulating information in more
subtle ways. In fact, as wikis develop, problems
with reliability may get worse: authors who wrote
an article several years ago won?t care so much
about its content and may not bother to check ed-
its. When obscure topics are covered by a wiki,
the community which is capable of checking facts
may be small.
Of course errors arise in old text too, but a
generally authoritative conventional article is un-
likely to contain a really major error about a cen-
tral topic. Different old text publications have
different perspectives, political or otherwise, but
the overall slant is usually generally known and
hence not problematic. Non-wiki web pages may
have unknown authors, but the domain offers some
guide to reliability and to likely skew and the
pages can be assessed as a whole. The issue here
is not the overall number of errors in wikis ver-
sus published text or web pages, but how a reader
can decide to trust a particular piece of informa-
tion when they cannot use the article as a whole as
a guide.
There is a need for automatic tools which could
provide an aid for the reader who needs to assess
trustworthiness and also for authors and modera-
tors scanning changes. Similarly, moderators need
tools for identification of vandalism, libel, adver-
tising and so on.
Questions:
1. Is wiki reliability really a problem for read-
ers, as I hypothesise? Perhaps readers who
are not expert in a topic can detect problem-
atic material in a wiki article, despite the mul-
tiple authorship.
2. Can we use language processing tools to help
readers identify errors and misinformation in
wiki pages?
2 Learning trustworthiness
The availability of change histories on wikis is
a resource which could be exploited for train-
ing purposes by language processing systems
designed to evaluate trustworthiness. If it is
possible to categorise users as trustworthy or
non-trustworthy/unknown by independent criteria
(such as overall contribution level), then we can
use changes made by trustworthy users that delete
additions made by the unknown users as a means
of categorising some text as bad. (Possibly the
9
comments made by the editors could lead to sub-
categorization of the badness as error vs vandalism
etc.) A tool for highlighting possible problem ed-
its in wikis might thus be developed on the basis
of a large amount of training data. Techniques de-
rived from areas such as language-based spam de-
tection, subjectivity measurement and so on could
be relevant. However, one of the relatively novel
aspects of the wiki problem is that we are look-
ing at categorisation of small text snippets rather
than larger quantities of text. Thus techniques that
rely on stylistic cues probably won?t work. Ide-
ally, we need to be able to identify the actual in-
formation provided by individual contributors and
classify this as reliable or unreliable. One way of
looking at this is by dividing text into factoids (in
the summarisation sense). Factoid identification is
a really hard problem, but maybe the wiki edits
themselves could help here.
Questions:
1. Can we automatically classify wiki contribu-
tors as reliable/unreliable?
2. Do trustworthy users? edits provide good
training data?
3. Are there any features of text snippets that al-
low classification of reliability? (My guess:
identification of vandalism will be possible
but more subtle effects won?t be detectable.)
4. What tools could be adapted from other ar-
eas of language processing to address these
issues?
3 An ontology of errors?
As an extension of the ideas in the previous sec-
tion, perhaps wiki histories could be mined as a
repository of commonly believed false informa-
tion. For instance, the EN wikipedia entry for
University of Cambridge currently (Jan 5th, 2006)
states:
Undergraduate admission to Cambridge
colleges used to depend on knowledge
of Latin and Ancient Greek, subjects
taught principally in the United King-
dom at fee-paying schools, called public
schools.
(?public schools? was linked)
One way in which this is wrong is that British
?public schools? (in this sense) are only a small
proportion of the fee-paying schools, but equat-
ing public schools with all fee-paying schools is a
common error. Suppose a trustworthy editor cor-
rects this particular error in this article (and per-
haps similar errors in the same or other articles). If
we can automatically analyse and store the correc-
tion, we could use it to check for the same error in
other text. As wikis get larger, this might become
a useful resource for error detection/evaluation of
many text types. Thus errors in wikis are an op-
portunity as well as a challenge.
10
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 57?64,
Prague, June 2007. c?2007 Association for Computational Linguistics
Co-occurrence Contexts for Noun Compound Interpretation
Diarmuid O? Se?aghdha
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
do242@cl.cam.ac.uk
Ann Copestake
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
aac10@cl.cam.ac.uk
Abstract
Contextual information extracted from cor-
pora is frequently used to model seman-
tic similarity. We discuss distinct classes
of context types and compare their effec-
tiveness for compound noun interpretation.
Contexts corresponding to word-word sim-
ilarity perform better than contexts corre-
sponding to relation similarity, even when
relational co-occurrences are extracted from
a much larger corpus. Combining word-
similarity and relation-similarity kernels fur-
ther improves SVM classification perfor-
mance.
1 Introduction
The compound interpretation task is frequently cast
as the problem of classifying an unseen compound
noun with one of a closed set of relation categories.
These categories may consist of lexical paraphrases,
such as the prepositions of Lauer (1995), or deeper
semantic relations, such as the relations of Girju et
al. (2005) and those used here. The challenge lies in
the fact that by their very nature compounds do not
give any surface realisation to the relation that holds
between their constituents. To identify the differ-
ence between bread knife and steel knife it is not suf-
ficient to assign correct word-senses to bread, steel
and knife; it is also necessary to reason about how
the entities referred to interact in the world. A com-
mon assumption in data-driven approaches to the
problem is that compounds with semantically sim-
ilar constituents will encode similar relations. If a
hearer knows that a fish knife is a knife used to eat
fish, he/she might conclude that the novel compound
pigeon fork is a fork used to eat pigeon given that
pigeon is similar to fish and knife is similar to fork.
A second useful intuition is that word pairs which
co-occur in similar contexts are likely to enter into
similar relations.
In this paper, we apply these insights to identify
different kinds of contextual information that cap-
ture different kinds of similarity and compare their
applicability using medium- to large-sized corpora.
In keeping with most other research on the prob-
lem,1 we take a supervised learning approach to
compound interpretation.
2 Defining Contexts for Compound
Interpretation
When extracting corpus information to interpret a
compound such as bread knife, there are a number
of context types that might plausibly be of interest:
1. The contexts in which instances of the com-
pound type appear (type similarity); e.g., all
sentences in the corpus that contain the com-
pound bread knife.
2. The contexts in which instances of each con-
stituent appear (word similarity); e.g., all sen-
tences containing the word bread or the word
knife.
3. The contexts in which both constituents appear
together (relation similarity); e.g., all sentences
containing both bread and knife.
4. The context in which the particular compound
token was found (token similarity).
1Such as Girju et al (2005), Girju (2006), Turney (2006).
Lapata and Keller?s (2004) unsupervised approach is a notable
exception.
57
A simple but effective method for exploiting these
contexts is to count features that co-occur with the
target items in those contexts. Co-occurrence may
be defined in terms of proximity in the text, lexi-
cal patterns, or syntactic patterns in a parse graph.
We can parameterise our notion of context further,
for example by enforcing a constraint that the co-
occurrence correspond to a particular type of gram-
matical relation or that co-occurrence features be-
long to a particular word class.2
Research in NLP frequently makes use of one or
more of these similarity types. For example, Culotta
and Sorensen (2004) combine word similarity and
relation similarity for relation extraction; Gliozzo et
al. (2005) combine word similarity and token simi-
larity for word sense disambiguation. Turney (2006)
discusses word similarity (which he calls ?attribu-
tional similarity?) and relation similarity, but fo-
cusses on the latter and does not perform a compar-
ative study of the kind presented here.
The experiments described here investigate type,
word and relation similarity. However, token simi-
larity clearly has a role to play in the interpretation
task, as a given compound type can have a differ-
ent meaning in different contexts ? for example, a
school book can be a book used in school, a book
belonging to a school or a book about a school. As
our data have been annotated in context, we intend
to model this dynamic in future work.
3 Experimental Setup
3.1 Data
We used the dataset of 1443 compounds whose
development is described in O? Se?aghdha (2007).
These compounds have been annotated in their sen-
tential contexts using the six deep semantic rela-
tions listed in Table 1. On the basis of a dual-
annotator study, O? Se?aghdha reports agreement of
66.2% (?? = 0.62) on a more general task of an-
notating a noisy corpus and estimated agreement of
73.6% (?? = 0.68) on annotating the six relations
used here. These figures are superior to previously
reported results on annotating compounds extracted
from corpora. Always choosing the most frequent
class (IN) would give accuracy of 21.34%, and we
2A flexible framework for this kind of context definition is
presented by Pado? and Lapata (2003).
Relation Distribution Example
BE 191 (13.24%) steel knife, elm tree
HAVE 199 (13.79%) street name, car door
IN 308 (21.34%) forest hut, lunch time
INST 266 (18.43%) rice cooker, bread knife
ACTOR 236 (16.35%) honey bee, bus driver
ABOUT 243 (16.84%) fairy tale, history book
Table 1: The 6 relation classes and their distribution
in the dataset
use this as a baseline for our experiments.
3.2 Corpus
The written section of the British National Corpus,3
consisting of around 90 million words, was used in
all our experiments. This corpus is not large com-
pared to other corpora used in NLP, but it has been
manually compiled with a view to a balance of genre
and should be more representative of the language in
general than corpora containing only newswire text.
Furthermore, the compound dataset was also ex-
tracted from the BNC and information derived from
it will arguably describe the data items more accu-
rately than information from other sources. How-
ever, this information may be very sparse given the
corpus? size. For comparison we also use a 187
million word subset of the English Gigaword Cor-
pus (Graff, 2003) to derive relational information
in Section 6. This subset consists of every para-
graph in the Gigaword Corpus belonging to articles
tagged as ?story? and containing both constituents of
a compound in the dataset, whether or not they are
compounded there. Both corpora were lemmatised,
tagged and parsed with RASP (Briscoe et al, 2006).
3.3 Learning Algorithm
In all our experiments we use a one-against-all im-
plementation of the Support Vector Machine.4 Ex-
cept for the work described in Section 6.2 we used
the linear kernel K(x, y) = x ?y to compute similar-
ity between vector representations of the data items.
The linear kernel consistently achieved superior per-
formance to the more flexible Gaussian kernel in
a range tests, presumably due to the sensitivity of
3http://www.natcorp.ox.ac.uk/
4The software used was LIBSVM (Chang and Lin, 2001).
58
the Gaussian kernel to its parameter settings.5 One-
against-all classification (training one classifier per
class) performed better than one-against-one (train-
ing one classifier for each pair of classes). We es-
timate test accuracy by 5-fold cross-validation and
within each fold we perform further 5-fold cross-
validation on the training set to optimise the single
SVM parameter C. An advantage of the linear kernel
is that learning is very efficient. The optimisation,
training and testing steps for each fold take from less
than a minute on a single processor for the sparsest
feature vectors to a few hours for the most dense, and
the folds can easily be distributed across machines.
4 Word Similarity
O? Se?aghdha (2007) investigates the effectiveness of
word-level co-occurrences for compound interpre-
tation, and the results presented in this section are
taken from that paper. Co-occurrences were identi-
fied in the BNC for each compound constituent in
the dataset, using the following context definitions:
win5, win10: Each word within a window of 5 or
10 words on either side of the item is a feature.
Rbasic, Rmod, Rverb, Rconj: These feature sets
use the grammatical relation output of the
RASP parser run over the written BNC. The
Rbasic feature set conflates information about
25 grammatical relations; Rmod counts only
prepositional, nominal and adjectival noun
modification; Rverb counts only relations
among subjects, objects and verbs; Rconj
counts only conjunctions of nouns.
The feature vector for each target constituent counts
its co-occurrences with the 10,000 words that most
frequently appear in the co-occurrence relations of
interest over the entire corpus. A feature vector for
each compound was created by appending the vec-
tors for its modifier and head, and these compound
vectors were used for SVM learning. To model as-
pects of co-occurrence association that might be ob-
scured by raw frequency, the log-likelihood ratio G2
(Dunning, 1993) was also used to transform the fea-
ture space.
5Keerthi and Lin (2003) prove that the Gaussian kernel will
always do as well as or better than the linear kernel for binary
classification. For multiclass classification we use multiple bi-
Raw G2
Accuracy Macro Accuracy Macro
w5 52.60% 51.07% 51.35% 49.93%
w10 51.84% 50.32% 50.10% 48.60%
Rbasic 51.28% 49.92% 51.83% 50.26%
Rmod 51.35% 50.06% 48.51% 47.03%
Rverb 48.79% 47.13% 48.58% 47.07%
Rconj 54.12% 52.44% 54.95% 53.42%
Table 2: Classification results for word similarity
Micro- and macro-averaged performance figures
are given in Table 2. The micro-averaged figure
is calculated as the overall proportion of items that
were classified correctly, whereas the macro-average
is calculated as the average of the accuracy on each
class and thus balances out any skew in the class
distribution. In all cases macro-accuracy is lower
than micro-accuracy; this is due to much better per-
formance on the relations IN, INST, ACTOR and
ABOUT than on BE and HAVE. This may be be-
cause those two relations are slightly rarer and hence
provide less training data, or it may reflect a dif-
ference in the suitability of co-occurrence data for
their classification. It is interesting that features de-
rived only from conjunctions give the best perfor-
mance; these features are the most sparse but ap-
pear to be of high quality. The information con-
tained in conjunctions is conceptually very close to
the WordNet-derived information frequently used in
word-similarity based approaches to compound se-
mantics, and the performance of these features is not
far off the 56.76% accuracy (54.6% macro-average)
reported for WordNet-based classification for the
same dataset by O? Se?aghdha (2007).
5 Type Similarity
Type similarity is measured by identifying co-
occurrences with each instance of the compound
type in the corpus. In effect, we are treating com-
pounds as single words and calculating their word
similarity with each other. The same feature extrac-
tion methods were used as in the previous section.
Classification results are given in Table 3.
This method performs very poorly. Sparsity is un-
doubtedly a factor: 513 of the 1,443 compounds oc-
nary classifiers with a shared set of parameters which may not
be optimal for any single classifier.
59
Accuracy Macro
win5 28.62% 27.71%
win10 30.01% 28.69%
Rbasic 29.31% 28.22%
Rmod 26.54% 25.30%
Rverb 25.02% 23.96%
Rconj 24.60% 24.48%
Table 3: Classification results for type similarity
cur 5 times or fewer in the BNC and 186 occur just
once. The sparser feature sets (Rmod, Rverb and
Rconj) are all outperformed by the more dense ones.
However, there is also a conceptual problem with
type similarity, in that the context of a compound
may contain information about the referent of the
compound but is less likely to contain information
about the implicit semantic relation. For example,
the following compounds all encode different mean-
ings but are likely to appear in similar contexts:
? John cut the bread with the kitchen knife.
? John cut the bread with the steel knife.
? John cut the bread with the bread knife.
6 Relation Similarity
6.1 Vector Space Kernels
The intuition underlying the use of relation similar-
ity is that while the relation between the constituents
of a compound may not be made explicit in the con-
text of that compound, it may be described in other
contexts where both constituents appear. For ex-
ample, sentences containing both bread and knife
may contain information about the typical interac-
tions between their referents. To extract feature vec-
tors for each constituent pair, we took the maximal
context unit to be each sentence in which both con-
stituents appear, and experimented with a range of
refinements to that context definition. The result-
ing definitions are given below in order of intuitive
richness, from measures based on word-counting to
measures making use of the structure of the sen-
tence?s dependency parse graph.
allwords All words in the sentence are co-
occurrence features. This context may be pa-
rameterised by specifying a limit on the win-
dow size to the left of the leftmost constituent
and to the right of the rightmost constituent i.e.,
the words between the two constituents are al-
ways counted.
midwords All words between the constituents are
counted.
allGRs All words in the sentence entering into a
grammatical relation (with any other word) are
counted. This context may be parameterised by
specifying a limit on the length of the shortest
path in the dependency graph from either of the
target constituents to the feature word.
shortest path All words on the shortest depen-
dency path between the two constituents are
features. If there is no such path, no features
are extracted.
path triples The shortest dependency path is de-
composed into a set of triples and these triples
are used as features. Each triple consists of a
node on the shortest path (the triple?s centre
node) and two edges connecting that node with
other nodes in the parse graph (not necessarily
nodes on the path). To generate further triple
features, one or both of the off-centre nodes is
replaced by part(s) of speech. For example, the
RASP dependency parse of The knife cut the
fresh bread is:
(|ncsubj| |cut:3_VVD| |knife:2_NN1| _)
(|dobj| |cut:3_VVD| |bread:6_NN1|)
(|det| |bread:6_NN1| |the:4_AT|)
(|ncmod| _ |bread:6_NN1| |fresh:5_JJ|)
(|det| |knife:2_NN1| |The:1_AT|)
The derived set of features includes the triples
{the:A:det?knife:N?cut:V:ncsubj,
A:det?knife:N?cut:V:ncsubj,
the:A:det?knife:N?V:ncsubj,
A:det?knife:N?V:ncsubj,
knife:N:ncsubj?cut:V?bread:N:dobj,
N:ncsubj?cut:V?bread:N:dobj,
knife:N:ncsubj?cut:V?N:dobj,
N:ncsubj?cut:V?N:dobj,. . .}
(The? and? arrows indicate the direction of
the head-modifier dependency)
60
0 100 200 300 400 500
30
35
40
45
50
55
60
Threshold
Ac
cu
rac
y
aw5
trip
Figure 1: Effect of BNC frequency on test item ac-
curacy for the allwords5 and triples contexts
Table 4 presents results for these contexts; in
the case of parameterisable contexts the best-
performing parameter setting is presented. We are
currently unable to present results for the path-based
contexts using the Gigaword corpus. It is clear
from the accuracy figures that we have not matched
the performance of the word similarity approach.
The best-performing single context definition is all-
words with a window parameter of 5, which yields
accuracy of 38.74% (36.78% macro-average). We
can combine the contributions of two contexts by
generating a new kernel that is the sum of the lin-
ear kernels for the individual contexts;6 the sum of
allwords5 and triples achieves the best performance
with 42.34% (40.20% macro-average).
It might be expected that the richer context def-
initions provide sparser but more precise informa-
tion, and that their relative performance might im-
prove when only frequently observed word pairs are
to be classified. However, thresholding inclusion
in the test set on corpus frequency belies that ex-
pectation; as the threshold increases and the test-
6The summed kernel function value for a pair of items is
simply the sum of the two kernel functions? values for the pair,
i.e.:
Ksum(x, y) = K1(?1(x), ?1(y)) +K2(?2(x), ?2(y))
where ?1, ?2 are the context representations used by the two
kernels. A detailed study of kernel combination is presented by
Joachims et al (2001).
0 100 200 300 400 500
0
50
0
10
00
15
00
Threshold
Siz
e GW
BNC
Figure 2: Effect of corpus frequency on dataset size
for the BNC and Gigaword-derived corpus
ing data contains only more frequent pairs, all con-
texts show improved performance but the effect is
strongest for the allwords and midwords contexts.
Figure 1 shows threshold-accuracy curves for two
representative contexts (the macro-accuracy curves
are similar).
For all frequency thresholds above 6, the number
of noun pairs with above-threshold corpus frequency
is greater for the Gigaword corpus than for the BNC,
and this effect is amplified with increasing threshold
(see Figure 2). However, this difference in sparsity
does not always induce an improvement in perfor-
mance, but nor does the difference in corpus type
consistently favour the BNC.
BNC Gigaword
Accuracy Macro Accuracy Macro
aw 35.97% 33.39% 34.58% 32.62%
aw5 38.74% 36.78% 37.28% 35.25%
mw 32.29% 30.38% 36.24% 34.25%
agr 35.34% 33.40% 35.34% 33.34%
agr2 36.73% 34.81% 37.28% 35.59%
sp 33.54% 31.51%
trip 35.62% 34.39%
aw5+ 42.34% 40.20%
trip
Table 4: Classification results for relation similarity
61
6.2 String Kernels
The classification techniques described in the pre-
vious subsection represent the relational context for
each word pair as a co-occurrence vector in an in-
ner product space and compute the similarity be-
tween two pairs as a function of their vector repre-
sentations. A different kind of similarity measure is
provided by string kernels, which count the num-
ber of subsequences shared by two strings. This
class of kernel function implicitly calculates an in-
ner product in a feature space indexed by all pos-
sible subsequences (possibly restricted by length or
contiguity), but the feature vectors are not explic-
itly represented. This approach affords our notion of
context an increase in richness (features can be se-
quences of length ? 1) without incurring the com-
putational cost of the exponential growth in the di-
mension of our feature space. A particularly flexible
string kernel is the gap-weighted kernel described by
Lodhi et al (2002), which allows the subsequences
to be non-contiguous but penalises the contribution
of each subsequence to the kernel value according to
the number of items occurring between the start and
end of the subsequence, including those that do not
belong to the subsequence (the ?gaps?).
The kernel is defined as follows. Let s and t
be two strings of words belonging to a vocabulary
?. A subsequence u of s is defined by a sequence
of indices i = (i1, . . . , i|u|) such that 1 ? i1 <
. . . < i|u| ? |s|, where s is the length of s. Let
l(i) = i|u|? i1 +1 be the length of the subsequence
in s. For example, if s is the string ?cut the bread
with the knife? and u is the subsequence ?cut with?
indexed by i then l(i) = 4. ? is a decay parameter
between 0 and 1. The gap-weighted kernel value for
subsequences of length n of strings s and t is given
by
KSn(s, t) =
?
u??n
?
i,j:s[i]=u=t[j]
?l(i)+l(j)
Directly computing this function would be in-
tractable, as the sum is over all |?|n possible sub-
sequences of length n; however, Lodhi et al (2002)
present an efficient dynamic programming algo-
rithm that can evaluate the kernel in O(n|s||t|) time.
Those authors? application of string kernels to text
categorisation counts sequences of characters, but it
is generally more suitable for NLP applications to
use sequences of words (Cancedda et al, 2003).
This kernel calculates a similarity score for a pair
of strings, but for context-based compound classi-
fication we are interested in the similarity between
two sets of strings. We therefore define a context
kernel, which sums the kernel scores for each pair
of strings from the two context sets C1, C2 and nor-
malises them by the number of pairs contributing to
the sum:
KCn(C1, C2) =
1
|C1||C2|
?
s?C1,t?C2
KSn(s, t)
That this is a valid kernel (i.e., defines an inner prod-
uct in some induced vector space) can be proven us-
ing the definition of the derived subsets kernel in
Shawe-Taylor and Cristianini (2004, p. 317). In our
experiments we further normalise the kernel to en-
sure that KCn(C1, C2) = 1 if and only if C1 = C2.
To generate the context set for a given word pair,
we extract a string from every sentence in the BNC
where the pair of words occurs no more than eight
words apart. On the hypothesis that the context
between the target words was most important and
to avoid the computational cost incurred by long
strings, we only use this middle context. To facilitate
generalisations over subsequences, the compound
head is replaced by a marker HEAD and the modifier
is replaced by a marker MOD. Word pairs for which
no context strings were extracted (i.e., pairs which
only occur as compounds in the corpus) are repre-
sented by a dummy string that matches no other. The
value of ? is set to 0.5 as in Cancedda et al (2003).
Table 5 presents results for the context kernels with
subsequence lengths 1,2,3 as well as the kernel sum
of these three kernels. These kernels perform better
than the relational vector space kernels, with the ex-
ception of the summed allwords5 + triples kernel.
7 Combining Contexts
We can use the method of kernel summation to com-
bine information from different context types. If our
intuition is correct that type and relation similarity
provide different ?views? of the same semantic rela-
tion, we would expect their combination to give bet-
ter results than either taken alone. This is also sug-
gested by the observation that the different context
62
Accuracy Macro
n = 1 15.94% 19.88%
n = 2 39.09% 37.23%
n = 3 39.29% 39.29%
?1,2,3 40.61% 38.53%
Table 5: Classification results for gap-weighted
string kernels with subsequence lengths 1,2,3 and
the kernel sum of these kernels
Accuracy Macro
Rconj-G2 + aw5 54.95% 53.50%
Rconj-G2 + triples 56.20% 54.54%
Rconj-G2 + aw5 + triples 55.86% 54.13%
Rconj-G2 + KC2 56.48% 54.89%
Rconj-G2 + KC? 56.55% 54.96%
Table 6: Classification results for context combina-
tions
types favour different relations: the summed string
kernel is the best at identifying IN relations (70.45%
precision, 46.67% recall), but Rconj-G2 is best at
identifying all others. This intuition is confirmed by
our experiments, the results of which appear in Ta-
ble 6. The best performance of 56.55% accuracy
(54.96% macro-average) is attained by the com-
bination of the G2-transformed Rconj word simi-
larity kernel and the summed string kernel KC? .
We note that this result, using only information ex-
tracted from the BNC, compares favourably with the
56.76% accuracy (54.60% macro-average) results
described by O? Se?aghdha (2007) for a WordNet-
based method. The combination of Rconj-G2 and
triples is also competitive, demonstrating that a less
flexible learning algorithm (the linear kernel) can
perform well if it has access to a richer source of
information (dependency paths).
8 Comparison with Prior Work
Previous work on compound semantics has tended
to concentrate on either word or relation similarity.
Approaches based on word similarity generally use
information extracted from WordNet. For example,
Girju et al (2005) train SVM classifiers on hyper-
nymy features for each constituent. Their best re-
ported accuracy with an equivalent level of supervi-
sion to our work is 54.2%; they then improve perfor-
mance by adding a significant amount of manually-
annotated semantic information to the data, as does
Girju (2006) in a multilingual context. It is difficult
to make any conclusive comparison with these re-
sults due to fundamental differences in datasets and
classification schemes.
Approaches based on relational similarity of-
ten use relative frequencies of fixed lexical se-
quences estimated from massive corpora. Lap-
ata and Keller (2004) use Web counts for phrases
Noun P Noun where P belongs to a predefined set
of prepositions. This unsupervised approach gives
state-of-the-art results on the assignment of prepo-
sitional paraphrases, but cannot be applied to deep
semantic relations which cannot be directly identi-
fied in text. Turney and Littman (2005) search for
phrases Noun R Noun where R is one of 64 ?join-
ing words?. Turney (2006) presents a more flexible
framework in which automatically identified n-gram
features replace fixed unigrams and additional word
pairs are generated by considering synonyms, but
this method still requires a Web-magnitude corpus
and a very large amount of computational time and
storage space. The latter paper reports accuracy of
58.0% (55.9% macro-average), which remains the
highest reported figure for corpus-based approaches
and demonstrates that relational similarity can per-
form well given sufficient resources.
We are not aware of previous work that compares
the effectiveness of different classes of context for
compound interpretation, nor of work that investi-
gates the utility of different corpora. We have also
described the first application of string kernels to the
compound task, though gap-weighted kernels have
been used successfully for related tasks such as word
sense disambiguation (Gliozzo et al, 2005) and re-
lation extraction (Bunescu and Mooney, 2005).
9 Conclusion and Future Work
We have defined four kinds of co-occurrence con-
texts for compound interpretation and demonstrated
that word similarity outperforms a range of relation
contexts using information derived from the British
National Corpus. Our experiments with the English
Gigaword Corpus indicate that more data is not al-
ways better, and that large newswire corpora may
not be ideally suited to general relation-based tasks.
63
On the other hand it might be expected to be very
useful for disambiguating relations more typical of
news stories (such as tax cut, rail strike).
Future research directions include developing
more sophisticated context kernels. Cancedda et
al. (2003) present a number of potentially useful re-
finements of the gap-weighted string kernel, includ-
ing ?soft matching? and differential values of ? for
different words or word classes. We intend to com-
bine the benefits of string kernels with the linguis-
tic richness of syntactic parses by computing subse-
quence kernels on dependency paths. We have also
begun to experiment with the tree kernels of Mos-
chitti (2006), but are not yet in a position to report
results. As mentioned in Section 2, we also intend
to investigate the potential contribution of the sen-
tential contexts that contain the compound tokens to
be classified (token similarity).
While the BNC has many desirable properties,
it may also be fruitful to investigate the utility of
a large encyclopaedic corpus such as Wikipedia,
which may be more explicit in its description of re-
lations between real-world entities than typical text
corpora. Wikipedia has shown promise as a re-
source for measuring word similarity (Strube and
Ponzetto, 2006) and relation similarity (Suchanek et
al. (2006)).
References
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the ACL-06 Interactive Presentation Sessions.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In Pro-
ceedings of the 19th Conference on Neural Informa-
tion Processing Systems.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean-
Michel Renders. 2003. Word-sequence kernels. Jour-
nal of Machine Learning Research, 3:1059?1082.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL-04.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19(4):479?496.
Roxana Girju. 2006. Out-of-context noun phrase seman-
tic interpretation with cross-linguistic evidence. In
Proceedings of CIKM-06.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL-05.
David Graff, 2003. English Gigaword. Linguistic Data
Consortium, Philadelphia.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cate-
gorisation. In Proceedings of ICML-01.
S. Sathiya Keerthi and Chih-Jen Lin. 2003. Asymptotic
behaviors of support vector machines with Gaussian
kernel. Neural Computation, 15:1667?1689.
Mirella Lapata and Frank Keller. 2004. The Web as a
baseline: Evaluating the performance of unsupervised
Web-based models for a range of NLP tasks. In Pro-
ceedings of HLT-NAACL-04.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML-06.
Sebastian Pado? and Mirella Lapata. 2003. Constructing
semantic space models from parsed corpora. In Pro-
ceedings of ACL-03.
Diarmuid O? Se?aghdha. 2007. Annotating and learn-
ing compound noun semantics. In Proceedings of the
ACL-07 Student Research Workshop.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, Cambridge.
Michael Strube and Simone Paolo Ponzetto. 2006.
WikiRelate! computing semantic relatedness using
Wikipedia. In Proceedings of AAAI-06.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. LEILA: Learning to extract infor-
mation by linguistic analysis. In Proceedings of the
ACL-06 Workshop on Ontology Learning and Popula-
tion.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1?3):251?278.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
64
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 73?80,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic composition with (Robust) Minimal Recursion Semantics
Ann Copestake
Computer Laboratory, University of Cambridge
JJ Thomson Avenue, Cambridge, UK
aac@cl.cam.ac.uk
Abstract
We discuss semantic composition in Mini-
mal Recursion Semantics (MRS) and Robust
Minimal Recursion Semantics (RMRS). We
demonstrate that a previously defined for-
mal algebra applies to grammar engineering
across a much greater range of frameworks
than was originally envisaged. We show
how this algebra can be adapted to compo-
sition in grammar frameworks where a lex-
icon is not assumed, and how this underlies
a practical implementation of semantic con-
struction for the RASP system.
1 Introduction
Minimal Recursion Semantics (MRS: Copestake et
al. (2005)) is a flat semantic representation which
factors semantics into elementary predications (EPs)
and allows for underspecification of scope. It has
been widely used, especially for HPSG. Robust
Minimal Recursion Semantics (RMRS: Copestake
(2003)) is a variant of MRS which takes this fac-
torisation further to allow underspecification of re-
lational information as well. While MRS has gen-
erally been used with hand-built HPSG grammars,
RMRS is also suitable for use with shallower ap-
proaches to analysis, including part-of-speech tag-
ging, noun phrase chunking and stochastic parsers
which operate without detailed lexicons. MRSs can
be converted into RMRSs: RMRS output from shal-
lower systems is less fully specified than the out-
put from deeper systems, but in principle fully com-
patible. In our work, the semantics produced by a
deep grammar is taken as normative when devel-
oping semantic representations from shallower pro-
cessing. For English, the target semantic represen-
tations are those produced by the English Resource
Grammar (ERG, Flickinger (2000)). The MRS/RMRS
approach has been adopted as a common framework
for the DELPH-IN initiative (Deep Linguistic Pro-
cessing with HPSG: http://www.delph-in.net).
An algebra for MRS was defined by Copestake et
al. (2001) (henceforth CLF) and forms the starting
point for the work reported here.
The aim of CLF was to formalise the notion of se-
mantic composition within grammars expressed in a
typed feature structure (TFS) logic. Here, we ex-
tend that work to non-lexicalist approaches and also
describe how the formal principles of composition
used in MRS can be adapted to produce a formalism
for RMRS composition. Thus we demonstrate that
the algebra applies to grammar engineering across
a much wider range of frameworks than was origi-
nally envisaged. Besides its theoretical interest, this
result has practical benefits when combining multi-
ple processing systems in that it allows compatible
semantic representations at a phrasal level as well as
at a sentence level.
The next section (?2) describes the most impor-
tant features of MRS, RMRS and the earlier work on
the algebra. We then outline how the algebra can
be used for implementing deep non-TFS approaches
(?3) and explain how it works with RMRS (?4). This
is followed by discussion of the extension to gram-
mars without a detailed lexicon (?5). To briefly illus-
trate the practical applications, section (?6) outlines
how RMRS semantics is constructed from RASP (Ro-
bust accurate domain-independent statistical pars-
ing: Briscoe and Carroll (2002)).
2 MRS, RMRS and the algebra
Details of MRS, RMRS and the algebra are given in
the cited papers, but we will briefly introduce them
here for convenience. Fig. 1 illustrates an MRS from
a deep grammar (based on the ERG output, but sim-
plified for expository purposes), an equivalent RMRS
and a very underspecified RMRS, derived from a
POS tagger.
MRS achieves a flat representation via the use of
labels on EPs, thus factoring out scopal relation-
ships. Scope constraints (HCONS) are shown as qeq
relationships (=q equality modulo quantifiers: the
73
MRS representation:
l0: the q(x0, h01, h02), l1: fat j(x1), l2: cat n(x2), l3: sit v 1(e3, x3), l4: on p(e4, e41, x4),
l5: a q(x5, h51, h52), l6: mat n 1(x6),
h01 =q l1, h51 =q l6
x0 = x1 = x2 = x3, e3 = e41, x4 = x5 = x6, l1 = l2, l3 = l4
RMRS equivalent to the MRS above:
l0: a0: the q(x0), l0: a0: RSTR(h01), l0: a0: BODY(h02), l1: a1: fat j(x1), l2: a2: cat n(x2),
l3: a3: sit v 1(e3), l3: a3: ARG1(x31), l4: a4: on p(e4, e41, x4), l4: a4: ARG1(e41), l4: a4: ARG2(x4),
l5: a5: a q(x5), l5: a5: RSTR(h51), l5: a5: BODY(h52), l6: a6: mat n 1(x6),
h01 =q l1, h51 =q l6
x0 = x1 = x2 = x3, e3 = e41, x4 = x5 = x6, l1 = l2, l3 = l4
Highly underspecified RMRS output:
l0: a0: the q(x0), l1: a1: fat j(x1), l2: a2: cat n(x2), l3: a3: sit v(e3), l4: a4: on p(e4),
l5: a5: a q(x5), l6: a6: mat n(x6)
Figure 1: MRS and RMRS for the fat cat sat on a mat
details are not important to understand this paper).
In MRS, implicit conjunction is indicated by equality
between labels. For instance, the labels on l1: fat(x)
and l2: cat1(x) are equated. In this figure, we show
MRS using explicit equalities (eqs: =) rather than
coindexation of variables since this corresponds to
the formalism used in the algebra.
RMRS uses the same approach to scope but
adopts a variant of a neo-Davidsonian representa-
tion, where arguments (ARGs) are represented as
distinct elements. In the very underspecified RMRS
at the bottom of Fig.1, no relational information is
known so there are no ARGs. Separating out ARGs
from the EPs and allowing them to be omitted per-
mits a straightforward notion of a specificity hierar-
chy in terms of information content. ARGs may also
be underspecified: e.g., ARGn indicates that there
is some argument relationship, but it is unknown
whether it is an ARG1, ARG2 or ARG3. In the
version of RMRS described in this paper, the ARGs
are related to the main EPs via an ?anchor? element.
An EP and its associated ARGs share a unique an-
chor. This version of RMRS uses exactly the same
mechanism for conjunction as does MRS: the anchor
elements are required so that ARGs can still be asso-
ciated with a single EP even if the label of the EP has
been equated with another EP. This is a change from
Copestake (2003): the reasons for this proposal are
discussed in ?4, below. The conjunction informa-
tion is not available from a POS tagger alone and so
is not present in the second RMRS in Fig.1.
The naming convention adopted for the relations
(e.g., sit v) allows them to be constructed without
access to a lexicon. ? v? etc are indications of the
coarse-grained sense distinctions which can be in-
ferred from part-of-speech information. Deep gram-
mars can produce finer-grained sense distinctions,
indicated by ? 1? etc, and there is an implicit hier-
archy such that sit v 1 is taken as being more spe-
cific than sit v. However, in what follows, we will
use simple relation names for readability. MRS and
RMRS both assume an inventory of features on vari-
ables which are used to represent tense etc, but these
will not be discussed in this paper.
2.1 The MRS algebra
In the algebra introduced by CLF, semantic struc-
tures (SEMENTS) for phrases consist of five parts:
1. Hooks: can be thought of as pointers into the
relations list. In a full grammar, hooks consist
of three parts: a label (l), an index (i) and an
external argument (omitted here for simplicity).
2. Slots: structures corresponding to syntac-
tic/semantic unsaturation ? they specify how
the semantics is combined. A slot in one sign is
instantiated by being equated with the hook of
another sign. (CLF use the term ?hole? instead
of ?slot?.) For the TFS grammars considered
in CLF, the slot corresponds to the part of the
TFS accessed via a valence feature. The inven-
tory of slot labels given by CLF is SUBJ, SPR,
SPEC, COMP1, COMP2, COMP3 and MOD.
74
3. rels: The bag of EPs.
4. hcons: qeq constraints (=q).
5. eqs: the variable equivalences which are the re-
sults of equating slots and hooks.
SEMENTs are: [l, i]{slots}[eps][hcons]{eqs}.
Some rules contribute their own semantics (con-
struction semantics: e.g., compound nouns). How-
ever, the MRS approach requires that this can al-
ways be treated as equivalent to having an additional
daughter in the rule. Thus construction semantics
need not be considered separately in the formal al-
gebra, although it does result in some syntactically
binary rules being semantically ternary (and so on).
The principles of composition are:
1. A (syntactically specified) slot in one structure
(the daughter which corresponds to the seman-
tic head) is filled by the hook of the other struc-
ture (by adding equalities).
2. The hook of the phrase is the semantic head?s
hook.
3. The eps of the phrase is equal to appending the
eps of the daughters.
4. The eqs of the phrase is equal to appending the
eqs of the daughters plus any eqs contributed
by the filling of the slot.
5. The slots of the phrase are the unfilled slots of
the daughters (although see below).
6. The hcons of the phrase is equal to appending
the hcons of the daughters.
Formally, the algebra is defined in terms of a se-
ries of binary operations, such as opspec, which
each correspond to the instantiation of a particular
labelled slot.
Fig. 2 illustrates this. The hook of cat instanti-
ates the SPEC slot of a, which is the semantic head
(though not the syntactic head in the ERG). This
leads to the equalities between the variables in the
result. Since the SPEC slot has been filled, it is not
carried up to the phrase. Thus, abstractly at least,
the semantics of the HPSG specifier-head rule cor-
responds to opspec.1
1As usual in MRS, in order to allow scope underspecifica-
tion, the label l4 of the quantifier?s hook is not coindexed with
any EP.
The MRS algebra was designed to abstract away
from the details of the syntax and of the syntax-
semantics interface, so that it can be applied to
grammars with differing feature geometry. The as-
sumption in CLF is simply that the syntax selects
the appropriate op and its arguments for each ap-
plication. i.e., semantic operations are associated
with HPSG constructions so that there is a mapping
from the daughters of the construction to the argu-
ments of the operation. The algebra does not attempt
to completely replicate all aspects of semantic con-
struction: e.g., the way that the features (represent-
ing tense and so on) are instantiated on variables is
not modelled. However, it does constrain semantic
construction compared with the possibilities for TFS
semantic compositional in general. For instance, as
discussed by CLF, it enforces a strong monotonic-
ity constraint. The algebra also contributes to limit-
ing the possibilities for specification of scope. These
properties can be exploited by algorithms that oper-
ate on MRS: e.g., generation, scope resolution.
2.2 The MRS algebra and the syntax-semantics
interface
CLF did not discuss the syntax-semantics interface
in detail, but we do so here for two reasons. Firstly,
it is a preliminary for discussing the use of the al-
gebra in frameworks other than HPSG in the fol-
lowing sections. Secondly, as CLF discuss, the con-
straints that the algebra imposes cannot be fully im-
plemented in a TFS. Thus, for grammar engineering
in TFS frameworks, an additional automatic checker
is needed to determine whether a grammar meets the
algebra?s constraints. This requires specification of
the syntax-semantics interface so that the checker
can extract the slots from the TFSs and determine
the slot operation(s) corresponding to a rule.
Unfortunately, CLF are imprecise about the alge-
bra in several respects. One problem is that they
gloss over the issue of slot propagation in real gram-
mars. CLF state that for an operation opx, the slot
corresponding to opx on the semantic head is instan-
tiated and all other slots appear on the result. For
instance, the definition of opspec states that for all
labels l 6= spec: slotl(opspec(a1, a2)) = slotl(a1)?
slotl(a2). However, this is inadequate for real gram-
mars, if a simple correspondence between the slot
names and the valence paths in the feature structure
75
hook slots rels eqs hcons
cat : [l1, x1] {} [l1 : cat(x1)] {} []
a : [l4, x2] {[l3, x2]spec} [l2 : a(x2, h2, h3)] {} [h2 =q l3]
a cat : [l4, x2] {} [l2 : a(x2, h2, h3), l1 : cat(x1)] {l3 = l1, x2 = x1} [h2 =q l3]
Figure 2: Example of the MRS algebra
is assumed. For instance, the passive rule involves
coindexing a COMP in the original lexical sign with
the SUBJ of the passive (informally, the complement
?becomes? the subject).
There are two ways round this problem. The first
is to keep the algebra unchanged, but to assume that,
for instance, the subject-head grammar rule corre-
sponds to opsubj in the algebra for non-passivized
cases and to opcomp1 for passives of simple tran-
sitives and so on. Though possible formally, this is
not in accord with the spirit of the approach since
selection of the appropriate algebra operation in the
syntax-semantics interface would require non-local
information. Practically, it also precludes the im-
plementation of an algebra checker, since keeping
track of the slot uses would be both complex and
grammar-specific. The alternative is to extend the
algebra to allow for slot renaming. For instance,
opcomp1-subj can be defined so that the COMP1 slot
on the daughter is a SUBJ slot on the mother.
1. For all labels l 6= comp1, l 6= subj:
slotl(opcomp1-subj(a)) = slotl(a)
2. slotsubj(opcomp1-subj(a)) = slotcomp1(a)
This means extending the inventory of operations,
but the choice of operation is then locally deter-
minable from the rule (e.g., the passive rule would
specify opcomp1-subj to be its operation).
Another issue arises in grammars which allow for
optional complements. For instance, one approach
to a verb like eat is to give it a single lexical en-
try which corresponds to both transitive and intran-
sitive uses. The complement is marked as optional
and the corresponding variable in the semantics is
assumed to be discourse bound if there is no syn-
tactic complement in the phrase. Optional comple-
ments can be discharged by a construction. This ap-
proach is (arguably) appropriate for eat because the
intransitive use involves an implicit patient (e.g., I
already ate means I already ate something), in con-
trast to a verb like kick. CLF do not discuss op-
tionality but it can be formalised in the algebra in
terms of a construction-specified sement which has
a hook containing the discourse referent and is oth-
erwise empty. For instance, an optional complement
construction corresponds to opcomp1(a1, a2) where
a1 is the head (and the only daughter appearing in
the TFS for the construction) and a2 is stipulated by
the rule to be [l, d]{}[][]{}, where d is the discourse-
bound referent.
3 The algebra in non-lexicalist grammars
CLF motivate the MRS algebra in terms of formalisa-
tion of the semantics of constraint-based grammars,
such as HPSG, but, as we outline here, it is equally
applicable to non-lexicalist frameworks. With a suit-
able definition of the syntax-semantics interface, the
algebra can be used with non-TFS-based grammars.
Fig. 3 sketches an example of MRS semantics for a
CFG. A syntax-semantic interface component of the
rule (shown in the second line of the figure) specifies
the ops and their daughters: the IOBJ slot of the verb
is instantiated with the first NP?s hook and the OBJ
slot of the result is instantiated with the hook of the
second NP. The idea is extremely similar to the use
of the algebra with TFS but note that with the ad-
dition of this syntax-semantic interface, the algebra
can be used directly to implement semantic compo-
sition for a CFG.
This still relies on the assumption that all slots
are known for every lexical item: semantically the
grammar is lexicalist even though it is not syntacti-
cally. In fact this is analogous to semantic compo-
sition in GPSG (Gazdar et al, 1985) in that conven-
tional lambda calculus also assumes that the seman-
tic properties are known at the lexical level.
4 RMRS composition with deep grammars
The use of the CLF algebra in RMRS composition
with deep lexicalist grammars is reasonably straight-
76
VP -> Vditrans NP1 NP2
opobj(opiobj(Vditrans, NP1), NP2)
MRSs for application of the rule to give a cat a rat.
hook slots rels eqs
give : [l1, e1] {[l1, x12]subj , [l1 : give(e1, x12, x13, x14)] {}
[l1, x13]obj , [l1, x14]iobj}
a cat : [l4, x2] {} [l2 : a(x2, h2, h3), l1 : cat(x1)] {l3 = l1, x2 = x1}
a rat : [l7, x5] {} [l5 : a(x5, h5, h6), l4 : rat(x4)] {l6 = l4, x5 = x4}
iobj : [l1, e1] {[l1, x12]subj , [l1, x13]obj} [l1 : give(e1, x12, x13, x14), {l3 = l1, x2 = x1,
l2 : a(x2, h2, h3), l1 : cat(x1)] l1 = l4, x14 = x2}
obj : [l1, e1] {[l1, x12]subj} [l1 : give(e1, x12, x13, x14), {l3 = l1, x2 = x1,
l2 : a(x2, h2, h3), l1 : cat(x1), l1 = l4, x14 = x2,
l5 : a(x5, h5, h6), l4 : rat(x4)] l1 = l7, x13 = x5}
Figure 3: MRS algebra with a CFG (hcons omitted for clarity)
forward.2 The differences between MRS and RMRS
are that RMRS uses anchors and factors out the
ARGs. Thus for RMRS, we need to redefine the no-
tion of a semantic entity from the MRS algebra to
add anchors. An RMRS EP thus contains:
1. a handle, which is the label of the EP
2. an anchor (a)
3. a relation
4. up to one argument of the relation
Hooks also include anchors: {[l, a, i]} is a hook.
Instead of the rels list only containing EPs, such
as l1:chase(e,x,y), it contains a mixture of EPs
and ARGs, with associated anchors, such as
l1:a1:chase(e), l1:a1:ARG1(x), l1:a1:ARG2(y). But
formally ARGs are EPs according to the definition
above, so this requires no amendment of the alge-
bra. Fig. 4 shows the RMRS version of Fig. 2.
As mentioned above, earlier forms of RMRS used
an explicit representation for conjunction: the in-
group, or in-g. Reasons to avoid explicit binary
conjunction were discussed with respect to MRS by
Copestake et al (2005) and readers are referred to
that paper for an explanation: essentially the prob-
lem is that the syntactic assumptions influence the
semantic representation. e.g., the order of combi-
nation of intersective modifiers affects the semantic
2Current DELPH-IN grammars generally construct MRSs
which may be converted into RMRSs. However, RMRS has
potential advantages, for instance in allowing more extensive
lexical underspecification than is possible with MRS: e.g.,
(Haugereid, 2004).
representation, though it has no effect on denotation.
The binary in-g suffers from this problem.
One alternative would be to use an n-ary conjunc-
tion symbol. However such representations cannot
be constructed compositionally if modification is bi-
nary branching as there is no way of incrementally
adding the conjuncts. Another option we considered
was the use of, possibly redundant, conjunction re-
lations associated with each element which could be
combined to produce a flat conjunction. This leads
to a spurious in-g in the case where there is no mod-
ifier. This looks ugly, but more importantly, does
not allow for incremental specialisation, although
the demonstration of this would take us too far from
the main point of this paper.
We therefore assume a modified version of RMRS
which drops in-g symbols but uses anchors instead.
This means that RMRS and MRS TFS grammars can
be essentially identical apart from lexical types. Fur-
thermore, it turns out that, for composition without
a lexicon, an anchor is needed in the hook regardless
of the treatment of conjunction (see below).
5 RMRS composition without a lexicon
We now discuss the algebra for grammars which
do not have access to subcategorization information
and thus are neither syntactically nor semantically
lexicalist. We concentrate in particular on composi-
tion for the grammar used in the RASP system. RASP
consists of a tokenizer, POS tagger, lemmatizer, tag
sequence grammar and statistical disambiguator. Of
the robust analysers we have looked at, RASP pro-
77
hook slots rels eqs hcons
cat : [l1, a1, x1] {} [l1 : a1 : cat(x1)] {} []
a : [l4, a2, x2] {[l3, a2, x2]spec} [l2 : a2 : a(x2), {} [h2 =q l3]
l2 : a2 : rstr(h2), l2 : a2 : body(h2)]
a cat : [l4, a4, x2] {} [l1 : a1 : cat(x1), l2 : a2 : a(x2), {l3 = l1, [h2 =q l3]
l2 : a2 : rstr(h2), l2 : a2 : body(h2) x2 = x1]
Figure 4: Example of the RMRS algebra.
vides the biggest challenge for the RMRS approach
because it provides quite detailed syntactic analy-
ses which are somewhat dissimilar to the ERG: it
is an intermediate rather than a shallow processor.
The RMRS approach can only be fully successful to
the extent that it abstracts away from the differences
in syntactic analyses assumed by different systems,
so intermediate processors are more difficult to deal
with than shallow ones.
Instead of normal lexical entries, RASP uses the
POS tags for the words in the input. For the exam-
ple in Fig. 1, the output of the POS tagging phase is:
the AT fat JJ cat NN1 sit+ed VVD on II a AT1
mat NN1
The semantics associated with the individual words
in the sentence can be derived from a ?lexicon? of
POS tags, which defines the EPs. Schematically:
AT lexrel q(x) NN1 lexrel n(x)
AT1 lexrel q(x) VVD lexrel v(epast)
JJ lexrel j(x) II lexrel p(e)
Here, ?lexrel? is a special symbol, which is to
be replaced by the individual lemma (with a
leading underscore) ? e.g., lexrel v(epast) yields
l1:a1: sit v(e). Producing the semantics from the
tagger output and this lexicon is a simple matter of
substitution. All EPs are labelled with unique labels
and all variables are different unless repeated in the
same lexical entry.
If the analysis were to stop at POS tagging, the
semantic composition rules would apply trivially.
There are no slots, the hooks are irrelevant and there
are no equalities. The composition principle of ac-
cumulation of elementary predications holds, so the
semantics of the result involves an accumulation of
the rels (see the example at the bottom of Fig. 1).
When using the full RASP parser, although we
cannot expect to obtain all the details available from
deep grammars, we can derive some relational struc-
ture. For instance, given a sentence such as the
cat chased the rat, it should be possible to derive
the ARG1 and ARG2 for chase by associating the
ARG1 with the application of the S/np_vp RASP
rule (i.e., S->NP VP) and the ARG2 with the appli-
cation of the V1/v_np rule. But since there can be
no slot information in the lexical structures (at least
not for open-class words), it is necessary to modify
the lexicalist approach to semantics taken so far.
We assume that both the ARGs and the slots are
specified at a phrasal level rather than lexically. As
mentioned in ?2.1, the MRS algebra allows for rules
to contribute semantics as though they were normal
phrases. The central idea in the application of the
algebra to RASP is to make use of construction se-
mantics in all rules. Fig. 5 illustrates this with the
V1/v_np rule (the NP has been simplified for clar-
ity) assuming the same sort of syntax-semantics in-
terface specification as shown earlier for the CFG.
This is semantically ternary because of the rule se-
mantics. The rule has an ARG2 slot plus a slot R
which is instantiated by the verb?s hook. In effect,
the rule adds a slot to the verb.
It is necessary for the anchor of the argument-
taking structure to be visible at all points where ar-
guments may be attached. For instance, in the ex-
ample above, the anchor of the verb chase has to
be accessible when the ARG1 relation is introduced.
Although generally the anchor will correspond to the
anchor of the semantic head daughter, this is not the
case if there is a scopal modifier (consider a cat did
not chase a rat: the ARG1 must be attached to chase
rather than to not). This is illustrated by not sleep
in Fig. 6. Because not is associated with a unique
tag in RASP, it can be assigned a slot and an ARG1
directly. The anchor of the result is equated with
the label of sleep and thus the subject ARG1 can be
appropriately attached. So the hook would have to
include an anchor even if explicit conjunction were
used instead of equating labels.
78
VP -> V NP
oparg2(opr(rule, V), NP)
chase : [l1, a1, e1] {} [l1 : a1 : chase(e1)] {}
rule : [l2, a2, e2] {[l2, a2, e2]r, [l2 : a2 : ARG2(x2)] {}
[l4, a4, x2]arg2}
(rule V)/r : [l2, a2, e2] {[l4, x2]arg2} [l2 : a1 : ARG2(x2), l1 : a1 : chase(e1)] {l1 = l2, e2 = e1}
it : [l3, a3, x3] {} [l3 : a3 : pron(x3)] {}
chase it : [l2, a2, e2] {} [l2 : a2 : ARG2(x2), l1 : a1 : chase(e1), {l1 = l2, e2 = e1,
l3 : a3 : pron(x3)] l4 = l3, x2 = x3}
Figure 5: RASP-RMRS algebra (hcons omitted)
not : [l1, a2, e2] {[l2, a3, e2]mod} [l1 : a1 : not(e2), l1 : a1 : ARG1(h4)] {} [h4 =q l2]
sleep : [l2, a2, e2] {} [l2 : a2 : sleep(e2)] {} []
not sleep : [l1, a2, e2] {} [l1 : a1 : not(e2), l1 : a1 : ARG1(h4), {} [h4 =q l3]
l2 : a2 : sleep(e2)]
Figure 6: RASP-RMRS illustrating the use of the anchor
6 Experiments with RASP-RMRS
In this section, we outline the practical implementa-
tion of the algebra for RASP-RMRS. The RASP tag
sequence grammar is formally equivalent to a CFG:
it uses phrase structure rules augmented with fea-
tures. As discussed, the algebra requires that ops
are specified for each rule application, and the eas-
iest way of achieving this is to associate semantic
composition rules with each rule name. Composi-
tion operates on the tree output from RASP, e.g.,:
(|T/txt-sc1/----|
(|S/np_vp|
(|NP/det_n1| |Every:1_AT1|
(|N1/n| |cat:2_NN1|))
(|V1/v| |bark+ed:3_VVD|)))
Composition operates bottom-up: the semantic
structures derived from the tags are combined ac-
cording to the semantics associated with the rule.
The implementation corresponds very directly to the
algebra, although the transitive closure of the equali-
ties is computed on the final structure, since nothing
requires that it be available earlier.
The notation used to specify semantics associated
with the rules incorporates some simplifications to
avoid having to explicitly specify the slot and ops.
The specification of equalities between variables and
components of the individual daughters? hooks is a
convenient shorthand for the full algebra.
rule V1/v_np
daughters V NP
semhead V
hook [l,a,e] rels {l:a:ARG2(x)}
eqs {x=NP.index,l=V.label,
a=V.anchor}
If no semantic rule is specified corresponding to
a rule used in a tree, the rels are simply appended.
Semantic composition is thus robust to omissions in
the semantic component of the grammar. In fact, se-
mantic rules can be constructed semi-automatically,
rather than fully manually, although we do not have
space to discuss this in detail here.
There are cases of incompatibility between RASP-
RMRS and ERG-RMRS. For example, the ERG treats
it as expletive in it rains: the lexical entry for rain
specifies an expletive subject (i.e., a semantically
empty it). RASP makes no such distinction, since
it lacks the lexical information and thus the sentence
has extraneous relations for the pronoun and an in-
correct ARG1 for rain. This is an inevitable conse-
quence of the lack of lexical information in RASP.
However, from the perspective of the evaluation of
the revised algebra, the issue is whether there are any
cases where compositional construction of RASP-
RMRSs which match ERG-RMRSs is impossible due
to the restrictions imposed by the algebra. No such
cases have been found.
79
7 Related work
Bos et al (2004) and Bos (2005) derive semantic in-
terpretations from a wide-coverage categorial gram-
mar. There are several differences between this and
RASP-RMRS, but the most important arise from the
differences between CCG and RASP. The CCG parser
relies on having detailed subcategorization infor-
mation (automatically derived from the CCG Bank
which was semi-automatically constructed from the
Penn Treebank), and thus semantic construction can
assume that the arity of the predicate is lexically
available. However, because CCG is purely lexical-
ist, phenomena that we expect to have construction
semantics (e.g., compound nouns, larger numbers)
have to be dealt with in a post-parsing phase rather
than compositionally.
Spreyer and Frank (2005) demonstrate RMRS con-
struction from TIGER dependencies, but do not at-
tempt to match a deep parser output.
8 Conclusion
We have demonstrated that the MRS algebra, orig-
inally intended as a formalisation of some aspects
of semantic composition in constraint-based gram-
mars, can be extended to RMRS and other types of
grammar framework and can be used as the basis of
a full implementation of composition. The algebra
can thus be used much more widely than originally
envisaged and could be exploited by a wide range of
parsers. Useful properties concerning monotonicity
and scope (see Fuchss et al (2004)) are thus guaran-
teed for a range of grammars. Phrasal-level com-
patibility of RMRS (to the extent that this is syn-
tactically possible) is also an important result. The
main practical outcome of this work so far has been
a semantic component for the RASP system which
produces representations compatible with that of the
ERG without compromising RASP speed or robust-
ness. RASP-RMRSs have already been used in sys-
tems for question answering, information extraction,
email response, creative authoring and ontology ex-
traction (e.g., Uszkoreit et al (2004), Watson et al
(2003), Herbelot and Copestake (2006)).
Acknowledgements
This work was funded by EPSRC (EP/C010035/1).
I am very grateful to the anonymous reviewers for
their insightful comments, which sadly I have had
to ignore due to the constraints of time and space. I
hope to address them in a later paper.
References
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran and Julia Hockenmaier 2004. Wide-Coverage
Semantic Representations from a CCG Parser. COL-
ING ?04, Geneva.
Johan Bos. 2005. Towards Wide-Coverage Semantic In-
terpretation. Sixth International Workshop on Compu-
tational Semantics IWCS-6. 42?53.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. LREC-2002, Las
Palmas, Gran Canaria.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An Algebra for Semantic Construction in
Constraint-based Grammars. ACL-01, Toulouse.
Ann Copestake. 2003. Report on the design of RMRS.
DeepThought project deliverable.
Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pol-
lard. 2005. Minimal Recursion Semantics: An in-
troduction. Research in Language and Computation
3(2?3), 281?332.
Dan Flickinger 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering,6:1,15?28.
Ruth Fuchss, Alexander Koller, Joachim Niehren, and
Stefan Thater 2004. Minimal Recursion Semantics as
Dominance Constraints: Translation, Evaluation, and
Analysis. Proceedings of the 42nd ACL, Barcelona.
Gerald Gazdar, Ewan Klein, Geoffrey Pullum and Ivan
Sag 1985. Generalized Phrase Structure Grammar.
Basil Blackwell, Oxford
Petter Haugereid 2004. Linking in Constructions.
HPSG2004, Leuven.
Aurelie Herbelot and Ann Copestake 2006. Acquir-
ing Ontological Relationships from Wikipedia Us-
ing RMRS. ISWC 2006 Workshop on Web Content
Mining with Human Language Technologies, Athens,
Georgia.
Kathrin Spreyer and Anette Frank. 2005 Projecting
RMRS from TIGER Dependencies. HPSG 2005, Lis-
bon. 354?363.
Hans Uszkoreit, Ulrich Callmeier, Andreas Eisele, Ul-
rich Schfer, Melanie Siegel, Jakob Uszkoreit. 2004.
Hybrid Robust Deep and Shallow Semantic Process-
ing for Creativity Support in Document Production.
KONVENS 2004, Vienna, Austria, 209?216.
Rebecca Watson, Judita Preiss and EJ Briscoe. 2003.
The Contribution of Domain-independent Robust
Pronominal Anaphora Resolution to Open-Domain
Question-Answering. Int. Symposium on Reference
Resolution and its Application to Question-Answering
and Summarisation, Venice.
80
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 54?62,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Cascaded Classifiers for Confidence-Based Chemical Named Entity
Recognition
Peter Corbett
Unilever Centre For
Molecular Science Informatics
Chemical Laboratory University Of Cambridge
CB2 1EW, UK
ptc24@cam.ac.uk
Ann Copestake
Computer Laboratory
University Of Cambridge
CB3 0FD, UK
aac10@cl.cam.ac.uk
Abstract
Chemical named entities represent an impor-
tant facet of biomedical text. We have de-
veloped a system to use character-based n-
grams, Maximum Entropy Markov Models
and rescoring to recognise chemical names
and other such entities, and to make confi-
dence estimates for the extracted entities. An
adjustable threshold allows the system to be
tuned to high precision or high recall. At a
threshold set for balanced precision and recall,
we were able to extract named entities at an
F score of 80.7% from chemistry papers and
83.2% from PubMed abstracts. Furthermore,
we were able to achieve 57.6% and 60.3% re-
call at 95% precision, and 58.9% and 49.1%
precision at 90% recall. These results show
that chemical named entities can be extracted
with good performance, and that the proper-
ties of the extraction can be tuned to suit the
demands of the task.
1 Introduction
Systems for the recognition of biomedical named
entities have traditionally worked on a ?first-best?
approach, where all of the entities recognised have
equal status, and precision and recall are given
roughly equal importance. This does not reflect that
fact that precision is of greater importance for some
applications, and recall is the key for others. Fur-
thermore, knowing the confidence1 with which the
1In this paper, we use ?confidence? to refer to a system?s
estimate of the probability that a potential named entity is a cor-
rect named entity.
system has assigned the named entities is likely to
be useful in a range of different applications.
Named entities of relevance to biomedical sci-
ence include not only genes and proteins but also
other chemical substances which can be of inter-
est as drugs, metabolites, nutrients, enzyme cofac-
tors, experimental reagents and in many other roles.
We have recently investigated the issue of chemical
named entities (Corbett et al, 2007), by compiling a
set of manual annotation guidelines, demonstrating
93% interannotator agreement and manually anno-
tating a set of 42 chemistry papers. In this paper we
demonstrate a named entity recogniser that assigns
a confidence score to each named entity, allowing it
to be tuned for high precision or recall.
Our review of the methods of chemical named
entity recognition showed a consistent theme: the
use of character-based n-Grams to identify chemi-
cal names via their constituent substrings (Wilbur et
al., 1999; Vasserman, 2004; Townsend et al, 2005).
This can be a powerful technique, due to systematic
and semisystematic chemical names and additional
conventions in drug names. However this technique
does not cover all aspects of chemical nomenclature.
Much current named entity work uses approaches
which combine the structured prediction abilities
of HMMs and their derivatives with techniques
which enable the use of large, diverse feature sets
such as maximum entropy (also known as logis-
tic regression). Maximum Entropy Markov Mod-
els, (MEMMs) (McCallum et al, 2000) provide a
relatively simple framework for this. MEMMs do
have a theoretical weakness, namely the ?label bias?
problem (Lafferty et al, 2001), which has been ad-
54
dressed with the development of Conditional Ran-
dom Fields (CRFs). CRFs are now a mainstay of
the field, being used in a high proportion of entries
in the latest BioCreative evaluation (Krallinger and
Hirschman, 2007). However, despite the label bias
problem, MEMMs still attract interest due to practi-
cal advantages such as shorter training cycles.
The framework of HMMs and their successors of-
fers three modes of operation; first-best, n-best and
confidence-based. In first-best NER, the Viterbi al-
gorithm is used to identify a single sequence of la-
bels for the target sentence. In n-best operation,
the n best sequences for the sentence are identi-
fied, along with their probabilities, for example by
coupling the Viterbi algorithm with A* search. In
confidence-based operation, potential entities (with
a probability above a threshold) are identified di-
rectly, without directly seeking a single optimal la-
belling for the entire sentence. This is done by
examining the probability of the label transitions
within the entity, and the forward and backward
probabilities at the start and end of the entity. This
mode has been termed the Constrained Forward-
Backward algorithm (Culotta andMcCallum, 2004).
Where a single unambiguous non-overlapping la-
belling is required, it can be obtained by identify-
ing cases where the entities overlap, and discarding
those with lower probabilities.
Confidence-based extraction has two main advan-
tages. First, it enables the balance between precision
and recall to be controlled by varying the probability
threshold. Second, confidence-based NER avoids
over-commitment in systems where it is used as a
preprocessor, since multiple overlapping options can
be used as input to later components.
The optimum balance between recall and preci-
sion depends on the application of the NER and on
the other components in the system. High precision
is useful in search even when recall is low when
there is a large degree of redundancy in the informa-
tion in the original documents. High precision NER
may also be useful in contexts such as the extraction
of seed terms for clustering algorithms. Balanced
precision/recall is often appropriate for search, al-
though in principle it is desirable to be able to shift
the balance if there are too many/too few results.
Balanced precision/recall is also generally assumed
for use in strictly pipelined systems, when a single
set of consistent NER results is to be passed on to
subsequent processing. Contexts where high recall
is appropriate include those where a search is being
carried out where there is little redundancy (cf Car-
penter 2007) or where the NER system is being used
with other components which can filter the results.
One use of our NER system is within a language
processing architecture (Copestake et al, 2006) that
systematically allows for ambiguity by treating the
input/output of each component as a lattice (repre-
sented in terms of standoff annotation on an orig-
inal XML document). This system exploits rela-
tively deep parsing, which is not fully robust to NER
errors but which can exploit complex syntactic in-
formation to select between candidate NER results.
NER preprocessing is especially important in the
context of chemistry terms which utilise punctuation
characters (e.g., ?2,4-dinitrotoluene?, ?2,4- and 2,6-
dinitrotoluene?) since failure to identify these will
lead to tokenisation errors in the parser. Such errors
frequently cause complete parse failure, or highly
inaccurate analyses. In our approach, the NER re-
sults contribute edges to a lattice which can (option-
ally) be treated as tokens by the parser. The NER
results may compete with analyses provided by the
main parser lexicon. In this context, some NER er-
rors are unimportant: e.g., the parser is not sensitive
to all the distinctions between types of named entity.
In other cases, the parser will filter the NER results.
Hence it makes sense to emphasise recall over pre-
cision. We also hypothesise that we will be able to
incorporate the NER confidence scores as features
in the parse ranking model.
Another example of the use of high-recall NER in
an integrated system is shown in the editing work-
flows used by the Royal Society of Chemistry in
their Project Prospect system (Batchelor and Cor-
bett, 2007), where chemical named entity recogni-
tion is used to produce semantically-enriched jour-
nal articles. In this situation, high recall is desirable,
as false positives can be removed in two ways; by
removing entities where a chemical structure cannot
be assigned, and by having them checked by a tech-
nical editor. False negatives are harder to correct.
The use of confidence-based recognition has been
demonstrated with CRFs in the domain of contact
details (Culotta and McCallum, 2004), and using
HMMs in the domain of gene annotation (Carpen-
55
ter, 2007). In the latter case, the LingPipe toolkit
was used in the BioCreative 2 evaluation without
significant adaptation. Although only 54% preci-
sion was achieved at 60% recall (the best systems
were achieving precision and recall scores in the
high eighties), the system was capable of 99.99%
recall with 7% precision, and 95% recall with 18%
precision, indicating that very high recall could be
obtained in this difficult domain.
Another potential use of confidence-based NER
is the potential to rescore named entities. In this
approach, the NER system is run, generating a set
of named entities. Information obtained about these
entities throughout the document (or corpus) that
they occur in can then be used in further classi-
fiers. We are not aware of examples of rescoring
being applied to confidence-based NER, but there
are precedents using other modes of operations. For
example, Krishnan and Manning (2006) describe a
system where a first-best CRF is used to analyse a
corpus, the results of which are then used to gener-
ate additional features to use in a second first-best
CRF. Similarly, Yoshida and Tsujii (2007) use an n-
best MEMM to generate multiple analyses for a sen-
tence, and re-rank the analyses based on information
extracted from neighbouring sentences.
Therefore, to explore the potential of these tech-
niques, we have produced a chemical NER system
that uses a MEMM for confidence-based extraction
of named entities, with an emphasis on the use of
character-level n-Grams, and a rescoring system.
2 Corpus
Previously, we have produced a set of annotation
guidelines for chemical named entities, and used
them to annotate a set of 42 chemistry papers (Cor-
bett et al, 2007). Inter-annotator agreement was
tested on 14 of these, and found to be 93%. The an-
notation guidelines specified five classes of named
entity, which are detailed in Table 1. The annotation
was performed on untokenised text.
To test the applicability of the method to a
different corpus, we retrieved 500 PubMed ab-
stracts and titles, and annotated them using the
same methods. The abstracts were acquired us-
ing the query metabolism[Mesh] AND drug
AND hasabstract. This produced a diverse set
of abstracts spanning a wide range of subject ar-
eas, but which contain a higher proportion of rele-
vant terms than PubMed overall. 445 out of 500 ab-
stracts contained at least one named entity, whereas
249 contained at least ten. Notably, the ASE class
was more common in the PubMed corpus than in
the chemistry papers, reflecting the important of en-
zymes to biological and medical topics.
In this study, we have left out the named entity
type CPR, as it is rare (<1%) and causes difficulties
with tokenisation. This entity type covers cases such
as the ?1,3-? in ?1,3-disubstituted?, and as such re-
quires the ?1,3-? to be a separate token or token se-
quence. However, we have found that recognition
of the other four classes is improved if words such
as ?1,3-disubstituted? are kept together as single to-
kens. Therefore it makes sense to treat the recogni-
tion of CPR as an essentially separate problem - a
problem that will not be addressed here.
Type Description Example nCh nPM
CM compound citric acid 6865 4494
RN reaction methylation 288 401
CJ adjective pyrazolic 60 87
ASE enzyme demethylase 31 181
CPR prefix 1,3- 53 21
Table 1: Named Entity types. nCh = number in Chem-
istry corpus, nPM = number in PubMed corpus.
3 Methods
Our system is quite complex, and as such we have
made the source code available (see below). The fol-
lowing gives an outline of the system:
3.1 External Resources
Chemical names were extracted from the chem-
ical ontology ChEBI (Degtyarenko et al, 2008),
and a standard English word list was taken from
/usr/share/dict/words on a Linux system2.
A list of chemical element names and symbols was
also compiled. To overcome the shortage of enti-
ties of type ASE, a list of words from enzyme names
2This dictionary was chosen as it contains inflectional forms
of English words. Our system does not perform stemming,
partly because suffixes are often good cues as to whether a word
is chemical or not.
56
ending in ?-ase? was extracted from the Gene Ontol-
ogy (GO), and hand sorted into words of type ASE,
and words not of type ASE.
3.2 Overview of operation
The text is tokenised before processing; this is
done using the tokeniser described in our previous
work (Corbett et al, 2007), which is adapted to
chemical text.
Our system uses three groups of classifiers to
recognise chemical names. The first classifier?the
?preclassifier??uses character-level n-grams to esti-
mate the probabilities of whether tokens are chemi-
cal or not. The output of this classification is com-
bined with information from the suffix of the word,
and is used to provide features for the MEMM.
The second group of classifiers constitute the
MEMM proper. Named entities are represented us-
ing an BIO-encoding, and methods analogous to
other confidence-based taggers (Culotta and McCal-
lum, 2004; Carpenter, 2007) are used to estimate
the conditional probability of tag sequences corre-
sponding to named entities. The result of this is
a list of potential named entities, with start posi-
tions, end positions, types and probabilities, where
all of the probabilities are above a threshold value.
A small set of hand-written filtering rules is used to
remove obvious absurdities, such as named entities
ending in the word ?the?, and simple violations of
the annotation guidelines, such as named entities of
type ASE that contain whitespace. These filtering
rules make very little difference at recall values up
to about 80%?however, we have found that they are
useful for improving precision at very high recall.
The third group of classifiers?one per entity
type?implement a rescoring system. After all of
the potential entities from a document have been
generated, a set of features is generated for each en-
tity. These features are derived from the probabili-
ties of other entities that share the same text string
as the entity, from probabilities of potential syn-
onyms found via acronym matching and other pro-
cesses, and most importantly, from the pre-rescoring
probability of the entities themselves. In essence,
the rescoring process performs Bayesian reasoning
by adjusting the raw probabilities from the previ-
ous stage up or down based on nonlocal information
within the document.
3.3 Overview of training
A form of training conceptually similar to cross-
validation is used to train the three layers of clas-
sifiers. To train the overall system, the set of docu-
ments used for training is split into three. Two thirds
are used to train a MEMM, which is then used to
generate training data for the rescorer using the held-
out last third. This process is repeated another two
times, holding out a different third of the training
data each time. Finally, the rescorer is trained using
all of the training data generated by this procedure,
and the final version of the MEMM is generated us-
ing all of the training data. This procedure ensures
that both the MEMM and the rescorer are able to
make use of all of the training data, and also that
the rescorer is trained to work with the output of a
MEMM that has not been trained on the documents
that it is to rescore.
A similar procedure is used when training the
MEMM itself. The available set of documents to use
as training data is divided into half. One half is used
to train the preclassifier and build its associated dic-
tionaries, which are then used to generate features
for the MEMM on the other half of the data. The
roles of each half are then reversed, and the same
process is applied. Finally, the MEMM is trained
using all of the generated features, and a new pre-
classifier is trained using all of the available training
data.
It should be noted that the dictionaries extracted
during the training of the preclassifier are also used
directly in the MEMM.
3.4 The character n-gram based preclassifier
During the training of the preclassifier, sets of to-
kens are extracted from the hand-annotated train-
ing data. A heuristic is used to classify these
into ?word tokens??those that match the regex
.*[a-z][a-z].*, and ?nonword tokens??those
that do not (this class includes many acronyms and
chemical formulae). The n-gram analysis is only
performed upon ?word tokens?.
The token sets that are compiled are chemi-
cal word tokens (those that only appear inside
named entities), nonchemical word tokens (those
that do not appear in entities), chemical nonword
tokens, nonchemical nonword tokens and ambigu-
57
ous tokens?those that occur both inside and out-
side of named entities. A few other minor sets are
collected to deal with tokens related to such proper
noun-containing entities as ?Diels?Alder reaction?.
Some of this data is combined with external dic-
tionaries to train the preclassifier, which works us-
ing 4-grams of characters and modified Kneser-Ney
smoothing, as described by Townsend et al (2005).
The set of ?chemical word tokens? is used as a set of
positive examples, along with tokens extracted from
ChEBI, a list of element names and symbols, and
the ASE tokens extracted from the GO. The negative
examples used are the extracted ?nonchemical word
tokens?, the non-ASE tokens from the GO and to-
kens taken from the English dictionary?except for
those that were listed as positive examples. This gets
around the problem that the English dictionary con-
tains the names of all of the elements and a number
of simple compounds such as ?ethanol?.
During operation, n-gram analysis is used to cal-
culate a score for each word token, of the form:
ln(P (token|chem)) ? ln(P (token|nonchem))
If this score is above zero, the preclassifier clas-
sifies the token as chemical and gives it a tentative
type, based on its suffix. This can be considered to
be a ?first draft? of its named entity type. For exam-
ple tokens ending in ?-ation? are given the type RN,
whereas those ending in ?-ene? are given type CM.
3.5 The MEMM
The MEMM is a first-order MEMM, in that it has a
separate maximum-entropy model for each possible
preceeding tag. No information about the tag se-
quence was included directly in the feature set. We
use the OpenNLP MaxEnt classifier3 for maximum-
entropy classification.
The feature set for the MEMM is divided into
three types of features; type 1 (which apply to the
token itself), type 2 (which can apply to the token it-
self, the previous token and the next token) and type
3 (which can act as type 2 features, and which can
also form bigrams with other type 3 features).
An example type 1 feature would be 4G=ceti,
indicating that the 4-gram ceti had been found
in the token. An example type 2 feature would be
3http://maxent.sourceforge.net/
c-1:w=in, indicating that the previous token was
?in?. An example bigram constructed from type 3
features would be bg:0:1:ct=CJ w=acid, in-
dicating that the preclassifier had classified the token
as being of type CJ, and having a score above zero,
and that the next token was ?acid?.
Type 1 features include 1, 2, 3 and 4-grams of
characters found within the token, whether the to-
ken appeared in any of the word lists, and features to
represent the probability and type given by the pre-
classifier for that token. Type 2 features include the
token itself with any terminal letter ?s? removed, the
token converted to lowercase (if it matched the regex
.*[a-z][a-z].*), and a three-character suffix
taken from the token. The token itself was usually
used as a type 2 feature, unless it unless it was short
(less than four characters), or had been found to be
an ambiguous token during preclassifier training, in
which case it was type 3. Other type 3 features in-
clude a word shape feature, and tentative type of the
token if the preclassifier had classed it as chemical.
A few other features were used to cover a few spe-
cial cases, and were found to yield a slight improve-
ment during development.
After generating the features, a feature selection
based on log-likelihood ratios is used to remove the
least informative features, with a threshold set to re-
move about half of them. This was found during
development to have only a very small beneficial ef-
fect on the performance of the classifier, but it did
make training faster and produced smaller models.
This largely removed rare features which were only
found on a few non-chemical tokens.
3.6 The rescorer
The rescoring system works by constructing four
maximum entropy classifiers, one for each entity
type. The output of these classifiers is a probabil-
ity of whether or not a potential named entity really
is a correct named entity. The generation of features
is done on a per-document basis.
The key features in the rescorer represent the
probability of the potential entity as estimated by
the MEMM. The raw probability p is converted to
the logit score
l = ln(p) ? ln(1 ? p)
This mirrors the way probabilities are represented
58
within maximum entropy (aka logistic regression)
classifiers. If l is positive, int(min(15.0, l) ? 50)
instances 4 of the feature conf+ are generated, and
a corresponding technique is used if l is negative.
Before generating further features, it is necessary
to find entities that are ?blocked??entities that over-
lap with other entities of higher confidence. For ex-
ample, consider ?ethyl acetate?, which might give
rise to the named entity ?ethyl acetate? with 98%
confidence, and also ?ethyl? with 1% confidence and
?acetate? with 1% confidence. In this case, ?ethyl?
and ?acetate? would be blocked by ?ethyl acetate?.
Further features are generated by collecting to-
gether all of the unblocked5 potential entities of a
type that share the same string, calculating the max-
imum and average probability, and calculating the
difference between the p and those quantities.
Some acronym and abbreviation handling is also
performed. The system looks for named entities that
are surrounded by brackets. For each of these, a list
of features is generated that is then given to every
other entity of the same string. If there is a potential
entity to the left of the bracketed potential abbre-
viation, then features are generated to represent the
probability of that potential entity, and how well the
string form of that entity matches the potential ab-
breviation. If no potential entity is found to match
with, then features are generated to represent how
well the potential abbreviation matches the tokens
to the left of it. By this method, the rescorer can
gather information about whether a potential abbre-
viation stands for a named entity, something other
than a named entity?or whether it is not an abbre-
viation at all, and use that information to help score
all occurrences of that abbreviation in the document.
4 Evaluation
The systems were evaluated by 3-fold cross-
validation methodology, whereby the data was split
into three equal folds (in the case of the chemistry
4We found that 15.0 was a good threshold by experimenta-
tion on development data: papers annotated during trial runs of
the annotation process.
5Doing this without regards for blocking causes problems.
In a document containing both ?ethyl acetate? and ?ethyl
group?, it would be detrimental to allow the low confidence
for the ?ethyl? in ?ethyl acetate? to lower the confidence of the
?ethyl? in ?ethyl group?.
papers, each fold consists of one paper per journal.
For the PubMed abstracts, each fold consists of one
third of the total abstracts). For each fold, the system
was trained on the other two folds and then evaluated
on that fold, and the results were pooled.
The direct output from the system is a list of
putative named entities with start positions, end
positions, types and confidence scores. This list
was sorted in order of confidence?most confident
first?and each entity was classified as a true posi-
tive or a false positive according to whether an ex-
act match (start position, end position and type all
matched perfectly) could be found in the annotated
corpus. Also, the number of entities in the annotated
corpus was recorded.
Precision/recall curves were plotted from these
lists by selecting the first n elements, and calculat-
ing precision and recall taking all of the elements in
this sublist as true or false positives, and all the enti-
ties in the corpus that were not in the sublist as false
negatives. The value of n was gradually increased,
recording the scores at each point. The area under
the curve (treating precision as zero at recall values
higher than the highest reported) was used to calcu-
late mean average precision (MAP). Finally, F were
generated by selecting all of the entities with a con-
fidence score of 0.3 or higher.
0.0 0.2 0.4 0.6 0.8 1.00
.0
0.
2
0.
4
0.
6
0.
8
1.
0
Recall
Pr
ec
is
io
n
Full System
No Rescorer
No Preclassifier
No n?Grams
Customised LingPipe HMM
Pure LingPipe HMM
Figure 1: Evaluation on chemistry papers.
The results of this evaluation on the corpus of
59
chemistry papers is show in Figure 1. The full sys-
tem achieves 57.6% recall at 95% precision, 58.9%
precision at 90% recall, and 78.7% precision and
82.9% recall (F = 80.7%) at a confidence threshold
of 0.3. Also shown are the results of successively
eliminating parts of the system. ?No Rescorer? re-
moves the rescorer. In ?No Preclassifier?, the pre-
classifier is disabled, and all of the dictionaries ex-
tracted during the training of the preclassifier are
also disabled. Finally, in ?No n-Grams?, the 1-, 2-
, 3- and 4-grams used directly by the MEMM are
also disabled, showing the results of using a sys-
tem where no character-level n-grams are used at all.
These modifications apply successively?for exam-
ple, in the ?No n-Grams? case the rescorer and pre-
classifier are also disabled. These results validate the
the cascade of classifiers, and underline the impor-
tance of character-level n-grams in chemical NER.
We also show comparisons to an HMM-based
approach, based on LingPipe 3.4.0.6 This is es-
sentially the same system as described by Corbett
et al (2007), but operating in a confidence-based
mode. The HMMs used make use of character-level
n-Grams, but do not allow the use of the rich fea-
ture set used by the MEMM. The line ?Customised
LingPipe HMM? shows the system using the cus-
tom tokenisation and ChEBI-derived dictionary used
in the MEMM system, whereas the ?Pure LingPipe
HMM? shows the system used with the default to-
keniser and no external dictionaries. In the region
where precision is roughly equal to recall (mimick-
ing the operation of a first-best system), the fact that
the MEMM-based system outperforms an HMM is
no surprise. However, it is gratifying that a clear
advantage can be seen throughout the whole recall
range studied (0-97%), indicating that the training
processes for the MEMM are not excessively at-
tuned to the first-best decision boundary. This in-
creased accuracy comes at a price in the speed of
development, training and execution.
It is notable that we were not able to achieve ex-
tremes of recall at tolerable levels of precision us-
ing any of the systems, whereas it was possible for
LingPipe to achieve 99.99% recall at 7% precision in
the BioCreative 2006 evaluation. There are a num-
ber of potential reasons for this. The first is that the
6http://alias-i.com/lingpipe/
tokeniser used in all systems apart from the ?Pure
LingPipe HMM? system tries in general to make
as few token boundaries as possible; this leads to
some cases where the boundaries of the entities to
be recognised in the test paper occur in the middle
of tokens, thus making those entities unrecognisable
whatever the threshold. However this does not ap-
pear to be the whole problem. Other factors that may
have had an influence include the more generous
method of evaluation at BioCreative 2006, (where
several allowable alternatives were given for diffi-
cult named entities), and the greater quantity and di-
versity (sentences selected from a large number of
different texts, rather than a relatively small number
of whole full papers) of training data. Finally, there
might be some important difference between chem-
ical names and gene names.
0.0 0.2 0.4 0.6 0.8 1.00
.0
0.
2
0.
4
0.
6
0.
8
1.
0
Recall
Pr
ec
is
io
n
Full System
No Rescorer
No Preclassifier
No n?Grams
Customised LingPipe HMM
Pure LingPipe HMM
Figure 2: Evaluation on PubMed abstracts.
Figure 2 shows the results of running the sys-
tem on the set of annotated PubMed abstracts. The
full system achieves 60.3% recall at 95% precision,
49.1% precision at 90% recall, and 85.0% preci-
sion and 81.6% recall (F = 83.2%) at a confidence
threshold of 0.3. In PubMed abstracts, it is common
to define ad-hoc abbreviations for chemicals within
an abstract (e.g., the abstract might say ?dexametha-
sone (DEX)?, and then use ?DEX? and not ?dexam-
ethasone? throughout the rest of the abstract). The
rescorer provides a good place to resolve these ab-
60
breviations, and thus has a much larger effect than
in the case of chemistry papers where these ad hoc
abbreviations are less common. It is also notable
that the maximum recall is lower in this case. One
system?the ?Pure LingPipe HMM?, which uses a
different, more aggressive tokeniser from the other
systems?has a clear advantage in terms of maxi-
mum recall, showing that overcautious tokenisation
limits the recall of the other systems.
In some cases the systems studied behave
strangely, having ?spikes? of lowered precision at
very low recall, indicating that the systems can occa-
sionally be overconfident, and assign very high con-
fidence scores to incorrect named entities.
Corpus System MAP F
Chemistry Full 87.1% 80.8%
Chemistry No Rescorer 86.8% 81.0%
Chemistry No Preclassifier 82.7% 74.8%
Chemistry No n-Grams 79.2% 72.2%
Chemistry Custom LingPipe 75.9% 74.6%
Chemistry Pure LingPipe 66.9% 63.2%
Chemistry No Overlaps 82.9% 80.8%
PubMed Full 86.1% 83.2%
PubMed No Rescorer 83.3% 79.1%
PubMed No Preclassifier 81.4% 73.4%
PubMed No n-Grams 77.6% 70.6%
PubMed Custom LingPipe 78.6% 75.6%
PubMed Pure LingPipe 71.9% 66.1%
Table 2: F scores (at confidence threshold of 0.3) and
Mean Average Precision (MAP) values for Figs. 1-3.
Neither corpus contains enough data for the re-
sults to reach a plateau?using additional training
data is likely to give improvements in performance.
The ?No Overlaps? line in Figure 3 shows the ef-
fect of removing ?blocked? named entities (as de-
fined in section 3.6) prior to rescoring. This sim-
ulates a situation where an unambiguous inline an-
notation is required?for example a situation where
a paper is displayed with the named entities being
highlighted. This condition makes little difference
at low to medium recall, but it sets an effective max-
imum recall of 90%. The remaining 10% of cases
presumably consist of situations where the recog-
niser is finding an entity in the right part of the text,
but making boundary or type errors.
0.0 0.2 0.4 0.6 0.8 1.00
.0
0.
2
0.
4
0.
6
0.
8
1.
0
Recall
Pr
ec
is
io
n
Full System
No Overlaps
Figure 3: Evaluation on chemistry papers, showing ef-
fects of disallowing overlapping entities.
5 Conclusion
We have demonstrated that MEMMs can be adapted
to recognise chemical named entities, and that the
balance between precision and recall can be tuned
effectively, at least in the range of 0 - 95% recall.
The MEMM system is available as part of the OS-
CAR3 chemical named entity recognition system. 7
Acknowledgements
PTC thanks Peter Murray-Rust for supervision. We
thank the Royal Society of Chemistry for provid-
ing the papers, and the EPSRC (EP/C010035/1) for
funding. We thank the reviewers for their helpful
suggestions and regret that we did not have the time
or space to address all of the issues raised.
References
Colin Batchelor and Peter Corbett. 2007. Semantic en-
richment of journal articles using chemical named en-
tity recognition Proceedings of the ACL 2007 Demo
and Poster Sessions, pp 45-48. Prague, Czech Repub-
lic.
Bob Carpenter. 2007. LingPipe for 99.99% Recall of
Gene Mentions Proceedings of the Second BioCre-
ative Challenge Evaluation Workshop, 307-309.
7https://sourceforge.net/projects/oscar3-chem
61
Ann Copestake, Peter Corbett, Peter Murray-Rust, C. J.
Rupp, Advaith Siddharthan, Simone Teufel and Ben
Waldron. 2006. An Architecture for Language Tech-
nology for Processing Scientific Texts. Proceedings of
the 4th UK E-Science All Hands Meeting, Nottingham,
UK.
Peter Corbett, Colin Batchelor and Simone Teufel. 2007.
Annotation of Chemical Named Entities BioNLP
2007: Biological, translational, and clinical language
processing, pp 57-64. Prague, Czech Republic.
Aron Culotta and Andrew McCallum 2004. Confidence
Estimation for Information Extraction Proceedings of
Human Language Technology Conference and North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pp 109-112. Boston,
MA.
Kirill Degtyarenko, Paula de Matos, Marcus Ennis, Janna
Hastings, Martin Zbinden, AlanMcNaught, Rafael Al-
cantara, Michael Darsow, Mickael Guedj and Michael
Ashburner. 2008. ChEBI: a database and ontology for
chemical entities of biological interest. Nucleic Acids
Res, Vol. 36, Database issue D344-D350.
The Gene Ontology Consortium 2000. Gene Ontology:
tool for the unification of biology. Nature Genetics,
Vol. 25, 26-29.
Martin Krallinger and Lynette Hirschman, editors. 2007.
Proceedings of the Second BioCreative Challenge
Evaluation Workshop.
Vijay Krishnan and Christopher D. Manning. 2006. An
Effective Two-Stage Model for Exploiting Non-Local
Dependencies in Named Entity Recognition. Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, 1121-1128.
Sindey, Australia.
John Lafferty, Andrew McCallum and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. Pro-
ceedings of the Eighteenth International Conference
on Machine Learning, 282-289.
Andrew McCallum, Dayne Freitag and Fernando Pereira.
2000. Maximum Entropy Markov Models for Infor-
mation Extraction and Segmentation Proceedings of
the Seventeenth International Conference on Machine
Learning, 591-598. San Fransisco, CA.
Joe A. Townsend, Ann Copestake, Peter Murray-Rust, Si-
mone H. Teufel and Christopher A. Waudby. 2005.
Language Technology for Processing Chemistry Pub-
lications Proceedings of the fourth UK e-Science All
Hands Meeting, 247-253. Nottingham, UK.
Alexander Vasserman 2004 Identifying Chemical
Names in Biomedial Text: An Investigation of the
Substring Co-occurence Based Approaches Proceed-
ings of the Student Research Workshop at HLT-NAACL
W. John Wilbur, George F. Hazard, Jr., Guy Divita,
James G. Mork, Alan R. Aronson and Allen C.
Browne. 1999 Analysis of Biomedical Text for Chem-
ical Names: A Comparison of Three Methods Proc.
AMIA Symp. 176-180.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for Biomedical Named-Entity Recognition BioNLP
2007: Biological, translational, and clinical language
processing, pp 57-64. Prague, Czech Republic.
62
Proceedings of the 12th European Workshop on Natural Language Generation, pages 130?137,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Investigating Content Selection for Language Generation using Machine
Learning
Colin Kelly
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge, UK
Ann Copestake
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge, UK
{colin.kelly,ann.copestake,nikiforos.karamanis}@cl.cam.ac.uk
Nikiforos Karamanis
Department of Computer Science
Trinity College Dublin
Dublin 2
Ireland
Abstract
The content selection component of a nat-
ural language generation system decides
which information should be communi-
cated in its output. We use informa-
tion from reports on the game of cricket.
We first describe a simple factoid-to-text
alignment algorithm then treat content se-
lection as a collective classification prob-
lem and demonstrate that simple ?group-
ing? of statistics at various levels of granu-
larity yields substantially improved results
over a probabilistic baseline. We addi-
tionally show that holding back of specific
types of input data, and linking database
structures with commonality further in-
crease performance.
1 Introduction
Content selection is the task executed by a natu-
ral language generation (NLG) system of decid-
ing, given a knowledge-base, which subset of the
information available should be conveyed in the
generated document (Reiter and Dale, 2000).
Consider the task of generating a cricket match
report, given the scorecard for that match. Such
a scorecard would typically contain a large num-
ber of statistics pertaining to the game as a whole
as well as individual players (e.g. see Figure 1).
Our aim is to identify which statistics should be
selected by the NLG system.
Much work has been done in the field of con-
tent selection, in a diverse range of domains e.g.
weather forecasts (Coch, 1998). Approaches are
usually domain specific and predominantly based
on structured tables of well-defined input data.
Duboue and McKeown (2003) attempted a sta-
tistical approach to content selection using a sub-
stantial corpus of biographical summaries paired
with selected content, where they extracted rules
and patterns linking the two. They then used ma-
chine learning to ascertain what was relevant.
Barzilay and Lapata (2005) extended this ap-
proach but applying it to a sports domain (Amer-
ican football), similarly viewing content selection
as a classification task and additionally taking ac-
count of contextual dependencies between data,
and found that this improved results compared to
a content-agnostic baseline. We aim throughout
to extend and improve upon Barzilay and Lapata?s
methods.
We emphasise that content selection through
statistical machine learning is a relatively new area
? approaches prior to Duboue and McKeown?s are,
in principle, much less portable ? and as such there
is not an enormous body of work to build upon.
This work offers a novel algorithm for data-to-
text alignment, presents a new ?grouping? method
for sharing knowledge across similar but distinct
learning instances and shows that holding back
certain data from the machine learner, and rein-
troducing it later on can improve results.
2 Data Acquisition & Alignment
We first must obtain appropriately aligned cricket
data, for the purposes of machine learning.
Our data comes from the online Wisden al-
manack (Cricinfo, 2007), which we used to down-
load 133 match report/scorecard pairs. We em-
ployed an HTML parser to extract the main text
from the match report webpage, and the match
data-tables from the scorecard webpage. An ex-
ample scorecard can be found in Figure 11.
1Cricket is a bat-and-ball sport, contested by two oppos-
ing teams of eleven players. Each side?s objective is to score
more ?runs? than their opponents. An ?innings? refers to the
collective performance of the batting team, and (usually) ends
when all eleven players have batted.
In Figure 1, in the batting section R stands for ?runs made?,
M for ?minutes played on the field?, B for ?number of balls
faced?. 4s and 6s are set numbers of runs awarded for hit-
ting balls that reach the boundary. SR is the number of runs
per 100 balls. In the bowling section, O stands for ?overs
130
Result India won by 63 runs
India innings (50 overs maximum) R M B 4s 6s SR
SC Ganguly? run out (Silva/Sangakarra?) 9 37 19 2 0 47.36
V Sehwag run out (Fernando) 39 61 40 6 0 97.50
D Mongia b Samaraweera 48 91 63 6 0 76.19
SR Tendulkar c Chandana b Vaas 113 141 102 12 1 110.78
. . .
Extras (lb 6, w 12, nb 7) 25
Total (all out; 50 overs; 223 mins) 304
Fall of wickets 1-32 (Ganguly, 6.5 ov), 2-73 (Sehwag, 11.2 ov), 3-172 (Mongia,
27.4 ov), 4-199 (Dravid, 32.1 ov), . . . , 10-304 (Nehra, 49.6 ov)
Bowling O M R W Econ
WPUJC Vaas 10 1 64 1 6.40 (2w)
DNT Zoysa 10 0 66 1 6.60 (6nb, 2w)
. . .
TT Samaraweera 8 0 39 2 4.87 (2w)
Figure 1: Statistics in a typical cricket scorecard.
2.1 Report Alignment
We use a supervised method to train our data, and
thus need to find all ?links? between the scorecard
and match report. We execute this alignment by
first creating tags with tag attributes according to
the common structure of the scorecards, and tag
values according to the data within a particular
scorecard. We then attempt to automatically align
the values of those tags with factoids, single pieces
of information found in the report.
For example, from Figure 1 the fact that Ten-
dulkar was the fourth player to bat on the first team
is captured by constructing a tag with tag attribute
team1 player4, and tag value ?SR Tendulkar?. The
fact he achieved 113 runs is encapsulated by an-
other tag, with tag attribute as team1 player4 R
and tag value as ?113?. Then if the report con-
tained the phrase ?Tendulkar made 113 off 102
balls? we would hope to match the ?Tendulkar?
factoid with our tag value ?SR Tendulkar?, the
?113? factoid with our tag value ?113? and replace
both factoids with their respective tag attributes, in
this case team1 player4 and team1 player4 R re-
spectively. Similar methods for this problem have
been employed by Barzilay and Lapata (2005) and
Duboue and McKeown (2003).
The basic idea behind our 6-step process for
alignment is that we align those factoids we are
bowled?, M for ?maiden overs?, R for ?runs conceded? and W
for ?wickets taken?. Econ is ?economy rate?, or number of
runs per over.
It is important to note that Figure 1 omits the opposing
team?s innings (comprising new instances of the ?Batting?,
?Fall of Wickets? and ?Bowling? sections), and some addi-
tional statistics found at the bottom of the scorecard.
most certain of first. The main obstacle we face
when aligning is the large incidence of repeated
numbers occurring within the scorecard, as this
would imply we have multiple, different tags all
with the same tag values. It is wholly possible
(and quite typical) that single figures will be re-
peated many times within a single scorecard2.
Therefore it would be advantageous for us to
have some means to differentiate amongst tags,
and hopefully select the correct tag when encoun-
tering a factoid which corresponds to repeated tag
values. Our algorithm is as follows:
Preprocessing We began by converting all ver-
balised numbers to their cardinal equivalents, e.g.
?one?, ?two? to ?1?, ?2?, and selected instances of
?a? into ?1?.
Proper Nouns In the first round of tagging we
attempt to match proper names from the scorecard
with strings within the report. Additionally, we
maintain a list of all players referenced thus far.
Player-Relevant Details Using the list of play-
ers we have accumulated, we search the report for
matches on tag values relating to only those play-
ers. This step was based on the assumption that a
factoid about a specific player is unlikely to appear
unless that player has been named.
Non-Player-Relevant Details The next stage
involves attempting to match factoids to tag values
whose attributes don?t refer to a particular player
e.g., more general match information as well as
team statistics.
2For example in Figure 1 we can see the number 6 appear-
ing four times: twice as the number of 4s for two different
players, once as an lb statistic and once as an nb statistic.
131
Anchor-Based Matching We next use sur-
rounding text anchor-based matching: for exam-
ple, if a sentence contains the string ?he bowled
for 3 overs? we will preferentially attempt to match
the factoid ?3? with tag values from tags which we
know refer to overs.
Remaining Matches The final step acts as our
?catch-all? ? we proceed through all remaining
words in the report and try to match each poten-
tial factoid with the first (if any) tag found whose
tag value is the same.
2.2 Evaluation
The output of our program is the original text with
all aligned figures and strings (factoids) replaced
with their corresponding tag attributes. We can see
an extract from an aligned report in Figure 2 where
we show the aligned factoids in bold, and their cor-
responding tag attributes in italics. We also note at
this point that much of commentary shown does
not in fact appear in the scorecard, and therefore
additional knowledge sources would typically be
required to generate a full match report ? this is
beyond the scope of our paper, but Robin (1995)
attempts to deal with this problem in the domain
of basketball using revision-based techniques for
including additional content.
We asked a domain expert to evaluate five of
our aligned match reports ? he did this by creat-
ing his own ?gold standard? for each report, a list
of aligned tags. Compared to our automatically
aligned tags, we obtained 79.0% average preci-
sion, 75.8% average recall and a mean F of 77.0%.
3 Categorization
We are using the methods of Barzilay and Lapata
(henceforth B&L) as our starting point, so we de-
scribe what we did to emulate and extend them.
3.1 Barzilay and Lapata?s Method
B&L?s corpus was composed of a relational
database of football statistics. Within the database
were multiple tables, which we will refer to as
?categories? (actions within a game, e.g. touch-
downs and fumbles). Each category was com-
posed of ?groups? (the rows within a category ta-
ble), with each row referring to a distinct player,
and each column referring to different types of ac-
tion within that category (?attributes?).
B&L?s technique for the purposes of the ma-
chine learning was to assign a ?1? or ?0? to each
NatWest Series (series), match 9 (team1 player1 R)
India v Sri Lanka (matchtitle)
At Bristol (venue town), July 11 (date) (day/night
(daynight)).
India (team1) won by 63 runs (winmethod).
India (team1) 5 (team1 points) pts.
Toss: India (team1).
The highlight of a meaningless match was a sublime in-
nings from Tendulkar (team1 player4), who resumed
his fleeting love affair with Nevil Road to the delight
of a flag-waving crowd. On India (team1)?s only other
visit to Bristol (venue town), for a World Cup game
in 1999 against Kenya, Tendulkar (team1 player4)
had creamed an unbeaten 140, and this time he drove
with elan to make 113 (team1 player4 R) off just 102
(team1 player4 B) balls with 12 (team1 player4 4s)
fours and a (team1 player4 6s) six.
. . .
Figure 2: Aligned match report extract
row, where a row would receive the value ?1? if
one or more of the entries in the row was ver-
balised in the report. In the context of our data
we could apply a similar division, for example, by
constructing a category entitled ?Batting? with at-
tributes (columns) ?Runs?, ?Balls?, ?Minutes?, ?4s?
and ?6s? etc., and rows corresponding to players.
In this case a group within that category would
correspond to one line of the ?Innings? table in Fig-
ure 1.
We note that B&L were selecting content on a
row basis, while we are aiming to select individual
tag attributes (i.e., specific row/column cell refer-
ences) within the categories, a more difficult task.
We discuss this further in Section 6.
The technique above allows the machine learn-
ing algorithm to be aware that different statistics
are semantically related ? i.e., each group within a
category contains the same ?type? of information.
We therefore think this is a logical starting point
for our work, and we aim to expand upon it.
3.2 Classifying Tags
The key step was deciding upon an appropriate
division of our scorecard into various categories
and the groups for each category in the style of
B&L. As can be seen from Figure 1 our input in-
formation is a mixture of structured (e.g. Bowling,
Batting sections), semi-structured (Fall of Wickets
section) and almost unstructured (Result) informa-
tion. This is somewhat unlike B&L?s data, which
was fully structured in database form. We deal
132
Category Attributes Verb
Batting 9 47.0
Bowling 11 10.2
Fall of Wickets 8 46.4
Match Details 11 75.2
Match Result 8 45.1
Officials 8 6.0
Partnerships 11 75.5
Team Statistics 13 46.2
Table 1: Number of attributes per category with
percent verbalised (Verb)
with this by enforcing a stronger structure ? di-
viding the information into eight of our own ?cat-
egories?, based roughly on the formatting of the
webpages. These are outlined in Table 1.
The first three categories in the table are quite
intuitive and implicit from the respective sections
of the scorecard. There is additional information
in a typical scorecard (not shown in Figure 1),
which we must also categorise. The ?Team Statis-
tics? category contains details about the ?extras?3
scored by each team, as well as the number of
points gained by the team towards that particular
series4. We divide the remaining tag attributes as
follows into three categories: ?Officials? ? persons
participating in the match, other than the teams
(e.g. umpires, referees); ?Match Details? ? infor-
mation that would have been known before the
match was played (e.g. venue, date, season); and
?Match Result? ? data that could only be known
once the match was over (e.g. final result, player
of the match).
Finally we have an additional ?Partnerships?5
category which is given explicitly on a separate
webpage referenced from each scorecard, but is
also implicit from information contained in the
?Fall of Wickets? and ?Batting? sections. We an-
ticipate that this category will help us manage the
issue of data sparsity. For instance, in our domain
we could group partnerships (which could con-
tain a multitude of player combinations and there-
3Additional runs awarded to the batting team for specific
actions executed by the bowling team. There are four types:
No Ball, Wide, Bye, Leg Bye.
4Each cricket game is part of a specific ?series? of games.
e.g. India would receive five points for their win within the
NatWest series.
5A ?partnership? refers to a pair of players who bat to-
gether, and usually comprises information such as the num-
ber of runs scored between them, the number of deliveries
faced and so on.
fore distinct tags) with the various possible binary
combinations of players together for shared learn-
ing. We discuss this further in Section 8.3.
Within 5 of the categories described above, we
are further able to divide the data into ?groups? -
the Batting, Bowling, Fall of Wickets and Partner-
ships categories refer to multiple players and thus
have multiple rows. The Team Statistics category
contains two groups, one for each team. The other
categories merely form one-line tables.
4 Machine Learning
Our task is to establish which tag attributes (and
hence tag values) should be included in the final
match report, and is a multi-label classification
problem. We chose to use BoosTexter (Schapire
and Singer, 2000) as it has been shown to be an
effective classifier (Yang, 1999), and it is one of
the few text classification tools which directly sup-
ports multi-label classification. This is also what
B&L used.
Schapire and Singer?s BoosTexter (2000) uses
?decision stumps?, or single level decision trees
to classify its input data. The predicates of these
stumps are defined, for text, by the presence or
absence of a single term, and, for numerical at-
tributes, whether the attribute exceeds a given
threshold, decided dynamically.
4.1 Running BoosTexter
BoosTexter requires two input files to train a hy-
pothesis, ?Names? and ?Data?.
Names The Names file contains, for each pos-
sible tag attribute, t, across all scorecards, the type
of its corresponding tag value. These are contin-
uous for numbers and text for normal text. From
our 133 scorecards we extracted a total of 61,063
tag values, of which 82.2% were continuous, the
remainder being text.
Data The Data file contains, for each scorecard,
a comma-delimited list of all tag values for a par-
ticular scorecard, with a ??? for unknown values,
followed by a list of the verbalised tag attributes.
Testing We can now run BoosTexter with a
user-defined number of rounds, T , which creates
a hypothesis file. Using this hypothesis file and
a test ?data? file (without the list of verbalised tag
attributes), BoosTexter will give its hypothesized
predictions, a value f for each tag attribute t. The
sign of f determines whether the classifier be-
lieves the tag value corresponding to t is relevant
133
to the test scorecard, while |f | is a measure of the
confidence the classifier has in its assertion.
4.2 Data Sparsity
The very nature of the data means that there are
a large number of tag values which do not occur
in every scorecard ? the average scorecard con-
tained 24 values, yet our ?names? file contained
1193 possible tag attributes. A lot of this was due
to partnership tag attributes which formed 43.6%
of the ?names? entries. This large figure is because
a large number of all possible binary combinations
of players existed in the training data across both
teams6. This implies we will be unable to train for
a significant number of tag attributes as many spe-
cific tag values occur very rarely. Indeed we found
that of 158,669 entries, 97,666 (61.55%) were ?un-
known?.
5 Evaluation Baselines
It is not clear what constitutes a suitable baseline
so we considered multiple options. The issue of
ambiguous reference baselines is not specific to
the cricket domain, as there is no standardized
baseline approach across the prior literature. We
employ ten-fold cross validation throughout.
5.1 Majority Baseline
B&L created a ?majority baseline? whereby they
returned those categories (i.e., tables) which were
verbalised more than half of the time in their
aligned reports.
As explained in Section 3.2 we divided our tag
attributes into 8 categories. We emulated B&L?s
baseline method as follows: For each category, if
any of the tag values within a particular ?group?
was tagged as verbalised, we counted that as a
?vote? for that particular category. We then cal-
culated the total number of ?votes? divided by the
total number of ?groups? within each category. All
categories which had a ratio of 50% or greater
in this calculation were considered to be ?major-
ity categories?. Our baseline Bmaj then consisted
of all tag attributes forming part of those majority
categories. As shown in Table 1 there were only
two categories which exceeded the 50% threshold,
?Match Details? and ?Partnerships?.
We can see that this baseline performs
abysmally. The reason for this poor behaviour is
693 of the possible 2
?10
i=1 i = 110 combinations oc-
curred.
Bmaj ? min max ?
Precision 0.0966 0.0333 0.1583 0.0250
Recall 0.4879 0.2727 0.7895 0.0977
F 0.1603 0.0620 0.2568 0.0384
Table 2: Majority Baseline, Bmaj
that since so many tag attributes contribute to the
categories we are including far too many possibil-
ities in our baseline.
5.2 Probabilistic Baseline
This baseline is based on the premise that those
tag attributes which occur with highest frequency
across the training data refer to those tag values
which will often occur in a typical match report.
To create our baseline set of tag attributes Bprob
we extract the a most frequently verbalised tag at-
tributes across all the training data where a is the
average length of the verbalised tag attribute lists
for each report/scorecard pair.
Bprob ? min max ?
Precision 0.5157 0.2174 0.7391 0.1010
Recall 0.5157 0.1389 0.7647 0.0990
F 0.5100 0.1695 0.6939 0.0852
Table 3: Probabilistic Baseline, Bprob
This baseline achieves a mean F score of 51%,
however the tag attributes being returned are very
inconsistent with a typical match report ? they
correspond in the majority to player names but
not one refers to any other tag attributes relevant
to those players. This renders the output mostly
meaningless in terms of our aim to select content
for an NLG system.
5.3 No-Player Probabilistic Baseline
Taking the above into account we create a base-
line which derives its choice of tag attributes from
match statistics only. This baseline is similar to
the Probabilistic Baseline above, with the excep-
tion that when summing the numbers of tag at-
tributes in the sets we do not consider player-name
tag attributes in our counts. Instead, we extract
the a? most frequent tag attributes, where a? is
the average size of the sets excluding player-name
tag attributes. To finally obtain our baseline set
Bnops we merge our a? most frequent tag attributes
134
with any and all corresponding player-name tag at-
tributes7.
Bnops ? min max ?
Precision 0.4923 0.1765 0.6875 0.0922
Recall 0.3529 0.1111 0.5625 0.0842
F 0.4064 0.1538 0.5946 0.0767
Table 4: No-Player Probabilistic Baseline, Bnops
As can be seen from Table 4, this method suffers
an absolute F-score drop of more than 10% from
the previous method. However if we analyse the
output more closely we can see that although the
accuracy has dropped, the returned tag attributes
are more thematically consistent with the training
data. This is our preferred baseline.
6 Evaluation Paradigm
The main difficulty we encountered arose when
we came to assessing the Precision and Recall fig-
ures as we have yet to decide on what level we are
considering the output of our system to be correct.
We see three possibilities for the level:
Category We could simply count the ?votes?
predicted on a per category basis (as described
in sections 3.1 and 5.1), and evaluate categories
based on the number of votes given for each. We
would expect this to generate very good results as
we are effectively overgrouping, once on a group
basis (grouping together all attributes) and once on
a category basis (unifying all groups within a cate-
gory), but the output would be so general and triv-
ial (effectively stating something to the effect that
?a match report should contain information about
batting, bowling and team statistics?) that it would
be of no use in an NLG system.
Groups Here we compare which ?groups? were
verbalised within each category, and which were
predicted to be verbalised (as we did for the Major-
ity Baseline of Section 5.1). Our implicit grouping
means that we do not have to necessarily return the
correct statistic pertaining to a group since each
group acts as a basket for the statistics contained
within it, and is susceptible to ?false positives?.
This method is most similar to B&L?s.
Tags Since we are trying to establish which tag
attributes should be included rather than which
groups are likely to contain verbalised tag at-
tributes, we could say that even the above method
7e.g., if team1 player4 R is in a? then we would also in-
clude team1 player4 in our final set.
is too liberal in its definition of correctness. Thus
we also evaluate our groups on the basis of their
component parts, i.e., if a particular group of tag
attributes is estimated to be verbalised by Boos-
Texter, then we include all attributes from that
group.
7 Initial Results
Our ?categorized? results are derived from present-
ing BoosTexter with each individual category as
described in Section 3.2, then merging the selected
tag attributes together and evaluating based on the
criteria described above. We then show BoosTex-
ter?s performance ?as is?, by running the program
on the full output of our alignment stage with no
categorization/grouping.
7.1 Categorized ? Groups Level
Our ?Categorized Groups? results can be found in
Figure 3 and Table 5. For each of our tests we vary
the value of T (the number of rounds) to see how
it affects our accuracy.
Here we see we have a maximum F score of
0.7039 for T = 25. This is a very high result,
performing far better than all our baselines, how-
ever we feel the ?basketing? mentioned in Section
6 means that the results are not particularly in-
structive ? instead of specific ?interesting? tag at-
tributes, we return a grouped list of tag attributes,
only some of which are likely to be ?interesting?.
Thus we decide to no longer pursue ?grouping?
as a valid evaluation method, and evaluate all our
methods at the ?tag attribute? level.
Best ? ?
Precision 0.7620 0.7473 0.0320
CG Recall 0.6795 0.6680 0.0322
F 0.7039 0.6897 0.0106
Table 5: Categorized Groups with Best value for
T = 25.
7.2 Categorized ? Tags Level
What is notable here is that, for all values of T
which we ran our tests on (ranging from 1 to
3000), we obtained just one set of results for ?Cat-
egorized Tags?, displayed in Table 6.
This behaviour indicates that the boosting is not
helping to improve the results. Rather, it is repeat-
edly producing the same hypotheses, with vary-
ing confidence levels. The low F score is due to
135
 0.4
 0.5
 0.6
 0.7
 0.8
 1  10  100  1000
T
Unassisted Boosting
Categorized Groups
No Players
Enhanced Categorization
Figure 3: All F scores Results
? min max ?
Precision 0.0880 0.0496 0.1933 0.0223
Recall 0.7872 0.5417 1.0000 0.1096
F 0.1575 0.0924 0.3151 0.0361
Table 6: Categorized Tags Results
the very low Precision value. This method is ef-
fectively a direct application of B&L?s method to
our domain, however because of our strict accu-
racy measurement, it does not perform particularly
well. In fact it is even worse thanBmaj, our worst-
performing baseline. We believe this is because
the Majority Baseline is limited in the breadth of
tags returned, whereas this method returns very
large sets of over 200 tag attributes (due to the
many contributing tag attributes of each category)
while the average size of the training sets is 24.
Ideally we want to strike a balance between
the improved granularity of the Categorized Tags
evaluation (without the low accuracy) with the
excellent performance of the Categorized Groups
evaluation (without the too-broad basketing).
7.3 Unassisted Boosting
Our results are in Table 7 (row UB) and Figure 3.
We can see F values are increasing on the whole,
and that we have nearly reached our Probabilis-
tic Baseline. Inspecting the contents of the sets
returned by BoosTexter, we see they are slightly
more in line with a typical training set, but still suf-
fer from an over-emphasis on player names. We
also believe the high number of rounds required
for our best result (T = 2250) is caused by the
sparsity issue described in Section 4.2.
Best ? ?
Precision 0.4965 0.4730 0.0253
UB Recall 0.4961 0.4723 0.0252
F 0.4907 0.4673 0.0249
Precision 0.4128 0.3976 0.0094
NP Recall 0.4759 0.4633 0.0126
F 0.4367 0.4227 0.0091
Precision 0.4440 0.4318 0.0136
EC Recall 0.5127 0.4753 0.0271
F 0.4703 0.4467 0.0194
Table 7: Unassisted Boosting (UB), No Players
(NP) and Enhanced Categorization (EC). Best val-
ues for T = 2250, 250 and 20 respectively.
8 No-Players & Enhanced
Categorization
We now consider alternative, novel methods for
improving our results.
8.1 Player Exclusion
We have thus far ignored coherency in our data
? for example we want to make sure that player
statistics will be accompanied by their correspond-
ing player name.
One problem so far with our approach has been
that we are effectively double-counting the play-
ers. Our methods inspect which player names
should appear at the same time as finding ap-
propriate match statistics, whereas we believe we
should instead be finding relevant statistics in the
first instance, holding back player names, then in-
cluding only those players to whom the statistics
refer. Thus we restate our task in this way.
This is also sensible as in previous incarnations
the learning algorithm had been learning from the
literal strings of the player names. Although a
player could be more likely to be named for vari-
ous reasons, these reasons would not appear in the
scorecard and we feel the strings are best ignored.
Thus we decide to remove all player names
from the machine learning input, reinstating only
relevant ones once BoosTexter has selected its
chosen tag attributes.
8.2 Player Exclusion Results
As can be seen from Table 7 (row NP) and Figure
3, we have a maximum F value of 0.4367 when
T = 250, and have achieved a 3% absolute in-
crease, over ourBnops baseline, a static implemen-
tation of the above ideas.
136
8.3 Enhanced Categorization
Our final method combines the ideas of Section
8.1 above with the benefits of categorization, and
handles data sparsity issues.
The method is identical to that of Section 3.1,
with two important exceptions: The first is that
we reintroduce player names after the learning, as
above. The second is that instead of just a bi-
nary include/don?t-include decision for each tag
attribute, we offer a list of verbalised tag attributes
to the learner, but anonymising them with respect
to the group in which they appear. This enables
the learner to, given any group, predict which tag
attributes should be returned, independent of the
group in question. This means groups with often-
empty tag values are able to leverage the informa-
tion from groups with usually populated tag val-
ues, hence solving our data-sparsity issues. For
example, this will solve the issue, referenced in
Section 4.2 of a lack of training data for particular
player-combination partnerships.
Having held back the group to which the tag at-
tributes belong, we reintroduce them enabling dis-
covery of the original tag attribute. This offers the
benefits of categorization, but with a finer-grained
approach to the returned sets of tag attributes.
8.4 Enhanced Categorization Results
Our results are in Table 7 (row EC) and Figure
3. We achieved our best F score result of 0.4703
for a relatively low value of T = 20, and we can
clearly see that boosting establishes a reasonable
ruleset after a small number of iterations ? we be-
lieve we have resolved the issue of data sparsity.
The fact that this grouping has improved our re-
sults compared to feeding the information in ?flat?
(as in Section 7.3) emphasises that the construc-
tion and make-up of the categories play a key role
in defining performance.
9 Conclusions & Future Work
This paper has presented an exploration of various
methods which could prove useful when select-
ing content given a partially structured database
of statistics and output text to emulate. We be-
gan by acquiring the necessary domain data, in the
form of scorecards and reports, and employed a
six-step process to align scorecard statistics ver-
balised in the reports. We next categorised our
statistics based on the scorecard format. We es-
tablished three baselines ? one ?unthinking? proba-
bilistic baseline, a ?sensible? probabilistic one, and
another using categorization.
We found that unassisted boosting actually per-
formed worse than our comparable probabilistic
baseline, Bprob, but its output was marginally
more in line with the typical training data. We
explored how categorization affected our results,
and showed that by grouping similar sets of tag
attributes together we achieved a 7.4% improve-
ment over the comparable baseline value, Bnops
(Table 4). We further improved this technique in
a novel way by sharing structural information be-
tween learning instances, and by holding back cer-
tain information from the learner. Our final best F-
value marked a relative 15.7% increase on Bnops.
There are multiple avenues still available for ex-
ploration. One possibility would be to further in-
vestigate the effects of categorization from Section
3.2, for example by varying the size and number of
categories. We would also like to apply our meth-
ods to another domain (e.g. rugby games) to es-
tablish the relative generality of our approach.
Acknowledgments
This paper is based on Colin Kelly?s M.Phil. thesis, written
towards his completion of the University of Cambridge Com-
puter Laboratory?s Computer Speech, Text and Internet Tech-
nology course. Grateful thanks go to the EPSRC for funding.
References
Regina Barzilay and Mirella Lapata. 2005. Collective Con-
tent Selection for Concept-To-Text Generation. In HLT
?05, pages 331?338. Association for Computational Lin-
guistics.
Jose Coch. 1998. Multimeteo: multilingual production of
weather forecasts. ELRA Newsletter, 3(2).
Cricinfo. 2007. Wisden Almanack.
http://cricinfo.com/wisdenalmanack. Retrieved 28
April 2007. Registration required.
Pablo A. Duboue and Kathleen R. McKeown. 2003. Statis-
tical Acquisition of Content Selection Rules for Natural
Language Generation. EMNLP ?03, pages 121?128.
Ehud Reiter and Robert Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University Press.
Jacques Robin. 1995. Revision-based generation of natu-
ral language summaries providing historical background:
corpus-based analysis, design, implementation and evalu-
ation. Ph.D. thesis, Columbia University.
Robert E. Schapire and Yoram Singer. 2000. BoosTexter:
A boosting-based system for text categorization. Machine
Learning, 39(2/3):135?168.
Yiming Yang. 1999. An evaluation of statistical approaches
to text categorization. Information Retrieval, 1(1/2):69?
90.
137
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 73?81,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Annotating Underquantification
Aurelie Herbelot
University of Cambridge
Cambridge, United Kingdom
ah433@cam.ac.uk
Ann Copestake
University of Cambridge
Cambridge, United Kingdom
aac10@cam.ac.uk
Abstract
Many noun phrases in text are ambigu-
ously quantified: syntax doesn?t explicitly
tell us whether they refer to a single en-
tity or to several, and what portion of the
set denoted by the Nbar actually takes part
in the event expressed by the verb. We
describe this ambiguity phenomenon in
terms of underspecification, or rather un-
derquantification. We attempt to validate
the underquantification hypothesis by pro-
ducing and testing an annotation scheme
for quantification resolution, the aim of
which is to associate a single quantifier
with each noun phrase in our corpus.
1 Quantification resolution
We are concerned with ambiguously quantified
noun phrases (NPs) and their interpretation, as il-
lustrated by the following examples:
1. Cats are mammals = All cats...
2. Cats have four legs = Most cats...
3. Cats were sleeping by the fire = Some cats...
4. The beans spilt out of the bag = Most/All of
the beans...
5. Water was dripping through the ceiling =
Some water...
We are interested in quantification resolution,
that is, the process of giving an ambiguously quan-
tified NP a formalisation which expresses a unique
set relation appropriate to the semantics of the ut-
terance. For instance, we wish to arrive at:
6. All cats are mammals.
|???| = |?|where ? is the set of all cats and
? the set of all mammals.
Resolving the quantification value of NPs is im-
portant for many NLP tasks. Let us imagine an in-
formation extraction system having retrieved the
triples ?cat ? is ? mammal? and ?cat ? chase ?
mouse? for inclusion in a factual database about
felines. The problem with those representation-
poor triples is that they do not contain the nec-
essary information about quantification to answer
such questions as ?Are all cats mammals?? or ?Do
all cats chase mice?? Or if they attempt to answer
those queries, they give the same answer to both.
Ideally, we would like to annotate such triples with
quantifiers which have a direct mapping to proba-
bility adverbs:
7. All cats are mammals AND Tom is a cat ?
Tom is definitely a mammal.
8. Some cats chase mice AND Tom is a cat ?
Tom possibly chases mice.
Adequate quantification is also necessary for in-
ference based on word-level entailment: an exis-
tentially quantified NP can be replaced by a suit-
able hypernym but this is not possible in non-
existential cases: (Some) cats are in my garden
entails (Some) animals are in my garden but (All)
cats are mammals doesn?t imply that (All) animals
are mammals.
In Herbelot (to appear), we provide a formal
semantics for ambiguously quantified NPs, which
relies on the idea that those NPs exhibit an under-
specified quantifier, i.e. that for each NP in a cor-
pus, a set relation can be agreed upon. Our formal-
isation includes a placeholder for the quantifier?s
set relation. In line with inference requirements,
we assume a three-fold partitioning of the quan-
tificational space, corresponding to the natural lan-
guage quantifiers some, most and all (in addition
to one, for the description of singular, unique enti-
ties). The corresponding set relations are:
9. some(?, ?) is true iff 0 < |? ? ?| < |?? ?|
10. most(?, ?) is true iff |???| ? |???| < |?|
11. all(?, ?) is true iff |? ? ?| = |?|
This paper is an attempt to show that our for-
malisation lends itself to evaluation by human an-
notation. The labels produced will also serve as
training and test sets for an automatic quantifica-
tion resolution system.
73
2 Under(specified) quantification
Before we present our annotation scheme, we will
spell out the essential idea behind what we call un-
derquantification.
The phenomenon of ambiguous quantification
overlaps with genericity (see Krifka et al 1995,
for an introduction to genericity). Generic NPs
are frequently expressed syntactically as bare plu-
rals, although they occur in definite and indefinite
singulars too, as well as bare singulars. There
are many views on the semantics of generics
(e.g. Carlson, 1995; Pelletier and Asher, 1997;
Heyer, 1990; Leslie, 2008) but one of them is that
they quantify (Cohen, 1996), although, puzzlingly
enough, not always with the same quantifier:
12. Frenchmen eat horsemeat = Some/Relatively-
many Frenchmen... (For the relatively many
reading, see Cohen, 2001.)
13. Cars have four wheels = Most cars...
14. Typhoons arise in this part of the Pacific =
Some typhoons... OR Most/All typhoons...
This behaviour has so far prevented linguists
from agreeing on a single formalisation for all
generics. The only accepted assumption is that an
operator GEN exists, which acts as a silent quan-
tifier over the restrictor (subject) and matrix (ver-
bal predicate) of the generic statement. The formal
properties of GEN are however subject to debate:
in particular, it is not clear which natural language
quantifier it would map onto (some view it as most,
but this approach requires some complex domain
restriction to deal with sentences such as 12).
In this paper, we take a different approach
which sidesteps some of the intractable prob-
lems associated with the literature on generics and
which also extends to definite plurals. Instead of
talking of ambiguous quantification, we will talk
of underspecified quantification, or underquan-
tification. By this, we mean that the bare plural,
rather than exhibiting a silent, GEN quantifier,
simply features a placeholder in the logical form
which must be filled with the appropriate quan-
tifier (e.g., uq(x, cat?(x), sleep?(x)), where uq is
the placeholder quantifier). This account caters
for the facts that so-called generics can so easily
be quantified via traditional quantifiers, thatGEN
is silent in all known languages, and it explains
also why it is the bare form which has the high-
est productivity, and can refer to a range of quan-
tified sets, from existentials to universals. Using
the underquantification hypothesis, we can para-
phrase any generic of the form ?X does Y? as ?there
is a set of things X, a certain number of which
do Y? (note the partitive construction). Such a
paraphrase allows us to also resolve ambiguously
quantified definite plurals, which have tradition-
ally been associated with universals, outside of the
genericity phenomenon (e.g. Lyons, 1999).
Because of space constraints, we will not give
our formalisation for underquantification in this
paper (see Herbelot,to appear, for details). It in-
volves a representation of the partitive construct
exemplified above and requires knowledge of the
distributive or collective status of the verbal pred-
icate. We also argue that if generics can always be
quantified, their semantics may involve more than
quantification. So we claim that in certain cases, a
double formalisation of the NP as a quantified en-
tity and a kind is desirable. We understand kinds
in the way proposed by Chierchia (1998), that is
as the plurality of all instances denoted by a given
word in the world under consideration. Under the
kind reading, we can interpret 12 as meaning Col-
lectively, the group of all Frenchmen has the prop-
erty of eating horsemeat.
3 Motivation
3.1 Linguistic motivation
It is usual to talk of ?annotation? generically, to
cover any process that involves humans using a set
of guidelines to mark some specific linguistic phe-
nomenon in some given text. However, we would
argue that, when considering the aims of an anno-
tation task and its relation to the existing linguistic
literature, it becomes possible to distinguish be-
tween various types of annotation. Further, we
will show that our own effort situates itself in a
little studied relation to formal semantics.
The most basic type of annotation is the
one where computational linguists mark large
amounts of textual data with well-known and well-
understood labels. The production of tree banks
like the Penn Treebank (Marcus et al 1993) makes
use of undisputed linguistic categories such as
parts of speech. The aim is to make the computer
learn and use irrefutable bits of linguistics. (Note
that, despite agreement, the representation of those
categories may differ: see for example the range
of available parts of speech tag sets.) This type
of task mostly involves basic syntactic knowledge,
but can be taken to areas of syntax and seman-
74
tics where the studied phenomena have a (some-
what) clear, agreed upon definition (Kingsbury et
al, 2002). We must clarify that in those cases, the
choice of a formalism may already imply a certain
theoretical position ? leading to potential incom-
patibilities between formalisms. However, the cat-
egories for such annotation are themselves fixed:
there is a generally agreed broad understanding of
concepts such as noun phrases and coordination.
Another type of annotation concerns tasks
where the linguistic categories at play are not
fixed. One example is discourse annotation ac-
cording to rhetorical function (Teufel et al 2006)
where humans are asked to differentiate between
several discursive categories such as ?contrast? or
?weakness?. In such a task, the computational lin-
guist develops a theory where different states or
values are associated with various phenomena. In
order to show that the world functions according to
the model presented, experimentation is required.
This usually takes the form of an annotation task
where several human subjects are required to mark
pieces of text following guidelines inferred from
the model. The intuition behind the annotation ef-
fort is that agreement between humans support the
claims of the theory (Teufel, in press). In particu-
lar, it may confirm that the phenomena in question
indeed exist and that the values attributed to them
are clearly defined and distinguishable. The work
is mostly of a descriptive nature ? it creates phe-
nomenological definitions that encompass bits of
observable language.
Our own work is similar to the latter type of
annotation in that it is trying to capture a phe-
nomenon that is still under investigation in the lin-
guistic literature. However, it is also different be-
cause the categories we use are fixed by language:
the quantifiers some, most and all exist and we as-
sume that their definition is agreed upon by speak-
ers of English. What we are trying to investigate
is whether those quantifiers should be used at all
in the context of ambiguous quantification.
The type of annotation carried out in this pa-
per can be said to have more formal aims than the
tasks usually attempted in computational linguis-
tics. In particular, it concerns itself with some of
the broad claims made by formal semantics: its
model-theoretical view and the use of generalised
quantifiers to formalise noun phrases.
In Section 1, we assumed that quantifiers de-
note relations between sets and presented the task
of quantification resolution as choosing the ?cor-
rect? set relation for a particular noun phrase in a
particular sentence ? implying some sort of truth
value at work throughout the process: the correct
set relation produces the sentence with truth value
1 while the other set relations produce a truth value
of 0. What we declined to discuss, though, is the
way that those reference sets were selected in nat-
ural language, i.e. we didn?t make claims about
what model, or models, are used by humans when
they compute the truth value of a given quantified
statement. The annotation task may not answer
this question but it should help us ascertain to what
extent humans share a model of the world.
In Section 2, we also argued that all subject
generic noun phrases could be analysed in terms
of quantification. That is, an (underspecified) gen-
eralised quantifier is at work in sentences that con-
tain such generic NPs. It is expected that if the
annotation is feasible and shows good agreement
between annotators, the quantification hypothesis
would be confirmed. Thus, annotation may allow
us to make semantic claims such as ?genericity
does quantify?. Note that the categories we assume
are intuitive and do not depend on a particular rep-
resentation: it is possible to reuse our annotation
with a different formalism as long as the theoreti-
cal assumption of quantification is agreed upon.
We are not aware of any annotation work in
computational linguistics that contributes to vali-
dating (or invalidating) a particular formal theory.
In that respect, the experiments presented in this
paper are of a slightly different nature than the
standard research on annotation (despite the fact
that, as we will show in the next section, they also
aim at producing data for a language analysis sys-
tem).
3.2 Previous work on genericity annotation
The aim of our work being the production of an au-
tomatic quantification resolution system, we need
an annotated corpus to train and test our machine
learning algorithm. There is no corpus that we
know of which would give us the required data.
The closest contestants are the ACE corpus (2008)
and the GNOME corpus (Poesio, 2000) which
both focus on the phenomenon of genericity, as de-
scribed in the linguistic literature. Unfortunately,
neither of those corpora are suitable for use in a
general quantification task.
The ACE corpus only distinguishes between
75
?generic? and ?specific? entities. The classification
proposed by the authors of the corpus is there-
fore a lot broader than the one we are attempt-
ing here and there is no direct correspondence
between their labels and natural language quanti-
fiers: we have shown in Section 2 that genericity
didn?t map to a particular division of the quantifi-
cational space. Furthermore, the ACE guidelines
contradict to some extent the literature on generic-
ity. They require for instance that a generic men-
tion be quantifiable with all, most or any. This
implies that statements such as Mosquitoes carry
malaria either refer to a kind only (i.e. they are
not quantified) or are not generic at all. Further,
despite the above reference to quantification, the
authors seem to separate genericity and universal
quantification as two antithetical phenomena, as
shown by the following quote: ?Even if the au-
thor may intend to use a GEN reading, if he/she
refers to all members of the set rather than the set
itself, use the SPC tag?.
The GNOME annotation scheme is closer in
essence to the literature on genericity and much
more detailed than the ACE guidelines. However,
the scheme distinguishes only between generic
and non-generic entities, as in the ACE corpus
case, and the corpus itself is limited to three gen-
res: museum labels, pharmaceutical leaflets, and
tutorial dialogues. The guidelines are therefore
tailored to the domains under consideration; for
instance, bare noun phrases are said to be typically
generic. This restricted solution has the advantage
of providing good agreement between annotators
(Poesio, 2004 reports a Kappa value of 0.82 for
this annotation).
4 Annotation corpus
We use as corpus a snapshot of the English ver-
sion of the online encyclopaedia Wikipedia.1 The
choice is motivated by the fact that Wikipedia can
be taken as a fairly balanced corpus: although it is
presented as an encyclopaedia, it contains a wide
variety of text ranging from typical encyclopaedic
descriptions to various types of narrative texts
(historical reconstructions, film ?spoilers?, fiction
summaries) to instructional material like rules of
games. Further, each article in Wikipedia is writ-
ten and edited by many contributors, meaning that
speaker heterogeneity is high. We would also ex-
pect an encyclopaedia to contain relatively many
1http://www.wikipedia.org
generics, allowing us to assess how our quantifi-
cational reading fares in a real annotation task. Fi-
nally, the use of an open resource means that the
corpus can be freely distributed.2
In order to create our annotation corpus, we first
isolated the first 100,000 pages in our snapshot
and parsed them into a Robust Minimal Recur-
sion Semantics (RMRS) representation (Copes-
take, 2004) using first the RASP parser (Briscoe
et al 2006) and the RASP to RMRS converter
(Ritchie, 2004). We then extracted all construc-
tions of the type Subject-Verb-Object from the ob-
tained corpus and randomly selected 300 of those
?triples? to be annotated. Another 50 random
triples were selected for the purpose of annotation
training (see Section 7.1).
We show in Figure 1 an example of an anno-
tation instance produced by the parser pipeline.
The data provided by the system consists of the
triple itself, followed by the argument structure
of that triple, including the direct dependents of
its constituents, the number and tense information
for each constituent, the file from which the triple
was extracted and the original sentence in which
it appeared. The information provided to annota-
tors is directly extracted from that representation.
(Note that the examples were not hand-checked,
and some parsing errors may have remained.)
5 Evaluating the annotation
In an annotation task, two aspects of agreement are
important when trying to prove or refute a partic-
ular linguistic model: stability and reproducibility
(Krippendorf, 1980). Reproducibility refers to the
consistency with which humans apply the scheme
guidelines, i.e. to the so-called inter-annotator
agreement. Stability relates to whether the same
annotator will consistently produce the same an-
notations at different points in time. The measure
for stability is called intra-annotator agreement.
Both measures concern the repeatability of an an-
notation experiment.
In this work, agreement is calculated for each
pair of annotators according to the Kappa mea-
sure. There are different versions of Kappa de-
pending on how multiple annotators are treated
and how the probabilities of classes are calculated
to establish the expected agreement between anno-
tators, Pr(e): we use Fleiss? Kappa (Fleiss, 1971),
which allows us to compute agreement between
2For access, contact the first author.
76
digraph G211 {
"TRIPLE: weed include pigra" [shape=box];
include -> weed [label="ARG1 n"];
include -> pigra [label="ARG2 n"];
invasive -> weed [label="ARG1 n"];
compound_rel -> pigra [label="ARG1 n"];
compound_rel -> mimosa [label="ARG2 n"];
"DNT INFO: lemma::include() tense::present lpos::v (arg::ARG1 var::weed() num::pl pos::)
(arg::ARG2 var::pigra() num::sg pos::)" [shape=box];
"FILE: /anfs/bigtmp/newr1-50/page101655" [shape=box];
"ORIGINAL: Invasive weeds include Mimosa pigra, which covers 80,000 hectares
of the Top End, including vast areas of Kakadu. " [shape=box]; }
Figure 1: Example of annotation instance
multiple annotators.
6 An annotation scheme for
quantification resolution
6.1 Scheme structure
Our complete annotation scheme can be found in
Herbelot (to appear). The scheme consists of five
parts. The first two present the annotation material
and the task itself. Some key definitions are given.
The following part describes the various quantifi-
cation classes to be used in the course of the an-
notation. Participants are then given detailed in-
structions for the labelling of various grammatical
constructs. Finally, in order to keep the demand
on the annotators? cognitive load to a minimum,
the last part reiterates the annotation guidelines in
the form of diagrammatic decision trees.
In the next sections, we give a walk-through of
the guidelines and definitions provided.
6.2 Material
Our annotators are first made familiar with the ma-
terial provided to them. This material consists
of 300 entries comprising a single sentence and
a triple Subject-Verb-Object which helps the an-
notator identify which subject noun phrase in the
sentence they are requested to label (the ?ORIG-
INAL? and ?TRIPLE? lines in the parser output ?
see Figure 1). No other context is provided. This
is partly to make the task shorter (letting us anno-
tate more instances) and partly to allow for some
limited comparison between human and machine
performance (by restricting the amount of infor-
mation given to our annotators, we force them ? to
some extent ? to use the limited information that
would be available to an automatic quantification
resolution system, e.g. syntax).
6.3 Definitions
In our scheme, we introduce the annotators to the
concepts of quantification and kind.3
Quantification is described in simple terms, as
the process of ?paraphrasing the noun phrase in
a particular sentence using an unambiguous term
expressing some quantity?. An example is given.
15. Europeans discovered the Tuggerah Lakes in
1796 = Some Europeans discovered the Tug-
gerah Lakes in 1796.
We only allow the three quantifiers some, most
and all. In order to keep the number of classes
to a manageable size, we introduce the additional
constraint that the process of quantification must
yield a single quantifier. We force the annotator
to choose between the three proposed options and
introduce priorities in cases of doubt: most has pri-
ority over all, some has priority over the other two
quantifiers. This ensures we keep a conservative
attitude with regard to inference (see Section 1).
Kinds are presented as denoting ?the group in-
cluding all entities described by the noun phrase
under consideration?, that is, as a supremum. (As
mentioned in Section 2, the verbal predicate ap-
plies collectively to that supremum in the corre-
sponding formalisation.)
Quantification classes are introduced in a sep-
arate part of the scheme. We define the five la-
bels SOME, MOST, ALL, ONE and QUANT (for al-
ready quantified noun phrases) and give examples
for each one of them.
We try, as much as possible, to keep annotators
away from performing complex reference resolu-
tion. Their first task is therefore to simply attempt
3Distributivity and collectivity are also introduced in the
scheme because they are a necessary part of our proposed for-
malisation. However, as this paper focuses on the annotation
of quantification itself, we will not discuss this side of the
annotation task.
77
to paraphrase the existing sentence by appending
a relevant quantifier to the noun phrase to be anno-
tated. In some cases, however, this is impossible
and no quantifier yields a correct English sentence
(this often happens in collective statements). To
help our annotators make decisions in those cases,
we ask them to distinguish what the noun phrase
might refer to when they first hear it and what it
refers to at the end of the sentence, i.e., when the
verbal predicate has imposed further constraints
on the quantification of the NP.
6.4 Guidelines
Guidelines are provided for five basic phrase
types: quantified noun phrases, proper nouns, plu-
rals, non-bare singulars and bare singulars.
6.4.1 Quantified noun phrases
This is the simplest case: a noun phrase that is
already quantified such as some people, 6 million
inhabitants or most of the workers. The annotator
simply marks the noun phrase with a QUANT label.
6.4.2 Proper nouns
Proper nouns are another simple case. But be-
cause what annotators understand as a proper noun
varies, we provide a definition. We note first that
proper nouns are often capitalised. It should how-
ever be clear that, while capitalised entities such
as Mary, Easter Island or Warner Bros refer to
singular, unique objects, others refer to groups or
instances of those groups: The Chicago Bulls, a
Roman. The latter can be quantified:
16. The Chicago Bulls won last week. (ALL ?
collective)
17. A Roman shows courage in battle. (MOST ?
distributive)
We define proper nouns as noun phrases that
?contain capitalised words and refer to a concept
which doesn?t have instances?. All proper nouns
are annotated as ONE.
6.4.3 Plurals
Plurals must be appropriately quantified and the
annotators must also specify whether they are
kinds or not. This last decision can simply be
made by attempting to paraphrase the sentence
with either a definite singular or an indefinite sin-
gular ? potentially leading to a typical generic
statement.
6.4.4 (Non-bare) singulars
Like plurals, singulars must be tested for a kind
reading. This is done by attempting to pluralise the
noun phrase. If pluralisation is possible, then the
kind interpretation is confirmed and quantification
is performed. If not (certain non-mass terms have
no identifiable parts), the singular refers to a single
entity and is annotated as ONE.
6.4.5 Bare singulars
We regard bare singulars as essentially plural, un-
der the linguistic assumption of non-overlapping
atomic parts ? for instance, water is considered a
collection of H2O molecules, rice is regarded as
a collection of grains of rice, etc (see Chierchia,
1998). In order to make this relation clear, we
ask annotators to try and paraphrase bare singulars
with an (atomic part) plural equivalent and follow,
as normal, the decision tree for plurals:
18. Free software allows users to co-operate in
enhancing and refining the programs they use
? Open source programs allow users...
When the paraphrase is impossible (as in certain
non-mass terms which have no identifiable parts),
the noun phrase is deemed a unique entity and la-
belled ONE.
7 Implementation and results
7.1 Task implementation
Three annotators were used in our experiment.
One annotator was one of the authors; the
other two annotators were graduate students (non-
linguists), both fluent in English. The two grad-
uate students were provided with individual train-
ing sessions where they first read the annotation
guidelines, had the opportunity to ask for clarifi-
cations, and subsequently annotated, with the help
of the author, the 50 noun phrases in the train-
ing set. The actual annotation task was performed
without communication with the scheme author or
the other annotators.
7.2 Kappa evaluation
We made an independence assumption between
quantification value and kind value, and evaluated
agreement separately for each type of annotation.
Intra-annotator agreement was calculated over
the set of annotations produced by one of the au-
thors. The original annotation experiment was re-
produced at three months? interval and Kappa was
78
Class Kind Quantification
Kappa 0.85 0.84
Table 1: Intra-annotator agreements for both tasks
Class Kind Quantification
Kappa 0.67 0.72
Table 2: Inter-annotator agreements for both tasks
computed between the original set and the new set.
Table 1 shows results over 0.8 for both tasks, cor-
responding to ?perfect agreement? according to the
Landis and Koch classification (1977). This indi-
cates that the stability of the scheme is high.
Table 2 shows inter-annotator agreements of
over 0.6 for both tasks, which correspond to ?sub-
stantial agreement?. This result must be taken with
caution, though. Although it shows good agree-
ment overall, it is important to ascertain in what
measure it holds for separate classes. In an ef-
fort to report such per class agreement, we cal-
culate Kappa values for each label by evaluating
each class against all others collapsed together (as
suggested by Krippendorf, 1980).
Table 3 indicates that substantial agreement is
maintained for separate classes in the kind annota-
tion task. Table 4, however, suggests that, if agree-
ment is perfect for the ONE and QUANT classes,
it is very much lower for the SOME, MOST and
ALL classes. While it is clear that the latter three
are the most complex to analyse, we can show
that the lower results attached to them are partly
due to issues related to Kappa as a measure of
agreement. Feinstein and Cicchetti (1990), fol-
lowed by Di Eugenio and Glass (2004) proved
that Kappa is subject to the effect of prevalence
and that different marginal distributions can lead
to very different Kappa values for the same ob-
served agreement. It can be shown, in particu-
lar, that an unbalanced, symmetrical distribution
of the data produces much lower figures than bal-
anced or unbalanced, asymmetrical distributions
because the expected agreement gets inflated. Our
confusion matrices indicate that our data falls into
the category of unbalanced, symmetrical distribu-
tion: the classes are not evenly distributed but an-
notators agree on the relative prevalence of each
class. Moreover, in the quantification task itself,
the ONE class covers roughly 50% of the data.
This means that, when calculating per class agree-
Class KIND NOT-KIND QUANT
Kappa 0.63 0.71 0.88
Table 3: Per class inter-annotator agreement for
the kind annotation
Class ONE SOME MOST ALL QUANT
Kappa 0.81 0.45 0.44 0.51 0.88
Table 4: Per class inter-annotator agreement for
the quantification annotation
ment, we get an approximately balanced distri-
bution for the ONE label and an unbalanced, but
still symmetrical, distribution for the other labels.
This leads to the expected agreement being rather
low for the ONE class and very high for the other
classes. Table 5 reproduces the per class agree-
ment figures obtained for the quantification task
but shows, in addition, the observed and expected
agreements for each label. Although the observed
agreement is consistently close to, or over, 0.9, the
Kappa values differ widely in conjunction with ex-
pected agreement. This results in relatively low re-
sults for SOME, MOST and ALL (the QUANT label
has nearly perfect agreement and therefore doesn?t
suffer from prevalence).
Class Kappa Pr(a) Pr(e)
ONE 0.814 0.911 0.521
SOME 0.445 0.893 0.808
MOST 0.438 0.931 0.877
ALL 0.509 0.867 0.728
QUANT 0.884 0.987 0.885
Table 5: The effect of prevalence on per class
agreement, quantification task. Pr(a) is the ob-
served agreement between annotators, Pr(e) the
expected agreement.
With regard to the purpose of creating a gold
standard for a quantification resolution system, we
also note that out of 300 quantification annota-
tions, there are only 14 cases in which a majority
decision cannot be found, i.e., at least two anno-
tators agreed in 95% of cases. Thus, despite some
low Kappa results, the data can adequately be used
for the production of training material.4
4As far as such data ever can be: Reidsma and Carletta,
2008, show that systematic disagreements between annota-
tors will produce bad machine learning, regardless of the
Kappa obtained on the data.
79
In Section 8, we introduce difficulties encoun-
tered by our subjects, as related in post-annotation
discussions. We focus on quantification.
8 Annotation issues
8.1 Reference
Although we tried to make the task as simple as
possible for the annotators by asking them to para-
phrase the sentences that they were reading, they
were not free from having to work out the refer-
ent of the NP (consciously or unconsciously) and
we have evidence that they did not always pick
the same referent, leading to disagreements at the
quantification stage. Consider the following:
19. Subsequent annexations by Florence in the
area have further diminished the likelihood of
incorporation.
In the course of post-annotation discussions, it
became clear that not all annotators had chosen the
same referent when quantifying the subject NP in
the first clause. One annotator had chosen as refer-
ent subsequent annexations, leading to the reading
Some subsequent annexations, conducted by Flo-
rence in the area, have further diminished the like-
lihood of incorporation. The other two annotators
had kept the whole NP as referent, leading to the
reading All the subsequent annexations conducted
by Florence in the area have further diminished
the likelihood of incorporation.
8.2 World knowledge
Being given only one sentence as context for the
NP to quantify, annotators sometimes lacked the
world knowledge necessary to make an informed
decision. This is illustrated by the following:
20. The undergraduate schools maintain a non-
restrictive Early Action admissions pro-
gramme.
Discussion revealed that all three annotators had
a different interpretation of what the mentioned
Early Action programme might refer to, and of
the duties of the undergraduate schools with re-
gard to it. This led to three different quantifica-
tions: SOME, MOST and ALL.
8.3 Interaction with time
The existence of interactions between NP quantifi-
cation and what we will call temporal quantifica-
tion is not surprising: we refer to the literature on
genericity and in particular to Krifka et al(1995)
who talk of characteristic predication, or habitual-
ity, as a phenomenon encompassed by genericity.
We do not intend to argue for a unified theory of
quantification, as temporal quantification involves
complexities which are beyond the scope of this
work. However, the interactions observed between
temporality and NP quantification might explain
further disagreements in the annotation task. The
following is a sentence that contains a temporal
adverb (sometimes) and that produced some dis-
agreement amongst annotators:
21. Scottish fiddlers emulating 18th-century
playing styles sometimes use a replica of the
type of bow used in that period.
Two annotators labelled the subject of that sen-
tence as MOST, while the third one preferred
SOME. In order to understand the issue, consider
the following, related, statement:
22. Mosquitoes sometimes carry malaria.
This sentence has the possible readings: Some
mosquitoes carry malaria or Mosquitoes, from
time to time in their lives, carry malaria. The first
reading is clearly the preferred one.
The structure of (21) is identical to that of (22)
and it should therefore be taken as similarly am-
biguous: it either means that some of the Scottish
fiddlers emulating 18th-century playing styles use
a replica of the bow used in that period, or that a
Scottish fiddler who emulates 18th-century play-
ing styles, from time to time, uses a replica of such
a bow. The two readings may explain the labels
given to that sentence by the annotators.
9 Conclusion
Taking prevalence effects into account, we believe
that our agreement results can be taken as evidence
that underquantification is analysable in a consis-
tent way by humans. We also consider them as
strong support for our claim that ?genericity quan-
tifies?. Our scheme could however be refined fur-
ther. In a future version, we would add guidelines
regarding the selection of the referent of the noun
phrase, encourage the use of external resources to
obtain the context of a given sentence (or simply
provide the actual context of the sentence), and
give some pointers as to how to resolve issues or
ambiguities caused by temporal quantification.
80
References
ACE. 2008. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Entities, Version 6.6
2008.06.13. Linguistic Data Consortium.
Edward Briscoe, John Carroll and Rebecca Watson.
2006. ?The Second Release of the RASP System?.
In Proceedings of the COLING/ACL 2006 Interac-
tive Presentation Sessions, Sydney, Australia, 2006.
Gregory Carlson. 1995. ?Truth-conditions of Generics
Sentences: Two Contrasting Views?. In Gregory N.
Carlson and Francis Jeffrey Pelletier, Editors, The
Generic Book, pages 224 ? 237. Chicago University
Press.
Gennaro Chierchia. 1998. ?Reference to kinds across
languages?. Natural Language Semantics, 6:339?
405.
Ariel Cohen. 1996. Think Generic: The Meaning
and Use of Generic Sentences. Ph.D. Dissertation.
Carnegie Mellon University at Pittsburgh. Published
by CSLI, Stanford, 1999.
Ann Copestake. 2004. ?Robust Minimal Recursion
Semantics?. www.cl.cam.ac.uk/?aac10/
papers/rmrsdraft.pdf.
Barbara Di Eugenio and Michael Glass. 2004. ?The
kappa statistic: a second look?. Computational Lin-
guistics, 30(1):95?101.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
?High agreement but low kappa: I. The problems of
two paradoxes?. Journal of Clinical Epidemiology,
43(6):543?549.
Joseph Fleiss. 1971. ?Measuring nominal scale agree-
ment among many raters?. Psychological Bulletin,
76(5):378-382.
Aurelie Herbelot. To appear. Underspecified quantifi-
cation. Ph.D. Dissertation. Computer Laboratory,
University of Cambridge, United Kingdom.
Gerhard Heyer. 1990. ?Semantics and Knowledge
Representation in the Analysis of Generic Descrip-
tions?. Journal of Semantics, 7(1):93?110.
Paul Kingsbury, Martha Palmer and Mitch Marcus.
2002. ?Adding Semantic Annotation to the Penn
TreeBank?. In Proceedings of the Human Language
Technology Conference (HLT 2002), San Diego,
California, pages 252?256.
Manfred Krifka, Francis Jeffry Pelletier, Gregory N.
Carlson, Alice ter Meulen, Godehard Link and Gen-
naro Chierchia. 1995. ?Genericity: An Introduc-
tion?. In Gregory N. Carlson and Francis Jeffry
Pelletier, Editors. The Generic Book, pages 1?125.
Chicago: Chicago University Press.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Newbury Park, CA:
Sage.
J. Richard Landis and Gary G. Koch. 1977. ?The
Measurement of Observer Agreement for Categor-
ical Data?. Biometrics, 33:159?174.
Sara-Jane Leslie. 2008. ?Generics: Cognition and Ac-
quisition.? Philosophical Review, 117(1):1?47.
Christopher Lyons. 1999. Definiteness. Cambridge
University Press, Cambridge, UK.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. ?Building a large annotated
corpus of english: The penn treebank?. Computa-
tional Linguistics, 19(2):313?330.
Francis Jeffry Pelletier and Nicolas Asher. 1997.
?Generics and defaults?. In: Johan van Benthem and
Alice ter Meulen, Editors, Handbook of Logic and
Language, pages 1125?1177. Amsterdam: Elsevier.
Massimo Poesio. 2000. ?The GNOME annota-
tion scheme manual?, Fourth Version. http:
//cswww.essex.ac.uk/Research/nle/
corpora/GNOME/anno_manual_4.htm
Massimo Poesio. 2004. ?Discourse Annotation and Se-
mantic Annotation in the GNOME Corpus?. In: Pro-
ceedings of the ACL Workshop on Discourse Anno-
tation, Barcelona, Spain.
Dennis Reidsma and Jean Carletta. 2008. ?Reliability
measurement without limits?. Computational Lin-
guistics, 34(3), pages 319?326.
Anna Ritchie. 2004. ?Compatible RMRS Repre-
sentations from RASP and the ERG?. http:
//www.cl.cam.ac.uk/TechReports/
UCAM-CL-TR-661.
Simone Teufel, Advaith Siddharthan, Dan Tidhar.
2006. ?An annotation scheme for citation function?.
In: Proceedings of Sigdial-06, Sydney, Australia,
pages 80?87.
Simone Teufel. 2010. The Structure of Scientific Arti-
cles: Applications to Summarisation and Citation
Indexing. CSLI Publications. In press.
81
Formalising and specifying underquantification
Aurelie Herbelot
University of Cambridge
ah433@cam.ac.uk
Ann Copestake
University of Cambridge
aac@cl.cam.ac.uk
Abstract
This paper argues that all subject noun phrases can be given a quantified formalisation in terms
of the intersection between their denotation set and the denotation set of their verbal predicate. The
majority of subject noun phrases, however, are only implicitely quantified and the task of retrieving
the most plausible quantifier for a given NP is non-trivial. We propose a formalisation which captures
the underspecification of the quantifier in subject NPs and we show that this formalisation is widely
applicable, including in statements involving kinds. We then present a baseline for a quantification
resolution system using syntactic features as basis for classification. Although the syntactic baseline
provides a respectable 78% precision, our error analysis shows that obtaining true performance on
the task requires information beyond syntax.
1 Quantification resolution
Most subject noun phrases in English are not explicitly quantified. Still, humans are able to give them
quantificational interpretations in context:
1. Cats are mammals = All cats...
2. Cats have four legs = Most cats...
3. Cats were sleeping by the fire = Some cats...
4. The beans spilt out of the bag = Most/All of the beans...
5. Water was dripping through the ceiling = Some water...
We refer to this process as quantification resolution, that is, the process of giving an implicitely quan-
tified NP a formalisation which expresses a unique set relation appropriate to the semantics of the utter-
ance. For instance, the most plausible resolution of 1 can be expressed as:
6. All cats are mammals.
|? ? ?| = |?| where ? is the set of all cats and ? the set of all mammals.
Resolving the quantification value of NPs is important for many NLP tasks, in particular for infer-
ence. We would like to be able to automatically perform the type of interpretations shown in 1 to 5.
It will allow us to draw conclusions such as If (all) cats are mammals and Tom is a cat, then Tom is a
mammal and If (some) cats are in my garden, then (some) animals are in my garden.1
The task of quantification resolution involves finding a semantic representation that goes beyond what
is directly obtainable from a sentence?s syntactic composition. We can write the(x, cat?(x), sleep?(x))
as we would write some(x, cat?(x), sleep?(x))2, but while the quantification semantics of some can be
1The type of entailment relying on word substitution is dependent on quantification: (All) cats are mammals doesn?t imply
that (All) animals are mammals.
2We use here a generalised quantifier notation were the first argument of the quantifier is the bound variable.
165
fully defined (given a singular NP, we are talking of one entity only), that of the cannot: in a singu-
lar NP introduced by the, the referent can either be a single entity or a plurality with various possible
quantificational interpretations (cf The cat is sleeping vs The cat is a mammal).
This paper is an attempt to provide a formal semantics for implicitely quantified NPs which a) sup-
ports the type of inferences required by NLP, b) has good empirical coverage (beyond ?standard? lin-
guistic examples), c) lends itself to evaluation by human annotation and d) can be derived automatically.
We draw on work in formal linguistics, but by formulating the problem as quantification resolution,
we obtain an account which is more tractable from an NLP perspective. We also present preliminary
experiments that automate quantification resolution using a syntax-driven classifier.
2 Under(specified) quantification
The phenomenon of ambiguous quantification overlaps with genericity. Generic NPs have tradition-
ally been described as referring to kinds (Krifka et al, 1995) and one of their most frequent syntactic
expressions is the bare plural, although they occur in definite and indefinite singulars too, as well as
bare singulars. There are many views on the semantics of generics (e.g. Carlson, 1995; Pelletier and
Asher, 1997; Heyer, 1990; Leslie, 2008) but one of them is that they quantify (Cohen, 1996), although,
puzzlingly enough, not always with the same quantifier:
7. Dogs are in my garden = Some dogs...
8. Frenchmen eat horsemeat = Some/Relatively-many Frenchmen... (For the relatively many reading,
see Cohen, 2001.)
9. Cars have four wheels = Most cars...
This behaviour has so far prevented linguists from agreeing on a single formalisation for all generics.
Note that relegating the various readings to a matter of pragmatics, formalising all bare plurals using an
existential, is no solution as we are then unable to explain the semantic difference between, for instance,
Mosquitoes carry malaria and Some mosquitoes carry malaria. The only accepted assumption is that
an operator GEN exists, which acts as a silent quantifier over the restrictor (subject) and matrix (verbal
predicate) of the generic statement.
In this paper, we take an approach which sidesteps some of the intractable problems associated with
the literature on generics and which also extends to definite plurals, as discussed below. Instead of
talking of ambiguous quantification, we will talk of underspecified quantification, or underquantifi-
cation. By this, we mean that the bare plural, rather than exhibiting a silent, GEN quantifier, simply
features a placeholder in the logical form which must be filled with the appropriate quantifier (e.g.,
uq(x, cat?(x), sleep?(x)), where uq is the placeholder quantifier). This account caters for the facts that
so-called generics can so easily be quantified via traditional quantifiers, that GEN is silent in all known
languages, and it explains also why it is the bare form which has the highest productivity, and can denote
a range of quantified entities, from existentials to universals. Using the underquantification hypothesis,
we can paraphrase any generic of the form ?X does Y? as ?there is a set of things X, a certain number of
which do Y? (note the partitive construction).
We now turn to definite plurals which have traditionally been thought to be outside of the genericity
phenomenon and associated with universals (e.g., Lyons, 1999). Definite plurals do exhibit a range of
quantificational behaviour and thus we argue that they should be studied as underquantified forms too.
Consider the following, from Dowty (1987):
10. At the end of the press conference, the reporters asked the president questions.
Dowty remarks that it is not necessary that all reporters ask questions for the sentence to be true. In fact,
it is only necessary that some of them did. Dowty says: ?The question of how many members of the
group referent of a definite NP must have the distributive property is in part lexically determined and in
part determined by the context, and only rarely is every member required to have these properties.?
Following the existential reading, we can write:
166
11. some(x, reporter?(x), askQuestion?(x))
The problem is that for Dowty, the NP refers to a ?group?, i.e., to the reporters as a whole, and not to
specific reporters. We don?t want to say ?there is a small set of reporters, each of which asked a question?;
we want to say ?there is a large set of reporters ? all those present at the press conference ? and some
of them asked a question?, i.e., we want to use a partitive construction. We follow Brogaard?s (2007)
account of definite plurals as partitive constructions, where she examines the following:
12. The students asked questions.
Brogaard argues that, given X , the denotation of the students, a subset Y of X is selected via the quan-
tifier some and that the verbal predicate applies (distributively) to Y . A similar account can be given
of (10): there is a set of reporters, and a certain number of elements in that set (some reporters) asked
questions ? which is our desired reading. Note that all definite plurals can have this interpretation (e.g.,
possessives and demonstratives also).
We will next argue that the partitive construct observed in definite plurals can be generally applied to
subject NPs and we will propose a single formalisation for all underquantified statements.
3 Formalisation
3.1 Link?s notation (1983)
In what follows, we briefly define each item of notation used in this work, as taken from Link (1983).
We illustrate the main points via examples over a closed worldW containing three cats (Kitty, Sylvester
and Bagpuss).
The background assumption for our formalisation is that, following Link, plurals can be represented
as lattices. The star sign ? generates all individual sums of members of the extension of predicate P . So
if P is cat?, the extension of ?P is a join-semilattice representing all possible sums of cats in the world
under consideration. The join-semilattice of cats in worldW is shown in Fig 1.
Figure 1: Join-semilattice of all cats in worldW
The sign ? is the sum operator. ?xPx represents the sum, or supremum, of all objects that are ?P .
??xPx represents the proper sum of Ps, that is, the supremum of all objects that are proper plural
predicates of P . The sum includes (non-plural) individuals such asK or S while the proper sum doesn?t.
In worlds where there is more than one object in the extension of ?P , ?xPx = ??xPx: e.g., in Fig 1,
the sum of all cats is the same as the proper sum of all cats, i.e., the set {K,S,B}. (Compare this with a
world where there is only one cat, say Kitty: then ?xPx = {K} while ??xPx = ?).
The product sign
?
expresses an individual-part relation. The ? sign in combination with ? indi-
cates atomic part. Following Chierchia (1998), we assume the same underlying lattice for both mass
terms and count nouns, so we use the
?
and ? operators for formalising quantification over mass entities.
3.2 Collective and distributive predicates
Some predicates are collective: they refer to a group as a whole and not to its instances (13). Other
predicates are always distributive (14):
167
13. Antelopes gather near water holes (*Andy the antelope gathers near water holes.)
14. Three soldiers were asleep (Tom was asleep, Bill was asleep, Cornelia was asleep.)
Most verbal phrases, though, are ?mixed predicates? that accept both readings:
15. Three soldiers stole wine from the canteen.
(Tom, Bill and Cornelia went together to the canteen to steal wine or Tom, Bill and Cornelia each
stole wine from the canteen.)
Collective predicates can be a source of confusion when trying to directly apply quantification to an
ambiguously quantified NP:
16. (*Some/Most/All) Americans elect a new president every five years.
Quantifying 16 seems initially impossible in shallow form: we cannot write all(x,american?(x),electPres?(x))
as it seems to imply distributivity. However, we refer to the reporter example (10) and the latent partitive
construct that we suggested existed in that (distributive) sentence. By similarity, we can say that there
is a set X of Americans able to vote, and a subset Y of those ? which in this case is selected by the
quantifier all and is therefore equal to X ? collectively elects the president.
3.3 Formalising the partitive construct
Following Link (1998) for the formalisation of collective and distributive predicates, we can write, for
10 and 16:
17. X = ??x reporterAtPressConference?(x) ? ?Y [Y ?X ? ?z[z ??Y ?askques?(z)]]
18. X = ??xvotingAmerican?(x) ? ?Y [Y ?X?electPresident?(Y )]3
For the collective case, we just apply the verbal predicate collectively.
We can then add the quantifier resolution. We assume a three-fold partitioning of the quantificational
space, corresponding to the natural language quantifiers some, most and all (in addition to one, for the
description of singular, unique entities). The corresponding set relations are:
19. if some(?, ?) then 0 < |? ? ?|
20. ifmost(?, ?) then |?? ?| ? |? ? ?|
21. if all(?, ?) then |?? ?| = 0
These set relations can be expressed in terms of the sets involved in the partitive construction: in 16,
ifX is the set of all Americans able to vote, Y the subset ofX selected by the quantifier, and Z the set of
all things that elect the president, then Y actually represents the intersection X ? Z. We can thus write:
22. X = ??x reporterAtPressConference?(x)? ?Y [Y ?X ? ?z[z ??Y ?askques?(z)]? (0 < |Y |)]
23. X = ??x votingAmerican?(x) ? ?Y [Y ?X?electPresident?(Y ) ? (|X ? Y | = 0)]
The same principle applies to mass nouns. We show below a distributive example.
24. Water was dripping through the ceiling.
X = ??x water?(x) ? ?Y [Y ?X ? ?z[z ??Y ?dripThroughCeiling?(z)] ? (0 < |Y |)]
We thus write the underspecified quantifier as:
25. X = ??x P ?(x) ? ?Y [Y ?X ?Q(Y )] ? quantConstraint(X,Y )]
where the quantConstraint ensures the correct cardinality of Y for various quantifiers and the predicateQ
applies distributively or collectively depending on the semantics of the sentence. X and Y respectively
denote the Nbar and NP referents in the quantified paraphrase of the statement.
3Note that in the two examples, we have restricted X to the relevant set of entities. We will not investigate here how this
particular reference resolution takes place.
168
4 Kinds
In order to argue that our formalisation is applicable to all subject noun phrases, we must briefly come
back to the case of generics which, in some linguistic accounts, are not seen as quantified (Carlson,
1977).4 According to those accounts, the subject NP in sentences such as The cat is a mammal (the
kind) can be regarded as an entity similar to proper nouns. The generic reading of the sentence then
takes a straightforward subject/predicate formalisation of the type mammal?(cat?). The main argument
in favour of such a representation is the existence of sentences where the verbal predicate seems to only
be applicable to a species rather than to its instances:
26. The dodo is extinct.
Such cases, we claim, do not preclude quantification. We use the accounts of Chierchia (1998) and
Krifka (2004), where a kind is defined as a function that returns the greatest element of the extension of
the property relevant to that kind: Kind(X) = ??x X ?(x). This gives us the following for 26:
27. X = ??x dodo?(x) ? ?Y [Y ?X ? extinct?(Y ) ? (|Y ?X| = 0)]
We stress however that we do not deny the validity of representations that involve a simple sub-
ject/predicate structure. It should be clear that the sentence The cat is a mammal has an interpretation
where the species ?cat? is attributed the property of being a mammal. What we argue is simply that the
meaning of the sentence also includes a quantificational aspect. We want, after all, to be able to make
natural inferences about individual cats: if the cat is a mammal then Tom the cat is a mammal. We believe
that both quantification and a subject/predicate formalisation are necessary to fully render the semantics
of such sentences. We will also argue in Section 7 that for the purposes of computational linguistics, it
is actually desirable to formalise the quantificational aspect separately, as part of the full semantics.
We should also note that the genericity phenomenon is usually seen as encompassing habitual con-
structions (Krifka et al, 1995). Our quantificational account of kinds will not necessarily be applicable
to quantification of events and we do not wish to make any claims with regard to habituality in this paper.
For completeness, we will however point out that, following Chierchia (1995) on indefinites, we see
quantification adverbs as able to bind, and therefore quantify over individuals: according to this view,
the most felicitous reading of Mosquitoes sometimes carry malaria is Some mosquitoes carry malaria,
formalisable with 25.
5 Automatic quantification: first attempts
To our knowledge, no attempt at the automatic specification of quantification has been made before. In
consequence, we start our investigation with the simplest possible type of machine learning algorithm,
using as determining features the direct syntactic context of the statement to be quantified. The general
idea of such a system is that grammatical information such as the number of a subject noun phrase and
the tense of its verbal predicate may be statistically related to its classification.
5.1 Gold standard
We built a gold standard by re-using and expanding the quantification annotations we produced in Herbe-
lot and Copestake (2010). This small corpus, which contains randomly extracted Wikipedia5 sentences,
provides 300 instances of triply annotated subject noun phrases. The categories used for annotation are
the natural language quantifiers ONE, SOME, MOST, ALL and the label QUANT (for noun phrases of the
type some cats, most turtles or more than 37 unicorns which, being explicitly quantified, do not enter our
underquantification account and must be marked with a separate label). In order to convert the multiple
4A more comprehensive discussion can be found in Herbelot (2010).
5http://www.wikipedia.org/
169
annotations to a gold standard, we used majority opinion when it was available and negotiation in cases
of complete disagreement. There were only 14 cases where a majority opinion cannot be obtained.
The main issue with the resulting gold standard is its relatively small size. The 300 data points it
provides are clearly insufficient for machine learning, but the annotation process is time-consuming and
we do not have the resources to set up a large-scale annotation effort. As a trade-off, the first author
of this paper annotated a further 300 noun phrases, thus doubling the size of the gold standard. As a
precaution, we ran the classifier presented later in this section over the original gold standard and over
the new annotations; no substantial difference in performance between the two runs was found.
Table 1 shows the class distribution of our five quantification labels over the 600 instances of the
extended gold standard.
Class Number of instances Percentage of corpus
ONE 367 61%
SOME 53 9%
MOST 34 6%
ALL 102 17%
QUANT 44 7%
Table 1: Class distribution over 600 instances
We note, first, that the number of explicitly quantified noun phrases amounts to only 7% of the an-
notation set. This shows that the resolution of underquantification has potentially high value for NLP
systems. Next, we remark that 61% of all instances simply denote a single entity, leaving 32% to under-
quantified plurals ? 189 instances. This imbalance is problematic for the machine learning task that we
set out to achieve. First, it means that the training data available for SOME, MOST and ALL annotations
is comparably sparse. Secondly, it implies that the baseline for our future classifier is relatively high:
assuming a most frequent class baseline, we must beat 61% precision.
5.2 Quantifying with syntax
Most of the remarks that can be found in the literature on the relation between syntax and quantification
have been written with respect to the generic versus non-generic distinction. Although we have moved
away from the terminology on genericity, the two following examples show the potential promises ?
and hurdles ? of using syntax to induce quantification annotations.
? Noun phrases which act as subjects of simple past tense verbs are usually non-generic: A cow says
?moo? / A cow said ?moo? (Gelman, 2004). However, the so-called ?historic past? is an exception
to this rule: The woolly mammoth roamed the earth many years ago.
? The combination of a bare plural and present tense is a prototypical indication of genericity: Tigers
are massive (Cimpian and Markman, 2008). But news headlines behave differently: Cambridge
students steal cow.
We informally investigate the distribution of various grammatical constructions with respect to quan-
tification, as obtained from our gold standard. Although some constructions give a clear majority to one
or another label, that majority is not always overwhelming. For instance, consistently annotating bare
plurals followed by a past tense as SOME would result in a precision of only 54%. It is therefore unclear
how accurate a classifier based only on syntax can be. (Note that the quantification phenomenon is un-
derstood to be semantically complex and that syntax is only one of many features used in the annotation
guidelines produced in Herbelot and Copestake, 2010.)
170
5.3 Features
We give the system article and number information for the noun phrase to be quantified, as well as the
tense of the verbal predicate following it. In order to cater for proper nouns, we also indicate whether the
head of the noun phrase is capitalised or not. Article, number and capitalisation information is similarly
provided for the object of the verb. All features are automatically extracted from the Robust Minimal
Recursion Semantics (RMRS, Copestake, 2004) representation of the sentence in which the noun phrase
appears (obtained via a RASP parse, Briscoe et al, 2006). The following shows an example of a feature
line for a particular noun phrase (the sentence in which the noun phrase appears is also given):
ORIGINAL: [His early blues influences] included artists such as Robert
Johnson, Bukka White, Skip James and Sleepy John Estes.
FEATURES: past,possessive,plural,nocap,bare,plural,nocap
Note that articles belonging to the same class are labelled according to that class: all possessive
articles, for instance, are simply marked as ?possessive?. This is the same for demonstrative articles.
5.4 Experiments and results
The aim of this work is not only to produce an automatic quantification system, but also, if possible,
to learn about the linguistic phenomena surrounding the underspecification of quantification. Because
of this, we choose a tree-based classifier which has the advantage of letting us see the rules that are
created by the system and thereby may allow us to make some linguistic observations with regard to the
cooccurrence of certain quantification classes with certain grammatical constructions. We use an off-the-
shelf implementation of the C4.5 classifier (Quinlan, 1993) included in the Weka data mining software.6
We perform a 6-fold cross-validation on the gold standard and report class precision, recall and F-score.
Class Precision Recall F-score
ONE 86% (362/422) 99% (362/367) 92%
SOME 60% (25/42) 47% (25/53) 53%
MOST 33% (2/6) 6% (2/34) 10%
ALL 53% (57/108) 56% (57/102) 54%
QUANT 100% (22/22) 50% (22/44) 67%
Table 2: Class precision and recall for the quantification task
The C4.5 classifier gives 78% overall precision to the quantification task. Tables 2 shows per class
results for the three tasks. The figures in brackets indicate the number of true positives for a particular
class, followed by the total number of instances annotated by the system as instances of that class. The
classifier performs extremely well with the ONE class, reaching 92% F-score. Already quantified noun
phrases yield perfect precision and mediocre recall, as might be expected since we do not provide the
system with a list of quantifiers. The system performs less well with the labels SOME, MOST and ALL.
In order to understand the distribution of errors, we perform a detailed analysis on the first fold of
our data. Out of 100 instances, the classifier assigns 25 to an incorrect class. The majority of those
errors (44%) are due to the fact that the classifier labels all singulars as ONE, missing out on generic
interpretations and in particular on the plural reading of mass terms: out of 11 errors, 5 are linked to
a bare singular). The next most frequent type of error, covering another 16% of incorrectly classified
instances, comes from already quantified noun phrases being labelled as another class. These errors
affect the recall of the QUANT class and the precision of the SOME, MOST and ALL labels in particular
(most of those errors occur in plural noun phrases). The coarseness of the rules is again to blame for
the remaining errors: looking at the decision tree produced by the classifier, we observe that all bare
6http://www.cs.waikato.ac.nz/ml/weka/
171
plurals followed by a present tense, as well as all definite plurals, are labelled as universals, while all
bare plurals followed by a past tense are labelled as SOME. This accounts for a further 7 errors. The last
three incorrect assignments are due to a dubious capitalisation rule.
5.5 Correspondence with linguistics
We observe that most definite plurals (including demonstratives and possessives) are classified as either
MOST or ALL. This fits the linguistic notion of a definite as being essentially universal (Lyons, 1999) but
also misses out on the correct quantification of statements such as 10.
We note also that non-capitalised bare plurals followed by a present tense are similarly classed as
ALL. This echoes the observation that the combination of bare plural and present is a typical manifes-
tation of genericity (if one understands genericity as a quantification phenomenon close to universality).
When followed by past or perfect tenses, an existential quantification with SOME is however preferred.
One of the puzzles opened by the classifier?s decision trees is the use of the direct object feature to
distinguish between MOST and ALL in the case of some definite plurals. Given Sentences 28 and 29, our
classifier would label the first one as ALL and the second one as MOST.
28. My cats like the armchair. ALL
29. My cats like the armchairs. MOST
At first glance, the rule seems to be a mere statistical effect of our data. We will however remark
that statements like 29 are reserved a special section in Link (1998), where they are introduced as ?rela-
tional plural sentences?. One of Link?s claims is that those sentences warrant four collective/distributive
combinations ? as opposed to two only in the case where the object is an individual. So we can say in
Sentence 29 that a collective of cats likes a collective of armchairs, or that this collective of cats likes
each armchair individually, etc. This proliferation of interpretations makes uncertainties more likely with
regard to who likes what, and to the quantification of the subject and object.
For now, we will simply conclude that, although a simple syntax-based classifier is able to classify
certain constructs with high precision, other constructs are beyond its capabilities. Further, it is difficult
to see how improvements can be made to the current classification without venturing outside of the
grammatical context. For instance, it seems practically impossible to improve on the high-precision rule
specifying that every singular noun phrase should be classified as ONE. Due to space constraints, we
will not report any further experiments in this paper. However, preliminary investigations into the use of
lexical similarity to resolve quantification ambiguity can be found in Herbelot (2010).
6 Previous work
The general framework of this proposal is an underspecification account close to that described in Pinkal
(1996) or Egg (2010). Computational approaches to underspecified quantification have so far focused
on the genericity phenomenon. Leaving aside the question of annotation, which is treated in Herbelot
and Copestake (2010), research on genericity can be classified within two strands: theoretical research
on defeasible reasoning and extraction of common sense knowledge. Attempts to model defeasible
reasoning were made in the 1980s with, for instance, the developments of default logic (Reiter, 1980)
and non-monotonic logic (McDermott and Doyle, 1982). With information extraction as aim, Suh et al
(2006) attempt to retrieve ?common sense? statements from Wikipedia. They posit that common sense
is contained in generic sentences. Their system, however, makes simplifying assumptions with regard to
syntax: in particular, all bare plurals (and bare plurals only) are considered generic. In general, common
sense extraction systems tend to restrict the data they mine to avoid the problem of identifying genericity
(e.g., Voelker et al, 2007).
172
7 Conclusion, with some remarks on semantics
We have shown in this paper that subject noun phrases that are not explicitly quantified could be rep-
resented in an underspecified form. We have also argued that this formalisation is applicable to all
constructs, including so-called generics. We have introduced a syntax-based classifier for quantification
resolution and discussed the limits of an approach relying on compositional information only.
We acknowledge that our quantificational account of noun phrases, and especially of generics, does
not satisfy the common requirement that a formalisation be a full description of the semantic particu-
larities of a linguistic phenomenon. We think, however, that this requirement has led to over-restrictive
approaches. One of the debates surrounding generics, for instance, relates to whether they should be
given a ?rules and regulations? or an inductivist truth condition (Carlson, 1995). Our view is that it would
be a mistake to exclude either interpretation. Burton-Roberts? (1977) A gentleman opens doors for ladies
clearly has normative force and without doubt, also allows the hearer to make their own conclusions with
regard to the intersection between the set of all gentlemen and the set of people opening doors for ladies.
Our view of semantics is that it is a layered system and that specifying the quantification semantics
of a noun phrase does not mean providing the full semantics of that noun phrase. It may be argued that
the ideal semantics of generics should be unified and integrate all possible aspects of meaning. But such
a theory is yet to be developed for genericity and, from a computational point of view, may not even
be desirable: a modular representation of meaning allows us to only formalise the aspects that we are
interested in for a particular task, leaving the rest out.
The approach presented here can be said to implement the idea of ?slacker? semantics (Copestake,
2009) in that a) our experiments try to derive a specification from compositional information only and
b) we only attempt to specify one aspect of the meaning of noun phrases (quantification), leaving other
aspects unspecified. In the future, we would like to take away some of the slack in a) by using lexical
semantics in the specification of quantification. In order to do this, a much larger corpus should be
created for the training and testing of the system, and this will be our next task.
References
Briscoe, T., J. Carroll, and R. Watson (2006). The second release of the RASP system. In Proceedings
of the COLING/ACL on Interactive presentation sessions, Morristown, NJ, USA, pp. 77?80.
Brogaard, B. (2007). The But Not All: A Partitive Account of Plural Definite Descriptions. Mind &
Language 22(4), 402?426.
Burton-Roberts, N. (1977). Generic sentences and analyticity. Studies in Language 1, 155?196.
Carlson, G. N. (1977). Reference to Kinds in English. Ph. D. thesis, University of Massachusetts at
Amherst.
Carlson, G. N. (1995). Truth-Conditions of Generic Sentences: Two Contrasting Views. In G. N. Carlson
and F. J. Pelletier (Eds.), The Generic Book, pp. 224?237. Chicago: University of Chicago Press.
Chierchia, G. (1995). Individual-level predicates as inherent generics. In G. N. Carlson and F. J. Pelletier
(Eds.), The Generic Book, pp. 176?223. Chicago: University of Chicago Press.
Chierchia, G. (1998). Reference to kinds across languages. Natural Language Semantics 6, 339?405.
Cimpian, A. and E. M. Markman (2008). Preschool children?s use of cues to generic meaning. Cogni-
tion 107(1), 19?53.
Cohen, A. (1996). Think Generic: The Meaning and Use of Generic Sentences. Ph. D. thesis, Carnegie-
Mellon University at Pittsburgh.
173
Copestake, A. (2004). Robust Minimal Recursion Semantics. http://www.cl.cam.ac.uk/
?aac10/papers/rmrsdraft.pdf.
Copestake, A. (2009). Slacker semantics : why superficiality , dependency and avoidance of commitment
can be the right way to go. In Proceedings of the 12th Conference of the European Chapter of the
Association for Computational Linguistics, Athens, Greece, pp. 1?9.
Dowty, D. (1987). Collective predicates, distributive predicates and all. In F. Marshall, A. Miller, and
Z.-s. Zhang (Eds.), The Third Eastern States Conference on Linguistics, Columbus, pp. 97?115. The
Ohio State University, Department of Linguistics.
Egg, M. (2010). Semantic Underspecification. Language and Linguistics Compass 4(3), 166?181.
Gelman, S. A. (2004). Learning words for kinds: Generic noun phrases in acquisition. In D. Hall and
S. Waxman (Eds.), Weaving a lexicon. Cambridge, MA: MIT Press.
Herbelot, A. (2010). Underspecified quantification. Ph. D. thesis, University of Cambridge.
Herbelot, A. and A. Copestake (2010). Annotating underquantification. In Proceedings of the Fourth
Linguistic Annotation Workshop, Uppsala, Sweden, pp. 73?81.
Heyer, G. (1990). Semantics and Knowledge Representation in the Analysis of Generic Descriptions.
Journal of Semantics 7(1), 93?110.
Krifka, M. (2004). Bare NPs: Kind-referring, Indefinites, Both, or Neither? In O. Bonami and P. Cabredo
Hofherr (Eds.), Empirical Issues in Formal Syntax and Semantics, pp. 111?132.
Krifka, M., F. J. Pelletier, G. N. Carlson, A. ter Meulen, G. Chierchia, and G. Link (1995). Genericity:
An Introduction. In G. N. Carlson and F. J. Pelletier (Eds.), The Generic Book, pp. 1?125. Chicago:
Chicago University Press.
Leslie, S.-J. (2008). Generics: Cognition and Acquisition. Philosophical Review 117(1), 1?47.
Link, G. (1983). The Logical Analysis of Plurals and Mass Terms: a Lattice-Theoretical Approach. In
R. Bauerle, C. Schwarze, and A. von Stechow (Eds.), Meaning, Use, and Interpretation of Language,
pp. 302?323. Berlin: de Gruyter.
Link, G. (1998). Plural. In Algebraic Semantics in Language and Philosophy. Stanford: CSLI Publica-
tions.
Lyons, C. (1999). Definiteness. Cambridge: Cambridge University Press.
McDermott, D. and J. Doyle (1982). Non-monotonic Logic I. Artificial Intelligence 13, 41?72.
Pelletier, F. J. and N. Asher (1997). Generics and Defaults. In J. van Bethem and A. ter Meulen (Eds.),
Handbook of Logic and Language, pp. 1125?1177. Amsterdam: Elsevier.
Pinkal, M. (1996). Radical Underspecification. In P. Dekker and M. Stokhof (Eds.), Proceedings of the
10th Amsterdam Colloquium, Amsterdam, pp. 479?498. de Gruyter.
Quinlan, J. (1993). Programs for Machine Learning. San Francisco, CA: Morgan Kaufmann.
Reiter, R. (1980). A logic for default reasoning. Artificial Intelligence 13(1-2), 81?132.
Suh, S., H. Halpin, and E. Klein (2006). Extracting Common Sense Knowledge from Wikipedia. In
Proceedings of the International Semantic Web Conference (ISWC-06). Workshop on Web Content
Mining with Human Language Technology, Athens, GA.
Voelker, J., P. Hitzler, and P. Cimiano (2007). Acquisition of OWL DL Axioms from Lexical Resources.
In Proceedings of the Fourth European conference on The Semantic Web: Research and Applications,
Innsbruck, Austria, pp. 670?685. Springer Verlag.
174
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 137?147,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Towards an on-demand Simple Portuguese Wikipedia
Arnaldo Candido Junior Ann Copestake
Institute of Mathematics and Computer Sciences Computer Laboratory 
University of S?o Paulo University of Cambridge 
arnaldoc at icmc.usp.br Ann.Copestake at cl.cam.ac.uk 
Lucia Specia Sandra Maria Alu?sio
Research Group in Computational Linguistics Institute of Mathematics and Computer Sciences 
University of Wolverhampton University of S?o Paulo
l.specia at wlv.ac.uk sandra at icmc.usp.br
Abstract
The  Simple  English Wikipedia  provides  a 
simplified  version  of  Wikipedia's  English 
articles  for  readers  with  special  needs. 
However,  there are fewer efforts  to make 
information  in  Wikipedia  in  other 
languages  accessible  to  a  large  audience. 
This work proposes the use of a syntactic 
simplification  engine  with  high  precision 
rules  to  automatically  generate  a  Simple 
Portuguese Wikipedia on demand, based on 
user interactions with the main Portuguese 
Wikipedia.  Our  estimates  indicated that  a 
human  can  simplify  about  28,000 
occurrences  of  analysed  patterns  per 
million  words,  while  our  system  can 
correctly simplify 22,200 occurrences, with 
estimated f-measure 77.2%. 
1 Introduction
The Simple English Wikipedia1 is an effort to make 
information  in  Wikipedia2 accessible  for  less 
competent  readers  of  English  by  using  simple 
words and grammar.  Examples of  intended users 
include  children  and  readers  with  special  needs, 
such as users with learning disabilities and learners 
of English as a second language. 
Simple English (or Plain English), used in this 
version  of  Wikipedia,  is  a  result  from the  Plain 
English movement that occurred in Britain and the 
United States in the late 1970?s as a reaction to the 
unclear language used in government and business 
forms and documents. Some recommendations on 
how  to  write  and  organize  information  in  Plain 
1 http://simple.wikipedia.org/
2 http://www.wikipedia.org/
Language (the set of guidelines to write simplified 
texts) are related to both syntax and lexical levels: 
use short sentences; avoid hidden verbs; use active 
voice; use concrete, short, simple words. 
A number of resources, such as lists of common 
words3,  are available for the English language to 
help users write in Simple English. These include 
lexical  resources  like  the  MRC  Psycholinguistic 
Database4 which  helps  identify  difficult  words 
using  psycholinguistic  measures.  However, 
resources as such do not exist for Portuguese. An 
exception is a small list of simple words compiled 
as part  of  the  PorSimples project  (Aluisio et  al., 
2008).
Although  the  guidelines  from  the  Plain 
Language  can  in  principle  be  applied  for  many 
languages and text genres, for Portuguese there are 
very  few  efforts  using  Plain  Language  to  make 
information accessible to a large audience. To the 
best  of  our  knowledge,  the  solution  offered  by 
Portugues  Claro5 to  help  organizations  produce 
European  Portuguese  (EP)  documents  in  simple 
language is the only commercial option in such a 
direction.  For  Brazilian  Portuguese  (BP),  a 
Brazilian  Law (10098/2000)  tries  to  ensure  that 
content  in  e-Gov sites  and  services  is  written  in 
simple  and  direct  language  in  order  to  remove 
barriers in communication and to ensure citizens' 
rights  to  information  and communication  access. 
However,  as  it  has  been  shown  in  Martins  and 
Filgueiras  (2007),  content  in  such  websites  still 
needs  considerable  rewriting  to  follow the  Plain 
Language guidelines. 
A few efforts from the research community have 
recently  resulted  in  natural  language  processing 
3 http://simple.wiktionary.org/
4 http://www2.let.vu.nl/resources/elw/resource/mrc.html
5  http://www.portuguesclaro.pt/
137
systems to simplify and make Portuguese language 
clearer. ReEscreve (Barreiro and Cabral, 2009) is a 
multi-purpose  paraphraser that  helps  users  to 
simplify their EP texts by reducing its ambiguity, 
number  of  words  and  complexity.  The  current 
linguistic phenomena paraphrased are support verb 
constructions,  which  are  replaced  by  stylistic 
variants.  In  the  case  of  BP,  the  lack  of 
simplification  systems  led  to  development  of 
PorSimples project (Alu?sio and Gasperin, 2010). 
This  project  uses  simplification  in  different 
linguistic levels to provide simplified text to poor 
literacy readers.
For  English,  automatic  text  simplification  has 
been  exploited  for  helping  readers  with  poor 
literacy (Max, 2006) and readers with other special 
needs,  such  as  aphasic  people  (Devlin  and 
Unthank,  2006;  Carroll  et  al.  1999).  It  has  also 
been used in bilingual education (Petersen, 2007) 
and  for  improving  the  accuracy  of  Natural 
Language Processing (NLP) tasks (Klebanov et al, 
2004; Vickrey and Koller, 2008).
Given the general scarcity of human resources to 
manually simplify large content  repositories such 
as Wikipedia, simplifying texts automatically can 
be  the  only  feasible  option.  The  Portuguese 
Wikipedia,  for  example,  is  the  tenth  largest 
Wikipedia (as of May 2011), with 683,215 articles 
and approximately 860,242 contributors6. 
In  this  paper  we  propose  a  new  rule-based 
syntactic simplification system to create a Simple 
Portuguese Wikipedia  on demand,  based on user 
interactions with the main Portuguese Wikipedia. 
We use a simplification engine to change passive 
into active voice and to break down and change the 
syntax of subordinate clauses.  We focus on these 
operations  because  they  are  more  difficult  to 
process  by  readers  with  learning  disabilities  as 
compared  to  others  such  as  coordination  and 
complex noun phrases (Abedi et al, 2011; Jones et 
al.,  2006;  Chappell,  1985).  User  interaction with 
Wikipedia can be performed by a system like the 
Facilita7 (Watanabe et al, 2009), a browser plug-in 
developed  in  the  PorSimples  project  to  allow 
automatic adaptation (summarization and syntactic 
simplification) of any web page in BP.
This  paper  is  organized  as  follows.  Section  2 
presents related work on syntactic  simplification. 
6 http://meta.wikimedia.org/wiki/List_of_Wikipedias#
Grand Total
7 http://nilc.icmc.usp.br/porsimples/facilita/
Section  3 presents the methodology to build and 
evaluate the simplification engine for BP. Section 4 
presents  the  results  of  the  engine  evaluation. 
Section  5 presents  an  analysis  on  simplification 
issues  and  discusses  possible  improvements. 
Section 6 contains some final remarks.
2 Related work
Given the dependence of  syntactic  simplification 
on  linguistic  information,  successful  approaches 
are  mostly  based  on  rule-based  systems. 
Approaches using operations learned from corpus 
have  not  shown  to  be  able  to  perform  complex 
operations  such  the  splitting  of  sentences  with 
relative clauses (Chandrasekar and Srinivas, 1997; 
Daelemans  et  al.,  2004;  Specia,  2010).  On  the 
other hand. the use of machine learning techniques 
to predict when to simplify a sentence, i.e. learning 
the properties of language that distinguish simple 
from normal  texts,  has  achieved relative  success 
(Napoles and Dredze, 2010). Therefore, most work 
on syntactic simplification still relies on rule-based 
systems to simplify a set of syntactic constructions. 
This is also the approach we follow in this paper. 
In what follows we review some relevant and work 
on syntactic simplification.
The seminal work of Chandrasekar and Srinivas 
(1997) investigated the induction of syntactic rules 
from a corpus annotated with part-of-speech tags 
augmented  by  agreement  and  subcategorization 
information.  They  extracted  syntactic 
correspondences  and  generated  rules  aiming  to 
speed up parsing and improving its accuracy, but 
not  working  on  naturally  occurring  texts. 
Daelemans et  al.  (2004)  compared both machine 
learning  and  rule-based  approaches  for  the 
automatic generation of TV subtitles for hearing-
impaired  people.  In  their  machine  learning 
approach,  a  simplification model  is  learned from 
parallel  corpora  with  TV  programme  transcripts 
and the associated subtitles. Their method used a 
memory-based learner and features such as words, 
lemmas,  POS tags,  chunk tags,  relation tags  and 
proper  name  tags,  among  others  features  (30  in 
total). However, this approach did not perform as 
well  as  the  authors  expected,  making errors  like 
removing sentence subjects or deleting a part of a 
multi-word  unit.   More  recently,  Specia  (2010) 
presented a new approach for text simplification, 
based  on  the  framework  of  Statistical  Machine 
138
Translation. Although the results are promising for 
lexical simplification, syntactic rewriting was not 
captured  by  the  model  to  address  long-distance 
operations,  since  syntactic  information  was  not 
included into the framework.
Inui et al (2003) proposed a rule-based system 
for text simplification aimed at deaf people. Using 
about  one  thousand  manually  created  rules,  the 
authors  generate  several  paraphrases  for  each 
sentence and train a classifier to select the simpler 
ones.  Promising  results  were  obtained,  although 
different  types  of  errors  on  the  paraphrase 
generation are encountered, such as problems with 
verb conjugation and regency.  Our work aims at 
making  Portuguese  Wikipedia  information 
accessible  to  a  large  audience  and  instead  of 
generating  several  possible  outputs  we  generate 
only one based on rules taken from a manual of 
simplification for BP.
Siddharthan  (2006)  proposed  a  syntactic 
simplification  architecture  that  relies  on  shallow 
parsing. The general goal of the architecture is to 
make texts more accessible to a broader audience 
instead of targeting any particular application. The 
system  simplifies  apposition,  relative  clauses, 
coordination  and  subordination.  Our  method,  on 
the other hand, relies on deep parsing (Bick, 2000) 
and focuses  on  changing passive  to  active voice 
and  changing  the  syntax  of  relative  clauses  and 
subordinate sentences.
Max  (2006)  applied  text  simplification  in  the 
writing process by embedding the simplifier into a 
word  processor.  Although  this  system  ensures 
accurate  output,  it  requires  manual  choices.  The 
suggested simplifications are ranked by a score of 
syntactic  complexity  and  potential  change  of 
meaning.  The writer  then chooses their  preferred 
simplification.  Our  method,  on  the  other  hand, 
offers the user only one simplification since it uses 
several  rules  to  better  capture  each  complex 
phenomenon. 
Inspired  by  Siddharthan  (2006),  Jonnalagadda 
and  Gonzalez  (2009)  present  an  approach  to 
syntactic  simplification  addressing  also  the 
problem of accurately determining the grammatical 
correctness  of  the  simplified  sentences.  They 
propose  the  combination  of  the  number  of  null 
links  and  disjunct  cost  (the  level  of 
inappropriateness,  caused  by  using  less  frequent 
rules in the linkage) from the cost vector returned 
by a Link Grammar8 parser. Their motivation is to 
improve the performance of systems for extracting 
Protein-Protein  Interactions  automatically  from 
biomedical  articles  by  automatically  simplifying 
sentences.  Besides  treating  the  syntactic 
phenomena described in Siddharthan (2006), they 
remove  describing  phrases  occurring  at  the 
beginning  of  the  sentences,  like  ?These  results 
suggest that? and ?As reported previously?. While 
they  focus  on  the  scientific  genre,  our  work  is 
focused on the encyclopedic genre.
In order to obtain a text easier to understand by 
children,  De  Belder  and  Moens  (2010)  use  the 
Stanford parser9 to select the following phenomena 
to syntactically simplify the sentences: appositions, 
relative  clauses,  prefix  subordination  and  infix 
subordination  and  coordination.  After  sentence 
splitting, they try to apply the simplification rules 
again to both of the new sentences. However, they 
conclude that  with the set  of  simplification rules 
used,  it  was  not  possible  to  reduce  the  reading 
difficulty for children and foresee the use of other 
techniques for this purpose, such as summarization 
and elaborations for difficult words.
3 Simplification engine
3.1 Engine development
The  development  of  a  syntactic  simplification 
engine  for  a  specific  task  and  audience  can  be 
divided  into  five  distinct  phases:  (a)  target 
audience analysis; (b) review of complex syntactic 
phenomena for such an audience; (c) formulation 
of simplification guidelines; (d) refinement of rules 
based  on  evidence  from  corpora;  and  (e) 
programming and evaluation of rules.
In this paper we focus on the last two phases. 
We  use  the  simplification  guidelines  from  the 
PorSimples  project,  but  these  are  based  on 
grammar  studies  and  corpora  analysis  for  a 
different  text  genre  (news).  Therefore  additional 
corpora  evidence  proved  to  be  necessary.  This 
resulted  in  the  further  refinement  of  the  rules, 
covering  different  cases  for  each  syntactic 
phenomenon.
The Simplification engine relies on the output of 
the  Palavras  Parser  (Bick,  2000)  to  perform 
constituent tree transformations (for example, tree 
8 http://www.abisource.com/projects/link-grammar/
9 http://nlp.stanford.edu/software/lex-parser.shtml
139
splitting).  Each  node  of  a  sentence  tree  is  fed 
(breadth-first  order)  to  the  simplification 
algorithms,  which can simplify the node (and its 
sub-tree) or skip it when the node does not meet 
the simplification prerequisites. Breadth-first order 
is chosen because several operations affect the root 
of a (sub)tree, while none of them affect leaves.
A development  corpus containing examples  of 
cases analysed for each syntactic phenomenon is 
used  to  test  and  refine  the  rules.  The  current 
version of the corpus has 156 sentences extracted 
from news text. The corpus includes negative and 
positive  examples  for  each  rule.  Negative 
examples  should  not  be  simplified.  They  were 
inserted  into  the  corpus  to  avoid  unnecessary 
simplifications. Each rule is first tested against its 
own positive and negative examples.  This test  is 
called  local  test.  After reaching a good precision 
on the local test, the rule is then tested against all 
the  sentences  in  the  corpus,  global  test.  In  the 
current corpus, the global test identified sentences 
correctly   simplified by  at  least  one  rule  (66%), 
sentences incorrectly simplified due to major errors 
in  parsing/rules  (7%)  (ungrammatical  sentences) 
and  non-simplified  sentences  (27%).  The  last 
includes  mainly  negative  examples,  but  also 
includes  sentences  not  selected  due  to  parsing 
errors, sentences from cases not yet implemented, 
and sentences from cases ignored due to ambiguity.
3.2 Passive voice
The default case for dealing with passive voice in 
our simplification engine is illustrated by the pair 
of  original-simplified sentences  in  example10 (1). 
Sentences  belonging  to  this  case  have  a  non-
pronominal subject and a passive agent. Also, the 
predicator has two verbs, the verb  to be followed 
by  a  verb  in  the  past  participle  tense.  The 
simplification consists  in  reordering the sentence 
components,  turning  the  agent  into  subject 
(removing the  by preposition), turning the subject 
into direct  object and adjusting the predicator by 
removing the verb to be and re-inflecting the main 
verb. The new tense of the main verb is the same 
as  the  one  of  the  to  be  verb  and  its  number  is 
defined according to the new subject.
10 Literal translations from Portuguese result in some 
sentences appearing ungrammatical in English. 
O: As[The] transfer?ncias[transfers] 
foram[were:plural] feitas[made] pela[by the] 
empresa[company]. (1)
S: A[The] empresa[company] fez[made:sing] 
as[the] transfer?ncias[transfers].
Other correctly processed cases vary according 
the  number  of  verbs  (three  or  four),  special 
subjects, and special agents. For cases comprising 
three or four verbs, the simplification rule must re-
inflect11 two verbs (2) (one of them should agree 
with  the  subject  and  the  other  receives  its  tense 
from  the  verb  to  be).   There  are  two  cases  of 
special subjects. In the first case, a hidden subject 
is turned into a pronominal direct object (3). In the 
second  case,  a  pronominal  subject  must  be 
transformed to oblique case pronoun and then to 
direct  object.  Special  agents  also  represent  two 
cases. In the first one, oblique case pronouns must 
be  transformed before  turning  the  agent  into  the 
subject. In the second case (4), a non-existent agent 
is turned into an undetermined subject (represented 
here by ?they?).
O: A[The] porta[door] deveria[should] ter[have] 
sido[been] trancada[locked:fem] por[by] John. (2)S: John deveria[should] ter[have] 
trancado[locked:masc] a[the] porta[door].
O: [I] fui[was] encarregado[entrusted] por[by] 
minha[my] fam?lia[family]. (3)S: Minha[My] fam?lia[family] 
encarregou[entrusted] me[me].
O: O[The] ladr?o[thief] foi[was] pego[caught]. (4)
S: [They] pegaram[caught] o[the] ladr?o[thief].
Two cases  are  not  processed because they are 
already considered easy enough: the syndetic voice 
and passive in non-root sentences. In those cases, 
the  proposed  simplification  is  generally  less 
understandable  than  the  original  sentence. 
Sentences  with  split  predicator  (as  in  ?the 
politician was very criticized by his electors?) are 
not  processed  for  the  time  being,  but  should  be 
incorporated in the pipeline in the future.
Table  1 presents the algorithm used to process 
the  default  case  rule  and  verb  case  rules. 
Simplification rules are applied against all nodes in 
constituent tree, one node at a time, using breadth-
first traversing.
11 Some reinflections may not be visible on example 
translation.
140
Step Description
1 Validate these prerequisites or give up:
1.1     Node must be root
1.2     Predictor must have an inflection of auxiliary   
    verb to be
1.3     Main verb has to be in past participle
2 Transform subject into direct object
3 Fix the predicator
3.1 If main verb is finite then:
    main verb gets mode and tense from to be
    main verb gets person according to agent
3.2 Else:
    main verb gets mode and tense from verb to be
    finite verb gets person according to agent
3.3 Remove verb to be
4 Transform passive agent into a new subject
Table 1: Algorithm for default and verb cases
3.3 Subordination
Types of subordinate clauses are presented in Table 
2. Two clauses are not processed: comparative and 
proportional.  Comparative  and  proportional 
clauses will be addressed in future work.
id Clause type Processed
d Relative Restrictive ?
e Relative  Non-restrictive ?
f Reason ?
g Comparative  
h Concessive ?
i Conditional ?
j Result ?
k Confirmative ?
l Final Purpose ?
m Time ?
w Proportional
Table 2: Subordinate clauses
Specific  rules  are  used  for  groups  of  related 
subordinate cases. At least one of two operations 
can  be  found  in  all  rules:  component  reordering 
and sentence splitting. Below, letter codes are used 
to  describe  rules  involving  these  two  and  other 
common operations:
A additional processing
M splitting-order main-subordinate 
P Also processes non-clause phrases and/or non-
finite clauses
R component reordering 
S splitting-order subordinate-main 
c clone subject or turn object of a clause into 
subject in another if it is necessary
d marker deletion
m marker replacement
v verb reinflection
[na] not simplified due ambiguity
[nf] not simplified, future case
[np] not simplified due parsing problems
2...8 covered cases (when more than one applies)
Table  3 presents the marker information. They 
are used to select sentences for simplification, and 
several  of  them  are  replaced  by  easier  markers. 
Cases  themselves  are  not  detailed since they are 
too  numerous  (more  than  40  distinct  cases). 
Operation  codes  used  for  each  marker  are 
described in column ?Op?. It is important to notice 
that  multi-lexeme  markers  also  face  ambiguities 
due to co-occurrence of its component lexemes12. 
The  list  does  not  cover  all  possible  cases,  since 
there  may  be  additional  cases  not  seen  in  the 
corpus. As relative clauses (d and e) require almost 
the same processing, they are grouped together.
Several  clauses  require  additional  processing. 
For  example,  some  conditional  clauses  require 
negating the main clause. Other examples include 
noun phrases replacing clause markers and clause 
reordering, both for relative clauses, as showed in 
(5). The marker  cujo (whose) in the example can 
refer to Northbridge or to the building. Additional 
processing  is  performed  to  try  to  solve  this 
anaphora13,  mostly  using  number  agreement 
between the each possible co-referent and the main 
verb in the subordinate clause. The simplification 
engine can give up in ambiguous cases (focusing 
on  precision)  or  elect  a  coreferent  (focusing  on 
recall),  depending  on  the  number  of  possible 
coreferents  and  on  a  confidence  threshold 
parameter, which was not used in this paper.
O: Ele[He] deve[should] visitar[visit] o[the] 
pr?dio[building] em[in] Northbridge 
cujo[whose] desabamento[landslide] 
matou[killed] 16 pessoas[people].
(5)S: Ele[He] deve[should] visitar[visit] o[the] 
pr?dio[building] em[in] Northbridge. O[The] 
desabamento[landslide] do[of  the] 
pr?dio[building] em[in] Northbridge 
matou[killed] 16 pessoas[people].
12 For example, words ?de?, ?sorte? and ?que? can be 
adjacent  to each other without the meaning of ?de sorte 
que? marker (?so that?).
13 We opted to solve this kind of anaphora instead of using 
pronoun insertion in order to facilitate the reading of the 
text.
141
3.4 Evaluation in the development corpus
Figure 1 provides statistics from the of processing 
all  identified  cases  in  the  development  corpus. 
These statistics cover number of cases rather than 
the  number  of  sentences  containing  cases.  The 
cases  ?incorrect  selection?  and  ?incorrect 
simplification?  affect  precision  by  generating 
ungrammatical  sentences.  The  former  refers  to 
sentences  that  should  not  be  selected  for  the 
simplification  process,  while  the  latter  refers  to 
sentences  correctly  selected  but  wrongly 
simplified.  There  are  three  categories  affecting 
recall, classified according to their priority in the 
simplification  engine.  Pending cases  are 
considered  to  be  representative,  with  higher 
priority.  Possible cases  are  considered  to  be 
unrepresentative. Having less priority, they can be 
handled in future versions of the engine.  Finally, 
Skipped cases  will  not  be  implemented,  mainly 
because  of  ambiguity,  but  also  due  to  low 
representativeness.  It  is  possible  to  observe  that 
categories  reducing  precision  (incorrect  selection 
and simplification) represent a smaller number of 
cases (5%) than categories reducing recall (45%). 
It  is  worth  noticing  that  our  approach  focus  on 
precision  in  order  to  make  the  simplification  as 
automatic  as  possible,  minimizing  the  need  for 
human interaction.
Figure 1: Performance on the development 
corpus
There are some important remarks regarding the 
development corpus used during the programming 
phase.  First,  some  cases  are  not  representative, 
therefore  the  results  are  expected  to  vary 
significantly in real texts. Second, a few cases are 
not orthogonal: i.e.,  there are sentences that can be 
classified  in  more  than  one  case.  Third,  several 
errors  refer  to  sub-cases  of  cases  being  mostly 
correctly  processed,  which are  expected to occur 
less frequently. Fourth, incorrect parsed sentences 
were not take in account in this phase. Although 
there may exist other cases not identified yet, it is 
plausible to estimate that only 5% of known cases 
are affecting the precision negatively.
id Marker Op id Marker Op id Markers Op
de que [that/which] 8MRAdv h se bem que [albeit] Mmv j tanto ? que [so ? that] [nf]
de o qual [which]* 8MRAdv h ainda que [even if] 2Mm j tal ? que [such ? that] [nf]
de como [as] [na] h mesmo que [even if] 2Mm j tamanho ? que [so ? that]* [nf]
de onde [where] [nf] h nem que [even if] 2Mm k conforme [as/according] 3PRAcm
de quando [when] [na] h por mais que [whatever] 2Mm k consoante [as/according] 3PRAcm
de quem [who/whom] [nf] h mas [but] [np] k segundo [as/according] 3PRAcm
de quanto [how much] [nf] i contanto que [provided that] 2Rmv k como [as] [na]
de cujo [whose]* MAd i caso [case] 2Rmv l a fim de [in order to] 2PMcm
de o que [what/which] Sd i se [if/whether] 2Rmv l a fim de que [in order that] 2PMcm
f j? que [since] Scm i a menos que [unless] 2RAmv l para que [so that] 2PMcm
f porquanto [in view of] Scm i a n?o ser que [unless] 2RAmv l porque [because] [na]
f uma vez que [since] Scm i exceto se [unless] 2RAmv m assim que [as soon as] 5PMAcvr
f visto que [since] Scm i salvo se [unless] 2RAmv m depois de [after] 5PMAcvr
f como [for] [na] i antes que [before] Rmv m depois que [after] 5PMAcvr
f porque [because] [na] i sem que [without] Rmv m logo que [once] 5PMAcvr
f posto que [since] [na] i desde que [since] RAmv m antes que [before] PSAcvr
f visto como [seen as] [na] j de forma que [so] 5Mmv m apenas [only] [na]
f pois que [since] [nf] j de modo que [so] 5Mmv m at? que [until] [na]
h apesar de que [although] Mmv j de sorte que [so that] 5Mmv m desde que [since] [na]
h apesar que [despite] Mmv j tamanho que [so that]* 5Mmv m cada vez que [every time] [nf]
h conquanto [although] Mmv j tal que [such that] 5Mmv m sempre que [whenever] [nf]
h embora [albeit] Mmv j tanto que [so that] (1)* [na] m enquanto [while] [nf]
h posto que [since] Mmv j tanto que [so that] (2) [na] m mal [just] [na]
h por muito que [although] Mmv j t?o ? que [so ? that] [nf] m quando [when] [na]
* gender and/or number variation
Table 3: Marker processing
correct 45%
incorrect
simplification 4% incorrectselection 1%
pending 17%
possible 7%
skipped 25%
142
4 Engine evaluation
4.1 Evaluation patterns
The  evaluation  was  performed  on  a  sample  of 
sentences  extracted  from Wikipedia's  texts  using 
lexical patterns. These patterns allows to filter the 
texts,  extracting  only  relevant  sentences  for 
precision and recall evaluation. They were created 
to  cover  both  positive  and  negative  sentences. 
They are applied before parsing or Part of Speech 
(PoS)  analysis.  For  passive  voice  detection,  the 
pattern is  defined as a sequence of  two or  more 
possible verbs (no PoS in use) in which at least one 
of them could be an inflection of verb to be. For 
subordination detection, the pattern is equivalent to 
the  discourse  markers  associated  with  each 
subordination type, as shown in Table 3. 
The  patterns  were  applied  against  featured 
articles  appearing  in  Wikipedia's  front  page  in 
2010 and 2011, including featured articles planned 
to be featured, but not featured yet. A maximum of 
30 sentences resulting from each pattern matching 
were then submitted to the simplification engine. 
Table 4 presents statistics from featured articles. 
texts 165
sentences 83,656
words 1,226,880
applied patterns 57,735
matched sentences 31,080
Table 4: Wikipedia's featured articles (2010/2011)
The number of applied patterns represents both 
patterns to be simplified (s-patterns) and patterns 
not  to  be  simplified  (n-patterns).  N-patterns 
represent both non-processable patterns due to high 
ambiguity (a-patterns) and pattern extraction false 
negatives. We observed a few, but very frequent, 
ambiguous patterns introducing noise, particularly 
se and  como.  In  fact,  these  two  markers  are  so 
noisy that  we were not  be able  to  provide good 
estimations  on  their  true  positives  distribution 
given the 30 sentences limit per pattern. Similarly 
to the number of applied patterns, the number of 
matched sentences correspond to both sentences to 
be simplified and not to be simplified.
Table  5 presents  additional  statistics  about 
characters,  words  and  sentences  calculated  in  a 
sample of 32 articles where the 12 domains of the 
Portuguese Wikipedia are balanced. The number of 
automatic simplified sentence is also presented. In 
Table  5,  simple  words refers  to  percentage  of 
words  which  are  listed  on  our  simple  word  list, 
supposed to be common to youngsters,  extracted 
from the dictionary described in (Biderman, 2005), 
containing 5,900 entries.  Figure 2 presents clause 
distribution per sentence in  the balanced sample. 
Zero  clauses refers  to  titles,  references,  figure 
labels, and other pieces of text without a verb. We 
observed  60%  of  multi-clause  sentences  in  the 
sample.
characters per word 5.22
words per sentence 21.17
words per text 8,476
simple words 75.52%
sentences per text 400.34
passive voice 15.11%
total sentences 13,091
simplified sentences 16,71%
Table 5: Statistics from the balanced text sample
Figure 2: Clauses per sentence in the sample
4.2 Simplification analysis
We manually analysed and annotated all sentences 
in  our  samples.  These  samples  were  used  to 
estimate  several statistics, including the number of 
patterns  per  million  words,  the  system precision 
and  recall  and  the  noise  rate.  We  opted  for 
analysing  simplified  patterns  per  million  words 
instead of  per simplified sentences. First, because 
an analysis based on sentences can be misleading, 
since  there  are  cases  of  long  coordinations  with 
many patterns, as well as succinct sentences with 
no patterns.  Moreover,  one incorrectly  simplified 
marker in a sentence could hide useful statistics of 
correctly  simplified  patterns  and  even  of  other 
incorrectly simplified patterns. 
The samples are composed by s-patterns and n-
patterns  (including  a-patterns).  In  total  1,243 
patterns were annotated.  Table  6 presents pattern 
estimates per million words. 
0 1 2 3 4 5 6 7 8
0,0000
0,0500
0,1000
0,1500
0,2000
0,2500
0,3000
clauses
dis
trib
uti
on
 [0
-1]
>7
143
Total patterns 70,834
Human s-patterns 33,906
Selection s-patterns 27,714
Perfect parser s-patterns 23,969
Obtained s-patterns 22,222
Table 6: Patterns per million words
Total patterns refers to the expected occurrences 
of  s-patterns  and  n-patterns  in  a  corpus  of  one 
million  words.  This  is  the  only  information 
extracted from the full corpus, while the remaining 
figures are estimates from the sample corpus.
Human s-patterns is an estimate of the number 
patterns that a human could simplify in the corpus. 
Unlike  other  s-pattern  estimates,  a-patterns  are 
included, since a human can disambiguate them. In 
other words, this is the total of positive patterns. 
The estimate  does  not  include very rare  (sample 
size equals to zero) or very noisy markers (patterns 
presenting 30 noisy sentences in its sample).
Selection  s-patterns are  an  estimate  of  the 
number  of  patterns  correctly  selected  for 
simplification,  regardless  of  whether  the  pattern 
simplification is correct or incorrect.  Precision and 
recall derived from this measure (Table 7) consider 
incorrectly simplified patterns,  and do not include 
patterns with parsing problems.  Its  purpose is  to 
evaluate how well the selection for simplification 
is performed. Rare or noisy patterns, whose human 
s-patterns  per  sample  is  lower  than  7,  are  not 
included.
Perfect  parser  s-patterns is  an  estimate  very 
similar to selection s-patterns, but considering only 
correctly  simplified  patterns.  As  in  selection  s-
patterns,  incorrect  parsed  sentences  are  not 
included in calculations. This is useful to analyse 
incorrect simplifications due to simplification rule 
problems, ignoring errors originating from parsing.
Finally,  obtained  s-patterns refers  to  the 
estimate of correct simplified patterns,  similar to 
perfect  parser  s-patterns,  but  including 
simplification  problems  caused  by  parsing.  This 
estimate  represents  the  real  performance  to  be 
expected from the system on Wikipedia's texts.
It is important to note that the real numbers of 
selection  s-patterns,  perfect  s-patterns  and 
obtained s-patterns  is expected to be bigger than 
the estimates,  since noisy and rare  pattern could 
not used be used in calculations (due the threshold 
of  7  human  s-patterns  per  sample).  The  data 
presented on Table 6 is calculated using estimated 
local precisions for each pattern. Table  7 presents 
global  precision,  recall  and  f-measure  related  to 
selection,  perfect  parser  and  obtained s-patterns. 
The  real  values  of  the  estimates  are  expected to 
variate up to +/- 2.48% .
Measures Precision Recall F-measure
Selection 99.05% 82.24% 89.86%
Perfect parser 85.66% 82.24% 83.92%
Obtained 79.42% 75.09% 77.20%
Table 7: Global estimated measures
Although the precision of the selection seems to 
be  impressive,  this  result  is  expected,  since  our 
approach  focus  on  the  processing  of  mostly 
unambiguous  markers,  with  sufficient  syntactic 
information. It is also due to the the threshold of 7 
human s-patterns  and the fact  that  a-patterns  are 
not  included.  Due to  these two restrictions,  only 
approximately 31.5% of unique patterns could be 
used for the calculations in Table  7. Interestingly, 
these unique patterns correspond to 82.5% of the 
total estimated human s-patterns. The majority of 
the 17.5%  remaining s-patterns refers to patterns 
too  noisy  to  be  analysed  and  to  a-patterns  (not 
processed  due  ambiguity),  and  also  others  n-
patterns which presented a low representativeness 
in  the  corpus.  The  results  indicate  good 
performance in rule formulation, covering the most 
important (and non-ambiguous) markers, which is 
also confirmed by the ratio between both selection 
s-patterns  and  human  s-patterns  previously 
presented on Table 6. 
An  alternative  analysis,  including  a-patterns, 
lowers recall and f-measure, but not precision (our 
focus in this work). In this case, recall drops from 
75.09% to  62.18%,  while  f-measure  drops  from 
77.20% to 70.18%.
Figure 3: Pattern distribution
Figure  3 presents  the  distribution  of  patterns 
according to their frequency per million words and 
their purity (1 - noisy rate). This data is useful to 
2,0 20,0 200,0 2000,0 20000,0 200000,0
0,0000
0,2000
0,4000
0,6000
0,8000
1,0000
1,2000
b-passiva
de-que
l-a_fim_de
j-tal_que
J-tanto_*_que
Frequency PMW
Pu
rity
144
identify  most  frequent  patterns  (such  as  passive 
voice in  b-passiva)  and patterns with medium to 
high  frequency,  which  are  easy  to  process  (not 
ambiguous), such as l-a_fim_de.
5 Issues on simplification quality
This analysis aims at identifying factors affecting 
the quality of simplifications considered as correct. 
Hence, factors affecting the overall simplified text 
quality  are  also  presented.  In  contrast,  the 
quantitative  analysis  presented  on  Section  4.2 
covered  the  ratio  between  incorrect  and  correct 
simplifications.
Three cases of clause disposition were identified 
as  important  factors  affecting  the  simplified 
sentence  readability.  These  cases  are  presented 
using  the  following  notation:  clauses  are 
represented  in  uppercase  letters;  clause 
concatenation represents coordination; parentheses 
represent  subordination;  c1 and  c2 represent 
clause/sentence  connectors  (including  markers); 
the  entailment  operator  (?)  represents  the 
simplification rule transforming clauses.
? ?A(B(c1 C)) ? A(B). c2 C?: the vertical case. 
In this scenario it is more natural to read c2 as 
connecting  C to the main clause  A, while  c1 
connects  C to  B,  as seen in (6). This is still 
acceptable for several sentences analysed, but 
we  are  considering to  simplify only level  2 
clauses in the future, splitting C  from B only 
if another rule splits A and B first.
? ?A(B)CD  ?  ACD.  c1 B?:  the  horizontal 
case. In this scenario, c1 correctly connects A 
and B, but long coordinations following A can 
impact  negatively on text  reading,  since the 
target  audience  may  forget  about  A when 
starting  to  read  B.  In  this  scenario, 
coordination  compromise  subordination 
simplification,  showing  the  importance  of 
simplifying coordination as well, even though 
they  are  considered  easier  to  read  than 
subordination.
? Mixed  case:  this  scenario  combines  the 
potential problems of horizontal and vertical 
cases.  It  may  occur  in  extremely  long 
sentences.
Besides  clause  disposition  factors,  clause 
inversions can also lead to  problems in sentence 
readability.  In  our  current  system,  inversion  is 
mainly used to produce simplified sentences in the 
cause-effect  order  or  condition-action  order. 
Reordering, despite using more natural orders, can 
transform  anaphors  into  cataphors.  A  good 
anaphora resolution system would be necessary to 
avoid  this  issue.  Another  problem  is  moving 
sentence connectors as in ?A. c1 BC. ? A. B. c2 c1 
C?,  while  ?A.  c1 B.  c2 C?  is  more  natural 
(maintaining c1 position). 
O: Ela[She] dissertou[talked] sobre[about] 
como[how] motivar[to motive] o[the] 
grupo[group] de_modo_que[so that] seu[their] 
desempenho[performance] melhore[improves] (6)S: [He/She] dissertou[talked] sobre[about] 
como[how] motivar[to motive] o[the] 
grupo[group]. Thus, seu[their] 
desempenho[performance] melhore[improves]
We  have  observed  some  errors  in  sentence 
parsing,  related  to  clause  attachment,  generating 
truncated ungrammatical text. As a result, a badly 
simplified key sentence can compromise the text 
readability more than several  correctly simplified 
sentences  can  improve  it,  reinforcing  the 
importance  of  precision  rather  than  recall  in 
automated text simplification.
Experienced  readers  analysed  the  simplified 
versions of the articles and considered them easier 
to read than the original ones in most cases, despite 
simplification  errors.  Particularly,  the  readers 
considered  that  the  readability  would  improve 
significantly  if  cataphor  and horizontal  problems 
were  addressed.  Evaluating  the  simplifications 
with readers from the target audience is left  as a 
future work, after  improvements in the identified 
issues.
6 Conclusions
We  have  presented  a  simplification  engine  to 
process texts from the Portuguese Wikipedia. Our 
quantitative  analysis  indicated  a  good  precision 
(79.42%),  and  reasonable  number  of  correct 
simplifications  per  million  words  (22,222). 
Although our focus was on the encyclopedic genre 
evaluation,  the  proposed  system  can  be  used  in 
other genres as well.
Acknowledgements
We thank FAPESP (p.  2008/08963-4)  and CNPq 
(p. 201407/2010-8) for supporting this work.
145
References
J.  Abedi,  S.  Leon,  J.  Kao,  R.  Bayley,  N.  Ewers,  J. 
Herman and K. Mundhenk. 2011. Accessible Reading 
Assessments for Students with Disabilities: The Role 
of  Cognitive,  Grammatical,  Lexical,  and 
Textual/Visual  Features.  CRESST  Report  785. 
National  Center  for  Research  on  Evaluation, 
Standards,  and  Student  Testing,  University  of 
California, Los Angeles.
S.  M.  Alu?sio,   C.  Gasperin.  2010.  Fostering  Digital 
Inclusion and Accessibility: The PorSimples project 
for Simplification of Portuguese Texts.  Proceedings 
of  the  NAACL  HLT  2010  Young  Investigators 
Workshop  on  Computational  Approaches  to 
Languages of the Americas. : ACL, New York, USA. 
v. 1. p. 46-53.
A.  Barreiro,  L.  M.  Cabral.  2009.  ReEscreve:  a 
translator-friendly  multi-purpose  paraphrasing 
software  tool.  The  Proceedings  of  the  Workshop 
Beyond  Translation  Memories:  New  Tools  for 
Translators,  The  Twelfth  Machine  Translation 
Summit. Ontario, Canada, pp. 1-8. 
E.  Bick.  2006.   The  parsing  system  ?Palavras?: 
Automatic grammatical  analysis of  Portuguese in a 
constraint  grammar  framework.  Thesis  (PhD). 
University of ?rhus, Aarhus, Denmark.
M.  T.  C.  Biderman.   2005.  Dicion?rio  Ilustrado  de 
Portugu?s. Editora ?tica. 1a. ed. S?o Paulo 
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin 
and  J.  Tait.  1999.  Simplifying  Text  for  Language-
Impaired  Readers,.  In  Proceedings  of  the  9th 
Conference  of  the  European  Chapter  of  the 
Association for Computational  Linguistics  (EACL), 
269-270.
R.  Chandrasekar  and  B.  Srinivas.  1997.  Automatic 
Induction  of  Rules  for  Text  Simplification. 
Knowledge-Based Systems, 10, 183-190.
G. E. Chappell.  1985. Description and assessment of 
language disabilities of junior high school students. 
In:  Communication  skills  and  classroom  success: 
Assessment  of  language-learning  disabled  students. 
College- Hill Press,  San Diego,  pp. 207-239.
W.  Daelemans,  A.  Hothker  and  E.  T.  K.  Sang.  2004. 
Automatic Sentence Simplification for Subtitling in 
Dutch  and  English.  In:  Proceedings  of  the  4th 
Conference on Language Resources and Evaluation, 
Lisbon, Portugal 1045-1048.
J. De Belder and M. Moens. 2010.  Text simplification 
for children. Proceedings of the SIGIR Workshop on 
Accessible Search Systems, pp.19-26.
S.  Devlin  and  G.  Unthank.  2006.  Helping  aphasic 
people process online information. In: Proceedings of 
the  ACM  SIGACCESS  2006,  Conference  on 
Computers  and  Accessibility.  Portland,  Oregon, 
USA , 225-226.
K. Inui, A. Fujita, T. Takahashi, R. Iida and T. Iwakura. 
2003.  Text Simplification for Reading Assistance: A 
Project  Note.  In  the  Proceedings  of  the  Second 
International Workshop on Paraphrasing, 9-16.
S.  Jonnalagadda  and  G.  Gonzalez.  2009.  Sentence 
Simplification  Aids  Protein-Protein  Interaction 
Extraction.  Proceedings  of  the  3rd  International 
Symposium on Languages in Biology and Medicine, 
Short  Papers,  pages  109-114,  Jeju  Island,  South 
Korea, 8-10 November 2009.
F.  W.  Jones,  K.  Long  and  W.  M.  L.  Finlay.  2006. 
Assessing the reading comprehension of adults with 
learning disabilities. Journal of Intellectual Disability 
Research, 50(6), 410-418. 
B.  Klebanov,  K.  Knight  and  D.  Marcu.  2004.  Text 
Simplification for Information-Seeking Applications. 
In:  On  the  Move  to  Meaningful  Internet  Systems. 
Volume  3290,  Springer-Verlag,  Berlin  Heidelberg 
New York, 735-747.
S. Martins, L. Filgueiras. 2007. M?todos de Avalia??o 
de Apreensibilidade das Informa??es Textuais:  uma 
Aplica??o  em  S?tios  de  Governo  Eletr?nico.  In 
proceeding  of  Latin  American  Conference  on 
Human-Computer Interaction (CLIHC 2007). Rio de 
Janeiro, Brazil.
A. Max. 2006. Writing for Language-impaired Readers. 
In: Proceedings of Seventh International Conference 
on  Intelligent  Text  Processing  and  Computational 
Linguistics. Mexico City, Mexico. Berlin Heidelberg 
New York, Springer-Verlag, 567-570.
C.  Napoles  and  M.  Dredze.  2010.  Learning  simple 
Wikipedia:  a  cogitation  in  ascertaining  abecedarian 
language. In the  Proceedings of the NAACL HLT 
2010  Workshop  on  Computational  Linguistics  and 
Writing:  Writing  Processes  and  Authoring  Aids 
(CL&W '10), 42-50.
S.  E.  Petersen.  2007.  Natural  Language  Processing 
Tools  for  Reading  Level  Assessment  and  Text 
Simplification for  Bilingual  Education.  PhD thesis. 
University of Washington.
A. Siddharthan. 2006. Syntactic simplification and text 
cohesion.  Research  on  Language  &  Computation, 
4(1):77-109.
L.  Specia.   2010.  Translating  from  Complex  to 
Simplified  Sentences.  9th  International  Conference 
146
on  Computational  Processing  of  the  Portuguese 
Language.  Lecture  Notes  in  Artificial  Intelligence, 
Vol. 6001, Springer, pp. 30-39.
D. Vickrey and D. Koller. 2008. Sentence Simplification 
for Semantic Role Labelling. In: Proceedings of the 
ACL-HLT. 344-352.
W. M. Watanabe,  A. Candido Jr, V. R. Uzeda, R. P. M. 
Fortes,  T.  A.  S.  Pardo  and  S.  M.  Alu?sio.  2009. 
Facilita:  Reading  Assistance  for  Low-literacy 
Readers.  In:  ACM  International  Conference  on 
Design of Communication (SIGDOC 2009), volume 
1, Bloomington, US,   29-36. 
147
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 45?53,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Exciting and interesting: issues in the generation of binomials
Ann Copestake
Computer Laboratory,
University of Cambridge,
15 JJ Thomson Avenue,
Cambridge, CB3 0FD, UK
ann.copestake@cl.cam.ac.uk
Aure?lie Herbelot
Institut fu?r Linguistik,
Universita?t Potsdam,
Karl-Liebknecht-Stra?e 24-25
D-14476 Golm, Germany
herbelot@uni-potsdam.de
Abstract
We discuss the preferred ordering of elements
of binomials (e.g., conjunctions such as fish
and chips, lager and lime, exciting and in-
teresting) and provide a detailed critique of
Benor and Levy?s probabilistic account of En-
glish binomials. In particular, we discuss the
extent to which their approach is suitable as
a model of language generation. We describe
resources we have developed for the investi-
gation of binomials using a combination of
parsed corpora and very large unparsed cor-
pora. We discuss the use of these resources in
developing models of binomial ordering, con-
centrating in particular on the evaluation is-
sues which arise.
1 Introduction
Phrases such as exciting and interesting and gin and
tonic (referred to in the linguistics literature as bi-
nomials) are generally described as having a seman-
tics which makes the ordering of the conjuncts irrel-
evant. For instance, exciting and interesting might
correspond to exciting?(x)? interesting?(x) which is
identical in meaning to interesting?(x)?exciting?(x).
However, in many cases, the binomial is realized
with a preferred ordering, and in some cases this
preference is so strong that the reverse is perceived
as highly marked and may even be difficult to under-
stand. For example, tonic and gin has a corpus fre-
quency which is a very small fraction of that of gin
and tonic. Such cases are referred to as irreversible
binomials, although the term is sometimes used
only for the fully lexicalised, non-compositional ex-
amples, such as odds and ends.
Of course, realization techniques that utilize very
large corpora to decide on word ordering will tend to
get the correct ordering for such phrases if they have
been seen sufficiently frequently in the training data.
But the phenomenon is nevertheless of some practi-
cal interest because rare and newly-coined phrases
can still demonstrate a strong ordering preference.
For instance, the ordering found in the names of
mixed drinks, where the alcoholic component comes
first, applies not just to the conventional examples
such as gin and tonic, but also to brandy and coke,
lager and lime, sake and grapefruit and (hopefully)
unseen combinations such as armagnac and black-
currant.1 A second issue is that data from an un-
parsed corpus can be misleading in deciding on bi-
nomial order. Furthermore, our own interest is pre-
dominantly in developing plausible computational
models of human language generation, and from this
perspective, using data from extremely large cor-
pora to train a model is unrealistic. Binomials are
a particularly interesting construction to look at be-
cause they raise two important questions: (1) to what
extent does lexicalisation/establishment of phrases
play a role in determining order? and (2) is a detailed
lexical semantic classification required to accurately
predict order?
As far as we are aware, the problem of developing
a model of binomial ordering for language genera-
tion has not previously been addressed. However,
Benor and Levy (2006) have published an important
and detailed paper on binomial ordering which we
draw on extensively in this work. Their research
has the objective of determining how the various
constraints which have been proposed in the lin-
guistic literature might interact to determine bino-
1One of our reviewers very helpfully consulted a bartender
about this generalization, and reports the hypothesis that the al-
cohol always comes first because it is poured first. However,
there is the counter-example gin and bitters (another name for
pink gin), where the bitters are added first (unless the drink is
made in a cocktail shaker, in which case ordering is irrelevant).
45
mial ordering as observed in a corpus. We present
a critical evaluation of that work here, in terms of
the somewhat different requirements for a model for
language generation.
The issues that we concentrate on in this paper
are necessary preliminaries to constructing corpus-
based models of binomial reversibility and ordering.
These are:
1. Building a suitable corpus of binomials.
2. Developing a corpus-based technique for eval-
uation.
3. Constructing an initial model to test the evalu-
ation methodology.
In ?2, we provide a brief overview of some of
the factors affecting binomial ordering and discuss
Benor and Levy?s work in particular. ?3 discusses
evaluation issues and motivates some of the deci-
sions we made in deciding on the resources we have
developed, described in ?4. ?5 illustrates the evalua-
tion of a simple model of binomial ordering.
2 Benor and Levy?s account
We do not have space here for a proper discussion of
the extensive literature on binomials, or indeed for a
full discussion of Benor and Levy?s paper (hence-
forth B+L) but instead summarise the aspects which
are most important for the current work.
For convenience, we follow B+L in referring to
the elements of an ordered binomial as A and B.
They only consider binomials of the form ?A and
B? where A and B are of the same syntactic cate-
gory. Personal proper names were excluded from
their analysis. Because they required tagged data,
they used a combination of Switchboard, Brown and
the Wall Street Journal portion of the Penn Treebank
to extract binomials, selecting 411 binomial types
and all of the corresponding tokens (692 instances).
B+L investigate a considerable number of con-
straints on binomial ordering which have been dis-
cussed in the linguistics literature. They group the
features they use into 4 classes: semantic, word
frequency, metrical and non-metrical phonological.
We will not discuss the last class here, since they
found little evidence that it was relevant once the
other features were taken into account. The metri-
cal constraints were lapse (2 consecutive weak syl-
lables are generally avoided), length (A should not
have more syllables than B) and stress (B should not
have ultimate (primary) stress: this feature was actu-
ally found to overlap almost entirely with lapse and
length). The frequency constraint is that B should
not be more frequent than A, based on corpus spe-
cific counts of frequency (unsurprisingly, frequency
correlates with the length feature).
The semantic constraints are less straightforward
since the linguistics literature has discussed many
constraints and a variety of possible generalisations.
B+L use:
Markedness Divided into Relative formal,
which includes cases like flowers and roses
(more general term first) among others and
Perception-based, which is determined by
extra-linguistic knowledge, including cases
like see and hear (seeing is more salient).
B should not be less marked than A. Un-
fortunately markedness is too complex to
summarise adequately here. It is clear that it
overlaps with other constraints in some cases,
including frequency, since unmarked terms
tend to be more frequent.
Iconicity Sequence ordering of events, numbered
entities and so on (e.g., shot and killed, eighth
and ninth). If there is such a sequence, the bi-
nomial ordering should mirror it.
Power Power includes gender relationships (dis-
cussed below), hierarchical relationships (e.g.,
clergymen and parishioners), the ?condiment
rule? (e.g., fish and chips) and so on. B should
not be more powerful than A.
Set Open Construction This is used for certain
conventional cases where a given A may occur
with multiple Bs: e.g., nice and.
Pragmatic A miscellaneous context-dependent
constraint, used, for instance, where the
binomial ordering mirrors the ordering of other
words in the sentence.
B+L looked at the binomials in sentential context
to assign the semantic constraints. The iconicity
46
constraint, in particular, is context-dependent. For
example, although the sequence ninth and eighth
looks as though it violates iconicity, we found that
a Google search reveals a substantial number of in-
stances, many of which refer to the ninth and eighth
centuries BC. In this case, iconicity is actually ob-
served, if we assume that temporal ordering deter-
mines the constraint, rather than the ordering of the
ordinals.
The aspect of binomials which has received most
attention in the literature is the effect of gender:
words which refer to (human) males tend to pre-
cede those referring to females. For instance (with
Google 3-gram percentages for binomials with the
masculine term first): men and women (85%), boys
and girls (80%), male and female (91%) (exceptions
are father and mother (51%) and mothers and fa-
thers (33%)). There is also an observed bias towards
predominantly male names preceding female names.
B+L, following previous authors, take gender as an
example of the Power feature. For reasons of space
we can only touch on this issue very superficially,
but it illustrates a distinction between semantic fea-
tures which we think important. Iconicity generally
refers to a sequence of real world events or enti-
ties occuring in a particular order, hence its context-
dependence. For verbs, at least, there is a truth con-
ditional effect of the ordering of the binomial: shot
and killed does not mean the same thing as killed
and shot. Power, on the other hand, is supposed to
be about a conventional relationship between the en-
tities. Even if we are currently more interested in
chips rather than fish or biscuits rather than tea, we
will still tend to refer to fish and chips and tea and
biscuits. The actual ordering may depend on cul-
ture,2 but the assumption is that, within a particular
community, the power relationship which the bino-
mial ordering depends on is fixed.
B+L analyse the effects of all the features in de-
tail, and look at a range of models for combining
features, with logistic regression being the most suc-
cessful. This predicts the ordering of 79.2% of the
binomial tokens and 76.7% of the types. When se-
mantic constraints apply, they tend to outrank the
metrical constraints. B+L found that iconicity, in
2Our favourite example is an English-French parallel text
where the order of Queen Elizabeth and President Mitterand is
reversed in the French.
particular, is a very strong predictor of binomial or-
der.
B+L?s stated assumption is that a speaker/writer
knows they want to generate a binomial with the
words A and B and decides on the order based on
the words and the context. It is this order that they
are trying to predict. Of course, it is clear that some
binomials are non-compositional multiword expres-
sions (e.g., odds and ends) which are listed in con-
ventional dictionaries. These can be thought of as
?words with spaces? and, we would argue that the
speaker does not have a choice of ordering in such
cases. B+L argue that using a model which listed
the fixed phrases would be valid in the prediction of
binomial tokens, but not binomial types. We do not
think this holds in general and return to the issue in
?3.
B+L?s work is important in being the first account
which examines the effect of the postulated con-
straints in combination. However, from our perspec-
tive (which is of course quite different from theirs),
there are a number of potential problems. The first is
data sparsity: the vast majority of binomial types in
their data occur only once. It is impossible to know
whether both orderings are frequent for most types.
Furthermore, the number of binomial types is rather
small for full investigation of semantic features: e.g.,
Power is marked on only 26 types. The second is-
sue is that the combined models which B+L exam-
ine are, in effect, partially trained on the test data, in
that the relative contribution of the various factors is
optimized on the test data itself. Thirdly, the seman-
tic factors which B+L consider have no independent
verification: they were assigned by the authors for
the binomials under consideration, a methodology
which makes it impossible to avoid the possibility of
bias. There was some control over this, in that it was
done independently by the two authors with subse-
quent discussion to resolve disagreements. How-
ever, we think that it would be hard to avoid the
possibility of bias in the ?Set open? and ?Pragmatic?
constraints in particular. Some of the choices seem
unintuitive: e.g., we are unsure why there is a Power
annotation on broccoli and cauliflower, and why go
and vote would be marked for Iconicity while went
and voted is not. It seems to us that the defini-
tion of some of these semantic factors in the liter-
ature (markedness and power in particular) is suf-
47
ficiently unclear for reproducible annotation of the
type now expected in computational linguistics to be
extremely difficult.
Both for practical and theoretical reasons, we are
interested in investigating alternative models which
rely on a corpus instead of explicit semantic fea-
tures. Native speakers are aware of some lexicalised
and established binomials (see (Sag et al 2002) for a
discussion of lexicalisation vs establishment in mul-
tiword expressions), and will tend to generate them
in the familiar order. Instead of explicit features be-
ing learned for the unseen cases, we want to investi-
gate the possible role of analogy to the known bino-
mials. For instance, if tea and biscuits is known,
coffee and cake might be generated in that order-
ing by semantic analogy. The work presented in
this paper is essentially preparatory to such experi-
ments, although we will discuss an extremely simple
corpus-based model in ?5.
3 Evaluating models of binomial ordering
In this section, we discuss what models of binomial
ordering should predict and how we might evaluate
those predictions.
The first question is to decide precisely what we
are attempting to model. B+L take the position that
the speaker/writer has in mind the two words of the
binomial and chooses to generate them in one order
or other in a particular context, but this seems prob-
lematic for the irreversible binomials and, in any
case, is not directly testable. Alternatively we can
ask: Given a corpus of sentences where the binomi-
als have been replaced with unordered pairs of AB,
can we generate the ordering actually found? Both
of these are essentially token-based evaluations, al-
though we could additionally count binomial types,
as B+L do.
One problem with these formulations is that, to do
them justice, our models would really have to incor-
porate features from the surrounding context. Fac-
tors such as postmodification of the binomial affect
the ordering. This type of evaluation would clearly
be the right one if we had a model of binomials in-
corporated into a general realisation model, but it is
not clear it is suitable for looking at binomials in iso-
lation.
Perhaps more importantly, to model the irre-
versible or semi-irreversible binomials, we should
take into account the order and degree of reversibil-
ity of particular binomial types. It seems problem-
atic to formulate the generation of a lexicalised bino-
mial, such as odds and ends, as a process of deciding
on the order of the components, since the speaker
must have the term in mind as a unit. In terms
of the corpus formulation, given the pair AB, the
first question in deciding how to realise the phrase
is whether the order is actually fixed. The case
of established but compositional binomials, such as
fish and chips, is slightly less clear, but there still
seem good grounds for regarding it as a unit (Cruse,
1986). Furthermore, in evaluating a token-based re-
alisation model, we should not penalise the wrong
ordering of a reversible binomial as severely as if
the binomial were irreversible. From these perspec-
tives, developing a model of ordering of binomial
types should be a preliminary to developing a model
of binomial tokens. Context would be important in
properly modelling the iconicity effect, but is less
of an issue for the other ordering constraints. And
even though iconicity is context-dependent, there is
a very strongly preferred ordering for many of the
binomial types where iconicity is relevant.
Thus we argue that it is appropriate to look at the
question: Given two words A, B which can be con-
joined, what order do we find most frequently in a
corpus? Or, in order to look at degree of reversibil-
ity: What proportion of the two orderings do we find
in a corpus? This means that we require relatively
large corpora to obtain good estimates in order to
evaluate a model.
Of course, if we are interested in analogical mod-
els of binomial ordering, as mentioned at the end of
?2, we need a reasonably large corpus of binomials
to develop the model. Ideally this should be a dif-
ferent corpus from the one used for evaluation. We
note that some experiments on premodifier order-
ing have found a considerable drop in performance
when testing on a different domain (Shaw and Hatzi-
vassiloglou, 1999). Using a single corpus split into
training and test data would, of course, be problem-
atic when working with binomial types. We have
thus developed a relatively novel methodology of us-
ing an automatically parsed corpus in combination
with frequencies from Web data. This is discussed
in the next section.
48
4 Binomial corpora and corpus
investigation
In this section, we describe the resources we have
developed for investigating binomials and address-
ing some of the evaluation questions introduced in
the previous section. We then present an initial anal-
ysis of some of the corpus data.
4.1 Benor and Levy data
The appendix of B+L?s paper3 contains a list of the
binomials they looked at, plus some of their markup.
Although the size of the B+L dataset is too small
for many purposes, we found it useful to consider
it as a clean source of binomial types for our initial
corpus investigation and evaluation. We produced a
version of this list excluding the 10 capitalised ex-
amples: some of these seem to arise from sentence
initial capitals while others are proper names which
we decided to exclude from this study. We produced
a manually lemmatised version of the list, which re-
sults in a slightly reduced number of binomial types:
e.g., bought and sold and buy and sell correspond to
a single type. The issue of lemmatisation is slightly
problematic in that a few examples are lexicalised
with particular inflections, such as been and gone.
However, our use of parsed data meant that we had
to use lemmatization decisions which were compat-
ible with the parser.
4.2 Wikipedia and the Google n-gram corpus
In line with B+L, we assume that binomials are
made of two conjuncts with the same part of speech.
It is not possible to use an unparsed corpus to ex-
tract such constructions automatically: first, the raw
text surrounding a conjunction may not correspond
to the actual elements of the coordination (e.g., the
trigram dictionary and phrase in She bought a dic-
tionary and phrase book); second, the part of speech
information is not available. Using a parsed corpus,
however, has disadvantages: in particular, it limits
the amount of data available and, consequently, the
number of times that a given type can be observed.
In this section, we discuss the use of Wikipedia,
which is small enough for parsing to be tractable but
3http://idiom.ucsd.edu/?rlevy/papers/
binomials-sem-alpha-formatted
which turns out to have a fairly representative distri-
bution of binomials. The latter point is demonstrated
by comparison with a large dataset: the Google n-
gram corpus (Brants and Franz, 2006). Although
the Google data is not suitable for the actual task
of extracting binomials, because it is not parsed, we
hypothesize it is usable to predict the preferred or-
der of a given binomial and to estimate the extent to
which it is reversible.
In order to build a corpus of binomials, we process
the parsed Wikipedia dump produced by Kummer-
feld et al(2010). The parse consists of grammatical
relations of the following form:
(gr word1 x word2 y ... wordn z)
where gr is the name of the grammatical relation,
word1...n are the arguments of the relation, and
x, y...z are the positions of the arguments in the sen-
tence. The lemmatised forms of the arguments, as
well as their part of speech, are available separately.
We used the first one million and coordinations in
the corpus in these experiments. The conjuncts are
required to have the same part of speech and to di-
rectly precede and follow the coordination. The lat-
ter requirement ensures that we retrieve true binomi-
als (phrases, as opposed to distant coordinates). For
each binomial in this data, we record a frequency
and whether it is found in the reverse order in the
same dataset. The frequency of the reverse ordering
is similarly collected. Since we intend to compare
the Wikipedia data to a larger, unparsed corpus, we
merge the counts of all possible parts of speech for
a given type in a given ordering, so the counts for
European and American as nouns and as adjectives,
for instance, are added together. We also record
the preferred ordering (the one with the highest fre-
quency) of the binomial and the ratio of the frequen-
cies as an indication of (ir)reversibility. In line with
our treatment of the B+L data, we disregarded the
binomials that coordinate proper names, but noted
that a large proportion of proper names found in
the Wikipedia data cannot be found in the Google
data.4 The Google corpus also splits (most) hyphen-
4This suggests that the Google n-gram corpus does not con-
tain much (if any) of the Wikipedia data: the particular dump
of Wikipedia from which the parsed data is extracted being in
any case several years later than the date that the Google n-gram
corpus was produced.
49
ated words. Since hyphenation is notoriously irreg-
ular in English, we disregarded all binomials con-
taining hyphenated words. The resulting data con-
tains 279136 unique binomial types. Around 7600
of those types have a frequency of 10 or more in our
Wikipedia subset. As expected, this leaves a large
amount of data with low frequency.
We then attempt to verify how close the sparse
Wikipedia data is to the Google 3-gram corpus. For
each binomial obtained from Wikipedia, we retrieve
the frequency of both its orderings in the Google
data and, as before, calculate the ratio of the frequen-
cies in the larger corpus. The procedure involves
converting the lemmatised forms in the Wikipedia
parse back into surface forms. Rather than using
a morphological generator, which would introduce
noise in our data, we search for the surface forms as
they appeared in the original Wikipedia data, as well
as for the coordinated base forms (this ensures high
recall in cases where the original frequency is low).
So for example, given the one instance of the bino-
mial ?sadden and anger? in Wikipedia, appearing as
Saddened and angered in the corpus, we search for
Saddened and angered, sadden and anger and anger
and sadden.
Around 30% of the Wikipedia binomials are not
in the Google data. We manually spot checked a
number of those and confirmed that they were un-
available from the Google data, regardless of inflec-
tion. Examples of binomials not found in the n-gram
corpus include dagger and saber, sagacious and
firm and (rather surprisingly) gay and flamboyant.
19% of the Wikipedia binomials have a different
preferred order in the Google corpus. As expected,
most of those have a low frequency in Wikipedia.
For the binomials with an occurrence count over 40,
the agreement on ordering is high (around 96%).
Furthermore, many of those disagreements are not
?real? in that they concern binomials found with a
high dispreferred to preferred order ratio. Disre-
garding cases where this ratio is over 0.3 lowers the
initial disagreement figure to 7%. We will argue in
?4.4 that true irreversibility can be shown to roughly
correspond to a ratio of 0.1. At this cutoff, the per-
centage of disagreements between the two corpora
is only 2%. Thus we found no evidence that the
encyclopaedic nature of Wikipedia has a significant
skewing effect on the frequencies. We thus believe
that Wikipedia is a suitable dataset for training an
automatic binomial ordering system.
4.3 Lexicalisation
Our basic methodology for investigation of lexi-
calisation was to check online dictionaries for the
phrases. However, deciding whether a binomial
should be regarded as a fixed phrase is not entirely
straightforward. For instance, consider warm and
fuzzy. At first sight, it might appear compositional,
but the particular use of fuzzy, referring to feelings,
is not the usual one. While warm and fuzzy is not
listed in most dictionaries we have examined, it has
an entry in the Urban Dictionary5 and is used in ex-
amples illustrating that particular usage of fuzzy in
the online Merriam-Webster.6 Another case from
the B+L data is nice and toasty, which again is used
in a Merriam-Webster example.7
We therefore used a manual search procedure
to check for lexicalisation of the B+L binomials.
We used a broad notion of lexicalisation, treat-
ing a phrase as lexicalised if it occurred as an en-
try in one or more online English dictionaries us-
ing Google search. We included a few phrases as
semi-lexicalised when they were given in examples
in dictionaries produced by professional lexicogra-
phers, but this was, to some extent, a subjective
decision. Since such a search is time-consuming,
we only checked examples which one of us (a na-
tive British English speaker) intuitively considered
might be lexicalised. We first validated that this
would not cause too great a loss of recall by check-
ing a small subset of the B+L data exhaustively: this
did not reveal any additional examples.
Using these criteria, we found 39 lexicalised bi-
nomial types in the B+L data, of which 7 were
semi-lexicalised.8 The phrases backwards and for-
wards, backward and forward, day and night, salt
and pepper and in and out are lexicalised (or semi-
lexicalised) in both orders.
5http://www.urbandictionary.com/
6http://www.merriam-webster.com/
7The convention of indicating semi-fixed phrases in exam-
ples is quite common in lexicography, especially in dictionaries
intended for language learners.
8There are 40 tokens, because cut and dry and cut and dried
are both lexicalised. An additional example, foot-loose and
fancy-free, might be included, but we did not find it in any dic-
tionary with that hyphenation.
50
4.4 Reversibility and corpus evidence
There are a number of possible reasons why a partic-
ular binomial type AB might (almost) always appear
in one ordering (A and B or B and A):
1. The phrase A and B (B and A) might be fully
lexicalised (word with spaces).
2. The binomial might have a compositional
meaning, but have a conventional ordering. A
particular binomial AB might be established
with that ordering (e.g., gin and tonic is es-
tablished for most British and American speak-
ers) or might belong to a conventional pattern
(e.g., armagnac and blackcurrant, sole and ar-
tichokes).
3. The binomial could refer to a sequence of real
world events or entities which almost invari-
ably occur in a particular order. For example,
shot and killed has a frequency of 241675 in
the Google 3-gram corpus, as opposed to 158
for killed and shot. This ratio is larger that that
of many of the lexicalised binomials.
Relatively few of the binomials from the B+L data
are completely irreversible according to the Google
3-gram data. There are instances of the reverse of
even obviously fixed phrases, such as odds and ends.
Of course, there is no available context in the 3-gram
data, but we investigated some of these cases by on-
line search for the reversed phrases. This indicates
a variety of sources of noise, including wordplay
(e.g., Beckett?s play Ends and Odds), different word
senses (e.g., toasty and nice occurs when toasty is
used to describe wine) and false positives from hy-
phenated words etc.
We can obtain a crude estimate of extent to which
binomials which should be irreversible actually turn
up in the ?wrong? order by looking at the clearly lex-
icalised phrases discussed in ?4.3. Excluding the
cases where both orders are lexicalised, the mean
proportion of inverted cases is about 3%. There are
a few outliers, such as there and back and now and
then which have more than 10% inverted: however,
these all involve very frequent closed class words
which are more likely to show up in spurious con-
texts. We therefore tentatively conclude that up to
10% of the tokens of a open-class irreversible bino-
mial could be inverted in the 3-gram corpus, but that
we can take higher ratios as evidence for a degree of
genuine reversibility.
5 An initial model
We developed an initial n-gram-based model for or-
dering using the Wikipedia-derived counts. The ap-
proach is very similar to that presented in (Malouf,
2000) for adjective ordering. We use the observed
order of binomials where possible and back off to
counts of a lexeme?s position as first or second con-
junct over all binomials (i.e., we use what Malouf
refers to as positional probabilities).
To be more precise, assume that the task is to pre-
dict the order a ? b or b ? a for a given lexeme pair
a,b. We use the notation C(a and b) and C(b and a)
to refer to the counts in a given corpus of the two
orderings of the binomial (i.e., we count all inflec-
tions of a and b). C(a and) refers to the count of all
binomials with the lexeme a as the first conjunct,
C(and a) all binomials with a as the second con-
junct, and so on. We predict a ? b
if C(a and b) > C(b and a)
or C(a and b) = C(a and b)
and
C(a and)C(and b) > C(b and)C(and a)
and conversely for b ? a. Most of the cases where
the condition C(a and b) = C(a and b) is true occur
when C(a and b) = C(a and b) = 0 but we also
use the positional probabilities to break ties in the
counts. We could, of course, define this in terms of
probability estimates and investigate various forms
of smoothing and interpolation, but for our initial
purposes it is adequate to see how this very simple
model behaves.
We obtained counts for the model from the
Wikipedia-derived data and evaluated it on the bino-
mial types derived from B+L (as described in ?4.1).
There were only 9 cases where there was no pre-
diction, so for the sake of simplicity, we default to
alphabetic ordering in those cases. In Table 1, we
show the results evaluating against the B+L major-
ity decision and against the Google 3-gram majority.
Because not all the B+L binomials are found in the
Google data, the numbers of binomial types evalu-
ated against the Google data is slightly lower. In
51
addition to the overall figures, we also show the rela-
tive accuracy of the bigram prediction vs the backoff
and the different accuracies on the lexicalised and
non-lexicalised data. In Table 2, we group the re-
sults according to the ratio of the less frequent order
in the Google data and by frequency.
Unsurprisingly, performance on more frequent bi-
nomials and lexicalised binomials is better and the
bigram performance, where available, is better than
the backoff to positional probabilities. The scores
when evaluated on the Google corpus are generally
higher than those on the B+L counts, as expected
given the noise created by the data sparsity in B+L
combined with the effect of frequency.
One outcome from our experiments is that it does
not seem essential to treat the lexicalised examples
separately from the high frequency, low reversibil-
ity cases. Since determining lexicalisation is time-
consuming and error-prone, this is a useful result.
The model described does not predict whether or
not a given binomial is irreversible, but our analy-
sis of the data strongly suggests that this would be
important in developing more realistic models. An
obvious extension would be to generate probability
estimates of orderings and to compare these with the
observed Google 3-gram data.
Although n-gram models are completely stan-
dard in computational linguistics, their applicabil-
ity to modelling human performance on a task is
not straightforward. Minimally, if we were to pro-
pose that humans were using such a model as part
of their decision on binomial ordering, it would be
necessary to demonstrate that the counts we are re-
lying on correspond to data which it is plausible to
assume that a human could have been exposed to.
This is not a trivial consideration. We would, of
course, expect to obtain higher scores on this task by
using counts derived from the Google n-gram cor-
pus rather than from Wikipedia, but this would be
completely unrealistic from a psycholinguistic per-
spective. We should emphasize, therefore, that the
model presented here is simply intended as an initial
exercise in developing distributional models of bi-
nomial ordering, which allows us to check whether
the resources we have developed might be an ade-
quate basis for more serious modelling and whether
the evaluation schemes are reasonable.
6 Conclusion
We have demonstrated that we can make use of a
combination of corpora to build resources for devel-
opment and evaluation of models of binomial order-
ing.9 One novel aspect is our use of an automatically
parsed corpus, another is the use of combined cor-
pora. If binomial ordering is primarily determined
by universal linguistic factors, we would not expect
the relative frequency to differ very substantially be-
tween large corpora. The cases where we did ob-
serve differences in preferred ordering between the
Wikipedia and Google data are predominantly ones
where the Wikipedia frequency is low or the bino-
mial is highly reversible. We have investigated sev-
eral properties of binomials using this data and pro-
duced a simple initial model. We tested this on the
relatively small number of binomials used by Benor
and Levy (2006), but in future work we will evalu-
ate on a much larger subset of our corpus. Our in-
tention is to develop further models which use anal-
ogy (morphological and distributional semantic sim-
ilarity) to known binomials to predict degree of re-
versibility and ordering. This will allow us to inves-
tigate whether human performance can be modelled
without the use of explicit semantic features.
We briefly touched on Malouf?s (2000) work on
prenominal adjective ordering in our discussion of
the initial model. There are some similarities be-
tween these tasks, and in fact adjectives in binomials
tend to occur in the same order when they appear as
prenominal adjectives (e.g., cold and wet and cold
wet are preferred over the inverse orders). However,
the binomial problem is considerably more complex.
Binomials are much more variable because they in-
volve all the main syntactic categories. Furthermore,
adjective ordering is considerably easier to investi-
gate because an unparsed corpus can be used, the se-
mantic features which have been postulated are more
straightforward than for binomials and lexicalisation
of adjective sequences is not an issue. We hypoth-
esize that it should be possible to develop similar
analogical models for adjective ordering and bino-
mials which could be relevant for other construc-
tions where ordering is only partially determined
by syntax. In the long term, we would like to in-
9Available from http://www.cl.cam.ac.uk/
research/nl/nl-download/binomials/
52
n B+L n Google accuracy B+L (%) accuracy Google (%)
Overall 380 305 69 79
Bigram 187 185 79 89
Pos Prob 184 117 61 65
Unknown 9 3 33 0
Lexicalised 34 34 87 94
Non-lexicalised 346 271 67 77
Table 1: Evaluation of initial model, showing effects of lexicalisation. (n B+L and n Google indicates the number of
binomial types evaluated)
n accuracy B+L (%) accuracy Google (%)
Google count 0 75 59 -
1?1000 71 56 68
1001?10000 81 70 67
> 10000 153 80 91
Google ratio 0 11 64 64
0?0.1 41 94 93
0.1?0.25 33 75 85
> 0.25 220 68 76
Table 2: Evaluation of initial model, showing effects of frequency and reversibility.
vestigate using such models in conjunction with a
grammar-based realizer (cf (Velldal, 2007), (Cahill
and Riester, 2009)). However, for an initial inves-
tigation of the role of semantics and lexicalisation,
looking at the binomial construction in isolation is
more tractable.
Acknowledgments
This work was partially supported by a fellowship
to Aure?lie Herbelot from the Alexander von Hum-
boldt Foundation. We are grateful to the reviewers
for their comments.
References
Sarah Benor and Roger Levy. 2006. The Chicken or the
Egg? A Probabilistic Analysis of English Binomials.
Language, 82 233?78.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram Corpus Version 1.1. LDC2006T13.
Aoife Cahill and Arndt Riester. 2009. Incorporating
Information Status into Generation Ranking. In Pro-
ceedings of the 47th Annual Meeting of the ACL, pp.
817-825, Suntec, Singapore. Association for Compu-
tational Linguistics.
D. Alan Cruse. 1986. Lexical Semantics. Cambridge
University Press.
Jonathan K. Kummerfeld, Jessika Rosener, Tim Daw-
born, James Haggerty, James R. Curran, Stephen
Clark. 2010. Faster parsing by supertagger adapta-
tion Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, Uppsala,
Sweden, pages 345?355
Rob Malouf. 2000. The order of prenominal adjectives
in natural language generation. In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2000), Hong Kong.
Ivan Sag, Tim Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions:
A pain in the neck for NLP. In Third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLING 2002), pages 1?15,
Mexico City, Mexico.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, pages 135?143, College Park, Maryland.
Eric Velldal. 2007. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo, Department of Infor-
matics.
53
