CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 248?252
Manchester, August 2008
Mixing and Blending Syntactic and Semantic Dependencies
Yvonne Samuelsson
Dept. of Linguistics
Stockholm University
yvonne.samuelsson@ling.su.se
Johan Eklund
SSLIS
University College of Bor?as
johan.eklund@hb.se
Oscar T
?
ackstr
?
om
Dept. of Linguistics and Philology
SICS / Uppsala University
oscar@sics.se
Mark Fi
?
sel
Dept. of Computer Science
University of Tartu
fishel@ut.ee
Sumithra Velupillai
Dept. of Computer and Systems Sciences
Stockholm University / KTH
sumithra@dsv.su.se
Markus Saers
Dept. of Linguistics and Philology
Uppsala University
markus.saers@lingfil.uu.se
Abstract
Our system for the CoNLL 2008 shared
task uses a set of individual parsers, a set of
stand-alone semantic role labellers, and a
joint system for parsing and semantic role
labelling, all blended together. The system
achieved a macro averaged labelled F
1
-
score of 79.79 (WSJ 80.92, Brown 70.49)
for the overall task. The labelled attach-
ment score for syntactic dependencies was
86.63 (WSJ 87.36, Brown 80.77) and the
labelled F
1
-score for semantic dependen-
cies was 72.94 (WSJ 74.47, Brown 60.18).
1 Introduction
This paper presents a system for the CoNLL 2008
shared task on joint learning of syntactic and se-
mantic dependencies (Surdeanu et al, 2008), com-
bining a two-step pipelined approach with a joint
approach.
In the pipelined system, eight different syntac-
tic parses were blended, yielding the input for two
variants of a semantic role labelling (SRL) system.
Furthermore, one of the syntactic parses was used
with an early version of the SRL system, to pro-
vide predicate predictions for a joint syntactic and
semantic parser. For the final submission, all nine
syntactic parses and all three semantic parses were
blended.
The system is outlined in Figure 1; the dashed
arrow indicates the potential for using the predi-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Process
Parse + SRL
Process
8 MaltParsers
Parser Blender
Process
2 Pipelined SRLs
SRL Blender
Joint Parser/SRL
Possible
Iteration
Figure 1: Overview of the submitted system.
cate prediction to improve the joint syntactic and
semantic system.
2 Dependency Parsing
The initial parsing system was created using Malt-
Parser (Nivre et al, 2007) by blending eight dif-
ferent parsers. To further advance the syntactic ac-
curacy, we added the syntactic structure predicted
by a joint system for syntactic and semantic depen-
dencies (see Section 3.4) in the blending process.
2.1 Parsers
The MaltParser is a dependency parser genera-
tor, with three parsing algorithms: Nivre?s arc
standard, Nivre?s arc eager (see Nivre (2004)
for a comparison between the two Nivre algo-
rithms), and Covington?s (Covington, 2001). Both
of Nivre?s algorithms assume projectivity, but
the MaltParser supports pseudo-projective parsing
(Nilsson et al, 2007), for projectivization and de-
projectivization.
248
WSJ Brown
Best single parse 85.22% 78.37%
LAS weights 87.00% 80.60%
Learned weights 87.36% 80.77%
Table 1: Labelled attachment score on the two test
sets of the best single parse, blended with weights
set to PoS labelled attachment score (LAS) and
blended with learned weights.
Four parsing algorithms (the two Nivre al-
gorithms, and Covington?s projective and non-
projective version) were used, creating eight
parsers by varying the parsing direction, left-to-
right and right-to-left. The latter was achieved by
reversing the word order in a pre-processing step
and then restoring it in post-processing. For the fi-
nal system, feature models and training parameters
were adapted from Hall et al (2007).
2.2 Blender
The single parses were blended following the pro-
cedure of Hall et al (2007). The parses of each
sentence were combined into a weighted directed
graph. The Chu-Liu-Edmonds algorithm (Chu and
Liu, 1965; Edmonds, 1967) was then used to find
the maximum spanning tree (MST) of the graph,
which was considered the final parse of the sen-
tence. The weight of each graph edge was calcu-
lated as the sum of the weights of the correspond-
ing edges in each single parse tree.
We used a simple iterative weight updating algo-
rithm to learn the individual weights of each single
parser output and part-of-speech (PoS) using the
development set. To construct an initial MST, the
labelled attachment score was used. Each single
weight, corresponding to an edge of the hypoth-
esis tree, was then iteratively updated by slightly
increasing or decreasing the weight, depending on
whether it belonged to a correct or incorrect edge
as compared to the reference tree.
2.3 Results
The results are summarized in Table 1; the parse
with LAS weights and the best single parse
(Nivre?s arc eager algorithm with left-to-right pars-
ing direction) are also included for comparison.
3 Semantic Role Labelling
The SRL system is a pipeline with three chained
stages: predicate identification, argument identifi-
cation, and argument classification. Predicate and
argument identification are treated as binary clas-
sification problems. In a simple post-processing
predicate classification step, a predicted predicate
is assigned the most frequent sense from the train-
ing data. Argument classification is treated as a
multi-class learning problem, where the classes
correspond to the argument types.
3.1 Learning and Parameter Optimization
For learning and prediction we used the freely
available support vector machine (SVM) imple-
mentation LIBSVM (version 2.86) (Chang and
Lin, 2001). The choice of cost and kernel parame-
ter values will often significantly influence the per-
formance of the SVM classifier. We therefore im-
plemented a parameter optimizer based on the DI-
RECT optimization algorithm (Gablonsky, 2001).
It iteratively divides the search space into smaller
hyperrectangles, sampling the objective function
in the centroid of each hyperrectangle, and select-
ing those hyperrectangles that are potentially opti-
mal for further processing. The search space con-
sisted of the SVM parameters to optimize and the
objective function was the cross-validation accu-
racy reported by LIBSVM.
Tests performed during training for predicate
identification showed that the use of runtime opti-
mization of the SVM parameters for nonlinear ker-
nels yielded a higher average F
1
-score effective-
ness. Surprisingly, the best nonlinear kernels were
always outperformed by the linear kernel with de-
fault settings, which indicates that the data is ap-
proximately linearly separable.
3.2 Filtering and Data Set Splitting
To decrease the number of instances during train-
ing, all predicate and argument candidates with
PoS-tags that occur very infrequently in the
training set were filtered out. Some PoS-tags
were filtered out for all three stages, e.g. non-
alphanumerics, HYPH, SYM, and LS. This ap-
proach was effective, e.g. removing more than half
of the total number of instances for predicate pre-
diction.
To speed up the SVM training and allow for
parallelization, each data set was split into several
bins. However, there is a trade-off between speed
and accuracy. Performance consistently deterio-
rated when splitting into smaller bins. The final
system contained two variants, one with more bins
based on a combination of PoS-tags and lemma
frequency information, and one with fewer bins
249
based only on PoS-tag information. The three
learning tasks used different splits. In general, the
argument identification step was the most difficult
and therefore required a larger number of bins.
3.3 Features
We implemented a large number of features (over
50)
1
for the SRL system. Many of them can be
found in the literature, starting from Gildea and
Jurafsky (2002) and onward. All features, except
bag-of-words, take nominal values, which are bi-
narized for the vectors used as input to the SVM
classifier. Low-frequency feature values (except
for Voice, Initial Letter, Number of Words, Rela-
tive Position, and the Distance features), below a
threshold of 20 occurrences, were given a default
value.
We distinguish between single node and node
pair features. The following single node features
were used for all three learning tasks and for both
the predicate and argument node:
2
? Lemma, PoS, and Dependency relation (DepRel) for
the node itself, the parent, and the left and right sibling
? Initial Letter (upper-case/lower-case), Number of
Words, and Voice (based on simple heuristics, only for
the predicate node during argument classification)
? PoS Sequence and PoS bag-of-words (BoW) for the
node itself with children and for the parent with chil-
dren
? Lemma and PoS for the first and last child of the node
? Sequence and BoW of Lemma and PoS for content
words
? Sequence and BoW of PoS for the immediate children?s
content words
? Sequence and BoW of PoS for the parent?s content
words and for the parent?s immediate children
? Sequence and BoW of DepRels for the node itself, for
the immediate children, and for the parent?s immediate
children
All extractors of node pair features, where the pair
consists of the predicate and the argument node,
can be used both for argument identification and
argument classification. We used the following
node pair features:
? Relative Position (the argument is before/after the pred-
icate), Distance in Words, Middle Distance in DepRels
? PoS Full Path, PoS Middle Path, PoS Short Path
1
Some features were discarded for the final system based
on Information Gain, calculated using Weka (Witten and
Frank, 2005).
2
For all features using lemma or PoS the (predicted) split
value is used.
The full path feature contains the PoS-tag of the ar-
gument node, all dependency relations between the
argument node and the predicate node and finally
the PoS-tag of the predicate node. The middle path
goes to the lowest common ancestor for argument
and predicate (this is also the distance calculated
by Middle Distance in DepRels) and the short path
only contains the dependency relation of the argu-
ment and predicate nodes.
3.4 Joint Syntactic and Semantic Parsing
When considering one predicate at a time, SRL be-
comes a regular labelling problem. Given a pre-
dicted predicate, joint learning of syntactic and se-
mantic dependencies can be carried out by simulta-
neously assigning an argument label and a depen-
dency relation. This is possible because we know
a priori where to attach the argument, since there
is only one predicate candidate
3
. The MaltParser
system for English described in Hall et al (2007)
was used as a baseline, and then optimized for this
new task, focusing on feature selection.
A large feature model was constructed, and
backward selection was carried out until no fur-
ther gain could be observed. The feature model of
MaltParser consists of a number of feature types,
each describing a starting point, a path through the
structure so far, and a column of the node arrived
at. The number of feature types was reduced from
37 to 35 based on the labelled F
1
-score.
As parsing is done at the same time as argu-
ment labelling, different syntactic structures risk
being assigned to the same sentence, depending
on which predicate is currently processed. This
means that several, possibly different, parses have
to be combined into one. In this experiment, the
head and the dependency label were concatenated,
and the most frequent one was used. In case of
a tie, the first one to appear was used. The like-
lihood of the chosen labelling was also used as a
confidence measure for the syntactic blender.
3.5 Blending and Post-Processing
Combining the output from several different sys-
tems has been shown to be beneficial (Koomen
et al, 2005). For the final submission, we com-
bined the output of two variants of the pipelined
SRL system, each using different data splits, with
3
The version of the joint system used in the submission
was based on an early predicate prediction. More accurate
predicates would give a major improvement for the results.
250
Test set Pred PoS Labelled F
1
Unlabelled F
1
WSJ All 82.90 90.90
NN* 81.12 86.39
VB* 85.52 96.49
Brown All 67.48 85.49
NN* 58.34 75.35
VB* 73.24 91.97
Table 2: Semantic predicate results on the test sets.
the SRL output of the joint system. A simple uni-
form weight majority vote heuristic was used, with
no combinatorial constraints on the selected argu-
ments. For each sentence, all predicates that were
identified by a majority of the systems were se-
lected. Then, for each selected predicate, its ar-
guments were picked by majority vote (ignoring
the systems not voting for the predicate). The best
single SRL system achieved a labelled F
1
-score
of 71.34 on the WSJ test set and 57.73 on the
Brown test set, compared to 74.47 and 60.18 for
the blended system.
As a final step, we filtered out all verbal and
nominal predicates not in PropBank or NomBank,
respectively, based on the predicted PoS-tag and
lemma. Each lexicon was expanded with lemmas
from the training set, due to predicted lemma er-
rors in the training data. This turned out to be a
successful strategy for the individual systems, but
slightly detrimental for the blended system.
3.6 Results
Semantic predicate results for WSJ and Brown can
be found in Table 2. Table 4 shows the results for
identification and classification of arguments.
4 Analysis and Conclusions
In general, the mixed and blended system performs
well on all tasks, rendering a sixth place in the
CoNLL 2008 shared task. The overall scores for
the submitted system can be seen in Table 3.
4.1 Parsing
For the blended parsing system, the labelled at-
tachment score drops from 87.36 for the WSJ test
set to 80.77 for the Brown test set, while the unla-
belled attachment score only drops from 89.88 to
86.28. This shows that the system is robust with
regards to the overall syntactic structure, even if
picking the correct label is more difficult for the
out-of-domain text.
The parser has difficulties finding the right head
for punctuation and symbols. Apart from errors re-
WSJ + Brown WSJ Brown
Syn + Sem 79.79 80.92 70.49
Syn 86.63 87.36 80.77
Sem 72.94 74.47 60.18
Table 3: Syntactic and semantic scores on the test
sets for the submitted system. The scores, from top
to bottom, are labelled macro F
1
, labelled attach-
ment score and labelled F
1
.
garding punctuation, most errors occur for IN and
TO. A majority of these problems are related to as-
signing the correct dependency. This is not surpris-
ing, since these are categories that focus on form
rather than function.
There is no significant difference in score for left
and right dependencies, presumably because of the
bi-directional parsing. However, the system over-
predicts dependencies to the root. This is mainly
due to the way MaltParser handles tokens not be-
ing attached anywhere during parsing. These to-
kens are by default assigned to the root.
4.2 SRL
Similarly to the parsing results, the blended SRL
system is less robust with respect to labelled F
1
-
score, dropping from 74.47 on the WSJ test set to
60.18 on the Brown test set. The corresponding
drop in unlabelled F
1
-score is from 82.90 to 75.49.
The simple method of picking the most com-
mon sense from the training data works quite well,
but the difference in domain makes it more diffi-
cult to find the correct sense for the Brown corpus.
In the future, a predicate classification module is
needed. For the WSJ corpus, assigning the most
common predicate sense works better with nomi-
nal than with verbal predicates, while verbal pred-
icates are handled better for the Brown corpus.
In general, verbal predicate-argument structures
are handled better than nominal ones, for both
test sets. This is not surprising, since nominal
predicate-argument structures tend to vary more in
their composition.
Since we do not use global constraints for the
argument labelling (looking at the whole argument
structure for each predicate), the system can out-
put the same argument label for a predicate several
times. For the WSJ test set, for instance, the ra-
tio of repeated argument labels is 5.4% in the sys-
tem output, compared to 0.3% in the gold standard.
However, since there are no confidence scores for
predictions it is difficult to handle this in the cur-
rent system.
251
PPOSS(pred) + ARG WSJ F
1
Brown F
1
NN* + A0 61.42 38.99
NN* + A1 67.07 53.10
NN* + A2 57.02 26.19
NN* + A3 63.08 (16.67)
NN* + AM-ADV 4.65 (-)
NN* + AM-EXT 44.78 (40.00)
NN* + AM-LOC 49.45 (-)
NN* + AM-MNR 53.51 21.82
NN* + AM-NEG 79.37 (46.15)
NN* + AM-TMP 67.23 (25.00)
VB* + A0 81.72 73.58
VB* + A1 81.77 67.99
VB* + A2 60.91 50.67
VB* + A3 61.49 (14.28)
VB* + A4 77.84 (40.00)
VB* + AM-ADV 47.49 30.33
VB* + AM-CAU 55.12 (35.29)
VB* + AM-DIR 41.86 37.14
VB* + AM-DIS 71.91 37.04
VB* + AM-EXT 60.38 (-)
VB* + AM-LOC 55.69 37.50
VB* + AM-MNR 49.54 36.25
VB* + AM-MOD 94.85 82.42
VB* + AM-NEG 93.45 77.08
VB* + AM-PNC 50.00 (62.50)
VB* + AM-TMP 69.59 49.07
VB* + C-A1 70.76 55.32
VB* + R-A0 83.68 70.83
VB* + R-A1 68.87 51.43
VB* + R-AM-LOC 38.46 (25.00)
VB* + R-AM-TMP 56.82 (58.82)
Table 4: Semantic argument results on the two
test sets, showing arguments with more than 20
instances in the gold test set (fewer instances for
Brown are given in parentheses).
Acknowledgements
This project was carried out within the course Ma-
chine Learning 2, organized by GSLT (Swedish
National Graduate School of Language Tech-
nology), with additional support from NGSLT
(Nordic Graduate School of Language Technol-
ogy). We thank our supervisors Joakim Nivre,
Bj?orn Gamb?ack and Pierre Nugues for advice and
support. Computations were performed on the
BalticGrid and UPPMAX (projects p2005008 and
p2005028) resources. We thank Tore Sundqvist at
UPPMAX for technical assistance.
References
Chang, Chih-Chung and Chih-Jen Lin, 2001. LIBSVM:
A library for support vector machines.
Chu, Y. J. and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Covington, Michael A. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of the
39th Annual Association for Computing Machinery
Southeast Conference, Athens, Georgia.
Edmonds, Jack. 1967. Optimum branchings. Jour-
nal of Research of the National Bureau of Standards,
71(B):233?240.
Gablonsky, J?org M. 2001. Modifications of the DI-
RECT algorithm. Ph.D. thesis, North Carolina State
University, Raleigh, North Carolina.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Hall, Johan, Jens Nilsson, Joakim Nivre, G?uls?en
Eryi?git, Be?ata Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended?
A study in multilingual parser optimization. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, Prague, Czech Republic.
Koomen, Peter, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning (CoNLL-2005), Ann Arbor,
Michigan.
Nilsson, Jens, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics, Prague, Czech Republic.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G?uls?en Eryi?git, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
2(13):95?135.
Nivre, Joakim. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the ACL?04
Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together, Barcelona, Spain.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008), Manchester, Great
Britain.
Witten, Ian H. and Eibe Frank. 2005. Data mining:
Practical machine learning tools and techniques.
Morgan Kaufmann, Amsterdam, 2nd edition.
252
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 64?70,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
TerrorCat: a Translation Error Categorization-based MT Quality Metric
Mark Fishel,? Rico Sennrich,? Maja Popovic?,? Ondr?ej Bojar?
? Institute of Computational Linguistics, University of Zurich
{fishel,sennrich}@cl.uzh.ch
? German Research Center for Artificial Intelligence (DFKI), Berlin
maja.popovic@dfki.de
? Charles University in Prague, Faculty of Mathematics and Physics,
Institute of Formal and Applied Linguistics
bojar@ufal.mff.cuni.cz
Abstract
We present TerrorCat, a submission to the
WMT?12 metrics shared task. TerrorCat uses
frequencies of automatically obtained transla-
tion error categories as base for pairwise com-
parison of translation hypotheses, which is in
turn used to generate a score for every trans-
lation. The metric shows high overall corre-
lation with human judgements on the system
level and more modest results on the level of
individual sentences.
1 The Idea
Recently a couple of methods of automatic trans-
lation error analysis have emerged (Zeman et al,
2011; Popovic? and Ney, 2011). Initial experiments
have shown that while agreement with human error
analysis is low, these methods show better perfor-
mance on tasks with a lower granularity, e.g. ranking
error categories by frequency (Fishel et al, 2012).
In this work we apply translation error analysis to a
task with an even lower granularity: ranking transla-
tions, one of the shared tasks of WMT?12.
The aim of translation error analysis is to identify
the errors that translation systems make and catego-
rize them into different types: e.g. lexical, reorder-
ing, punctuation errors, etc. The two tools that we
will use ? Hjerson and Addicter ? both rely on a ref-
erence translation. The hypothesis translation that is
being analyzed is first aligned to the reference on the
word level, and then mistranslated, misplaced, mis-
inflected, missing or superfluous words and other er-
rors are identified.
The main idea of our work is to quantify trans-
lation quality based on the frequencies of different
error categories. The basic assumption is that differ-
ent error categories have different importance from
the point of view of overall translation quality: for
instance, it would be natural to assume that punc-
tuation errors influence translation quality less than
missing words or lexical choice errors. Furthermore,
an error category can be more important for one out-
put language than the other: for example, word or-
der can influence the meaning in an English sentence
more than in a Czech or German one, whereas in-
flection errors are probably more frequent in the lat-
ter two and can thus cause more damage.
In the context of the ranking task, the absolute
value of a numeric score has no importance, apart
from being greater than, smaller than or equal to the
other systems? scores. We therefore start by per-
forming pairwise comparison of the translations ?
the basic task is to compare two translations and re-
port which one is better. To conform with the WMT
submission format we need to generate a numeric
score as the output ? which is obtained by compar-
ing every possible pair of translations and then using
the (normalized) total number of wins per translation
as its final score.
The general architecture of the metric is thus this:
? automatic error analysis is applied to the sys-
tem outputs, yielding the frequencies of every
error category for each sentence
? every possible pair of all system outputs is rep-
resented as a vector of features, based on the
error category frequencies
64
? a binary classifier takes these feature vectors as
input and assigns a win to one of the sentences
in every pair (apart from ties)
? the final score of a system equals to the normal-
ized total number of wins per sentence
? the system-level score is averaged out over the
individual sentence scores
An illustrative example is given in Figure 1.
We call the result TerrorCat, the translation error
categorization-based metric.
2 The Details
In this section we will describe the specifics of
the current implementation of the TerrorCat met-
ric: translation error analysis, lemmatization, binary
classifier and training data for the binary classifier.
2.1 Translation Error Analysis
Addicter (Zeman et al, 2011) and Hjerson (Popovic?
and Ney, 2011) use different methods for automatic
error analysis. Addicter explicitly aligns the hy-
pothesis and reference translations and induces error
categories based on the alignment coverage while
Hjerson compares words encompassed in the WER
(word error rate) and PER (position-independent
word error rate) scores to the same end.
Previous evaluation of Addicter shows that
hypothesis-reference alignment coverage (in terms
of discovered word pairs) directly influences er-
ror analysis quality; to increase alignment cover-
age we used Berkeley aligner (Liang et al, 2006)
and trained it on and applied it to the whole set of
reference-hypothesis pairs for every language pair.
Both tools use word lemmas for their analysis;
we used TreeTagger (Schmid, 1995) for analyzing
English, Spanish, German and French and Morc?e
(Spoustova? et al, 2007) to analyze Czech. The same
tools are used for PoS-tagging in some experiments.
2.2 Binary Classification
Pairwise comparison of sentence pairs is achieved
with a binary SVM classifier, trained via sequential
minimal optimization (Platt, 1998), implemented in
Weka (Hall et al, 2009).
The input feature vectors are composed of fre-
quency differences of every error category; since the
Source: Wir sind Meister!
Translations:
Reference: We are the champions!
HYP-1: Us champions!
HYP-2: The champions we are .
HYP-3: We are the champignons!
Error Frequencies:
HYP-1: 1?inflection, 2?missing
HYP-2: 2?order, 1?punctuation
HYP-3: 1?lex.choice
Classifier Output: (or manually created
input in the training phase)
HYP-1 < HYP-2
HYP-1 < HYP-3
HYP-2 > HYP-3
Scores:
HYP-1: 0
HYP-2: 1
HYP-3: 0.5
Figure 1: Illustration of TerrorCat?s process for a single
sentence: translation errors in the hypothesis translations
are discovered by comparing them to the reference, error
frequencies are extracted, pairwise comparisons are done
by the classifier and then converted to scores. The shown
translation errors correspond to Hjerson?s output.
maximum (normalized) frequency of any error rate
is 1, the feature value range is [?1, 1]. To include
error analysis from both Addicter and Hjerson their
respective features are used side-by-side.
2.3 Data Extraction
Training data for the SVM classifier is taken from
the WMT shared task manual ranking evaluations
of previous years (2007?2011), which consist of tu-
ples of 2 to 5 ranked sentences for every language
pair. Equal ranks are allowed, and translations of
the same sentence by the same pair of systems can
be present in several tuples, possibly having conflict-
ing comparison results.
To convert the WMT manual ranking data into
the training data for the SVM classifier, we collect
all rankings for each pair of translation hypothe-
65
2007-2010 2007-2011
fr-en 34 152 46 070
de-en 36 792 53 790
es-en 30 374 41 966
cs-en 19 268 26 418
en-fr 22 734 35 854
en-de 36 076 56 054
en-es 19 352 35 700
en-cs 31 728 52 954
Table 1: Dataset sizes for every language pair, based
on manual rankings from WMT shared tasks of previ-
ous years: the number of pairs with non-conflicting, non-
equivalent ranks.
ses. Pairs with equal ranks are discarded, conflicting
ranks for the same pairs are resolved with voting. If
the voting is tied, the pair is also discarded.
The kept translation pairs are mirrored (i.e. both
directions of every pair are added to the training set
as independent entries) to ensure no bias towards the
first or second translation in a pair. We will later
present analysis of how well that works.
2.4 TerrorCat+You
TerrorCat is distributed via GitHub; information on
downloading and using it can be found online.1 Ad-
ditionally we are planning to provide more recent
evaluations with new datasets, as well as pre-trained
models for various languages and language pairs.
3 The Experiments
In the experimental part of our work, we search for
the best performing model variant, the aim of which
is to evaluate different input features, score calcula-
tion strategies and other alternations. The search is
done empirically: we evaluate one alternation at a
time, and if it successful, it is added to the system
before proceeding to test further alternations.
Performance of the models is estimated on a held-
out development set, taken from the WMT?11 data;
the training data during the optimization phase is
composed of ranking data from WMT 2007?2010.
In the end we re-trained our system on the whole
data set (WMT 2007?2011) and applied it to the un-
1http://terra.cl.uzh.ch/terrorcat.html
labeled data from this year?s shared task. The result-
ing dataset sizes are given in Table 1.
All of the resulting scores obtained by different
variants of our metric are presented in Tables 2 (for
system-level correlations) and 3 (for sentence-level
correlations), compared to BLEU and other selected
entries in the WMT?11 evaluation shared task. Cor-
relations are computed in the same way as in the
WMT evaluations.
3.1 Model Optimization
The following is a brief description of successful
modifications to the baseline system.
Weighted Wins
In the baseline model, the score of the winning
system in each pairwise comparison is increased by
1. To reduce the impact of low-confidence decisions
of the classifier on the final score we tested replac-
ing the constant rewards to the winning system with
variable ones, proportional to the classifier?s confi-
dence ? a measure of which was obtained by fitting
a logistic regression model to the SVM output.
As the results show, this leads to minor improve-
ments in sentence-level correlation and more notice-
able improvements in system-level correlation (es-
pecially English-French and Czech-English). A pos-
sible explanation for this difference in performance
on different levels is that low classification confi-
dence on the sentence-level does not necessarily af-
fect our ranking for that sentence, but reduces the
impact of that sentence on the system-level ranking.
PoS-Split Features
The original model only makes a difference be-
tween individual error categories as produced by
Hjerson and Addicter. It seems reasonable to assume
that errors may be more or less important, depending
on the part-of-speech of the words they occur in. We
therefore tested using the number of errors per er-
ror category per PoS-tag as input features. In other
words, unlike the baseline, which relied on counts
of missing, misplaced and other erroneous words,
this alternation makes a difference between miss-
ing nouns/verbs/etc., misplaced nouns, misinflected
nouns/adjectives, and so on.
The downside of this approach is that the number
of features is multiplied by the size of the PoS tag
66
Metric fr-en de-en es-en cs-en *-en en-fr en-de en-es en-cs en-*
TerrorCat:
Baseline 0.73 0.74 0.82 0.76 0.76 0.70 0.81 0.69 0.84 0.76
Weighted wins 0.73 0.74 0.82 0.79 0.77 0.75 0.81 0.69 0.84 0.77
PoS-features 0.87 0.76 0.80 0.86 0.82 0.76 0.86 0.74 0.87 0.81
GenPoS-features 0.86 0.77 0.84 0.88 0.84 0.80 0.85 0.75 0.90 0.83
No 2007 data (GenPoS) 0.89 0.80 0.80 0.95 0.86 0.85 0.84 0.81 0.90 0.85
Other:
BLEU 0.85 0.48 0.90 0.88 0.78 0.86 0.44 0.87 0.65 0.70
mp4ibm1 0.08 0.56 0.12 0.91 0.42 0.61 0.91 0.71 0.76 0.75
MTeRater-Plus 0.93 0.90 0.91 0.95 0.92 ? ? ? ? ?
AMBER ti 0.94 0.63 0.85 0.88 0.83 0.84 0.54 0.88 0.56 0.70
meteor-1.3-rank 0.93 0.71 0.88 0.91 0.86 0.85 0.30 0.74 0.65 0.63
Table 2: System-level Spearman?s rank correlation coefficients (?) between different variants of TerrorCat and hu-
man judgements, based on WMT?11 data. Other metric submissions are shown for comparison. Highest scores per
language pair are highlighted in bold separately for TerrorCat variants and for other metrics.
set. Additionally, too specific distinctions can cause
data sparsity, especially on the sentence level.
As shown by the results, PoS-tag splitting of the
features is successful on the system level, but quite
hurtful to the sentence-level correlations. The poor
performance on the sentence level can be attributed
to the aforementioned data sparsity: the number of
different features is higher than the number of words
(and hence, the biggest possible number of errors)
in the sentences. However, we cannot quite ex-
plain, how a sum of these less reliable sentence-level
scores leads to more reliable system-level scores.
To somewhat relieve data sparsity we defined sub-
sets of the original PoS tag sets, mostly leaving out
morphological information and keeping just the gen-
eral word types (nouns, verbs, adjectives, etc.). This
reduced the number of PoS-tags (and thus, the num-
ber of input features) from 2 to 4 times and produced
further increase in system-level and a smaller de-
crease in sentence-level scores, see GenPoS results.
To avoid splitting the metric into different ver-
sions for system-level and sentence-level, we gave
priority to system-level correlations and adopted the
generalized PoS-splitting of the features.
Out-of-Domain Data
The human ranking data from WMT of previ-
ous years do not constitute a completely homo-
geneous dataset. For starters, the test sets are
taken from different domains (News/News Com-
mentary/Europarl), whereas the 2012 test set is from
the News domain only. Added to this, there might be
a difference in the manual data, coming from differ-
ent organization of the competition ? e.g. WMT?07
was the only year when manual scoring of the trans-
lations with adequacy/fluency was performed, and
ranking had just been introduced into the competi-
tion. Therefore we tested whether some subsets of
the training data can result in better overall scores.
Interestingly enough, leaving out News Commen-
tary and Europarl test sets caused decreased correla-
tions, although these account for just around 10%
of the training data. On the other hand, leaving out
the data from WMT?07 led to a significant gain in
overall performance.
3.2 Error Meta-Analysis
To better understand why sentence-level correlations
are low, we analyzed the core of TerrorCat ? its pair-
wise classifier. Here, we focus on the most success-
ful variant of the metric, which uses general PoS-
tags and was trained on the WMT manual rankings
from 2008 to 2010. Table 4 presents the confusion
matrices of the classifier (one for precision and one
for recall), taking into consideration the confidence
estimate.
Evaluation is based on the data from 2011; the
prediction data was mirrored in the same way as for
67
Metric fr-en de-en es-en cs-en *-en en-fr en-de en-es en-cs en-*
TerrorCat:
Baseline 0.20 0.22 0.33 0.25 0.25 0.30 0.19 0.24 0.20 0.23
Weighted wins 0.20 0.23 0.33 0.25 0.25 0.31 0.20 0.24 0.20 0.24
PoS-features 0.13 0.18 0.24 0.15 0.18 0.27 0.15 0.15 0.17 0.19
GenPoS-features 0.16 0.24 0.31 0.22 0.23 0.27 0.18 0.22 0.19 0.22
No 2007 data (GenPoS) 0.21 0.30 0.33 0.23 0.27 0.29 0.20 0.23 0.20 0.23
Other:
mp4ibm1 0.15 0.16 0.18 0.12 0.15 0.21 0.13 0.13 0.06 0.13
MTeRater-Plus 0.30 0.36 0.45 0.36 0.37 ? ? ? ? ?
AMBER ti 0.24 0.26 0.33 0.27 0.28 0.32 0.22 0.31 0.21 0.27
meteor-1.3-rank 0.23 0.25 0.38 0.28 0.29 0.31 0.14 0.26 0.19 0.23
Table 3: Sentence-level Kendall?s rank correlation coefficients (? ) between different variants of TerrorCat and hu-
man judgements, based on WMT?11 data. Other metric submissions are shown for comparison. Highest scores per
language pair are highlighted in bold separately for TerrorCat variants and for other metrics.
the training set. Our aim was to measure the bias
of the classifier towards first or second translations
in a pair (which is obviously an undesired effect).
It can be seen that the confusion matrices are com-
pletely symmetrical, indicating no position bias of
the classifier ? even lower-confidence decisions are
absolutely consistent.
To make sure that this can be attributed to the mir-
roring of the training set, we re-trained the classifier
on non-mirrored training sets. As a result, 9% of the
instances were labelled inconsistently, with the av-
erage confidence of such inconsistent decisions be-
ing extremely low (2.1%, compared to the overall
average of 28.4%). The resulting correlations have
slightly dropped as well ? all indicating that mirror-
ing the training sets does indeed remove the posi-
tional bias and leads to slightly better performance.
Looking at the confusion matrices overall, most
decisions fall within the main diagonals (i.e. the
cells indicating correct decisions of the classifier).
Looking strictly at the classifier?s decisions, the re-
calls and precisions of the non-tied comparison out-
puts (?<? and ?>?) are 57% precision, 69% recall.
However, such strict estimates are too pessimistic in
our case, since the effect of the classifier?s decisions
is proportional to the confidence estimate. On the
sentence level it means that low-confidence decision
errors have less effect on the total score of a system.
A definite source of error is the instability of the in-
dividual translation errors on the sentence level, an
effect both Addicter and Hjerson are known to suffer
from (Fishel et al, 2012).
The precision of the classifier predictably drops
together with the confidence, and almost half of the
misclassifications come from unrecognized equiva-
lent translations ? as a result the recall of such pairs
of equivalent translations is only 20%. This can be
explained by the fact that the binary classifier was
trained on instances with just these two labels and
with no ties allowed.
On the other hand the classifier?s 0-confidence de-
cisions have a high precision (84%) on detecting the
equivalent translations; after re-examining the data
it turned out that 96% of the 0-confidence decisions
were made on input feature vectors containing only
zero frequency differences. Such vectors represent
pairs of sentences with identical translation error
analyses, which are very often simply identical sen-
tences ? in which case the classifier cannot (and in
fact, should not) make an informed decision of one
being better than the other.
4 Related Work
Traditional MT metrics such as BLEU (Papineni et
al., 2002) are based on a comparison of the trans-
lation hypothesis to one or more human references.
TerrorCat still uses a human reference to extract fea-
tures from the error analysis with Addicter and Hjer-
son, but at the core, TerrorCat compares hypotheses
not to a reference, but to each other.
68
Manual Classifier Output and Confidence: Precision
label < < or > >
0.6?1.0 0.3?0.6 0.0?0.3 0.0 0.0?0.3 0.3?0.6 0.6?1.0
< 81% 60% 45% 8% 32% 23% 10%
= 9% 17% 23% 84% 23% 17% 9%
> 10% 23% 32% 8% 45% 60% 81%
Manual Classifier Output and Confidence: Recall
label < < or > >
0.6?1.0 0.3?0.6 0.0?0.3 0.0 0.0?0.3 0.3?0.6 0.6?1.0
< 23% 18% 28% 1% 20% 7% 3%
= 5% 9% 26% 20% 26% 9% 5%
> 3% 7% 20% 1% 28% 18% 23%
Table 4: The precision and recall confusion matrices of the classifier ? judgements on whether one hypothesis is worse
than, equivalent to or better than another hypothesis are compared to the classifier?s output and confidence.
It is thus most similar to SVM-RANK and Tesla
metrics, submissions to the WMT?10 shared met-
rics task (Callison-Burch et al, 2010) which also
used SVMs for ranking translations. However, both
metrics used SVMrank (Joachims, 2006) directly for
ranking (unlike TerrorCat, which uses a binary clas-
sifier for pairwise comparisons). Their features in-
cluded some of the metric outputs (BLEU, ROUGE,
etc.) for SVM-RANK and similarity scores between
bags of n-grams for Tesla (Dahlmeier et al, 2011).
5 Conclusions
We introduced the TerrorCat metric, which performs
pairwise comparison of translation hypotheses based
on frequencies of automatically obtained error cate-
gories using a binary classifier, trained on manually
ranked data. The comparison outcome is then con-
verted to a numeric score for every sentence or doc-
ument translation by averaging out the number of
wins per translation system.
Our submitted system achieved an average
system-level correlation with human judgements in
the WMT?11 development set of 0.86 for transla-
tion into English and 0.85 for translations from En-
glish into other languages. Particularly good per-
formance was achieved on translations from English
into Czech (0.90) and back (0.95). Sentence-level
scores are more modest: average 0.27 for transla-
tion into English and 0.23 for those out of English.
The scores remain to be checked against the human
judgments from WMT?12.
The introduced TerrorCat metric has certain de-
pendencies. For one thing, in order to apply it to
new languages, a training set of manual rankings is
required ? although this can be viewed as an advan-
tage, since it enables the user to tune the metric to
his/her own preference. Additionally, the metric de-
pends on lemmatization and PoS-tagging.
There is a number of directions to explore in the
future. For one, both Addicter and Hjerson report
MT errors related more to adequacy than fluency, al-
though it was shown last year (Parton et al, 2011)
that fluency is an important component in rating
translation quality. It is also important to test how
well the metric performs if lemmatization and PoS-
tagging are not available.
For this year?s competition, training data was
taken separately for every language pair; it remains
to be tested whether combining human judgements
with the same target language and different source
languages leads to better or worse performance.
To conclude, we have described TerrorCat, one
of the submissions to the metrics shared task of
WMT?12. TerrorCat is rather demanding to apply on
one hand, having more requirements than the com-
mon reference-hypothesis translation pair, but at the
same time correlates rather well with human judge-
ments on the system level.
69
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011.
Tesla at wmt 2011: Translation evaluation and tunable
metric. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 78?84, Edinburgh,
Scotland.
Mark Fishel, Ondr?ej Bojar, and Maja Popovic?. 2012.
Terra: a collection of translation error-annotated cor-
pora. In Proceedings of the 8th LREC, page in print,
Istanbul, Turkey.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
Philadelphia, USA.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the HLT-
NAACL Conference, pages 104?111, New York, NY.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 108?115, Edinburgh, Scot-
land.
John C. Platt. 1998. Using analytic qp and sparseness
to speed training of support vector machines. In Pro-
ceedings of Neural Information Processing Systems
11, pages 557?564, Denver, CO.
Maja Popovic? and Hermann Ney. 2011. Towards au-
tomatic error analysis of machine translation output.
Computational Linguistics, 37(4):657?688.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to german. In Proceedings
of the ACL SIGDAT-Workshop, Dublin, Ireland.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
Czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Daniel Zeman, Mark Fishel, Jan Berka, and Ondr?ej Bo-
jar. 2011. Addicter: What is wrong with my transla-
tions? The Prague Bulletin of Mathematical Linguis-
tics, 96:79?88.
70
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 405?407,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Ranking Translations using Error Analysis and Quality Estimation
Mark Fishel
Institute of Computational Linguistics
University of Zurich, Switzerland
fishel@cl.uzh.ch
Abstract
We describe TerrorCat, a submission to
this year?s metrics shared task. It is a ma-
chine learning-based metric that is trained
on manual ranking data from WMT shared
tasks 2008?2012. Input features are
generated by applying automatic transla-
tion error analysis to the translation hy-
potheses and calculating the error cate-
gory frequency differences. We addition-
ally experiment with adding quality es-
timation features in addition to the er-
ror analysis-based ones. When evaluated
against WMT?2012 rankings, the system-
level agreement is rather high for several
language pairs.
1 Introduction
Recently a couple of methods of automatic analy-
sis of translation errors have been described (Ze-
man et al, 2011; Popovic? and Ney, 2011). Both of
these compare a hypothesis translation to a refer-
ence and draw detailed conclusions from the dif-
ferences between the two.
TerrorCat, a metric submitted to the metrics
shared task of WMT?2012 (Callison-Burch et al,
2012) used the output of those two error analysis
methods as input features, which yielded mildly
promising results (Fishel et al, 2012). How-
ever the submitted version of TerrorCat was lan-
guage pair-specific, which means that the classifier
model used by the metric has to be retrained on
new manual pairwise ranking data for every new
language pair. This in turn complicates its usage.
Our main aim in this work is to make Terror-
Cat usable out-of-the-box. We compare models
specific to the language pair (baseline), target lan-
guage and a universal model for all languages. The
updated metric is applied to the WMT?13 metrics
shared task.
An additional modification to the metric uses
input features from quality estimation. Using the
resources of the quality estimation shared task of
WMT?13 the modified model is applied to the
English?Spanish language pair.
We start by briefly re-introducing the TerrorCat
metric.
2 Baseline
The baseline TerrorCat metric is a machine
learning-based metric: it uses manually ranked
translation hypothesis pairs to train a classifier
model. The trained model is then used to predict a
ranking for new sentence pairs that have not been
ranked yet.
To convert the binary comparisons between
translation hypothesis pairs into a numeric score
per translation hypothesis the wins per hypothe-
sis are summed together. Previous year?s work
has shown (Fishel et al, 2012) that weighting
the wins with the classifier?s confidence for the
summed score improves agreement with human
judgements.
The input features for learning and classifica-
tion are obtained by
1. applying translation error analysis software
to the compared hypotheses,
2. getting the frequencies of every error type,
i.e. the ratio of words marked with that er-
ror type to the hypothesis sentence length,
3. and using each error type?s frequency differ-
ences between the two hypotheses as input
features.
Relative frequencies are used on both system and
segment level: i.e. the ratios of words marked with
a particular error type to the hypothesis translation
length. This guarantees that feature values lie in
the [?1, 1] range.
405
Translation error analysis is done with two
tools: Addicter (Zeman et al, 2011) and Hjer-
son (Popovic? and Ney, 2011). Both perform error
analysis by comparing the hypothesis and refer-
ence translations on word level and treating each
difference as an error of one or the other kind.
Translation error taxonomies as well as the way
word differences and their contexts are interpreted
differ between the two tools. In order to enable
independent input from both tools the feature vec-
tors obtained from them both tools are concate-
nated.
To increase the level of detail the frequencies of
each error category are counted separately for each
part-of-speech separately. As a result, e.g. instead
of having the information of order errors having a
particular frequency, the classifier will separately
see the frequencies of misplaced nouns, adjectives,
particles, etc.
3 Experiments
The usage of part-of-speech tags improves agree-
ment with human judgements (Fishel et al, 2012);
however, it also introduces language dependency
for the metric. In the first set of experiments we try
to remove this imposed dependency without losing
the achieved benefit.
3.1 Common Settings
We focused on six language pairs: between En-
glish and German, French and Spanish. Manual
ranking data for training was taken from WMT
shared task evaluations 2008?2011; data from
WMT?2012 was used as a development set to as-
sess the performance of metric variations.
Final models for the WMT?2013 shared task
were re-trained on the whole set of manual rank-
ings, from WMT 2008?2012.
The classifier used by TerrorCat is an SVM with
a linear kernel; more powerful kernels, such as ra-
dial basis function-based ones scaled poorly to the
high number of features and thus were not tested.
PoS-tagging was done using TreeTagger
(Schmid, 1995) with the pre-trained models for
English, German, French and Spanish.
3.2 Language Independence
It is natural to expect error categories to have
varying importance on the quality comparison be-
tween two translation candidates. For instance,
one might expect order differences between trans-
lations into functional languages (e.g. English,
Chinese) to have a greater importance than in case
of languages with a more flexible word order (e.g.
German, Russian); on the other hand inflection er-
rors are likely to do more damage to the meaning
in morphologically complex languages (e.g. Rus-
sian, Finnish) than in languages with simpler mor-
phology (e.g. English, French). However, we want
to see whether we can train a classifier that would
generalize over all language pairs.
The main obstacle for training a general model
on all language pairs are the different taxonomies
of part-of-speech tags for different target lan-
guages: the arity of the input feature vectors is dif-
ferent for different target languages, which makes
the data incompatible between them.
To overcome the difference we define a map-
ping from every used taxonomy to a common gen-
eral set of PoS-tags, which is supposed to cover
any language. It consists of general part-of-speech
categories (such as noun, verb, particle, etc., a to-
tal of 15), without any morphological information
(tense, case, person, etc.).
By using the same set of generalized PoS-tags
for every language we ensure that the used Terror-
Cat classifier model is language-independent; the
PoS-tagging step is naturally language-dependent
still.
Tables 1 and 2 present system-level and
segment-level correlations of TerrorCat based on
this common PoS-tag set and three models, spe-
cific to the language pair, target language only
and a general model for any language. Both sets
of results show that using a language-independent
model neither improves nor worsens the perfor-
mance.
3.3 Quality Estimation for Ranking
To further improve the agreement between Terror-
Cat and human assessment we experimented with
adding input features from quality estimation.
The input features were adopted from this year?s
shared task on quality estimation. We selected the
smaller set of black-box features, which included
the sentence lenghts, their language model proba-
bilities, average numbers of translations per word,
percentages of uni-, bi- and tri-grams in the dif-
ferent frequency quartiles, etc. All resources were
taken from the shared task, which also meant that
this modified model was applied only to English?
Spanish.
406
de-en en-de es-en en-es fr-en en-fr
Language pair-specific 0.94 0.56 0.94 0.59 0.85 0.82
Target language-specific 0.92 0.56 0.97 0.59 0.84 0.82
Language-independent 0.93 0.71 0.94 0.66 0.84 0.88
BLEU 0.67 0.22 0.87 0.40 0.81 0.71
METEOR 0.89 0.18 0.95 0.45 0.84 0.82
TER 0.62 0.41 0.92 0.45 0.82 0.66
Table 1: System-level correlation between TerrorCat and human ranking. Correlations of BLEU, ME-
TEOR and TER scores are given for comparison.
de-en en-de es-en en-es fr-en en-fr
Language pair-specific 0.31 0.18 0.24 0.21 0.23 0.20
Target language-specific 0.31 0.18 0.28 0.21 0.23 0.20
Language-independent 0.28 0.20 0.27 0.22 0.24 0.21
Table 2: Segment-level correlation between TerrorCat and human ranking.
Training the model on quality estimation fea-
tures alone yields a system-level score of 0.56. Al-
though this is lower than the TerrorCat baseline,
it beats the correlations of BLEU, TER and ME-
TEOR (see Table 1). The segment-level correla-
tion is -0.01.
Next we combined features from error analysis
and quality estimation by concatenating them into
a single input feature vector. As a result system-
level correlation improved to 0.72, which is higher
than all TerrorCat variants so far (best correlation:
0.66). Segment-level correlation remained practi-
cally the same (0.22).
4 Conclusion
We have applied TerrorCat to the shared metrics
task of WMT?2013. Just like last year, the results
are mildly promising.
We were successful at achieving language inde-
pendence, provided that PoS-tagging is done be-
fore applying the metric.
The trained model as well as the metric imple-
mentation with all the necessary scripts is avail-
able online1.
It remains to be tested, whether quality es-
timation features fit well with the language-
independent models. As the extracted feature val-
ues are based on completely different, language-
specific resources, this does not seem to be a likely
outcome.
1https://github.com/fishel/TerrorCat
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada.
Mark Fishel, Rico Sennrich, Maja Popovic?, and Ondr?ej
Bojar. 2012. Terrorcat: a translation error
categorization-based mt quality metric. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 64?70, Montre?al, Canada.
Maja Popovic? and Hermann Ney. 2011. Towards au-
tomatic error analysis of machine translation output.
Computational Linguistics, 37(4):657?688.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
Proceedings of the ACL SIGDAT-Workshop, Dublin,
Ireland.
Daniel Zeman, Mark Fishel, Jan Berka, and Ondr?ej Bo-
jar. 2011. Addicter: What is wrong with my trans-
lations? The Prague Bulletin of Mathematical Lin-
guistics, 96:79?88.
407
