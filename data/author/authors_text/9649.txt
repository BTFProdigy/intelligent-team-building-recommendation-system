Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 324?333,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
 
Improving Chinese Semantic Role Classification  
with Hierarchical Feature Selection Strategy 
Weiwei Ding 
Institute of Computational Linguistics 
Peking University 
Beijing, 100871, China 
weiwei.ding.pku@gmail.com 
Baobao Chang 
Institute of Computational Linguistics 
Peking University 
Beijing, 100871, China 
chbb@pku.edu.cn 
 
 
 
 
 
 
Abstract 
In recent years, with the development of Chi-
nese semantically annotated corpus, such as 
Chinese Proposition Bank and Normalization 
Bank, the Chinese semantic role labeling 
(SRL) task has been boosted. Similar to Eng-
lish, the Chinese SRL can be divided into two 
tasks: semantic role identification (SRI) and 
classification (SRC). Many features were in-
troduced into these tasks and promising re-
sults were achieved. In this paper, we mainly 
focus on the second task: SRC. After exploit-
ing the linguistic discrepancy between num-
bered arguments and ARGMs, we built a se-
mantic role classifier based on a hierarchical 
feature selection strategy. Different from the 
previous SRC systems, we divided SRC into 
three sub tasks in sequence and trained models 
for each sub task. Under the hierarchical ar-
chitecture, each argument should first be de-
termined whether it is a numbered argument 
or an ARGM, and then be classified into fine-
gained categories. Finally, we integrated the 
idea of exploiting argument interdependence 
into our system and further improved the per-
formance. With the novel method, the classi-
fication precision of our system is 94.68%, 
which outperforms the strong baseline signifi-
cantly. It is also the state-of-the-art on Chi-
nese SRC. 
1 Introduction 
Semantic Role labeling (SRL) was first defined in 
Gildea and Jurafsky (2002). The purpose of SRL 
task is to identify and classify the semantic roles of 
each predicate in a sentence. The semantic roles 
are marked and each of them is assigned a tag 
which indicates the type of the semantic relation 
with the related predicate. Typical tags include 
Agent, Patient, Source, etc. and some adjuncts 
such as Temporal, Manner, Extent, etc. Since the 
arguments can provide useful semantic information, 
the SRL is crucial to many natural language proc-
essing tasks, such as Question and Answering (Na-
rayanan and Harabagiu 2004), Information Extrac-
tion (Surdeanu et al 2003),  and Machine Transla-
tion(Boas 2002). With the efforts of many re-
searchers (Carreras and M?rquez 2004, 2005, Mo-
schitti 2004, Pradhan et al2005, Zhang et al2007), 
different machine learning methods and linguistics 
resources are applied in this task, which has made 
SRL task progress fast.   
Compared to the research on English, the re-
search on Chinese SRL is still in its infancy stage. 
Previous work on Chinese SRL mainly focused on 
how to transplant the machine learning methods 
which has been successful with English, such as 
Sun and Jurafsky (2004), Xue and Palmer (2005) 
and Xue (2008). Sun and Jurafsky (2004) did the 
preliminary work on Chinese SRL without any 
large semantically annotated corpus of Chinese. 
They just labeled the predicate-argument structures 
of ten specified verbs to a small collection of Chi-
nese sentences, and used Support Vector Machines 
to identify and classify the arguments. This paper 
made the first attempt on Chinese SRL and pro-
duced promising results. After the PropBank (Xue 
and Palmer 2003) was built, Xue and Palmer (2005) 
and Xue (2008) have produced more complete and 
systematic research on Chinese SRL. 
Moschitti et al (2005) has made some prelimi-
nary attempt on the idea of hierarchical semantic 
324
 role labeling. However, without considerations on 
how to utilize the characteristics of linguistically 
similar semantic roles, the purpose of the hierar-
chical system is to simplify the classification proc-
ess to make it less time consuming. So the hierar-
chical system in their paper performs a little worse 
than the traditional SRL systems, although it is 
more efficient.  
Xue and Palmer (2004) did very encouraging 
work on the feature calibration of semantic role 
labeling. They found out that different features 
suited for different sub tasks of SRL, i.e. semantic 
role identification and classification. For semantic 
analysis, developing features that capture the right 
kind of information is crucial. Experiments on 
Chinese SRL (Xue and Palmer 2005, Xue 2008) 
reassured these findings.  
In this paper, we mainly focus on the semantic 
role classification (SRC) process. With the find-
ings about the linguistic discrepancy of different 
semantic role groups, we try to build a 2-step se-
mantic role classifier with hierarchical feature se-
lection strategy. That means, for different sub tasks, 
different models will be trained with different fea-
tures. The purpose of this strategy is to capture the 
right kind of information of different semantic role 
groups. It is hard to do manual selection of features 
since there are too many feature templates which 
has been proven to be useful in SRC; so, we de-
signed a simple feature selection algorithm to se-
lect useful features automatically from a large set 
of feature templates.  With this hierarchical feature 
selection architecture, our system can outperform 
previous systems. The selected feature templates 
for each process of SRC can in turn reassure the 
existence of the linguistic discrepancy. At last, we 
also integrate the idea of exploiting argument in-
terdependence to make our system perform better. 
 The rest of the paper is organized as follows. In 
section 2, the semantically annotated corpus - Chi-
nese Propbank is discussed. The architecture of our 
method is described in section 3. The feature selec-
tion strategy is discussed in section 4. The settings 
of experiments can be found in section 5. The re-
sults of the experiments can be found in section 6, 
where we will try to make some linguistic explana-
tions of the selected features.  Section 7 is conclu-
sions and future work. 
  
Figure 1. an example from PropBank 
 
2 The Chinese PropBank 
The Chinese PropBank has labeled the predicate-
argument structures of sentences from the Chinese 
TreeBank (Xue et al 2005). It is constituted of two 
parts. One is the labeled data, which indicates the 
positions of the predicates and its arguments in the 
Chinese Treebank. The other is a dictionary which 
IP 
?? ?? ???? 
P NN NT 
NP-PN-SBJ VP 
PP-BNF VP 
VV 
NP-OBJ NP 
NN 
?? ?? ??
f1 NN 
????? ? 
AD NN P 
ARG2ADVP
ARG0 PP-TMP ARGM-TMP 
has the Sanxia Project insurance provide 
ARGM-ADV
ARG1 
service forthe insurance company now until 
Until now,          the insurance company     has       provided   insurance services       for       the Sanxia Project. 
325
 lists the frames of all the labeled predicates. Figure 
1 is an example from the PropBank1. We put the 
word-by-word translation and the translation of the 
whole sentence below the example.  
It is quite a complex sentence, as there are many 
semantic roles in it. In this sentence, all the seman-
tic roles of the verb ?? (provide) are presented in 
the syntactic tree. We can separate the semantic 
roles into two groups. 
The first group of semantic roles can be called 
the core arguments, which capture the core rela-
tions. In this sentence, there are three arguments of 
verb ?? (provide) in this sentence. ???? 
(the insurance company) is labeled as ARG0, 
which is the proto-agent of the verb. Specifically to 
the verb ?? (provide), it is the provider. ???
? (insurance services) is the direct object of the 
verb, and it is the proto-patient, which is labeled as 
ARG1. Specifically to the verb ?? (provide), it 
represents things provided. ????? (for the 
Sanxia Project) is  another kind of argument, 
which is labeled as ARG1, and it represents the 
receiver. 
The other group of semantic roles is called ad-
juncts. They are always used to reveal the periph-
eral information. There are two adjuncts of the tar-
get verb in this sentence: ???? (until recently) 
and ? (has), both of which are labeled as ARGM. 
However, the two ARGMs reveal information of 
different aspects. Besides the ARGM tags, the sec-
ondary tags ?TMP? and ?ADV? are assigned to the 
two semantic roles respectively. ?TMP? indicates 
that???? (until recently)  is a modifier repre-
senting the temporal information, and ?ADV? in-
dicates that? (has) is an adverbial modifier.  
In the Chinese PropBank, the difference of the 
two groups is obvious. The core arguments are all 
labeled with numbers, and they are also called the 
numbered arguments. The numbers range from 0 to 
4 in Chinese PropBank. The adjuncts are labeled 
with ?ARGM?. 
3 Building a Hierarchical Semantic Role 
Classifier 
In this section, we will discuss the linguistic fun-
daments of the construction of a hierarchical se-
                                                 
1 This sentence is extracted from chtb_082.fid of Chinese 
PropBank 1.0, and we made some simplifications on it. 
mantic role classifier. We use ?hierarchical? to dis-
tinguish our classifier from the previous ?flat? ones.  
3.1 Linguistic Discrepancy of Different Se-
mantic Role Groups 
The purpose of the SRC task is to assign a tag to 
all the semantic roles which have been identified. 
The tags include ARG0-4, and 17 kinds of 
ARGMs (with functional tags). Previous SRC sys-
tems treat all the tags equally, and view the SRC as 
a multi-category classification task. However, we 
have different opinions of the traditional architec-
ture. 
Due to the discussions in section 2, we noticed 
that the semantic roles can be divided into two 
groups naturally according to the different kinds of 
semantic information represented by them. Here 
we will make some linguistic analysis of the two 
semantic role groups. Conversely to the process of 
the syntactic realization of semantic roles, we want 
to find out what linguistic features make a con-
stituent ARG0 instead of ARG1, or another con-
stituent ARGM-TMP instead of ARGM-ADV, i.e. 
what features capture the most crucial information 
of the two groups. 
As what we have assumed, the linguistic fea-
tures which made a syntactic constituent labeled as 
either one of the core arguments or one of the ad-
juncts varies greatly. Take the sentence in section 2 
as an example, even if the only information we 
have about the phrase ???? (until now) is that 
it is an adjunct of the verb, we can almost confirm, 
no matter where this node takes place in the pars-
ing tree, this constituent will be labeled as ARGM-
TMP. ? (has) is also the same. According to its 
meaning, the only category can be assigned to it is 
ARGM-ADV. But, things are quite different to the 
core argument. In the same sentence, ???? 
(the insurance company) is a good example. If we 
limit our observation to the phrase itself, we can 
hardly assert that it is the ARG0 of the target verb. 
Only when we extend our observation to the syn-
tactic structure level,  find out it is the subject of 
this sentence, and the voice of the sentence is ac-
tive, the semantic type of???? (the insurance 
company) is finally confirmed. If we have another 
sentence in which ???? (the insurance com-
pany) is not the subject, but rather the object, and 
the target verb is ?? (set up), then it will proba-
bly be labeled as ARG1.  
326
 Due to the analysis above, we can conclude the 
linguistic discrepancy of the two semantic role 
groups as follows. Core arguments and adjuncts 
share different kinds of inner linguistic consistency 
respectively. For the core arguments, the specific 
type cannot be determined with the information of 
the arguments only. At this level, the core argu-
ments are dependent on other information except 
the information about themselves. For example, the 
information of syntactic structures is crucial to the 
determination of the types of core arguments, and 
trivial differences of the syntactic structures will 
lead to the different output. Because of this, we can 
say that the core arguments are sensitive to the 
syntactic structures. Compared to the core argu-
ments, adjuncts are the opposite. They are rela-
tively independent on other information, since 
most of the adjuncts can be easily classified just 
based on the information about themselves2. And 
although the positions of the adjuncts in the syntac-
tic structure can vary, the types of the adjuncts are 
fixed. In this sense, the adjuncts are insensitive to 
the syntactic structures.  
After we made the linguistic discrepancy of the 
two semantic role groups, we can make a bold as-
sumption that the differences of the two groups can 
be reflected in the capability of different kinds of 
features to capture the crucial information for the 
two groups. For example, the ?voice? features 
seems to be crucial to the core arguments but use-
less to the adjuncts. This assumption provided us 
with the idea of a hierarchical feature selection sys-
tem.  
In this system, we first classify the constituents 
into two classes: core arguments and adjuncts. And 
then, the system classifies core arguments and ad-
juncts separately. For different subtasks we only 
select the most useful features and discard the less 
pertinent ones. We hope to take utilization of the 
most crucial features to improve semantic role 
classification. 
3.2 System Architecture 
Previous semantic role classifiers always did the 
classification problem in one-step. However, in 
this paper, we did SRC in two steps. The architec-
tures of hierarchical semantic role classifiers can 
                                                 
2 Extra features e.g. predicate may be still useful because that 
the information, provided by the high-level description of self-
descriptive features, e.g. phrase type, are limited. 
be found in figure 2, which is similar with that in 
Moschitti et al (2005). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. The architecture of our hierarchical SRC 
system 
As what has been shown in figure 2, a semantic 
role will first be determined whether it is a num-
bered argument or an ARGM by a binary-category 
classifier. And, then if the semantic role is a num-
bered argument, it will be determined by a 5-
category classifier designed for ARGX, i.e. the 
numbered arguments. If it is an ARGM, the func-
tional tag will be assigned by a 17-category classi-
fier built for ARGMs. Accordingly, with this hier-
archical architecture, the SRC problem is divided 
into 3 sub tasks, each of which has an independent 
classifier. 
3.3 Integrating the Idea of Exploiting Argu-
ment Interdependence 
Jiang et al (2005) has built a semantic role classi-
fier exploiting the interdependence of semantic 
roles. It has turned the single point classification 
problem into the sequence labeling problem with 
the introduction of semantic context features. Se-
mantic context features indicates the features ex-
tracted from the arguments around the current one. 
We can use window size to represent the scope of 
the context. Window size [-m, n] means that, in the 
sequence that all the arguments has constructed, 
the features of previous m and following n argu-
ments will be utilized for the classification of cur-
rent semantic role. There are two kinds of argu-
ment sequences in Jiang et al (2005), and we only 
test the linear sequence. Take the sentence in fig-
ure 1 as an example. The linear sequence of the 
arguments in this sentence is: ????(until then), 
 
Input Semantic Roles 
A binary-category classifier 
A 5-category 
classifier for 
ARGXs
A 17-category 
classifier for 
ARGMs
Output: Semantic Role tags 
327
 ???? (the insurance company), ? (has), ?
???? (for the Sanxia Project), ???? (in-
surance services). For the argument? (has), if the 
semantic context window size is [-1,2], the seman-
tic context features e.g. headword, phrase type and 
etc. of  ???? (the insurance company), ??
???  (for the Sanxia Project) and ???? 
(insurance services) will be utilized to serve the 
classification task of? (has). 
While their paper has improved the SRC per-
formance on English, it also has one potential dis-
advantage, which is that they didn?t separate the 
core arguments and ARGMs. The influence and 
explanations of this defect are presented in Section 
6. But in our hierarchical system, this problem can 
be solved. Since in the first step, we have separated 
the numbered arguments and ARGMs. We suppose 
that with the separation of the two semantic role 
groups, the system performance will be further im-
proved.  
4 Feature Selection Strategy 
Due to what we have discussed in the section 3.1, 
we need to select different features for the three 
sub task of SRC. In this paper, we did not make the 
selection manually; however, we make a simple 
greedy strategy for feature selection to do it auto-
matically. Although the best solution may not be 
found, automatic selection of features can try far 
more combinations of feature templates than man-
ual selection. Because of this, this strategy possibly 
can produce a better local optional solution. 
First, we built a pool of feature templates which 
has proven to be useful on the SRC. Most of the 
feature templates are standard, so only the new 
ones will be explained. The candidate feature tem-
plates include: 
Voice from Sun and Jurafsky (2004). 
Head word POS, Head Word of Prepositional 
Phrases, Constituent tree distance, from Pradhan 
etc. (2004). 
Position, subcat frame, phrase type, first word, 
last word, subcat frame+, predicate, path, head 
word and its POS, predicate + head word, predi-
cate + phrase type, path to BA and BEI, verb 
class 3 , verb class + head word, verb class + 
phrase type, from Xue (2008).  
                                                 
3 It is a bit different from Xue (2008), since we didn?t use the 
syntactic alternation information. 
predicate POS, first word +  last word, phrase 
type of the sibling to the left, phrase type of the 
sibling to the right, verb + subcate frame+, verb 
POS + subcat frame+, the amount of VPs in path, 
phrase type + phrase type of parent node, which 
can be easily understood by name. 
voice position, indicates whether the voice 
marker (BA, BEI) is before or after the constituent 
in focus. 
subcat frame*, the rule that expands the parent 
node of the constituent in focus. 
subcat frame@, the rule that expands the con-
stituent in focus. 
layer of the constituent in focus, the number of 
constituents in the ascending part of the path sub-
tracted by the number of those in the descending 
part of path, e.g. if the path is PP-BNF?VP?VP
?VV, the feature extracted by this template will 
be -1. 
 SemCat (semantic category) of predicate, Sem-
Cat of first word, SemCat of head word, SemCat of 
last word, SemCat of predicate + SemCat of  first 
word, SemCat of predicate + SemCat of  last word, 
predicate + SemCat of head word, SemCat of 
predicate + head word. The semantic categories of 
verbs and other words are extracted from the Se-
mantic Knowledge-base of Contemporary Chinese 
(Wang et al 2003).  
verb AllFrameSets, the combination of all the 
framesets of a predicate. 
 verb class + verb AllFrameSets, verb AllFra-
meSets + head word, verb AllFrameSets + phrase 
type. 
There are more than 40 feature templates, and it 
is quite difficult to traverse all the possible combi-
nations and get the best one. So we use a greedy 
algorithm to get an approximate optimal solution.  
The feature selection algorithm is as follows. 
Each time we choose one of the feature templates 
and add it into the system. The one, after which is 
added, the performance is the highest, will be cho-
sen. Then we continue to choose feature templates 
until there are no one left. In the end, there are a 
series of feature sets, which recorded the process 
of feature selection. Then we choose the feature set 
which can perform the best on development set. 
The code of feature selection algorithm is designed 
in Figure 3. 
328
 1. add all feature templates to set S ,the set of 
selected feature templates C0 is null 
2. for i = 0 to n-1, n is the number of elements 
in S 
3.        Pi =0  
4.        for each feature template ftj in set S 
5.               C?i  = Ci + ftj 
6.            train a model with features extracted 
by C? i and test on development set 
7.            if the result P?  > Pi 
8.               Pi = P? , k= j 
9.         end for 
10.        Ci+1  = Ci + ftk 
11.        S  = S ? ftk 
12. end for 
13. the set Cm correspondent to Pm, which is 
the highest, will be chosen. 
Figure 3. the greedy feature selection algorithm 
To make a comparison, we also built a tradi-
tional 1-step semantic role classifier based on this 
feature selection strategy. We will take this classi-
fier as the baseline system. 
5 Experiment Settings 
5.1 Classifier 
In our SRL system, we use a Maximum Entropy 
toolkit with tunable Gaussian Prior and L-BFGS 
parameter estimation, which is implemented by 
Zhang Le. This toolkit is available at 
http://homepages.inf.ed.ac.uk/s045
0736/maxent_toolkit.html. It can well 
handle the multi-category classification problem 
and it is quite efficient. 
5.2 Data 
We use Chinese PropBank 1.0 (LDC number: 
LDC2005T23) in our experiments. PropBank 1.0 
includes the annotations for files chtb_001.fid to 
chtb_931.fid, or the first 250K words of the 
Chinese TreeBank 5.1. For the experiments, the 
data of PropBank is divided into three parts. 648 
files (from chtb_081 to chtb_899.fid) are used as 
the training set. The development set includes 40 
files, from chtb_041.fid to chtb_080.fid. The test 
set includes 72 files, which are chtb_001 to 
chtb_041, and chtb_900 to chtb_931. We use the 
same data setting with Xue (2008), however a bit 
different from Xue and Palmer (2005). 
6 Results and Discussion 
The results of the feature selection are presented in 
table1. In this table, ?Baseline? indicates the 1-step 
architecture, and ?Hierarchical? indicates the ?hi-
erarchical feature selection architecture? imple-
mented in this paper. ?X_M?, ?ARGX? and 
?ARGM? indicate the three sub-procedures of the 
hierarchical architecture, which are ?ARGX and 
ARGM separation?, ?ARGX classification?, 
?ARGM classification? respectively. ?Y? in the 
table indicates that the feature template has been 
selected for the sub task. 
According to table 1, we can find some interest-
ing facts, which in turn prove what we found about 
semantic role groups in section 3.1.  
In table 1, feature templates related to the syn-
tactic structure includes: voice-related group (voice, 
voice information, path to BA and BEI), frame-
related group (verb class, verb class + head word, 
verb class + phrase type, all frames of verb, verb 
class + all frames of verb), the layer of argument, 
position and 4 kinds of subcat frames. As we as-
sumed before, these features are crucial to core 
arguments but of little use to adjuncts. The results 
have proven this assumption. Of the entire 14 syn-
tactic structure-related feature templates, 8 were 
selected by the ARGX process but only 2 was se-
lected by the ARGM process. The two exceptions 
should be viewed as the result of random impact, 
which cannot be avoided in automatic feature se-
lection. 
Compared with the different features selected by 
these tasks, we can find other interesting results. 
Few of the features selected by the X_M process 
also have related with the verb or the syntactic 
structures, which is quite similar with the ARGM 
process. This is probably because most of ARGMs 
are easy to be identified without syntactic structure 
information, which makes the opposite of ARGMs, 
i.e. the ARGXs easy to be filtered. Besides, the 
features selected by the baseline system have much 
in common with those selected by the ARGX 
process. This can be explained by the fact that both 
in the development and test set, the amount of core 
arguments outperforms that of adjuncts. The pro-
portions between core arguments and adjuncts are 
1.79:1 on the development set, and 1.63:1 on the 
test set. Because of the bias, the baseline system 
will tend to choose more syntactic structure-related 
features to label core arguments precisely. 
329
 Hierarchical 
Baseline 
X_M ARGX ARGM
Feature Name 
  Y  predicate 
Y  Y  predicate POS 
 Y  Y first word 
   Y first word + last word 
Y  Y  head word 
Y    head word POS 
Y Y   phrase type 
 Y  Y phrase type + phrase type of parent node 
   Y phrase type of the sibling to the left 
Y  Y  phrase type of the sibling to the right 
Y Y   position 
  Y  voice 
Y    voice position 
Y  Y  path to BA and BEI 
Y Y Y  verb class 
   Y verb class + head word 
Y Y   verb class + phrase type 
Y  Y  verb AllFrameSets 
Y  Y  verb class + verb AllFrameSets 
  Y  subcat frame 
 Y   subcat frame* 
  Y  subcate frame@ 
   Y subcat frame+ 
Y  Y  layer of the constituent in focus 
 Y Y Y predicate + head word  
Y Y Y Y predicate + phrase type 
Y Y Y  SemCat of predicate 
Y    SemCat of first word 
Y  Y  SemCat of last word 
   Y SemCat of predicate + SemCat of last word 
Y  Y  SemCat of head word 
Table 1. Feature selection results for the baseline and the hierarchical system 
  Baseline Hierarchical
DEV 95.15% 95.94% 
TEST 93.38% 94.31% 
Table 2. Comparison of the performance between the 
baseline and hierarchical system 
With this new architecture, we have achieved 
improvement on the performance of the semantic 
role classification, which can be found in table 2. 
Our classifier performs better both on the devel-
opment and the test set. The labeled precision on 
the development set is from 95.15% to 95.94%, 
and the test set is from 93.38% to 94.31%, with an 
ERR (error reduction rate) of 14.05%. Both of the 
improvements are statistically significant (?2 test 
with p= 0.05). The errors of SRC have three ori-
gins, which are correspondent to the three sub 
tasks of the hierarchical architecture. Detailed in-
formation of the comparison between the two sys-
tems can be found in table 3, which can tell us 
where the improvements come from. 
 Baseline Hierarchical
ARGX/ARGM errors 1.66% 1.75% 
inner ARGX errors 3.59% 2.75% 
inner ARGM errors 1.37% 1.19% 
TOTAL 6.62% 5.69% 
Table 3 Error rate analysis on the test set 
330
 In table 3, the percentages are calculated the 
way that the number of the errors was divided by 
the number of the arguments in the test set. 
ARGX/ARGM errors represent the errors that the 
semantic roles are classified into wrong group, e.g. 
ARGXs are labeled as ARGMs and vice versa. The 
inner errors represent the errors in a group, e.g. 
ARG0 are labeled as ARG1.  From this table, we 
can find that ARGX is the most difficult task. X-M 
and ARGM are less challenging. Besides the rela-
tively little error reduction in the ARGM process, 
the greatest part of improvement comes from the 
process of the most difficult sub task: the ARGX 
sub task. It is a bit surprising that the first step of 
the X_M in the hierarchical system process did not 
perform better than that in the baseline system. 
 Baseline Hierarchical Sum
ARG0 96.14% 96.58% 2046
ARG1 92.75% 94.60% 2428
ARG2 78.46% 78.85% 260
ARG3 60.00% 76.00% 25 
ARG4 40.00% 100.00% 5 
ARGM-ADV 96.64% 96.85% 1490
ARGM-ASP 100.00% 0.00% 1 
ARGM-BNF 91.30% 86.96% 23 
ARGM-CND 77.78% 77.78% 9 
ARGM-CRD N/A N/A 0 
ARGM-DGR N/A N/A 0 
ARGM-DIR 54.84% 58.06% 31 
ARGM-DIS 79.38% 79.38% 97 
ARGM-EXT 50.00% 25.00% 8 
ARGM-FRQ N/A N/A 0 
ARGM-LOC 90.91% 92.21% 308
ARGM-MNR 89.92% 91.13% 248
ARGM-PRD N/A N/A 0 
ARGM-PRP 97.83% 97.83% 46 
ARGM-TMP 95.41% 96.30% 675
ARGM-TPC 33.33% 8.33% 12 
TBERR4 0.00% 0.00% 2 
Table 4 Detailed labeled precision on the test set 
Table 4 presented the labeled precision of each 
type of semantic role. It demonstrates that with 
respect to ARGMs and ARGXs, the hierarchical 
system outperforms the baseline system. Further-
more, the improvement on ARGXs is greater than 
                                                 
4 From the name, TBERR possibly indicates the labeled errors 
in Chinese PropBank. However, we did not find any explana-
tions, so we just put it here and group it to ARGM. 
that of ARGMs. All types of numbered arguments 
get improvement in the hierarchical architecture, 
especially ARG1, ARG4 and ARG3. Although the 
performances of some types of the ARGMs de-
creased, the performances of all types of the 
ARGMs which occurs more than 100 times in-
creased, including ADV (adverbials), LOC (loca-
tives), MNR (manner markers) and TMP (temporal 
markers). 
After the hierarchical system was built, we tried 
to integrate the idea of exploiting argument inter-
dependence into our system. We extract the seman-
tic context features in a linear order, with the win-
dow size from [0,0] to [-3,3]. Larger window sizes 
are of little value since too few arguments have 
more than 6 other arguments in context. The re-
sults are presented in table 5. 
 Baseline Hierarchical
Base 93.38% 94.31% 
+window selection 93.38% 94.68% 
Table 5 integrating window selection into our system 
?Base? stands for the hierarchical system built 
above, without semantic context features. 
?+window selection? indicates the new system 
which has utilized the idea of exploiting argument 
interdependence. The best window sizes for the 
baseline system, ARGX and ARGM processes in 
the hierarchical system are [0,0], [-1,1], [0,0] re-
spectively, which were determined by testing on 
the development set. We can find that only for the 
ARGX process, the semantic context features are 
useful. For the baseline system and the ARGM 
process, exploiting argument interdependence does 
not help improve the performance. This conclusion 
is different from Jiang et al (2004), but it can be 
explained in the following way. 
In fact, the interdependence only exists among 
core arguments. For ARGMs, it is a different thing. 
An ARGM cannot provide any information about 
the type of the arguments close to it and the seman-
tic context features does not help the classification 
of ARGMs. Also, take the sentence in section 2 as 
an example, the fact that ???? (until now) is 
ARGM-TMP cannot raise the probability that??
?? (the insurance company) is ARG0 or ? (has) 
is ARGM-ADV and vice versa. However, if we 
know that ???? (the insurance company) is 
ARG0, at least the phrase ????  (insurance 
services) can never be ARG0. The semantic con-
text features extracted from or for ARGMs will do 
331
 harm to the improvement of the system, since they 
are irrelative information. Because of the same rea-
son, the performance of base system also decreased 
when semantic context features were extracted, 
since the core arguments and the ARGMs are 
mixed together in the baseline system.  
But for the ARGX sub task of our hierarchical 
system, since we have separated the numbered ar-
guments and ARGMs first, the influences of 
ARGMs can be eliminated. This made the interde-
pendence of core arguments can be directly ex-
plored from the extraction of semantic context fea-
tures. So the ARGX sub task is improved. 
To prove that our method is effective, we also 
make a comparison between the performances of 
our system and Xue and Palmer (2005), Xue 
(2008). Xue (2008) is the best SRL system until 
now and it has the same data setting with ours. The 
results are presented in Table 6. 
X & P (2005) Xue(2008) Ours 
93.9% 94.1% 94.68%
Table 6. Comparison with previous systems 
We have to point out that all the three systems 
are based on Gold standard parsing. From the table 
6, we can find that our system is better than both of 
the related systems. Our system has outperformed 
Xue (2008) with a relative error reduction rate of 
9.8%.  
7 Conclusions and Future Work 
In this paper, we have divided all the semantic 
roles into two groups according to their semantic 
relations with the verb. After the grouping of the 
semantic roles was made, we designed a hierarchi-
cal semantic role classifier. To capture the accurate 
information of different semantic role groups, we 
designed a simple feature selection algorithm to 
calibrate features for each sub task of SRC. It was 
very encouraging that the hierarchical SRC system 
outperformed the strong baseline built with tradi-
tional methods. And the selected features could be 
explained, which in turn proves that the linguistic 
discrepancy of semantic role groups not only exists 
but also can be captured. Then we integrated the 
idea of exploiting argument interdependence to 
further improve the performance of our system and 
explained linguistically why the results of our sys-
tem were different from the ones in previous re-
search. 
Although we make discriminations of arguments 
and adjuncts, the analysis is still coarse-grained. Yi 
et al (2007) has made the first attempt working on 
the single semantic role level to make further im-
provement. However, the impact of this idea is 
limited due to that the amount of the research tar-
get, ARG2, is few in PropBank. What if we could 
extend the idea of hierarchical architecture to the 
single semantic role level? Would that help the 
improvement of SRC?  
Acknowledgements 
This work was supported by National Natural Sci-
ence Foundation of China under Grant No. 
60303003 and National Social Science Foundation 
of China under Grant No. 06BYY048.  
We want to thank Nianwen Xue, for his gener-
ous help at the beginning of this work. And thanks 
to the anonymous reviewers, for their valuable 
comments on this paper. 
References 
Baker, Collin F., Charles J. Fillmore, and John B. Lowe. 
1998. The Berkeley FrameNet project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics, Montreal, Canada. 
Boas, Hans C. 2002. Bilingual FrameNet dictionaries 
for machine translation. In Proceedings of LREC 
2002, Las Palmas, Spain. 
Carreras, Xavier and Llu?s M?rquez. 2004. Introduction 
to the conll-2004 shared task: Semantic role labeling. 
In Proceedings of the Eighth Conference on Natural 
Language Learning, Boston, Massachusetts. 
Carreras, Xavier and Llu?s M?rquez. 2005. Introduction 
to the conll-2005 shared task: Semantic role labeling. 
In Proceedings of the Nineth Conference on Natural 
Language Learning, Ann Arbor, Michigan. 
M?rquez, Llu?s, Xavier Carreras, Kenneth C. Litkowski, 
Suzanne Stevenson. 2008. Semantic Role Labeling: 
An Introduction to the Special Issue, Computational 
linguistics. 34(2):146-159.. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguistics, 
28(3): 245-288. 
Jiang, Zheng Ping, Jia Li, Hwee Tou Ng. 2005. Seman-
tic Argument Classification Exploiting Argument In-
terdependence. In 19th International Joint Confer-
ence on Artificial Intelligence. Edinburgh, Scotland. 
332
 Kingsbury, Paul and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and 
Evaluation, Las Palmas, Spain. 
Kipper, Karin, Hoa Trang Dang, and Martha Palmer. 
2000. Class-based construction of a verb lexicon. In 
Proceedings of the Seventeenth National Conference 
on Artificial Intelligence and Twelfth Conference on 
Innovative Applications of Artificial Intelligence, 
Austin, Texas, USA. 
Moschitti. Alessandro. 2004. A Study on Convolution 
Kernels for Shallow Statistic Parsing. In Proceedings 
of the 42nd Meeting of the Association for Computa-
tional Linguistics, Barcelona, Spain. 
Moschitti, Alessandro, Ana-Maria Giuglea, Bonaven-
tura Coppola, and Roberto Basili. 2005. Hierarchical 
semantic role labeling. In Proceedings of the Nineth 
Conference on Natural Language Learning, Ann Ar-
bor, Michigan. 
Narayanan, Srini and Sanda Harabagiu. 2004.  Question 
answering based on semantic structures. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics, Geneva, Switzerland. 
Pradhan, Sameer, Kadri Hacioglu, Valerie Kruglery, 
Wayne Ward, James H. Martin, Daniel Jurafskyz. 
2004. Support vector learning for semantic argument 
classification. Machine Learning Journal, 60(1-3):11-
39. 
Sun, Honglin and Daniel Jurafsky. 2004. Shallow Se-
mantic Parsing of Chinese. In Proceedings of the 
Human Language Technology Conference of the 
North American Chapter of the Association for Com-
putational Linguistics, Boston, Massachusetts. 
Surdeanu, Mihai, Sanda Harabagiu, John Williams, and 
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of the 
41st Annual Meeting of the Association for Computa-
tional Linguistics, Ann Arbor, Michigan. 
Wang, Hui, Weidong Zhan, Shiwen Yu. 2003. The 
Specification of The Semantic Knowledge-base of 
Contemporary Chinese, In Journal of Chinese Lan-
guage and Computing, 13(2):159-176. 
Xue, Nianwen and Martha Palmer. 2003. Annotating the 
Propositions in the Penn Chinese Treebank. In Pro-
ceedings of the 2nd SIGHAN Workshop on Chinese 
Language Processing, Sapporo, Japan. 
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, Barcelona, Spain. 
Xue, Nianwen and Martha Palmer. 2005. Automatic 
semantic role labeling for Chinese verbs. In 19th In-
ternational Joint Conference on Artificial Intelligence. 
Edinburgh, Scotland. 
Xue, Nianwen, Fei Xia, Fu dong Chiou, and Martha 
Palmer. 2005. The Penn Chinese TreeBank: Phrase 
Structure Annotation of a Large Corpus. Natural 
Language Engineering, 11(2):207?238. 
Xue, Nianwen. 2008. Labeling Chinese predicates with 
semantic roles. Computational linguistics. 34(2):225-
255. 
Yi, Szu-ting, Edward Loper, Martha Palmer. 2007. Can 
Semantic Roles Generalize Across Genres? In Pro-
ceedings of Human Language Technologies  and The 
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, Rochester, 
NY, USA. 
Zhang, Min, Wanxiang Che, Ai Ti Aw, Chew Lim Tan, 
Guodong Zhou, Ting Liu, Sheng Li. 2007. A Gram-
mar-driven Convolution Tree Kernel for Semantic 
Role Classification, in Proceedings of the 45th An-
nual Meeting of the Association of Computational 
Linguistics, Prague, Czech Republic. 
333
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 1017 ? 1028, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Extracting Terminologically Relevant Collocations in the 
Translation of Chinese Monograph* 
Byeong-Kwu Kang, Bao-Bao Chang, Yi-Rong Chen, and Shi-Wen Yu 
The Institute of Computational Linguistics, Peking University, Beijing, 100871?China 
{kbg43, chbb, chenyr, yusw}@pku.edu.cn 
Abstract. This paper suggests a methodology which is aimed to extract the 
terminologically relevant collocations for translation purposes. Our basic idea is 
to use a hybrid method which combines the statistical method and linguistic 
rules. The extraction system used in our work operated at three steps: (1) 
Tokenization and POS tagging of the corpus; (2) Extraction of multi-word units 
using statistical measure; (3) Linguistic filtering to make use of syntactic 
patterns and stop-word list. As a result, hybrid method using linguistic filters 
proved to be a suitable method for selecting terminological collocations, it has 
considerably improved the precision of the extraction which is much higher 
than that of purely statistical method. In our test, hybrid method combining 
?Log-likelihood ratio? and ?linguistic rules? had the best performance in the 
extraction. We believe that terminological collocations and phrases extracted in 
this way, could be used effectively either to supplement existing terminological 
collections or to be used in addition to traditional reference works. 
1   Introduction 
Communication between different individuals and nations is not always easy, 
especially when more than one language is involved. This kind of communication can 
include translation problems, which can be solved by the translators who bridge the 
gap between two different languages.  
Through the past decade, China and Korea have been undergoing large economic, 
cultural exchange, which invariably affects all aspects of communication, particularly 
translation. New international contacts, foreign investments as well as cross-cultural 
communication have caused an enormous increase in the volume of translations 
produced and required. But by now, most of all this translation work has been 
conducted by translators alone, which bears the burden of an enormous translation 
task to them.  
In order to accomplish these tasks with maximum efficiency and quality, a new 
translation method supported by computer technology has been suggested. MAHT, 
also known as computer-assisted translation involves some interaction between 
translator and the computer. It seems to be more suited for the needs of many 
                                                          
*  This work has been supported by The National Basic Research Program of China(973 
program, No. 2004CB318102) and the 863 program (No. 2001AA114210, 2002AA117010). 
1018 B.-K. Kang et al 
organizations which have to handle the translation of the documents.  Computer-
assisted translation systems are based on ?translation memory? and ?terminology 
databases?. With translation memory tools, translators have immediate access to 
previous translations of the text, which they can then accept or modify.   
Terminology management systems also can prove very useful in supporting 
translator?s work [2, 11]. Most translators use some sort of glossary or terminology 
database, especially in the translation of the technical documents or academic 
monograph. Many translation bureaux have the collection of the terminology data 
bases. But time pressure and costs make it difficult to get glossary building task done 
fully manually. Thus there is a pressing need for the tool which is computationally 
supported. For Chinese, other than for English, terminology management tools are not 
so sophisticated that they could provide wide enough coverage to be directly usable 
for the translators.  
We are contemplating, in this article, situations where computational support is 
sought to extract the term candidate, construct or enhance such terminology 
databases. Our work will be more focused on the problem of terminologically relevant 
collocation extraction.  
In order to extract multiword terms from the domain corpus, three main strategies 
have been proposed in the literature. First, linguistic rule-based systems propose to 
extract relevant terms by making use of parts of speech, lexicons, syntax or other 
linguistic structure [2, 4]. This methodology is language dependent rather than 
language independent, and the system requires highly specialized linguistic 
techniques to identify the possible candidate terms. Second, purely statistical systems 
extract discriminating multiword terms from the text corpora by means of association 
measures [5, 6, 7]. As they use plain text corpora and only require the information 
appearing in texts, such systems are highly flexible and extract relevant units 
independently from the domain and the language of the input text. Finally, hybrid 
methodologies define co-occurrences of interest in terms of syntactical patterns and 
statistical regularities [1, 3, 9].  
There is no question that the term extraction work comes into play when the tools 
are parameterized in such a way as to provide as much relevant material (maximizing 
recall and precision), and as little ?noise? as possible.  As seen in the literature, 
neither purely rule-based approach nor statistic based approach could bring an 
encouraging result alone[3, 4]. The main problem is the "noise".  So we need to find a 
combined technique for reducing this ?noise?.  In this paper, we have taken a hybrid 
approach which combines the linguistic rules and statistical method. First, we applied 
a linguistic filter which selects candidates from the corpus. Second, the statistical 
method was used to extract the word class  combinations. And then, the results of 
several experiments were evaluated and compared with each other. 
2   Methodology Overview 
The basic idea in our work is that the extraction tool operates on pre-processed corpus 
which contains the results of tokenizing word and word class annotation (POS-
tagging). Figure1 contains an annotated sentence from one of the Chinese academic 
monograph[18]. 
 Extracting Terminologically Relevant Collocations in the Translation 1019 
<s id=2> 
??/p  ??/n  ??/n  ?/u  ??/d  ???/v  ?/w  ??/n  ???/d  ?
?/a  ?/u  ??/v  ?/p  ??/n  ??/n  ?/p  ???/n  ??/v  ??/n  ?
/w 
Fig. 1. Sample annotated text (tagged by the Peking University Tagger) 
And the extraction routine used in our work operated at three steps: 
(1)Tokenization and POS Tagging; (2)Extraction of the candidates from the corpus; 
(3)Linguistic  filtering(making use of syntactic patterns and stop-word list). The 
schema in Figure2 summarizes the three steps of pre-processing and extracting the 
term candidate. The extraction is automatic once the appropriate templates are 
designed. 
 
Fig. 2. Simplified schema of term extraction from a corpus 
3   Statistical Method 
Statistical methods in computational linguistics generally share the fundamental 
approach to language viewed as a string of characters, tokens or other units, where 
patterns are discovered on the basis of their recurrence and co-occurrence. 
Accordingly, when we approach the extraction of multi-word terms from a statistical 
point of view, we initially retrieve the word sequences which are not only frequent in 
their occurrence but also collocating each other.  
Before a statistical methodology could be developed, some characteristics of terms 
in Chinese had to be established. In Chinese, the length of terms can vary from single 
word to multi-words(n-gram), with the majority of entries being less than 4-word 
items, usually two word items(bi-gram) (See in 4.3). The number of n-grams with n>4 
Raw Corpus 
Annotated  text 
List of extraction result 
List of Term candidate 
filtered
Step 2. Extraction based on statistical information
Step 3.    Linguistic  filtering  
Step 1.   Tokenization and POS Tagging 
1020 B.-K. Kang et al 
is very small, and the occurrence of which is also rare. Therefore, the problems of bi-
grams, tri-grams and 4-grams are primarily taken into considerations in our work. 
Now let us consider the correlation between two neighboring words A and B. 
Assuming that these two words are terminologically relevant units, we can intuitively 
expect that they occur more often than random chance. From a statistical point of 
view, this probability can be measured by several statistical methods, such as ?co-
occurrence frequency?, ?Mutual Information?, ?Dice coefficient?, ?Chi-square test?, 
?log-likelihood?, etc[1, 6, 15]. 
Table 1 lists several statistical measures which have been widely used in extracting 
collocations. In table 1: XY represents any two word item? X stands for all words 
except X?  N is the size of corpus? Xf  and XP  are frequency and probability of X 
respectively? XYf  and XYP  are frequency and probability of XY respectively?And 
assuming that two words X and Y are independent of each other, the formulas are 
represented as follows: 
Table 1. Statistical methods used in multi word extraction 
Method Formula 
Frequency(Freq) XYf  
Mutual Information 
(MI) 2log
XY
X Y
P
P P
 
Dice Formula 
(Dice) 
2 XY
X Y
f
f f+  
Log-likelihood(Log-L) 
( )
2 log
( ) ( )
Y
XY XY
f
X Y X Y
ff
XY XY X Y XY
P P P P
P P P P
?  
Chi-squared(Chi) 
2( )
( )( )( )( )
XY XY XY XY
XY XYXY XY XY XY XY XY
N f f f f
f f f f f f f f
?
+ + + +
 
For the purposes of this work, we used these five statistics to measure the 
correlation of neighboring words. The statistical criterion of judgments is the value of 
measures which can judge the probability whether they belong to the rigid 
collocations or not. From a statistical point of view, we can say that if the value of 
measure is high, the two word combination is more likely to be a rigid collocation. 
And XY could be accepted as a collocation if its statistical value is larger than a given 
threshold. Those bi-gram candidates with correlation coefficient smaller than a pre-
defined threshold are considered to occur randomly and should be discarded. Others 
are sorted according to their correlation coefficient in descending order.  
Tri-gram and 4-gram candidates were processed in the same way. To compute the 
correlation coefficient of all tri-grams, we just considered a tri-gram as the 
 Extracting Terminologically Relevant Collocations in the Translation 1021 
combination of one bi-gram and one word, and then calculated their correlation 
coefficient. Similarly, a 4-gram was considered either as the combination of a tri-
gram and a word, or the combination of two bi-grams [12].  
As mentioned before, our methodology was tested on pre-processed corpus which 
contained the result of word class annotation. The extraction test was delivered on word 
sequence (POS tags) combinations. And the test corpus was a Chinese academic 
monograph [18]. The size of this corpus is 0.2 million Chinese characters, including 
about 5,000 sentences. In our test, the extraction of multi-word units was based on 
65,663 candidate bi-grams. Among these candidates, when their correlation coefficients 
were higher than a given threshold, they were considered as multi-word unit, and then 
sorted in descending order. The results of experiment are shown in Figure3. 
0
10
20
30
40
50
60
70
80
90
100 200 300 400 500 1000
Highly valued candidates
P
r
e
c
i
s
i
o
n
(
%
)
MI
DICE
LogL
Chi
 
Fig. 3. Comparison of Extraction Performance between different statistical measures 
Table 2. The sample result sorted by Chi Square value 
1stWord 2ndWord Chi LogL DICE MI 
?? 
?
? 7822.14 1278.48 517.581 5.28183 
??
 ?? 4233.43 42.8348 2520 10.4636 
?? ?
?
 3085.64 160.647 4560 7.59925 
?? ? 1461.41 424.818 767.36 3.90891 
?? ? 809.168 38.2226 844 7.66964 
?
? 
??
 752.637 124.353 787.243 5.16173 
?? ? 619.694 111.527 1600 5.02194 
??
 
?
 582.425 52.0341 516.444 6.40501 
?? ? 549.119 286.884 17037.1 2.66906 
?
?
 
?
 336.283 58.0757 2166.67 5.13281 
?
 
?
? 296.196 52.8541 544.348 4.96744 
? ? 228.596 122.119 523.597 2.2667 
1022 B.-K. Kang et al 
An examination of the results first showed a significant difference in precision. 
Checked by hand, the precisions of Chi-square value and Log-likelihood ratio were 
relatively high. In contrast, the precisions of Mutual information and Dice formula 
were not so ideal.  
Considering the size of the corpus and the terminological richness of the texts, this 
result is not very encouraging. Regardless of any statistical measure, the precision and 
coverage of the extraction are not so high that could be directly used in the application 
system.  
More over, as shown in table 2, the purely statistical system extracts all multi-word 
units regardless of their types, so that we can also find sequences like ???
[zengjia](add)? [le](auxiliary word)?, ??? [butong](different)? [de](auxiliary 
word)?, ??[ye](also)?[shi](be)?, ???[zhuanhuan](change)?[wei](become)?, etc., 
for which we have no use in terminology. Clearly the output must be thoroughly 
filtered before the result can be used in any productive way. 
On the whole, the somewhat disappointing outcome of the statistical method 
provoked us to rethink the methodology and tried to include more linguistic 
information in the extraction of terminology. 
4   Hybrid Method Combining Statistical Method and Linguistic 
Rules 
To improve the precision and recall of the extraction system, it was decided to use 
two criteria determining whether a sequence was terminologically relevant or not. The 
first was to use the frequent syntactic patterns of terms. The idea underlying this 
method is that multi-word terms are constructed according to more or less fixed 
syntactic patterns, and if such patterns are identified for each language, it is possible 
to extract them from a POS tagged corpus. The second was to use a stop-word filter 
that a term can never begin or end in a stop-word. This would filter out things not 
relevant with the domain-specific collocation or term. 
4.1   Syntactic Patterns of Terms in Chinese 
Before a methodology for extracting the terminologically relevant word units could be 
developed, some characteristics of terms in Chinese had to be established. We were 
especially interested in the following: How many words do terms usually have in 
Chinese? What is the structure of multi-word units in terms of syntax and morphology? 
What kind of terms can be successfully retrieved by computational methods?  
To find answers to the above questions, an existing terminology database could be 
used as a sample. Because the source text to be tested in our work is related with 
computational or linguistic domain, we selected the terminology database of 
computational linguistics which was constructed by Peking University. This term 
bank currently contains over 6,500 entries in English and Chinese.  
An analysis of 6,500 term entries in Chinese showed that the length of terms can 
vary from 1 to over 6 words, with the majority of entries being two-word items, 
usually a ?noun+noun? sequence. The second most frequent type is a single-word 
term. As less than 5% of all entries exceed 4 words and single word terms can be 
 Extracting Terminologically Relevant Collocations in the Translation 1023 
identified with the use of monolingual or bilingual dictionary 1 , we decided that 
automatic extraction should be limited to sequences of 2-4 words. 
 
0 1000 2000 3000 4000
1 word
2 word
3 word
4 word
5 word
6 word
etc
  
0 500 1000 1500
n+n
vn+n
n+vn
n+v
a+n
b+n
v+v
v+n
vn+Ng
n+Ng
d+v
etc
 
 Fig. 4. Length of Chinese terms     Fig. 5. Syntactic patterns of two word terms 
As the next step we manually analyzed the syntactic patterns of Chinese terms and 
ordered them according to frequency. These patterns were needed for the second part 
of the experiment, the ?linguistically motivated? filtering. According to the analysis 
of the existing terms, multi-word terms have some kinds of fixed syntactic patterns. In 
many cases, these syntactic patterns are based on the combinations of different two 
word classes, such as ?noun+noun?, ?gerend verb+noun?, ?adjective+noun?, 
?noun+suffix? etc.  We found that there were about 30 syntactic patterns which 
covered almost 95% in the two word combinations. Therefore, we decided that these 
patterns could be used filtering in the extraction. In figure 6, certain types of word 
combinations are more typical for technical vocabulary than for general language.  
More than three word combinations also can be divided into two small parts whose 
syntactic structures are the same as those of two word terms. For example: ?(n+n)+n?, 
?(vn+n)+n?, ?(v+n)+(n+vn)?, ?(a+n)+(vn+n)?, etc. Therefore when we extracted 
three-word or four-word units, we didn?t set another syntactic rule for them. We just 
considered tri-gram as the combination of one bi-gram and one word. Similarly, 4-
gram was considered as the combination of different two bi-grams.  
Although we admit that these syntactic patterns are typical for certain type of 
technical prose only, we don?t think that they could filter out all the irrelevant units. If 
                                                          
1
  To extract a glossary of terms from a corpus, we must first identify single-word terms. But it 
might be slightly confusing for the computer to identify the single word terms alone. So we 
would like to set aside this problem for the sake of achieving efficiency. But we believe that 
the translator might not be troubled with single terms if he has some kind of dictionary in the 
translation of the source text. 
1024 B.-K. Kang et al 
we extract all combinations of a certain POS-shape, additional filters are needed 
afterwards, to identify those combinations which are terminologically relevant. 
Char*Patterns={"n+n","vn+n","n+vn","n+v","a+n","b+n","v+v","v+n"
,"vn+Ng","n+Ng","d+v","m+n","h+n","f+v","a+v","f+n","j+n","a+Ng
","vn+k","b+vn","b+Ng","Ag+n","v+Ng","a+nz","vn+v","nz+n","b+k
","v+k","j+n","nz+v",null}; 
Fig. 6. The syntactic patterns for filtering2 
4.2   Stop-Word Filter in Chinese 
When we examine multi word units regardless of their type, we can easily find some 
words which have no use in terminology. These irrelevant or meaningless data is a 
noise for extracting desired data. To resolve this problem, we can make use of the stop 
word list to be filtered. In the system, it would filter out things irrelevant with the 
domain-specific collocation or term. But how can we make the set of stop words? 
Indeed, the stop word list is rather flexible than firmly fixed in their usage. Whenever 
the words are frequent and meaningless in text, they can be stop words in a given task. 
For practical purposes, we used the word frequency data of the large technical 
domain corpora which was constructed by Beijing Language and Cultural University. 
In this data, we randomly selected the 2,000 words most highly frequent in their 
usage. And then we examined whether the frequent words were terminologically 
relevant or not. The analysis of the word data showed that 77.6% were domain 
dependent which could be the part of term, and 22.4% were general words. It means 
that terminologically irrelevant words amounted to about 450 words of the highly 
frequent 2000 words in technical corpora. The results are shown in Table 3. 
Table 3. The results of analysis on the high frequency words  
Frequency TerminologicallyRelevant words 
Terminologically
Irrelevant words Example 
1-100 44(44%) 56(56%) ?(aux), ?(be), ?(and), ?(at), ?(middle), etc. 
101-200 58(58%) 42(42%) ??(provide),?(to),?(serve 
as),??(possess), etc. 
201-500 229(76.3%) 71(23.7%) ? (good),?? (for),?(some),
?(only),??(other), etc. 
501-1000 408(81.6%) 92(18.4%) ? ? (quite), ? (see), ? ?(arose), ??(indicate),etc. 
1001-2000 813(81.3%) 187(18.7%) ??(leave),??(engage),?
?(even),??(need not),etc 
Total 1552(77.6%) 448(22.4%)  
                                                          
2
 These POS patterns are based on the tag sets of Peking University. 
 Extracting Terminologically Relevant Collocations in the Translation 1025 
According to these analyzed data, we made the set of stop words which amounted 
to about 450 words. And we used them for filtering out the frequent, meaningless 
words in a given text before the output can be used in any productive way.  
5   Experiments  
The hybrid methods combining statistical measure and linguistic rules were tested on 
pre-processed corpus. Based on the statistical method, the extraction test was limited 
to the boundary of the frequent syntactic patterns first, and then filtered out by the 
stop word list. Three different statistical measures were used to enhance the precision 
of the extraction, such as Log-likelihood ratio, Chi-square test and Mutual 
information. Because of the poor performance in our first test, Dice formula was not 
used in hybrid method any more. Therefore, we have delivered three different 
experiments using like ?LogL + Liguistic Filter?, ?Chi + Liguistic Filter?, ?MI + 
Liguistic Filter? methods.  
In Figure 7, we present the comparative results of precision rate among these 
different experiments. In order to measure the precision rate of the result, we used the 
grammatical criterion: A multi word n-gram could be considered as accurate result if 
it is grammatically appropriate. By grammatical appropriation, we refer to compound 
noun phrase or compound verb phrase, since with majority of multi-word terms have 
these structures.  
As a result, hybrid method using linguistic filters proved to be a suitable method for 
selecting terminological collocations, and it has considerably improved the precision of 
the extraction. The precision was much higher than that of purely statistical method, 
retrieving appropriate result almost 10%-20% higher than in the first experiment. In 
our test, hybrid method combining ?Log-likelihood ratio? and ?linguistic rules? had the 
best performance in the extraction. The precision was higher than 90%. According to 
their performance, the results of different experiments can be arranged like: 
LogL+Filter   >   Chi+Filter  >  MI+Filter  >  LogL  >  Chi  >  MI  >  Dice 
0
20
40
60
80
100
120
100 200 300 400 500 1000
Highly valued candidates
P
r
e
c
i
s
i
o
n
(
%
)
MI
DICE
LogL
Chi
MI+Filter
LogL+Filter
Chi+Filter
 
Fig. 7. Comparison of Extraction Performance between statistical measures and hybrid measure 
1026 B.-K. Kang et al 
In the analysis of the extraction data, we examined the precision of every 100 
multi-word candidates which sorted in descending order. Considering the size of 
corpus, we compared the results within the highly valued 1000 candidates. A sample 
of the highly valued output is seen in Table 4. 
Table 4. The sample result sorted by Log-likelihood ratio 
1stWord 2ndWord LogL+Filter CHI+Filter MI+Filter 
?? ?? 1026.38 3748.65 4.20189 
?? ?? 1020.43 5102.98 4.93017 
?? ?? 981.323 3651.52 4.23672 
?? ?? 899.731 7805.59 6.16647 
?? ?? 734.213 2284.06 3.76964 
?? ??? 718.016 14931.3 7.80401 
??? ??? 557.888 13569.4 8.11656 
?? ?? 537.196 2361.49 4.60008 
? ?? 500.011 12919.7 8.26776 
?? ?? 363.259 3535.04 6.19858 
?? ?? 355.499 2053.22 5.29117 
? ?? 345.551 6733.13 7.55539 
?? ?? 339.45 6092.73 7.41208 
?? ?? 329.536 1061.09 3.76944 
?? ?? 316.792 8130.74 8.08733 
As seen in Table 4, although not all these units would be considered terms in the 
traditional sense of the word, most of them either contain terms or include 
terminologically relevant collocations. Besides, our extraction started from these two 
word items, expanded to extract multi-word units like three word or four word units. 
Finally we could extract multi word units such as the following sample: 
Table 5. The sample of multi-word terms 
 Terminologically relevant units 
Two word units 
? ? ? ? (grammatical function), ? ? ? ? (directional 
complement),  ?????(specification), ????(container 
classifier), ????(usage frequency), etc. 
Three word units 
??????(grammatical knowledge-base), ??????
(Chinese Information Processing), ? ? ? ? ? ? (speech 
recognition system), etc. 
Four word units  
????????(MT system design), ????????
(language information processing technology), ???????
(context free grammar), etc.  
On the whole, as we think that the performance of the extraction was quite good, 
this method could be applicable in the translation system.  
 Extracting Terminologically Relevant Collocations in the Translation 1027 
6   Conclusions and Future Work 
The paper presents a methodology for the extraction of terminological collocations 
from academic documents for translation purposes. It shows that statistical methods 
are useful because they can automatically extract all the possible multi word units 
according to the correlation coefficient. But the purely statistical system extracts all 
multi-word units regardless of their types, so that we also find sequences which are 
meaningless in terminology. Clearly the output must be thoroughly filtered before the 
result can be used in any productive way. To improve the precision of the extraction 
system, we decided to use linguistic rules determining whether a sequence was 
terminologically relevant or not.  The frequent syntactic patterns of terminology and 
the stop-word list were used to filter out the irrelevant candidates. As a consequence, 
hybrid method using linguistic filters proved to be a suitable method for selecting 
terminological collocations, and it has considerably improved the precision of the 
extraction. The precision was much higher than that of purely statistical method.  
We believe that terminological collocations and phrases extracted in this way, 
could be used effectively either to supplement existing terminological collections or 
to be used in addition to traditional reference works. 
In future we envisage the development of techniques for the alignment of exact 
translation equivalents of multi-word terms in Chinese and Korean, and one way of 
doing so is by finding correspondences between syntactic patterns in both languages. 
Translation memory systems already store translations in a format similar to a parallel 
corpus, and terminology tools already involve functions such as ?auto-translate? that 
statistically calculate the most probable translation equivalent. By refining these 
functions and making them language specific, we could soon be facing a new 
generation of tools for translators. It remains to be seen, however, whether they can 
really be implemented into translation environments on broad scale. 
References 
1. Chang Bao-Bao, Extraction of Translation Equivalent Pairs from Chinese-English Parallel 
Corpus, Terminology Standardization and Information Technology, pp24-29, 2002. 
2. Bourigault, D. Lexter, A Natural Language Processing Tool for Terminology Extraction. 
In Proceedings of  7th EURALEX International Congress, 1996. 
3. Daille, B. Study and Implementation of Combined Techniques for Automatic Extraction of 
Terminology. In The balancing act combining symbolic and statistical approaches to 
language. MIT Press,  1995. 
4. Ulrich Heid, A linguistic bootstrapping approach to the extraction of term candidates from 
German text, http://www.ims.uni-stuttgart.de/~uli/papers.html, 2000 . 
5. Sayori Shimohata, Toshiyuki Sugio, JunjiI Nagata, Retrieving Domain-Specific 
Collocations By Co-Occurrences and Word Order Constraints, Computational 
Intelligence, Vol 15, pp92-100, 1999. 
6. Shengfen Luo, Maosong Sun Nation,Two-Character Chinese Word Extraction Based on 
Hybrid of Internal and Contextual Measures, 2003 
7. Smadja, F. Retrieving Collocations From Text: XTRACT. In Computational Linguistics, 
19(1) (pp 143--177).1993. 
1028 B.-K. Kang et al 
8. David Vogel, Using Generic Corpora to Learn Domain-Specific Terminology, Workshop 
on Link Analysis for Detecting Complex Behavior, 2003 
9. Dias, G. & Guillor?, S. & Lopes, J.G.P. Multiword Lexical Units Extraction. In 
Proceedings of the International Symposium on Machine Translation and Computer 
Language Information Processing. Beijing, China. 1999. 
10. Feng Zhi-Wei, An Introduction to Modern Terminology, Yuwen press, China, 1997. 
11. Ga?l Dias etc, Combining Linguistics with Statistics for Multiword Term Extraction, In 
Proc. of Recherche d'Informations Assistee par Ordinateur, 2000. 
12. Huang Xuan-jing & Wu Li-de & Wang Wen-xin, Statistical Acquisition of Terminology 
Dictionary, the Fifth Workshop on Very Large Corpora, 1997 
13. Jiangsheng Yu? ? ?Automatic Detection of Collocation http://icl.pku.edu.cn/yujs/ 2003 
14. Jong-Hoon Oh, Jae-Ho Kim, Key-Sun Choi, Automatic Term Recognition Through EM 
Algorithm, http://nlplab.kaist.ac.kr/, 2003 
15. Patrick Schone and Daniel Jurafsky, Is Knowledge-Free Induction of Multiword Unit 
Dictionary Headwords a Solved Problem?, In proceedings of EMNLP, 2001. 
16. Philip Resnik, I. Dan Melamed, Semi-Automatic Acquisition of Domain-Specific 
Translation Lexicons, Proceedings of the fifth conference on Applied natural language 
processing, pp 340-347, 1997. 
17. Sui Zhi-Fang, Terminology Standardization using the NLP Technology, Issues in Chinese 
Information Processing,pp341-352, 2003. 
18. Yu Shi-wen, A Complete Specification on The Grammatical Knowledge-base of 
Contemporary Chinese, Qinghua Univ. Press, 2003 
Extraction of Translation Unit from Chinese-English Parallel Corpora 
 
CHANG Baobao 
Institute of Computational Linguistics 
Peking University,  
Beijing, P.R.China, 100871  
chbb@pku.edu.cn 
Pernilla DANIELSSON and 
Wolfgang TEUBERT 
Centre for Corpus Linguistics 
Birmingham University,  
Birmingham, B15 2TT United Kingdom 
pernilla@ccl.bham.ac.uk 
teubertw@hhs.bham.ac.uk 
 
Abstract  
More and more researchers have recognized 
the potential value of the parallel corpus in the 
research on Machine Translation and Machine 
Aided Translation. This paper examines how 
Chinese English translation units could be 
extracted from parallel corpus. An iterative 
algorithm based on degree of word association is 
proposed to identify the multiword units for 
Chinese and English. Then the Chinese-English 
Translation Equivalent Pairs.are extracted from 
the parallel corpus. We also made comparison 
between different statistical association 
measurement in this paper.  
 
Keywords: Parallel Corpus, Translation 
Unit , Automatic Extraction of Translation 
unit  
Introduction 
The field of machine translation has changed 
remarkably little since its earliest days in the 
fifties. So far, useful machine translation could 
only obtained in very restricted domain. We 
believe one of the problems of traditional 
machine translation lies in how it deals with unit 
of translation. Normally Rule-Based Machine 
Translation system takes word as basic 
translation unit. However, word is normally 
polysemous and therefore ambiguous, which 
causes many difficulties in selecting proper 
target equivalent words in machine translation, 
especially in translation between unrelated 
language pairs, such as Chinese and English. On 
the other hand, human translation is rarely 
word-based. Human translators always translate 
group of words as a whole, which means human 
do not view words as the basic translation units, 
and it seems they view language expressions that 
can transfer meaning unambiguously as basic 
translation units instead. Following this 
observation, we believe translation unit shall be 
not only words but also words groups 
(Multi-Word Unit) and a collection of bilingual 
translation unit will be certainly a very useful 
resource to machine translation.    
Manual compilation of such a database of 
translation unit is certainly labor intensive. But 
following the recent progress in Corpus 
Linguistics, especially in parallel corpus 
research such as Gale,W. (1991), Tufis,D. (2001), 
Wu, D., Xia, X.(1994). Automatic identification of 
translation unit and its target equivalents from 
existed authentic translation might be a feasible 
solution; at least it can be used to produce a 
candidate list of bilingual translation unit.  
As a first step towards building a database of 
bilingual translation units, we selected the Hong 
Kong Legal Documents Corpus (HKLDC) as the 
parallel corpus for the feasibility study. This 
paper elaborates the methods we adopted. We 
will first give our model of (semi-) automatic 
acquisition of bilingual translation unit based on 
parallel corpora in section 1. Then we will show 
how the corpus could be preprocessed in section 
2. In section 3, several statistic measurements 
will be introduced which will serve as a basis for 
late steps in extracting of bilingual translation 
units. Section 4 will focuses on identification of 
multi-word units. Section 5 will describe how 
translation equivalents could be extracted. In 
section 6, we give some evaluation regarding to 
the performance in extracting the translation 
equivalent pairs.  
1 Framework of automatic acquisition of 
bilingual translation unit 
The whole process of identification of bilingual 
translation unit could be further divided into 
three major steps as depicted in Figure 1. 
(1) Preprocessing of the parallel corpora  
For the purpose of extracting bilingual 
translation unit, some prior processing of the 
corpus is necessary. These include alignment of 
the bilingual texts at sentence level and 
unilingual annotation of the Chinese and English 
texts respectively. 
(2)Identification of multi-word unit in the 
aligned the texts 
As we mentioned before, translation unit shall 
not be only single words, but also multi-word 
units. In this step, Both the Chinese and English 
multi-word units are identified separately from 
the corpus. 
(3) Extraction of the bilingual translation units     
After identification of the multi-word units for 
texts of both languages, this step tries to set the 
correspondence between Chinese and English 
translation units. The result of this step will be a 
list of bilingual Translation Equivalent Pairs and 
every TEP is composed of a Chinese Translation 
unit and an English Translation unit. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Framework of translation unit 
acquisition 
2 Preprocessing the corpus 
The Hong Kong legal documents were collected 
from Internet. The corpus is composed of laws 
and amendments issued by the Hong Kong 
Special Administration Region (HKSAR) during. 
All the texts in it are in both Chinese and 
English. We selected about 6 million words of 
both Chinese texts and English words 
(6,833,762 Chinese words and 6,391,919 
English words). 
All the Chinese texts in the corpus are 
encoded with Big-5 code. Since all our Chinese 
tools can only deal with Chinese GB code. We 
firstly converted all the Chinese texts from 
Big-5 code into GB code. Then the Corpus was 
aligned with a length-based sentence aligner. For 
the legal documents have been already well 
arranged with section by section, which makes 
the sentence alignment much easier and the 
precision is high. The Chinese texts were then 
segmented and pos-tagged with a program 
developed by the institute of Computational 
linguistics, Peking University. And all the 
English Texts were tokenized, lemmatized, and 
pos-tagged with a freely available tree-based 
tagger. Two tag sets were used for Chinese and 
English respectively, ICL/PKU tag set for 
Chinese texts and UPENN tag set for English 
texts. Figure 2. shows a sample of the corpus 
after preprocessing. 
Chinese texts English Texts 
? 
<s id=5> 
?  r 
??  n 
?  d 
? 
??  n 
??  n 
?  w 
?  w 
<s id=6> 
??  n 
?  w 
? 
? 
<s_id=5> 
This  DT this 
Ordinance NN ordinance 
may  MD may 
... 
General  JJ general 
Clauses  NNS clause 
Ordinance NN ordinance 
.  . . 
<s_id=6> 
Remarks NNS remark 
: : : 
Figure 2. Samples of the corpus after 
preprocessing 
In Figure 2., both corpus was arranged one token 
per line. The XML-like tag <s> marks the start 
of the sentence. The single-letter tags right to the 
Chinese tokens are their part of speech tags. The 
two columns right to the English tokens are part 
of speech tags and lemmas. 
3 Statistical measurement used 
Four statistical measurements were used in 
identification of unilingual multi-word units and 
the correspondences of the bilingual translation 
(3) TEP Extractor 
(1) 
Sentence Alignment 
Chinese Annotation English Annotation
(2) 
Chinese MWU 
Identification 
English MWU 
Identification 
units. All four statistical formulas measures the 
degree of association of two random events. 
Given two random events, X and Y, they 
might be two Chinese words appears in the 
Chinese texts and two translation units appears 
in an aligned region of the corpus. The 
distribution of the two events could be depicted 
by a 2 by 2 contingency table. 
 Y  Y?  
X  a b 
X?  c d 
Figure 3. A 2 by 2 contingency table 
The numbers in the four cells of the table has the 
following meanings: 
a : all counts of the cases the two events X 
and Y co-occur. 
b : all counts of the cases that X occurs but 
Y does not 
c : all counts of the cases that X does not 
occur but Y does 
d : all counts of the cases that both X and Y 
do not occur 
Based on the above-mentioned contingency 
table, different kinds of measurements could be 
used. We have tried four of them, namely, 
point-wise mutual information, DICE coefficient, 
2? score and Log-likelihood score. One other 
measurement used by Gale(1991) is 2? score, 
which is equivalent to the 2?  score. All the 
four measurements could be easily calculated 
using the following formula. 
(1) Point-wise mutual information  
)()(
log),( 2 caba
anttstMI +?+
?=  
(2) DICE coefficient 
)()(
2),( caba
attstDICE +?+=  
(3) 2? score 
)()()()(
)(),(?
2
2
dcdbcaba
cbdanttst +?+?+?+
????=  
(4) Log-Likelihood score 
)
)()(
log
)()(
log                
)()(
log
)()(
log(2),(
dbdc
nddcadc
ncc
dbba
nbbcaba
naattstLL
+?+
??++?+
??+
+?+
??++?+
???=
 
4. Identification of multi-word units 
What might constitute multi-word units is 
probably a question critical to identification of 
them. It seems rational to assume Multi-word 
units are something between phrases and words, 
which might have the following properties: 
1) The component words of a multi-word 
unit should tend to co-occur frequently. 
In the significance of statistics, 
multi-word unit should be word group 
that co-occur more frequently than 
expectation. 
2) Multi-words units are not arbitrary 
combinations of arbitrary words; they 
shall form valid syntactic structure in 
the meaning of linguistics. 
Based on the above-mentioned observations, 
we used an iterative algorithm using both 
statistical and linguistics means. The algorithm 
runs as follows: firstly the algorithm tries to find 
all word pairs that show strong coherence. This 
could be done using the measurements listed in 
section 3. After this step, all the word pairs in 
both of Chinese texts and English Texts whose 
association value is greater than a predefined 
threshold are marked. But this can only list of 
word groups of length of 2. Word groups of 
length more than 3 words could not be found by 
only one run of the algorithm. But apparently 
they could be found by a series of runs until 
there are no word groups having greater 
association value than the threshold anymore. 
The algorithm is designed as recursive structure, 
it marks longer word groups by viewing the 
shorter word group marked in the previous run 
as one word.  
It is no doubt that pure statistics cannot 
perform very reliable. Some word groups found 
by the algorithm are awkward to be accepted as 
multi-word unit. The result of the algorithm 
shall be viewed as a candidate list of 
multi-words units. Some kind of refinement of 
the results might be required. For thinking that 
multi-word unit shall form valid syntactic 
pattern, we use a filter module which check all 
the word groups found and see if they fall into a 
set of predefined syntactic patterns.  
"a+n",  
"b+n",  
"n+n",  
? 
"MWU+n", 
"n+MWU",  
"MWU+MWU" 
"NN+NN",  
"NN+NNS", 
?? 
"NN+IN<of>" 
"JJ+NN", 
? 
"MWU+MWU" 
Figure 4. Syntactic patterns 
 Figure 4. shows some patterns used by the 
filter. Patterns in the left side are for Chinese 
while the right side for English. 
5. Extracting of the bilingual translation units 
We adopt the same hypothesis-testing approach 
to set the correspondence between the 
Chinese-English translation units. It follows the 
observations that words are translation of each 
other are more likely to appear in aligned 
regions(Gale,W. (1991), Tufis,D. (2001)). But we 
also take the multi-word units into 
consideration. 
The whole procedure could be divided 
logically into two phases. The first phase could 
be called a generative phase, which lists all 
possible translation equivalent pairs from the 
aligned corpus. And the second phase can be 
viewed as a testing operation, which selects the 
Translation Equivalent Correspondences show 
an association measure higher than expected 
under the independence assumption as 
translation equivalence pairs. Again we use 
DICE coefficient, point-wise mutual information, 
LL score and 2?  score to measure the degree 
of association. 
One of problems of above-mentioned 
approach is its inefficiency in processing large 
corpus. Because in the generative phase, the 
above-mentioned approach will list all 
translation equivalent pairs and can lead to huge 
search space. To make the approach more 
efficient, we adopted the following assumption: 
Source translation units tend to be translated into 
translation units of the same syntactic categories. 
For example, English nouns tend to be translated 
into Chinese nouns, and English pattern 
?JJ+NN? tend to be translated into Chinese 
pattern ?a+n? or ?b+n?. Apparently, this 
assumption is not always true for translation of 
Chinese into English and vice versa. But it really 
makes the algorithm much more efficient while 
the precision does not fall severely. 
6. Experiments and Results 
We have performed some preliminary 
experiments to test the performance of different 
statistic measurements, performance change 
when the categorial hypothesis is used. 
For the experiments, we used a very small 
portion of the corpus of 500 sentence pairs. 
Figure 5. show the performance of Chinese 
MultiWord Unit Identification, we count how 
many correct MWUs are there in the first 
hundred of candidate MWUs produced by the 
program.  
 MI DICE LL 2?  
Correct 63 31 76 74 
Incorrect 37 69 24 26 
Accuracy  63? 31? 76? 74% 
Figure 5. Performance variations of different 
statistical measurements for identification of 
MWU 
Figure 6 shows the performance of the TEP 
extraction using different statitical means. we 
count how many correct and partially correct 
correspondences there are in the first hundred of 
translation equivalent pairs produced by the 
algorithm. 
 MI DICE LL 2?  
Correct 39 5 70 75 
Partially correct 5 1 10 6 
Accuracy  44? 6? 80? 81% 
Figure 6. Performance variations of different 
statistical measurements for TEP extraction 
  
Both Figure 5. and Figure 6 shows LL score and 
2?  score achieves better accuracy over mutual 
information and DICE coefficient.  
Experiments also show the categorial 
hypothesis might lead to fall in accuracy, we did 
tests on the above-mention 500 sentence pair 
corpus using the hypothesis, the precision fall by 
4% but the efficiency improved by more than 
200%. 
Figure 7. shows a sample of extracted 
translation equivalent pair from the test corpus. 
Some of them are wrong(see no 2), but most of 
them are correct translation equivalent pairs.The 
numbers in the right are 2?  scores 
 1. ? see /* 496.471 */ 
2. ???_? see /* 496.471 */ 
3. ? subsection  /* 496.237 */ 
4. ?? repeal /* 495.814 */ 
5. ?? order /* 493.195 */ 
7. ?? exemption /* 490.829 */ 
25. ??_?? subsidiary_legislation /* 477.173 */ 
26. ??_?? public_body /* 475.711 */ 
28. ???_?? Financial_Secretary /* 475.711 */ 
31. ?? ordinance /* 470.081 */ 
34. ??_?? primary_instrument /* 468.068 */ 
41. ??_?? health_officer /* 468.068 */ 
42. ??? magistrate /* 468.068 */ 
43. ?? discharge /* 468.068 */ 
45. ?? contract /* 468.068 */ 
46. ??_??_??_??  
Chief_Justice_of_Final Appeal /* 468.068 */ 
53. ??_??_???  
Hong_Kong_Special_Administrative_region  
/* 448.576 */ 
63. ??? tribunal /* 420.579 */ 
64. ?? declare /* 420.579 */  
Figure 7. sample of results extracted from the 
corpus 
Conclusion 
As we see in the last section, the approach used 
in this paper does really list many real 
translation equivalent pairs from the corpus. It 
seems not all the results could be taken as 
translation units, but it really offers a candidate 
list from which useful translation unit could be 
selected by means of human validation. For a 
complete evaluation of the approach, large scale 
experiments are still needed, which are now 
underway.    
Acknowledgements 
We would like to give our thanks to Professor 
Dan Tufis. His help in the lexical alignment and 
suggestions are very important for our work. We 
also would like to give thanks to all our 
colleagues who help us in many kinds of forms. 
References  
Teubert,W.(1997). Translation and the corpus, 
proceedings of the second TELRI seminar, 147-164.  
Gale,W. (1991). Identifying words correspondences 
in parallel Texts, DARPA speech and Natural 
language workshop. Asilomar, CA.  
Tufis,D. (2001), Computational bilingual 
lexicography: automatic extraction of translation 
dictionaries, In Journal of Information Science and 
Technology, Romanian Academy, Vol. 4, No. 3  
Maynard, D., Term Recognition using Combined 
Knowledge Sources, PH. D. thesis, Manchester 
University, United Kingdom. 
Yu Shiwen, Specification of Chinese text 
segmentation and POS tagging, see: 
http://www.icl.pku.edu.cn/research/corpus/coprus-an
notation.htm 
Manual of Upenn Tree bank tag set, see: 
http://www.cis.upenn.edu/~treebank/ 
Wu, D., Xia, X.(1994), Leaning an English-Chinese 
Lexicon from a Parallel Corpus, in AMTA-94, 
Association for MT in the Americas, Columbia, MD 
Chinese Word Segmentation at Peking University 
Duan Huiming  Bai Xiaojing  Chang Baobao  Yu Shiwen 
Institute of Computational Linguistics, Peking University 
{duenhm, baixj, chbb, yusw}@pku.edu.cn 
 
Abstract 
 
Word segmentation is the first step in Chinese 
information processing, and the performance 
of the segmenter, therefore, has a direct and 
great influence on the processing steps that 
follow. Different segmenters will give 
different results when handling issues like 
word boundary. And we will present in this 
paper that there is no need for an absolute 
definition of word boundary for all segmenters, 
and that different results of segmentation shall 
be acceptable if they can help to reach a 
correct syntactic analysis in the end.  
Keyword: automatic Chinese word 
segmentation, word segmentation evaluation, 
corpus, natural language processing 
 
 
1. Introduction 
 
On behalf of the Institute of Computational 
Linguistics, Peking University, we would like 
to thank ACL-SIGHAN for sponsoring the 
First International Chinese Word 
Segmentation Bakeoff, which provides us an 
opportunity to present our achievement of the 
past decade. 
We know for sure that it is very difficult to 
settle on a scientific and appropriate method 
of evaluation, and it might be even more 
difficult than word segmentation itself. We are 
also clear that each step in Chinese 
information processing requires great efforts, 
and a satisfactory result in word segmentation, 
though critical, does not necessarily guarantee 
good results in the following steps. 
From the test results of this evaluation, we 
are very gratified to see that we have done a 
good job both as a test corpus provider and as 
a participant. According to the rule, we did not 
test on the corpus we provided, but it is quite 
encouraging that our supply tops the test 
corpus list to be elected by other participants. 
Section 2 and Section 3 describes our work 
in the Bakeoff as the test corpus provider and 
the participant respectively. 
 
2. The test corpus provider 
 
2.1 Corpus 
The corpus we provided to the sponsor 
includes: 
? A training set from People?s Daily 
(January, 1998) 
? A test set from People?s Daily (Page 4 of 
January 1, 1998) 
Data from People?s Daily features standard 
Chinese, little language error, a wide coverage 
of linguistic phenomenon and topics, which 
are required for statistic training. Meanwhile, 
the corpus we provided is a latest version 
manually validated, hence a high level of 
correctness and consistency. 
 
2.2 Specification 
When processing a corpus, we need a detailed 
and carefully designed specification for 
guidance. And when using the corpus for NLP 
evaluation, we also need such a specification 
to ensure a fair contest for different systems 
within a common framework. 
We provided the latest version of our 
specification, which has been published in the 
Journal of Chinese Information Processing. 
Based on our experience of large-scale corpus 
processing in recent years, the current version 
gave us different perspectives in a consistent 
way, and we hope it will also help others in 
this field know better of our segmented and 
POS-tagged corpus. 
 
3. The participant 
 
3.1 Training and testing 
Our research on word segmentation has been 
focusing on People?s Daily. As we are one of 
the two providers of Chinese corpora in GB 
code in this Bakeoff, we had to test on the 
Penn Chinese treebank. 
Not all the training and test corpus we got 
came from the Mainland China. Some were 
GB data converted from BIG5 texts of Taiwan. 
It is commonly known that in the Mainland, 
Hong Kong and Taiwai, the Chinese langauge 
is used diversely not only in the sense of 
different coding systems, but in respect of 
different wordings as well. 
While training our segmenter, we studied 
the guidelines and training corpus of Penn 
Chinese treebank, tracing the differences and 
working on them. The main difference 
between the work of U. Penn and that of ours 
is notion of ?word?. For instance:
Differences of ?Word? U. Penn PKU 
Chinese name ??????? ?  ??, ?  ?? 
Number + ??|???? 11.6????????? 11.6?  ??????  ?  ?
Monosyllabic verb + complement ???????? ?  ???  ???  ? 
Time word ?????????? ??  ??????  ?? 
Noun + suffix ??? ????????? ???  ?????  ? 
Disyllabic verb + ??? ??????? ??  ????  ? 
? ?   
These are different combinations in regard 
of words which follow certain patterns, and 
can therefore be handled easily by applying 
rules to the grogram. The real difficulty for us, 
however, is the following items: 
U. Penn PKU 
??  ?? ???? 
??  ?? ???? 
??  ?? ???? 
??  ?? ???? 
??  ?? ???? 
?  ?  ? ??? 
? ? ? ? 
The Open Track allows us to use our own 
recourses, so we had to find the lexical 
correspondence to reduce the negtive effect 
caused by the difference between Penn 
Chinese treebank and our own corpus. 
However, as the training corpus is small, we 
could not remove all the negative effect, and 
the untackled problems remained to affect our 
test result. 
Further, as we have been working on 
language data from the Mainland China, the 
lexicon of our segmenter does not contain 
words used in Taiwan. Such being the case, 
we added into our lexicon the entries that were 
not known (i.e., not found in the training set) 
and could not be handled by the rule-based 
makeshift either. But because we are not very 
familiar with the Chinese language used in 
Taiwan, we could not make a complete patch 
due to the limit of time. 
 
3.2 Result analysis 
From the test result that the sponsor provided, 
we can see our segmenter failed to score when 
the notion of ?word? and the recognition of 
unknown words are involved. 
 
Example 1: 
[U. Penn] ?? ?? ? ? ???   ? 
?? ?? ??? ?? ? ? ?? ? 
?? ? ? ? ?? ?? ? ?? ?? 
? ? ? ? ?? ? ?? ?? ?? ?
?? ?? ? ?? ? ?? ? ?? ? 
? ?? ?? ? ??? ? 
[PKU] ?? ?? ? ? ? ? ? ? 
?? ?? ??? ?? ? ? ?? ? 
?? ? ??  ?? ?? ? ?? ?? 
? ? ? ? ?? ? ?? ?? ?? ?
?? ?? ? ?? ? ?? ? ?? ? 
? ?? ?? ? ??? ? 
 
Example 2: 
[U. Penn] ? ? ??  ?? ?? ?? 
? ? ?? ?? ? ? ?? ? ?? ? 
?? ? ?? ?? ? ?? ?? ???
? ? ? ? ? ? ? ? ?? ? ? ?
? ??? ? ? ? ? ? ? ? ? ? 
?? ? ?? ? ? ?? ? ?? ? 
[PKU] ? ? ? ? ?? ?? ?? ? 
? ?? ?? ? ? ?? ? ?? ? ?
? ? ?? ?? ? ?? ?? ???
? ? ? ? ? ? ? ? ?? ? ? ?
? ??? ??  ? ? ? ? ? ??  
?? ? ?? ? ? ?? ? ?? ? 
 
In addition, there are also cognitive 
differences concerning the objective world, 
which did come up to influence our fine score. 
 
Example 3: 
[U. Penn] ??? ? ? ??? ? ? ? 
?? ?? ? ? ? ? ? ? ? ?? ? 
? ? ?? ?? ? CPU ? ?? ? ? 
? ? ? ? ? ?? ??? ? ? ?
? ? 
[PKU] ??? ? ? ??? ? ? ? 
?? ?? ? ? ? ??  ? ? ?? ? 
? ? ?? ?? ? CPU ? ?? ? ? 
? ? ? ? ? ?? ??? ? ??
? ? 
 
Example 4: 
[U. Penn] ? ? ????  ?? ? ? 
?? ? ? ? ? ? ?? ? ? ?? ? 
? ?? ? ? ?? ? ? ? ?? ? ?
? ?? ?? ?? ?? ? ? ??  
?? ? 
[PKU] ? ? ? ??? ?? ? ? ?
? ? ? ? ??  ?? ? ? ?? ? 
? ?? ? ? ?? ??  ? ?? ? ?
? ?? ?? ?? ?? ? ? ? ? 
?? ? 
 
The recognition of unknown words has long 
been a bottleneck for word segmentation 
technique. So far we have not found a good 
solution, but we are confident about a progress 
in this respect in the near future. 
 
4. Conclusion 
 
Word segmentation is the first step yet a key 
step in Chinese information processing, but 
we have not found a perfect solution up till 
now. From an engineering perspective, we 
think there is no need for a unique result of 
segmentation. All roads lead to Rome. The 
approach you take, technical or non-technical, 
will be a good one if the expected result is 
achieved. And it would be more desirable if 
the processing program in each step can 
tolerate or even correct the errors made in the 
previous step. 
We learn from our experience that the 
computer processing of natural language is a 
complex issue, which requires a solid 
fundamental research (on the language itself) 
to ensure a higher accuracy of automation. It 
is definitely hard to achieve an increase of one 
percent or even less in the accuracy of word 
segmentation, but we are still confident and 
will keep working in this respect. 
 
Finally, we would like to thank Dr. Li Baoli 
and Dr. Bing SWEN for their great efforts on 
the maintenance of our segmentation program. 
 
Reference 
Yu, Shiwen, DUAN, Hui-ming, ZHU, Xue-feng, 
Bing SWEN. 2002. The Specification of Basic 
Processing of Contemporary Chinese Corpus. 
Journal of Chinese Information Processing, 
Issue 5 & Issue 6, 2002.  
Yu, Shiwen, et al 2002. The Grammatical 
Knowledge-base of Contemporary Chinese ? 
A Complete Specification (Second Version). 
Beijing: Tsinghua University Press. 
Liu, Yuan, et al 1994. Specification and 
Automation of Word Segmentation of 
Contemporary Chinese for Information 
Processing. Beijing: Tsinghua University 
Press. 
Fie Xia. 2000. The segmentation guidelines for 
the Penn Chinese tree bank (3.0). see 
http://www.cis.upenn.edu/~chinese/segguide.3
rd.ch.pdf 
 
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 355?364, Dublin, Ireland, August 23-29 2014.
Inducing Word Sense with Automatically Learned Hidden Concepts
Baobao Chang Wenzhe Pei Miaohong Chen
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
Beijing, P.R.China, 100871
{chbb,peiwenzhe,miaohong-chen}@pku.edu.cn
Abstract
Word Sense Induction (WSI) aims to automatically induce meanings of a polysemous word from
unlabeled corpora. In this paper, we first propose a novel Bayesian parametric model to WSI.
Unlike previous work, our research introduces a layer of hidden concepts and view senses as
mixtures of concepts. We believe that concepts generalize the contexts, allowing the model to
measure the sense similarity at a more general level. The Zipf?s law of meaning is used as a
way of pre-setting the sense number for the parametric model. We further extend the parametric
model to non-parametric model which not only simplifies the problem of model selection but
also brings improved performance. We test our model on the benchmark datasets released by
Semeval-2010 and Semeval-2007. The test results show that our model outperforms state-of-the-
art systems.
1 Introduction
Word Sense Induction (WSI) aims to automatically induce meanings of a polysemous word from unla-
beled corpora. It discriminates among meanings of a word by identifying clusters of similar contexts.
Unlike the task of Word Sense Disambiguation (WSD), which classifies polysemous words according
to a pre-existing and usually hand-crafted inventory of senses, WSI makes it attractive to researchers by
eliminating dependence on a particular sense inventory and learning word meaning distinction directly
based on the contexts as observed in corpora.
Almost all WSI work relies on the distributional hypothesis, which states that words occurring in
similar contexts will have similar meanings. To effectively discriminate among contexts, proper repre-
sentation of contexts would be a key issue. Basically, context can be represented as a vector of words
co-occurring with the target word within a fixed context window. The similarity between two contexts
of the target word can then be measured by the geometrical distance between the corresponding vectors.
To ease the sparse problem and capture more semantic content, some kinds of generalizations or abstrac-
tions are needed. For example, a context of bank including money may not share similarity with that
including cash measured at word level. However, given the conceptual relationship between money and
cash, the two contexts actually share high similarity.
One straightforward way of introducing conceptualization is to assign semantic code to context words,
where semantic codes could be derived from WordNet or other resources like thesauruses. However, two
problems remain to be tackled. The first one concerns ambiguities of context words. Context words may
have multiple semantic codes and thus word sense disambiguation to context words or other extra cost
is needed. The second one concerns the nature of WSI task. WSI actually is target-word-specific, which
means the conceptualization should be done specifically to different target words. A general purpose
conceptualization defined by a thesaurus may not well meet this requirement and may not be equally
successful in discriminating contexts of different target words.
To address these problems, we first propose a parametric Bayesian model which jointly finds concep-
tual representations of context words and the sense of the target word. We do this by introducing a layer
of target-specific conceptual representation between the target sense layer and the context words layer
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
355
Figure 1: Architecture of our model Figure 2: Graphical notation of the Basic Model
through a Bayesian framework as illustrated in Figure 1. From the generative perspective, the sense of
the target word is first sampled. Then the sense generates different conceptual configurations which in
turn generate different contexts. With a deeper architecture, our model makes it possible to induce word
senses at a more abstract level, i.e. the concept level, which is not only less sparse but also more seman-
tically oriented. Both the senses of the target word and the latent concepts are inferred automatically and
unsupervisedly with inference procedure given enough contexts involving a target word. The latent con-
cepts inferred with the model share similarities with those defined in thesauruses, as both of them cluster
semantically related words. However, since the latent concepts are inferred with regard to individual
target words, they are target-word-specific and thus fit the WSI task better than general purpose concepts
defined in thesauruses. Context words may still correspond to multiple latent concepts. However, the
disambiguation is implicitly done in the process of the word sense induction.
Setting the number of senses that the algorithm should arrive at is another problem frequently exer-
cising the minds of WSI people. Instead of trying different sense numbers on a word-by-word basis,
we propose to use Zipf?s law of meaning (Zipf, 1945) to guide the selection of the sense numbers in
this paper. With the law of meaning, sense numbers could be set on an all-word basis, rather than on a
word-by-word basis. This is not only simple but also efficient, especially in the case where there are a
large number of target words to be concerned.
We further extend the parametric model into a non-parametric model, as it allows adaptation of model
complexity to data. By extending our model to non-parametric model, the need to preset the numbers of
senses and latent concepts are totally removed and, moreover, the model performance is also improved.
We evaluate our model on the commonly used benchmark datasets released by both Semeval-2010
(Manandhar et al., 2010) and Semeval-2007 (Agirre and Soroa, 2007). The test results show that our
models perform much better than the state-of-the-art systems.
2 The parametric model
2.1 Basic Model
The main point of our work is that different senses are signaled by contexts with different concept con-
figurations, where different concepts are formally defined as different distributions over context words.
Formally, we denote by P (s) the global multinomial distribution over senses of an ambiguous word
and by P (w|z) the multinomial distributions over context words w given concept z. Context words are
generated by a mixture of different concepts whose mixture proportion is defined by P (z|s), such that:
P (w
i
) =
?
j
P (s = j)
?
k
P (z
i
= k|s = j)P (w
i
|z
i
= k)
Following the model, each context word w
i
surrounding the target word is generated as follows: First, a
sense s is sampled from P (s) for the target word. Then for each context word position i, a concept z
i
is
sampled according to mixture proportion P (z|s) and w
i
is finally sampled from P (w|z).
Figure 2 shows the model with the graphical notation, where M is the number of instances of contexts
regarding to a concerned target word and N
m
is the number of word tokens in context m. s
m
is the
sense label for target word in context m. w
m,n
is the n-th context word in context m. z
m,n
is the
concept label associated with w
m,n
. I is the total number of senses to be induced. J is the total number
356
Figure 3: Graphical notation of the non-parametric WSI model
of concepts.
~
? is the notational shorthand for the sense distribution P (s), ~?
i
is the shorthand for the
i-th sense-concept distribution P (z|s = i), and ~?
j
is the j-th concept-word distribution P (w|z = j).
Following conventional Bayesian practice,
~
?, ~?
i
and ~?
j
are assumed to be drawn from Dirichlet priors
with symmetric parameter ?, ?, ? respectively. The observed variable is represented with shaded node
and hidden variable with unshaded node.
2.2 Zipf?s law of meaning
Most of the WSI work requires that the number of senses to be induced be specified ahead of time.
One straightforward way to deal with this problem is to repeatedly try different numbers of senses on
a development set and select the best performed number. However, this should be done in principle on
a word-by-word basis, and thus could be time-consuming and prohibitive when there are lots of target
words to be concerned. A more systematic way of setting sense numbers in Bayesian models is extending
the parametric model into a non-parametric model, which will be described in detail in section 3.
To work with our parametric model, we propose in this paper that an empirical law, Zipf?s law of
meaning (Zipf, 1945), could be used to guide the sense number selection. Zipf?s law of meaning states
that the number of sense of a word is proportional to its frequency as shown in the following equation:
I = K ? f
b
(1)
where I is the number of word senses and f is the frequency of the word. K is the coefficient of
proportionality which is unknown and b is about 0.404 according to an experimental study done by
Edmonds (2006).
Certainly, Zipf?s law of meaning is not as strict as a rigorous mathematical law. However, it sketches
the distribution of the sense numbers with word frequencies of all words and allows us to estimate the
sense numbers on an all-word basis by selecting appropriate coefficient K. This is not only simple but
also efficient, especially in the case that there are a large number of target words to be concerned.
3 Non-parametric Model
A limitation of the parametric model is that the sense number I of the target word and the number J
of latent concepts need to be fixed beforehand. Bayesian non-parametric (BNP) models offer elegant
approach to the problem of model selection and adaption. Rather than comparing models that vary in
complexity, the BNP approach is to fit a single model that can adapt its complexity to the data. Unlike
the parametric approach, BNP approach assumes an infinite number of clusters, among which only a few
are active given the training data. Our basic model can be naturally extended into a BNP model as shown
in Figure 3. Instead of assuming a finite number of senses, we place a nonparametric, Dirichlet process
(DP) prior on the sense distribution as follows:
G ? DP (?,H)
s
m
? G,m = 1, 2, . . . ,M
where ? is the concentration parameter and H is the base measure of the Dirichlet process.
357
For each sense s
i
of the target words, we place a Hierarchical Dirichlet process (HDP) prior on the
mixture proportion to latent concepts shown as follows:
G
0
? DP (?,H
0
)
G
i
? DP (?,G
0
), i = 1, 2, . . .
z
m,n
? G
i
, n = 1, 2, . . . , N
m
w
m,n
? ~?
z
m,n
where ? and ? are concentration parameters to G
0
and G
i
, H
0
is the base measure of G
0
.
By using HDP priors, we make sure that the same set of concept-word distributions is shared across
all senses and all contexts of a target word, since each random measure G
i
inherits its set of concepts
from the same G
0
.
As in parametric model, ~?
j
is the j-th concept-word distribution P (w|z = j), however, there are now
an infinite number of such distributions. So is the number of senses. However, with a fixed number of
contexts of the target word, only a finite number of senses and concepts are active and they could be
inferred automatically by the inference procedure.
4 Model Inference
We use Gibbs sampling (Casella and George, 1992) for inference to both the parametric and nonpara-
metric model. As a particular Markov Chain Monte Carlo (MCMC) method, Gibbs sampling is widely
used for inference in various Bayesian models (Teh et al., 2006; Li and Li, 2013; Li and Cardie, 2014).
4.1 The Parametric Model
For the parametric model, we use collapsed Gibbs sampling, in which the sense distribution
~
?, sense-
concept distribution ~?
i
and concept-word distribution ~?
j
are integrated out. At each iteration, the sense
label s
m
of the target word in context m is sampled from conditional distribution p(s
m
|~s
?m
, ~z, ~w),
and the concept label z
m,n
for the context word w
m,n
is sampled from conditional distribution
p(z
m,n
|~s, ~z
?(m,n)
, ~w). Here ~s
?m
refers to all current sense assignments other than s
m
and ~z
?(m,n)
refers
to all current concept assignment other than z
m,n
.
The conditional distribution p(s
m
|~s
?m
, ~z, ~w) and p(z
m,n
|~s, ~z
?(m,n)
, ~w) can be derived as shown in
equation (2) and (3) respectively:
p(s
m
= i|~s
?m
, ~z, ~w;?, ?, ?) ? (c
?m
i
+ ?) ?
?
J
j=1
?
f
m,j
x=1
(c
?m
i,j
+ ? + x? 1)
?
f
m,?
x=1
(
?
J
j=1
c
?m
i,j
+ J ? ? + x? 1)
(2)
p(z
m,n
= j|~s, ~z
?(m,n)
, ~w;?, ?, ?) ? (c
?(m,n)
s
m
,j
+ ?) ?
(c
?(m,n)
j,w
m,n
+ ?)
?
V
t=1
c
?(m,n)
j,t
+ V ? ?
(3)
Here, c
?m
i
is the number of instances with sense i. c
?m
i,j
is the number of concept j in instances with sense
i. Both of them are counted without the m-th instance of the target word. c
?(m,n)
s
m
,j
is defined in a similar
way with c
?m
i,j
but without counting the word position (m,n). c
?(m,n)
j,w
m,n
is the number of times word w
m,n
is assigned to concept j without counting word position (m,n). f
m,j
is the number of concept j assigned
to context words in instance m and f
m,?
is the total number of words in contexts of instance m. V stands
for the size of the word dictionary, i.e. the number of different words in the data. x is an index which
iterates from 1 to f
m,?
.
~
?, ~?
i
and ~?
j
can be estimated in a similar way, we now only show as example the estimation of ~?
i
,
parameters for sense-concept distributions. According to their definitions as multinomial distributions
with Dirichlet prior, applying Bayes? rule yields:
p(~?
i
|~z;~?) =
p(~?
i
;~?) ? p(~z|~?
i
;~?)
Z
~?
i
= Dir(~?
i
|~c
i
+ ~?)
358
where ~c
i
is the vector of concept counts for sense i. Using the expectation of the Dirichlet distribution,
values of ?
i,j
can be worked out as follows:
?
i,j
=
c
i,j
+ ?
?
J
k=1
c
i,k
+ J ? ?
Different read-outs of ?
i,j
are then averaged to produce the final estimation.
4.2 The Non-parametric Model
Chinese restaurant process (CRP) and Chinese restaurant franchise (CRF) process (Teh et al., 2006)
have been widely used as sampling scheme for DP and HDP respectively. As our non-parametric model
involves both DP and HDP, we use both CRP and CRF based sampling for model inference.
In the CRP metaphor to DP, there is one Chinese restaurant with an infinite number of tables, each of
which can seat an infinite number of customers. The first customer enters the restaurant and sits at the
first table. The second customer enters and decides either to sit with the first customer or by herself at
a new table. In general, the n + 1st customer either joins an already occupied table k with probability
proportional to the number n
k
of customers already sitting there, or sits at a new table with probability
proportional to ?. As in our model, when we sample the sense s
m
for each context, we assume that
tables correspond to senses of target words and customers correspond to whole contexts in which the
target word occurs.
In the CRF metaphor to HDP, there are multiple Chinese restaurants, and each one has infinitely many
tables. On each table the restaurant serves one of infinitely many dishes that other restaurants may serve
as well. At each table of each restaurant one dish is ordered from the menu by the first customer who
sits there, and it is shared among all customers who sit at that table. The menu is shared by all the
restaurants. To be specific to our model, when we sample the concept z
m,n
for each context word, we
assume each sense s
m
of the target word corresponds to a restaurant and each word w
m,n
corresponds
to a customer while concept z
m,n
corresponds to the dishes served to the customer by the restaurant.
Neither the number of restaurant nor the number of dishes is finite in our model.
For model inference, we first sample s
m
using CRP-based sampling and then we sample z
m,n
for each
s
m
using CRF-based sampling. The sampling of s
m
and z
m,n
are done alternately, but not independently.
The sampling of s
m
is conditional on the current value of z
m,n
and vice versa, conforming to the scheme
of Gibbs Sampling.
The equation for sampling s
m
is derived as in equation (4):
p(s
m
= i|~s
?m
, ~z, ~w) ?
{
c
?m
i
? p(~z
m
|~z
?m
, s
m
= i) if i = old
? ? p(~z
m
|~z
?m
, s
m
= i
new
) else
where
p(~z
m
|~z
?m
, s
m
= i) =
?
J
j=1
?
f
m,j
x=1
(c
?m
i,j
+ ? ?
c
?m
t,j
c
?m
t,?
+?
+ x? 1)
?
f
m,?
x=1
(
?
J
j=1
c
?m
i,j
+ ? + x? 1)
(4)
Here p(~z
m
|~z
?m
, s
m
= i) is estimated block-wise for context m according to the CRF metaphor. c
?m
i
and c
?m
i,j
are defined in the same way as that in equation (2). c
?m
t,j
is the number of tables with dish j in
all restaurants but m and c
?m
t,?
means the number of tables in all restaurants but m. x is an index which
iterates from 1 to f
m,?
.
Sampling z
m,n
needs more steps than sampling s
m
as we need to record the table assignment for each
dish (concept). For each dish z
m,n
of a customer w
m,n
, we first sample the table at which the customer
sits according to the following equations:
p(t
m,n
= t|
~
t
?(m,n)
, ~z
?(m,n)
, w
m,n
, s
m
= i) ?
{
c
?(m,n)
i,t
? p
?(m,n)
j
(w
m,n
) if t = old
? ? p(w
m,n
|
~
t
?(m,n)
, t
m,n
= t, ~z
?(m,n)
, w
m,n
) else
where
p
?(m,n)
j
(w
m,n
) = p(w
m,n
|z
m,n
= j, ~w
?(m,n)
) =
c
?(m,n)
j,w
m,n
+ ?
?
V
t=1
c
?(m,n)
j,t
+ V ?
359
Basic Model BNP
? 1.0 0.2
? 0.05 0.01
? 0.05 0.2
? N/A 0.001
K 0.27 N/A
Concept number 20 N/A
Context window ? 5 words ? 9 words
Table 1: Hyperparamters of our models
Here c
?(m,n)
i,t
is the number of customers on table t in restaurant i and c
?(m,n)
j,w
m,n
has the same meaning as
in equation (3). If the sampled table t is previously occupied, then z
m,n
is set to the dish j assigned to
t according to the CRF metaphor. If the sampled table t is new, the probability p(w
m,n
|
~
t
?(m,n)
, t
m,n
=
t, ~z
?(m,n)
, w
m,n
) is calculated using equation (5), which is the sum of the probability of all previously
ordered dishes and the newly ordered dish.
p(w
m,n
|
~
t
?(m,n)
, t
m,n
= t, ~z
?(m,n)
, w
m,n
) =
J
?
j=1
c
?(m,n)
t,j
c
?(m,n)
t,?
+ ?
? p
?(m,n)
j
(w
m,n
) +
?
c
?(m,n)
t,?
+ ?
? p
?(m,n)
j
new
(5)
Because a new table is added, we then sample a new dish for this table according to equation (6).
p(z
m,n
= j|
~
t, ~z
?(m,n)
) ?
{
c
?(m,n)
t,j
? p
?(m,n)
j
(w
m,n
) if j = old
? ? p
?(m,n)
j
new
(w
m,n
) if j = new
(6)
After the dish j is sampled, it is assigned to the new table and the number of table serving dish j is added.
Parameters
~
?, ~?
i
and ~?
j
can be estimated in the same way as described in section 4.1.
5 Experiment
5.1 Experiment Setup
Data Our primary WSI evaluation is based on the standard dataset in Semeval-2010 Word sense induc-
tion & Disambiguation task (Manandhar et al., 2010). The target word dataset consists of 100 words, 50
nouns and 50 verbs. There are a total number of 879,807 sentences in training set and 8,915 sentences in
testing set. The average number of word senses in the data is 3.79.
Model Selection The trail data of Semeval-2010 WSI task is used as development set for parameter
tuning, which consists of training and test portions of 4 verbs. The 4 verbs are different words than the
100 target words in the training data. There are only about 138 instances on average for each target word
in the training part of the trial data. To make a development set of more reasonable size, the trial data
are supplemented with 6K instances of the 4 verbs extracted from the British National Corpus (BNC)
1
corpus. As we use the Zipf?s law of meaning to guide the selection of number of senses, BNC was also
used to count word frequencies.
The final hyper-parameters are set as in Table 1. In all the following experiments, Gibbs sampler is
run for 2000 iterations with burn-in period of 500 iterations. Every 10th sample is read out for parameter
estimating after the burn-in period to avoid autocorrelation. Due to the randomized property of Gibbs
sampler, all results in the next sections are averaged over 5 runs. The average running time for each target
word is about 7 minutes on a computer equipped with an Intel Core i5 processor working at 3.1GHz and
8GB RAM.
Pre-Processing For each instance of the target word in training data and testing data, all words are
lemmatized and stop words like ?of ?, ?the?, ?a? which are irrelevant to word sense distinction are filtered.
Words occurring less than twice are removed.
Evaluation method Semeval-2010 WSI task presents two evaluation schemes which are supervised
evaluation and unsupervised evaluation. In supervised evaluation, the gold standard dataset is split into
1
www.natcorp.ox.ac.uk/
360
Model
Supervised Evaluation Unsupervised Evaluation
Averaged #s
80-20 split 60-40 split V-Measure Paired-Fscore
Basic Model 64.12 63.68 11.52 44.42 5
Basic Model + Zipf 66.4 65.25 15.2 35.12 7.66
BNP 69.3 68.9 21.4 23.1 15.62
Table 2: Test results with different configurations.
Figure 4: Examples of concepts induced with the BNP model specific to the target word address.n (with
c
i
denoting concept)
a mapping and an evaluation parts. The first part is used to map the automatically induced senses to
gold standard senses. The mapping is then used to calculate the system?s F-Score on the second part.
According to the size of mapping data and evaluation data, the evaluation results are measured on two
different splits which are 80-20 splits and 60-40 splits. 80-20 splits means that 80% of the test data are
used for mapping and 20% are used for evaluation. In unsupervised evaluation, the system outputs are
compared by using metrics V-Measure (Rosenberg and Hirschberg, 2007) and Paired F-Score (Artiles et
al., 2009).
5.2 Experiment Results
Table 2 lists all experiment results. The Basic Model stands for the parametric model with fixed number
of senses for all target words. The number of senses is set to 5 which gives the best performance on
development set. Basic Model + Zipf is the model with the number of sense estimated by Zipf?s law of
meaning. BNP stands for our non-parametric model. As we can see, compared with the Basic Model with
fixed sense number, the model using Zipf?s law of meaning achieves improved performance. This means
Zipf?s law of meaning has positive effect in setting the sense number of the WSI task. BNP achieves the
best performance on both supervised evaluation and V-measure evaluation. In terms of Paired F-score,
however, the Basic Model gets the best results while BNP performs worst. This is consistent with what
claimed by Manandhar et al. (2010), that Paired F-score tends to penalize the model with higher number
of clusters.
As stated before, our models not only perform word sense induction but also group the context words
into concepts. Figure 4 shows 4 of the concepts induced by BNP with regard to the target word address.n.
Senses of address.n are defined as the mixture of concepts and concepts are defined as distributions over
context words. We only list the top five words with the highest probabilities under each concept. As
shown in Table 2, the non-parametric model induces much finer granularity of senses than the gold
standard, it makes distinction among email address, web address, and even ip address. A possible
solution is to further measure the closeness of senses based on the sense representations induced and
merge similar senses to produce coarser granularity of senses.
361
Model F-score(%)
BNP+position 69.7
BNP 69.3
Basic Model + Zipf 66.4
Basic Model 64.1
HDP 65.8
HDP+position (Lau et al., 2012) 68
distNB (Choe and Charniak, 2013) 65.4
UoY (Korkontzelos and Manandhar, 2010) 62.4
Model F-score(%)
BNP+position 88.0
BNP 86.1
HDP (Yao and Van Durme, 2011) 85.7
HDP+position (Lau et al., 2012) 86.9
Feature-LDA (Brody and Lapata, 2009) 85.5
1-layer-LDA (Brody and Lapata, 2009) 84.6
HRG (Klapaftis and Manandhar, 2010) 87.6
I2R (Niu et al., 2007) 86.8
Table 3: Comparison with state-of-the-arts on Semeval-2010 data (left) and Semeval-2007 data (right)
5.3 Comparison with previous work
Much previous work (Brody and Lapata, 2009; Klapaftis and Manandhar, 2010; Yao and Van Durme,
2011) tested their models only on Semeval-2007 dataset (Agirre and Soroa, 2007) which consists of
roughly 27K instances of 65 target verbs and 35 target nouns, coming from the Wall Street Journal
corpus (WSJ) (Agirre and Soroa, 2007). For a complete comparison, we also test our model on the
Semeval-2007 dataset. Since training data was not provided as part of the original Semeval-2007 dataset,
we follow the approach of previous work (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et
al., 2012) to construct training data for each target word by extracting instances from the BNC corpus.
Following paractices as much previous work (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau
et al., 2012) did, we compare with previous work with supervised F-score on 80-20 data split in Semeval-
2010 and noun data in Semeval-2007.
Table 3 (left) compares our models against the state-of-the-art systems tested on 80-20 data split in
Semeval-2010. HDP+position (Lau et al., 2012) improved the HDP model (Yao and Van Durme, 2011)
by including a position feature. distNB (Choe and Charniak, 2013) extends the naive Bayes model
by reweighting the conditional probability of a context word given the sense by its distance to the tar-
get word. UoY (Korkontzelos and Manandhar, 2010) is the best performing system in Semeval-2010
competition which used a graph-based model. We re-implemented and tested the HDP model on the
Semeval-2010 dataset since Yao and Van Durme (2011) and Lau et al. (2012) did not report their HDP
results on this dataset.
Different with normal practice in WSI work, there is no feature engineering in our model. However,
our BNP model outperformed all the systems on supervised evaluation. Even the Basic Model outper-
formed the best performing Semeval-2010 system. Especially, our BNP model performs much better
than the HDP model. Both Lau et al. (2012) and Choe and Charniak (2013) show benefit of using po-
sitional information. Since our model does not exclude further feature engineering, we also introduce a
position feature
2
into our non-parametric model (BNP+position) as in Lau et al. (2012). This contributes
to a further 0.4% rise in performance.
Table 3 (right) compares our models with previous work on the nouns dataset in Semeval-2007. We
divides systems being compared into two groups. The first group model the WSI task with Bayesian
framework, while the second group uses models other than Bayesian model. Feature-LDA is the LDA-
based model proposed by Brody and Lapata (2009) which incorporates a large number of features into the
model. The 1-layer-LDA is their model with only bag-of-words features. HRG is a hierarchical random
graph model. I2R is the best performing system in Semeval-2007. As shown in Table 3 (right), our
BNP model with position feature (BNP+position) outperforms all systems. If we restrict our attention
to the first group in which all models are Bayesian model, our BNP model without feature engineering
outperforms the HDP model which is also non-parametric model without feature engineering.
6 Related Work
A large body of previous work is devoted to the task of Word Sense Induction. Almost all work relies
on the distributional hypothesis, which states that words occurring in similar contexts will have similar
meanings. Different work exploits distributional information in different forms, including context clus-
tering models (Sch?utze, 1998; Niu et al., 2007; Pedersen, 2010; Elshamy et al., 2010; Kern et al., 2010),
graph-based models (Korkontzelos and Manandhar, 2010; Klapaftis and Manandhar, 2010) and Bayesian
2
Formally, the position feature is the context words with its relative position to the target word.
362
models. For Bayesian models, Brody and Lapata (2009) firstly introduced a Bayesian model to WSI task.
They used the LDA-based model in which contexts of target word were viewed as documents as in the
LDA model (Blei et al., 2003) and senses as topics. They trained a separate model for each target word
and included a variety of features such as words, part-of-speech and dependency information. Yao and
Van Durme (2011) extended LDA-based model into non-parametric HDP model but removed the feature
engineering. Lau et al. (2012) showed improved supervised F-score by including position feature to the
HDP model. Choe and Charniak (2013) proposed a reweighted naive Bayes model by incorporating the
idea that words closer to the target word are more relevant in predicting the sense.
Our model differs from the context clustering models and graph-based models, as it is a Bayesian
probabilistic model. Our work also differs from the LDA-based models. LDA topics were actually
re-interpreted as senses of target word as Brody and Lapata (2009) applied the LDA to WSI tasks, so
did Yao and Van Durme (2011) and Lau et al. (2012). They induced word senses by firstly tagging
(sampling) senses (of target words) to context words and selecting the mostly tagged sense as sense of
target words. Our model could be viewed as an extension of LDA, but fit the WSI task more naturally
and much better. We distinguish senses of target words from concepts of context words and assume that
they are separate. Therefore, our model has two hidden layers corresponding to the sense of the target
word and the concepts of the context words respectively. Basically, one decide the sense of the target
word based on the concept configuration of context words, instead of tagging senses of target word to
context words. The separation of senses of target word and concepts of context words is actually not
only required by linguistic intuition but also leads to improvement by our experiment. Our model is also
different from the naive Bayes model since our model induces senses of the target word at concept level
while naive Bayes model works at word level and does not involve conceptualization to context words at
all.
7 Conclusion
In this paper, we first proposed a parametric Bayesian generative model to the task of Word Sense Induc-
tion. It is distinct from previous work in that it introduces a layer of latent concepts that generalize the
context words and thus enable the model to measure the sense similarity at a more general level. We also
show in this paper that Zipf?s law of meaning can be used to guide the setting of sense numbers on an
all-word basis, which is not only simple but also independent of the clustering methods being used. We
further extend our parametric model to non-parametric model which not only simplifies the problem of
model selection but also bring improved performance. The test results on the benchmark datasets show
that our model outperforms the state-of-the-art systems.
Acknowledgments
This work is supported by National Natural Science Foundation of China under Grant No. 61273318
and National Key Basic Research Program of China 2014CB340504.
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination
systems. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7?12. Association
for Computational Linguistics.
Javier Artiles, Enrique Amig?o, and Julio Gonzalo. 2009. The role of named entities in web people search. In
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume
2, pages 534?542. Association for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference
of the European Chapter of the Association for Computational Linguistics, pages 103?111. Association for
Computational Linguistics.
George Casella and Edward I George. 1992. Explaining the gibbs sampler. The American Statistician, 46(3):167?
174.
363
Do Kook Choe and Eugene Charniak. 2013. Naive Bayes word sense induction. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing, pages 1433?1437, Seattle, Washington,
USA, October. Association for Computational Linguistics.
Phillip Edmonds. 2006. Disambiguation, lexical. Encyclopedia of Language and Linguistics. Second Edition.
Elsevier.
Wesam Elshamy, Doina Caragea, and William H Hsu. 2010. Ksu kdd: Word sense induction by clustering in topic
space. In Proceedings of the 5th international workshop on semantic evaluation, pages 367?370. Association
for Computational Linguistics.
Roman Kern, Markus Muhr, and Michael Granitzer. 2010. Kcdc: Word sense induction by using grammatical
dependencies and sentence phrase structure. In Proceedings of the 5th international workshop on semantic
evaluation, pages 351?354. Association for Computational Linguistics.
Ioannis P Klapaftis and Suresh Manandhar. 2010. Word sense induction & disambiguation using hierarchical
random graphs. In Proceedings of the 2010 conference on empirical methods in natural language processing,
pages 745?755. Association for Computational Linguistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010. Uoy: Graphs of unambiguous vertices for word sense in-
duction and disambiguation. In Proceedings of the 5th international workshop on semantic evaluation, pages
355?358. Association for Computational Linguistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense induction
for novel sense detection. In Proceedings of the 13th Conference of the European Chapter of the Association
for Computational Linguistics, pages 591?601. Association for Computational Linguistics.
Jiwei Li and Claire Cardie. 2014. Timeline generation: Tracking individuals on twitter. In Proceedings of the
23rd international conference on World wide web, pages 643?652.
Jiwei Li and Sujian Li. 2013. Evolutionary hierarchical dirichlet process for timeline summarization. In Proceed-
ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),
pages 556?560, Sofia, Bulgaria, August. Association for Computational Linguistics.
Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach, and Sameer S Pradhan. 2010. Semeval-2010 task
14: Word sense induction & disambiguation. In Proceedings of the 5th International Workshop on Semantic
Evaluation, pages 63?68. Association for Computational Linguistics.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 2007. I2r: Three systems for word sense discrimination,
chinese word sense disambiguation, and english word sense disambiguation. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages 177?182. Association for Computational Linguistics.
Ted Pedersen. 2010. Duluth-wsi: Senseclusters applied to the sense induction task of semeval-2. In Proceed-
ings of the 5th international workshop on semantic evaluation, pages 363?366. Association for Computational
Linguistics.
Andrew Rosenberg and Julia Hirschberg. 2007. V-measure: A conditional entropy-based external cluster evalua-
tion measure. In EMNLP-CoNLL, volume 7, pages 410?420.
Hinrich Sch?utze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical dirichlet processes.
Journal of the american statistical association, 101(476).
Xuchen Yao and Benjamin Van Durme. 2011. Nonparametric bayesian word sense induction. In Graph-based
Methods for Natural Language Processing, pages 10?14.
George Kingsley Zipf. 1945. The meaning-frequency relationship of words. The Journal of General Psychology,
33(2):251?256.
364
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 789?798,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Enhancing domain portability of Chinese segmentation model 
using chi-square statistics and bootstrapping 
 
Baobao Chang, Dongxu Han 
Institute of Computational Linguistics, Peking University 
Key Laboratory of Computational Linguistics(Peking University), Ministry Education, China 
Beijing, 100871, P.R.China 
chbb@pku.edu.cn,hweibo@126.com 
 
 
 
 
 
 
Abstract 
Almost all Chinese language processing tasks 
involve word segmentation of the language 
input as their first steps, thus robust and reli-
able segmentation techniques are always re-
quired to make sure those tasks well-
performed. In recent years, machine learning 
and sequence labeling models such as Condi-
tional Random Fields (CRFs) are often used in 
segmenting Chinese texts. Compared with 
traditional lexicon-driven models, machine 
learned models achieve higher F-measure 
scores. But machine learned models heavily 
depend on training materials. Although they 
can effectively process texts from the same 
domain as the training texts, they perform 
relatively poorly when texts from new do-
mains are to be processed. In this paper, we 
propose to use ?2 statistics when training an 
SVM-HMM based segmentation model to im-
prove its ability to recall OOV words and then 
use bootstrapping strategies to maintain its 
ability to recall IV words. Experiments show 
the approach proposed in this paper enhances 
the domain portability of the Chinese word 
segmentation model and prevents drastic de-
cline in performance when processing texts 
across domains. 
1 Introduction 
Chinese word segmentation plays a fundamental 
role in Chinese language processing tasks, because 
almost all Chinese language processing tasks are 
assumed to work with segmented input. After in-
tensive research for more than twenty years, the 
performance of Chinese segmentation made con-
siderable progress. The bakeoff series hosted by 
the Chinese Information Processing Society (CIPS) 
and ACL SIGHAN shows that an F measure of 
0.95 can be achieved in the closed test tracks, in 
which only specified training materials can be used 
in learning segmentation models1.    
Traditional word segmentation approaches are 
lexicon-driven (Liang, 1987) and assume prede-
fined lexicons of Chinese words are available. 
Segmentation results are obtained by finding a best 
match between the input texts and the lexicons. 
Such lexicon-driven approaches can be rule-based, 
statistic-based or in some hybrid form. 
Xue (2003) proposed a novel way of segmenting 
Chinese texts, and it views the Chinese word seg-
mentation task as a character tagging task. Accord-
ing to Xue?s approach, no predefined Chinese 
lexicons are required; a tagging model is learned 
by using manually segmented training texts. The 
model is then used to assign each character a tag 
indicating the position of this character within a 
word. Xue?s approach has become the most popu-
lar approach to Chinese word segmentation for its 
high performance and unified way of dealing with 
out-of-vocabulary (OOV) issues. Most segmenta-
tion work began to follow this approach later. Ma-
jor improvements in this line of research include: 1) 
More sophisticated learning models were intro-
duced other than the maximum entropy model that 
Xue used, such as the conditional random fields 
(CRFs) model which fits the sequence tagging 
tasks much better than the maximum entropy 
model (Tseng et al,2005). 2) More tags were in-
                                                          
1 http://www.sighan.org/bakeoff2005/data/results.php.htm 
789
troduced, as Zhao et al (2006) shows 6 tags are 
superior to 4 tags. 3) New feature templates were 
added, such as the templates that were used in rep-
resenting numbers, dates, letters etc. (Low et al, 
2005)  
Character tagging approaches require manually 
segmented training texts to learn models usually in 
a supervised way. The performance is always eva-
luated on a test set from the same domain as the 
training set. Such evaluation does not reveal its 
ability to deal with domain variation. Actually, 
when test set is from other domains than the do-
main where training set is from, the learned model 
normally underperforms substantially.   
One of the main reasons of such performance 
degradation lies in the model?s ability to cope with 
OOV words. Actually, even when the test set has 
the same domain properties as the training set, the 
ability of the model to recall OOV words is still the 
main obstacle to achieve better performance of 
segmentation. However, when the test set is differ-
ent with the training set in nature, the OOV recall 
normally drops much more substantially, and be-
comes much lower. 
Apart from the supervised approach, Sun et al 
(2004) proposed an unsupervised way of Chinese 
word segmentation. The approach did not use any 
predefined lexicons or segmented texts. A statistic 
named as md, combining the mutual information 
and t score, was proposed to measure whether a 
string of characters forms word. The unsupervised 
nature of the approach means good ability to deal 
with domain variation. However, the approach did 
not show a segmentation performance as good as 
that of the supervised approach. The approach was 
not evaluated in F measurement, but in accuracy of 
word break prediction. As their experiment showed, 
the approach successfully predicted 85.88% of the 
word breaks, which is much lower than that of the 
character tagging approach if in terms of F meas-
urement.   
Aiming at preventing the OOV recall from 
dropping sharply and still maintaining an overall 
performance as good as that of the state-of-art 
segmenter when working with heterogeneous test 
sets, we propose in this paper to use a semi-
supervised way for Chinese word segmentation 
task. Specifically, we propose to use ?2 statistics 
together with bootstrapping strategies to build Chi-
nese word segmentation model. The experiment 
shows the approach can effectively promote the 
OOV recall and lead to a higher overall perform-
ance. In addition, instead of using the popular CRF 
model, we use another sequence labeling model in 
this paper --- the hidden Markov Support Vector 
Machines (SVM-HMM) Model (Altun et al, 2003). 
We just wish to show that there are alternatives 
other than CRF model to use and comparable re-
sults can be obtained. 
Our work differs from the previous supervised 
work in its ability to cope with domain variation 
and differs from the previous unsupervised work in 
its much better overall segmentation performance.   
The rest of the paper is organized as follows: In 
section 2, we give a brief introduction to the hid-
den Markov Support Vector Machines, on which 
we rely to build the segmentation model. In section 
3, we list the segmentation tags and the basic fea-
ture templates we used in the paper. In section 4 
we show how ?2 statistics can be encoded as fea-
tures to promote OOV recall. In section 5 we give 
the bootstrapping strategy. In section 6, we report 
the experiments and in section 7 we present our 
conclusions. 
2 The hidden Markov support vector ma-
chines  
The hidden Markov support vector machine 
(SVM-HMM) is actually a special case of the 
structural support vector machines proposed by 
Tsochantaridis et al (2005). It is a powerful model 
to solve the structure predication problem. It dif-
fers from support vector machine in its ability to 
model complex structured problems and shares the 
max-margin training principles with support vector 
machines. The hidden Markov support vector ma-
chine model is inspired by the hidden Markov 
model and is an instance of structural support vec-
tor machine dedicated to solve sequence labeling 
learning, a problem that CRF model is assumed to 
solve. In the SVM-HMM model, the sequence la-
beling problem is modeled by learning a discrimi-
nant function F: X?Y?R over the pairs of input 
sequence and label sequence, thus the prediction of 
the label sequence can be derived by maximizing F 
over all possible label sequences for a specific giv-
en input sequence x. 
);,(maxarg);( wyxwx
y
Ff
Y?
=  
 
In the structural SVMs, F is assumed to be linear 
790
in some combined feature representation of the 
input sequence and the label sequence ?(x,y), i.e. 
),(,);,( yx?wwyx =F  
Where w denotes a parameter vector, for the SVM-
HMMs, the discriminant function is defined as fol-
lows. 
? ??
??
?=
=
?? ??
+
??
+
=
1..1
..1
'
1
', )',(),(?
),()(,);,(
Tt
Tt
y y
tt
yy
y
tt
y
yyyy
yyyxF
???
?
w
x?ww
 
Here )?,( www =  , ?(xt) is the vector of features of 
the input sequence. ? (yt, y) is the Kronecker func-
tion, i.e., 
??
?
?
==
yy
yy
yy t
t
t
 if0
 if1
),(?  
The first term of the discriminant function is used 
to model the interactions between input features 
and labels, and the second term is used to model 
interactions between nearby labels. ? > 0 is a scal-
ing factor which balances the two types of contri-
butions. (Tsochantaridis et al, 2005) 
Like SVMs, parameter vector w is learned with 
the maximum margin principle by using training 
data. To control the complexity of the training 
problem, the cutting plane method is used to solve 
the resulted constrained optimization problem. 
Thus only a small subset of constraints from the 
full-sized optimization is checked to ensure a suf-
ficiently accurate solution. Roughly speaking, 
SVM-HMM differs from CRF in its principle of 
training, and both of them could be used to deal 
with sequence labeling problem like Chinese word 
segmentation. 
3 The tag set and the basic feature tem-
plates 
As in most other work on segmentation, we use a 
4-tag tagset, that is S for the character being a sin-
gle-character-word by itself, B for the character 
beginning a multi-character-word, E for the char-
acter ending a multi-character-word and M for a 
character occurring in the middle of a multi-
character-word. 
We use the following feature templates, as are 
widely used in most segmentation work: 
(a) Cn (n = -2, -1, 0, 1, 2) 
(b) CnCn+1 (n = -2, -1, 0, 1) 
(c) C-1C+1  
Here C refers to a character; n refers to the position 
index relative to the current character. By setting 
the above feature templates, we actually set a 5-
character window to extract features, the current 
character, 2 characters to its left and 2 characters to 
its right.   
In addition, we also use the following feature 
templates to extract features representing the char-
acter type: 
(d) Tn (n = -2, -1, 0, 1, 2) 
(e) TnTn+1 (n = -2, -1, 0, 1) 
(f) T-1T+1 
Here T refers to a character type, and its value can 
be digit, letter, punctuation or Chinese character. 
The type feature is important, for there are two 
versions of Arabic numbers, Latin alphabets and 
punctuations in the Chinese texts. This is because 
all three kinds of characters have their internal 
codes defined in ASCII table, but the Chinese en-
coding standard like GB18030 assigns them with 
other double-byte codes. This causes problems for 
model learning as we encounter in the experiment. 
The training data we adopt in this paper only use 
numbers, letters and punctuation of double-byte 
codes. But the test data use both the double-byte 
and single-byte codes. If the type features are not 
introduced, most of the numbers, letters and punc-
tuation of single-byte can not be segmented cor-
rectly. The type feature establishes links between 
the two versions of codes, for both versions of a 
digit, a letter or punctuation share the same type 
feature value. Actually, the encoding problem 
could be alternatively solved by a character nor-
malization process. That is the mapping all single-
byte versions of digits, letters and punctuations in 
the test sets into their double-byte counterparts as 
in the training set. We use the type features here to 
avoid any changes to the test sets. 
4 The ?2 statistic features 
?2 test is one of hypothesis test methods, which can 
be used to test if two events co-occur just by 
chance or not. A lower ?2 score normally means 
the two co-occurred events are independent; oth-
erwise they are dependent on each other. ?2 score 
is widely used in computational linguistics to ex-
tract collocations or terminologies. Unsupervised 
segmentation approach also mainly relies on mu-
tual information and t-score to identify words in 
Chinese texts (Sun et al, 2004). Inspired by their 
791
work, we believe that ?2 statistics could also be 
incorporated into supervised segmentation models 
to deal with the OOV issue. The idea is very 
straightforward. If two continuous characters in the 
test set have a higher ?2 score, it is highly likely 
they form a word or are part of a word even they 
are not seen in the training set.  
The ?2 score of a character bigram (i.e. two con-
tinuous characters in the text) C1C2 can be com-
puted by the following formula. 
 
)()()()(
)(),(
2
21
2
dcdbcaba
cbdanCC +?+?+?+
????=?  
Here,  
a refers to all counts of bigram C1C2 in the text; 
b refers to all counts of bigrams that C1 oc-curs 
but C2 does not; 
c refers to all counts of bigrams that C1 does not 
occur but C2 occurs; 
d refers to all counts of bigrams that both C1 and 
C2 do not occur.  
n refers to total counts of all bigrams in the text, 
apparently, n= a + b + c + d. 
We do the ?2 statistics computation to the train-
ing set and the test set respectively. To make the ?2 
statistics from the training set and test set compa-
rable, we normalize the ?2 scores by the following 
formula.  
??
???
? ??
?= 10),(),( 2
min
2
max
2
min21
2
21
2
??
??? CCCCnorm  
To make the learned model sensitive to the ?2 sta-
tistics, we then add two more feature templates as 
follows: 
(g) XnXn+1 (n = -2, -1, 0, 1) 
(h) X-1X+1 
The value of the feature XnXn+1 is the normalized ?2 
score of the bigram CnCn+1. Note we also compute 
the normalized ?2 score to bigram C-1C+1, which is 
to measure the association strength of two inter-
vened characters. 
By using the ?2 features, statistics from the test 
set are introduced into segmentation model, and it 
makes the resulted model more aware of the test 
set and therefore more robust to test domains other 
than training domains. 
Because the normalized ?2 score is one of 11 
possible values 0, 1, 2, ?, 10,  templates (g)-(h) 
generate 55 features in total.   
All features generated from the templates (a)-(f) 
together with the 55 ?2 features form the whole 
feature set. The training set and test set are then 
converted into their feature representations. The 
feature representation of the training set is then 
used to learn the model and the feature representa-
tion of the test set is then used for segmentation 
and evaluated by comparison with gold standard 
segmentation. The whole process is shown in Fig-
ure-1. 
 
 
Figure-1. The workflow 
 
By this way, an OOV word in the test set might be 
found by the segmentation model if the bigrams 
extracted from this word take higher ?2 scores. 
5 the bootstrapping strategy 
The addition of the ?2 features can be also prob-
lematic as we will see in the experiments. Even 
though it could promote the OOV recall signifi-
cantly, it also leads to drops in in-vocabulary (IV) 
recall.  
We are now in a dilemma. If we use ?2 features, 
we get high OOV recall but a lower IV recall. If 
we do not use the ?2 feature, we get a lower OOV 
recall but a high IV recall. To keep the IV recall 
from falling, we propose to use a bootstrapping 
method. Specifically, we choose to use both mod-
els with ?2 features and without ?2 features. We 
training set test set 
?2 score computation ?2 score computation 
feature  
represenation 
feature set 
feature  
represenation 
model learning 
segmentation 
model segmentation 
segmentation 
result 
792
train two models firstly, one is ?2-based and the 
other not. Then we do the segmentation for the test 
set with the two models simultaneously. Two seg-
mentation results can be obtained. One result is 
produced by the ?2-based model and has a high 
OOV recall. The other result is produced by the 
non- ?2-based model and has a higher IV recall. 
Then we compare the two results and extract all 
sentences that have equal segmentations with the 
two models as the intersection of the two results. It 
is not difficult to understand that the intersection of 
the two results has both high OOV recall and high 
IV recall, if we also extract these sentences from 
the gold standard segmentation and perform 
evaluations. We then put the intersection results 
into the training set to form a new training set. By 
this new training set, we train again to get two new 
models, one ?2-based and the other not. Then the 
two new models are used to segment the test set. 
Then we do again intersection to the two results 
and their common parts are again put into the train-
ing set. We repeat this process until a plausible 
result is obtained. 
The whole process can be informally described 
as the following algorithm: 
1. let training set T to be the original training set; 
2. for I = 0 to K 
1) train the ?2-based model by using training 
set T; 
2) train the non- ?2-based model by using 
training set T; 
3) do segmentation by using the ?2-based 
model; 
4) do segmentation by using the non- ?2-
based model; 
5) do intersection to the two segmentation re-
sults 
6) put the intersection results into the training 
set and get the enlarged training set T 
3. train the non- ?2-based model using training 
set T, and take the output of this model as the 
final output; 
4. end. 
6 The experiments and discussions 
6.1  On the training set and test set 
For training the segmentation model, we use the 
training data provided by Peking University for 
bakeoff 20052 . The training set has about 1.1 mil-
lion words in total. The PKU training data is actu-
ally consisted of all texts of the People?s Daily 
newspaper in January of 1998. So the training data 
represents very formal written Chinese and mainly 
are news articles. A characteristic of the PKU data 
is that all Arabic numbers, Latin letters and punc-
tuations in the data are all double-byte GB codes; 
there are no single-byte ASCII versions of these 
characters in the PKU training data. 
We use three different test sets. The first one 
(denoted by A) is all texts of the People?s Daily of 
February in 19983 . Its size and the genre of the 
texts are very similar to the training data. We use 
this test set to show how well the SVM-HMM can 
be used to model segmentation problem and the 
performance that a segmentation model achieves 
when applied to the texts from the same domain. 
The second and the third test sets are set to test 
how well the segmentation model can apply to 
texts from other domains. The second test set (de-
noted by B) is from the literature domain and the 
third (denoted by C) from computer domain. We 
segmented them manually according to the guide-
lines of Peking University4 to use as gold standard 
segmentations. The genres of the two test set are 
very different from the training set. There are even 
typos in the texts. In the computer test set, there are 
many numbers and English words. And most of the 
numbers and letters are single-byte ASCII codes.   
The sizes and the OOV rates of the three test 
sets are shown in Table-1. 
 
Table-1. Test sets statistics 
test set domain word count OOV rate
A Newspaper 1,152,084 0.036 
B Literature 72,438 0.058 
C Computer 69,671 0.159 
 
For all the experiments, we use the same evalua-
tion measure as most of previous work on segmen-
tation, that is the Recall(R), Precision(P), F 
measure (F=2PR/(P+R)), IV word recall and OOV 
word recall. In addition, we also evaluate all the 
test results with sentence accuracies (SA), which is 
the proportion of the correctly segmented sen-
tences in the test set.  
                                                          
2 can be download from http://www.sighan.org/bakeoff2005/ 
3 The corpus can be licensed from Peking University. 
4 See http:// www.sighan.org/bakeoff2005/ 
793
6.1 SVM-HMM vs. CRF 
To show how well the SVM-HMM model can be 
used to model segmentation tasks and its perform-
ance compared to that of CRF model, we use the 
training set to train two models, one with SVM-
HMM and the other with CRF.  
The implementations of SVM-HMM and CRF 
model we use in the paper can be found and down-
loaded respectively via Internet. 5 
To make the results comparable, we use the 
same feature templates, that is feature template (a)-
(c). However, SVM-HMM takes interactions be-
tween nearby labels into the model, which means 
there is a label bigram feature template implicitly 
used in the SVM-HMM. So when training the CRF 
model we also use explicitly the label bigram fea-
                                                          
5 http://www.cs.cornell.edu/People/tj/svm_light/ 
svm_hmm.html, and http://sourceforge.net/projects/crfpp/ 
ture template to model interactions between nearby 
labels6.   
For the SVM-HMM model, we set ? to 0.25. 
This is a parameter to control the accuracy of the 
solution of the optimization problem. We set C to 
half of the number of the sentences in the training 
data according to our understanding to the models. 
The C parameter is set to trade off the margin size 
and training error. For CRF model, we use all pa-
rameters to their default value.  We do not do pa-
rameter optimizations to both models with respect 
their performances.   
We use test set A to test both models. For both 
models, we use the same cutoff frequency to fea-
ture extraction. Only those features that are seen 
more than three times in texts are actually used in 
the models. The performances of the two models 
are shown in Table-2, which shows SVM-HMM 
can be used to model Chinese segmentation tasks 
                                                          
6 specified by the B template as the toolkit requires.  
Table-2. Performance of the SVM-HMM  and CRF model 
Models P R F Riv Roov SA 
SVM-HMM 0.9566 0.9528 0.9547 0.9620 0.7041 0.5749 
CRF 0.9541 0.9489 0.9515 0.9570 0.7185 0.5570 
 
Table-3. Performance of the basic model 
test set P R F Riv Roov SA 
A 0.9566 0.9528 0.9547 0.9620 0.7041 0.5749 
B 0.9135 0.9098 0.9116 0.9295 0.5916 0.4698 
C 0.7561 0.8394 0.7956 0.9325 0.3487 0.2530 
 
Table-4. Performance of the type sensitive model 
test set P R F Riv Roov SA 
A 0.9576 0.9522? 0.9549 0.9610? 0.7161 0.5766 
B 0.9176 0.9095? 0.9136 0.9273? 0.6228 0.4832 
C 0.9141 0.8975 0.9057 0.9381 0.6839 0.4287 
 
Table-5. Performance of the ?2-based model 
test set P R F Riv Roov SA 
A 0.9585 0.9518? 0.9552 0.9602? 0.7274 0.5736? 
B 0.9211 0.8971? 0.9090? 0.9104? 0.6825 0.4648? 
C 0.9180 0.8895? 0.9035? 0.9209? 0.7239 0.4204? 
 
Table-6. Performance of the bootstrapping model 
test set P R F Riv Roov SA 
B 0.9260 0.9183 0.9221 0.9329 0.6830 0.5120 
C 0.9113? 0.9268 0.9190 0.9482 0.8138 0.5039 
 
794
and comparable results can be achieved like CRF 
model.   
6.2 The baseline model 
To test how well the segmentation model applies to 
other domain texts, we only use the SVM-HMM 
model with the same parameters as in section 6.1 
and the same cutoff frequency.  
For a baseline model, we only use feature tem-
plates (a)-(c), the performances of the basic model 
on the three test sets are shown in Table-3. 
For the test set A, which is from the same do-
main as the training data, an F-score 0.95 is 
achieved. 
For test set B and C, both are from different do-
mains with the training data, the F-scores drop sig-
nificantly. Especially the OOV recalls fall 
drastically, which means the model is very sensi-
tive to the domain variation. Even the IV recalls 
fall significantly. This also shows the domain port-
ability of the segmentation model is still an obsta-
cle for the segmentation model to be used in cross-
domain applications.  
6.3 The type features 
As we noted before, there are different encoding 
types for the Arabic numbers, Latin letters and 
punctuations. Especially, test set C is full of single-
byte version of such numbers, letters and punctua-
tions. The introduction of type features may im-
prove performance of the model to the test set. 
Therefore, we use the feature tem-plates (a)-(f) to 
train a type sensitive model with the training data. 
This gives segmentation results shown in table-4. 
(The symbol ? means performance drop compared 
with a previous model) 
As we can see, for test set A, the type features 
almost contribute nothing; the F-score has a very 
slight change. The IV recall even has a slight fall 
while the OOV recall rises a little. 
For test set C, the type features bring about very 
significant improvement. The F-score rises from 
0.7956 to 0.9057, and the OOV recall rises from 
0.3487 to 0.6839. Different with the test set A, 
even the IV recall for test set C rises slightly. The 
reason of such a big improvement lies in that there 
are many single-byte digits, letters and punctua-
tions in the texts.    
 Unlike test set C, there are not so many single-
byte characters in test set B. Even though the OOV 
recall does rise significantly, the change in OOV 
recall for test set B is not as much as that for test 
set B. Type features contribute much to cross do-
main texts. 
6.4 The ?2-based model 
Compared with OOV recall for test set A, the OOV 
recall for test set B and C are still lower. To pro-
mote the OOV recall, we use the feature templates 
(a)-(h) to train a ?2-based model with the training 
data. This gives segmentation results shown in ta-
ble-5.   
  As we see from table-5, the introduction of the 
?2 features does not improve the overall perform-
ance. Only F-score for test set A improves slightly, 
the other two get bad. But the OOV recall for the 
three test sets does improve, especially for test set 
B and C. The IV recalls for the three test sets drop, 
especially for test set B and C. That's why the F 
scores for test B and C drop.  
6.5 Bootstrapping  
To increase the OOV recall and prevent the IV re-
call from falling, we use the bootstrapping strategy 
in section 5. 
We set K = 3 and run the algorithm shown in 
section 5. We just do the bootstrapping to test set B 
and C, because what we are concerned with in this 
paper is to improve the performance of the model 
to different domains. This gives results shown in 
Table-6. As we see in Table-6, almost all evalua-
tion measurements get improved. Not only the 
OOV recall improves significantly, but also the IV 
recall improves compared with the type-sensitive 
model.  
To illustrate how the bootstrapping strategy 
works, we also present the performance of the in-
termediate models on test set C in each pass of the 
bootstrapping in table-7 and table-8. Table-7 is 
results of the intermediate ?2-based models for test 
set C. Table-8 is results of the intermediate non-
 ?2-based models for test set C. Figure-2 illustrates 
changes in OOV recalls of both non- ?2-based 
models and ?2-based models as the bootstrapping 
algorithm advances for test set C. Figure-3 illus-
trates changes in IV re-calls of both non- ?2-based 
models and ?2-based models for test set C. As we 
can see from Figure-2 and Figure-3, the ability of 
non- ?2-based model gets improved to the OOV 
795
recall of the ?2-based model as the bootstrapping 
algorithm advances. The abilities to recall IV 
words of both models improve, and even the final 
IV recall of the ?2-based model surpasses the IV 
recall of the type sensitive model shown in Table-3. 
(0.9412 vs. 0.9381). To save the space of the paper, 
we do not list all the intermediate results for test 
set B. We just show the changes in OOV recalls 
and IV recalls as illustrated in Figure-4 and Figure-
5. One can see from Figure-4 and Figure-5, the 
bootstrapping strategy also works for test set B in a 
similar way as it works for test set C.  
 
 
0.6
0.65
0.7
0.75
0.8
0.85
0 1 2 3 4
without chi-square features with chi-square features  
 
Figure-2 the Changes in OOV recalls for test set C 
as boot-strapping algorithm advances 
0.905
0.91
0.915
0.92
0.925
0.93
0.935
0.94
0.945
0.95
0.955
0 1 2 3 4
without chi-square features with chi-square features   
 
Figure-3 the Changes in IV recalls for test set C as 
boot-strapping algorithm advances 
 
0.6
0.62
0.64
0.66
0.68
0.7
0.72
0 1 2 3 4
without chi-square features with chi-square features   
 
Figure-4 the Changes in OOV recalls for test set B 
as boot-strapping algorithm advances 
 
Table-7. Performance of the intermediate ?2-based models for test set C 
I P R F Riv Roov SA 
0 0.9180 0.8895 0.9035 0.9209 0.7239 0.4204
1 0.9084 0.9186 0.9134 0.9387 0.8126 0.4762
2 0.9083 0.9187 0.9134 0.9386 0.8138 0.4822
3 0.9068 0.9208 0.9137 0.9412 0.8131 0.4816
 
Table-8. Performance of the intermediate non-?2-based models  
for test set C 
I P R F Riv Roov SA 
0 0.9141 0.8975 0.9057 0.9381 0.6839 0.4287
1 0.9070 0.9249 0.9159 0.9478 0.8044 0.4869
2 0.9093 0.9254 0.9173 0.9476 0.8087 0.4947
3 0.9111 0.9266 0.9188 0.9481 0.8133 0.5030
4 0.9113 0.9268 0.9190 0.9482 0.8138 0.5039
 
Table-9. Performance of the intersection of the intermediate ?2-based 
model and non-?2-based model for test C 
I P R F Riv Roov SA 
0 0.9431 0.9539 0.9485 0.9664 0.8832 0.6783
1 0.9259 0.9434 0.9345 0.9609 0.8491 0.5992
2 0.9178 0.9379 0.9277 0.9582 0.8316 0.5724
3 0.9143 0.9347 0.9244 0.9559 0.8250 0.5616
 
796
 0.9
0.905
0.91
0.915
0.92
0.925
0.93
0.935
1 2 3 4 5
without chi-square features with chi-square features   
 
Figure-5 the Changes in IV recalls for test set B 
as boot-strapping algorithm advances 
 
As we mentioned in section 5, the intersection of 
the results produced by ?2-based model and non-
 ?2-based model has both high OOV recall and 
high IV recall, that?s the reason why bootstrapping 
strategy works. This can be seen from Table-9. 
However, as the algorithm progresses, both the 
OOV recall and IV recall of the intersection results 
fall, but are still higher than OOV recall and IV 
recall of the final results on the whole test set. 
As we said before, we give also sentence accu-
racies of all segmentation models. With the ?2 sta-
tistics and bootstrapping strategies, the sentence 
accuracy also rises. 2.8% more sentences on test 
set B and 7.5% more sentences on test set C are 
correctly segmented, compared with the type-
sensitive model.     
7 Conclusions 
Sequence labeling models are widely used in Chi-
nese word segmentation recently. High perform-
ance can be achieved when the test data is from the 
same domain as the training data. However, if the 
test data is assumed to be from other domains than 
the domain of the training data, the segmentation 
models always underperform substantially. To en-
hance the portability of the sequence labeling seg-
mentation models to other domains, this paper 
proposes to use ?2 statistics and bootstrapping 
strategy. The experiment shows the approach sig-
nificantly increases both IV recall and OOV recall 
when processing texts from different domains.  
We also show in this paper that hidden Markov 
support vector machine which is also a sequence 
labeling model like CRF can be used to model the 
Chinese word segmentation problem, by which 
high F-score results can be obtained like those of 
CRF model. 
One concern to the bootstrapping approach in 
this paper is that it takes time to work with, which 
will make it difficult to be incorporated into lan-
guage applications that need to responses in real 
time. However, we believe that such an approach 
can be used in offline contexts. For online use in a 
specified domain, one can first train models by 
using the approach in the paper with prepared raw 
texts from the specified domain and then use the 
final non-?2-based model to segment new texts of 
the same domain, since statistics of the target do-
main are more or less injected into the model by 
the iteration of bootstrapping.    
Acknowledgments 
This work is supported by National Natural Sci-
ence Foundation of China under Grant No. 
60975054 and National Social Science Foundation 
of China under Grant No. 06BYY048.  
We would like to give thanks to Prof. Duan 
Huiming for her work in preparing the gold stan-
dard segmentation and to the anonymous reviewers 
for their comments to the paper. 
References  
Altun,Yasemin et al,2003, Hidden Markov Support 
Vector Machines. Proceedings of the Twentieth Iter-
national Conference on Machine Learning (ICML-
2003), Washington DC, 2003. 
Gao, Jianfeng et al, 2005, Chinese Word Segmentation 
and Named Entity Recognition: A Pragmatic Ap-
proach, Computational Linguis-tics,Vol.31, No.4, 
pp531-574. 
Huang, Changning et al 2007, Chinese word segmenta-
tion: a decade review. Journal of Chinese Informa-
tion Processing, Vol.21, NO.3,pp8?19.(in Chinese) 
Liang, Nanyuan, 1987. ??written Chinese text segmenta-
tion system--cdws?. Journal of Chinese Information 
Processing, Vol.2, NO.2, pp44?52.(in Chinese) 
Low, Jin Kiat et al,2005, A Maximum Entropy Ap-
proach to Chinese Word Segmentation. Proceedings 
of the Fourth SIGHAN Workshop on Chinese Lan-
guage Processing, Jeju Island, Ko-rea,. pp161-164 
Sun, Maosong et al, 2004, Chinese word segmentation 
without using dictionary based on unsupervised 
learning strategy. Chinese Journal of Computers. 
Vol.27, No.6, pp736-742. (in Chinese)  
797
Tseng, Huihsin et al, 2005, A conditional random field 
word segmenter for SIGHAN 2005, Proceedings of 
the fourth SIGHAN workshop on Chinese language 
processing. Jeju Island, Korea. pp168-171 
Tsochantaridis,Ioannis et al, 2005, Large Margin Meth-
ods for Structured and Interdependent Output Vari-
ables, Journal of Machine Learning Research 
(JMLR), No.6, pp1453-1484.  
Xue, Nianwen, 2003, Chinese Word Segmentation as 
Character Tagging, Computational Linguistics and 
Chinese Language Processing. Vol.8, No.1, pp29-48. 
Zhao, Hai et al, 2006, Effective tag set selection in 
Chinese word segmentation via conditional random 
field modeling, Proceedings of the 20th Pacific Asia 
Conference on language, Information and Computa-
tion (PACLIC-20), Wuhan, China, pp87-94  
798
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1?11,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Event-based Time Label Propagation for Automatic Dating of News Articles
Tao Ge Baobao Chang? Sujian Li Zhifang Sui
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
No.5 Yiheyuan Road, Haidian District, Beijing, P.R.China, 100871
{getao,chbb,lisujian,szf}@pku.edu.cn
Abstract
Since many applications such as timeline sum-
maries and temporal IR involving temporal
analysis rely on document timestamps, the
task of automatic dating of documents has
been increasingly important. Instead of using
feature-based methods as conventional mod-
els, our method attempts to date documents
in a year level by exploiting relative tempo-
ral relations between documents and events,
which are very effective for dating documents.
Based on this intuition, we proposed an event-
based time label propagation model called
confidence boosting in which time label in-
formation can be propagated between docu-
ments and events on a bipartite graph. The ex-
periments show that our event-based propaga-
tion model can predict document timestamps
in high accuracy and the model combined with
a MaxEnt classifier outperforms the state-of-
the-art method for this task especially when
the size of the training set is small.
1 Introduction
Time is an important dimension of any informa-
tion space and can be useful in information re-
trieval, question-answering systems and timeline
summaries. In the applications involving tempo-
ral analysis, document timestamps are very useful.
For instance, temporal information retrieval mod-
els take into consideration the document?s creation
time for document retrieval and ranking (Kalczyn-
ski and Chou, 2005; Berberich et al, 2007) for bet-
ter dealing with time-sensitive queries; some infor-
?Corresponding author
mation retrieval applications such as Google Scholar
can list articles published during the time a user
specifies for better satisfying users? needs. In addi-
tion, timeline summarization techniques (Hu et al,
2011; Binh Tran et al, 2013) and some event-event
ordering models (Chambers and Jurafsky, 2008;
Yoshikawa et al, 2009) also rely on the timestamps.
Unfortunately, many documents on the web do not
have a credible timestamp, as Chambers (2012) re-
ported. Therefore, it is significant to date docu-
ments, that is to predict document creation time.
One typical method for dating document is based
on temporal language models, which were first used
for dating by de Jong et al (2005). They learned
language models (unigram) for specific time periods
and scored articles with normalized log-likelihood
ratio scores. The other typical approach for the task
was proposed by Nathanael Chambers (2012). In
Chambers?s work, discriminative classifiers ? max-
imum entropy (MaxEnt) classifiers were used by
incorporating linguistic features and temporal con-
straints for training, which outperforms the previous
temporal language models on a subset of Gigaword
Corpus (Graff et al, 2003).
However, the conventional methods have some
limitations because they predict creation time of
documents mainly based on feature-based models
without understanding content of documents, which
may lead to wrong predictions in some cases. For
instance, assume that D1 and D2 are documents
whose content is given as follows:
(D1) Sudan last year accused Eritrea of
backing an offensive by rebels in the east-
ern border region.
1
(D2) Two years ago, Sudan accused Er-
itrea of backing an offensive by rebels in
the eastern border region.
SinceD1 andD2 share many important features, the
previous dating methods are very likely to predict
the same timestamp for the two documents. How-
ever, it will be easy to infer that the creation time of
D1 should be one year earlier than that of D2 if we
analyze the content of the two documents.
Unlike the previous methods, this paper exploits
relative temporal relations between events and doc-
uments for dating documents on the basis of an un-
derstanding of document content.
It is known that each event in a news article has
a relative temporal relation with the document. By
analyzing the relative temporal relation, time of the
event can be known if we know the document times-
tamp; on the other hand, if the time of an event is
known, it can also be used to predict the creation
time of documents mentioning the event, which can
be best demonstrated with the above-mentioned ex-
ample of D1 and D2. In the example, ?last year?
is an important cue to infer that the event mentioned
by the documents occurred in 2002 if we know the
timestamp of D1 is 2003. With the information that
the event occurred in 2002, it can also be inferred
from the temporal expression ?Two years ago? that
D2 was written in 2004. In this way, the timestamp
of the labeled document (D1) is propagated to the
unlabeled document (D2) through the event both of
them mention, which is the main intuition of this pa-
per.
In fact, this intuition seems practical to date doc-
uments on the web because web data is very re-
dundant. Many documents on the web can be con-
nected via events because an event is usually men-
tioned by different documents. According to our
analysis of a collection of news articles spanning 5
years, it is found that an event is mentioned by 3.44
news articles on average; on the other hand, a doc-
ument usually refers to multiple events. Therefore,
if one knows a document timestamp, time of events
the document mentions can be obtained by analyz-
ing the relative temporal relations between the doc-
ument and the events. Likewise, if the time of an
event is known, then it can be used to predict cre-
ation time of the documents which mention it.
Based on the intuition, we proposed an event-
based time label propagation model called confi-
dence boosting in which timestamps are propagated
according to relative temporal relations between
documents and events. In this way, documents can
be dated with an understanding of content so that
this model can date document more credibly. To our
knowledge, it is the first time that the relative tempo-
ral relations between documents and events are ex-
ploited for dating documents, which is proved to be
effective by the experimental results.
2 Event-based Time Label Propogation
As mentioned above, the relative temporal relations
between documents and events are useful for dat-
ing documents. By analyzing the temporal relations,
even if there are only a small number of documents
labeled with timestamps, this information can be
propagated to documents connected with them on a
bipartite graph using breadth first traversal (BFS).
Figure 1: An example of BFS-based propagation
As shown in figure 1, there are two kinds of nodes
in the bipartite graph. A document node is a single
document while an event node represents an event.
The edge between a document node and an event
node means that the document mentions the event.
Also, the edge carries the information of the rela-
tive temporal relation between the document and the
event. The label propagation from node i to node j
will occur if BFS condition which is defined as fol-
lows is satisfied:{
eij ? E
i ? L and j /? L
(BFS condition)
When the timestamp of i is propagated to j:
Y (j) = Y (i) + ?(i, j)
L = L ? {j}
where E is the set of edges of the bipartite graph,
eij denotes the edge between node i and j, L is the
set of nodes which have been already labeled with
timestamps, Y (i) is the year of node i and ?(i, j) is
the relative temporal relation between node i and j.
2
In figure 1, the timestamp of document D1 is 2003,
which is known. This information can be propagated
to its adjacent nodes i.e. the event nodes it men-
tions according to the relative temporal relations.
Then, these event nodes propagate their timestamps
to other documents which mention them. By re-
peating this process, the timestamp of the document
can be propagated to documents which are reachable
from the initially labeled document on the bipartite
graph.
Although the BFS-based propagation process can
propagate timestamps from few labeled documents
to a large number of unlabeled ones, it has two short-
comings for this task. First, once one timestamp is
propagated incorrectly, this error will lead to more
mistakes in the following propagations. If such an
error occurred at the beginning of the propagation
process, it would lead to propagation of errors. Sec-
ond, BFS-based method cannot address conflict of
predictions during propagation, which is shown in
figure 2.
Figure 2: Conflict of predictions during propagation
To address the problems of the BFS-based
method, we proposed a novel propagation model
called confidence boosting model which improves
the BFS-based model by optimizing the global con-
fidence of the bipartite graph. In the confidence
boosting model, every node in the bipartite graph
has a confidence which measures the credibility of
the predicted timestamp of the node. When the
timestamp of a node is propagated to other nodes,
its confidence will be also propagated to the tar-
get nodes with some loss. The loss of confi-
dence is called confidence decay. Formally, the
confidence decay process is described as follows:
c(j) = c(i)? ?(i, j)
where c(i) denotes confidence of node i and
?(i, j) is the decay factor from node i to
node j. For guaranteeing that timestamps
can be propagated on the bipartite graph cred-
ibly, we define the following condition which
is called CB (Confidence Boosting) condition:{
eij ? E
c(i)? ?(i, j) > c(j)
(CB condition)
In the confidence boosting model, propagation from
node i to node j will occur only if CB condition is
satisfied. When timestamps are propagated on the
bipartite graph, timestamps and confidence of nodes
will be updated dynamically. A node with high con-
fidence is more active than nodes with low confi-
dence to propagate its timestamp because a node
with high confidence is more likely to satisfy the CB
condition for propagating its timestamp. Moreover,
a prediction with low confidence can be corrected by
the prediction with high confidence. Therefore, the
confidence boosting model can address both prop-
agation of errors and conflict of predictions which
cannot be tackled by the BFS-based model.
However, there are challenges for running such
propagation models in practice. First, the relative
temporal relations between documents and events
are usually unavailable. Second, events extracted
from different documents do not have any connec-
tion even if they refer to the same event. There-
fore, each event is connected with only one docu-
ment in the bipartite graph and thus cannot prop-
agate its timestamp to other documents unless we
perform event coreference resolution. Third, propa-
gations from generic events are very likely to lead to
propagation errors because generic events can hap-
pen in any year. Also, how to set the confidence and
decay factors reasonably in practice for a confidence
boosting model is worthy of investigation. All these
challenges for the propagation models and their cor-
responding solutions will be discussed in Section 3.
3 Details of Event-based Propagation
Models
In this section, details of the event-based time la-
bel propagation models including challenges and
their corresponding solutions are presented. We first
discuss the event extraction and processing involv-
ing relative temporal relation mining, event coref-
erence resolution and distinguishing specific extrac-
tions from generic ones in Section 3.1. Then, we
show the confidence boosting algorithm in detail in
Section 3.2.
3
3.1 Event extraction and processing
As mentioned in previous sections, events play a key
role in the propagation models. We define an event
as a Subject-Predicate-Object (SPO) triple. To ex-
tract events from raw text, an open information ex-
traction software - ReVerb (Fader et al, 2011) is
used. ReVerb is a program that automatically iden-
tifies and extracts relationships from English sen-
tences. It takes raw text as input and outputs SPO
triples which are called extractions.
However, extractions extracted by ReVerb cannot
be used directly for our propagation models for three
main reasons. First, the relative temporal relations
between documents and the extractions are unavail-
able. Second, the extractions extracted from differ-
ent documents do not have any connection even if
they refer to the same event. Third, propagations
from generic events are very likely to lead to propa-
gation errors.
For addressing the three challenges for the prop-
agation models, we first presented a rule-based
method for mining the relative temporal relations be-
tween extractions and documents in Section 3.1.1.
Then, an efficient event coreference resolution
method is introduced in Section 3.1.2. Finally, the
method for distinguishing specific extractions from
generic ones is shown in Section 3.1.3.
3.1.1 Relative temporal relation mining
We used a rule-based method to extract temporal
expressions and used Stanford parser (De Marneffe
et al, 2006) to analyze association between the tem-
poral expressions and the extractions. Specifically,
we define that an extraction is associated with a tem-
poral expression if there is an arc from the predicate
of the extraction to the temporal expression in the
dependency tree. For a certain extraction, there are
the following four cases whose instances are shown
in table 1 for handling.
Case 1: The extraction is associated with an abso-
lute temporal expressions with year mentions in the
sentence.
In this case, the time of the extraction is equal to
the year mention:
Y (ex) = Y earMention
For the example in table 1, Y (ex) = 1999.
Case 2: The extraction is associated with a relative
temporal expression (not involving year) in the sen-
Case Instance
1 In 1999, South Korea exported 89,000
tons of pork to Japan.
2
In April, however, the BOI investments
showed marked improvement.
Last month, Kazini vowed to resign his
top army job.
3 Julius Erving moved with his family to
Florida three years ago.
4 The meeting focused on ways to revive
the stalled Mideast peace process.
Table 1: Instances of various temporal expressions
tence.
In this case, the time of the extraction is equal to
the creation time of the document:
Y (ex) = Y (d)
Case 3: The extraction is associated with a relative
temporal expression (involving specific year gap) in
the sentence.
In this case, the time of the extraction is computed
as follows:
Y (ex) = Y (d)? Y earGap
For the example in table 1, Y (ex) = Y (d)? 3.
Case 4: The extraction is not associated with any
temporal expression in the sentence or the other
cases.
In this case, it is difficult to recognize the rela-
tive temporal relations. However, timeliness can be
leveraged to determine the relations as a heuristic
method. It is known that timeliness is an important
feature of news so that events reported by a news ar-
ticle usually took place a couple of days or weeks
before the article was written. Therefore, we heuris-
tically consider the year of the extraction is the same
with that of its source document in this case:
Y (ex) = Y (d)
In the cases except case 1, the relative tempo-
ral relation between an extraction and the docu-
ment it comes from can be determined. To evalu-
ate the performance of the rule-based method, we
sampled 3,000 extractions from documents written
in the year of 1995-1999 of Gigaword corpus and
manually labeled these extractions with a timestamp
based on their context and their corresponding docu-
ment timestamps as golden standard. Table 2 shows
4
the accuracy of each case which will be used as a
part of the decay factor in the confidence boosting
model.
Case Accuracy
1 0.774(168/217)
2 0.994(844/849)
3 0.836(281/336)
4 0.861(1376/1598)
Total 0.890(2669/3000)
Table 2: Accuracy of the four cases
We define the set of these determined relative tem-
poral relations R as follows:
R = {rd,ex|d = doc(ex), ex ? C2 ? C3 ? C4}
rd,ex =< d, ex, ?(d, ex) >
?(d, ex) = ??(ex, d) = {0,?1,?2,?3, ...}
where Ck is the set of extractions in case k and
doc(ex) is the document which extraction ex comes
from. rd,ex is a triple describing the relative tempo-
ral relation between d and ex. For example, triple
rd,ex =< d, ex,?1 > means that the time of ex-
traction ex is one year before the time of document
d.
3.1.2 Event coreference resolution
Extractions from different documents have no
connections. However, there are a great number of
extractions referring to the same event. For find-
ing such coreferential event extractions efficiently,
hierarchical agglomerative clustering (HAC) is used
to cluster highly similar extractions into one cluster.
We use cosine to measure the similarity between ex-
tractions and select bag of words as features. Note
that it is less meaningful to cluster the extractions
from the same document because coreferential ex-
tractions from the same document are not helpful for
timestamp propagations. For this reason, similarity
between extractions from the same documents is set
to 0.
For HAC, selection of threshold is important. If
the threshold is set too high, only a few extractions
can be clustered despite high purity; on the contrary,
if the threshold is set too low, purity of clusters will
descend. In fact, selection of threshold is a trade-off
between the precision and recall of event corefer-
ence resolution. For selecting a suitable threshold,
extractions from documents written in 1995-1999
are used as a development set.
In practice, it is difficult for us to directly evalu-
ate the performance of the coreference resolution of
event extractions without golden standard which re-
quires much labors for manual annotations. Alterna-
tively, entropy which measures the purity of clusters
is used for evaluation because it can indirectly re-
flect the precision of coreference resolution to some
extent:
Entropy = ?
?
j
nj
n
?
i
P (i, j)? log2 P (i, j)
where P (i, j) is the probability of finding an extrac-
tion whose timestamp is i in the cluster j, nj is the
number of items in cluster j and n is the total num-
ber of extractions. Note that timestamp of an extrac-
tion is assigned based on its document timestamp
using the method proposed in Section 3.1.1.
Figure 3 shows the effect of selection of the
threshold on cluster performance. It can be found
that when the threshold reaches 0.8, the entropy
starts descending gently and is low enough. Since
we want to find as many coreferential extractions as
possible on the premise that the precision is good,
the threshold is set to 0.8. Note that extractions
which are single in one cluster will be filtered out
because they do not have any connections with any
other documents.
0.6 0.65 0.7 0.75 0.8 0.85 0.90.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
Threshold
Ent
ropy
Figure 3: Entropy of clusters under different thresholds
3.1.3 Distinguishing specific events from
generic ones
Not all extractions extracted by ReVerb refer to
a specific event. For instance, the extraction ?Ger-
many?s DAX index was down 0.2 percent? is un-
desirable for our task because it refers to a generic
5
event and this event may occur in any year. In other
words, it is not able to indicate a certain timestamp
and thus propagations from a generic event node are
very likely to result in propagation errors. In con-
trast, the extraction ?some of the provinces in China
were hit by SARS? refers to a specific event which
took place in 2003. For our task, such specific event
extractions which are associated with one certain
timestamp are desirable. For the sake of distinguish-
ing such extractions from the generic ones, a Max-
Ent classifier is used to classify extractions as either
specific ones or generic ones.
Training Set Generation A training set is indis-
pensable for training a MaxEnt classifier. In order
to generate training examples, we performed HAC
discussed in Section 3.1.2 for event coreference res-
olution on extractions from all documents written
in May and June of 1995-1999 and then analyzed
each cluster. If extractions in a cluster have different
timestamps, then the extractions in this cluster will
be labeled as generic extractions (negative); other-
wise, extractions in the cluster are labeled as spe-
cific ones (positive). In this way, the training set can
be generated without manually labeling. To avoid
bias of positive and negative examples, we sampled
3,500 positive examples and 3,500 negative exam-
ples to train the model.
Feature Selection The following features were se-
lected for training:
Named Entities: People and places are often dis-
cussed during specific time periods, particularly in
news genre. Intuitively, if an extraction contains
specific named entities then this extraction is less
likely to be a generic event. If an extraction con-
tains named entities, types and uninterrupted tokens
of the named entities will be included as features.
Numeral: According to our analysis of the train-
ing set generated by the above-mentioned method,
generic extractions usually contain numerals. For
example, the extraction ?15 people died in this ac-
cident? and the extraction ?225 people died in this
accident? have the same tokens except numerals and
they are labeled as a generic event because they are
clustered into one group due to high similarity but
they in fact refer to different events happening in
different years. Therefore, if an extraction contains
numerals, the feature ?NUM? will be included.
Bag of words: Bag of words can also be an indicator
of specific extractions and generic ones. For exam-
ple, an extraction containing ?stock?, ?index?, ?fell?
and ?exchange? is probably a generic one.
The model obtained after training can be used to
predict whether an extraction is a specific one. We
define P (S = 1|ex) as the probability that an ex-
traction is a specific one, which can be provided by
the classifier. Extractions whose probability to be a
specific one is less than 0.05 are filtered out. For the
other extractions, this probability is used as a part of
the decay factor in the confidence boosting model,
which will be discussed in detail in Section 3.2.
3.2 Confidence boosting
After extracting and processing the event extrac-
tions, relative temporal relations between documents
and events can be constructed. This can be for-
mally represented by a bipartite graph G=?V,E?.
There are two kinds of nodes on the bipartite graph:
document nodes and event nodes. Slightly dif-
ferent with the event node mentioned in Section
2, an event node in practice is a cluster of coref-
erential extractions and it can be connected with
multiple document nodes. Note that the bipar-
tite graph does not contain any isolate node. For
briefness, we define DNode as the set of docu-
ment nodes and ENode as the set of event nodes.
The set of edges E is formally defined as follows:
E = {eij , eji|i ? DNode, j ? ENode, ri,j ? R}
where R is the set of relative temporal relations de-
fined as Section 3.1.1.
3.2.1 Confidence and decay factor
As mentioned in Section 2, the confidence of a
node measures the credibility of the predicted times-
tamp. According to the definition, we set the confi-
dence of initially labeled nodes to 1 and set confi-
dence of nodes without any timestamp to 0 in prac-
tice. When the timestamp of a node is propagated
to another node, its confidence will be propagated to
the target node with some loss, as discussed in Sec-
tion 2. The confidence loss is caused by two factors
in practice. The first one is the credibility of the rel-
ative temporal relation between two nodes and the
other one depends on whether an extraction refers to
a specific event.
Relative temporal relations between documents
6
and extractions we mined using the rule-based
method in Section 3.1.1 are not absolutely correct.
The credibility of the relations has an effect on the
confidence decay. Formally, we used pi(i, j) to de-
note the credibility of the relative temporal relation
between node i and node j. The credibility of a rel-
ative temporal relation in each case can be estimated
through table 2. If the credibility of the relative tem-
poral relation between i and j is low, propagation
from node i to j probably leads to error. Therefore,
the confidence loss should be much in this case. On
contrary, if the relation is highly credible, it will be
less likely that propagation errors occur. Therefore,
the confidence loss should be little.
In addition, whether an extraction refers to a
generic event or a specific one exerts an impact on
the confidence loss. If an extraction refers to a
generic event, then the extractions in the same clus-
ter with it probably have different timestamps. Since
our propagation model assumes that extractions in a
cluster are coreferent and thus they should have the
same timestamp, propagations from a generic event
node are very likely to result in propagation errors.
Therefore, the timestamp of a generic event node
in fact is less credible for propagations and confi-
dence of such event nodes should be low for limiting
propagations from the nodes. For this reason, prop-
agation from a document node to a generic event
node leads to much loss of confidence. We define
the probability that an event node refers to a specific
event as follows:
P (S = 1|enode) =
1
|C|
?
ex?C
P (S = 1|ex)
where C is the set of extractions in the event node
and P (S = 1|ex) is the probability that an extrac-
tion refers to a specific event, which can be provided
by the MaxEnt classifier discussed in Section 3.1.3.
Considering the two factors for confidence loss,
we formally define the decay factor by (1).
?(s, t) = (1)
{
pi(s, t) if t ? DNode
pi(s, t)? P (S = 1|t) otherwise
3.2.2 Confidence boosting algorithm
In confidence boosting model, the propagation
from i to j will occur only if the CB condition is
Figure 4: Algorithm of confidence boosting
satisfied. The confidence boosting propagation pro-
cess can be described as figure 4.
Whenever timestamps are propagated to other
nodes, the global confidence of the bipartite graph
will increase. For this reason, this propagation pro-
cess is called confidence boosting. In this model,
a node with high confidence is more active than
nodes with low confidence to propagate its times-
tamp. Moreover, a prediction with low confidence
can be corrected by the prediction with high con-
fidence. Therefore, the confidence boosting model
can alleviate the problem of propagation of errors
to some extent and handle conflict of predictions.
Thus, it can propagate timestamps more credibly
than the BFS-based model. It can also be proved
that each node on the bipartite graph must reach the
highest confidence it can reach so that the global
confidence of the bipartite graph must be optimal
when the confidence boosting propagation process
ends regardless of propagation orders, which will be
discussed in Section 3.2.3.
3.2.3 Proof of the optimality of confidence
boosting
Proof by contradiction can be used to prove that
propagation orders do not affect the optimality of the
confidence boosting model.
Proof Assume by contradiction that there is some
node that does not reach its highest confidence it can
reach when a confidence boosting process in propa-
gation order A ends:
?vt s.t. cA(vt) < c?(vt)
where cA(vt) is the confidence of vt when the
propagation process in order A ends and c?(vt) is
the highest confidence that vt can reach. Assume
that (v1, v2, ? ? ? , vt?1, vt) is the optimal propagation
7
path from the propagation source node v1 to the
node vt that leads to the highest confidence of vt,
which means that c?(vt) = c?(vt?1) ? ?(vt?1, vt),
c?(vt?1) = c?(vt?2) ? ?(vt?2, vt?1), ..., c?(v2) =
c?(v1) ? ?(v1, v2). Then according to CB condi-
tion, since cA(vt?1) ? ?(vt?1, vt) ? cA(vt) <
c?(vt) = c?(vt?1) ? ?(vt?1, vt), the inequality
cA(vt?1) < c?(vt?1) must hold. Similarly, it can be
easily inferred that cA(vt?2) < c?(vt?2) and finally
cA(v1) < c?(v1). Since v1 is the source node whose
timestamp is initially labeled and its confidence is 1,
the inequality cA(v1) < c?(v1) cannot hold. Thus,
the assumption that cA(vt) < c?(vt) cannot be sat-
isfied. Therefore, it can be proved that each node
on the bipartite graph must reach the highest con-
fidence it can reach so that the global confidence of
the bipartite graph must be optimal when confidence
boosting propagation process ends no matter what
order time labels are propagated in.
4 Experiments
In this section, we evaluate the performance of our
time label propagation models and different auto-
matic document dating models on the Gigaword
dataset. We first present the experimental setting.
Then we show experimental results and perform an
analysis.
4.1 Experimental Setting
Dataset To simulate the environment of the web
where data is very redundant, we use all documents
written in April, June, July and September of 2000-
2004 of Gigaword Corpus as dataset instead of sam-
pling a subset of documents from each period. The
dataset contains 900,199 news articles.
Pre-processing Many extractions extracted by Re-
Verb are short and uninformative and do not carry
any valuable information for propagating temporal
information. Also, some extractions do not refer
to events which already happened. These extrac-
tions may affect the performance of event corefer-
ence resolution and the rule-based method proposed
in Section 3.1.1 for mining relative temporal rela-
tions. Therefore, we filter out these undesirable ex-
tractions in advance with a rule-based method. The
rules are shown in table 3. This preprocessing re-
moves large numbers of ?bad? extractions which are
undesirable for our task. As a result, not only com-
putation efficiency but also precision of event coref-
erence resolution will be improved.
Rule1 If the number of tokens of the extrac-
tion is less than 5 then this extraction
will be filtered out.
Rule2 If the maximum idf of terms of the ex-
traction is less than 3.0 then this ex-
traction will be filtered out.
Rule3 If the tense of the extraction is not past
tense then this extraction will be fil-
tered out.
Rule4 If the extraction is the content of di-
rect quotation then this extraction will
be filtered out.
Table 3: Pre-processing Rules
|DNode| 550,124
|ENode| 968,064
|E| 3,104,666
Table 4: Basic information of the bi-partite graph
Basic information of the document-event bipartite
graph constructed is shown in table 4.
Evaluation To evaluate the performance of the
propagation models for the task of dating on differ-
ent sizes of the training set, we used different sizes
of the labeled documents for training and consid-
ered the remaining documents as the test set. Note
that the training set is randomly sampled from the
dataset. To be more persuasive, we repeated above
experiments for five times.
However, in the time label propagation process,
not all documents can be labeled. For those doc-
uments which cannot be labeled in the process of
propagation, a MaxEnt classifier serves as a comple-
mentary approach to predict their timestamps. For
the MaxEnt classifier, unigrams and named entities
are simply selected as features and the initially la-
beled documents as well as documents labeled dur-
ing propagation process are used for training.
Baseline methods are temporal language models
proposed by de Jong et al (2005) and the state-of-
the-art discriminative classifier with linguistic fea-
tures and temporal constraints which was proposed
8
Initially Labeled 1k 5k 10k 50k 100k 200k 500k
Reached Min 443980 448653 453022 484562 518603 599724 732701
Reached Max 444266 448998 454028 484996 519333 579878 732799
Reached Avg 444107 448742 453786 484622 519110 579835 732758
Prop Ratio 444.1 89.7 45.4 9.7 5.2 2.9 1.5
Prop acc(BFS) 0.438 0.515 0.551 0.646 0.691 0.725 0.775
Prop acc(CB) 0.494 0.569 0.603 0.701 0.746 0.776 0.807
Table 5: Performance of Propagation
Initially Labeled 1k 5k 10k 50k 100k 200k 500k
Temporal LMs 0.277 0.323 0.353 0.412 0.422 0.425 0.420
Maxent(Unigrams) 0.326 0.378 0.407 0.486 0.517 0.553 0.590
Maxent(Unigrams+NER) 0.331 0.383 0.418 0.506 0.549 0.590 0.665
Chambers?s 0.331 0.386 0.423 0.524 0.571 0.615 0.690
BFS+Maxent 0.459 0.508 0.533 0.595 0.626 0.658 0.707
CB+Maxent 0.486 0.535 0.559 0.624 0.655 0.685 0.726
Table 6: Overall accuracy of dating models
by Nathanael Chambers (2012). In Chambers?s joint
model, the interpolation parameter ? is set to 0.35
which is considered optimal in his work.
4.2 Experimental Results
Table 5 shows the performance of propagation mod-
els where Reached denotes the number of docu-
ments labeled when the propagation process ends,
prop ratio and prop accuracy are defined as follows:
Prop Ratio =
#ReachedDocNodes
#LabeledDocNodes
Prop Accuracy =
#CorrectDocNodes?#LabeledDocNodes
#ReachedDocNodes?#LabeledDocNodes
where #LabeledDocNodes is the number of ini-
tially labeled document nodes which are documents
in the training set and #ReachedDocNodes is the
number of document nodes labeled when the propa-
gation process ends.
Note that prop ratio and accuracy in table 5 are
the mean of the prop ratio and accuracy of the five
groups of experiments. It is clear that confidence
boosting model improves the prop accuracy over
BFS-based model. When only 1,000 documents
are initially labeled with timestamps, the confidence
boosting model can propagate their timestamps to
more than 400,000 documents with an accuracy of
0.494, approximately 12.8% relative improvement
over the BFS counterpart, which proves effective-
ness of the confidence boosting model.
However, as shown in table 5, hardly can the prop-
agation process propagate timestamps to all doc-
uments. One reason is that the number of docu-
ment nodes on the bipartite graph is only 550,124,
approximately 61.1% of all documents. The other
documents may not mention events which are also
mentioned by other documents, which means they
are isolate and thus are excluded from the bipartite
graph. Also, the event coreference resolution phase
does not guarantee finding all coreferential extrac-
tions; in other words, recall of event coreference res-
olution is not 100%. The other reason is that some
documents are unreachable from the initially labeled
nodes even if they are in the bipartite graph.
The overall accuracy of different dating models
is shown in table 6. As with table 5, overall accu-
racy in table 6 is the average performance of mod-
els in the five groups of experiments. As reported
by Nathanael Chambers (2012), the discriminative
classifier performs much better than the temporal
language models on the Gigaword dataset. In the
case of 500,000 training examples, the Maxent clas-
sifier using unigram features outperforms the tem-
poral language models by 40.5% relative accuracy.
If the size of the training set is large enough, named
9
entities and linguistic features as well as temporal
constraints will improve the overall accuracy sig-
nificantly. However, if the size of the training set
is small, these features will not result in much im-
provement.
Compared with the previous models, the propaga-
tion models predict the document timestamps much
more accurately especially in the case where the size
of the training set is small. When the size of the
training set is 1,000, our BFS-based model and con-
fidence boosting model combined with the MaxEnt
classifier outperform Chambers?s joint model which
is considered the state-of-the-art model for the task
of automatic dating of documents by 38.7% and
46.8% relative accuracy respectively. This is be-
cause the feature-based methods are not very reli-
able especially when the size of the training set is
small. In contrast, our propagation models can pre-
dict timestamps of documents with an understand-
ing of document content, which allows our method
to date documents more credibly than the baseline
methods. Also, by comparing table 5 with table 6,
it can be found that prop accuracy is almost always
higher than overall accuracy, which also verifies that
the propagation models are more credible for dat-
ing document than the feature-based models. More-
over, data is so redundant that a great number of
documents can be connected with events they share.
Therefore, even if a small number of documents are
labeled, the labeled information can be propagated
to large numbers of articles through the connections
between documents and events according to relative
time relations. Even if the size of the training set
is large, e.g. 500,000, our propagation models still
outperform the state-of-the-art dating method. Ad-
ditionally, some event nodes on the bipartite graph
may be labeled with a timestamp during the process
of propagation as a byproduct. The temporal infor-
mation of the events would be useful for other tem-
poral analysis tasks.
5 Related Work
In addition to work of de Jong et al (2005) and
Chambers (2012) introduced in previous sections,
there is also other research focusing on the task of
document dating. Kanhabua and Norvag (2009) im-
proved temporal language models by incorporating
temporal entropy and search statistics and apply-
ing two filtering techniques to the unigrams in the
model. Kumar et al (2011) is also based on the
temporal language models, but more historically-
oriented, which models the timeline from the present
day back to the 18th century. In addition, they used
KL-divergence instead of normalized log likelihood
ratio to measure differences between a document
and a time period?s language model.
However, these methods are based on tempo-
ral language models so they also suffer from the
problem of the method of de Jong et al (2005).
Therefore, they inevitably make wrong predictions
in some cases, just as mentioned in Section 1. Com-
pared with these methods, our event-based propaga-
tion models exploit relative temporal relations be-
tween documents and events for dating document
on a basis of an understanding of document content,
which is more reasonable and also proved to be more
effective by the experimental results.
6 Conclusion
The main contribution of this paper is exploiting
relative temporal relations between events and doc-
uments for the document dating task. Different
with the conventional work which dates documents
with feature-based methods, we proposed an event-
based time label propagation model called confi-
dence boosting in which timestamps are propagated
on a document-event bipartite graph according to
relative temporal relations between documents and
events for dating documents on a basis of an under-
standing of document content. We discussed chal-
lenges for the propagation models and gave the cor-
responding solutions in detail. The experimental re-
sults show that our event-based propagation model
can predict document timestamps in high accuracy
and the model combined with a MaxEnt classifier
outperforms the state-of-the-art method on a data-
redundant dataset.
Acknowledgements
We thank the anonymous reviewers for their valu-
able suggestions. This paper is supported by
NSFC Project 61075067, NSFC Project 61273318
and National Key Technology R&D Program (No:
2011BAH10B04-03).
10
References
Klaus Berberich, Srikanta Bedathur, Thomas Neumann,
and Gerhard Weikum. 2007. A time machine for
text search. In Proceedings of the 30th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 519?526.
ACM.
Giang Binh Tran, Mohammad Alrifai, and Dat
Quoc Nguyen. 2013. Predicting relevant news events
for timeline summaries. In Proceedings of the 22nd
international conference on World Wide Web compan-
ion, pages 91?92. International World Wide Web Con-
ferences Steering Committee.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
698?706. Association for Computational Linguistics.
Nathanael Chambers. 2012. Labeling documents with
timestamps: Learning from their time expressions. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers-
Volume 1, pages 98?106. Association for Computa-
tional Linguistics.
FMG de Jong, Henning Rode, and Djoerd Hiemstra.
2005. Temporal language models for the disclosure
of historical text. Royal Netherlands Academy of Arts
and Sciences.
Marie-Catherine De Marneffe, Bill MacCartney, Christo-
pher D Manning, et al 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, volume 6, pages 449?454.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1535?1545. Association for Computational Linguis-
tics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2003. English gigaword. Linguistic Data Consortium,
Philadelphia.
Po Hu, Minlie Huang, Peng Xu, Weichang Li, Adam K
Usadi, and Xiaoyan Zhu. 2011. Generating
breakpoint-based timeline overview for news topic ret-
rospection. In Data Mining (ICDM), 2011 IEEE 11th
International Conference on, pages 260?269. IEEE.
Pawel Jan Kalczynski and Amy Chou. 2005. Temporal
document retrieval model for business news archives.
Information processing management, 41(3):635?650.
Nattiya Kanhabua and Kjetil N?rva?g. 2009. Us-
ing temporal language models for document dating.
In Machine Learning and Knowledge Discovery in
Databases, pages 738?741. Springer.
Abhimanu Kumar, Matthew Lease, and Jason Baldridge.
2011. Supervised language modeling for temporal res-
olution of texts. In Proceedings of the 20th ACM in-
ternational conference on Information and knowledge
management, pages 2069?2072. ACM.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with markov logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language: Volume 1-Volume 1, pages 405?
413. Association for Computational Linguistics.
11
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 854?863,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Joint Model for Unsupervised Chinese Word Segmentation
Miaohong Chen Baobao Chang Wenzhe Pei
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
Beijing, P.R.China, 100871
miaohong-chen@foxmail.com,{chbb,peiwenzhe}@pku.edu.cn
Abstract
In this paper, we propose a joint model for
unsupervised Chinese word segmentation
(CWS). Inspired by the ?products of ex-
perts? idea, our joint model firstly com-
bines two generative models, which are
word-based hierarchical Dirichlet process
model and character-based hidden Markov
model, by simply multiplying their proba-
bilities together. Gibbs sampling is used
for model inference. In order to further
combine the strength of goodness-based
model, we then integrated nVBE into our
joint model by using it to initializing the
Gibbs sampler. We conduct our experi-
ments on PKU and MSRA datasets pro-
vided by the second SIGHAN bakeoff.
Test results on these two datasets show
that the joint model achieves much bet-
ter results than all of its component mod-
els. Statistical significance tests also show
that it is significantly better than state-
of-the-art systems, achieving the highest
F-scores. Finally, analysis indicates that
compared with nVBE and HDP, the joint
model has a stronger ability to solve both
combinational and overlapping ambigui-
ties in Chinese word segmentation.
1 Introduction
Unlike English and many other western languages,
there are no explicit word boundaries in Chinese
sentences. Therefore, word segmentation is a cru-
cial first step for many Chinese language process-
ing tasks such as syntactic parsing, information re-
trieval and machine translation. A great deal of su-
pervised methods have been proposed for Chinese
word segmentation. While successful, they re-
quire manually labeled resources and often suffer
from issues like poor domain adaptability. Thus,
unsupervised word segmentation methods are still
attractive to researchers due to its independence on
domain and manually labeled corpora.
Previous unsupervised approaches to word seg-
mentation can be roughly classified into two types.
The first type uses carefully designed goodness
measure to identify word candidates. Popular
goodness measures include description length gain
(DLG) (Kit and Wilks, 1999), accessor variety
(AV) (Feng et al., 2004), boundary entropy (BE)
(Jin and Tanaka-Ishii, 2006) and normalized vari-
ation of branching entropy (nVBE) (Magistry and
Sagot, 2012) etc. Goodness measure based model
is not segmentation model in a very strict mean-
ing and is actually strong in generating word list
without supervision. It inherently lacks capabil-
ity to deal with ambiguous string, which is one of
main sources of segmentation errors and has been
extensively explored in supervised Chinese word
segmentation.
The second type focuses on designing sophis-
ticated statistical model, usually nonparametric
Bayesian models, to find the segmentation with
highest posterior probability, given the observed
character sequences. Typical statistical mod-
els includes Hierarchical Dirichlet process (HDP)
model (Goldwater et al., 2009), Nested Pitman-
Yor process (NPY) model (Mochihashi et al.,
2009) etc, which are actually nonparametric lan-
guage models and therefor can be categorized as
word-based model. Word-based model makes de-
cision on wordhood of a candidate character se-
quence mainly based on information outside the
sequence, namely, the wordhood of character se-
quences being adjacent to the concerned sequence.
Inspired by the success of character-based
model in supervised word segmentation, we pro-
pose a Bayesian HMM model for unsupervised
Chinese word segmentation. With the Bayesian
HMM model, we formulate the unsupervised seg-
mentation tasks as procedure of tagging positional
854
tags to characters. Different from word-based
model, character-based model like HMM-based
model as we propose make decisions on word-
hood of a candidate character sequence based on
information inside the sequence, namely, ability of
characters to form words. Although the Bayesian
HMM model alone does not produce competi-
tive results, it contributes substantially to the joint
model as proposed in this paper.
Our joint model takes advantage from three dif-
ferent models: namely, a character-based model
(HMM-based), a word-based model (HDP-based)
and a goodness measure based model (nVBE
model). The combination of HDP-based model
and HMM-based model enables to utilize infor-
mation of both word-level and character-level. We
also show that using nVBE model as initialization
model could further improve the performance to
outperform the state-of-the-art systems and leads
to improvement in both wordhood judgment and
disambiguation ability.
Word segmentation systems are usually eval-
uated with metrics like precision, recall and F-
Score, regardless of supervised or unsupervised.
Following normal practice, we evaluate our model
and compare it with state-of-the-art systems us-
ing F-Score. However, we argue that the ability
to solve segmentation ambiguities is also impor-
tant when evaluating different types of unsuper-
vised word segmentation systems.
This paper is organized as follows. In Section
2, we will introduce several related systems for
unsupervised word segmentation. Then our joint
model is presented in Section 3. Section 4 shows
our experiment results on the benchmark datasets
and Section 5 concludes the paper.
2 Related Work
Unsupervised Chinese word segmentation has
been explored in a number of previous works and
by various methods. Most of these methods can
be divided into two categories: goodness measure
based methods and nonparametric Bayesian meth-
ods.
There have been a plenty of work that is based
on a specific goodness measure. Zhao and Kit
(2008) compared several popular unsupervised
models within a unified framework. They tried
various types of goodness measures, such as De-
scription Length Gain (DLG) proposed by Kit and
Wilks (1999), Accessor Variety (AV) proposed by
Feng et al. (2004) and Boundary Entropy (Jin and
Tanaka-Ishii, 2006). A notable goodness-based
method is ESA: ?Evaluation, Selection, Adjust-
ment?, which is proposed by Wang et al. (2011)
for unsupervised Mandarin Chinese word segmen-
tation. ESA is an iterative model based on a new
goodness algorithm that adopts a local maximum
strategy and avoids threshold setting. One disad-
vantage of ESA is that it needs to iterate the pro-
cess several times on the corpus to get good perfor-
mance. Another disadvantage is the requirement
for a manually segmented training corpus to find
best value for parameters (they called it proper ex-
ponent). Another notable work is nVBE: Mag-
istry and Sagot (2012) proposed a model based
on the Variation of Branching Entropy. By adding
normalization and viterbi decoding, they improve
performance over Jin and Tanaka-Ishii (2006)
and remove most of the parameters and thresholds
from the model.
Nonparametric Bayesian models also achieved
state-of-the-art performance in unsupervised word
segmentation. Goldwater et al. (2009) introduced
a unigram and a bigram model for unsupervised
word segmentation, which are based on Dirichlet
process and hierarchical Dirichlet process (Teh et
al., 2006) respectively. The main drawback is that
it needs almost 20,000 iterations before the Gibbs
sampler converges. Mochihashi et al. (2009) ex-
tended this method by introducing a nested charac-
ter model and an efficient blocked Gibbs sampler.
Their method is based on what they called nested
Pitman-Yor language model.
One disadvantage of goodness measure based
methods is that they do not have any disambigua-
tion ability in theory in spite of their competitive
performances. This is because once the goodness
measure is given, the decoding algorithm will seg-
ment any ambiguous strings into the same word
sequences, no matter what their context is. In
contrast, nonparametric Bayesian language mod-
els aim to segment character string into a ?reason-
able? sentence according to the posterior probabil-
ity. Thus, theoretically, this method should have
better ability to solve ambiguities over goodness
measure based methods.
3 Joint Model
In this section, we will discuss our joint model in
detail.
855
3.1 Combining HDP and HMM
In supervised Chinese word segmentation lit-
erature, word-based approaches and character-
based approaches often have complementary ad-
vantages (Wang et al., 2010).Since the two types
of model try to solve the problem from different
perspectives and by utilizing different levels of in-
formation (word level and character level). In un-
supervised Chinese word segmentation literature,
the HDP-base model can be viewed as a typi-
cal word-based method. And we can also build
a character-based unsupervised model by using a
hidden Markov model. We believe that the HDP-
based model and the HMM-based model are also
complementary with each other, and a combina-
tion of them will take advantage of both and thus
capture different levels of information.
Now the problem we are facing is how to com-
bine these two models. To keep the joint model
simple and involve as little extra parameters as
possible, we combine the two baseline models by
just multiplying their probabilities together and
then renormalizing it. Let C = c
1
c
2
? ? ? c
|C|
be a
string of characters andW = w
1
w
2
? ? ?w
|W |
is the
corresponding segmented words sequence. Then
the conditional probability of the segmentation W
given the character string C in our joint model is
defined as:
P
J
(W |C) =
1
Z(C)
P
D
(W |C)P
M
(W |C) (1)
where P
D
(W |C) is the probability from the HDP
model as given in Equation 6 and P
M
(W |C)
is the probability given by the Bayesian HMM
model as given in Equation 2. Z(C) is a nor-
malization term to make sure that P
J
(W |C) is a
probability distribution. The combining method is
inspired by Hinton (1999), which proved that it is
possible to combine many individual expert mod-
els by multiplying the probabilities and then renor-
malizing it. They called it ?product of experts?.
We can see that combining models in this way
does not involve any extra parameters and Gibbs
sampling can be easily used for model inference.
3.2 Bayesian HMM
The dominant method for supervised Chinese
word segmentation is character-based model
which was first proposed by Xue (2003). This
method treats word segmentation as a tagging
problem, each tag indicates the position of a char-
acter within a word. The most commonly used
tag set is {Single, Begin, Middle, End}. Specifi-
cally, S means the character forms a single word,
B/E means the character is the begining/ending
character of the word, and M means the charac-
ter is in the middle of the word. Existing models
are trained on manually annotated data in a super-
vised way based on discriminative models such as
Conditional Random Fields (Peng et al., 2004;
Tseng et al., 2005). Supervised character-based
methods make full use of character level informa-
tion and thus have been very successful in the last
decade. However, no unsupervised model has uti-
lized character level information in the way as su-
pervised method does.
We can also build a character-based model for
Chinese word segmentation using hidden Markov
model(HMM) as formulated in the following
equation:
P
M
(W |C) =
|C|
?
i=1
P
t
(t
i
|t
i?1
)P
e
(c
i
|t
i
) (2)
where C and W have the same meaning as be-
fore. P
t
(t
i
|t
i?1
) is the transition probability of
tag t
i
given its former tag t
i?1
and P
e
(c
i
|t
i
) is the
emission probability of character c
i
given its tag t
i
.
This model can be easily trained with Maximum
Likelihood Estimation (MLE) on annotated data
or with Expectation Maximization (EM) on raw
texts. But using any of this methods will make it
difficult to combine it with the HDP-based model.
Instead, we propose a Bayesian HMM for unsu-
pervised word segmentation. The Bayesian HMM
model is defined as follows:
t
i
|t
i?1
= t, p
t
? Mult(p
t
)
c
i
|t
i
= t, e
t
? Mult(e
t
)
p
t
|? ? Dirichlet(?)
e
t
|? ? Dirichlet(?)
where p
t
and e
t
are transition and emission dis-
tributions, ? and ? are the symmetric parameters
of Dirichlet distributions. Now suppose we have
observed tagged text h, then the conditional prob-
ability P
M
(w
i
|w
i?1
= l, h) can be obtained:
P
M
(w
i
|w
i?1
= l, h)
=
|w
i
|
?
j=1
P
t
(t
j
|t
j?1
, h)P
e
(c
j
|t
j
, h) (3)
where < w
i?1
, w
i
> is a word bigram, l is the in-
dex of word w
i?1
, c
j
is the jth character in word
856
wi
and t
j
is the corresponding tag.P
t
(t
j
|t
j?1
, h)
and P
e
(c
j
|t
j
, h) are the posterior probabilities,
they are given as:
P
t
(t
j
|t
j?1
, h) =
n
<t
j?1
,t
j
>
+ ?
n
<t
j?1
,?>
+ T?
(4)
P
e
(c
j
|t
j
, h) =
n
<t
j
,c
j
>
+ ?
n
<t
j
,?>
+ V ?
(5)
where n
<t
j?1
,t
j
>
is the tag bigram count of <
t
j?1
, t
j
> in h, n
<t
j
,c
j
>
denotes the number of oc-
currences of tag t
j
and character c
j
, and ? means
a sum operation. T and V are the size of character
tag set (we follow the commonly used {SBME}
tag set and thus T = 4 in this case) and character
vocabulary.
3.3 HDP Model
Goldwater et al. (2009) proposed a nonparametric
Bayesian model for unsupervised word segmenta-
tion which is based on HDP (Teh et al., 2006). In
this model, the conditional probability of the seg-
mentation W given the character string C is de-
fined as:
P
D
(W |C) =
|W |
?
i=0
P
D
(w
i
|w
i?1
) (6)
where w
i
is the ith word in W . This is actually
a nonparametric bigram language model. This bi-
gram model assumes that each different word has
a different distribution over words following it, but
all these different distributions are linked through
a HDP model:
w
i
|w
i?1
= l ? G
l
G
l
? DP (?
1
, G
0
)
G
0
? DP (?,H)
where DP denotes a Dirichlet process.
Suppose we have observed segmentation re-
sult h, then we can get the posterior probability
P
D
(w
i
|w
i?1
= l, h) by integrating out G
l
:
P
D
(w
i
|w
i?1
= l, h)
=
n
<w
i?1
,w
i
>
+ ?
1
P
D
(w
i
|h)
n
<w
i?1
,?>
+ ?
1
(7)
where n
<w
i?1
,w
i
>
denotes the total number of oc-
currences of the bigram < w
i?1
, w
i
> in the ob-
servation h. And P
D
(w
i
|h) can be got by integrat-
ing out G
0
:
P
D
(w
i
|h) =
t
w
i
+ ?H(w
i
)
t+ ?
(8)
where t
w
i
denotes the number of tables associ-
ated with w
i
in the Chinese Restaurant Franchise
metaphor (Teh et al., 2006), t is the total number
of tables and H(w
i
) is the base measure of G
0
. In
fact, H(w
i
) is the prior distribution over words, so
prior knowledge can be injected in this distribution
to enhance the performance.
In Goldwater et al. (2009)?s work, the base
measureH(w
i
) are defined as a character unigram
model:
H(w
i
) = (1? p
s
)
|w
i
|?1
p
s
?
j
P (c
ij
)
where, p
s
is the probability of generating a word
boundary. P (c
ij
) is the probability of the jth char-
acter c
ij
in word w
i
, this probability can be esti-
mated from the training data using maximum like-
lihood estimation.
3.4 Initializing with nVBE
Among various goodness measure based models,
we choose nVBE (Magistry and Sagot, 2012) to
initialize our Gibbs sampler with its segmentation
results. nVBE achieved a relatively high perfo-
mance over other goodness measure based meth-
ods. And it?s very simple as well as efficient.
Theoretically, the Gibbs sampler may be initial-
ized at random or using any other methods. Initial-
ization does not make a difference since the Gibbs
sampler will eventually converge to the posterior
distribution if it iterates as much as possible. This
is an essential attribute of Gibbs sampling. How-
ever, we believe that initializing the Gibbs sam-
pler with the result of nVBE will benefit us in
two ways. On one hand, in consideration of its
combination of nonparametric Bayesian method
and goodness-based method, it will improve the
overall performance as well as solve more seg-
mentation ambiguities with the help of HDP-based
model. On the other hand, it makes the conver-
gence of Gibbs sampling faster. In practice, ran-
dom initialization often leads to extremely slow
convergence.
3.5 Inference with Gibbs Sampling
In our proposed joint model, Gibbs sam-
pling (Casella and George, 1992) can be easily
used to identify the highest probability segmen-
tation from among all possibilities. Following
Goldwater et al. (2009), we can repeatedly sample
from potential word boundaries. Each boundary
857
variable can only take on two possible values, cor-
responding to a word boundary or not word bound-
ary.
For instance, suppose we have obtained a seg-
mentation result ?|c
i?2
c
i?1
c
i
c
i+1
c
i+2
|?, where ?
and ? are the words sequences to the left and
right and c
i?2
c
i?1
c
i
c
i+1
c
i+2
are characters be-
tween them. Now we are sampling at location i
to decide whether there is a word boundary be-
tween c
i
and c
i+1
. Denote h
1
as the hypothesis
that it forms a word boundary (the correspond-
ing result is ?w
1
w
2
? where w
1
= c
i?2
c
i?1
c
i
and
w
2
= c
i+1
c
i+2
), and h
2
as the opposite hypoth-
esis (then the corresponding result is ?w? where
w = c
i?2
c
i?1
c
i
c
i+1
c
i+2
). The posterior probabil-
ity for these two hypotheses would be:
P (h
1
|h
?
) ? P
D
(h
1
|h
?
)P
M
(h
1
|h
?
) (9)
P (h
2
|h
?
) ? P
D
(h
2
|h
?
)P
M
(h
2
|h
?
) (10)
where P
D
(h|h
?
) and P
M
(h|h
?
) are the pos-
terior probabilities in HDP-based model and in
HMM-based model, and h
?
denotes the current
segmentation results for all observed data except
c
i?2
c
i?1
c
i
c
i+1
c
i+2
. Note that the normalization
term Z(C) can be ignored during inference. The
posterior probabilities for these two hypotheses in
the HDP-based model is given as:
P
D
(h
1
|h
?
) = P
D
(w
1
|w
l
, h
?
)
? P
D
(w
2
|w
1
, h
?
)P
D
(w
r
|w
2
, h
?
) (11)
P
D
(h
2
|h
?
) = P
D
(w|w
l
, h
?
)
? P
D
(w
r
|w, h
?
) (12)
where w
l
(w
r
) is the first word to the left (right) of
w. And the posterior probabilities for the Bayesian
HMM model is given as:
P
M
(h
1
|h
?
)
?
i+2
?
j=i?2
P
t
(t
j
|t
j?1
, h
?
)P
e
(c
j
|t
j
, h
?
) (13)
P
M
(h
2
|h
?
)
?
i+2
?
j=i?2
P
t
(t
j
|t
j?1
, h
?
)P
e
(c
j
|t
j
, h
?
) (14)
where P
t
(t
j
|t
j?1
, h
?
) and P
e
(c
j
|t
j
, h
?
) are given
in Equation 4 and 5. The difference is that un-
der hypothesis h
1
, c
i?2
c
i?1
c
i
c
i+1
c
i+2
are tagged
as ?BMEBE? and under hypothesis h
2
as ?BM-
MME?.
Once the Gibbs sampler is converged, a natu-
ral way to is to treat the result of last iteration as
the final segmentation result, since each set of as-
signments to the boundary variables uniquely de-
termines a segmentation.
4 Experiments
In this section, we test our joint model on PKU
and MSRA datesets provided by the Second Seg-
mentation Bake-off (SIGHAN 2005) (Emerson,
2005). Most previous works reported their results
on these two datasets, this will make it convenient
to directly compare our joint model with theirs.
4.1 Setting
The second SIGHAN Bakeoff provides several
large-scale labeled data for evaluating the per-
formance of Chinese word segmentation systems.
Two of the four datasets are used in our exper-
iments. Both of the dataset contains only sim-
plified Chinese. Table 1 shows the statistics of
the two selected corpus. For development set, we
randomly select a small subset (about 10%) of
the training data. Specifically, 2000 sentences are
selected for PKU corpus and 8000 sentences for
MSRA corpus. The rest training data plus the test
set is then combined for segmentation but only test
data is used for evaluation. The development set is
used to tune parameters of the HDP-based model
and HMM-based model separately. Since our joint
model does not involve any additional parameters,
we reuse the parameters of the HDP-based model
and HMM-based model in the joint model. Specif-
ically, we set ?
1
= 1000.0, ? = 10.0, p
s
= 0.5 for
the HDP-based model and set ? = 1.0, ? = 0.01
for the HMM-based model.
For evaluation, we use standard F-Score on
words for all following experiments. F-Score is
the harmonic mean of the word precision and re-
call. Precision is given as:
P =
#correct words in result
#total words in result
and recall is given as:
R =
#correct words in result
#total words in gold corpus
then F-Score is calculated as:
F =
2?R ? F
R + F
858
Corpus TrainingSize (words) TestSize (words)
PKU 1.1M 104K
MSRA 2.37M 107K
Table 1: Statistics of training and testing data
Huang and Zhao (2007) provided an empirical
method to estimate the consistency between the
four different segmentation standards involved in
the Bakeoff-3. A lowest consistency rate 84.8%
is found among the four standards. Zhao and Kit
(2008) considered this figure as the upper bound
for any unsupervised Chinese word segmentation
systems. We also use it as the topline in our com-
parison.
4.2 Prior Knowledge Used
When it comes to the evaluation and compari-
son for unsupervised word segmentation systems,
an important issue is what kind of pre-processing
steps and prior knowledge are needed. To be fully
unsupervised, any prior knowledge such as punc-
tuation information, encoding scheme and word
length could not be used in principle. Neverthe-
less, information like punctuation can be easily in-
jected to most existing systems and significantly
enhance the performance. The problem we are
faced with is that we don?t know for sure what
kind of prior information are used in other sys-
tems. One may use a small punctuation set to
segment a long sentence into shorter ones, while
another may write simple regular expressions to
identify dates and numbers. Lot of work we com-
pare to don?t even mention this subject.
Fortunately, we notice that Wang et al. (2011)
provided four kinds of preprocessings (they call
settings). In their settings 1 and 2, punctuation
and other encoding information are not used. In
setting 3, punctuation is used to segment charac-
ter sequences into sentences, and both punctuation
and other encoding information are used in setting
4. Then the results reported in Magistry and Sagot
(2012) relied on setting 3 and setting 4. In order
to make the comparison as fair as possible, we use
setting 3 in our experiment, i.e., only a punctua-
tion set for simplified Chinese is used in all our
experiments. We will compare our experiment re-
sults to previous work on the same setting if they
are provided .
4.3 Experiment Results
Table 2 summarizes the F-Scores obtained by dif-
ferent models on PKU and MSRA corpus, as well
as several state-of-the-art systems. Detailed infor-
mation about the presented models are listed as
follows:
? nVBE: the model based on Variation of
Branching Entropy in Magistry and Sagot
(2012). We re-implement their model on set-
ting 3
1
.
? HDP: the HDP-based model proposed by
Goldwater et al. (2009), initialized randomly.
? HDP+HMM: the model combining HDP-
based model and HMM-based model as pro-
posed in Section 3, initialized randomly.
? HDP+nVBE: the HDP-based model, initial-
ized with the results of nVBE model.
? Joint: the ?HDP+HMM? model initialized
with nVBE model.
? ESA: the model proposed in Wang et al.
(2011), as mentioned above, the conducted
experiments on four different settings, we re-
port their results on setting 3.
? NPY(2): the 2-gram language model pre-
sented by Mochihashi et al. (2009).
? NPY(3): the 3-gram language model pre-
sented by Mochihashi et al. (2009).
For all of our Gibbs samplers, we run 5 times to
get the averaged F-Scores. We also give the vari-
ance of the F-Scores in Table 2. For each run, we
find that random initialization takes around 1,000
iterations to converge, while initialing with nVBE
only takes as few as 10 iterations. This makes
1
The results we got with our implementation is slightly
lower than what was reported in Magistry and Sagot (2012).
According to Pei et al. (2013), they had contacted the authors
and confirmed that the higher results was due to a bug in code.
So we report the results with our bug free implementation as
Pei et al. (2013) did. Our reported results are identical to
those of Pei et al. (2013)
859
System
PKU MSRA
R P F R P F
nVBE 78.3 77.5 77.9 79.1 77.3 78.2
HDP 69.0 68.4 68.7(0.012) 70.4 69.4 69.9(0.020)
HDP+HMM 77.5 73.2 75.3(0.005) 79.9 73.0 76.3(0.013)
HDP+nVBE 80.7 77.9 79.3(0.012) 81.8 77.3 79.5(0.005)
Joint 83.1 79.2 81.1(0.002) 84.2 79.3 81.7(0.005)
ESA N/A N/A 77.4 N/A N/A 78.4
NPY(2) N/A N/A N/A N/A N/A 80.2
NPY(3) N/A N/A N/A N/A N/A 80.7
Topline N/A N/A 84.8 N/A N/A 84.8
Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote
the variance the of F-Scores.
our joint model very efficient and possible to work
in practical applications as well. At last, a single
sample (the last one) is used for evaluation.
From Table 2, we can see that the joint
model (Joint) outperforms all the presented sys-
tems in F-Score on all testing corpora. Specifi-
cally, comparing ?HDP+HMM? with ?HDP?, the
former model increases the overall F-Score from
68.7% to 75.3% (+6.6%) in PKU corpora and
from 69.9% to 76.3% (+6.4%) in MSRA corpora,
which proves that the character information in
the HMM-based model can actually enhance the
performance of the HDP-based model. Compar-
ing ?HDP+nVBE? with ?HDP?, the former model
also increases the overall F-Score by 10.6%/9.6%
in PKU/MSRA corpora, which demonstrates that
initializing the HDP-based model with nVBE will
improve the performance by a large margin. Fi-
nally, the joint model ?Joint? take advantage from
both from the character-based HMM model and
the nVBE model, it achieves a F-Score of 81.1%
on PKU and 81.7% on MSRA. This result outper-
forms all its component baselines such as ?HDP?,
?HDP+HMM? and ?HDP+nVBE?.
Our joint model also shows competitive advan-
tages over several state-of-the-art systems. Com-
pared with nVBE,the F-Score increases by 3.2%
on PKU corpora and by 3.5% on MSRA cor-
pora. Compared with ESA, the F-Score increases
by 3.7%/3.3% in PKU/MSRA corpora. Lastly,
compared to the nonparametric Bayesian models
(NPY(n)), our joint model still increases the F-
Score by 1.5% (NPY(2)) and 1.0% (NPY(3)) on
MSRA corpora. Moreover, compared with the
empirical topline figure 84.8%, our joint model
achieves a pretty close F-Score. The differences
are 3.7% on PKU corpora and 3.1% on MSRA
corpora.
An phenomenon we should pay attention to is
the poor performance of the HMM-based model.
With our implementation of the Bayesian HMM,
we achieves a 34.3% F-Score on PKU corpora and
a 34.9% F-Score on MSRA corpora, just slightly
better than random segmentation. The result show
that the hidden Markov Model alone is not suit-
able for character-based Chinese word segmenta-
tion problem. However, it still substantially con-
tributes to the joint model.
We find that the variance of the results are rather
small, this shows the stability of our Gibbs sam-
plers. From the segmentation results generated
by the joint model, we also found that quite a
large amount of errors it made are related to dates,
numbers (both Chinese and English) and English
words. This problem can be easily addressed dur-
ing preprocessing by considering encoding infor-
mation as previous work, and we believe this will
bring us much better performance.
4.4 Disambiguation Ability
Previous unsupervised work usually evaluated
their models using F-score, regardless of goodness
measure based model or nonparametric Bayesian
model. However, segmentation ambiguity is
a very important factor influencing accuracy of
Chinese word segmentation systems (Huang and
Zhao, 2007). We believe that the disambigua-
tion ability of the models should also be consid-
ered when evaluating different types of unsuper-
vised segmentation systems, since different type
of models shows different disambiguation ability.
We will compare the disambiguation ability of dif-
860
ferent systems in this section.
In general, there are mainly two kinds of ambi-
guity in Chinese word segmentation problem:
? Combinational Ambiguity: Given charac-
ter strings ?A? and ?B?, if ?A?, ?B?, ?AB?
are all in the vocabulary, and ?AB? or ?A-B?
(here ?-? denotes a space) occurred in the real
text,then ?AB? can be called a combinational
ambiguous string.
? Overlapping Ambiguity: Given character
strings ?A?, ?J? and ?B?, if ?A?, ?B?, ?AJ?
and ?JB? are all in the vocabulary, and ?A-
JB? or ?AJ-B? occurred in the real text, then
?AJB? can be called an overlapping ambigu-
ous string.
We count the total number of mistakes differ-
ent systems made at ambiguous strings (the vo-
cabulary is obtained from the gold standard an-
swer of testing set). As we have mentioned in
Section 2, goodness measure based methods such
as nVBE do not have any disambiguation ability
in theory. Our observation is identical to this ar-
gument. We find that nVBE always segments am-
biguous strings into the same result. Take a combi-
national string ??k? as an example, ?? (just)?,
?k (have)? and ??k (only)? are all in the vo-
cabulary. In the PKU test set, this string occurs
14 times as ??-k (just have)? and 18 times as
??k (only)?, 32 times in total. nVBE segments
all the 32 strings into ??k (only)? (i.e. 18 of
them are correct), while the joint model segments
it 22 times as ??k (only)? and 10 times as ??-
k (just have)? according to its context, and 24 of
them are correct.
Table 3 and 4 show the statistics of combi-
national ambiguity and overlapping ambiguity re-
spectively. The numbers in parentheses denote the
total number of ambiguous strings. From these
tables, we can see that HDP+nVBE makes less
mistakes than nVBE in most circumstances, ex-
cept that it solves less combinational ambigui-
ties on MSRA corpora. But our proposed joint
model solves the most combinational and over-
lapping ambiguities, on both PKU and MSRA
corpora. Specifically, compared to nVBE, the
joint model correctly solves 171/871 more com-
binational ambiguities on PKU/MSRA corpora,
which is a 0.6%/13.8% relative error reduction.
It also solves 28/45 more overlapping ambiguities
on PKU/MSRA corpora, which is a 11.5%/23.4%
relative error reduction. This indicates that the
joint model has a stronger ability of disambigua-
tion over the compared systems.
System PKU(35371) MSRA(38506)
nVBE 8087 7236
HDP+nVBE 7970 7500
Joint 7916 6305
Table 3: Statistics of combinational ambiguity.
This table shows the total number of mistakes
made by different systems at combinational am-
biguous strings. The numbers in parentheses de-
note the total number of combinational ambiguous
strings.
System PKU(603) MSRA(467)
nVBE 244 192
HDP+nVBE 239 164
Joint 216 157
Table 4: Statistics of overlapping ambiguity. This
table shows the total number of mistakes made
by different systems at overlapping ambiguous
strings. The numbers in parentheses denote the to-
tal number of overlapping ambiguous strings.
4.5 Statistical Significance Test
The main results presented in Table 2 has shown
that our proposed joint model outperforms the
two baselines as well as state-of-the-art systems.
But it is also important to know if the improve-
ment is statistically significant over these sys-
tems. So we conduct statistical significance tests
of F-scores among these various models. Follow-
ing Wang et al. (2010), we use the bootstrapping
method (Zhang et al., 2004).
Here is how it works: suppose we have a testing
set T
0
to test several word segmentation systems,
there are N testing examples (sentences or line of
characters) in T
0
. We create a new testing set T
1
with N examples by sampling with replacement
from T
0
, then repeat these process M ? 1 times.
And we will have a total M +1 testing sets. In our
test procedures, M is set to 2000.
Since we just implement our joint model and
its component models, we can not generate paired
samples for other models (i.e. ESA and NPY(n)).
Instead, we follow Wang et al. (2010)?s method
and first calculate the 95% confidence interval for
861
our proposed model. Then other systems can be
compared with the joint model in this way: if the
F-score of system B doesn?t fall into the 95% con-
fidence interval of system A, they are considered
as statistically significantly different from each
other.
For all significant tests, we measure the 95%
confidence interval for the difference between
two models. First, the test results show that
?HDP+nVBE? and ?HDP+HMM? are both sig-
nificantly better than ?HDP?. Second, the
?Joint? model significantly outperforms all its
component models, including ?HDP?, ?nVBE?,
?HDP+nVBE? and ?HDP+HMM?. Finally, the
comparison also shows that the joint model signif-
icantly outperforms state-of-the-art systems like
ESA and NPY(n).
5 Conclusion
In this paper, we proposed a joint model for un-
supervised Chinese word segmentation. Our joint
model is a combination of the HDP-based model,
which is a word-based model, and HMM-based
model, which is a character-based model. The
way we combined these two component base-
lines makes it natural and simple to inference with
Gibbs sampling. Then the joint model take ad-
vantage of a goodness-based method (nVBE) by
using it to initialize the sampler. Experiment re-
sults conducted on PKU and MSRA datasets pro-
vided by the second SIGHAN Bakeoff show that
the proposed joint model not only outperforms the
baseline systems but also achieves better perfor-
mance (F-Score) over several state-of-the-art sys-
tems. Significance tests showed that the improve-
ment is statistically significant. Analysis also in-
dicates that the joint model has a stronger abil-
ity to solve ambiguities in Chinese word segmen-
tation. In summary, the joint model we pro-
posed combines the strengths of character-based
model, nonparametric Bayesian language model
and goodness-based model.
Acknowledgments
The contact author of this paper, according to
the meaning given to this role by Key Labora-
tory of Computational Linguistics, Ministry of Ed-
ucation, School of Electronics Engineering and
Computer Science, Peking University, is Baobao
Chang. And this work is supported by National
Natural Science Foundation of China under Grant
No. 61273318 and National Key Basic Research
Program of China 2014CB340504.
References
George Casella, Edward I. George. 1992. Explain-
ing the Gibbs sampler. The American Statistician,
46(3): 167-174.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. Proceedings of
the Fourth SIGHANWorkshop on Chinese Language
Processing, 133. MLA.
Haodi Feng, Kang Chen, Xiaotie Deng, et al. 2004.
Accessor variety criteria for Chinese word extraction
Computational Linguistics, 30(1): 75-93.
Sharon Goldwater, Thomas L. Griffiths, Mark Johnson.
2009. A Bayesian framework for word segmenta-
tion: Exploring the effects of context. Cognition
112(1): 21-54.
Geoffrey E. Hinton. 1999. Products of experts. Arti-
ficial Neural Networks. Ninth International Confer-
ence on Vol. 1.
Changning Huang, Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21(3): 8-20.
Zhihui Jin, Kumiko Tanaka-Ishii. 2006. Unsupervised
segmentation of Chinese text by use of branching
entropy. Proceedings of the COLING/ACL on Main
conference poster sessions, page 428-435.
Chunyu Kit, Yorick Wilks. 1999. Unsupervised
learning of word boundary with description length
gain. Proceedings of the CoNLL99 ACL Workshop.
Bergen, Norway: Association for Computational
Linguis-tics, page 1-6.
Pierre Magistry, Benoit Sagot. 2012. Unsuper-
vized word segmentation: the case for mandarin
chinese. Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Short Papers-Volume 2. Association for Computa-
tional Linguistics, page 383-387.
Daichi Mochihashi, Takeshi Yamada, Naonori Ueda.
2009. Bayesian unsupervised word segmentation
with nested Pitman-Yor language modeling. Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1-Volume 1. Association for Com-
putational Linguistics, page 100-108.
Wenzhe Pei, Dongxu Han, Baobao Chang. 2013. A
Refined HDP-Based Model for Unsupervised Chi-
nese Word Segmentation. Chinese Computational
Linguistics and Natural Language Processing Based
on Naturally Annotated Big Data. Springer Berlin
Heidelberg, page 44-51.
862
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, et
al. 2006. Sharing Clusters among Related Groups:
Hierarchical Dirichlet Processes. NIPS.
Fuchun Peng, Fangfang Feng, Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. Proceedings
of COLING, page 562-568.
Huihsin Tseng, Pichuan Chang, Galen Andrew, et al.
2005. A conditional random field word segmenter
for sighan bakeoff 2005. Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, Vol. 171.
Kun Wang, Chengqing Zong, Keh-Yih Su. 2010. A
character-based joint model for Chinese word seg-
mentation. Proceedings of the 23rd International
Conference on Computational Linguistics. Associa-
tion for Computational Linguistics, page 1173-1181.
Hanshi Wang, Jian Zhu, Shiping Tang, et al. 2011. A
new unsupervised approach to word segmentation.
Computational Linguistics, 37(3): 421-454.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1): 29-48.
Hai Zhao, Chunyu Kit. 2008. An Empirical Compar-
ison of Goodness Measures for Unsupervised Chi-
nese Word Segmentation with a Unified Framework.
IJCNLP, page 6-16.
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? LREC.
863
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 293?303,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Max-Margin Tensor Neural Network for Chinese Word Segmentation
Wenzhe Pei Tao Ge Baobao Chang
?
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
Beijing, P.R.China, 100871
{peiwenzhe,getao,chbb}@pku.edu.cn
Abstract
Recently, neural network models for nat-
ural language processing tasks have been
increasingly focused on for their ability
to alleviate the burden of manual feature
engineering. In this paper, we propose a
novel neural network model for Chinese
word segmentation called Max-Margin
Tensor Neural Network (MMTNN). By
exploiting tag embeddings and tensor-
based transformation, MMTNN has the
ability to model complicated interactions
between tags and context characters. Fur-
thermore, a new tensor factorization ap-
proach is proposed to speed up the model
and avoid overfitting. Experiments on the
benchmark dataset show that our model
achieves better performances than previ-
ous neural network models and that our
model can achieve a competitive perfor-
mance with minimal feature engineering.
Despite Chinese word segmentation being
a specific case, MMTNN can be easily
generalized and applied to other sequence
labeling tasks.
1 Introduction
Unlike English and other western languages, Chi-
nese do not delimit words by white-space. There-
fore, word segmentation is a preliminary and im-
portant pre-process for Chinese language process-
ing. Most previous systems address this problem
by treating this task as a sequence labeling prob-
lem where each character is assigned a tag indi-
cating its position in the word. These systems
are effective because researchers can incorporate a
large body of handcrafted features into the models.
However, the ability of these models is restricted
?
Corresponding author
by the design of features and the number of fea-
tures could be so large that the result models are
too large for practical use and prone to overfit on
training corpus.
Recently, neural network models have been in-
creasingly focused on for their ability to mini-
mize the effort in feature engineering. Collobert et
al. (2011) developed the SENNA system that ap-
proaches or surpasses the state-of-the-art systems
on a variety of sequence labeling tasks for English.
Zheng et al (2013) applied the architecture of
Collobert et al (2011) to Chinese word segmenta-
tion and POS tagging and proposed a perceptron-
style algorithm to speed up the training process
with negligible loss in performance.
Workable as previous neural network models
seem, a limitation of them to be pointed out is
that the tag-tag interaction, tag-character inter-
action and character-character interaction are not
well modeled. In conventional feature-based lin-
ear (log-linear) models, these interactions are ex-
plicitly modeled as features. Take phrase ???
?(play basketball)? as an example, assuming we
are labeling character C
0
=???, possible features
could be:
f
1
=
{
1 C
?1
=??? and C
1
=??? and y
0
=?B?
0 else
f
2
=
{
1 C
0
=??? and y
0
=?B? and y
?1
=?S?
0 else
To capture more interactions, researchers have de-
signed a large number of features based on linguis-
tic intuition and statistical information. In previ-
ous neural network models, however, hardly can
such interactional effects be fully captured rely-
ing only on the simple transition score and the sin-
gle non-linear transformation (See section 2). In
order to address this problem, we propose a new
model called Max-Margin Tensor Neural Network
(MMTNN) that explicitly models the interactions
293
between tags and context characters by exploiting
tag embeddings and tensor-based transformation.
Moreover, we propose a tensor factorization ap-
proach that effectively improves the model effi-
ciency and prevents from overfitting. We evalu-
ate the performance of Chinese word segmentation
on the PKU and MSRA benchmark datasets in the
second International Chinese Word Segmentation
Bakeoff (Emerson, 2005) which are commonly
used for evaluation of Chinese word segmentation.
Experiment results show that our model outper-
forms other neural network models.
Although we focus on the question that how
far we can go without using feature engineering
in this paper, the study of deep learning for NLP
tasks is still a new area in which it is currently
challenging to surpass the state-of-the-art with-
out additional features. Following Mansur et al
(2013), we wonder how well our model can per-
form with minimal feature engineering. There-
fore, we integrate additional simple character bi-
gram features into our model and the result shows
that our model can achieve a competitive perfor-
mance that other systems hardly achieve unless
they use more complex task-specific features.
The main contributions of our work are as fol-
lows:
? We propose a Max-Margin Tensor Neu-
ral Network for Chinese word segmentation
without feature engineering. The test re-
sults on the benchmark dataset show that our
model outperforms previous neural network
models.
? We propose a new tensor factorization ap-
proach that models each tensor slice as the
product of two low-rank matrices. Not only
does this approach improve the efficiency of
our model but also it avoids the risk of over-
fitting.
? Compared with previous works that use a
large number of handcrafted features, our
model can achieve a competitive perfor-
mance with minimal feature engineering.
? Despite Chinese word segmentation being a
specific case, our approach can be easily gen-
eralized to other sequence labeling tasks.
The remaining part of this paper is organized as
follows. Section 2 describes the details of con-
ventional neural network architecture. Section 3
Figure 1: Conventional Neural Network
describes the details of our model. Experiment re-
sults are reported in Section 4. Section 5 reviews
the related work. The conclusions are given in
Section 6.
2 Conventional Neural Network
2.1 Lookup Table
The idea of distributed representation for symbolic
data is one of the most important reasons why the
neural network works. It was proposed by Hin-
ton (1986) and has been a research hot spot for
more than twenty years (Bengio et al, 2003; Col-
lobert et al, 2011; Schwenk et al, 2012; Mikolov
et al, 2013a). Formally, in the Chinese word seg-
mentation task, we have a character dictionary D
of size |D|. Unless otherwise specified, the char-
acter dictionary is extracted from the training set
and unknown characters are mapped to a special
symbol that is not used elsewhere. Each character
c ? D is represented as a real-valued vector (char-
acter embedding) Embed(c) ? R
d
where d is the
dimensionality of the vector space. The charac-
ter embeddings are then stacked into a embedding
matrix M ? R
d?|D|
. For a character c ? D that
has an associated index k, the corresponding char-
acter embedding Embed(c) ? R
d
is retrieved by
the Lookup Table layer as shown in Figure 1:
Embed(c) = Me
k
(1)
Here e
k
? R
|D|
is a binary vector which is zero in
all positions except at k-th index. The Lookup Ta-
ble layer can be seen as a simple projection layer
where the character embedding for each context
294
character is achieved by table lookup operation ac-
cording to their indices. The embedding matrix
M is initialized with small random numbers and
trained by back-propagation. We will analyze in
more detail about the effect of character embed-
dings in Section 4.
2.2 Tag Scoring
The most common tagging approach is the win-
dow approach. The window approach assumes
that the tag of a character largely depends on its
neighboring characters. Given an input sentence
c
[1:n]
, a window of size w slides over the sentence
from character c
1
to c
n
. We set w = 5 in all
experiments. As shown in Figure 1, at position
c
i
, 1 ? i ? n, the context characters are fed into
the Lookup Table layer. The characters exceeding
the sentence boundaries are mapped to one of two
special symbols, namely ?start? and ?end? sym-
bols. The character embeddings extracted by the
Lookup Table layer are then concatenated into a
single vector a ? R
H
1
, where H
1
= w ? d is
the size of Layer 1. Then a is fed into the next
layer which performs linear transformation fol-
lowed by an element-wise activation function g
such as tanh, which is used in our experiments:
h = g(W
1
a+ b
1
) (2)
where W
1
? R
H
2
?H
1
, b
1
? R
H
2
?1
, h ? R
H
2
. H
2
is a hyper-parameter which is the number of hid-
den units in Layer 2. Given a set of tags T of size
|T |, a similar linear transformation is performed
except that no non-linear function is followed:
f(t|c
[i?2:i+2]
) = W
2
h+ b
2
(3)
where W
2
? R
|T |?H
2
, b
2
? R
|T |?1
.
f(t|c
[i?2:i+2]
) ? R
|T |
is the score vector for each
possible tag. In Chinese word segmentation, the
most prevalent tag set T is BMES tag set, which
uses 4 tags to carry word boundary information. It
uses B, M, E and S to denote the Beginning, the
Middle, the End of a word and a Single character
forming a word respectively. We use this tag set in
our method.
2.3 Model Training and Inference
Despite sharing commonalities mentioned above,
previous work models the segmentation task dif-
ferently and therefore uses different training and
inference procedure. Mansur et al (2013) mod-
eled Chinese word segmentation as a series of
classification task at each position of the sentence
in which the tag score is transformed into proba-
bility using softmax function:
p(t
i
|c
[i?2:i+2]
) =
exp(f(t
i
|c
[i?2:i+2]
))
?
t
?
exp(f(t
?
|c
[i?2:i+2]
))
The model is then trained in MLE-style which
maximizes the log-likelihood of the tagged data.
Obviously, it is a local model which cannot cap-
ture the dependency between tags and does not
support to infer the tag sequence globally.
To model the tag dependency, previous neural
network models (Collobert et al, 2011; Zheng
et al, 2013) introduce a transition score A
ij
for
jumping from tag i ? T to tag j ? T . For a
input sentence c
[1:n]
with a tag sequence t
[1:n]
, a
sentence-level score is then given by the sum of
transition and network scores:
s(c
[1:n]
, t
[1:n]
, ?) =
n
?
i=1
(A
t
i?1
t
i
+f
?
(t
i
|c
[i?2:i+2]
))
(4)
where f
?
(t
i
|c
[i?2:i+2]
) indicates the score output
for tag t
i
at the i-th character by the network with
parameters ? = (M,A,W
1
, b
1
,W
2
, b
2
). Given
the sentence-level score, Zheng et al (2013)
proposed a perceptron-style training algorithm in-
spired by the work of Collins (2002). Compared
with Mansur et al (2013), their model is a global
one where the training and inference is performed
at sentence-level.
Workable as these methods seem, one of the
limitations of them is that the tag-tag interaction
and the neural network are modeled seperately.
The simple tag-tag transition neglects the impact
of context characters and thus limits the ability
to capture flexible interactions between tags and
context characters. Moreover, the simple non-
linear transformation in equation (2) is also poor
to model the complex interactional effects in Chi-
nese word segmentation.
3 Max-Margin Tensor Neural Network
3.1 Tag Embedding
To better model the tag-tag interaction given the
context characters, distributed representation for
tags instead of traditional discrete symbolic repre-
sentation is used in our model. Similar to character
embeddings, given a fixed-sized tag set T , the tag
embeddings for tags are stored in a tag embedding
matrix L ? R
d?|T |
, where d is the dimensionality
295
Figure 2: Max-Margin Tensor Neural Network
of the vector space (same with character embed-
dings). Then the tag embedding Embed(t) ? R
d
for tag t ? T with index k can be retrieved by the
lookup operation:
Embed(t) = Le
k
(5)
where e
k
? R
|T |?1
is a binary vector which is
zero in all positions except at k-th index. The tag
embeddings start from a random initialization and
can be automatically trained by back-propagation.
Figure 2 shows the new Lookup Table layer with
tag embeddings. Assuming we are at the i-th char-
acter of a sentence, besides the character embed-
dings, the tag embeddings of the previous tags are
also considered
1
. For a fast tag inference, only
the previous tag t
i?1
is used in our model even
though a longer history of tags can be considered.
The concatenation operation in Layer 1 then con-
catenates the character embeddings and tag em-
bedding together into a long vector a. In this way,
the tag representation can be directly incorporated
in the neural network so that the tag-tag interac-
tion and tag-character interaction can be explicitly
modeled in deeper layers (See Section 3.2). More-
over, the transition score in equation (4) is not
necessary in our model, because, by incorporating
tag embedding into the neural network, the effect
of tag-tag interaction and tag-character interaction
are covered uniformly in one same model. Now
1
We also tried the architecture in which the tag embedding
of current tag is also considered, but this did not bring much
improvement and runs slower
Figure 3: The tensor-based transformation in
Layer 2. a is the input from Layer 1. V is the
tensor parameter. Each dashed box represents one
of the H
2
-many tensor slices, which defines the
bilinear form on vector a.
equation (4) can be rewritten as follows:
s(c
[1:n]
, t
[1:n]
, ?) =
n
?
i=1
f
?
(t
i
|c
[i?2:i+2]
, t
i?1
)
(6)
where f
?
(t
i
|c
[i?2:i+2]
, t
i?1
) is the score output for
tag t
i
at the i-th character by the network with pa-
rameters ?. Like Collobert et al (2011) and Zheng
et al (2013), our model is also trained at sentence-
level and carries out inference globally.
3.2 Tensor Neural Network
A tensor is a geometric object that describes rela-
tions between vectors, scalars, and other tensors.
It can be represented as a multi-dimensional array
of numerical values. An advantage of the tensor
is that it can explicitly model multiple interactions
in data. As a result, tensor-based model have been
widely used in a variety of tasks (Salakhutdinov et
al., 2007; Krizhevsky et al, 2010; Socher et al,
2013b).
In Chinese word segmentation, a proper model-
ing of the tag-tag interaction, tag-character inter-
action and character-character interaction is very
important. In linear models, these kinds of inter-
actions are usually modeled as features. In con-
ventional neural network models, however, the in-
put embeddings only implicitly interact through
the non-linear function which can hardly model
the complexity of the interactions. Given the ad-
vantage of tensors, we apply a tensor-based trans-
formation to the input vector. Formally, we use a
3-way tensor V
[1:H
2
]
? R
H
2
?H
1
?H
1
to directly
model the interactions, where H
2
is the size of
296
Layer 2 and H
1
= (w + 1) ? d is the size of con-
catenated vector a in Layer 1 as shown in Figure
2. Figure 3 gives an example of the tensor-based
transformation
2
. The output of a tensor product is
a vector z ? R
H
2
where each dimension z
i
is the
result of the bilinear form defined by each tensor
slice V
[i]
? R
H
1
?H
1
:
z = a
T
V
[1:H
2
]
a; z
i
= a
T
V
[i]
a =
?
j,k
V
[i]
jk
a
j
a
k
(7)
Since vector a is the concatenation of character
embeddings and the tag embedding, equation (7)
can be written in the following form:
z
i
=
?
p,q
?
j,k
V
[i]
(p,q,j,k)
E
[p]
j
E
[q]
k
where E
[p]
j
is the j-th element of the p-th embed-
ding in Lookup Table layer and V
[i]
(p,q,j,k)
is the cor-
responding coefficient for E
[p]
j
and E
[q]
k
in V
[i]
.
As we can see, in each tensor slice i, the em-
beddings are explicitly related in a bilinear form
which captures the interactions between charac-
ters and tags. The multiplicative operations be-
tween tag embeddings and character embeddings
can somehow be seen as ?feature combination?,
which are hand-designed in feature-based models.
Our model learns the information automatically
and encodes them in tensor parameters and em-
beddings. Intuitively, we can interpret each slice
of the tensor as capturing a specific type of tag-
character interaction and character-character inter-
action.
Combining the tensor product with linear trans-
formation, the tensor-based transformation in
Layer 2 is defined as:
h = g(a
T
V
[1:H
2
]
a+W
1
a+ b
1
) (8)
where W
1
? R
H
2
?H
1
, b
1
? R
H
2
?1
, h ? R
H
2
.
In fact, equation (2) used in previous work is a
special case of equation (8) when V is set to 0.
3.3 Tensor Factorization
Despite tensor-based transformation being effec-
tive for capturing the interactions, introducing
tensor-based transformation into neural network
models to solve sequence labeling task is time pro-
hibitive since the tensor product operation drasti-
cally slows down the model. Without consider-
ing matrix optimization algorithms, the complex-
ity of the non-linear transformation in equation (2)
2
The bias term is omitted in Figure 3 for simplicity
Figure 4: Tensor product with tensor factorization
is O(H
1
H
2
) while the tensor operation complex-
ity in equation (8) is O(H
2
1
H
2
). The tensor-based
transformation is H
1
times slower. Moreover, the
additional tensor could bring millions of param-
eters to the model which makes the model suf-
fer from the risk of overfitting. To remedy this,
we propose a tensor factorization approach that
factorizes each tensor slice as the product of two
low-rank matrices. Formally, each tensor slice
V
[i]
? R
H
1
?H
1
is factorized into two low rank
matrix P
[i]
? R
H
1
?r
and Q
[i]
? R
r?H
1
:
V
[i]
= P
[i]
Q
[i]
, 1 ? i ? H
2
(9)
where r  H
1
is the number of factors. Substi-
tuting equation (9) into equation (8), we get the
factorized tensor function:
h = g(a
T
P
[1:H
2
]
Q
[1:H
2
]
a+W
1
a+ b
1
) (10)
Figure 4 illustrates the operation in each slice of
the factorized tensor. First, vector a is projected
into two r-dimension vectors f
1
and f
2
. Then the
output z
i
for each tensor slice i is the dot-product
of f
1
and f
2
. The complexity of the tensor op-
eration is now O(rH
1
H
2
). As long as r is small
enough, the factorized tensor operation would be
much faster than the un-factorized one and the
number of free parameters would also be much
smaller, which prevent the model from overfitting.
3.4 Max-Margin Training
We use the Max-Margin criterion to train our
model. Intuitively, the Max-Margin criterion pro-
vides an alternative to probabilistic, likelihood-
based estimation methods by concentrating di-
rectly on the robustness of the decision boundary
of a model (Taskar et al, 2005). We use Y (x
i
)
to denote the set of all possible tag sequences for
297
a given sentence x
i
and the correct tag sequence
for x
i
is y
i
. The parameters of our model are
? = {W
1
, b
1
,W
2
, b
2
,M,L, P
[1:H
2
]
, Q
[1:H
2
]
}. We
first define a structured margin loss 4(y
i
, y?) for
predicting a tag sequence y? for a given correct tag
sequence y
i
:
4(y
i
, y?) =
n
?
j
?1{y
i,j
6= y?
j
} (11)
where n is the length of sentence x
i
and ? is a dis-
count parameter. The loss is proportional to the
number of characters with an incorrect tag in the
proposed tag sequence, which increases the more
incorrect the proposed tag sequence is. For a given
training instance (x
i
, y
i
), we search for the tag se-
quence with the highest score:
y
?
= argmax
y??Y (x)
s(x
i
, y?, ?) (12)
where the tag sequence is found and scored by the
Tensor Neural Network via the function s in equa-
tion (6). The object of Max-Margin training is that
the highest scoring tag sequence is the correct one:
y
?
= y
i
and its score will be larger up to a margin
to other possible tag sequences y? ? Y (x
i
):
s(x, y
i
, ?) ? s(x, y?, ?) +4(y
i
, y?)
This leads to the regularized objective function for
m training examples:
J(?) =
1
m
m
?
i=1
l
i
(?) +
?
2
||?||
2
l
i
(?) = max
y??Y (x
i
)
(s(x
i
, y?, ?) +4(y
i
, y?))
?s(x
i
, y
i
, ?)) (13)
By minimizing this object, the score of the correct
tag sequence y
i
is increased and score of the high-
est scoring incorrect tag sequence y? is decreased.
The objective function is not differentiable due
to the hinge loss. We use a generalization of gra-
dient descent called subgradient method (Ratliff et
al., 2007) which computes a gradient-like direc-
tion. The subgradient of equation (13) is:
?J
??
=
1
m
?
i
(
?s(x
i
, y?
max
, ?)
??
?
?s(x
i
, y
i
, ?)
??
)+??
where y?
max
is the tag sequence with the highest
score in equation (13). Following Socher et al
(2013a), we use the diagonal variant of AdaGrad
PKU MSRA
Identical words 5.5? 10
4
8.8? 10
4
Total words 1.1? 10
6
2.4? 10
6
Identical characters 5? 10
3
5? 10
3
Total characters 1.8? 10
6
4.1? 10
6
Table 1: Details of the PKU and MSRA datasets
Window size w = 5
Character(tag) embedding size d = 25
Hidden unit number H
2
= 50
Number of factors r = 10
Initial learning rate ? = 0.2
Margin loss discount ? = 0.2
Regularization ? = 10
?4
Table 2: Hyperparameters of our model
(Duchi et al, 2011) with minibatchs to minimize
the objective. The parameter update for the i-th
parameter ?
t,i
at time step t is as follows:
?
t,i
= ?
t?1,i
?
?
?
?
t
?=1
g
2
?,i
g
t,i
(14)
where ? is the initial learning rate and g
?
? R
|?
i
|
is the subgradient at time step ? for parameter ?
i
.
4 Experiment
4.1 Data and Model Selection
We use the PKU and MSRA data provided by the
second International Chinese Word Segmentation
Bakeoff (Emerson, 2005) to test our model. They
are commonly used by previous state-of-the-art
models and neural network models. Details of the
data are listed in Table 1. For evaluation, we use
the standard bake-off scoring program to calculate
precision, recall, F1-score and out-of-vocabulary
(OOV) word recall.
For model selection, we use the first 90% sen-
tences in the training data for training and the rest
10% sentences as development data. The mini-
batch size is set to 20. Generally, the number of
hidden units has a limited impact on the perfor-
mance as long as it is large enough. We found
that 50 is a good trade-off between speed and
model performance. The dimensionality of char-
acter (tag) embedding is set to 25 which achieved
the best performance and faster than 50- or 100-
dimensional ones. We also validated on the num-
ber of factors for tensor factorization. The per-
formance is not boosted and the training time in-
298
P R F OOV
CRF 87.8 85.7 86.7 57.1
NN 92.4 92.2 92.3 60.0
NN+Tag Embed 93.0 92.7 92.9 61.0
MMTNN 93.7 93.4 93.5 64.2
Table 3: Test results with different configurations.
NN stands for the conventional neural network.
NN+Tag Embed stands for the neural network
with tag embeddings.
creases drastically when the number of factors is
larger than 10. We hypothesize that larger factor
size results in too many parameters to train and
hence perform worse. The final hyperparameters
of our model are set as in Table 2.
4.2 Experiment Results
We first perform a close test
3
on the PKU dataset
to show the effect of different model configura-
tions. We also compare our model with the CRF
model (Lafferty et al, 2001), which is a widely
used log-linear model for Chinese word segmen-
tation. The input feature to the CRF model is sim-
ply the context characters (unigram feature) with-
out any additional feature engineering. We use
an open source toolkit CRF++
4
to train the CRF
model. All the neural networks are trained us-
ing the Max-Margin approach described in Sec-
tion 3.4. Table 3 summarizes the test results.
As we can see, by using Tag embedding, the F-
score is improved by +0.6% and OOV recall is
improved by +1.0%, which shows that tag embed-
dings succeed in modeling the tag-tag interaction
and tag-character interaction. Model performance
is further boosted after using tensor-based trans-
formation. The F-score is improved by +0.6%
while OOV recall is improved by +3.2%, which
denotes that tensor-based transformation captures
more interactional information than simple non-
linear transformation.
Another important result in Table 3 is that our
neural network models perform much better than
CRF-based model when only unigram features are
used. Compared with CRF, there are two differ-
ences in neural network models. First, the discrete
feature vector is replaced with dense character em-
beddings. Second, the non-linear transformation
3
No other material or knowledge except the training data
is allowed
4
http://crfpp.googlecode.com/svn/
trunk/doc/index.html?source=navbar
?(one) ?(Li) ?(period)
?(two) ?(Zhao) ?(comma)
?(three) ?(Jiang) ?(colon)
?(four) ?(Kong) ?(question mark)
?(five) ?(Feng) ?(quotation mark)
?(six) ?(Wu) ?(Chinese comma)
Table 4: Examples of character embeddings
is used to discover higher level representation. In
fact, CRF can be regarded as a special neural net-
work without non-linear function (Wang and Man-
ning, 2013). Wang and Manning (2013) conduct
an empirical study on the effect of non-linearity
and the results suggest that non-linear models are
highly effective only when distributed representa-
tion is used. To explain why distributed represen-
tation captures more information than discrete fea-
tures, we show in Table 4 the effect of character
embeddings which are obtained from the lookup
table of MMTNN after training. The first row lists
three characters we are interested in. In each col-
umn, we list the top 5 characters that are near-
est (measured by Euclidean distance) to the cor-
responding character in the first row according to
their embeddings. As we can see, characters in
the first column are all Chinese number characters
and characters in the second column and the third
column are all Chinese family names and Chinese
punctuations respectively. Therefore, compared
with discrete feature representations, distributed
representation can capture the syntactic and se-
mantic similarity between characters. As a re-
sult, the model can still perform well even if some
words do not appear in the training cases.
We further compare our model with previous
neural network models on both PKU and MSRA
datasets. Since Zheng et al (2013) did not
report the results on the these datasets, we re-
implemented their model and tested it on the test
data. The results are listed in the first three rows
of Table 5, which shows that our model achieved
higher F-score than the previous neural network
models.
4.3 Unsupervised Pre-training
Previous work found that the performance can
be improved by pre-training the character em-
beddings on large unlabeled data and using the
obtained embeddings to initialize the charac-
ter lookup table instead of random initialization
299
Models PKU MSRA
P R F OOV P R F OOV
(Mansur et al, 2013) 87.1 87.9 87.5 48.9 92.3 92.2 92.2 53.7
(Zheng et al, 2013) 92.8 92.0 92.4 63.3 92.9 93.6 93.3 55.7
MMTNN 93.7 93.4 93.5 64.2 94.6 94.2 94.4 61.4
(Mansur et al, 2013) + Pre-training 91.2 92.7 92.0 68.8 93.1 93.1 93.1 59.7
(Zheng et al, 2013) + Pre-training 93.5 92.2 92.8 69.0 94.2 93.7 93.9 64.1
MMTNN + Pre-training 94.4 93.6 94.0 69.0 95.2 94.6 94.9 64.8
Table 5: Comparison with previous neural network models
(Mansur et al, 2013; Zheng et al, 2013). Mikolov
et al (2013b) show that pre-trained embeddings
can capture interesting semantic and syntactic in-
formation such as king?man+woman ? queen
on English data. There are several ways to learn
the embeddings on unlabeled data. Mansur et al
(2013) used the model proposed by Bengio et al
(2003) which learns the embeddings based on neu-
ral language model. Zheng et al (2013) followed
the model proposed by Collobert et al (2008).
They constructed a neural network that outputs
high scores for windows that occur in the cor-
pus and low scores for windows where one char-
acter is replaced by a random one. Mikolov et
al. (2013a) proposed a faster skip-gram model
word2vec
5
which tries to maximize classification
of a word based on another word in the same sen-
tence. In this paper, we use word2vec because pre-
liminary experiments did not show differences be-
tween performances of these models but word2vec
is much faster to train. We pre-train the embed-
dings on the Chinese Giga-word corpus (Graff and
Chen, 2005). As shown in Table 5 (last three
rows), both the F-score and OOV recall of our
model boost by using pre-training. Our model still
outperforms other models after pre-training.
4.4 Minimal Feature Engineering
Although we focus on the question that how far we
can go without using feature engineering in this
paper, the study of deep learning for NLP tasks
is still a new area in which it is currently chal-
lenging to surpass the state-of-the-art without ad-
ditional features. To incorporate features into the
neural network, Mansur et al (2013) proposed
the feature-based neural network where each con-
text feature is represented as feature embeddings.
The idea of feature embeddings is similar to that
of character embeddings described in section 2.1.
5
https://code.google.com/p/word2vec/
Model PKU MSRA
Best05(Chen et al, 2005) 95.0 96.0
Best05(Tseng et al, 2005) 95.0 96.4
(Zhang et al, 2006) 95.1 97.1
(Zhang and Clark, 2007) 94.5 97.2
(Sun et al, 2009) 95.2 97.3
(Sun et al, 2012) 95.4 97.4
(Zhang et al, 2013) 96.1 97.4
MMTNN 94.0 94.9
MMTNN + bigram 95.2 97.2
Table 6: Comparison with state-of-the-art systems
Formally, we assume the extracted features form a
feature dictionary D
f
. Then each feature f ? D
f
is represented by a d-dimensional vector which is
called feature embedding. Following their idea,
we try to find out how well our model can perform
with minimal feature engineering.
A very common feature in Chinese word seg-
mentation is the character bigram feature. For-
mally, at the i-th character of a sentence c
[1:n]
, the
bigram features are c
k
c
k+1
(i ? 3 < k < i + 2).
In our model, the bigram features are extracted in
the window context and then the corresponding
bigram embeddings are concatenated with char-
acter embeddings in Layer 1 and fed into Layer
2. In Mansur et al (2013), the bigram embed-
dings are pre-trained on unlabeled data with char-
acter embeddings, which significantly improves
the model performance. Given the long time for
pre-training bigram embeddings, we only pre-train
the character embeddings and the bigram embed-
dings are initialized as the average of character
embeddings of c
k
and c
k+1
. Further improve-
ment could be obtained if the bigram embeddings
are also pre-trained. Table 6 lists the segmenta-
tion performances of our model as well as pre-
vious state-of-the-art systems. When bigram fea-
tures are added, the F-score of our model improves
300
from 94.0% to 95.2% on PKU dataset and from
94.9% to 97.2% on MSRA dataset. It is a com-
petitive result given that our model only use sim-
ple bigram features while other models use more
complex features. For example, Sun et al (2012)
uses additional word-based features. Zhang et al
(2013) uses eight types of features such as Mu-
tual Information and Accessor Variety and they
extract dynamic statistical features from both an
in-domain corpus and an out-of-domain corpus us-
ing co-training. Since feature engineering is not
the main focus of this paper, we did not experi-
ment with more features.
5 Related Work
Chinese word segmentation has been studied with
considerable efforts in the NLP community. The
most popular approach treats word segmentation
as a sequence labeling problem which was first
proposed in Xue (2003). Most previous systems
address this task by using linear statistical mod-
els with carefully designed features such as bi-
gram features, punctuation information (Li and
Sun, 2009) and statistical information (Sun and
Xu, 2011). Recently, researchers have tended to
explore new approaches for word segmentation
which circumvent the feature engineering by au-
tomatically learning features with neural network
models (Mansur et al, 2013; Zheng et al, 2013).
Our study is consistent with this line of research,
however, our model explicitly models the interac-
tions between tags and context characters and ac-
cordingly captures more semantic information.
Tensor-based transformation was also used in
other neural network models for its ability to cap-
ture multiple interactions in data. For example,
Socher et al (2013b) exploited tensor-based func-
tion in the task of Sentiment Analysis to cap-
ture more semantic information from constituents.
However, given the small size of their tensor ma-
trix, they do not have the problem of high time
cost and overfitting problem as we faced in mod-
eling a sequence labeling task like Chinese word
segmentation. That?s why we propose to decrease
computational cost and avoid overfitting with ten-
sor factorization.
Various tensor factorization (decomposition)
methods have been proposed recently for tensor-
based dimension reduction (Cohen et al, 2013;
Van de Cruys et al, 2013; Chang et al, 2013).
For example, Chang et al (2013) proposed the
Multi-Relational Latent Semantic Analysis. Sim-
ilar to LSA, a low rank approximation of the ten-
sor is derived using a tensor decomposition ap-
proch. Similar ideas were also used for collab-
orative filtering (Salakhutdinov et al, 2007) and
object recognition (Ranzato et al, 2010). Our ten-
sor factorization is related to these work but uses
a different tensor factorization approach. By in-
troducing tensor factorization into the neural net-
work model for sequence labeling tasks, the model
training and inference are speeded up and overfit-
ting is prevented.
6 Conclusion
In this paper, we propose a new model called Max-
Margin Tensor Neural Network that explicitly
models the interactions between tags and context
characters. Moreover, we propose a tensor fac-
torization approach that effectively improves the
model efficiency and avoids the risk of overfitting.
Experiments on the benchmark datasets show that
our model achieve better results than previous neu-
ral network models and that our model can achieve
a competitive result with minimal feature engi-
neering. In the future, we plan to further extend
our model and apply it to other structure predic-
tion problems.
Acknowledgments
This work is supported by National Natural
Science Foundation of China under Grant No.
61273318 and National Key Basic Research Pro-
gram of China 2014CB340504.
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602?1612, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon
Sun. 2005. Unigram language model for chi-
nese word segmentation. In Proceedings of the 4th
SIGHAN Workshop on Chinese Language Process-
ing, pages 138?141. Association for Computational
Linguistics Jeju Island, Korea.
301
Shay B Cohen, Giorgio Satta, and Michael Collins.
2013. Approximate pcfg parsing using tensor de-
composition. In Proceedings of NAACL-HLT, pages
487?496.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1?8.
Association for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 999999:2121?2159.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, volume 133.
David Graff and Ke Chen. 2005. Chinese gigaword.
LDC Catalog No.: LDC2003T09, ISBN, 1:58563?
230.
Geoffrey E Hinton. 1986. Learning distributed repre-
sentations of concepts. In Proceedings of the eighth
annual conference of the cognitive science society,
pages 1?12. Amherst, MA.
Alex Krizhevsky, Geoffrey E Hinton, et al 2010. Fac-
tored 3-way restricted boltzmann machines for mod-
eling natural images. In International Conference
on Artificial Intelligence and Statistics, pages 621?
628.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Computational Linguistics, 35(4):505?512.
Mairgup Mansur, Wenzhe Pei, and Baobao Chang.
2013. Feature-based neural language model and
chinese word segmentation. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746?751.
Marc?Aurelio Ranzato, Alex Krizhevsky, and Geof-
frey E Hinton. 2010. Factored 3-way restricted
boltzmann machines for modeling natural images.
Nathan D Ratliff, J Andrew Bagnell, and Martin A
Zinkevich. 2007. (online) subgradient methods for
structured prediction.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey
Hinton. 2007. Restricted boltzmann machines for
collaborative filtering. In Proceedings of the 24th in-
ternational conference on Machine learning, pages
791?798. ACM.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
HLT, pages 11?19. Association for Computational
Linguistics.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with composi-
tional vector grammars. In Annual Meeting of the
Association for Computational Linguistics (ACL).
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. EMNLP.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 970?979. As-
sociation for Computational Linguistics.
Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun?ichi Tsujii. 2009. A dis-
criminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 56?64. Association for Computational Lin-
guistics.
Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast on-
line training with frequency-adaptive learning rates
for chinese word segmentation and new word detec-
tion. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 253?262, Jeju Island,
302
Korea, July. Association for Computational Linguis-
tics.
Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proceed-
ings of the 22nd international conference on Ma-
chine learning, pages 896?903. ACM.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, volume
171.
Tim Van de Cruys, Thierry Poibeau, and Anna Ko-
rhonen. 2013. A tensor-based factorization model
of semantic compositionality. In Proceedings of
NAACL-HLT, pages 1142?1151.
Mengqiu Wang and Christopher D Manning. 2013.
Effect of non-linear deep architecture in sequence
labeling. In Proceedings of the Sixth International
Joint Conference on Natural Language Processing.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29?48.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In ANNUAL MEETING-ASSOCIATION FOR COM-
PUTATIONAL LINGUISTICS, volume 45, page 840.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro
Sumita. 2006. Subword-based tagging by condi-
tional random fields for chinese word segmentation.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 193?196. Association for Com-
putational Linguistics.
Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for Chinese word seg-
mentation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 311?321, Seattle, Washington, USA,
October. Association for Computational Linguistics.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for Chinese word segmenta-
tion and POS tagging. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 647?657, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
303
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 585?589,
Dublin, Ireland, August 23-24, 2014.
SSMT: A Machine Translation Evaluation View to Paragraph-to-Sentence
Semantic Similarity
Pingping Huang
Department of Linguistic Engineering
School of Software and Microelectronics
Peking University, China
girlhpp@163.com
Baobao Chang
Key Laboratory of Computational
Linguistics, Ministry of Education
Institute of Computational Linguistics
Peking University, China
chbb@pku.edu.cn
Abstract
This paper presents the system SSMT
measuring the semantic similarity between
a paragraph and a sentence submitted to
the SemEval 2014 task3: Cross-level Se-
mantic Similarity. The special difficulty
of this task is the length disparity between
the two semantic comparison texts. We
adapt several machine translation evalua-
tion metrics for features to cope with this
difficulty, then train a regression model for
the semantic similarity prediction. This
system is straightforward in intuition and
easy in implementation. Our best run gets
0.808 in Pearson correlation. METEOR-
derived features are the most effective
ones in our experiment.
1 Introduction
Cross level semantic similarity measures the simi-
larity between different levels of text unit, for ex-
ample, between a document and a paragraph, or
between a phrase and a word.
Paragraph and sentence are the natural language
units to convey opinions or state events in daily
life. We can see posts on forums, questions and
answers in Q&A communities and customer re-
views on E-commerce websites, are mainly organ-
ised in these two units. Better similarity measure-
ment across them will be helpful in clustering sim-
ilar answers or reviews.
The paragraph-to-sentence semantic similarity
subtask in SemEval2014 task3 (Jurgens et al.,
2014) is the first semantic similarity competition
across these two language levels. The special
difficulty of this task is the length disparity be-
tween the compared pair: a paragraph contains
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
3.67 times the words of a sentence on average in
the training set.
Semantic similarity on different levels, for ex-
ample, on word level (Mikolov et al., 2013), sen-
tences level (B?ar et al., 2012), document level
(Turney and Pantel, 2010), have been well studied,
yet methods on one level can hardly be applied to
a different level, let alone be applied for the cross-
level tasks. The work of Pilehvar et al.(2013) was
an exception. They proposed a unified method for
semantic comparison at multi-levels all the way
from comparing word senses to comparing text
documents
Our work is inspired by automatic machine
translation(MT) evaluation, in which different
metrics are designed to compare the adequacy and
fluency of a MT system?s output, called hypothe-
sis, against a gold standard translation, called ref-
erence. As MT evaluation metrics measure sen-
tence pair similarity, it is a natural idea to general-
ize them for paragraph-sentence pair.
In this paper, we follow the motivations of sev-
eral MT evaluation metrics yet made adaption to
cope with the length disparity difficulty of this
task, and combine these features in a regression
model. Our system SSMT (Semantic Similarity in
view of Machine Translation evaluation) involves
no extensive resource or strenuous computation,
yet gives promising result with just a few simple
features.
2 Regression Framework
In our experiment, we use features adapted from
some MT evaluation metrics and combine them
in a regression model for the semantic similarity
measurement. We exploit the following two sim-
ple models:
A linear regression model is presented as:
y = w
1
x
i
+ w
2
x
i
..+ w
n
x
n
+ ?
585
A log-linear model is presented as:
y = x
w
1
1
? x
w
2
2
.. ? x
w
n
n
? e
?
Where y is the similarity score, {x
1
, x
2
.., x
n
} are
the feature values.
We can see that in a log-linear model, if any
feature x
i
get a value of 0, the output y will suck in
0 forever no matter what the values other features
get. In our experiment we resort to smoothing to
avoid this ?0-trap? for some features (Section 4.3).
3 Features
MT evaluation metrics vary from lexical level to
syntactic level to semantic level. We consider only
lexical ones to avoid complicated steps like pars-
ing or semantic role labelling, which are computa-
tional expensive and may bring extra noise.
But instead of directly using the MT evaluation
metrics, we use the factors in them as features, the
idea is that the overall score of the original metric
is highly related to the length of both of the com-
pared pair, but its factors are often related to the
length of just one side yet still carry useful simi-
larity information.
3.1 BLEU-Derived Features
As the most wildly used MT evaluation metric,
BLEU (Papineni et al., 2002) uses the geomet-
ric mean of n-gram precisions to measure the hy-
potheses against references. It is a corpus-based
and precision-based metric, and uses ?brevity
penalty? as a replacement for recall. Yet this
penalty is meaningless on sentence level. There-
fore we considers only the precision factors in
BLEU:
P
n
BLEU
=
Ngram
ref
?Ngram
hyo
Ngram
ref
We use the modified n-gram precision here and
regard ?paragraph? as ?reference?, and ?sentence?
as the ?hypothesis?. N= 1,2,3,4. We call these
four features BLEU-derived features.
3.2 ROUGE-L-Derived Features
ROUGE-L (Lin and Och, 2004) measures the
largest common subsequence(LCS) between a
compared pair. BLEU implies the n-gram to be
consecutive, yet ROUGE-L allows for gaps be-
tween them. By considering only in-sequence
words, ROUGE-L captures sentence level struc-
ture in a natural way, then:
R
lcs
=
LCS(ref, hyo)
length(hyo)
P
lcs
=
LCS(ref, hyo)
length(ref)
F
lcs
=
(1 + ?
2
)R
lcs
P
lcs
)
R
lcs
+ ?
2
P
lcs
Where LCS(ref, hoy) is the length of LCS of the
compared pair. We set ? = 1, which means we
don?t want to make much distinction between the
?reference? and ?hypothesis? here. We call these
three features ROUGE-L-derived features.
3.3 ROUGE-S-Derived Features
ROUGE-S (Lin and Och, 2004) uses skip-bigram
co-occurrence statistics for similarity measure-
ment. One advantage of skip-bigram over BLEU
is that it does not require consecutive matches but
is still sensitive to word order. Given the reference
of length n, and hypothesis of length m, then:
P
skip2
=
skip2(ref, hyo)
C(m, 2)
R
skip2
=
skip2(ref, hyo)
C(n, 2)
F
skip2
=
(1 + ?
2
)P
skip2
R
skip2
R
skip2
+ ?
2
P
skip2
Where C is combination, and skip2(ref, hyo) is
the number of common skip-bigrams. We also
set ? = 1 here, and call these three indicators
ROUGE-S-derived features.
3.4 METEOR-Derived Features
METEOR (Banerjee and Lavie, 2005) evaluates
a hypothesis by aligning it to a reference trans-
lation and gives sentence-level similarity scores.
It uses a generalized concept of unigram mapping
that matches words in the following types: ex-
act match on words surface forms , stem match
on words stems, synonym match according to the
synonym sets in WordNet, and paraphrase match
(Denkowski and Lavie, 2010).
METEOR also makes distinction between con-
tent words and function words. Each type of
matchm
i
is weighted by w
i
, let (m
i
(h
c
),m
i
(h
f
))
be the number of content and function words
covered by this type in the hypothesis, and
586
(m
i
(r
c
),m
i
(r
f
)) be the counts in the reference,
then:
P =
?
i=1
w
i
? (? ?m
i
(h
i
) + (1? ?) ?m
i
(h
f
))
?? | h
c
| +(1? ?)? | h
f
|
R =
?
i=1
w
i
? (? ?m
i
(r
i
) + (1? ?) ?m
i
(r
f
))
?? | r
c
| +(1? ?)? | r
f
|
F
mean
=
P ?R
?P + (1? ?)R
To account for word order difference, the frag-
mentation penalty is calculated using the total
number of matched words(m) and the number of
chunks
1
(ch) in the hypothesis:
Pen = ? ?
(
ch
m
)
?
And the final METEOR score is:
Score = (1? Pen) ? F
mean
Parameters ?, ?, ?, ?and w
i
...w
n
are tuned to
maximize correlation with human judgements
(Denkowski and Lavie, 2014). We use Meteor1.5
system
2
for scoring. Parameters are tuned on
WMT12, and the paraphrase table is extracted on
the WMT data.
We use the p, r, frag(frag = ch/m) and
score as features and call them METEOR-derived
features.
4 Experiment and Discussion
4.1 Data Set
The SemEval2014 task3 subtask gives a train-
ing set of 500 paragraph-sentence pairs, with hu-
man annotated continuous score of 0 ? 4. These
pairs are labelled with genres of ?Newswire/ cqa
3
/
metaphoric/ scientific/ travel/ review?. Systems
are asked to predict the similarity scores for 500
pairs in the test set. Performance is evaluated in
Pearson correlation and Spearman correlation.
4.2 Data Processing
To avoid meaningless n-gram match ?the a?, or
words surface form difference, we employ very
simple data processings here: for features derived
from BLEU, ROUGE-L and ROUGE-S, we re-
move stop words and stem the sentences with
1
Chunk is defined as a series of matched unigrams that is
contiguous and identically ordered in both sentences
2
https://www.cs.cmu.edu/ alavie/METEOR/
3
cqa:Community Question Answering site text
coreNLP
4
. For METEOR-derived features, we use
the tool?s option for text normalization before
matching.
4.3 Result
Though texts with different genres may have dif-
ferent regression parameters, we just train one
model for all for simplicity. Table 1 compares
the result. Run1 is submitted as SSMT in the
official evaluation. It?s a log-linear model. We
choose more dense features for log-linear model
and use smoothing to avoid the ?0-trap? men-
tioned in (Section 2). The features include
P
1,2
BLEU
,P
ROUGE?L
,P
ROUGE?S
4 features, and
4 METEOR-derived features, altogether 8 fea-
tures. When calculation the first 4 features, we
plus 1 to both numerator and denominator as
smoothing. Run2 is a linear-regression model with
the same features as Run1. Run3 is a simple linear
regression model, which is free from the ?0-trap?,
thus we use all the 14 features without smoothing.
We use Matlab for regression. The baseline is of-
ficially given using LCS.
Run Regression Pearson Spearman
Baseline LCS 0.527 0.613
run1 log-linear 0.789 0.777
run2 linear 0.794 0.777
run3 linear 0.808 0.792
Table 1: System Performance.
4.4 System Analysis
We compares the effectiveness of different fea-
tures in a linear regression model. Table 2
shows the result. ?All? refers to all the fea-
tures, ?-METEOR? means the feature set ex-
cludes METEOR-derived features. We can see the
METEOR-derived features are the most effective
ones here.
Figure 1 shows the performance of our sys-
tem submitted as SSMT in the SemEval2014 task3
competition. It shows quite good correlation with
the gold standard.
A well predicted example is the #trial-p2s-5 pair
in the trial set:
Paragraph: Olympic champion Usain Bolt re-
gained his 100m world title and won a fourth in-
dividual World Championships gold with a sea-
son?s best of 9.77 seconds in Moscow. In heavy
4
http://nlp.stanford.edu/software/corenlp.shtml
587
Feature Pearson Spearman
All 0.808 0.792
- METEOR 0.772 0.756
- ROUGE-L 0.802 0.789
- ROUGE-S 0.807 0.793
- BLEU 0.807 0.790
Table 2: Effectiveness of Different Features.
?-METEOR? means the feature set excluding
METEOR-derived features.
Figure 1: Result Scatter of SSMT.
rain, the 26-year-old Jamaican made amends for
his false start in Daegu two years ago and fur-
ther cemented his status as the greatest sprinter
in history. The six-time Olympic champion over-
took Justin Gatlin in the final stages, forcing the
American to settle for silver in 9.85. Bolt?s com-
patriot Nesta Carter (9.95) claimed bronze, while
Britain?s James Dasaolu was eighth (10.21).
Sentence: Germany?s Robert Harting beats
Iran?s Ehsan Hadadi and adds the Olympic discus
title to his world crown.
The system gives a prediction of 1.253 against
the gold standard 1.25. We can see that topic
words like ?Olympic? , ?world crown?, ?beats? in
the short text correspond to expressions of ?world
title ? , ?champion? across several sentences in the
long text, but this pair of texts are not talking about
the same event. The model captures and models
this commonness and difference very well .
But Figure 1 also reveals an interesting phe-
nomenon: the system seldom gives the boundary
scores of 0 or 4. In other words, it tends to over-
score or underscore the boundary conditions. An
example in point is the #trial-p2s-17 pair in the
trial data, it is actually the worst predicted pair by
our system in the trail set:
Paragraph: A married couple who met at work
is not a particularly rare thing. Three in ten work-
ers who have dated a colleague said in a recent
survey by CareerBuilder.com that their office ro-
mance eventually led to marriage.
Sentence: Marrying a coworker isn?t uncom-
mon given that 30% of workers who dated a
coworker ended up marrying them.
The system gives a 1.773 score against the gold
standard of 4. It should fail to detect the equality
of expressions between ?three in ten? and ?30%?.
Thus better detection of phrase similarity is de-
sired. We think this is the main reason to under-
score the similarity. For test pairs with the genre of
?Metaphoric?, the system almost underscores all
of them. This failure has been expected, though.
Because ?Metaphoric? pairs demand full under-
standing of the semantic meaning and paragraph
structure, which is far beyond the reach of lexical
match metrics.
5 Conclusion
MT evaluation metrics have been directly used
as features in paraphrase (Finch et al., 2005) de-
tection and sentence pair semantic comparison
(Souza et al., 2012). But paragraph-to-sentence
pair faces significant length disparity, we try a way
out to alleviate this impact yet still follow the mo-
tivations underlying these metrics. By factorizing
down the original metrics, the linear model can
flexibly pick out factors that are not sensitive to
the length disparity problem.
We derive features from BLEU, ROUGE-
L, ROUGE-S and METEOR, and show that
METEOR-derived features make the most signifi-
cant contributions here. Being easy and light, our
submitted SSMT achieves 0.789 in Pearson and
0.777 in Spearman correlation, and ranks 11 out
of the 34 systems in this subtask. Our best try
achieves 0.808 in Pearson and 0.786 in Spearman
correlation.
Acknowledgements
This work is supported by National Natu-
ral Science Foundation of China under Grant
No.61273318 and National Key Basic Research
Program of China 2014CB340504.
588
References
Andrew Finch, Yong S. Hwang, Eiichiro Sumita. Us-
ing machine translation evaluation techniques to de-
termine sentence-level semantic equivalence. Pro-
ceedings of the Third International Workshop on
Paraphrasing(IWP2005), 2005: 17-24.
Chin Y. Lin,Franz J. Och. Automatic evaluation of ma-
chine translation quality using longest common sub-
sequence and skip-bigram statistics. Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics. ACL, 2004: 605.
Daniel B?ar, Chris Biemann, Iryna Gurevych, et al.
Ukp: Computing semantic textual similarity by
combining multiple content similarity measures.
Proceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation. ACL, 2012: 435-
440.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. SemEval-2014 Task 3: Cross-
Level Semantic Similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014)., August 23-24, 2014, Dublin, Ire-
land.
George Miller,Christiane Fellbaum. WordNet.
http://wordnet.princton.edu/, 2007.
Jos?e G C de Souza, Matteo Negri, Yashar Mehdad.
FBK: machine translation evaluation and word sim-
ilarity metrics for semantic textual similarity. Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation. ACL, 2012: 624-
630.
Kishore Papineni, Salim Roukos, Todd Ward, et al.
BLEU: a method for automatic evaluation of ma-
chine translation. Proceedings of the 40th annual
meeting on association for computational linguis-
tics. ACL, 2002: 311-318.
Michael Denkowski, Alon Lavie. Extending the ME-
TEOR machine translation evaluation metric to the
phrase level. Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics. ACL, 2010: 250-253.
Michael Denkowski, Alon Lavie. Meteor Univer-
sal: Language Specific Translation /Evaluation for
Any Target Language translation. Proceedings of the
EACL 2014 Workshop on Statistical Machine Trans-
lation, 2014.
Mohammad T Pilehvar, David Jurgens, Roberto Nav-
igli. Align, Disambiguate and Walk: A Unified
Approach for Measuring Semantic Similarity Pro-
ceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics,ACL, 2013:
1341-1351.
Peter D. Turney and Patrick Pantel. From frequency to
meaning: Vector space models of semantics Artifi-
cial Intelligence Research, 2010. 37(1): 141-188
Satanjeev Banerjee, Alon Lavie. METEOR: an auto-
matic metric for MT Evaluation with improved cor-
relation with human judgements. Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summa-
rization., 2005: 65-72.
Tomas Mikolov, Kai Chen, Greg Corrado, et al. Ef-
ficient estimation of word representations in vector
space. 2013. arXiv preprint arXiv:1301.3781,
589
Chinese word segmentation model using bootstrapping 
Baobao Chang and Mairgup Mansur 
Institute of Computational Linguistics, Peking University 
Key Laboratory of Computational Linguistics(Peking University),  
Ministry Education, China 
chbb@pku.edu.cn, mairgup@yahoo.com.cn 
 
Abstract 
We participate in the CIPS-SIGHAN-
2010 bake-off task of Chinese word 
segmentation. Unlike the previous 
bakeoff series, the purpose of the 
bakeoff 2010 is to test the cross-
domain performance of Chinese seg-
mentation model. This paper summa-
rizes our approach and our bakeoff re-
sults. We mainly propose to use ?2 sta-
tistics to increase the OOV recall and 
use bootstrapping strategy to increase 
the overall F score. As the results 
shows, the approach proposed in the 
paper does help, both of the OOV re-
call and the overall F score are im-
proved. 
1 Introduction 
After more than twenty years of intensive re-
searches, considerable progress has been made 
in improving the performance of Chinese word 
segmentation. The bakeoff series hosted by the 
ACL SIGHAN shows that high F scores can be 
achieved in the closed test tracks, in which 
only specified training materials can be used in 
learning segmentation models. 
Instead of using lexicon-driven approaches, 
state-of-art Chinese word segmenter now use 
character tagging model as Xue(2003) firstly 
proposed. In character tagging model, no pre-
defined Chinese lexicons are required; a tag-
ging model is learned using manually seg-
mented training texts. The model is then used 
to assign each character a tag indicating the 
position of this character within word. Xue?s 
approach has been become the most popular 
approach to Chinese word segmentation for its 
high performance and unified way to deal with 
OOV issues. Most of the segmentation works 
since then follow this approach. Major im-
provements in this line of research including: 1) 
More sophisticated learning models were in-
troduced instead of the maximum entropy 
model that Xue used, like conditional random 
fields (CRFs) model which fit the sequence 
tagging tasks much better than maximum en-
tropy model (Tseng et al,2005). 2) More tags 
were introduced, as Zhao et al (2006) shows 6 
tags are superior to 4 tags in achieving high 
performance. 3) New feature templates were 
added, such as templates used in representing 
numbers, dates, letters etc. (Low et al, 2005)  
Usually, the performance of segmentation 
model is evaluated on a test set from the same 
domain as the training set. Such evaluation 
does not reveal its ability to deal with domain 
variation. It is believed that, when test set is 
from other domains than the domain where 
training set is from, the learned model nor-
mally underperforms substantially.  
The CIPS-SIGHAN-2010 bake-off task of 
Chinese word segmentation is set to focus on 
the cross-domain performance of Chinese 
word segmentation model.  
We participate in the closed test track for 
simplified Chinese. Different with the previous 
bakeoffs, CIPS-SIGHAN-2010 bake-off pro-
vides both label corpus and unlabeled corpora. 
The labeled corpus is composed of texts from 
newspaper and has about 1.1 million words in 
total. The two unlabeled corpora cover two 
domains: literature and computer science, and 
each domain have about 100K characters in 
size. The test corpora cover four domains, two 
of which are literature and computer science, 
and the other two domains are unknown before 
releasing. 
We build the Chinese word segmenter fol-
lowing the character tagging model. Instead of 
using CRF model, we use the hidden Markov 
support vector machines (Altun et al, 2003), 
which is also a sequence labeling model like 
CRF. We just show it can also be used to 
model Chinese segmentation tasks as an alter-
native other than CRF. To increase the ability 
of the model to recall OOV words, we propose 
to use ?2 statistics and bootstrapping strategy 
to the overall performance of the model to out-
of-domain texts.  
2 The hidden Markov support vector 
machines 
The hidden Markov support vector machine 
(SVM-HMM) is actually a special case of the 
structural support vector machines proposed by 
Tsochantaridis et al(2005) which is a powerful 
model to structure predication problem. It dif-
fers from support vector machine in its ability 
to model complex structured problems and 
shares the max-margin training principles with 
support vector machines. The hidden Markov 
support vector machine model is inspired by 
the hidden Markov model and is an instance of 
structural support vector machine dedicated to 
solve sequence labeling learning, a problem 
that CRF model is assumed to solve. In the 
SVM-HMM model, the sequence labeling 
problems is modeled by learning a discrimi-
nant function F: X?Y?R over the input se-
quence and the label sequence pairs, thus pre-
diction of label sequence can be derived by 
maximizing F over all possible label sequences 
for a specific given input sequence x.  
);,(maxarg);( wyxwx
y
Ff
Y?
=  
In the structural SVMs, F is assumed to be lin-
ear in some combined feature representation of 
the input sequence and the label sequence 
?(x,y), i.e. 
),(,);,( yx?wwyx =F  
where w denotes a parameter vector. For the 
SVM-HMMs, the discriminant function is de-
fined as follows.  
? ??
??
?=
=
?? ??
+
??
+
=
1..1
..1
'
1
', )',(),(?
),()(,);,(
Tt
Tt
y y
tt
yy
y
tt
y
yyyy
yyyxF
???
?
w
x?ww
 
Here )?,( www = , ?(xt) is the vector of fea-
tures of the input sequence.  
Like SVMs, parameter vector w is learned 
with maximum margin principle using training 
data. To control the complexity of the training 
problem, cutting plane method is proposed to 
solve the resulted constrained optimization 
problem. Thus only small subset of constraints 
from the full-sized optimization is checked to 
ensure a sufficiently accurate solution. 
Roughly speaking, SVM-HMM differs with 
CRF in its principle of training, both of them 
could be used to deal with sequence labeling 
problem like Chinese word segmentation. 
3 The tag set and the basic feature 
templates 
As most of other works on segmentation, we 
use a 4-tag tagset, that is S for character being 
a single-character-word by itself, B for charac-
ter beginning a multi-character-word, E for 
character ending a multi-character-word and M 
for a character occurring in the middle of a 
multi-character-word. 
We use the following feature template, like 
most of segmentation works widely used: 
(a) Cn (n = -2, -1, 0, 1, 2) 
(b) CnCn+1 (n = -2, -1, 0, 1) 
(c) C-1C+1  
Here C refers to character; n refers to the posi-
tion index relative to the current character. By 
setting the above feature templates, we actually 
set a 5-character window to extract features, 
the current character, 2 characters to its left 
and 2 characters to its right.   
In addition, we also use the following fea-
ture templates to extract features representing 
character type. The closed test track of CIPS-
SIGHAN-2010 bake-off allows participants to 
use four character types, which are Chinese 
Character, English Letter, digits and punctua-
tions: 
(d) Tn (n = -2, -1, 0, 1, 2) 
(e) TnTn+1 (n = -2, -1, 0, 1) 
(f) T-1T+1 
Here T refers to character type, its value can be 
digit, letter, punctuation or Chinese character.  
4 The ?2 statistic features 
One of reasons of the performance degrada-
tion lies in the model?s ability to cope with 
OOV words while working with the out-of-
domain texts. Aiming at preventing the OOV 
recall from dropping sharply, we propose to 
use ?2 statistics as features to the segmentation 
model. 
?2 test is one of hypothesis test methods, 
which can be used to test if two events co-
occur just by chance or not. A lower ?2 score 
normally means the two co-occurred events are 
independent; otherwise they are dependent on 
each other. Hence, ?2 statistics could also be 
used to deal with the OOV issue in segmenta-
tion models. The idea is very straightforward. 
If two adjacent characters in the test set have a 
higher ?2 score, it is highly likely they form a 
word or are part of a word even they are not 
seen in the training set.  
We only compute ?2 score for character bi-
grams in the training texts and test texts. The 
?2 score of a bigram  C1C2 can be computed by 
the following way. 
)()()()(
)(),(
2
21
2
dcdbcaba
cbdanCC +?+?+?+
????=?
Here,  
a refers to all counts of bigram C1C2 in the 
text; 
b refers to all counts of bigrams that C1 oc-
curs but C2 does not; 
c refers to all counts of bigrams that C1 does 
not occur but C2 occurs; 
d refers to all counts of bigrams that both C1 
and C2 do not occur.  
n refers to total counts of all bigrams in the 
text, apparently, n=a+b+c+d. 
We do the ?2 statistics computation to the 
training texts and the test texts respectively. To 
make the ?2 statistics from the training texts 
and test texts comparable, we normalize the ?2 
score by the following formula.  
??
???
? ??
?= 10),(),( 2
min
2
max
2
min21
2
21
2
??
??? CCCCnorm  
Then we incorporate the normalized ?2 statis-
tics into the SVM-HMM model by adding two 
more feature templates as follows: 
(g) XnXn+1 (n = -2, -1, 0, 1) 
(h) X-1X+1 
The value of the feature XnXn+1 is the normal-
ized ?2 score of the bigram CnCn+1. Note we 
also compute the normalized ?2 score to bi-
gram C-1C+1. 
Because the normalized ?2 score is one of 
11 possible values 0, 1, 2, ?, 10,  templates 
(g)-(h) generate 55 features in total.   
All features generated from the templates 
(a)-(f) together with the 55 ?2 features form the 
whole feature set. The training texts and test 
texts are then converted into their feature rep-
resentations. The feature representation of the 
training texts is then used to learn the model 
and the feature representation of the test texts 
is used for segmentation. By this way, we ex-
pect that an OOV word in the test texts might 
be found by the segmentation model if the bi-
grams extracted from this word take higher ?2 
scores.  
5 the bootstrapping strategy 
The addition of the ?2 features can be also 
harmful. Even though it could increase the 
OOV recall, it also leads to drops in IV recall 
as we found. To keep the IV recall from falling 
down, we propose to use bootstrapping strat-
egy. Specifically, we choose to use both mod-
els with ?2 features and without ?2 features. 
We train two models firstly, one is ?2-based 
and another not. Then we do the segmentation 
to the test text with the two models simultane-
ously. Two segmentation results can be ob-
tained. One result is produced by the ?2-based 
model and has a high OOV recall. The other 
result is produced by the non-?2-based model 
and has higher IV recall. Then we do intersec-
tion operation to the two results. It is not diffi-
cult to understand that the intersection of the 
two results has both high OOV recall and high 
IV recall. We then put the intersection results 
into the training texts to form a new training 
set. By this new training set, we train again to 
get two new models, one ?2-based and another 
not. Then the two new models are used to 
segment the test texts. Then we do again inter-
section to the two results and the common 
parts are again put into the training texts. We 
repeat this process until a plausible result is 
obtained. 
The whole process can be informally de-
scribed as the following algorithm: 
1. let training set T to be the original 
training set; 
2. for I = 0 to K 
1) train a ?2-based model and a non-
?2-base model separately using 
training set T; 
2) use both models to segment test 
texts; 
3) do intersection to the two segmen-
tation results 
4) put the intersection results into the 
training set and get the enlarged 
training set T 
3. train the non-?2-based model using 
training set T, and take the output of 
this model as the final output; 
4. end. 
6 The evaluation results 
The labeled training texts released by the 
bakeoff are mainly composed of texts from 
newspaper. A peculiarity of the training data is 
that all Arabic numbers, Latin letters and punc-
tuations in the data are double-byte codes. As 
in Chinese texts, there are actually two ver-
sions of codes for Arabic numbers, Latin let-
ters and punctuations: one is single-byte codes 
defined by the western character encoding 
standard; another is double-byte codes defined 
by the Chinese character encoding standards. 
Chinese normally use both versions without 
distinguishing them strictly. 
The four final test sets released by the bake-
off cover four domains, the statistics of the test 
sets are shown in table-1. (the size is measured 
in characters)  
 
 
 
Table-1. Test sets statistics 
test set domain  size  OOV rate
A Literature 51K 0.069
B Computer 64K 0.152
C Medicines 52K 0.110
D Finance 56K 0.087
 
We train all models using SVM-HMMs1, we 
set ? to 0.25. This is a parameter to control the 
accuracy of the solution of the optimization 
problem. We set C to half of the number of the 
sentences in the training data. The C parameter 
is set to trade off the margin size and training 
error. We also set a cutoff frequency to feature 
extraction. Only features are seen more than 
three times in training data are actually used in 
the models. We set K = 3 and run the algo-
rithm shown in section 5. This gives our final 
bakeoff results shown in Table-2. 
To illustrate whether the ?2 statistics and 
bootstrapping strategy help or not, we also 
show two intermediate results using the online 
scoring system provided by the bakeoff2.Table-
3 shows the results of the initial non-?2-based 
model using feature template (a)-(f), table-4 
shows results of the initial ?2-based model us-
ing feature template (a)-(h). 
As we see from the table-1, table-3 and ta-
ble-4, the approach present in this paper does 
improve both the overall performance and the 
OOV recalls in all four domains.  
 
Table-3 Results of initial non-?2-based model 
test set R P F Roov
A 0.921 0.924 0.923 0.632
B 0.930 0.904 0.917 0.758
C 0.919 0.906 0.913 0.687
D 0.946 0.924 0.935 0.750
 
                                                 
1http://www.cs.cornell.edu/People/tj/svm_light/svm_hm
m.html 
2 http://nlp.ict.ac.cn/demo/CIPS-SIGHAN2010/# 
Table-2. The bakeoff results 
test set R P F Riv Roov
A 0.925 0.931 0.928 0.944 0.667
B 0.941 0.916 0.928 0.967 0.796
C 0.928 0.918 0.923 0.953 0.730
D 0.948 0.928 0.937 0.965 0.761
 
Table-4 Results of initial ?2-based model 
test set R P F Roov
A 0.898 0.921 0.910 0.673
B 0.925 0.914 0.920 0.801
C 0.916 0.922 0.919 0.764
D 0.931 0.937 0.934 0.821
 
We also do a rapid manual check to the final 
results; one of the main sources of errors lies in 
the approach failing to recall numbers encoded 
by one-byte codes digits. For the labeled train-
ing corpus provided by the bakeoff almost do 
not use one-byte codes for digits, and the type 
feature seems do not help too much. Actually, 
such numbers can be recalled by simple heuris-
tics using regular expressions. We do a simple 
number recognition to the test set of domain D. 
this will increase the F score from 0.937 to 
0.957.  
  
7 Conclusions 
This paper introduces the approach we used 
in the CIPS-SIGHAN-2010 bake-off task of 
Chinese word segmentation. We propose to 
use ?2 statistics to increase OOV recall and use 
bootstrapping strategy to increase the overall 
performance. As our final results shows, the 
approach works in increasing both of the OOV 
recall and overall F-score.    
We also show in this paper that hidden 
Markov support vector machine can be used to 
model the Chinese word segmentation problem, 
by which high f-score results can be obtained 
like CRF model. 
Acknowledgements 
This work was supported by National Natu-
ral Science Foundation of China under Grant 
No. 60975054 and National Social Science 
Foundation of China under Grant No. 
06BYY048. 
We want to thank Professor Duan Huiming 
and Mr. Han Dongxu for their generous help at 
the data preprocessing works. 
References 
Liang, Nanyuan, 1987. ??written Cinese text seg-
mentation system--cdws?. Journal of Chinese In-
formation Processing, Vol.2, NO.2,pp44?52.(in 
Chinese) 
Gao, Jianfeng et al, 2005, Chinese Word Segmen-
tation and Named Entity Recognition: A Prag-
matic Approach, Computational Linguis-
tics,Vol.31, No.4, pp531-574. 
Huang, Changning et al 2007, Chinese word seg-
mentation: a decade review. Journal of Chinese 
Information Processing, Vol.21, NO.3,pp8?
19.(in Chinese) 
Tseng, Huihsin et al, 2005, A conditional random 
field word segmenter for SIGHAN 2005, Pro-
ceedings of the fourth SIGHAN workshop on 
Chinese language processing. Jeju Island, Korea. 
pp168-171 
Xue, Nianwen, 2003, Chinese Word Segmentation 
as Character Tagging, Computational Linguistics 
and Chinese Language Processing. Vol.8, No.1, 
pp29-48. 
Zhao, Hai et al, 2006, Effective tag set selection in 
Chinese word segmentation via conditional ran-
dom field modeling, Proceedings of the 20th Pa-
cific Asia Conference on language, Information 
and Computation (PACLIC-20), Wuhan, China, 
pp87-94  
Tsochantaridis,Ioannis et al, 2005, Large Margin 
Methods for Structured and Interdependent Out-
put Variables, Journal of Machine Learning Re-
search (JMLR), No.6, pp1453-1484.  
Altun,Yasemin et al,2003, Hidden Markov Support 
Vector Machines. Proceedings of the Twentieth 
International Conference on Machine Learning 
(ICML-2003), Washington DC, 2003. 
Low, Jin Kiat et al,2005, A Maximum Entropy 
Approach to Chinese Word Segmentation. Pro-
ceedings of the Fourth SIGHAN Workshop on 
Chinese Language Processing, Jeju Island, Ko-
rea,. pp161-164 
 
