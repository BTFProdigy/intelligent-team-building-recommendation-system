Disambiguation of Finite-State Transducers
N. Smaili and P. Cardinal and G. Boulianne and P. Dumouchel
Centre de Recherche Informatique de Montre?al.
{nsmaili, pcardinal, gboulian, Pierre.Dumouchel}@crim.ca
Abstract
The objective of this work is to
disambiguate transducers which have
the following form: T = R ?D and to
be able to apply the determinization
algorithm described in (Mohri, 1997).
Our approach to disambiguating
T = R ?D consists first of computing
the composition T and thereafter to
disambiguate the transducer T . We
will give an important consequence of
this result that allows us to compose
any number of transducers R with
the transducer D, in contrast to the
previous approach which consisted in
first disambiguating transducers D
and R to produce respectively D? and
R? , then computing T ? = R? ?D? where
T ? is unambiguous. We will present
results in the case of a transducer
D representing a dictionary and R
representing phonological rules.
Keywords: ambiguity, determinis-
tic, dictionary, transducer.
1 Introduction
The task of speech recognition can be
decomposed into several steps, where
each step is represented by a finite-
state transducer (Mohri et al, 1998).
The search space of the recognizer is
defined by the composition of trans-
ducers T = A ? C ?R ?D ?M . Trans-
ducer A converts a sequence of obser-
vations O to a sequence of context-
dependent phones.
Transducer C converts a sequence
of context-dependent phones to a
sequence of context-independent
phones. Transducer R is a mapping
from phones to phones which imple-
ments phonological rules. Transducer
D is the pronunciations dictionary.
It converts a sequence of context-
independent phones to a sequence of
words. Transducer M represents a
language model: it converts sequences
of words into sequences of words, while
restricting the possible sequences or
assigning a score to the sequences.
The speech recognition problem con-
sists of finding the path of least cost
in transducer O ? T , where O is a
sequence of acoustic observations.
The pronunciations dictionary rep-
resenting the mapping from pronun-
ciations to words can show an inher-
ent ambiguity: a sequence of phones
can correspond to more than one word,
so we cannot apply the transducer de-
terminization algorithm (an operation
which reduces the redundancy, search
time and possibly space). This prob-
lem is usually handled by adding spe-
cial symbols to the dictionary to re-
move the ambiguity in order to be
able to apply the determinization al-
gorithm (Koskenniemi, 1990). Never-
theless, when we compose the dictio-
nary with the phonological rules, we
must take into account special sym-
bols. This complicates the construc-
tion of transducers representing these
rules and leads to size explosion. It
would be simpler to compose the rules
with the dictionary, then remove the
ambiguity in the result and then apply
the determinization algorithm.
2 Notations and
definitions
Formally, a weighted transducer over a
semiring K = (K,?,?, 0?, 1?) is defined
as a 6-tuple T = (Q, I,?1,?2, E, F )
where Q is a finite set of states, I ?
Q is a finite set of initial states, ?1 is
the input alphabet, ?2 is the output
alphabet, E is a finite set of transitions
and F ? Q is a finite set of final states.
A transition is an element of Q??1?
?2 ?Q?K.
Transitions are of the form
t = (p(t), i(t), o(t), n(t), w(t)), t ? E
where p(t) denotes the transition?s
origin state, i(t) its input label, o(t)
its output label, n(t) the transition?s
destination state and w(t) ? K is the
weight of t. The tropical semiring
defined as (R+ ? ?, min,+,?, 0) is
commonly used in speech recogni-
tion, but our results are applicable
to the case of general semirings as well.
A path pi = t1 ? ? ? tn of T is an ele-
ment of E? verifying
n(ti?1) = p(ti) for 2 ? i ? n.
We can easily extend the functions p
and n to those paths:
p(pi) = p(t1), (1)
n(pi) = n(tn). (2)
We denote by P (r, s) the set of paths
whose origin is state r and whose des-
tination is state s. We can also extend
the function P to the sets R ? Q and
S ? Q:
P (R, S) =
?
r?R, s?S
P (r, s)
We can extend the functions i and o to
the paths by taking the concatenations
of the input and output symbols:
i(pi) = i(t1) ? ? ? i(tn), (3)
o(pi) = o(t1) ? ? ?o(tn). (4)
Definition 1 (unambiguous trans-
ducer, (Berstel, 1979))
A transducer T is said to be unam-
biguous if for each w ? ??1, there
exists at most one path pi in T such
that i(pi) = w.
Definition 2 (ambiguous paths)
Two paths pi and ? are ambiguous if
pi 6= ? and i(pi) = i(?).
Remark 1 : To remove the ambiguity
between two paths pi and ?, it suffices
to modify i(pi) by changing the first in-
put label of the path pi. This is done
by introducing an auxiliary symbol such
that: i(pi) 6= i(?).
Figure 1a shows an ambiguous
transducer. It is ambiguous since
for the input string ?s e [z]?, there
are two paths representing the out-
put strings {ces, ses}. In this figure,
?eps? stands for epsilon or null symbol.
To disambiguate a transducer, we
first group the ambiguous paths; we
then remove the ambiguity in each
group by adding auxiliary labels as
shown in Figure 1b. Unfortunately, it
is infeasible to enumerate all the paths
in a cyclic transducer. However, in
(Smaili, 2001) it is shown that cyclic
transducers of the type studied in this
work can be disambiguated by trans-
forming to a corresponding acyclic sub-
transducer such that T ? ? T . This
(a)
0
1s:ses
3s:ces
5a:amis
7
k:cadeau
2
E:eps
4E:eps
6m:eps
8a:eps
10
[z]:eps
#:#
[z]:eps
i:eps
9d:eps
o:eps
(b)
0
1s:ses
3s-2:ces
5a:amis
7
k:cadeau
2
E:eps
4E:eps
6m:eps
8a:eps
10
[z]:eps
#:#
[z]:eps
i:eps
9d:eps
o:eps
Figure 1: (a) Ambiguous transducer
(b) Disambiguated transducer
fundamental property is described in
detail in section 2.1. Accordingly, we
apply the appropriate transformation
to the input transducer.
2.1 Fundamental Property
We are interested in the transducer
T = (Q, I, ?, ?, E, F ) with ? =
?0 ] ?1 verifying the following prop-
erty:
Any cycle in T contains at least a
transition t such that i(t) ? ?1.
We denote by E0 and E1 the follow-
ing sets: E0 = {t ? E : i(t) ? ?0}
and E1 = {t ? E : i(t) ? ?1}. Notice
that E = E0 ] E1.
We can give a characterization of the
ambiguous paths verifying the funda-
mental property. Before, let?s make the
following remark:
Remark 2 Any path pi in T has the
following form:
pi = f0 pi0 f1 pi1 ? ? ?pin?1 fn pin
with pii ? E+0 , fi ? E+1 for 1 ? i ?
n, f0 ? E?1 and pi0 ? E?0 if n ? 1.
If n = 0 then pi = f0 pi0.
Proposition 1 (characterization of
ambiguous paths)
Let pi and ? be two paths such that:
pi = f0 pi0 f1 pi1 ? ? ?pin?1 fn pin and
? = g0 ?0 g1 ?1 ? ? ??k?1 gk ?k.
pi and ? are ambiguous if and only if
?
?
?
k = n
?i and pii are ambiguous (0 ? i ? n).
fi and gi are ambiguous (0 ? i ? n).
We will assume that the first transi-
tion?s path belongs to E0, i.e. f0 = .
Recall that if we want to avoid cy-
cles, we just have to remove from T
all transitions t ? E1. According to
Proposition 1, ambiguity needs to be
removed only in paths that use tran-
sitions t ? E0, namely the path pii
that performs the decomposition given
in Remark 2. Disambiguation consists
only of introducing auxiliary labels in
the ambiguous paths. We denote by
Asrc the set of origin states of transi-
tions belonging to E1 and by Adst the
set of destination states of transitions
belonging to E2.
Asrc = {p(t) : t ? E1}
Adst = {n(t) : t ? E1}
According to Proposition 1 and what
precedes, it would be equivalent and
simpler to disambiguate an acyclic
transducer obtained from T in which
we have removed all E1 transitions.
Therefore, we introduce the operator
? : {Tin} ?? {Tout} which accom-
plishes this construction.
Let T = (Q, I,?1,?2, E, F ). Then
?(T ) = (Q, I1,?1,?2, ET , F1) where:
1. I1 = I ? Adst ? {i}, with i 6? Q.
2. F1 = F ? Asrc ? {f}, with f 6? Q.
3. ET = E \E1?{(i, q, , , 0), q ?
I1} ? {(q, f, , , 0), q ? F1}.
The third condition insures the connec-
tivity of ?(T ) if T is itself connected.
It suffices to disambiguate the acyclic
transducer ?(T ), then reinsert the
transitions of E1 in ?(T ). The set of
paths in ?(T ) is then P(I1, F1).
2.2 Algorithm
Input:
T = (Q, i, X, Y, E, F ) is an
ambiguous transducer verifying the
fundamental property.
Output:
T1 = (Q, i, X ?X1, Y, ET , F ) is an
unambiguous transducer, X1 is the set
of auxiliary symbols.
1. Tacyclic ? ?(T ).
2. Path ? set of paths of Tacyclic.
3. Disambiguate the set Path (creat-
ing the set X1).
4. T0 ? build the unambiguous
transducer which has unambigu-
ous paths.
5. T1 ? ??1(T0) (consists of rein-
serting in T0 the transitions of T
which where removed).
6. return T1
Now, we will study an important
class of transducers verifying the fun-
damental property. This class is ob-
tained by doing the composition of a
transducer D verifying the fundamen-
tal property with a transducer R. The
composition of two transducers is an
efficient algebraic operation for build-
ing more complex transducers. We
give a brief definition of composition
and the fundamental theorem that in-
sures the invariance of the fundamental
property by composition.
3 Composition
The transducer T created by the com-
position of two transducers R and D,
denoted T = R?D, performs the map-
ping of word x to word z if and only
if R maps x to y and D maps y to z.
The weight of the resulting word is the
?-product of the weights of y and z
(Pereira and Riley, 1997).
Definition 3 (Transitions) Let t =
(q, a, b, q1, w1) and e = (r, b, c, r1,
w2) be two transitions. We define the
composition t with e by:
t ? e = ((q, r), a, c, (q1, r1), w1 ? w2).
Note that, in order to make the com-
position possible, we must have o(t) =
i(e).
Definition 4 (Composition)
Let R = (QR, IR, X, Y, ER, FR)
and S = (QS, IS, Y, Z, ES, FS) be
two transducers. The composi-
tion of R with S is a transducer
R ? S = (Q,Q,X, Z,E, F ) defined by:
1. i = (iR, iS),
2. Q = QR ?QS,
3. F = FR ? FS,
4. E = {eR?eS : eR ? ER, eS ? ES}.
Let D = (QD, ID, Y, Z, ED, FD) be a
transducer verifying the fundamental
property. We can write Y = Y0 ] Y1
where Y0 = {i(t) : t ? E0} and
Y1 = {i(t) : t ? E1}.
Theorem 1 (Fundamental) Let
R = (QR, IR, X, Y, ER, FR) verifying
the following condition:
(C) ?t ? ER, o(t) ? Y1 ? i(t) ? Y1.
Then the transducer T = R?D verifies
the fundamental property.
Proof :
Let X1 = {i(t) : t ? ER and o(t) ?
Y1} ? Y1 and X0 = X \ X1. We will
prove that any path in T contains at
least a transition t such that i(t) ? X1.
Let pi be a cycle in T . Then, there
exists two cycles piR and piD in R and
in D respectively such that pi = piR ?
piD. The paths piR and piD have the
following form:
piD = g1 ? ? ? gn,
with gi ? ED for 1 ? i ? n;
piR = f1 ? ? ? fn,
with fi ? ER for 1 ? i ? n;
pi = piR ? piD = (f1 ? g1) ? ? ? (fn ? gn).
There is an index k such that i(gk) ?
Y1 since D verifies the fundamental
property. We also necessarily have
i(gk) = o(fk) . According to condi-
tion (C) of Theorem 1, we deduce that
i(fk) ? Y1. Knowing that fk ? ER, we
deduce that i(fk) ? X1, which implies
i(fk ? gk) = i(fk) ? X1.
3.1 Consequence
The restriction to the case X = Y
allows us to build a large class of
transducers verifying the fundamental
property. In fact, if two transducers
R = (QR, IR, Y, Y, ER, FR) and S =
(QS, IS, Y, Y, ES, FS) verify the condi-
tion (C) of Theorem 1, then S ?R ver-
ifies the condition (C), associativity of
? implies:
S ? (R ?D) = (S ?R) ?D.
Suppose that we have m transducers
Ri ( 1 ? i ? m ) verifying the con-
dition (C) of Theorem 1 and that we
want to reduce the size of the trans-
ducer:
Tm = Rm ?Rm?1 ? ? ?R1 ?D.
To this end, we proceed as follows: we
add the auxiliary symbols to disam-
biguate the transducer; then we apply
determinization and finally we remove
the auxiliary labels. These three oper-
ations are denoted by ?.
Ti =
{
?(D) if i = 0.
?(Ri ? ?(Ti?1)) if i ? 1.
The size of transducer Tm can also
be reduced by computing:
Tm = ?(Rm ?Rm?1 ? ? ?R1 ?D).
The old approach:
T ?m = R
?
m ?R
?
m?1 ? ? ?R
?
1 ?D
?.
has several disadvantages. The size of
R?i for 1 ? i ? m increases consid-
erably since the auxiliary labels intro-
duced in each transducer have to be
taken into account in all others. This
fact limits the number of transducers
that can be composed with D.
4 Application and Results
We will now apply our algorithm to
transducers involved in speech recog-
nition. Transducer D represents the
pronunciation dictionary and possesses
the fundamental property. The set of
transitions of D is defined as
E = E0 ] {(f,#, x, 0, w)}
where f is the unique final state ofD, 0
is the unique initial state of D, x is any
symbol and # is a symbol represent-
ing the end of a word. All transitions
t ? E0 are such that i(t) 6= #. Any
path pi in E?0 is acyclic. The transducer
R representing a phonological rule is
constructed to fulfill condition (C) of
the fundamental theorem. The trans-
ducer D represents a French dictionary
with 20000 words and their pronuncia-
tions. The transducer R represents the
phonological rule that handles liaison
in the French language. This liaison,
which is represented by a phoneme ap-
pearing at the end of some words, must
be removed when the next word be-
gins with a consonant since the liaison
phoneme is never pronounced in that
case. However, if the next word begins
with a vowel, the liaison phoneme may
or may not be pronounced and thus be-
comes optional.
0
p:p
#:#
1
eps:[x]
2
[x]:[x]
p:p
#:#
v:v
#:#
Figure 2: Transducer used to handle
the optional liaison rule.
Figure 2 shows the transducer that
handles this rule. In the figure, p
denotes all phonemes, v the vowels
and [x] the liaison phonemes.
Table 1 shows the results of our al-
gorithm using the dictionary and the
phonological rule previously described.
Transducer States Transitions
D 115941 136001
?(D) 17607 42140
R ?D 115943 151434
?(R ?D) 17955 50769
R ? ?(D) 17611 53209
?(R ? ?(D)) 17587 49620
Table 1: Size reduction on a French
dictionary
As we can see in Table 1, the opera-
tor ? produces a smaller transducer in
all the cases considered here.
5 Conclusion and future
work
We have been able to disambiguate
an important class of cyclic and am-
biguous transducers, which allows us
to apply the determinization algorithm
(Mohri, 1997); and then to reduce the
size of those transducers. With our
new approach, we do not have to take
into account the number of transduc-
ers Ri and their auxiliary labels as was
the case with the approach used be-
fore. Thus, new transducers Ri such
as phonological rules can be easily in-
serted in the chain.
The major disadvantage of our ap-
proach is that disambiguating a trans-
ducer increases its size systematically.
Our future work will consist of develop-
ing a more effective algorithm for dis-
ambiguating an acyclic transducer.
References
J. Berstel. 1979. Transductions and
Context-Free Languages. Teubner
Studienbucher, Stuttgart, Germany.
G. Boulianne, J. Brousseau, P. Ouel-
let, and P. Dumouchel. 2000. French
large vocabulary recognition with
cross-word phonology transducers.
In Proceedings ICASSP 2000, June.
Istanbul, Turkey.
S. Eilenberg. 1974-1976. Automata,
Language and Machines, volume A-
B. Academic Press, New York.
R. Kaplan and M. Kay. 1994. Reg-
ular models of phonological rule
systems. Computational linguistics,
20(3):331?378.
K. Koskenniemi. 1990. Finite state
parsing and disambiguation. In Pro-
ceedings of the 13th International
Conference on Computational Lin-
guistics (COLING?90), volume 2.
Helsinki, Finland.
M. Mohri, M. Riley, D. Hindle,
A. Ljolje, and F. Pereira. 1998.
Full expansion of context-dependent
networks in large vocabulary
speech recognition. In Proceedings
of the International Conference
on Acoustics, Speech, and Signal
Proceesing(ICASSP? 98). Seattle,
Washington.
M. Mohri. 1997. Finite-state trans-
ducers in language and speech pro-
cessing. Computational linguistics,
23(2).
F. Pereira and M. Riley, 1997.
Speech recognition by composition
of weighted finite automata. Em-
manuel Roche and Yves Schabes,
Cambridge, Massachusetts, a brad-
ford book, the mit press edition.
Nasser Smaili. 2001.
De?sambigu??sation de transduc-
teurs en reconnaissance de la parole.
Universite? du Que?bec a` Montre?al.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 113?116,
Prague, June 2007. c?2007 Association for Computational Linguistics
Real-Time Correction of Closed-Captions
P. Cardinal, G. Boulianne, M. Comeau, M. Boisvert
Centre de recherche Informatique de Montreal (CRIM)
Montreal, Canada
patrick.cardinal@crim.ca
Abstract
Live closed-captions for deaf and hard of
hearing audiences are currently produced
by stenographers, or by voice writers us-
ing speech recognition. Both techniques can
produce captions with errors. We are cur-
rently developing a correction module that
allows a user to intercept the real-time cap-
tion stream and correct it before it is broad-
cast. We report results of preliminary ex-
periments on correction rate and actual user
performance using a prototype correction
module connected to the output of a speech
recognition captioning system.
1 Introduction
CRIM?s automatic speech recognition system has
been applied to live closed-captioning of french-
canadian television programs (Boulianne et al,
2006). The low error rate of our approach depends
notably on the integration of the re-speak method
(Imai et al, 2002) for a controlled acoustic environ-
ment, automatic speaker adaptation and dynamic up-
dates of language models and vocabularies, and was
deemed acceptable by several Canadian broadcast-
ers (RDS,CPAC,GTVA and TQS) who have adopted
it over the past few years for captioning sports, pub-
lic affairs and newscasts.
However, for sensitive applications where error
rates must practically be zero, or other situations
where speech recognition error rates are too high,
we are currently developing a real-time correction
interface. In essence, this interface allows a user to
correct the word stream from speech recognition be-
fore it arrives at the closed-caption encoder.
2 Background
Real-time correction must be done within difficult
constraints : with typical captioning rates of 130
words per minute, and 5 to 10% word error rate,
the user must correct between 6 and 13 errors per
minute. In addition, the process should not introduce
more than a few seconds of additional delay over the
3 seconds already needed by speech recognition.
In a previous work, (Wald et al, 2006) ex-
plored how different input modalities, such as
mouse/keyboard combination, keyboard only or
function keys to select words for editing, could re-
duce the amount of time required for correction. In
(Bateman et al, 2000), the correction interface con-
sisted in a scrolling window which can be edited by
the user using a text editor style interface. They
introduced the idea of a controllable delay during
which the text can be edited.
Our approach combines characteristics of the two
previous systems. We use a delay parameter, which
can be modified online, for controlling the output
rate. We also use the standard mouse/keyboard com-
bination for selecting and editing words. However
we added, for each word, a list of alternate words
that can be selected by a simple mouse click; this
simplifies the edition process and speeds up the cor-
rection time. However, manual word edition is still
available.
Another distinctive feature of our approach is
the fixed word position. When a word appears on
screen, it will remain in its position until it is sent
113
out. This allows the user to focus on the words
and not be distracted by word-scrolling or any other
word movement.
3 Correction Software
The correction software allows edition of the closed-
captions by intercepting them while they are being
sent to the encoder. Both assisted and manual cor-
rections can be applied to the word stream.
Assisted correction reduces the number of opera-
tions by presenting a list of alternate words, so that
a correction can be done with a simple mouse click.
Manual correction requires editing the word to be
changed and is more expensive in terms of delay.
As a consequence, the number of these operations
should be reduced to a strict minimum.
The user interface shown in figure 1 has been de-
signed with this consideration in mind. The princi-
pal characteristic of the interface is that there is no
scrolling. Words never move; instead the matrix is
filled from left to right, top to bottom, with words
coming from the speech recognition, in synchroni-
sation with the audio. When the bottom right of
the matrix is reached, filling in starts from the upper
left corner again. Words appear in blue while they
are editable, and in red once they have been sent to
the caption encoder. Thus a blue ?window?, cor-
responding to the interval during which words can
be edited, moves across the word matrix, while the
words themselves remain fixed.
For assisted correction, the list of available alter-
natives is presented in a list box under each word.
These lists are always present, instead of being pre-
sented only upon selection of a word. In this way
the user has the opportunity of scanning the lists in
advance whenever his time budget alows.
The selected word can also be deleted with a sin-
gle click. Different shortcut corrections, as sug-
gested in (Wald et al, 2006) can also be applied
depending on the mouse button used to select the
word: a left button click changes the gender (mas-
culin or feminin) of the word while a right button
click changes the plurality (singular or plural) of the
word. These available choices are in principle ex-
cluded from the list box choices.
To apply a manual correction, the user simply
clicks the word with the middle button to make it
editable; modifications are done using the keyboard.
Two users can run two correction interfaces in
parallel, on alternating sentences. This configuration
avoids the accumulation of delays. This functional-
ity may prove useful if the word rate is so high that it
becomes too difficult to keep track of the word flow.
In this mode, the second user can begin the correc-
tion of a new sentence even if the first has not yet
completed the correction of his/her sentence. Only
one out of two sentences is editable by each user.
The synchronisation is on a sentence basis.
3.1 Alternate word lists
As described in the previous section, the gen-
der/plurality forms of the word are implicitly in-
cluded and accessible through a simple left/right
mouse click. Other available forms explicitly appear
in a list box. This approach has two major benefits.
First, when a gender/plurality error is detected by the
user, no delay is incurred from scanning the choices
in the list box. Second, since the gender/plurality
forms are not included in the list box, their place be-
comes available for additional alternate words.
The main problem is to establish word lists short
enough to reduce scanning time, but long enough to
contain the correct form. For a given word output by
the speech recognition system, the alternate words
should be those that are most likely to be confused
by the recognizer.
We experimented with two pre-computed sources
of alternate word lists:
1. A list of frequently confused words was com-
puted from all the available closed-captions of
our speech recognition system for which corre-
sponding exact transcriptions exist. The train-
ing and development sets were made up of
1.37M words and 0.17M words, respectively.
2. A phoneme based confusion matrix was used
for scoring the alignment of each word of the
vocabulary with every other word of the same
vocabulary. The alignment program was an im-
plementation of the standard dynamic program-
ming technique for string alignment (Cormen
et al, 2001).
Each of these techniques yields a list of alternate
words with probabilities based on substitution like-
114
Figure 1: Real-time corrector software.
Source of alternates coverage (%)
Word confusion matrix 52%
Phoneme confusion matrix 37%
Combined 60%
Table 1: Coverage of substitutions (dev set).
lihoods. Table 1 shows how many times substitu-
tions in the development set could be corrected with
a word in the list, for each list and their combination.
To combine both lists, we take this coverage into
consideration and the fact that 48% of the words
were common to both lists. On this basis, we have
constructed an alternate list of 10 words comprised
of the most likely 7 words of case 1; the remaining 3
words are the most probable substitutions from the
remaining words of both lists.
3.2 Real-time List Update
The previous technique can only handle simple sub-
stitutions: a word that is replaced by another one.
Another frequent error in speech recognition is the
replacement of a single word by several smaller
ones. In this case, the sequence of errors contains
one substitution and one or more insertions. From
the interface point of view, the user must delete some
words before editing the last word in the sequence.
To assist the user in this case, we have imple-
mented the following procedure. When a word is
deleted by the user, the phonemes of this word are
concatenated with those of the following words. The
resulting sequence of phonemes is used to search the
dictionary for the most likely words according to the
pronunciation. These words are dynamically added
to the list appearing under the preceding word. The
search technique used is the same alignment proce-
dure implemented for computing the confusion ma-
trix based on phoneme confusion.
4 Results
In this section we present the results of two prelim-
inary experiments. In the first one, we simulated
a perfect correction, as if the user had an infinite
amount of time, to determine the best possible re-
sults that can be expected from the alternate word
lists. In the second experiment, we submitted a pro-
totype to users and collected performance measure-
ments.
4.1 Simulation Results
The simulation is applied to a test set consisting
of a 30 minute hockey game description for which
closed-captions and exact transcripts are available.
We aligned the produced closed-captions with their
corrected transcripts and replaced any incorrect
word by its correct counterpart if it appeared in the
alternate list. In addition, all insertion errors were
deleted. Table 2 shows the word error rate (WER)
115
Source of alternates WER
Original closed-captions 5.8%
Phoneme confusion matrix 4.4%
Word confusion matrix 3.1%
Combined 2.9%
Table 2: Error rate for perfect correction.
Delay
2 seconds 15 seconds
test duration 30 minutes 8 minutes
# of words 4631 1303
# of editions 21 28
WER before 6.8% 6.2%
WER after 6.1% 2.5%
Gain (relative %) 8.1% 58.7%
Table 3: Error rate after user correction.
obtained for different alternate word lists.
The word confusion matrix captures most of the
substitutions. This behavior was expected since the
matrix has been trained explicitely for that purpose.
The performance should increase in the future as the
amount of training data grows. In comparison, the
contribution of words from the phoneme confusion
matrix is clearly limited.
The corrected word was the first in the list 35%
of the time, while it was in the first three 59% of
the time. We also simulated the effect of collaps-
ing words in insertion-substitution sequences to al-
low corrections of insertions : the increase in perfor-
mance was less than 0.5%.
4.2 User Tests
Experiments were performed by 3 unacquainted
users of the system on hockey game descriptions.
In one case, we allowed a delay of 15 seconds; the
second case allowed a 2 second delay to give a pre-
liminary assessment of user behavior in the case
of minimum-delay real-time closed-captioning. Ta-
ble 3 shows the error rate before and after correction.
The results show that a significant WER decrease
is achieved by correcting using a delay of 15 sec-
onds. The reduction with a 2 second delay is minor;
with appropriate training, however, we can expect
the users to outperform these preliminary results.
5 Conclusion and Future Work
We are currently developing a user interface for cor-
recting live closed-captions in real-time. The inter-
face presents a list of alternatives for each automati-
cally generated word. The theoretical results that as-
sumes the user always chooses the correct suggested
word shows the potential for large error reductions,
with a minimum of interaction. When larger delays
are allowed, manual edition of words for which there
is no acceptable suggested alternative can yield fur-
ther improvements.
We tested the application for real-time text cor-
rection produced in a real-world application. With
users having no prior experience and with only a 15
second delay, the WER dropped from 6.1% to 2.5%.
In the future, users will be trained on the system
and we expect an important improvement in both
accuracy and required delay. We will also experi-
ment the effect of running 2 corrections in parallel
for more difficult tasks. Future work also includes
the integration of an automatic correction tool for
improving or highlighting the alternate word list.
References
A. Bateman, J. Hewitt, A. Ariyaeeinia, P. Sivakumaran,
and A. Lambourne. 2000. The Quest for The Last
5%: Interfaces for Correcting Real-Time Speech-
Generated Subtitles Proceedings of the 2000 Confer-
ence on Human Factors in Computing Systems (CHI
2000), April 1-6, The Hague, Netherlands.
T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein
2001. Introduction to Algorithms second edition, MIT
Press, Cambridge, MA.
G. Boulianne, J.-F. Beaumont, M. Boisvert, J. Brousseau,
P. Cardinal, C. Chapdelaine, M.Comeau, P. Ouellet,
and F. Osterrath. 2006. Computer-assisted closed-
captioning of live TV broadcasts in French Proceed-
ings of the 2006 Interspeech - ICSLP, September 17-
21, Pittsburg, US.
T. Imai, A. Matsui, S. Homma, T. Kobayakawa, O.
Kazuo, S. Sato, and A. Ando 2002. Speech Recogni-
tion with a respeak method for subtiling live broadcast
Proceedings of the 2002 ICSLP, September 16-20, Or-
lando, US.
Wald, M. 2006 Creating Accessible Educational Multi-
media through Editing Automatic Speech Recognition
Captioning in Real Time. International Journal of In-
teractive Technology and Smart Education : Smarter
Use of Technology in Education 3(2) pp. 131-142
116
