Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Construction of an Objective Hierarchy of Abstract Concepts           
via Directional Similarity  
Kyoko Kanzaki  Eiko Yamamoto Hitoshi Isahara 
Computational Linguistics Group, 
National Institute of Information and Communications 
Technology 
3-5 Hikari-dai, Seika-cho, Souraku-gun, Kyoto, Japan,  
{kanzaki, eiko, isahara}@nict.go.jp 
Qing Ma 
Faculty of Science  
and Technology 
Ryukoku University 
Seta, Otsu,520-2194, Japan 
qma@math.ryukoku.ac.jp
Abstract 
The method of organization of word mean-
ings is a crucial issue with lexical databases. 
Our purpose in this research is to extract word 
hierarchies from corpora automatically. Our 
initial task to this end is to determine adjec-
tive hyperonyms. In order to find adjective 
hyperonyms, we utilize abstract nouns. We 
constructed linguistic data by extracting se-
mantic relations between abstract nouns and 
adjectives from corpus data and classifying 
abstract nouns based on adjective similarity 
using a self-organizing semantic map, which 
is a neural network model (Kohonen 1995). 
In this paper we describe how to hierarchi-
cally organize abstract nouns (adjective hy-
peronyms) in a semantic map mainly using 
CSM. We compare three hierarchical organi-
zations of abstract nouns, according to CSM, 
frequency (Tf.CSM) and an alternative simi-
larity measure based on coefficient overlap, to 
estimate hyperonym relations between words. 
1. Introduction 
A lexical database is necessary for computers, 
and even humans, to fully understand a word's 
meaning because the lexicon is the origin of lan-
guage understanding and generation. Progress is 
being made in lexical database research, notably 
with hierarchical semantic lexical databases such 
as WordNet, which is used for NLP research 
worldwide.  
When compiling lexical databases, it is impor-
tant to consider what rules or phenomena should 
be described as lexical meanings and how these 
lexical meanings should be formalized and stored 
electronically. This is a common topic of discus-
sion in computational linguistics, especially in 
the domain of computational lexical semantics. 
The method of organization of word meanings 
is also a crucial issue with lexical databases. In 
current lexical databases and/or thesauri, abstract 
nouns indicating concepts are identified manually 
and words are classified in a top-down manner 
based on human intuition. This is a good way to 
make a lexical database for users with a specific 
purpose. However, word hierarchies based on 
human intuition tend to vary greatly depending 
on the lexicographer, and there is often dis-
agreement as to the make-up of the hierarchy. If 
we could find an objective method to organize 
word meanings based on real data, we would 
avoid this variability. 
Our purpose in this research is to extract word 
hierarchies from corpora automatically. Our ini-
tial task to this end is to determine adjective hy-
peronyms. In order to find adjective hyperonyms, 
we utilize abstract nouns. Past linguistic research 
has focused on classifying the semantic relation-
ship between abstract nouns and adjectives 
(Nemoto 1969, Takahashi 1975).  
We constructed linguistic data by extracting 
semantic relations between abstract nouns and 
adjectives from corpus data and classifying ab-
stract nouns based on adjective similarity using a 
self-organizing semantic map (SOM), which is a 
neural network model (Kohonen 1995). The rela-
tive proximity of words in the semantic map in-
dicates their relative similarity.  
In previous research, word meanings have 
been statistically modeled based on syntactic in-
formation derived from a corpus. Hindle (1990) 
used noun-verb syntactic relations, and Hatzivas-
siloglou and McKeown (1993) used coordinated 
adjective-adjective modifier pairs. These meth-
ods are useful for the organization of words deep 
within a hierarchy, but do not seem to provide a 
solution for the top levels of the hierarchy.  
To find an objective hierarchical word struc-
ture, we utilize the complementary similarity 
measure (CSM), which estimates a one-to-many 
relation, such as superordinate?subordinate rela-
tions (Hagita and Sawaki 1995, Yamamoto and 
Umemura 2002).  
In this paper we propose an automated method 
for constructing adjective hierarchies by connect-
ing strongly related abstract nouns in a top-down 
fashion within a semantic map, mainly using 
CSM. We compare three hierarchical organiza-
tions of abstract nouns, according to CSM, fre-
quency (Tf.CSM) and an alternative similarity 
measure based on coefficient overlap, to estimate 
hyperonym relations between words. 
2. Linguistic clues to extract adjective hy-
peronyms from corpora 
In order to automatically extract adjective hy-
peronyms we use syntactic and semantic relations 
between words.  
There is a good deal of linguistic research fo-
cused on the syntactic and semantic functions of 
abstract nouns, including Nemoto (1969), Taka-
hashi (1975), and Schmid (2000). Takahashi 
(1975) illustrated the sentential function of ab-
stract nouns with the following examples. 
a.  Yagi  wa  seishitsu  ga  otonashii. 
(goat) topic (nature) subject (gentle) 
        The nature of goats is gentle 
b.   Zou    wa   hana   ga     nagai. 
    (elephant) topic  (a nose) subject  (long) 
         The nose of an elephant is long 
He examined the differences in semantic func-
tion between ?seishitsu (nature)? in (a) and ?hana 
(nose)? in (b), and explained that ?seishitsu (na-
ture)? in (a) indicates an aspect of something, i.e., 
the goat, and ?hana (nose)? in (b) indicates part 
of something, i.e., the elephant. He recognized 
abstract nouns in (a) as a hyperonym of the at-
tribute that the predicative adjectives express. 
Nemoto (1969) identified expressions such as 
?iro ga akai (the color is red)? and ?hayasa ga 
hayai (the speed is fast)? as a kind of meaning 
repetition, or tautology.  
In this paper we define such abstract nouns 
that co-occur with adjectives as adjective hy-
peronyms. We semi-automatically extracted from 
corpora 365 abstract nouns used as this kind of 
head noun, according to the procedures described 
in Kanzaki et al (2000). We collected abstract 
nouns from two year's worth of articles from the 
Mainichi Shinbun newspaper, and extracted ad-
jectives co-occurring with abstract nouns in the 
manner of (a) above from 100 novels, 100 essays 
and 42 year's worth of newspaper articles, includ-
ing 11 year's worth of Mainichi Shinbun articles, 
10 year's worth of Nihon Keizai Shinbun (Japa-
nese economic newspaper) articles, 7 year's wor-
th of Sangyoukinyuuryuutsu Shinbun (an eco-
nomic newspaper) articles, and 14 year's worth of 
Yomiuri Shinbun articles. The total number of 
abstract noun types is 365, the number of adjec-
tive types is 10,525, and the total number of ad-
jective tokens is 35,173. The maximum number 
of co-occurring adjectives for a given abstract 
noun is 1,594. 
3. On the Self-Organizing Semantic Map  
3.1  Input data 
Abstract nouns are located in the semantic map 
based on the similarity of co-occurring adjectives 
after iteratively learning over input data. 
In this research, we focus on abstract nouns 
co-occurring with adjectives. In the semantic 
map, there are 365 abstract nouns co-occurring 
with adjectives. The similarities between the 365 
abstract nouns are determined according to the 
number of common co-occurring adjectives. We 
made a list such as the following. 
OMOI (feeling): ureshii (glad), kanashii (sad), 
shiawasena (happy), ? 
KIMOCHI (though): ureshii (glad), tanoshii (pleased), 
hokorashii (proud), ? 
KANTEN (viewpoint): igakutekina (medical), 
rekishitekina (historical), ... 
When two (or more) sets of adjectives with 
completely different characteristics co-occur with 
an abstract noun and the meanings of the abstract 
noun can be distinguished correspondingly, we 
treat them as two different abstract nouns. For 
example, the Japanese abstract noun ?men? is 
treated as two different abstract nouns with 
?men1? meaning ?one side (of the characteristics 
of someone or something)? and ?men2? meaning 
?surface?. The former co-occurs with ?gentle?, 
?kind? and so on. The latter co-occurs with 
?rough?, ?smooth? and so on. 
3.2  The Self-Organizing Semantic Map 
Ma (2000) classified co-occurring words using 
a self-organizing semantic map (SOM). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   
We made a semantic map of the above-
mentioned 365 abstract nouns using SOM, based 
on the cosine measure. The distribution of the 
words in the map gives us a sense of the semantic 
distribution of the words. However, we could not 
precisely identify the relations between words in 
the map (Fig 1). In Fig. 1 lines on the maps indi-
cate close relations between word pairs. In the 
cosine-based semantic map, there is no clear cor-
respondence between word similarities and the 
distribution of abstract nouns in the map.    
To solve this problem we introduced the 
complementary similarity measure (CSM). This 
similarity measure estimates one-to-many 
relations, such as superordinate?subordinate 
relations (Hagita and Sawaki 1995, Yamamoto 
and Umemura 2002). We can find the 
hierarchical distribution of words in the semantic 
map according to the value of CSM (Fig 2). In 
the CSM-based SOM, lines are concentrated at 
the bottom right hand corner, that is, most ab-
stract nouns are located at the bottom right-hand 
corner.  
Next, we find hierarchical relations between 
whole abstract nouns, not between word pairs, on 
the map automatically. 
4. How to construct hierarchies of nominal 
adjective hyperonyms in the Semantic 
Map 
4.1 Similarity measures, CSM and Yates? 
correction 
A feature of CSM is its ability to estimate hi-
erarchical relations between words. This similar-
ity measure was developed for the recognition of 
degraded machine-printed text (Hagita and Sa-
waki, 1995). Yates? correction is often used in 
order to increase the accuracy of approximation. 
Hierarchical relations can be extracted accurately 
when the CSM value is high. Yates? correction 
can extract different relations from high CSM 
values. When the CSM value is low, the result is 
not reliable, in which case we use Yates? correc-
tion. 
According to Yamamoto and Umemura (2002), 
who adopted CSM to classify words, CSM is cal-
culated as follows. 
))(( dbca
bcadCSM
++
?
=  
Yates? correction is calculated as follows. 
))()()((
)2/|(| 2
dbcadcba
nbcadnYates
++++
??
=  
Here n is the sum of the number of co-
occurring adjectives; a indicates the number of 
times the two labels appear together; b indicates 
the number of times ?label 1? occurs but ?label 
2? does not; c is the number of times ?label 2? 
occurs but ?label 1? does not; and d is the num-
ber of times neither label occurs. In our research, 
each ?label? is an abstract noun, a indicates the 
number of adjectives co-occurring with both ab-
stract nouns, b and c indicate the number of ad-
jectives co-occurring with either abstract noun 
Figure 1. The Cosine-based SOM of word similarity Figure 2. The CSM-based SOM of word similarity
(?label 1? and ?label 2?, respectively), and d in-
dicates the number of adjectives co-occurring 
with neither abstract noun. We calculated hierar-
chical relations between word pairs using these 
similarity measures. 
4.2 Construction of a hierarchy of abstract 
nouns using CSM and Yates' correc-
tion 
The hierarchy construction process is as fol-
lows: 
1) Based on the results of CSM, ?koto (mat-
ter)? is the hyperonym of all abstract nouns. 
First, we connect super/sub-ordinate words 
with the highest CSM value while keeping the 
super-subordinate relation.  
2) When the normalized value of CSM is 
lower, the number of extracted word pairs be-
comes increasing overwhelmingly, and the reli-
ability of CSM diminishes. Word pairs with a 
normalized CSM value of less than 0.4 are lo-
cated far from the common hyperonym ?koto 
(matter)? on the semantic map. If we construct a 
hierarchy using CSM values only, a long hierar-
chy containing irrelevant words emerges. In this 
case, the word pairs calculated by Yates' correc-
tion are more accurate than those from CSM. We 
combine words using Yates? correction, when the 
value of CSM is less than 0.4. When we connect 
word pairs with a high Yates? value, we find a 
hyperonym of the super-ordinate noun of the pair 
and connect the pair to the hyperonym. If a word 
pair appears only in the Yates' correction data, 
that is, we cannot connect the pair with high 
Yates? value to the hyperonym with high CSM 
value, they are combined with ?koto (matter)?. 
3) Finally, if a short hierarchy is contained in a 
longer hierarchy, it is merged with the longer 
hierarchy and we insert ?koto (matter)? at the 
root of all hierarchies. 
4.3  Results 
The number of groups obtained was 161. At its 
deepest, the hierarchy was 15 words deep, and at 
its shallowest, it was 4 words deep. The 
following is a breakdown of the number of 
groups at different depths in the hierarchy.  
The greatest concentration of groups is at 
depth 7. There are 140 groups from depth 5 to 
depth 10, which is 87% of all groups. 
 
 
 
 
 
 
 
The word that has the strongest relation with 
?koto (matter)? is ?men1 (side1)?. The number of 
groups in which ?koto (matter)? and ?men1 
(side1)? are hyperonyms is 96 (59.6%). The larg-
est number of groups after that is a group in 
which ?koto (matter)?, ?men1 (side1)? and 
?imeeji (image)? are hyperonyms. The number of 
groups in this case is 59 groups, or 36.6% of the 
total. With respect to the value of CSM, the co-
occurring adjectives are similar to ?men1 (side1)? 
and ?imeeji (image)?.  
Other words that have a direct relation with 
?koto (matter)? are ?joutai (state)? and ?toki 
(when)?. They have the most number of groups 
after ?men1 (side1)? among all the children of 
?koto (matter)?. The number of groups subsumed 
by ?joutai (state)? group and ?toki (when)? are 21 
and 19, respectively. Other direct hyponyms of 
?koto (matter)? are: 
ki (feeling): 6 groups  
ippou (while or grow ?er and er): 3 groups  
me2 (eyes): 3 groups  
katachi1 (in the form of): 3 groups  
iikata (how to say): 2 groups  
yarikata (how to): 2 groups 
There is little hierarchical structure to these 
groups, as they co-occur with few adjectives. 
4.4 The Hierarchies of abstract concepts in 
the semantic map 
In the following semantic maps, where abstract 
nouns are distributed using SOM and CSM (see 
Section 3), hierarchies of abstract nouns are 
drawn with lines. The bottom right hand corner is 
?koto (matter)?, a starting point for the distribu-
tion of abstract nouns.  
Five main types of hierarchies are found from 
patterns of lines on the map, as follows: 
The first figure, Fig.3, is hierarchies of ?kanji 
(feeling), kimochi (feeling) ?? on the semantic 
map. The location of hierarchies of ?yousu (as-
pect), omomochi (look), kaotsuki (on one?s face), 
?? is similar to this type of the location. Hierar-
chies of ?sokumen (one side), imi (meaning), 
kanten (viewpoint),  kenchi (standpoint) ?? on 
Depth 4 5 6 7 8 9 
Groups 3 16 27 32 23 23 
Depth 10 11 12 13 14 15 
Groups 19 7 3 4 3 1 
Table 1: The depth of the hierarchy by CSM
  
 
 
 
 
 
the map are shown in Fig. 4. The lines of the hi-
erarchies go up from the bottom right hand cor-
ner to the upper left hand corner and then turn 
towards the upper right hand corner. The loca-
tion of hierarchies of ?nouryoku (ability), sainou 
(talent) ?? is similar to this one. 
The hyperonym of ?teido (degree)? is ?joutai 
(state)?. In Fig.5 these abstract nouns are located 
at the bottom of the map. The location of hierar-
chies of ?kurai (rather than)? and ?hou (compara-
tively)? are similar to this one. The hierarchies of 
?joutai (state), joukyou (situation), yousou (as-
pect), jousei (the state of affairs)? are shown in 
Fig.6. The lines are found at a higher location 
than the line of ?teido(degree)?. The lines of the 
hierarchies of ?joutai (state), ori (when), sakari 
(in the hight of), sanaka (while)? are similar to 
these lines. 
The lines of the hierarchies of ?seikaku (char-
acter)?, ?gaikan (appearance)?and ?utsukushisa 
(beauty)? are similar to each other. We show the 
hierarchies of ?seikaku (character)? in Fig.7. The-
se lines in Fig.7 are located from the right end to 
the upper left hand corner. From the following, 
we can find five main types of hierarchies. 
From the starting point ? koto (matter)?, 
-The hierarchies of ?men (side), inshou (impres-
sion), kanji (feeling), kibun (mood), kimochi 
(feeling)? 
-The hierarchies of ?men (side), sokumen (one- 
side), imi (meaning), kanten (viewpoint), kenchi 
(standpoint)? 
-The hierarchies of ?joutai (state), teido (degree)? 
-The hierarchies of ?joutai (state), jousei (situa-
tion)?  
-The hierarchies of ?men (side), inshou (impres-
sion), seikaku (character) or gaikan (appear-
ance) or utsukushisa (beauty)?.  
The lines in Fig.8 are not peculiar, and appear 
in an area of the hierarchies of ?seikaku (charac-
Fig.3: Hierarchies of  
?kimochi (feeling)? 
Fig.4:Hierarchies of 
?sokumen (one side)? 
Fig.5:Hierarchies of 
?teido (degree)? 
Fig8: Hierarchies of 
?kanshoku (feel)? 
Fig.6:  Hierarchies of  
?jousei (situation)? 
Fig.7:Hierarchies of 
?seikaku (character)? 
ter)? in Fig.7. As Fig.8 shows, the hierarchies of 
?men (side), inshou (impression), kanji (feeling), 
kanshoku (feel) or kansei (sensitivity)? are lo-
cated in the area of the hierarchies of ?seikaku 
(character)?, above the hierarchies of ?kimochi 
(feeling)? in Fig.3. 
5. Comparison of hierarchies of super-
ordinate nouns of adjectives. 
We compare the hierarchy mentioned above 
with ones obtained from two kinds of data. 
1) Hierarchies obtained by: 
CSM and Yate?s correction 
corpus occurrence data (no frequency). 
2) Hierarchies obtained by: 
Tf.CSM and Yate?s correction 
corpus frequency data. 
3) Hierarchies obtained by: 
Overlap coefficient and Yates' correction 
corpus occurrence data (no frequency). 
 
As both CSM and the Overlap coefficient are 
?measures of inclusion?, we compared CSM and 
Tf.CSM with the Overlap coefficient. 
The number of groups that were obtained by 
CSM, Tf.CSM and the Overlap coefficient are 
the following. 
Table 2. Total number of groups obtained from CSM, 
Tf.CSM and Ovlp (Overlap) 
 groups 
CSM 161 
Tf.CSM 158 
Ovlp 240 
The Depth of hierarchies obtained from CSM, 
Tf.CSM, and the Overlap coefficient are as fol-
lows: 
Table 3. The hierarchy depth for CSM, Tf.CSM,  
and the Overlap coefficient 
 
In the case of CSM, there are 32 groups at 
depth 7, which is the greatest number of groups. 
The greatest concentration of groups is at depth 5 
to 10. In the case of Tf.CSM, the greatest number 
of groups is 25 at depth 8. The greatest concen-
tration of groups is at depth 5 to 13. In the case of 
the overlap coefficient, the greatest number of 
groups is 61 at depth 5. The greatest concentra-
tion of groups is at depth 3 to 7. 
0
10
20
30
40
50
60
70
3 4 5 6 7 8 9 10 11 12 13 14 15
CSM
Tf.CSM
Ovlp
 
 
 
From this result, we can see that hierarchies 
generated by Tf.CSM are relatively deep, and 
those generated by the Overlap coefficient are 
relatively shallow.  
In the case of the Overlap coefficient, abstract 
nouns in lower layers are sometimes directly re-
lated to abstract nouns in the highest layers. On 
the other hand, in hierarchies generated by CSM 
and Tf.CSM, abstract nouns in the highest layers 
are related to those in the lowest layers via ab-
stract nouns in the middle layers. The following 
indicates the number of overlapping hierarchies 
for CSM, Tf.CSM and Overlap. 
Table 4. The number of overlapping hierarchies 
among CSM, Tf.CSM and Overlap 
CSM&Tf.CSM 37 
CSM&Ovlp 7 
Tf.CSM&Ovlp 2 
CSM&Tf.CSM&Ovlp 7 
The hierarchy generated by Tf.CSM is the 
deepest, and includes some hierarchies generated 
by CSM and the Overlap coefficient. The hierar-
chy generated by CSM is more similar to the one 
made by Tf.CSM than that for the Overlap coef-
ficient: the number of completely corresponding 
hierarchies for CSM and Tf.CSM is 37, that for 
CSM and the Overlap coefficient is 7, and that 
for Tf.CSM and the Overlap coefficient is 2. The 
total number of hierarchies that correspond com-
pletely between CSM, Tf.CSM and the Overlap 
coefficient is 7, and the number of hierarchies 
which are generated by two of the methods and 
included in the third is 57. 
depth 3 4 5 6 7 8 9
CSM 0 3 16 27 32 23 23
Tf.CSM 1 5 10 18 13 25 11
Ovlp 32 56 61 57 21 7 2
depth 10 11 12 13 14 15 
CSM 19 7 3 4 3 1 
Tf.CSM 24 13 14 14 7 2 
Ovlp 2 0 0 0 0 0 
Figure 9. Distribution of hierarchy depth for CSM, 
Tf.CSM, and Overlap coefficient 
We investigated these 64 hierarchies precisely, 
checking adjectives appearing at each depth as 
indicated by an abstract noun in this paper.  In 6 
of these hierarchies, the same adjectives were 
found at all levels of the hierarchy. In 14 of the 
remaining 58 hierarchies, the same adjectives 
were found in all but the deepest level.  These 
20 hierarchies are the most plausible in the strict 
sense of the word. Below, we give examples of 
these hierarchies. In the next stage of this re-
search, we intend to investigate the remaining 44 
hierarchies to determine the reason for the differ-
ence in adjective content. 
The common hyperonym: koto (matter) --- 
men1 (side) --- 
sokumen (one side) --- 
imi (meaning) --- 
kanten (viewpoint) --- 
me2 (eyes) --- 
mikata (view) --- 
hyouka (evaluation) --- 
ippou (while or grow -er and er) --- 
ikioi (force) --- 
sokudo (speed) --- 
jikoku (time) --- 
6. Conclusion 
We have suggested how to make a hierarchy 
of adjectives automatically by connecting 
strongly-related abstract nouns in a top-down 
fashion. We generated a word hierarchy from 
corpus data by using a combination of two 
methods: a self-organizing semantic map and a 
directional similarity measure. As our directional 
similarity measure, we utilized the complement-
ary similarity measure (CSM). Then we com-
pared the hierarchy generated by CSM with that 
generated by Tf.CSM and the Overlap coefficient. 
In the case of Tf.CSM, the hierarchy is deeper 
than the others because there are more abstract 
nouns in the middle layer. In the case of the 
Overlap coefficient, the hierarchy is shallow, but 
there are more hyponyms in the lower layer than 
with the other two methods. As a result, the 
hierarchies generated by CSM have more com-
mon hierarchical relations than those generated 
by the other two methods. In future work, we will 
analyze common hierarchies made by the three 
methods in detail and examine differences among 
them in order to generate an abstract conceptual 
hierarchy of adjectives. We will then compare 
our hierarchy with thesauri compiled manually. 
After we have completed the experiment on Jap-
anese adjectives, we are keen to investigate dif-
ferences and similarities in adjective hypero-
nyms between Japanese and other languages such 
as English by means of our method. 
Acknowledgement 
We would like to thank Dr. Masaki Murata of 
NICT for allowing us to use his drawing tool. 
References  
Nemoto, K. 1969. The combination of the noun with 
?ga-Case? and the adjective, Language research2 
for the computer, National Language Research In-
stitute: 63-73/ 
Takahashi, T. 1975. A various phase related to the 
part-whole relation investigated in the sentence, 
Studies in the Japanese language 103, The society 
of Japanese Linguistics: 1-16. 
Kohonen, T. 1995. Self-Organizing Maps, Springer. 
Hindle, D. 1990. Noun Classification From Predicate-
Argument Structures, In the Proceedings of the 28th 
Annual Meeting of the Association for Computa-
tional Linguistics: 268-275 
Hatzivassiloglou,V. and McKeown,R.K. 1993. To-
wards the Automatic Identification of Adjectival 
Scales: Clustering Adjectives According to Mean-
ing, In the Proceedings of the 31st Annual Meeting 
of the Association for Computational Linguistics: 
172-182.  
Hagita, N. and Sawaki, M. 1995. Robust Recognition 
of Degraded Machine-Printed Characters using 
Complimentary Similarity Measure and Error-
Correction Learning?In the Proceedings of the 
SPIE ?The International Society for Optical Engi-
neering, 2442: 236-244. 
Yamamoto, E. and Umemura, K. 2002. A Similarity 
Measure for estimation of One?to-Many Relation-
ship in Corpus, Journal of Natural Language Proc-
essing: 45-75.  
Hans-Jorg Shmid. 2000. English Abstract Nouns as 
Conceptual Shells, Mouton de Gruyter. 
Kanzaki, K., Ma., Q. and Isahara, H. (2000), Similari-
ties and Differences among Semantic Behaviors of 
Japanese Adnominal Constituents, In the Proceed-
ings of the Syntactic and Semantic Complexity in 
Natural Language Processing Systems, ANLP and 
NAACL. 
Ma, Q., Kanzaki, K., Murata, M., Uchimoto, K. and 
Isahara, H. 2000. Self-Organization Semantic Maps 
of Japanese Noun in Terms of Adnominal Constitu-
ents, In Proceedings of IJCNN?2000, Como, Italy, 
vol.6.: 91-96. 
Hierarchy Extraction based on Inclusion of Appearance  
Eiko Yamamoto Kyoko Kanzaki Hitoshi Isahara 
Computational Linguistics Group, 
National Institute of Information and Communications Technology 
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan. 
eiko@nict.go.jp kanzaki@nict.go.jp isahara@nict.go.jp 
 
Abstract 
In this paper, we propose a method of auto-
matically extracting word hierarchies based on 
the inclusion relation of appearance patterns 
from corpora. We apply a complementary 
similarity measure to find a hierarchical word 
structure. This similarity measure was devel-
oped for the recognition of degraded machine-
printed text in the field and can be applied to 
estimate one-to-many relations. Our purpose is 
to extract word hierarchies from corpora 
automatically. As the initial task, we attempt 
to extract hierarchies of abstract nouns co-
occurring with adjectives in Japanese and 
compare with hierarchies in the EDR elec-
tronic dictionary.  
1 Introduction 
The hierarchical relations of words are useful as 
language resources. Hierarchical semantic lexical 
databases such as WordNet (Miller et al, 1990) 
and the EDR electronic dictionary (1995) are used 
for NLP research worldwide to fully understand a 
word meaning. In current thesauri in the form of 
hierarchical relations, words are categorized manu-
ally and classified in a top-down manner based on 
human intuition. This is a good way to make a 
lexical database for users having a specific purpose. 
However, word hierarchies based on human intui-
tion tend to vary greatly depending on the lexicog-
rapher. In addition, hierarchical relations based on 
various data may be needed depending on each 
user.  
Accordingly, we try to extract a hierarchical re-
lation of words automatically and statistically. In 
previous research, ways of extracting from defini-
tion sentences in dictionaries (Tsurumaru et al, 
1986; Shoutsu et al, 2003) or from a corpus by 
using patterns such as ?a part of?, ?is-a?, or ?and? 
(Berland and Charniak, 1999; Caraballo, 1999) 
have been proposed. Also, there is a method that 
uses the dependence relation between words taken 
from a corpus (Matsumoto et al, 1996). In contrast, 
we propose a method based on the inclusion rela-
tion of appearance patterns from corpora. 
In this paper, to verify the suitability of our 
method, we attempt to extract hierarchies of ab-
stract nouns co-occurring with adjectives in Japa-
nese. We select two similarity measures to estimate 
the inclusion relation between word appearance 
patterns. One is a complementary similarity meas-
ure; i.e., a similarity measure developed for the 
recognition of degraded machine-printed text in the 
field (Hagita and Sawaki, 1995). This measure can 
be used to estimate one-to-many relations such as 
superordinate?subordinate relations from appear-
ance patterns (Yamamoto and Umemura, 2002). 
The second similarity measure is the overlap coef-
ficient, which is a similarity measure to calculate 
the rate of overlap between two binary vectors. 
Using each measure, we extract hierarchies from a 
corpus. After that, we compare these with the EDR 
electronic dictionary. 
2 Experiment Corpus 
A good deal of linguistic research has focused on 
the syntactic and semantic functions of abstract 
nouns (Nemoto, 1969; Takahashi, 1975; Schmid, 
2000; Kanzaki et al, 2003). In the example, ?Yagi 
(goat) wa seishitsu (nature) ga otonashii (gentle) 
(The nature of goats is gentle).?, Takahashi (1975) 
recognized that the abstract noun ?seishitsu (na-
ture)? is a hypernym of the attribute that the predi-
cative adjective ?otonashi (gentle)? expresses. 
Kanzaki et al (2003) defined such abstract nouns 
that co-occur with adjectives as adjective hy-
pernyms, and extracted these co-occurrence rela-
tions between abstract nouns and adjectives from 
many corpora such as newspaper articles. In the 
linguistic data, there are sets of co-occurring 
adjectives for each abstract noun ? the total num-
ber of abstract noun types is 365 and the number of 
adjective types is 10,525. Some examples are as 
follows.  
OMOI  (feeling): ureshii (glad), kanashii (sad), 
shiawasena (happy), ? 
KANTEN (viewpoint): igakutekina (medical), 
rekishitekina (historical), ... 
3 Complementary Similarity Measure 
The complementary similarity measure (CSM) is 
used in a character recognition method for binary 
images which is robust against heavy noise or 
graphical designs (Sawaki and Hagita, 1996). Ya-
mamoto et al (2002) applied CSM to estimate one-
to-many relations between words. They estimated 
one-to-many relations from the inclusion relations 
between the appearance patterns of two words.  
The appearance pattern is expressed as an n-
dimensional binary feature vector. Now, let F = (f1, 
f2, ?, fn) and T = (t1, t2, ?, tn) (where fi, ti = 0 or 
1) be the feature vectors of the appearance patterns 
for a word and another word, respectively. The 
CSM of F to T is defined as 
dcban
tfdtfc
tfbtfa
dbca
bcadTFCSM
n
i ii
n
i ii
n
i ii
n
i ii
+++=
???=??=
??=?=
++
?=
??
??
==
==
,)1()1(,)1(
,)1(,
))((
),(
11
11
 
The CSM of F to T represents the degree to 
which F includes T; that is, the inclusion relation 
between the appearance patterns of two words.  
In our experiment, each ?word? is an abstract 
noun. Therefore, n is the number of adjectives in 
the corpus, a indicates the number of adjectives co-
occurring with both abstract nouns, b and c indi-
cate the number of adjectives co-occurring with 
either abstract noun, and d indicates the number of 
adjectives co-occurring with neither abstract noun. 
4 Overlap Coefficient 
The overlap coefficient (OVLP) is a similarity 
measure for binary vectors (Manning and Schutze, 
1999). OVLP is essentially a measure of inclusion. 
It has a value of 1.0 if every dimension with a non-
zero value for the first vector is also non-zero for 
the second vector or vice versa. In other words, the 
value is 1.0 when the first vector completely in-
cludes the second vector or vice versa. OVLP of F 
and T is defined as 
),(),(
),(
cabaMIN
a
TFMIN
TF
TFOVLP ++==
I
 
5 EDR hierarchy 
The EDR Electronic Dictionary (1995) was de-
veloped for advanced processing of natural lan-
guage by computers and is composed of eleven 
sub-dictionaries. The sub-dictionaries include a 
concept dictionary, word dictionaries, bilingual 
dictionaries, etc. We verify and analyse the hierar-
chies that are extracted based on a comparison with 
the EDR dictionary. However, the hierarchies in 
EDR consist of hypernymic concepts represented 
by sentences. On the other hand, our extracted hi-
erarchies consist of hypernyms such as abstract 
nouns. Therefore, we have to replace the concept 
composed of a sentence with the sequence of the 
words. We replace the description of concepts with 
entry words from the ?Word List by Semantic 
Principles? (1964) and add synonyms. We also add 
to abstract nouns in order to reduce any difference 
in representation. In this way, conceptual hierar-
chies of adjectives in the EDR dictionary are de-
fined by the sequence of words. 
6 Hierarchy Extraction Process 
The processes for hierarchy extraction from the 
corpus are as follows. ?TH? is a threshold value for 
each pair under consideration. If TH is low, we can 
obtain long hierarchies. However, if TH is too low, 
the number of word pairs taken into consideration 
increases overwhelmingly and the measurement 
reliability diminishes. In this experiment, we set 
0.2 as TH. 
1. Compute the similarity between appear-
ance patterns for each pair of words. The 
hierarchical relation between the two 
words in a pair is determined by the simi-
larity value. We express the pair as (X, Y), 
where X is a hypernym of Y and Y is a 
hyponym of X. 
2. Sort the pairs by the normalized similari-
ties and reduce the pairs where the simi-
larity is less than TH.  
3. For each abstract noun, 
A) Choose a pair (B, C) where word B is 
the hypernym with the highest value. 
The hierarchy between B and C is set 
to the initial hierarchy.  
B) Choose a pair (C, D) where hyponym 
D is not contained in the current hier-
archy and has the highest value in pairs 
where the last word of the current hier-
archy C is a hypernym. 
C) Connect hyponym D with the tail of 
the current hierarchy.  
D) While such a pair can be chosen, repeat 
B) and C). 
E) Choose a pair (A, B) where hypernym 
A is not contained in the current hier-
archy and has the highest value in pairs 
where the first word of the current hi-
erarchy B is a hypernym. 
F) Connect hypernym A with the head of 
the current hierarchy. 
G) While such a pair can be chosen, repeat 
E) and F). 
4. For the hierarchies that are built, 
A) If a short hierarchy is included in a 
longer hierarchy with the order of the 
words preserved, the short one is 
dropped from the list of hierarchies. 
B) If a hierarchy has only one or a few 
different words from another hierarchy, 
the two hierarchies are merged. 
7 Extracted Hierarchy 
Some extracted hierarchies are as follows. In our 
experiment, we get koto (matter) as the common 
hypernym.  
koto (matter) -- joutai (state) -- kankei (relation) 
-- kakawari (something to do with) -- tsukiai 
(have an acquaintance with) 
koto (matter) -- toki (when) -- yousu (aspect) -- 
omomochi (one?s face) -- manazashi (a look) -- 
iro (on one?s face) -- shisen (one?s eye) 
8 Comparison 
We analyse extracted hierarchies by using the 
number of nodes that agree with the EDR hierar-
chy. Specifically, we count the number of nodes 
(nouns) which agree with a word in the EDR hier-
archy, preserving the order of each hierarchy. Here, 
two hierarchies are ?A - B - C - D - E? and ?A - B 
- D - F - G.? They have three agreement nodes; ?A 
- B - D.?  
Table 1 shows the distribution of the depths of a 
CSM hierarchy, and the number of nodes that 
agree with the EDR hierarchy at each depth. Table 
2 shows the same for an OVLP one. ?Agreement 
Level? is the number of agreement nodes. The bold 
font represents the number of hierarchies com-
pletely included in the EDR hierarchy.  
8.1 Depth of Hierarchy 
The number of hierarchies made from the EDR 
dictionary (EDR hierarchy) is 932 and the deepest 
level is 14. The number of CSM hierarchies is 105 
and the depth is from 3 to 14 (Table 1). The num-
ber of OVLP hierarchies is 179 and the depth is 
from 2 to 9 (Table 2). These results show that 
CSM builds a deeper hierarchy than OVLP, though 
the number of hierarchies is less than OVLP. Also, 
the deepest level of CSM equals that of EDR. 
Therefore, comparison with the EDR dictionary is 
an appropriate way to verify the hierarchies that we 
have extracted.  
In both tables, we find most hierarchies have an 
agreement level from 2 to 4. The deepest agree-
ment level is 6. For an agreement level of 5 or bet-
ter, the OVLP hierarchy includes only two hierar-
chies while the CSM hierarchy includes nine hier-
archies. This means CSM can extract hierarchies 
having more nodes which agree with the EDR hi-
erarchy than is possible with OVLP.  
 
Depth of 
Hierarchy
Agreement Level 
  1        2        3       4       5       6  
3 1 4 1  
4 8 6 2 
5 9 8  1
6 8 9 4 1
7 2 6 1 1
8 1 5 2 2
9 3 2 3 1
10  1  2
11  4 1 
12  1  1
13  1  2
14    1
Table 1: Distribution of CSM hierarchy for each 
depth 
Depth of 
Hierarchy
Agreement Level 
1        2         3        4       5       6 
2 1   
3 2 8 1  
4 25 9 1 
5 24 13 7 
6 21 31 5 
7 5 12 1 1
8 3 5 2 1
9 1 3 1 
Table 2: Distribution of OVLP hierarchy for 
each depth 
Also, many abstract nouns agree with the hy-
peronymic concept around the top level. In current 
thesauri, the categorization of words is classified in 
a top-down manner based on human intuition. 
Therefore, we believe the hierarchy that we have 
built is consistent with human intuition, at least 
around the top level of hyperonymic concepts. 
9 Conclusion 
We have proposed a method of automatically ex-
tracting hierarchies based on an inclusion relation 
of appearance patterns from corpora. In this paper, 
we attempted to extract objective hierarchies of 
abstract nouns co-occurring with adjectives in 
Japanese. In our experiment, we showed that com-
plementary similarity measure can extract a kind of 
hierarchy from corpora, though it is a similarity 
measure developed for the recognition of degraded 
machine-printed text. Also, we can find interesting 
hierarchies which suit human intuition, though 
they are different from exact hierarchies. Kanzaki 
et al (2004) have applied our approach to verify 
classification of abstract nouns by using self-
organization map. We can look a suitability of our 
result at that work. 
In our future work, we will use our approach for 
other parts of speech and other types of word. 
Moreover, we will compare with current alterna-
tive approaches such as those based on sentence 
patterns.  
References  
Berland, M. and Charniak, E. 1999. Finding Parts 
in Very Large Corpora, In Proceedings of the 
37th Annual Meeting of the Association for Com-
putational Linguistics, pp.57-64. 
Caraballo, S. A. 1999. Automatic Construction of a 
Hypernym-labeled Noun Hierarchy from Text, 
In Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics, 
pp.120-126. 
EDR Electronic Dictionary. 1995. 
 http://www2.nict.go.jp/kk/e416/EDR/index.html 
Hagita, N. and Sawaki, M. 1995. Robust Recogni-
tion of Degraded Machine-Printed Characters us-
ing Complementary Similarity Measure and Er-
ror-Correction Learning?In Proceedings of the 
SPIE ?The International Society for Optical En-
gineering, 2442: pp.236-244. 
Kanzaki, K., Ma, Q., Yamamoto, E., Murata, M., 
and Isahara, H. 2003. Adjectives and their Ab-
stract concepts --- Toward an objective thesaurus 
from Semantic Map. In Proceedings of the Sec-
ond International Workshop on Generative Ap-
proaches to the Lexicon, pp.177-184. 
Kanzaki, K., Ma, Q., Yamamoto, E., Murata, M., 
and Isahara, H. 2004. Extraction of Hyperonymy 
of Adjectives from Large Corpora by using the 
Neural Network Model. In Proceedings of the 
Fourth International Conference on Language 
Resources and Evaluation, Volume II, pp.423-
426. 
Kay, M. 1986. Parsing in Functional Unification 
Grammar. In ?Readings in Natural Language 
Processing?, Grosz, B. J., Spark Jones, K. and 
Webber, B. L., ed., pp.125-138, Morgan Kauf-
mann Publishers, Los Altos, California. 
Manning, C. D. and Schutze, H. 1999. Foundations 
of Statistical Natural Language Processing, The 
MIT Press, Cambridge MA. 
Matsumoto, Y. and Sudo, S., Nakayama, T., and 
Hirao, T. 1996. Thesaurus Construction from 
Multiple Language Resources, In IPSJ SIG 
Notes NL-93, pp.23-28 (In Japanese). 
Miller, A., Beckwith, R., Fellbaum, C., Gros, D., 
Millier, K., and Tengi, R. 1990. Five Papers on 
WordNet, Technical Report CSL Report 43, 
Cognitive Science Laboratory, Princeton Univer-
sity. 
Mosteller, F. and Wallace, D. 1964. Inference and 
Disputed Authorship: The Federalist. Addison-
Wesley, Reading, Massachusetts. 
Nemoto, K. 1969. The combination of the noun 
with ?ga-Case? and the adjective, Language re-
search2 for the computer, National Language 
Research Institute, pp.63-73 (In Japanese). 
Shmid, H-J. 2000. English Abstract Nouns as Con-
ceptual Shells, Mouton de Gruyter. 
Shoutsu, Y., Tokunaga, T., and Tanaka, H. 2003. 
The integration of Japanese dictionary and the-
saurus, In IPSJ SIG Notes NL-153, pp.141-146 
(In Japanese). 
Sparck Jones, K. 1972. A statistical interpretation 
of term specificity and its application in retrieval. 
Journal of Documentation, 28(1): pp.11-21. 
Takahashi, T. 1975. A various phase related to the 
part-whole relation investigated in the sentence, 
Studies in the Japanese language 103, The 
Society of Japanese Linguistics, pp.1-16 (In 
Japanese). 
Tsurumaru, H., Hitaka, T., and Yoshita, S. 1986. 
Automatic extraction of hierarchical relation be-
tween words, In IPSJ SIG Notes NL-83, pp.121-
128 (In Japanese). 
Yamamoto, E. and Umemura, K. 2002. A Similar-
ity Measure for Estimation of One?to-Many Re-
lationship in Corpus, In Journal of Natural Lan-
guage Processing, pp.45-75 (In Japanese). 
Word List by Semantic Principles. 1964. National 
Language Research Institute Publications, Shuei 
Shuppan (In Japanese). 
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 141?144,
Prague, June 2007. c?2007 Association for Computational Linguistics
Extracting Word Sets with Non-Taxonomical Relation 
Eiko Yamamoto Hitoshi Isahara 
Computational Linguistics Group 
National Institute of Information and Communications Technology 
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan 
{eiko, isahara}@nict.go.jp 
Abstract 
At least two kinds of relations exist among 
related words: taxonomical relations and 
thematic relations. Both relations identify 
related words useful to language under-
standing and generation, information re-
trieval, and so on. However, although 
words with taxonomical relations are easy 
to identify from linguistic resources such as 
dictionaries and thesauri, words with the-
matic relations are difficult to identify be-
cause they are rarely maintained in linguis-
tic resources. In this paper, we sought to 
extract thematically (non-taxonomically) 
related word sets among words in docu-
ments by employing case-marking particles 
derived from syntactic analysis. We then 
verified the usefulness of word sets with 
non-taxonomical relation that seems to be a 
thematic relation for information retrieval. 
1. Introduction 
Related word sets are useful linguistic resources 
for language understanding and generation, infor-
mation retrieval, and so on. In previous research on 
natural language processing, many methodologies 
for extracting various relations from corpora have 
been developed, such as the ?is-a? relation (Hearst 
1992), ?part-of? relation (Berland and Charniak 
1999), causal relation (Girju 2003), and entailment 
relation (Geffet and Dagan 2005).  
Related words can be used to support retrieval in 
order to lead users to high-quality information. 
One simple method is to provide additional words 
related to the key words users have input, such as 
an input support function within the Google search 
engine. What kind of relation between the key 
words that have been input and the additional word 
is effective for information retrieval? 
As for the relations among words, at least two 
kinds of relations exist: the taxonomical relation 
and the thematic relation. The former is a relation 
representing the physical resemblance among ob-
jects, which is typically a semantic relation such as 
a hierarchal, synonymic, or antonymic relation; 
the latter is a relation between objects through a 
thematic scene, such as ?milk? and ?cow? as recol-
lected in the scene ?milking a cow,? and ?milk? 
and ?baby,? as recollected in the scene ?giving 
baby milk,? which include causal relation and en-
tailment relation. Wisniewski and Bassok (1999) 
showed that both relations are important in recog-
nizing those objects. However, while taxonomical 
relations are comparatively easy to identify from 
linguistic resources such as dictionaries and 
thesauri, thematic relations are difficult to identify 
because they are rarely maintained in linguistic 
resources. 
In this paper, we sought to extract word sets 
with a thematic relation from documents by em-
ploying case-marking particles derived from syn-
tactic analysis. We then verified the usefulness of 
word sets with non-taxonomical relation that seems 
to be a thematic relation for information retrieval. 
2. Method 
In order to derive word sets that direct users to 
obtain information, we applied a method based on 
the Complementary Similarity Measure (CSM), 
which can determine a relation between two words 
in a corpus by estimating inclusive relations 
between two vectors representing each appearance 
pattern for each words (Yamamoto et al 2005).  
141
We first extracted word pairs having an inclu-
sive relation between the words by calculating the 
CSM values. Extracted word pairs are expressed 
by a tuple <wi, wj>, where CSM(Vi, Vj) is greater 
than CSM(Vj, Vi) when words wi and wj have each 
appearance pattern represented by each binary vec-
tor Vi and Vj. Then, we connected word pairs with 
CSM values greater than a certain threshold and 
constructed word sets. A feature of the CSM-based 
method is that it can extract not only pairs of re-
lated words but also sets of related words because 
it connects tuples consistently. 
Suppose we have <A, B>, <B, C>, <Z, B>, <C, 
D>, <C, E>, and <C, F> in the order of their CSM 
values, which are greater than the threshold. For 
example, let <B, C> be an initial word set {B, C}. 
First, we find the tuple with the greatest CSM 
value among the tuples in which the word C at the 
tail of the current word set is the left word, and 
connect the right word behind C. In this example, 
word ?D? is connected to {B, C} because <C, D> 
has the greatest CSM value among the three tuples 
<C, D>, <C, E>, and <C, F>, making the current 
word set {B, C, D}. This process is repeated until 
no tuples exist. Next, we find the tuple with the 
greatest CSM value among the tuples in which the 
word B at the head of the current word set is the 
right word, and connect the left word before B. 
This process is repeated until no tuples exist. In 
this example, we obtain the word set {A, B, C, D}.  
Finally, we removed ones with a taxonomical 
relation by using thesaurus. The rest of the word 
sets have a non-taxonomical relation ? including 
a thematic relation ? among the words. We then 
extracted those word sets that do not agree with the 
thesaurus as word sets with a thematic relation. 
3. Experiment 
In our experiment, we used domain-specific Japa-
nese documents within the medical domain 
(225,402 sentences, 10,144 pages, 37MB) gathered 
from the Web pages of a medical school and the 
2005 Medical Subject Headings (MeSH) thesau-
rus 1 . Recently, there has been a study on query 
expansion with this thesaurus as domain informa-
tion (Friberg 2007). 
                                                 
1 The U.S. National Library of Medicine created, maintains, 
and provides the MeSH? thesaurus.  
We extracted word sets by utilizing inclusive re-
lations of the appearance pattern between words 
based on a modified/modifier relationship in 
documents. The Japanese language has case-
marking particles that indicate the semantic rela-
tion between two elements in a dependency rela-
tion. Then, we collected from documents depend-
ency relations matching the following five pat-
terns; ?A <no (of)> B,? ?P <wo (object)> V,? ?Q 
<ga (subject)> V,? ?R <ni (dative)> V,? and ?S 
<ha (topic)> V,? where A, B, P, Q, R, and S are 
nouns, V is a verb, and <X> is a case-marking par-
ticle. From such collected dependency relations, 
we compiled the following types of experimental 
data; NN-data based on co-occurrence between 
nouns for each sentence, NV-data based on a de-
pendency relation between noun and verb for each 
case-marking particle <wo>, <ga>, <ni>, and <ha>, 
and SO-data based on a collocation between sub-
ject and object that depends on the same verb V 
as the subject. These data are represented with a 
binary vector which corresponds to the appearance 
pattern of a noun and these vectors are used as ar-
guments of CSM. 
We translated descriptors in the MeSH thesaurus 
into Japanese and used them as Japanese medical 
terms. The number of terms appearing in this ex-
periment is 2,557 among them. We constructed 
word sets consisting of these medical terms. Then, 
we chose 977 word sets consisting of three or more 
terms from them, and removed word sets with a 
taxonomical relation from them with the MeSH 
thesaurus in order to obtain the rest 847 word sets 
as word sets with a thematic relation. 
4. Verification 
In verifying the capability of our word sets to re-
trieve Web pages, we examined whether they 
could help limit the search results to more informa-
tive Web pages with Google as a search engine.  
We assume that addition of suitable key words 
to the query reduces the number of pages retrieved 
and the remaining pages are informative pages. 
Based on this assumption, we examined the de-
crease of the retrieved pages by additional key 
words and the contents of the retrieved pages in 
order to verify the availability of our word sets.  
Among 847 word sets, we used 294 word sets in 
which one of the terms is classified into one cate-
gory and the rest are classified into another. 
142
ovary - spleen - palpation (NN) 
variation - cross reactions - outbreaks - secretion (Wo) 
bleeding - pyrexia - hematuria - consciousness disorder  
- vertigo - high blood pressure (Ga) 
space flight - insemination - immunity (Ni) 
cough - fetus  
- bronchiolitis obliterans organizing pneumonia (Ha) 
latency period - erythrocyte - hepatic cell (SO) 
Figure 1. Examples of word sets used to verify. 
Figure 1 shows examples of the word sets, 
where terms in a different category are underlined. 
In retrieving Web pages for verification, we in-
put the terms composed of these word sets into the 
search engine. We created three types of search 
terms from the word set we extracted. Suppose the 
extracted word set is {X1, ..., Xn, Y}, where Xi is 
classified into one category and Y is classified into 
another. The first type uses all terms except the one 
classified into a category different from the others: 
{X1, ..., Xn} removing Y. The second type uses all 
terms except the one in the same category as the 
rest: {X1, ..., Xk-1, Xk+1, ..., Xn} removing Xk from 
Type 1. In our experiment, we removed the term 
Xk with the highest or lowest frequency among Xi. 
The third type uses terms in Type 2 and Y: {X1, ..., 
Xk-1, Xk+1, ..., Xn, Y}. 
In other words, when we consider the terms in 
Type 2 as base key words, the terms in Type 1 are 
key words with the addition of one term having the 
highest or lowest frequency among the terms in the 
same category; i.e., the additional term has a fea-
ture related to frequency in the documents and is 
taxonomically related to other terms. The terms in 
Type 3 are key words with the addition of one term 
in a category different from those of the other 
component terms; i.e., the additional term seems to 
be thematically related ? at least non-
taxonomically related ? to other terms. 
First, we quantitatively compared the retrieval 
results. We used the estimated number of pages 
retrieved by Google?s search engine. Suppose that 
we first input Type 2 as key words into Google, 
did not satisfy the result extracted, and added one 
word to the previous key words. We then sought to 
determine whether to use Type 1 or Type 3 to ob-
tain more suitable results. The results are shown in 
Figures 2 and 3, which include the results for the 
highest frequency and the lowest frequency, re-
spectively. In these figures, the horizontal axis is 
the number of pages retrieved with Type 2 and the 
vertical axis is the number of pages retrieved when 
 
 
 
 
 
 
 
 
 
 
 1
10
100
1000
10000
100000
1000000
10000000
100000000
1 10 100 1000 10000 100000 1000000 10000000 100000000 1000000000
Number of Web pages retrieved with Type2 (base key words)
Nu
m
be
r 
of
 W
eb
 p
ag
es
 r
et
rie
ve
d 
wh
en
 a
 t
er
m
 is
 a
dd
ed
 t
o 
Ty
pe
2
Type3: With additional term in a different category Type1: With additional term in same category
Figure 2. Fluctuation of number of pages retrieved 
(with the high frequency term). 
NV Type of Data NN 
Wo Ga Ni Ha
Word sets for verification 175 43 23 13 26
Cases in which Type 3 
defeated Type 1 in retrieval 108 37 15 12 18
Table 1. Number of cases in which Type 3 de-
feated Type 1 with the high frequency term. 
a certain term is added to Type 2. The circles (?) 
show the retrieval results with additional key word 
related taxonomically (Type 1). The crosses (?) 
show the results with additional key word related 
non-taxonomically (Type 3). The diagonal line 
shows that adding one term to the base key words 
does not affect the number of Web pages retrieved. 
In Figure 2, most crosses fall further below the 
line. This graph indicates that when searching by 
Google, adding a search term related non-
taxonomically tends to make a bigger difference 
than adding a term related taxonomically and with 
high frequency. This means that adding a term re-
lated non-taxonomically to the other terms is cru-
cial to retrieving informative pages; that is, such 
terms are informative terms themselves. Table 1 
shows the number of cases in which term in differ-
ent category decreases the number of hit pages 
more than high frequency term. By this table, we 
found that most of the additional terms with high 
frequency contributed less than additional terms 
related non-taxonomically to decreasing the num-
ber of Web pages retrieved. This means that, in 
comparison to the high frequency terms, which 
might not be so informative in themselves, the 
terms in the other category ? related non-
taxonomically ? are effective for retrieving useful 
Web pages. 
In Figure 3, most circles fall further below the 
line, in contrast to Figure 2. This indicates that 
143
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Fluctuation of number of pages retrieved 
(with the low frequency term). 
NV Type of Data NN 
Wo Ga Ni Ha
Word sets for verification 175 43 23 13 26
Cases in which Type 3 
defeated Type 1 in retrieval 61 18 7 6 13
Table 2. Number of cases in which Type 3 de-
feated Type 1 with the low frequency term. 
adding a term related taxonomically and with low 
frequency tends to make a bigger difference than 
adding a term with high frequency. Certainly, addi-
tional terms with low frequency would be informa-
tive terms, even though they are related taxonomi-
cally, because they may be rare terms on the Web 
and therefore the number of pages containing the 
term would be small. Table 2 shows the number of 
cases in which term in different category decreases 
the number of hit pages more than low frequency 
term. In comparing these numbers, we found that 
the additional term with low frequency helped to 
reduce the number of Web pages retrieved, making 
no effort to determine the kind of relation the term 
had with the other terms. Thus, the terms with low 
frequencies are quantitatively effective when used 
for retrieval. However, if we compare the results 
retrieved with Type 1 search terms and Type 3 
search terms, it is clear that big differences exist 
between them.  
For example, consider ?latency period - erythro-
cyte - hepatic cell? obtained from SO-data in Fig-
ure 1. ?Latency period? is classified into a category 
different from the other terms and ?hepatic cell? 
has the lowest frequency in this word set. When we 
used all the three terms, we obtained pages related 
to ?malaria? at the top of the results and the title of 
the top page was ?What is malaria?? in Japanese. 
With ?latency period? and ?erythrocyte,? we again 
obtained the same page at the top, although it was 
not at the top when we used ?erythrocyte? and 
?hepatic cell? which have a taxonomical relation. 
Type3: With additional term in a different category Type1: With additional term in same category
1
10
100
1000
10000
100000
1000000
10000000 As we showed above, the terms with thematic 
relations with other search terms are effective at 
directing users to informative pages. Quantitatively, 
terms with a high frequency are not effective at 
reducing the number of pages retrieved; qualita-
tively, low frequency terms may not effective to 
direct users to informative pages. We will continue 
our research in order to extract terms in thematic 
relation more accurately and verify the usefulness 
of them more quantitatively and qualitatively. 
5. Conclusion 
We sought to extract word sets with a thematic 
relation from documents by employing case-
marking particles derived from syntactic analysis. 
We compared the results retrieved with terms re-
lated only taxonomically and the results retrieved 
with terms that included a term related non-
taxonomically to the other terms. As a result, we 
found adding term which is thematically related to 
terms that have already been input as key words is 
effective at retrieving informative pages.  
References 
Berland, M. and Charniak, E. 1999. Finding parts in 
very large corpora, In Proceedings of ACL 99, 57?64. 
Friberg, K. 2007. Query expansion using domain infor-
mation in compounds, In Proceedings of NAACL-
HLT 2007 Doctoral Consortium, 1?4. 
Geffet, M. and Dagan, I. 2005. The distribution inclu-
sion hypotheses and lexical entailment. In Proceed-
ings of ACL 2005, 107?114. 
Girju, R. 2003. Automatic detection of causal relations 
for question answering. In Proceedings of ACL 
Workshop on Multilingual summarization and ques-
tion answering, 76?114. 
Hearst, M. A. 1992, Automatic acquisition of hyponyms 
from large text corpora, In Proceedings of Coling 92, 
539?545. 
Wisniewski, E. J. and Bassok. M. 1999. What makes a 
man similar to a tie? Cognitive Psychology, 39: 208?
238. 
Yamamoto, E., Kanzaki, K., and Isahara, H. 2005. Ex-
traction of hierarchies based on inclusion of co-
occurring words with frequency information. In Pro-
ceedings of IJCAI 2005, 1166?1172. 
1000 00
1 10 100 1000 10000 100000 1000000 10000000 100000000 1000000000 10000000000
Number of Web pages retrieved with Type2 (base key words)
um
be
r 
of
 W
eb
 p
ag
es
 r
et
rie
ve
d 
w
he
n 
a 
te
rm
 is
 a
dd
ed
 t
o 
Ty
pe
2
000
N
144
Extraction and Verification of KO-OU Expressions from Large Corpora 
Atsuko kida?,  Eiko Yamamoto?,  Kyoko Kanzaki?,  and  Hitoshi Isahara? 
?The Institute of Behavioral Sciences                       ?Communications Research Laboratory 
2-9 Honmura-cho, Ichigaya, Shinjuku-ku,                 3-5 Hikari-dai, Seika-cho, Souraku-gun, 
Tokyo, 162-0845, Japan                                              Kyoto, 619-0289, Japan 
akida@ibs.or.jp {eiko,kanzaki,isahara}@crl.go.jp
 
 
 
Abstract 
In the Japanese language, as a predicate is 
placed at the end of a sentence, the con-
tent of a sentence cannot be inferred until 
reaching the end. However, when the con-
tent is complicated and the sentence is 
long, people want to know at an earlier 
stage in the sentence whether the content 
is negative, affirmative, or interrogative. 
In Japanese, the grammatical form called 
the KO-OU relation exists. The KO-OU 
relation is a kind of concord. If a KO ele-
ment appears, then an OU element ap-
pears in the latter part of a sentence. It is 
being pointed out that the KO-OU relation 
gives advance notice to the element that 
appears in the latter part of a sentence. In 
this paper, we present the method of ex-
tracting automatically the KO-OU expres-
sion data from large-scale electronic 
corpus and verify the usefulness of the 
KO-OU expression data. 
1 Introduction 
The Japanese language has a grammatical form 
called the KO-OU relation. The KO-OU relation is 
a kind of concord, also referring to a sort of bound 
relation that a KO element appearing in a sentence 
is followed by an OU element in the latter part of 
the same sentence. On the contrary, the cooccur-
rence relation refers to two words appearing in the 
same sentence. 
Because Japanese predicates are usually located 
at the end of sentences, the contents of Japanese 
sentences cannot be decided until reaching the end. 
Furthermore, in Japanese, it is hard to comprehend 
the meaning of the sentence without reading 
through the entire sentence. The KO-OU relation is 
the grammatical form which can be helpful for un-
derstanding the sentence meaning at the early stage. 
While in archaic Japanese, KAKARI-MUSUBI, 
which had morphemic KO-OU relation between 
KAKARI-JOSI1 and the conjugation at the end of a 
sentence, had been used. KAKARI-MUSUBI gave 
advance notice to the elements that would appear 
toward the end of a sentence due to KAKARI-JOSI. 
Today, KAKARI-MUSUBI has dropped out of use. 
However, the KO-OU relation such as "sika-nai 
(only)" or "kessite-nai (never)" is present. In this 
research, we have attempted to collect such ele-
ments to extract KO-OU expression data. In this 
paper, the main points of argument are as follows: 
(1) Method of extracting automatically the KO-OU 
expression data. 
(2) What the KO-OU expression data can be used 
for. 
2 The Previous Works and How to Posi-
tion this Study 
(Ohno, 1993) pointed out that there were expres-
sions that try to give advance notice to whether a 
sentence is affirmative, negative, or interrogative at 
the early stage of a language expression which 
continues timewise. It suggested that there were 
certain adverbs that have replaced KAKARI-JOSI 
in the archaic Japanese words. 
(Masuoka, 1991) described the KO-OU relation 
of sentence elements. According to Masuoka, some 
sentences have the KO-OU expressions as shown 
in Table 1.  
However, this has the following weaknesses. 
The KO and OU elements in a KO-OU relation are 
placed together in the same category, and there is 
                                                          
1 A Japanese particle. 
no description as to the OU element. Furthermore, 
only a limited number of elements are listed. And 
the objectivity of the KO and OU elements is not 
guaranteed. 
The KO-OU expression data is useful as basic 
data to dissolve ambiguity in parsing and to decide 
on the modification relation. However, first of all, 
it is necessary for the data to have a certain length 
for being useful basic data. Secondly, it also needs 
to be objective. Therefore, we have attempted to 
extract KO-OU relations automatically from large-
scale corpus. 
 
Table 1 Masuoka?s KO-OU expression data 
3 Assumed Usage of KO-OU Expression 
Data 
3.1 To Dissolve Ambiguity 
The KO-OU expression data is useful for dissolv-
ing ambiguity of parsing. Furthermore, it is useful 
for deciding the modification relation (Figure 1). 
3.2 Gradual Understanding 
Using the KO-OU expression data will enable the 
reader to guess the end expression midway through 
a sentence. This is because as the KO elements 
appear it is possible to predict the appearance of 
the OU elements (Figure 2). It can be used as a 
basic data for understanding sentences. In addition, 
this technology can be used to guess the point in 
the minutes of a meeting at which the speakers 
change. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1 To Dissolve Ambiguity 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2 Gradual Understanding 
 
KO element OU element 
Nee,  oi te-kudasai,  naa 
tabun,  doumo daro-u,  rasii,  you-da 
kessite,  kanarazu-si-mo nai 
conviction  
(If you chew it, you will certainly taste salmon.) 
                                                                          ??? 
 
??????????  ?? ?  ?  ?  ??     ?????  ? 
kamisimereba        kitto    sake  no  aji  ga   suru    ni-chiga-inai 
 
 
 
 
 
 
 
 
 
 
??????????  ?? ?  ?  ?  ??     ?????  ? 
kamisimereba        kitto    sake  no  aji  ga   suru    ni-chiga-inai 
 
To Dissolve 
Ambiguity  refer
KO element  OU element    Similarity score   Distance        Meaning 
 
kitto               ni-chigai-nai    0.004726           6.062697       conviction 
kitto               koto-daro-u      0.00418           11.297666       guess 
kitto hazu 0.003722 12.702345 conviction
KO-OU relation data (image) 
(Works that you see at the open seaside should look attractive.) 
 
???  ?? ? ?? ?? ?  ??? ???  ? ?? ?? ?? 
koudaina umibe de miru sakuhin wa   kitto  miryokuteki ni utsuru hazu  da 
 
 
? ? ? ? ? ? ? ? ?  ? ?  Gradual Understanding 
 
 
 
 
 
 
 
 
KO element  OU element    Similarity score   Distance        Meaning 
 
kitto               ni-chigai-nai    0.004726           6.062697       conviction
kitto               koto-daro-u      0.00418           11.297666       guess 
kitto               hazu               0.003722         12.702345     conviction
refer guess ? conviction ? 
 KO-OU relation data (image) 
4 Extraction of KO-OU Expression Data 
4.1 Method 
(Yamamoto and Umemura, 2002) considered the 
estimation of the one-to-many relation between 
entities in corpus. They carried out experiments 
on extracting one-to-many relation of phenomena 
from corpus using complementary similarity 
measure (CSM) which can cope very well with 
inclusion relation of appearance patterns. The KO-
OU relation in this research can be regarded as a 
type of one-to-many relation.  
4.2 Data Used 
In this paper, we dealt with what is called FUKU-
JOSI2, KAKARI-JOSI, and some adverbs shown 
below. We proceeded on the assumption that these 
are the KO elements in the KO-OU relation. For 
our research, we used newspaper articles from the 
Mainichi Shimbun, Nihon Keizai Shimbun, and 
Yomiuri Shimbun issued between 1991 and 2000. 
 
[Target words] 
koso, sika, sae, ha, mo, bakari, nomi, sura, nara, 
kurai, dake, nannte, kessite, osoraku, tabun, zehi, 
marude, mosi, kitto 
 
Figure 3 Process flow 
4.3 Process Flow 
Process flow is shown in Figure 3. 
(1) We calculated the similarity measure using 
CSM for newspaper articles data that had been 
morphologically analyzed with ChaSen3. 
(2) We extracted pairs containing the target words 
from the results of similarity measure calculation. 
                                                          
2 A Japanese particle. 
3 Morphological Analyzer ChaSen. See http://chasen.aist-
nara.ac.jp/. 
(3) Out of the pairs in (2), we extracted words that 
appeared in the order of KO and OU elements. 
(We judge the pairs based on this word order.) 
(4) We carried out judgment based on reliability. 
As a result of this process, we obtained 14 pairs 
of data which had "kesshite" as KO element, 16 
which had "sae," and 23 which had "wa." Data of 
approximately 20 pairs was obtained per target 
word.  
5 Verification of KO-OU Expression 
Data 
5.1 Necessity to Give Meaning/Information 
If the KO-OU expression data is used for gradual 
understanding of sentences, it was necessary for 
the data to be given meaning/information. When 
the KO element appears, it will be possible to suf-
ficiently grasp or guess the contents of a sentence 
by referring the KO-OU expression data (Figure2). 
However it is difficult to give mean-
ing/information using the data obtained from the 
process in Chapter 4 because the data is broken 
down into each morpheme by the morphological 
analysis, and each element is too short.  
In Japanese sentences, there are many cases in 
which continuation of a particle and an auxiliary 
verb builds a predicate. This continuation plays an 
important role in determining the event of the sen-
tence. Particles and auxiliary verbs are functional 
words. Therefore, it is not possible to determine 
the meaning of some of the particles and auxiliary 
verbs when they appeared independently. Fur-
thermore, there are some cases in which they 
change their meaning when paired with another 
word.  
Table 2 shows the OU elements obtained pursu-
ant to the procedure in Chapter 4 for KO element 
"kitto". "Da" listed in Table 2 has an assertive 
meaning when used in a sentence like "kyou wa 
ame da . (It is raining today.)" On the other hand, it 
has an inferential meaning in the context of "asu 
wa hareru daro-u . (It should be fine tomorrow.)" In 
addition, although "nai" is a negative auxiliary 
verb, when it is paired as in "ka-mo-shire-nai (may 
be)" and "chigai-nai (must be)," the negative mean-
ing disappears. And the overall pairing stands for 
guess and conviction. 
 
 
morphologically  
analyzed data
(1) Calculate similarity 
KO-OU expression data
(2) Extract pairs 
(3) Judgment based on word
(4) Judgment based on reliability. 
Table 2  KO-OU expression data 
KO element OU element KO element OU element 
Kitto u (auxiliary) kitto yo (particle) 
kitto da (auxiliary) kitto chigai (noun) 
kitto to (particle) kitto ka (particle) 
kitto omou (verb) kitto Ne (particle) 
kitto nai (auxiliary) kitto you (noun) 
kitto hazu (noun) : : 
5.2 Verification of OU Element Using 
"Kitto" 
In this section, we carry out an analytical example 
using OU element for KO element "kitto (cer-
tainly)." We can classify the OU elements obtained 
from the procedure in Chapter 4, as follows: 
(a) It can be an OU element by itself,  
(b) It can become an OU element when paired 
with others,  
(c) It does not have the possibility of becoming an 
OU element. 
Words of (c) were not found in the OU ele-
ments obtained for KO element "kitto." In the fol-
lowing, we will describe the details on (a) and (b). 
 (a) OU element by itself 
Out of the OU elements for KO element "kitto" in 
Table 2, "hazu" can be an OU element by itself. 
 
[1] koudaina umibe de miru sakuhin wa kitto miryokuteki 
ni utsuru hazu da . 
(Works that you see at the open seaside should look attractive.) 
 
This is the only sentence with an independent 
OU element for "kitto" in the data obtained from 
the process in Chapter 4. The same can be said of 
data for KO elements other than "kitto." Because 
of morphological analysis, the row of letters has 
been shortened. As a result, there are few ele-
ments that can be regarded as an OU element by 
itself. And just looking at this element does not 
determine the meaning. 
 (b) OU element when paired with others 
When "chigai" is paired with "ni" and "nai" to 
make "ni-chigai-nai (must be)," it becomes an OU 
element. Similarly, pairing "da" with "u" results in 
an OU element "daro-u (perhaps)." "Da" is the 
original form of "daro" and becomes "daro-u" 
when paired with "u." 
 
[2] kitto kintyou suru daro-u . 
(It is certain that one will be nervous.) 
[3] kamisimereba , kitto sake no aji ga suru ni-chiga-inai . 
(If you chew it, you will certainly taste salmon.) 
 
If we look over the entire pairing shown above, 
we can give meaning to such guess and conviction. 
6 Questions for the Future 
As we described in Chapter 5, it is necessary to 
pair multiple elements before giving mean-
ing/information. We currently persuade the issue 
of automatic generation of pairing multiple ele-
ments. Now, we are carrying out experiments on 
calculating the similarity measure of pairing of 
elements. These will give us pairing of automati-
cally generated elements and the similarity meas-
ure of the pairings. This should be useful data for 
resolving ambiguity (Figure 1).  
7 Conclusion 
This paper presented the process of extracting 
KO-OU expression data using CSM and the use-
fulness of the extracted KO-OU expression data. 
We are planning to report on the findings of ex-
periments on automatic generation of OU ele-
ments pairings.  
 
Acknowledgments To compile this paper, we used 
newspaper articles from The Mainichi Newspapers, 
The Yomiuri Shimbun, and Nihon Keizai Shimbun.  
We would like to sincerely thank Dr. M. Utiyama 
of the Communications Research Laboratory for allow-
ing us to use a KWIC tool "tea4." 
References 
A.Kida, E.Yamamoto and H.Isahara. 2002. Analysis of 
expression which projects the following elements 
beforehand. IPSJ SIG Notes NL-152, pp.137-143. 
A.Kida, E.Yamamoto, K.Kanzaki and H.Isahara. 2003. 
The key on the syntax which brings forth a concord 
relation. Proceedings of the 9th Annual Meeting of 
the Association for NLP. pp.23-26.  
T.Masuoka. 1991. Grammar of modality. Kurosio-
syuppan. 
S.Ohno. 1993. Research of a KAKARI-MUSUBI. Iwa-
nami-Shoten. 
E.Yamamoto and K.Umemura. 2002. A similarity 
Measure for Estimation of One-to-Many Relation-
ship in Corpus. Jourmal of Natural Lamguage Proc-
essing. Vol.9 No.2. pp.45-75. 
                                                          
4 See http://www2.crl.go.jp/jt/a132/members/mutiyama/ 
software.html. 
Dynamic Programming Matching for Large Scale Information Retrieval
Eiko Yamamoto
Communications Research Laboratory, Kyoto Japan
eiko@crl.go.jp
Masahiro Kishida Yoshinori Takenami
Sumitomo Electric Information Systems Co., Ltd., Osaka Japan
{kishida-masahiro, takenami-yoshinori}@sei.co.jp
Yoshiyuki Takeda Kyoji Umemura
Toyohashi University of Technology, Aichi Japan
{take@ss.ics, umemura@tutics}.tut.ac.jp
Abstract
Though dynamic programming matching
can carry out approximate string matching
when there may be deletions or insertions
in a document, its effectiveness and
efficiency are usually too poor to use it for
large-scale information retrieval. In this
paper, we propose a method of dynamic
programming matching for information
retrieval. This method is as effective as a
conventional information retrieval system,
even though it is capable of approximate
matching. It is also as efficient as a
conventional system.
Keywords: Dynamic programming,
Corpus-based, Japanese.
1 Introduction
The dynamic programming method is well-known
for its ability to calculate the edit distance between
strings. The method can also be applied to informa-
tion retrieval. Dynamic programming matching can
measure the similarity between documents, even if
there are partial deletions or insertions. However,
there are two problems in applying this method to
information retrieval. One problem is search effec-
tiveness. It is poor because dynamic programming
matching lacks an adequate weighting schema. The
second problem is computational efficiency. Also,
lack of an adequate indexing schema means that dy-
namic programming matching usually has to process
the entire document.
Yamamoto et al proposed a method of dynamic
programming matching with acceptable search ef-
fectiveness (Yamamoto et al, 2000; Yamamoto,
Takeda, and Umemura, 2003). They report that
the effectiveness of dynamic programming match-
ing improves by introducing an IDF (Inverse Doc-
ument Frequency) weighting schema for all strings
that contribute similarity. They calculate matching
weights not only for words but also for all strings.
Although they report that effectiveness is improved,
the speed of their method is slower than that of
conventional dynamic programming matching, and
much slower than that of a typical information re-
trieval system.
In this paper, we aim to improve the retrieval ef-
ficiency of the dynamic programming method while
keeping its search effectiveness. From a mathemat-
ical point of view, we have only changed the defini-
tion of the weighting. The mathematical structure of
similarity remains the same as that of the dynamic
programming method proposed by (Yamamoto et
al., 2000; Yamamoto, Takeda, and Umemura, 2003).
Although it has the same definition, the new weight-
ing method makes it possible to build a more effi-
cient information retrieval system by creating the in-
dex in advance. To our surprise, we have observed
that our proposed method is not only more efficient
but also more effective.
2 Similarities Based on Dynamic
Programming Matching
In this section, we introduce several similarities
proposed by (Yamamoto et al, 2000; Yamamoto,
Takeda, and Umemura, 2003). All of them are a
form of dynamic programming matching. These
similarities include translation of the edit distance.
This distance has been described by several authors.
We have adopted Korfhage?s definition: ?the edit
distance is the minimum number of edit operations,
such as insertion and deletion, which are required to
map one string into the other? (Korfhage, 1997).
There are three related similarities. The first is dy-
namic programming matching, which is simply con-
version of the edit distance. The second similarity
is an extension of the first similarity, introducing a
character weighting for each contributing character.
The third and proposed similarity is an extension of
the second one, using string weight instead of char-
acter weight.
2.1 Dynamic Programming Matching
As stated above, dynamic programming (DP)
matching is a conversion of edit distance. We call
this similarity SIM1. While the edit distance (ED) is
a measure of difference, counting different charac-
ters between two strings , SIM1 is a measure of sim-
ilarity, counting matching characters between two
strings. ED and SIM1 are defined as follows:
Definition 2.1 Edit Distance (Korfhage, 1997)
Let ? and ? be strings, x and y be a character, and
?? be empty string.
? If both strings are empty then
ED(??, ??) = 0.0
? If x 6= y then
ED(x, y) = 1.0
? If their first characters are the same then
ED(x?, x?) =
MIN(ED(?, x?), ED(x?, ?),
ED(?, ?) + 1.0)
? Otherwise
ED(x?, y?) =
MIN(ED(?, y?), ED(x?, ?),
ED(?, ?))
Definition 2.2 SIM1
Let ? and ? be strings, x and y be a character, and
?? be empty string.
? If both strings are empty then
SIM1(??, ??) = 0.0
? If x 6= y then
SIM1(x, y) = 0.0
? If their first characters are the same then
SIM1(x?, x?) =
MAX(SIM1(?, x?), SIM1(x?, ?),
SIM1(?, ?) + 1.0)
? Otherwise
SIM1(x?, y?) =
MAX(SIM1(?, y?), SIM1(x?, ?),
SIM1(?, ?))
2.2 Character Weight DP Similarity
SIM1 adds 1.0 to the similarity between two strings
for every matching character, and this value is con-
stant for all the time. Our assumption for the new
function is that different characters make different
contributions. For example, in Japanese informa-
tion retrieval, Hiragana characters are usually used
for functional words and make a different contribu-
tion than Kanji characters, which are usually used
for content words. Thus, it is natural to assign a dif-
ferent similarity weight according to the nature of
the character. The below method of defining Charac-
ter Weight DP Similarity adds not 1.0 but a specific
weight depending on the matching character. We
call this similarity SIM2. It resembles Ukkonen?s
Enhanced Dynamic Programming ASM (Approxi-
mate String Matching) (Berghel and Roach, 1996).
The weight is expressed by a function called Score.
SIM2 is defined as follows:
Definition 2.3 SIM2
Let ? and ? be strings, x and y be a character, and
?? be empty string.
? If both strings are empty then
SIM2(??, ??) = 0.0
? If x 6= y then
SIM2(x, y) = 0.0
? If their first characters are the same then
SIM2(x?, x?) =
MAX(SIM2(?, x?), SIM2(x?, ?),
SIM2(?, ?) + Score(x))
? Otherwise
SIM2(x?, y?) =
MAX(SIM2(?, y?), SIM2(x?, ?),
SIM2(?, ?))
2.3 String Weight DP Similarity
DP procedure usually considers just a single char-
acter at a time, but since some long substrings can
receive good scores, it is natural to consider all pre-
fixes of the longest common prefix, not just the next
character.
While SIM2 uses a character weight whenever a
character matches between strings, a single char-
acter may not be enough. In some cases, even
when each character has a low weight, the string
as a whole may be a good clue for information re-
trieval. For example, ?chirimenjyako? is a Japanese
word that could be a retrieval key word. This word,
which means ?boiled and dried baby sardines,? con-
sists only of Hiragana characters ?chi-ri-me-n-jya-
ko? but each character would make a small contri-
bution in SIM2.
The proposed similarity is called String Weight
DP Similarity, which is a generalization of SIM2.
We call this similarity SIM3. It considers the weight
of all matching strings and is defined as follows:
Definition 2.4 SIM3
Let ? and ? be strings, x and y be a character, and
?? be empty string.
? If both strings are empty then
SIM3(??, ??) = Score(??) = 0.0
? Otherwise
SIM3(?, ?) =
MAX(SIM3s(?, ?), SIM3g(?, ?))
? SIM3s(??, ??) =
MAX(Score(?) + SIM3(??, ??))
where ?(= ??) is the maximum length
string matching from the first character.
? SIM3g(x?, y?) =
MAX(SIM3(?, y?), SIM3(x?, ?),
SIM3(?, ?))
2.4 Weighting Function
Yamamoto et al have used IDF (Inverse Document
Frequency) as a weight for each string. The weight
is computed using a Score function as follows:
Definition 2.5 Yamamoto et al?s Score function
Let ? be string, df(?) the frequency of documents
including ? in the document set for retrieval, and N
be the number of documents in the set.
Score(?) = IDF (?) = ?log(df(?)/N)
The standard one-character-at-a-time DP method
assumes that long matches cannot receive exception-
ally good scores. In other words, it regards Score(?)
as 0 if the length of ? is greater than one. If the
Score function obeys the inequality, Score(??) <
Score(?) + Score(?) for all substrings ? and ?,
the best path would consist of a sequence of sin-
gle characters, and we would not need to consider
long phrases. However, we are proposing a different
Score function. It sometimes assigns good scores to
long phrases, and therefore SIM2 has to be extended
into SIM3 to establish a DP procedure that considers
more than just one character at a time.
3 Proposed Weighting Function
Although SIM3, as shown in Section 2.3, has rea-
sonable effectiveness, its computation is harder than
that of the edit distance, and much harder than that
of the similarity used in a conventional information
retrieval system. In this paper, we have modified
the weighting function so that it keeps its effective-
ness while improving efficiency. To achieve this im-
provement, we use the SIM3 with the same defini-
tion but with a different score function.
3.1 Proposed String Weighting
We reduce the computational cost by limiting strings
that have positive scores. First, we select bigrams as
such strings. In other words, we assign a score of
zero if the length of the string does not equal to 2.
Several language systems use Kanji characters (e.g.
Chinese and Japanese), and bigram is an effective
indexing unit for information retrieval for these lan-
guage systems (Ogawa and Matsuda, 1997). In addi-
tion, we may assume that the contribution of a longer
string is approximated by the total bigram weight-
ing. We have also restricted our attention to infre-
quent bigrams. Thus, we have restricted the weight-
ing function Score as follows, where K is the num-
ber decided by the given query.
? If string length is 2 and cf(?) < K then
Score(?) = ?log(df(?)/N)
? Otherwise Score(?) = 0.0
3.2 Using a Suffix Array for Indexing
Since we have restricted the number of match-
ing strings, and all the matching strings appear in
a query, we can collect all the positions of such
strings. To make it possible, we need some index-
ing in advance. We have used a suffix array for this
index. Below we summarize our proposed algorithm
using a suffix array:
I. Make a suffix array of the document set.
II. For each query,
A. Make a set of substrings consisting of two
characters (bigram).
B. For a given number n, extract the total n of
less frequent bigrams, calculating corpus
frequency.
C. For each bigram from step B,
i. Record all positions in which the bi-
gram appears in the query and docu-
ment set,
ii. Record all documents that contain the
bigram.
D. For each document recorded,
i. Compute the similarity between the
query and the document with SIM3,
using the recorded position of the cor-
responding bigram.
ii. Assign the similarity to the document.
E. Extract the most similar 1000 documents
from the recorded documents as a retrieval
result for the query.
We call the retrieval method described above Fast
Dynamic Programming (FDP). In general, retrieval
systems use indexes to find documents. FDP also
uses an index as a usual method. However, unlike
conventional methods, FDP requires information not
only on the document identification but also on the
position of bigrams.
Manber and Myers proposed a data structure
called ?suffix array.? (Manber and Myers, 1993)
Figure 1 shows an example of suffix array. Each
suffix is expressed by one integer corresponding to
its position. We use this suffix array to find out the
position of selected bigrams. A suffix array can be
created in O(N log(N)) time because we need to
sort all suffixes in alphabetical order. We can get
the position of any string in O(log(N)) time by a
binary search of suffixes and by then obtaining its
corresponding position.
4 Experiment
In the experiment, we compared the proposed FDP
method with SIM1, SIM2, and SIM3, which were
described in Section 2. We measured three values:
Figure 1: Suffix Array
search effectiveness, memory usage, and execution
time.
We used the NTCIR1 collection (NTCIR Project,
1999). This collection consists of 83 retrieval topics
and roughly 330,000 documents of Japanese tech-
nical abstracts. The 83 topics include 30 training
topics (topic01-30); the rest are for testing (topic31-
83). The testing topics were more difficult than the
training topics. Each topic contains five parts, ?TI-
TLE?, ?DESCRIPTION?, ?NARRATIVE?, ?CON-
CEPT?, and ?FIELD.? We retrieved using ?DE-
SCRIPTION,? which is retrieval query and a short
sentence.
All the experiments reported in this section were
conducted using a dual AMD Athlon MP 1900+
with 3GB of physical memory, running TurboLinux
7.0.
4.1 Search Effectiveness
The proposed FDP method restricts the number of
bigrams that can contribute to string matching. That
is, only a small number of strings are considered. It
was not clear whether FDP maintains its effective-
ness like SIM3. To verify it, we compared the effec-
tiveness of FDP with that of SIM1, SIM2, and SIM3.
We also needed to know how the effectiveness might
vary by the number of bigrams. We set number n
at 5, 10, 15, 20, 30, 50, and 500. They were named
FDP5, FDP10, FDP15, FDP20, FDP30, FDP50, and
FDP500, respectively.
Table 1: Statistical Significant Test for difference of MAP (? = 0.005, ? = 83? 1)
SIM2 SIM3 FDP5 FDP10 FDP15 FDP20 FDP30 FDP50 FDP500
SIM1 << << << << << << << << <<
SIM2 << = < << << << << <<
SIM3 = = < << << << <<
FDP5 = << << << << <<
FDP10 = << < < <
FDP15 < = = =
FDP20 = = =
FDP30 = =
FDP50 =
Table 2: Search Effectiveness for Topic01-30
Method 11 pt. average R-precision
SIM1 0.1349 0.1790
SIM2 0.1948 0.2296
SIM3 0.2691 0.3024
FDP5 0.2547 0.2649
FDP10 0.2948 0.3089
FDP15 0.3109 0.3446
FDP20 0.3207 0.3574
FDP30 0.3176 0.3421
FDP50 0.3131 0.3377
FDP500 0.3172 0.3419
The NTCIR1 collection also contains a relevance
judgment. We obtained the 11-point average pre-
cision and R-precision using standard tools called
TRECEVAL. And we tested about statistical signif-
icance for difference of MAP (Mean Average Preci-
sion) (Kishida et al, 2002).
Tables 2 and 3 show the search effectiveness for
all methods. We found that FDP20 is the most ef-
fective. Table 1 shows the results of one-sided t-test
for difference of MAP x?i ? y?i, where x?i and y?i are
MAP of i-th method in the first row and MAP of
i-th method in the first column, respectively. The
level of significance ? is 0.005 and the degree of
freedom ? is 83 ? 1. The Symbols <<,<,= rep-
resent ?much less than ??, ?less than ?, and ?not
less than ??, respectively. We found that except for
FDP5 and FDP10, the other FDPs are significantly
more effective than SIM3 at a level of significance
0.005. In additional, this shows that FDP30, FDP50,
and FDP500 are not significantly more effective than
FDP20. These have demonstrated our proposed FDP
Table 3: Search Effectiveness for Topic31-83
Method 11 pt. average R-precision
SIM1 0.0545 0.0845
SIM2 0.1245 0.1596
SIM3 0.1807 0.2083
FDP5 0.1277 0.1505
FDP10 0.1766 0.2013
FDP15 0.2144 0.2280
FDP20 0.2398 0.2621
FDP30 0.2353 0.2485
FDP50 0.2354 0.2488
FDP500 0.2350 0.2477
method maintains its effectiveness, even though the
strings that contribute similarity are restricted to a
small number of bigrams. Also, it is interesting that
the FDP with 20 bigrams is significantly more effec-
tive than the one with many more bigrams.
4.2 Memory Usage
The proposed method needs to record all the posi-
tions considered bigrams. A memory area is there-
fore required to hold position information; in the
worst case, the memory size required is the prod-
uct of the number of documents and the number of
substrings in a query. This means the memory re-
quirement could be very large. However, using FDP,
we have found that the amount of memory requested
is of a reasonable size.
In other words, the size of the memory area is the
total sum of collection frequency for all strings that
contribute similarity. We examined the amount of
memory used by comparison for the total sum of col-
lection frequency.
020000000
40000000
60000000
80000000
100000000
1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81
Query
Tota
l Co
llect
ion F
requ
ency
AllNgram20BigramAllBigram
Figure 2: Memory Usage (Total Number of Collection Frequency for Each String)
 
0
500000
1000000
1500000
2000000
2500000
3000000
3500000
4000000
4500000
1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81
Query
Tota
l Co
llect
ion 
Freq
uenc
y
20BigramAllBigram
Figure 3: Memory Usage for Different Number of Restricted Bigrams
Figure 2 shows the total sum of collection fre-
quency for three kinds of string sets. In the fig-
ure, AllNgram is for sets of all substrings consid-
ered by SIM3, AllBigram is for sets of all bigrams,
and 20Bigram is for sets of 20 bigrams considered
by FDP20. The field surrounded by the plot line
and the horizontal axis represents the total sum of
collection frequency. As the figure shows, AllBi-
gram and 20Bigram occupy a much smaller field
than AllNgram. This means the memory require-
ment of FDP is much smaller than that of SIM3.
This result shows that FDP is possible to efficiently
perform large-scale information retrieval on a com-
puter with a reasonable amount of memory.
Figure 3 shows enlarged graphs of AllBigram and
20Bigram from Figure 2. The figure shows that
20Bigram equals AllBigram for most queries, but
not always. However, as shown in Table 2 and Ta-
ble 3, FDP20 actually has the highest precision in all
FDPs. This means that considering more bigrams is
not necessarily an advantage. Probably, by choosing
substrings with a high contribution, we manage to
get rid of noisy strings.
4.3 Execution Time
We measured execution time under the same con-
ditions as described in Section 4.1. Notice we im-
plemented SIM1, SIM2, and SIM3 in C language.
On the other hand, FDP is implemented in Java
(JDK1.3.1.04). When we noted the time required
to make a suffix array, we found that FDP took 1.5
times as long as SIM in Figure 4. Thus, for the same
algorithm, the execution speed of Java is generally
slower than that of C language.
Figures 5 and 6 show the time taken to retrieve
for each topic01-30 and topic31-83. In the figures,
the vertical axis is the number of documents, and the
horizontal axis is the execution time. We found that
all SIMs took much longer than FDPs. This demon-
strates that our algorithm in Section 3 sharply im-
proves execution speed. Moreover, we found that
execution time did not increase exponentially even
if the candidate documents for retrieval increased;
instead, the retrieval collection becomes larger and
larger. This suggests that FDP is an effective DP
technique for large-scale information retrieval.
5 Related Work
Our proposed technique is a type of DP matching.
The most typical application of DP matching is gene
information research, because DP is effective for
gene information matching. However, this system
has a very slow processing speed.
In recent years, advances in this field of re-
search have meant that high-speed systems have
been required for gene information retrieval. A
high-speed gene information retrieval system called
BLAST was developed (Setubal and Meidanis,
2001). BLAST has achieved higher processing
speed by using heuristics that specify characteristic
gene arrangements, rather than using DP matching.
In contrast, we have managed to achieve fast match-
ing using the DP technique.
Moreover, in music information retrieval, an error
in copying a tune corresponds to a deficit (deletion)
and insertion of data. For this reason, a music search
engine has been built based on the DP technique (Hu
and Dannenberg, 2002). Since there is a great deal
of music information available these days, scalabil-
ity is also an important problem for music informa-
tion retrieval systems. Our proposed DP method is
scalable and can cope with deficits. It therefore has
potential applications in music information retrieval.
6 Conclusion
In this study, we proposed a DP matching method
for large-scale information retrieval. To improve
its efficiency, this method selects the strings that
contribute more to retrieval. This selection process
reduces the memory requirement and frequency of
memory access. We conclude that our method is
suitable for large-scale information retrieval where
approximate matching is required.
Acknowledgement
This work was supported in The 21st Century COE
Program ?Intelligent Human Sensing,? from the
Ministry of Education, Culture, Sports, Science, and
Technology.
References
Hal Berghel and David Roach. 1996. An extension of
Ukkonen?s enhanced dynamic programming ASM al-
gorithm. Journal of ACM TOIS, 4(1):94?106.
Kazuaki Kishida, Makoto Iwayama, and Koji Eguchi.
2002. Methodology and Pragmatics of Retrieval Ex-
periments at NTCIR Workshop. Pre-meeting Lecture
at the NTCIR-3 Workshop.
Ning Hu and Roger B. Dannenberg. 2002. Comparison
of Melodic Database Retrieval Techniques Using Sung
Queries. Proceedings of JCDL 2002, 301?307.
Robert R. Korfhage. 1997. Information Storage and
Retrieval. WILEY COMPUTER PUBLISHING, 291?
303.
Udi Manber and Gene Myers. 1993. Suffix arrays: a
new method for on-line string searches. SIAM Journal
of Computing, 22(5):935?948.
NTCIR Project. http://research.nii.ac.jp/ntcir/.
Yasushi Ogawa and Toru Matsuda. 1997. Overlapping
statistical word indexing: A new indexing method for
Japanese text. Proceedings of SIGIR97, 226?234.
Joao Carlos Setubal and Joao Meidanis. 2001.
Introduction to Computational Molecular Biology.
BrooksCole Publishing Company.
Eiko Yamamoto, Mikio Yamamoto, Kyoji Umemura, and
Kenneth W. Church. 2000. Dynamic Prgramming:
A Method for Taking Advantage of Technical Ter-
minology in Japanese Documents. Proceedings of
IRAL2000, 125?131.
Eiko Yamamoto, Yoshiyuki Takeda, and Kyoji Umemura.
2003. An IR Similarity Measure which is Tolerant
for Morphological Variation. Journal of Natural Lan-
guage Processing, 10(1):63?80. (in Japanese).
 7.93
17.31 23.37
57.63 108.17
185.42 263.57
391.11 541.42 698.15
856.31 1512.88
10.31
23.24
50.68 81.12
151.05 272.29
354.78 561.57
788.54 1031.59 1279.31 1530.43
1
10
100
1000
10000
5000 10000 20000 30000 50000 80000 100000 150000 200000 250000 300000 allThe Number of Documents
Mak
ing T
ime 
[sec]
SIMFDP
Figure 4: Suffix Array Generation Time
1
10
100
1000
10000
5000 10000 20000 30000 50000 80000 100000 150000 200000 250000 300000 all
The Number of Documents
Exe
cusi
on T
ime 
[sec
]
SIM1 SIM2 SIM3 FDP5 FDP10FDP15 FDP20 FDP30 FDP50
Figure 5: Execution Time for Topic01-30
1
10
100
1000
10000
5000 10000 20000 30000 50000 80000 100000 150000 200000 250000 300000 all
The Number of Documents
Exe
cus
ion 
Tim
e [s
ec]
SIM1 SIM2 SIM3 FDP5 FDP10FDP15 FDP20 FDP30 FDP50
Figure 6: Execution Time for Topic31-83
