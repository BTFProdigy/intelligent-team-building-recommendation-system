Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809?816,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Names and Similarities on the Web: Fact Extraction in the Fast Lane
Marius Pas?ca
Google Inc.
Mountain View, CA 94043
mars@google.com
Dekang Lin
Google Inc.
Mountain View, CA 94043
lindek@google.com
Jeffrey Bigham?
University of Washington
Seattle, WA 98195
jbigham@cs.washington.edu
Andrei Lifchits?
University of British Columbia
Vancouver, BC V6T 1Z4
alifchit@cs.ubc.ca
Alpa Jain?
Columbia University
New York, NY 10027
alpa@cs.columbia.edu
Abstract
In a new approach to large-scale extrac-
tion of facts from unstructured text, dis-
tributional similarities become an integral
part of both the iterative acquisition of
high-coverage contextual extraction pat-
terns, and the validation and ranking of
candidate facts. The evaluation mea-
sures the quality and coverage of facts
extracted from one hundred million Web
documents, starting from ten seed facts
and using no additional knowledge, lexi-
cons or complex tools.
1 Introduction
1.1 Background
The potential impact of structured fact reposito-
ries containing billions of relations among named
entities on Web search is enormous. They en-
able the pursuit of new search paradigms, the pro-
cessing of database-like queries, and alternative
methods of presenting search results. The prepa-
ration of exhaustive lists of hand-written extrac-
tion rules is impractical given the need for domain-
independent extraction of many types of facts from
unstructured text. In contrast, the idea of boot-
strapping for relation and information extraction
was first proposed in (Riloff and Jones, 1999), and
successfully applied to the construction of seman-
tic lexicons (Thelen and Riloff, 2002), named en-
tity recognition (Collins and Singer, 1999), extrac-
tion of binary relations (Agichtein and Gravano,
2000), and acquisition of structured data for tasks
such as Question Answering (Lita and Carbonell,
2004; Fleischman et al, 2003). In the context of
fact extraction, the resulting iterative acquisition
?Work done during internships at Google Inc.
framework starts from a small set of seed facts,
finds contextual patterns that extract the seed facts
from the underlying text collection, identifies a
larger set of candidate facts that are extracted by
the patterns, and adds the best candidate facts to
the previous seed set.
1.2 Contributions
Figure 1 describes an architecture geared towards
large-scale fact extraction. The architecture is sim-
ilar to other instances of bootstrapping for infor-
mation extraction. The main processing stages are
the acquisition of contextual extraction patterns
given the seed facts, acquisition of candidate facts
given the extraction patterns, scoring and ranking
of the patterns, and scoring and ranking of the can-
didate facts, a subset of which is added to the seed
set of the next round.
Within the existing iterative acquisition frame-
work, our first contribution is a method for au-
tomatically generating generalized contextual ex-
traction patterns, based on dynamically-computed
classes of similar words. Traditionally, the ac-
quisition of contextual extraction patterns requires
hundreds or thousands of consecutive iterations
over the entire text collection (Lita and Carbonell,
2004), often using relatively expensive or restric-
tive tools such as shallow syntactic parsers (Riloff
and Jones, 1999; Thelen and Riloff, 2002) or
named entity recognizers (Agichtein and Gravano,
2000). Comparatively, generalized extraction pat-
terns achieve exponentially higher coverage in
early iterations. The extraction of large sets of can-
didate facts opens the possibility of fast-growth it-
erative extraction, as opposed to the de-facto strat-
egy of conservatively growing the seed set by as
few as five items (Thelen and Riloff, 2002) after
each iteration.
809
Acquisition of contextual extraction patterns
Distributional similaritiesText collection
Candidate facts
Acquisition of candidate facts
Occurrences of extraction patterns
Validation of candidate facts
Scored extraction patternsScored candidate facts
Scoring and ranking
Validated candidate facts
Seed facts
Occurrences of seed facts Extraction patterns
Validated extraction patterns
Validation of patterns
Generalized extraction patterns
Figure 1: Large-scale fact extraction architecture
The second contribution of the paper is a
method for domain-independent validation and
ranking of candidate facts, based on a similar-
ity measure of each candidate fact relative to the
set of seed facts. Whereas previous studies as-
sume clean text collections such as news cor-
pora (Thelen and Riloff, 2002; Agichtein and Gra-
vano, 2000; Hasegawa et al, 2004), the valida-
tion is essential for low-quality sets of candidate
facts collected from noisy Web documents. With-
out it, the addition of spurious candidate facts to
the seed set would result in a quick divergence of
the iterative acquisition towards irrelevant infor-
mation (Agichtein and Gravano, 2000). Further-
more, the finer-grained ranking induced by simi-
larities is necessary in fast-growth iterative acqui-
sition, whereas previously proposed ranking crite-
ria (Thelen and Riloff, 2002; Lita and Carbonell,
2004) are implicitly designed for slow growth of
the seed set.
2 Similarities for Pattern Acquisition
2.1 Generalization via Word Similarities
The extraction patterns are acquired by matching
the pairs of phrases from the seed set into docu-
ment sentences. The patterns consist of contigu-
ous sequences of sentence terms, but otherwise
differ from the types of patterns proposed in earlier
work in two respects. First, the terms of a pattern
are either regular words or, for higher generality,
any word from a class of similar words. Second,
the amount of textual context encoded in a pat-
tern is limited to the sequence of terms between
(i.e., infix) the pair of phrases from a seed fact that
could be matched in a document sentence, thus ex-
cluding any context to the left (i.e., prefix) and to
the right (i.e., postfix) of the seed.
The pattern shown at the top of Figure 2, which
(Irving Berlin, 1888)
    NNP       NNP       CD
Infix
Aurelio de la Vega was born November 28 , 1925 , in Havana , Cuba .
    FW       FW FW  NNP VBD  VBN      NNP           CD  ,    CD    ,  IN    NNP      ,   NNP    .
foundnot found
Infix
not found
Prefix PostfixInfix
Matching on sentences
Seed fact Infix?only pattern
The poet was born Jan. 13 , several years after the revolution .
not found
British ? native Glenn Cornick of Jethro Tull was born April 23 , 1947 .
   NNP     :      JJ         NNP        NNP       IN    NNP     NNP  VBD  VBN   NNP   CD  ,   CD     .
Infix
foundfound
Chester Burton Atkins was born June 20 , 1924 , on a farm near Luttrell .
   NNP          NNP       NNP     VBD  VBN  NNP  CD  ,   CD     ,  IN DT  NN     IN       NNP       .
Infix
Infix
found
The youngest child of three siblings , Mariah Carey was born March 27 ,
1970 in Huntington , Long Island in New York .
  DT       JJS            NN     IN   CD        NNS       ,    NNP        NNP    VBD  VBN    NNP     CD  ,
  CD    IN       NNP             ,    JJ         NN      IN  NNP    NNP   .
found
foundfound
(S1)
(S2)
(S3)
(S4)
(S5)
(Jethro Tull, 1947)  (Mariah Carey, 1970)  (Chester Burton Atkins, 1924)
Candidate facts
  DT    NN   VBD  VBN  NNP CD ,       JJ           NNS     IN     DT        NN           .
N/A          CL1 born CL2 00 ,              N/A
Figure 2: Extraction via infix-only patterns
contains the sequence [CL1 born CL2 00 .], illus-
trates the use of classes of distributionally similar
words within extraction patterns. The first word
class in the sequence, CL1, consists of words such
as {was, is, could}, whereas the second class in-
cludes {February, April, June, Aug., November}
and other similar words. The classes of words are
computed on the fly over all sequences of terms
in the extracted patterns, on top of a large set of
pairwise similarities among words (Lin, 1998) ex-
tracted in advance from around 50 million news
articles indexed by the Google search engine over
three years. All digits in both patterns and sen-
tences are replaced with a common marker, such
810
that any two numerical values with the same num-
ber of digits will overlap during matching.
Many methods have been proposed to compute
distributional similarity between words, e.g., (Hin-
dle, 1990), (Pereira et al, 1993), (Grefenstette,
1994) and (Lin, 1998). Almost all of the methods
represent a word by a feature vector, where each
feature corresponds to a type of context in which
the word appeared. They differ in how the feature
vectors are constructed and how the similarity be-
tween two feature vectors is computed.
In our approach, we define the features of a
word w to be the set of words that occurred within
a small window of w in a large corpus. The context
window of an instance of w consists of the clos-
est non-stopword on each side of w and the stop-
words in between. The value of a feature w? is de-
fined as the pointwise mutual information between
w? and w: PMI(w?, w) = ? log( P (w,w?)P (w)P (w?)). The
similarity between two different words w1 and w2,
S(w1, w2), is then computed as the cosine of the
angle between their feature vectors.
While the previous approaches to distributional
similarity have only applied to words, we applied
the same technique to proper names as well as
words. The following are some example similar
words and phrases with their similarities, as ob-
tained from the Google News corpus:
? Carey: Higgins 0.39, Lambert 0.39, Payne
0.38, Kelley 0.38, Hayes 0.38, Goodwin 0.38,
Griffin 0.38, Cummings 0.38, Hansen 0.38,
Williamson 0.38, Peters 0.38, Walsh 0.38, Burke
0.38, Boyd 0.38, Andrews 0.38, Cunningham
0.38, Freeman 0.37, Stephens 0.37, Flynn 0.37,
Ellis 0.37, Bowers 0.37, Bennett 0.37, Matthews
0.37, Johnston 0.37, Richards 0.37, Hoffman
0.37, Schultz 0.37, Steele 0.37, Dunn 0.37, Rowe
0.37, Swanson 0.37, Hawkins 0.37, Wheeler 0.37,
Porter 0.37, Watkins 0.37, Meyer 0.37 [..];
? Mariah Carey: Shania Twain 0.38, Christina
Aguilera 0.35, Sheryl Crow 0.35, Britney Spears
0.33, Celine Dion 0.33, Whitney Houston 0.32,
Justin Timberlake 0.32, Beyonce Knowles 0.32,
Bruce Springsteen 0.30, Faith Hill 0.30, LeAnn
Rimes 0.30, Missy Elliott 0.30, Aretha Franklin
0.29, Jennifer Lopez 0.29, Gloria Estefan 0.29,
Elton John 0.29, Norah Jones 0.29, Missy
Elliot 0.29, Alicia Keys 0.29, Avril Lavigne
0.29, Kid Rock 0.28, Janet Jackson 0.28, Kylie
Minogue 0.28, Beyonce 0.27, Enrique Iglesias
0.27, Michelle Branch 0.27 [..];
? Jethro Tull: Motley Crue 0.28, Black Crowes
0.26, Pearl Jam 0.26, Silverchair 0.26, Black Sab-
bath 0.26, Doobie Brothers 0.26, Judas Priest 0.26,
Van Halen 0.25, Midnight Oil 0.25, Pere Ubu 0.24,
Black Flag 0.24, Godsmack 0.24, Grateful Dead
0.24, Grand Funk Railroad 0.24, Smashing Pump-
kins 0.24, Led Zeppelin 0.24, Aerosmith 0.24,
Limp Bizkit 0.24, Counting Crows 0.24, Echo
And The Bunnymen 0.24, Cold Chisel 0.24, Thin
Lizzy 0.24 [..].
To our knowledge, the only previous study that
embeds similarities into the acquisition of extrac-
tion patterns is (Stevenson and Greenwood, 2005).
The authors present a method for computing pair-
wise similarity scores among large sets of poten-
tial syntactic (subject-verb-object) patterns, to de-
tect centroids of mutually similar patterns. By as-
suming the syntactic parsing of the underlying text
collection to generate the potential patterns in the
first place, the method is impractical on Web-scale
collections. Two patterns, e.g. chairman-resign
and CEO-quit, are similar to each other if their
components are present in an external hand-built
ontology (i.e., WordNet), and the similarity among
the components is high over the ontology. Since
general-purpose ontologies, and WordNet in par-
ticular, contain many classes (e.g., chairman and
CEO) but very few instances such as Osasuna,
Crewe etc., the patterns containing an instance
rather than a class will not be found to be simi-
lar to one another. In comparison, the classes and
instances are equally useful in our method for gen-
eralizing patterns for fact extraction. We merge
basic patterns into generalized patterns, regardless
of whether the similar words belong, as classes or
instances, in any external ontology.
2.2 Generalization via Infix-Only Patterns
By giving up the contextual constraints imposed
by the prefix and postfix, infix-only patterns rep-
resent the most aggressive type of extraction pat-
terns that still use contiguous sequences of terms.
In the absence of the prefix and postfix, the outer
boundaries of the fact are computed separately for
the beginning of the first (left) and end of the sec-
ond (right) phrases of the candidate fact. For gen-
erality, the computation relies only on the part-
of-speech tags of the current seed set. Starting
forward from the right extremity of the infix, we
collect a growing sequence of terms whose part-
of-speech tags are [P1+ P2+ .. Pn+], where the
811
notation Pi+ represents one or more consecutive
occurrences of the part-of-speech tag Pi. The se-
quence [P1 P2 .. Pn] must be exactly the sequence
of part of speech tags from the right side of one of
the seed facts. The point where the sequence can-
not be grown anymore defines the boundary of the
fact. A similar procedure is applied backwards,
starting from the left extremity of the infix. An
infix-only pattern produces a candidate fact from
a sentence only if an acceptable sequence is found
to the left and also to the right of the infix.
Figure 2 illustrates the process on the infix-
only pattern mentioned earlier, and one seed fact.
The part-of-speech tags for the seed fact are [NNP
NNP] and [CD] for the left and right sides respec-
tively. The infix occurs in all sentences. How-
ever, the matching of the part-of-speech tags of the
sentence sequences to the left and right of the in-
fix, against the part-of-speech tags of the seed fact,
only succeeds for the last three sentences. It fails
for the first sentence S1 to the left of the infix, be-
cause [.. NNP] (for Vega) does not match [NNP
NNP]. It also fails for the second sentence S2 to
both the left and the right side of the infix, since [..
NN] (for poet) does not match [NNP NNP], and
[JJ ..] (for several) does not match [CD].
3 Similarities for Validation and Ranking
3.1 Revisiting Standard Ranking Criteria
Because some of the acquired extraction patterns
are too generic or wrong, all approaches to iter-
ative acquisition place a strong emphasis on the
choice of criteria for ranking. Previous literature
quasi-unanimously assesses the quality of each
candidate fact based on the number and qual-
ity of the patterns that extract the candidate fact
(more is better); and the number of seed facts ex-
tracted by the same patterns (again, more is bet-
ter) (Agichtein and Gravano, 2000; Thelen and
Riloff, 2002; Lita and Carbonell, 2004). However,
our experiments using many variations of previ-
ously proposed scoring functions suggest that they
have limited applicability in large-scale fact ex-
traction, for two main reasons. The first is that
it is impractical to perform hundreds of acquisi-
tion iterations on terabytes of text. Instead, one
needs to grow the seed set aggressively in each
iteration. Previous scoring functions were im-
plicitly designed for cautious acquisition strate-
gies (Collins and Singer, 1999), which expand the
seed set very slowly across consecutive iterations.
In that case, it makes sense to single out a small
number of best candidates, among the other avail-
able candidates. Comparatively, when 10,000 can-
didate facts or more need to be added to a seed set
of 10 seeds as early as after the first iteration, it
is difficult to distinguish the quality of extraction
patterns based, for instance, only on the percent-
age of the seed set that they extract. The second
reason is the noisy nature of the Web. A substan-
tial number of factors can and will concur towards
the worst-case extraction scenarios on the Web.
Patterns of apparently high quality turn out to pro-
duce a large quantity of erroneous ?facts? such as
(A-League, 1997), but also the more interesting
(Jethro Tull, 1947) as shown earlier in Figure 2, or
(Web Site David, 1960) or (New York, 1831). As
for extraction patterns of average or lower quality,
they will naturally lead to even more spurious ex-
tractions.
3.2 Ranking of Extraction Patterns
The intuition behind our criteria for ranking gen-
eralized pattern is that patterns of higher preci-
sion tend to contain words that are indicative of
the relation being mined. Thus, a pattern is more
likely to produce good candidate facts if its in-
fix contains the words language or spoken if ex-
tracting Language-SpokenIn-Country facts, or the
word capital if extracting City-CapitalOf-Country
relations. In each acquisition iteration, the scor-
ing of patterns is a two-pass procedure. The first
pass computes the normalized frequencies of all
words excluding stopwords, over the entire set of
extraction patterns. The computation applies sep-
arately to the prefix, infix and postfix of the pat-
terns. In the second pass, the score of an extraction
pattern is determined by the words with the high-
est frequency score in its prefix, infix and postfix,
as computed in the first pass and adjusted for the
relative distance to the start and end of the infix.
3.3 Ranking of Candidate Facts
Figure 3 introduces a new scheme for assessing the
quality of the candidate facts, based on the compu-
tation of similarity scores for each candidate rela-
tive to the set of seed facts. A candidate fact, e.g.,
(Richard Steele, 1672), is similar to the seed set if
both its phrases, i.e., Richard Steele and 1672, are
similar to the corresponding phrases (John Lennon
or Stephen Foster in the case of Richard Steele)
from the seed facts. For a phrase of a candidate
fact to be assigned a non-default (non-minimum)
812
...
Lennon
Lambert
McFadden
Bateson
McNamara
Costello
Cronin
Wooley
Baker
...
Foster
Hansen
Hawkins
Fisher
Holloway
Steele
Sweeney
Chris
John
James
Andrew
Mike
Matt
Brian
Christopher
...
John Lennon         1940
Seed facts
Stephen Foster      1826
Brian McFadden           1980
(4)(3)
Robert S. McNamara    1916
(6)(5)
Barbara Steele               1937
(7) (2)
Stan Hansen                  1949
(9)(8)
Similar wordsSimilar words
for: John
Similar words
for: Stephen
for: Lennon
Similar words
for: Foster
...
Stephen
Robert
Michael
Peter
William
Stan
Richard(1)
Barbara
(3)
(5)
(7) (2)
(8)
(9)
(4)
(6)
(2)(1)
Candidate facts
Jethro Tull                     1947
Richard Steele               1672
Figure 3: The role of similarities in estimating the
quality of candidate facts
similarity score, the words at its extremities must
be similar to one or more words situated at the
same positions in the seed facts. This is the case
for the first five candidate facts in Figure 3. For ex-
ample, the first word Richard from one of the can-
didate facts is similar to the first word John from
one of the seed facts. Concurrently, the last word
Steele from the same phrase is similar to Foster
from another seed fact. Therefore Robert Foster
is similar to the seed facts. The score of a phrase
containing N words is:
{
C1 +
?N
i=1 log(1 + Simi) , if Sim1,N > 0
C2 , otherwise.
where Simi is the similarity of the component
word at position i in the phrase, and C1 and C2
are scaling constants such that C2C1. Thus,
the similarity score of a candidate fact aggregates
individual word-to-word similarity scores, for the
left side and then for the right side of a candidate
fact. In turn, the similarity score of a component
word Simi is higher if: a) the computed word-to-
word similarity scores are higher relative to words
at the same position i in the seeds; and b) the com-
ponent word is similar to words from more than
one seed fact.
The similarity scores are one of a linear com-
bination of features that induce a ranking over the
candidate facts. Three other domain-independent
features contribute to the final ranking: a) a phrase
completeness score computed statistically over the
entire set of candidate facts, which demotes candi-
date facts if any of their two sides is likely to be
incomplete (e.g., Mary Lou vs. Mary Lou Retton,
or John F. vs. John F. Kennedy); b) the average
PageRank value over all documents from which
the candidate fact is extracted; and c) the pattern-
based scores of the candidate fact. The latter fea-
ture converts the scores of the patterns extracting
the candidate fact into a score for the candidate
fact. For this purpose, it considers a fixed-length
window of words around each match of a candi-
date fact in some sentence from the text collection.
This is equivalent to analyzing all sentence con-
texts from which a candidate fact can be extracted.
For each window, the word with the highest fre-
quency score, as computed in the first pass of the
procedure for scoring the patterns, determines the
score of the candidate fact in that context. The
overall pattern-based score of a candidate fact is
the sum of the scores over all its contexts of occur-
rence, normalized by the frequency of occurrence
of the candidate over all sentences.
Besides inducing a ranking over the candidate
facts, the similarity scores also serve as a valida-
tion filter over the candidate facts. Indeed, any
candidates that are not similar to the seed set can
be filtered out. For instance, the elimination of
(Jethro Tull, 1947) is a side effect of verifying that
Tull is not similar to any of the last-position words
from phrases in the seed set.
4 Evaluation
4.1 Data
The source text collection consists of three chunks
W1, W2, W3 of approximately 100 million doc-
uments each. The documents are part of a larger
snapshot of the Web taken in 2003 by the Google
search engine. All documents are in English.
The textual portion of the documents is cleaned
of Html, tokenized, split into sentences and part-
of-speech tagged using the TnT tagger (Brants,
2000).
The evaluation involves facts of type Person-
BornIn-Year. The reasons behind the choice of
this particular type are threefold. First, many
Person-BornIn-Year facts are probably available
on the Web (as opposed to, e.g., City-CapitalOf-
Country facts), to allow for a good stress test
for large-scale extraction. Second, either side of
the facts (Person and Year) may be involved in
many other types of facts, such that the extrac-
tion would easily divergence unless it performs
correctly. Third, the phrases from one side (Per-
son) have an utility in their own right, for lexicon
813
Table 1: Set of seed Person-BornIn-Year facts
Name Year Name Year
Paul McCartney 1942 John Lennon 1940
Vincenzo Bellini 1801 Stephen Foster 1826
Hoagy Carmichael 1899 Irving Berlin 1888
Johann Sebastian Bach 1685 Bela Bartok 1881
Ludwig van Beethoven 1770 Bob Dylan 1941
construction or detection of person names.
The Person-BornIn-Year type is specified
through an initial set of 10 seed facts shown in Ta-
ble 1. Similarly to source documents, the facts are
also part-of-speech tagged.
4.2 System Settings
In each iteration, the case-insensitive matching of
the current set of seed facts onto the sentences pro-
duces basic patterns. The patterns are converted
into generalized patterns. The length of the infix
may vary between 1 and 6 words. Potential pat-
terns are discarded if the infix contains only stop-
words.
When a pattern is retained, it is used as an
infix-only pattern, and allowed to generate at most
600,000 candidate facts. At the end of an itera-
tion, approximately one third of the validated can-
didate facts are added to the current seed set. Con-
sequently, the acquisition expands the initial seed
set of 10 facts to 100,000 facts (after iteration 1)
and then to one million facts (after iteration 2) us-
ing chunk W1.
4.3 Precision
A separate baseline run extracts candidate facts
from the text collection following the traditional
iterative acquisition approach. Pattern general-
ization is disabled, and the ranking of patterns
and facts follows strictly the criteria and scoring
functions from (Thelen and Riloff, 2002), which
are also used in slightly different form in (Lita
and Carbonell, 2004) and (Agichtein and Gravano,
2000). The theoretical option of running thou-
sands of iterations over the text collection is not
viable, since it would imply a non-justifiable ex-
pense of our computational resources. As a more
realistic compromise over overly-cautious acqui-
sition, the baseline run retains as many of the top
candidate facts as the size of the current seed,
whereas (Thelen and Riloff, 2002) only add the
top five candidate facts to the seed set after each it-
eration. The evaluation considers all 80, a sample
of the 320, and another sample of the 10,240 facts
retained after iterations 3, 5 and 10 respectively.
The correctness assessment of each fact consists
in manually finding some Web page that contains
clear evidence that the fact is correct. If no such
page exists, the fact is marked as incorrect. The
corresponding precision values after the three iter-
ations are 91.2%, 83.8% and 72.9%.
For the purpose of evaluating the precision of
our system, we select a sample of facts from
the entire list of one million facts extracted from
chunk W1, ranked in decreasing order of their
computed scores. The sample is generated auto-
matically from the top of the list to the bottom, by
retaining a fact and skipping the following consec-
utive N facts, where N is incremented at each step.
The resulting list, which preserves the relative or-
der of the facts, contains 1414 facts. The 115 facts
for which a Web search engine does not return any
documents, when the name (as a phrase) and the
year are submitted together in a conjunctive query,
are discarded from the sample of 1414 facts. In
those cases, the facts were acquired from the 2003
snapshot of the Web, but queries are submitted to
a search engine with access to current Web doc-
uments, hence the difference when some of the
2003 documents are no longer available or index-
able.
Based on the sample set, the average preci-
sion of the list of one million facts extracted from
chunk W1 is 98.5% over the top 1/100 of the list,
93.1% over the top half of the list, and 88.3% over
the entire list of one million facts. Table 2 shows
examples of erroneous facts extracted from chunk
W1. Causes of errors include incorrect approxima-
tions of the name boundaries (e.g., Alma in Alma
Theresa Rausch is incorrectly tagged as an adjec-
tive), and selection of the wrong year as birth year
(e.g., for Henry Lumbar).
In the case of famous people, the extracted facts
tend to capture the correct birth year for several
variations of the names, as shown in Table 3. Con-
versely, it is not necessary that a fact occur with
high frequency in order for it to be extracted,
which is an advantage over previous approaches
that rely strongly on redundancy (cf. (Cafarella et
al., 2005)). Table 4 illustrates a few of the cor-
rectly extracted facts that occur rarely on the Web.
4.4 Recall
In contrast to the assessment of precision, recall
can be evaluated automatically, based on external
814
Table 2: Incorrect facts extracted from the Web
Spurious Fact Context in Source Sentence
(Theresa Rausch, Alma Theresa Rausch was born
1912) on 9 March 1912
(Henry Lumbar, Henry Lumbar was born 1861
1937) and died 1937
(Concepcion Paxety, Maria de la Concepcion Paxety
1817) b. 08 Dec. 1817 St. Aug., FL.
(Mae Yaeger, Ella May/Mae Yaeger was born
1872) 20 May 1872 in Mt.
(Charles Whatley, Long, Charles Whatley b. 16
1821) FEB 1821 d. 29 AUG
(HOLT George W. HOLT (new line) George W. Holt
Holt, 1845) was born in Alabama in 1845
(David Morrish David Morrish (new line)
Canadian, 1953) Canadian, b. 1953
(Mary Ann, 1838) had a daughter, Mary Ann, who
was born in Tennessee in 1838
(Mrs. Blackmore, Mrs. Blackmore was born April
1918) 28, 1918, in Labaddiey
Table 3: Birth years extracted for both
pseudonyms and corresponding real names
Pseudonym Real Name Year
Gloria Estefan Gloria Fajardo 1957
Nicolas Cage Nicolas Kim Coppola 1964
Ozzy Osbourne John Osbourne 1948
Ringo Starr Richard Starkey 1940
Tina Turner Anna Bullock 1939
Tom Cruise Thomas Cruise Mapother IV 1962
Woody Allen Allen Stewart Konigsberg 1935
lists of birth dates of various people. We start by
collecting two gold standard sets of facts. The first
set is a random set of 609 actors and their birth
years from a Web compilation (GoldA). The sec-
ond set is derived from the set of questions used
in the Question Answering track (Voorhees and
Tice, 2000) of the Text REtrieval Conference from
1999 through 2002. Each question asking for the
birth date of a person (e.g., ?What year was Robert
Frost born??) results in a pair containing the per-
son?s name and the birth year specified in the an-
swer keys. Thus, the second gold standard set
contains 17 pairs of people and their birth years
(GoldT ). Table 5 shows examples of facts in each
of the gold standard sets.
Table 6 shows two types of recall scores com-
puted against the gold standard sets. The recall
scores over ?Gold take into consideration only the
set of person names from the gold standard with
some extracted year(s). More precisely, given that
some years were extracted for a person name, it
verifies whether they include the year specified in
the gold standard for that person name. Compar-
atively, the recall score denoted AllGold is com-
Table 4: Extracted facts that occur infrequently
Fact Source Domain
(Irvine J Forcier, 1912) geocities.com
(Marie Louise Azelie Chabert, 1861) vienici.com
(Jacob Shalles, 1750) selfhost.com
(Robert Chester Claggett, 1898) rootsweb.com
(Charoltte Mollett, 1843) rootsweb.com
(Nora Elizabeth Curran, 1979) jimtravis.com
Table 5: Composition of gold standard sets
Gold Set Composition and Examples of Facts
GoldA Actors (Web compilation) Nr. facts: 609
(Andie MacDowell, 1958), (Doris Day,
1924), (Diahann Carroll, 1935)
GoldT People (TREC QA track) Nr. facts: 17
(Davy Crockett, 1786), (Julius Caesar,
100 B.C.), (King Louis XIV, 1638)
puted over the entire set of names from the gold
standard.
For the GoldA set, the size of the ?Gold set of
person names changes little when the facts are ex-
tracted from chunk W1 vs. W2 vs. W3. The re-
call scores over ?Gold exhibit little variation from
one Web chunk to another, whereas the AllGold
score is slightly higher on the W3 chunk, prob-
ably due to a higher number of documents that
are relevant to the extraction task. When the facts
are extracted from a combination of two or three
of the available Web chunks, the recall scores
computed over AllGold are significantly higher as
the size of the ?Gold set increases. In compar-
ison, the recall scores over the growing ?Gold
set increases slightly with larger evaluation sets.
The highest value of the recall score for GoldA
is 89.9% over the ?Gold set, and 70.7% over
AllGold. The smaller size of the second gold stan-
dard set, GoldT , explains the higher variation of
the values shown in the lower portion of Table 6.
4.5 Comparison to Previous Results
Another recent approach specifically addresses the
problem of extracting facts from a similarly-sized
collection of Web documents. In (Cafarella et al,
2005), manually-prepared extraction rules are ap-
plied to a collection of 60 million Web documents
to extract entities of types Company and Country,
as well as facts of type Person-CeoOf-Company
and City-CapitalOf-Country. Based on manual
evaluation of precision and recall, a total of 23,128
company names are extracted at precision of 80%;
the number decreases to 1,116 at precision of 90%.
In addition, 2,402 Person-CeoOf-Company facts
815
Table 6: Automatic evaluation of recall, over two
gold standard sets GoldA (609 person names) and
GoldT (17 person names)
Gold Set Input Data Recall (%)
(Web Chunk) ?Gold AllGold
GoldA W1 86.4 49.4
W2 85.0 50.5
W3 86.3 54.1
W1+W2 88.5 64.5
W1+W2+W3 89.9 70.7
GoldT W1 81.8 52.9
W2 90.0 52.9
W3 100.0 64.7
W1+W2 81.8 52.9
W1+W2+W3 91.6 64.7
are extracted at precision 80%. The recall value is
80% at precision 90%. Recall is evaluated against
the set of company names extracted by the system,
rather than an external gold standard with pairs of
a CEO and a company name. As such, the result-
ing metric for evaluating recall used in (Cafarella
et al, 2005) is somewhat similar to, though more
relaxed than, the recall score over the ?Gold set
introduced in the previous section.
5 Conclusion
The combination of generalized extraction pat-
terns and similarity-driven ranking criteria results
in a fast-growth iterative approach for large-scale
fact extraction. From 10 Person-BornIn-Year facts
and no additional knowledge, a set of one million
facts of the same type is extracted from a collec-
tion of 100 million Web documents of arbitrary
quality, with a precision around 90%. This cor-
responds to a growth ratio of 100,000:1 between
the size of the extracted set of facts and the size
of the initial set of seed facts. To our knowledge,
the growth ratio and the number of extracted facts
are several orders of magnitude higher than in any
of the previous studies on fact extraction based on
either hand-written extraction rules (Cafarella et
al., 2005), or bootstrapping for relation and infor-
mation extraction (Agichtein and Gravano, 2000;
Lita and Carbonell, 2004). The next research steps
converge towards the automatic construction of a
searchable repository containing billions of facts
regarding people.
References
E. Agichtein and L. Gravano. 2000. Snowball: Extracting
relations from large plaintext collections. In Proceedings
of the 5th ACM International Conference on Digital Li-
braries (DL-00), pages 85?94, San Antonio, Texas.
T. Brants. 2000. TnT - a statistical part of speech tagger.
In Proceedings of the 6th Conference on Applied Natural
Language Processing (ANLP-00), pages 224?231, Seattle,
Washington.
M. Cafarella, D. Downey, S. Soderland, and O. Etzioni.
2005. KnowItNow: Fast, scalable information extrac-
tion from the web. In Proceedings of the Human Lan-
guage Technology Conference (HLT-EMNLP-05), pages
563?570, Vancouver, Canada.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of the 1999
Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora (EMNLP/VLC-99),
pages 189?196, College Park, Maryland.
M. Fleischman, E. Hovy, and A. Echihabi. 2003. Offline
strategies for online question answering: Answering ques-
tions before they are asked. In Proceedings of the 41st
Annual Meeting of the Association for Computational Lin-
guistics (ACL-03), pages 1?7, Sapporo, Japan.
G. Grefenstette. 1994. Explorations in Automatic Thesaurus
Discovery. Kluwer Academic Publishers, Boston, Mas-
sachusetts.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discover-
ing relations among named entities from large corpora. In
Proceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-04), pages 415?
422, Barcelona, Spain.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of the 28th Annual
Meeting of the Association for Computational Linguistics
(ACL-90), pages 268?275, Pittsburgh, Pennsylvania.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of the 17th International Confer-
ence on Computational Linguistics and the 36th Annual
Meeting of the Association for Computational Linguistics
(COLING-ACL-98), pages 768?774, Montreal, Quebec.
L. Lita and J. Carbonell. 2004. Instance-based ques-
tion answering: A data driven approach. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-04), pages 396?403,
Barcelona, Spain.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clus-
tering of english words. In Proceedings of the 31st Annual
Meeting of the Association for Computational Linguistics
(ACL-93), pages 183?190, Columbus, Ohio.
E. Riloff and R. Jones. 1999. Learning dictionaries for in-
formation extraction by multi-level bootstrapping. In Pro-
ceedings of the 16th National Conference on Artificial In-
telligence (AAAI-99), pages 474?479, Orlando, Florida.
M. Stevenson and M. Greenwood. 2005. A semantic ap-
proach to IE pattern induction. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL-05), pages 379?386, Ann Arbor, Michigan.
M. Thelen and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern con-
texts. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-02),
pages 214?221, Philadelphia, Pennsylvania.
E.M. Voorhees and D.M. Tice. 2000. Building a question-
answering test collection. In Proceedings of the 23rd
International Conference on Research and Development
in Information Retrieval (SIGIR-00), pages 200?207,
Athens, Greece.
816
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 501?509,
Beijing, August 2010
FactRank: Random Walks on a Web of Facts
Alpa Jain
Yahoo! Labs
alpa@yahoo-inc.com
Patrick Pantel
Microsoft Research
ppantel@microsoft.com
Abstract
Fact collections are mostly built using
semi-supervised relation extraction tech-
niques and wisdom of the crowds meth-
ods, rendering them inherently noisy. In
this paper, we propose to validate the re-
sulting facts by leveraging global con-
straints inherent in large fact collections,
observing that correct facts will tend to
match their arguments with other facts
more often than with incorrect ones. We
model this intuition as a graph-ranking
problem over a fact graph and explore
novel random walk algorithms. We
present an empirical study, over a large set
of facts extracted from a 500 million doc-
ument webcrawl, validating the model and
showing that it improves fact quality over
state-of-the-art methods.
1 Introduction
Fact bases, such as those contained in Freebase,
DBpedia, KnowItAll, and TextRunner, are increas-
ingly burgeoning on the Internet, in government,
in high tech companies and in academic laborato-
ries. Bar the accurate manual curation typified by
Cyc (Lenat, 1995), most fact bases are built us-
ing either semi-supervised techniques or wisdom
of the crowds techniques, rendering them inher-
ently noisy. This paper describes algorithms to
validate and re-rank fact bases leveraging global
constraints imposed by the semantic arguments
predicated by the relations.
Facts are defined as instances of n-ary typed re-
lations such as acted-in?movie, actor?, director-
of?movie, director?, born-in?person, date?, and
buy?person, product, person?. In all but very
small fact bases, relations share an argument
type, such as movie for the relations acted-in and
director-of in the above example. The hypothesis
explored in this paper is that when two fact in-
stances from two relations share the same value
for a shared argument type, then the validity of
both facts should be increased. Conversely, we
also hypothesize that an incorrect fact instance
will tend to match a shared argument with other
facts far less frequently. For example, consider
the following four facts from the relations acted-
in, director-of, and is-actor:
t1: acted-in?Psycho, Anthony Perkins?
t2: *acted-in?Walt Disney Pictures, Johnny Depp?
t3: director-of?Psycho, Alfred Hitchcock?
t4: is-actor?Anthony Perkins?
Our confidence in the validity of t1 increases
with the knowledge of t3 and t4 since the argu-
ment movie is shared with t3 and actor with t4.
Similarly, t1 increases our confidence in the va-
lidity of t3 and t4. For t2, we expect to find few
facts that will match a movie argument with Walt
Disney Pictures. Facts that share the actor argu-
ment Johnny Depp with t2 will increase its valid-
ity, but the lack of matches on its movie argument
will decrease its validity.
In this paper, we present FactRank, which for-
malizes the above intuitions by constructing a fact
graph and running various random walk graph-
ranking algorithms over it to re-rank and validate
the facts. A collection of facts is modeled in the
form of a graph where nodes are fact instances and
edges connect nodes that have the same value for a
shared argument type (e.g., t1 would be linked by
an edge to both t3 and t4.) Given a graph represen-
tation of facts, we explore various random walk
algorithms to propagate our confidence in individ-
ual facts through the web of facts. We explore
algorithms such as PageRank (Page et al, 1999)
as well as propose novel algorithms that lever-
age several unique characteristics of fact graphs.
Finally, we present an empirical analysis, over a
large collection of facts extracted from a 500 mil-
501
lion document webcrawl, supporting our model
and confirming that global constraints in a fact
base can be leveraged to improve the quality of
the facts. Our proposed algorithms are agnostic to
the sources of a fact base, however our reported
experiments were carried over a state-of-the-art
semi-supervised extraction system. In summary,
the main contributions of this paper are:
? We formalize the notion of ranking facts in
a holistic manner by applying graph-based
ranking algorithms (Section 2).
? We propose novel ranking algorithms using
random walk models on facts (Section 3).
? We establish the effectiveness of our ap-
proach through an extensive experimental
evaluation over a real-life dataset and show
improvements over state-of-the-art ranking
methods (Section 4).
2 Fact Validation Revisited
We denote an n-ary relation r with typed argu-
ments t1, t2, ? ? ? , tn as r?t1, t2, ? ? ? tn?. In this pa-
per, we limit our focus to unary and binary re-
lations. A fact is an instance of a relation. For
example, acted-in?Psycho, Anthony Perkins? is a
fact from the acted-in?movie, actor? relation.
Definition 2.1 [Fact base]: A fact base is a col-
lection of facts from several relations. Textrunner
and Freebase are example fact bases (note that
they also contain knowledge beyond facts such as
entity lists and ontologies.) 2
Definition 2.2 [Fact farm]: A fact farm is a sub-
set of interconnected relations in a fact base that
share arguments among them. 2
For example, consider a fact base consisting of
facts for relations involving movies, organiza-
tions, products, etc., of which the relations acted-
in and director-of could form a MOVIES fact farm.
Real-world fact bases are built in many ways.
Semi-supervised relation extraction methods in-
clude KnowItAll (Etzioni et al, 2005), TextRun-
ner (Banko and Etzioni, 2008), and many others
such as (Riloff and Jones, 1999; Pantel and Pen-
nacchiotti, 2006; Pas?ca et al, 2006; Mintz et al,
2009). Wisdom of the crowds methods include
DBpedia (Auer et al, 2008) and Freebase which
extracts facts from various open knowledge bases
and allow users to add or edit its content.
Most semi-supervised relation extraction meth-
ods follow (Hearst, 1992). Starting with a rela-
tively small set of seed facts, these extractors it-
eratively learn patterns that can be instantiated to
identify new facts. To reflect their confidence in
an extracted fact, extractors assign an extraction
score with each fact. Methods differ widely in
how they define the extraction score. Similarly,
many extractors assign a pattern score to each
discovered pattern. In each iteration, the high-
est scoring patterns and facts are saved, which are
used to seed the next iteration. After a fixed num-
ber of iterations or when a termination condition
is met, the instantiated facts are ranked by their
extraction score.
Several methods have been proposed to gen-
erate such ranked lists (e.g., (Riloff and Jones,
1999; Banko and Etzioni, 2008; Matuszek et al,
2005; Pantel and Pennacchiotti, 2006; Pas?ca et al,
2006). In this paper, we re-implement the large-
scale state-of-the-art method proposed by Pas?ca et
al. (2006). This pattern learning method generates
binary facts and computes the extraction scores of
a fact based on (a) the scores of the patterns that
generated it, and (b) the distributional similarity
score between the fact and the seed facts. We
computed the distributional similarity between ar-
guments using (Pantel et al, 2009) over a large
crawl of the Web (described in Section 4.1). Other
implementation details follow (Pas?ca et al, 2006).
In our experiments, we observed some interest-
ing ranking problems as illustrated by the follow-
ing example facts for the acted-in relation:
id: Facts (#Rank)
t1: acted-in?Psycho, Anthony Perkins? (#26)
t2: *acted-in?Walt Disney Pictures, Johnny Depp? (#9)
Both t1 and t2 share similar contexts in documents
(e.g., ?movie? film starring ?actor? and ?movie?
starring ?actor?), and this, in turn, boosts the
pattern-based component of the extraction scores
for t1. Furthermore, due to the ambiguity of the
term psycho, the distributional similarity-based
component of the scores for fact t2 is also lower
than that for t1.
502
Relations id : Facts
acted-in t1 : ?Psycho, Anthony Perkins?
t2 : *?Walt Disney Pictures, Johnny Depp?
director-of t3 : ?Psycho, Alfred Hitchcock?
producer-of t4 : ?Psycho, Hilton Green?
is-actor t5 : ?Anthony Perkins?
t6 : ?Johnny Depp?
is-director t7 : ?Alfred Hitchcock?
is-movie t8 : ?Psycho?
Table 1: Facts share arguments across relations
which can be exploited for validation.
Our work in this paper is motivated by the
following observation: the ranked list generated
by an individual extractor does not leverage any
global information that may be available when
considering a fact farm in concert. To under-
stand the information available in a fact farm,
consider a MOVIES fact farm consisting of rela-
tions, such as, acted-in, director-of, producer-of,
is-actor, is-movie, and is-director. Table 1 lists
sample facts that were generated in our experi-
ments for these relations1. In this example, we
observe that for t1 there exist facts in foreign re-
lations, namely, director-of and producer-of that
share the same value for the Movie argument, and
intuitively, facts t3 and t4 add to the validity of t1.
Furthermore, t1 shares the same value for the Ac-
tor argument with t5. Also, t3, which is expected
to boost the validity of t1, itself shares values for
its arguments with facts t4 and t7, which again in-
tuitively adds to the validity of t1. In contrast to
this web of facts generated for t1, the fact t2 shares
only one of its argument value with one other fact,
i.e., t6.
The above example underscores an important
observation: How does the web of facts gener-
ated by a fact farm impact the overall validity of
a fact? To address this question, we hypothesize
that facts that share arguments with many facts are
more reliable than those that share arguments with
few facts. To capture this hypothesis, we model a
web of facts for a farm using a graph-based repre-
sentation. Then, using graph analysis algorithms,
we propagate reliability to a fact using the scores
of other facts that recursively connect to it.
Starting with a fact farm, to validate the facts in
each consisting relation, we:
1The is-actor?actor?, is-director?director?, and is-movie?movie? rela-
tions are equivalent to the relation is-a?c-instance, class? where class ?
{actor, director,movie}.
(1) Identify arguments common to relations in the farm.
(2) Run extraction methods to generate each relation.
(3) Construct a graph-based representation of the extracted
facts using common arguments identified in Step (1)
(see Section 3.1 for details on constructing this graph.)
(4) Perform link analysis using random walk algorithms
over the generated graph, propagating scores to each
fact through the interconnections (see Section 3.2 for
details on various proposed random walk algorithms).
(5) Rank facts in each relation using the scores generated
in Step (4) or by combining them with the original ex-
traction scores.
For the rest of the paper, we focus on generating
better ranked lists than the original rankings pro-
posed by a state-of-the-art extractor.
3 FactRank: Random Walk on Facts
Our approach considers a fact farm holistically,
leveraging the global constraints imposed by the
semantic arguments of the facts in the farm. We
model this idea by constructing a graph represen-
tation of the facts in the farm (Section 3.1) over
which we run graph-based ranking algorithms.
We give a brief overview of one such ranking al-
gorithm (Section 3.2) and present variations of it
for fact re-ranking (Section 3.3). Finally, we in-
corporate the original ranking from the extractor
into the ranking produced by our random walk
models (Section 3.4).
3.1 Graph Representation of Facts
Definition 3.1 We define a fact graph FG(V, E),
with V nodes and E edges, for a fact farm, as a
graph containing facts as nodes and a set of edges
between these nodes. An edge between nodes vi
and vj indicates that the facts share the same
value for an argument that is common to the re-
lations that vi and vj belong to. 2
Figure 1 shows the fact graph for the example
in Table 1 centered around the fact t1.
Note on the representation: The above graph
representation is just one of many possible op-
tions. For instance, instead of representing facts
by nodes, nodes could represent the arguments of
facts (e.g., Psycho) and nodes could be connected
by edges if they occur together in a fact. The task
of studying a ?best? representation remains a fu-
ture work direction. However, we believe that our
proposed methods can be easily adapted to other
such graph representations.
503
<Ps
ych
o, A
ntho
ny P
erki
ns>
<Ps
ych
o, m
ovie
>
<Ps
ych
o, H
ilto
n G
reen
>
<Al
fred
 Hit
chc
hoc
k, d
irec
tor>
<An
thon
y P
erki
ns, 
acto
r>
<Ps
ych
o, A
lfre
d H
itch
coc
k>
Figure 1: Fact graph centered around t1 in Table 1.
3.2 The FactRank Hypothesis
We hypothesize that connected facts increase our
confidence in those facts. We model this idea
by propagating extraction scores through the fact
graph similarly to how authority is propagated
through a hyperlink graph of the Web (used to es-
timate the importance of a webpage). Several link
structure analysis algorithms have been proposed
for this goal, of which we explore a particular ex-
ample, namely, PageRank (Page et al, 1999). The
premise behind PageRank is that given the hyper-
link structure of the Web, when a page v generates
a link to page u, it confers some of its importance
to u. Therefore, the importance of a webpage u
depends on the number of pages that link to u and
furthermore, on the importance of the pages that
link to u. More formally, given a directed graph
G = (V,E) with V vertices and E edges, let I(u)
be the set of nodes that link to a node u and O(v)
be the set of nodes linked by v. Then, the impor-
tance of a node u is defined as:
p(u) =
X
v?I(u)
p(v)
|O(v)| (1)
The PageRank algorithm iteratively updates the
scores for each node in G and terminates when a
convergence threshold is met. To guarantee the al-
gorithm?s convergence, G must be irreducible and
aperiodic (i.e., a connected graph). The first con-
straint can be easily met by converting the adja-
cency matrix for G into a stochastic matrix (i.e.,
all rows sum up to 1.) To address the issue of peri-
odicity, Page et al (1999) suggested the following
modification to Equation 1:
p(u) = 1 ? d|V | + d ?
X
v?I(u)
p(v)
|O(v)| (2)
where d is a damping factor between 0 and 1,
which is commonly set to 0.85. Intuitively, Page-
Rank can be viewed as modeling a ?random
walker? on the nodes inG and the score of a node,
i.e., PageRank, determines the probability of the
walker arriving at this node.
While our method makes use of the PageRank
algorithm, we can also use other graph analysis
algorithms (e.g., HITS (Kleinberg, 1999)). A par-
ticularly important property of the PageRank al-
gorithm is that the stationary scores can be com-
puted for undirected graphs in the same manner
described above, after replacing each undirected
edge by a bi-directed edge. Recall that the edges
in a fact graph are bi-directional (see Figure 1).
3.3 Random Walk Models
Below, we explore various random walk models
to assign scores to each node in a fact graph FG.
3.3.1 Model Implementations
Pln: Our first method applies the traditional Page-
Rank model to FG and computes the score of a
node u using Equation 2.
Traditional PageRank, as is, does not make use
of the strength of the links or the nodes connected
by an edge. Based on this observation, researchers
have proposed several variations of the PageRank
algorithm in order to solve their problems. For
instance, variations of random walk algorithms
have been applied to the task of extracting impor-
tant words from a document (Hassan et al, 2007),
for summarizing documents (Erkan and Radev,
2004), and for ordering user preferences (Liu and
Yang, 2008). Following the same idea, we build
upon the discussion in Section 3.2 and present
random walk models that incorporate the strength
of an edge.
Dst: One improvement over Pln is to distinguish
between nodes in FG using the extraction scores
of the facts associated with them: extraction meth-
ods such as our reimplementation of (Pas?ca et al,
2006) assign scores to each output fact to reflect
its confidence in it (see Section 3.2). Intuitively, a
higher scoring node that connects to u should in-
crease the importance of umore than a connection
from a lower scoring node. Let I(u) be the set of
nodes that link to u and O(v) be the set of nodes
504
linked by v. Then, if w(u) is the extraction score
for the fact represented by node u, the score for
node u is defined:
p(u) = 1 ? d|V | + d ?
X
v?I(u)
w(v) ? p(v)
|O(v)| (3)
where w(v) is the confidence score for the fact
represented by v. Naturally, other (externally de-
rived) extraction scores can also be substituted for
w(v).
Avg: We can further extend the idea of deter-
mining the strength of an edge by combining the
extraction scores of both nodes connected by an
edge. Specifically,
p(u) = 1 ? d|V | + d ?
X
v?I(u)
avg(u, v) ? p(v)
|O(v)| (4)
where avg(u, v) is the average of the extraction
scores assigned to the facts associated with nodes
u and v.
Nde: In addition to using extraction scores, we
can also derive the strength of a node depending
on the number of distinct relations it connects to.
For instance, in Figure 1, t1 is linked to four dis-
tinct relations, namely, director-of, producer-of,
is-actor, is-movie, whereas, t2 is linked to one re-
lation, namely, is-actor. We compute p(u) as:
p(u)=1 ? d|V | +d ?
X
v?I(u)
(? ? w(v)+(1 ? ?) ? r(v)) ? p(v)
|O(v)| (5)
where w(v) is the confidence score for node v and
r(v) is the fraction of total number of relations in
the farm that contain facts with edges to v.
3.3.2 Dangling nodes
In traditional hyperlink graphs for the Web,
dangling nodes (i.e., nodes with no associated
edges) are considered to be of low importance
which is appropriately represented by the scores
computed by the PageRank algorithm. How-
ever, an important distinction from this setting is
that fact graphs are sparse causing them to have
valid facts with no counterpart matching argu-
ments in other relation, thus rendering them dan-
gling. This may be due to several reasons, e.g.,
extractors often suffer from less than perfect recall
and they may miss valid facts. In our experiments,
about 10% and 40% of nodes from acted-in and
director-of, respectively, were dangling nodes.
Handling dangling nodes in our extraction-
based scenario is a particularly challenging issue:
while demoting the validity of dangling nodes
could critically hurt the quality of the facts, lack
of global information prevents us from systemati-
cally introducing them into the re-ranked lists. We
address this issue by maintaining the original rank
positions when re-ranking dangling nodes.
3.4 Incorporating Extractor Ranks
Our proposed random walk ranking methods ig-
nore the ranking information made available by
the original relation extractor (e.g., (Pas?ca et al,
2006) in our implementation). Below, we pro-
pose two ways of combining the ranks suggested
by the original ranked list O and the re-ranked list
G, generated using the algorithms in Section 3.3.
R-Avg: The first combination method computes
the average of the ranks obtained from the two
lists. Formally, if O(i) is the original rank for fact
i and G(i) is the rank for i in the re-ranked list,
the combined rank M(i) is computed as:
M(i) = O(i) +G(i)2 (6)
R-Wgt: The second method uses a weighted aver-
age of the ranks from the individual lists:
M(i) = wo ? O(i) + (1 ? wo) ? G(i)2 (7)
In practice, this linear combination can be learned;
in our experiments, we set them towo = 0.4 based
on our observations over an independent training
set. Several other combination functions could
also be applied to this task. For instance, we ex-
plored the min and max functions but observed lit-
tle improvements.
4 Experimental Evaluation
4.1 Experimental Setup
Extraction method: For our extraction method,
we reimplemented the method described in (Pas?ca
et al, 2006) and further added a validation layer
on top of it based on Wikipedia (we boosted the
scores of a fact if there exists a Wikipedia page
for either of the fact?s arguments, which mentions
the other argument.) This state-of-the-art method
forms a strong baseline in our experiments.
Corpus and farms: We ran our extractor over a
large Web crawl consisting of 500 million English
505
25000 2000025000 150002000025000
f nodes
10000150002000025000
mber o
0500010000150002000025000
Nu
0500010000150002000025000
1
2
4
8
16
32
64
128
256
Node 
degree
0500010000150002000025000
1
2
4
8
16
32
64
128
256
Node 
degree
Figure 2: Degree distribution for MOVIES.
webpages crawled by the Yahoo! search engine.
We removed paragraphs containing fewer than 50
tokens and then removed all duplicate sentences.
The resulting corpus consists of over 5 million
sentences. We defined a farm, MOVIES, with rela-
tions, acted-in, director-of, is-movie, is-actor, and
is-director.
Evaluation methodology: Using our extraction
method over the Web corpus, we generate over
100,000 facts for the above relations. However, to
keep our evaluation manageable, we draw a ran-
dom sample from these facts. Specifically, we
first generate a ranked list using the extraction
scores output by our extractor. We will refer to
this method as Org (original). We then generate
a fact graph over which we will run our methods
from Section 3.3 (each of which will re-rank the
facts). Figure 2 shows the degree, i.e., number
of edges, distribution of the fact graph generated
for MOVIES. We ran Avg, Dst, Nde, R-Avg, and
R-Wgt on this fact graph and using the scores we
re-rank the facts for each of the relations. In Sec-
tion 4.2, we will discuss our results for the acted-
in and director-of relations.
Fact Verification: To verify whether a fact is
valid or not, we recruit human annotators using
the paid service Mechanical Turk. For each fact,
two annotations were requested (keeping the total
cost under $100). The annotators were instructed
to mark incorrect facts as well as disallow any val-
ues that were not ?well-behaved.? For instance,
acted-in?Godfather, Pacino? is correct, but acted-
in?The, Al Pacino? is incorrect. We manually ad-
judicated 32% of the facts where the judges dis-
agreed.
Evaluation metrics: Using the annotated facts,
we construct a goldset S of facts and compute the
precision of a list L as: |L?S||S| . To compare theeffectiveness of the ranked lists, we use average
precision, a standard measure in information re-
trieval for evaluating ranking algorithms, defined
Method Average precision
30% 50% 100%
Org 0.51 0.39 0.38
Pln 0.44 0.35 0.32
Avg 0.55 0.44 0.42
Dst 0.54 0.44 0.41
Nde 0.53 0.40 0.41
R-Avg 0.58 0.46 0.45
R-Wgt 0.60 0.56 0.44
Table 2: Average precision for acted-in for vary-
ing proportion of fact graph of MOVIES.
Method Average precision
30% 50% 100%
Org 0.64 0.69 0.66
Pln 0.69 0.67 0.59
Avg 0.69 0.70 0.64
Dst 0.67 0.69 0.64
Nde 0.69 0.69 0.64
R-Avg 0.70 0.70 0.64
R-Wgt 0.71 0.71 0.69
Table 3: Average precision for director-of for
varying proportion of fact graph of MOVIES.
as: Ap(L) =
P|L|
i=1 P (i)?isrel(i)
P|L|
i=1 isrel(i)
, where P (i) is the
precision of L at rank i, and isrel(i) is 1 if the fact
at rank i is in S, and 0 otherwise. We also study
the precision values at varying ranks in the list.
For robustness, we report the results using 10-fold
cross validation.
4.2 Experimental Results
Effectiveness of graph-based ranking: Our
first experiment studies the overall quality of the
ranked lists generated by each method. Table 2
compares the average precision for acted-in, with
the maximum scores highlighted for each column.
We list results for varying proportions of the orig-
inal fact graph (30%, 50%, and 100%). Due to
our small goldset sizes, these results are not sta-
tistically significant over Org, however we con-
sistently observed a positive trend similar to those
reported in Table 2 over a variety of evaluation
sets generated by randomly building 10-folds of
all the facts.
Overall, the Avg method offers a competitive
alternative to the original ranked list generated
by the extractor Org: not only are the average
precision values for Avg higher than Org, but
as we will see later, the rankings generated by
our graph-based methods exhibits some positive
unique characteristics. These experiments also
506
R Org Pln Avg Dst Nde R-Avg R-Wgt
5 0.44 0.40 0.52 0.48 0.40 0.52 0.56
10 0.36 0.36 0.42 0.38 0.36 0.36 0.36
15 0.287 0.24 0.30 0.28 0.26 0.30 0.30
20 0.26 0.26 0.26 0.26 0.26 0.27 0.27
21 0.27 0.27 0.27 0.27 0.27 0.27 0.27
Table 4: Precision at varying ranks for the acted-
in relation (R stands for Ranks).
R Org Pln Avg Dst Nde R-Avg R-Wgt
5 0.58 0.68 0.70 0.68 0.64 0.66 0.70
10 0.60 0.57 0.59 0.58 0.59 0.6 0.69
15 0.57 0.53 0.58 0.56 0.56 0.56 0.60
20 0.57 0.57 0.58 0.58 0.58 0.58 0.60
25 0.60 0.54 0.56 0.57 0.56 0.57 0.57
30 0.57 0.57 0.57 0.57 0.57 0.58 0.59
33 0.56 0.56 0.56 0.56 0.56 0.56 0.56
Table 5: Precision at varying ranks for the
director-of relation (R stands for Ranks).
confirm our initial observations: using traditional
PageRank (Pln) is not desirable for the task of re-
ranking facts (see Section 3.3). Our modifications
to the PageRank algorithm (e.g., Avg, Dst, Nde)
consistently outperform the traditional PageRank
algorithm (Pln). The results also underscore the
benefit of combining the original extractor ranks
with those generated by our graph-based rank-
ing algorithms with R-Wgt consistently leading to
highest or close to the highest average precision
scores.
In Table 3, we show the average precision val-
ues for director-of. In this case, the summary
statistic, average precision, does not show many
differences between the methods. To take a finer
look into the quality of these rankings, we investi-
gated the precision scores at varying ranks across
the methods. Table 4 and Table 5 show the preci-
sion at varying ranks for acted-in and director-of
respectively. The maximum precision values for
each rank are highlighted.
For acted-in again we see that Avg, R-Avg, R-
Wgt outperform Org and Pln at all ranks, and
Dst outperforms Org at two ranks. While the
method Nde outperforms Org for a few cases, we
expected it to perform better. Error analysis re-
vealed that the sparsity of our fact graph was the
problem. In our MOVIES fact graph, we observed
very few nodes that are linked to all possible re-
lation types, and the scores used by Nde rely on
being able to identify nodes that link to numer-
ous relation types. This problem can be alleviated
#Relation Avg Dst Nde
2 0.35 0.34 0.33
3 0.35 0.35 0.34
4 0.37 0.36 0.35
5 0.38 0.38 0.37
6 0.42 0.41 0.41
Table 6: Average precision for acted-in for vary-
ing number of relations in the MOVIES fact farm.
by reducing the sparsity of the fact graphs (e.g.,
by allowing edges between nodes that are ?simi-
lar enough?), which we plan to explore as future
work. For director-of, Table 5 now shows that for
small ranks (less than 15), a small (but consistent
in our 10-folds) improvement is observed when
comparing our random walk algorithms over Org.
While our proposed algorithms show a con-
sistent improvement for acted-in, the case of
director-of needs further discussion. For both av-
erage precision and precision vs. rank values, Avg,
R-Avg, and R-Wgt are similar or slightly better
than Org. We observed that the graph-based algo-
rithms tend to bring together ?clusters? of noisy
facts that may be spread out in the original ranked
list of facts. To illustrate this point, we show the
ten lowest scoring facts for the director-of rela-
tion. Table 7 shows these ten facts for Org as well
as Avg. These examples highlight the ability of
our graph-based algorithms to demote noisy facts.
Effect of number of relations: To understand
the effect of the number of relations in a farm
(and hence connectivity in a fact graph), we veri-
fied the re-ranking quality of our proposed meth-
ods on various subsets of the MOVIES fact farm.
We generated five different subsets, one with 2 re-
lations, another with 3 relations, and three more
with four, five, and six relations (note that al-
though we have 5 relations in the farm, is-movie
can be used in combination with both acted-in
and director-of, thus yielding six relations to ab-
late.) Table 6 shows the results for acted-in. Over-
all, performance improves as we introduce more
relations (i.e., more connectivity). Once again,
we observe that the performance deteriorates for
sparse graphs: using very few relations results in
degenerating the average precision of the original
ranked list. The issue of identifying the ?right?
characteristics of the fact graph (e.g., number of
relations, degree distribution, etc.) remains future
work.
507
Org Avg
?david mamet, bob rafelson? ? drama, nicholas ray?
?cinderella, wayne sleep? ? drama, mitch teplitsky official?
?mozartdie zauberflte, julie taymor? ? hollywood, marta bautis?
?matthew gross, julie taymor? ? hollywood, marek stacharski?
?steel magnolias, theater project? ? drama, kirk shannon-butts?
?rosie o?donnell, john badham? ? drama, john pietrowski?
?my brotherkeeper, john badham? ? drama, john madden starring?
?goldie hawn, john badham? ? drama, jan svankmajer?
?miramaxbad santa, terry zwigoff? ? drama, frankie sooknanan?
?premonition, alan rudolph? ? drama, dalia hager?
Table 7: Sample facts for director-of at the bot-
tom of the ranked list generated by (a) Org and
(b) Avg.
Evaluation conclusion: We demonstrated the ef-
fectiveness of our graph-based algorithms for re-
ranking facts. In general, Avg outperforms Org
and Pln, and we can further improve the perfor-
mance by using a combination-based ranking al-
gorithm such as R-Wgt. We also studied the im-
pact of the size of the fact graphs on the quality
of the ranked lists and showed that increasing the
density of the fact farms improves the ranking us-
ing our methods.
5 Related Work
Information extraction from text has received sig-
nificant attention in the recent years (Cohen and
McCallum, 2003). Earlier approaches relied
on hand-crafted extraction rules such as (Hearst,
1992), but recent efforts have developed su-
pervised and semi-supervised extraction tech-
niques (Riloff and Jones, 1999; Agichtein and
Gravano, 2000; Matuszek et al, 2005; Pan-
tel and Pennacchiotti, 2006; Pas?ca et al, 2006;
Yan et al, 2009) as well as unsupervised tech-
niques (Davidov and Rappoport, 2008; Mintz
et al, 2009). Most common methods today
use semi-supervised pattern-based learning ap-
proaches that follow (Hearst, 1992), as dis-
cussed in Section 2. Recent work has also ex-
plored extraction-related issues such as, scal-
ability (Pas?ca et al, 2006; Ravichandran and
Hovy, 2002; Pantel et al, 2004; Etzioni et al,
2004), learning extraction schemas (Cafarella et
al., 2007a; Banko et al, 2007), and organizing ex-
tracted facts (Cafarella et al, 2007b). There is
also a lot of work on deriving extraction scores
for facts (Agichtein and Gravano, 2000; Downey
et al, 2005; Etzioni et al, 2004; Pantel and Pen-
nacchiotti, 2006).
These extraction methods are complementary
to our general task of fact re-ranking. Since our
proposd re-ranking algorithms are agnostic to the
methods of generating the initial facts and since
they do not rely on having available corpus statis-
tics, we can use any of the available extractors in
combination with any of the scoring methods. In
this paper, we used Pas?ca et al?s (2006) state-of-
the-art extractor to learn a large set of ranked facts.
Graph-based ranking algorithms have been ex-
plored for a variety of text-centric tasks. Random
walk models have been built for document sum-
marization (Erkan and Radev, 2004), keyword ex-
traction (Hassan et al, 2007), and collaborative
filtering (Liu and Yang, 2008). Closest to our
work is that of Talukdar et al (2008) who pro-
posed random walk algorithms for learning in-
stances of semantic classes from unstructured and
structured text. The focus of our work is on ran-
dom walk models over fact graphs in order to re-
rank collections of facts.
6 Conclusion
In this paper, we show how information avail-
able in a farm of facts can be exploited for re-
ranking facts. As a key contribution of the pa-
per, we modeled fact ranking as a graph ranking
problem. We proposed random walk models that
determine the validity of a fact based on (a) the
number of facts that ?vote? for it, (b) the validity
of the voting facts, and (c) the extractor?s confi-
dence in these voting facts. Our experimental re-
sults demonstrated the effectiveness of our algo-
rithms, thus establishing a stepping stone towards
exploring graph-based frameworks for fact vali-
dation. While this paper forms the basis of em-
ploying random walk models for fact re-ranking,
it also suggests several interesting directions for
future work. We use and build upon PageRank,
however, several alternative algorithms from the
link analysis literature could be adapted for rank-
ing facts. Similarly, we employ a single (simple)
graph-based representation that treats all edges the
same and exploring richer graphs that distinguish
between edges supporting different arguments of
a fact remains future work.
508
References
[Agichtein and Gravano2000] Agichtein, Eugene and Luis
Gravano. 2000. Snowball: Extracting relations from
large plain-text collections. In DL-00.
[Auer et al2008] Auer, S., C. Bizer, G. Kobilarov,
J. Lehmann, R. Cyganiak, and Z. Ives. 2008. Dbpedia: A
nucleus for a web of open data. In ISWC+ASWC 2007.
[Banko and Etzioni2008] Banko, Michele and Oren Etzioni.
2008. The tradeoffs between open and traditional relation
extraction. In ACL-08.
[Banko et al2007] Banko, Michele, Michael J. Cafarella,
Stephen Soderland, Matthew Broadhead, and Oren Et-
zioni. 2007. Open information extraction from the web.
In Proceedings of IJCAI-07.
[Cafarella et al2007a] Cafarella, Michael, Dan Suciu, and
Oren Etzioni. 2007a. Navigating extracted data with
schema discovery. In Proceedings of WWW-07.
[Cafarella et al2007b] Cafarella, Michael J., Christopher Re,
Dan Suciu, Oren Etzioni, and Michele Banko. 2007b.
Structured querying of web text: A technical challenge.
In Proceedings of CIDR-07.
[Cohen and McCallum2003] Cohen, William and Andrew
McCallum. 2003. Information extraction from the World
Wide Web (tutorial). In KDD.
[Davidov and Rappoport2008] Davidov, Dmitry and Ari
Rappoport. 2008. Unsupervised discovery of generic re-
lationships using pattern clusters and its evaluation by au-
tomatically generated sat analogy questions. In ACL-08.
[Downey et al2005] Downey, Doug, Oren Etzioni, and
Stephen Soderland. 2005. A probabilistic model of re-
dundancy in information extraction. In Proceedings of
IJCAI-05.
[Erkan and Radev2004] Erkan, Gu?nes? and Dragomir R.
Radev. 2004. Lexrank: Graph-based lexical centrality
as salience in text summarization. JAIR, 22:457?479.
[Etzioni et al2004] Etzioni, Oren, Michael J. Cafarella, Doug
Downey, Stanley Kok, Ana-Maria Popescu, Tal Shaked,
Stephen Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in KnowItAll. In
Proceedings of WWW-04.
[Etzioni et al2005] Etzioni, Oren, Michael Cafarella, Doug
Downey, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates. 2005.
Unsupervised named-entity extraction from the web: an
experimental study. Artif. Intell., 165:91?134.
[Hassan et al2007] Hassan, Samer, Rada Mihalcea, and Car-
men Banea. 2007. Random-walk term weighting for im-
proved text classification. ICSC.
[Hearst1992] Hearst, Marti A. 1992. Automatic acquisition
of hyponyms from large text corpora. In Proceedings of
COLING-92.
[Kleinberg1999] Kleinberg, Jon Michael. 1999. Authorita-
tive sources in a hyperlinked environment. Journal of the
ACM, 46(5):604?632.
[Lenat1995] Lenat, Douglas B. 1995. Cyc: a large-scale in-
vestment in knowledge infrastructure. Commun. ACM,
38(11).
[Liu and Yang2008] Liu, Nathan and Qiang Yang. 2008.
Eigenrank: a ranking-oriented approach to collaborative
filtering. In SIGIR 2008.
[Matuszek et al2005] Matuszek, Cynthia, Michael Witbrock,
Robert C. Kahlert, John Cabral, Dave Schneider, Purvesh
Shah, and Doug Lenat. 2005. Searching for common
sense: Populating cyc from the web. In AAAI-05.
[Mintz et al2009] Mintz, Mike, Steven Bills, Rion Snow, and
Daniel Jurafsky. 2009. Distant supervision for relation
extraction without labeled data. In ACL-09.
[Pas?ca et al2006] Pas?ca, Marius, Dekang Lin, Jeffrey
Bigham, Andrei Lifchits, and Alpa Jain. 2006. Organiz-
ing and searching the world wide web of facts - step one:
The one-million fact extraction challenge. In Proceedings
of AAAI-06.
[Page et al1999] Page, Lawrence, Sergey Brin, Rajeev Mot-
wani, and Terry Winograd. 1999. The PageRank citation
ranking: Bringing order to the Web. Technical Report
1999/66, Stanford University, Computer Science Depart-
ment.
[Pantel and Pennacchiotti2006] Pantel, Patrick and Marco
Pennacchiotti. 2006. Espresso: leveraging generic pat-
terns for automatically harvesting semantic relations. In
ACL/COLING-06.
[Pantel et al2004] Pantel, Patrick, Deepak Ravichandran,
and Eduard Hovy. 2004. Towards terascale knowledge
acquisition. In COLING-04.
[Pantel et al2009] Pantel, Patrick, Eric Crestan, Arkady
Borkovsky, Ana-Maria Popescu, and Vishnu Vyas. 2009.
Web-scale distributional similarity and entity set expan-
sion. In EMNLP-09.
[Ravichandran and Hovy2002] Ravichandran, Deepak and
Eduard Hovy. 2002. Learning surface text patterns for
a question answering system. In Proceedings of ACL-08,
pages 41?47. Association for Computational Linguistics.
[Riloff and Jones1999] Riloff, Ellen and Rosie Jones. 1999.
Learning dictionaries for information extraction by multi-
level bootstrapping. In Proceedings of AAAI-99.
[Talukdar et al2008] Talukdar, Partha Pratim, Joseph
Reisinger, Marius Pasca, Deepak Ravichandran, Rahul
Bhagat, and Fernando Pereira. 2008. Weakly-supervised
acquisition of labeled class instances using graph random
walks. In Proceedings of EMNLP-08.
[Yan et al2009] Yan, Yulan, Yutaka Matsuo, Zhenglu Yang,
and Mitsuru Ishizuka. 2009. Unsupervised relation ex-
traction by mining wikipedia texts with support from web
corpus. In ACL-09.
509
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 510?518,
Beijing, August 2010
Open Entity Extraction from Web Search Query Logs
Alpa Jain
Yahoo! Labs
alpa@yahoo-inc.com
Marco Pennacchiotti
Yahoo! Labs
pennac@yahoo-inc.com
Abstract
In this paper we propose a completely un-
supervised method for open-domain en-
tity extraction and clustering over query
logs. The underlying hypothesis is that
classes defined by mining search user activ-
ity may significantly differ from those typ-
ically considered over web documents, in
that they better model the user space, i.e.
users? perception and interests. We show
that our method outperforms state of the art
(semi-)supervised systems based either on
web documents or on query logs (16% gain
on the clustering task). We also report evi-
dence that our method successfully supports
a real world application, namely keyword
generation for sponsored search.
1 Introduction
Search engines are increasingly moving beyond the
traditional keyword-in document-out paradigm, and
are improving user experience by focusing on user-
oriented tasks such as query suggestions and search
personalization. A fundamental building block of
these applications is recognizing structured infor-
mation, such as, entities (e.g., mentions of people,
organizations, or locations) or relations among en-
tities (Cao et al, 2008; Hu et al, 2009). For this,
search engines typically rely on large collections of
entities and relations built using information extrac-
tion (IE) techniques (Chaudhuri et al, 2009).
Commonly used IE techniques follow two main
assumptions: (1) IE focuses on extracting infor-
mation from syntactically and semantically ?well-
formed? pieces of texts, such as, news corpora and
web documents (Pennacchiotti and Pantel, 2009);
(2) extraction processes are bootstrapped with some
pre-existing knowledge of the target domain (e.g
entities are typically extracted for pre-defined cat-
egories, such as Actors, Manufacturers, Persons,
Locations (Grishman and Sundheim, 1996)). Prior
work (Banko et al, 2007), has looked into relax-
ing the second assumption and proposed open in-
formation extraction (OIE), a domain-independent
and scalable extraction paradigm, which however
focuses mostly on web corpora.
In this paper, we argue that for user-oriented ap-
plications discussed earlier, IE techniques should
go beyond the traditional approach of using ?well-
formed? text documents. With this in mind, we ex-
plore the utility of search query logs, a rich source
of user behaviors and perception, and build tech-
niques for open entity extraction and clustering
over query logs. We hypothesize that web docu-
ments and query logs model two different spaces:
web documents model the web space, i.e. general
knowledge about entities and concepts in an objec-
tive and generic way; search query logs model the
user space, i.e. the users? view and perception of
the world in a more specific fashion, where avail-
able information directly expresses users? needs
and intents. For example, in a web space, ?brit-
ney spears? will tend to be similar and be clus-
tered with other singers, such as ?celine dion? and
?bruce springsteen?. On the contrary, in the users?
space, she is highly similar and clustered with other
gossiped celebrities like ?paris hilton? and ?serena
williams?: the users? space better models the users?
perception of that person; such a space is then
highly valuable for all those applications where
users? perceptions matters.
To computationally model our hypothesis for
OIE over search query logs, we present a two phase
approach to OIE for search query logs. The first
phase (entity extraction) extracts entities from the
search query logs using an unsupervised approach,
by applying pattern-based heuristics and statistical
measures. The second phase (entity clustering) in-
duces classes over these entities by applying clus-
tering techniques. In summary, our main contribu-
510
tions are: (1) We propose and instantiate a novel
model for open information extraction over web
search query logs; and we apply it to the task of
entity extraction and clustering. (2) We show how
we characterize each extracted entity to capture the
?user space?, and induce classes over the entities.
(3) We present an extensive evaluation over real-life
datasets showing that query logs is a rich source for
domain-independent user-oriented extraction tasks
(Section 3). We also show the practicality of our
approach by incorporating it into a real-world appli-
cation, namely keyword suggestions for sponsored
search (Section 4).
2 Open Entity Extraction on Query Log
In this section, we present our method for open
entity extraction from query logs. We first de-
scribe our heuristic method for extracting entities
(Section 2.1), and then three different feature ?user
spaces? to cluster the entities (Section 2.2).
2.1 Entity Extraction
In our setting, entities correspond to Named Enti-
ties. i.e. they are defined using the standard named
entity types described in (Sekine et al, 2002)1. In
this paper, we use a set of entities extracted from
query log, obtained by applying a simple algorithm
(any other query log entity extraction method would
apply here, e.g. (Pasca, 2007b)). The algorithm is
based on the observation that oftentimes users con-
struct their search query by copy-pasting phrases
from existing texts. Due to this phenomenon, user
queries often carry over surface-level properties
such as capitalization and tokenization information.
Our approach realizes this observation by iden-
tifying contiguous capitalized words from a user
query. (In our experiments, we observed that 42%
of the queries had at least one upper-case character.)
Specifically, given a query Q = q1 q2 q3 ? ? ? qn,
we define a candidate entity E = e1 e2 ? ? ? em as
the maximal sequence of words (i.e., alpha-numeric
characters) in the query such that each word ei in
the entity begins with an uppercase character. The
set of candidate entities is then cleaned by apply-
ing a set of heuristics, thus producing the final set
of entities. In particular, for each extracted entity,
1We exclude ?Time? and ?Numerical Expressions?, which
are out of the scope of our study.
we assign two confidence scores: a Web-based rep-
resentation score and a query-log-based standalone
score. The representation score checks if the case-
sensitive representation observed for E in Q, is the
most likely representation for E, as observed on
a Web corpus (e.g., ?DOor HANGing TIps? is as-
signed a low representation score). The standalone
score is based on the observation that a candidate
E should often occur in a standalone form among
the search query logs, in order to get the status of
a proper named entity as defined in (Sekine et al,
2002; Grishman and Sundheim, 1996). In practice,
among the query logs we must find queries of the
form Q == E, capturing the fact that users are
looking to learn more about the given entity2.
2.2 Entity Clustering
The clustering phase takes as input any of the fea-
ture spaces presented in the rest of this section, and
groups the entities according to the similarity of
their vectors in the space. The desiderata for a clus-
tering algorithm for the task of open-domain infor-
mation extraction are the following: (1) The algo-
rithm must be highly scalable, efficient, and able
to handle high dimensionality, since the number of
queries and the size of the feature vectors can be
large; (2) We do not know in advance the number
of clusters; therefore, the algorithm needs not to re-
quire a pre-defined number of clusters.
Any clustering algorithm fulfilling the above re-
quirements would fit here. In our experiments, we
adopt a highly scalable Map-Reduce implementa-
tion of the hard-clustering version of Clustering by
Committee (CBC), a state-of-the-art clustering al-
gorithm presented in (Pantel and Lin, 2002).
Context Feature Space. The basic hypothesis for
the context feature space, is that an entity can be ef-
fectively represented by the set of contexts in which
it appears in queries. This allows to capture the
users? view of the entity, i.e. what people query,
and want to know about the entity. This is similar
to that proposed by Pasca (2007b; 2007a), i.e. that
queries provide good semantics cues for modeling
named entities.
Our query log feature space may significantly
differ from a classical contextual feature space com-
2We refer the readers to (Jain and Pennacchiotti, 2010) for
details on the entity extraction algorithms.
511
puted over a Web corpus, since the same entity can
be differently perceived and described in the two
corpora (query log and Web). Consider for exam-
ple the entity ?galapagos islands?. Typical contexts
on the Web and query log for this entity are:
web: endemic birds
web: big turtles
web: charles darwin foundation
web: sensitive water
qlog : trip to
qlog : diving
qlog : where are the
qlog : travel package
The difference between the two representations
implies that entities that are similar on the Web, are
not necessarily similar on query logs. For exam-
ple, on the Web ?galapagos islands? is very simi-
lar to other countries such as ?tasmania?, ?guinea?
and ?luxemburg?; while on query log is similar to
other sea-side travel destination and related con-
cepts, such as ?greek isle?, ?kauai snorkeling? and
?south america cruise?. Our new similarity com-
puted over query log, is potentially useful for those
applications in which is more important to represent
users? intents, than an objective description of enti-
ties (e.g. in query suggestion and intent modeling).
To obtain our contextual representation we pro-
ceed as follows. For each entity e, we identify
all queries in the query log, in which e appears.
Then, we collect the set of all suffixes and postfixes
of the entity in those queries. For example, given
the entity ?galapagos islands? and the query ?sum-
mer 2008 galapagos islands tour?, the contexts are:
?summer 2008? and ?tour?.
Once the set of all contexts of all entities has been
collected, we discard contexts appearing less than
? -times in the query log, so to avoid statistical bi-
ases due to data sparseness (in the reported experi-
ments we set ? = 200). We then compute the cor-
rected pointwise mutual information (cpmi) (Pan-
tel and Ravichandran, 2004) between each instance
and each context c as:
cpmi(e, c) = log2
f(e, c) ? f(?, ?)
f(e) ? f(c) ?M (1)
where f(e, c) is the number of times e and c
occur in the same query; f(e) and f(c) is the
count of the entity and the context in the query
log; f(?, ?) the overall count of all co-occurrences
between contexts and entities; and M is the correc-
tion factor presented in (Pantel and Ravichandran,
2004), that eases the pmi?s bias towards infrequent
entities/features. Each instance is then represented
in the feature space of all contexts, by the computed
pmi values. Note that our method does not use any
NLP parsing, since queries rarely present syntactic
structure. This guarantees the method to be com-
putationally inexpensive and easily adaptable to
languages other than English.
Clickthrough Feature Space. During a search
session, users issue a search query for which the
search engine presents a list of result urls. Of the
search results, users choose those urls that are rep-
resentative of their intent. This interaction is cap-
tured by means of a click, which is logged by most
search engines as click-through data. For instance,
a search log may contain the following clicked urls
for a query ?flv converter?, for different users:
user1: www.flv-converter.com
user2: www.videoconverterdownload.com/flv/
user3: www.ripzor.com/flv.html
Our main motivation behind clustering entities
based on past user click behavior is that non-
identical queries that generate clicks on the same
urls capture similar user intent. Thus, grouping en-
tities that were issued as a query and generated user
clicks on the same url may be considered similar.
For instance, the query ?convert flv? may also gen-
erate clicks on one of the above urls, thus hinting
that the two entities are similar. We observed that
websites tend to dedicate a url per entity. There-
fore, grouping by click urls can lead to clusters with
synonyms (i.e., different ways of representing the
same entity) or variants (e.g., spelling errors). To
get more relevant clusters, instead of grouping en-
tities by the click urls, we use the base urls. For
instance, the url www.ripzor.com/flv.html
is generalized to www.ripzor.com.
With the advent of encyclopedic web-
sites such as, www.wikipedia.org and
wwww.youtube.com, naively clustering entities
by the clickthrough data can led to non-similar
entities to be placed in the same cluster. For
instance, we observed the most frequently clicked
base url for both ?gold retriever? and ?abraham
lincoln? is www.wikipedia.org. To address
this issue, in our experiments we employed a
512
stop-list by eliminating top-5 urls based on their
inverse document frequency, where an entity is
intended as the ?document?.
In practice, each extracted entity e is represented
by a feature vector of size equal to the number of
distinct base urls in the click-through data, across
all users. Each dimension in the vector represents a
url in the click-through information. The value f of
an entity e for the dimension associated with url j
is computed as:
f(e, j) =
?
?
?
w(e,j)??|U|
i w(e,i)
2
if url j clicked for query e;
0 otherwise.
where U is the set of base urls found in click-
through data when entity e was issued as a query;
and w(e, i) is the number of time the base url i was
clicked when e was a query.
Hybrid Feature Space. We also experiment a hy-
brid feature space, which is composed by the nor-
malized union of the two feature spaces above (i.e.
context and clickthrough). Though more complex
hybrid models could be applied, such as one based
on ensemble clustering, we here opt for a simple
solution which allows to better read and compare to
other methods.
3 Experimental Evaluation
In this section, we report experiments on our clus-
tering method. The goal of the experiment is two-
fold: (1) evaluate the intrinsic quality of the cluster-
ing methods, i.e. if two entities in the same cluster
are similar or related from a web user?s perspec-
tive; (2) verify if our initial hypothesis holds, i.e.
if query log based features spaces capture different
properties than Web based feature spaces (i.e. the
?user space?). In Section 3.1 we describe our ex-
perimental setup; and, in 3.2 we provide the results.
We couple this intrinsic evaluation with an extrinsic
application-driven one in Section 4.
3.1 Experimental Settings
In the experiments we use the following datasets:
Query log: A random sample of 100 million, fully
anonymized queries collected by the Yahoo! search
engine in the first 3 months of 2009, along with their
frequency. This dataset is used to generate both the
context and the clickthrough feature spaces for the
clustering step.
Web documents: A collection of 500 million web
pages crawled by a Yahoo! search engine crawl.
This data set is used to implement a web-based fea-
ture space that we will compare to in Section 3.2.
Entity set: A collection of 2,067,385 entities, ex-
tracted with the method described in 2.1, which
shows a precision of 0.705 ?0.044. Details on
the evaluation of such method are available in (Jain
and Pennacchiotti, 2010), where a full comparison
with state-of-the-art systems such as (Pasca, 2007b)
and (Banko et al, 2007) are also reported.
Evaluation methodology: Many clustering evalu-
ation metrics have been proposed, ranging from Pu-
rity to Rand-statistics and F-Measure. We first se-
lect from the original 2M entity set, a random set of
n entities biased by their frequency in query logs,
so to keep the experiment more realistic (more fre-
quent entities have more chances to be picked in
the sample). For each entity e in the sample set,
we derived a random list of k entities that are clus-
tered with e. In our experiments, we set n = 10
and k = 20. We then present to a pool of paid edi-
tors, each entity e along with the list of co-clustered
entities. Editors are requested to classify each co-
clustered entity ei as correct or incorrect. An entity
ei is deemed as correct, if it is similar or related to e
from a web user?s perspective: to capture this intu-
ition, the editor is asked the question: ?If you were
interested in e, would you be also interested in ei
in any intent??.3 Annotators? agreement over a ran-
dom set of 30 entities is kappa = 0.64 (Marques
De Sa?, 2003), corresponding to substantial agree-
ment. Additionally, we ask editors to indicate the
relation type between e and ei (synonyms, siblings,
parent-child, topically related).
Compared methods:
CL-CTX: A CBC run, based on the query log con-
text feature space (Section 2.2).
CL-CLK: A CBC run, based on the clickthrough
feature space (Section 2.2).
3For example, if someone is interested in ?hasbro?, he could
be probably also be interested in ?lego?, when the intent is buy-
ing a toy. The complete set of annotation guidelines is reported
in (Jain and Pennacchiotti, 2010).
513
method # cluster avg cluster size
CL-Web 1,601 240
CL-CTX 875 1,182
CL-CLK 4,385 173
CL-HYB 1,580 478
Table 1: Statistics on the clustering results.
CL-HYB: A CBC run, based on the hybrid space
that combines CL-CTXand CL-CLK(Section 2.2).
CL-Web: A state-of-the-art open domain method
based on features extracted from the Web docu-
ments data set (Pantel et al, 2009). This method
runs CBC over a space where features are the con-
texts in which an entity appears (noun chunks pre-
ceding and following the target entity); and feature
value is the pmi between the entity and the chunks.
Evaluation metrics: We evaluate each method us-
ing accuracy, intended as the percentage of correct
judgments.
3.2 Experimental Results
Table 3 reports accuracy results. CL-HYB is the
best performing method, achieving 0.85 accuracy,
respectively +4% and +11% above CL-CLK and
CL-Web. CL-CTX shows the lowest performance.
Our results suggest that query log spaces are more
suitable to model the ?user space? wrt web features.
Specifically, clickthrough information are most use-
ful confirming our hypothesis that queries that gen-
erate clicks on the same urls capture similar user
intents.
To have an anecdotal and practical intuition on
the results, in Table 2 we report some entities and
examples of other entities from the same clusters, as
obtained from the CL-HYB and CL-Web methods.
The examples show that CL-HYB builds clusters
according to a variety of relations, while CL-Web
mostly capture sibling-like relations.
One relevant of such relations is topicality. For
example, for ?aaa insurance? the CL-HYB cluster
mostly contains entities that are topically related to
the American Automobile Association, while the
CL-Web cluster contains generic business compa-
nies. In this case, the CL-HYB approach sim-
ply chose to group together entities having clicks
to ?aaa.com? and appearing in contexts as ?auto
club?. On the contrary, CL-Web grouped accord-
ing to contexts such as ?selling? and ?company?.
The entity ?hip osteoarthritis? shows a similar be-
entity CL-HYB CL-Web
aaa insurance roadside assistance loanmax
personal liability insurance pilot car service
international driving permits localnet
aaa minnesota fibermark
travelers checks country companies
insurance
paris hilton brenda costa julia roberts
adriana sklenarikova brad pitt
kelly clarkson nicole kidman
anja rubik al pacino
federica ridolfi tom hanks
goldie hawn bonnie hunt julia roberts
brad pitt brad pitt
tony curtis nicole kidman
nicole kidman al pacino
nicholas cage tom hanks
basic algebra numerical analysis math tables
discrete math trigonometry help
lattice theory mathtutor
nonlinear physics surface area formula
ramsey theory multiplying fractions
hip osteoarthritis atherosclerosis wrist arthritis
pneumonia disc replacement
hip fracture rotator cuff tears
breast cancer shoulder replacement
anorexia nervosa american orthopedic
society
acer america acer aspire accessories microsoft
aspireone casio computer
acer monitors borland software
acer customer service sony
acer usa nortel networks
Table 2: Sample of the generated entity clusters.
havior: CL-HYB groups entities topically related
to orthopedic issues, since most of the entities are
sharing contexts such as ?treatment? and ?recovery?
and, at the same time, clicks to urls such as ?or-
thoinfo.aaos.org? and ?arthirtis.about.com?.
Another interesting observation regards entities
referring to people. The ?paris hilton? and ?goldie
hawn? examples show that the CL-Web approach
groups famous people according to their category
? i.e. profession in most cases. On the contrary,
query log approaches tend to group people accord-
ing to their social attitude, when this prevails over
the profession. In the example, CL-HYB clusters
the actress ?goldie hawn? with other actors, while
?paris hilton? is grouped with an heterogeneous set
of celebrities that web users tend to query and click
in a same manner: In this case, the social per-
sona of ?paris hilton? prevails over its profession
(actress/singer). This aspect is important in many
applications, e.g. in query suggestion, where one
wants to propose to the user entities that have been
similarly queried and clicked.
In order to check if the above observations are
not anecdotal, we studied the relation type annota-
tion provided by the editors (Table 4). Table shows
514
method Precision
CL-Web 0.735
CL-CTX 0.460
CL-CLK 0.815 ?
CL-HYB 0.850 ?
Table 3: Precision of various clustering methods
(? indicates statistical-significant better than the
CL-Web method, using t-test).
that query log based methods are more varied in the
type of clusters they build. Table 5 shows the dif-
ference between the clustering obtained using the
different methods and the overlap between the pro-
duced clusters. For example, 40% of the relations
for the CL-HYB system are topical, while 32% are
sibiling ones. On the contrary, the CL-Web method
is highly biased towards sibling relations.
As regard a more attentive analysis of the dif-
ferent query log based methods, CL-CTX has the
lowest performance. This is mainly due to the fact
that contextual data are sometimes too sparse and
generic. For example ?mozilla firefox? is clustered
with ?movie program? and ?astro reading? because
they share only some very generic contexts such as
?free downloads?. In order to get more data, one op-
tion is to relax the ? threshold (see Section 2) so to
include more contexts in the semantic space. Unfor-
tunately, this would have a strong drawback, in that
low-frequency context tend to be idiosyncratic and
spurious. A typical case regards recurring queries
submitted by robots for research purposes, such as
?who is X?, ?biography of X?, or ?how to X?. These
queries tend to build too generic clusters containing
people or objects. Another relevant problem of the
CL-CTX method is that even when using a high ?
cut, clusters still tend to be too big and generic, as
statistics in Table 4 shows.
CL-CTX, despite the low performance, is very
useful when combined with CL-CLK. Indeed the
CL-HYB system improves +4% over the CL-CLK
system alone. This is because the CL-HYB method
is able to recover some misleading or incomplete
evidence coming from the CL-CLK using features
provided by CL-CLK. For example, editors judged
as incorrect 11 out of 20 entities co-clustered with
the entity ?goldie hawn? by CL-CLK. Most of these
errors are movies (e.g. ?beverly hills cops?) soap
operas (e.g. ?sortilegio?) and directors, because all
have clicks to ?imdb.com? and ?movies.yahoo.com?.
class method
CL-Web CL-CTX CL-CLK CL-HYB
topic 0.27 0.46 0.46 0.40
sibling 0.72 0.43 0.29 0.32
parent - 0.09 0.13 0.09
child 0.01 - 0.01 0.02
synonym 0.01 0.03 0.12 0.16
Table 4: Fraction of entities that have been classi-
fied by editors in the different relation types.
method labelled clusters
CL-CTX CL-CLK CL-HYB CL-Web
CL-CTX - 0.2 0.53 0.29
CL-CLK 0.21 - 0.54 0.34
CL-HYB 0.53 0.51 - 0.31
CL-Web 0.33 0.35 0.41 -
Table 5: Purity of clusters for each method using
clusters from other methods as ?labelled? data.
CL-HYB recovers these errors by including features
coming from CL-CTX such as ?actress?.
In summary, query log spaces group together en-
tities that are similar by web users (this being topi-
cal similarity or social attitude), thus constituting a
practical model of the ?user space? to be leveraged
by web applications.
4 Keywords for Sponsored Search
In this section we explore the use of our methods for
keyword generation for sponsored search. In spon-
sored search, a search company opens an auction,
where on-line advertisers bid on specific keywords
(called bidterms). The winner is allowed to put its
ad and link on the search result page of the search
company, when the bidterm is queried. Compa-
nies such as Google and Yahoo are investing efforts
for improving their bidding platforms, so to attract
more advertisers in the auctions. Bidterm sugges-
tion tools (adWords, 2009; yahooTool, 2009) are
used to help advertiser in selecting bidterms: the
advertisers enters a seed keyword (seed) express-
ing the intent of its ad, and the tool returns a list
of suggested keywords (suggestions) that it can use
for bidding ? e.g for the seed ?mp3 player?, a sug-
gestion could be ?ipod nano?. The task of gen-
erating bid suggestions (i.e. keyword generation)
is typically automatic, and has received a grow-
ing attention in the search community for its im-
pact on search company revenue. The main prob-
lem of existing methods for suggestion (adWords,
2009; yahooTool, 2009; wordTracker, 2009) is that
515
they produce only suggestions that contain the ini-
tial seed (e.g. ?belkin mp3 player? for the seed ?mp3
player?), while nonobvious (and potentially less ex-
pensive) suggestions not containing the seed are ne-
glected (e.g. ?ipod nano? for ?mp3 player?). For
example for ?galapagos islands?, a typical produc-
tion system suggests ?galapagos islands tour? which
cost almost 5$ per click; while the less obvious ?isla
santa cruz? would cost only 0.35$. Below we show
our method to discover such nonobvious sugges-
tions, by retrieving entities in the same cluster of
a given seed.
4.1 Experimental Setting
We evaluate the quality of the suggestions proposed
by different methods for a set of seed bidterms.,
adopting the evaluation schema in (Joshi and Mot-
wani, 2006)
Dataset Creation. To create the set of seeds, we
use Google skTool4. The tool provides a list of
popular bid terms, organized in a taxonomy of ad-
vertisement topics. We select 3 common topics:
tourism, vehicles and consumer-electronics. For
each topic, we randomly pick 5 seeds among the
800 most popular bid terms, which also appear in
our entity set described in Section 3.1.5. We evalu-
ate a system by collecting all its suggestions for the
15 seeds, and then extracting a random sample of
20 suggestions per seed.
Evaluation and Metrics. We use precision and
Nonobviousness. Precision is computed by ask-
ing two experienced human experts to classify each
suggestion of a given seed, as relevant or irrelevant.
A suggestion is deemed as relevant if any advertiser
would likely choose to bid for the suggestion, hav-
ing as intent the seed. Annotator agreement, evalu-
ated on a subset of 120 suggestions is kappa = 0.72
(substantial agreement). Precision is computed as
the percentage of suggestions judged as relevant.
Nonobviousness is a metric introduced in (Joshi
and Motwani, 2006), capturing how nonobvious the
suggestions are. It simply counts how many sug-
4http://www.google.com/sktool
5The final set of 15 bid terms is: tourism:galapagos
islands,holiday insurance,hotel booking,obertauern,wagrain;
vehicles:audi q7,bmw z4,bmw dealers,suzuki grand vi-
tara,yamaha banshee; consumer electr:canon rebel xti,divx
converter,gtalk,pdf reader,flv converter.
gestions for a given seed do not contain the seed it-
self (or any of its variants): this metric is computed
automatically using string matching and a simple
stemmer.
Comparisons. We compare the suggestions pro-
posed by CL-CTX, CL-CLK, and CL-HYB, against
Web and two reference state-of-the-art produc-
tion systems: Google AdWords (GOO) and Yahoo
Search Marketing Tool (YAH). As concerns our
methods, we extract as suggestions the entities that
occur in the same cluster of a given seed. For the
production systems, we rely on the suggestions pro-
posed on the website of the tools.
4.2 Experimental Results
Precision results are reported in the second column
of Table 6. Both CL-CLK and CL-HYB outper-
form Web in precision, CL-HYB being close to the
upper-bound of the two production systems. As ex-
pected, production systems show a very high pre-
cision but their suggestions are very obvious. Our
results are fairly in line with those obtained on a
similar dataset, by Joshi and Motwani (2006).
A closer look at the results shows that most of the
errors for CL-CTX are caused by the same problem
outlined in Section 3.2: Some entities are wrongly
assigned to a cluster, because they have some high
cpmi context feature which is shared with the clus-
ter centroid, but which is not very characteristic
for the entity itself. This is particularly evident for
some of the low frequency entities, where cpmi val-
ues could not reflect the actual semantics of the en-
tity. For example the entity ?nickelodeon? (a kids tv
channel in UK) is assigned to the cluster of ?galapa-
gos islands?, because of the feature ?cruise?: indeed,
some people query about ?nickelodeon cruise? be-
cause the tv channel organizes some kids cruises.
Other mistakes are due to feature ambiguity. For
example, the entity ?centurion boats? is assigned
to the cluster of ?obertauern? (a ski resort in Aus-
tria), because they share the ambiguous feature ?ski?
(meaning either winter-ski or water-ski). As for the
CL-CLK system, some of the errors are caused by
the fact that some base url can refer to very differ-
ent types of entities. For example the entity ?color
copier? is suggested for the the camera ?canon rebel
xti?, since they both share clicks to the Canon web-
site. The CL-HYB system achieves a higher preci-
516
method Precision Nonobviousness
GOO 0.982 0.174
YAH 0.966 0.195
Web 0.814 0.827
CL-CTX 0.547 0.963
CL-CLK 0.827 0.630
CL-HYB 0.946 0.567
Table 6: Results for keyword generation.
sion wrt CL-CTX and CL-CLK: the combination of
the two spaces decreases the impact of misleading
features ?e.g. for ?yamaha bunshee?, all CL-HYB ?s
suggestions are correct, while almost all CL-CLK ?s
suggestions are incorrect: the hybrid system recov-
ered the negative effect of the misleading feature
ebay.com, by backing up on features from the
contextual subspace (e.g. ?custom?, ?specs?, ?used
parts?).
Nonobviousness results are reported in column
three of Table 6. All our systems return a high num-
ber of nonobvious suggestions (all above 50%).6
On the contrary, GOO and YAH show low perfor-
mance, as both systems are heavily based on the
substring matching technique. This strongly moti-
vates the use of semantic approaches as those we
propose, that guarantee at the same time both a
higher linguistic variety and an equally high preci-
sion wrt the production systems. For example, for
the seeds ?galapagos islands?, GOO returns simple
suggestions such as ?galapagos islands vacations?
and ?galapagos islands map?; while CL-HYB re-
turns ?caribbean mexico? and ?pacific dawn?, two
terms that are semantically related but dissimilar
from the seed. Remember that these letter terms are
related to the seed because they are similar in the
user space, i.e. users looking at ?galapagos islands?
tend to similarly look for ?caribbean mexico? and
?pacific dawn?. These suggestions would then be
very valuable for tourism advertisers willing to im-
prove their visibility through a non-trivial and pos-
sibly less expensive set of bid terms.
5 Related Work
While literature abounds with works on entity ex-
traction from web documents (e.g. (Banko et al,
2007; Chaudhuri et al, 2009; Pennacchiotti and
Pantel, 2009)), the extraction of classes of entities
6Note that very high values for CL-CTX may be mislead-
ing, as many of the suggestions proposed by this system are
incorrect (see precision results) and hence non-obvious (e.g.,
?derek lewis? for ?galapagos islands?).
over query logs is a pretty new task, recently intro-
duced in (Pasca, 2007b). Pasca?s system extracts
entities of pre-defined classes in a semi-supervised
fashion, starting with an input class represented by a
set of seeds, which are used to induce typical query-
contexts for the class. Contexts are then used to
extract and select new candidate instances for the
class. A similar approach is also adopted in (Sekine
and Suzuki, 2007). Pasca shows an improvement
of about 20% accuracy, compared to existing Web-
based systems. Our extraction algorithm differs
from Pasca?s work in that it is completely unsuper-
vised. Also, Pasca?s cannot be applied to OIE, i.e.
it only works for pre-defined classes. Our cluster-
ing approach is related to Lin and Wu?s work (Lin
and Wu, 2009). Authors propose a semi-supervised
algorithm for query classification. First, they ex-
tract a large set of 20M phrases from a query log, as
those unique queries appearing more than 100 times
in a Web corpus. Then, they cluster the phrases
using the K-means algorithm, where features are
the phrases? bag-of-words contexts computed over
a web corpus. Finally, they classify queries using
a logistic regression algorithm. Our work differs
from Lin and Wu, as we focus on entities instead of
phrases. Also, the features we use for clustering are
from query logs and click data, not web contexts.
6 Conclusions
We presented an open entity extraction approach
over query logs that goes beyond the traditional web
corpus, with the goal of modeling a ?user-space? as
opposed to an established ?web-space?. We showed
that the clusters generated by query logs substan-
tially differ from those by a Web corpus; and that
our method is able to induce state-of-the-art qual-
ity classes on a user-oriented evaluation on the real
world task of keyword generation for sponsored
search. As future work we plan to: (i) experiment
different clustering algorithms and feature models,
e.g. soft-clustering for handling ambiguous enti-
ties; (ii) integrate the Web space and the query log
spaces; (iii) embed our methods in in existing tools
for intent modeling, query suggestion and similia,
to check its impact in production systems.
517
References
adWords. 2009. Google adwords. ad-
words.google.com/select/keywordtoolexternal.
Banko, Michele, Michael Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of IJCAI.
Cao, Huanhuan, Daxin Jiang, Jian Pei, Qi He, Zhen
Liao, Enhong Chen, and Hang Li. 2008. Context-
aware query suggestion by mining click-through and
session data. In Proceedings of KDD-08.
Chaudhuri, Surajit, Venkatesh Ganti, and Dong Xin.
2009. Exploiting web search to generate synonyms
for entities. In Proceedings of WWW-09.
Grishman, R. and B. Sundheim. 1996. Message under-
standing conference- 6: A brief history. In Proceed-
ings of COLING.
Hu, Jian, Gang Wang, Fred Lochovsky, Jian tao Sun,
and Zheng Chen. 2009. Understanding user?s query
intent with Wikipedia. In Proceedings of WWW-09.
Jain, Alpa and Marco Pennacchiotti. 2010. Open In-
formation Extraction from Web Search Query Logs.
Technical Report YL-2010-003, Yahoo! Labs.
Joshi, Amruta and Rajeev Motwani. 2006. Keyword
generation for search engine advertising. In Proceed-
ings of Sixth IEEE-ICDM.
Lin, Dekang and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL-
IJCNLP-2009.
Marques De Sa?, Joaquim P. 2003. Applied Statistics.
Springer Verlag.
Pantel, Patrick and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings KDD-02.
Pantel, Patrick and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceeding of
HLT-NAACL-2004.
Pantel, Patrick, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP-09.
Pasca, Marius. 2007a. Organizing and searching the
world wide web of facts - step two: Harnessing the
wisdom of the crowds. In Proceedings of the WWW-
2007.
Pasca, Marius. 2007b. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of CIKM-2007.
Pennacchiotti, Marco and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In Proceedings of
EMNLP-2009.
Sekine, Satoshi and Hisami Suzuki. 2007. Acquiring
ontological knowledge from query logs. In Proceed-
ings of WWW-07.
Sekine, Satoshi, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Proceed-
ings of LREC-2002.
wordTracker. 2009. Word tracker.
www.wordtracker.com.
yahooTool. 2009. Yahoo search marketing. searchmar-
keting.yahoo.com.
518
