Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 840?848,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning about Voice Search for Spoken Dialogue Systems 
Rebecca J. Passonneau1, Susan L. Epstein2,3, Tiziana Ligorio2,  
Joshua B. Gordon4, Pravin Bhutada4 
1Center for Computational Learning Systems, Columbia University 
2Department of Computer Science, Hunter College of The City University of New York 
3Department of Computer Science, The Graduate Center of The City University of New York 
4Department of Computer Science, Columbia University 
becky@cs.columbia.edu, susan.epstein@hunter.cuny.edu, tligorio@gc.cuny.edu, 
joshua@cs.columbia.edu, pravin.bhutada@gmail.com 
Abstract 
In a Wizard-of-Oz experiment with multiple 
wizard subjects, each wizard viewed automated 
speech recognition (ASR) results for utterances 
whose interpretation is critical to task success: 
requests for books by title from a library data-
base. To avoid non-understandings, the wizard 
directly queried the application database with 
the ASR hypothesis (voice search). To learn 
how to avoid misunderstandings, we investi-
gated how wizards dealt with uncertainty in 
voice search results. Wizards were quite suc-
cessful at selecting the correct title from query 
results that included a match. The most suc-
cessful wizard could also tell when the query 
results did not contain the requested title. Our 
learned models of the best wizard?s behavior 
combine features available to wizards with 
some that are not, such as recognition confi-
dence and acoustic model scores.  
1 Introduction 
Wizard-of-Oz (WOz) studies have long been used 
for spoken dialogue system design. In a relatively 
new variant, a subject (the wizard) is presented 
with real or simulated automated speech recogni-
tion (ASR) to observe how people deal with incor-
rect speech recognition output (Rieser, Kruijff-
Korbayov?, & Lemon, 2005; Skantze, 2003; 
Stuttle, Williams, & Young, 2004; Williams & 
Young, 2003, 2004; Zollo, 1999). In these experi-
ments, when a wizard could not interpret the ASR 
output (non-understanding), she rarely asked users 
to repeat themselves. Instead, the wizard found 
other ways to continue the task.  
This paper describes an experiment that pre-
sented wizards with ASR results for utterances 
whose interpretation is critical to task success: re-
quests for books from a library database, identified 
by title. To avoid non-understandings, wizards 
used voice search (Wang et al, 2008): they direct-
ly queried the application database with ASR out-
put. To investigate how to avoid errors in 
understanding (misunderstandings), we examined 
how wizards dealt with uncertainty in voice search 
results. When the voice search results included the 
requested title, all seven of our wizards were likely 
to identify it. One wizard, however, recognized far 
better than the others when the voice search results 
did not contain the requested title. The experiment 
employed a novel design that made it possible to 
include system features in models of wizard beha-
vior. The principal result is that our learned models 
of the best wizard?s behavior combine features that 
are available to wizards with some that are not, 
such as recognition confidence and acoustic model 
scores. 
The next section of the paper motivates our ex-
periment. Subsequent sections describe related 
work, the dialogue system and embedded wizard 
infrastructure, experimental design, learning me-
thods, and results. We then discuss how to general-
ize from the results of our study for spoken 
dialogue system design. We conclude with a sum-
mary of results and their implications. 
2 Motivation 
Rather than investigate full dialogues, we ad-
dressed a single type of turn exchange or adjacency 
pair (Sacks et al, 1974): a request for a book by its 
840
title. This allowed us to collect data exclusively 
about an utterance type critical for task success in 
our application domain. We hypothesized that low-
level features from speech recognition, such as 
acoustic model fit, could independently affect 
voice search confidence. We therefore applied a 
novel approach, embedded WOz, in which a wizard 
and the system together interpret noisy ASR. 
To address how to avoid misunderstandings, we 
investigated how wizards dealt with uncertainty in 
voice search returns. To illustrate what we mean 
by uncertainty, if we query our book title database 
with the ASR hypothesis: 
ROLL DWELL 
our voice search procedure returns, in this order: 
CROMWELL 
ROBERT LOWELL 
ROAD TO WEALTH 
The correct title appears last because of the score it 
is assigned by the string similarity metric we use.  
Three factors motivated our use of voice search 
to interpret book title requests: noisy ASR, un-
usually long query targets, and high overlap of the 
vocabulary across different query types (e.g., au-
thor and title) as well as with non-query words in 
caller utterances (e.g., ?Could you look up . . .?).  
First, accurate speech recognition for a real-
world telephone application can be difficult to 
achieve, given unpredictable background noise and 
transmission quality. For example, the 68% word 
error rate (WER) for the fielded version of Let?s 
Go Public! (Raux et al, 2005) far exceeded its 
17% WER under controlled conditions. Our appli-
cation handles library requests by telephone, and 
would benefit from robustness to noisy ASR. 
Second, the book title field in our database dif-
fers from the typical case for spoken dialogue sys-
tems that access a relational database. Such 
systems include travel booking (Levin et al, 2000), 
bus route information (Raux et al, 2006), restau-
rant guides (Johnston et al, 2002; Komatani et al, 
2005), weather (Zue et al, 2000) and directory 
services (Georgila et al, 2003). In general for these 
systems, a few words are sufficient to retrieve the 
desired attribute value, such as a neighborhood, a 
street, or a surname. Mean utterance length in a 
sample of 40,000 Let?s Go Public! utterances, for 
example, is 2.4 words. The average book title 
length in our database is 5.4 words. 
Finally, our dialogue system, CheckItOut, al-
lows users to choose whether to request books by 
title, author, or catalogue number. The database 
represents 5028 active patrons (with real borrow-
ing histories and preferences but fictitious personal 
information), 71,166 book titles and 28,031 au-
thors. Though much smaller than a database for a 
directory service application (Georgila et al, 
2003), this is much larger than that of many current 
research systems. For example, Let?s Go Public! 
accesses a database with 70 bus routes and 1300 
place names. Titles and author names contribute 
50,394 words to the vocabulary, of which 57.4% 
occur only in titles, 32.1% only in author names, 
and 10.5% in both. Many book titles (e.g., You See 
I Haven?t Forgotten, You Never Know) have a high 
potential for confusability with non-title phrases in 
users? book requests. Given the longer database 
field and the confusability of the book title lan-
guage, integrating voice search is likely to have a 
relatively larger impact in CheckItOut.  
We seek to minimize non-understandings and 
misunderstandings for several reasons. First, user 
corrections in both situations have been shown to 
be more poorly recognized than non-correction ut-
terances (Litman et al, 2006). Non-understandings 
typically result in re-prompting the user for the 
same information. This often leads to hyper-
articulation and concomitant degradation in recog-
nition performance. Second, users seem to prefer 
systems that minimize non-understandings and mi-
sunderstandings, even at the expense of dialogue 
efficiency. Users of the TOOT train information 
spoken dialogue system preferred system-initiative 
to mixed- or user-initiative, and preferred explicit 
confirmation to implicit or no confirmation 
(Litman & Pan, 1999). This was true despite the 
fact that a mixed-initiative, implicit confirmation 
strategy led to fewer turns for the same task. Most 
of the more recent work on spoken dialogue sys-
tems focuses on mixed-initiative systems in labora-
tory settings. Still, recent work suggests that while 
mixed- or user-initiative is rated highly in usability 
studies, under real usage it ?fails to provide [a] ro-
bust enough interface? (Turunen et al, 2006). In-
corporating accurate voice search into spoken 
dialogue systems could lead to fewer non-
understandings and fewer misunderstandings. 
3 Related Work 
Our approach to noisy ASR contrasts with many 
other information-seeking and transaction-based 
dialogue systems. Those systems typically perform 
841
natural language understanding on ASR output be-
fore database query with techniques that try to im-
prove or expand ASR output. None that we know 
of use voice search. For one directory service ap-
plication, users spell the first three letters of sur-
names, and then ASR results are expanded using 
frequently confused phones (Georgila et al, 2003). 
A two-pass recognition architecture added to Let?s 
Go Public! improved concept recognition in post-
confirmation user utterances (Stoyanchev & Stent, 
2009). In (Komatani et al, 2005), a shallow se-
mantic interpretation phase was followed by deci-
sion trees to classify utterances as relevant either to 
query type or to specific query slots, to narrow the 
set of possible interpretations. CheckItOut is most 
similar in spirit to the latter approach, but relies on 
the database earlier, and only for semantic interpre-
tation, not to also guide the dialogue strategy. 
Our approach to noisy ASR is inspired by pre-
vious WOz studies with real (Skantze, 2003; Zollo, 
1999) or simulated ASR (Kruijff-Korbayov? et al, 
2005; Rieser et al, 2005; Williams & Young, 
2004). Simulation makes it possible to collect di-
alogues without building a speech recognizer, and 
to control for WER. In the studies that involved 
task-oriented dialogues, wizards typically focused 
more on the task and less on resolving ASR errors 
(Williams & Young, 2004; Skantze, 2003; Zollo, 
1999). In studies more like the information-seeking 
dialogues addressed here, an entirely different pat-
tern is observed (Kruijff-Korbayov? et al, 2005; 
Rieser et al, 2005). 
Zollo collected seven dialogues with different 
human-wizard pairs to develop an evacuation plan. 
The overall WER was 30%. Of the 227 cases of 
incorrect ASR, wizard utterances indicated a fail-
ure to understand for only 35% of them. Wizards 
ignored words not salient in the domain and hy-
pothesized words based on phonetic similarity. In 
(Skantze, 2003), both users and wizards knew 
there was no dialogue system; 44 direction-finding 
dialogues were collected with 16 subjects. Despite 
a WER of 43%, the wizard operators signaled mis- 
understanding only 5% of the time, in part because 
they often ignored ASR errors and continued the 
dialogue. For the 20% of non-understandings, op-
erators continued a route description, asked a task-
related question, or requested a clarification.  
Williams and Young collected 144 dialogues 
simulating tourist requests for directions and other 
negotiations. WER was constrained to be high, 
medium, or low. Under medium WER, a task-
related question in response to a non-understanding 
or misunderstanding led to full understanding more 
often than explicit repairs. Under high WER, how-
ever, the reverse was true. Misunderstandings sig-
nificantly increased when wizards followed non-
understandings or misunderstandings with a task-
related question instead of a repair. 
In (Rieser et al, 2005), wizards simulated a 
multimodal MP3 player application with access to 
a database of 150K music albums. Responses 
could be presented verbally or graphically. In the 
noisy transcription condition, wizards made clarifi-
cation requests about twice as often as that found 
in similar human-human dialogue.  
In a system like CheckItOut, user utterances that 
request database information must be understood. 
We seek an approach that would reduce the rate of 
misunderstandings observed for high WER in 
(Williams & Young, 2004) and the rate of clarifi-
cation requests observed in (Rieser et al, 2005). 
4 CheckItOut and Embedded Wizards 
CheckItOut is modeled on library transactions at 
the Andrew Heiskell Braille and Talking Book Li-
brary, a branch of the New York Public Library 
and part of the National Library of Congress. Bor-
rowing requests are handled by telephone. Books, 
mainly in a proprietary audio format, travel by 
mail. In a dialogue with CheckItOut, a user identi-
fies herself, requests books, and is told which are 
available for immediate shipment or will go on re-
serve. The user can request a book by catalogue 
number, title, or author. 
CheckItOut builds on the Olympus/RavenClaw 
framework (Bohus & Rudnicky, 2009) that has 
been the basis for about a dozen dialogue systems 
in different domains, including Let?s Go Public! 
(Raux et al, 2005). Speech recognition relies on 
PocketSphinx. Phoenix, a robust context-free 
grammar (CFG) semantic parser, handles natural 
language understanding (Ward & Issar, 1994). The 
Apollo interaction manager (Raux & Eskenazi, 
2007) detects utterance boundaries using informa-
tion from speech recognition, semantic parsing, 
and Helios, an utterance-level confidence annotator 
(Bohus & Rudnicky, 2002). The dialogue manager 
is implemented in RavenClaw. 
842
To design CheckItOut?s dialogue manager, we 
recorded 175 calls (4.5 hours) from patrons to li-
brarians. We identified 82 book request calls, tran-
scribed them, aligned the utterances with the 
speech signal, and annotated the transcripts for di-
alogue acts. Because active patrons receive 
monthly newsletters listing new titles in the desired 
formats, patrons request specific items with ad-
vance knowledge of the author, title, or catalogue 
number. Most book title requests accurately repro-
duce the exact title, the title less an initial deter-
miner (?the,? ?a?), or a subtitle.  
We exploited the Galaxy message passing archi-
tecture of Olympus/RavenClaw to insert a wizard 
server into CheckItOut. The hub passes messages 
between the system and a wizard?s graphical user 
interface (GUI), allowing us to collect runtime in-
formation that can be included in models of wi-
zards? actions.  
For speech recognition, CheckItOut relies on 
PocketSphinx 0.5, a Hidden Markov Model-based 
recognizer. Speech recognition for this experiment, 
relied on the freely available Wall Street Journal 
?read speech? acoustic models. We did not adapt 
the models to our population or to spontaneous 
speech, thus insuring that wizards would receive 
relatively noisy recognition output.  
We built trigram language models from the 
book titles using the CMU Statistical Language 
Modeling Toolkit. Pilot tests with one male and 
one female native speaker indicated that a lan-
guage model based on 7500 titles would yield 
WER in the desired range. (Average WER for the 
book title requests in our experiment was 71%.) To 
model one aspect of the real world useful for an ac-
tual system, titles with below average circulation 
were eliminated. An offline pilot study had demon-
strated that one-word titles were easy for wizards, 
so we eliminated those as well. A random sample 
of 7,500 was chosen from the remaining 19,708 
titles to build the trigram language model. 
We used Ratcliff/Obersherhelp (R/O) to meas-
ure the similarity of an ASR string to book titles in 
the database (Ratcliff & Metzener, 1988). R/O cal-
culates the ratio r of the number of matching cha-
racters to the total length of both strings, but 
requires O(r2) time on average and O(r3) time in 
the worst case. We therefore computed an upper 
bound on the similarity of a title/ASR pair prior to 
full R/O to speed processing.  
5 Experimental Design 
In this experiment, a user and a wizard sat in sepa-
rate rooms where they could not overhear one 
another. Each had a headset with microphone and a 
GUI. Audio input on the wizard?s headset was dis-
abled. When the user requested a title, the ASR 
hypothesis for the title appeared on the wizard?s 
GUI. The wizard then selected the ASR hypothesis 
to execute a voice search against the database.  
Given the ASR and the query return, the wi-
zard?s task was to guess which candidate in the 
query return, if any, matched the ASR hypothesis. 
Voice search accessed the full backend of 71,166 
titles. The custom query designed for the experi-
ment produced four types of return, in real time, 
based on R/O scores: 
? Singleton: a single best candidate (R/O ? 0.85) 
? AmbiguousList: two to five moderately good 
candidates (0.85 > R/O ? 0.55) 
? NoisyList: six to ten poor but non-random can-
didates (0.55 > R/O ? 0.40) 
? Empty: No candidate titles (max R/O < 0.40) 
In pilot tests, 5%-10% of returns were empty ver-
sus none in the experiment. The distribution of 
other returns was: 46.7% Singleton, 50.5% Ambi-
guousList, and 2.8% NoisyList. 
Seven undergraduate computer science majors 
at Hunter College participated. Two were non-
native speakers of English (one Spanish, one Ro-
manian). Each of the possible 21 pairs of students 
met for five trials. During each trial, one student 
served as wizard and the other as user for a session 
of 20 title cycles. They immediately reversed roles 
for a second session, as discussed further below. 
The experiment yielded 4172 title cycles rather 
than the full 4200, because users were permitted to 
end sessions early. All titles were selected from the 
7500 used to construct the language model.  
Each user received a printed list of 20 titles and 
a brief synopsis of each book. The acoustic quality 
of titles read individually from a list is unlikely to 
approximate that of a patron asking for a specific 
title. Therefore, immediately before each session, 
the user was asked to read a synopsis of each book, 
and to reorder the titles to reflect some logical 
grouping, such as genre or topic. Users requested 
titles in this new order that they had created.  
Participants were encouraged to maximize a ses-
sion score, with a reward for the experiment win-
ner. Scoring was designed to foster cooperative 
843
strategies. The wizard scored +1 for a correctly 
identified title, +0.5 for a thoughtful question, and 
-1 for an incorrect title. The user scored +0.5 for a 
successfully recognized title. User and wizard 
traded roles for the second session, to discourage 
participants from sabotaging the others? scores.  
The wizard?s GUI presented a real-time live 
feed of ASR hypotheses, weighted by grayscale to 
reflect acoustic confidence. Words in each candi-
date title that matched a word in the ASR appeared 
darker: dark black for Singleton or AmbiguousList, 
and medium black for NoisyList. All other words 
were in grayscale in proportion to the degree of 
character overlap. The wizard queried the database 
with a recognition hypothesis for one utterance at a 
time, but could concatenate successive utterances, 
possibly with some limited editing.  
After a query, the wizard?s GUI displayed can-
didate matches in descending order of R/O score. 
The wizard had four options: make a firm choice of 
a candidate, make a tentative choice, ask a ques-
tion, or give up to end the title cycle. Questions 
were recorded. The wizard?s GUI showed the suc-
cess or failure of each title cycle before the next 
one began. The user?s GUI posted the 20 titles to 
be read during the session. On the GUI, the user 
rated the wizard?s title choices as correct or incor-
rect. Titles were highlighted green if the user 
judged a wizard?s offered title correct, red if incor-
rect, yellow if in progress, and not highlighted if 
still pending. The user also rated the wizard?s 
questions. Average elapsed time for each 20-title 
session was 15.5 minutes. 
A questionnaire similar to the type used in 
PARADISE evaluations (Walker et al, 1998) was 
administered to wizards and users for each pair of 
sessions. On a 5-point Likert scale, the average re-
sponse to the question ?I found the system easy to 
use this time? was 4 (sd=0; 4=Agree), indicating 
that participants were comfortable with the task. 
All other questions received an average score of 
Neutral (3) or Disagree (2). For example, partici-
pants were neutral (3) regarding confidence in 
guessing the correct title, and disagreed (2) that 
they became more confident as time went on. 
6 Learning Method and Goals 
To model wizard actions, we assembled 60 fea-
tures that would be available at run time. Part of 
our task was to detect their relative independence, 
meaningfulness, and predictive ability. Features 
described the wizard?s GUI, the current title ses-
sion, similarity between ASR and candidates, ASR 
relevance to the database, and recognition and con-
fidence measures. Because the number of voice 
search returns varied from one title to the next, fea-
tures pertaining to candidates were averaged.  
We used three machine-learning techniques to 
predict wizards? actions: decision trees, linear re-
gression, and logistic regression. All models were 
produced with the Weka data mining package, us-
ing 10-fold cross-validation (Witten & Frank, 
2005). A decision tree is a predictive model that 
maps feature values to a target value. One applies a 
decision tree by tracing a path from the root (the 
top node) to a leaf, which provides the target value. 
Here the leaves are the wizard actions: firm choice, 
tentative choice, question, or give up. The algo-
rithm used is a version of C4.5 (Quinlan, 1993), 
where gain ratio is the splitting criterion. 
To confirm the learnability and quality of the 
decision tree models, we also trained logistic re-
gression and linear regression models on the same 
data, normalized in [0, 1]. The logistic regression 
model predicts the probability of wizards? actions 
by fitting the data to a logistic curve. It generalizes 
the linear model to the prediction of categorical da-
ta; here, categories correspond to wizards? actions. 
The linear regression models represent wizards? 
actions numerically, in decreasing value: firm 
choice, tentative choice, question, give up.  
Although analysis of individual wizards has not 
been systematic in other work, we consider the 
variation in human performance significant. Be-
cause we seek excellent, not average, teachers for 
CheckItOut, our focus is on understanding good 
wizardry. Therefore, we learned two kinds of mod-
els with each of the three methods: the overall 
model using data from all of our wizards, and indi-
vidual wizard models.  
Preliminary cross-correlation confirmed that 
many of the 60 features were heavily interdepen-
dent. Through an initial manual curation phase, we 
isolated groups of features with R2 > 0.5. When 
these groups referenced semantically similar fea-
tures, we selected a single representative from the 
group and retained only that one. For example, the 
features that described similarity between hypo-
theses and candidates were highly correlated, so 
we chose the most comprehensive one: the number 
of exact word matches. We also grouped together 
844
and represented by a single feature: three features 
that described the gaps between exact word 
matches, three that described the data presented to 
the wizard, nine that described various system con-
fidence scores, and three that described the user?s 
speaking rate. This left 28 features.  
Next we ran CfsSubsetEval, a supervised 
attribute selection algorithm for each model 
(Witten & Frank, 2005). This greedy, hill-climbing 
algorithm with backtracking evaluates a subset of 
attributes by the predictive ability of each feature 
and the degree of redundancy among them. This 
process further reduced the 28 features to 8-12 fea-
tures per model. Finally, to reduce overfitting for 
decision trees, we used pruning and subtree rising. 
For linear regression we used the M5 method, re-
peatedly removing the attribute with the smallest 
standardized coefficient until there was no further 
improvement in the error estimate given by the 
Akaike information criterion. 
7 Results 
Table 1 shows the number of title cycles per wi-
zard, the raw session score according to the formu-
la given to the wizards, and accuracy. Accuracy is 
the proportion of title cycles where the wizard 
found the correct title, or correctly guessed that the 
correct title was not present (asked a question or 
gave up). Note that score and accuracy are highly 
correlated (R=0.91, p=0.0041), indicating that the 
instructions to participants elicited behavior con-
sistent with what we wanted to measure. 
Wizards clearly differed in performance, large-
ly due to their response when the candidate list did 
not include the correct title. Analysis of variance 
with wizard as predictor and accuracy as the de-
pendent variable is highly significant (p=0.0006); 
significance is somewhat greater (p=0.0001) where 
session score is the dependent variable. Table 2 
shows the distribution of correct actions: to offer a 
candidate at a given position in the query return 
(Returns 1 through 9), or to ask a question or give 
up. As reflected in Table 2, a baseline accuracy of 
about 65% could be achieved by offering the first 
return. The fifth column of Table 1 shows how of-
ten wizards did that (Offered Return 1), and clearly 
illustrates that those who did so most often (W3 
and W6) had accuracy results closest to the base-
line. The wizard who did so least often (W4) had 
the highest accuracy, primarily because she more 
often correctly offered no title, as shown in the last 
column of Table 1. We conclude that a spoken di-
alogue system would do well to emulate W4. 
Overall, our results in modeling wizards? actions 
were uniform across the three learning methods, 
gauged by accuracy and F measure. For the com-
bined wizard data, logistic regression had an accu-
racy of 75.2%, and F measures of 0.83 for firm 
choices and 0.72 for tentative choices; the decision 
tree accuracy was 82.2%, and the F measures for 
firm versus tentative choices were respectively 
0.82 and 0.71. The decision tree had a root mean 
squared error of 0.306, linear regression 0.483. Ta-
ble 3 shows the accuracy and F measures on firm 
choices for the decision trees by individual wizard, 
along with the numbers of attributes and nodes per 
Table 1. Raw session score, accuracy, proportion of offered titles that were listed first in the query return, and 
frequency of correct non-offers for seven participants. 
 
Participant Cycles Session Score Accuracy Offered Return 1 Correct Non-Offers 
W4 600 0.7585 0.8550 0.70 0.64 
W5 600 0.7584 0.8133 0.76 0.43 
W7 599 0.6971 0.7346 0.76 0.14 
W1 593 0.6936 0.7319 0.79 0.16 
W2 599 0.6703 0.7212 0.74 0.10 
W3 581 0.6648 0.6954 0.81 0.20 
W6 600 0.6103 0.6950 0.86 0.03 
Table 2. Distribution of correct actions 
 
Correct Action N % 
Return 1 2722 65.2445 
Return 2 126 3.0201 
Return 3 56 1.3423 
Return 4 46 1.1026 
Return 5 26 0.6232 
Return 7 7 0.1678 
Return 8 1 0.0002 
Return 9 2 0.0005 
Question or Giveup 1186 28.4276 
Total 4172 1.0000 
845
tree. Although relatively few attributes appeared in 
any one tree, most attributes appeared in multiple 
nodes. W1 was the exception, with a very small 
pruned tree of 7 nodes. 
Accuracy of the decision trees does not correlate 
with wizard rank. In general, the decision trees 
could consistently predict a confident choice (0.80 
? F ? 0.87), but were less consistent on a tentative 
choice (0.60 ? F ? 0.89), and could predict a ques-
tion only for W4, the wizard with the highest accu-
racy and greatest success at detecting when the 
correct title was not in the candidates.  
What wizards saw on the GUI, their recent suc-
cess, and recognizer confidence scores were key 
attributes in the decision trees. The five features 
that appeared most often in the root and top-level 
nodes of all tree models reported in Table 3 were: 
? DisplayType of the return (Singleton, Ambi-
guous List, NoisyList) 
? RecentSuccess, how often the wizard chose the 
correct title within the last three title cycles 
? ContiguousWordMatch, the maximum number 
of contiguous exact word matches between a 
candidate and the ASR hypothesis (averaged 
across candidates) 
? NumberOfCandidates, how many titles were re-
turned by the voice search 
? Confidence, the Helios confidence score 
DisplayType, NumberOfCandidates and Conti-
guousWordMatch pertain to what the wizard could 
see on her GUI. (Recall that DisplayType is distin-
guished by font darkness, as well as by number of 
candidates.) The impact of RecentSuccess might 
result not just from the wizard?s confidence in her 
current strategy, but also from consistency in the 
user?s speech characteristics. The Helios confi-
dence annotation uses a learned model based on 
features from the recognizer, the parser, and the di-
alogue state. Here confidence primarily reflects 
recognition confidence; due to the simplicity of our 
grammar, parse results only indicate whether there 
is a parse. In addition to these five features, every 
tree relied on at least one measure of similarity be-
tween the hypothesis and the candidates.  
W4 achieved superior accuracy: she knew when 
to offer a title and when not to. In the learned tree 
for W4, if the DisplayType was NoisyList, W4 
asked a question; if DisplayType was Ambiguous-
List, the features used to predict W4?s action in-
cluded the five listed above, along with the acous-
tic model score, word length of the ASR, number 
of times the wizard had asked the user to repeat, 
and the maximum size of the gap between words in 
the candidates that matched the ASR hypothesis. 
To focus on W4?s questioning behavior, we 
trained an additional decision tree to learn how W4 
chose between two actions: offering a title versus 
asking a question. This 37-node, 8-attribute tree 
was based on 600 data points, with F=0.91 for 
making an offer and F=0.68 for asking a question. 
The tree is distinctive in that it splits at the root on 
the number of frames in the ASR. If the ASR is 
short (as measured both by the number of recogni-
tion frames and the words), W4 asks a question 
when DisplayType = AmbiguousList or NoisyList, 
either RecentSuccess ? 1 or ContiguousWord-
Match = 0, and the acoustic model score is low. 
Note that shorter titles are more confusable. If the 
ASR is long, W4 asks a question when Conti-
guousWordMatch ? 1, RecentSuccess ? 2, and ei-
ther CandidateDisplay = NoisyList, or Confidence 
is low, and there is a choice of titles. 
8 Discussion 
Our experiment addressed whether voice search 
can compensate for incorrect ASR hypotheses and 
permit identification of a user?s desired book, giv-
en a request by title. The results show that with 
high WER, a baseline dialogue strategy that always 
offers the highest-ranked database return can nev-
ertheless achieve moderate accuracy. This is true 
even with the relatively simplistic measure of simi-
larity between the ASR hypothesis and candidate 
titles used here. As a result, we have integrated 
voice search into CheckItOut, along with a linguis-
tically motivated grammar for book titles. Our cur-
rent Phoenix grammar relies on CFG rules 
automatically generated from dependency parses 
of the book titles, using the MICA parser 
Table 3. Learning results for wizards 
 
Tree Rank Nodes Attributes Accuracy F firm 
W4 1  55 12 75.67 0.85 
W5 2  21 10 76.17 0.85 
W1 3  7 8 80.44 0.87 
W7 4  45 11 73.62 0.83 
W3 5  33 10 77.42 0.84 
W2 6  35 10 78.49 0.85 
W6 7  23 10 85.19 0.80 
 
846
(Bangalore et al, 2009). As described in (Gordon 
& Passonneau, 2010), a book title parse can con-
tain multiple title slots that consume discontinuous 
sequences of words from the ASR hypothesis, thus 
accommodating noisy ASR. For the voice search 
phase, we now concatenate the words consumed by 
a sequence of title slots. We are also experimenting 
with a statistical machine learning approach that 
will replace or complement the semantic parsing. 
Computers clearly do some tasks faster and 
more accurately than people, including database 
search. To benefit from such strengths, a dialogue 
system should also accommodate human prefe-
rences in dialogue strategy. Previous work has 
shown that user satisfaction depends in part on task 
success, but also on minimizing behaviors that can 
increase task success but require the user to correct 
the system (Litman et al, 2006). 
The decision tree that models W4 has lower ac-
curacy than other models? (see Table 3), in part be-
cause her decisions had finer granularity. A spoken 
dialogue system could potentially do as well as or 
better than the best human at detecting when the 
title is not present, given the proper training data. 
To support this, a dataset could be created that was 
biased toward a larger proportion of cases where 
not offering a candidate is the correct action.  
9 Conclusion and Current Work 
This paper presents a novel methodology that em-
beds wizards in a spoken dialogue system, and col-
lects data for a single turn exchange. Our results 
illustrate the merits of ranking wizards, and learn-
ing from the best. Our wizards were uniformly 
good at choosing the correct title when it was 
present, but most were overly eager to identify a 
title when it was not among the candidates. In this 
respect, the best wizard (W4) achieved the highest 
accuracy because she demonstrated a much greater 
ability to know when not to offer a title. We have 
shown that it is feasible to replicate this ability in a 
model learned from features that include the pres-
entation of the search results (length of the candi-
date list, amount of word overlap of candidates 
with the ASR hypothesis), recent success at select-
ing the correct candidate, and measures pertaining 
to recognition results (confidence, acoustic model 
score, speaker rate). If replicated in a spoken di-
alogue system, such a model could support integra-
tion of voice search in a way that avoids 
misunderstandings. We conclude that learning 
from embedded wizards can exploit a wider range 
of relevant features, that dialogue managers can 
profit from access to more fine-grained representa-
tions of user utterances, and that machine learners 
should be selective about which people to model. 
That wizard actions can be modeled using sys-
tem features bodes well for future work. Our next 
experiment will collect full dialogues with embed-
ded wizards whose actions will again be restricted 
through an interface. This time, NLU will integrate 
voice search with the linguistically motivated CFG 
rules for book titles described earlier, and a larger 
language model and grammar for database entities. 
We will select wizards who perform well during 
pilot tests. Again, the goal will be to model the 
most successful wizards, based upon data from 
recognition results, NLU, and voice search results. 
Acknowledgements 
This research was supported by the National 
Science Foundation under IIS-0745369, IIS-
084966, and IIS-0744904. We thank the anonym-
ous reviewers, the Heiskell Library, our CMU col-
laborators, our statistical wizard Liana Epstein, and 
our enthusiastic undergraduate research assistants. 
References 
Bangalore, Srinivas; Bouillier, Pierre; Nasr, Alexis; 
Rambow, Owen; Sagot, Benoit (2009). MICA: a 
probabilistic dependency parser based on tree 
insertion grammars. Application Note. Human 
Language Technology and North American Chapter 
of the Association for Computational Linguistics, 
pp. 185-188.  
Bohus, D.; Rudnicky, A.I. (2009). The RavenClaw 
dialog management framework: Architecture and 
systems. Computer Speech and Language, 23(3), 
332-361. 
Bohus, Daniel; Rudnicky, Alex (2002). Integrating 
multiple knowledge sources for utterance-level 
confidence annotation in the CMU Communicator 
spoken dialog system (Technical Report No. CS-
190): Carnegie Mellon University. 
Georgila, Kallirroi; Sgarbas, Kyrakos; Tsopanoglou, 
Anastasios; Fakotakis, Nikos; Kokkinakis, George 
(2003). A speech-based human-computer interaction 
system for automating directory assistance services. 
International Journal of Speech Technology, Special 
Issue on Speech and Human-Computer Interaction, 
6(2), 145-59. 
847
Gordon, Joshua, B.; Passonneau, Rebecca J. (2010). An 
evaluation framework for natural language 
understanding in spoken dialogue systems. Seventh 
International Conference on Language Resources 
and Evaluation (LREC). 
Johnston, Michael; Bangalore, Srinivas; Vasireddy, 
Gunaranjan; Stent, Amanda; Ehlen, Patrick; Walker, 
Marilyn A., et al (2002). MATCH--An architecture 
for multimodal dialogue systems. Proceedings of the 
40th Annual Meeting of the Association for 
Computational Linguistics, pp. 376-83.  
Komatani, Kazunori; Kanda, Naoyuki; Ogata, Tetsuya; 
Okuno, Hiroshi G. (2005). Contextual constraints 
based on dialogue models in database search task 
for spoken dialogue systems. The Ninth European 
Conference on Speech Communication and 
Technology (Eurospeech), pp. 877-880.  
Kruijff-Korbayov?, Ivana; Blaylock, Nate; 
Gerstenberger, Ciprian; Rieser, Verena; Becker, 
Tilman; Kaisser, Michael, et al (2005). An 
experiment setup for collecting data for adaptive 
output planning in a multimodal dialogue system. 
10th European Workshop on Natural Language 
Generation (ENLG), pp. 191-196.  
Levin, Esther; Narayanan, Shrikanth; Pieraccini, 
Roberto; Biatov, Konstantin; Bocchieri, E.; De 
Fabbrizio, Giuseppe, et al (2000). The AT&T-
DARPA Communicator Mixed-Initiative Spoken 
Dialog System. Sixth International Conference on 
Spoken Dialogue Processing (ICLSP), pp. 122-125.  
Litman, Diane; Hirschberg, Julia; Swerts, Marc (2006). 
Characterizing and predicting corrections in spoken 
dialogue systems. Computational Linguistics, 32(3), 
417-438. 
Litman, Diane; Pan, Shimei (1999). Empirically 
evaluating an adaptable spoken dialogue system. 7th 
International Conference on User Modeling (UM), 
pp. 55-46.  
Quinlan, J. Ross (1993). C4.5: Programs for Machine 
Learning. San Mateo, CA: Morgan Kaufmann. 
Ratcliff, John W.; Metzener, David (1988). Pattern 
Matching: The Gestalt Approach. Dr. Dobb's 
Journal, 46 
Raux, Antoine; Bohus, Dan; Langner, Brian; Black, 
Alan W.; Eskenazi, Maxine (2006). Doing research 
on a deployed spoken dialogue system: one year of 
Let's Go! experience. Ninth International 
Conference on Spoken Language Processing 
(Interspeech/ICSLP).  
Raux, Antoine; Eskenazi, Maxine (2007). A Multi-layer 
architecture for semi-synchronous event-driven 
dialogue management.IEEE Workshop on 
Automatic Speech Recognition and Understanding 
(ASRU 2007), Kyoto, Japan. 
Raux, Antoine; Langner, Brian; Black, Alan W.; 
Eskenazi, Maxine (2005). Let's Go Public! Taking a 
spoken dialog system to the real world.Interspeech 
2005 (Eurospeech), Lisbon, Portugal. 
Rieser, Verena; Kruijff-Korbayov?, Ivana; Lemon, 
Oliver (2005). A corpus collection and annotation 
framework for learning multimodal clarification 
strategies. Sixth SIGdial Workshop on Discourse 
and Dialogue, pp. 97-106.  
Sacks, Harvey; Schegloff, Emanuel A.; Jefferson, Gail 
(1974). A simplest systematics for the organization 
of turn-taking for conversation. Language, 50(4), 
696-735. 
Skantze, Gabriel (2003). Exploring human error 
handling strategies: Implications for Spoken 
Dialogue Systems. Proceedings of ISCA Tutorial 
and Research Workshp on Error Handling in Spoken 
Dialogue Systems, pp. 71-76.  
Stoyanchev, Svetlana; Stent, Amanda (2009). 
Predicting concept types in user corrections in 
dialog. Proceedings of the EACL Workshop SRSL 
2009, the Second Workshop on Semantic 
Representation of Spoken Language, pp. 42-49.  
Turunen, Markku; Hakulinen, Jaakko; Kainulainen, 
Anssi (2006). Evaluation of a spoken dialogue 
system with usability tests and long-term pilot 
studies. Ninth International Conference on Spoken 
Language Processing (Interspeech 2006 - ICSLP). 
Walker, M A.; Litman, D, J.; Kamm, C. A.; Abella, A. 
(1998). Evaluating Spoken Dialogue Agents with 
PARADISE: Two Case Studies. Computer Speech 
and Language, 12, 317-348. 
Wang, Ye-Yi; Yu, Dong; Ju, Yun-Cheng; Acero, Alex 
(2008). An introduction to voice search. IEEE 
Signal Process. Magazine, 25(3). 
Ward, Wayne; Issar, Sunil (1994). Recent improvements 
in the CMU spoken language understanding 
system.ARPA Human Language Technology 
Workshop, Plainsboro, NJ. 
Williams, Jason D.; Young, Steve (2004). 
Characterising Task-oriented Dialog using a 
Simulated ASR Channel. Eight International 
Conference on Spoken Language Processing 
(ICSLP/Interspeech), pp. 185-188.  
Witten, Ian H.; Frank, Eibe (2005). Data Mining: 
Practical Machine Learning Tools and Techniques 
(2nd ed.). San Francisco: Morgan Kaufmann. 
Zollo, Teresa (1999). A study of human dialogue 
strategies in the presence of speech recognition 
errors. Proceedings of AAAI Fall Symposium on 
Psychological Models of Communication in 
Collaborative Systems, pp. 132-139.  
Zue, Victor; Seneff, Stephanie; Glass, James; Polifroni, 
Joseph; Pao, Christine; Hazen, Timothy J., et al 
(2000). A Telephone-based conversational interface 
for weather information. IEEE Transactions on 
Speech and Audio Processing, 8, 85-96. 
 
848
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 248?258,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Embedded Wizardry
Rebecca J. Passonneau1, Susan L. Epstein2,3, Tiziana Ligorio3 and Joshua Gordon1
1Columbia University
New York, NY, USA
(becky|joshua)@cs.columbia.edu
2,3Hunter College
3The Graduate Center of the City University of New York
New York, NY, USA (susan.epstein@hunter|tligorio@gc).cuny.edu
Abstract
This paper presents a progressively challeng-
ing series of experiments that investigate clar-
ification subdialogues to resolve the words in
noisy transcriptions of user utterances. We fo-
cus on user utterances where the user?s spe-
cific intent requires little additional inference,
given sufficient understanding of the form. We
learned decision-making strategies for a dia-
logue manager from run-time features of our
spoken dialogue system and from observation
of human wizards we had embedded within it.
Results show that noisy ASR can be resolved
based on predictions from context about what
a user might say, and that dialogue manage-
ment strategies for clarifications of linguistic
form benefit from access to features from spo-
ken language understanding.
1 Introduction
Utterances have literal meaning derived from their
linguistic form, and pragmatic intent, the actions
speakers aim to achieve through words (Austin,
1962). Because the channel is usually not noisy
enough to impede communication, misunderstand-
ings that arise between adult human interlocutors
are more often due to confusions about intent, rather
than about words. Between humans and machines,
however, verbal interaction has a much higher rate
of linguistic misunderstandings because the channel
is noisy, and machines are not as adept at using spo-
ken language. It is difficult to arrive at accurate rates
for misunderstandings of form versus intent in hu-
man conversation, because the two types cannot al-
ways be distinguished (Schlangen and Fern?andez,
2005). However, one estimate of the rate of mis-
understandings of literal meaning between humans,
based on text transcripts of the British National Cor-
pus, is in the low range of 4% (Purver et al, 2001),
compared with a 30% estimate for human-computer
dialogue (Rieser and Lemon, 2011). The thesis
of our work is that misunderstandings of linguis-
tic form in human-machine dialogue are more ef-
fectively resolved through greater reliance on con-
text, and through closer integration of spoken lan-
guage understanding (SLU) with dialogue manage-
ment (DM). We investigate these claims by focusing
on noisy speech recognition for utterances where the
user?s specific intent requires little additional infer-
ence, given sufficient understanding of the form.
This paper presents three experiments that pro-
gressively address SLU methods to compensate for
poor automated speech recognition (ASR), and com-
plementary DM strategies. In two of the experi-
ments, human wizards are embedded in the spoken
dialogue system while run-time SLU features are
collected. Many wizard-of-Oz investigations have
addressed the noisy channel issue for SDS (Zollo,
1999; Skantze, 2003; Williams and Young, 2004;
Skantze, 2005; Rieser and Lemon, 2006; Schlangen
and Fern?andez, 2005; Rieser and Lemon, 2011).
Like them, we study how human wizards solve the
joint problem of interpreting users? words and in-
ferring users? intents. Our work differs in its ex-
ploration of the role context can play in the literal
interpretation of noisy language. We rely on knowl-
edge in the backend database to propose candidate
linguistic forms for noisy ASR.
Our principal results are that both wizards and our
248
SDS can achieve high accuracy interpretations, in-
dicating that predictions about what the user might
be saying can play a significant role in resolving
noise. We show it is possible to achieve low rates
of unresolved misunderstanding, even at word error
rates (WER) as poor as 50%-70%. We achieve this
through machine learned models of DM actions that
combine standard DM features with a rich number
and variety of SLU features. The learned models
predict DM actions to determine whether a reliable
candidate interpretation exists for a noisy utterance,
and if not, what action to take. The results support
an approach to DM design that integrates the two
problems of understanding form and intent.
The next sections present related work, our library
domain and our baseline SDS architecture. Subse-
quent sections discuss the SLU settings across the
three experiments, and present the experimental de-
signs and results, discussion and conclusion.
2 Related Work
Previous WOz studies of wizards? ability to pro-
cess noisy transcriptions of speaker utterances in-
clude the use of real (Skantze, 2003; Zollo, 1999)
or simulated ASR (Kruijff-Korbayova? et al, 2005;
Williams and Young, 2004). WOz studies that
directed their attention to the wizard include ef-
forts to predict: the wizard?s response when the
user is not understood (Bohus 2004); the wizard?s
use of multimodal clarification strategies (Rieser
and Lemon, 2006; Rieser and Lemon, 2011); and
the wizard?s use of application-specific clarification
strategies (Skantze, 2003; Skantze, 2005). WOz
studies that address real or simulated ASR reveal
that wizards can find ways to not respond to utter-
ances they fail to understand (Zollo, 1999; Skantze,
2003; Kruijff-Korbayova? et al, 2005; Williams and
Young, 2004). For example, they can prompt the
user for an alternative attribute of the same object.
Our work differs in that we address clarifications
about the words used, and rely on a rich set of SLU
features. Further, we compare behavior across wiz-
ards. Our SDS benefits from models of the most
skilled wizards.
To limit communication errors incurred by faulty
ASR, an SDS can rely on strategies to detect and re-
spond to incorrect recognition output (Bohus, 2004).
The SDS can repeatedly request user confirmation
to avoid misunderstanding, or ask for confirmation
using language that elicits responses from the user
that the system can handle (Raux and Eskenazi,
2004). When the user adds unanticipated informa-
tion in response to a system prompt, two-pass recog-
nition can rely on a concept-specific language model
to improve the recognition of the domain concepts
within the utterance containing unknown words, and
thereby achieve better recognition (Stoyanchev and
Stent, 2009). An SDS could take this approach one
step further and use context-specific language for in-
cremental understanding of noisy input throughout
the dialogue (Aist et al, 2007).
Current work on error recovery and grounding for
SDS assumes that the primary responsibility of a
dialogue management strategy is to understand the
user?s intent. Errors of understanding are addressed
by ignoring the utterances where understanding fail-
ures occur, asking users to repeat, or pursuing clari-
fications about intent. These strategies typically rely
on knowledge sources that follow the SLU stage.
The RavenClaw dialogue manager, which represents
domain-dependent (task-based) DM strategy as a
tree of goals, triggers error handling by means of a
single confidence score associated with the concepts
hypothesized to represent the user?s intent (Bohus
and Rudnicky, 2002; Bohus and Rudnicky, 2009).
Features for reinforcement learning of MDP-based
DM strategies include a few lexical features and a
measure of noise analogous to WER (Rieser and
Lemon, 2011). The WOz studies reported here yield
learned models of specific actions in response to
noisy input, such as whether to treat a candidate in-
terpretation as correct, or to pursue one of many pos-
sible clarification strategies, including clarifications
of form or intent. These models rely on relatively
large numbers of features from all phases of spoken
language understanding, as well as on typical dia-
logue management features.
3 CheckItOut
3.1 Domain
Our domain of investigation simulates book orders
from the Andrew Heiskell Braille and Talking Book
Library, part of the New York Public Library and the
Library of Congress. Patrons order books by tele-
249
phone during conversation with a librarian, and re-
ceive them by mail. Patrons typically have identify-
ing information for the books they seek, which they
get from monthly newsletters. In a corpus of eighty
two calls recorded at the library, we found that most
book requests by title were very faithful to the actual
title. Challenges to SLU in this domain include the
size of the database, the size of the vocabulary, and
the average sentence length.
While large databases have been used for inves-
tigations of phonological query expansion (Georgila
et al, 2003), much of the research on DM strategy
relies on relatively small databases. A recent study
of reinforcement learning of DM strategy modeled
as a Markov Decision Process reported in (Rieser
and Lemon, 2011) relies on a database of 438 items.
In (Gordon and Passonneau, 2011) we compared
the SLU challenges faced by CheckItOut and the
Let?s Go bus schedule information system, both of
which rely on the same architecture (Raux et al,
2005). The Let?s Go corpus contained 70 bus routes
names and 1300 place names, and a mean utterance
length of 4.4 words. The work reported here uses the
full 2007 version of Heiskell?s database of 71,166
books and 28,031 authors, and a sanitized version
of its 2007 patron database of 5,028 active patrons.
Authors and titles contribute 45,636 distinct words,
with a 10.43% overlap between the two. Average
book title length is 5.4 words; 26% of titles are 1-2
words, 44% are 3-5 words, 20% are 6 to 10. Con-
sequently, our domain has relatively long utterances.
The syntax of book titles is much richer than typical
SDS slot fillers, such as place or person names.
To achieve high-confidence SLU, we integrate
voice search into the SLU components of our two
SDS experiments (Wang et al, 2008).1 Our custom
voice search query relies on Ratcliff/Obershershelp
(R/O) pattern matching (Ratcliff and Metzener,
1988), the ratio of the number of matching charac-
ters to the total length of both strings. This simple
metric captures gross similarities without overfitting
to a specific application domain. The criteria for se-
lecting R/O derive from our first offline experiment,
described in Section 4.2.
For an experiment focused only on a single turn
1In concurrent work on a new SDS architecture, we use en-
sembles of SLU strategies (Gordon and Passonneau, 2011; Gor-
don et al, 2011).
(a) Baseline CheckItOut
(b) Embedded Wizard
Figure 1: CheckItOut information pipeline
exchange beginning with a user book request, we
queried the backend directly with the ASR string.
For a subsequent experiment on full dialogues, we
queried the backend with a modified ASR string, be-
cause the SDS architecture we used permits backend
queries to occur only during the dialogue manage-
ment phase, after natural language understanding.
The next section describes this architecture.
3.2 Architecture
CheckItOut, our baseline SDS, employs the Olym-
pus/RavenClaw architecture developed at Carnegie
Mellon University (CMU) (Raux et al, 2005; Bo-
hus and Rudnicky, 2009). SDS modules commu-
nicate via message passing, controlled by a central
hub. However, the information flow is largely a
pipeline, as depicted in Figure 1(a). The Pocket-
Sphinx recognizer (Huggins-Daines et al, 2006) re-
ceives acoustic data segmented by the audio man-
ager, and passes a single recognition hypothesis to
the Phoenix parser (Ward and Issar, 1994). Phoenix
sends one or more equivalently ranked semantic
parses to the Helios confidence annotator (Bohus
and Rudnicky, 2002), which selects a parse and as-
signs a confidence score. The Apollo interaction
manager (Raux and Eskenazi, 2007) monitors the
three SLU modules?the recognizer, the semantic
parser, and the confidence annotator?to determine
whether the user or SDS has the current turn. To
a limited degree, Apollo can override the early seg-
mentation decisions based solely on pause length.
250
Confidence-annotated concepts from the semantic
parse are passed to the RavenClaw DM, which de-
cides when to prompt the user, present information
to her, or query the backend database.
A wizard server communicates with other mod-
ules via the hub, as shown in Figure 1(b). For each
wizard experiment, we constructed a graphical user
interface (GUI). Wizard GUIs display information
for the wizard in a manageable form, and allow the
wizard to query the backend or select communica-
tive actions that result in utterances directed to the
user. Figure 1(b) shows an arrow from the speech
recognizer directly to the wizard: the recognition
string has been vetted by Apollo before it is dis-
played to the wizard.
4 Experiments and Results
The experiments reported here are an off-line pilot
study to identify book titles under worst case recog-
nition (Title Pilot), an embedded WOz study of a
single turn exchange involving book requests by ti-
tle (Turn Exchange), and an embedded WOz study
of dialogues where users followed scenarios that in-
cluded four books at a time (Full WOz). To evaluate
the impact of learned models of wizard actions from
the Full WOz wizard data, we evaluated CheckItOut
before and after the dialogue manager was enhanced
with wizard models for specific actions.
4.1 Experimental Settings
All three experiments use the full database for
search. To control for WER, the knowledge sources
for speech recognition and semantic parsing vary
across experiments. For each experiment, Table 1
indicates the acoustic model (AM) used, the num-
ber of hours of domain-specific spontaneous speech
used for AM adaptation, the number of titles used
to construct the language model (LM), the type of
LM, the type of grammar rules in the Phoenix book
title subgrammar, and average WER as measured by
Levenstein word edit distance (Levenshtein, 1996).
For the first two experiments, we used CMU?s
Open Source WSJ1 dictation AMs for wideband
(16kHz) microphone (dictation) speech. For Full
WOz we adapted narrowband (8kHz) WSJ1 dicta-
tion speech with about eight hours of data collected
from Turn Exchange and two hours of scripted spon-
taneous speech typical of CheckItOut dialogues.
Logios is a CMU toolkit for generating a pseudo-
corpus from a Phoenix grammar. It produces a set
of strings generated by Phoenix production rules,
which in turn are used to build an LM (Carnegie
Mellon University Speech Group, 2008). Before we
explain the three rightmost columns in Table 1, we
first briefly describe Phoenix, the Phoenix book title
subgrammar, and how we combine title strings with
a Logios pseudo-corpus.
Phoenix is a context-free grammar (CFG) parser
that produces one or more semantic frames per
parse. A semantic frame has slots, where each slot is
a concept with its own CFG productions (subgram-
mar). To accommodate noisy ASR, the parser can
skip words between frames or slots. Phoenix is well-
suited for restricted domains, where a frame repre-
sents a particular type of subdialogue (e.g., ordering
a plane ticket), and slots represent constrained con-
cepts (e.g., departure city, destination city). Phoenix
is not well-suited for book titles, which have a rich
vocabulary and syntax, and no obvious component
slots. The CFG rules for the Turn Exchange book ti-
tle subgrammar consisted of a verbatim rule for each
book title. Rules that consisted of a bag-of-words
(BOW; i.e., unordered) for each title proved to be
too unconstrained.2 In Turn Exchange, interpreta-
tion of ASR consisted primarily of voice search; the
highly constrained CFG rules (exact words in exact
order) had little impact on performance. For base-
line CheckItOut dialogues, and for Full WOz, we
required more constrained grammar rules that would
preserve Phoenix?s robustness to noise.
To avoid the brittleness of exact string CFG rules,
and the massive over-generation of BOW CFG rules,
we wrote a transducer that mapped dependency
parses of book titles to CFG rules. When ASR
words are skipped, book title parses can consist of
multiple slots. We used MICA, a broad-coverage
dependency grammar (Bangalore et al, 2009) to
parse the entire book title database. When a set
of titles is selected for an experiment, the corre-
sponding MICA parses are transduced to the rele-
vant CFG productions, and inserted into a Phoenix
grammar. Productions for the author subgrammar
2BOW Phoenix rules for book titles are used in a more re-
cent Olympus/RavenClaw system inspired in part by Check-
ItOut (Lee et al, 2010), with a database of 15,088 eBooks.
251
Exp. AM Adapted # Titles for LM LM Grammar rules WER
Title Pilot WSJ1 16kHz NA 500 unigram NA 0.76
Turn Exchange WSJ1 16kHz NA 7,500 trigram title strings 0.71
Full WOz WSJ1 8kHz 10 hr. 3,000 Logios + book data Mica-based 0.50 (est)
Table 1: SLU settings across experiments
consist largely of a first name slot followed by a last
name slot. The remaining portions of the Phoenix
CheckItOut grammar consist of subgrammars for
book request prefixes and affixes (e.g., ?I would like
the book called?), for confirmations and rejections,
phone numbers, book catalogue numbers, and mis-
cellaneous additional concepts. The set of subgram-
mars excluding the book title and author subgram-
mars (book requests, confirmations, and so on; the
grammar shell) are the same for all experiments.
The MICA-based book title grammar also provides
several features (e.g., number of slots in a parse) for
machine learning.
The Title Pilot LM consisted of unigram frequen-
cies of the 1400 word types from a random sample
(without replacement) of 500 titles. For Turn Ex-
change, a trigram LM was constructed from 7,500
titles randomly selected from the 19,708 titles that
remained after we eliminated one-word titles and ti-
tles with below average circulation. For Full WOz,
3,000 books were randomly selected from the full
book database (with no more than three titles by
the same author, and no one-word titles). Logios
was used on the grammar shell to generate an initial
pseudo-corpus, which was combined with the book
title and author strings to generate a full pseudo-
corpus for the trigram LM (denoted as ?Logios +
book data? in Table 1).
4.2 Title Pilot
The Title Pilot (Passonneau et al, 2009) was an of-
fline investigation of how reliance on prior knowl-
edge in the database might facilitate interpretation
of noisy ASR. It demonstrates that given the context
of things a user might say, ASR that is otherwise un-
intelligible becomes intelligible.
Three males each read 50 randomly selected ti-
tles from the LM subset of 500 (see Table 1). Their
average WER was 0.75, 0.83 and 0.69, respectively.
Three undergraduates (A, B, C) were each given one
of the sets of 50 recognition strings from a different
speaker. Each also received a plain text file listing all
the titles in the database, and word frequency statis-
tics for the book titles. Their task was to try to find
the correct title, and to provide a brief description of
their overall strategy.
A was accurate on 66.7% of the titles he matched,
B and C on 71.7%. We identified similar strate-
gies for A and B, including number of exact word
matches, types of exact word matches (e.g., content
words were favored over stop words), rarity of ex-
act word matches, and phonetic similarity. Analysis
of C?s responses showed dependency on number and
types of exact word matches, and on miscellaneous
strategies that could not be grouped. Through in-
spection, we determined that similarity in length and
number of words were important factors. From this
experiment, we concluded that humans are adept at
interpreting noisy ASR when provided with context;
that voice search (queries to the backend with ASR)
would prove useful, given an appropriate similarity
metric; and that there would likely always be uncer-
tain cases that might lead to false hits. As we discuss
below, two of seven Turn Exchange wizards were
fairly adept, and five of six Full WOz wizards were
very adept, at avoiding false hits from voice search.
4.3 Turn Exchange
The offline Title Pilot suggested that voice search
could lead to far fewer non-understandings, given
some predictions as to the actual words a noisy ASR
string might represent. The next experiment ad-
dressed, in real time, the question of what level of
accuracy might be achieved through an online im-
plementation of voice search for book requests by
title (Passonneau et al, 2010; Ligorio et al, 2010b).
We embedded wizards into the CheckItOut SDS to
present them with live ASR, and to collect runtime
recognition features. On the GUI, variations in the
display fonts for ASR and voice search returns cued
the wizard to gross differences in word-level recog-
nition confidence, and similarities between an ASR
string and each candidate returned by the search.
Learned models of wizard actions indicated that
252
recognition features such as acoustic model fit and
speech rate, along with various measures of sim-
ilarity between the ASR output string and candi-
date titles, number of books ordered thus far (Re-
centSuccess), and number of relatively close candi-
date matches, were useful in modeling the most ac-
curate wizards. These results show that DM strat-
egy for determing what actions to take, given an in-
terpretation of a user request, can depend on subtle
recognition metrics.
In Turn Exchange, users requested books by ti-
tle from embedded wizards. Speech input and out-
put was by microphone and headset, with wizards
and users seated in separate rooms, each using a dif-
ferent GUI. Seven undergraduates (one female and
six males, including two non-native speakers of En-
glish) participated as paid subjects. Each of the 21
possible pairs of students met for five trials. A trial
had two sessions. In the first, one student served as
wizard and the other as user for a session in which
the user requested 20 books by title. In the second
session, the students reversed roles. We collected
4,192 turn exchanges.
The GUI displayed the ASR corresponding to the
user utterance, with confident words in bolder font.
The wizard could query the backend with some or
all of the ASR. Voice search results displayed a sin-
gle candidate above a high R/O threshold with all
matching words in boldface, or three candidates of
moderate similarity with matching words in medium
bold, or five to ten candidates of lower similarity in
grayscale. There were four available wizard actions:
to offer a candidate title to the user in a confident
manner (through Text-to-Speech), to offer a title ten-
tatively, to select two or more candidates and ask a
free-form question about them (here the user would
hear the wizard?s speech), or to give up. The user in-
dicated whether an offered candidate was correct, or
indicated the quality and appropriateness of a wiz-
ard?s question. A prize would go to the wizard who
offered the most correct titles.
The top ranked search return was correct 65.24%
of the time. The two wizards who most often offered
the top ranked return (81% and 86% of the time)
both achieved 69.5% accuracy. The two best wiz-
ards (W4 and W5) could detect search returns that
did not contain the correct title, thus avoiding false
hits. On average, they offered the top return only
73% of the time and both achieved the highest accu-
racy (83.4%).
Several classification methods were used to pre-
dict the four wizard actions: firm offer, tentative of-
fer, question, and give up. Features (N=60) included
many ASR metrics, such as word-level confidence,
AM fit, and three measures of speech rate; various
measures of the average similarity or overlap be-
tween the ASR string and the candidate titles from
the R/O query; the dialogue history; the number of
candidates titles returned; and so on. The learned
classifiers, including C4.5 decision trees (Quinlan,
1993), all had similar performance. Learned trees
for W4 and W5 both had F measures of 0.85. De-
cision trees give a transparent view of the relative
importance of features; those nearer the root have
greater discriminatory power. Common features at
the tops of trees for all wizards were the type and
size of the query return, how often the wizard had
chosen the correct title in the last three title cycles,
the average of the maximum number of contiguous
exact word matches between the ASR string and the
candidate titles, and the Helios confidence score.
We trained an additional decision tree to learn
how W4 (the best wizard) chose between offering
a title versus asking a question (F=0.91 for making
an offer; F=0.68 for asking a question). The tree
is distinctive in that it splits at the root on a mea-
sure of speech rate. If the ASR is short (as mea-
sured both by the number of recognition frames and
the words), W4 asks a question if the query return
is not a single title, and either RecentSuccess=1 or
ContiguousWord-Match=0, and the acoustic model
score is low. Note that shorter titles are more con-
fusable. If the ASR is long, W4 asks a question
when ContiguousWordMatch=1, RecentSuccess=2,
and either CandidateDisplay = NoisyList, or Helios
Confidence is low, and there is a choice of titles.
4.4 Full WOz
The third experiment was a full WOz study demon-
strating that embedded wizards could achieve high
task success by relying on a large number of actions
that included clarifications of utterance form or in-
tent. Here we briefly report results on task success
and time on task in a comparision of baseline Check-
ItOut with an enhanced version, CheckItOut+, that
incorporates learned models of wizard actions. The
253
evaluation demonstrates improved performance with
more books ordered, more correct books ordered,
and less elapsed time per book, or per correct book.
For Full WOz (Ligorio et al, 2010a), CheckItOut
relied on VOIP (Voice over Internet Protocol) tele-
phony. Users interacted with the embedded wizards
by telephone, and wizards took over after Check-
ItOut answered the phone. After familiarization
with the task and GUI, nine wizards auditioned and
six were selected. There were ten users. Both groups
were evenly balanced for gender. Users were di-
rected to a website that presented scenarios for each
call. The scenario page gave the user a patron iden-
tity and phone number, and author, title and cata-
logue number information for four books they were
to order. Each user was to make at least fifteen calls
to each wizard; we recorded 913 usable calls.
A single trainer prepared the original nine wizard
volunteers one at a time. First, each trainee practiced
on data from the experiments described above. Next,
the trainer explained the wizard GUI and demon-
strated it, serving as wizard on a sample call. Fi-
nally, the trainee served as wizard on five test calls
with guidance from the trainer. The trainer chose the
six most skilled and motivated trainees as wizards.
The GUI had two screens, one for user login
and one for book requests. Users identified them-
selves by scenario phone number. The book re-
quest screen had a scrollable frame displaying the
ASR for each user utterance. Separate frames on
the GUI displayed the query return, dialogue history,
basic actions (e.g., querying the backend with a cus-
tom R/O query, or prompting the user for a book),
and auxiliary actions (e.g., removing a book from
the order in progress). Finally, wizards could select
among four types of dialogue acts: signals of non-
understanding, or clarifications about the ASR, the
book request or the query return. A dialogue act se-
lected by the wizard was passed to a template-based
natural language generator, and then to a Text-to-
Speech component. Due to their complexity, calls
could be time consuming. A clock on the GUI indi-
cated call duration; wizards were instructed to finish
the current book request and then terminate the call
after six minutes.
A wizard?s precision is the proportion of books
she offer that correctly match the user?s request; five
of the six wizards had precision over 90%. A wiz-
ard?s recall is the number of books in the scenario
that she correctly identified. The two best wizards,
WA and WB, had the highest recall, 63% and 67%
respectively.
The number of book requests per dialogue was
tallied automatically. Some dialogues were termi-
nated before all scenario books could be requested.
Also, a wizard who experienced problems with a
book request could abandon the current request and
prompt the user for a new book. The user could re-
sume the abandoned book request later in the dia-
logue. In such cases, the abandoned and resumed re-
quests for the same book would count as two distinct
book requests. Given these facts, the ratio of number
of correct books to number of book requests yields
only an approximate estimate of how many scenario
books were correctly identified. WA correctly iden-
tified 2.69 books per call from 3.64 requests per call,
yielding a total success rate of 73.9% per book re-
quest, and 67.25% per 4-book scenario. WB cor-
rectly identified 2.54 books per call from 4.44 re-
quests per call, yielding success rates of 57.21% per
request and 63.50% per 4-book scenario. WA and
WB had quite distinct strategies. WA persisted with
each book request and exploited a wide range of
the available GUI actions, with the greatest num-
ber of actions per book request among all wizards
(N=8.24). WB abandoned book requests early and
moved on to the next book request, exploited rela-
tively fewer GUI actions, and had the fewest actions
per book request (N=5.10).
From 163 features that characterize the ASR,
search, current user utterance, current turn ex-
change, current book request, and the entire dia-
logue, we learned models for three types of wiz-
ard actions: select a non-understanding prompt, per-
form a search, or select a prompt to disambiguate
among search returns. We used three machine learn-
ing methods for classification: decision trees, logis-
tic regression and support vector machines. Table 2
gives the accuracies and overall F measures for de-
cision trees that model WA and WB. (All learning
methods have similar performance.)
Of note here is the range of features that predict
when the best wizards selected a non-understanding,
shown in Table 3. In addition, the two models de-
pend partly on different features. Trees for the other
actions in Table 2 have similarly diverse features.
254
Wizard Action Acc F
A Non-Understanding 0.71 0.71
B Non-Understanding 0.73 0.73
A Disambiguate 0.80 0.81
B Disambiguate 0.86 0.87
A Search 0.94 0.95
B Search 0.93 0.94
Table 2: Performance of learned trees
To evaluate the benefit of learned models of wiz-
ard actions for SDS, we conducted two data collec-
tions where subjects placed calls following the same
types of scenarios used in Full WOz. For our base-
line evaluation of CheckItOut, 10 subjects were re-
cruited from Columbia University and Hunter Col-
lege. Each was to place a minimum of 50 calls over
a period of three days; 562 calls were collected. For
each call, subjects visited a web page that presented
a new scenario. Each scenario included mock patron
data for the caller to use (e.g., name, address and
phone number), a list of four books, and instructions
to request one book by catalogue number, one by
title, one by author, and one by any of those meth-
ods. At three points during their calls, subjects com-
pleted a user satisfaction survey containing eleven
questions adapted from (Hone and Graham, 2006).
CheckItOut+ is an enhanced version of our SDS
in which the DM was modified to include learned
models for three decisions. The first determines
whether the system should signal non-understanding
in response to the caller?s last utterance, and exe-
cutes before voice search would take place. The
second determines whether to perform voice search
with the ASR (i.e., before the parse, in contrast to
CheckItOut). The third executes after voice search,
and determines whether to offer the candidate with
the highest R/O score to the user. The evaluation
setup for CheckItOut+ also included 10 callers who
were to place 50 calls each; 505 calls were collected.
Here we report results that compare the number
of books ordered per call, the number of correct
books per call, the elapsed time per book ordered,
and elapsed time per correct book. T-tests show all
differences to be highly significant. (A full discus-
sion of the evaluation results will appear in future
publications.) Callers to CheckItOut+ nearly always
ordered four books (3.998), compared with 3.217 for
the baseline (p < 0.0001). There was an increase
of correct books in the order from 2.40 in the base-
Feature WA WB
# books ordered so far Y Y
% unparsed ASR words Y N
Avg. word confidence Y N
# explicit confirms in call Y Y
# MICA slots per concept Y N
# searches in call Y N
Most recent wizard action N Y
Most frequent concept in call N Y
Speech rate N Y
# user utts. this request N Y
# author searches in call Y Y
Normalized LM score this utt Y Y
Table 3: Features that predict wizards? non-
understanding
line to 2.70 in CheckItOut+ (p < 0.0001). The total
elapsed time per call increased by only 13 seconds
from 210.93 to 223.96 (p < 0.0175). Given that
CheckItOut+ callers ordered more books and more
correct books, CheckItOut+ performed much faster.
The elapsed time per ordered book decreased from
65.57 to 56.01 seconds, and decreased from 87.89 to
82.95 seconds per correct books.
5 Discussion
Spoken language understanding has been relatively
under-investigated in SDS design. Our experiments
suggest that tighter integration of all phases of SLU
with dialogue management can lead to more robust
system behavior. We illustrate here with an exam-
ple of WA?s strategic questioning in which a non-
understanding is avoided, and WA builds on partial
understanding to identify the user?s objective.
In response to ASR MARY .JO. EARTH, where
the ?.?s bracket an unconfident word, WA?s search
returned three authors with first name Mary, and
last names that had moderate character overlap with
.JO. EARTH. WA first asked whether the book
was by Mary O?Hara. When the user responded
negatively, WA confirmed the first name, prompted
for the last name, and got SURE as the ASR. WA fi-
nally confirmed that the book was by Mary Stewart.
Although it took four turns, WA was able to identify
the correct book.
In general, the Full WOz corpus contains a very
high proportion of wizard questions. In the to-
tal corpus of 20,415 caller utterances, there were
11,562 wizard questions. The types of questions
255
S1: What?s the next book?
U1: .BARBARA. THREE THREE
S2: Is the author barbara freethy?
U2: YES
S3: Would you like ?some kind of wonderful? by
BARBARA FREETHY?
U3: YES
(a) Example 1
S1: Sorry, I misunderstood. Can you repeat the author?
U1: DIDN?T I AM THE .GOLDEN. ARM
S2: Is the title ?THE man with THE golden ARM ??
U2: NO
S3: Sorry, I misunderstood. Can you repeat the title please?
U3: .A. .AMBLING. .THE. .GAME. .EDELMAN. STORY
S4: Is the title ?up and running the jami goldman STORY ??
U4: YES
(b) Example 2
Figure 2: Sample Clarification Subdialogues
wizard?s ask not only often lead to successful con-
cept identification, they also avoid prompting the
user to repeat what they said. Previous work has
presented results showing that the hyperarticulation
associated with user repetitions often leads users to
slow their speech, speak more loudly, and pronounce
words more carefully, which hurts recognition per-
formance (Hirschberg et al, 2004).
Figure 2 illustrates two clarification subdialogues
from CheckItOut+. The first illustrates how prior
knowledge about what a user might say provides
sufficient constraints to interpret ASR that would
otherwise be unintelligible. The first word in the
ASR for the caller?s first utterance is bracketed by
?.?, which again represents low word confidence.
The high confidence words THREE THREE are
phonologically and orthographically similar to the
actual author name, Freethy. Note that from the
caller?s point of view, the same question shown
in S3 could be motivated by confusion over the
words alone, as in this case, or confusion over the
words and multiple candidate referents (e.g., Bar-
bara Freethy versus Freeling).
The second clarification subdialogue illustrates
how confusions about the linguistic input can be
resolved through strategies that combine questions
about words and intents. The prompt at system turn
3 indicates that the system believes that the caller
provided a title in user turn 1, which is incorrect.
The caller responds with the title, however, which
provides an alternative means to guess the intended
book, Jami Goldman?s memoir Up and Running.
6 Conclusion
The studies reported here are premised on two hy-
potheses about the role spoken language understand-
ing plays in SDS design. First, prior knowledge
derived from the context in which a dialogue takes
place can yield predictions about the words a user
might produce, and that these predictions can play
a key role in interpreting noisy ASR. Here we have
used context derived from knowledge in the appli-
cation database. Similar results could follow from
predictions from other sources, such as an explicit
model of the alignment of linguistic representa-
tions proposed in the work of Pickering and Gar-
rod (e.g., (Pickering and Garrod, 2006). Second,
closer integration of spoken language understanding
and dialogue management affords a wider range of
clarification subdialogues.
Our results from the experiments reported here
support both hypotheses. Our first experiment
demonstrated that words obscured by very noisy
ASR (50% ? WER ? 75%) can be inferred by re-
liance on what might have been said, predictions
that came from the database of entities in the do-
main. We assume that an SDS that interacts well
when ASR quality is poor will perform all the better
when ASR quality is good. Our second experiment
demonstrated that two of five human wizards were
able to achieve high accuracy in on-line resolution
of noisy ASR, when presented with no more than ten
candidate matches. Run-time recognition features
not available to the wizards were nonetheless useful
in modeling the ability of the two best wizards to
avoid false hits. Our third experiment demonstrated
that wizards could achieve high task success on full
dialogues where callers requested four books, and
an enhancement of our baseline SDS with learned
models of three wizard actions led to improved task
success with less time per subtask. The variety of
features that contribute to learned models of wiz-
ard actions demonstrates the advantages of embed-
ded wizardry, as well as the benefit of DM clarifica-
tion strategies that include features from all phases
of SLU.
256
Acknowledgments
The Loqui project is funded by the National Science
Foundation under awards IIS-0745369, IIS-0744904
and IIS-084966. We thank those at Carnegie Mel-
lon University who helped us construct Check-
ItOut through tutorials and work sessions held at
Columbia University and Carnegie Mellon Univer-
sity, and who responded to numerous emails about
the Olympus/RavenClaw architecture and compo-
nent modules: Alex Rudnicky, Brian Langner,
David Huggins-Daines, and Antoine Raux. We also
thank the many undergraduates from Columbia Col-
lege, Barnard College, and Hunter College who as-
sisted with tasks that supported the implementation
of CheckItOut, including the telephony.
References
Gregory Aist, James Allen, Ellen Campana, Car-
los Gomez Gallo, Scott Stoness, Mary Swift, and
Michael K. Tanenhaus. 2007. Incremental dialogue
system faster than and preferred to its nonincremental
counterpart. In COGSCI 2007, pages 779?74.
John L. Austin. 1962. How to Do Things with Words.
Oxford University Press, New York.
Srinivas Bangalore, Pierre B. Boullier, Alexis Nasr,
Owen Rambow, and Beno??it Sagot. 2009. Mica: a
probabilistic dependency parser based on tree insertion
grammars. In NAACL/HLT, pages 185?188.
Dan Bohus and Alex Rudnicky. 2002. Integrating multi-
ple knowledge sources for utterance-level confidence
anno-tation in the CMU Communicator spoken dia-
logue system. Technical Report CS-02-190, Carnegie
Mellon University, Department of Computer Science.
Dan Bohus and Alex Rudnicky. 2009. The RavenClaw
dialog management framework. Computer Speech and
Language, 23:332?361.
Dan Bohus. 2004. Error awareness and recovery in con-
versational spoken language interfaces. Ph.D. thesis,
Carnegie Mellon University, Computer Science.
Carnegie Mellon University Speech Group. 2008.
The Logios tool. https://cmusphinx.svn.
sourceforge.net/svnroot/cmusphinx/
trunk/logios.
Kallirroi Georgila, Kyrakos Sgarbas, Anastasios
Tsopanoglou, Nikos Fakotakis, and George Kokki-
nakis. 2003. A speech-based human-computer
interaction system for automating directory assistance
services. International Journal of Speech Technology,
Special Issue on Speech and Human-Computer
Interaction, 6:145?59.
Joshua Gordon and Rebecca J. Passonneau. 2011.
An evaluation framework for natural language under-
standing in spoken dialogue systems. In 7th LREC.
Joshua Gordon, Rebecca J. Passonneau, and Susan L. Ep-
stein. 2011. Helping agents help their users despite
imperfect speech recognition. In Proceedings of the
AAAI Spring Symposium 2011 (SS11): Help Me Help
You: Bridging the Gaps in Human-Agent Collabora-
tion.
Julia Hirschberg, Diane Litman, and Marc Swerts. 2004.
Prosodic and other cues to speech recognition failures.
Speech Communication, 43(1-2):155?75.
Kate S. Hone and Robert Graham. 2006. Towards a tool
for the subjective assessment of speech system inter-
faces (sassi). Natural Language Engineering, Special
ISsue on Best Practice in Spoken Dialogue Systems,
6(3-4):287?303.
David Huggins-Daines, Mohit Kumar, Arthur Chan,
Allen W. Black, Mosur Ravishankar, and Alex I. Rud-
nicky. 2006. PocketSphinx: A free, real-time contin-
uous speech recognition system for hand-led devices.
In Proceedings of ICASSP, volume I, pages 185?188.
Ivana Kruijff-Korbayova?, Nate Blaylock, Ciprian Ger-
stenberger, Verena Rieser, Tilman Becker, Michael
Kaisser, Peter Poller, and Jan Schehl. 2005. An ex-
periment setup for collecting data for adaptive output
planning in a multimodal dialogue system. In 10th
ENLG, pages 191?196.
Cheongjae Lee, Alexander Rudnicky, and Gary Geunbae
Lee. 2010. Let?s buy books: finding ebooks using
voice search. In IEEE-SLT 2010, pages 442?447.
Vladimir I. Levenshtein. 1996. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710.
Tiziana Ligorio, Susan L. Epstein, and Rebecca J. Pas-
sonneau. 2010a. Wizards? dialogue strategies to han-
dle noisy speech recognition. In IEEE-SLT 2010.
Tiziana Ligorio, Susan L. Epstein, Rebecca J. Passon-
neau, and Joshua Gordon. 2010b. What you did
and didn?t mean: Noise, context and human skill. In
COGSCI 10.
Rebecca J. Passonneau, Susan L. Epstein, and Joshua
Gordon. 2009. Help me understand you: Address-
ing the speech recognition bottleneck. In Proceedings
of the AAAI Spring Symposium 2009 (SS09): Agents
that Learn from Human Teachers, pages 23?25.
Rebecca J. Passonneau, Susan L. Epstein, Tiziana Ligo-
rio, Joshua Gordon, and Pravin Bhutada. 2010. Learn-
ing about voice search for spoken dialogue systems. In
NAACL-HLT 2010, pages 840?848.
Martin J. Pickering and Simon Garrod. 2006. Alignment
as the basis for successful communication. Research
on Language and Communication, 4(2):203?228.
Matthew Purver, Jonathan Ginzburg, and Patrick Healey.
2001. On the means for clarification in dialogue.
In Proceedings of the 2nd SIGdial Workshop on Dis-
course and Dialogue, pages 116?125.
257
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
John W. Ratcliff and David Metzener. 1988. Pattern
matching: the gestalt approach.
Antoine Raux and Maxine Eskenazi. 2004. Non-native
users in the Let?s Go! spoken dialogue systems. In
HLT/NAACL, pages 217?224.
Antoine Raux and Maxine A. Eskenazi. 2007. A multi-
layer architecture for semi-synchronous event-driven
dialogue management. In ASRU 2007, pages 514?519.
Antoine Raux, Brian Langner, Allan W. Black, and Max-
ine Eskenazi. 2005. Let?s Go Public! taking a spoken
dialogue system to the real world. In Interspeech - Eu-
rospeech 2005, pages 885?888.
Verena Rieser and Oliver Lemon. 2006. Using ma-
chine learning to explore human multimodal clarifica-
tion strategies. In COLING/ACL, pages 659?666.
Verena Rieser and Oliver Lemon. 2011. Learning and
evaluation of dialogue strategies for new applications:
Empirical methods for optimization from small data
sets. Computational Linguistics, 37:153?96.
David Schlangen and Raquel Fern?andez. 2005. Speak-
ing through a noisy channel ? experiments on induc-
ing clarification behaviour in human-human diaogue.
In 8th Annual Converence of the International Speech
Communication Association (INTERSPEECH 2007),
pages 1266?1269.
Gabriel Skantze. 2003. Exploring human error handling
strategies: Implications for spoken dialogue systems.
In Proceedings of ISCA Tutorial and Research Work-
shop on Error Handling in Spoken Dialogue Systems,
pages 71?76.
Gabriel Skantze. 2005. Exploring human recovery
strategies: Implications for spoken dialogue systems.
Speech Communication, 45:325?41.
Svetlana Stoyanchev and Amanda Stent. 2009. Predict-
ing concept types in user corrections in dialog. In
EACL Workshop SRSL, pages 42?49.
Ye-Yi Wang, Yu Dong, Yun-Cheng Ju, and Alex Acero.
2008. An introduction to voice search. IEEE Signal
Processing Magazine: Special ISsue on Spoken Lan-
guage Technology, 25(3):28?38.
Wayne Ward and Sunil Issar. 1994. Recent improve-
ments in the CMU spoken language understanding
system. In Proceedings of the ARPA Human Language
Technology Workshop, pages 213?216.
Jason D. Williams and Steve Young. 2004. Characteriz-
ing task-oriented dialog using a simulated ASR chan-
nel. In ICSLP/Interspeech, pages 185?188.
Teresa Zollo. 1999. A study of human dialogue strate-
gies in the presence of speech recognition errors. In
Proceedings of the AAAI Fall Symposium on Psycho-
logical Models of Communication in Collaborative
Systems, pages 132?139.
258
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 266?271,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Learning to Balance Grounding Rationales for Dialogue Systems 
 
 
Joshua Gordon  Susan L. Epstein 
Department of Computer Science Department of Computer Science 
Rebecca J. Passonneau Hunter College and 
Center for Computational Learning Systems The Graduate Center of the City University 
of New York Columbia University 
New York, NY, USA New York, NY, USA 
(joshua|becky)@cs.columbia.edu susan.epstein@hunter.cuny.edu 
 
 
 
Abstract 
This paper reports on an experiment that 
investigates clarification subdialogues in 
intentionally noisy speech recognition. 
The architecture learns weights for mix-
tures of grounding strategies from exam-
ples provided by a human wizard 
embedded in the system. Results indicate 
that the architecture learns to eliminate 
misunderstandings reliably despite high 
word error rate. 
1 Introduction 
We seek to develop spoken dialogue systems 
(SDSs) that communicate effectively despite un-
certain input. Our thesis is that a task-oriented 
SDS can perform well despite a high degree of 
recognizer noise by relying on context. The SDS 
described here uses FORRSooth, a semi-
synchronous architecture under development for 
task-oriented human-computer dialogue. Our 
immediate goals are to reduce non-
understandings of user utterances (where the 
SDS produces no interpretation) and to eliminate 
misunderstandings (where the SDS misinterprets 
user utterances). The experiment recounted here 
investigates subdialogues consisting of an initial 
user response to a system prompt, and any sub-
sequent turns that might be needed to result in 
full understanding of the original response. Our 
principal finding is that a FORRSooth-based 
SDS learns to build on partial understandings 
and to eliminate misunderstandings despite noi-
sy ASR. 
A FORRSooth-based SDS is intended to inte-
ract effectively ?without the luxury of perfect 
components? (Paek and Horvitz, 2000), such as 
high-performance ASR. FORRSooth relies on 
portfolios of strategies for utterance interpreta-
tion and grounding, and learns to balance them 
from its experience. Its confidence in its inter-
pretations is dynamically calibrated against its 
past experience. At each user utterance, FORR-
Sooth selects grounding actions modulated to 
build upon partial interpretations in subsequent 
exchanges with the user. 
The experiment presented here bootstraps the 
SDS with human expertise. In a Wizard of Oz 
(WOz) study, a person (the wizard) replaces se-
lected SDS components. Knowledge is then ex-
tracted from the wizard?s behavior to improve 
the SDS. FORRSooth uses the Relative Support 
Weight Learning (RSWL) algorithm (Epstein and 
Petrovic, 2006) to learn weights that balance its 
individual strategies. Training examples for 
grounding strategies are based upon examples 
produced by an ablated wizard who was re-
stricted to the same information and actions as 
the system (Levin and Passonneau, 2006). 
Our domain is the Andrew Heiskell Braille 
and Talking Book Library. Heiskell?s patrons or-
der their books by telephone, during conversa-
tion with a librarian. The next section of this 
paper presents related work. Subsequent sections 
describe the weight learning, the SDS architec-
ture, and an experiment that challenges the ro-
bustness of utterance interpretation and 
grounding with intentionally noisy ASR. We 
266
conclude with a discussion of the results.  
2 Related Work  
Despite increasingly accurate ASR methods, di-
alogue systems often contend with noisy ASR, 
which can arise from performance phenomena 
such as filled pauses (er, um), false starts (fir- 
last name), or noisy transmission conditions. 
SDSs typically experience a higher WER when 
deployed. For example, the WER reported for 
Carnegie Mellon University?s Let?s Go Public! 
went from 17% under controlled conditions to 
68% in the field (Raux et al, 2005).  
To limit communication errors, an SDS can 
rely on strategies to detect and recover from in-
correct recognition output (Bohus, 2007). One 
such strategy, to ask the user to repeat a poorly 
understood utterance, can result in hyperarticula-
tion and decreased recognition (Litman, 
Hirschberg and Swerts, 2006). Prior work has 
shown that users prefer explicit confirmation 
over dialogue efficiency (fewer turns) (Litman 
and Pan, 1999). We hypothesize that this results 
from an inherent tradeoff between efficiency and 
user confidence. We assume that evidence of 
partial understanding increases user confidence 
more than evidence of non-understanding does. 
FORRSooth learns to ask more questions that 
build on partial information, and to make fewer 
explicit confirmations and requests to the user to 
repeat herself. 
While many techniques exist in the literature 
for semantic interpretation in task-oriented, in-
formation-seeking dialogue systems, there is no 
single preferred approach. SDSs rarely combine 
a portfolio of NLU (natural language under-
standing) resources. FORRSooth relies on ?mul-
tiple processes for interpreting utterances (e.g., 
structured parsing versus statistical techniques)? 
as in (Lemon, 2003). These range from voice 
search (querying a database directly with ASR 
results) to semantic parsing.  
Dialogue systems should ground their under-
standing of the user?s objectives. To limit com-
munication errors, an SDS can rely on strategies 
to detect and recover from incorrect recognition 
output (Bohus, 2007). In others? work, the 
grounding status of an utterance is typically bi-
nary (i.e., understood or not) (Allen, Ferguson 
and Stent, 2001; Bohus and Rudnicky, 
2005; Paek and Horvitz, 2000) or ternary (i.e., 
understood, misunderstood, not understood) 
(Bohus and Rudnicky, 2009). FORRSooth?s 
grounding decisions rely on a mixture of strate-
gies, are based on degrees of evidence (Bohus 
and Rudnicky, 2009; Roque and Traum, 2009), 
and disambiguate among candidate interpreta-
tions. Work in (DeVault and Stone, 2009) on 
disambiguation in task-oriented dialogue differs 
from ours in that it addresses genuine ambigui-
ties rather than noise resulting from inaccurate 
ASR.  
3 FORR and RSWL 
FORRSooth is based on FORR (FOr the Right 
Reasons), an architecture for learning and prob-
lem solving (Epstein, 1994). FORR uses se-
quences of decisions from multiple rationales to 
solve problems. Implementations have proved 
robust in game learning, simulated pathfinding, 
and constraint solving. FORR relies on an adap-
tive, hierarchical mixture of resource-bounded 
procedures called Advisors. Each Advisor em-
bodies a decision rationale. Advisors? opinions 
(comments) are combined to arrive at a decision. 
Each comment pairs an action with a strength 
that indicates some degree of support for or op-
position to that action. An Advisor can make 
multiple comments at once, and can base its 
comments upon descriptives. A descriptive is a 
shared data structure, computed on demand, and 
refreshed only when required. For each decision, 
FORR consults three tiers of Advisors, one tier 
at a time, until some tier reaches a decision.  
FORR learns weights for its tier-3 Advisors 
with RSWL. Relative support is a measure of the 
normalized difference between the comment 
strength (confidence) with which an Advisor 
supports an action compared to other available 
choices. RSWL learns Advisors? weights from 
their comments on training examples. The de-
gree of reinforcement (positive or negative) to 
an Advisor's weight is proportional to its 
strength and relative support for a decision. 
4 FORRSooth 
FORRSooth is a parallelized version of FORR. 
It models task-oriented dialogue with six FORR-
based services that operate concurrently: INTE-
267
RACTION, INTERPRETATION, SATISFACTION, 
GROUNDING, GENERATION, and DISCOURSE. 
These services interpret user utterances with re-
spect to system expectations, manage the con-
versational floor, and consider competing 
interpretations, partial understandings, and alter-
native courses of action. All services have 
access to the same data, represented by descrip-
tives. In this section, we present background on 
SATISFACTION and INTERPRETATION, and pro-
vide additional detail on GROUNDING.  
The role of SATISFACTION is to represent di-
alogue goals, and to progress towards those 
goals through spoken interaction. Dialogue goals 
are represented as agreements. An agreement is 
a subdialogue about a target concept (such as a 
specific book) whose value must be grounded 
through collaborative dialogue between the sys-
tem and the user (Clark and Schaefer, 1989). 
Agreements are organized into an agreement 
graph that represents dependencies among them. 
Task-based agreements are domain specific, 
while grounding agreements are domain inde-
pendent (cf. (Bohus, 2007)). An interpretation 
hypothesis represents the system?s belief that the 
value of a specific target (e.g., a full name or a 
first name) occurred in the user?s speech.  
The role of INTERPRETATION is to formulate 
hypotheses representing the meaning of what the 
user has said. INTERPRETATION relies on tier-3 
Advisors (essentially, mixtures of heuristics). 
Each Advisor constructs comments on speech 
recognition hypotheses. A comment is a seman-
tic concept (hypothesis) with an associated 
strength. More than one Advisor can vote for the 
same hypothesis. Confidence in any one hypo-
thesis is a function of votes, learned weights for 
Advisors, and comment strengths.  
In previous work, we showed that INTERPRE-
TATION Advisors can produce relatively reliable 
hypotheses given noisy ASR, with graceful de-
gradation  as recognition performance decreases 
(Gordon, Passonneau and Epstein, 2011). For 
example, at WER between 0.2 and 0.4, the con-
cept accuracy of the top hypothesis was 80%. 
That work left open how to decide whether to 
use the top INTERPRETATION hypothesis. Here 
FORRSooth learns how to assess its INTERPRE-
TATION confidence, and what grounding actions 
to take given different levels of confidence. 
Over the life of a FORRSooth SDS, INTER-
PRETATION produces hypotheses for the values 
of target concepts. FORRSooth records the mean 
and variance of the comment strengths for each 
INTERPRETATION hypothesis, and uses them to 
calculate INTERPRETATION?s merit. Merit 
represents FORRSooth?s INTERPRETATION con-
fidence as a dynamic, normalized estimate of the 
percentile in which the value falls. Merit compu-
tations improve initially with use of the SDS, 
and can then shift with the user population and 
the data. FORRSooth?s approach differs from 
supervised confidence annotation methods that 
learn a fixed confidence threshold from a corpus 
of human-machine dialogues (Bohus, 2007). 
The role of GROUNDING is to monitor the sys-
tem?s confidence in its interpretation of each us-
er utterance, to provide evidence to the user of 
its interpretation, and to elicit corroboration, fur-
ther information, or tacit agreement. To ground a 
target concept, FORRSooth considers one or 
more hypotheses for the value the user intended, 
and chooses a grounding action commensurate 
with its understanding and confidence.  
GROUNDING updates the agreement graph by 
adding grounding agreements to elicit confirma-
tions or rejections of target concepts, or to dis-
ambiguate among target concepts. A grounding 
agreement?s indicator target represents the ex-
pectation of a user response. Once a sufficiently 
confident INTERPRETATION hypothesis is bound 
to an indicator target, the grounding agreement 
executes side effects that strengthen or weaken 
the hypothesis being grounded. Recursive 
grounding (where the system grounds the user?s 
response to the system?s previous grounding ac-
tion) can result if the system?s expectation has 
not been met by the next system turn.  
GROUNDING makes two kinds of decisions, 
each with its own set of tier-3 Advisors. The 
first, commit bindings, indicates that the system 
is confident in the value of a target concept. In 
this experiment, decisions to commit to a value 
are irrevocable. The other kind of decision se-
lects the next grounding utterance for any target 
concepts that have not yet been bound. The deci-
sion to ground a target concept is made by tier-3 
Advisors that consider the distribution of hypo-
thesis merit, as well as the success or failure of 
the grounding actions taken thus far. 
268
5 FX2 
FX2 is a FORRSooth SDS constructed for the 
current experiment. The ten FX2 INTERPRETA-
TION Advisors are described in (Gordon, 
Passonneau and Epstein, 2011). Here we de-
scribe its GROUNDING actions and Advisors.  
FX2 can choose among six grounding actions. 
Given high confidence in a single interpretation, 
it commits to the binding of a target value with-
out confirmation. At slightly lower confidence 
levels, it chooses to implicitly confirm a target 
binding, with or without a hedge (e.g., the tag 
question ?right??). At even lower confidence, 
the grounding action is to explicitly confirm. 
Given competing interpretations with similarly 
high confidence, the grounding action is to dis-
ambiguate between the candidates. Finally, FX2 
can request the user to repeat herself. 
We give two examples of the twenty-three 
FX2 grounding Advisors. Given two interpreta-
tion hypotheses with similar confidence scores, a 
disambiguation Advisor votes to prompt the user 
to disambiguate between them. The strength for 
this grounding action is proportional to the ratio 
of the two hypotheses? scores. To avoid repeated 
execution of the same grounding action, one 
grounding Advisor votes against actions to re-
peat a prompt for the same target, especially if 
ASR confidence is low. In FX2, RSWL facili-
tates the use of multiple Advisors for INTERPRE-
TATION and GROUNDING by learning weights for 
them that reflect their relative reliability. We de-
scribe next how we collect training examples 
through an ablated wizard experiment. 
6 Experimental Design 
This experiment tests FX2?s ability to learn IN-
TERPRETATION and GROUNDING weights. In 
each dialogue, FX2 introduces itself, prompts 
the subject for her name or a book title, and then 
continues the dialogue until FX2 commits to a 
binding for the concept, or gives up. 
Four undergraduate native English speakers 
(two female, two male) participated. Speech in-
put and output was through a microphone head-
set. The PocketSphinx speech recognizer 
produced ASR output (Huggins-Daines et al, 
2006) with Wall-Street Journal dictation acous-
tic models adapted with ten hours of spontane-
ous speech. We built distinct trigram statistical 
language models for each type of agreement us-
ing names and titles from the Heiskell database. 
We collected three data sets, referenced here 
as baseline, wizard, and learning. Each had two 
agreement graphs: UserName seeks a grounded 
value for the patron's full name, and BookTitle 
seeks a grounded value for a book title. 120 di-
alogues were collected for each dataset.  
FX2 includes an optional wizard component. 
When active, the wizard component displays a 
GUI showing the current interpretation hypo-
theses for target concepts, along with their re-
spective merit. A screen shot for the wizard GUI 
appears in Figure 1. 
A wizard dialogue activates the wizard com-
ponent and uses INTERPRETATION as usual, but 
embeds a person (the wizard) in GROUNDING. 
The wizard?s purpose in this experiment is to 
provide training data for GROUNDING. After 
each user turn, the wizard makes two decisions 
based on data from the GUI: whether to consider 
any target as grounded, and which in a set of 
possible grounding actions to use next. The GUI 
displays what FX2 would choose for each deci-
sion; the wizard can either accept or override it. 
Ordinarily, a FORR-based system begins with 
uniform Advisor weights and learns more ap-
propriate values during its experience. Because 
correct interpretation and grounding are difficult 
tasks, however, we chose here to prime these 
weights and hypothesis merits using training ex-
amples collected during development. Develop-
ment data for INTERPRETATION included 200 
patron names, 400 book titles, and 50 indicator 
Figure 1. The wizard GUI displays hypotheses for a title from a user utterance. 
269
concepts. ASR output for each item, along with 
its correct value, became a training example. 
Development data for GROUNDING came from 
20 preliminary wizard dialogues. The develop-
ment data also served to prime hypothesis merit. 
Each subject had 30 dialogues with the sys-
tem for the baseline dataset. For the wizard data 
set, FX2 used the same primed weights and me-
rits as the baseline. The wizard?s grounding ac-
tions and the target graphs on which they were 
based were saved as training examples. Weights 
for GROUNDING Advisors were learned from the 
development data training examples and the 
training examples saved from the wizard data set 
together before collecting the learned data set.  
7 Results and Discussion 
We assess system performance as follows. A 
true positive (tp) here is a dialogue that made no 
grounding errors and successfully grounded the 
root task agreement; a false positive (fp) made at 
least one grounding error (where the system en-
tirely misunderstood the user). A false negative 
(fn) occurs when the system gives up on the 
task. Precision is tp/(tp+fp), recall is tp/(tp+fn), 
and F is their mean. We measure WER using 
Levenshtein edit distance (Levenshtein, 1966). 
Because the audio data is not yet transcribed, we 
estimated average WER from the speaker's first 
known utterance (n=360). Overall estimated 
WER was 66% (54% male, 78% female).  
An ideal system engages in dialogues that 
have high precision, high recall, and economical 
dialogue length (as measured by number of sys-
tem turns). Table 1 reports that data. There is a 
significant increase in precision across the three 
data sets, a small corresponding decrease in re-
call, and an overall gain in F measure. The pre-
cision demonstrated by the system during 
dialogues in the learned data set is as good or 
better than that reported for our best embedded 
human wizards in full dialogue experiments 
(Ligorio, Epstein and Passonneau, 2010).  
Table 2 shows the distribution of the system's 
grounding actions for the three data sets. The 
grounding actions in the learned data set are 
similar to the wizard?s; clearly, RSWL learned 
well. Figure 2 illustrates an effective learned 
grounding behavior in which GROUNDING votes 
to abandon the current grounding strategy on 
turn 5 in favor of an approach designed to elicit 
an utterance from the user more likely to be 
processed correctly by the speech recognizer.  
Our experiment suggests that misunderstand-
ings due to ASR errors can be significantly re-
duced by a close integration between spoken 
language understanding resources and grounding 
behavior, together with the use of fine-grained 
confidence measures. Despite intentionally noisy 
ASR with an estimated average WER of 66%, 
FX2 did not experience a single misunderstand-
ing during 120 clarification dialogues after it had 
trained with RSWL on examples provided by an 
embedded wizard. After training, the system's 
actions closely resembled the wizard's, indicat-
ing that the system?s grounding strategies were 
sufficiently expressive to approximate the wi-
zard's actions. FX2 accommodates a variety of 
independent spoken language understanding re-
sources. It relies on RSWL and on merit, a self-
normalizing estimate of the confidence percen-
tile in which an interpretation hypothesis falls.  
Turn Utterance ASR 
1 What title would you like?  
2 Family and Friends family .friends. 
3 
I have two guesses. The first is 
Family and Friends. The 
second is Family Happiness. 
Is it either of these? 
 
4 The first one .nest. .first. 
5 Let?s try something else. Is the full title Family and Friends?  
6 Yes yes 
Condition Precision Recall F Length 
Baseline 0.65 0.78 0.72 4.36 
Wizard 0.89 0.76 0.83 4.05 
Learned 1.00 0.71 0.86 3.86 
Condition Conf Disambig Repeat Other 
Baseline 0.23 0.19 0.50 0.08 
Wizard 0.09 0.50 0.35 0.06 
Learned 0.15 0.52 0.32 0.01 
Table 1. Performance across three data sets.  Table 2. Distribution of grounding actions. 
 
Figure 2. Example of learned GROUNDING behavior. 
The rightmost column is the top ASR hypothesis. 
Periods delimit unconfident words in the ASR. 
270
References 
James Allen, George Ferguson and Amanda Stent. 
2001. An architecture for more realistic 
conversational systems. Proc. 6th Int'l Conference 
on Intelligent User Interfaces. ACM: 1-8. 
Dan Bohus. 2007. Error awareness and recovery in 
conversational spoken language interfaces. Ph.D. 
thesis, Carnegie Mellon University, Pittsburgh,PA. 
Dan Bohus and Alexander I. Rudnicky. 2005. Error 
handling in the RavenClaw dialog management 
framework. Proc. Human Language Technology 
and Empirical Methods in Natural Language 
Processing, ACL: 225-232. 
Dan Bohus and Alexander I. Rudnicky. 2009. The 
RavenClaw dialog management framework: 
Architecture and systems. Comput. Speech Lang. 
23(3): 332-361. 
Herbert H. Clark and Edward F. Schaefer. 1989. 
Contributing to discourse. Cognitive Science 
13(2): 259 - 294. 
David Devault and Matthew Stone. 2009. Learning to 
interpret utterances using dialogue history. Proc. 
12th Conference of the European Chapter of the 
Association for Computational Linguistics. ACL: 
184-192. 
Susan L. Epstein. 1994. For the Right Reasons: The 
FORR Architecture for Learning in a Skill 
Domain. Cognitive Science 18(3): 479-511. 
Susan L. Epstein and Smiljana Petrovic. 2006. 
Relative Support Weight Learning for Constraint 
Solving. AAAI Workshop on Learning for Search: 
115-122. 
Joshua B. Gordon, Rebecca J. Passonneau and Susan 
L. Epstein. 2011. Helping Agents Help Their 
Users Despite Imperfect Speech Recognition. 
AAAI Symposium Help Me Help You: Bridging the 
Gaps in Human-Agent Collaboration. 
David Huggins-Daines, Mohit Kumar, Arthur Chan, 
Alan W. Black, Mosur Ravishankar and Alex I. 
Rudnicky. 2006. Pocketsphinx: A Free, Real-Time 
Continuous Speech Recognition System for Hand-
Held Devices. In Proc. IEEE ICASSP, 2006. 185-
188. 
Oliver Lemon. 2003. Managing dialogue interaction: 
A multi-layered approach. In Proc. 4th SIGDial 
Workshop on Discourse and Dialogue. 
Vladimir Levenshtein. 1966. Binary codes capable of 
correcting deletions, insertions, and reversals. So-
viet Physics Doklady. 10: 707-710. 
Esther Levin and Rebecca Passonneau. 2006. A WOz 
Variant with Contrastive Conditions. In Proc. of 
Interspeech 2006 Satelite Workshop: Dialogue on 
Dialogues. 
Tiziana Ligorio, Susan L. Epstein and Rebecca J. 
Passonneau. 2010. Wizards' dialogue strategies to 
handle noisy speech recognition. IEEE workshop 
on Spoken Language Technology (IEEE-SLT 
2010). Berkeley, CA. 
Diane Litman, Julia Hirschberg and Marc Swerts. 
2006. Characterizing and predicting corrections in 
spoken dialogue systems. Comput. Linguist. 32(3): 
417-438. 
Diane J. Litman and Shimei Pan. 1999. Empirically 
evaluating an adaptable spoken dialogue system. 
Proc. 7th Int'l Conference on User Modeling. 
Springer-Verlag New York, Inc.: 55-64. 
Tim Paek and Eric Horvitz. 2000. Conversation as 
action under uncertainty. Proc. 16th Conference 
on Uncertainty in Artificial Intelligence, Morgan 
Kaufmann Publishers Inc.: 455-464. 
Rebecca J. Passonneau, Susan L. Epstein, Tiziana 
Ligorio, Joshua B. Gordon and Pravin Bhutada. 
2010. Learning about voice search for spoken 
dialogue systems. Human Language 
Technologies: NAACL 2010. ACL: 840-848. 
Antoine Raux, Brian Langner, Allan W. Black and 
Maxine Eskenazi. 2005. Let's Go Public! Taking a 
spoken dialog system to the real world. 
Interspeech 2005 (Eurospeech). Lisbon, Portugal. 
Antonio Roque and David Traum. 2009. Improving a 
virtual human using a model of degrees of 
grounding. Proc. IJCAI-2009. Morgan Kaufmann 
Publishers Inc.: 1537-1542.  
271
