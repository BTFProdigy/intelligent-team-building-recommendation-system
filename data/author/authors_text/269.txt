Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 9?12,
New York, June 2006. c?2006 Association for Computational Linguistics
Museli: A Multi-Source Evidence Integration Approach to Topic Seg-
mentation of Spontaneous Dialogue 
 
  
Jaime Arguello Carolyn Ros? 
Language Technologies Institute Language Technologies Institute 
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15213 Pittsburgh, PA 15213 
jarguell@andrew.cmu.edu cprose@cs.cmu.edu 
Abstract 
We introduce a novel topic segmentation 
approach that combines evidence of topic 
shifts from lexical cohesion with linguistic 
evidence such as syntactically distinct fea-
tures of segment initial contributions.  Our 
evaluation demonstrates that this hybrid 
approach outperforms state-of-the-art algo-
rithms even when applied to loosely struc-
tured, spontaneous dialogue. 
1 Introduction    
Use of topic-based models of dialogue has 
played a role in information retrieval (Oard et al, 
2004), information extraction (Baufaden, 2001), 
and summarization (Zechner, 2001). However, 
previous work on automatic topic segmentation has 
focused primarily on segmentation of expository 
text.  We present Museli, a novel topic segmenta-
tion approach for dialogue that integrates evidence 
of topic shifts from lexical cohesion with linguistic 
indicators such as syntactically distinct features of 
segment initial contributions. 
Our evaluation demonstrates that approaches de-
signed for text do not generalize well to dialogue.  
We demonstrate a significant advantage of Museli 
over competing approaches.  We then discuss why 
models based entirely on lexical cohesion fail on 
dialogue and how our algorithm compensates with 
other topic shift indicators.  
2 Previous Work 
Existing topic segmentation approaches can be 
loosely classified into two types: (1) lexical cohe-
sion models, and (2) content-oriented models.  The 
underlying assumption in lexical cohesion models 
is that a shift in term distribution signals a shift in 
topic (Halliday and Hassan, 1976). The best known 
algorithm based on this idea is TextTiling (Hearst, 
1997). In TextTiling, a sliding window is passed 
over the vector-space representation of the text. At 
each position, the cosine correlation between the 
upper and lower region of the sliding window is 
compared with that of the peak cosine correlation 
values to the left and right of the window.  A seg-
ment boundary is predicted when the magnitude of 
the difference exceeds a threshold.    
One drawback to relying on term co-occurrence 
to signal topic continuity is that synonyms or re-
lated terms are treated as thematically-unrelated. 
One solution to this problem is using a dimension-
ality reduction technique such as Latent Semantic 
Analysis (LSA) (Landauer and Dumais, 1997). 
Two such algorithms for segmentation are de-
scribed in (Foltz, 1998) and (Olney and Cai, 2005).  
Both TextTiling and Foltz?s approach measure 
coherence as a function of the repetition of the-
matically-related terms. TextTiling looks for co-
occurrences of terms or term-stems and Foltz uses 
LSA to measure semantic relatedness between 
terms.  Olney and Cai?s orthonormal basis ap-
proach also uses LSA, but allows a richer represen-
tation of discourse coherence, which is that coher-
ence is a function of how much new information a 
discourse unit (e.g. a dialogue contribution) adds  
(informativity) and how relevant it is to the local 
context (relevance) (Olney and Cai, 2005). 
Content-oriented models, such as (Barzilay and 
Lee, 2004), rely on the re-occurrence of patterns of 
topics over multiple realizations of thematically 
similar discourses, such as a series of newspaper 
articles about similar events. Their approach util-
izes a hidden Markov model where states corre-
spond to topics, and state transition probabilities 
correspond to topic shifts. To obtain the desired 
9
number of topics (states), text spans of uniform 
length (individual contributions, in our case) are 
clustered. Then, state emission probabilities are 
induced using smoothed cluster-specific language 
models. Transition probabilities are induced by 
considering the proportion of documents in which 
a contribution assigned to the source cluster (state) 
immediately precedes a contribution assigned to 
the target cluster (state). Using an EM-like Viterbi 
approach, each contribution is reassigned to the 
state most likely to have generated it.  
3 Overview of Museli Approach 
We will demonstrate that lexical cohesion alone 
does not adequately mark topic boundaries in dia-
logue.  Nevertheless, it can provide one meaning-
ful source of evidence towards segmenting dia-
logue. In our hybrid Museli approach, we com-
bined lexical cohesion with features that have the 
potential to capture something about the linguistic 
style that marks shifts in topic: word-unigrams, 
word-bigrams, and POS-bigrams for the current 
and previous contributions; the inclusion of at least 
one non-stopword term (contribution of content); 
time difference between contributions; contribution 
length; and the agent role of the previous and cur-
rent contribution.  
We cast the segmentation problem as a binary 
classification problem where each contribution is 
classified as NEW_TOPIC if the contribution in-
troduces a new topic and SAME_TOPIC other-
wise.  We found that using a Na?ve Bayes classifier 
(John & Langley, 1995) with an attribute selection 
wrapper using the chi-square test for ranking at-
tributes performed better than other state-of-the-art 
machine learning algorithms, perhaps because of 
the evidence integration oriented nature of the 
problem.  We conducted our evaluation using 10-
fold cross-validation, being careful not to include 
instances from the same dialogue in both the train-
ing and test sets on any fold so that the results we 
report would not be biased by idiosyncratic com-
municative patterns associated with individual 
conversational participants picked up by the 
trained model.  
Using the complete set of features enumerated 
above, we perform feature selection on the training 
data for each fold of the cross-validation sepa-
rately, training a model with the top 1000 features, 
and applying that trained model to the test data.  
Examples of high ranking features confirm our 
intuition that contributions that begin new topic 
segments are syntactically marked.  For example, 
many typical selected word bigrams were indica-
tive of imperatives, such as lets-do, do-the, ok-lets, 
ok-try, lets-see, etc.  Others included time oriented 
discourse markers such as now, then, next, etc. 
To capitalize on differences in conversational 
behavior between participants assigned to different 
roles in the conversation (i.e., student and tutor in 
our evaluation corpora), we learn separate models 
for each role in the conversation1. This decision is 
based on the observation that participants with dif-
ferent agent-roles introduce topics with a different 
frequency, introduce different types of topics, and 
may introduce topics in a different style that dis-
plays their status in the conversation. For instance, 
a tutor may introduce new topics with a contribu-
tion that ends with an imperative. A student may 
introduce new topics with a contribution that ends 
with a wh-question.   
4 Evaluation 
In this section we evaluate Museli in comparison 
to the best performing state-of-the-art approaches, 
demonstrating that our hybrid Museli approach 
out-performs all of these approaches on two differ-
ent dialogue corpora by a statistically significant 
margin (p < .01), in one case reducing the prob-
ability of error as measured by Beeferman's Pk to 
only 10% (Beeferman et al, 1999). 
4.1 Experimental Corpora 
We used two different dialogue corpora for our 
evaluation.  The first corpus, which we refer to as the 
Olney & Cai corpus, is a set of dialogues selected ran-
domly from the same corpus Olney and Cai selected 
their corpus from (Olney and Cai, 2005). The second 
corpus is a locally collected corpus of thermodynamics 
tutoring dialogues, which we refer to as the Thermo 
corpus. This corpus is particularly appropriate for ad-
dressing the research question of how to automatically 
segment dialogue for two reasons: First, the explora-
tory task that students and tutors engaged in together is 
more loosely structured than many task oriented do-
mains typically investigated in the dialogue commu-
nity, such as flight reservation or meeting scheduling.  
Second, because the tutor and student play asymmetric 
roles in the interaction, this corpus allows us to explore 
                                                 
1 Dissimilar agent-roles occur in other domains as well (e.g. 
Travel Agent and Customer)
10
how conversational role affects how speakers mark 
topic shifts.   
Table 1 presents statistics describing characteris-
tics of these two corpora.  Similar to (Passonneau 
and Litman, 1993), we adopt a flat model of topic-
segmentation for our gold standard based on dis-
course segment purpose, where a shift in topic cor-
responds to a shift in purpose that is acknowledged 
and acted upon by both conversational agents. We 
evaluated inter-coder reliability over 10% of the 
Thermo corpus mentioned above.  3 annotators 
were given a 10 page coding manual with explana-
tion of our informal definition of shared discourse 
segment purpose as well as examples of segmented 
dialogues.  Pairwise inter-coder agreement was 
above 0.7 kappa for all pairs of annotators. 
 
 Olney & Cai  
Corpus 
Thermo 
Corpus 
# Dialogues 42 22 
Contributions/ 
Dialogue 
195.40 217.90 
Contributions/ 
Topic 
24.00 13.31 
Topics/Dialogue 8.14 16.36 
Words/ 
Contribution 
28.63 5.12 
Table 1: Evaluation Corpora Statistics 
4.2 Baseline Approaches 
We evaluate Museli against the following algo-
rithms: (1) Olney and Cai (Ortho), (2) Barzilay and 
Lee (B&L), (3) TextTiling (TT), and (4) Foltz.  
As opposed to the other baseline algorithms, 
(Olney and Cai, 2005) applied their orthonormal 
basis approach specifically to dialogue, and prior 
to this work, report the highest numbers for topic 
segmentation of dialogue. Barzilay and Lee?s ap-
proach is the state of the art in modeling topic 
shifts in monologue text. Our application of B&L 
to dialogue attempts to harness any existing and 
recognizable redundancy in topic-flow across our 
dialogues for the purpose of topic segmentation.  
We chose TextTiling for its seminal contribution 
to monologue segmentation. TextTiling and Foltz 
consider lexical cohesion as their only evidence of 
topic shifts. Applying these approaches to dialogue 
segmentation sheds light on how term distribution 
in dialogue differs from that of expository mono-
logue text (e.g. news articles).  
The Foltz and Ortho approaches require a 
trained LSA space, which we prepared as de-
scribed in (Olney and Cai, 2005). Any parameter 
tuning for approaches other than our hybrid ap-
proach was computed over the entire test set, giv-
ing competing algorithms the maximum advantage. 
In addition to these approaches, we include 
segmentation results from three degenerate ap-
proaches: (1) classifying all contributions as 
NEW_TOPIC (ALL), (2) classifying no contribu-
tions as NEW_TOPIC (NONE), and (3) classifying 
contributions as NEW_TOPIC at uniform intervals 
(EVEN), corresponding to the average reference 
topic length (see Table 1). 
As a means for comparison, we adopt two evalua-
tion metrics: Pk and f-measure. An extensive argu-
ment of Pk?s robustness (if k is set to ? the average 
reference topic length) is present in (Beeferman, et al 
1999).  Pk measures the probability of misclassifying 
two contributions a distance of k contributions apart, 
where the classification question is are the two con-
tributions part of the same topic segment or not?  
Lower Pk values are preferred over higher ones. It 
equally captures the effect of false-negatives and 
false-positives and it favors near misses. F-measure 
punishes false positives equally, regardless of the 
distance to the reference boundary.  
4.3 Results 
Results for all approaches are displayed in Table 
2.  Note that lower values of Pk are preferred over 
higher ones. The opposite is true of F-measure.  In 
both corpora, Museli performed significantly better 
than all other approaches (p <  .01).   
 
 Olney & Cai Corpus Thermo Corpus 
 Pk F Pk F 
NONE 0.4897 -- 0.4900 -- 
ALL 0.5180 -- 0.5100 -- 
EVEN 0.5117 -- 0.5132 -- 
TT 0.6240 0.1475 0.5353 0.1614 
B&L 0.6351 0.1747 0.5086 0.1512 
Foltz 0.3270 0.3492 0.5058 0.1180 
Ortho 0.2754 0.6012 0.4898 0.2111 
Museli 0.1051 0.8013 0.4043 0.3693 
Table 2: Results on both corpora 
4.4 Error Analysis 
Results for all approaches are better on the Ol-
ney and Cai corpus than the Thermo corpus. The 
Thermo corpus differs profoundly from the Olney 
and Cai corpus in ways that very likely influenced 
the performance. For instance, in the Thermo cor-
pus each dialogue contribution is an average of 5 
words long, whereas in the Olney and Cai corpus 
11
each dialogue contribution contains an average of 
28 words. Thus, the vector space representation of 
the dialogue contributions is much more sparse in 
the Thermo corpus, which makes shifts in lexical 
coherence less reliable as topic shift indicators.   
In terms of Pk, TextTiling (TT) performed worse 
than the degenerate algorithms. TextTiling meas-
ures the term-overlap between adjacent regions in 
the discourse. However, dialogue contributions are 
often terse or even contentless. This produces 
many islands of contribution-sequences for which 
the local lexical cohesion is zero. TextTiling 
wrongfully classifies all of these as starts of new 
topics.  A heuristic improvement to prevent 
TextTiling from placing topic boundaries at every 
point along a sequence of contributions failed to 
produce a statistically significant improvement. 
The Foltz and the orthonormal basis approaches 
rely on LSA to provide strategic semantic gener-
alizations. Following (Olney and Cai, 2005), we 
built our LSA space using dialogue contributions 
as the atomic text unit.  However, in corpora such 
as the Thermo corpus, this may not be effective 
because of the brevity of contributions. 
Barzilay and Lee?s algorithm (B&L) did not 
generalize well to either dialogue corpus. One rea-
son could be that such probabilistic methods re-
quire that reference topics have significantly dif-
ferent language models, which was not true in ei-
ther of our evaluation corpora. We also noticed a 
number of instances in the dialogue corpora where 
participants referred to information from previous 
topic segments, which consequently may have 
blurred the distinction between the language mod-
els assigned to different topics. 
5 Current Directions 
In this paper we address the problem of auto-
matic topic segmentation of spontaneous dialogue.  
We demonstrated with an empirical evaluation that 
state-of-the-art approaches fail on spontaneous dia-
logue because word-distribution patterns alone are 
insufficient evidence of topic shifts in dialogue.  
We have presented a supervised learning algorithm 
for topic segmentation of dialogue that combines 
linguistic features signaling a contribution?s func-
tion with lexical cohesion. Our evaluation on two 
distinct dialogue corpora shows a significant im-
provement over the state of the art approaches.  
The disadvantage of our approach is that it re-
quires hand-labeled training data. We are currently 
exploring ways of bootstrapping a model from a 
small amount of hand labeled data in combination 
with lexical cohesion (tuned for high precision and 
consequently low recall) and some reliable dis-
course markers.  
Acknowledgments 
This work was funded by Office of Naval Re-
search, Cognitive and Neural Science Division, 
grant number N00014-05-1-0043. 
References  
Regina Barzilay and Lillian Lee (2004). Catching the 
drift: Probabilistic Content Models, with Applications 
to Generation and Summarization. In Proceedings of 
HLT-NAACL 2004.  
Doug Beeferman, Adam Berger, John D. Lafferty 
(1999).  Statistical Models for Text Segmentation. 
Machine Learning 34 (1-3): 177-210. 
Narj?s Boufaden, Guy Lapalme, Yoshua Bengio (2001). 
Topic Segmentation: A first stage to Dialog-based In-
formation Extraction. In Proceedings of NLPRS 2001. 
P.W. Foltz, W. Kintsch, and Thomas Landauer (1998). 
The measurement of textual cohesion with latent se-
mantic analysis. Discourse Processes, 25, 285-307. 
M. A. K. Halliday and Ruqaiya Hasan (1976). Cohesion 
in English. London: Longman. 
Marti Hearst. 1997. TextTiling: Segmenting Text into 
Multi-Paragragh Subtopic Passages. Computational 
Linguistics, 23(1), 33 ? 64.   
George John & Pat Langley (1995).  Estimating Con-
tinuous Distributions in Bayesian Classifiers.  In Pro-
ceedings of UAI 2005. 
Thomas Landauer, & Susan Dumais (1997). A Solution 
to Plato?s Problem: The Latent Semantic Analysis of 
Acquisition, Induction, and Representation of Knowl-
edge. Psychological Review, 104, 221-240.  
Douglas Oard, Bhuvana Ramabhadran, and Samuel 
Gustman (2004). Building an Information Retrieval 
Test Collection for Spontaneous Conversational 
Speech. In Proceedings of SIGIR 2004.  
Andrew Olney and Zhiqiang Cai (2005). An Orthonor-
mal Basis for Topic Segmentation of Tutorial Dia-
logue. In Proceedings of HLT-EMNLP 2005. 
Rebecca Passonneau and Diane Litman (1993). Inten-
tion-Based Segmentation: Human Reliability and 
Correlation with Linguistic Cues. In Proceedings 
ACL 2003.  
Klaus Zechner (2001). Automatic Generation of Con-
cise Summaries of Spoken Dialogues in Unrestricted 
Domains. In Proceedings of SIGIR 2001.  
12
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 253?256,
New York City, June 2006. c?2006 Association for Computational Linguistics
InfoMagnets: Making Sense of Corpus Data 
 
 Jaime Arguello Carolyn Ros? 
Language Technologies Institute Language Technologies Institute 
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15216 Pittsburgh, PA 15216 
jarguell@andrew.cmu.edu cprose@cs.cmu.edu 
 
  
Abstract 
We introduce a new interactive corpus 
exploration tool called InfoMagnets. In-
foMagnets aims at making exploratory 
corpus analysis accessible to researchers 
who are not experts in text mining. As 
evidence of its usefulness and usability, it 
has been used successfully in a research 
context to uncover relationships between 
language and behavioral patterns in two 
distinct domains: tutorial dialogue 
(Kumar et al, submitted) and on-line 
communities (Arguello et al, 2006). As 
an educational tool, it has been used as 
part of a unit on protocol analysis in an 
Educational Research Methods course.  
1 Introduction 
Exploring large text corpora can be a daunting 
prospect. This is especially the case for behavioral 
researchers who have a vested interest in the latent 
patterns present in text, but are less interested in 
computational models of text-representation (e.g. 
the vector-space model) or unsupervised pattern-
learning (e.g. clustering). Our goal is to provide 
this technology to the broader community of learn-
ing scientists and other behavioral researchers who 
collect and code corpus data as an important part 
of their research.  To date none of the tools that are 
commonly used in the behavioral research com-
munity, such as HyperResearch, MacShapa, or 
Nvivo, which are used to support their corpus 
analysis efforts, make use of technology more ad-
vanced than simplistic word counting approaches.  
With InfoMagnets, we are working towards bridg-
ing the gap between the text-mining community 
and the corpus-based behavioral research commu-
nity. The purpose of our demonstration is to make 
the language technologies community more aware 
of opportunities for applications of language tech-
nologies to support corpus oriented behavioral re-
search. 
 
 
Figure 1: InfoMagnets Screenshot 
 
InfoMagnet?s novelty is two-fold: First, it pro-
vides an intuitive visual metaphor that allows the 
user to get a sense of their data and organize it for 
easy retrieval later. This is important during the 
sense making stage of corpus analysis work just 
before formal coding scheme development begins.  
Secondly, it allows the user to interact with cluster-
ing technology, and thus influence its behavior, in 
effect introducing human knowledge into the clus-
tering process.  Because of this give and take be-
tween the clustering technology and the human 
253
influence, the tool is able to achieve an organiza-
tion of textual units that is not just optimal from an 
algorithmic stand-point, but also optimal for the 
user?s unique purpose, which non-interactive clus-
tering algorithms are not in general capable of 
achieving.  
Using visual metaphors to convey to the user 
proximity and relations between documents and 
automatically generated clusters is not a new tech-
nique (Chalmers and Chitson, 1992; Dubin, 1995; 
Wise et al, 1995; Leuski and Allan, 2000; Ras-
mussen and Karypis, 2004). InfoMagnet?s novelty 
comes from giving the user more control over the 
ultimate clustering organization. The user is able to 
incrementally influence the formation and reor-
ganization of cluster centroids and immediately see 
the effect on the text-to-cluster assignment. Thus, 
the user can explore the corpus in more effective 
and meaningful ways.  
In what follows, we more concretely elaborate 
on InfoMagnet?s functionality and technical de-
tails. We then motivate its usability and usefulness 
with a real case study.   
2 Functionality  
Exploring a textual corpus in search of interest-
ing topical patterns that correlate with externally 
observable variables is a non-trivial task.  Take as 
an example the task of characterizing the process 
by which students and tutors negotiate with one 
another over a chat interface as they navigate in-
structional materials together in an on-line explora-
tory learning environment. A sensible approach is 
to segment all dialogue transcripts into topic-
oriented segments and then group the segments by 
topic similarity. If done manually, this is a chal-
lenging task in two respects. First, to segment each 
dialogue the analyst must rely on their knowledge 
of the domain to locate where the focus of the dia-
logue shifts from one topic to the next. This, of 
course, requires the analyst to know what to look 
for and to remain consistent throughout the whole 
set of dialogues. More importantly, it introduces 
into the topic analysis a primacy bias. The analyst 
may miss important dialogue digressions simply 
because they are not expected based on observa-
tions from the first few dialogues viewed in detail.  
InfoMagnets addresses these issues by offering 
users a constant bird?s eye view of their data.  See 
Figure 1. 
As input, InfoMagnets accepts a corpus of tex-
tual documents. As an option to the user, the docu-
ments can be automatically fragmented into 
topically-coherent segments (referred to also as 
documents from here on), which then become the 
atomic textual unit1. The documents (or topic seg-
ments) are automatically clustered into an initial 
organization that the user then incrementally ad-
justs through the interface. Figure 1 shows the ini-
tial document-to-topic assignment that 
InfoMagnets produces as a starting point for the 
user. The large circles represent InfoMagnets, or 
topic oriented cluster centroids, and the smaller 
circles represent documents. An InfoMagnet can 
be thought of as a set of words representative of a 
topic concept. The similarity between the vector 
representation of the words in a document and that 
of the words in an InfoMagnet translate into attrac-
tion in the two-dimensional InfoMagnet space.  
This semantic similarity is computed using Latent 
Semantic Analysis (LSA) (Landauer et al,  1998). 
Thus, a document appears closest to the InfoMag-
net that best represents its topic.  
A document that appears equidistant to two In-
foMagnets shares its content equally between the 
two represented topics. Topics with lots of docu-
ments nearby are popular topics. InfoMagnets with 
only a few documents nearby represent infrequent 
topics. Should the user decide to remove an In-
foMagnet, any document with some level of attrac-
tion to that InfoMagnet will animate and reposition 
itself based on the topics still represented by the 
remaining InfoMagnets.  At all times, the In-
foMagnets interface offers the analyst a bird?s eye 
view of the entire corpus as it is being analyzed 
and organized. 
Given the automatically-generated initial topic 
representation, the user typically starts by brows-
ing the different InfoMagnets and documents. Us-
ing a magnifying cross-hair lens, the user can view 
the contents of a document on the top pane. As 
noted above, each InfoMagnet represents a topic 
concept through a collection of words (from the 
corpus) that convey that concept. Selecting the In-
foMagnet displays this list of words on the left 
pane. The list is shown in descending order of im-
portance with respect to that topic. By browsing 
each InfoMagnet?s list of words and browsing 
                                                          
1 Due to lack of space, we do not focus on our topic-
segmentation algorithm. We intend to discuss this in the demo.  
254
nearby documents, the user can start recognizing 
topics represented in the InfoMagnet space and can 
start labeling those InfoMagnets.  
InfoMagnets with only a few neighboring 
documents can be removed. Likewise, InfoMag-
nets attracting too many topically-unrelated docu-
ments can be split into multiple topics. The user 
can do this semi-automatically (by requesting a 
split, and allowing the algorithm to determine 
where the best split is) or by manually selecting a 
set of terms from the InfoMagnet?s word list and 
creating a new InfoMagnet using those words to 
represent the new InfoMagnet?s topic. If the user 
finds words in an InfoMagnet?s word list that lack 
topical relevance, the user can remove them from 
InfoMagnet?s word list or from all the InfoMag-
nets? word lists at once. 
Users may also choose to manually assign a seg-
ment to a topic by ?snapping? that document to an 
InfoMagnet. ?Snapping? is a way of overriding the 
attraction between the document and other In-
foMagnets.  By ?snapping? a document to an In-
foMagnet, the relationship between the ?snapped? 
document and the associated InfoMagnet remains 
constant, regardless of any changes made to the 
InfoMagnet space subsequently.  
If a user would like to remove the influence of a 
subset of the corpus from the behavior of the tool, 
the user may select an InfoMagnet and all the 
documents close to it and place them in the ?quar-
antine? area of the interface. When placed in the 
quarantine, as when ?snapped?, a document?s as-
signment remains unchanged. This feature is used 
to free screen space for the user.  
If the user opts for segmenting each input dis-
course and working with topic segments rather 
than whole documents, an alternative interface al-
lows the user to quickly browse through the corpus 
sequentially (Figure 2).  By switching between this 
view and the bird?s eye view, the user is able to see 
where each segment fits sequentially into the larger 
context of the discourse it was extracted from.  The 
user can also use the sequential interface for mak-
ing minor adjustments to topic segment boundaries 
and topic assignments where necessary.  Once the 
user is satisfied with the topic representation in the 
space and the assignments of all documents to 
those topics, the tool can automatically generate an 
XML file, where all documents are tagged with 
their corresponding topic labels. 
 
 
Figure 2. InfoMagnet?s alternative sequential view 
3 Implementation 
As mentioned previously, InfoMagnets uses La-
tent Semantic Analysis (LSA) to relate documents 
to InfoMagnets. LSA is a dimensionality reduction 
technique that can be used to compute the semantic 
similarity between text spans of arbitrary size. For 
a more technical overview of LSA, we direct the 
reader to (Landauer et al, 1998).  
The LSA space is constructed using the corpus 
that the user desires to organize, possibly aug-
mented with some general purpose text (such as 
newsgroup data) to introduce more domain-general 
term associations. The parameters used in building 
the space are set by the user during pre-processing, 
so that the space is consistent with the semantic 
granularity the user is interested in capturing.  
Because documents (or topic-segments) tend to 
cover more than one relevant topic, our clustering 
approach is based on what are determined heuristi-
cally to be the most important terms in the corpus, 
and not on whole documents. This higher granular-
ity allows us to more precisely capture the topics 
discussed in the corpus by not imposing the as-
sumption that documents are about a single topic. 
First, all terms that occur less than n times and in 
less than m documents are removed from consid-
eration2. Then, the remaining terms are clustered 
via average-link clustering, using their LSA-based 
vector representations and using cosine-correlation 
as a vector similarity measure. Our clustering algo-
rithm combines top-down clustering (Bisecting K-
Means) and bottom-up clustering (Agglomerative 
Clustering) (Steinbach et al, 2000). This hybrid 
                                                          
2 n and m are parameters set by the user. 
255
clustering approach leverages the speed of bisect-
ing K-means and the greedy search of agglomera-
tive clustering, thus achieving a nice effectiveness 
versus efficiency balance.   
Cluster centroids (InfoMagnets) and documents 
(or topic segments) are all treated as bag-of-words. 
Their vector-space representation is the sum of the 
LSA vectors of their constituent terms. When the 
user changes the topic-representation by removing 
or adding a term to an InfoMagnet, a new LSA 
vector is obtained by projecting the new bag-of-
words onto the LSA space and re-computing the 
cosine correlation between all documents and the 
new topic.     
4 An Example of Use 
InfoMagnets was designed for easy usability by 
both computational linguistics and non-technical 
users.  It has been successfully used by social psy-
chologists working on on-line communities re-
search as well as learning science researchers 
studying tutorial dialogue interactions (which we 
discuss in some detail here).   
Using InfoMagnets, a thermodynamics domain 
expert constructed a topic analysis of a corpus of 
human tutoring dialogues collected during class-
room study focusing on thermodynamics instruc-
tion (Ros? et al, 2005). Altogether each student?s 
protocol was divided into between 10 and 25 seg-
ments such that the entire corpus was divided into 
approximately 379 topic segments altogether.  Us-
ing InfoMagnets, the domain expert identified 15 
distinct topics such that each student covered be-
tween 4 and 11 of these topics either once or mul-
tiple times throughout their interaction. 
The topic analysis of the corpus gives us a way 
of quickly getting a sense of how tutors divided 
their instructional time between different topics of 
conversation.  Based on this topic analysis of the 
human-tutoring corpus, the domain expert de-
signed 12 dialogues, which were then implemented 
using a dialogue authoring environment called 
TuTalk (Gweon et al, 2005).  In a recent very suc-
cessful classroom evaluation, we observed the in-
structional effectiveness of these implemented 
tutorial dialogue agents, as measured by pre and 
post tests. 
 
Acknowledgments 
This work was funded by Office of Naval Re-
search, Cognitive and Neural Science Division, 
grant number N00014-05-1-0043. 
References  
Jaime Arguello, Brian S. Butler, Lisa Joyce, Robert 
Kraut, Kimberly S. Ling, Carolyn Rose and Xiaoqing 
Wang (2006). Talk to Me: Foundations of Successful 
Individual-Group Interactions in Online Communi-
ties. To appear in Proceedings of CHI: Human Fac-
tors in Computing. 
Matthew Chalmers and Paul Chitson (1992). Bead: Ex-
plorations in Information Visualization. In Proceed-
ings of ACM SIGIR,  330-337 
David Dubin (1995). Document Analysis for Visualiza-
tion. In Proceedings of ACM SIGIR, 199-204. 
Gahgene Gweon, Jaime Arguello, Carol Pai, Regan 
Carey, Zachary Zaiss, and Carolyn Ros? (2005).  
Towards a Prototyping Tool for Behavior Oriented 
Authoring of Conversational Interfaces, Proceedings 
of the ACL Workshop on Educational Applications of 
NLP. 
Rohit Kumar, Carolyn Ros?, Vincent Aleven, Ana Igle-
sias, Allen Robinson (submitted). Evaluating the Ef-
fectiveness of Tutorial Dialogue Instruction in an 
Exploratory Learning Context, Submitted to ITS ?06 
Thomas Landauer, Peter W. Foltz, and Darrell Laham 
(1998).  Introduction to Latent Semantic Analysis. 
Discourse Processes, 25, 259-284.  
Anton Leuski and James Allan (2002). Lighthouse: 
Showing the Way to Relevant Information. In Pro-
ceedings of the IEEE InfoVis  2000 
Matt Rasmussen and George Karypis (2004). gCLUTO: 
An Interactive Clustering, Visualization, and Analy-
sis System. Technical Report # 04-021 
Carolyn Ros?, Vincent Aleven, Regan Carey, Allen 
Robinson, and Chih Wu (2005). A First Evaluation 
of the Instructional Value of Negotiable Problem 
Solving Goals on the Exploratory Learning Contin-
uum, Proceedings of AI in Education ?05 
Michael Steinbach, George Karypis, and Vipin Kuma 
(2000). A comparison of document clustering tech-
niques. In KDD Workshop on Text Mining. 
James A. Wise, James J. Thomas, Kelly Pennock, David 
Lantrip, Marc Pottier, and Anne Schur (1995).  Visu-
alizing the Non-Visual: Spatial Analysis and Interac-
tion with Information from Text Documents. In 
Proceedings of IEEE InfoVis ?95, 51-58. 
256
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 45?52, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Towards a Prototyping Tool for Behavior Oriented Authoring of  
Conversational Agents for Educational Applications 
 
 
Gahgene Gweon, Jaime Arguello, Carol Pai, Regan Carey, Zachary 
Zaiss, Carolyn Ros? 
Human-Computer Interaction Institute/ Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue, Pittsburgh, PA 15213 USA 
Ggweon,jarguell,cpai,rcarey,zzaiss,cp3a@andrew.cmu.edu 
 
   
Abstract 
Our goal is to develop tools for facili-
tating the authoring of conversational 
agents for educational applications, and 
in particular to enable non-
computational linguists to accomplish 
this task efficiently.  Such a tool would 
benefit both learning researchers, al-
lowing them to study dialogue in new 
ways, and educational technology re-
searchers, allowing them to quickly 
build dialogue based help systems for 
tutoring systems. We argue in favor of 
a user-centered design methodology.  
We present our work-in-progress de-
sign for authoring, which is motivated 
by our previous tool development ex-
periences and preliminary contextual 
interviews and then refined through 
user testing and iterative design.   
1 Introduction 
This paper reports work in progress towards 
developing TuTalk, an authoring environment 
developed with the long term goal of enabling 
the authoring of effective tutorial dialogue 
agents.  It was designed for developers without 
expertise in knowledge representation, artificial 
intelligence, or computational linguistics.  In our 
previous work we have reported progress to-
wards the development of authoring tools spe-
cifically focusing on robust language 
understanding capabilities (Ros? et al, 2003; 
Ros? & Hall, 2004; Ros?, et al, 2005).  In this 
paper, we explore issues related to authoring 
both at the dialogue and sentence level, as well 
as the interaction between these two levels of 
authoring.  Some preliminary work on the un-
derlying architecture is reported in (Jordan, Ro-
s?, & VanLehn, 2001; Aleven & Ros?, 2004; 
Ros? & Torrey, 2004).  In this paper we focus 
on the problem of making this computational 
linguistics technology accessible to our target 
user population.   
We are developing the TuTalk authoring en-
vironment in connection with a number of exist-
ing local research projects related to educational 
technology in general and tutorial dialogue in 
particular.  It is being developed primarily for 
use within the Pittsburgh Sciences of Learning 
Center (PSLC) data shop, which includes devel-
opment efforts for a suite of authoring tools to 
be used for building the infrastructure for 7 dif-
ferent computer enhanced courses designated as 
LearnLab courses.  These LearnLab courses, 
which are conducted within local secondary 
schools as well as universities, and which in-
clude Chinese, French, English as a Second 
Language, Physics, Algebra, Geometry, and 
Chemistry, involve heavy use of technology 
both for the purpose of supporting learning as 
well as for the purpose of conducting learning 
research in a classroom setting.  Other local pro-
jects related to calculus and thermodynamics 
45
tutoring also have plans to use TuTalk.  In this 
paper we will discuss specifically how we have 
used corpora related to ESL, physics, thermody-
namics, and calculus in our development effort. 
To support this multi-domain effort, it is es-
sential that the technology we develop be do-
main independent and usable by a non-technical 
user population, or at least a user population not 
possessing expertise in knowledge representa-
tion, artificial intelligence, or computational lin-
guistics.  Thus, we are employing a corpus based 
methodology that bootstraps domain specific 
authoring using examples of desired conversa-
tional behavior for the domain. 
2 A Historical Perspective 
While a focus on design based on standards 
and practices from human-computer interaction 
community have not received a great deal of 
attention in previously published tool develop-
ment efforts known to the computational linguis-
tics community, our experience tells us that 
insufficient attention to these details leads to the 
development of tools that are unusable, particu-
larly to the user population that we target with 
our work. 
Some desiderata related to the design of our 
system are obvious based on our target user 
population.  Currently, many educational tech-
nology oriented research groups do not have 
computational linguists on their staff with the 
expertise required to author domain specific 
knowledge sources for use with sophisticated 
state-of-the-art understanding systems, such as 
CARMEL (Ros?, 2000) or TRIPS (Allen et al, 
2001). However, previous studies have shown 
that, while scaffolding and guidance is required 
to support the authoring process, non-
computational linguists possess many of the ba-
sic skills required to author conversational inter-
faces (Ros?, Pai, & Arguello, 2005). Because the 
main barrier of entry to such sophisticated tools 
are expertise in understanding the underlying 
data structures and linguistically motivated rep-
resentation, our tools should have an interface 
that masks the unnecessary details and provides 
intuitive widgets that manipulate the data in 
ways that are consistent with the mental models 
the users bring with them to the authoring proc-
ess.  In order to be maximally accessible to de-
velopers of educational technology, the system 
should involve minimal programming.   
The design of Carmel-Tools (Ros? et al, 
2003; Ros? & Hall, 2004), the first generation of 
our authoring tools, was based on these obvious 
desiderata and not on any in-depth analysis of 
data collected from our target user population.  
While an evaluation of the underlying computa-
tional linguistics technology showed promise 
(Ros? & Hall, 2004), the results from actual au-
thoring use were tremendously disappointing.  
A formal study reported in (Ros?, et al, 2005) 
demonstrates that even individuals with exper-
tise in computational linguistics have difficulty 
predicting the coverage of knowledge sources 
that would be generated automatically from ex-
ample texts annotated with desired representa-
tions. Informal user studies involving actual use 
of Carmel-Tools then showed that a conse-
quence of this lack of ability is that authors were 
left without a clear strategy for moving through 
their corpus.  As a result, time was lost from an-
notating examples that did not yield the maxi-
mum amount of new knowledge in the generated 
knowledge sources.  Furthermore, since authors 
tended not to test the generated knowledge 
sources as they were annotating examples, errors 
were difficult for them to track later, despite fa-
cilities designed to help them with that task.   
Another finding from our user studies was 
that although the interface prevented authors 
from violating the constraints they designed into 
their predicate language, it did not keep authors 
from annotating similar texts with very different 
representations, thus introducing a great deal of 
spurious ambiguity.  Thus, they did not naturally 
maintain consistency in their application of their 
own designed meaning representation languages 
across example texts.  An additional problem 
was that authors sometimes decomposed exam-
ples in ways that lead to overly general rules, 
which then lead to incorrect analyses when these 
rules matched inappropriate examples.   
These disappointing results convinced us of 
the importance of taking a user-centered design 
approach to our authoring interface redesign 
process. 
 
 
46
3 Preliminary Design Intents from 
Contextual Interviews 
The core essence of the user-centered design 
approach is designing from data rather than from 
preconceived notions of what will be useful and 
what will work well.  Expert blind spots often 
lead to designs based on intuitions that overlook 
needs or overly emphasize issues that are not 
centrally important (Koedinger & Nathan, 2004; 
Nathan & Koedinger, 2000).  Contextual inquiry 
is used at an early stage in the user-centered de-
sign process to collect the foundational data on 
which to build a design (Beyer and Holtzbatt, 
2000). Contextual Inquiry is a popular method 
developed within the Human Computer Interac-
tion community where the design team gathers 
data from end users while watching what the 
users do in context of their work. Contextual 
interviews are used to illuminate these observa-
tions by engaging end-users in interviews in 
which they show specific instances within their 
work life that are relevant for the design process.  
These methods help define requirements as well 
as plan and prioritize important aspects of func-
tionality.  At the same time, the system design-
ers get a chance to gain insights about the users? 
environment, tasks, cultural influences and diffi-
culties in the current processes.  
Many aspects of the Tutalk tool were de-
signed based on contextual inquiry (CI) data. 
The design team conducted five CIs with users 
who have experience in using existing authoring 
tools such as Carmel-Tools (Ros? & Hall, 2004). 
The design team leader also spent one week ob-
serving novice tool users working with the cur-
rent set of tools at an Intelligent Tutoring 
Summer School.  Here we will discuss some 
findings from those CIs and observations and 
how they motivated some general design intents, 
which we flesh out later in the paper.  
A common pattern we observed in our CIs 
was that having different floating windows for 
different tasks fills up the computer screen rela-
tively quickly and confuses authors as to where 
they are in the process of authoring.  The TuTalk 
design addresses this observed problem by an-
choring the main window and switching only the 
components of the window as needed.  A stan-
dard logic for layout and view switching helps 
authors know what to expect in different con-
texts.  Placement of buttons in TuTalk is consis-
tently near the textboxes that they control, and a 
bounding box is drawn around related sets of 
controls so that the user does not get lost trying 
to figure out where the buttons are or what they 
are for.   
We observed that authors needed to refer to 
cheat sheets and user documentation to use their 
current tools effectively and that different users 
did not employ the same terminology to refer to 
similar functionality, which made communica-
tion difficult.  Furthermore, their current suites 
of tools were not designed as one integrated en-
vironment.  Thus, a lot of shuffling of files from 
one directory to another was required in order to 
complete the authoring process.  Users without 
Unix operating system experience found this 
especially confusing.  Our goal is to require only 
very minimal documentation that can be ob-
tained on-line in the context of use.   
TuTalk is a single, integrated environment 
that makes use of GUI widgets for actions rather 
then requiring any text-based commands or file 
system activity.  In this way we hope to avoid 
requiring the users to use a manual or a ?cheat-
sheet? reference for the commands they forget. 
As is common practice, TuTalk also uses consis-
tent labels throughout the interface to promote 
understandability and communication with tool 
developers as well as other dialogue system de-
velopers. 
4 Exploring the User?s Mental Model 
through User Studies 
As an additional way of gaining insights into 
what sort of interface would make the process of 
authoring conversational interfaces accessible, 
we conducted a small, exploratory user study in 
which we examined how members of our target 
user population think about the structure of lan-
guage.   
Two groups of college-level participants with 
no deep linguistics training were asked to read 
three transcribed conversations about ordering 
from a menu at a restaurant from our English as 
a Second Language corpus.  The three specific 
restaurant dialogues were chosen because of 
their breadth of topic coverage and richness in 
linguistic expression.  Participants were asked to 
perform tasks with these dialogues to mimic 
47
three levels of conversational interface author-
ing: 
 
Macro Organization Tasks (dialogue level) 
Level 1. How authors understand, seg-
ment, and organize dialogue topics 
Level 2.  How authors generalize across 
dialogues as part of constructing a 
?model? script 
Micro Organization Task (sentence level) 
Level 3.  How authors categorize and 
decompose sentences within these dia-
logues 
 
The first group (Group A, five participants) 
was asked to perform Macro Organization Tasks 
before processing sentences for the Micro Or-
ganization Tasks.  The second group (Group B, 
four participants) was asked to perform these 
sets of tasks in the opposite order. 
Our findings for the Macro Organization 
Tasks showed that participants effectively broke 
down dialogues into segments that reflected in-
tuitive breaks in the conversation.  These topics 
were then organized into semantically related 
categories.  Although participants were not ex-
plicitly instructed on how to organize the topics, 
every participant used spatial proximity as a rep-
resentation for semantic relatedness. Another 
finding was the presence of primacy effects in 
the ?model? restaurant scripts they were asked to 
construct. These scripts were heavily influenced 
by the first dialogue read. As a result, important 
topics that surfaced in the other two dialogues 
were omitted from the model scripts. 
Furthermore, we found that participants in 
Group B took much longer in completing the 
Micro Organization Task (35-40 minutes as op-
posed to 25-30 minutes) without performing the 
Macro Organization Tasks first. In general, we 
found that participants clustered sentences based 
on surface characteristics rather than creating 
ontologically similar classes that would be more 
useful from a system development perspective. 
In a follow-up study we are exploring ways of 
guiding users to cluster sentences in ways that 
are more useful from a system building perspec-
tive. 
Our preliminary findings show that getting an 
overall sense of the corpus facilitates micro-
level organization. This is hindered by two fac-
tors:  First, primacy effects interfere with macro-
level comprehension. Second, system developers 
struggle to strategically select portions of their 
corpus on which to focus their initial efforts.  
5 Stage One: Corpus Organization 
While existing tools from our previous work 
required authors to organize their corpus data 
prior to their interaction with the tools, both our 
contextual research and user studies indicated 
that support for organizing corpus data prior to 
authoring is important.   
In light of this concern, the TuTalk authoring 
process consists of three main stages.  Corpus 
collection, corpus data organization through 
what we call the InfoMagnet interface, and au-
thoring propper. First, a corpus is collected by 
asking users to engage in conversation using 
either a typed or spoken chat interface. In the 
case of spoken input, the speech is then tran-
scribed into textual form. Second, the raw cor-
pus data is automatically preprocessed for 
display and interactive organization using the 
InfoMagnet interface.  As part of the preprocess-
ing, dialogue protocols are segmented automati-
cally at topic boundaries, which can be adjusted 
by hand later during authoring propper.  The 
topic oriented segments are then clustered semi-
automatically into topic based classes. The out-
put from this stage is an XML file where dia-
logue segments are reassembled into their 
original dialogue contexts, with each utterance 
labeled by topic. This XML file is finally passed 
onto the authoring environment propper, which 
is then used for finer grained processing, such as 
shifting topic segment boundaries and labeling 
more detailed utterance functionality.   
Our design is for knowledge sources that are 
runable from our dialogue system engine to be 
generated directly from the knowledge base cre-
ated during the fine-grained authoring process as 
in Carmel-Tools (Ros? & Hall, 2004), however 
currently our focus is on iterative development 
of a prototype of the authoring interaction de-
sign.  Thus, more work is required to create the 
final end-to-end implementation.  In this section 
we focus on the design of the corpus collection 
and organization part of the authoring process. 
 
48
5.1 Corpus Collection  
An important part of our mission is developing 
technology that can use collected and automati-
cally pre-processed corpus data to guide and 
streamline the authoring process. Prior to the 
arduous process of organizing and extracting 
meaningful data, a corpus must be collected.  
As part of the PSLC and other local tutorial 
dialogue efforts we have collected corpus data 
from multiple domains that we have made use of 
in our development process. In particular, we 
have been working with data collected in con-
nection with the PSLC Physics and English as a 
Second Language LearnLab courses as well as 
local Calculus and Thermodynamics tutoring 
projects.  Currently we have physics tutoring 
data primarily from one physics tutor (interac-
tions with 40 students), thermodynamics data 
from four different tutors (interactions with 27 
students), Calculus data from four different tu-
tors (84 dialogues), and ESL dialogues collected 
from 15 pairs of students (30 dialogues alto-
gether).  
While we have drawn upon data from all of 
these domains for testing the underlying lan-
guage processing technology for our develop-
ment effort, for our user studies we have so far 
mainly drawn upon our ESL corpus, which in-
cludes conversations between students about 
every-day tasks such as ordering from a restau-
rant or about their pets.  We chose the language 
ESL data for our initial user tests because we 
expected it to be easy for a general population to 
relate to, but we plan to begin using calculus 
data as well.   
5.2 InfoMagnets Interface 
As mentioned previously, once the raw dia-
logue corpus is collected, the next step is to sift 
through this data and assign utterances (or 
groups of utterances) to classes conceptualized 
by the author. Clustering is a natural step in this 
kind of exploratory data analysis, as it promotes 
learning by grouping and generalizing from 
what we know about some of the objects in a 
cluster. For this purpose we have designed the 
InfoMagnets interface, which introduces a non-
technical metaphor to the task of iterative docu-
ment clustering. The InfoMagnets interface was 
designed to address the problems identified in 
the user study discussed above in Section 4.  
Specifically, we expected that those problems 
could be addressed with an interface that:  
1. Divides dialogues into topic based 
segments and automatically clusters 
them into conceptually similar classes 
2. Eliminates primacy effects of sequen-
tial dialogue consumption by creating an 
inclusive compilation of all dialogue 
topics 
3. Makes the topic similarity of docu-
ments easily accessible to the user  
 
The InfoMagnets interface is displayed in 
Figure 1.  The larger circles (InfoMagnets) cor-
respond to cluster centroids and the smaller ones 
(particles) correspond to actual spans of text. 
Lexical cohesion in the vector space translates 
into attraction in the InfoMagnet space. The at-
traction from each particle to each InfoMagnet is 
evident from the particle?s position with respect 
to all InfoMagnets and its reaction-time when an 
InfoMagnet is moved by the user, which causes 
the documents that have some attraction with it 
to redistribute themselves in the InfoMagnet 
space.  
 
 
Figure 1 InfoMagnets Interface 
 
Being an unsupervised learning method, clus-
tering often requires human-intervention for 
fine-tuning (e.g. removing semantically-weak 
discriminators, culling meaningless clusters, or 
deleting/splitting clusters too fine/coarse for the 
author?s purpose). The InfoMagnets interface 
provides all this functionality, while shielding 
the author from the computational details inher-
ent in these tasks 
49
Initially, the corpus is clustered using the Bi-
secting K-means Algorithm described in (Kumar 
et al, 1998).  Although this is a hard clustering 
algorithm, the InfoMagnet interface shows the 
particles association with all clusters, given by 
the position of the particle. Using a cross-hair 
lens, the author is able to view the contents of 
each cluster centroid and each particle. The au-
thor is able to select a group of particles and 
view the common features between these parti-
cles and any InfoMagnet in the space. The inter-
face allows the editing of InfoMagnets by 
adding and removing features, splitting In-
foMagnets, and removing InfoMagnets. When 
the user edits an InfoMagnets, the effect in the 
particle distribution is shown immediately and in 
an animated way.  
5.3 XML format 
The data collected from the conversations 
in .txt format are reformatted into XML format 
before being displayed with InfoMagnet tool.  
The basic XML file contains a transcription of 
the conversational data and has the following 
structure: Under the top root tag, there is <dia-
logue> tag which designates the conversion 
about a topic. It has an ?id? attribute so that we 
can keep track of each separate conversation. 
Then each sentence has a <sentence> tag with 
two attributes ?uid? and ?agent?. ?uid? is a uni-
versal id and ?agent? tells who was speaking.  
Additionally, sentences are grouped into seg-
ments, marked off with a <subtopic> tag. 
The user?s interaction with the InfoMagnet in-
terface adds a ?subtopic-name? attribute to the 
subtopic tag. Then, the authoring interface 
proper, described below, allows for further ad-
justments and additions to the xml tags.  The 
final knowledge sources will be generated from 
this XML based representation. 
6 Authoring 
The authoring environment proper consists of 
two main views, namely the authoring view and 
tutoring view. The authoring view is where the 
author designs the behavior of the conversa-
tional agent. The authoring view has two levels; 
the topic level and the subtopic level. The tutor-
ing view is what a student will be looking at 
when interacting with the conversational agent. 
Our focus here is on the Authoring view. 
Authoring View: Topic Level 
The Topic level of the authoring view allows for 
manipulating the relationship between subtopics 
as well as the definition of the subtopic. Figure 2 
shows the topic level authoring view, which 
consists of two panels. In the left, the author in-
puts the description of the task that the student 
will engage in with the agent. The author can 
specify whether the student will be typing or 
talking, the title of the topic, the task description, 
an optional picture that aids with the task (such 
as a menu or a map of a city), and a time limit.  
In the right panel of the topic level authoring 
view, the structure imposed on the data by inter-
action with the InfoMagnets interface is dis-
played in sequential form. The top section of the 
interface (figure 2, section A) has a textbox for 
specifying an xml file to read. The next section 
(figure 2, section B), ?Move / Rename Subtopic? 
displays the subtopics. The order of the subtop-
ics displayed in this section acts as a guideline 
for the agent to follow during the conversation. 
Double-clicking on a subtopic will display a 
subtopic view on the right panel. This view acts 
as a reference for the agent?s conversation 
within the subtopic and is explained in the next 
section. The author can also rearrange the order 
of subtopics by selecting a subtopic and using 
the ?>? and ?<? buttons to move the subtopic 
right or left respectively. ?x? is used to delete 
the subtopic. The author can also specify 
whether the discussion of a subtopic is required 
(displayed in red) or optional (in green) using 
the checkbox that is labeled ?required?. Clicking 
on the ?Hide Opt? button will only display the 
required subtopics. 
The last section of the right panel in topic 
level authoring view (figure 2, section C) is ti-
tled ?move subtopic divider?. A blue line de-
notes the border of the subtopic. The author can 
move the line up or down to move the boundary 
of the subtopics automatically inserted by the 
InfoMagnets interface. The author can also click 
on any part of conversation and press the ?split? 
button to split the subtopic in two sections. In 
addition, she can change the label of the sub-
topic segment using the drop down list. 
50
  
 
 
Figure 2: Topic Level Authoring View 
 
Authoring View: Subtopic Level 
While the Topic View portion of the authoring 
interface proper allows specification of which 
subtopics can occur as part of a dialogue, which 
are required and which are optional, and what 
the default ordering is, the Subtopic Level is for 
specification of the low level turn-by-turn details 
of what happens within a subtopic segment.  
This section reports early work on the design of 
this portion of the interface. 
The subtopic view displays a structure that the 
conversational agent refers to in deciding what 
its next contribution should be.  The building 
blocks from which knowledge sources for the 
dialogue engine will be generated are templates 
abstracted from example dialogue segments, 
similar to KCD specifications (Jordan, Ros?, & 
VanLehn, 2001; Ros? & Torry, 2004).  As part 
of the process of abstracting templates, each ut-
terance is tagged with its utterance type using a 
menu-based interface as in (Gweon et al, sub-
mitted).  The utterance type determines what 
would be an appropriate form for a response.  
Identifying this is meant to allow the dialogue 
manager to maintain coherence in the emerging 
dialogue.  Users may also trim out undesired 
portions of text from the actual example frag-
ments in abstracting out templates to be used for 
generating knowledge sources. 
Each utterance type has sets of template re-
sponse types associated with them. The full set 
of utterance types includes Open questions, 
Closed questions, Understanding check ques-
tions, Assertions, Commands/Requests, Ac-
knowledgements, Acceptances, and Rejections. 
The templates will not be used in their authored 
form.  Instead, they will be used to generate 
knowledge sources in the form required by the 
backend dialogue system as in (Ros? & Hall, 
2004), although this is still work in progress.  
Each template is composed of one or more ex-
changes during which the speaker who initiated 
the segment maintains conversational control. If 
control shifts to the other speakers, a new tem-
plate is used to guide the conversation.  After 
each of the controlling speaker?s turns within the 
segment are listed a number of prototypical re-
sponses.  One of these responses is a default re-
sponse that signals that the dialogue should 
proceed to the next turn in the template.  The 
other prototypical responses are associated with 
subgoals that are in turn associated with other 
templates.  Thus, the dialogue takes on a hierar-
chical structure.   
Mixed initiative interaction is meant to 
emerge from the underlying template-based 
structure by means of the multi-threaded dis-
course management approach discussed in (Ros? 
& Torrey, 2004).  To this end, templates are 
meant to be used in two ways.  The first way is 
51
when the dialogue system has conversational 
control.  In this case, conversations can be man-
aged as in (Ros? et al, 2001). The second way in 
which templates are used is for determining how 
to respond when user?s have conversational con-
trol.  Provided that the user?s utterances match 
what is expected of the conversational partici-
pant who is in control based on the current tem-
plate, then the system can simply pick one of the 
expected responses.  Otherwise if at some point 
the user?s response does not match, the system 
should check whether the user is initiating yet a 
different segment.  If not, then the system should 
take conversational control. 
7 Future Plans 
In this paper we have discussed our user re-
search and design process to date for the devel-
opment of TuTalk, an authoring environment for 
conversational agents for educational purposes.  
We are continuing our user research and design 
iteration with the plan of end-to-end system test-
ing in actual use starting this summer. 
 
Acknowledgements 
This work was supported in part by Office of Naval 
Research, Cognitive and Neural Sciences Division 
Grant N00014-05-1-0043 and NSF Grant 
SBE0354420.  
References  
Aleven , V. and Ros?, C. P. 2004.  Towards Easier 
Creation of Tutorial Dialogue Systems: Integration 
of Authoring Environments for Tutoring and Dia-
logue Systems, Proceedings of the ITS Workshop 
on Tutorial Dialogue Systems  
Allen, J., Byron, D., Dzikovska, M., Ferguson, G., 
Galescu, L., & Stent, A. 2000. An Architecture for 
a Generic Dialogue Shell. NLENG: Natural Lan-
guage Engineering, Cambridge University Press, 6 
(3), 1-16. 
Beyer, H. & Holtzblatt, K. (1998). Contextual De-
sign, Morgan Kaufmann Publishers. 
Gweon, G., Ros?, C., Wittwer, J., Nueckles, M. 
(submitted).  Supporting Efficient and Reliable 
Content Analysis with Automatic Text Processing 
Technology, Submitted to INTERACT ?05. 
Jordan, P., Ros?, C. P., & VanLehn, K. (2001). Tools 
for Authoring Tutorial Dialogue Knowledge. In J. 
D. Moore, C. L. Redfield, & W. L. Johnson (Eds.), 
Proceedings of AI-ED 2001 (pp. 222-233). Am-
sterdam, IOS Press. 
Koedinger, K. R. & Nathan, M. J. (2004).  The real 
story behind story problems: Effects of representa-
tions on quantitative reasoning.  The Journal of the 
Learning Sciences, 13(2). 
Nathan, M. J. & Koedinger, K. R. (2000).  Moving 
beyond teachers? intuitive beliefs about algebra 
learning.  Mathematics Teacher, 93, 218-223. 
Porter, M. 1980.  An Algorithm for Suffix Stripping, 
Program 14 {3}:130 ? 137. 
Robertson, S. and Walker, S., 1994.  Some simple 
effective approximations to the 2-poisson model 
for probabilistic weighted retrieval Proceedings of 
SIGIR-94. 
Ros?, C. P., and Torrey, C. (2004). ,DRESDEN: To-
wards a Trainable Tutorial Dialogue Manager to 
Support Negotiation Dialogues for Learning and 
Reflection, Proceedings of the Intelligent Tutoring 
Systems Conference. 
Ros?, C. P. and Hall, B. (2004). A Little Goes a Long 
Way: Quick Authoring of Semantic Knowledge 
Sources for Interpretation, Proceedings of SCa-
NaLu ?04. 
Ros?, C. P. 2000. A framework for robust semantic 
interpretation. In Proceedings of the First Meeting 
of the North American Chapter of the Association 
for Computational Linguistics, pages 311?318. 
Ros?, C. P., Pai, C., Arguello, J. 2005. Enabling Non-
Linguists to Author Advanced Conversational In-
terfaces Easily. Proceedings of FLAIRS 2005.  
Steinbach, Kepis, and Kumar, A Comparison of 
Document Clustering Techniques, pg. 8. 
http://lucene.apache.org 
52
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 42?49,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Topic Segmentation of Dialogue 
 
Jaime Arguello Carolyn Ros? 
Language Technologies Institute Language Technologies Institute 
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15217 Pittsburgh, PA 15217 
jarguell@andrew.cmu.edu cprose@cs.cmu.edu 
  
 
Abstract 
We introduce a novel topic segmentation 
approach that combines evidence of topic 
shifts from lexical cohesion with linguistic 
evidence such as syntactically distinct fea-
tures of segment initial and final contribu-
tions.  Our evaluation shows that this hy-
brid approach outperforms state-of-the-art 
algorithms even when applied to loosely 
structured, spontaneous dialogue.  Further 
analysis reveals that using dialogue ex-
changes versus dialogue contributions im-
proves topic segmentation quality. 
1 Introduction 
In this paper we explore the problem of topic 
segmentation of dialogue. Use of topic-based mod-
els of dialogue has played a role in information 
retrieval (Oard et al, 2004), information extraction 
(Baufaden, 2001), and summarization (Zechner, 
2001), just to name a few applications. However, 
most previous work on automatic topic segmenta-
tion has focused primarily on segmentation of ex-
pository text. This paper presents a survey of the 
state-of-the-art in topic segmentation technology. 
Using the definition of topic segment from (Pas-
sonneau and Litman, 1993) applied to two different 
dialogue corpora, we present an evaluation includ-
ing a detailed error analysis, illustrating why ap-
proaches designed for expository text do not gen-
eralize well to dialogue.  
We first demonstrate a significant advantage of 
our hybrid, supervised learning approach called 
Museli, a multi-source evidence integration ap-
proach, over competing algorithms. We then ex-
tend the basic Museli algorithm by introducing an 
intermediate level of analysis based on Sinclair and 
Coulthard?s notion of a dialogue exchange (Sin-
clair and Coulthard, 1975). We show that both our 
baseline and Museli approaches obtain a signifi-
cant improvement when using perfect, hand-
labeled dialogue exchanges, typically in the order 
of 2-3 contributions, as the atomic discourse unit in 
comparison to using the contribution as the unit of 
analysis. We further evaluate our success towards 
automatic classification of exchange boundaries 
using the same Museli framework.  
2 Defining Topic 
In the most general sense, the challenge of topic 
segmentation can be construed as the task of find-
ing locations in the discourse where the focus 
shifts from one topic to another. Thus, it is not pos-
sible to address topic segmentation of dialogue 
without first addressing the question of what a 
?topic? is. We began with the goal of adopting a 
definition of topic that meets three criteria. First, it 
should be reproducible by human annotators. Sec-
ond, it should not rely heavily on domain-specific 
knowledge or knowledge of the task structure. Fi-
nally, it should be grounded in generally accepted 
principles of discourse structure.  
The last point addresses a subtle, but important, 
criterion necessary to adequately serve down-
stream applications using our dialogue segmenta-
tion. Topic analysis of dialogue concerns itself 
mainly with thematic content. However, bounda-
ries should be placed in locations that are natural 
turning points in the discourse. Shifts in topic 
should be readily recognizable from surface char-
acteristics of the language. 
With these goals in mind, we adopted a defini-
tion of ?topic? that builds upon Passonneau and 
Litman?s seminal work on segmentation of mono-
logue (Passonneau and Litman, 1993).  They found 
that human annotators can successfully accomplish 
a flat monologue segmentation using an informal 
notion of speaker intention. 
42
Dialogue is inherently hierarchical in structure. 
However, a flat segmentation model is an adequate 
approximation. Passonneau and Litman?s pilot 
studies confirmed previously published results 
(Rotondo, 1984) that human annotators cannot re-
liably agree on a hierarchical segmentation of 
monologue. Using a stack-based hierarchical 
model of discourse, Flammia (1998) found that 
90% of all information-bearing dialogue turns re-
ferred to the discourse purpose at the top of the 
stack.  
We adopt a flat model of topic segmentation 
based on discourse segment purpose, where a shift 
in topic corresponds to a shift in purpose that is 
acknowledged and acted upon by both conversa-
tional participants. We place topic boundaries on 
contributions that introduce a speaker?s intention to 
shift the purpose of the discourse, while ignoring 
expressed intentions to shift discourse purposes 
that are not taken up by the other participant. We 
adopt the dialogue contribution as the basic unit of 
analysis, refraining from placing topic boundaries 
within a contribution. This decision is analogous to 
Hearst?s (Hearst, 1994, 1997) decision to shift the 
TextTiling induced boundaries to their nearest ref-
erence paragraph boundary.  
We evaluated the reproducibility of our notion 
of topic segment boundaries by assessing inter-
coder reliability over 10% of the corpus (see Sec-
tion 5.1).  Three annotators were given a 10 page 
coding manual with explanation of our informal 
definition of shared discourse segment purpose as 
well as examples of segmented dialogues.  Pair-
wise inter-coder agreement was above 0.7 for all 
pairs of annotators. 
3 Previous Work 
Existing topic segmentation approaches can be 
loosely classified into two types: (1) lexical cohe-
sion models, and (2) content-oriented models.  The 
underlying assumption in lexical cohesion models 
is that a shift in term distribution signals a shift in 
topic (Halliday and Hassan, 1976). The best known 
algorithm based on this idea is TextTiling (Hearst, 
1997). In TextTiling, a sliding window is passed 
over the vector-space representation of the text. At 
each position, the cosine correlation between the 
upper and lower regions of the sliding window is 
compared with that of the peak cosine correlation 
values to the left and right of the window.  A seg-
ment boundary is predicted when the magnitude of 
the difference exceeds a threshold.    
One drawback to relying on term co-occurrence 
to signal topic continuity is that synonyms or re-
lated terms are treated as thematically-unrelated. 
One proposed solution to this problem is Latent 
Semantic Analysis (LSA) (Landauer and Dumais, 
1997). Two LSA-based algorithms for segmenta-
tion are described in (Foltz, 1998) and (Olney and 
Cai, 2005). Foltz?s approach differs from 
TextTiling mainly in its use of an LSA-based vec-
tor space model. Olney and Cai address a problem 
not addressed by TextTiling or Foltz?s approach, 
which is that cohesion is not just a function of the 
repetition of thematically-related terms, but also a 
function of the presentation of new information in 
reference to information already presented. Their 
orthonormal basis approach allows for segmenta-
tion based on relevance and informativity.  
Content-oriented models, such as (Barzilay and 
Lee, 2004), rely on the re-occurrence of patterns of 
topics over multiple realizations of thematically 
similar discourses, such as a series of newspaper 
articles about similar events. Their approach util-
izes a hidden Markov model where states corre-
spond to topics and state transition probabilities 
correspond to topic shifts. To obtain the desired 
number of topics (states), text spans of uniform 
length (individual contributions, in our case) are 
clustered. Then, state emission probabilities are 
induced using smoothed cluster-specific language 
models. Transition probabilities are induced by 
considering the proportion of documents in which 
a contribution assigned to the source cluster (state) 
immediately precedes a contribution assigned to 
the target cluster (state). Following an EM-like 
approach, contributions are reassigned to states 
until the algorithm converges. 
4 Overview of Museli Approach 
We cast the segmentation problem as a binary 
classification problem where each contribution is 
classified as NEW_TOPIC if it introduces a new 
topic and SAME_TOPIC otherwise. In our hybrid 
Museli approach, we combined lexical cohesion 
with features that have the potential to capture 
something about the linguistic style that marks 
shifts in topic. Table 1 lists our features.  
 
 
 
43
Feature Description 
Lexical  
Cohesion 
Cosine correlation of adjacent 
regions in the discourse. Term 
vectors of adjacent regions are 
stemmed and stopwords are re-
moved. 
Word-
unigram 
Unigrams in previous and cur-
rent contributions 
Word-bigram Bigrams in previous and current 
contributions 
Punctuation Punctuation of previous and cur-
rent contributions. 
Part-of-
Speech (POS)  
Bigram 
POS-Bigrams in previous and 
current contributions.  
Time  
Difference 
Time difference between previ-
ous and current contribution, 
normalized by: 
(X ? MIN)/ (MAX ? MIN), 
where X corresponds to this time 
difference and MIN & MAX are 
with respect to the whole corpus. 
Content  
Contribution 
Binary-valued, is there a non-
stopword term in the current 
contribution? 
Contribution 
Length 
Number of words in the current 
contribution, normalized by:  
(X ? MIN) / (MAX ? MIN). 
Previous 
Agent1
Binary-valued, was the speaker 
of the previous contribution the 
student or the tutor? 
Table 1. Museli Features. 
 
  We found that using a Na?ve Bayes classifier 
with an attribute selection wrapper using the chi-
square test for ranking attributes performed better 
than other state-of-the-art machine learning algo-
rithms on our task, perhaps because of the evi-
dence integration oriented nature of the problem.  
We conducted our evaluation using 10-fold cross-
validation, being careful not to include instances 
from the same dialogue in both the training and 
test sets on any fold to avoid biasing the trained 
model with idiosyncratic communicative patterns 
associated with individual dialogue participants.  
To capitalize on differences in conversational 
behavior between participants assigned to different 
                                                 
1  The current contribution?s agent is implicit in the fact that 
we learn separate models for each agent-role (student & tutor). 
roles in the conversation (i.e., student and tutor), 
we learn separate models for each role. This deci-
sion is motivated by observations that participants 
with different speaker-roles, each with different 
goals in the conversation, introduce topics with a 
different frequency, introduce different types of 
topics, and may introduce topics in a different style 
that displays their status in the conversation. For 
instance, a tutor may be more likely to introduce 
new topics with a contribution that ends with an 
imperative. A student may be more likely to intro-
duce new topics with a contribution that ends with 
a wh-question. Dissimilar agent-roles also occur in 
other domains such as Travel Agent and Customer 
in flight booking scenarios. 
Using the complete set of features enumerated 
above, we perform feature selection on the training 
data for each fold of the cross-validation sepa-
rately, training a model with the top 1000 features, 
and applying that trained model to the test data.  
Examples of high ranking features output by our 
chi-squared feature selection wrapper confirm our 
intuition that initial and final contributions of a 
segment are marked differently. Moreover, the 
highest ranked features are different for our two 
speaker-roles. Some features highly-correlated 
with student-initiated segments are am_trying, 
should, what_is, and PUNCT_question, which re-
late to student questions and requests for informa-
tion. Some features highly-correlated with tutor-
initiated segments include ok_lets, do, see_what, 
and BEGIN_VERB (the POS of the first word in 
the contribution is VERB), which characterize im-
peratives, and features such as now, next, and first, 
which characterize instructional task ordering. 
5 Evaluation   
We evaluate Museli in comparison to the best 
performing state-of-the-art approaches, demon-
strating that our hybrid Museli approach out-
performs all of these approaches on two different 
dialogue corpora by a statistically significant mar-
gin (p < .01), in one case reducing the probability 
of error, as measured by Pk (Beeferman et al, 
1999), to about 10%. 
5.1 Experimental Corpora 
We used two different dialogue corpora from the 
educational domain for our evaluation. Both cor-
pora constitute of dialogues between a student and 
44
a tutor (speakers with asymmetric roles) and both 
were collected via chat software.  The first corpus, 
which we call the Olney & Cai corpus, is a set of 
dialogues selected randomly from the same corpus 
Olney and Cai obtained their corpus from (Olney 
and Cai, 2005). The dialogues discuss problems 
related to Newton?s Three Laws of Motion. The 
second corpus, the Thermo corpus, is a locally col-
lected corpus of thermodynamics tutoring dia-
logues, in which tutor-student pairs work together 
to solve an optimization task. Table 2 shows cor-
pus statistics from both corpora. 
 
 Olney & Cai 
 Corpus 
Thermo 
Corpus 
#Dialogues 42 22 
Conts./Dialogue 195.40 217.90 
Conts./Topic 24.00 13.31 
Topics/Dialogue 8.14 16.36 
Words/Cont. 28.63 5.12 
Student Conts. 4113 1431 
Tutor Conts. 4094 3363 
Table 2. Evaluation Corpora Statistics  
 
Both corpora seem adequate for attempting to 
harness systematic differences in how speakers 
with asymmetric roles may initiate or close topic 
segments. The Thermo corpus is particularly ap-
propriate for addressing the research question of 
how to automatically segment natural, spontaneous 
dialogue. The exploratory task is more loosely 
structured than many task-oriented domains inves-
tigated in the dialogue community, such as flight 
reservation or meeting scheduling. Students can 
interrupt with questions and tutors can digress in 
any way they feel may benefit the completion of 
the task. In the Olney and Cai corpus, the same 10 
physics problems are addressed in each session and 
the interaction is almost exclusively a tutor initia-
tion followed by student response, evident from the 
nearly equal number of student and tutor contribu-
tions. 
5.2 Baseline Approaches 
We evaluate Museli against the following four 
algorithms: (1) Olney and Cai (Ortho), (2) Barzilay 
and Lee (B&L), (3) TextTiling (TT), and (4) Foltz.  
As opposed to the other baseline algorithms, 
(Olney and Cai, 2005) applied their orthonormal 
basis approach specifically to dialogue, and prior 
to this work, report the highest numbers for topic 
segmentation of dialogue. Barzilay and Lee?s ap-
proach is the state of the art in modeling topic 
shifts in monologue text. Our application of B&L 
to dialogue attempts to harness any existing and 
recognizable redundancy in topic-flow across our 
dialogues for the purpose of topic segmentation.  
We chose TextTiling for its seminal contribution 
to monologue segmentation. TextTiling and Foltz 
consider lexical cohesion as their only evidence of 
topic shifts. Applying these approaches to dialogue 
segmentation sheds light on how term distribution 
in dialogue differs from that of expository mono-
logue text (e.g. news articles). The Foltz and Ortho 
approaches require a trained LSA space, which we 
prepared the same way as described in (Olney and 
Cai, 2005). Any parameter tuning for approaches 
other than our Museli was computed over the en-
tire test set, giving baseline algorithms the maxi-
mum advantage.  
In addition to these approaches, we include 
segmentation results from three degenerate ap-
proaches: (1) classifying all contributions as 
NEW_TOPIC (ALL), (2) classifying no contribu-
tions as NEW_TOPIC (NONE), and (3) classifying 
contributions as NEW_TOPIC at uniform intervals 
(EVEN), separated by the average reference topic 
length (see Table 2). 
As a means for comparison, we adopt two 
evaluation metrics: Pk and f-measure. An extensive 
argument in support of Pk?s robustness (if k is set 
to ? the average reference topic length) is pre-
sented in (Beeferman, et al 1999).  Pk measures the 
probability of misclassifying two contributions a 
distance of k contributions apart, where the classi-
fication question is are the two contributions part 
of the same topic segment or not?  Pk is the likeli-
hood of misclassifying two contributions, thus 
lower Pk values are preferred over higher ones. It 
equally captures the effect of false-negatives and 
false-positives and favors predictions that that are 
closer to the reference boundaries. F-measure pun-
ishes false positives equally, regardless of their 
distance to reference boundaries.  
5.3 Results 
Table 3 shows our evaluation results.  Note that 
lower values of Pk are preferred over higher ones. 
The opposite is true of F-measure.  In both cor-
pora, the Museli approach performed significantly 
better than all other approaches (p < .01).  
 
45
 Olney and Cai 
Corpus 
Thermo Corpus 
 Pk F Pk F 
NONE 0.4897 -- 0.4900 -- 
ALL 0.5180 -- 0.5100 -- 
EVEN 0.5117 -- 0.5131 -- 
TT 0.6240 0.1475 0.5353 0.1614 
B&L 0.6351 0.1747 0.5086 0.1512 
Foltz 0.3270 0.3492 0.5058 0.1180 
Ortho 0.2754 0.6012 0.4898 0.2111 
Museli 0.1051 0.8013 0.4043 0.3693 
Table 3. Results on both corpora 
5.4 Error Analysis 
Results for all approaches are better on the Ol-
ney and Cai corpus than the Thermo corpus. The 
Thermo corpus differs profoundly from the Olney 
and Cai corpus in ways that very likely influenced 
the performance. For instance, in the Thermo cor-
pus each dialogue contribution is on average 5 
words long, whereas in the Olney and Cai corpus 
each dialogue contribution contains an average of 
28 words. Thus, the vector space representation of 
the dialogue contributions is more sparse in the 
Thermo corpus, which makes shifts in lexical co-
herence less reliable as topic shift indicators.   
In terms of Pk, TextTiling (TT) performed worse 
than the degenerate algorithms. TextTiling meas-
ures the term overlap between adjacent regions in 
the discourse. However, dialogue contributions are 
often terse or even contentless. This produces 
many islands of contribution-sequences for which 
the local lexical coherence is zero. TextTiling 
wrongly classifies all of these as starts of new top-
ics. A heuristic improvement to prevent TextTiling 
from placing topic boundaries at every point along 
a sequence of contributions failed to produce a sta-
tistically significant improvement. 
The Foltz and the Ortho approaches rely on LSA 
to provide strategic semantic generalizations capa-
ble of detecting shifts in topic. Following (Olney 
and Cai, 2005), we built our LSA space using dia-
logue contributions as the atomic text unit.  In cor-
pora such as the Thermo corpus, however, this may 
not be effective due to the brevity of contributions. 
Barzilay and Lee?s algorithm (B&L) did not 
generalize well to either dialogue corpus. One rea-
son could be that probabilistic methods, such as 
their approach, require that reference topics have 
significantly different language models, which was 
not true in either of our evaluation corpora. We 
also noticed a number of instances in the dialogue 
corpora where participants referred to information 
from previous topic segments, which consequently 
may have blurred the distinction between the lan-
guage models assigned to different topics. 
6 Dialogue Exchanges  
Although results are reliably better than our 
baseline algorithms in both corpora, there is much 
room for improvement, especially in the more 
spontaneous Thermo corpus. We believe that an 
improvement can come from a multi-layer segmen-
tation approach, where a first pass segments a dia-
logue into dialogue exchanges and a second classi-
fier assigns topic shifts based on exchange initial 
contributions. Dialogue is hierarchical in nature. 
Topic and topic shift comprise only one of the 
many lenses through which dialogue behaves in 
seemingly structured ways. Thus, it seems logical 
that exploiting more fine-grained sub-parts of dia-
logue than our definition of topic might help us do 
better at predicting shifts in topic.  One such sub-
part of dialogue is the notion of dialogue exchange, 
typically between 2-3 contributions. 
Stubbs (1983) motivates the definition of an ex-
change with the following observation. In theory, 
there is no limit to the number of possible re-
sponses to the clause ?Is Harry at home??. How-
ever, constraints are imposed on the interpretation 
of the contribution that follows it: yes or no. Such a 
constraint is central to the concept of a dialogue 
exchange. Informally, an exchange is made from 
an initiation, for which the possibilities are open-
ended, followed by dialogue contributions that are 
pre-classified and thus increasingly restricted. A 
contribution is part of the next exchange when the 
constraint on its communicative act is lifted.  
Sinclair and Coulthard (1975) introduce a more 
formal definition of exchange with their Initiative-
Response-Feedback or IRF structure. An initiation 
produces a response and a response happens as 
direct consequence to an initiation. Feedback 
serves to close an exchange. Sinclair and Coulthard 
posit that if exchanges constitute the minimal unit 
of interaction, IRF is a primary structure of interac-
tive discourse in general.  
To measure the benefits of exchange boundaries 
in detecting topic shift in dialogue, we coded the 
Thermo corpus with exchanges following Sinclair 
46
and Coulthard?s IRF structure. The coder who la-
beled dialogue exchanges had no knowledge of our 
definition of topic or our intention to do topic-
analyses of the corpus. Any correlation between 
exchange boundaries and topic boundaries is not a 
bias introduced during the hand-labeling process.     
7 Topic Segmentation with Exchanges 
In our corpus, as we believe is true in domain-
general dialogue, knowledge of an exchange-
boundary increases the probability of a topic-
boundary significantly. One way to quantify this 
relation is with the following observation. In our 
experimental Thermo corpus, there are 4794 dia-
logue contributions, 360 topic shifts, and 1074 ex-
change shifts. Using maximum likelihood estima-
tion, the likelihood of being correct if we say that a 
randomly chosen contribution is a topic shift is 
0.075 (# topic shifts / # contributions). However, 
the likelihood of being correct if we have prior 
knowledge that an exchange-shift also occurs in 
that contribution is 0.25. Thus, knowledge that the 
contribution introduces a new exchange increases 
our confidence that it also introduces a new topic. 
More importantly, the probability that a contribu-
tion does not mark a topic shift, given that it does 
not mark an exchange-shift, is 0.98. Thus, ex-
changes show great promise in narrowing the 
search-space of tentative topic shifts. 
In addition to possibly narrowing the space of 
tentative topic-boundaries, exchanges are helpful 
in that they provide more coarse-grain building 
blocks for segmentation algorithms that rely on 
term-distribution as a proxy for dialogue coher-
ence, such as TextTiling (Hearst, 1994, 1997), the 
Foltz algorithm (Foltz, 1998), Orthonormal Basis 
(Olney and Cai, 2005), and Barzilay and Lee?s 
content modeling approach (Barzilay and Lee, 
2004).  At the heart of all these approaches is the 
assumption that a change in term distribution sig-
nals a shift in topic. When applied to dialogue, the 
major weakness of these approaches is that contri-
butions are often times contentless: terse and ab-
sent of thematically meaningful terms. Thus, a 
more coarse-grained discourse unit is needed.  
8 Barzilay and Lee with Exchanges 
Barzilay and Lee (2004) offer an attractive 
frame work for constructing a context-specific 
Hidden Markov Model (HMM) of topic drift. In 
our initial evaluation, we used dialogue contribu-
tions as the atomic discourse unit. Using contribu-
tions, our application of Barzilay and Lee?s algo-
rithm for segmenting dialogue fails at least in part 
because the model learns states that are not the-
matically meaningful, but instead relate to other 
systematic phenomena in dialogue, such as fixed 
expressions and discourse cues. Figure 1 shows the 
cluster (state) size distribution in terms of the per-
centage of the total discourse units (exchanges vs. 
contributions) in the Thermo corpus assigned to 
each cluster. In the horizontal axis, clusters (states) 
are sorted by size from largest to smallest.  
 
% of Total Discourse Units per Cluster
(clusters sorted by size, largest-to-smallest)
0%
10%
20%
30%
40%
50%
60%
70%
80%
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Cluster Rank 
%
 o
f 
D
is
co
u
rs
e 
U
n
it
s 
in
 C
lu
st
er
CONTRIBUTIONS EXCHANGES
 
Figure 1. Exchanges produce a more evenly dis-
tributed cluster size distribution. 
   
The largest cluster contains 70% of all contribu-
tions in the corpus. The second largest cluster only 
generates 10% of the contributions. In contrast, 
when using exchanges as the atomic unit, the clus-
ter size distribution is less skewed and corresponds 
more closely to a topic analysis performed by a 
domain expert.  In this analysis, the number of de-
sired cluster (states), which is an input to the algo-
rithm, was set to 16, the same number identified in 
a domain expert?s analysis of the Thermo corpus. 
Examples of such topics include high-level ones 
such as greeting, setup initialization, and general 
thermo concepts, as well as task-specific ones like 
sensitivity analysis and regeneration. 
A closer examination of the clusters (states) con-
firms our intuition that systematic topic-
independent phenomena in dialogue, coupled with 
the terse nature of contributions in spontaneous 
dialogue, leads to an overly skewed cluster size 
distribution. Examining the terms with the highest 
emission probabilities, the largest states contain 
47
topical terms like cycle, efficiency, increase, qual-
ity, plot, and turbine intermixed with terms like 
think, you, right, make, yeah, fine, and ok. Also the 
sets of topical terms in these larger states do not 
seem coherent with respect to the expert induced 
topics. This suggests that thematically ambiguous 
fixed expressions blur the distinction between the 
different topic-centered language models, produc-
ing an overly heavy-tailed cluster size distribution. 
One might argue that a possible solution to this 
problem would be to remove these fixed expres-
sions as part of pre-processing. However, that re-
quires knowledge of the particular domain and 
knowledge of the interaction style characteristic to 
the context. We believe that a more robust solution 
is to use exchanges as the atomic unit of discourse. 
9 Evaluation with Exchanges 
To show the value of dialogue exchanges in 
topic segmentation, in this section we re-formulate 
our problem from classifying contributions into 
NEW_TOPIC and SAME_TOPIC to classifying 
exchange initial contributions into NEW_TOPIC 
and SAME_TOPIC. For all algorithms, we con-
sider only predictions that coincide with hand-
coded exchange initial contributions. We show 
that, except for our own Museli approach, using 
exchange boundaries improves segmentation qual-
ity across all algorithms (p < .05) when compared 
to their respective counterparts that ignore ex-
changes. Using exchanges gives the Museli ap-
proach a significant advantage based on F-measure 
(p < .05), but only a marginally significant advan-
tage based on Pk. These results confirm our intui-
tion that what gives our Museli approach an advan-
tage over baseline algorithms is its ability to har-
ness the lexical, syntactic, and phrasal cues that 
mark shifts in topic. Given that shift-in-topic corre-
lates highly with shift-in-exchange, these features 
are discriminatory in both respects.  
 Of the degenerate strategies in section 5.2, only 
ALL lends itself to our reformulation of the topic 
segmentation problem. For the ALL heuristic, we 
classify all exchange initial contributions into 
NEW_TOPIC.  This degenerate heuristic alone 
produces better results than all algorithms classify-
ing utterances (Table 4). In our implementation of 
TextTiling (TT) with exchanges, we only consider 
predictions on contributions that coincide with ex-
change initial contributions, while ignoring predic-
tions made on contributions that do not introduce a 
new exchange. Consistent with our evaluation 
methodology from Section 5, we optimized the 
window size using the entire corpus and found an 
optimal window size of 13 contributions. Without 
exchanges, the optimal window size was 6 contri-
butions. The higher optimal window-size hints to 
the possibility that by using exchange initial con-
tributions an approach based on lexical cohesion 
may broaden its horizon without losing precision. 
 
 Thermo Corpus 
(Contributions) 
Thermo Corpus 
(Exchanges) 
 Pk F Pk F 
NONE 0.4900 -- N/A -- 
ALL 0.5100 -- 0.4398 0.3809 
EVEN 0.5132 -- N/A -- 
TT 0.5353 0.1614 0.4328 0.3031 
B&L 0.5086 0.1512 0.3817 0.3840 
Foltz 0.5058 0.1180 0.4242 0.3296 
Ortho 0.4898 0.2111 0.4398 0.3813 
Museli 0.4043 0.3693 0.3737 0.3897 
Table 4. Results using perfect exchange boundaries 
 
In this version of B&L, we use exchanges to 
build the initial clusters (states) and the final 
HMM. B&L with exchanges significantly im-
proves over B&L with contributions, in terms of 
both Pk and F-measure (p < .005) and significantly 
improves over our ALL heuristic (where all ex-
change initial contributions introduce a new topic) 
in terms of Pk (p < .0005). Thus, its use of ex-
changes goes beyond merely narrowing the space 
of possible NEW_TOPIC contributions: it also 
uses these more coarse-grained discourse units to 
build a more thematically-motivated topic model.  
Foltz?s and Olney and Cai?s (Ortho) approach 
both use an LSA space trained on the dialogue 
corpus. Instead of training the LSA space with in-
dividual contributions, we train the LSA space us-
ing exchanges. We hope that by training the space 
with more contentful text units LSA might capture 
more topically-meaningful semantic relations.  In 
addition, only exchange initial contributions where 
used for the logistic regression training phase. 
Thus, we aim to learn the regression equation that 
best discriminates between exchange initial contri-
butions that introduce a topic and those that do not. 
Both Foltz and Ortho improve over their non ex-
change counterparts, but neither improves over the 
ALL heuristic by a significant margin.  
48
For Museli with exchanges, we tried both train-
ing the model using only exchange initial contribu-
tions, and applying our previous model to only ex-
change initial contributions. Training our models 
using only exchange initial contributions produced 
slightly worse results. We believe that the reduc-
tion of the amount of training data prevents our 
models from learning good generalizations. Thus, 
we trained our models using contributions (as in 
Section 5) and consider predictions only on ex-
change initial contributions. The Museli approach 
offers a significant advantage over TT in terms of 
Pk and F-measure. Using perfect-exchanges, it is 
not significantly better than Barzilay and Lee. It is 
significantly better than Foltz?s approach based on 
F-measure and significantly better than Olney and 
Cai based on Pk (p < .05). 
These experiments used hand coded exchange 
boundaries.  We also evaluated our ability to 
automatically predict exchange boundaries.  On the 
Thermo corpus, Museli was able to predict ex-
change boundaries with precision = 0.48, recall = 
0.62, f-measure = 0.53, and Pk = 0.14. 
10 Conclusions and Current Directions 
In this paper we addressed the problem of auto-
matic topic segmentation of spontaneous dialogue.  
We demonstrated with an empirical evaluation that 
state-of-the-art approaches fail on spontaneous dia-
logue because term distribution alone fails to pro-
vide adequate evidence of topic shifts in dialogue.   
We have presented a supervised learning algo-
rithm for topic segmentation of dialogue called 
Museli that combines linguistic features signaling a 
contribution?s function with local context indica-
tors. Our evaluation on two distinct corpora shows 
a significant improvement over the state-of-the-art 
algorithms. We have also demonstrated that a sig-
nificant improvement in performance of state-of-
the-art approaches to topic segmentation can be 
achieved when dialogue exchanges, rather than 
contributions, are used as the basic unit of dis-
course.  We demonstrated promising results in 
automatically identifying exchange boundaries. 
Acknowledgments 
This work was funded by Office of Naval Re-
search, Cognitive and Neural Science Division; 
grant number N00014-05-1-0043. 
 
References  
Regina Barzilay and Lillian Lee. 2004. Catching the 
drift: Probabilistic Content Models, with Applica-
tions to Generation and Summarization. In Proceed-
ings of HLT-NAACL, 113 - 120.  
Doug Beeferman, Adam Berger, John D. Lafferty. 1999.  
Statistical Models for Text Segmentation. Machine 
Learning, 34 (1-3): 177-210. 
Narj?s Boufaden, Guy Lapalme, Yoshua Bengio. 2001. 
Topic Segmentation: A first stage to Dialog-based In-
formation Extraction. In Proceedings of NLPRS.  
Giovanni Flammia. 1998. Discourse Segmentation of 
Spoken Dialogue, PhD Thesis. Massachusetts Insti-
tute of Technology.  
Peter Foltz, Walter Kintsch, and Thomas Landauer. 
1998. The measurement of textual cohesion with 
LSA. Discourse Processes, 25, 285-307. 
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion 
in English. London: Longman. 
Marti Hearst. 1997. TextTiling: Segmenting Text into 
Multi-Paragragh Subtopic Passages. Computational 
Linguistics, 23(1), 33 ? 64.   
Thomas Landauer and Susan Dumais. A Solution to 
Plato?s Problem: The Latent Semantic Analysis of 
Acquisition, Induction, and Representation of 
Knowledge. Psychological Review, 104, 221-240.  
Douglas Oard, Bhuvana Ramabhadran, and Samuel 
Gustman. 2004. Building an Information Retrieval 
Test Collection for Spontaneous Conversational 
Speech. In Proceedings of SIGIR. 
Andrew Olney and Zhiqiang Cai. 2005. An Orthonor-
mal Basis for Topic Segmentation of Tutorial Dia-
logue. In Proceedings of HLT/EMNLP. 971-978. 
Rebecca Passonneau and Diane Litman. 1993. Inten-
tion-Based Segmentation: Human Reliability and 
Correlation with Linguistic Cues. In Proceedings of 
ACL, 148 ? 155.  
John Rotondo, 1984, Clustering Analysis of Subject 
Partitions of Text. Discourse Processes, 7:69-88 
John Sinclair and Malcolm Coulthard. 1975. Towards 
an Analysis of Discourse: the English Used by 
Teachers and Pupils. Oxford University Press.  
Michael Stubbs. 1983. Discourse Analysis. A Sociolin-
guistic Analysis of Natural Language. Basil Black-
well.  
Klaus Zechner. 2001. Automatic Summarization of Spo-
ken Dialogues in Unrestricted Domains. Ph.D. The-
sis. Carnegie Mellon University.
49
