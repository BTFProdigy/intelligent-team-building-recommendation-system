Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 802?806,
Dublin, Ireland, August 23-24, 2014.
UTH_CCB: A Report for SemEval 2014 ? Task 7 Analysis of Clinical Text  
   Yaoyun Zhang1   Jingqi Wang1   Buzhou Tang2   Yonghui Wu1   Min Jiang1              Yukun Chen3  Hua Xu1* 1University of Texas School of Biomedical Informatics at Houston Houston, TX, 77030, USA 
2Harbin Institute of Technology Shenzhen Graduate School Shenzhen, 518055, China 
3Vanderbilt University Department of Biomedical Informatics Nashville, TN, 37240, USA {Yaoyun.Zhang, Yonghui.Wu, Min.Jiang, Hua.Xu} @uth.tmc.edu tangbuzhou@gmail.com yukun.chen@Vanderbilt.Edu      Abstract This work describes the participation of the University of Texas Health Science Center at Houston (UTHealth) team on the SemEval 2014 ? Task 7 analysis of clinical text challenge. The task consisted of two subtasks: (1) disorder entity recognition,  recognizing mentions of disorder concepts; (2) disorder entity encoding, mapping each mention to a unique Concept Unique Identifier (CUI) defined in Unified Medical Language System (UMLS). We developed three ensemble learning approaches for recognizing disorder entities and a Vector Space Model based method for encoding. Our approaches achieved top rank in both subtasks, with the best F measure of 0.813 for entity recognition and the best accuracy of 74.1% for encoding, indicating the proposed approaches are promising.  1 Introduction  In recent years, clinical natural language processing (NLP) has received great attention for its critical role in unlocking information embedded in clinical documents. Leveraging such information can facilitate the secondary1 use of electronic health record (EHR) data to                                                      This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details:http://creativecommons.org/licenses/by/4.0/ 
promote clinical and translational research.  Clinical entity recognition, which recognizes mentions of clinically relevant concepts (e.g., disorders, procedures, drugs etc.) in narratives,   and clinical entity encoding, which maps the recognized entities to concepts in standard vocabularies (e.g., UMLS CUI (Bodenreider, 2004)), are among the fundamental tasks in clinical NLP research. Many systems have been developed to extract clinical concepts from various types of clinical notes in last two decades, ranging from early symbolic NLP systems heavily dependent on domain knowledge to machine learning algorithm based systems driven by increasingly available annotated clinical corpora. The representative systems include MedLEE (Friedman et al., 1994), MetaMap (Aronson and Lang, 2010), KnowledgeMap (Denny et al., 2003), cTAKES (Savova et al., 2010), etc. Clinical NLP challenges organized by the Center for Informatics for Integrating Biology & the Beside (i2b2) have promoted research using machine learning algorithms to recognize clinical entities (Uzuner et al., 2010; Uzuner et al., 2011).  Unlike the previous i2b2 challenges, the ShARe/CLEF challenge of clinical disorder extraction and encoding held in 2013 took the initiative to recognize disjoint entities, in addition to entities made up of consecutive words (Chapman et al., 2013). ShARe/CLEF challenge also required encoding of the disorder entities to Systematized Nomenclature Of Medicine Clinical Terms (SNOMED-CT) (using UMLS CUIs).  
802
In this paper, we describe our system for Task 7 of SemEval 2014, which followed the requirements of 2013 ShARe/CLEF challenge. Our system employed ensemble learning based approaches for disorder entity recognition and a Vector Space Model (VSM) based method for mapping extracted entities to CUIs of SNOMED-CT concepts. Our system was top-ranked among all participating teams according to evaluation by the organizer. 2 Method Our end-to-end system for Task 7 of SemEval 2014 consists of two components: disorder entity recognition and encoding. The raw clinical notes first went through the pre-processing modules for rule-based sentence boundary detection and tokenization. Extracted features were then used to train two machine learning algorithm-based entity recognition models, Conditional Random Fields (CRFs) (Lafferty et al., 2001) and Structural Support Vector Machines (SSVMs) (Tsochantaridis et al., 2005), respectively. These two models were ensembled with MetaMap, a symbolic biomedical NLP system, by three different approaches. Recognized entities were mapped to SNOWMED-CT CUIs in the encoding component. Detailed information of the components are presented in the following sections.  2.1 Dataset The training and test sets of 2013 ShARe/CLEF challenge were used as the training and development sets respectively for system development in SemEval 2014 Task 7. The training set consists of 199 notes and the development set has 99 notes, both of which were collected from four types of clinical notes including discharge summaries (DIS), radiology reports (RAD), and ECG/ECHO reports. Based on a pre-defined guideline, disorder entities were annotated for each note and then mapped to UMLS CUIs of SNOMED-CT concepts. Disorder entities not found in SNOMED-CT were marked as ?CUI-less?. The training set contained 5811 disorder entities which were mapped to 1007 unique CUIs or CUI-less. The development set contained 5340 disorder entities mapped to 795 CUIs or CUI-less. The test set contained 133 notes, all of which were discharge summaries. As the gold-standard annotation of the test set is not released by the organizer, the detailed annotation information of the test set is 
not available. Table 1 shows the total counts of notes, entities and CUIs in the three datasets.   Dataset Type Note Entity CUI CUI-less Train ALL 199 5816 4177 1639 ECHO 42 828 662 166 RAD 42 555 392 163 DIS 61 3589 2646 943 ECG 54 193 103 90 Dev ALL 99 5340 3619 1721 ECHO 12 338 241 97 RAD 12 162 126 36 DIS 75 4840 3252 1588 ECG 0 0 0  Test ALL 133 - -  DIS 133 - -   Table 1. Statistics of the dataset.  2.2 Disorder entity recognition The disorder entity recognition component consists of two modules: 1) the machine learning (e.g., CRF and SSVM) based named entity recognition (NER) module and 2) the   ensemble learning module. For the challenge of this year, we mainly focused on the second ensemble learning module. Machine learning based NER Module. This module was built based on our previous challenge participation in the 2013 ShARe/CLEF challenge (Tang et al., 2013). Annotated data were typically converted into a BIO format in machine learning-based NER systems. Each word was assigned one of the three labels: B for beginning of an entity, I for inside an entity, and O for outside of an entity. A unique challenge of this task is the high frequency (>10%) of disjoint disorders. For example, in the sentence ?the left atrium is not moderately dilated?, the discontinuous phrase ?left atrium?dilated? is defined as a disjoint disorder. Such entities could not be directly represented using the traditional BIO approach. Therefore, in addition to traditional BIO tags used for labeling words in the consecutive disorder entities, two sets of tags were created for disjoint entities: (1) D{B, I} was used to label disjoint entity words that are not shared by multiple concepts; and (2) H{B, I} was used to label head words that belonged to more than two disjoint concepts. Ultimately, we assigned one of the seven labels {B, I, O, DB, DI, HB, HI} to each word. A few simple rules were then defined to convert labeled words to entities (Tang et al., 2013).  
803
We exploited two state-of-the-art machine learning algorithms for disorder entity recognition, namely CRF (Lafferty et al., 2001) and SSVM (Tsochantaridis et al., 2005). CRFsuite and SVMhmm were used to implement CRF and SSVM respectively. For features, we used bag-of-word, part-of-speech from Stanford tagger, type of notes, section information, word representation from Brown clustering (Brown et al., 1992), random indexing (Lund and Burgess, 1996) and semantic categories of words based on UMLS lookup, MetaMap, and cTAKES outputs. More detailed information of this module can be found in our paper for 2013 ShARe/CLEF challenge (Tang et al., 2013).  One thing to note is that for word representation features like Brown clustering and random indexing, we only use the combination of traning and development and test datasets for feature extraction. The non-annotated corpus provided by the SemEval organizers was not employed currently. We do plan to pre-generate word clusters and random indexing using the provided corpus in the near future. Ensemble Learning Module. Three approaches were employed to consolidate the CRF-model, SSVM-model and the MetaMap outputs, namely machine learning classifier based ensemble (ensembleML), majortiy voting based ensemble (ensembleMV) and direct merging of the entity recognition results from the three models (ensembleDM). In the ensembleML approach, a binary classifier was trained to determine if the entities recognized by the CRF-model, SSVM-model and MetaMap were true positives. A new set of features were then extracted for each candidate entity, that included the specific models recognizing the entity, the entity itself, n-gram and word shape features of the first/last word of the entity. A sliding window based feature was extracted to check whether there was any recognized entity within 20 characters before the first and after the last word. Some features extracted from the first module were also employed. We used the open source toolkit Liblinear (Fan et al., 2008), to build the binary classifier for ensembleML.  2.3 Disorder Entity Encoding We developed a Vector Space Model (VSM) based approach to find the most suitable CUI for a given disorder entity. The disorder entity was 
used as query and all the UMLS terms were treated as documents. We used the cosine-similarity score to rank the candidate terms. For post-processing, if the top-ranked CUI was not a disorder CUI, it was replaced with ?CUI-less?.  ?CUI-less? was also assigned to entities without any retrieved candidate CUI. 2.4 Experiments and Evaluation Our system was developed and trained using the enlarged training set by merging the 199 notes in the training set and the 99 notes in the development set. All parameters of CRF, SSVM and Liblinear were optimized by 10-fold cross-validation on the enlarged training dataset. The performance of disorder entity recognition was evaluated by precision, recall and F-measure, which were measured in both ?strict? and ?re-laxed? modes. The ?strict? mode was defined as follows: a concept is correctly recognized if and only if it can be matched exactly to a disorder mention in the gold standard, and the ?relaxed? mode means that a disorder mention is correctly recognized if it overlaps with any disorder men-tion in the gold standard. For entity encoding, all participating systems were evaluated using accu-racy, in ?strict? and ?relaxed? modes, as defined in (Suominen et al., 2013). 3 Results Table 2 and Table 3 show the best performance of our systems in the SemEval 2014 Task 7 as reported by the organizers, where ?P?, ?R?, ?F? denote precision, recall and F-measure respectively. For disorder entity recognition, the ensembleML based system outperformed the other two ensemble approaches, achieving the best F-measure of 0.813 under ?strict? criterion and was ranked first in the challenge. For encoding, our system achieved an accuracy of 0.741 by ensembleDM under ?strict? criterion and was again ranked first in the challenge.   Strict Relaxed P R F P R F ensembleML 84.3 78.6 81.3 93.6 86.6 90.0 Table 2. The disorder recognition performance of our system for the SemEval 2014 task 7 (%).   Accuracy Strict Relaxed ensembleDM 0.741 0.873 Table 3. The SNOMED encoding performance of our system for the SemEval 2014 task 7.  
804
4 Discussion In this study, we developed an ensemble learning-based approach to recognize disorder entities and a vector space model-based method to encode disorders to UMLS CUIs.  Our system was top-ranked among all participating teams. However, there are still expectations for further improvement.  For disorder entity recognition, directly merging the entity recognition results of the three models (ensembleDM) achieved the highest encoding accuracy of 0.741. This shows the great potential of performance enhancement by combining different models. However, the precision of ensembleDM was much lower than the current machine learning-based ensemble approach ensembleML. ensembleML improved the precision to 84.3%, with the lowest recall of 78.6% among the three ensemble approaches. Further investigations for balancing and enhancing both precision and recall simultaneously by combining different models will be pursued in the follow-up studies. For encoding, when a disorder entity can be labelled with multiple CUIs in different contexts, a more effective disambiguation model could be exploited. Further, query expansion techniques may be helpful and worth investigating. The above methods should be potentially helpful to address the problems caused by synonyms or spelling variants.  5 Conclusion We developed a clinical disorder recognition and encoding system that consists of a ensemble learning-based approach to recognize disorder entities and a vector space model-based method to encode the identified disorders to UMLS CUIs of SNOMED-CT concepts. The performance of our system was top-ranked in the SemEval 2014 Task 7, indicating that our approaches are promising. However, further improvements are needed in order to enhance performance on concept extraction and encoding in clinical text. Acknowledgments This study is supported in part by grants from NLM R01LM010681, NCI 1R01CA141307, NIGMS 1R01GM102282 and CPRIT R1307 (H.X).    
Reference Aronson, A. R., & Lang, F.-M. (2010). An overview of MetaMap: historical perspective and recent advances. Journal of the American Medical Informatics Association: JAMIA, 17(3), 229?236.  Bodenreider, O. (2004). The unified medical language system (UMLS): integrating biomedical terminology. Nucleic Acids Research, 32(suppl 1), 267?270. Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra, V. J. D., & Lai, J. C. (1992). Class-Based n-gram Models of Natural Language. Computational Linguistics, 18, 467?479. Denny, J. C., Irani, P. R., Wehbe, F. H., Smithers, J. D., & Spickard, A. (2003). The KnowledgeMap Project: Development of a Concept-Based Medical School Curriculum Database. AMIA Annual Symposium Proceedings, 2003, 195?199. Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., & Lin, C.-J. (2008). LIBLINEAR: A library for large linear classification. The Journal of Machine Learning Research, 9, 1871?1874. Friedman, C., Alderson, P. O., Austin, J. H., Cimino, J. J., & Johnson, S. B. (1994). A general natural-language text processor for clinical radiology. Journal of the American Medical Informatics Association, 1(2), 161?174. Lafferty, J., McCallum, A., & Pereira, F. C. N. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Departmental Papers (CIS).  Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, & Computers, 28(2), 203?208.  Savova, G. K., Masanz, J. J., Ogren, P. V., Zheng, J., Sohn, S., Kipper-Schuler, K. C., & Chute, C. G. (2010). Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. Journal of the American Medical Informatics Association: JAMIA, 17(5), 507?513.  Suominen, H., Salanter?, S., Velupillai, S., Chapman, W. W., Savova, G., & Elhadad, N. (2013). Overview of the ShARe/CLEF eHealth Evaluation Lab 2013. Information Access Evaluation. Multilinguality, Multimodality, and Visualization, 2013, 212?231. Tang, B., Cao, H., Wu, Y., Jiang, M., & Xu, H. (2013). Recognizing and Encoding Discorder Concepts in Clinical Text using Machine Learning and Vector Space Model. Workshop of ShARe/CLEF eHealth Evaluation Lab 2013.  
805
Tsochantaridis, I., Joachims, T., Hofmann, T., & Altun, Y. (2005). Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6, 1453?1484. Uzuner, ?., Solti, I., & Cadag, E. (2010). Extracting medication information from clinical text. Journal of the American Medical Informatics Association, 17(5), 514?518.  Uzuner, ?., South, B. R., Shen, S., & DuVall, S. L. (2011). 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association: JAMIA, 18(5), 552?556.    
806
Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 1?8
Manchester, August 2008
Semantic Chunk Annotation for complex questions using Conditional 
Random Field 
Shixi Fan 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
fanshixi@hit.edu.cn 
Yaoyun Zhang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
Xiaoni5122@gmail.com 
Wing W. Y. Ng 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wing@hitsz.edu.cn 
Xuan Wang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wangxuan@insun.hit.edu.cn 
Xiaolong Wang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wangxl@insun.hit.edu.cn 
 
 
Abstract 
This paper presents a CRF (Conditional 
Random Field) model for Semantic 
Chunk Annotation in a Chinese Question 
and Answering System (SCACQA). The 
model was derived from a corpus of real 
world questions, which are collected 
from some discussion groups on the 
Internet. The questions are supposed to 
be answered by other people, so some of 
the questions are very complex. Mutual 
information was adopted for feature se-
lection.  The training data collection con-
sists of 14000 sentences and the testing 
data collection consists of 4000 sentences. 
The result shows an F-score of 93.07%. 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
1 Introduction 
1.1 Introduction of Q&A System 
Automated question answering has been a hot 
topic of research and development since the ear-
liest AI applications (A.M. Turing, 1950). Since 
then there has been a continual interest in proc-
essing knowledge and retrieving it efficiently to 
users automatically. The end of the 1980s saw a 
boost in information retrieval technologies and 
applications, with an unprecedented growth in 
the amount of digital information available, an 
explosion of growth in the use of computers for 
communications, and the increasing number of 
users that have access to all this information 
(Diego Moll?and Jose?Luis Vicedo, 2007).  
Search engines such as Google, Yahoo, Baidu 
and etc have made a great success for people?s 
information need. 
Anyhow, search engines are keywords-based 
which can only return links of relevant web 
pages, failing to provide a friendly user-interface 
with queries expressed in natural language sen-
tences or questions, or to return precise answers 
to users. Especially from the end of the 1990s, as 
1
information retrieval technologies and method-
ologies became mature and grew more slowly in 
pace, automated question answering(Q&A) sys-
tems which accept questions in free natural lan-
guage formations and return exactly the answer 
or a short paragraph containing relevant informa-
tion has become an urgent necessity. Major in-
ternational evaluations such as TREC, CLEF and 
NTCIR have attracted the participation of many 
powerful systems.  
The architecture of a Q&A system generally in-
cludes three modules: question processing, can-
didate answer/document retrieval, and answer 
extraction and re-ranking.      
1.2 Introduction of Question Analyzing      
Question Analyzing, as the premise and founda-
tion of the latter two modules, is of paramount 
importance to the integrated performance of a 
Q&A system. The reason is quite intuitive: a 
question contains all the information to retrieve 
the corresponding answer. Misinterpretation or 
too much loss of information during the process-
ing will inevitably lead to poor precision of the 
system. 
The early research efforts and evaluations in 
Q&A were focused mainly on factoid questions 
asking for named entities, such as time, numbers, 
and locations and so on. The questions in the test 
corpus of TREC and other organizations are also 
in short and simple form. Complex hierarchy in 
question types (Dragomir Radev et al 2001), 
question templates (Min-Yuh Day et al 2005), 
question parsing (Ulf Hermjakob, 2001) and 
various machine learning methods (Dell Zhang 
and Wee Sun Lee, 2003)are used for factoid 
question analysis, aiming to find what named 
entity is asked in the question. There are some 
questions which are very complicated or even 
need domain restricted knowledge and reasoning 
technique. Automatic Q&A system can not deal 
with such questions with current technique.    
In china, there is a new kind of web based Q&A 
system which is a special kind of discussion 
group. Unlike common discussion group, in the 
web based Q&A system one user posts a ques-
tion, other users can give answers to it. It is 
found that at least 50% percent questions 
(Valentin Jijkoun and Maarten de Rijke, 
2005)posted by users are non-factoid and surely 
more complicated both in question pattern and 
information need than those questions in the test 
set of TREC and other FAQ.  An example is as 
follows: 
 
This kind of Q&A system can complement the 
search engines effectively.  As the best search 
engines in china, Baidu open the Baidu Knowl-
edge2 Q&A system from 2003, and now it has 
more than 29 million question-answer pairs. 
There are also many other systems of this kind 
such as Google Groups, Yahoo Answers and 
Sina Knowledge3. This kind of system is a big 
question-answer pair database which can be 
treated as a FAQ database. How to search from 
the database and how to analyze the questions in 
the database needs new methods and techniques.   
More deeper and precise capture of the semantics 
in those complex questions is required. This phe-
nomenon has also been noticed by some re-
searchers and organizations. The spotlight gradu-
ally shifted to the processing and semantic un-
derstanding of complex questions. From 2006, 
TREC launched a new annually evaluation 
CIQ&A (complex, interactive Question Answer-
ing), aiming to promote the development of in-
teractive systems capable of addressing complex 
information needs. The targets of national pro-
grams AQUAINT and QUETAL are all at new 
interface and new enhancements to current state-
of-the-art Q&A systems to handle more complex 
inputs and situations. 
A few researchers and institutions serve as pio-
neers in complex questions study. Different tech-
nologies, such as definitions of different sets of 
question types, templates and sentence patterns 
(Noriko Tomuro, 2003) (Hyo-Jung Oh et al 
2005) machine learning methods (Radu Soricut 
and Eric Brill, 2004), language translation model 
(Jiwoon Jeon, W et al 2005), composition of 
information needs of the complex question 
(Sanda Harabagiu et al 2006) and so on, have 
been experimented on the processing of complex 
question, gearing the acquired information to the 
facility of other Q&A modules.  
Several major problems faced now by researcher 
of complex questions are stated as follow:  
First: Unlike factoid questions, it is very dif-
ficult to define a comprehensive type hierarchy 
for complex questions. Different domains under 
research may require definitions of different sets 
of question types, as shown in (Hyo-Jung Oh et 
al, 2005). Especially, the types of certain ques-
                                                 
2 http://zhidao.baidu.com/ 
3 http://iask.sina.com.cn/ 
2
tions are ambiguous and hard to identify. For 
example: 
 
This question type can be treated as definition, 
procedure or entity. 
Second: Lack of recognition of different seman-
tic chunks and the relations between them. 
FAQFinder (Radu Soricut and Eric Brill, 2004) 
also used semantic measure to credit the similar-
ity between different questions. Nevertheless, the 
question similarity is only a simple summation of 
the semantic similarity between words from the 
two question sentences. Question pattern are very 
useful and easy to implement, as justified by pre-
vious work. However, just like the problem with 
question types, question patterns have limitation 
on the coverage of all the variations of complex 
question formation. Currently, after the question 
processing step in most systems, the semantic 
meaning of large part of complex questions still 
remain vague. Besides, confining user?s input 
only within the selection of provided pattern may 
lead to unfriendly and unwelcome user interface. 
(Ingrid Zukerman and Eric Horvitz, 2001) used 
decision tree to model and recognize the infor-
mation need, question and answer coverage, 
topic, focus and restrictions of a question. Al-
though features employed in the experiments 
were described in detail, no selection process of 
those feature, or comparison between them was 
mentioned. 
This paper presents a general method for Chinese 
question analyzing. Our goal is to annotate the 
semantic chunks for the question automatically.  
2 Semantic Chunk Annotation 
Chinese language differs a lot from English in 
many aspects. Mature methodologies and fea-
tures well-justified in English Q&A systems are 
valuable sources of reference, but no direct copy 
is possible.  
The Ask-Answer system 4  is a Chinese online 
Q&A system where people can ask and answer 
questions like other web based Q&A system. The 
characteristic of this system is that it can give the 
answer automatically by searching from the 
asked question database when a new question is 
presented by people. The architecture of the 
automatically answer system is shown in figure 1.  
The system contains a list of question-answer 
pairs on particular subject. When users input a 
                                                 
 
 
 
4 http://haitianyuan.com/qa 
question from the web pages, the question is 
submitted to the system and then question-
answer pair is returned by searching from the 
questions asked before. The system includes four 
main parts: question pre-processing, question 
analyzing, searching and answer getting.  
The question pre-processing part will segment 
the input questions into words, label POS tags 
for every word.  Sometimes people ask two or 
more questions at one time, the questions should 
be made into simple forms by conjunctive struc-
ture detection. The question analyzing program 
will find out the question type, topic, focus and 
etc. The answer getting part will get the answer 
by computing the similarity between the input 
question and the questions asked before. The 
question analyzing part annotates the semantic 
chunks for the question. So that the question can 
be mapped into semantic space and the question 
similarity can be computed semantically. The 
Semantic chunk annotation is the most important 
part of the system. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Question Pre- processing 
 
Segmentation and  pos 
tagging 
Detect conjunctive structure  
Question Analyzing 
Semantic chunk annotationGet and extend key words
Question pattern and knowledge base 
Search reference question-answer pairs form database 
Answer getting 
Score the constituent 
answers 
Out put the top 
five answers 
 
Figure 1 the architecture of the automatically 
answer system 
Currently, no work has been reported yet on the 
question semantic chunk annotation in Chinese. 
The prosperity of major on-line discussion 
groups provides an abundant ready corpus for 
question answering research. Using questions 
collected from on-line discussion groups; we 
make a deep research on semantic meanings and 
build a question semantic chunk annotation 
model based on Conditional Random Field. 
Five types of semantic chunks were defined: 
Topic, Focus, Restriction, Rubbish information 
and Interrogative information. The topic of a 
3
question which is the topic or subject asked is the 
most important semantic chunk. The focus of a 
question is the asking point of the question. The 
restriction information can restrict the question?s 
information need and the answers. The rubbish 
information is those words in the question that 
has no semantic meanings for the question. Inter-
rogative information is a semantic tag set which 
corresponds to the question type. The interroga-
tive information includes interrogative words, 
some special verbs and nouns words and all these 
words together determine the question type. The 
semantic chunk information is shown in table 1.  
 
Semantic 
chunk   tag 
Abbreviation Meaning 
Topic T The question subject 
Focus F The additional information 
of topic 
Restrict 
 
Re Such as Time restriction and 
location restriction 
Rubbish 
information 
Ru Words no meaning for the 
question 
Other O other information without 
semantic meaning 
The following is interrogative information 
Quantity Wqua  
Description Wdes The answer need description
Yes/No Wyes The answer should be yes or 
no 
List Wlis The answer should be a list 
of entity 
Definition Wdef The answer is the definition 
of topic 
Location Wloc The answer is location 
Reason Wrea The answer can explain the 
question 
Contrast Wcon The answer is the compari-
son of the items proposed in 
the question 
People Wwho The answer is about the 
people?s information 
Choice Wcho The answer is one of the 
choice proposed in the ques-
tion 
Time Wtim The answer is the data or 
time length about the event 
in the question 
Entity Went The answer is the attribute 
of the topic. 
Table 1: Semantic chunks  
An annotation example question is as follows: 
 
This question can be annotated as follows: 
 
This kind of annotation is not convenient for CRF 
model, so the tags were transfer into the B I O 
form. (Shown as follows) 
 
Then the Semantic chunk annotation can be 
treated as a sequence tag problem.  
3 Semantic Chunk Annotation model 
3.1 Overview of the CRF model 
The conditional random field (CRF) is a dis-
criminative probabilistic model proposed by John 
Lafferty, et al(2001) to overcome the long-range 
dependencies problems associated with genera-
tive models. CRF was originally designed to la-
bel and segment sequences of observations, but 
can be used more generally. Let X, Y be random 
variables over observed data sequences and cor-
responding label sequences, respectively. For 
simplicity of descriptions, we assume that the 
random variable sequences X and Y have the 
same length, and use [ ]mxxxx ......, 21=   
and [ ]myyyy ......, 21=  to represent instances of 
X and Y, respectively. CRF defines the condi-
tional probability distribution P(Y |X) of label 
sequences given observation sequences as fol-
lows 
)),(exp(
)(
1
)|(
1
?
=
=
n
i
ii YXfXZ
XYP ?
?
?    (1) 
Where  is the normalizing factor that 
ensures equation 2. 
)(XZ?
 ? =y xyP 1)|(?                   (2) 
In equation 2 the i? is a model parameter and 
 is a feature function (often binary-
valued) that becomes positive (one for binary-
valued feature function) when X contains a cer-
tain feature in a certain position and Y takes a 
certain label, and becomes zero otherwise. 
Unlike Maximum Entropy model which use sin-
gle normalization constant to yield a joint distri-
bution, CRFs use the observation-dependent 
normalization  for conditional distribu-
tions. So CRFs can avoid the label biased prob-
lem. Given a set of training data 
),( YXfi
)(XZ?
}....2,1),,{( nkyxT kk ==  
 With an empirical distribution , CRF ),(
~
YXP
4
determines the model parameters }{ i?? =  by 
maximizing the log-likelihood of the training set 
)|(log),(
)|(log)(
,
~
1
xyPyxP
xyPP
yx
N
k
kk
?
??
?
?
?
=?
=                       (3) 
3.2 Features for the model 
The following features, which are used for train-
ing the CRF model, are selected according to the 
empirical observation and some semantic mean-
ings. These features are listed in the following 
table. 
 
Feature type in-
dex 
Feature type name 
1 Current word 
2 Current POS tag 
3 Pre-1 word POS tag 
4 Pre-2 word POS tag 
5 Post -1 word POS tag 
6 Post -2 word POS tag 
7 Question pattern 
8 Question type 
9 Is pattern key word 
10 Pattern tag 
Table 2: the Features for the model 
Current word: 
The current word should be considered when 
adding semantic tag for it. But there are too 
many words in Chinese language and only part 
of them will contribute to the performance, a set 
of words was selected. The word set includes 
segment note and some key words such as time 
key word and rubbish key word. When the cur-
rent word is in the word set the current word fea-
ture is the current word itself, and null on the 
other hand. 
Current POS tag: 
Current POS tag is the part of speech tag for the 
current word. 
Pre-1 word POS tag: 
Pre- 1 word POS tag is the POS tag of the first 
word before the labeling word in the sentence. If 
the Pre-1 word does not exit (the current is the 
first word in the sentence), the Pre- 1 word POS 
tag is set to null. 
Pre-2 word POS tag: 
Pre- 2 word POS tag is the POS tag of the second 
word before the labeling word in the sentence. If 
the Pre-2 word does not exit, the Pre- 2 word 
POS tag is set to null. 
Post -1 word POS tag: 
Post - 1 word POS tag is the POS tag of the first 
word after the labeling word in the sentence. If 
the Post -1 word does not exit (the current is the 
first word in the sentence), the Post - 1 word POS 
tag is set to null. 
Post -2 word POS tag: 
Post - 2 word POS tag is the POS tag of the sec-
ond word after the labeling word in the sentence. 
If the Post-2 word does not exit, the Pre- 2 word 
POS tag is set to null. 
Question pattern: 
Question pattern which is associated with ques-
tion type, can locate question topic, question fo-
cus by surface string matching. For example, 
(where is <topic>). The patterns are extracted 
from the training data automatically. When a pat-
tern is matched, it is treated as a feature. There 
are 1083 question patterns collected manually.  
Question type: 
Question type is an important feature for ques-
tion analyzing. The question patterns have the 
ability of deciding the question type. If there is 
no question pattern matching the question, the 
question type is defined by a decision tree algo-
rithm. 
Is pattern key word: 
For each question pattern, there are some key 
words. When the current word belongs to the 
pattern key word this feature is set to ?yes?, else 
it is set to ?no?. 
Pattern tag: 
When a pattern is matched, the topic, focus and 
restriction can be identified by the pattern. We 
can give out the tags for the question and the tags 
are treated as features. If there is no pattern is 
matched, the feature is set to null.   
4 Feature Selection experiment 
Feature selection is important in classifying sys-
tems such as neural networks (NNs), Maximum 
Entropy, Conditional Random Field and etc. The 
problem of feature selection has been tackled by 
many researchers. Principal component analysis 
(PCA) method and Rough Set Method are often 
used for feature selection. Recent years, mutual 
information has received more attention for fea-
ture selection problem.  
According to the information theory, the uncer-
tainty of a random variable X can be measured 
by its entropy . For a classifying problem, 
there are class label set represented by C and fea-
ture set represented by F. The conditional en-
tropy  measures the uncertainty about 
)(XH
)|( FCH
5
C when F is known, and the Mutual information 
I(C, F) is defined as:  
 F)|(C -(C));( HHFCI =                   (4) 
The feature set is known; so that the objective of 
training the model is to minimize the conditional 
entropy   equally maximize the mutual 
information . In the feature set F, some 
features are irrelevant or redundant. So that the 
goal of a feature selection problem is to find a 
feature S ( ), which achieve the higher 
values of . The set S is a subset of F and 
its size should be as small as possible. There are 
some algorithms for feature selection problem. 
The ideal greedy selection algorithm using mu-
tual information is realized as follows (Nojun 
Kwak and Chong-Ho Choi, 2002): 
)|( FCH
);( FCI
FS ?
);( FCI
 Input:   S- an empty set 
             F- The selected feature set 
Output:  a small reduced feature set S which is 
equivalent to F 
Step 1: calculate the MI with the Class 
set C , , compute  Ffi ?? );( ifCI
Step 2: select the feature that maximizes , 
set  
);( ifCI
}{},{\ ii fSfFF ??
Step 3: repeat until desired number of features 
are selected. 
1) Calculate the MI with the Class set C and S, 
Ffi ?? , compute  ),;( ifSCI
2) Select the feature that maximizes , 
set 
),;( ifSCI
}{},{\ ii fSfFF ??  
Step 4: Output the set S  that contains the se-
lected features 
To calculate MI the PDFs (Probability Distribu-
tion Functions) are required. When features and 
classing types are dispersing, the probability can 
be calculated statistically.  In our system, the 
PDFs are got from the training corpus statistically. 
The training corpus contains 14000 sentences. 
The training corpus was divided into 10 parts, 
with each part 1400 sentences.  And each part is 
divided into working set and checking set. The 
working set, which contains 90% percent data, 
was used to select feature by MI algorithm. The 
checking set, which contains 10% percent data, 
was used to test the performance of the selected 
feature sequence. When the feature sequence was 
selected by the MI algorithm, a sequence of CRF 
models was trained by adding one feature at each 
time. The checking data was used to test the per-
formance of these models.  
 The open test result 
Selected feature 
sequence 
1 2 3 4 5 6 7 8 9 10 
7, 10, 3, 1, 5, 2, 
4, 6, 8?9 
0.5104 0.8764 0.8864 0.8918 0.8925 0.8977 0.8992 0.9023 0.9025 0.9018 
7, 10, 1, 3, 5, 2, 
4?6?8?9 
0.5241 0.8775 0.8822 0.8911 0.8926 0.8956 0.8967 0.9010 0.9005 0.9007 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5090 0.8691 0.8748 0.8851 0.8852 0.8914 0.8929 0.8955 0.8955 0.8949 
7, 10, 1, 3, 5, 2, 
4, 6?9?8 
0.5157 0.8769 0.8823 0.8913 0.8925 0.8978 0.8985 0.9017 0.9018 0.9010 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5144 0.8821 0.8856 0.8921 0.8931 0.8972 0.8981 0.9010 0.9009 0.9007 
7, 10, 3, 1, 5, 2, 
4?6?8?9 
0.5086 0.8795 0.8876 0.8914 0.8919 0.8960 0.8967 0.9016 0.9013 0.9011 
7, 10, 1, 3, 5, 2, 
4, 6, 8, 9 
0.5202 0.8811 0.8850 0.8920 0.8931 0.8977 0.8980 0.9015 0.9013 0.9009 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5015 0.8858 0.8879 0.8948 0.8942 0.8998 0.8992 0.9033 0.9027 0.9023 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5179 0.8806 0.8805 0.8898 0.8908 0.8954 0.8958 0.8982 0.8982 0.8986 
7, 10, 1, 3, 5, 2, 
4, 6, 8?9 
0.5153 0.8921 0.8931 0.9006 0.9012 0.9041 0.9039 0.9071 0.9068 0.9067 
Table 3: the feature selection result and the test result 
In table 3, each row contains data corresponding 
to one part of the training corpus so there are ten 
rows with data in the table. The third row corre-
sponds to the first part and the last row corre-
sponds to the tenth part. There are eleven col-
umns in the table, the first columns is the fea-
tures sequence selected by the mutual informa-
tion algorithm for each part. The second column 
is the open test result with the first feature in the 
feature sequence. The third column is the open 
test result with the first two features in the fea-
ture sequence and so on. From the table, it is 
6
clear that the feature 7(Question pattern) and 
10(Pattern tag) are very important, while the fea-
ture 8(Question type) and 9(Is pattern key word) 
are not necessary. The explanation about this 
phenomenon is that the ?pattern key word? and 
?Question type? information can be covered by 
the Question patterns. So feature 8 and 9 are not 
used in the Conditional Random Field model. 
5 Semantic Chunk Annotation Experi-
ment 
The test and training data used in our system are 
collected from the website (Baidu knowledge 
and the Ask-Answer system), where people pro-
posed questions and answers. The training data 
consists of 14000 and the test data consists of 
4000 sentences. The data set consists of word 
tokens, POS and semantic chunk tags. The POS 
and semantic tags are assigned to each word to-
kens.  
The performance is measured with three rates: 
precision (Pre), recall (Rec) and F-score (F1). 
Pre = Match/Model                     (5) 
Rec=Match/Manual                    (6) 
F1=2*Pre*Rec/(Pre+Rec)              (7) 
Match is the count of the tags that was predicted 
right. Model is the count of the tags that was pre-
dicted by the model. Manual is the count of the 
tags that was labeled manually. 
Table 4 shows the performance of annotation of 
different semantic chunk types. The first column 
is the semantic chunk tag. The last three columns 
are precision, recall and F1 value of the semantic 
chunk performance, respectively.   
 
Label Manual Model Match Pre.() Rec.() F1 
B-T?I-T 17061?78462 16327?80488 14825?76461 90.80?95.00 86.89?97.45 88.80?96.21 
B-F?I-F 5072?13029 5079?13583 4657?12259 91.69?90.25 91.82?94.09 91.75?92.13 
B-Ru?I-Ru 775?30 11?0 2?0 18.18?0.00 0.26?0.00 0.51?0.00 
O 8354 8459 6676 78.92 79.91 79.41 
B-Wqua?I-Wqua 1363?934 1327?1028 1298?881 97.81?85.70 95.23?94.33 96.51?89.81 
B-Wyes?I-Wyes 5669?1162 5702?1098 5550?1083 97.33?98.63 97.90?93.20 97.62?95.84 
B-Wdes?I-Wdes 2907?278 2855?185 2779?184 97.34?99.46 95.60?66.19 96.46?79.48 
B-Wlis?I-Wlis 603?257 563?248 560?248 99.47?100 92.87?96.50 96.05?98.22 
B-Wdef?I-Wdef 1420?1813 1430?1878 1280?1695 89.51?90.26 90.14?93.49 89.82?91.85 
B-Wloc?I-Wloc 683?431 665?395 661?392 99.40?99.24 96.78?90.95 98.07?94.92 
B-Wrea?I-Wrea 902?159 873?83 843?82 96.56?98.80 93.46?51.57 94.99?67.77 
B-Wcon?I-Wcon 552?317 515?344 503?291 97.67?84.59 91.12?91.80 94.28?88.05 
B-Wwho?I-Wwho 420?364 357?350 348?336 97.48?96.00 82.86?92.31 89.58?94.12 
B-Wcho?I-Wcho 857?85 738?0 686?0 92.95?0.00 80.05?0.00 86.02?0.00 
B-Wtim?I-Wtim 408?427 401?419 355?380 88.53?90.69 87.01?88.99 87.76?89.83 
B-Went?I-Went 284?150 95?81 93?80 97.89?98.77 32.75?53.33 49.08?69.26 
Avg 145577 145577 135488 93.07 93.07 93.07 
Table 4: the performance of different semantic chunk
 
The semantic chunk type of ?Topic? and ?Focus? 
can be annotated well. Topic and focus semantic 
chunks have a large percentage in all the seman-
tic chunks and they are important for question 
analyzing. So the result is really good for the 
whole Q&A system. 
As for ?Rubbish? semantic chunk, it only has 
0.51 and 0.0 F1 measure for B-Ru and I-Ru. One 
reason is lacking enough training examples, for 
there are only 1031 occurrences in the training 
data. Another reason is sometimes restriction is 
complex. 
6 Conclusion and future work 
This paper present a new method for Chinese 
question analyzing based on CRF. The features 
are selected by using mutual information algo-
rithm. The selected features work effectively for 
the CRF model. The experiments on the test data 
set achieve 93.07% in F1 measure. In the future, 
new features should be discovered and new 
methods will be used.  
Acknowledgment  
This work is supported by Major Program of Na-
tional Natural Science Foundation of China 
(No.60435020 and No. 90612005) and the High 
Technology Research and Development Program 
of China (2006AA01Z197). 
 
References 
A.M. Turing. 1950. Computing Machinery and 
Intelligence. Mind, 236 (59): 433~460. 
Diego Moll?, Jose?Luis Vicedo. 2007. Question 
Answering in Restricted Domains: An Overview. 
Computational Linguistics, 33(1),  
7
Dragomir Radev, WeiGuo Fan, Leila Kosseim. 2001. 
The QUANTUM Question Answering System. 
TREC. 
Min-Yuh Day, Cheng-Wei Lee, Shih-Hung WU, 
Chormg-Shyong Ong,  Wen-Lian Hsu. 2005. An 
Integrated Knowledge-based and Machine 
Learning Approach for Chinese Question 
Classification. Proceedings of the IEEE 
International Conference on Natural Language 
Processing and Knowledge Engineering, Wuhan, 
China,:620~625. 
Ulf Hermjakob. 2001. Parsing and Question 
Classification for Question Answering.  
Proceedings of the ACL Workshop on Open-
Domain Question Answering, Toulouse,:19~25. 
Dell Zhang, Wee Sun Lee. 2003. Question 
classification using support vector machines. 
Proceedings of the 26th Annual International ACM 
Conference on Research and Development in 
Information Retrieval(SIGIR), Toronto, Canada,26 
~ 32. 
Valentin Jijkoun, Maarten de Rijke.2005. Retrieving 
Answers from Frequently Asked Questions Pages 
on the Web. CIKM?05, Bermen, Germany. 
Noriko Tomuro. 2003. Interrogative Reformulation 
Patterns and Acquisition of Question Paraphrases. 
Proceeding of the Second International Workshop 
on Paraphrasing, :33~40. 
Hyo-Jung Oh, Chung-Hee Lee, Hyeon-Jin Kim, 
Myung-Gil Jang. 2005. Descriptive Question 
Answering in Encyclopedia. Proceedings of the 
ACL Interactive Poster and Demonstration Sessions, 
pages 21?24, Ann Arbor. 
Radu Soricut, Eric Brill. 2004, Automatic Question 
Answering: Beyond the Factoid.  Proceedings of 
HLT-NAACL ,:57~64. 
Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005. 
Finding Similar Questions in Large Question and 
Answer Archives. CIKM?05, Bremen, Germany. 
Sanda Harabagiu, Finley Lacatusu and Andrew Hickl. 
2006 . Answering Complex Questions with Random 
Walk Models. SIGIR?06, Seattle, Washington, 
USA.pp220-227. 
Ingrid Zukerman, Eric Horvitz. 2001. Using Machine 
Learning Techniques to Interpret WH-questions. 
ACL. 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. Conditional Random Fields: probabilistic 
Models for Segmenting and Labeling Sequence 
Data. Proceedings of the Eighteenth International 
Conference on Machine Learning, p.282-289. 
Nojun Kwak and Chong-Ho Choi. 2002. Input 
feature selection for classification problems. 
IEEE Trans on Neural Networks,,13(1):143-
159 
 
8
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 115?122,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
A Hybrid Model for Grammatical Error Correction 
 
 
Yang Xiang, Bo Yuan, Yaoyun Zhang*, Xiaolong Wang?, 
Wen Zheng, Chongqiang Wei 
Intelligent Computing Research Center, Key Laboratory of Network Oriented Intelligent 
Computation, Computer Science and technology Department, 
Harbin Institute of Technology Shenzhen Graduate School, 
Shenzhen, Guangdong, 518055, P.R. China 
{windseedxy, yuanbo.hitsz, xiaoni5122, zhengwen379, weichongqiang}@gmail.com  
wangxl@insun.hit.edu.cn? 
 
 
 
Abstract 
This paper presents a hybrid model for the 
CoNLL-2013 shared task which focuses on the 
problem of grammatical error correction. This 
year?s task includes determiner, preposition, 
noun number, verb form, and subject-verb 
agreement errors which is more comprehen-
sive than previous error correction tasks. We 
correct these five types of errors in different 
modules where either machine learning based 
or rule-based methods are applied. Pre-
processing and post-processing procedures are 
employed to keep idiomatic phrases from be-
ing corrected. We achieved precision of 
35.65%, recall of 16.56%, F1 of 22.61% in the 
official evaluation and precision of 41.75%, 
recall of 20.29%, F1 of 27.3% in the revised 
version. Some further comparisons employing 
different strategies are made in our experi-
ments.   
1 Introduction 
Automatic Grammatical Error Correction (GEC) 
for non-native English language learners has at-
tracted more and more attention with the devel-
opment of natural language processing, machine 
learning and big-data techniques. ?The CoNLL-
2013 shared task focuses on the problem of GEC 
in five different error types including determiner, 
preposition, noun number, verb form, and sub-
ject-verb agreement which is more complicated 
and challenging than previous correction tasks. 
Other than most previous works which concen-
trate most on determiner and preposition errors, 
more error types introduces the possibility of 
correcting multiple interacting errors such as de-
                                                 
? Corresponding author 
terminer vs. noun number and preposition vs. 
verb form. 
Generally, for GEC on annotated data such as 
the NUCLE corpus (Dahlmeier et al, 2013) in 
this year?s shared task which contains both origi-
nal errors and human annotations, there are two 
main types of approaches. One of them is the 
employment of external language materials. Alt-
hough there are minor differences on strategies, 
the main idea of this approach is to use frequen-
cies as a filter, such as n-gram counts, and take 
those phrases that have relatively high frequen-
cies as the correct ones. Typical works are shown 
in (Yi et al, 2008) and (Bergsma et al, 2009). 
Similar methods also exist in HOO shared tasks1 
such as the web 1TB n-gram features used by 
Dahlmeier and Ng (2012a) and the large-scale n-
gram model described by Heilman et al (2012). 
The other type is machine learning based ap-
proach which considers most on local context 
including syntactic and semantic features. Han et 
al. (2006) take maximum entropy as their classi-
fier and apply some simple parameter tuning 
methods. Felice and Pulman (2008) present their 
classifier-based models together with a few rep-
resentative features. Seo et al (2012) invite a 
meta-learning approach and show its effective-
ness. Dahlmeier and Ng (2011) introduce an al-
ternating structure optimization based approach. 
Most of the works mentioned above focus on 
determiner and preposition errors. Besides, Lee 
and Seneff (2008) propose a method to correct 
verb form errors through combining the features 
of parse trees and n-gram counts. To our 
knowledge, no one focused on noun form errors 
in specific researches. 
In this paper, we propose a hybrid model to 
solve the problem of GEC for five error types. 
                                                 
1 http://clt.mq.edu.au/research/projects/hoo/hoo2012 
115
Machine learning based methods are applied to 
solve determiner (ArtOrDet), preposition (Prep) 
and noun form (Nn) problems while rule-based 
methods are proposed for subject-verb agreement 
(SVA) and verb form (Vform) problems. We 
treat corrections of errors in each type as indi-
vidual sub problems the results of which are 
combined through a result combination module. 
Solutions on interacting error corrections were 
considered originally but dropped at last because 
of the bad effects brought about by them such as 
the accumulation of errors which lead to a very 
low performance. We perform feature selection 
and confidence tuning in machine learning based 
modules which contribute a lot to our perfor-
mance. Also, pre-processing and post-processing 
procedures are employed to keep idiomatic 
phrases from being corrected.  
Through experiments, we found that the result 
of the system was affected by many factors such 
as the selection of training samples and features, 
and the settings of confidence parameters in clas-
sifiers. Some of the factors make the whole sys-
tem too sensitive that it can easily be trapped into 
a local optimum. Some comparisons are shown 
in our experiments section. 
No other external language materials are in-
cluded in our model except for several NLP tools 
which will be introduced in ?5.2. We achieved 
precision of 35.65%, recall of 16.56% and F1 of 
22.61% in the official score of our submitted re-
sult. However, it was far from satisfactory main-
ly due to the ill settings of confidence parameters. 
Trying to find out a set of optimal confidence 
parameters, our model is able to reach an upper 
bound of precision of 34.23%, recall of 25.56% 
and F1 of 29.27% on the official test set. For the 
revised version, we achieved precision of 
41.75%, recall of 20.29%, and F1 of 27.3%. 
The remainder of this paper is arranged as fol-
lows. The next section introduces our system 
architecture. Section 3 describes machine learn-
ing based modules. Section 4 shows rule based 
modules. Experiments and analysis are arranged 
in Section 5. Finally, we give our discussion and 
conclusion in Section 6 and 7. 
2 System Architecture 
Initially, we treat errors of each type as individu-
al sub problems. Machine learning based meth-
ods are applied to solve ArtOrDet, Prep and Nn 
problems where similar problem solving steps 
are shared: sample generation, feature extraction, 
training, confidence tuning in development data, 
and testing. We apply some hand-crafted heuris-
tic rules in solving subject-verb agreement (SVA) 
and verb form (Vform) problems. Finally, results 
from different modules are combined together. 
The whole architecture of this GEC system is 
described in Figure 1. 
A pre-processing and a post-processing filter 
are utilized which include filters for some idio-
matic phrases extracted from the training dataset. 
The Frequent Pattern Growth Algorithm (FP-
Growth) is widely used for frequent pattern min-
ing in machine learning. In pre-processing, we 
firstly apply FP-Growth to gather the frequent 
items in the training set. Through some manual 
refinements, a few idiomatic phrases are re-
moved from the candidate set to be corrected. In 
post-processing, the idiomatic phrase list is used 
to check whether a certain collocation is still 
grammatical after several corrections are per-
formed. There are 996 idiomatic phrases in our 
list which is composed by mainly patterns from 
the training set and a series of hand-crafted ones. 
Typical phrases we extracted are in general, 
have/need to be done, on the other hand, a 
large/big number/amount of, at the same time, in 
public, etc.  
 
Figure 1. Architecture of our GEC system. 
3 Machine Learning Based Modules 
For the error types ArtOrDet, Prep and Nn, we 
choose machine learning based methods because 
we consider there is not enough evidence to di-
rectly determine which word or form to be used. 
Moreover, it is impossible to transfer all the cas-
es we encounter into rules. In this section, we 
describe our processing ideas for each error type 
respectively and then specifically introduce our 
feature selection and confidence tuning approach. 
3.1 Determiners 
Determiners in the error type ?ArtOrDet? contain 
articles a/an, the and other determiners such as 
Original texts 
Pre-processing 
Machine learning 
based modules 
Rule based mod-
ules 
Post-processing 
Corrected texts Result combination 
116
this, those, etc. This type of error accounts for a 
large proportion which is of great impact on the 
final result. We consider only articles since the 
other determiners are rarely used and the usages 
of them are sometimes ambiguous. Like ap-
proaches described in some previous works 
(Dahlmeier and Ng, 2012a; Felice and Pulman, 
2008), we assign three types a/an, the and empty 
for each article position and build a multi-class 
classifier.  
For training, developing and testing, all noun 
phrases (NPs) are chosen as candidate samples to 
be corrected. For NPs whose articles have been 
annotated in the corpus, the correct ones are their 
target categories, and for those haven?t been an-
notated, the target categories are their observed 
article types. Samples we make use of can be 
divided into two basic types in each category: 
with and without a wrong article. Two examples 
are shown below: 
with: a/empty big apples ~ empty category 
without: the United States ~ the category 
For each category in a, the, and empty, we use 
the whole with data and take samples of without 
ones from the set of correct NPs to make up 
training instances of one category. The reason 
why we make samples of the without ones is for 
the consideration that the classifier would always 
predicts the observed article and never proposes 
any corrections if given too many without sam-
ples, the case of which is mentioned in (Dahl-
meier and Ng, 2012a). However, we found that 
the ratio of with-without shows little effect in our 
model. The article a is regulated to a or an ac-
cording to pronunciation. 
Syntactic and semantic features are considered 
in feature extraction with the help of WordNet 
and the ?.conll? file provided. We adopt syntac-
tic features such as the surface word, phrase, 
part-of-speech, n-grams, constituent parse tree, 
dependency parse tree and headword of an NP; 
semantic features like noun category and hyper-
nym. Some expand operations are also done 
based on them (reference to Dahlmeier and Ng, 
2012a; Felice and Pulman, 2008). After feature 
extraction, we apply a genetic algorithm to do 
feature subset selection in order to reduce dimen-
sionality and filter out noisy features which is to 
be described in ?3.4. 
Maximum Entropy (ME) has been proven to 
behave well for heterogeneous features in natural 
language processing tasks and we adopt it to 
train our model. We have also tried several other 
classifiers including SVM, decision tree, Na?ve 
Bayes, and RankSVM but finally find ME per-
forms well and stably. It provides confidence 
scores for each category which we will make use 
of downstream.  
3.2 Prepositions 
Preposition error correction task is similar to the 
previous one except the different categories and 
corresponding features. Since there are 36 com-
mon prepositions listed by the shared task, origi-
nally, we assign 37 types including 36 preposi-
tions and empty for each preposition position and 
build a multi-class classifier. For training, devel-
oping and testing, each preposition as well as the 
empty position directly after a verb is considered 
as a candidate. Syntactic and semantic features 
extracted are similar to those in article error cor-
rection except for some specific cases for prepo-
sitions such as the verbs related to prepositions 
and the dependency relations. Similarly, we treat 
those preposition phrases with and without a cer-
tain preposition as the two types of samples in 
training (as described in ?3.1). Two examples are 
listed below: 
with: on/in the 1860s~ in category 
without: have to be done ~ to category 
Through statistics on the training data, we 
found that most prepositions have very few sam-
ples which may not contribute to the perfor-
mance at all and even bring about noise when 
assigned to wrong categories. After several 
rounds of experiments, we finally adopt a classi-
fier with seven prepositions which are frequently 
used in the whole corpus. They are on, of, in, at, 
to, with and for. As to the classifier, ME also 
outperforms the others. 
3.3 Noun Form 
Noun form may be interacting with determiners 
and verbs which may also have errors in the orig-
inal text. So errors may occur in the context fea-
tures extracted from the original text. However, 
if we use the context features that have been cor-
rected, more errors would be employed due to 
the low performance of the previous steps. 
Through statistics, we found that co-occurrence 
between two types of errors such as SVA and 
ArtOrDet only accounts for a small proportion. 
After a few experiments, we decided to give up 
interacting errors so as to avoid accumulated er-
rors.  
This is a binary classification problem. All 
head nouns in NPs are considered as candidates. 
Each category contains with and without samples 
similar to the cases in ?3.1 and ?3.2. Features are 
highly related to the deterministic factors for the 
117
head noun form such as the countability, Word-
Net type, name entity and whether there some 
specific dependency relations including det, 
amod etc.  
ME also outperforms other classifiers. 
3.4 Feature Selection Using Genetic Algo-
rithm 
Features we extracted are excessive and sparse 
after binarization. They bring noise in quality as 
well as complexity in computation and need to 
be selected a priori. In our work, it is a wrapper 
feature selection task. That is, we have to select a 
combination of features that perform well to-
gether rather than make sure each of them be-
haves well. This GEC task is interesting in fea-
ture selection because word surface features that 
are observed only once are also effective while 
we think that they overfit. Genetic algorithm 
(GA) has been proven to be useful in selecting 
wrapper features in classification (ElAlami, 2009; 
Anba-rasi et al 2010). We used GA to select fea-
tures as well as reduce feature dimensionality.  
We convert the features into a binary sequence 
in which each character represents one dimen-
sion.  Let ?1? indicates that we keep this dimen-
sion while ?0? means that we drop it, we use a 
binary sequence such as ?0111000?100? to de-
note a combination of feature dimensions. GA 
functions on the feature sequences and finally 
decides which features should be kept. The fit-
ness function we used is the evaluation measure 
F1 described in ?5.3. 
3.5 Confidence Tuning 
The Maximum Entropy classifier returns a confi-
dence score for each category given a testing 
sample. However, for different samples, the dis-
tribution of predicted scores varies a lot. For 
some samples, the classifier may have a very 
high predicted score for a certain category which 
means the classifier is confident enough to per-
form this prediction. But for some other samples, 
two or more categories may share close scores, 
the case of which means the classifier hesitates 
when telling them apart. 
We introduce a confidence tuning approach on 
the predicted results through a comparison be-
tween the observed category and the predicted 
category which is similar to the ?thresholding? 
approach described in Tetreault and Chodorow 
(2008). The main idea of the confidence tuning 
algorithm is: the choice between keep and drop is 
based on the difference between the confidence 
scores of the predicted category and the observed 
category. If this difference goes beyond a thresh-
old t, the prediction is kept while if it is under t, 
we won?t do any corrections. We believe this 
tuning strategy is especially appropriate in this 
task since to distinguish whether the observed 
category is correct or not affects a lot to the pre-
dicted result.  
The confidence threshold for each category is 
generated through a hill climbing algorithm in 
the development data aimed at maximizing F1-
meaure of the result.  
4 Rule-based Modules 
A few hand-crafted rules are applied to solve the 
verb related corrections including SVA and 
Vform. In these cases, the verb form is only re-
lated to some specific features as described by 
Lee and Seneff (2008). 
4.1 SVA 
SVA (Subject-verb-agreement) is particularly 
related to the noun subject that a verb determines. 
In the dependency tree, the number of the noun 
which has a relation nsubj with the verb deter-
mines the form of this verb. Through observation, 
we find that the verbs to be considered in SVA 
contain only bes (including am, is, are, was, 
were) and the verbs in simple present tense 
whose POSs are labeled with VBZ (singular) or 
VBP(plural).  
To pick out the noun subject is easy except for 
the verb that contained in a subordinate clause. 
We use semantic role labeling (SRL) to help 
solve this problem in which the coordinated can 
be extracted through a trace with the label ?R-
Argument?. The following Figure is an example 
generated by the SRL toolkit mate-tools (Bernd 
Bohnet, 2010)2. 
 
 
Figure 2. SRL for the demo sentence ?Jack, who 
will show me the way, is very tall.? The subject of 
the verb show can be traced through R-A0 -> A0. 
 
  However, the performance of this part is partly 
correlated with the noun form that may have er-
rors in the original text and the wrong SRL result 
brought about because of wrong sentence gram-
mars. 
                                                 
2 http://code.google.com/p/mate-tools/ 
118
4.2 Verb Form 
The cases are more complicated in the verb form 
error correction task. Modal, aspect and voice are 
all forms that should be considered for a verb. 
And sometimes, two or more forms are com-
bined together to perform its role in a sentence. 
For example, in the sentence: 
He has been working in this position for a 
long time. 
The bolded verb has been working is a com-
bination of the active voice work, the progressive 
aspect be+VBG and the perfect aspect has+VBN. 
It is a bit difficult for us to take all cases into 
consideration, so we just apply several simple 
rules and solve a subset of problems for this type. 
Some typical rules are listed below: 
1. The verb that has a dependency relation aux 
to preposition to is modified to its base form. 
2. The verb that has a dependency relation 
pcomp to preposition by is modified to its past 
form. 
3. The verb related to other prepositions (ex-
cept to and by) is modified to ~ing form. 
4. The verb depends on auxiliary do and mod-
al verb (including its inflections and negative 
form) is modified to its base form. 
We have also tried to use SRL and transitivity 
of a verb to determine the active and passive 
voice but it didn?t work well. 
5 Experiments and Analysis 
5.1 Data Description 
The NUCLE corpus introduced by NUS (Nation-
al University of Singapore) contains 1414 essays 
written by L2 students with relatively high profi-
ciency of English in which grammatical errors 
have been well annotated by native tutors. It has 
a small proportion of annotated errors which is 
much lower than other similar corpora (Dahl-
meier et al, 2013). In our experiments, we divide 
the whole corpus into 80%, 10% and 10% for 
training, developing and testing. And we use 90% 
and 10% for training and developing for the final 
test. 
5.2 External tools and corpora 
External tools we used include WordNet (Fell-
baum, 1998) for word base form and noun cate-
gory generation, Morphg (Minnen et al, 2000)3 
to generate inflections of nouns and verbs, mate-
tools (Bohnet, 2010) for SRL, Stanford-ner 
                                                 
3 http://www.informatics.sussex.ac.uk/research/groups/nlp 
/carroll/morph.html 
(Finkel et al, 2005)4 for name entity extraction 
and Longman online dictionary5  for generation 
of noun countability and verb transitivity.  
We didn?t employ any external corpora in our 
system. 
5.3 Experiments 
The performance of each machine learning mod-
ule is affected by the selection of training sam-
ples, features and confidence tuning for the max-
imum entropy classifier. All these factors con-
tribute more or less to the final performance and 
need to be carefully developed. In our experi-
ments, we focus on machine learning based 
modules and make comparisons on sample selec-
tion, confidence tuning and feature selection and 
list a series of results before and after applying 
our strategies.  
In our experiment, the performance is meas-
ured with precision, recall and F1-measure where 
1
2 precision recallF precision recall
? ?? ?
 
Precision is the amount of predicted correc-
tions that are also corrected by the manual anno-
tators divided by the whole amount of predicted 
corrections. Recall has the same numerator as 
precision while its denominator is the amount of 
manually corrected errors. They are in accord-
ance with those measurements generated by the 
official m2scorer (Dahlmeier and Ng, 2012c) to a 
great extent and easily to be integrated in our 
program. 
As we have mentioned in Section 3, we don?t 
employ all samples but make use of all with 
(with errors and annotations) instances and sam-
ple the without ones (without errors) for training. 
And the sampling for without type is totally ran-
dom without loss of generality. We apply the 
same strategy in all of these three error types 
(ArtOrDet, Prep and Nn) and try several ratios of 
with-without to find out whether this ratio has 
great impact on the final result and which ratio 
performs best. We use the 80%-10%-10% data 
(mentioned in ?5.1) for our experiments and 
make comparisons of different ratios on develop-
ing data. The experimental results are described 
in detail in Figure 3. 
Confidence tuning is applied in all these three 
error types which contributes most to the final 
performance in our model. We compare the re-
sults before and after tuning in all sample ratios 
                                                 
4 http://www-nlp.stanford.edu/software/CRF-NER.shtml 
5 http://www.ldoceonline.com/ 
119
that we designed and they are also depicted in 
Figure 3.  
Sample with:without
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.2
.4
.6
.8
precision before and after tuning
recall before and after tuning
F1 before and after tuning
 
Figure 3-1. Comparisons before and after tuning 
in ArtOrDet. 1:all means to use the whole without 
samples. 
Sample with:without
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.1
.2
.3
.4
.5
.6
presision before and after tuning
recall before and after tuning
F1 before and after tuning
 
Figure 3-2. Comparisons before and after tuning in 
Prep.  
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.2
.4
.6
.8
1.0
precision before and after tuning
recall before and after tuning
F
1
 before and after tuning
Sample with:without
 
Figure 3-3. Comparisons before and after tuning 
in Nn. 
 
From the three groups of data in Figure 3, we 
notice that the ratio of samples has little impact 
on F1. This phenomenon shows that our conclu-
sion goes against the previous work by Dahl-
meier and Ng (2012a). We believe it is mainly 
due to our confidence tuning which makes the 
parameters vary much under different sample 
ratios, that is, if given the same parameters, the 
effect of sample ratio selection may become ob-
vious. Unfortunately, we didn?t do such a sys-
tematic comparison in our work. The improve-
ment under confidence tuning can be seen clearly 
in all ratios of with-without samples. The confi-
dence tuning algorithm employed in our work is 
better than the traditional tuning methods that 
assign a fixed threshold for each category or for 
all categories (about 1%~2% better measured by 
F1).  
However, although we are able to pick out the 
training data with a high F1 through confidence 
tuning for the developing data, it is difficult for 
us to choose a set of confidence parameters that 
also fits the test data well. Given several close 
F1s, the numerical values of denominators and 
numerators which determine the precision and 
recall can vary a lot. For example, one set that 
has a high precision and low recall may share the 
similar F1 with another set that has a low preci-
sion and high recall. Our work lacked of the de-
velopment on how to control the number of pro-
posed errors to make leverage on the perfor-
mance between developing set and testing set. It 
resulted in that the developing set and the testing 
set were not balanced at all, and our model was 
not able to keep the sample distribution as the 
training set. This is the main factor that leads to a 
low performance in our submitted result which 
can be clearly seen in Table 1. The upper bound 
performance of our system achieves precision of 
34.23%, recall of 25.56% and F1 of 29.27%, in 
which the F1 goes 7% beyond our submitted sys-
tem. We notice that results of all metrics of the 
three error types where machine learning algo-
rithms are applied improve with the simultaneous 
increase of numerators and denominators. This is 
especially noticeable in Prep. 
For the other two types SVA and Vform, we 
just apply several heuristic rules to solve a subset 
of problems and the case of Vform has not been 
solved well such as tense and voice. 
Genetic Algorithm (GA) is applied to process 
feature reduction and subset selection. This is 
done in ArtOrDet type in which we extract as 
many as 350,000 binary features. For error type 
Prep and Nn, the feature dimensionalities we 
constructed were not as high as that in ArtOrDet, 
and the improvements under GA were not obvi-
ous which we would not discuss in this work. 
Through experiments on a few sample ratios, we 
notice that feature selection using genetic algo-
rithm is able to reduce the feature dimensionality 
to about 170,000 which greatly lowers down the 
120
downstream computational complexity. However, 
the improvement contributed by GA after confi-
dence tuning is not obvious as that before confi-
dence tuning. We think it is partly because of the 
bad initialization of GA which is to be improved 
in our future work. The unfixed parameters may 
also lead to such a result which we didn?t discuss 
enough in our work. The comparison before and 
after GA is described in Figure 4. 
 
 Our submission% Upper bound% 
P(Det) 41.38(168/406) 36.44(254/697) 
R(Det) 24.35(168/690) 36.81(254/690) 
F1(Det) 30.66 36.63 
P(Prep) 13.79(4/29) 26.12(35/134) 
R(Prep) 1.29(4/311) 11.25(35/311) 
F1(Prep) 2.35 15.73 
P(Nn) 24.81(65/262) 27.27(102/374) 
R(Nn) 16.41(65/396) 25.76(102/396) 
F1(Nn) 19.76 26.49 
P(SVA) 
R(SVA) 
F1(SVA) 
24.42(21/86) 
16.94(21/124) 
20.00 
24.42(21/86) 
16.94(21/124) 
20.00 
P(Vform) 
R(Vform) 
F1(Vform) 
19.35(6/31) 
4.92(6/122) 
7.84 
19.35(6/31) 
4.92(6/122) 
7.84 
P(all) 35.65(272/763) 34.23(420/1227) 
R(all) 16.56(272/1643) 25.56(420/1643) 
F1(all) 22.61 29.27 
Table 1. Different performances according to dif-
ferent confidence parameters. Det stands for Ar-
tOrDet. 
 
Pre-processing and post-processing we pro-
pose also contribute to some extent which we 
could see from Table 2. Some idiomatic phrases 
are excluded from being corrected in pre-
processing which enhances precision while some 
are being modified in post-processing to improve 
recall. 
 
 Without pre-processing 
and post-processing% 
Final% 
P 
R 
F1 
33.72(265/768) 
16.13(265/1643) 
21.82 
35.65(272/763) 
16.56(272/1643) 
22.61 
Table 2. Comparison with and without pre-
processing and post-processing. 
 
We didn?t do much on the interacting errors 
problem since we didn?t work out perfect plans 
to solve it. So, in the result combination module, 
we just simply combine the result of each part 
together. 
Sample positive:negative
1:1 1:2 1:3 1:6
F
1
0.00
.05
.10
.15
.20
.25
.30
ME
ME+GA
ME+Tuning
ME+GA+Tuning
 
Figure 4. Comparisons before and after Genet-
ic Algorithm on ArtOrDet error type. ME, GA, 
and Tuning stand for Maximum Entropy, Ge-
netic Algorithm and confidence tuning. 
 
In the revised version, under further correc-
tions for the gold annotations, our model 
achieves precision of 41.75%, recall of 20.19% 
and F1 of 27.3%. 
6  Discussion 
Which factor contributes most to the final result 
in the problem of grammatical error correction? 
Since we didn?t include any external corpora, we 
discuss it here only according to the local classi-
fiers and context features.  
Based on our experiments, we find that, in our 
machine learning based modules, a tiny modifi-
cation of confidence parameter setting for each 
category, no matter which type of error, can have 
great impact on the final result. It results in that 
our model is much too sensitive to parameters 
which may easily lead to a poor behavior. Per-
haps a sufficient consideration of how to keep 
the distribution of samples, such as cross-
validation, may be helpful. In addition, the selec-
tion of classifiers, features and training samples 
all have effect on the result more or less, but not 
as obvious as that of the confidence threshold 
setting. 
7 Conclusion 
In this paper, we propose a hybrid model 
combining machine learning based modules and 
rule-based modules to solve the grammatical er-
ror correction task. We are able to solve a subset 
of the correction problems in which ArtOrDet 
and Nn perform better. However, our result in 
the testing data shows that our model is sensitive 
121
to parameters. How to keep the distribution of 
training samples needs to be further developed. 
Acknowledgement 
This work is supported in part by the National 
Natural Science Foundation of China (No. 612-
72383 and 61173075). 
References  
Bernd Bohnet. Top Accuracy and Fast Depend-
ency Parsing is not a Contradiction. In Pro-
ceedings of COLING, 2010. 
C. Fellbaum. WordNet: An Electronic Lexical 
Data-base. MIT Press. 1998. 
Daniel Dahlmeier, and Hwee Tou Ng. Grammat-
ical error correction with alternating structure 
optimization. In Proceedings of ACL. Associa-
tion for Computational Linguistics, 2011. 
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun 
Feng Ng. NUS at the HOO 2012 Shared Task. 
In Proceedings of the Seventh Workshop on 
Building Educational Applications Using NLP. 
Association for Computational Linguistics, 
2012a. 
Daniel Dahlmeier and Hwee Tou Ng. A beam-
search decoder for grammatical error correc-
tion. In Proceedings of the EMNLP. Associa-
tion for Computational Linguistics, 2012b. 
Daniel Dahlmeier and Hwee Tou Ng. Better 
Evaluation for Grammatical Error Correction. 
In Proceedings of NAACL, Association for 
Computational Linguistics, 2012c. 
Daniel Dahlmeier, Hwee Tou Ng and Siew Mei 
Wu. Building a Large Annotated Corpus of 
Learner English: The NUS Corpus of Learner 
English. In Proceedings of the 8th Workshop 
on Innovative Use of NLP for Building Educa-
tional Applications (BEA), 2013. 
De Felice, Rachele and Stephen G. Pulman. A 
classifier-based approach to preposition and 
determiner error correction in L2 English. In 
Proceedings of COLING. Association for 
Computational Linguistics, 2008. 
G. Minnen, J. Carroll and D. Pearce. Robust, 
applied morphological generation. In Proceed-
ings of the 1st International Natural Language 
Generation Conference, 2000. 
Jenny Rose Finkel, Trond Grenager, and Chris-
topher Manning. Incorporating Non-local In-
formation into Information Extraction Systems 
by Gibbs Sampling. In Proceedings of ACL , 
2005. 
Joel R. Tetreault and Martin Chodorow. The ups 
and downs of preposition error detection in 
ESL writing. In Proceedings of COLING, As-
sociation for Computational Linguistics, 2008. 
John Lee and Stephanie Seneff. Correcting mis-
use of verb forms. In Proceedings of ACL: 
HLT, 2008. 
M Anbarasi, E Anupriya, and NC Iyengar. En-
hanced prediction of heart disease with feature 
subset selection using genetic algorithm. In-
ternational Journal of Engineering Science 
and Technology,Vol.2(10),2010: 5370-5376. 
ME ElAlami. A filter model for feature subset 
selection based on genetic algorithm. 
Knowledge-Based Systems,Vol.22(5), 2009: 
356-362. 
Michael Heilman, Aoife Cahill, and Joel 
Tetreault. Precision isn't everything: a hybrid 
approach to grammatical error detection. In 
Proceedings of the Seventh Workshop on 
Building Educational Applications Using NLP. 
Association for Computational Linguistics, 
2012. 
Hongsuck Seo et al A meta learning approach to 
grammatical error correction. In Proceedings 
of ACL. Association for Computational Lin-
guistics, 2012. 
N.R. Han, M. Chodorow, and C. Leacock. 2006. 
Detecting errors in English article usage by 
non-native speakers. Natural Language Engi-
neering, Vol.12(02):115-129. 
S. Bergsma, D. Lin, and R. Goebel. 2009. Web-
scale ngram models for lexical disambiguation. 
In Proceedings of IJCAI.2009. 
X. Yi, J. Gao, and W.B. Dolan. 2008. A web-
based English proofing system for English as a 
second language users. In Proceedings of 
IJCNLP.2008. 
122
