Bootstrapping Both Product Features and Opinion Words from Chi-
nese Customer Reviews with Cross-Inducing1   
Bo Wang 
Institute of Computational Linguistics 
Peking University 
Beijing, 100871, China 
wangbo@pku.edu.cn  
Houfeng Wang 
Institute of Computational Linguistics 
Peking University 
Beijing, 100871, China 
wanghf@pku.edu.cn  
 
 
Abstract 
We consider the problem of 1  identifying 
product features and opinion words in a 
unified process from Chinese customer re-
views when only a much small seed set of 
opinion words is available. In particular, 
we consider a problem setting motivated by 
the task of identifying product features 
with opinion words and learning opinion 
words through features alternately and it-
eratively. In customer reviews, opinion 
words usually have a close relationship 
with product features, and the association 
between them is measured by a revised 
formula of mutual information in this paper. 
A bootstrapping iterative learning strategy 
is proposed to alternately both of them. A 
linguistic rule is adopted to identify low-
frequent features and opinion words. Fur-
thermore, a mapping function from opinion 
words to features is proposed to identify 
implicit features in sentence. Empirical re-
sults on three kinds of product reviews in-
dicate the effectiveness of our method. 
1 Introduction 
With the rapid expansion of network application, 
more and more customer reviews are available on-
line, which are beneficial for product merchants to 
track the viewpoint of old customers and to assist 
potential customers to purchase products. However, 
                                                 
1 Supported by National Natural Science Foundation of China 
under grant No.60675035 and Beijing Natural Science Foun-
dation under grant No.4072012 
it?s time-consuming to read all reviews in person. 
As a result, it?s significant to mine customer re-
views automatically and to provide users with 
opinion summary. 
In reality, product features and opinion words 
play the most important role in mining opinions of 
customers. One customer review on some cell 
phone is given as follows: 
 
     (a) ?????????????????(The 
appearance is beautiful, the screen is big 
and the photo effect is OK.)  
 
Product features are usually nouns such as ??
?? (appearance) and ???? (screen) or noun 
phrases such as ?????? (photo effect) express-
ing which attributes the customers are mostly con-
cerned. Opinion words (opword is short for ?opin-
ion word?) are generally adjectives used to express 
opinions of customers such as ???? (beautiful), 
??? (big) and ??? (well). As the core part of an 
opinion mining system, this paper is concentrated 
on identifying both product features and opinion 
words in Chinese customer reviews. 
There is much work on feature extraction and 
opinion word identification. Hu and Liu (2004) 
makes use of association rule mining (Agrawal and 
Srikant, 1994) to extract frequent features, the sur-
rounding adjectives of any extracted feature are 
considered as opinion words. Popescu and Etzioni 
(2005) has utilized statistic-based point-wise mu-
tual information (PMI) to extract product features. 
Based on the association of opinion words with 
product features, they take the advantage of the 
syntactic dependencies computed by the MINIPAR 
parser (Lin, 1998) to identify opinion words. Tur-
289
ney (2002) applied a specific unsupervised learn-
ing technique based on the mutual in-formation 
between document phrases and two seed words 
?excellent? and ?poor?.  
Nevertheless, in previous work, identifying 
product features and opinion words are always 
considered two separate tasks. Actually, most 
product features are modified by the surrounding 
opinion words in customer reviews, thus they are 
highly context dependent on each other, which is 
referred to as context-dependency property hence-
forth. With the co-occurrence characteristic, identi-
fying product features and opinion words could be 
combined into a unified process. In particular, it is 
helpful to identify product features by using identi-
fied opinion words and vice versa. That implies 
that such two subtasks can be carried out alter-
nately in a unified process.  Since identifying 
product features are induced by opinion words and 
vice versa, this is called cross-inducing.  
As the most important part of a feature-based 
opinion summary system, this paper focuses on 
learning product features and opinion words from 
Chinese customer reviews. Two sub-tasks are in-
volved as follows: 
Identifying features and opinion words: Resort-
ing to context-dependency property, a bootstrap-
ping iterative learning strategy is proposed to iden-
tify both of them alternately. 
Identifying implicit features: Implicit features 
occur frequently in customer reviews. An implicit 
feature is defined as a feature that does not appear 
in an opinion sentence. The association between 
features and opinion words calculated with the re-
vised mutual information is used to identify im-
plicit features.  
This paper is sketched as follows: Section 2 de-
scribes the approach in detail; Experiment in sec-
tion 3 indicates the effectiveness of our approach. 
Section 4 presents related work and section 5 con-
cludes and presents the future work. 
2 The Approach 
Figure 1 illustrates the framework of an opinion 
summary framework, the principal parts related to 
this paper are shown in bold. The first phase 
?identifying features and opinion words?, works 
iteratively to identify features with the opinion 
words identified and learn opinion words through 
the product features identified alternately. Then, 
one linguistic rule is used to identify low-frequent 
features and opinion words. After that, a mapping 
function is designed to identify implicit features. 
 
 
Figure 1. The framework of an opinion summary 
system 
2.1 Iterative Learning Strategy 
Product features and opinion words are highly con-
text-dependent on each other in customer reviews, 
i.e., the feature ???? (body) for digital camera 
often co-occur with some opinion words such as 
??? (big) or ???? (delicate) while the feature 
????? (the proportion of performance to price)  
often co-occurs with the opinion word ??? (high).  
Product features can be identified resorting to 
the surrounding opinion words identified before 
and vice versa. A bootstrapping method that works 
iteratively is proposed in algorithm 1.  
Algorithm 1 works as follows: given the seed 
opinion words and all the reviews, all noun phrases 
(noun phrases in the form ?noun+?) form CandFe-
aLex (the set of feature candidates) and all adjec-
tives compose of CandOpLex (the set of the candi-
dates of opinion words). The sets ResFeaLex and 
ResOpLex are used to store final features and opin-
ion words. Initially, ResFeaLex is set empty while 
ResOpLex is composed of all the seed opinion 
words. At each iterative step, each feature candi-
date in CandFeaLex is scored by its context-
dependent association with each opword in ResO-
pLex, the candidate whose score is above the pre-
specified threshold Thresholdfeature is added to Res 
290
Algorithm 1. Bootstrap learning product features and opinion words with cross-inducing 
Bootstrap-Learning (ReviewData, SeedOpLex, Thresholdfeature, Thresholdopword) 
1   Parse(ReviewData); 
2   ResFeaLex = {}, ResOpLex = SeedOpLex; 
3   CandFeaLex = all noun phrases in ReviewData; 
4   CandOpLex = all adjectives in ReviewData; 
5   while  (CandFeaLex?{} && CanOpLex?{})  
6        do for each candfea?CandFeaLex 
7              do for each opword?ResOpLex  
8                    do calculate RMI(candfea,opword) with ReviewData; 
9                score(canfea)=?opword?ResOpLexRMI(candfea,opword)/|ResOpLex|; 
10         sort CandFeaLex by score; 
11         for each candfea?CandFeaLex  
12               do  if  (score(candfea)> Thresholdfeature) 
13                       then   ResFeaLex=ResFeaLex+{candfea}; 
14                                CanFeaLex=CandFeaLex ? {candfea}; 
15          for each candop?CandOpLex  
16                 do for each feature?ResFeaLex  
17                      do calculate RMI(candop,feature) with D; 
18                 score(candop)=?feature?ResFeaLexRMI(feature,candop)/|ResFeaLex| ; 
19          sort  CandOpLex by score; 
20          for each candop?CandOpLex 
21       do  if  (score (candop)>Thresholdopword) 
22               then  ResOpLex=ResOpLex+{candop }; 
23                     CanOpLex=CandOpLex ? {candop}; 
24          if  (neither candfea and candop is learned) then break;  
25   return ResFeaLex, ResOpLex; 
 
FeaLex and subtracted from CandFeaLex. Simi-
larly, opinion words are processed in this way, but 
the scores are related to features in ResFeaLex. 
The iterative process continues until neither Res-
FeaLex nor ResOpLex is altered. Any feature can-
didate and opinion word candidate, whose relative 
distance in sentence is less than or equal to the 
specified window size Minimum-Offset, are re-
garded to co-occur with each other. The associa-
tion between them is calculated by the revised mu-
tual information denoted by RMI, which will be 
described in detail in the following section and 
employed to identify implicit features in sentences. 
2.2  Revised Mutual Information 
In customer reviews, features and opinion words 
usually co-occur frequently, features are usually 
modified by the surrounding opinion words. If the 
absolute value of the relative distance in a sentence 
for a feature and an opinion word is less than 
Minimum-Offset, they are considered context-
dependent. 
Many methods have been proposed to measure 
the co-occurrence relation between two words such 
as ?2 (Church and Mercer,1993) , mutual informa-
tion (Church and Hanks, 1989; Pantel and Lin, 
2002), t-test (Church and Hanks, 1989), and log-
likelihood (Dunning,1993). In this paper a revised 
formula of mutual information is used to measure 
the association since mutual information of a low-
frequency word pair tends to be very high.  
Table 1 gives the contingency table for two 
words or phrases w1 and  w2, where A is the num-
ber of reviews where w1 and w2 co-occur; B indi-
cates the number of reviews where w1 occurs but 
does not co-occur with w2; C denotes the number 
of reviews where w2 occurs but does not co-occur 
with w1; D is number of reviews where neither w1 
nor w2 occurs; N = A + B + C + D. 
With the table, the revised formula of mutual in-
formation is designed to calculate the association 
of w1 with w2 as formula (1). 
 
 w2 ~w2
w1 A B 
~w1 C D 
Table 1:  Contingency table 
291
 1 2
1 2 1 2
1 2
( , )
( , ) ( , ) log
( ) ( )
p w w
RMI w w freq w w
p w p w
= ? i                             
log
( ) (
N A
A
)A B A C
?= ? + ? +                           (1) 
 
2.3 Identifying Low-Frequent Features and 
Opinion Words 
In Chinese reviews, one linguistic rule ?noun+ ad-
verb* adjective+? occurs frequently and most of 
the instances of the rule are used to express posi-
tive or negative opinions on some features, i.e., ??
?/noun ??/adverb ??/adjective? (The body is 
rather delicate) , where each Chinese word and its 
part-of-speech is separated by the symbol ?/?.  
Intuitively, this linguistic rule can be used to 
improve the output of the iterative learning. For 
each instance of the rule, if ?noun+? exists in Res-
FeaLex, the ?adjective? part would be added to 
ResOpLex, and if ?adjective+? exists in ResOpLex, 
the noun phrase ?noun+? part will be added to 
ResFeaLex. After that, most low-frequent features 
and opinion words will be recognized. 
2.4 Identifying Implicit Features 
The context-dependency property indicates the 
context association between product features and 
opinion words. As a result, with the revised mutual 
information, the implicit features can be deduced 
from opinion words. A mapping function f: op-
word? feature is used to deduce the mapping fea-
ture for opword , where f(opword) is defined as the 
feature with the largest association with opinion 
word. 
If an opinion sentence contains opinion words, 
but it does not have any explicit features, the map-
ping function f: opword? feature is employed to 
generate the implicit feature for each opinion word 
and the feature is considered as an implicit feature 
in the opinion sentence. Two instances are given in 
(b) and (c), where the implicit features are inserted 
in suitable positions and they are separated in pa-
rentheses. Since f (???? (beautiful)) = ???? 
(appearance) and f (???? (fashionable)) = ??
?? (appearance), ???? (appearance) is an im-
plicit feature in (b). Similarly, the implicit features 
in (c) are ???? (performance) and ???? (pic-
ture). 
 
(b) (??)????(??)???It?s (appear-
ance) beautiful and (appearance) fashion-
able. 
(c) (??)??????(??)????It?s 
(performance) very stable and (picture) 
very clear.  
3 Experiment 
3.1 Data Collection 
We have gathered customer reviews of three kinds 
of electronic products from http://it168.com: digi-
tal camera, cell-phone and tablet. The first 300 re-
views for each kind of them are downloaded. One 
annotator was asked to label each sentence with 
product features (including implicit features) and 
opinion words. The annotation set for features and 
opinion words are shown in table 2. 
 
Product 
Name 
No. of Fea-
tures 
No. of Opin-
ion Words 
digital camera 135 97 
cell-phone 155 125 
tablet 96 83 
Table 2 . Annotation set for product features and 
opinion words   
 
Unlike English, Chinese are not separated by 
any symbol. Therefore, the reviews are tokenized 
and tagged with part-of-speech by a tool 
ICTCLAS2.One example of the output of this tool 
is as (d).  
 
(d) ??/n  ??/n  ?/d  ?/d  ?/a  ?/w  ??
/n  ???/n  ??/v  ?/d  ??/v  ??/v  
??/n  ??/n  ?/w  ??/n  ??/vn  ??
/vn  ?/d  ?/d  ??/a  ?/w 
The seed opinion words employed in the itera-
tive learning are: ???? (clear), ??? (quick),  
??? (white), ???? (weak). ??? (good), ???? 
(good), ??? (high), ??? (little), ??? (many), 
? ? ? (long). Empirically, Thresholdfeature and 
Thresholdopword in Algorithm 1 is set to 0.2, Mini-
mum-Offset is set to 4.  
                                                 
2 http://www.nlp.org.cn   
292
On Set On Sentence  Product Name
Precision Recall F-Score Precision Recall F-Score 
digital camera 64.03% 45.92% 53.49% 46.62% 65.72% 54.55% 
cell-phone 54.43% 43.87% 48.58% 34.17% 55.15% 42.19% 
tablet 51.45% 59.38% 55.13% 41.39% 60.21% 49.06% 
average 56.64% 49.72% 52.40% 40.73% 60.36% 48.60% 
Table 3. Evaluation of apriori algorithm 
 
On Set On Sentence Type Product Name 
Precision Recall F-Score Precision Recall F-score
73.57% 54.81% 62.82% 55.80% 68.69% 61.58%
digital camera 
78.20% 73.33% 75.69% 54.71% 70.80% 63.49%
80.92% 45.81% 58.50% 47.31% 58.59% 52.35%
cell-phone 82.30% 66.46% 73.53% 49.22% 61.63% 54.73%
72.73% 57.29% 64.09% 49.79% 61.03% 54.84%
tablet 77.99% 73.96% 75.92% 52.54% 64.43% 57.88%
75.74% 52.64% 61.80% 50.97% 62.77% 56.26%
feature 
average 79.50% 71.25% 75.05% 52.16% 65.62% 58.70%
89.02% 38.02% 53.28% 72.35% 50.24% 59.30%digital camera 
87.31% 60.94% 71.78% 69.40% 85.28% 76.53%
87.95% 30.80% 45.63% 66.44% 42.84% 52.09%
cell-phone 88.49% 51.90% 65.43% 63.14% 79.51% 70.39%
77.94% 30.64% 43.98% 61.30% 42.69% 50.34%
tablet 80.73% 50.87% 62.41% 63.92% 81.02% 71.46%
84.97% 33.15% 47.63% 66.70% 45.26% 53.91%
opword 
average 85.51% 54.57% 66.54% 65.49% 81.94% 72.79%
Table 4. Evaluation of iterative learning (the upper) and the combination of iterative learning and the 
linguistic rule (the lower). 
3.2 Evaluation Measurement 
As Hu and Liu (2004), the features mined form the 
result set while the features in the manually anno-
tated corpus construct the answer set. With the two 
sets, precision, recall and f-score are used to evalu-
ate the experiment result on set level.  
In our work, the evaluation is also conducted on 
sentence for three factors: Firstly, each feature or 
opinion word may occur many times in reviews but 
it just occurs once in the corresponding answer set; 
Secondly, implicit features should be evaluated on 
sentence; Besides, to generate an opinion summary, 
the features and the opinion words should be iden-
tified for each opinion sentence.  
On sentence, the features and opinion words 
identified for each opinion sentence are compared 
with the annotation result in the corresponding sen-
tence. Precision, recall and f-score are also used to 
measure the performance. 
3.3 Evaluation 
Hu and Liu (2004) have adopted associate rule 
mining to mine opinion features from customer 
reviews in English. Since the original corpus and 
source code is not available for us, in order to 
make comparison with theirs, we have re-
implemented their algorithm, which is denoted as 
apriori method as follows. To be pointed out is that, 
the two pruning techniques proposed in Hu and Liu 
(2004): compactness pruning and redundancy 
pruning, were included in our experiment. The 
evaluation on our test data is listed in table 3. The 
row indexed by average denotes the average per-
formance of the corresponding column and each 
entry in it is bold. 
Table 4 shows our testing result on the same 
data, the upper value in each entry presents the re-
sult for iterative learning strategy while the lower 
values denote that for the combination of iterative 
learning and the linguistic rule. The average row 
293
shows the average performance for the correspond-
ing columns and each entry in the row is shown in 
bold. 
On feature, the average precision, recall and f-
score on set or sentence increase according to the 
order apriori < iterative <  ite+rule, where apriori 
indicates Hu and Liu?s method, iterative represents 
iterative strategy and iterative+rule denotes the 
combination of iterative strategy and the linguistic 
rule. The increase range from apriori to itera-
tive+rule of f-score on set gets to 22.65% while on 
sentence it exceeds 10%. The main reason for the 
poor performance on set for apriori is that many 
common words such as ???? (computer), ???? 
(China) and ???? (time of use) with high fre-
quency are extracted as features. Moreover, the 
poor performance on sentence for apriori method is 
due to that it can?t identify implicit features. Fur-
thermore, the increase in f-score from iterative to 
ite+rule on set and on sentence shows the perform-
ance can be enhanced by the linguistic rule. 
Table 4 also shows that the performance in 
learning opinion words has been improved after 
the linguistic rule has been used. On set, the aver-
age precision increases from 84.97% to 85.51% 
while the average recall from 33.15% to 54.57%. 
Accordingly, the average f-score increase signifi-
cantly by about 18.91%. 
On sentence, although there is a slow decrease 
in the average precision, there is a dramatic in-
crease in the average recall, thus the average f-
score has increased from 53.91% to 72.79%. Fur-
thermore, the best f-score (66.54%) on set and the 
best f-score (72.79%) on sentence indicate the ef-
fectiveness of ite+rule on identifying opinion 
words. 
4 Related Work 
Our work is much related to Hu?s system (Hu and 
Liu,2004), in which association rule mining is used 
to extract frequent review noun phrase as features. 
After that, two pruning techniques: compactness 
pruning and redundancy pruning, are utilized. Fre-
quent features are used to find potential opinion 
words (adjectives) and WordNet syno-
nyms/antonyms in conjunction with a set of seed 
words are used in order to find actual opinion 
words. Finally, opinion words are used to extract 
associated infrequent features. The system only 
extracts explicit features. Our work differs from 
hers at two aspects: (1) their method can?t identify 
implicit features which occur frequently in opinion 
sentences; (2) Product features and opinion words 
are identified on two separate steps in Hu?s system 
but they are learned in a unified process here and 
induced by each other in this paper. 
Popescu and Etzioni (2005) has used web-based 
point-wise mutual information (PMI) to extract 
product features and use the identified features to 
identify potential opinion phrases with co-
occurrence association. They take advantage of the 
syntactic dependencies computed by the MINIPAR 
parser. If an explicit feature is found in a sentence, 
10 extraction rules are applied to find the heads of 
potential opinion phrases. Each head word together 
with its modifier is returned as a potential opinion 
phrase. Our work is different from theirs on two 
aspects: (1) Product features and opinion words are 
identified separately but they are learned simulta-
neously and are boosted by each other here. (2) 
They have utilized a syntactic parser MINIPAR, 
but there?s no syntactic parser available in Chinese, 
thus the requirement of our algorithm is only a 
small seed opinion word lexicon. Although co-
occurrence association is used to derive opinion 
words from explicit features in their work, the way 
how co-occurrence association is represented is 
different. Besides, the two sub-tasks are boosted by 
each other in this paper. 
On identifying opinion words, Morinaga et al
(2002)has utilized information gain to extract clas-
sification features with a supervised method; Hat-
zivassiloglou and Wiebe (1997) used textual  junc-
tions such as ?fair and legitimate? or ?simplistic 
but well-received? to separate similarity- and op-
positely-connoted words; Other methods are pre-
sent in (Riloff et al 2003; Riloff and Wiebe, 2003; 
Gamon and Aue, 2005; Wilson et al 2006) The 
principal difference from previous work is that, 
they have considered extracting opinion words as a 
separate work but we have combined identifying 
features and opinion words in a unified process. 
Besides, the opinion words are identified for sen-
tences but in their work they are identified for re-
views. 
5 Conclusion 
In this paper, identifying product features and 
opinion words are induced by each other and are 
combined in a unified process. An iterative learn-
294
ing strategy based on context-dependence property 
is proposed to learn product features and opinion 
words alternately, where the final feature lexicon 
and opinion word lexicon are identified with very 
few knowledge (only ten seed opinion words) and 
augmented by each other alternately. A revised 
formula of mutual information is used to calculate 
the association between each feature and opinion 
word. A linguistic rule is utilized to recall low-
frequent features and opinion words. Besides, a 
mapping function is designed to identify implicit 
features in sentence. In addition to evaluating the 
result on set, the experiment is evaluated on sen-
tence. Empirical result indicates that the perform-
ance of iterative learning strategy is better than 
apriori method and that features and opinion words 
can be identified with cross-inducing effectively. 
Furthermore, the evaluation on sentence shows the 
effectiveness in identifying implicit features. 
In future, we will learn the semantic orientation 
of each opinion word, calculate the polarity of each 
subjective sentence, and then construct a feature-
based summary system. 
References 
Ana Maria Popescu and Oren Etzioni. 2005. Extracting 
Product Features and Opinions from Reviews. Pro-
ceedings of HLT-EMNLP (2005) 
De-Kang Lin. 1998. Dependency-Based Evaluation of 
MINIPAR. In:Proceedings of the Workshop on the 
Evaluation of Parsing Systems, Granada, Spain, 1998, 
298?312 
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. 
Learning Subjective Nouns Using Extraction Pattern 
Bootstrapping. Seventh Conference on Natural Lan-
guage Learning (CoNLL-03). ACL SIGNLL. Pages 
25-32. 
Ellen Riloff and Janyce Wiebe. 2003. Learning Extrac-
tion Patterns for Subjective Expressions. Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP-03). ACL SIGDAT. 2003, 105-112. 
Kenneth Ward Church and Robert L. Mercer. 1993.  
Introduction to the special issue on computational 
linguistics using large corpora. Computational Lin-
guistics 19:1-24 
Kenneth Ward Church and Patrick Hanks. 1989. Word 
Association Norms, Mutual Information and Lexi-
cography. Proceedings of the 26th Annual Confer-
ence of the Association for Computational Linguis-
tics(1989). 
Michael Gamon and Anthony Aue. 2005. Automatic 
identification of sentiment vocabulary: exploiting low 
association with known sentiment terms. In :ACL 
2005 Workshop on Feature Engineering,2005.  
Minqing Hu and Bing Liu. 2004. Mining Opinion Fea-
tures in Customer Reviews. Proceedings of Nineteeth 
National Conference on Artificial Intellgience 
(AAAI-2004), San Jose, USA, July 2004. 
Patrick Pantel and Dekang Lin. 2002. Document Clus-
tering with Committees. In Proceedings of ACM 
Conference on Research and Development in Infor-
mation Retrieval (SIGIR-02). pp. 199-206. Tampere, 
Finland. 
Peter D. Turney. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised Clas-
sification of Reviews. ACL 2002: 417-424 
Rakesh Agrawal and Ramakrishan Srikant. 1994. Fast 
algorithm for mining association rules. VLDB?94, 
1994. 
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and 
Toshikazu Fukushima. 2002. Mining Product Repu-
tations on the WEB, Proceedings of 8th ACM 
SIGKDD International Conference on Knowledge. 
Discover and Data Mining, (2002) 341-349 
Ted Dunning. 1993.  Accurate methods for the statistics 
of surprise and coincidence. Computational Linguis-
tics 19:61-74 
Theresa Wilson , Janyce Wiebe, and Rebecca Hwa. 
2006. Recognizing strong and weak opinion clauses.  
Computational Intelligence 22 (2): 73-99. 
295
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1121?1128
Manchester, August 2008
Diagnostic Evaluation of Machine Translation Systems Using Auto-
matically Constructed Linguistic Check-Points 
Ming Zhou1, Bo Wang2, Shujie Liu2, Mu Li1, Dongdong Zhang1, Tiejun Zhao2 
1Microsoft Research Asia 
Beijing, China 
{mingzhou,muli,dozhang} 
@microsoft.com 
 
2Harbin Institute of Technology 
Harbin, China 
{bowang,Shujieliu,tjzhao} 
@mtlab.hit.edu.cn 
 
 
?Abstract 
We present a diagnostic evaluation plat-
form which provides multi-factored eval-
uation based on automatically con-
structed check-points. A check-point is a 
linguistically motivated unit (e.g. an am-
biguous word, a noun phrase, a verb~obj 
collocation, a prepositional phrase etc.), 
which are pre-defined in a linguistic tax-
onomy. We present a method that auto-
matically extracts check-points from pa-
rallel sentences. By means of check-
points, our method can monitor a MT 
system in translating important linguistic 
phenomena to provide diagnostic evalua-
tion. The effectiveness of our approach 
for diagnostic evaluation is verified 
through experiments on various types of 
MT systems. 
1 Introduction 
Automatic MT evaluation is a crucial issue for 
MT system developers. The state-of-the-art me-
thods for automatic MT evaluation are using an 
n-gram based metric represented by BLEU (Pa-
pineni et al, 2002) and its variants. Ever since its 
invention, the BLEU score has been a widely 
accepted benchmark for MT system evaluation. 
Nevertheless, the research community has been 
aware of the deficiencies of the BLEU metric 
(Callison-Burch et al, 2006). For instance, 
BLEU fails to sufficiently capture the vitality of 
natural languages: all grams of a sentence are 
                                                 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
treated equally ignoring their linguistic signific-
ance; only consecutive grams are considered ig-
noring the skipped grams of certain linguistic 
relations; candidate translation gets acknowl-
edged only if it uses exactly the same lexicon as 
the reference ignoring the variation in lexical 
choice. Furthermore, BLEU is useful for opti-
mizing and improving statistical MT systems but 
it has shown to be ineffective in comparing sys-
tems with different architectures (e.g., rule-based 
vs. phrase-based) (Callison-Burch et al,  2006).  
    Another common deficiency of the state-of-
the-art evaluation approaches is that they cannot 
clearly inform MT developers on the detailed 
strengths and flaws of an MT system, and there-
fore there is no way for us to understand the ca-
pability of certain modules of an MT system, and 
the capability of translating certain kinds of lan-
guage phenomena. For the purpose of system 
development, MT developers need a diagnostic 
evaluation approach to provide the feedback on 
the translation ability of an MT system with re-
gard to various important linguistic phenomena.     
    We propose a novel diagnostic evaluation ap-
proach. Instead of assigning a general score to an 
MT system we evaluate the capability of the sys-
tem in handling various important linguistic test 
cases called Check-Points. A check-point is a 
linguistically motivated unit, (e.g. an ambiguous 
word, a noun phrase, a verb~obj collocation, a 
prepositional phrase etc.) which are pre-defined 
in a linguistic taxonomy for diagnostic evalua-
tion. The reference of a check-point is its corres-
ponding part in the target sentence. The evalua-
tion is performed by matching the candidate 
translation corresponding to the references of the 
check-points. The extraction of the check-points 
is an automatic process using word aligner and 
parsers. We control the noise of the word aligner 
and parsers within tolerable scope by selecting 
1121
reliable subset of the check-points and weighting 
the references with confidence.  
    The check-points of various kinds extracted in 
this way have shown to be effective in perform-
ing diagnostic evaluation of MT systems. In ad-
dition, scores of check-points are also approved 
to be useful to improve the ranking of MT sys-
tems as additional features at sentence level and 
document level. 
The rest of the paper is structured in the fol-
lowing way:  Section 2 gives the overview of the 
process of the diagnostic evaluation. Section 3 
introduces the design of check-point taxonomy. 
Section 4 explains the details of construction of 
check-point database and the methods of reduc-
ing the noise of aligner and parsers. Section 5 
explains the matching approach. In Section 6, we 
introduce the experiments on different MT sys-
tems to demonstrate the capability of the diag-
nostic evaluation. In Section 7, we show that the 
check-points can be used to improve the current 
ranking methods of MT systems. Section 8 com-
pares our approach with related evaluation ap-
proaches. We conclude this work in Section 9. 
2 Overview of Diagnostic Evaluation 
In our implementation, we first build a check-
point database encoded in XML by associating a 
test sentence with qualified check-points it con-
tains. This process can be described as following 
steps: 
 
? Collect a large amount of parallel sen-
tences from the web or book collections. 
? Parse the sentences of source language 
and target language. 
? Perform the word alignments between 
each sentence pair. 
? For each category of check-points, extract 
the check-points from the parsed sentence 
pairs. 
? Determine the references of each check-
point in source language based on the 
word alignment.  
 
   With the extracted check-point database, the 
diagnostic evaluation of an MT system is per-
formed with the following steps: 
 
? The test sentences are selected from the 
database based on the selected categories 
of check-points to be evaluated. 
? For each check-point, we calculate the 
number of matched n-grams of the refer-
ences against the translated sentence of 
the MT system.  The credit of the MT sys-
tem in translating this check-point is ob-
tained after necessary normalization. 
? The credit of a category can be obtained 
by summing up the credits of all check-
points of this category. Then the credit of 
an MT system can be obtained by sum-
ming up the credits of all categories. 
? Finally, scores of system, category groups 
(e.g. Words), single category (e.g. Noun), 
and detail information of n-gram matching 
of each check-point are all provided to the 
developers to diagnose the MT system. 
3 Linguistic Check-Point Taxonomy 
The taxonomy of automatic diagnostic evaluation 
should be widely accepted so that the diagnostic 
results can be explained and shared with each 
other. We will also need to remove the sophisti-
cated categories that are out of the capability of 
current NLP tools to recognize.  
In light of this consideration, for Chinese-
English machine translation, we adopted the ma-
nual taxonomy introduced by (Lv, 2000; Liu, 
2002) and removed items that are beyond the 
capability of our parsers. The taxonomy includes 
typical check-pints at word, phrase and sentence 
levels. Some examples of the representative 
check-points at different levels are provided be-
low: 
 
?  Word level check-points: 
    a. Preposition word e.g., ?(in), ?(at) 
    b. Ambiguous word e.g., ?(play) 
    c. New word1 e.g., ??(Punk)  
?  Phrase level check-points: 
    a. Collocation. e.g., ??-??(fired ? food)  
    b. Repetitive word combination. e.g., ??
(have a look) 
    c. Subjective-predicate phrase e.g., ?*?, 
(he*said) 
     ? Sentence level check-points:  
a. ?BA? sentence 2 : ?? (BA)???? . 
(He took away the book.) 
b. ?BEI? sentence3????(BEI)???. 
(The vase was broken.)   
                                                 
1 New words are the terms extracted from web which can be 
a name entity or popular words emerging recently.   
2 In a ?BA? sentence, the object which normally follows the 
verb occurs preverbally, marked by word ?BA?. 
3 ?BEI? sentence is a kind of passive voice in Chinese 
marked by word ?BEI?. 
1122
    Our implementation of Chinese-English 
check-point taxonomy contains 22 categories and 
English-Chinese check-point taxonomy contains 
20 categories. Table 1 and 2 show the two 
taxonomies. In practice, any tag in parsers (e.g. 
NP) can be easily added as new category. 
 
Word level 
Ambiguous word New word Idiom 
Noun Verb Adjective 
Pronoun Adverb Preposition 
Quantifier Repetitive word Collocation 
Phrase level 
Subject-predicate 
phrase 
Predicate-object 
 phrase 
Preposition-
object phrase 
Measure phrase Location phrase  
Sentence level 
BA sentence BEI sentence SHI sentence 
YOU sentence Compound sentence 
Table 1:  Chinese check-point taxonomy 
 
Word level 
Noun Verb (with Tense) Modal verb 
Adjective Adverb Pronoun 
Preposition Ambiguous word Plurality 
Possessive Comparative & Superlative  degree 
Phrase level 
Noun phrase Verb phrase Adjective 
phrase 
Adverb phrase Preposition phrase  
Sentence level 
Attribute clause Adverbial clause Noun clause 
Hyperbaton  
Table 2: English check-point taxonomy 
4 Construction of Check-Point Data-
base 
Given a bilingual corpus with word alignment, 
the construction of check-point database consists 
of following two steps. First, the information of 
pos-tag, dependency structure and constituent 
structure can be identified with parsers. Then 
check-points of different categories are identified. 
Check-points of word-level categories such as 
Chinese idiom and English ambiguous words are 
extracted with human-made dictionaries, and the 
check-points of New-Word are extracted with a 
new word list mined from the web. A set of hu-
man-made rules are employed to extract certain 
categories involving sentence types such as com-
pound sentence.  
    Second, for a check-point, with the word 
alignment information, the corresponding target 
language portion is identified as the reference of 
this check-point. The following example illu-
strates the process of extracting check-points 
from a parallel sentence pair.  
?  A Chinese-English sentence pair: 
  ?????????. 
    They opposed the building of reserve funds. 
?  Word segmentation and pos-tagging: 
  ??/R ??/V ??/V ???/N ./P 
?  Parsing result (e.g.  a dependency result): 
    (SUB, 1/??, 0/??)  (OBJ, 1/??, 2/?
?) (OBJ, 2/??, 3/???) 
?  Word alignment: 
     (1; 1); (2; 2); (3; 4); (4; 6,7);   
?   The check-points in table 3 are extracted: 
 
Table 3: Example of check-point extraction 
 
    To extract the categories of check-points of 
different schema of syntactic analysis such as 
constitute structure and dependency structure, 
three parsers including a Chinese skeleton parser 
(a kind of dependency parser) (Zhou, 2000), 
Stanford statistical parser and Berkeley statistical 
parser (Klein 2003) are used to parse the Chinese 
and English sentences.  As explained in next sec-
tion, these multiple parsers are also used to select 
high confident check-points. To get word align-
ment, an existing tool GIZA++ (Och 2003) is 
used.  
4.1 Reducing the Noise of the Parser 
The reliability of the check-points mainly 
depends on the accuracy of the parsers. We can 
achieve high quality word level check-points 
with the state-of-the-art POS tagger (94% 
precision) and dictionaries of various purposes. 
For sentence level categories, the parser tags and 
manually compiled rules can also achieve 95% 
accuracy. For some kinds of categories at phrase 
level which parsers cannot produce high 
accuracy, we only select the check-points which 
can be identified by multiple parsers, that is, 
adopt the intersection of the parsers results. 
Table 4 shows the improvement brought by this 
approach. Column 1 and 2 shows the precision of 
6 major types of phrases in Stanford and 
Berkeley parser. Column 3 shows the precision 
of intersection and column 4 shows the reduction 
of the number of check-points when adopting the 
intersection results. The test corpus is a part of 
Category Check-point Reference 
New word ??? reserve funds 
Ambiguous word ?? building 
Predicate ? object 
phrase 
????? the building of  
reserve funds 
Subject-predicate 
phrase 
???? They opposed 
1123
Penn Chinese Treebank which is not contained in 
the training corpus of two statistical parsers. 
(Klein 2003).  
 
 Stf% Brk% Inter% Tpts redu% 
NP 87.37 86.03 95.83 17.06 
VP 87.34 82.87 95.23 19.68 
PP 90.60 88.56 96.00 11.50 
QP 98.12 92.90 99.21 6.31 
ADJP 91.95 90.87 96.41 10.20 
ADVP 95.21 94.25 92.64 3.92 
Table 4:  Precision of parsers and their intersec-
tion (Stf is Stanford, Brk is Berkelry) 
 
4.2 Alleviating the Impact of Alignment 
Noise 
Except for sentence level check-points whose 
references are the whole sentences and New 
Word, Idiom check-points whose references are 
extracted from dictionary, the quality of the ref-
erences are impacted by the alignment accuracy. 
To alleviate the noise of aligner we use the lexi-
cal dictionary to check the reliability of refer-
ences. Suppose c is a check-point, for each refer-
ence c.r of c we calculate the dictionary match-
ing degree DM(c.r) with the source side c.s of c: 
 
)1()).(
)).(,.(,1.0().( rcWordCnt
scDicrcCoCntMaxrcDM ?  
 
    Where Dic(x) is a word bag contains all words 
in the dictionary translations of each source word 
in x. CoCnt(x, y) is the count of the common 
words in x and y. WordCnt(x) is the count of 
words in x. Specially, if c.r is not obtained based 
on alignment DM(c.r) will be 1. Because the li-
mitation of dictionary, a zero DM score not al-
ways means the reference is completely wrong, 
so we force the DM score to be not less than a 
minimum value (e.g. 0.1). DM score will further 
be used in evaluation in section 5.  
    To better understand the reliability of the ref-
erences and explore whether increasing the num-
ber of check-points could also alleviate the im-
pact of noise, we built two check-point databases 
from a human-aligned corpus (with 60,000 sen-
tence pairs) and an automatically aligned corpus 
(using GIZA++) respectively and tested 10 dif-
ferent SMT systems4 with them. The Spearman 
correlation is calculated between two ranked lists 
of the 10 evaluation results against the two data-
                                                 
4 These systems are derived from an in-house phrase based 
SMT engine with different parameter sets. 
bases. A higher correlation score means that the 
impact of the mistakes in word alignment is 
weaker. The experiment is repeated on 6 subsets 
of the database with the size from 500 sentences 
to 16K sentences to check the impact of the cor-
pus size. 
    At system level, high correlations are found at 
different corpus sizes. At category level, correla-
tions are found to be low for some categories at 
small size and become higher at larger corpus 
size. The results indicate that the impact of the 
alignment quality can be ignored if the corpus 
size is at large scale. As the check-points can be 
extracted fully automatically, increasing the size 
of check-point database will not bring extra cost 
and efforts. Empirically, the proper scale is set to 
be 2000 or more sentences according to the Ta-
ble 6. 
 
Table 6: Impact of word alignment at different 
sizes of test corpus. 
5 Matching Check-Points for Evalua-
tion 
Evaluation can be carried out at multiple options: 
for certain linguistic category, a group of catego-
ries or entire taxonomy. For instance, in Chinese-
English translation task, if a MT developer 
would like to know the ability to translate idiom, 
then a number of parallel sentences containing 
idiom check-points are selected from the data-
base. Then the system translation sentences are 
matched to the references of the check-points of 
idioms.  
 500 1K 2K 4K 8K 16K 
Ambiguous 
word 
0.98 0.98 0.98 0.98 0.96 0.98 
Noun 0.93 0.99 0.99 0.89 0.8 0.86 
Verb 0.97 0.97 0.99 0.99 0.95 0.92 
Adjective 0.16 0.19 0.57 0.75 0.77 0.97 
Pronoun 0.96 1 0.93 0.99 0.97 0.99 
Adverb 0.38 0.77 0.8 0.96 0.72 0.84 
Preposition 0.56 0.86 0.9 0.9 0.97 0.96 
Quantifier 1 0.46 0.46 0.98 0.85 0.96 
Repetitive 
Word 
0.99 0.99 0.97 0.89 0.73 0.95 
Collocation 0.42 0.77 0.77 0.77 0.73 0.88 
Subject-
predicate 
phrase 
0.06 0.8 0.95 1 0.96 0.84 
Predicate-
object phrase 
0.84 0.96 0.78 0.7 0.78 0.88 
Preposition-
object phrase 
0.51 0.5 0.93 0.95 0.87 0.99 
Measure 
phrase 
0.91 0.67 0.95 0.95 0.87 0.97 
Location 
phrase 
0.62 0.54 0.55 0.55 0.85 0.89 
SYSTEM 0.95 0.95 0.98 0.99 0.97 0.98 
1124
To calculate the credit at different occasions of 
matching, similar to BLEU, we split each refer-
ence of a check-point into a set of n-grams and 
sum up the gains over all grams as the credit of 
this check-point. Especially, if the check-point is 
not consecutive we use a special token (e.g. ?*?) 
to represent a component which can be wildcard 
matched by any word sequence. We use the fol-
lowing examples to demonstrate the splitting and 
matching of grams.  
 
?  Consecutive check-point: 
    Check- point: ??? 
    Reference: playing a drum 
    Candidate translation:  He is playing a drum.  
    Matched n-grams: playing; a; drum; playing a; 
a drum; playing a drum  
 
?   Not consecutive check-point: 
    Check- point: ??*?   
    Reference: They*playing   
    Candidate translation: They are playing cop 
per drum. 
    Matched n-grams: They; playing; They * play-
ing 
    Additionally, to match word inflections, 3 dif-
ferent options of matching granularity are de-
fined as follows.  
?  Normal: matching with exact form. 
?  Lower-case: matching with lowercase. 
?  Stem: matching with the stem of the word. 
 
    For a check-point c and references set R of c, 
we select the r* in R which matches the transla-
tion best based on formula (2).  
 
 
 
    
 
When we calculate the recall of a set of check-
points C (C can be a single check-point, a cate-
gory or a category group), r* of each check-point 
c in C are merged into one reference set R* and 
the recall of C is obtained using formula (3) on 
R*. 
 
 
 
 
 
 
A penalty is also introduced to punish the re-
dundancy of candidate sentences, where length(T) 
is the average length of all translation sentences 
and length(R) is the average length of all refer-
ence sentences. 
 
 
 
 
 
Then, the final score of C will be: 
 
)5()(Re)( PenaltyCCScore ??
 
6 Experiments on MT System Diagnos-
es 
In this section, to demonstrate the ability of our 
approach in the diagnoses of MT systems, we 
apply diagnostic evaluation to 3 statistical MT 
(SMT) systems and a rule-based MT (RMT) sys-
tem respectively. We compare two SMT systems 
to understand the strength and shortcoming of 
each of them, and also compare a SMT system 
with the RMT system. The test corpus is NIST05 
test data with 54852 check-points. 
    First SMT system (system A) is an implemen-
tation of classical phrase based SMT. The second 
SMT system (system B) shares the same decoder 
with system A and introduces a preprocess to 
reorder the long phrases in source sentences ac-
cording to the syntax structure before decoding 
(Chiho Li et al, 2007). The third SMT system 
(system C) is a popular internet service and the 
RMT system (system D) is a popular commercial 
system.  
    In the first experiment, we diagnose the sys-
tem A and B and compare the results as shown in 
table 7. When evaluated using BLEU, system B 
achieved a 0.005 points increase on top of system 
A which is not a very significant difference. The 
diagnostic results in table 7 provide much richer 
information. The results indicate that two sys-
tems perform similar at the word level categories 
while at all phrase level categories, system B 
performs better. This result reflects the benefit 
from the reordering of complex phrases in sys-
tem B. Paired t-statistic score for each pair of 
category scores is also calculated by repeating 
the evaluation on a random copy of the test set 
with replacement (Koehn 2004). An absolute 
score beyond 2.17 of paired t-statistic means the 
difference of the samples is statistically signifi-
cant (above 95%). Table 8 and 9 show an in-
stance of the check-point and its evaluation in 
this experiment. 
)2()
)'(
)(
)((maxarg
'
*
?
?
??
??
? ?
?
??
rgramn
rgramn
Rr gramnCount
gramnMatch
rDMr
)4(
1
)()(
)(
)(
??
?
?
? ?
?
Otherwise
RlengthTlengthif
Tlength
Rlength
Penalty
)3(
))'()((
))()((
)Re(
*
*
' ''
'? ?
? ?
? ??
? ??
??
??
?
Rr rgran
Rr rgramn
gramnCountrDM
gramnMatchrDM
C
1125
 System A System B T score 
WORDs 
Idiom 0.1933 0.2370 13.38 
Adjective 0.5836 0.5577 -17.43 
Pronoun 0.7566 0.7344 -13.49 
Adverb 0.5365 0.5433 7.11 
Preposition 0.6529 0.6456 -6.21 
Repetitive word 0.3363 0.3958 9.86 
PHRASEs 
Subject-predicate 0.5117 0.5206 7.36 
Predicate-object 0.4041 0.4180 15.52 
Predicate-complement 0.4409 0.5125 9.51 
Measure phrase 0.5030 0.5092 3.56 
Location phrase 0.5245 0.5338 2.83 
GROUPs 
WORDs 0.4839 0.4855 2.03 
PHRASEs 0.4744 0.4964 13.97 
SYSTEM (Linguistic) 0.4263 0.4370 16.50 
SYSTEM (BLEU) 0.3564 0.3614 7.91 
Table 7: Diagnose of SMT systems 
 
Source Sentence ????????????????
??????? 
Category Preposition_Object_Phrase 
Check-Point ? ? ?? 
Reference 1 in this country  DM = 0.5 
Reference 2 in his country  DM = 0.5 
System A Translation but the prime minister of thailand Dex-
in vowed to continue in domestic the 
search. 
System B Translation but the prime minister of thailand Dex-
in vowed to continue the search in his 
country. 
Table 8: An instance of the check-point. 
 
 System A System B 
Ref 1: Match/Total 1/6 2/6 
Ref 2: Match/Total 1/6 6/6 
Score 0.17 1 
Table 9: N-gram matching rate and scores. 
 
Table 10: Diagnose of SMT and RMT. 
 
In the second experiment, we diagnose system 
C and D and compare the results. The BLEU 
score of system C is 0.3005 and system D is 
0.2606. Table 10 shows the diagnostic results on 
categories with significant differences. Scores 
calculated with 3 matching options described in 
section 5 are given (?Lower? means Lowercase. 
The scores are listed in the form ?SMT 
score/RMT score?). The diagnostic results indi-
cate that system C performs better on most cate-
gories than system D, but system D performs 
better on categories like idiom, pronoun and pre-
position. This result reveals a key difference be-
tween two types of MT systems: the SMT works 
well on the open categories that can be handled 
by context, while the RMT works well on closed 
categories which are easily translated by linguis-
tic rules. 
    As the results of two experiments demonstrate, 
the diagnostic evaluation provides rich informa-
tion of the capability of translating various im-
portant linguistic categories beyond a single sys-
tem score. It successfully distinguishes the spe-
cific difference between the MT systems whose 
system level performance is similar. It can also 
diagnose the MT system with different architec-
tures. Diagnostic evaluation tells the developers 
about the direction to improve the system. Along 
with the scores of categories, the diagnostic 
evaluation provides the system translation and 
references at every check-point so that the devel-
opers can trace and understand about how the 
MT system works on every single instance. 
7 Experiments on Ranking MT Systems 
Offering a general evaluation at system level is 
the major goal of state-of-the-art evaluation me-
thods including widely accepted n-gram metrics. 
The absence of linguistic knowledge in BLEU 
motivated many work to integrate linguistic fea-
tures into evaluation metric. In (Yang 2007), the 
evaluation of SMT systems is alternately formu-
lated as a ranking problem. Different linguistic 
features are combined with BLEU such as 
matching rate of dependency relations of transla-
tion candidates against the reference sentences. 
The experiments demonstrate that the dependen-
cy matching rate feature can increase the ranking 
accuracy in some cases. Compared to dependen-
cy structure, the linguistic categories in our ap-
proach showcase more extensive features. It 
would be interesting to see whether the linguistic 
categories can be used to further improve the 
ranking of SMT systems.  
    In experiments, we use the scores of linguistic 
categories, dependency matching rate, scores of 
BLEU and other popular metrics as ranking fea-
tures of MT systems and trained by Ranking 
SVM of SVMlight (Joachims, 1998). We per-
formed the ranking experiments on ACL 2005 
workshop data, ranking 7 MT translations with 
three-fold cross-validation both on sentence level 
and document level. The Spearman score is used 
Type Normal Lower Stem 
Ambiguous word 0.49/0.42 0.50/0.42 0.53/0.46 
New word 0.13/0.13 0.37/0.32 0.42/0.35 
Idiom 0.43/0.66 0.46/0.67 0.51/0.71 
Pronoun 0.60/0.68 0.69/0.75 0.66/0.75 
Preposition 0.38/0.42 0.42/0.45 0.43/0.46 
Collocation 0.66/0.54 0.66/0.55 0.70/0.56 
Subject-predicate 
phrase 
0.46/0.30 0.51/0.36 0.58/0.42 
Predicate-object 
phrase 
0.37/0.25 0.37/0.26 0.47/0.29 
Compound sentence 0.22/0.16 0.23/0.16 0.23/0.17 
1126
to calculate the correlation with human assess-
ments. Table 11 and 12 show the results of the 
different feature sets on sentence level and doc-
ument level respectively. 
As shown in experiment results linguistic cat-
egories (LC), when used alone, are better related 
with human assessments than BLEU and GTM. 
When combined with the baseline metrics 
(BLEU & NIST), LC scores further improve the 
correlation score, better than dependence match-
ing rate (DP). LC scores are obtained by match-
ing the exact form of the words as ME-
TEOR(exact) does. NIST+LC combination score 
is better than METEOR(exact) at sentence and 
document level, and also better than ME-
TEOR(exact&syn) (syn means wn_synonymy 
module in METEOR) at document level. This 
results indicate the ability of linguistic features in 
improving the performance of ranking task. 
 
 Mean Correlation 
BLEU 4 0.245 
NIST 5 0.307 
GTM (e=2) 0.251 
METEOR(exact) 0.306 
METEOR(exact&syn) 0.327 
DP 0.246 
LC 0.263 
BLEU+DP 0.270 
BLEU+ LC 0.288 
BLEU+ DP +LC 0.307 
NIST+ LC 0.322 
NIST+ DP +LC 0.333 
 Table11: Sentence level ranking (DP means 
dependency and LC means linguistic categories)  
 
 Mean Correlation 
BLEU 4 0.305 
NIST 5 0.373 
GTM (e=2) 0.327 
METEOR(exact) 0.363 
METEOR(exact&syn) 0.394 
DP 0.323 
LC 0.369 
BLEU+DP 0.325 
BLEU+ LC 0.387 
BLEU+ DP +LC 0.332 
NIST+ LC 0.409 
NIST+ DP +LC 0.359 
Table 12: Document level ranking 
8 Comparison with Related Work 
This work is inspired by (Yu, 1993) with many 
extensions. (Yu, 1993) proposed MTE evaluation 
system based on check-points for English-
Chinese machine translation systems with human 
craft linguistic taxonomy including 3,200 pairs of 
sentences containing 6 classes of check-points. 
Their check-points were manually constructed by 
human experts, therefore it will be costly to build 
new test corpus while the check-points in our 
approach are constructed automatically. Another 
limitation of their work is that only binary score 
is used for credits while we use n-gram matching 
rate which provides a broader coverage of differ-
ent levels of matching.   
    There are many recent work motivated by n-
gram based approach. (Callison-Burch et al, 
2006) criticized the inadequate accuracy of eval-
uation at the sentence level. (Lin and Och, 2004) 
used longest common subsequence and skip-
bigram statistics. (Banerjee and Lavie, 2005) cal-
culated the scores by matching the unigrams on 
the surface forms, stemmed forms and senses. 
(Liu et al, 2005) used syntactic features and un-
labeled head-modifier dependencies to evaluate 
MT quality, outperforming BLEU on sentence 
level correlations with human judgment. (Gime-
nez and Marquez, 2007) showed that linguistic 
features at more abstract levels such as depen-
dency relation may provide more reliable system 
rankings. (Yang et al, 2007) formulates MT 
evaluation as a ranking problems leading to 
greater correlation with human assessment at the 
sentence level.  
There are many differences between these n-
gram based methods and our approach. In n-
gram approach, a sentence is viewed as a collec-
tion of n-grams with different length without dif-
ferentiating the specific linguistic phenomena. In 
our approach, a sentence is viewed as a collec-
tion of check-points with different types and 
depth, conforming to a clear linguistic taxonomy. 
Furthermore, in n-gram approach, only one gen-
eral score at the system level is provided which 
make it not suitable for system diagnoses, while 
in our approach we can give scores of linguistic 
categories and provide much richer information 
to help developers to find the concrete strength 
and flaws of the system, in addition to the gener-
al score. The n-gram based metric is not very 
effective when comparing the systems with dif-
ferent architectures or systems with similar gen-
eral score, while our approach is more effective 
in both cases by digging into the multiple lin-
guistic levels and disclosing the latent differenc-
es of the systems. 
9 Conclusion and Future Work 
This paper presents an automatically diagnostic 
evaluation methods on MT based on linguistic 
check-points automatically constructed. In con-
trast with the metrics which only give a general 
score, our evaluation system can give developers 
1127
feedback about the faults and strength of an MT 
system regarding specific linguistic category or 
category group. Different with the existing work 
based on check-points, our work presents an ap-
proach to automatically generate the check-point 
database. We show that although there is some 
noise brought from word alignment and parsing, 
we can effectively alleviate the problem by refin-
ing the parser results, weighting the reference 
with confidence score and providing large quan-
tity of check-points.  
    The experiments demonstrate that this method 
can uncover the specific difference between MT 
systems with similar architectures and different 
architectures. It is also demonstrated that the lin-
guistic check-points can be used as new features 
to improve the ranking task of MT systems.   
    Although we present the diagnostic evaluation 
method with Chinese-English language pair, our 
approach can be applied to other language pair if 
syntax parser and word aligner are available. 
    The taxonomy used in current proposal is 
based on the human-made linguistic system. An 
interesting problem to be explored in the future is 
whether the taxonomy could be constructed au-
tomatically from the parsing results.  
References 
Statanjeev Banerjee, Alon Lavie. 2005. METEOR: An 
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization 2005. 
Chris Callison-Burch, Miles Osborne, Philipp Koehn. 
2006. Re-evaluating the Role of Bleu in Machine 
Translation Research. In Proceedings of the Euro-
pean Chapter of the ACL 2006. 
Martin Chodorow, Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors, 
In 1st Meeting of the North America Chapter of the 
ACL, pp.140?147, 2000. 
Thorsten Joachims. 1998. Making Large-scale Sup-
port Vector Machine Learning Practical, In B. 
Scholkopf, C. Burges, A. Smola. Advances in Ker-
nel Methods: Support VectorMachines, MIT Press, 
Cambridge, MA, December. 
Jesus Gimenez and Llis Marquez. 2007. Linguistic 
features for automatic evaluation of heterogeneous 
MT systems, Workshop of statistical machine trans-
lation in conjunction with 45th ACL, 2007. 
Dan Klein, Christopher Manning. 2003. Accurate 
Unlexicalized Parsing, Proceedings of the 41th 
Meeting of the ACL, pp. 423-430. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of the 
EMNLP, Barcelona, Spain. 
Chiho Li, Minghui Li, Dongdong Zhang, Mu Li, 
Ming Zhou, Yi Guan. 2007. A Probabilistic Ap-
proach to Syntax-based Reor-dering for SMT. In 
Proceedings of the 45th  ACL, 2007. 
Chin-Yew Lin and Franz Josef Och. 2004. Automatic 
evaluation of machine translation quality using 
longest common subsequence and skip-bigram sta-
tistics. In Proceedings of the 42th ACL 2004.  
Ding Liu, Daniel Gildea. 2005. Syntactic Features for 
Evaluation of Machine Translation, ACL Work-
shop on Intrinsic and Extrinsic Evaluation Meas-
ures for Machine Translation and/or Summariza-
tion. 
Shuxin Liu. 2002. Linguistics of Contemporary Chi-
nese Language (in Chinese), Advanced Education 
Publisher. 
Jiping Lv. 2000. Foundation of Mandarin Grammar 
(in Chinese), Shangwu Publisher. 
Franz Josef Och, Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Mod-
els, Computational Linguistics, volume 29, number 
1, pp. 19-51 March 2003. 
Kishore Papieni, Salim Roukos, Todd Ward, Wei-Jing 
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation, In Proceedings of the 
ACL 2002. 
Shiwen Yu. 1993. Automatic evaluation of output 
quality for machine translation systems, In Pro-
ceedings of the evaluators? forum, April 21-24, 
1991, Les Rasses, Vaud, 1993.  
Yang Ye, Ming Zhou, Chinyew Lin. 2007. Sentence 
level machine translation evaluation as a ranking 
problem: one step aside from BLEU, In Workshop 
of statistical machine translation, in conjunction 
with 45th ACL, 2007. 
Ming Zhou. 2000, A Block-Based Robust Dependency 
Parser for Unrestricted Chinese Text. Proceedings 
of Second Chinese Language Processing Workshop, 
2000, held in conjunction with ACL, 2000.  
 
1128
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 125?128,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Statistical Machine Translation Model Based on a Synthetic
Synchronous Grammar
Hongfei Jiang, Muyun Yang, Tiejun Zhao, Sheng Li and Bo Wang
School of Computer Science and Technology
Harbin Institute of Technology
{hfjiang,ymy,tjzhao,lisheng,bowang}@mtlab.hit.edu.cn
Abstract
Recently, various synchronous grammars
are proposed for syntax-based machine
translation, e.g. synchronous context-free
grammar and synchronous tree (sequence)
substitution grammar, either purely for-
mal or linguistically motivated. Aim-
ing at combining the strengths of differ-
ent grammars, we describes a synthetic
synchronous grammar (SSG), which ten-
tatively in this paper, integrates a syn-
chronous context-free grammar (SCFG)
and a synchronous tree sequence substitu-
tion grammar (STSSG) for statistical ma-
chine translation. The experimental re-
sults on NIST MT05 Chinese-to-English
test set show that the SSG based transla-
tion system achieves significant improve-
ment over three baseline systems.
1 Introduction
The use of various synchronous grammar based
formalisms has been a trend for statistical ma-
chine translation (SMT) (Wu, 1997; Eisner, 2003;
Galley et al, 2006; Chiang, 2007; Zhang et al,
2008). The grammar formalism determines the in-
trinsic capacities and computational efficiency of
the SMT systems.
To evaluate the capacity of a grammar formal-
ism, two factors, i.e. generative power and expres-
sive power are usually considered (Su and Chang,
1990). The generative power refers to the abil-
ity to generate the strings of the language, and
the expressive power to the ability to describe the
same language with fewer or no extra ambigui-
ties. For the current synchronous grammars based
SMT, to some extent, the generalization ability of
the grammar rules (the usability of the rules for the
new sentences) can be considered as a kind of the
generative power of the grammar and the disam-
biguition ability to the rule candidates can be con-
sidered as an embodiment of expressive power.
However, the generalization ability and the dis-
ambiguition ability often contradict each other in
practice such that various grammar formalisms
in SMT are actually different trade-off be-
tween them. For instance, in our investiga-
tions for SMT (Section 3.1), the Formally SCFG
based hierarchical phrase-based model (here-
inafter FSCFG) (Chiang, 2007) has a better gen-
eralization capability than a Linguistically moti-
vated STSSG based model (hereinafter LSTSSG)
(Zhang et al, 2008), with 5% rules of the former
matched by NIST05 test set while only 3.5% rules
of the latter matched by the same test set. How-
ever, from expressiveness point of view, the for-
mer usually results in more ambiguities than the
latter.
To combine the strengths of different syn-
chronous grammars, this paper proposes a statisti-
cal machine translation model based on a synthetic
synchronous grammar (SSG) which syncretizes
FSCFG and LSTSSG. Moreover, it is noteworthy
that, from the combination point of view, our pro-
posed scheme can be considered as a novel system
combination method which goes beyond the ex-
isting post-decoding style combination of N -best
hypotheses from different systems.
2 The Translation Model Based on the
Synthetic Synchronous Grammar
2.1 The Synthetic Synchronous Grammar
Formally, the proposed Synthetic Synchronous
Grammar (SSG) is a tuple
G = ??
s
,?
t
, N
s
, N
t
, X,P?
where ?
s
(?
t
) is the alphabet set of source (target)
terminals, namely the vocabulary; N
s
(N
t
) is the
alphabet set of source (target) non-terminals, such
125
? ? ???
Figure 1: A syntax tree pair example. Dotted lines
stands for the word alignments.
as the POS tags and the syntax labels; X repre-
sents the special nonterminal label in FSCFG; and
P is the grammar rule set which is the core part of
a grammar. Every rule r in P is as:
r = ??, ?,A
NT
, A
T
, ???
where ? ? [{X}, N
s
,?
s
]
+
is a sequence of one or
more source words in ?
s
and nonterminals sym-
bols in [{X}, N
s
];? ? [{X}, N
t
,?
t
]
+
is a se-
quence of one or more target words in ?
t
and non-
terminals symbols in [{X}, N
t
]; A
T
is a many-to-
many corresponding set which includes the align-
ments between the terminal leaf nodes from source
and target side, and A
NT
is a one-to-one corre-
sponding set which includes the synchronizing re-
lations between the non-terminal leaf nodes from
source and target side; ?? contains feature values
associated with each rule.
Through this formalization, we can see that
FSCFG rules and LSTSSG rules are both in-
cluded. However, we should point out that the
rules with mixture of X non-terminals and syn-
tactic non-terminals are not included in our cur-
rent implementation despite that they are legal
under the proposed formalism. The rule extrac-
tion in current implementation can be considered
as a combination of the ones in (Chiang, 2007)
and (Zhang et al, 2008). Given the sentence pair
in Figure 1, some SSG rules can be extracted as
illustrated in Figure 2.
2.2 The SSG-based Translation Model
The translation in our SSG-based translation
model can be treated as a SSG derivation. A
derivation consists of a sequence of grammar rule
applications. To model the derivations as a latent
variable, we define the conditional probability dis-
tribution over the target translation e and the cor-
Input: A source parse tree T (f
J
1
)
Output: A target translation e?
for u := 0 to J ? 1 do
for v := 1 to J ? u do
foreach rule r = ??, ?,A
NT
, A
T
, ??? spanning
[v, v + u] do
if A
NT
of r is empty then
Add r into H[v, v + u];
end
else
Substitute the non-terminal leaf node pair
(N
src
, N
tgt
) with the hypotheses in the
hypotheses stack corresponding with N
src
?s
span iteratively.
end
end
end
end
Output the 1-best hypothesis in H[1, J] as the final translation.
Figure 3: The pseudocode for the decoding.
responding derivation d of a given source sentence
f as
(1) p
?
(d, e|f) =
exp
?
k
?
k
H
k
(d, e, f)
?
?
(f)
where H
k
is a feature function ,?
k
is the corre-
sponding feature weight and ?
?
(f) is a normal-
ization factor for each derivation of f. The main
challenge of SSG-based model is how to distin-
guish and weight the different kinds of derivations
. For a simple illustration, using the rules listed in
Figure 2, three derivations can be produced for the
sentence pair in Figure 1 by the proposed model:
d
1
= (R
4
, R
1
, R
2
)
d
2
= (R
6
, R
7
, R
8
)
d
3
= (R
4
, R
7
, R
2
)
All of them are SSG derivations while d
1
is also a
FSCFG derivation, d
2
is also a LSTSSG deriva-
tion. Ideally, the model is supposed to be able
to weight them differently and to prefer the better
derivation, which deserves intensive study. Some
sophisticated features can be designed for this is-
sue. For example, some features related with
structure richness and grammar consistency
1
of a
derivation should be designed to distinguish the
derivations involved various heterogeneous rule
applications. For the page limit and the fair com-
parison, we only adopt the conventional features
as in (Zhang et al, 2008) in our current implemen-
tation.
1
This relates with reviewers? questions: ?can a rule ex-
pecting an NN accept an X?? and ?. . . the interaction between
the two typed of rules . . . ?. In our study in progress, we
would design some features to distinguish the derivation steps
which fulfill the expectation or not, to measure how much
heterogeneous rules are applied in a derivation and so on.
126
R6
1?
BA
VV[2]NN[1]
1
VB[2] NP[1]?
PN
to me
TO PRP
PP
1
R7
penthe
DT NN
NP
??
NN
1
R4 Give 1? 1 X[1] X[2]X[2]? X[1] R5 X[1]X[1] ? 2 the pen 1 to 2me1??
R1 penthe 1?? 1 R3 theGive 2 pen 1? 2?? 1R2 to me 1? 1
R8
?
VV
Give
VB
11
Figure 2: Some synthetic synchronous grammar rules can be extracted from the sentence pair in Figure
1. R
1
-R
3
are bilingual phrase rules, R
4
-R
5
are FSCFG rules and R
6
-R
8
are LSTSSG rules.
2.3 Decoding
For efficiency, our model approximately search for
the single ?best? derivation using beam search as
(2) (
?
e,
?
d) = argmax
e,d
{
?
k
?
k
h
k
(d, e, f)
}
.
The major challenge for such a SSG-based de-
coder is how to apply the heterogeneous rules in a
derivation. For example, (Chiang, 2007) adopts a
CKY style span-based decoding while (Liu et al,
2006) applies a linguistically syntax node based
bottom-up decoding, which are difficult to inte-
grate. Fortunately, our current SSG syncretizes
FSCFG and LSTSSG. And the conventional de-
codings of both FSCFG and LSTSSG are span-
based expansion. Thus, it would be a natural way
for our SSG-based decoder to conduct a span-
based beam search. The search procedure is given
by the pseudocode in Figure 3. A hypotheses
stack H[i, j] (similar to the ?chart cell? in CKY
parsing) is arranged for each span [i, j] for stor-
ing the translation hypotheses. The hypotheses
stacks are ordered such that every span is trans-
lated after its possible antecedents: smaller spans
before larger spans. For translating each span
[i, j], the decoder traverses each usable rule r =
??, ?,A
NT
, A
T
, ???. If there is no nonterminal
leaf node in r, the target side ? will be added into
H[i, j] as the candidate hypothesis. Otherwise, the
nonterminal leaf nodes in r should be substituted
iteratively by the corresponding hypotheses until
all nonterminal leaf nodes are processed. The key
feature of our decoder is that the derivations are
based on synthetic grammar, so that one derivation
may consist of applications of heterogeneous rules
(Please see d
3
in Section 2.2 as a simple demon-
stration).
3 Experiments and Discussions
Our system, named HITREE, is implemented in
standard C++ and STL. In this section we report
Extracted(k) Scored(k)(S/E%) Filtered(k)(F/S%)
BP 11,137 4,613(41.4%) 323(0.5%)
LSTSSG 45,580 28,497(62.5%) 984(3.5%)
FSCFG 59,339 25,520(43.0%) 1,266(5.0%)
HITREE 93,782 49,404(52.7%) 1,927(3.9%)
Table 1: The statistics of the counts of the rules in
different phases. ?k? means one thousand.
on experiments with Chinese-to-English transla-
tion base on it. We used FBIS Chinese-to-English
parallel corpora (7.2M+9.2M words) as the train-
ing data. We also used SRI Language Model-
ing Toolkit to train a 4-gram language model on
the Xinhua portion of the English Gigaword cor-
pus(181M words). NIST MT2002 test set is used
as the development set. The NIST MT2005 test
set is used as the test set. The evaluation met-
ric is case-sensitive BLEU4. For significant test,
we used Zhang?s implementation (Zhang et al,
2004)(confidence level of 95%). For comparisons,
we used the following three baseline systems:
LSTSSG An in-house implementation of linguis-
tically motivated STSSG based model similar
to (Zhang et al, 2008).
FSCFG An in-house implementation of purely
formally SCFG based model similar to (Chiang,
2007).
MBR We use an in-house combination system
which is an implementation of a classic sentence
level combination method based on the Minimum
Bayes Risk (MBR) decoding (Kumar and Byrne,
2004).
3.1 Statistics of Rule Numbers in Different
Phases
Table 1 summarizes the statistics of the rules for
different models in three phases: after extrac-
tion (Extracted), after scoring(Scored), and af-
ter filtering (Filtered) (filtered by NIST05 test
set just, similar to the filtering step in phrase-
based SMT system). In Extracted phase, FSCFG
127
ID System BLEU4 #of used rules(k)
1 LSTSSG 0.2659?0.0043 984
2 FSCFG 0.2613?0.0045 1,266
3 HITREE 0.2730?0.0045 1,927
4 MBR(1,2) 0.2685?0.0044 ?
Table 2: The Comparison of LSTSSG, FSCFG
,HITREE and the MBR.
has obvious more rules than LSTSSG. However,
in Scored phase, this situation reverses. Inter-
estingly, the situation reverses again in Filtered
phase. The reasons for these phenomenons are
that FSCFG abstract rules involves high-degree
generalization. Each FSCFG abstract rule aver-
agely have several duplicates
2
in the extracted rule
set. Then, the duplicates will be discarded dur-
ing scoring. However, due to the high-degree gen-
eralization , the FSCFG abstract rules are more
likely to be matched by the test sentences. Con-
trastively, LSTSSG rules have more diversified
structures and thus weaker generalization capabil-
ity than FSCFG rules. From the ratios of two tran-
sition states, Table 1 indicates that HITREE can
be considered as compromise of FSCFG between
LSTSSG.
3.2 Overall Performances
The performance comparison results are presented
in Table 2. The experimental results show that
the SSG-based model (HITREE) achieves signifi-
cant improvements over the models based on the
two isolated grammars: FSCFG and LSTSSG
(both p < 0.001). From combination point of
view, the newly proposed model can be consid-
ered as a novel method going beyond the con-
ventional post-decoding style combination meth-
ods. The baseline Minimum Bayes Risk com-
bination of LSTSSG based model and FSCFG
based model (MBR(1, 2)) obtains significant im-
provements over both candidate models (both p <
0.001). Meanwhile, the experimental results show
that the proposed model outperforms MBR(1, 2)
significantly (p < 0.001). These preliminary re-
sults indicate that the proposed SSG-based model
is rather promising and it may serve as an alterna-
tive, if not superior, to current combination meth-
ods.
4 Conclusions
To combine the strengths of different gram-
mars, this paper proposes a statistical machine
2
Rules with identical source side and target side are du-
plicated.
translation model based on a synthetic syn-
chronous grammar (SSG) which syncretizes a
purely formal synchronous context-free gram-
mar (FSCFG) and a linguistically motivated syn-
chronous tree sequence substitution grammar
(LSTSSG). Experimental results show that SSG-
based model achieves significant improvements
over the FSCFG-based model and LSTSSG-based
model.
In the future work, we would like to verify
the effectiveness of the proposed model on vari-
ous datasets and to design more sophisticated fea-
tures. Furthermore, the integrations of more dif-
ferent kinds of synchronous grammars for statisti-
cal machine translation will be investigated.
Acknowledgments
This work is supported by the Key Program of
National Natural Science Foundation of China
(60736014), and the Key Project of the National
High Technology Research and Development Pro-
gram of China (2006AA010108).
References
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In computational linguistics, 33(2).
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL 2003.
Galley, M. and Graehl, J. and Knight, K. and Marcu,
D. and DeNeefe, S. and Wang, W. and Thayer, I.
2006. Scalable inference and training of context-
rich syntactic translation models In Proceedings of
ACL-COLING.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
04.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine transla-
tion. In Proceedings of ACL-COLING.
Keh-Yin Su and Jing-Shin Chang. 1990. Some key
Issues in Designing Machine Translation Systems.
Machine Translation, 5(4):265-300.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proceedings of LREC 2004, pages 2051-2054.
Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li,
Chew Lim Tan and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-HLT.
128
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 37?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
References Extension for the Automatic Evaluation of MT by             Syntactic Hybridization   Bo Wang, Tiejun Zhao, Muyun Yang, Sheng Li School of Computer Science and Technology Harbin Institute of Technology Harbin, China {bowang,tjzhao,ymy,sl}@mtlab.hit.edu.cn       Abstract Because of the variations of the languages, the coverage of the references is very important to the reference based automatic evaluation of machine translation systems. We propose a method to extend the reference set of the au-tomatic evaluation only based on multiple manual references and their syntactic struc-tures. In our approach, the syntactic equiva-lents in the reference sentences are identified and hybridized to generate new references. The new method need no external knowledge and can obtain the equivalents of long sub-segments of reference sentences. The experi-mental results show that using the extended reference set the popular automatic evaluation metrics achieve better correlations with the human assessments. 1 Introduction While human evaluation of machine translation output remains the most reliable method to assess translation quality, it is a costly and time consum-ing process. The development of automatic ma-chine translation evaluation metrics enables the rapid assessment of system output. By providing immediate feedback on the effectiveness of various techniques, these metrics have guided machine translation research and have facilitated rapid ad-vances in the state of the art. In addition, automatic evaluation metrics are useful in comparing the per-formance of multiple machine translation systems 
on a given translation task. Since automatic evalua-tion metrics are meant to serve as a surrogate for human judgments, their quality is determined by how well they correlate with assessors? preferences and how accurately they predicts human judg-ments. Although current methods for automatically evaluating machine translation output do not re-quire humans to assess individual system output, humans are nevertheless needed to generate a number of reference translations. The quality of machine-generated translations is determined by automatically comparing system output with these references. All current automatic evaluation met-rics are based on the various measures of the gen-eral similarity between the system translation and manual references. This kind of method has an ob-vious drawback: it does not account for combina-tions of lexical and syntactic differences that might occur between a perfectly fluent and accurately-translated machine output and a human reference translation (beyond variations already captured by the different reference translations themselves). Moreover, the set of human reference translations is unlikely to be an exhaustive inventory of ?good translations? for any given foreign language sen-tence. Therefore, it would be highly desirable to extend the coverage of the references for the simi-larity based evaluation methods. To match the system translation with various presentation of the same meaning, many work ha-ven been proposed to extend the references by generating lexical variations. The first strategy fo-cuses on the extension based on paraphrase identi-
37
fication (Lepage and Denoual, 2005; Lassner et al 2005; Zhou et al 2006; Kauchak and Barzilay, 2006; Owczarzak et al 2006; Owczarzak et al 2007). In this kind of method, the quality of system translations can be viewed as the extent to which the conveyed meaning matches the semantics of the reference translations, independent of sub-strings they may share. In short, all paraphrases of human-generated references should be considered ?good? translations. The second strategy extends the references with the synonymy (Banerjee and Lavie, 2005; Lassner et al 2005). This is an alter-nation to obtain lexical variations with synonymy dictionaries instead of the paraphrase. In this kind of method, the reference is matched against to the system translation with the pack of the synonymies of the reference words instead of the exact match-ing. Both two strategies can successfully capture the lexical variations and greatly extend the coverage of the references. But they still have two common deficiencies. The first is the demand of the external knowledge. Paraphrase based method need a mass of external corpus to extract paraphrases and syn-onymy based method need manually constructed semantic dictionaries. These demands seriously limit the application on various languages for which the external knowledge is absent. Another deficiency is that the two strategies cannot capture the equivalents of long sub-segments such as a clause. Synonymy based me-thod can only capture the equivalents of single words. Paraphrase based method can capture the equivalents of longer units but the length is still very narrow. In many cases, some long sub-segments can be varied with an entirely different presentation which cannot be decomposed into the variations of words or phrases. To address these problems we propose a novel strategy to generate variations presentation only using existing multiple manual references without any external knowledge. We identify the syntactic components on different level as the replaceable units and determine the syntactic equivalents of the components in the corresponding references. Then the equivalents of the syntactic components are hybridized into new references. The rest of the paper is organized as follows. Section 2 introduces the concept and identification of the syntactic equivalents. Section 3 proposes a process to hybridize the syntactic equivalents effi-
ciently. Experimental results are illustrated in sec-tion 4. We also include some related discussion in Section 5. Finally this work is concluded in Sec-tion 6. 2 Syntactic Equivalents  In our approach, we propose a novel method to obtain the equivalents of the sub-segments from the corresponding references to a single source sentence. A sub-segment can be a word, a phrase or longer unit such as a clause. As we know, the variations of the sentences to the same meaning can be distinguished into two categories. The first is the structural variations. In this case, presenta-tions employ the same words but arrange them in different structure. The second is lexical variations. In this case, presentations have the same structure but employ the different words. In practice, one reference sentence often has both of the two kinds of variations comparing with other corresponding reference sentences. As the previous works, we also focus on the lexical variations. The approach is that the equiva-lents of the words are not obtained by external knowledge. In our strategy, generally speaking, the equivalents of a sub-segment S in a reference sen-tence are identified as the sub-segments which play the same syntactic role in the same structure in the other corresponding references. The equivalents obtained in this way are called syntactic equiva-lents.  Suppose R1 and R2 is a corresponding reference sentence pair. T1 and T2 are the consecutive syntac-tic trees of R1 and R2 respectively. We formally define a syntactic equivalent pair between R1 and R2 with a 4-tuple:  <N1, N2, S1, S2>  where Ni is a non-terminal node in Ti and Si is the sub-segment which is covered by Ni. Then, all the syntactic equivalent pair R1 and R2 can be recur-sively identified using following process:  ?  The first syntactic equivalent pair <N1, N2, S1, S2> is identified where Ni is the root of Ti and Si= Ri. ?  Suppose <N1, N2, S1, S2> is a syntactic equivalent pair. {N11, N12, ?N1m} and { N21, N22, ?N2n} are the child nodes sequences of 
38
N1 and N2 respectively. If n=m and N1i= N2i (i.e. the child nodes sequence of N1 and N2 are exactly the same), for each node pair N1i and N2i a syntactic equivalent pair is identi-fied as < N1i, N2i, S1i, S2i>.  With this process, all equivalent pairs on differ-ent syntactic level can be identified by synchro-nously traveling the two trees from top to bottom. The following is an example of the identification of the equivalent pairs. Figure 1 gives out a refer-ence sentence pair and their syntactic trees. The nodes which are included in certain equivalent pair are surrounded by a rectangle.   (a)   (b)  Figure 1 An example of the identification of the syn-tactic equivalent pairs.    In this example, five equivalent pairs can be identified:  ?  <S, S, ?Machine translation develops con-stantly?, ?MT progresses persistently?> ?  <NP, NP, ?Machine translation?, ?MT?> ?  <VP, VP, ?develops constantly?, ?progresses persistently?> ?  <VV, VV, ?develops?, ?progresses?> ?  <ADV, ADV, ?constantly?, ?persistently?> 3 Hybridization of Syntactic Equivalents  The indentified syntactic equivalents pairs include the sub-segments which sharing the same role in the same syntactic structure. Because of this, we 
can obtain a variation of a reference sentence by switching the two sub-segments of an equivalent pair in this sentence. This operation did not change the structure of the sentence but only replace a sub-segment in the structure with its equivalent.  Consequently, two new references can be gener-ated by switching the two sub-segments of an equivalent pair between two reference sentences. Furthermore when we switch the sub-segments of all equivalent pairs between the two references, multiple new references are generated with various combinations of the switches. This operation is called the syntactic hybridization of the references which can be illustrated by following steps: Suppose R={ri}i=1?n is a reference set containing n reference sentences to a single source sentence. R? is the new reference set containing the original reference sentences and the hybridized reference sentences. R? can be obtained by formula (1):    where rooti is the root node of the syntactic tree of ri. Equ(nt) returns the set of all equivalent of the sub-segments covered by the tree node nt. The de-tailed process of Equ(nt) is:  Equ(nt):  Define set  equ = ? Add Seg(nt) to equ If nt is included in an equivalent pair <nt, nt?, s, s?> Add p? to equ Define childi=1?m is the m children of nt Define hybr = Equ(child1)?Equ(child2)??Equ(childm) Merge hybr into equ Return equ  where Seg(nt) is the sub-segment covered by the tree node nt. Operation S1? S2 generates the Carte-sian product of the sub-segment set S1 and S2, i.e. for each arbitrary sub-segment pair s1 and s2 se-lected from S1 and S respectively, we concatenate s1 and s2. Finally, the reduplicate references in R? are removed.   For the example in Section 2, eight hybridized references can be generated including the original two sentences:  
39
?  Machine Translation develops constantly ?  Machine Translation develops persistently ?  Machine Translation progresses constantly ?  Machine Translation progresses persistently  ?  MT develops constantly ?  MT develops persistently ?  MT progresses constantly ?  MT progresses persistently 4 Experiments  We will show experimental results in this section to verify the effectiveness of the extended set of hybridized reference sentences. In the experiments, multiple translations of the source language sen-tences are evaluated with several popular auto-matic evaluation metrics. The evaluation is carried out on sentence level using the original reference set and the extended reference set respectively. Finally, the Pearson?s correlations between the human assessments and evaluation scores using two reference set are calculated and compared.   The multiple translations and human assess-ments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 & 2 describes the detail of the two datasets. The popular automatic evaluation metrics in-clude BLEU (Papieni et al, 2002), GTM (Me-lamed et al, 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are ob-tained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43.  Table 3 & 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in BLEU is 4. The exponent of GTM is 2. ROUGE uses skip-bigram with a window of nine words. And METEOR is run in ?exact? mode.   Release Year 2006 Genre Newswire Number of segments 919 Source Language Chinese 
Target Language English Number of system transla-tions 7 Number of reference trans-lations 4 Human assessment scores Score 1-5, ade-quacy & fluency Table 1 Description of LDC2006T04  Release Year 2008 Genre Newswire Number of segments 249 Source Language Arabic Target Language English Number of system transla-tions 8 Number of reference trans-lations 4 Human assessment scores Score 1-7, ade-quacy  Table 2 Description of LDC2008E43  After the hybridization, each source sentence in LDC2006T04 has 31 corresponding reference sen-tences in average and each source sentence in LDC2008E43 has 66 corresponding reference sen-tences in average. The number of the references is greatly increased. And as shown in the results, the usage of the extended reference set improves the correlations with human assessments for all the metrics in most cases except the ROUGE on LDC 2008E43.  Metric Original Extended BLEU 0.3488 0.3564 GTM 0.3671 0.3681 ROUGE 0.4252 0.4325 METEOR 0.4686         0.4723 Table 3 Pearson?s correlations with human assess-ments on sentence level on LDC2006T04  Metric Original Extended BLEU 0.6092 0.6109 GTM 0.5434 0.5438 ROUGE 0.6628 0.6582 METEOR 0.7053         0.7089 Table 4 Pearson?s correlations with human assess-ments on sentence level on LDC2008E43  The following is a real instance in the experi-ments from LDC2008E43:  Four original references: 
40
 ?  Ten churches burned down in 10 days in the American state of Alabama ?  Burning of ten churches in ten days in the American state of Alabama ?  Ten churches set on fire in ten days in American state of Alabama ?  Torching of ten churches within ten days in American state of Alabama  Six additional references:  ?  Torching of ten churches in ten days in the American state of Alabama ?  Torching of ten churches within ten days in the American state of Alabama ?  Torching of ten churches in ten days in American state of Alabama ?  Burning of ten churches within ten days in American state of Alabama ?  Burning of ten churches within ten days in the American state of Alabama ?  Burning of ten churches in ten days in American state of Alabama  The syntactic structure of the original references:  ?  (TOP (S (NPB (CD Ten) (NNS Churches)) (VP (VBN Burned) (PP (IN Down) (PP (IN in) (NP (NPB (CD 10) (NNS Days)) (PP (IN in) (NP (NPB (DT the) (NNP American) (NNP State)) (PP (IN of) (NPB (NNP Ala-bama))))))))))) ?  (TOP (NP (NPB (NN Burning)) (PP (IN of) (NP (NPB (CD Ten) (NNS Churches)) (PP (IN in) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (DT the) (NNP American) (NNP State)) (PP (IN of) (NPB (NNP Alabama))))))))))) ?  (TOP (S (NPB (CD Ten) (NNS Churches)) (VP (VB Set) (PP (IN on) (NPB (NN Fire))) (PP (IN in) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (NNP Ameri-can) (NNP State)) (PP (IN of) (NPB (NNP Alabama)))))))))) ?  (TOP (NP (NPB (NNP Torching)) (PP (IN of) (NP (NPB (CD Ten) (NNS Churches)) (PP (IN within) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (NNP Ameri-
can) (NNP State)) (PP (IN of) (NPB (NNP Alabama)))))))))))  To investigate the distribution of the equivalents we also perform several statistics about the count and the length of the syntactic nodes. In table 5, we list the information about the count of the nodes. The first row is the average words count per refer-ence sentence. The second and third row is the count of all tree nodes and equivalent nodes in all references respectively. The fourth and fifth row is the average count of tree nodes and equivalent nodes per reference sentence respectively.   2006T04 2008E43 Average length of  reference 31.52 34.43 Total tree nodes 211231 62569 Total equivalent nodes 21807 10073 Average tree nodes 57.46 62.82 Average equivalent nodes 5.93 10.11 Table 5 Counts of the tree nodes and equivalent nodes in references.  We also investigate the distribution of the length (count of covered words) of the nodes. First, we count the tree nodes and equivalent nodes whose length is from 1 word to 50 words. Then we calcu-late the pro-portion of equivalent nodes and tree nodes for each length. Figure 2 and 3 illustrate the distribution of absolute count of the equivalent nodes. The X-axis is the length of the nodes and the Y-axis is the count. Figure 4 and 5 illustrate the distribution of the proportions on two datasets re-spectively. The X-axis is the length of the nodes and the Y-axis is the proportion. The investigation reveals four main messages. First, the absolute counts of the short equivalents are much more than those of long equivalents as expected. Second, the proportion of the long equivalents is greater than those of short equiva-lents, this clarify that the reason of large amount of short equivalents is the large amount of short tree nodes. Third, also from the proportion of view we can see that the new method comparably bias to the long equivalents. This happens because the method adopts a top-down survey of the tree. Forth, the multiple references in Arabic-English data seem to match each other better than the references 
41
in Chinese-English data. Arabic-English references have much more equivalents than Chinese-English data and bias to long equivalents more significant.  
 Figure 2 Distribution of absolute length of equivalent node on LDC2006T04  
  Figure 3 Distribution of absolute length of equivalent node on LDC2008E43  
  Figure 4 Distribution of length proportion of equiva-lent nodes on LDC2006T04 
 Figure 5 Distribution of length proportion of equiva-lent nodes on LDC2008E43 5 Discussion  The experimental results verify the positive effect of the hybridized reference for the automatic eval-
evaluation in most cases. Though the improvement of the correlations is not very significant it is stable across the metrics in various styles. Compared with the previous works based on pa-raphrase and synonym the new method has three important advantages. The first is that the hybrid-ized reference can switch the long span sub-segments beyond the words and phrases.  The second is that the switch can be per-formed in multiple levels, i.e. a sub-segment can not only be replaced as a single unit but also can be varied by replacing some child sub-segments of it. It?s noticeable that the multiple level switches also make it possible to present some structural varia-tions by means of the lexical variations. In hybridi-zation, we can realize some structural variation between syntactic nodes by switch their parent node instead of reordering them directly.  The third advantage is that the new method needs no external knowledge which greatly facili-tates the application. But this advantage also re-sults in the main deficiency of this approach: the hybridization references cannot adopt any novel equivalents which are absent in existing references. This deficiency can be overcome by introducing the paraphrase and synonym into the syntactic hy-bridization. It should be indicated that though the hybridiza-tion process generate many new references not all of the new references are reasonable.  In table 6 we compare the effect of hybridized references and manual references with more details on LDC2006T04. In the table, the first column is the contents of the references for each source sen-tence. ?Manual? means the manual references and the number in front of it indicates how many man-ual references are provided. ?Hybr? means the hy-bridized references generated from the manual references in front of the ?+?. The second column is the Pearson?s correlations between human as-sessments and the BLEU scores using the corre-sponding reference set. Besides the set containing 4 references the other correlations are the average of the correlations based on all possible subset con-taining certain number of references. For example correlation of ?2 Manual? is the average of the cor-relations based on 6 possible subset containing 2 references.  Reference Set Correlation 1 Manual 0.2565 
42
2 Manual 0.3057 2 Manual+ Hybr 0.3082 3 Manual 0.3316 3 Manual + Hybr 0.3369 4 Manual 0.3488 4 Manual+ Hybr 0.3564 Table 6 Pearson?s correlations based on incremental reference set  As shown in the Table 6 hybridized references can improve the correlations with human assess-ments on different sizes of manual references set. But it also indicated that though hybridization can generate a mass of novel references the new refer-ences is always not more effective than even one additional manual references. This tells us that the quality of the hybridized references still need to be further refined. Another message revealed by the table is that with the increase of the number of manual refer-ences the improvement of correlation made by ad-ditional manual references is decreasing. However, the improvement made by the hybridized is in-creasing. This happens because the number of hy-bridized references increases much faster than the number of manual references. There are still several noticeable deficiencies of this work. First, it only works when there are more than two existing references. This make it cannot be used to extend the single reference in mass bi-lingual corpus. Second, which is also the most im-portant one is that this method strongly focuses on the precision at the cost of recall. Though we have recognized many equivalents for each sentence but there are still many equivalents that share different context cannot be recognized. This will be our main future work. The last deficiency is the bias to the long equivalents. This problem is caused by the same reason with the second deficiency: this method define the equivalent with the same syntac-tic context. If two sub-nodes do not share the same parent it often have different brothers. 6 Conclusions and Future Work  In this work we present a novel method to extend the coverage of the reference set for the automatic evaluation of machine translation. The new method decomposes the existing references into sub-segments according to the syntactic structure. And then generate new reference sentences by hybridiz-
ing the equivalents of the segments which play the same syntactic role in corresponding references. In this way the new method can not only capture the equivalents of words and phrases like the other methods but also capture the equivalents of long sub-segments which are out of the capability of the other methods. Another important advantage of the new method is the no use of the external knowl-edge which greatly facilitates the application. Experimental results show that with the ex-tended reference set the state-of-the-arts automatic evaluation metrics achieve better correlation with the human assessments. In the future work, we will relax the restriction of the equivalent definition and try to recognize more equivalents. We will also introduce the para-phrase and synonyms into our method to see fur-ther improvement. Another interesting challenge is to hybridize the equivalents in the different order and present the structural variations directly. Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 60773066 and 60736014, the National High Tech-nology Development 863 Program of China under Grant No. 2006AA010108. References  Statanjeev Banerjee, Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgements. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Trans-lation and/or Summarization. M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. PhD Dissertation, Uni-versity of Pennsylvania. I. Dan Melamed, Ryan Green, Joseph P. Turian, 2003, Precision and recall of machine translation, In Pro-ceedings of HLT/NAACL 2003. David Kauchak, Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation, In Proceedings of the NAACL 2006. Dan Klein, Christopher Manning. 2003. Accurate Un-lexicalized Parsing. In Proceedings of the 41th Meet-ing of the ACL, pp. 423-430. Yves Lepage, Etienne Denoual. 2005. Automatic gen-eration of paraphrases to be used as translation refer-
43
ences in objective evaluation measures of ma-chine translation, In Proceedings of the IWP 2005. Karolina Owczarzak, Declan Groves, Josef Van Ge-nabith ,Andy Way. 2006. Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation, In Pro-ceedings of the Workshop on Statistical Ma-chine Translation. Karolina Owczarzak, Josef Van Genabith, Andy Way. 2007. Dependency-Based Automatic Evaluation for Machine Translation, In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation. Kishore Papieni, Salim Roukos, Todd Ward, Wei-Jing Zhu. 2002. BLEU: a method for automatic evalua-tion of machine translation, In Proceedings of the 40th Meeting of the ACL. Grazia Russo-Lassner, Jimmy Lin, Philip Resnik. 2005. Re-evaluating Machine Translation Results with Pa-raphrase Support, Technical Report LAMP-TR-125/CS-TR-4754/UMIACS-TR-2005-57, University of Maryland, College Park, MD. Chin-Yew Lin, Franz Josef Och. 2004. Automatic evaluation of machine translation quality using long-est common subsequence and skip-bigram sta-tistics. In Proceedings of the 42th  Meeting of the ACL. Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-evaluating Machine Translation Results with Pa-raphrase Support, In Proceedings of the EMNLP 2006. 
44
Coling 2010: Poster Volume, pages 1533?1540,
Beijing, August 2010
All in Strings: a Powerful String-based Automatic MT  
Evaluation Metric with Multiple Granularities 
 
Junguo Zhu1, Muyun Yang1, Bo Wang2, Sheng Li1, Tiejun Zhao1 
 
1 School of Computer Science and Technology, Harbin Institute of Technology 
{jgzhu; ymy; tjzhao; lish}@mtlab.hit.edu.cn 
2 School of Computer Science and Technology, Tianjin University  
bo.wang.1979@gmail.com 
 
 
Abstract 
String-based metrics of automatic ma-
chine translation (MT) evaluation are 
widely applied in MT research. Mean-
while, some linguistic motivated me-
trics have been suggested to improve 
the string-based metrics in sentence-
level evaluation. In this work, we at-
tempt to change their original calcula-
tion units (granularities) of string-based 
metrics to generate new features. We 
then propose a powerful string-based 
automatic MT evaluation metric, com-
bining all the features with various 
granularities based on SVM rank and 
regression models. The experimental 
results show that i) the new features 
with various granularities can contri-
bute to the automatic evaluation of 
translation quality; ii) our proposed 
string-based metrics with multiple gra-
nularities based on SVM regression 
model can achieve higher correlations 
with human assessments than the state-
of-art  automatic metrics. 
1 Introduction 
The automatic machine translation (MT) eval-
uation has aroused much attention from MT 
researchers in the recent years, since the auto-
matic MT evaluation metrics can be applied to 
optimize MT systems in place of the expensive 
and time-consuming human assessments. The 
state-of-art strategy to automatic MT evalua-
tion metrics estimates the system output quali-
ty according to its similarity to human refer-
ences. To capture the language variability ex-
hibited by different reference translations, a 
tendency is to include deeper linguistic infor-
mation into machine learning based automatic 
MT evaluation metrics, such as syntactic and 
semantic information (Amig? et al, 2005; Al-
brecht and Hwa, 2007; Gim?nez and M?rquez, 
2008). Generally, such efforts may achieve 
higher correlation with human assessments by 
including more linguistic features. Neverthe-
less, the complex and variously presented lin-
guistic features often prevents the wide appli-
cation of the linguistic motivated metrics. 
Essentially, linguistic motivated metrics in-
troduce additional restrictions for accepting the 
outputs of translations (Amig? et al, 2009).  
With more linguistic features attributed, the 
model is actually capturing the sentence simi-
larity in a finer granularity. In this sense, the 
practical effect of employing various linguistic 
knowledge is changing the calculation units of 
the matching in the process of the automatic 
evaluation. 
Similarly, the classical string-based metrics 
can be changed in their calculation units direct-
ly. For example, the calculation granularity in 
BLEU (Papineni et al, 2002) metric is word: 
n-grams are extracted on the basis of single 
word as well as adjacent multiple words. And 
the calculation granularity in PosBLEU 
(Popovi? and Ney, 2009) metric is Pos tag, 
which correlate well with the human assess-
ments. Therefore, it is straight forward to apply 
the popular string-based automatic evaluation 
metrics, such as BLEU, to compute the scores 
of the systems outputs in the surface or linguis-
1533
tic tag sequences on various granularities le-
vels. 
In this paper, we attempt to change the orig-
inal calculation units (granularities) of string-
based metrics to generate new features. After 
that, we propose a powerful string-based au-
tomatic MT evaluation metric, combining all 
the features with various granularities based on 
SVM rank (Joachims, 2002) and regression 
(Drucker et al, 1996) models. Our analysis 
indicates that: i) the new features with various 
granularities can contribute to the automatic 
evaluation of translation quality; ii) our pro-
posed string-based metrics with multiple gra-
nularities based on SVM regression model can 
achieve higher correlations with human as-
sessments than the state-of-art automatic me-
trics . 
The remainder of this paper is organized as 
follows: Section 2 reviews the related re-
searches on automatic MT evaluation. Section 
3 describes some new calculation granularities 
of string-based metrics on sentence level. In 
Section 4, we propose string-based metrics 
with multiple granularities based on SVM rank 
and regression models. In Section 5, we 
present our experimental results on different 
sets of data. And conclusions are drawn in the 
Section 6. 
2 Related Work on Automatic Ma-
chine Translation Evaluation 
The research on automatic string-based ma-
chine translation (MT) evaluation is targeted at 
a widely applicable metric of high consistency 
to the human assessments. WER (Nie?en et al, 
2000), PER (Tillmann et al, 1997), and TER 
(Snover et al, 2006) focuses on word error rate 
of translation output. GTM (Melamed et al, 
2003) and the variants of ROUGE (Lin and 
Och, 2004) concentrate on matched longest 
common substring and discontinuous substring 
of translation output according to the human 
references. BLEU (Papineni et al, 2002) and 
NIST (Doddington, 2002) are both based on 
the number of common n-grams between the 
translation hypothesis and human reference 
translations of the same sentence. BLEU and 
NIST are widely adopted in the open MT eval-
uation campaigns; however, the NIST MT 
evaluation in 2005 indicates that they can even 
error in the system level (Le and Przybocki, 
2005). Callison-Burch et al (2006) detailed the 
deficits of the BLEU and other similar metrics, 
arguing that the simple surface similarity cal-
culation between the machines translations and 
the human translations suffers from morpho-
logical issues and fails to capture what are im-
portant for human assessments.  
In order to attack these problems, some me-
trics have been proposed to include more lin-
guistic information into the process of match-
ing, e.g., Meteor (Banerjee and Lavie, 2005) 
metric and MaxSim (Chan nad Ng, 2008) me-
trics, which improve the lexical level by the 
synonym dictionary or stemming technique. 
There are also substantial studies focusing on 
including deeper linguistic information in the 
metrics (Liu and Gildea, 2005; Owczarzak et 
al., 2006; Amig? et al, 2006; Mehay and Brew, 
2007; Gim?nez and M?rquez, 2007; Owczar-
zak et al, 2007; Popovic and Ney, 2007; 
Gim?nez and M?rquez, 2008b). 
A notable trend improving the string-based 
metric is to combine various deeper linguistic 
information via machine learning techniques in 
the metrics (Amig? et al, 2005; Albrecht and 
Hwa, 2007; Gim?nez and M?rquez, 2008). 
Such efforts are practically amount of intro-
ducing additional linguistic restrictions into the 
automatic evaluation metrics (Amig? et al 
2009), achiving a higher performance at the 
cost of lower adaptability to other languages 
owing to the language dependent linguistics 
features. 
Previous work shows that including the new 
features into the evaluation metrics may bene-
fit to describe nature language accurately. In 
this sense, the string-based metrics will be im-
proved, if the finer calculation granularities are 
introduced into the metrics.  
Our study analyzes the role of the calcula-
tion granularities in the performance of metrics. 
We find that the new features with various 
granularities can contribute to the automatic 
evaluation of translation quality. Also we pro-
pose a powerful string based automatic MT 
evaluation metric with multiple granularities 
combined by SVM. Finally, we seek a finer 
feature set of metrics with multiple calculation 
granularities. 
1534
3 The New Calculation Granularities 
of String-based Metrics on Sentence 
Level  
The string-based metrics of automatic machine 
translation evaluation on sentence level adopt a 
common strategy: taking the sentences of the 
documents as plain strings. Therefore, when 
changing the calculation granularities of the 
string-based metrics we can simplify the in-
formation of new granularity with plain strings.  
In this work, five kinds of available calculation 
granularities are defined: ?Lexicon?, ?Letter?, 
?Pos?, ?Constitute? and ?Dependency?.  
Lexicon: The calculation granularity is 
common word in the sentences of the docu-
ments, which is popular practice at present. 
Letter: Split the granularities of ?Lexical? 
into letters. Each letter is taken as a matching 
unit. 
Pos: The Pos tag of each ?Lexicon? is taken 
as a matching unit in this calculation granulari-
ty. 
Constitute: Syntactic Constitutes in a tree 
structure are available through the parser tools. 
We use Stanford Parser (Klein and Manning, 
2003a; Klein and Manning, 2003b) in this 
work.  The Constitute tree is changed into 
plain string, travelling by BFS (Breadth-first 
search traversal) 1.  
Dependency: Dependency relations in a de-
pendency structure are also available through 
the parser tools. The dependency structure can 
also be formed in a tree, and the same 
processing of being changed into plain string is 
adopted as ?Constitute?. 
The following serves as an example:  
Sentence:  
I have a dog 
Pos tag:  
I/PRON have/V a/ART dog/N 
Constitute tree:  
 
                                                 
1 We also attempt some other traversal algorithms, in-
cluding preorder, inorder and postorder traversal, the 
performance are proved to be similar.  
Dependency tree:  
 
Then, we can change the sentence into the 
plain string in multiple calculation granulari-
ties as follows: 
Lexicon string:  
I have a dog 
Letter string:  
I h a v e a d o g 
Pos string: 
PRON V ART N 
Constitute string:  
PRON V ART N NP NP VP S 
Dependency string: 
 a I dog have 
The translation hypothesis and human refer-
ence translations are both changed into those 
strings of various calculation granularities. The 
strings are taken as inputs of the string-based 
automatic MT evaluation metrics. The outputs 
of each metric are calculated on different 
matching units. 
4 String-based Metrics with Multiple 
Granularities Combined by SVM 
Introducing machine learning methods to es-
tablished MT evaluation metric is a popular 
trend. Our study chooses rank and regression 
support vector machine (SVM) as the learning 
model. Features are important for the SVM 
models. 
Plenty of scores can be generated from the 
proposed metrics. In fact, not all these features 
are needed. Therefore, feature selection should 
be a necessary step to find a proper feature set 
and alleviate the language dependency by us-
ing fewer linguistic features. 
Feature selection is an NP-Complete prob-
lem; therefore, we adopt a greedy selection 
algorithm called ?Best One In? to find a local 
optimal feature set. Firstly, we select the fea-
ture among all the features which best corre-
lates with the human assessments.  Secondly, a 
feature among the rest features is added in to 
the feature set, if the correlation with the hu-
man assessments of the metric using new set is 
1535
the highest among all new metrics and higher 
than the previous metric in cross training cor-
pus. The cross training corpus is prepared by 
dividing the training corpus into five parts. 
Each four parts of the five are for training and 
the rest one for testing; then, we integrate 
scores of the five tests as scores of cross train-
ing corpus.  The five-fold cross training can 
help to overcome the overfitting. At the end, 
the feature selection stops, if adding any of the 
rest features cannot lead to higher correlation 
with human assessments than the current me-
tric.  
5 Experiments 
5.1 The Impact of the Calculation Granu-
larities on String-based Metrics 
In this section, we use the data from NIST 
Open MT 2006 evaluation (LDC2008E43), 
which is described in Table 1.  It consists of 
249 source sentences that were translated by 
four human translators as well as 8 MT sys-
tems. Each machine translated sentence was 
evaluated by human judges for their adequacy 
on a 7-point scale. 
 
 NIST 2002  
NIST 
2003  
NIST 
Open 
MT 2006
LDC 
corpus 
LDC2003
T17 
LDC2006
T04 
LDC2008
E43 
Type Newswire Newswire Newswire
Source Chinese Chinese Arabic 
Target English English English 
# of  
sentences 878 919 249 
# of 
systems 3 7 8 
#  of 
references 4 4 4 
Score 
1-5, 
adequacy 
& fluency
1-5, 
adequacy 
& fluency 
1-7 
adequacy
Table 1: Description of LDC2006T04, 
LDC2003T17 and LDC2008E43 
 
To judge the quality of a metric, we com-
pute Spearman rank-correlation coefficient, 
which is a real number ranging from -1 (indi-
cating perfect negative correlations) to +1 (in-
dicating perfect positive correlations), between 
the metric?s scores and the averaged human 
assessments on test sentences. 
We select 21 features in ?lexicon? calcula-
tion granularity and 11?4 in the other calcula-
tion granularities. We analyze the correlation 
with human assessments of the metrics in mul-
tiple calculation granularities.  Table 2 lists the 
optimal calculation granularity of the multiple 
metrics on sentence level in the data 
(LDC2008E43).  
 
Metric Granularity 
BLEU-opt Letter 
NIST-opt Letter 
GTM(e=1) Dependency 
TER Letter 
PER Lexicon 
WER Dependency 
ROUGE-opt Letter 
Table 2 The optimal calculation granularity of the 
multiple metrics 
 
The most remarkable aspect is that not all 
the best metrics are based on the ?lexicon? cal-
culation granularities, such as the ?letter? and 
?dependency?. In other words, the granulari-
ties-shifted string-based metrics are promising 
to contribute to the automatic evaluation of 
translation quality. 
5.2 Correlation with Human Assessments 
of String-based Metrics with Multiple 
Granularities Based on SVM Frame 
We firstly train the SVM rank and regression 
models on LDC2008E43 using all the features 
(21+11? 4 species), without any selection. 
Secondly, the other two SVM rank and regres-
sion models are trained on the same data using 
the feature set via feature selection, which are 
described in Table 3. We have four string-
based evaluation metrics with multiple granu-
larities on rank and regression SVM frame 
?Rank_All, Regression_All, Rank_Select and 
Regression_Select?.  Then we apply the four 
metrics to evaluate the sentences of the test 
data (LDC2006T04 and LDC2003T17). The 
results of Spearman correlation with human 
assessments is summarized in Table 3. For 
comparison, the results from some state-of-art 
metrics (Papineni et al, 2002; Doddington, 
1536
2002; Melamed et al, 2003; Banerjee and La-
vie, 2005; Snover et al, 2006; Liu and Gildea, 
2005) and two machine learning methods (Al-
brecht and Hwa, 2007; Ding Liu and Gildea, 
2007) are also included in Table 3. Of the two 
machine learning methods, both trained on the 
data LDC2006T04. The ?Albrecht, 2007? 
score reported a result of Spearman correlation 
with human assessments on the data 
LDC2003T17 using 53 features, while the 
?Ding Liu, 2007? score reported that under 
five-fold cross validation on the data 
LDC2006T04 using 31 features. 
 
 Feature number 
LDC
2003
T17 
LDC
2006
T04 
Rank_All 65 0.323 0.495
Regression_All 65 0.345 0.507
Rank_Select 16 0.338 0.491
Regression_Select 8 0.341 0.510
Albrecht, 2007 53 0.309 -- 
Ding Liu, 2007 31 -- 0.369
BLEU-opt2 -- 0.301 0.453
NIST-opt -- 0.219 0.417
GTM(e=1) -- 0.270 0.375
METEOR3 -- 0.277 0.463
TER -- -0.250 -0.302
STM-opt -- 0.205 0.226
HWCM-opt -- 0.304 0.377
 
Table 3: Comparison of Spearman correlations with 
human assessments of our proposed metrics and 
some start-of-art metrics and two machine learning 
methods 
?-opt? stands for the optimum values of the pa-
rameters on the metrics 
 
Table 3 shows that the string-based meta-
evaluation metrics with multiple granularities 
based on SVM frame gains the much higher 
Spearman correlation than other start-of-art 
metrics on the two test data and, furthermore, 
our proposed metrics also are higher than the 
machine learning metrics (Albrecht and Hwa, 
2007; Ding Liu and Gildea, 2007).  
The underlining is that our proposed metrics 
are more robust than the aforementioned two 
                                                 
2 The result is computed by mteval11b.pl.  
3 The result is computed by meteor-v0.7. 
machine learning metrics. As shown in Table 1 
the heterogeneity between the training and test 
data in our method is much more significant 
than that of the other two machine learning 
based methods.  
In addition, the ?Regression_Select? metric 
using only 8 features can achieve a high corre-
lation rate which is close to the metric pro-
posed in ?Albrecht, 2007? using 53 features, 
?Ding Liu, 2007? using 31 features, ?Regres-
sion_All? and ?Rank_All? metrics using  65 
features and ?Rank_Select? metric using 16 
features. What is more, ?Regression_Select? 
metric is better than ?Albrecht, 2007?, and 
slightly lower than ?Regression_All? on the 
data LDC2003T17; and better than both ?Re-
gression_All? and ?Rank_All? metrics on the 
data LDC2006T04. That confirms that a small 
cardinal of feature set can also result in a me-
tric having a high correlation with human as-
sessments, since some of the features represent 
the redundant information in different forms. 
Eliminating the redundant information is bene-
fit to reduce complexity of the parameter 
searching and thus improve the metrics per-
formance based on SVM models. Meanwhile, 
fewer features can relieve the language depen-
dency of the machine learning metrics. At last, 
our experimental results show that regression 
models perform better than rank models in the 
string-based metrics with multiple granularities 
based on SVM frame, since ?Regres-
sion_Select? and ?Regression_All? achieve 
higher correlations with human assessments 
than the others. 
5.3 Reliability of Feature Selection  
The motivation of feature selection is keeping 
the validity of the feature set and alleviating 
the language dependency. We also look for-
ward to the higher Spearman correlation on the 
test data with a small and proper feature set.  
We use SVM-Light (Joachims, 1999) to 
train our learning models using NIST Open 
MT 2006 evaluation data (LDC2008E43), and 
test on the two sets of data, NIST?s 2002 and 
2003 Chinese MT evaluations. All the data are 
described in Table 1. To avoid the bias in the 
distributions of the two judges? assessments in 
NIST?s 2002 and 2003 Chinese MT evalua-
tions, we normalize the scores following (Blatz 
et al, 2003). 
1537
We trace the process of the feature selection. 
The selected feature set of the metric based on 
SVM rank includes 16 features and that of the 
metric based on SVM regression includes 8 
features. The selected features are listed in Ta-
ble 4. The values in Table 4 are absolute 
Spearman correlations with human assess-
ments of each single feature score.  The prefix-
es ?C_?, ?D_?, ?L_?, ?P_?, and ?W_? 
represent ?Constitute?, ?Dependency?, ?Let-
ter?, ?Pos? and ?Lexicon? respectively. 
 
Rank spear-man Regression
spear-
man
C_PER .331 C_PER .331
C_ROUGE-W .562 C_ROUGE-W .562
D_NIST9 .479 D_NIST9 .479
D_ROUGE-W .679 D_ROUGE-L .667
L_BLEU6 .702 L_BLEU6 .702
L_NIST9 .691 L_NIST9 .691
L_ROUGE-W .634 L_ROUGE-W .634
P_PER .370 P_ROUGE-W .683
P_ROUGE-W .616  
W_BLEU1_ind .551  
W_BLEU2 .659  
W_GTM .360  
W_METEOR .693  
W_NIST5 .468  
W_ROUGE1 .642  
W_ROUGE-W .683  
 
Table 4: Feature sets of SVM rank and regression 
 
Table 4 shows that 8 features are selected 
from 65 features in the process of feature se-
lection based on SVM regression while 16 fea-
tures based on SVM rank. Fewer features 
based on SVM regression are selected than 
SVM rank. Only one feature in feature set 
based on SVM regression does not occur in 
that based on SVM rank. The reason is that 
there are more complementary advantages be-
tween the common selected features.  
Next, we will verify the reliability of our 
feature selection algorithm. Figure 1 and Fig-
ure 2 show the Spearman correlation values 
between our SVM-based metrics (regression 
and rank) and the human assessments on both 
training data (LDC2008E43) and test data 
(LDC2006T04 and LDC2003T17).  
 
 
 
Figure 1: The Spearman correlation values between 
our SVM rank metrics and the human assessments 
on both training data and test data with the exten-
sion of the feature sets 
 
 
 
Figure 2: The Spearman correlation values between 
our SVM regression metrics and the human as-
sessments on both training data and test data with 
the extension of the feature sets 
 
From Figure 1 and Figure 2, with the exten-
sion of the feature sets, we can find that the 
tendency of correlation obtained by each me-
tric based on SVM rank or regression roughly 
the same on both the training data and test data. 
Therefore, the two feature sets of SVM rank 
and regression models are reliable. 
6 Conclusion 
In this paper we propose an integrated platform 
for automatic MT evaluation by improving the 
string based metrics with multiple granularities. 
Our proposed metrics construct a novel inte-
grated platform for automatic MT evaluation 
based on multiple features. Our  key contribu-
tion consists of two parts: i) we suggest a strat-
egy  of changing the various complex features 
into plain string form. According to the strate-
gy, the automatic MT evaluation frame are 
1538
much more clarified, and the computation of 
the similarity is much more simple, since the 
various linguistic features may express in the 
uniform strings with multiple calculation gra-
nularities. The new features have the same 
form and are dimensionally homogeneous; 
therefore, the consistency of the features is 
enhanced strongly. ii) We integrate the features 
with machine learning and proposed an effec-
tive approach of feature selection. As a result, 
we can use fewer features but obtain the better 
performance. 
In this framework, on the one hand, string-
based metrics with multiple granularities may 
introduce more potential features into automat-
ic evaluation, with no necessarily of new simi-
larity measuring method, compared with the 
other metrics. On the other hand, we succeed 
in finding a finer and small feature set among 
the combinations of plentiful features, keeping 
or improving the performance. Finally, we 
proposed a simple, effective and robust string-
based automatic MT evaluation metric with 
multiple granularities. 
Our proposed metrics improve the flexibility 
and performance of the metrics based on the 
multiple features; however, it still has some 
drawbacks: i) some potential features are not 
yet considered, e.g. the semantic roles; and ii) 
the loss of information exists in the process of 
changing linguistic information into plain 
strings. For example, the dependency label in 
the calculation granularity ?Dependency? is 
lost when changing information into string 
form. Though the final results obtain the better 
performance than the other linguistic metrics, 
the performance is promising to be further im-
proved if the loss of information can be prop-
erly dealt with. 
Acknowledgement 
This work is supported by Natural Science 
foundation China (Grant No.60773066 & 
60736014) and National Hi-tech Program 
(Project No.2006AA010108), and the Natural 
Scientific Reserach Innovation Foundation in 
Harbin Institute of Technology (Grant No. 
HIT.NSFIR.20009070). 
 
References 
Albrecht S. Joshua and Rebecca Hwa. 2007. A 
Reexamination of Machine Learning Approaches 
for Sentence-Level MT Evaluation. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 880-
887. 
Amig? Enrique, Julio Gonzalo, Anselmo P?nas, 
and Felisa Verdejo. 2005. QARLA: a Framework 
for the Evaluation of Automatic Summarization. 
In Proceedings of the 43th Annual Meeting of 
the Association for Computational Linguistics. 
Amig? Enrique, Jes?s Gim?nez, Julio Gonzalo,  
Felisa Verdejo. 2009. The Contribution of Lin-
guistic Features to Automatic Machine Transla-
tion Evaluation. In proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL 
and the 4th International Joint Conference on 
Natural Language Processing of the AFNLP. 
Amig? Enrique, Jes?s Gim?nez, Julio Gonzalo, and 
Llu?s M?rquez. 2006. MT Evaluation: Human- 
Like vs. Human Acceptable. In Proceedings of 
the Joint 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Lin-
guistic, pages 17?24. 
Banerjee Satanjeev and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation 
with improved correlation with human judg-
ments. In Proceedings of the ACL Workshop on 
Intrinsic and Extrinsic Evaluation Measures. 
Blatz John, Erin Fitzgerald, George Foster, Simona 
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto 
Sanchis, and Nicola Ueffing. 2003. Confidence 
estimation for machine translation. In Technical 
Report Natural Language Engineering Workshop 
Final Report, pages 97-100. 
 Callison-Burch Chris, Miles Osborne, and Philipp 
Koehn. 2006. Re-evaluating the Role of BLEU in 
Machine Translation Research. In Proceedings 
of 11th Conference of the European Chapter of 
the Association for Computational Linguistics  
Chan S. Yee and Hwee T. Ng. 2008. MAXSIM: A 
maximum similarity metric for machine transla-
tion evaluation. In Proceedings of ACL-08: HLT, 
pages 55?62. 
Doddington George. 2002. Automatic Evaluation of 
Machine Translation Quality Using N-gram Co- 
Occurrence Statistics. In Proceedings of the 2nd 
International Conference on Human Language 
Technology, pages 138?145. 
1539
Drucker Harris, Chris J. C. Burges, Linda Kaufman, 
Alex Smola, Vladimir Vapnik. 1996. Support 
vector regression machines.  In NIPS. 
Gim?nez Jes?s and Llu?s M?rquez. 2007. Linguistic 
Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL 
Workshop on Statistical Machine Translation. 
Gim?nez Jes?s and Llu?s M?rquez. 2008a. Hetero-
geneous Automatic MT Evaluation Through 
Non-Parametric Metric Combinations. In Pro-
ceedings of IJCNLP, pages 319?326. 
Gim?nez Jes?s and Llu?s M?rquez. 2008b. On the 
Robustness of Linguistic Features for Automatic 
MT Evaluation. 
Joachims Thorsten. 2002. Optimizing search en-
gines using clickthrough data. In KDD. 
Klein Dan and Christopher D. Manning. 2003a. 
Fast Exact Inference with a Factored Model for 
Natural Language Parsing. In Advances in 
Neural Information Processing Systems 15, pp. 
3-10.  
Klein Dan and Christopher D. Manning. 2003b. 
Accurate Unlexicalized Parsing. Proceedings of 
the 41st Meeting of the Association for Compu-
tational Linguistics, pp. 423-430. 
Le Audrey and Mark Przybocki. 2005. NIST 2005 
machine translation evaluation official results. 
In Official release of automatic evaluation scores 
for all submission. 
Lin Chin-Yew and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quali-
ty Using Longest Common Subsequence and 
Skip-Bigram Statistics. Proceedings of the 42nd 
Annual Meeting of the Association for Computa-
tional Linguistics, pp. 605-612. 
Liu Ding and Daniel Gildea. 2005. Syntactic Fea-
tures for Evaluation of Machine Translation. In 
Proceedings of ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or 
Summarization, pages 25?32. 
Liu Ding and Daniel Gildea. 2007. Source Lan-
guage Features and Maximum Correlation 
Training for Machine Translation Evaluation. In 
proceedings of NAACL HLT 2007, pages 41?48 
Mehay Dennis and Chris Brew. 2007. BLEUATRE: 
Flattening Syntactic Dependencies for MT Eval-
uation. In Proceedings of the 11th Conference on 
Theoretical and Methodological Issues in Ma-
chine Translation. 
Melamed Dan I., Ryan Green, and Joseph P. Turian. 
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on 
Human Language Technology and the North 
American Chapter of the Association for Com-
putational Linguistics. 
Nie?en Sonja, Franz Josef Och, Gregor Leusch, and 
Hermann Ney. 2000. An Evaluation Tool for 
Machine  Translation: Fast Evaluation for MT 
Research. In Proceedings of the 2nd Internation-
al Conference on Language Resources and Eval-
uation . 
Owczarzak Karolina, Declan Groves, Josef Van 
Genabith, and Andy Way. 2006. Contextual Bi-
text- Derived Paraphrases in Automatic MT 
Evaluation. In Proceedings of the 7th Confe-
rence of the Association for Machine Translation 
in the Americas, pages 148?155. 
Owczarzak Karolina, Josef van Genabith, and Andy 
Way. 2007. Labelled Dependencies in Machine 
Translation Evaluation. In Proceedings of the 
ACL Workshop on Statistical Machine Transla-
tion, pages 104?111. 
Papineni Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In 
Proceedings of 40th Annual Meeting of the As-
sociation for Computational Linguistics. 
Popovi? Maja and Hermann Ney. 2007. Word Er-
ror Rates: Decomposition over POS classes and 
Applications for Error Analysis. In Proceedings 
of the Second Workshop on Statistical Machine 
Translation, pages 48?55. 
Popovi? Maja and Hermann Ney. 2009. Syntax-
oriented evaluation measures for machine trans-
lation output. In Proceedings of the 4th EACL 
Workshop on Statistical Machine Translation, 
pages 29?32. 
Snover Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted hu-
man annotation. In Proceedings of AMTA, pag-
es 223?231. 
Tillmann Christoph, Stefan Vogel, Hermann Ney, 
A. Zubiaga, and H. Sawaf. 1997. Accelerated 
DP based Search for Statistical Translation. In 
Proceedings of European Conference on Speech 
Communication and Technology. 
1540
