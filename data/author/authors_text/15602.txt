Proceedings of the 6th Workshop on Statistical Machine Translation, pages 284?293,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Investigations on Translation Model Adaptation Using Monolingual Data
Patrik Lambert, Holger Schwenk, Christophe Servan and Sadaf Abdul-Rauf
LIUM, University of Le Mans
72085 Le Mans, France
FirstName.LastName@lium.univ-lemans.fr
Abstract
Most of the freely available parallel data to
train the translation model of a statistical ma-
chine translation system comes from very spe-
cific sources (European parliament, United
Nations, etc). Therefore, there is increasing
interest in methods to perform an adaptation
of the translation model. A popular approach
is based on unsupervised training, also called
self-enhancing. Both only use monolingual
data to adapt the translation model. In this pa-
per we extend the previous work and provide
new insight in the existing methods. We report
results on the translation between French and
English. Improvements of up to 0.5 BLEU
were observed with respect to a very com-
petitive baseline trained on more than 280M
words of human translated parallel data.
1 Introduction
Adaptation of a statistical machine translation sys-
tem (SMT) is a topic of increasing interest during
the last years. Statistical (n-gram) language models
are used in many domains and several approaches to
adapt such models were proposed in the literature,
for instance in the framework of automatic speech
recognition. Many of these approaches were suc-
cessfully used to adapt the language model of an
SMT system. On the other hand, it seems more chal-
lenging to adapt the other components of an SMT
system, namely the translation and reordering mod-
els. In this work we consider the adaptation of the
translation model of a phrase-based SMT system.
While rule-based machine translation rely on
rules and linguistic resources built for that purpose,
SMT systems can be developed without the need of
any language-specific expertise and are only based
on bilingual sentence-aligned data (?bitexts?) and
large monolingual texts. However, while monolin-
gual data are usually available in large amounts and
for a variety of tasks, bilingual texts are a sparse re-
source for most language pairs.
Current parallel corpora mostly come from one
domain (proceedings of the Canadian or European
Parliament, or of the United Nations). This is prob-
lematic when SMT systems trained on such corpora
are used for general translations, as the language jar-
gon heavily used in these corpora is not appropriate
for everyday life translations or translations in some
other domain. This problem could be attacked by ei-
ther searching for more in-domain training data, e.g.
by exploring comparable corpora or the WEB, or by
adapting the translation model to the task. In this
work we consider translation model adaptation with-
out using additional bilingual data. One can dis-
tinguish two types of translation model adaptation:
first, adding new source words or/and new transla-
tions to the model; and second, modifying the prob-
abilities of the existing model to better fit the topic
of the task. These two directions are complementary
and could be simultaneously applied. In this work
we focus on the second type of adaptation.
In this work, we focus on statistical phrase-
based machine translations systems (PBSMT), but
the methods could be also applied to hierarchical
systems. In PBSMT, the translation model is rep-
resented by a large list of all known source phrases
and their translations. Each entry is weighted us-
ing several probabilities, e.g. the popular Moses
284
system uses phrase translation probabilities in the
forward and backward direction, as well as lexical
probabilities in both directions. The entries of the
phrase-table are automatically extracted from sen-
tence aligned parallel data and they are usually quite
noisy. It is not uncommon to encounter several hun-
dreds, or even thousands of possible translations of
frequent source phrases. Many of these automati-
cally extracted translations are probably wrong and
are never used since their probabilities are (fortu-
nately) small in comparison to better translations.
Therefore, several approaches were proposed to fil-
ter these phrase-tables, reducing considerably their
size without any loss of the quality, or even achiev-
ing improved performance (Johnson et al, 2007).
Given these observations, adaptation of the trans-
lation model of PBSMT systems could be performed
by modifying the probability distribution of the ex-
isting phrases without necessarily modifying the en-
tries. The idea is of course to increase the prob-
abilities of translations that are appropriate to the
task and to decrease the probabilities of the other
ones. Ideally, we should also add new translations or
source phrase, but this seems to be more challenging
without any additional parallel data.
A common way to modify a statistical model is to
use a mixture model and to optimize the coefficients
to the adaptation domain. This was investigated in
the framework of SMT by several authors, for in-
stance for word alignment (Civera and Juan, 2007),
for language modeling (Zhao et al, 2004; Koehn
and Schroeder, 2007) and to a lesser extent for the
translation model (Foster and Kuhn, 2007; Chen et
al., 2008). This mixture approach has the advan-
tage that only few parameters need to be modified,
the mixture coefficients. On the other hand, many
translation probabilities are modified at once and it
is not possible to selectively modify the probabilities
of particular phrases.
Another direction of research is self-enhancing of
the translation model. This was first proposed by
Ueffing (2006). The idea is to translate the test data,
to filter the translations with help of a confidence
score and to use the most reliable ones to train an
additional small phrase table that is jointly used with
the generic phrase table. This could be also seen as a
mixture model with the in-domain component being
build on-the-fly for each test set. In practice, such
an approach is probably only feasible when large
amounts of test data are collected and processed at
once, e.g. a typical evaluation set up with a test set of
about 50k words. This method of self-enhancing the
translation model seems to be more difficult to apply
for on-line SMT, e.g. a WEB service, since often the
translation of some sentences only is requested. In
follow up work, this approach was refined (Ueffing
et al, 2007). Domain adaptation was also performed
simultaneously for the translation, language and re-
ordering model (Chen et al, 2008).
A somehow related approach was named lightly-
supervised training (Schwenk, 2008). In that work
an SMT system is used to translate large amounts of
monolingual texts, to filter them and to add them to
the translation model training data. This approach
was reported to obtain interesting improvements
in the translations quality (Schwenk and Senellart,
2009; Bertoldi and Federico, 2009). In comparison
to self enhancing as proposed by Ueffing (2006),
lightly-supervised training does not adapt itself to
the test data, but large amounts of monolingual train-
ing data are translated and a completely new model
is built. This model can be applied to any test data,
including a WEB service.
In this paper we propose to extend this approach
in several ways. First, we argue that the automatic
translations should not be performed from the source
to the target language, but in the opposite direction.
Second, we propose to use the segmentation ob-
tained during translation instead of performing word
alignments with GIZA++ (Och and Ney, 2003) of
the automatic translations. Finally, we propose to
enrich the vocabulary of the adapted system by de-
tecting untranslated words and automatically infer-
ring possible translations from the stemmed form
and the existing translations in the phrase table.
This paper is organized as follows. In the next
section we first describe our approach in detail. Sec-
tion 3 describes the considered task, the available
resources and the baseline PBSMT system. Results
are summarized in section 4 and the paper concludes
with a discussion and perspectives of this work.
2 Architecture of the approach
In this paper we propose to extend in several ways
the translation model adaptation by unsupervised
285
training as proposed by Schwenk (2008). In that
paper the authors propose to first build a PBSMT
system using all available human translated bi-
texts. This system is then used to translate large
amounts of monolingual data in the source language.
These automatic translations are filtered using the
sentence-length normalized log score of Moses, i.e.
the sum of the log-scores of all feature functions.
Putting a threshold on this score, only the most re-
liable translations are kept. This threshold was de-
termined experimentally. The automatic translations
were added to the parallel training data and a new
PBSMT model was build, performing the complete
pipeline of word alignment with GIZA++, phrase
extraction and scoring and tuning the system on
development data with MERT. In Schwenk (2009)
significant improvement were obtained by this ap-
proach when translating from Arabic to French.
2.1 Choice of the translation direction
First, we argue that it should be better to translate
monolingual data in the opposite translation direc-
tion of the system that we want to improve, i.e. from
the target into the source language. When translat-
ing large amounts of monolingual data, the system
will of course produce some wrong translations with
respect to choice of the vocabulary, to word order,
to morphology, etc. If we translate from the source
to the target language, these wrong translations are
added to the phrase table and may be used in future
translations performed by the adapted system. When
we add the automatic translations performed in the
opposite direction to the training data, the possibly
wrong translations will appear on the source side of
the entries in the adapted phrase table. PBSMT sys-
tems segment the source sentence according to the
available entries in the phrase table. Since the source
sentence is usually grammatically and semantically
correct, with the eventual exception of speech trans-
lation, it is unlikely that the wrong entries in the
phrase table will be ever used, e.g. phrases with bad
word choice or wrong morphology.
The question of the choice of the translation di-
rection was already raised by Bertoldi and Fed-
erico (2009). However, when data in the source
language is available they adapt only the translation
model (TM), while they adapt the TM and the lan-
guage model (LM) when data in the target language
is given. Of course the system with adapted LM is
much better, but this doesn?t prove that target mono-
lingual data are better than source monolingual data
for TM adaptation. In our paper, we use the same,
best, LM for all systems and we adapt the baseline
system with bitexts synthesized from source or tar-
get monolingual data.
2.2 Word alignment
In the work of Schwenk (2008), the filtered auto-
matic translation were added to the parallel training
data and the full pipeline to build a PBSMT sys-
tem was performed again, including word alignment
with GIZA++. Word alignment of bitexts of several
hundreds of millions of words is a very time con-
suming step. Therefore we propose to use the seg-
mentation into phrases and words obtained implic-
itly during the translation of the monolingual data
with the moses toolkit. These alignments are simply
added to the previously calculated alignments of the
human translated bitexts and a new phrase table is
built.
This new procedure does not only speed-up the
overall processing, but there are also investigations
that these alignments obtained by decoding are more
suitable to extract phrases than the symmetrized
word alignments produced by GIZA++. For in-
stance, Wuebker et al (2010) proposed to trans-
late the training data, using forced alignment and
a leave-one-out technique, and to use the induced
alignments to extract phrases. They have observed
improvements with respect to word alignment ob-
tained by GIZA++. On the other hand, Bertoldi and
Federico (2009) adapted an SMT system with au-
tomatic translations and trained the translation and
reordering models on the word alignment used by
moses. They reported a very small drop in per-
formance with respect to training word alignments
with GIZA++. Similar ideas were also used in pivot
translation. Bertoldi et al (2008) translated from the
pivot language to the source language to create par-
allel training data for the direct translation.
2.3 Treatment of unknown words
Statistical machine translation systems have some
trouble dealing with morphologically rich lan-
guages. It can happen, in function of the avail-
able training data, that translations of words are only
286
Source language Source language Target language
French stemmed form English
finies fini finished
efface?s efface? erased
hawaienne hawaien Hawaiian
... ... ...
Table 1: Example of translations from French to English
which are automatically extracted from the phrase-table
with the stemmed form.
known in some forms and not in others. For in-
stance, for a user of MT technology it is quite dif-
ficult to understand why the system can translate
the French word ?je pense?1, but not ?tu penses?2.
There have been attempts in the literature to address
this problem, for instance by Habash (2008) to deal
with the Arabic language. It is actually possible to
automatically infer possible translations when trans-
lating from a morphologically rich language, to a
simpler language. In our case we use this approach
to translate from French to English.
Several of the unknown words are actually adjec-
tives, nouns or verbs in a particular form that itself
is not known, but the phrase table would contain the
translation of a different form. As an example we
can mention the French adjective finies which is in
the female plural form. After stemming we may be
able to find the translation in a dictionary which is
automatically extracted from the phrase-table (see
Table 1). This idea was already outlined by (Bo-
jar and Tamchyna, 2011) to translate from Czech to
English.
First, we automatically extract a dictionary from
the phrase table. This is done, be detecting all 1-to-1
entries in the phrase table. When there are multi-
ple entries, all are kept with their lexical translations
probabilities. Our dictionary has about 680k unique
source words with a total of almost 1M translations.
source segment les travaux sont finis
stemmed les travaux sont fini
segment les travaux sont <n translation=?finished||ended?
proposed prob=?0.008||0.0001?>finis</n>
Table 2: Example of the treatment of an unknown French
word and its automatically inferred translation.
The detection of unknown words is performed by
1I think
2you think
comparing the n-grams contained in the phrase ta-
ble and the source segment in order to detect iden-
tical words. Once the unknown word is selected,
we are looking for its stemmed form in the dictio-
nary and propose some translations for the unknown
word based on lexical score of the phrase table (see
Table 2 for some examples). The stemmer used is
the snowball stemmer3. Then the different hypothe-
sis are evaluated with the target language model.
This kind of processing could be done either be-
fore running the Moses decoder, i.e. using the
XML mark-up of Moses, or after decoding by post-
processing the untranslated words. In both cases, we
are unable to differentiate the possible translations
of the same source phrase with meaningful transla-
tion probabilities, and they won?t be added to the
phrase-table, nor put into a context with other words
that may trigger their use.
Therefore, we propose to use this technique to re-
place unknown words during the translation of the
monolingual data that we use to adapt the transla-
tion model. By these means, the automatically in-
duced translations of previously unknown morpho-
logical forms will be put into a context and actually
appear in the new adapted phrase-table. The corre-
sponding translation probabilities will be those cor-
responding to their frequency in the monolingual in-
domain data.
This procedure has been implemented, but we
were not able to obtain improvements in the BLEU
score. However, one can ask if automatic metrics,
evaluated on a test corpus of limited size, are the best
choice to judge this technique. In fact, in our setting
we have observed that less than 0.2% of the words
in the test set are unknown. We argue that the ability
to complement the phrase-table with many morpho-
logical forms of other wise known words, can only
improve the usability of SMT systems.
3 Task Description and resources
In this paper, we consider the translation of news
texts between French and English, in both direc-
tions. In order to allow comparisons, we used ex-
actly the same data as those allowed for the inter-
national evaluation organized in the framework of
the sixth workshop on SMT, to be held in Edinburgh
3http://snowball.tartarus.org/
287
Parallel data Size English/French French/English
[M words] Dev Test Dev Test
Eparl + nc 54 26.20 (0.06) 28.06 (0.2) 26.70 (0.06) 27.41 (0.2)
Eparl + nc + crawled1 168 26.84 (0.09) 29.08 (0.1) 27.96 (0.09) 28.20 (0.04)
Eparl + nc + crawled2 286 26.95 (0.04) 29.29 (0.03) 28.20 (0.03) 28.57 (0.1)
Eparl + nc + un 379 26.57 28.52 - -
Eparl + nc + crawled1 + un 514 26.87 28.99 - -
Eparl + nc + crawled2 + un 631 26.99 29.26 - -
Table 4: Case sensitive BLEU scores as a function of the amount of parallel training data. (Eparl=Europarl, nc=News
Commentary, crawled1/2=sub-sampled crawled bitexts, un=sub-sampled United Nations bitexts).
Corpus English French
Bitexts:
Europarl 50.5M 54.4M
News Commentary 2.9M 3.3M
United Nations 344M 393M
Crawled (109 bitexts) 667M 794M
Development data:
newstest2009 65k 73k
newstest2010 62k 71k
Monolingual data:
LDC Gigaword 4.1G 920M
Crawled news 2.6G 612M
Table 3: Available training data for the translation be-
tween French and English for the translation evaluation
at WMT?11 (number of words after tokenisation).
in July 2011. Preliminary results of this evaluation
are available on the Internet.4 Table 3 summarizes
the available training and development data. We op-
timized our systems on newstest2009 and used
newstest2010 as internal test set. For both cor-
pora, only one reference translations is available.
Scoring was performed with NIST?s implementation
of the BLEU score (?mt-eval? version 13).
3.1 Baseline system
The baseline system is a standard phrase-based SMT
system based on the the Moses SMT toolkit (Koehn
et al, 2007). It uses fourteen features functions
for translation, namely phrase and lexical translation
probabilities in both directions, seven features for
the lexicalized distortion model, a word and a phrase
penalty, and a target language model. It is con-
4http://matrix.statmt.org
structed as follows. First, word alignments in both
directions are calculated. We used a multi-threaded
version of the GIZA++ tool (Gao and Vogel, 2008).
Phrases and lexical reorderings are extracted using
the default settings of the Moses toolkit. All the bi-
texts were concatenated. The parameters of Moses
are tuned on the development data using the MERT
tool. For most of the runs, we performed three op-
timizations using different starting points and report
average results. English and French texts were to-
kenised using a modified version of the tools of the
Moses suite. Punctuation and case were preserved.
The language models were trained on all the avail-
able data, i.e. the target side of the bitexts, the whole
Gigaword corpus and the crawled monolingual data.
We build 4-gram back-off LMs with the SRI LM
toolkit using Modified Kneser-Ney and no cut-off
on all the n-grams. Past experience has shown that
keeping all n-grams slightly improves the perfor-
mance although this produces quite huge models
(10G and 30G of disk space for French and English
respectively).
Table 4 gives the baseline results using various
amounts of bitexts. Starting with the Europarl and
the News Commentary corpora, various amounts of
human translated data were added. The organizers
of the evaluation provide the so called 109 French-
English parallel corpus which contains almost 800
million words of data crawled from Canadian and
European Internet pages. Following works from the
2010 WMT evaluation (Lambert et al, 2010), we
filtered this data using IBM-1 probabilities and lan-
guage model scores to keep only the most reliable
translations. Two subsets were built with 115M and
232M English words respectively (using two differ-
288
alignment Dev Test
BLEU BLEU TER
giza 27.34 (0.01) 29.80 (0.06) 55.34 (0.06)
reused giza 27.40 (0.05) 29.82 (0.10) 55.30 (0.02)
reused moses 27.42 (0.02) 29.77 (0.06) 55.27 (0.03)
Table 5: Results for systems trained via different word alignment configurations. The values are the average over
3 MERT runs performed with different seeds. The numbers in parentheses are the standard deviation of these three
values. Translation was performed from English to French, adding 45M words of automatic translations (translated
from French to English) to the baseline system ?eparl+nc+crawled2?.
ent settings of the filter thresholds). They are re-
ferred to as ?crawled1? and ?crawled2? respectively.
Adding this data improved the BLEU score of al-
most 1 BLEU point (28.30 ? 29.27). This is our
baseline system to be improved by translation model
adaptation. Using the UN data gave no significant
improvement despite its huge size. This is probably
a typical example that it is not necessarily useful to
use all available parallel training data, in particular
when a very specific (out-of domain) jargon is used.
Consequently, the UN data was not used in the sub-
sequent experiments.
We were mainly working on the translation from
English to French. Therefore only one baseline sys-
tem was build for the reverse translation direction.
4 Experimental Evaluation
The system trained on Europarl, News Commen-
tary and the sub-sampled version of the 109 bitexts
(?eparl+nc+crawled2?, in the third line of Table 3),
was used to translate parts of the crawled news in
French and English. Statistics on the translated data
are given in Table 6.
We focused on the most recent data since the
time period of our development and test data was
end of 2008 and 2009 respectively. In the future
we will translate all the available monolingual data
and make it available to the community in order to
ease the widespread use of this kind of translation
model adaptation methods. These automatic trans-
lations were filtered using the sentence normalized
log-score of the decoder, as proposed by (Schwenk,
2008). However, we did not perform systematic ex-
periments to find the optimal threshold on this score,
but simply used a value which seems to be a good
compromise of quality and quantity of the transla-
tions. This gave us about 45M English words of
Corpus French (fe) English (ef)
available filtered available filtered
2009 92 31 121 45
2010 43 12 112 49
2011 8 2 15 6
total 219 45 177 100
Table 6: Monolingual data used to adapt the systems,
given in millions of English words. Under ?French (fe)?,
we indicated the number of translated English words
from French, and under ?English (ef)? we reported the
number of source English words translated into French.
Thus ?fe? and ?ef? refer respectively to French?English
and English?French translation direction of monolingual
data. In the experiments we used the 100M English?
French (ef) filtered monolingual data, as well as a 45M-
word subset (in order to have the same amount of data as
for French?English) and a 65M-word subset.
automatic translations from French, as well as the
translations into French of 100M English words, to
be used to adapt the baseline systems.
4.1 Word alignment
In order to build a phrase table with the translated
data, we re-used the word alignment obtained dur-
ing the translation with the moses toolkit. We com-
pared the system trained via these alignments to
the systems built by running GIZA++ on all the
data. When word alignments of the baseline corpus
(not adapted) are trained together with the translated
data, they could be affected by phrase pairs com-
ing from incorrect translations. To measure this ef-
fect, we trained an additional system, for which the
alignments of the baseline corpus are those trained
without the translated data. For the translated data,
we re-use the GIZA++ alignments trained on all the
data. Results for these three alignment configura-
289
baseline translated bitexts Dev Test
BLEU BLEU TER
Eparl + nc - 26.20 (0.06) 28.06 (0.22) 56.85 (0.09)
news fe 45M 27.18 (0.09) 29.03 (0.07) 55.97 (0.07)
news ef 45M 26.15 (0.04) 28.44 (0.09) 56.56 (0.11)
Eparl + nc + crawled2 - 26.95 (0.04) 29.29 (0.03) 55.77 (0.19)
news fe 45M 27.42 (0.02) 29.77 (0.06) 55.27 (0.03)
news ef 45M 26.75 (0.04) 28.88 (0.10) 56.06 (0.05)
Table 7: Translation results of the English?French systems augmented with a bitext obtained by translating news data
from English to French (ef) and French to English (fe). 45M refers to the number of English running words.
baseline translated bitexts Dev Test
BLEU BLEU TER
Eparl + nc - 26.70 (0.06) 27.41 (0.24) 55.07 (0.17)
news fe 45M 27.47 (0.08) 27.77 (0.23) 54.84 (0.13)
news ef 45M 27.55 (0.05) 28.51 (0.10) 54.12 (0.09)
news ef 65M 27.58 (0.03) 28.70 (0.09) 54.06 (0.17)
news ef 100M 27.63 (0.06) 28.68 (0.06) 54.02 (0.06)
Eparl + nc + crawled2 - 28.20 (0.03) 28.54 (0.12) 54.17 (0.15)
news fe 45M 28.02 (0.11) 28.40 (0.10) 54.45 (0.06)
news ef 45M 28.24 (0.06) 28.93 (0.22) 53.90 (0.08)
news ef 65M 28.16 (0.19) 28.75 (0.06) 54.03 (0.14)
news ef 100M 28.28 (0.09) 28.96 (0.03) 53.79 (0.09)
Table 8: Translation results of the French?English systems augmented with a bitext obtained by translating news data
from English to French (ef) and French to English (fe). 45M/65M/100M refers to the number of English running
words.
tions are presented in Table 5. In these systems
French sources and English translations (45 mil-
lion words) were added to the ?eparl+nc+crawled2?
baseline corpus. According to BLEU and TER met-
rics, reusing Moses alignments to build the adapted
phrase table has no significant impact on the system
performance. We repeated the experiment without
the 109 corpus and with the smaller selection of 109
(crawled1) and arrived to the same conclusion.
However, the re-use of Moses alignments saves time
and resources. On the larger baseline corpus, the
mGiza process lasted 46 hours with two jobs of 4
thread running and a machine with two Intel X5650
quad-core processors.
4.2 Choice of the translation direction
A second point under study in this work is the effect
of the translation direction of the monolingual data
used to adapt the translation model. Tables 7 and
8 present results for, respectively, English?French
and French?English systems adapted with news data
translated from English to French (ef) and French
to English (fe). The experiment was repeated with
two baseline corpora. The results show clearly
that target to source translated data are more use-
ful than source to target translated data. The im-
provement in terms of BLEU score due to the use of
target-to-source translated data instead of source-to-
target translated data ranges from 0.5 to 0.9 for the
French?English and English?French systems. For
instance, when translating from English to French
(Table 7), the baseline system ?eparl+nc? achieves
a BLEU score of 28.06 on the test set. This could
be improved to 29.03 using automatic translations
in the reverse direction (French to English), while
we only achieve a BLEU score of 28.44 when us-
ing automatic translation performed in the same di-
rection as the system to be adapted. The effect is
even clearer when we try to adapt the large system
290
?eparl+nc+crawled2?. Adding automatic transla-
tions translated from English-to-French did actually
lead to a lower BLEU score (29.29 ? 28.88) while
we observe an improvement of nearly 0.5 BLEU in
the other case.
With target-to-source translated news data,
the gain with respect to the baseline corpus
for English-French systems (Table 7) is nearly
1 BLEU for ?Eparl+nc? and 0.5 BLEU for
?Eparl+nc+crawled2?. With the same amount
of translated data (45 million English words),
approximately the same gains are observed in
French?English systems. Due to the larger avail-
ability of English news data, we were able to use
larger sets of target-to-source translated data for
French-English systems, as can be seen in Table 8.
With a bitext containing additionally 20 million
English words, we get a further improvement of
0.2 BLEU for ?Eparl+nc? (28.51 ? 28.70), but no
improvement for ?Eparl+nc+crawled2? (the BLEU
score is even lower, but the scores lie within the
error interval). No further gain on the test data is
achieved if we add again 35 million English words
(total of 100M words) to the system ?Eparl+nc?.
With the ?Eparl+nc+crawle2? baseline, no sig-
nificant improvement is observed if we adapt the
system with 100M words instead of only 45M.
4.3 Result analysis
To get more insight into what happens to the model
when we add the automatic translations, we cal-
culated some statistics of the phrase table, pre-
sented in Table 9. Namely, we calculated the
number of entries in the phrase table, the aver-
age number of translation options of each source
phrase, the average entropy for each source phrase,
the average source phrase length (in words) and
the average target phrase length. The entropy is
calculated over the probabilities of all translation
options for each source phrase. Comparing the
baseline with ?Eparl+nc? and the baseline with
?Eparl+nc+crawl2?, we can observe that the aver-
age number of translation options was nearly mul-
tiplied by 3 with the addition of 230 million words
of human translated bitexts. As a consequence the
average entropy was increased from 1.84 to 2.08.
On the contrary, adding 100 million words of in-
domain automatic translations, the average num-
ber of translation options increased by only 5%
for the ?Eparl+nc? baseline, and decreased for the
?Eparl+nc+crawl2? baseline. A decrease may occur
if new source phrases with less translation options
than the average are added. Furthermore, with the
addition of 45 million words of in-domain data, the
average entropy dropped from 1.84 to 1.33 or 1.60
for the ?Eparl+nc? baseline, and from 2.08 to 1.81 or
1.96 for the ?Eparl+nc+crawl2? baseline. With both
baselines, the more translations are added to the sys-
tem, the lower the entropy, although in some case
the number of translation options increases (this is
the case when we pass from 65M to 100M words
of synthetic data). These results illustrate the fact
that the automatic translations only reinforce some
probabilities in the model, with the subsequent de-
crease in entropy, while human translations add new
vocabulary. Note also that in the corpus using au-
tomatic translations, new words can only occur in
the source side. Thus when translating from French
to English, automatic translations from English to
French are expected to yield more translation op-
tions and a higher entropy than the automatic trans-
lations from French to English. This is what is ef-
fectively observed in Table 9.
5 Conclusion
Unsupervised training is widely used in other ar-
eas, in particular large vocabulary speech recogni-
tion. The statistical models in speech recognition
use a generative approach based on small units, usu-
ally triphones. Each triphone is modeled by a hid-
den Markov model and Gaussian mixture probabil-
ity distributions (plus many improvements like pa-
rameter tying etc). Many methods were developed
to adapt such models. The corresponding model
in statistical machine translation is the phrase table,
a long list of known words with their translations
and probabilities. It seems much more challenging
to adapt this kind of statistical model with unsuper-
vised training, i.e. monolingual data. Nevertheless,
we believe that unsupervised training can be also
very useful in SMT. To the best of our knowledge,
work in this area is very recent and only in its begin-
nings. This paper tries to give additional insights in
this promising method.
Our work is based on the approach initially pro-
291
baseline translated bitexts entries (M) translations entropy src size trg size
Eparl + nc - 7.16 83.83 1.84 1.80 2.81
news fe 45M 7.42 70.00 1.33 1.83 2.80
news ef 45M 8.24 81.58 1.60 1.86 2.79
news ef 65M 8.42 81.58 1.55 1.88 2.79
news ef 100M 9.21 85.93 1.54 1.90 2.79
Eparl + nc + crawl2 - 25.42 235.16 2.08 1.76 2.93
news fe 45M 25.54 217.21 1.81 1.77 2.93
news ef 45M 26.09 228.07 1.96 1.78 2.93
news ef 65M 26.21 226.45 1.91 1.78 2.93
news ef 100M 26.79 227.08 1.89 1.79 2.93
Table 9: Phrase table statistics for French?English systems augmented with bitexts built via automatic translations.
Only the entries useful to translate the development set were present in the considered phrase table.
posed in (Schwenk, 2008): build a first SMT sys-
tem, use it to translate large amounts of monolingual
data, filter the obtained translations, add them to the
bitexts and build a new system from scratch.
We proposed several extensions to this technique
which seem to improve the translations quality in
our experiments. First of all, we have observed that
it is clearly better to add automatically translated
texts to the translations model training data which
were translated from the target to the source lan-
guage. This seems to ensure that potentially wrong
translations are not used in the new model.
Second, we were able to skip the process of per-
forming word alignment of this additional parallel
data without any significant loss in the BLEU score.
Performing word alignments with GIZA++ can eas-
ily take several days when several hundred millions
of bitexts are available. Instead, we directly used the
word alignments produced by Moses when translat-
ing the monolingual data. This resulted in an appre-
ciable speed-up of the procedure, but has also inter-
esting theoretical aspects. Reusing the word align-
ment from the translation process is expected to re-
sult in a phrase extraction process that is more con-
sistent with the use of the phrases.
Finally, we outlined a method to automatically
add new translations without any additional parallel
training data. In fact, when translating from a mor-
phologically rich language to an easier one, in our
case from French to English, it is often possible to
infer the translations of unobserved morphological
forms of nouns, verbs or adjectives. This is obtained
by looking up the stemmed form in an automati-
cally constructed dictionary. This kind of approach
could be also applied to a classical PBSMT system,
by adding various forms to the phrase table, but it
is not obvious to come up with reasonable transla-
tions probabilities for these new entries. In our ap-
proach, the unknown word forms are processed in
large amounts of monolingual data and the induced
translations will appear in the context of complete
sentences. Wrong translations can be blocked by the
language model and the new translations can appear
in phrases of various lengths.
This paper provided a detailed experimental eval-
uation of these methods. We considered the trans-
lation between French and English using the same
data than was made available for the 2011 WMT
evaluation. Improvement of up to 0.5 BLEU were
observed with respect to an already competitive sys-
tem trained on more than 280M words of human
translated parallel data.
Acknowledgments
This work has been partially funded by the French
Government under the project COSMAT (ANR
ANR-09-CORD-004.) and the European Commis-
sion under the project EUROMATRIXPLUS (ICT-
2007.2.2-FP7-231720).
292
References
Nicola Bertoldi and Marcello Federico. 2009. Domain
adaptation for statistical machine translation. In Forth
Workshop on SMT, pages 182?189.
Nicola Bertoldi, Madalina Barbaiani, Marcello Federico,
and Roldano Cattoni. 2008. Phrase-based statistical
machine translation with pivot languages. In IWSLT,
pages 143?149.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for SMT self-
enhancement. In ACL, pages 157?160.
Jorge Civera and Alfons Juan. 2007. Domain adaptation
in statistical machine translation with mixture mod-
elling. In Second Workshop on SMT, pages 177?180,
June.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In EMNLP, pages 128?135.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In ACL 08.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In EMNLP, pages
967?975, Prague, Czech Republic.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Second Workshop on SMT, pages 224?227, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk.
2010. LIUM SMT machine translation system for
WMT 2010. In Workshop on SMT, pages 121?126.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Holger Schwenk and Jean Senellart. 2009. Translation
model adaptation for an Arabic/French news trans-
lation system by lightly-supervised training. In MT
Summit.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In ACL, pages 25?32.
Nicola Ueffing. 2006. Using monolingual source-
language data to improve MT performance. In IWSLT,
pages 174?181.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In ACL, pages 475?484, Uppsala, Sweden, July.
Association for Computational Linguistics.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Coling.
293
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 464?469,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIUM?s SMT Machine Translation Systems for WMT 2011
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Haithem Afli, Sadaf Abdul-Rauf and Kashif Shah
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French statisti-
cal machine translation systems for the 2011
WMT shared task evaluation. Our main sys-
tems were standard phrase-based statistical
systems based on the Moses decoder, trained
on the provided data only, but we also per-
formed initial experiments with hierarchical
systems. Additional, new features this year in-
clude improved translation model adaptation
using monolingual data, a continuous space
language model and the treatment of unknown
words.
1 Introduction
This paper describes the statistical machine trans-
lation systems developed by the Computer Science
laboratory at the University of Le Mans (LIUM) for
the 2011 WMT shared task evaluation. We only
considered the translation between French and En-
glish (in both directions). The main differences
with respect to previous year?s system (Lambert et
al., 2010) are as follows: use of more training data
as provided by the organizers, improved translation
model adaptation by unsupervised training, a con-
tinuous space language model for the translation
into French, some attempts to automatically induce
translations of unknown words and first experiments
with hierarchical systems. These different points are
described in the rest of the paper, together with a
summary of the experimental results showing the
impact of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used to
train the translation and language models of the sys-
tem.
2.1 Bilingual data
Our system was developed in two stages. First,
a baseline system was built to generate automatic
translations of some of the monolingual data avail-
able. These automatic translations were then used
directly with the source texts to create additional bi-
texts. In a second stage, these additional bilingual
data were incorporated into the system (see Sec-
tion 5 and Tables 4 and 5).
The latest version of the News-Commentary (NC)
corpus and of the Europarl (Eparl) corpus (version
6) were used. We also took as training data a sub-
set of the French?English Gigaword (109) corpus.
We applied the same filters as last year to select this
subset. The first one is a lexical filter based on the
IBM model 1 cost (Brown et al, 1993) of each side
of a sentence pair given the other side, normalised
with respect to both sentence lengths. This filter was
trained on a corpus composed of Eparl, NC, and UN
data. The other filter is an n-gram language model
(LM) cost of the target sentence (see Section 3), nor-
malised with respect to its length. This filter was
trained with all monolingual resources available ex-
cept the 109 data. We generated two subsets, both
by selecting sentence pairs with a lexical cost infe-
rior to 4, and an LM cost respectively inferior to 2.3
(1091, 115 million English words) and 2.6 (10
9
2, 232
million English words).
464
2.2 Use of Automatic Translations
Available human translated bitexts such as the Eu-
roparl or 109 corpus seem to be out-of domain for
this task. We used two types of automatically ex-
tracted resources to adapt our system to the task do-
main.
First, we generated automatic translations of the
provided monolingual News corpus and selected the
sentences with a normalised translation cost (re-
turned by the decoder) inferior to a threshold. The
resulting bitext contain no new translations, since
all words of the translation output come from the
translation model, but it contains new combinations
(phrases) of known words, and reinforces the prob-
ability of some phrase pairs (Schwenk, 2008). This
year, we improved this method in the following way.
In the original approach, the automatic translations
are added to the human translated bitexts and a com-
plete new system is build, including time consuming
word alignment with GIZA++. For WMT?11, we
directly used the word-to-word alignments produced
by the decoder at the output instead of GIZA?s align-
ments. This speeds-up the procedure and yields the
same results in our experiments. A detailed compar-
ison is given in (Lambert et al, 2011).
Second, as in last year?s evaluation, we automat-
ically extracted and aligned parallel sentences from
comparable in-domain corpora. We used the AFP
and APW news texts since there are available in the
French and English LDC Gigaword corpora. The
general architecture of our parallel sentence extrac-
tion system is described in detail by Abdul-Rauf and
Schwenk (2009). We first translated 91M words
from French into English using our first stage SMT
system. These English sentences were then used to
search for translations in the English AFP and APW
texts of the Gigaword corpus using information re-
trieval techniques. The Lemur toolkit (Ogilvie and
Callan, 2001) was used for this purpose. Search
was limited to a window of ?5 days of the date of
the French news text. The retrieved candidate sen-
tences were then filtered using the Translation Er-
ror Rate (TER) with respect to the automatic trans-
lations. In this study, sentences with a TER below
75% were kept. Sentences with a large length differ-
ence (French versus English) or containing a large
fraction of numbers were also discarded. By these
means, about 27M words of additional bitexts were
obtained.
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. In
addition, LDC?s Gigaword collection was used for
both languages. Data corresponding to the develop-
ment and test periods were removed from the Giga-
word collections.
2.4 Development data
All development was done on newstest2009, and
newstest2010 was used as internal test set. The de-
fault Moses tokenization was used. However, we
added abbreviations for the French tokenizer. All
our models are case sensitive and include punctua-
tion. The BLEU scores reported in this paper were
calculated with the tool multi-bleu.perl and are case
sensitive.
3 Architecture of the SMT system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sen-
tence f . Our main system is a phrase-based system
(Koehn et al, 2003; Och and Ney, 2003), but we
have also performed some experiments with a hier-
archical system (Chiang, 2007). Both use a log lin-
ear framework in order to introduce several models
explaining the translation process:
e? = argmax p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models
and the ?i weights are typically optimized to maxi-
mize a scoring function on a development set (Och
and Ney, 2002). The phrase-based system uses four-
teen features functions, namely phrase and lexical
translation probabilities in both directions, seven
features for the lexicalized distortion model, a word
and a phrase penalty and a target language model
(LM). The hierarchical system uses only 8 features:
a LM weight, a word penalty and six weights for the
translation model.
Both systems are based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
465
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++ that
can appear with rare words.
Phrases, lexical reorderings or hierarchical rules
are extracted using the default settings of the Moses
toolkit. The parameters of Moses were tuned on
newstest2009, using the ?new? MERT tool. We re-
peated the training process three times, each with a
different seed value for the optimisation algorithm.
In this way we have an rough idea of the error intro-
duced by the tuning process.
4-gram back-off LMs were used. The word list
contains all the words of the bitext used to train the
translation model and all words that appear at least
ten times in the monolingual corpora. Words of the
monolingual corpora containing special characters
or sequences of uppercase characters were not in-
cluded in the word list. Separate LMs were build on
each data source with the SRI LM toolkit (Stolcke,
2002) and then linearly interpolated, optimizing the
coefficients with an EM procedure. The perplexities
of these LMs were 99.4 for French and 129.7 for
English. In addition, we build a 5-gram continuous
space language model for French (Schwenk, 2007).
This model was trained on all the available French
texts using a resampling technique. The continu-
ous space language model is interpolated with the
4-gram back-off model and used to rescore n-best
lists. This reduces the perplexity by about 8% rela-
tive.
4 Treatment of unknown words
Finally, we propose a method to actually add new
translations to the system inspired from (Habash,
2008). For this, we propose to identity unknown
words and propose possible translations.
Moses has two options when encountering an un-
known word in the source language: keep it as it is
or drop it. The first option may be a good choice
for languages that use the same writing system since
the unknown word may be a proper name. The sec-
ond option is usually used when translating between
language based on different scripts, e.g. translating
1The source is available at http://www.cs.cmu.edu/
?qing/
Source language Source language Target language
French stemmed form English
finies fini finished
efface?s efface? erased
hawaienne hawaien Hawaiian
... ... ...
Table 1: Example of translations from French to English
which are automatically extracted from the phrase-table
with the stemmed form.
from Arabic to English. Alternatively, we propose to
infer automatically possible translations when trans-
lating from a morphologically rich language, to a
simpler language. In our case, we use this approach
to translate from French to English.
Several of the unknown words are actually adjec-
tives, nouns or verbs in a particular form that itself
is not known, but the phrase table would contain the
translation of a different form. As an example we
can mention the French adjective finies which is in
the female plural form. After stemming we may be
able to find the translation in a dictionary which is
automatically extracted from the phrase-table (see
Table 1). This idea was already outlined by (Bo-
jar and Tamchyna, 2011) to translate from Czech to
English.
First, we automatically extract a dictionary from
the phrase table. This is done, be detecting all 1-to-1
entries in the phrase table. When there are multi-
ple entries, all are kept with their lexical translations
probabilities. Our dictionary has about 680k unique
source words with a total of almost 1M translations.
source segment les travaux sont finis
target segment works are finis
stemmed word found fini
translations found finished, ended
segment proposed works are finished
works are ended
segment kept works are finished
Table 2: Example of the treatment of an unknown French
word and its automatically inferred translation.
The detection of unknown words is performed by
comparing the source and the target segment in order
to detect identical words. Once the unknown word
is selected, we are looking for its stemmed form in
the dictionary and propose some translations for the
unknown word based on lexical score of the phrase
table (see Table 2 for some examples). The snowball
466
Bitext #Fr Words PT size newstest2009 newstest2010
(M) (M) BLEU BLEU TER METEOR
Eparl+NC 56 7.1 26.74 27.36 (0.19) 55.11 (0.14) 60.13 (0.05)
Eparl+NC+1091 186 16.3 27.96 28.20 (0.04) 54.46 (0.10) 60.88 (0.05)
Eparl+NC+1092 323 25.4 28.20 28.57 (0.10) 54.12 (0.13) 61.20 (0.05)
Eparl+NC+news 140 8.4 27.31 28.41 (0.13) 54.15 (0.14) 61.13 (0.04)
Eparl+NC+1092+news 406 25.5 27.93 28.70 (0.24) 54.12 (0.16) 61.30 (0.20)
Eparl+NC+1092+IR 351 25.3 28.07 28.51 (0.18) 54.07 (0.06) 61.18 (0.07)
Eparl+NC+1092+news+IR 435 26.1 27.99 28.93 (0.02) 53.84 (0.07) 61.46 (0.07)
+larger beam+pruned PT 435 8.2 28.44 29.05 (0.14) 53.74 (0.16) 61.68 (0.09)
Table 4: French?English results: number of French words (in million), number of entries in the filtered phrase-table
(in million) and BLEU scores in the development (newstest2009) and internal test (newstest2010) sets for the different
systems developed. The BLEU scores and the number in parentheses are the average and standard deviation over 3
values (see Section 3)
corpus newstest2010 subtest2010
number of sentences 2489 109
number of words 70522 3586
number of UNK detected 118 118
nbr of sentences containing UNK 109 109
BLEU Score without UNK process 29.43 24.31
BLEU Score with UNK process 29.43 24.33
TER Score without UNK process 53.08 58.54
TER Score with UNK process 53.08 58.59
Table 3: Statistics of the unknown word (UNK) process-
ing algorithm on our internal test (newstest2010) and its
sub-part containing only the processed sentences (sub-
test2010).
stemmer2 was used. Then the different hypothesis
are evaluated with the target language model.
We processed the produced translations with this
method. It can happen that some words are transla-
tions of themselves, e.g. the French word ?duel? can
be translated by the English word ?duel?. If theses
words are present into the extracted dictionary, we
keep them. If we do not find any translation in our
dictionary, we keep the translation. By these means
we hope to keep named entities.
Several statistics made on our internal test (new-
stest2010) are shown in Table 3. Its shows that the
influence of the detected unknown words is minimal.
Only 0.16% of the words in the corpus are actually
unknown. However, the main goal of this process
is to increase the human readability and usefulness
without degrading automatic metrics. We also ex-
pect a larger impact in other tasks for which we have
2http://snowball.tartarus.org/
smaller amounts of parallel training data. In future
versions of this detection process, we will try to de-
tect unknown words before the translation process
and propose alternatives hypothesis to the Moses de-
coder.
5 Results and Discussion
The results of our SMT system for the French?
English and English?French tasks are summarized
in Tables 4 and 5, respectively. The MT metric
scores are the average of three optimisations per-
formed with different seeds (see Section 3). The
numbers in parentheses are the standard deviation
of these three values. The standard deviation gives
a lower bound of the significance of the difference
between two systems. If the difference between two
average scores is less than the sum of the standard
deviations, we can say that this difference is not sig-
nificant. The reverse is not true. Note that most of
the improvements shown in the tables are small and
not significant. However many of the gains are cu-
mulative and the sum of several small gains makes a
significant difference.
Baseline French?English System
The first section of Table 4 shows results of the de-
velopment of the baseline SMT system, used to gen-
erate automatic translations.
Although no French translations were generated,
we did similar experiments in the English?French
direction (first section of Table 5).
467
Bitext #En Words newstest2009 newstest2010
(M) BLEU BLEU TER
Eparl+NC 52 26.20 28.06 (0.22) 56.85 (0.08)
Eparl+NC+1091 167 26.84 29.08 (0.12) 55.83 (0.14)
Eparl+NC+1092 284 26.95 29.29 (0.03) 55.77 (0.19)
Eparl+NC+1092+news 299 27.34 29.56 (0.14) 55.44 (0.18)
Eparl+NC+1092+IR 311 27.14 29.43 (0.12) 55.48 (0.06)
Eparl+NC+1092+news+IR 371 27.32 29.73 (0.21) 55.16 (0.20)
+rescoring with CSLM 371 27.46 30.04 54.79
Table 5: English?French results: number of English words (in million) and BLEU scores in the development (new-
stest2009) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and the number
in parentheses are the average and standard deviation over 3 values (see Section 3.)
In both cases the best system is the one trained
on the Europarl, News-commentary and 1092 cor-
pora. This system was used to generate the auto-
matic translations. We did not observe any gain
when adding the United Nations data, so we dis-
carded this data.
Impact of the Additional Bitexts
With the baseline French?English SMT system (see
above), we translated the French News corpus to
generate an additional bitext (News). We also trans-
lated some parts of the French LDC Gigaword cor-
pus, to serve as queries to our IR system (see section
2.2). The resulting additional bitext is referred to as
IR. The second section of Tables 4 and 5 summarize
the system development including the additional bi-
texts.
With the News additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the baseline system used to generate the
automatic translations, but with less than half of
the data. Adding the News corpus to a larger cor-
pus, such as Eparl+NC+1092, has less impact but
still yields some improvement: 0.1 BLEU point in
French?English and 0.3 in English?French. Thus,
the News bitext translated from French to English
may have more impact when translating from En-
glish to French than in the opposite direction. This
effect is studied in detail in a separate paper (Lam-
bert et al, 2011). With the IR additional bitext added
to Eparl+NC+1092, we observe no improvement in
French to English, and a very small improvement
in English to French. However, added to the base-
line system (Eparl+NC+1092) adapted with the News
data, the IR additional bitexts yield a small (0.2
BLEU) improvement in both translation directions.
Final System
In both translation directions our best system was the
one trained on Eparl+NC+1092+News+IR. We fur-
ther achieved small improvements by pruning the
phrase-table and by increasing the beam size. To
prune the phrase-table, we used the ?sigtest-filter?
available in Moses (Johnson et al, 2007), more pre-
cisely the ??  filter3.
We also build hierarchical systems on the various
human translated corpora, using up to 323M words
(corpora Eparl+NC+1092). The systems yielded sim-
ilar results than the phrase-based approach, but re-
quired much more computational resources, in par-
ticular large amounts of main memory to perform
the translations. Running the decoder was actually
only possible with binarized rule-tables. Therefore,
the hierarchical system was not used in the evalua-
tion system.
3The p-value of two-by-two contingency tables (describing
the degree of association between a source and a target phrase)
is calculated with Fisher exact test. This probability is inter-
preted as the probability of observing by chance an association
that is at least as strong as the given one, and hence as its sig-
nificance. An important special case of a table occurs when a
phrase pair occurs exactly once in the corpus, and each of the
component phrases occurs exactly once in its side of the paral-
lel corpus (1-1-1 phrase pairs). In this case the negative log of
the p-value is ? = logN (N is number of sentence pairs in the
corpus). ? ?  is the largest threshold that results in all of the
1-1-1 phrase pairs being included.
468
6 Conclusions and Further Work
We presented the development of our statistical ma-
chine translation systems for the French?English
and English?French 2011 WMT shared task. In the
official evaluation the English?French system was
ranked first according to the BLEU score and the
French?English system second.
Acknowledgments
This work has been partially funded by the Euro-
pean Union under the EuroMatrixPlus project ICT-
2007.2.2-FP7-231720 and the French government
under the ANR project COSMAT ANR-09-CORD-
004.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 16?
23, Athens, Greece.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In ACL 08.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk.
2010. LIUM SMT machine translation system for
WMT 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 121?126, Uppsala, Sweden, July.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on translation
model adaptation using monolingual data. In Sixth
Workshop on SMT, page this volume.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments using
the Lemur toolkit. In In Proceedings of the Tenth Text
Retrieval Conference (TREC-10), pages 103?108.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proc. of the Int. Conf. on Spoken Lan-
guage Processing, pages 901?904, Denver, CO.
469
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 369?373,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LIUM?s SMT Machine Translation Systems for WMT 2012
Christophe Servan, Patrik Lambert, Anthony Rousseau,
Holger Schwenk and Lo??c Barrault
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French statisti-
cal machine translation systems for the 2012
WMT shared task evaluation. We developed
phrase-based systems based on the Moses de-
coder, trained on the provided data only. Ad-
ditionally, new features this year included im-
proved language and translation model adap-
tation using the cross-entropy score for the
corpus selection.
1 Introduction
This paper describes the statistical machine trans-
lation systems developed by the Computer Science
laboratory at the University of Le Mans (LIUM) for
the 2012 WMT shared task evaluation. We only
considered the translation between French and En-
glish (in both directions). The main differences with
respect to previous year?s system (Schwenk et al,
2011) are as follows: (i) use of more training data as
provided by the organizers and (ii) better selection
of the monolingual and parallel data according to
the domain, using the cross-entropy difference with
respect to in-domain and out-of-domain language
models (Moore and Lewis, 2010). We kept some
previous features: the improvement of the transla-
tion model adaptation by unsupervised training, a
parallel corpus retrieved by Information Retrieval
(IR) techniques and finally, the rescoring with a con-
tinuous space target language model for the trans-
lation into French. These different points are de-
scribed in the rest of the paper, together with a sum-
mary of the experimental results showing the impact
of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used to
train the translation and language models of the sys-
tem.
2.1 Bilingual data
The latest version of the News-Commentary (NC)
corpus and of the Europarl (Eparl) corpus (version
7) were used. We also took as training data a subset
of the French?English Gigaword (109) corpus. This
year we changed the filters applied to select this sub-
set (see Sect. 2.4). We also included in the training
data the test sets from previous shared tasks, that we
called the ntsXX corpus and which was composed
of newstest2008, newstest2009, newssyscomb2009.
2.2 Development data
Development was initially done on newstest2010,
and newstest2011 was used as internal test set (Sec-
tion 3.1). The development and internal test sets
were then (Section 4) switched (tuning was done
on newstest2011 and internal evaluation on new-
stest2010). The default Moses tokenization was
used. However, we added abbreviations for the
French tokenizer. All our models are case sensitive
and include punctuation. The BLEU scores reported
in this paper were calculated with the mteval-v13
tool and are case insensitive.
2.3 Use of Automatic Translations
Available human translated bitexts such as the Eu-
roparl or 109 corpus seem to be out-of domain for
this task. We used two types of automatically ex-
tracted resources to adapt our system to the domain.
369
First, we generated automatic translations of the
provided monolingual News corpus in French and
English, for years 2009, 2010 and 2011, and selected
the sentences with a normalised translation cost (re-
turned by the decoder) inferior to a threshold. The
resulting bitexts contain no new translations, since
all words of the translation output come from the
translation model, but they contain new combina-
tions (phrases) of known words, and reinforce the
probability of some phrase pairs (Schwenk, 2008).
Like last year, we directly used the word-to-word
alignments produced by the decoder at the output
instead of GIZA?s alignments. This speeds-up the
procedure and yields the same results in our experi-
ments. A detailed comparison is given in (Lambert
et al, 2011).
Second, as in last year?s evaluation, we auto-
matically extracted and aligned parallel sentences
from comparable in-domain corpora. We used the
AFP (Agence France Presse) and APW (Associated
Press Worldstream Service) news texts since there
are available in the French and English LDC Giga-
word corpora. The general architecture of our par-
allel sentence extraction system is described in de-
tail by Abdul-Rauf and Schwenk (2009). We first
translated 91M words from French into English us-
ing our first stage SMT system. These English sen-
tences were then used to search for translations in
the English AFP and APW texts of the Gigaword
corpus using information retrieval techniques. The
Lemur toolkit (Ogilvie and Callan, 2001) was used
for this purpose. Search was limited to a window of
?5 days of the date of the French news text. The re-
trieved candidate sentences were then filtered using
the Translation Error Rate (TER) with respect to the
automatic translations. In this study, sentences with
a TER below 75% were kept. Sentences containing
a large fraction of numbers were discarded. By these
means, about 27M words of additional bitexts were
obtained.
2.4 Domain-based Data selection
Before training the target language models, a text se-
lection has been made using the cross-entropy differ-
ence method (Moore and Lewis, 2010). This tech-
nique works by computing the difference between
two cross-entropy values.
We first score an out-of-domain corpus against
a language model trained on a set of in-domain
data and compute the cross-entropy for each sen-
tence. Then, we score the same out-of-domain cor-
pus against a language model trained on a random
sample of itself, with a size roughly equal to the in-
domain corpus. From this point, the difference be-
tween in-domain cross-entropy and out-of-domain
cross-entropy is computed for each sentence, and
these sentences are sorted regarding this score.
By estimating and minimizing on a development
set the perplexity of several percentages of the sorted
out-of-domain corpus, we can then estimate the the-
oretical best point of data size for this specific cor-
pus. According the original paper and given our re-
sults, this leads to better selection than the simple
perplexity sorting (Gao et al, 2002). This way, we
can be assured to discard the vast majority of noise
in the corpora and to select data well-related to the
task.
In this task, the French and English target lan-
guage models were trained on data selected from all
provided monolingual corpora. In addition, LDC?s
Gigaword collection was used for both languages.
Data corresponding to the development and test pe-
riods were removed from the Gigaword collections.
We had time to apply the domain-based data selec-
tion only for French. Thus all data were used for
English.
We used this method to filter the French?English
109 parallel corpus as well, based on the differ-
ence between in-domain cross-entropy and out-of-
domain cross-entropy calculated for each sentence
of the English side of the corpus. We kept 49 mil-
lion words (in the English side) to train our models,
called 109f .
3 Architecture of the SMT system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sentence
f . We have build phrase-based systems (Koehn et
al., 2003; Och and Ney, 2003), using the standard
log linear framework in order to introduce several
models explaining the translation process:
e? = argmax p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
370
The feature functions hi are the system models
and the ?i weights are typically optimized to maxi-
mize a scoring function on a development set (Och,
2003). The phrase-based system uses fourteen fea-
tures functions, namely phrase and lexical transla-
tion probabilities in both directions, seven features
for the lexicalized distortion model, a word and a
phrase penalty and a target language model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and is constructed as follows.
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++ that
can appear with rare words.
Phrases and lexical reorderings are extracted us-
ing the default settings of the Moses toolkit. The
parameters of Moses were tuned using the MERT
tool. We repeated the training process three times,
each with a different seed value for the optimisation
algorithm. In this way we have a rough idea of the
error introduced by the tuning process.
4-gram back-off LMs were used. The word list
contains all the words of the bitext used to train the
translation model and all words that appear at least
ten times in the monolingual corpora. Words of the
monolingual corpora containing special characters
or sequences of uppercase characters were not in-
cluded in the word list. Separate LMs were build
on each data source with the SRI LM toolkit (Stol-
cke, 2002) and then linearly interpolated, optimizing
the coefficients with an EM procedure. The perplex-
ities of these LMs on newstest2011 were 119.1 for
French and 174.8 for English. In addition, we build a
5-gram continuous space language model for French
(Schwenk, 2007). These models were trained on
all the available texts using a resampling technique.
The continuous space language model is interpo-
lated with the 4-gram back-off model and used to
rescore n-best lists. This reduces the perplexity by
about 13% relative.
3.1 Number translation
We have also performed some experiments with
number translation. English and French do not use
1The source is available at http://www.cs.cmu.edu/
?qing/
the same conventions for integer and decimal num-
bers. For example, the English decimal number 0.99
is translated in French by 0,99. In the same way,
the English integer 32,000 is translated in French by
32 000. It should be possible to perform these mod-
ifications by rules.
In this study, we first replaced the numbers by a
tag @@NUM for integer and @@DEC for decimal num-
bers. Integers in the range 1 to 31 were not replaced
since they appear in dates. Then, we created the tar-
get language model using the tagged corpora. Ta-
ble 1 shows results of experiments performed with
and without rule-based number translation.
Corpus NT BLEU TER
NC no 26.57 (0.07) 58.13 (0.06)
NC yes 26.84 (0.15) 57.71 (0.34)
Eparl+NC no 29.28 (0.11) 55.28 (0.13)
Eparl+NC yes 29.26 (0.10) 55.44 (0.29)
Table 1: Results of the study on number translation (NT)
from English to French
We did observe small gains in the translation
quality when only the news-commentary bitexts are
used, but there were no differences when more train-
ing data is available. Due to time constraints, this
procedure was not used in the submitted system.
4 Results and Discussion
The results of our SMT systems are summarized in
Table 2. The MT metric scores for the development
set are the average of three optimisations performed
with different seeds (see Section 3). For the test set,
they are the average of four values: the three val-
ues corresponding to these different optimisations,
plus a fourth value obtained by taking as weight for
each model, the average of the weights obtained in
the three optimisations (Cettolo et al, 2011). The
numbers in parentheses are the standard deviation of
these three or four values. The standard deviation
gives a lower bound of the significance of the differ-
ence between two systems. If the difference between
two average scores is less than the sum of the stan-
dard deviations, we can say that this difference is not
significant. The reverse is not true.
The results of Table 2 show that adding several
adapted corpora (the filtered 109 corpus, the syn-
371
Bitext #Source newstest2011 newstest2010
Words (M) BLEU TER BLEU TER
Translation : En?Fr
Eparl+NC 57 30.91 (0.05) 53.61 (0.12) 28.45 (0.08) 56.29 (0.20)
Eparl+NC+ntsXX 58 31.12 (0.08) 53.67 (0.08) 28.49 (0.04) 56.45 (0.12)
Eparl+NC+ntsXX+109f 107 31.67 (0.06) 53.29 (0.03) 29.38 (0.12) 55.45 (0.15)
Eparl+NC+ntsXX+109f+IR 133 32.41 (0.02) 52.20 (0.02) 29.48 (0.11) 55.33 (0.20)
Eparl+NC+ntsXX+109f+news+IR 162 32.26 (0.04) 52.24 (0.12) 29.79 (0.12) 55.04 (0.20)
Translation : Fr?En
Eparl+NC 64 29.59 (0.12) 51.86 (0.06) 28.12 (0.05) 53.19 (0.06)
Eparl+NC+ntsXX 64 29.59 (0.04) 51.89 (0.14) 28.32 (0.08) 53.22 (0.08)
Eparl+NC+ntsXX+109f 120 30.69 (0.06) 50.77 (0.04) 28.95 (0.14) 52.62 (0.14)
Eparl+NC+ntsXX+109f+IR 149 30.56 (0.02) 50.94 (0.15) 28.67 (0.11) 52.78 (0.06)
Eparl+NC+ntsXX+109f+news+IR 179 30.85 (0.07) 50.72 (0.03) 28.94 (0.05) 52.57 (0.02)
Table 2: English?French and French?English results: number of source words (in million) and scores on the develop-
ment (newstest2011) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and
the number in parentheses are the average and standard deviation over 3 or 4 values when available (see Section 4.)
thetic corpus and the corpus retrieved via IR meth-
ods) to the Eparl+NC+ntsXX baseline, a gain of 1.1
BLEU points and 1.4 TER points was achieved for
the English?French system.
On the other hand, adding the bitexts extracted
from the comparable corpus (IR) does actually hurt
the performance of the French?English system: the
BLEU score decreases from 28.95 to 28.67 on our
internal test set. During the evaluation period, we
added all the corpora at once and we observed this
only in our analysis after the evaluation.
In both translation directions our
best system was the one trained on
Eparl+NC+ntsXX+109f+News+IR. Finally, we
applied a continuous space language model for the
system translating into French.
Acknowledgments
This work has been partially funded by the Euro-
pean Union under the EuroMatrixPlus project ICT-
2007.2.2-FP7-231720 and the French government
under the ANR project COSMAT ANR-09-CORD-
004.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 16?
23, Athens, Greece.
Mauro Cettolo, Nicola Bertoldi, and Marcello Federico.
2011. Methods for smoothing the optimizer instability
in SMT. In Proc. of Machine Translation Summit XIII,
Xiamen, China.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to statistical
language modeling for chinese. In ACM Transactions
on Asian Language Information Processing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on translation
372
model adaptation using monolingual data. In Sixth
Workshop on SMT, pages 284?293.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of the ACL 2010 Conference Short Papers.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
Paul Ogilvie and Jamie Callan. 2001. Experiments using
the Lemur toolkit. In In Proceedings of the Tenth Text
Retrieval Conference (TREC-10), pages 103?108.
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Sadaf Abdul-Rauf, Haithem Afli,
and Kashif Shah. 2011. Lium?s smt machine trans-
lation systems for WMT 2011. In Proceedings of
the Sixth Workshop on Statistical Machine Translation,
pages 464?469, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proc. of the Int. Conf. on Spoken Lan-
guage Processing, pages 901?904, Denver, CO.
373
