Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 827?834,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Infrastructure for standardization of Asian language resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Chu-Ren Huang
Academia Sinica
Xia YingJu
Fujitsu R&D Center
Yu Hao
Fujitsu R&D Center
Laurent Prevot
Academia Sinica
Shirai Kiyoaki
JAIST
Abstract
As an area of great linguistic and cul-
tural diversity, Asian language resources
have received much less attention than
their western counterparts. Creating a
common standard for Asian language re-
sources that is compatible with an interna-
tional standard has at least three strong ad-
vantages: to increase the competitive edge
of Asian countries, to bring Asian coun-
tries to closer to their western counter-
parts, and to bring more cohesion among
Asian countries. To achieve this goal, we
have launched a two year project to create
a common standard for Asian language re-
sources. The project is comprised of four
research items, (1) building a description
framework of lexical entries, (2) building
sample lexicons, (3) building an upper-
layer ontology and (4) evaluating the pro-
posed framework through an application.
This paper outlines the project in terms of
its aim and approach.
1 Introduction
There is a long history of creating a standard
for western language resources. The human
language technology (HLT) society in Europe
has been particularly zealous for the standardiza-
tion, making a series of attempts such as EA-
GLES1, PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Calzolari et al, 2003) and LIRICS2.
These continuous efforts has been crystallized as
activities in ISO-TC37/SC4 which aims to make
an international standard for language resources.
1http://www.ilc.cnr.it/Eagles96/home.html
2lirics.loria.fr/documents.html
(1) Description 
framework of lexical 
entries
(2) Sample lexicons
(4) Evaluation 
through application
(3) Upper layer 
ontologyrefinement
description classification
refinement
evaluationevaluation
Figure 1: Relations among research items
On the other hand, since Asia has great lin-
guistic and cultural diversity, Asian language re-
sources have received much less attention than
their western counterparts. Creating a common
standard for Asian language resources that is com-
patible with an international standard has at least
three strong advantages: to increase the competi-
tive edge of Asian countries, to bring Asian coun-
tries to closer to their western counterparts, and to
bring more cohesion among Asian countries.
To achieve this goal, we have launched a two
year project to create a common standard for
Asian language resources. The project is com-
prised of the following four research items.
(1) building a description framework of lexical
entries
(2) building sample lexicons
(3) building an upper-layer ontology
(4) evaluating the proposed framework through
an application
Figure 1 illustrates the relations among these re-
search items.
Our main aim is the research item (1), building
a description framework of lexical entries which
827
fits with as many Asian languages as possible, and
contributing to the ISO-TC37/SC4 activities. As
a starting point, we employ an existing descrip-
tion framework, the MILE framework (Bertagna
et al, 2004a), to describe several lexical entries of
several Asian languages. Through building sam-
ple lexicons (research item (2)), we will find prob-
lems of the existing framework, and extend it so
as to fit with Asian languages. In this extension,
we need to be careful in keeping consistency with
the existing framework. We start with Chinese,
Japanese and Thai as target Asian languages and
plan to expand the coverage of languages. The re-
search items (2) and (3) also comprise the similar
feedback loop. Through building sample lexicons,
we refine an upper-layer ontology. An application
built in the research item (4) is dedicated to evalu-
ating the proposed framework. We plan to build an
information retrieval system using a lexicon built
by extending the sample lexicon.
In what follows, section 2 briefly reviews the
MILE framework which is a basis of our de-
scription framework. Since the MILE framework
is originally designed for European languages, it
does not always fit with Asian languages. We ex-
emplify some of the problems in section 3 and sug-
gest some directions to solve them. We expect
that further problems will come into clear view
through building sample lexicons. Section 4 de-
scribes a criteria to choose lexical entries in sam-
ple lexicons. Section 5 describes an approach
to build an upper-layer ontology which can be
sharable among languages. Section 6 describes
an application through which we evaluate the pro-
posed framework.
2 The MILE framework for
interoperability of lexicons
The ISLE (International Standards for Language
Engineering) Computational Lexicon Working
Group has consensually defined the MILE (Mul-
tilingual ISLE Lexical Entry) as a standardized
infrastructure to develop multilingual lexical re-
sources for HLT applications, with particular at-
tention toMachine Translation (MT) and Crosslin-
gual Information Retrieval (CLIR) application
systems.
The MILE is a general architecture devised
for the encoding of multilingual lexical informa-
tion, a meta-entry acting as a common representa-
tional layer for multilingual lexicons, by allowing
integration and interoperability between different
monolingual lexicons3.
This formal and standardized framework to en-
code MILE-conformant lexical entries is provided
to lexicon and application developers by the over-
all MILE Lexical Model (MLM). As concerns
the horizontal organization, the MLM consists of
two independent, but interlinked primary compo-
nents, the monolingual and the multilingual mod-
ules. The monolingual component, on the vertical
dimension, is organized over three different repre-
sentational layers which allow to describe differ-
ent dimensions of lexical entries, namely the mor-
phological, syntactic and semantic layers. More-
over, an intermediate module allows to define
mechanisms of linkage and mapping between the
syntactic and semantic layers. Within each layer, a
basic linguistic information unit is identified; basic
units are separated but still interlinked each other
across the different layers.
Within each of the MLM layers, different types
of lexical object are distinguished :
? the MILE Lexical Classes (MLC) represent
the main building blocks which formalize
the basic lexical notions. They can be seen
as a set of structural elements organized in
a layered fashion: they constitute an on-
tology of lexical objects as an abstraction
over different lexical models and architec-
tures. These elements are the backbone of
the structural model. In the MLM a defini-
tion of the classes is provided together with
their attributes and the way they relate to each
other. Classes represent notions like Inflec-
tionalParadigm, SyntacticFunction, Syntac-
ticPhrase, Predicate, Argument,
? the MILE Data Categories (MDC) which
constitute the attributes and values to adorn
the structural classes and allow concrete en-
tries to be instantiated. MDC can belong to
a shared repository or be user-defined. ?NP?
and ?VP? are data category instances of the
class SyntacticPhrase, whereas and ?subj?
and ?obj? are data category instances of the
class SyntacticFunction.
? lexical operations, which are special lexical
entities allowing the user to define multilin-
3MILE is based on the experience derived from exist-
ing computational lexicons (e.g. LE-PAROLE, SIMPLE, Eu-
roWordNet, etc.).
828
gual conditions and perform operations on
lexical entries.
Originally, in order to meet expectations placed
upon lexicons as critical resources for content pro-
cessing in the Semantic Web, the MILE syntactic
and semantic lexical objects have been formalized
in RDF(S), thus providing a web-based means to
implement the MILE architecture and allowing for
encoding individual lexical entries as instances of
the model (Ide et al, 2003; Bertagna et al, 2004b).
In the framework of our project, by situating our
work in the context of W3C standards and relying
on standardized technologies underlying this com-
munity, the original RDF schema for ISLE lexi-
cal entries has been made compliant to OWL. The
whole data model has been formalized in OWL by
using Prote?ge? 3.2 beta and has been extended to
cover the morphological component as well (see
Figure 2). Prote?ge? 3.2 beta has been also used as
a tool to instantiate the lexical entries of our sam-
ple monolingual lexicons, thus ensuring adherence
to the model, encoding coherence and inter- and
intra-lexicon consistency.
3 Existing problems with the MILE
framework for Asian languages
In this section, we will explain some problematic
phenomena of Asian languages and discuss pos-
sible extensions of the MILE framework to solve
them.
Inflection The MILE provides the powerful
framework to describe the information about in-
flection. InflectedForm class is devoted to de-
scribe inflected forms of a word, while Inflec-
tionalParadigm to define general inflection rules.
However, there is no inflection in several Asian
languages, such as Chinese and Thai. For these
languages, we do not use the Inflected Form and
Inflectional Paradigm.
Classifier Many Asian languages, such as
Japanese, Chinese, Thai and Korean, do not dis-
tinguish singularity and plurality of nouns, but use
classifiers to denote the number of objects. The
followings are examples of classifiers of Japanese.
? inu
(dog)
ni
(two)
hiki
(CL)
? ? ? two dogs
? hon
(book)
go
(five)
satsu
(CL)
? ? ? five books
?CL? stands for a classifier. They always follow
cardinal numbers in Japanese. Note that differ-
ent classifiers are used for different nouns. In the
above examples, classifier ?hiki? is used to count
noun ?inu (dog)?, while ?satsu? for ?hon (book)?.
The classifier is determined based on the semantic
type of the noun.
In the Thai language, classifiers are used in var-
ious situations (Sornlertlamvanich et al, 1994).
The classifier plays an important role in construc-
tion with noun to express ordinal, pronoun, for in-
stance. The classifier phrase is syntactically gener-
ated according to a specific pattern. Here are some
usages of classifiers and their syntactic patterns.
? Enumeration
(Noun/Verb)-(cardinal number)-(CL)
e.g. nakrian
(student)
3 khon
(CL)
? ? ? three students
? Ordinal
(Noun)-(CL)-/thi:/-(cardinal number)
e.g. kaew
(glass)
bai
(CL)
thi: 4
(4th)
? ? ? the 4th glass
? Determination
(Noun)-(CL)-(Determiner)
e.g. kruangkhidlek
(calculator)
kruang
(CL)
nii
(this)
? ? ? this calculator
Classifiers could be dealt as a class of the part-
of-speech. However, since classifiers depend on
the semantic type of nouns, we need to refer to
semantic features in the morphological layer, and
vice versa. Some mechanism to link between fea-
tures beyond layers needs to be introduced into the
current MILE framework.
Orthographic variants Many Chinese words
have orthographic variants. For instance, the con-
cept of rising can be represented by either char-
acter variants of sheng1: ? or ?. However,
the free variants become non-free in certain com-
pound forms. For instance, only? allowed for?
? ?liter?, and only? is allowed for?? ?to sub-
lime?. The interaction of lemmas and orthographic
variations is not yet represented in MILE.
Reduplication as a derivational process In
some Asian languages, reduplication of words de-
rives another word, and the derived word often has
a different part-of-speech. Here are some exam-
ples of reduplication in Chinese. Man4 ? ?to be
slow? is a state verb, while a reduplicated form
829
Inflectional
Paradigm
Lexical Entry SyntacticUnit
Form Lemmatized Form Stem
Inflected Form
Combiner
Calculator Mrophfeat
Operation Argument
Morph
DataCats
0..*
0..* 0..*
0..*
0..*
0..1
0..*
0..*
1..*
<LemmatizedForm rdf:ID="LFstar">
  <hasInflectedForm>
    <InflectedForm rdf:ID="stars">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="pl">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    plural
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
  <hasInflectedForm>
    <InflectedForm rdf:ID="star">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="sg">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    singular
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
</LemmatiedForm>
Figure 2: Formalization of the morphological layer and excerpt of a sample RDF instantiation
man4-man4 ?? is an adverb. Another example
of reduplication involves verbal aspect. Kan4 ?
?to look? is an activity verb, while the reduplica-
tive form kan4-kan4 ??, refers to the tentative
aspect, introducing either stage-like sub-division
or the event or tentativeness of the action of the
agent. This morphological process is not provided
for in the current MILE standard.
There are also various usages of reduplication in
Thai. Some words reduplicate themselves to add a
specific aspect to the original meaning. The redu-
plication can be grouped into 3 types according to
the tonal sound change of the original word.
? Word reduplication without sound change
e.g. /dek-dek/ ? ? ? (N) children, (ADV) child-
ishly, (ADJ) childish
/sa:w-sa:w/ ? ? ? (N) women
? Word reduplication with high tone on the first
word
e.g. /dam4-dam/ ? ? ? (ADJ) extremely black
/bo:i4-bo:i/ ? ? ? (ADV) really often
? Triple word reduplication with high tone on
the second word
e.g. /dern-dern4-dern/ ?? (V) intensively walk
/norn-norn4-norn/??(V) intensively sleep
In fact, only the reduplication of the same sound
is accepted in the written text, and a special sym-
bol, namely /mai-yamok/ is attached to the origi-
nal word to represent the reduplication. The redu-
plication occurs in many parts-of-speech, such as
noun, verb, adverb, classifier, adjective, preposi-
tion. Furthermore, various aspects can be added
to the original meaning of the word by reduplica-
tion, such as pluralization, emphasis, generaliza-
tion, and so on. These aspects should be instanti-
ated as features.
Change of parts-of-speech by affixes Af-
fixes change parts-of-speech of words in
Thai (Charoenporn et al, 1997). There are
three prefixes changing the part-of-speech of the
original word, namely /ka:n/, /khwa:m/, /ya:ng/.
They are used in the following cases.
? Nominalization
/ka:n/ is used to prefix an action verb and
/khwa:m/ is used to prefix a state verb
in nominalization such as /ka:n-tham-nga:n/
(working), /khwa:m-suk/ (happiness).
? Adverbialization
An adverb can be derived by using /ya:ng/ to
prefix a state verb such as /ya:ng-di:/ (well).
Note that these prefixes are also words, and form
multi-word expressions with the original word.
This phenomenon is similar to derivation which
is not handled in the current MILE framework.
Derivation is traditionally considered as a different
phenomenon from inflection, and current MILE
focuses on inflection. The MILE framework is al-
ready being extended to treat such linguistic phe-
nomenon, since it is important to European lan-
guages as well. It would be handled in either the
morphological layer or syntactic layer.
830
Function Type Function types of predicates
(verbs, adjectives etc.) might be handled in a
partially different way for Japanese. In the syn-
tactic layer of the MILE framework, Function-
Type class is prepared to denote subcategorization
frames of predicates, and they have function types
such as ?subj? and ?obj?. For example, the verb
?eat? has two FunctionType data categories of
?subj? and ?obj?. Function types basically stand
for positions of case filler nouns. In Japanese,
cases are usually marked by postpositions and case
filler positions themselves do not provide much in-
formation on case marking. For example, both of
the following sentences mean the same, ?She eats
a pizza.?
? kanojo
(she)
ga
(NOM)
piza
(pizza)
wo
(ACC)
taberu
(eat)
? piza
(pizza)
wo
(ACC)
kanojo
(she)
ga
(NOM)
taberu
(eat)
?Ga? and ?wo? are postpositions which mark
nominative and accusative cases respectively.
Note that two case filler nouns ?she? and ?pizza?
can be exchanged. That is, the number of slots is
important, but their order is not.
For Japanese, we might use the set of post-
positions as values of FunctionType instead of
conventional function types such as ?subj? and
?obj?. It might be an user defined data category or
language dependent data category. Furthermore,
it is preferable to prepare the mapping between
Japanese postpositions and conventional function
types. This is interesting because it seems more
a terminological difference, but the model can be
applied also to Japanese.
4 Building sample lexicons
4.1 Swadesh list and basic lexicon
The issue involved in defining a basic lexicon for a
given language is more complicated than one may
think (Zhang et al, 2004). The naive approach of
simply taking the most frequent words in a lan-
guage is flawed in many ways. First, all frequency
counts are corpus-based and hence inherit the bias
of corpus sampling. For instance, since it is eas-
ier to sample written formal texts, words used pre-
dominantly in informal contexts are usually under-
represented. Second, frequency of content words
is topic-dependent and may vary from corpus to
corpus. Last, and most crucially, frequency of a
word does not correlate to its conceptual necessity,
which should be an important, if not only, criteria
for core lexicon. The definition of a cross-lingual
basic lexicon is even more complicated. The first
issue involves determination of cross-lingual lexi-
cal equivalencies. That is, how to determine that
word a (and not a?) in language A really is word b
in language B. The second issue involves the deter-
mination of what is a basic word in a multilingual
context. In this case, not even the frequency of-
fers an easy answer since lexical frequency may
vary greatly among different languages. The third
issue involves lexical gaps. That is, if there is a
word that meets all criteria of being a basic word
in language A, yet it does not exist in language D
(though it may exist in languages B, and C). Is this
word still qualified to be included in the multilin-
gual basic lexicon?
It is clear not all the above issues can be un-
equivocally solved with the time frame of our
project. Fortunately, there is an empirical core lex-
icon that we can adopt as a starting point. The
Swadesh list was proposed by the historical lin-
guist Morris Swadesh (Swadesh, 1952), and has
been widely used by field and historical linguists
for languages over the world. The Swadesh list
was first proposed as lexico-statistical metrics.
That is, these are words that can be reliably ex-
pected to occur in all historical languages and can
be used as the metrics for quantifying language
variations and language distance. The Swadesh
list is also widely used by field linguists when
they encounter a new language, since almost all
of these terms can be expected to occur in any
language. Note that the Swadesh list consists of
terms that embody human direct experience, with
culture-specific terms avoided. Swadesh started
with a 215 items list, before cutting back to 200
items and then to 100 items. A standard list of
207 items is arrived at by unifying the 200 items
list and the 100 items list. We take the 207 terms
from the Swadesh list as the core of our basic lex-
icon. Inclusion of the Swadesh list also gives us
the possibility of covering many Asian languages
in which we do not have the resources to make a
full and fully annotated lexicon. For some of these
languages, a Swadesh lexicon for reference is pro-
vided by a collaborator.
4.2 Aligning multilingual lexical entries
Since our goal is to build a multilingual sample
lexicon, it is required to align words in several
831
Asian languages. In this subsection, we propose
a simple method to align words in different lan-
guages. The basic idea for multilingual alignment
is an intermediary by English. That is, first we
prepare word pairs between English and other lan-
guages, then combine them together to make cor-
respondence among words in several languages.
The multilingual alignment method currently we
consider is as follows:
1. Preparing the set of frequent words of each
language
Suppose that {Jw
i
}, {Cw
i
}, {Tw
i
} is the
set of frequent words of Japanese, Chinese
and Thai, respectively. Now we try to con-
struct a multilingual lexicon for these three
languages, however, our multilingual align-
ment method can be easily extended to han-
dle more languages.
2. Obtaining English translations
A word Xw
i
is translated into a set of En-
glish words EXw
ij
by referring to the bilin-
gual dictionary, where X denotes one of our
languages, J , C or T . We can obtain map-
pings as in (1).
Jw
1
: EJw
11
, EJw
12
, ? ? ?
Jw
2
: EJw
21
, EJw
22
, ? ? ?
...
Cw
1
: ECw
11
, ECw
12
, ? ? ?
Cw
2
: ECw
21
, ECw
22
, ? ? ?
...
Tw
1
: ETw
11
, ETw
12
, ? ? ?
Tw
2
: ETw
21
, ETw
22
, ? ? ?
...
(1)
Notice that this procedure is automatically
done and ambiguities would be left at this
stage.
3. Generating new mapping
From mappings in (1), a new mapping is gen-
erated by inverting the key. That is, in the
new mapping, a key is an English word Ew
i
and a correspondence for each key is sets
of translations XEw
ij
for 3 languages, as
shown in (2):
Ew
1
: (JEw
11
, JEw
12
, ? ? ?)
(CEw
11
, CEw
12
, ? ? ?)
(TEw
11
, TEw
12
, ? ? ?)
Ew
2
: (JEw
21
, JEw
22
, ? ? ?)
(CEw
21
, CEw
22
, ? ? ?)
(TEw
21
, TEw
22
, ? ? ?)
...
(2)
Notice that at this stage, correspondence be-
tween different languages is very loose, since
they are aligned on the basis of sharing only
a single English word.
4. Refinement of alignment
Groups of English words are constructed by
referring to the WordNet synset information.
For example, suppose that Ew
i
and Ew
j
be-
long to the same synset S
k
. We will make a
new alignment by making an intersection of
{XEw
i
} and {XEw
j
} as shown in (3).
Ew
i
: (JEw
i1
, ??) (CEw
i1
, ??) (TEw
i1
, ??)
Ew
j
: (JEw
j1
, ??)(CEw
j1
, ??)(TEw
j1
, ??)
? intersection
S
k
: (JEw?
k1
, ??)(CEw?
k1
, ??)(TEw?
k1
, ??)
(3)
In (3), the key is a synset S
k
, which is sup-
posed to be a conjunction of Ew
i
and Ew
j
,
and the counterpart is the intersection of set
of translations for each language. This oper-
ation would reduce the number of words of
each language. That means, we can expect
that the correspondence among words of dif-
ferent languages becomes more precise. This
new word alignment based on a synset is a
final result.
To evaluate the performance of this method,
we conducted a preliminary experiment using the
Swadesh list. Given the Swadesh list of Chi-
nese, Italian, Japanese and Thai as a gold stan-
dard, we tried to replicate these lists from the En-
glish Swadesh list and bilingual dictionaries be-
tween English and these languages. In this experi-
ment, we did not perform the refinement step with
WordNet. From 207 words in the Swadesh list,
we dropped 4 words (?at?, ?in?, ?with? and ?and?)
due to their too many ambiguities in translation.
As a result, we obtained 181 word groups
aligned across 5 languages (Chinese, English, Ital-
ian, Japanese and Thai) for 203 words. An
aligned word group was judged ?correct? when the
words of each language include only words in the
Swadesh list of that language. It was judged ?par-
tially correct? when the words of a language also
include the words which are not in the Swadesh
list. Based on the correct instances, we obtain
0.497 for precision and 0.443 for recall. These fig-
ures go up to 0.912 for precision and 0.813 for re-
call when based on the partially correct instances.
This is quite a promising result.
832
5 Upper-layer ontology
The empirical success of the Swadesh list poses
an interesting question that has not been explored
before. That is, does the Swadesh list instantiates a
shared, fundamental human conceptual structure?
And if there is such as a structure, can we discover
it?
In the project these fundamental issues are as-
sociated with our quest for cross-lingual interop-
erability. We must make sure that the items of
the basic lexicon are given the same interpreta-
tion. One measure taken to ensure this consists in
constructing an upper-ontology based on the ba-
sic lexicon. Our preliminary work of mapping the
Swadesh list items to SUMO (Suggested Upper
Merged Ontology) (Niles and Pease, 2001) has al-
ready been completed. We are in the process of
mapping the list to DOLCE (Descriptive Ontology
for Linguistic and Cognitive Engineering) (Ma-
solo et al, 2003). After the initial mapping, we
carry on the work to restructure the mapped nodes
to form a genuine conceptual ontology based on
the language universal basic lexical items. How-
ever one important observation that we have made
so far is that the success of the Swadesh list is
partly due to its underspecification and to the lib-
erty it gives to compilers of the list in a new lan-
guage. If this idea of underspecification is essen-
tial for basic lexicon for human languages, then we
must resolve this apparent dilemma of specifying
them in a formal ontology that requires fully spec-
ified categories. For the time being, genuine ambi-
guities resulted in the introduction of each disam-
biguated sense in the ontology. We are currently
investigating another solution that allows the in-
clusion of underspecified elements in the ontology
without threatening its coherence. More specifi-
cally we introduce a underspecified relation in the
structure for linking the underspecified meaning
to the different specified meaning. The specified
meanings are included in the taxonomic hierarchy
in a traditional manner, while a hierarchy of un-
derspecified meanings can be derived thanks to the
new relation. An underspecified node only inherits
from the most specific common mother of its fully
specified terms. Such distinction avoids the clas-
sical misuse of the subsumption relation for rep-
resenting multiple meanings. This method does
not reflect a dubious collapse of the linguistic and
conceptual levels but the treatment of such under-
specifications as truly conceptual. Moreover we
Internet
Query
Local 
DB
User interest
 model
Topic
Feedback
Search
engine
Crawler
Retrieval
results
Figure 3: The system architecture
hope this proposal will provide a knowledge rep-
resentation framework for the multilingual align-
ment method presented in the previous section.
Finally, our ontology will not only play the role
of a structured interlingual index. It will also serve
as a common conceptual base for lexical expan-
sion, as well as for comparative studies of the lex-
ical differences of different languages.
6 Evaluation through an application
To evaluate the proposed framework, we are build-
ing an information retrieval system. Figure 3
shows the system architecture.
A user can input a topic to retrieve the docu-
ments related to that topic. A topic can consist
of keywords, website URL?s and documents which
describe the topic. From the topic information, the
system builds a user interest model. The system
then uses a search engine and a crawler to search
for information related to this topic in WWW and
stores the results in the local database. Generally,
the search results include many noises. To filter
out these noises, we build a query from the user
interest model and then use this query to retrieve
documents in the local database. Those documents
similar to the query are considered as more related
to the topic and the user?s interest, and are returned
to the user. When the user obtains these retrieval
results, he can evaluate these documents and give
the feedback to the system, which is used for the
further refinement of the user interest model.
Language resources can contribute to improv-
ing the system performance in various ways.
Query expansion is a well-known technique which
expands user?s query terms into a set of similar and
related terms by referring to ontologies. Our sys-
tem is based on the vector space model (VSM) and
traditional query expansion can be applicable us-
ing the ontology.
There has been less research on using lexical in-
833
formation for information retrieval systems. One
possibility we are considering is query expansion
by using predicate-argument structures of terms.
Suppose a user inputs two keywords, ?hockey?
and ?ticket? as a query. The conventional query
expansion technique expands these keywords to
a set of similar words based on an ontology. By
referring to predicate-argument structures in the
lexicon, we can derive actions and events as well
which take these words as arguments. In the above
example, by referring to the predicate-argument
structure of ?buy? or ?sell?, and knowing that
these verbs can take ?ticket? in their object role,
we can add ?buy? and ?sell? to the user?s query.
This new type of expansion requires rich lexical
information such as predicate argument structures,
and the information retrieval system would be a
good touchstone of the lexical information.
7 Concluding remarks
This paper outlined a new project for creating a
common standard for Asian language resources
in cooperation with other initiatives. We start
with three Asian languages, Chinese, Japanese
and Thai, on top of the existing framework which
was designed mainly for European languages.
We plan to distribute our draft to HLT soci-
eties of other Asian languages, requesting for
their feedback through various networks, such
as the Asian language resource committee net-
work under Asian Federation of Natural Language
Processing (AFNLP)4, and Asian Language Re-
source Network project5. We believe our ef-
forts contribute to international activities like ISO-
TC37/SC46 (Francopoulo et al, 2006) and to the
revision of the ISO Data Category Registry (ISO
12620), making it possible to come close to the
ideal international standard of language resources.
Acknowledgment
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004a. Content interoperability of lexical re-
sources, open issues and ?MILE? perspectives. In
4http://www.afnlp.org/
5http://www.language-resource.net/
6http://www.tc37sc4.org/
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC2004),
pages 131?134.
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004b. The MILE lexical classes: Data cat-
egories for content interoperability among lexicons.
In A Registry of Linguistic Data Categories within
an Integrated Language Resources Repository Area
? LREC2004 Satellite Workshop, page 8.
N. Calzolari, F. Bertagna, A. Lenci, and M. Mona-
chini. 2003. Standards and best practice for mul-
tilingual computational lexicons. MILE (the mul-
tilingual ISLE lexical entry). ISLE Deliverable
D2.2&3.2.
T. Charoenporn, V. Sornlertlamvanich, and H. Isahara.
1997. Building a large Thai text corpus ? part-
of-speech tagged corpus: ORCHID?. In Proceed-
ings of the Natural LanguageProcessing Pacific Rim
Symposium.
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006 (forthcoming).
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
C. Masolo, A. Borgo, S.; Gangemi, N. Guarino, and
A. Oltramari. 2003. Wonderweb deliverable d18
?ontology library (final)?. Technical report, Labo-
ratory for Applied Ontology, ISTC-CNR.
I. Niles and A Pease. 2001. Towards a standard upper
ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001).
V. Sornlertlamvanich, W. Pantachat, and S. Mek-
navin. 1994. Classifier assignment by corpus-
based approach. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING-94), pages 556?561.
M. Swadesh. 1952. Lexico-statistical dating of pre-
historic ethnic contacts: With special reference to
north American Indians and Eskimos. In Proceed-
ings of the American Philo-sophical Society, vol-
ume 96, pages 452?463.
H. Zhang, C. Huang, and S. Yu. 2004. Distributional
consistency: A general method for defining a core
lexicon. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC2004), pages 1119?1222.
834
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 69?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Rethinking Chinese Word Segmentation: Tokenization, Character
Classification, or Wordbreak Identification
Chu-Ren Huang
Institute of Linguistics
Academia Sinica,Taiwan
churen@gate.sinica.edu.tw
Petr S?imon
Institute of Linguistics
Academia Sinica,Taiwan
sim@klubko.net
Shu-Kai Hsieh
DoFLAL
NIU, Taiwan
shukai@gmail.com
Laurent Pre?vot
CLLE-ERSS, CNRS
Universite? de Toulouse, France
prevot@univ-tlse2.fr
Abstract
This paper addresses two remaining chal-
lenges in Chinese word segmentation. The
challenge in HLT is to find a robust seg-
mentation method that requires no prior lex-
ical knowledge and no extensive training to
adapt to new types of data. The challenge
in modelling human cognition and acqui-
sition it to segment words efficiently with-
out using knowledge of wordhood. We pro-
pose a radical method of word segmenta-
tion to meet both challenges. The most
critical concept that we introduce is that
Chinese word segmentation is the classifi-
cation of a string of character-boundaries
(CB?s) into either word-boundaries (WB?s)
and non-word-boundaries. In Chinese, CB?s
are delimited and distributed in between two
characters. Hence we can use the distri-
butional properties of CB among the back-
ground character strings to predict which
CB?s are WB?s.
1 Introduction: modeling and theoretical
challenges
The fact that word segmentation remains a main re-
search topic in the field of Chinese language pro-
cessing indicates that there maybe unresolved theo-
retical and processing issues. In terms of processing,
the fact is that none of exiting algorithms is robust
enough to reliably segment unfamiliar types of texts
before fine-tuning with massive training data. It is
true that performance of participating teams have
steadily improved since the first SigHAN Chinese
segmentation bakeoff (Sproat and Emerson, 2004).
Bakeoff 3 in 2006 produced best f-scores at 95%
and higher. However, these can only be achieved af-
ter training with the pre-segmented training dataset.
This is still very far away from real-world applica-
tion where any varieties of Chinese texts must be
successfully segmented without prior training for
HLT applications.
In terms of modeling, all exiting algorithms suffer
from the same dilemma. Word segmentation is sup-
posed to identify word boundaries in a running text,
and words defined by these boundaries are then com-
pared with the mental/electronic lexicon for POS
tagging and meaning assignments. All existing seg-
mentation algorithms, however, presuppose and/or
utilize a large lexical databases (e.g. (Chen and Liu,
1992) and many subsequent works), or uses the po-
sition of characters in a word as the basis for seg-
mentation (Xue, 2003).
In terms of processing model, this is a contradic-
tion since segmentation should be the pre-requisite
of dictionary lookup and should not presuppose lex-
ical information. In terms of cognitive modeling,
such as for acquisition, the model must be able to ac-
count for how words can be successfully segmented
and learned by a child/speaker without formal train-
ing or a priori knowledge of that word. All current
models assume comprehensive lexical knowledge.
2 Previous work
Tokenization model. The classical model, de-
scribed in (Chen and Liu, 1992) and still adopted in
many recent works, considers text segmentation as a
69
tokenization. Segmentation is typically divided into
two stages: dictionary lookup and out of vocabulary
(OOV) word identification. This approach requires
comparing and matching tens of thousands of dic-
tionary entries in addition to guessing thousands of
OOV words. That is, this is a 104x104 scale map-
ping problem with unavoidable data sparseness.
More precisely the task consist in finding
all sequences of characters Ci, . . . , Cn such that
[Ci, . . . Cn] either matches an entry in the lexicon
or is guessed to be so by an unknown word resolu-
tion algorithm. One typical kind of the complexity
this model faces is the overlapping ambiguity where
e.g. a string [Ci ? 1, Ci, Ci + 1] contains multiple
substrings, such as [Ci ? 1, Ci, ] and [Ci,Ci + 1],
which are entries in the dictionary. The degree of
such ambiguities is estimated to fall between 5% to
20% (Chiang et al, 1996; Meng and Ip, 1999).
2.1 Character classification model
A popular recent innovation addresses the scale
and sparseness problem by modeling segmentation
as character classification (Xue, 2003; Gao et al,
2004). This approach observes that by classifying
characters as word-initial, word-final, penultimate,
etc., word segmentation can be reduced to a simple
classification problem which involves about 6,000
characters and around 10 positional classes. Hence
the complexity is reduced and the data sparseness
problem resolved. It is not surprising then that the
character classification approach consistently yields
better results than the tokenization approach. This
approach, however, still leaves two fundamental
questions unanswered. In terms of modeling, us-
ing character classification to predict segmentation
not only increases the complexity but also necessar-
ily creates a lower ceiling of performance In terms
of language use, actual distribution of characters is
affected by various factors involving linguistic vari-
ation, such as topic, genre, region, etc. Hence the
robustness of the character classification approach
is restricted.
The character classification model typically clas-
sifies all characters present in a string into at least
three classes: word Initial, Middle or Final po-
sitions, with possible additional classification for
word-middle characters. Word boundaries are in-
ferred based on the character classes of ?Initial? or
?Final?.
This method typically yields better result than the
tokenization model. For instance, Huang and Zhao
(2006) claims to have a f-score of around 97% for
various SIGHAN bakeoff tasks.
3 A radical model
We propose a radical model that returns to the
core issue of word segmentation in Chinese. Cru-
cially, we no longer pre-suppose any lexical knowl-
edge. Any unsegmented text is viewed as a string
of character-breaks (CB?s) which are evenly dis-
tributed and delimited by characters. The characters
are not considered as components of words, instead,
they are contextual background providing informa-
tion about the likelihood of whether each CB is also
a wordbreak (WB). In other words, we model Chi-
nese word segmentation as wordbreak (WB) iden-
tification which takes all CB?s as candidates and
returns a subset which also serves as wordbreaks.
More crucially, this model can be trained efficiently
with a small corpus marked with wordbreaks and
does not require any lexical database.
3.1 General idea
Any Chinese text is envisioned as se-
quence of characters and character-boundaries
CB0C1CB1C2 . . . CBi?1CiCBi . . . CBn?1CnCBn The
segmentation task is reduced to finding all CBs
which are also wordbreaks WB.
3.2 Modeling character-based information
Since CBs are all the same and do not carry any
information, we have to rely on their distribution
among different characters to obtain useful infor-
mation for modeling. In a segmented corpus, each
WB can be differentiated from a non-WB CB by the
character string before and after it. We can assume
a reduced model where either one character imme-
diately before and after a CB is considered or two
characters (bigram). These options correspond to
consider (i) only word-initial and word-final posi-
tions (hereafter the 2-CB-model or 2CBM) or (ii) to
add second and penultimate positions (hereafter the
4-CB-model or 4CBM). All these positions are well-
attested as morphologically significant.
70
3.3 The nature of segmentation
It is important to note that in this approaches,
although characters are recognized, unlike (Xue,
2003) and Huang et al (2006), charactes simply
are in the background. That is, they are the neces-
sary delimiter, which allows us to look at the string
of CB?s and obtaining distributional information of
them.
4 Implementation and experiments
In this section we slightly change our notation to
allow for more precise explanation. As noted be-
fore, Chinese text can be formalized as a sequence
of characters and intervals as illustrated in we call
this representation an interval form.
c1I1c2I2 . . . cn?1In?1cn.
In such a representation, each interval Ik is either
classified as a plain character boundary (CB) or as
a word boundary (WB).
We represent the neighborhood of the character
ci as (ci?2, Ii?2, ci?1, Ii?1, ci, Ii, ci+1, Ii+1), which
we can be simplified as (I?2, I?1, ci, I+1, I+2) by
removing all the neighboring characters and retain-
ing only the intervals.
4.1 Data collection models
This section makes use of the notation introduced
above for presenting several models accounting for
character-interval class co-occurrence.
Word based model. In this model, statistical data
about word boundary frequencies for each character
is retrieved word-wise. For example, in the case of
a monosyllabic word only two word boundaries are
considered: one before and one after the character
that constitutes the monosyllabic word in question.
The method consists in mapping all the Chinese
characters available in the training corpus to a vector
of word boundary frequencies. These frequencies
are normalized by the total frequency of the char-
acter in a corpus and thus represent probability of a
word boundary occurring at a specified position with
regard to the character.
Let us consider for example, a tri-syllabic word
W = c1c2c3, that can be rewritten as the following
interval form as W I = IB?1c1I
N
1 c2I
N
2 c3I
B
3 .
In this interval form, each interval Ik is marked
as word boundary B or N for intervals within words.
When we consider a particular character c1 in W ,
there is a word boundary at index?1 and 3. We store
this information in a mapping c1 = {?1 : 1, 3 : 1}.
For each occurrence of this character in the corpus,
we modify the character vector accordingly, each
WB corresponding to an increment of the relevant
position in the vector. Every character in every word
of the corpus in processed in a similar way.
Obviously, each character yields only information
about positions of word boundaries of a word this
particular character belongs to. This means that the
index I?1 and I3 are not necessarily incremented
everytime (e.g. for monosyllabic and bi-syllabic
words)
Sliding window model. This model does not op-
erate on words, but within a window of a give size
(span) sliding through the corpus. We have exper-
imented this method with a window of size 4. Let
us consider a string, s = ?c1c2c3c4? which is not
necessarily a word and is rewritten into an interval
form as sI = ?c1I1c2I2c3I3c4I4?. We store the
co-occurrence character/word boundaries informa-
tion in a fixed size (span) vector.
For example, we collect the information for
character c3 and thus arrive at a vector c3 =
(I1, I2, I3, I4), where 1 is incremented at the respec-
tive position ifIk = WB, zero otherwise.
This model provides slightly different informa-
tion that the previous one. For example, if
a sequence of four characters is segmented as
c1IN1 c2I
B
2 c3I
B
3 c4I
B
4 (a sequence of one bi-syllabic
and two monosyllabic words), for c3 we would also
get probability of I4, i.e. an interval with index +2
. In other words, this model enables to learn WB
probability across words.
4.2 Training corpus
In the next step, we convert our training corpus into
a corpus of interval vectors of specified dimension.
Let?s assume we are using dimension span = 4.
Each value in such a vector represents the proba-
bility of this interval to be a word boundary. This
probability is assigned by character for each position
with regard to the interval. For example, we have
segmented corpus C = c1I1c2I2 . . . cn?1In?1cn,
where each Ik is labeled as B for word boundary
or N for non-boundary.
71
In the second step, we move our 4-sized window
through the corpus and for each interval we query
a character at the corresponding position from the
interval to retrieve the word boundary occurrence
probability. This procedure provides us with a vec-
tor of 4 probability values for each interval. Since
we are creating this training corpus from an already
segmented text, a class (B or N ) is assigned to each
interval.
The testing corpus (unsegmented) is encoded in a
similar way, but does not contain the class labels B
and N .
Finally, we automatically assign probability of 0.5
for unseen events.
4.3 Predicting word boundary with a classifier
The Sinica corpus contains 6820 types of characters
(including Chinese characters, numbers, punctua-
tion, Latin alphabet, etc.). When the Sinica corpus is
converted into our interval vector corpus, it provides
14.4 million labeled interval vectors. In this first
study we have implement a baseline model, without
any pre-processing of punctuation, numbers, names.
A decision tree classifier (Ruggieri, 2004) has
been adopted to overcome the non-linearity issue.
The classifier was trained on the whole Sinica cor-
pus, i.e. on 14.4 million interval vectors. Due to
space limit, actual bakeoff experiment result will be
reported in our poster presentation.
Our best results is based on the sliding window
model, which provides better results. It has to be
emphasized that the test corpora were not processed
in any way, i.e. our method is sufficiently robust to
account for a large number of ambiguities like nu-
merals, foreign words.
5 Conclusion
In this paper, we presented a radical and robust
model of Chinese segmentation which is supported
by initial experiment results. The model does not
pre-suppose any lexical information and it treats
character strings as context which provides infor-
mation on the possible classification of character-
breaks as word-breaks. We are confident that once
a standard model of pre-segmentation, using tex-
tual encoding information to identify WB?s which
involves non-Chinese characters, will enable us to
achieve even better results. In addition, we are look-
ing at other alternative formalisms and tools to im-
plement this model to achieve the optimal results.
Other possible extensions including experiments to
simulate acquisition of wordhood knowledge to pro-
vide support of cognitive modeling, similar to the
simulation work on categorization in Chinese by
(Redington et al, 1995). Last, but not the least,
we will explore the possibility of implementing a
sharable tool for robust segmentation for all Chinese
texts without training.
References
Academia Sinica Balanced Corpus of Modern Chinese.
http://www.sinica.edu.tw/SinicaCorpus/
Chen K.J and Liu S.H. 1992. Word Identification for
Mandarin Chinese sentences. Proceedings of the 14th
conference on Computational Linguistics, p.101-107,
France.
Chiang,T.-H., J.-S. Chang, M.-Y. Lin and K.-Y. Su. 1996.
Statistical Word Segmentation. In C.-R. Huang, K.-J.
Chen and B.K. T?sou (eds.): Journal of Chinese Lin-
guistics, Monograph Series, Number 9, Readings in
Chinese Natural Language Processing, pp. 147-173.
Gao, J. and A. Wu and Mu Li and C.-N.Huang and H. Li
and X. Xia and H. Qin. 2004. Adaptive Chinese Word
Segmentation. In Proceedings of ACL-2004.
Meng, H. and C. W. Ip. 1999. An Analytical Study of
Transformational Tagging for Chinese Text. In. Pro-
ceedings of ROCLING XII. 101-122. Taipei
Ruggieri S. 2004. YaDT: Yet another Decision Tree
builder. Proceedings of the 16th International Con-
ference on Tools with Artificial Intelligence (ICTAI
2004): 260-265. IEEE Press, November 2004.
Richard Sproat and Thomas Emerson. 2003. The
First International Chinese Word Segmentation Bake-
off. Proceedings of the Second SIGHAN Workshop on
Chinese Language Processing, Sapporo, Japan, July
2003.
Xue, N. 2003. Chinese Word Segmentation as Charac-
ter Tagging. Computational Linguistics and Chinese
Language Processing. 8(1): 29-48
Redington, M. and N. Chater and C. Huang and L. Chang
and K. Chen. 1995. The Universality of Simple Dis-
tributional Methods: Identifying Syntactic Categories
in Mandarin Chinese. Presented at the Proceedings of
the International Conference on Cognitive Science and
Natural Language Processing. Dublin City University.
72
91
92
93
94
95
96
97
98
99
100
101
102
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 19?27,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Wiktionary and NLP: Improving synonymy networks
Emmanuel Navarro
IRIT, CNRS &
Universit? de Toulouse
navarro@irit.fr
Franck Sajous
CLLE-ERSS, CNRS &
Universit? de Toulouse
sajous@univ-tlse2.fr
Bruno Gaume
CLLE-ERSS & IRIT, CNRS &
Universit? de Toulouse
gaume@univ-tlse2.fr
Laurent Pr?vot
LPL, CNRS &
Universit? de Provence
laurent.prevot@lpl-aix.fr
Hsieh ShuKai
English Department
NTNU, Taiwan
shukai@gmail.com
Kuo Tzu-Yi
Graduate Institute of Linguistics
NTU, Taiwan
tzuyikuo@ntu.edu.tw
Pierre Magistry
TIGP, CLCLP, Academia Sinica,
GIL, NTU, Taiwan
pmagistry@gmail.com
Huang Chu-Ren
Dept. of Chinese and Bilingual Studies
Hong Kong Poly U. , Hong Kong.
churenhuang@gmail.com
Abstract
Wiktionary, a satellite of the Wikipedia
initiative, can be seen as a potential re-
source for Natural Language Processing.
It requires however to be processed be-
fore being used efficiently as an NLP re-
source. After describing the relevant as-
pects of Wiktionary for our purposes, we
focus on its structural properties. Then,
we describe how we extracted synonymy
networks from this resource. We pro-
vide an in-depth study of these synonymy
networks and compare them to those ex-
tracted from traditional resources. Fi-
nally, we describe two methods for semi-
automatically improving this network by
adding missing relations: (i) using a kind
of semantic proximity measure; (ii) using
translation relations of Wiktionary itself.
Note: The experiments of this paper are based on Wik-
tionary?s dumps downloaded in year 2008. Differences may
be observed with the current versions available online.
1 Introduction
Reliable and comprehensive lexical resources con-
stitute a crucial prerequisite for various NLP tasks.
However their building cost keeps them rare. In
this context, the success of the Princeton Word-
Net (PWN) (Fellbaum, 1998) can be explained by
the quality of the resource but also by the lack of
serious competitors. Widening this observation to
more languages only makes this observation more
acute. In spite of various initiatives, costs make
resource development extremely slow or/and re-
sult in non freely accessible resources. Collabo-
rative resources might bring an attractive solution
to this difficult situation. Among them Wiktionary
seems to be the perfect resource for building com-
putational mono-lingual and multi-lingual lexica.
This paper focuses therefore on Wiktionary, how
to improve it, and on its exploitation for creating
resources.
In next section, we present some relevant infor-
mation about Wiktionary. Section 3 presents the
lexical graphs we are using and the way we build
them. Then we pay some attention to evaluation
(?4) before exploring some tracks of improvement
suggested by Wiktionary structure itself.
2 Wiktionary
As previously said, NLP suffers from a lack of
lexical resources, be it due to the low-quality or
non-existence of such resources, or to copyrights-
related problems. As an example, we consider
French language resources. Jacquin et al (2002)
highlighted the limitations and inconsistencies
from the French EuroWordnet. Later, Sagot and
Fi?er (2008) explained how they needed to re-
course to PWN, BalkaNet (Tufis, 2000) and other
resources (notably Wikipedia) to build WOLF, a
free French WordNet that is promising but still a
very preliminary resource. Some languages are
straight-off purely under-resourced.
The Web as Corpus initiative arose (Kilgarriff
and Grefenstette, 2003) as an attempt to design
tools and methodologies to use the web for over-
coming data sparseness (Keller and Lapata, 2002).
Nevertheless, this initiative raised non-trivial tech-
nical problems described in Baroni et al (2008).
Moreover, the web is not structured enough to eas-
ily and massively extract semantic relations.
In this context, Wiktionary could appear to be
a paradisiac playground for creating various lexi-
19
cal resources. We describe below the Wiktionary
resource and we explain the restrictions and prob-
lems we are facing when trying to exploit it. This
description may complete few earlier ones, for ex-
ample Zesch et al (2008a).
2.1 Collaborative editing
Wiktionary, the lexical companion to Wikipedia,
is a collaborative project to produce a free-content
multilingual dictionary.
1
As the other Wikipedia?s
satellite projects, the resource is not experts-led,
rather filled by any kind of users. The might-be
inaccuracy of the resulting resource has lengthily
been discussed and we will not debate it: see Giles
(2005) and Britannica (2006) for an illustration
of the controversy. Nevertheless, we think that
Wiktionary should be less subject (so far) than
Wikipedia to voluntary misleading content (be it
for ideological, commercial reasons, or alike).
2.2 Articles content
As one may expect, a Wiktionary article
2
may (not
systematically) give information on a word?s part
of speech, etymology, definitions, examples, pro-
nunciation, translations, synonyms/antonyms, hy-
pernyms/hyponyms, etc.
2.2.1 Multilingual aspects
Wiktionary?s multilingual organisation may be
surprising and not always meet one?s expectations
or intuitions. Wiktionaries exist in 172 languages,
but we can read on the English language main
page, ?1,248,097 entries with English definitions
from over 295 languages?. Indeed, a given wik-
tionary describes the words in its own language
but also foreign words. For example, the English
article moral includes the word in English (adjec-
tive and noun) and Spanish (adjective and noun)
but not in French. Another example, boucher,
which does not exist in English, is an article of the
English wiktionary, dedicated to the French noun
(a butcher) and French verb (to cork up).
A given wiktionary?s ?in other languages? left
menu?s links, point to articles in other wiktionar-
ies describing the word in the current language.
For example, the Fran?ais link in the dictionary
article of the English wiktionary points to an arti-
cle in the French one, describing the English word
dictionary.
1
http://en.wiktionary.org/
2
What article refers to is more fuzzy than classical entry
or acceptance means.
2.2.2 Layouts
In the following paragraph, we outline wik-
tionary?s general structure. We only consider
words in the wiktionary?s own language.
An entry consists of a graphical form and a cor-
responding article that is divided into the follow-
ing, possibly embedded, sections:
? etymology sections separate homonyms when
relevant;
? among an etymology section, different parts
of speech may occur;
? definitions and examples belong to a part of
speech section and may be subdivided into sub-
senses;
? translations, synonyms/antonyms and hy-
pernyms/hyponyms are linked to a given part of
speech, with or without subsenses distinctions.
In figure 1 is depicted an article?s layout example.
Figure 1: Layout of boot article (shortened)
About subsenses, they are identified with an in-
dex when first introduced but they may appear as
a plain text semantic feature (without index) when
used in relations (translations, synonyms, etc.). It
is therefore impossible to associate the relations
arguments to subsenses. Secondly, subsense index
appears only in the current word (the source of the
relation) and not in the target word?s article it is
linked to (see orange French N. and Adj., Jan. 10,
2008
3
).
A more serious issue appears when relations are
shared by several parts of speech sections. In Ital-
3
http://fr.wiktionary.org/w/index.php?
title=orange&oldid=2981313
20
ian, both synonyms and translations parts are com-
mon to all words categories (see for example car-
dinale N. and Adj., Apr. 26, 2009
4
).
2.3 Technical issues
As Wikipedia and the other Wikimedia Founda-
tion?s projects, the Wiktionary?s content manage-
ment system relies on the MediaWiki software
and on the wikitext. As stated in Wikipedia?s
MetaWiki article, ?no formal syntax has been de-
fined? for the MediaWiki and consequently it is
not possible to write a 100% reliable parser.
Unlike Wikipedia, no HTML dump is available
and one has to parse the Wikicode. Wikicode
is difficult to handle since wiki templates require
handwritten rules that need to be regularly up-
dated. Another difficulty is the language-specific
encoding of the information. Just to mention one,
the target language of a translation link is iden-
tified by a 2 or 3 letters ISO-639 code for most
languages. However in the Polish wiktionary the
complete name of the language name (angielski,
francuski, . . . ) is used.
2.4 Parsing and modeling
The (non-exhaustive) aforementioned list of diffi-
culties (see ?2.2.2 and ?2.3) leads to the following
consequences:
? Writing a parser for a given wiktionary is
possible only after an in-depth observation of its
source. Even an intensive work will not prevent
all errors as long as (i) no syntax-checking is made
when editing an article and (ii) flexibility with the
?tacitly agreed? layout conventions is preserved.
Better, flexibility is presented as a characteristic of
the framework:
?[. . . ] it is not a set of rigid rules. You may
experiment with deviations, but other editors
may find those deviations unacceptable, and
revert those changes. They have just as much
right to do that as you have to make them.
5
?
Moreover, a parser has to be updated every new
dump, as templates, layout conventions (and so
on) may change.
?Writing parsers for different languages is not a
simple adjustment, rather a complete overhaul.
? When extracting a network of semantic rela-
tions from a given wiktionary, some choices are
more driven by the wiktionary inner format than
scientific modelling choices. An illustration fol-
4
http://it.wiktionary.org/w/index.php?
title=cardinale&oldid=758205
5
http://en.wiktionary.org/wiki/WT:ELE
lows in ?3.2. When merging information extracted
from several languages, the homogenisation of the
data structure often leads to the choice of the poor-
est one, resulting in a loss of information.
2.5 The bigger the better?
Taking advantage of colleagues mastering various
languages, we studied the wiktionary of the fol-
lowing languages: French, English, German, Pol-
ish and Mandarin Chinese. A first remark con-
cerns the size of the resource. The official num-
ber of declared articles in a given wiktionary in-
cludes a great number of meta-articles which are
not word entries As of April 2009, the French wik-
tionary reaches the first rank
6
, before the English
one. This can be explained by the automated im-
port of public-domain dictionaries articles (Littr?
1863 and Dictionnaire de l?Acad?mie Fran?aise
1932-1935). Table 1 shows the ratio between the
total number of articles and the ?relevant? ones
(numbers based on year 2008 snapshots).
Total Meta
?
Other
??
Relevant
fr 728,266 25,244 369,948 337,074 46%
en 905,963 46,202 667,430 192,331 21%
de 88,912 7,235 49,672 32,005 36%
pl 110,369 4,975 95,241 10,153 9%
zh 131,752 8,195 112,520 1,037 0.7%
?
templates definitions, help pages, user talks, etc.
??
other languages, redirection links, etc.
Table 1: Ratio of ?relevant? articles in wiktionaries
By ?relevant?, we mean an article about a word
in the wiktionary?s own language (e.g. not an
article about a French word in the English Wik-
tionary). Among the ?relevant? articles, some
are empty and some do not contain any transla-
tion nor synonym link. Therefore, before deciding
to use Wiktionary, it is necessary to compare the
amount of extracted information contribution and
the amount of work required to obtain it .
3 Study of synonymy networks
In this section, we study synonymy networks built
from different resources. First, we introduce
some general properties of lexical networks (?3.1).
Then we explain how we build Wiktionary?s syn-
onymy network and how we analyse its proper-
ties. In ?3.3, we show how we build similar graphs
from traditional resources for evaluation purposes.
3.1 Structure of lexical networks
In the following sections, a graph G = (V,E)
is defined by a set V of n vertices and a set
E ? V
2
of m edges. In this paper, V is
6
http://meta.wikimedia.org/wiki/List_
of_Wiktionaries
21
a set of words and E is defined by a relation
E
R
7?? E : (w
1
, w
2
) ? E if and only if w
1
R
? w
2
.
Most of lexical networks, as networks extracted
from real world, are small worlds (SW) net-
works. Comparing structural characteristics of
wiktionary-based lexical networks to some stan-
dard resource should be done according to well-
known properties of SW networks (Watts and
Strogatz, 1998; Barabasi et al, 2000; Newman,
2003; Gaume et al, 2008). These properties are:
? Edge sparsity: SW are sparse in edges
m = O(n) or m = O(n log(n))
? Short paths: in SW, the average path length
(L)
7
is short. Generally there is at least one short
path between any two nodes.
? High clustering: in SW, the clustering coef-
ficient (C) that expresses the probability that two
distinct nodes adjacent to a given third one are ad-
jacent, is an order of magnitude higher than for
Erdos-Renyi (random) graphs: C
SW
 C
random
;
this indicates that the graph is locally dense, al-
though it is globally sparse.
?Heavy-tailed degree distribution: the distri-
bution of the vertices incidence degrees follows a
power law in a SW graph. The probability P (k)
that a given node has k neighbours decreases as a
power law, P (k) ? k
a
(a being a constant charac-
teristic of the graph). Random graphs conforms to
a Poisson Law.
3.2 Wiktionary?s network
Graph extraction Considering what said in
?2.2.2 and ?2.4, we made the following choices:
8
? Vertices: a vertex is built for each entry?s part
of speech.
? Parts of speech: when modeling the links
from X (X having for part of speech Pos
X
) to
one of its synonyms Y , we assume that Pos
Y
=
Pos
X
, thus building vertex Pos
Y
.Y.
? Subsenses: subsenses are flattened. First, the
subsenses are not always mentioned in the syn-
onyms section. Second, if we take into account
the subsenses, they only appear in the source of the
relation. For example, considering in figure 1 the
relation boot
syn
??? kick (both nouns), and given the
10 subsenses for boot and the 5 ones for kick, we
should build 15 vertices. And we should then add
7
Average length of the shortest path between any two
nodes.
8
These choices can clearly be discussed from a linguis-
tic point of view and judged to be biased. Nevertheless, we
adopted them as a first approximation to make the modelling
possible.
all the links between the mentioned boot?s sub-
senses and the 5 kick?s existing subsenses. This
would lead to a high number of edges, but the
graph would not be closer to the reality. The way
subsenses appear in Wiktionary are unpredictable.
"Subsenses" correspond sometimes to homonyms
or clear-cut senses of polysemous words, but can
also correspond to facets, word usage or regu-
lar polysemy. Moreover, some entries have no
subsenses distinction whereas it would be wor-
thy. More globally, the relevance of discrete word
senses has been seriously questioned, see (Victorri
and Fuchs, 1996) or (Kilgarriff, 1997) for very
convincing discussions. Two more practical rea-
sons led us to this choice. We want our method to
be reproducible for other languages and some wik-
tionaries do not include subsenses. At last, some
gold standard resources (eg. Dicosyn) have their
subsenses flattened too and we want to compare
the resources against each other.
? Edges: wiktionary?s synonymy links are ori-
ented but we made the graph symmetric. For ex-
ample, boot does not appear in kick?s synonyms.
Some words even appear as synonyms without be-
ing an entry of Wiktionary.
From the boot example (figure 1), we extract ver-
tices {N.boot, V.boot}, build {N.buskin,
N.kick, V.kick} and we add the follow-
ing (symmetrized) edges: N.boot?N.buskin,
N.boot?N.kick and V.boot?V.kick.
Graph properties By observing the table 2, we
can see that the graphs of synonyms extracted
from Wiktionary are all typical small worlds. In-
deed their l
lcc
remains short, their C
lcc
is always
greater or equal than 0.2 and their distribution
curves of the vertices incidence degree is very
close to a power law (a least-square method gives
always exponent a
lcc
? ?2.35 with a confidence
r
2
lcc
always greater than 0.89). It can also be seen
that the average incidence k
lcc
ranges from 2.32
to 3.32.
9
It means that no matter which language
9
It is noteworthy that the mean incidence of vertices is al-
most always the same (close to 2.8) no matter the graph size
is. If we assume that all wiktionary?s graphs grow in a similar
way but at different speed rates (after all it is the same frame-
work), graphs (at least their statistical properties) from differ-
ent languages can be seen as snapshots of the same graph at
different times. This would mean that the number of graphs
edges tends to grow proportionally with the number of ver-
tices. This fits with the dynamic properties of small worlds
(Steyvers and Tenenbaum, 2005). It means that for a wik-
tionary system, even with many contributions, graph density
is likely to remain constant and we will see that in compar-
ison to traditional lexical resources this density is quite low.
22
graph n m n
lcc
m
lcc
k
lcc
l
lcc
C
lcc
a
lcc
r
2
lcc
fr-N 18017 9650 3945 4690 2.38 10.18 0.2 -2.03 0.89
fr-A 5411 2516 1160 1499 2.58 8.86 0.23 -2.04 0.95
fr-V 3897 1792 886 1104 2.49 9.84 0.21 -1.65 0.91
en-N 22075 11545 3863 4817 2.49 9.7 0.24 -2.31 0.95
en-A 8437 4178 2486 3276 2.64 8.26 0.2 -2.35 0.95
en-V 6368 3274 2093 2665 2.55 8.33 0.2 -2.01 0.93
de-N 32824 26622 12955 18521 2.86 7.99 0.28 -2.16 0.93
de-A 5856 6591 3690 5911 3.2 6.78 0.24 -1.93 0.9
de-V 5469 7838 4574 7594 3.32 5.75 0.23 -1.92 0.9
pl-N 8941 4333 2575 3143 2.44 9.85 0.24 -2.31 0.95
pl-A 1449 731 449 523 2.33 7.79 0.21 -1.71 0.94
pl-V 1315 848 601 698 2.32 5.34 0.2 -1.61 0.92
n: number of vertices m: number of edges
k: avg. number of neighbours per vertex l: avg. path length between vertices
C: clustering rate a: power law exponent with r
2
confidence
_
lcc
: denotes on largest connected component
Table 2: Wiktionary synonymy graphs properties
or part of speech, m = O(n) as for most of SW
graphs (Newman, 2003; Gaume et al, 2008).
3.3 Building synonymy networks from
known standards
WordNet There are many possible ways for
building lexical networks from PWN. We tried
several methods but only two of them are worth
to be mentioned here. The graphs we built have
words as vertices, not synsets or senses. A first
straightforward method (method A) consists in
adding an edge between two vertices only if the
corresponding words appear as elements of the
same synset. This method produced many discon-
nected graphs of various sizes. Both the compu-
tational method we planned to use and our intu-
itions about such graphs were pointing towards a
bigger graph that would cover most of the lexical
network.
We therefore decided to exploit the hypernymy
relation. Traditional dictionaries indeed propose
hypernyms when one look for synonyms of very
specific terms, making hypernymy the closest re-
lation to synonymy at least from a lexicographic
viewpoint. However, adding all the hypernymy re-
lations resulted in a network extremely dense in
edges with some vertices having a high number of
neighbours. This was due to the tree-like organi-
sation of WordNet that gives a very special impor-
tance to higher nodes of the tree.
In the end we retained method B that consists in
adding edges in following cases:
? if two words belong to the same synset;
? if a word only appears in a synset that is a leaf
of the tree and contains only this word, then cre-
ate edges linking to words included in the hyper-
nym(s) synset.
We would like to study the evolution through time of wik-
tionaries, however this is outside the scope of this paper.
Therefore when a vertice w do not get any neigh-
bour according to method A, method B adds edges
linking w to words included in the hypernym(s)
synset of the synset {w}. We only added hyper-
nyms for the leaves of the tree in order to keep our
relations close to the synonymy idea. This idea has
already been exploited for some WordNet-based
semantic distances calculation taking into account
the depth of the relation in the tree (Leacock and
Chodorow, 1998).
Dicosyn graphs Dicosyn is a compilation of
synonym relations extracted from seven dictionar-
ies (Bailly, Benac, Du Chazaud, Guizot, Lafaye,
Larousse and Robert):
10
there is an edge r ? s if
and only if r and s have the same syntactic cate-
gory and at least one dictionary proposes s being
a synonym in the dictionary entry r. Then, each
of the three graphs (Nouns, Verbs, Adjectives) ob-
tained is made symmetric (dicosyn-fr-N, dicosyn-
fr-V and dicosyn-fr-A).
Properties of the graphs extracted Table 3
sums-up the structural properties of the synonyms
networks built from standard resources.
We can see that all the synonymy graphs ex-
tracted from PWN or Dicosyn are SW graphs.
Indeed their l
lcc
remains short, their C
lcc
is al-
ways greater or equal than 0.35 and their distri-
bution curves of the vertices incidence degree is
very close to a power law (a least-square method
gives always exponent a
lcc
near of ?2.30 with a
confidence r
2
lcc
always greater than 0.85). It can
also be observed that no matter the part of speech,
the average incidence of Dicosyn-based graphs is
always lower than WordNet ones.
10
Dicosyn has been first produced at ATILF, before being
corrected at CRISCO laboratory.
(http://elsap1.unicaen.fr/dicosyn.html)
23
graph n m n
lcc
m
lcc
k
lcc
l
lcc
C
lcc
a
lcc
r
2
lcc
pwn-en-N-A 117798 104929 12617 28608 4.53 9.89 0.76 -2.62 0.89
pwn-en-N-B 117798 168704 40359 95439 4.73 7.79 0.72 -2.41 0.91
pwn-en-A-A 21479 22164 4406 11276 5.12 9.08 0.75 -2.32 0.85
pwn-en-A-B 21479 46614 15945 43925 5.51 6.23 0.78 -2.09 0.9
pwn-en-V-A 11529 23019 6534 20806 6.37 5.93 0.7 -2.34 0.87
pwn-en-V-B 11529 40919 9674 39459 8.16 4.66 0.64 -2.06 0.91
dicosyn-fr-N 29372 100759 26143 98627 7.55 5.37 0.35 -2.17 0.92
dicosyn-fr-A 9452 42403 8451 41753 9.88 4.7 0.37 -1.92 0.92
dicosyn-fr-V 9147 51423 8993 51333 11.42 4.2 0.41 -1.88 0.91
Table 3: Gold standard?s synonymy graphs properties
4 Wiktionary graphs evaluation
Coverage and global SW analysis By compar-
ing tables 2 and 3, one can observe that:
? The lexical coverage of Wiktionary-based syn-
onyms graphs is always quantitatively lower than
those of standard resources although this may
change. For example, to horn (in PWN), absent
from Wiktionary in 2008, appeared in 2009. At
last, Wiktionary is more inclined to include some
class of words such as to poo (childish) or to
prefetch, to google (technical neologisms).
? The average number of synonyms for an en-
try of a Wiktionary-based resource is smaller than
those of standard resources. For example, com-
mon synonyms such as to act/to play appear in
PWN and not in Wiktionary. Nevertheless, some
other appear (rightly) in Wiktionary: to reduce/to
decrease, to cook/to microwave.
? The clustering rate of Wiktionary-based
graphs is always smaller than those of standard re-
sources. This is particularly the case for English.
However, this specificity might be due to differ-
ences between the resources themselves (Dicosyn
vs. PWN) rather than structural differences at the
linguistic level.
Evaluation of synonymy In order to evaluate
the quality of extracted synonymy graphs from
Wiktionary, we use recall and precision measure.
The objects we compare are not simple sets but
graphs (G = (V ;E)), thus we should compare
separately set of vertices (V ) and set of edges (E).
Vertices are words and edges are synonymy links.
Vertices evaluation leads to measure the resource
(a) English Wiktionary vs. Wordnet
Precision Recall
Nouns 14120/22075 = 0.64 14120/117798 = 0.12
Adj. 5874/8437 = 0.70 5874/21479 = 0.27
Verbs 5157/6368 = 0.81 5157/11529 = 0.45
(b) French Wiktionary vs. Dicosyn
Precision Recall
Nouns 10393/18017 = 0.58 10393/29372 = 0.35
Adj. 3076/5411 = 0.57 3076/9452 = 0.33
Verbs 2966/3897 = 0.76 2966/9147 = 0.32
Table 4: Wiktionary coverage
coverage whereas edges evaluation leads to mea-
sure the quality of the synonymy links in Wik-
tionary resource.
First of all, the global picture (table 4) shows
clearly that the lexical coverage is rather poor. A
lot of words included in standard resources are not
included yet in the corresponding wiktionary re-
sources. Overall the lexical coverage is always
lower than 50%. This has to be kept in mind while
looking at the evaluation of relations shown in ta-
ble 5. To compute the relations evaluation, each
resource has been first restricted to the links be-
tween words being present in each resource.
About PWN, since every link added with
method A will also be added with method B, the
precision of Wiktionary-based graphs synonyms
links will be always lower for "method A graphs"
than for "method B graphs". Precision is rather
good while recall is very low. That means that a
lot of synonymy links of the standard resources
are missing within Wiktionary. As for Dicosyn,
the picture is similar with even better precision but
very low recall.
5 Exploiting Wiktionary for improving
Wiktionary
As seen in section 4, Wiktionary-based resources
are very incomplete with regard to synonymy. We
propose two tasks for adding some of these links:
Task 1: Adding synonyms to Wiktionary by
taking into account its Small World characteristics
for proposing new synonyms.
(a) English wiktionary vs. Wordnet
Precision Recall
Nouns (A) 2503/6453 = 0.39 2503/11021 = 0.23
Nouns (B) 2763/6453 = 0.43 2763/18440 = 0.15
Adj. (A) 786/3139 = 0.25 786/5712 = 0.14
Adj. (B) 1314/3139 = 0.42 1314/12792 = 0.10
Verbs (A) 866/2667 = 0.32 866/10332 = 0.08
Verbs (B) 993/2667 = 0.37 993/18725 = 0.05
(b) French wiktionary vs. Dicosyn
Precision Recall
Nouns 3510/5075 = 0.69 3510/44501 = 0.08
Adj. 1300/1677 = 0.78 1300/17404 = 0.07
Verbs 899/1267 = 0.71 899/23968 = 0.04
Table 5: Wiktionary synonymy links precision & recall
24
Task 2: Adding synonyms to Wiktionary by
taking into account the translation relations.
We evaluate these two tasks against the bench-
marks presented in section 3.2.
5.1 Improving synonymy in Wiktionary by
exploiting its small world structure
We propose here to enrich synonymy links of Wik-
tionary by taking into account that lexical net-
works have a high clustering coefficient. Our hy-
pothesis is that missing links in Wiktionary should
be within clusters.
A high clustering coefficient means that two
words which are connected to a third one are likely
to be connected together. In other words neigh-
bours of my neighbours should also be in my
neighbourhood. We propose to reverse this prop-
erty to the following hypothesis: "neighbour of my
neighbours which are not in my neighbourhood
should be a good neighbour candidate". Thus the
first method we test consist simply in connecting
every vertex to neighbours of its neighbours. One
can repeat this operation until the expected num-
ber of edges is obtained.
11
Secondly we used the PROX approach pro-
posed by (Gaume et al, 2009). It is a stochastic
method designed for studying ?Hierarchical Small
Worlds?. Briefly put, for a given vertex u, one
computes for all other vertices v the probability
that a randomly wandering particle starting from
u stands in v after a fixed number of steps. Let
P (u, v) be this value. We propose to connect u
to the k first vertices ranked in descending order
with respect of P (u, v). We always choose k pro-
portionally to the original degree of u (number of
neighbours of u).
For a small number of steps (3 in our case) ran-
dom wanderings tend to be trapped into local clus-
ter structures. So a vertex v with a high P (u, v) is
likely to belong to the same cluster as u, which
means that a link u?v might be relevant.
Figure 2 shows precision, recall and f-score
evolution for French verbs graph when edges are
added using ?neighourhood? method (neigh), and
using ?Prox? method. Dashed line correspond to
the value theoretically obtained by choosing edges
at random. First, both methods are clearly more
efficient than a random addition, which is not sur-
prising but it seems to confirm our hypothesis that
missing edges are within clusters. Adding sharply
11
We repeat it only two times, otherwise the number of
added edges is too large.
0 2000 4000 6000 8000 10000 12000 140000.0
0.10.2
0.30.4
0.50.6
0.70.8
P
prox3neighrandom
0 2000 4000 6000 8000 10000 12000 140000.03
0.040.05
0.060.07
0.080.09
R
0 2000 4000 6000 8000 10000 12000 140000.05
0.060.07
0.080.09
0.100.11
0.120.13
F
fr.V
Figure 2: Precision, recall and F-score of French verbs
graph enlarged using only existing synonymy links
neighbours of neighbours seems to be as good as
adding edges ranked by Prox, anyway the rank
provided by Prox permits to add a given number
of edges. This ranking can also be useful to order
potential links if one think about a user validation
system. Synonyms added by Prox and absent from
gold standards are not necessarily false.
For example Prox proposes a relevant link ab-
solve/forgive, not included in PWN. Moreover,
many false positive are still interesting to consider
for improving the resource. For example, Prox
adds relations such as hypernyms (to uncover/to
peel) or inter-domain ?synonyms? (to skin/to peel).
This is due to high clustering (see ?3.1) and to
the fact that clusters in synonymy networks corre-
lates with language concepts (Gaume et al, 2008;
Duvignau and Gaume, 2008; Gaume et al, 2009;
Fellbaum, 1999).
Finally note that results are similar for other
parts of speech and other languages.
5.2 Using Wiktionary?s translation links to
improve its synonymy network
Assuming that two words sharing many transla-
tions in different languages are likely to be syn-
onymous, we propose to use Wiktionary?s transla-
tion links to enhance the synonymy network of a
given language.
In order to rank links to be potentially added,
we use a simple Jaccard measure: let T
w
be the set
of a word w?s translations, then for every couple
of words (w,w
?
) we have:
Jaccard(w,w
?
) =
|T
w
? T
w
?
|
|T
w
? T
w
?
|
We compute this measure for every possible pair
of words and then, starting from Wiktionary?s syn-
onymy graph, we incrementally add links accord-
ing to their Jaccard rank.
25
We notice first that most of synonymy links
added by this method were not initially included
in Wiktionary?s synonymy network. For exam-
ple, regarding English verbs, 95% of 2000 best
ranked proposed links are new. Hence this method
may be efficient to improve graph density. How-
ever one can wonder about the quality of the new
added links, so we discuss precision in the next
paragraph.
In figure 3 is depicted the evolution of precision,
recall and F-score for French verbs in the enlarged
graph in regard of the total number of edges. We
use Dicosyn graph as a gold standard. The dashed
line corresponds to theoretical scores one can ex-
pect by adding randomly chosen links.
First we notice that both precision and recall
are significantly higher than we can expect from
random addition. This confirms that words shar-
ing the same translations are good synonym candi-
dates. Added links seem to be particularly relevant
at the beginning for higher Jaccard scores. From
the first dot to the second one we add about 1000
edges (whereas the original graph contains 1792
edges) and the precision only decreases from 0.71
to 0.69.
The methods we proposed in this section are
quite simple and there is room for improvement.
First, both methods can be combined in order
to improve the resource using translation links
and then using clusters structure. One can also
think to the corollary task that would consists in
adding translation links between two languages
using synonymy links of others languages.
0 2000 4000 6000 8000 10000 120000.0
0.10.2
0.30.4
0.50.6
0.70.8
P
random
0 2000 4000 6000 8000 10000 120000.02
0.040.06
0.080.10
0.120.14
0.16
R
0 2000 4000 6000 8000 10000 120000.04
0.060.08
0.100.12
0.140.16
0.180.20
0.22
F
fr.V
Figure 3: Precision, recall and F-score of French verbs
graph enlarged using translation links
6 Conclusion and future work
This paper gave us the opportunity to share some
Wiktionary experience related lexical resources
building. We presented in addition two approaches
for improving these resources and their evaluation.
The first approach relies on the small world struc-
ture of synonymy networks. We postulated that
many missing links in Wiktionary should be added
among members of the same cluster. The second
approach assumes that two words sharing many
translations in different languages are likely to be
synonymous. The comparison with traditional re-
sources shows that our hypotheses are confirmed.
We now plan to combine both approaches.
The work presented in this paper combines a
NLP contribution involving data extraction and
rough processing of the data and a mathematical
contribution concerning graph-like resource. In
our viewpoint the second aspect of our work is
therefore complementary of other NLP contribu-
tions, like (Zesch et al, 2008b), involving more
sophisticated NLP processing of the resource.
Support for collaborative editing Our results
should be useful for setting up a more efficient
framework for Wiktionary collaborative editing.
We should be able to always propose a set of syn-
onymy relations that are likely to be. For exam-
ple, when a contributor creates or edits an arti-
cle, he may think about adding very few links but
might not bother providing an exhaustive list of
synonyms. Our tool can propose a list of potential
synonyms, ordered by relevancy. Each item of this
list would only need to be validated (or not).
Diachronic study An interesting topic for future
work is a "diachronic" study of the resource. It
is possible to access Wiktionary at several stages,
this can be used for studying how such resources
evolve. Grounded on this kind of study, one may
predict the evolution of newer wiktionaries and
foresee contributors? NLP needs. We would like
to set up a framework for everyone to test out new
methodologies for enriching and using Wiktionary
resources. Such observatory, would allow to fol-
low not only the evolution of Wiktionary but also
of Wiktionary-grounded resources, that will only
improve thanks to steady collaborative develop-
ment.
Invariants and variabality Wiktionary as a
massively mutiligual synonymy networks is an
extremely promising resource for studying the
(in)variability of semantic pairings such as
house/family, child/fruit, feel/know... (Sweetser,
1991; Gaume et al, 2009). A systematic study
within the semantic approximation framework
presented in the paper on Wiktionary data will be
carried on in the future.
26
References
A-L. Barabasi, R. Albert, H. Jeong, and G. Bianconi.
2000. Power-Law Distribution of the World Wide
Web. Science, 287. (in Technical Comments).
M. Baroni, F. Chantree, A. Kilgarriff, and S. Sharoff.
2008. Cleaneval: a Competition for Cleaning
Web Pages. In Proceedings of the Conference on
Language Resources and Evaluation (LREC), Mar-
rakech.
Encyclopaedia Britannica. 2006. Fatally flawed: re-
futing the recent study on encyclopedic accuracy by
the journal Nature.
K. Duvignau and B. Gaume. 2008. Between words
and world: Verbal "metaphor" as semantic or prag-
matic approximation? In Proceedings of Interna-
tional Conference "Language, Communication and
Cognition", Brighton.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
C. Fellbaum. 1999. La repr?sentation des verbes
dans le r?seau s?mantique Wordnet. Langages,
33(136):27?40.
B. Gaume, K. Duvignau, L. Pr?vot, and Y. Desalle.
2008. Toward a cognitive organization for electronic
dictionaries, the case for semantic proxemy. In Col-
ing 2008: Proceedings of the Workshop on Cogni-
tive Aspects of the Lexicon (COGALEX 2008), pages
86?93, Manchester.
B. Gaume, K. Duvignau, and M. Vanhove. 2009. Se-
mantic associations and confluences in paradigmatic
networks. In M. Vanhove, editor, From Polysemy to
Semantic Change: Towards a Typology of Lexical
Semantic Associations, pages 233?264. John Ben-
jamins Publishing.
J. Giles. 2005. Internet encyclopaedias go head to
head. Nature, 438:900?901.
C. Jacquin, E. Desmontils, and L. Monceaux. 2002.
French EuroWordNet Lexical Database Improve-
ments. In Proceedings of the Third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLING 2002), Mexico
City.
F. Keller and M. Lapata. 2002. Using the web to over-
come data sparseness. In Proceedings of EMNLP-
02, pages 230?237.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29:333?347.
A. Kilgarriff. 1997. I don?t believe in word senses.
Computers and the humanities, 31(2):91?113.
C. Leacock and M. Chodorow. 1998. Combining local
context and wordnet similarity for word sense iden-
tification. In C. Fellbaum, editor, WordNet: An elec-
tronic lexical database, pages 265?283. MIT Press.
M. Newman. 2003. The structure and function of com-
plex networks.
B. Sagot and D. Fi?er. 2008. Building a Free French
Wordnet from Multilingual Resources. In Proceed-
ings of OntoLex 2008, Marrackech.
M. Steyvers and J. B. Tenenbaum. 2005. The large-
scale structure of semantic networks: Statistical
analyses and a model of semantic growth. Cogni-
tive Science, 29:41?78.
E. Sweetser. 1991. From etymology to pragmatics.
Cambridge University Press.
D. Tufis. 2000. Balkanet design and development of a
multilingual balkan wordnet. Romanian Journal of
Information Science and Technology, 7(1-2).
B. Victorri and C. Fuchs. 1996. La polys?mie, con-
struction dynamique du sens. Herm?s.
D.J. Watts and S.H. Strogatz. 1998. Collective dynam-
ics of small-world networks. Nature, 393:440?442.
T. Zesch, C. M?ller, and I. Gurevych. 2008a. Extract-
ing Lexical Semantic Knowledge from Wikipedia
and Wiktionary. In Proceedings of the Conference
on Language Resources and Evaluation (LREC),
Marrakech.
T. Zesch, C. Muller, and I. Gurevych. 2008b. Using
wiktionary for computing semantic relatedness. In
Proceedings of 23rd AAAI Conference on Artificial
Intelligence.
27
Coling 2010: Poster Volume, pages 63?71,
Beijing, August 2010
A Formal Scheme for Multimodal Grammars
Philippe Blache & Laurent Pr?vot
LPL-CNRS, Universit? de Provence
blache@lpl-aix.fr
Abstract
We present in this paper a formal approach
for the representation of multimodal in-
formation. This approach, thanks to the
to use of typed feature structures and hy-
pergraphs, generalizes existing ones (typ-
ically annotation graphs) in several ways.
It first proposes an homogenous represen-
tation of different types of information
(nodes and relations) coming from differ-
ent domains (speech, gestures). Second,
it makes it possible to specify constraints
representing the interaction between the
different modalities, in the perspective of
developing multimodal grammars.
1 Introduction
Multimodality became in the last decade an im-
portant challenge for natural language processing.
Among the problems we are faced with in this do-
main, one important is the understanding of how
does the different modalities interact in order to
produce meaning. Addressing this question re-
quires to collect data (building corpora), to de-
scribe them (enriching corpora with annotations)
and to organize systematically this information
into a homogeneous framework in order to pro-
duce, ideally, multimodal grammars.
Many international projects address this ques-
tion from different perspectives: data represen-
tation and coding schemes (cf. ISLE (Dybk-
jaer, 2001), MUMIN (Allwood, 2005), etc.), cor-
pus annotation (cf. LUNA (Rodriguez, 2007) or
DIME (Pineda, 2000), etc.), annotation and edit-
ing tools (such as NITE NXT (Carletta, 2003),
Anvil (Kipp, 2001), Elan (Wittenburg, 2006),
Praat (Boersma, 2009), etc.).
We propose in this paper a generic approach
addressing both formal representation and con-
crete annotation of multimodal data, that relies on
typed-feature structure (TFS), used as a descrip-
tion language on graphs. This approach is generic
in the sense that it answers to different needs: it
provides at the same time a formalism directly us-
able for corpus annotation and a description lan-
guage making it possible to specify constraints
that constitute the core of a multimodal grammar.
In the first section, we motivate the use of TFS
and present how to concretely implement them for
multimodal annotation. We address in the second
section one of the most problematic question for
multimodal studies: how to represent and imple-
ment the relations between the different domains
and modalities (a simple answer in terms of time
alignment being not powerful enough). In the last
section, we describe how to make use of this rep-
resentation in order to specify multimodal gram-
mars.
2 Typed-feature structures modeling
Information representation is organized in two di-
mensions: type hierarchies and constituency re-
lations (typically, a prosodic unit is a set of syl-
lables, which in turn are sets of phonemes). The
former corresponds to an is-a relation, the latter to
a part-of one. For example intonational phrase is
a subtype of prosodic phrase, and phonemes are
constituents of syllables.
Such an organization is directly represented by
means of typed feature structures. They can be
considered as a formal annotation schema, used as
63
a preliminary step before the definition of the con-
crete coding scheme1. This step is necessary when
bringing together information (and experts) from
different fields: it constitutes a common represen-
tation framework, homogenizing information rep-
resentation. Moreover, it allows to clearly distin-
guish between knowledge representation and an-
notation. The coding scheme, at the annotation
level (labels, features, values), is deduced from
this formal level.
The remaining of the section illustrates how
to represent objects from different domains by
means of TFS. The Figure 1 presents the type hi-
erarchy and the constituency structure of objects
taken here as example.
2.1 Phonetics
The phoneme is used as primary data: this object
is at the lowest level of the constituent hierarchy
(most of the objects are set of phonemes). The fol-
lowing feature structure proposes a precise encod-
ing of the main properties describing a phoneme,
including articulatory gestures.
phon
?
???????????????????
SAMPA_LABEL sampa_unit
CAT
{
vowel, consonant
}
TYPE
{
occlusive, fricative, nasal, etc.
}
ARTICULATION
?
?????????
LIP
[
PROTUSION string
APERTURE aperture
]
TONGUE
?
??
TIP
[
LOCATION string
DEGREE string
]
BODY
[
LOCATION string
DEGREE string
]
?
??
VELUM aperture
GLOTTIS aperture
?
?????????
ROLE
[
EPENTHETIC boolean
LIAISON boolean
]
?
???????????????????
Phonemes being at the lowest level, they do not
have any constituents. They are not organized
into precise subtypes. The feature structure rep-
resent then the total information associated with
this type.
2.2 Prosody
As seen above, prosodic phrases are of two differ-
ent subtypes: ap (accentual phrases) and ip (into-
national phrases). The prosodic type hierarchy is
represented as follows:
1This approach has been first defined and experimented
in the XXXX project, not cited for anonymity reasons.
pros_phr


HHH
H
ap[
LABEL AP
CONSTS list(syl)
] ip?
???
LABEL IP
CONSTS list(ap)
CONTOUR
[
DIRECTION string
POSITION string
FUNCTION string
]
?
???
Accentual phrases have two appropriate fea-
tures: the label which is simply the name of the
corresponding type, and the list of constituents, in
this case a list of syllables. The objects of type ip
contain the list of its constituents (a set of aps) as
well as the description of its contour. A contour is
a prosodic event, situated at the end of the ip and
is usually associated to an ap.
The prosodic phrases are defined as set of syl-
lables. They are described by several appropriate
features: the syllable structure, its position in the
word, its possibility to be accented or prominent:
syl
?
??????
STRUCT syl_struct
POSITION
[
RANK
{
integer
}
SYL_NUMBER
{
integer
}
]
ACCENTUABLE boolean
PROMINENCE boolean
CONSTITUENTS list(const_syl)
?
??????
Syllable constituents (objects of type const_syl)
are described by two different features: the set of
phonemes (syllable constituents), and the type of
the constituent (onset, nucleus and coda). Note
that each syllable constituent can contain a set of
phonemes.
const_syl
[
PHON list(phon)
CONST_TYPE
{
onset, nucleus, coda
}
]
2.3 Disfluencies
We can distinguish two kinds of disfluencies: non
lexicalized (without any lexical material, such as
lengthening, silent pauses or filled pauses) and
lexicalized (non-voluntary break in the phrasal
flow, generating a word or a phrase fragment).
Lexicalized disfluencies have a particular organi-
zation with three subparts (or constituents):
? Reparandum: the word or phrase fragment,
in which the break occurs
? Break: a point or an interval that can eventu-
ally be filled by a fragment repetition, paren-
thetical elements, etc.
64
object



  
 
@@
@
PPPP
PPPP
PP
pros_phr
 Hip ap
phono
 HHsyllable phoneme
disfluence
 Hlex non-lex
gest
 HHHhand head ...
IP ::= AP?
AP ::= SYL+
SYL ::= CONST_SYL+
CONST_SYL ::= PHON+
DISF ::= REPRANDUM BREAK REPRANS
Figure 1: Type and constituent hierarchies
? Reparans: all that follow the break and
recovers the reparandum (in modifying or
completing it) or simply left it uncompleted.
The general disfluency type hierarchy, with the
appropriate features at each level is given in the
following figure:
disfluency


HHH
HH
lex[
REPRANDUM frag
BREAK_INT break
]


HHH
H
repaired[
TYPE rep
REPRANS change
] incomplete[
DIS_TYPE inc
]
non_lex
 HHfilled[
TYPE fill
] silent[
TYPE sil
]
2.4 Gestures
Besides verbal communication, gestures consti-
tute the main aspect of multimodality. In multi-
modal annotation, this is probably the most dif-
ficult and time-consuming task. Moreover, only
few works really focus on a precise description of
all the different domains of verbal and non verbal
modalities. The TFS-based approach proposed
here answers to the first need in such a perspec-
tive: a common representation framework.
We give in this section a brief illustration of
the representation of one gesture (hands). It re-
lies on adaptation of different proposals, espe-
cially (Kipp03) or MUMIN (Allwood, 2005), both
integrating McNeill?s gesture description (Mc-
Neill05).
The following structure encodes the description
of gesture phases, phrases (representing different
semiotic types), the hand shape as well as its ori-
entation, the gesture space, and the possible con-
tact with bodies or objects. A last feature also
describes the movement itself: trajectory, qual-
ity (fast, normal or slow) and amplitude (small,
medium and large).
hands_type
?
?????????????????????????
SYMMETRY boolean
PHASE Phase_Type
PHRASE
?
??????
SEMIOTIC Type Semiotic_Type
EMBLEM Emblem_Type
DEICTIC Deictic_Type
METAPHORIC Metaphoric_Type
PASSIVE_HAND boolean
ACTIVE_HAND boolean
ICONIC Iconic_Type
?
??????
HANDSHAPE
[SHAPE HandShape_Type
LAX boolean
]
GESTURESPACE Space_Type
ORIENTATION Orientation_Type
CONTACT
[ADAPTOR Adaptor_Type
CONTACT PART Contact_Type
]
MOVEMENT
[TRAJECTORY Trajectory_Type
AMPLITUDE Amplitude_Type
QUALITY quality_Type
]
?
?????????????????????????
2.5 Application
We have experimented this modeling in the com-
plete annotation of a multimodal corpus (see
(Blache, 2010)). In this project, a complete TFS
model has been first designed, covering all the
different domains (prosody, syntax, gestures, dis-
course, etc.). From this model, the annotations
have been created, leading to a 3-hours corpus of
narrative dialogs, fully transcribed. The corpus
is fully annotated for some domains (phonetics,
prosody and syntax) and partly for others (ges-
tures, discourse, disfluencies, specific phenom-
ena). The result is one of the first large annotated
multimodal corpus.
3 Graphs for Multimodal Annotation
Graphs are frequently used in the representation
of complex information, which is the case with
multimodality. As for linguistic annotation, one
of the most popular representations is Annotation
Graphs (Bird, 2001). They have been proposed
in particular in the perspective of anchoring dif-
ferent kinds of information in the same reference,
65
making it possible to align them2. In AGs, nodes
represent positions in the signal while edges bear
linguistic information. Two edges connecting the
same nodes are aligned: they specify different in-
formation on the same part of the input. Implic-
itly, this means that these edges bear different fea-
tures of the same object.
Such a representation constitutes the basis of
different approaches aiming at elaborating generic
annotation formats, for example LAF (and its ex-
tension GrAF (Ide, 2007)). In this proposal, edge
labels can be considered as nodes in order to build
higher level information. One can consider the re-
sult as an hypergraph, in which nodes can be sub-
graphs.
We propose in this section a more generalized
representation in which nodes are not positions in
the signal, but represent directly objects (or set of
objects). All nodes have here the same structure,
being them nodes or hypernodes. The main inter-
est of this proposal, on top of having an homoge-
neous representation, is the possibility to anchor
information in different references (temporal, spa-
tial or semantic).
3.1 Nodes
As seen above, multimodal annotation requires
the representation of different kinds of informa-
tion (speech signal, video input, word strings, im-
ages, etc.). The objects3 that will be used in the
description (or the annotation) of the input are of
different nature: temporal or spatial, concrete or
abstract, visual or acoustic, etc. A generic de-
scription requires first a unique way of locating
(or indexing) all objects, whatever their domain.
In this perspective, an index (in the HPSG sense)
can be specified, relying on different information:
? LOCATION: objects can in most of the cases
be localized in reference to a temporal or
a spatial situation. For example, phonemes
have a temporal reference into the speech
2Another important interest of AGs is that they can
constitute the basis for an exchange format, when think-
ing on annotation tools interoperability (a proposal is cur-
rently elaborated under auspices of the MITRE program, see
http://www.mitre.org/).
3We call object any annotation that participates to the de-
scription: phoneme, words, gestures, but also phrases, emo-
tions, etc.
signal, physical objects have spatial local-
ization that can be absolute (spatial coordi-
nates), or relative (with respect to other ob-
jects).
? REALIZATION: data can either refer to con-
crete or physical objects (phonemes, ges-
tures, referential elements, etc.) as well as
abstract ones (concepts, emotions, etc.).
? MEDIUM: specification of the different
modalities: acoustic, tactile and visual.4
? ACCESSIBILITY: some data are directly ac-
cessible from the signal or the discourse, they
have a physical existence or have already
been mentioned. In this case, they are said
to be ?given? (e.g. gestures, sounds, physical
objects). Some other kinds of data are de-
duced from the context, typically the abstract
ones. They are considered as ?accessible".
A generic structure node can be given, gather-
ing the index and the some other object properties.
node
?
?????????????
ID
DOMAIN
{
prosody, syntax, pragmatics, ...
}
INDEX
?
???????
LOCATION
{
TEMPORAL
[
START value
END value
]
SPATIAL coord
}
REALIZATION
{
concrete, abstract
}
MEDIUM
{
acoustic, tactile, visual
}
ACCESSIBILITY
{
given, accessible
}
?
???????
FEATURES object_type
?
?????????????
This structure relies on the different informa-
tion. Besides INDEX, some other features com-
plete the description:
? ID: using an absolute ID is useful in the per-
spective of graph representation, in which
nodes can encode any kind of information
(atomic or complex, including subgraphs).
? DOMAIN: specification of the domain to
which the information belongs. This feature
is useful in the specification of generic inter-
action constraints between domains.
? FEATURES: nodes have to bear specific lin-
guistic indications, describing object proper-
ties. This field encodes the type of informa-
tion presented in the first section.
4See the W3C EMMA recommenda-
tion (Extensible Multi-Modal Annotations,
http://www.w3.org/2002/mmi/.
66
The following examples illustrate the represen-
tation of atomic nodes from different domains: a
phoneme (node n1) and a gesture (node n2), that
are temporally anchored, and a physical object
(node n3) which is spatially situated. This last ob-
ject can be used as a referent, for example by a
deictic gesture.?
????????????
ID n1
DOMAIN phonetics
INDEX
?
???
TEMP
[
START 285
END 312
]
REALIZATION concrete
MEDIUM acoustic
ACCESSIBILITY given
?
???
FEATURES
phoneme
[
LABEL /u/
CAT vowel
...
]
?
????????????
?
?????????
ID n2
DOMAIN gesture
INDEX
[
TEMP
[
START 200
END 422
]
...
]
FEAT
hand
[
PHRASE deictic
ORIENTATION front
...
]
?
?????????
?
??????
ID n3
DOMAIN context
INDEX
[
LOC | SPATIAL <x=242, y=422, z=312 >
]
FEATURES
discourse_referent
[
SEM book?
COLOR red
...
]
?
??????
3.2 Relations
Linguistic information is usually defined in terms
of relations between (sets of) objects, which can
be atomic or complex. For example, a phrase is
defined by syntactic relations (government, agree-
ment, linearity, etc.) between its constituents. In
some cases, these relations can concern objects
from the same domain (e.g. syntax in the previous
example). In other cases, different domains can
be involved. For example, a long break (greater
than 200ms) usually precedes a left corner of a
new phrase.
The nature of the relation can also be differ-
ent according to the kind of information to be en-
coded. Many relations are binary and oriented
(precedence, dependency, etc.). Some others only
consists in gathering different objects. A con-
struction (in the sense of Construction Grammars,
see (Fillmore96)) is precisely that: a set of ob-
ject or properties that, put together, form a spe-
cific phenomenon. It is then useful in our rep-
resentation to distinguish between oriented rela-
tions and set relations. Oriented relations (for ex-
ample precedence) connect a source and a target,
that can be eventually formed with set of objects.
Set relations are used to gather a set of objects,
without orientation or order (e.g. the constituency
relation).
On top of this distinction, it is also necessary
to give an index to the relations, in order to make
their reference possible by other objects. As for
nodes, an index is used, even though its form is
simple and does not need a complex anchor. Fi-
nally, for the same reasons as for nodes, the speci-
fication of the domain is necessary. The following
feature structure gives a first view of this organi-
zation:
relation
?
?????
INDEX
DOMAIN
{
prosody, syntax, pragmatics, ...
}
REL_TYPE
?
?
?
?
?
ORIENTED_REL
[
SOURCE index
TARGET index
]
SET_REL
?
node list
?
?
?
?
?
?
?
?????
Besides these information, a relation descrip-
tion has to be completed with other information:
? TYPE: different types of relations can be
implemented in such representation, such
as dependency, precedence, constituency,
anaphore, etc.
? SCOPE: a relation can be specific to a con-
struction or at the opposite valid whatever
the context. For example, the precedence
relation [V ? Clit[nom]] is only valid
in the context of interrogative constructions
whereas the relation exluding the realization
of a backchannel5 after a connective is valid
whatever the context. We distinguish then
between local and global scopes.
? POLARITY: a relation can be negated, imple-
menting the impossibility of a relation in a
given context.
? CONSTRUCTION: in the case of a local rela-
tion, it is necessary to specify the construc-
tion to which it belongs.
? STRENGTH: some relation are mandatory,
some other optional. As for constraints, we
distinguish then between hard and soft rela-
tions, depending on their status.
Finally, a last property has to be precisely de-
fined: the synchronization between two objects
5A backchannel is a reaction, verbal or gestual, of the
adressee during a conversation.
67
coming from different domains (for example ges-
tures and words). In some cases, both objects
have to be strictly aligned, with same boundaries.
For example, a syllable has to be strictly aligned
with its set of phonemes: the left syllable bound-
ary (resp. the right) has to be the same as that
of the first syllable phoneme (resp. the last). In
other cases, the synchronization must not be strict.
For example, a deictic gesture is not necessarily
strictly aligned with a referential pronoun. In this
case, boundaries of both objects only have to be
roughly in the same part of the signal.
We propose the definition of alignment opera-
tors adapted from (Allen, 1985) as follows:
= same boundaries have to be equal
<? before b1 <? b2 means b1 value is lowerthan b2, with b2 ? b1 ? ?
>? after b1 >? b2 means that the boundaryb1 follows b2, with b1 ? b2 ? ?
?? almost boundaries are neighbors, withoutorder relation, with | b1 ? b2 |? ?
This set of operators allow to specify alignment
equations between different objects. The advan-
tage of this mechanism is that an equation system
can describe complex cases of synchronization.
For example, a construction can involve several
objects from different domains. Some of these ob-
jects can be strictly aligned, some others not.
The final TFS representation is as follows:
relation
?
??????????????????
INDEX
DOMAIN
{
prosody, syntax, pragmatics, ...
}
REL_TYPE
?
?
?
?
?
ORIENTED_REL
[
SOURCE index
TARGET index
]
SET_REL
?
node list
?
?
?
?
?
?
TYPE
{
dependency, precedence, etc.
}
SCOPE
{
global, local
}
POLARITY
{
plus, minus
}
CONSTRUCTION contruction_type
STRENGTH
{
hard, soft
}
ALIGNMENT
?
alignment_equations
?
?
??????????????????
The following feature structure shows an exam-
ple of a global relation indicating that a verbal nu-
cleus usually comes with a minor raising of the
intonation (only main features are indicated here).
This information is represented by an implica-
tion relation, which is oriented from the syntac-
tic category to the prosodic phenomenon. Align-
ment equations stipulate a strict synchronization
between object.
relation
?
???????
INDEX
REL_TYPE | ORIENTED_REL
[
SOURCE VN1
TARGET mr2
]
TYPE
{
implication
}
STRENGTH
{
soft
}
ALIGNMENT
?
lb1=lb2; rb1=rb2
?
?
???????
4 Representation with Hypergraphs
Nodes and relations can be combined and form
higher level nodes, representing constructions
which are a set of objects (the constituents) plus
a set of relations between them. Such nodes are
in fact hypernodes and bear two kinds of informa-
tion: the properties characterizing the object plus
a set of relations between the constituents (repre-
senting a subgraph). In the syntactic domain, for
example, they represent phrases, as follows:
?
???????????????????
DOMAIN syntax
INDEX | LOCATION | TEMPORAL
[
START 122
END 584
]
FEATURES
[
CAT VP
]
RELATIONS
?
???????????
???????????
?
??
INDEX r1
REL_TYPE | SET_REL
?
V, NP, Adv
?
TYPE constituency
STRENGTH hard
?
??;
?
???
INDEX r2
REL_TYPE | ORIENTED_REL
[
SOURCE NP
TARGET V
]
TYPE dependency
STRENGTH hard
?
???
?
???????????
???????????
?
???????????????????
In the same way, the interaction between dif-
ferent objects from different domains can involve
several relations. For example, a deictic con-
struction can be made of the conjunction of an
anaphoric pronoun, a deictic gesture and a physi-
cal object (for example a book on a shelf). Such
a construction can be described by the following
structure:
?
????????????????
INDEX | LOCATION | TEMPORAL
[
START 841
END 1520
]
FEATURES
[
SEM book?
]
RELATIONS
?
??????????
??????????
?
???
INDEX r3
SET_REL
?
Pro1, Dx_gest2, Ph_object3
?
TYPE constituency
ALIGNMENT
?
lb1 ??lb2; rb1 ??rb2
?
?
???;
?
??
INDEX r4
ORIENTED_REL
[
SOURCE Pro1
TARGET Ph_object3
]
TYPE reference
?
??
?
??????????
??????????
?
????????????????
This construction indicates some properties
(limited here to the semantic value) and two re-
68
lations between the different objects: one con-
stituency, indicating the different objects involved
in the construction and their (fuzzy) alignment
and a reference relation between the pronoun and
a physical object (here, a book).
This structure represents an hypergraph: it is
a graph connecting different nodes, each of them
being to its turn described by another graph, as
shown above. The main interest of such a repre-
sentation is its flexibility: all kinds of information
can be described, at any level. Graphs being less
constrained than trees, and edges (or relations) be-
ing typed, we can gather different levels, different
domains and different granularities. For example,
an agreement relation can be specified thanks to
the deictic construction, besides the constituency
one, making it possible to instanciate the agree-
ment value of the pronoun.
Note that hypergraphs are also investigated in
other knowledge representation, their properties
are well known (Hayes, 2004) and the implemen-
tation of specific hypergraphs as the one presented
here could be done in RDF graphs for example as
suggested in (Cassidy, 2010).
5 Constraints for Multimodal
Grammars
In the same way as typed feature structures can
implement constraints and constitute a description
language on linguistic structures (cf. HPSG, ),
the same approach can be generalized to multi-
modal information. SOme recent works have been
done in this direction (see (Alahverdzhieva, 2010;
?)). The representation we propose can implement
generic information about multimodal construc-
tions. We illustrate in the following this aspect
with two phenomena: backchannels and disloca-
tion.
Several studies on conversational data (see for
example (Bertrand09)) have described backchan-
nels (that can be vocal or gestual) and their con-
text. They have in particular underline some reg-
ularities on the left context:
? backchannels usually follow: major intona-
tive phrases (IP), flat contours, end of conver-
sational turn (i.e. saturated from a semantic,
syntactic and pragmatic point of view)
? backchannels never appear after connectives
These constraints can be implemented by
means of a feature structure (representing an hy-
pernode) with a set of precedence relations. The
different objects involved in the description of the
phenomenon (IP, flat contour, conversational turn,
connective) are indicated with an indexed ID, re-
ferring to their complete feature structure, not pre-
sented here.
?
???????????????????????????????????
ID 1
DOMAIN pragmatics
FEATURES
[
TYPE 2
]
RELATIONS
?
??????????????????????????????
??????????????????????????????
?
??
INDEX r5
SET_REL
?
IP 3 , FLAT_CONTOUR 4 ,
CONV_TURN 5 , CONNECTIVE 6
?
TYPE constituency
?
??;
?
??
INDEX r6
ORIENTED_REL
[
SOURCE
?
3 , 4 , 5
?
TARGET 1
]
TYPE precedence
?
??;
?
????
INDEX r7
ORIENTED_REL
[
SOURCE 6
TARGET 1
]
TYPE precedence
POLARITY minus
?
????
?
????
INDEX r8
ORIENTED_REL
[
SOURCE 3
TARGET vocal_ 2
]
TYPE precedence
STRENGTH hard
?
????
?
??????????????????????????????
??????????????????????????????
?
???????????????????????????????????
Figure 2: Backchannel Constraint
This structure (cf. Figure 2) represents a con-
straint that backchannels have to satisfy. The
first relation specifies the constituents and their
indexes, with which the different precedence con-
straints are represented. The relation r6 indicates
all kinds of object that should precede a backchan-
nel. This constraint subsumes the most specific
relation r8 stipulating that a vocal backchannel is
always preceded with an IP (this is a hard con-
straint). The relation r7 excludes the possibility
for a backchannel to be preceded with a connec-
tive.
The second example (cf. Figure 3) proposes a
constraint system describing dislocated structures.
We propose in this description to distinguish two
syntactic constituents that form the two parts of
the dislocation: the dislocated phrase (called S1)
and the sentence from which the phrase has been
69
extracted (called S2). Usually (even if not al-
ways), S2 contains a clitic referring to S1. We
note in the following this clitic with the notation
S2//Clit. For readability reasons, we only present
in this structure the relations.
This structure describes the case of a left dislo-
cation (with S1 preceding S2, the constraint being
hard). In such cases, S1 is usually realized with
a minor raising contour. The constraint r13 im-
plements the anaphoric relation between the clitic
and the dislocated element. Finally, the relation
r14 indicates an agreement relation between the
clitic and S1 and in particular the fact that the case
has to be the same for both objects.
?
????????????????????????????
DOMAIN syntax
RELATIONS
?
??????????????????????????
??????????????????????????
?
??
INDEX r11
SET_REL
?
S1 1 , S2 2 , MINOR_RAISING 3 ,
S2//CLIT 4
?
TYPE constituency
?
??;
?
??
INDEX r12
ORIENTED_REL
[
SOURCE 1
TARGET 2
]
TYPE precedence
?
??;
?
??
INDEX r13
ORIENTED_REL
[
SOURCE 1
TARGET 4
]
TYPE anaphor
?
??
?
??
INDEX r14
ORIENTED_REL
[
SOURCE 1 [CASE 3 ]
TARGET 4 [CASE 3 ]
]
TYPE agreement
?
??
?
??????????????????????????
??????????????????????????
?
????????????????????????????
Figure 3: Dislocation Constraint
6 Conclusion
Linguistic annotation in general, and multimodal-
ity in particular, requires high level annotation
schemes making it possible to represent in an ho-
mogeneous way information coming from the dif-
ferent domains and modalities involved in human
communication.
The approach presented in this paper general-
izes previous methods (in particular annotation
graphs) thanks to two proposals: first in providing
a way to index objects without strict order relation
between nodes and second in specifying a precise
and homogeneous representation of the objects
and their relations. This approach has been devel-
oped into a formal scheme, typed feature struc-
tures, in which all the different domains can be
represented, and making it possible to implement
directly hypergraphs. TFS and hypergraphs are
particularly well adapted for the specification of
interaction constraints, describing interaction re-
lations between modalities. Such constraints con-
stitute the core of the definition of future multi-
modal grammars.
From a practical point of view, the proposal
described in this paper is currently under exper-
imentation within the OTIM project (see (Blache,
2010)). An XML scheme has been automatically
generated starting from TFS formal scheme. The
existing multimodal annotations, created with ad
hoc annotation schemes, are to their turn automat-
ically translated following this format. We obtain
then, for the first time, a large annotated multi-
modal corpus, using an XML schema based on a
formal specification.
References
Alahverdzhieva, K. and A. Lascarides (2010)
?Analysing Language and Co-verbal Gesture and
Constraint-based Grammars?, in Proceedings of
the 17th International Conference on Head-Driven
Phase Structure Grammar.
Allen F. and P. J. Hayes (1985) ?A common-sense the-
ory of time?, in 9th International Joint Conference
on Artificial Intelligence.
Allwood J., L. Cerrato, L. Dybkjaer and al. (2005)
The MUMIN Multimodal Coding Scheme, NorFA
yearbook 2005
Bertrand R., M. Ader, P. Blache, G. Ferr?, R. Es-
pesser, S. Rauzy (2009) ?Repr?sentation, ?dition et
exploitation de donn?es multimodales : le cas des
backchannels du corpus CID?, in Cahiers de lin-
guistique fran?aise, 33:2.
Blache P., R. Bertrand, and G. Ferr? (2009) ?Creat-
ing and Exploiting Multimodal Annotated Corpora:
The ToMA Project?. in Kipp, Martin, Paggio and
Heylen (eds.) Multimodal Corpora: From Models
of Natural Interaction to Systems and Applications,
LNAI 5509, Springer.
Blache P. et al (2010) ?Multimodal Annotation of
Conversational Data?, in proceedings of LAW-IV -
The Linguistic Annotation Workshop
Bird S., Day D., Garofolo J., Henderson J., Laprun C.
& Liberman M. (2000) ?ATLAS : A Flexible and
Extensible Architecture for Linguistic Annotation",
in procs of LREC00
70
Bird S., M. Liberman (2001) ?A formal framework
for linguistic annotation" Speech Communication,
Elsevier
Boersma P. & D. Weenink (2009) Praat: doing pho-
netics by computer, http://www.praat.org/
Carletta, J., J. Kilgour, and T. O?Donnell (2003) ?The
NITE Object Model Library for Handling Struc-
tured Linguistic Annotation on Multimodal Data
Sets" in procs of the EACL Workshop on Language
Technology and the Semantic Web
Carpenter B. (1992) The Logic of Typed Feature
Structures. Cambridge University Press.
Cassidy S. (2010) An RDF Realisation of LAF in the
DADA Annotation Server. Proceedings of ISA-5,
Hong Kong, January 2010.
Dipper S., M. Goetze and S. Skopeteas (eds.) (2007)
Information Structure in Cross-Linguistic Corpora:
Annotation Guidelines for Phonology, Morphol-
ogy, Syntax, Semantics and Information Structure,
Working Papers of the SFB 632, 7:07
Dybkjaer L., S. Berman, M. Kipp, M. Wegener Olsen,
V. Pirrelli, N .Reithinger, C. Soria (2001) ?Sur-
vey of Existing Tools, Standards and User Needs for
Annotation of Natural Interaction and Multimodal
Data", ISLE Natural Interactivity and Multimodal-
ity Working Group Deliverable D11.1
Fillmore C. & P. Kay (1996) Construction Grammar,
Manuscript, University of California at Berkeley
Department of linguistics.
Gruenstein A., J. Niekrasz, and M. Purver. (2008)
?Meeting structure annotation: Annotations col-
lected with a general purpose toolkit?. In L. Dybk-
jaer and W. Minker, editors, Recent Trends in Dis-
course and Dialogue, Springer-Verlag.
Hayes J. and Gutierrez C. (2004) Bipartite graphs as
intermediate model for RDF. Proceedings of ISWC
2004, 3rd International Semantic Web Conference
(ISWC2004), Japan.
Ide N. and K. Suderman (2007) ?GrAF: A Graph-
based Format for Linguistic Annotations? in pro-
ceedings of the Linguistic Annotation Workshop
(LAW-07)
Ide N. and Suderman K. (2009) Bridging the Gaps:
Interoperability for GrAF, GATE, and UIMA. Pro-
ceedings of the Third Linguistic Annotation Work-
shop, held in conjunction with ACL 2009, Singa-
pore.
Kipp M. (2001) ?Anvil-a generic annotation tool for
multimodal dialogue" in procs of 7th European
Conference on Speech Communication and Tech-
nology
Kipp, M. (2003) Gesture Generation by Immitation:
From Human Behavior to Computer Character An-
imation, PhD Thesis, Saarland University.
Lascarides, A. and M. Stone (2009) ?A Formal Se-
mantic Analysis of Gesture?, in Journal of Seman-
tics, 26(4).
McNeill, D. (2005) Gesture and Thought, The Univer-
sity of Chicago Press.
Pineda, L., and G. Garza (2000) ?A Model for Mul-
timodal Reference Resolution", in Computational
Linguistics, Vol. 26 no. 2
Rodriguez K., Stefan, K. J., Dipper, S., Goetze,
M., Poesio, M., Riccardi, G., Raymond, C., Wis-
niewska, J. (2007) ?Standoff Coordination for
Multi-Tool Annotation in a Dialogue Corpus", in
procs of the Linguistic Annotation Workshop at the
ACL?07 (LAW-07)
Wegener Knudsen M.and al. (2002) Survey of Multi-
modal Coding Schemes and Best Practice, ISLE
Wittenburg, P.; Brugman, H.; Russel, A.; Klassmann,
A. and Sloetjes, H. (2006) ?ELAN: a Professional
Framework for Multimodality Research?. In pro-
ceedings of LREC 2006
71
Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 86?93
Manchester, August 2008
Toward a cognitive organization for electronic dictionaries, the case for
semantic proxemy
Bruno Gaume, Karine Duvignau, Laurent Pr?vot, Yann Desalle
Universit? de Toulouse, CNRS
{gaume,duvignau,prevot,desalle}@univ-tlse2.fr
Abstract
We compare a psycholinguistic approach
of mental lexicon organization with a com-
putational approach of implicit lexical or-
ganization as found in dictionaries. In this
work, we associate dictionaries with ?small
world? graphs. This multidisciplinary ap-
proach aims at showing that implicit struc-
ture of dictionaries, mathematically iden-
tified, fits the way young children catego-
rize. These dictionary graphs might there-
fore be considered as ?cognitive artifacts?.
This shows the importance of semantic
proximity both in cognitive and computa-
tional organization of verbs lexicon.
1 Introduction
According to (Dik, 1991) a linguistic theory
should be compatible with psycholinguistic re-
search on language acquisition, treatment, pro-
duction, interpretation and memorization of lin-
guistic expressions. We agree with this view and
postulate that elaborating electronic dictionaries
on the ground of a linguistic theory, satisfying
Dik?s principle, will confer them good ergonomics
that will increase their usability. Our approach is
to some extent comparable to WordNet initiative
(Fellbaum, 1998), in the sense that we are trying
to characterize speakers? mental lexicon.
In this paper, we focus on verb lexical orga-
nization through the examination of verbal pivot
metaphorical utterances (VPMU). Such utterances
involve an understudied structural aspect of the
lexicon: interdomain co-hyponymy (Duvignau,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
2002; Duvignau and Gaume, 2008). In this
context, we take semantic proximity as a cen-
tral principle for cognitive ergonomics influenc-
ing dynamic lexical acquisition and adult lexical
organization. VPMU generally consists in substi-
tuting elements from different semantic domains.
They are usually considered as deviants while they
might constitute a linguistic illustration of the cat-
egorial flexibility advocated in (Piaget, 1945; Ny,
1979; Hofstadter, 1995). They might therefore
reveal an early lexical structuring mode that may
form a ground for improving electronic dictionar-
ies.
This paper presents a mathematical method able
to discover the areas in which this structuring
mode appears in dictionaries. Our approach is to
take advantage of the mathematical structure of the
network generated by verb definitions. This struc-
ture has been mentioned in (Watts and Strogatz,
1998), studied for WordNet by (Sigman and Cec-
chi, 2002), refined in (Gaume et al, 2002) and ex-
ploited in the current proposal.
The paper is organized as follows. The next sec-
tion brings evidence of categorization by seman-
tic proximity from early lexicon acquisition ex-
periments. Section 3 presents the computational
model, hereafter ?proxemy?. Section 4 details our
work on lexical graphs while section 5 compares
the results of experimental studies with those of
the computational model.
2 Toward a categorization by semantic
proximity: evidences from early lexicon
acquisition
In order to show the importance of semantic ap-
proximation, we have chosen to support our claim
with productions observed at the crucial period
of lexical construction (between 2 and 4 years-of-
86
age) and to compare these with adult speakers that
have a stabilized lexicon.
2.1 Inter-domains vs. intra-domain semantic
approximations
Studies in this field are almost exclusively lim-
ited to nominal utterances. (Duvignau et al,
2005) established the existence of the production
of metaphor-like utterances with a verbal pivot in
2-4 years-old children and proposed to consider
them, at this stage of language development, as se-
mantic approximations and not as mistakes or true
metaphors. Duvignau distinguished two kinds of
semantic approximations: Inter-domains proxim-
ity and intra domain proximity between verbs (Du-
vignau, 2002).
- Inter-domains proximity / co-hyponymy be-
tween verbs : a ?linguistic approximation?
(1) Elle d?shabille l?orange (She undresses the
orange) [Age: 3 years] [movie: a lady peels
an orange]
In this category of approximation, the verb used
by the speaker constitutes a reference to a semantic
domain different from the one of element it is com-
bined to (?undress? / ?orange?). For this reason, the
approximate character of the verb is understand-
able independently of the context of the utterance:
detecting the approximation occurs at the linguis-
tic level. We call this type of production ?semantic
approximation?. They might constitute a metaphor
or an ?analogic surextention?.
When someone has a conventional verb in the
mental lexicon (?to peel?) and use a non conven-
tional but relevant verb like ?to undress the orange?
for the action [to peel the orange his verbal seman-
tic approximation constitutes a metaphor. On the
contrary when someone does not have a conven-
tional verb in the mental lexicon but manages to
use a non conventional but relevant verb in saying
?to undress? for this action, his verbal semantic ap-
proximation constitutes a ?surextension? but not an
error because of the lexical relation that links these
verbs. In fact, according to (Duvignau and Gaume,
2008) ?to undress? and ?to peel? are related by an
inter-domains synonymic relation.
- Intra-domain proximity / co-hyponymy be-
tween verbs: a ?pragmatic approximation? In
this category, illustrated by (2) the approximate
character of the verb comes only from a non-
correspondence between the verb used and the re-
ality it designates. This happens with utterances in
which the use of the verbal form does not create
any semantic tension within the utterance but des-
ignates a way of carrying out an activity that does
not correspond precisely to the action undertaken.
(2) Elle coupe l?orange (She cuts the or-
ange)[age: 3 years][movie: a lady peels an
orange]
We propose an experimental study of the produc-
tion of verbal semantic approximations like (2) or
(1) by way of a naming task of 17 action-movies
with young children (from 2 to 4 years old). We
compare their performances with adult?s ones.
2.2 Experimental Design
In order to elicit the production of semantic ap-
proximations we proposed to all our participants
an action-video naming task. The population sam-
ple consisted of:
? 54 non-disturbed children (2-4 years old),
monolingual in French
? 77 non-disturbed young adults (18-40 years
old), monolingual in French
The action movies sequences are coming from the
Approx protocol (Duvignau et al, 2005). The ma-
terial consists in 17 action-movies sequences de-
scribed in table 1.
The 17 action movies are presented in random
order to each participant. Instructions were given
at the time the action in the movie was completed
and its results were visible (e.g when the glass is
broken). At that moment a question was asked
to the participant:?What did the woman do? (just
now)?
2.3 Results
Each of the children produced between 2 and 5 ap-
proximations: ?Elle casse la tomate? -She breaks a
tomato ? [action = to squash], ?Elle ?pluche le bois?
- She peels the wood? [action = to strip the bark off
a log]. Globally, children produced semantic ap-
proximations for 34 % of the naming tasks, which
were distributed as follows: 24 % intra-domain
semantic approximations, 10 % inter-domains se-
mantic approximations. They produced them sig-
nificantly more frequent than adults : 5 % with
4% intra-domain semantic approximations and 1
% inter-domains semantic approximations.
87
Table 1: Approx 17 action movies
The student Test shows the difference between
children and adults in terms of production of se-
mantic approximation is very significant: here p <
0, 01 while p < 0, 05 is enough.
These results signal the importance of seman-
tic approximations and of semantic proximity be-
tween verbs in the cognitive organization of verbs
lexicon.
In the rest of the paper we present a computa-
tional model of semantic proximity and then com-
pare this model with the experimental data ob-
tained from the children.
3 Proxemy: a computational approach
A theory of language useful for computational
work must account for language statistical regu-
larities. Zipf law (Zipf, 1949) satisfy this obser-
vation but provides little insight on lexical struc-
tural organization. More recent graph theory stud-
ies (Ferrer-i-Cancho and Sole, 2001; Sigman and
Cecchi, 2002), capitalizing on results in other sci-
entific domains, provided interesting contributions
to the establishment of such a theory of language.
All structures discovered in this field research sat-
isfy the ?hierarchical small word? (HSW) defini-
tion (see section 3.1). Our approach takes place in
this general framework. Our specificities are:
? a new linguistic and psycholinguistic insight
that guides us and help us on our results vali-
dation;
? the kind of objects studied (dictionaries);
? our analysis of graph structure resulting in
a computational model of semantic proxim-
ity among vertices (here vertices are French
verbs).
The study by (Resnik and Diab, 2000) signaled
that although existing models for verb similarity
performed reasonably well against human judg-
ments, none managed to handle certain types of
metaphorical pairs such as to undress / to peel off
that are nonetheless declared to be rather similar
by speakers. We aim to develop a model address-
ing this issue.
3.1 Small World Networks
Networks corresponding to structures found in real
world (henceforth real world networks) are sparse:
in a graph with n nodes, the maximum num-
ber of possible edges is O(n
2
) while the number
of edges in real networks is generally inferior to
O(nlog(n)). Watts and Strogatz (Watts and Stro-
gatz, 1998) proposed two indicators to characterize
a large sparse graph G:
? L : the characteristic path length, i.e the mean
of the shortest path between two nodes of G
? C : the clustering coefficient, C ? [0, 1], it
measures the graph tendency to host zones
very dense in edges. (The more clustered the
graph is, the more the graph?s C approaches
1, whereas in random graphs C is very close
to 0).
In applying these criteria to different types of
graphs, Watts and Strogatz found that:
? real world networks have a tendency to have
a small L: generally there is at least one short
path between any two nodes ;
? real world networks have a tendency to have
a large C: this reflects a relative tendency for
two neighbors on the same node to be inter-
connected;
? random graphs have a small L: If someone
builds a graph randomly with a density of
edges comparable to real world networks, it
will obtain graphs with a small L;
? random graphs have a small C: They are not
composed of aggregates. In a random graph
88
there is no reason why neighbors of a same
node are more likely to be connected than any
two other nodes, hence their poor tendency to
form aggregates.
Watts and Strogatz proposed to call the graphs
having these two characteristics (a small L and
a large C) small worlds (SW). They recognized
these SW in all the real world networks they ob-
served, and therefore postulated for being a SW
was an universal property of real world networks.
A complete presentation of Small Words can be
found, for example, in (Newman, 2003).
More recent research has shown that most SW
also have a hierarchical structure (hereafter hier-
archical small worlds, HSW ). The distribution
of the vertices incidence degrees follows a power
law. The probability P (k) that a given node has k
neighbors decreases as a power law, P (k) ? k
??
,
where ? is a constant characteristic of the graph
(Barab?si and Albert, 1999), while random graphs
conforms to a Poisson Law.
In the next section, we present ?proxemy?, a se-
mantic proximity measure based on a distance we
define. A interesting particularity of this distance
is to calculate the distance between two vertices on
the ground of the complete graph, and not only on
their direct neighbors.
3.2 The mathematical model
PROX (PROXemy) is a stochastic method de-
signed for studying ?Hierarchical Small Worlds?.
1
This method takes graph as input and transform
them in a Markov chain whose states are graph ver-
tices. Metaphorically, energy particles wander ran-
domly from vertex to vertex through the edges of
the graph. It is their trajectory dynamics that give
us the structural properties of the graph.
PROX takes a graph in input and output a simi-
larity measure between the vertices of the graph.
Our problem is therefore the opposite than the
one of Pathfinder networks (PFNETs see (Schvan-
eveldt et al, 1988)). PFNETs take a full proximity
matrix in input and output a sparse graph. Their
goal is to minimize the number of edges required
in the sparse graph to be able to approximate the
full distance matrix corresponding to the initial full
proximity matrix.
1
In this paper we will use the term ?proxemy? to refer to the
obtained by PROX algorithm. It corresponds to some kind of
semantic proximity.
PROX build a similarity measure between the
vertices. The hypothesis is that areas having a
high density in edges (hereafter, these areas will
be called aggregates) correspond to closely related
verb meanings (in a graph of verbs).
Given a graph with n vertices, G = (V,E), we
will note [G] the matrix n ? n such that ?r, s ?
V , [G]
r,s
= 0 if {r, s} 6? E and 1 otherwise. [G]
is called the adjacency matrix of G.
Given G = (V,E) a reflexive graph with n ver-
tices. [
?
G] is a n ? n matrix defined by ?r, s ?
V , [
?
G]
r,s
=
[G]
r,s
?
x?V
{[G]
r,x
}
. [
?
G] is the Markovian
matrix of G.
[
?
G] is the n ? n matrix is a transition matrix of
the homogeneous Markov chain whose states are
the vertices of the graph such that the probability
of going from one vertex r ? V at an instant t onto
another s ? V at the instant t+ 1 is equal to:
? 0 if {r, s} 6? E (s is not neighbor of r)
? 1/D if {r, s} ? E and r has D neighbors (s
is a neighbor of r)
2
Given G = (V,E) a reflexive graph with n ver-
tices and [
?
G] its Markovian matrix, ?r, s ? V,?t ?
N
?
, PROX(G, t, r, s) = [
?
G
t
]
r,s
PROX(G, t, r, s) is therefore the probability
for a particle departing from r at the instant zero
to be on s at the instant t.
Therefore when, PROX(G, t, r, s) >
PROX(G, t, r, u), the particle has more proba-
bility to be, at instant t on s than on u and it is
graph structure that determine these probabilities.
For the rest of this paper we will set the value
of t to 4 since L is less than 4 in the kind of
graph we are concerned with. Therefore, we take
into account the global graph simply by calculating
PROX(G; 4; r; s).
Now we have defined our model we will present
lexical graphs on which we apply it.
4 Lexical graphs
Several types of lexical graphs can be built accord-
ing to the type of the semantic relation used for
defining the graph?s edges. The two principal types
of relations used are:
2
In the context of this presentation of the model we do not
consider weighted graphs. However when building the graphs
we do consider information, such as the position of the word
in the definition, for giving weight to the edges.s
89
? Syntagmatic relationships, like co-occurrence
relationships: they define edges between
nodes corresponding to words found near to
each other in a corpus.
? Paradigmatic relationships, like synonymy:
they define, on the ground of lexical databases
such as WordNet (Fellbaum, 1998), edges be-
tween nodes of words being in a synonymy
relationship in such resource.
Moreover, we are interested into less spe-
cific relations, called semantic proximity relations
or semantic relatedness, and which covers both
paradigmatic and syntagmatic dimensions.
4.1 Dictionary graphs
Meaning in dictionary definition is at least partially
brought by the relations they create between the
words constituting the entries. Our approach con-
sists in exploiting the small word properties of the
graphs corresponding to dictionaries. More pre-
cisely, we are taking advantage of our hypothe-
sis that aggregates correspond to areas of closely
related senses. We illustrate our approach on
two kinds of dictionary, two traditional dictionar-
ies, Le Grand Robert
3
and TLFi
4
, and an syn-
onym dictionary (Dicosyn) made of compilation
of synonym relations extracted from seven other
dictionaries (Bailly, Benac, Du Chazaud, Guizot,
Lafaye, Larousse et Robert).
5
We create a graph from a dictionary in the fol-
lowing way. The entries constituted the vertices.
Edges between two vertices A and B were added
if and only if B appears in A?s lemmatized defini-
tion
6
as illustrated in Figure 4.1
We proceed in this way for each entry and ob-
tained a graph of the dictionary. By extracting the
subgraph composed only of verbs, the ?neighbor-
hood? we get for the verb ??corcer? is illustrated
by Figure 4.1. Then we render the graph sym-
metric and reflexive. These modifications on the
graphs are allowed thanks to its paradigmatic na-
ture. Graphs created in this way are typical small
3
A significant amount of work has been done to encode
?Le Grand Robert in a graph.
4
We would like to thank ATILF for making the TLFi re-
source available to us.
5
Dicosyn has been first realized at ATILF (Analyse
et Traitement Informatique de la Langue Fran?aise),
before being corrected at CRISCO laboratory
(http://elsap1.unicaen.fr/dicosyn.html).
6
Lemmatization has been realized
with TreeTagger (http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger/).
Figure 1: Sub-graph near ??corcer (to bark ? a
tree?)? from Le Grand Robert
world network. For example, DicoSyn-Verb has
9043 vertices and 50948 edges, its L is 4,1694 and
its C 0,3186.
Figure 2: Sub-graph of the verbs near ??corcer?
from Robert
(Duvignau, 2002) has shown that co-hyponymy
verb lexical organization according fits with a
power law distribution of incidence degrees. In our
opinion, (i) the hierarchical organization of dictio-
naries is a consequence of the special role of the
hypernymy relation together with the polysemy of
some specific vertices; (ii) the strong C reflects
the role of interdomain co-hyponyny (Duvignau,
2002; Duvignau and Gaume, 2003). For example,
in French language, ?casser (to break)? appears in
many definitions: ??mietter (to crumble)?, ?frag-
menter (to fragment)?, ?d?t?riorer (to damage)?,
?r?voquer (to dismiss)?, ?abroger (to abrogate)?.
This results in a very high incidence for the vertex
?casser (to break)?. Moreover, many triangles ex-
ist ( {casser, ?mietter, fragmenter}, {casser, r?vo-
quer, abroger}...,) and they help to create aggre-
gates. These areas that are bringing co-hyponyms
closer in the resulting graph.
4.2 Disambiguization for creation dictionary
graphs
Word Sense Disambiguation is a general issue for
natural language processing that we need to ad-
dress when we build our graphs. We need to
disambiguate the verbs we found in the defini-
tion facing a similar problem as (Harabagiu et
90
al., 1999). For example, in French dictionary Le
Grand Robert, there are two distinct entries for the
verb ?causer?: to cause (3) and to chat (4).
(3) CAUSER-1: ?tre la cause de. (to be the
cause of)
(4) CAUSER-2: S?entretenir famili?rement
avec qqn. to chat with
Of course, the word ?causer? may appear in other
definitions like ?bavarder? (to chat) . Although
a French speaker knows that the ?causer? in (5)
refers to the definition (4) our system for building
the graph cannot disambiguate. The solution we
propose is to (i) first create a fictive vertex which
is not a dictionary entry and then (ii) adds two
edges {CAUSER, CAUSER-1} and {CAUSER,
CAUSER-2 }. When ?causer? is found in another
definition like (5), we add the edge { BAVARDER,
CAUSER } as illustrated in Figure (5).
(5) BAVARDER ?Parler beaucoup, longtemps
ou parler ensemble de choses superfi-
cielles. - Parler; babiller, bavasser (fam.),
cailleter, caqueter, causer, discourir, dis-
cuter, jaboter, jacasser, jaser, jaspiner (ar-
got), lantiponner (vx), papoter, potiner.
Bavarder avec qqn ... ?
Figure 3: Disambiguation: ?Causer?, fictive vertice
In Figure (5), many edges are hidden for clarity
reasons. Dashed edges ({Discuter, Causer2}) re-
sult from the fact ?Discuter? and ?Parler? are in the
definition of ?Causer-2?.
At this stage, we apply PROX to such graph as
the one Figure (5) in order to get a matrix [
?
G
4
]
as defined in section 3.2. [
?
G
4
]
bavarder,causer?1
<
[
?
G
4
]
bavarder,causer?2
. This comparison allows us
to disambiguate.
More generally, let suppose we found a word
with k entries in a definition, we will then have
S
1
, . . . , S
k
vertices corresponding to the entries a
fictive vertex S. In case there is an edge {A,S}
it is replaced by {A,S
i
} where S
i
is such that
[
?
G
4
]
A,S
i
= MAX
0<i?k
{
?
G
4
]
A,S
i
}. Then we re-
move all fictive vertices from the graph to get a
disambiguated graph.
We can then apply PROX a last time on the
disambiguated graph in order to get the closest
word of a word according to our proxemy mea-
sure. For example, the PROX-closest words of
?corcer (to bark ?a tree?), calculated with t =
6 are: 1 ECORCER (to bark), 2 D?POUILLER
(strip), 3 PELER (peel), 4 TONDRE (mow, shear),
5 ?TER (remove), 6 ?PLUCHER (peel, pare),
7 RASER (shave), 8 D?MUNIR (divest), 9 D?-
CORTIQUER (decorticate), 10 ?GORGER (slit
the throat of), 11 ?CORCHER (skin), 12 ?CALER
(husk), 13 VOLER (steal), 14 TAILLER (prune), 15
R?PER (grate), 16 PLUMER (pluck), 17 GRAT-
TER (scrape), 18 ENLEVER (remove), 19 D?-
SOSSER (bone), 20 D?POSS?DER (dispossess),
21 COUPER (cut), 22 BRETAUDER (shear slop-
pily), 23 INCISER (incise), 24 GEMMER (tap), 25
D?MASCLER (remove first layer of cork)
7
5 Proxemy and Experimental studies
Prox is a robust method: changing randomly a
few edges does not change significantly the results.
The repartition of aggregates is not strongly af-
fected by a random redistribution of some edges.
However the relevance of our proxemy approach
of lexical networks is tied to the linguistic rep-
resentativity of the networks we use. Therefore,
we tested the PROX model of four different dictio-
nary graphs and we compared them to the psycho-
linguistic experimental results presented in section
2. The graph we compared were:
1. Graph.TLFI.Verb, a graph built as explained
in 4.1 from TLFi
8
dictionary,
2. Graph.Robert.Verb, a graph built as explained
in 4.1 from Le Grand Robert dictionary,
3. Graph.DicoSyn.Verb, in which there is a edge
between two verbs if there are given as syn-
7
Proposing a translation for such fine grained and some-
times polysemous words is impossible since proposing the
translation include a certain form of disambiguisation as it is
suggested by the work of (Gale et al, 1992).
8
http://atilf.atilf.fr/tlf.htm
91
onyms by one of the synonym dictionary
composing DicoSyn
4. Graph.DicoSyn_20 built from
Graph.DicoSyn but in which 20% of the
edges are randomly removed and re-added.
For each of these graphs we looked at two vari-
ables to be related with the psycho-linguistics ex-
periments: the answers incidence and the proxim-
ity of answers to a ?reference verb?
Answers incidence We compare in the graph the
average incidence degree between adult (ID
adult
)
and children answers (ID
children
).
Table 2: Results for ?Answers incidence?
The proximity of answers to a ?reference verb?
Three linguist judges determined together for each
movie which was the most appropriate verb to de-
scribe the action performed in the movie (hereafter
R
i
is the reference verb for the movie M
i
). For
a given movie M
i
, an answer may therefore be
ranked according to its proxemy according to R
i
.
For a lexical graph G = (V,E) composed of n
words, and for a reference verb R
i
? V , one can
define rank
Ri
for ranking all the vertices of V in
decreasing order resulting from a PROX iteration
PROX(G, t,Ri, ?) on V (see section 3.2).
Table 3: Proximity between answers and reference
verb
Our first hypothesis was that ID
adult
<
ID
children
. According to the hypothesis children
would learn first words corresponding to high in-
cidence vertices. Then they would use them for
talking about an large lexical area (e.g ?casser? (to
break) is used by children while adults use a more
precise verb like ?d?chirer? (to tear) which has a
lower incidence in dictionary graphs).
Our second hypothesis was that the mean of the
rank of the children answers according to the ref-
erence verb is higher that the adult ones. When a
child is attempting to communicate an event (e.g
d?chirer un livre, to tear a book) for which he does
not have an already constituted verbal category, he
would do an analogy with a past event (e.g to break
a glass) and use this verb for describing the cur-
rent event (e.g casser un livre, to break a book).
The adult could use a number of more accurate
verbs but their proxemic rank, with regard to the
reference verb, is generally lower than the children
ones.
The table 2 shows the results concerning an-
swers incidence. Although some variability is ob-
served across the graphs, our first hypothesis is val-
idated for the 4 graphs. On the three first graphs
the average incidence of answers is roughly twice
as the adults one.
The table 3 illustrates the results concerning
proxemic rank of answers according to the ref-
erence verb. Again, in spite of some variability
across the graphs our second hypothesis is vali-
dated as well. Moreover, having in mind that the
graph has about 10 000 vertices, we observe that
although less close that adults answers, the chil-
dren answers remain relatively close to the refer-
ence verb according to our proxemic measure.
6 Conclusion
Our psycholinguistic approach allows us to estab-
lish that semantic proximity between verbs play a
fundamental role during the period of early lexi-
cal acquisition. We signaled the existence in the
organization of the lexicon of a relation of co-
hyponymy between verbs. Based on these first
observations we consider that productions based
on semantic proximity are particularly interesting:
they manifest the existence, at the surface level of
discourse, of a lexical relation of inter-domain ?se-
mantic proximity? between verbs not yet consid-
ered in linguistics.
Moreover we have seen that semantic approxi-
mations for verbs appear to fit the proximity values
calculated by PROX. On the ground of these first
results, we postulate that constructing electronic
dictionaries on the ground of linguistic theory of
lexical semantic organization that fits with early
lexicon acquisition as well with adult lexical orga-
nization will provide them interesting ergonomics
properties. This should increase their usability and
92
might be taken into account for normalizing elec-
tronic dictionaries.
For example, we are developing a ?proxemic
electronic dictionary? from TLFi. Such dictionar-
ies enable to find an uncommon but precise verb
like ?to bark? by using (i) a common verb like ?to
undress? which is related to ?to bark? by seman-
tic proximity and (ii) a word (e.g ?tree?) bringing a
relevant semantic domain. Moreover, in the def-
inition of ?to bark? one can find: ?tree?, ?grain?
?fruit? which are close from each other accord-
ing to PROX ran on nouns. Finally, when we
look for verbs that are close from both ?to un-
dress? and ?tree?, PROX provides the verbs: ?to
cut, to ring, to peel, to notch, to bark, to incise,...?
which constitute relevant verbs. Such a dictio-
nary can be useful for didactic studies where it can
complements approaches like and NLP for word
sense desambiguization (Gaume et al, 2004) or
de-metaphorization.
References
Barab?si, Albert-L?szl? and R?ka Albert. 1999. Emer-
gence of scaling in random networks. Science,
286:509?512, October.
Dik, S. 1991. Functional grammar. In Droste, F. and
J. Joseph., editors, Linguistic theory and grammati-
cal description. Amsterdam : Benjamins.
Duvignau, K. and B. Gaume. 2003. Linguistic, psy-
cholinguistic and computational approaches to the
lexicon: Contributions to early verb-learning. Jour-
nal of the European Society for the Study of Cogni-
tive Systems, 6(1).
Duvignau, Karine and Bruno Gaume. 2008. Between
words and world: Verbal ?metaphor? as semantic or
pragmatic approximation? In Proceedings of In-
ternational Conference ?Language, Communication
and Cognition?.
Duvignau, K., B. Gaume, and S. Kern. 2005. Seman-
tic approximations intraconcept
?
avs. interconcepts in
early verbal lexicon: flexibility against error. In Pro-
ceedings of ELA 2005, Emergence of language abil-
ities: ontogeny and phylogeny.
Duvignau, K. 2002. La m?taphore berceau et enfant
de la langue. Ph.D. thesis, Universit? Toulouse
?
U Le
mirail.
Fellbaum, C., editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Ferrer-i-Cancho, Ramon and Ricard V. Sole. 2001. The
small world of human language. Proceedings of The
Royal Society of London. Series B, Biological Sci-
ences, 268(1482):2261?2265, November.
Gale, W., K. Church, and D. Yarowsky. 1992. A
method for disambiguating word senses in a large
corpus. Computers and the humanities, 26(2):415?
439.
Gaume, B., K. Duvignau K., O. Gasquet O., and M-
D. Gineste. 2002. Forms of meaning, meaning of
forms. Journal of Experimental and Theoretical Ar-
tificial Intelligence, 14:61?74.
Gaume, B., N. Hathout, and P. Muller. 2004. D?sam-
biguisation par proximit? structurelle. In Proceed-
ings of TALN 2004.
Harabagiu, Sanda M., George A. Miller, and Dan I.
Moldovan. 1999. Wordnet 2 - a morphologically
and semantically enhanced resource. In SIGLEX
1999.
Hofstadter, D. 1995. Fluid concepts and creative
analogies. New York : Basic Books.
Newman, M. 2003. The structure and function of com-
plex networks.
Ny, J-F. Le. 1979. La s?mantique psychologique. PUF.
Piaget, J. 1945. La formation du symbole chez l?en-
fant,. Delachaux et Niestl?.
Resnik, P. and M. Diab. 2000. Measuring verb similar-
ity. In Proceedings of the 22nd Annual Meeting of
the Cognitive Science Society.
Schvaneveldt, R. W., D. W D.W Dearholt, and F.T
Durso. 1988. Graph theoric foundations of
pathfinder networks. Computers and Mathematics
with Applications, 15:337?445.
Sigman, M. and G.A. Cecchi. 2002. Global organiza-
tion of the wordnet lexicon. Proc. Natl. Acad. Sci.,
99(3):1741?1747.
Watts, D.J. and S.H. Strogatz. 1998. Collective dynam-
ics of small-world networks. Nature, 393:440?442.
Zipf, G. K. 1949. Human behavior and the principle
of least effort. Addison-Wesley.
93
Proceedings of the SIGDIAL 2013 Conference, pages 87?91,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
A quantitative view of feedback lexical markers in conversational French
Laurent Pre?vot Brigitte Bigi
Aix Marseille Universite? & CNRS
Laboratoire Parole et Langage
Aix-en-Provence (France)
firstname.lastname@lpl-aix.fr
Roxane Bertrand
Abstract
This paper presents a quantitative descrip-
tion of the lexical items used for linguis-
tic feedback in the Corpus of Interactional
Data (CID). The paper includes the raw
figures for feedback lexical item as well
as more detailed figures concerning inter-
individual variability. This effort is a first
step before a broader analysis including
more discourse situations and featuring
communicative function annotation.
Index Terms: Feedback, Backchannel, Corpus,
French Language
1 Objectives
Conversational feedback is mostly performed
through short utterances such as yeah, mh, okay
not produced by the main speaker but by one of
the other participants of a conversation. Such ut-
terances are among the most frequent in conver-
sational data (Stolcke et al, 2000). They also
have been described in psycho-linguistic models
of communication as a crucial communicative tool
for achieving coordination or alignment in dia-
logue (Clark, 1996).
The general objective of the project (ANR
CoFee: Conversational Feedback)1(Pre?vot and
Bertrand, 2012) in which this work takes place
is to propose a fine grained model of the
form/function relationship concerning feedback
behaviors in conversation. The present study is
first exploration aiming at knowing better the dis-
tribution of these items in one of our corpus. More
precisely, we would to verify how much inter-
individual variability we will face in further study
and whether we can identify a structure in this
variability (e.g speaker profiles). Second, we tried
1See the project website: http://cofee.hypotheses.org
to check there some strong trends in terms of evo-
lution of use of these items in the course of the
conversation. This later point was not conclusive
and is not developed in this paper.
Some data-intensive works exist for English
(Gravano et al, 2012), Japanese (Kamiya et al,
2010; Misu et al, 2011) or Swedish (Allwood et
al., 1992; Cerrato, 2007; Neiberg et al, 2013) but
not on many other languages such as French for
example. On French, the work of (Muller and
Pre?vot, 2003; Muller and Pre?vot, 2009) concerned
a smaller scale (A hour corpus) and very specific
task. (Bertrand et al, 2007) was focussed on the
feedback inviting cues and also on a smaller scale
(2 ? 15 minutes). They showed that particular
pitch contours and discursive markers play a sys-
tematic role as inviting-cues both for vocal and
gestural back-channels.
The paper is structured as follow. Section 2
presents the conversational corpus used for this
study, then section 3 presents how this corpus has
been processed. Section 4 is related to general fig-
ures for the feedback lexical items, followed by
more detailed information about inter-individual
variability (section 5).
2 The corpus
The Corpus of Interactional Data (CID) (Bertrand
et al, 2008; Blache et al, 2009)2 is an audio-video
recording of 8 hours of spontaneous French dia-
logues, 1 hour of recording per session. Each di-
alogue involved two participants of the same gen-
der. One of the following two topics of conver-
sation was suggested to participants: conflicts in
their professional environment or unusual situa-
tions in which participants may have found them-
selves. It features a nearly free conversational
style with only a single theme proposed to the par-
ticipants at the beginning of the experiment. This
2http://www.sldr.org/sldr000027/en
87
corpus is fully transcribed and forced-aligned at
phone level. Moreover, it has been annotated with
various linguistic information (Prosodic Phrasing,
Discourse units, Syntactic tags, ...) (Blache et al,
2010) which will allow us later to take advantage
of these levels of analysis.
Numerous studies have been carried out in pre-
pared speech. However, conversational speech
refers to a more informal activity, in which par-
ticipants have constantly to manage and negotiate
turn-taking, topic changes (among other things)
without any preparation. As a consequence, nu-
merous phenomena appear such as hesitations, re-
peats, backchannels, etc. Phonetic phenomena
such as non-standard elision, reduction phenom-
ena, truncated words, and more generally, non-
standard pronunciations are also very frequent.
All these phenomena can impact on the phoneti-
zation, then on alignment.
3 Processing the corpus
The transcription process is done following spe-
cific conventions derived from that of the GARS
(Blanche-Benveniste and Jeanjean, 1987). The
result is what we call an enriched orthographic
transcription (EOT), from which two derived tran-
scriptions are generated automatically : the stan-
dard orthographic transcription (the list of ortho-
graphic tokens) and a specific transcription from
which the phonetic tokens are obtained to be
used by the grapheme-phoneme converter. From
the phoneme sequence and the audio signal, the
aligner outputs for each phoneme its time localiza-
tion. This corpus has been processed with several
aligners. The first and main one (Brun et al, 2004)
is HMM-based, it uses a set of 10 macro-classes
of vowel (7 oral and 3 nasal), 2 semi-vowels and
15 consonants. Finally, from the time aligned
phoneme sequence plus the EOT, the orthographic
tokens is time-aligned.
The alignment for this paper is another ver-
sion that has been carried out using SPPAS3 (Bigi,
2012). SPPAS is a tool to produce automatic anno-
tations which include utterance, word, syllabic and
phonemic segmentations from a recorded speech
sound and its transcription.
Alignment of items of the list given in (1) were
then manually verified. Largest errors were cor-
rected to obtain reliable alignments.
DM prononciations are the standard ones except
3http://www.lpl-aix.fr/?bigi/sppas/
for a few cases. There are only two items with
non standard cases that are over 2 occurrences:
sampa: m.w.e.) that is an hybrid between mh
and ouais, and sampa w.a.l.a, a reduction of
v.w.a.l.a voila`.
The extraction themselves have been realized
by the authors with a Python script and all the
statistical analyses and plots have been produced
with R statistical analysis tool.
4 Descriptive statistics for the lexical
markers used in feedback
All the lexical items of the list given in (1) were
automatically extracted and categorized into two
categories: (i) Isolated items are items or sequence
of items surrounded by pauses of at least 200 ms
and not including any extra material than the items
of this list ; (ii) Initial items (or sequence items)
are located in front of some other items (but there
is no other material within the sequence). Most
of these items also occur in final or even sur-
rounded positions but we did not consider these
cases since they do are not clearly related to feed-
back. More precisely surrounded items are mostly
consisting in breaks of disfluencies or genuinely
integrated construction (e.g j?e?tais d?accord avec
lui / I agreed with him). Final ones can play a
role in eliciting feedback or sometimes bring some
kind of closure at the end of the utterance (what
has been described as Pivot Ending in (Gravano et
al., 2012)).
(1) ah (ah), bon (well), ben (well), euh (err,
uh), mh (mh), ouais (yeah), oui (yes), non
(no), d?accord (agreed), OK (okay), voila`
(that?s it, right)
Strictly speaking, the list (1) is not exhaustive.
However, other items are already in the thin part
of the distribution?s tail. Moreover, some of the
items such as euh / err are not necessarily related
to feedback. However, by crossing lexical values
with position we expect to get close enough the
full set of tokens involved in feedback. For exam-
ple, initial euh not followed by a feedback related
item will not be included in the final dataset. This
is also an objective of the present work to identify
these situations.
The different markers exhibit very different fig-
ures with regard to their location as it can be seen
in 1. While some are specialized in isolated feed-
back such as the continuer mh which is most of the
88
time backchanneled, others are found at the begin-
ning of utterances such as euh, ah. The later makes
sense since euh is also a filled pause.
ah 
ah_
oua
is 
ah_
oui ben bon 
dacc
ord euh mh 
mh_
mh non
 
oua
is 
oua
is_o
uais
 
oui voila
 0
100
200
300
400
500
initialisolated
Figure 1: Distribution of isolated vs. initial posi-
tion for the most frequent lexical items
In total 197 different combinations of the ba-
sic markers were identified. The most frequent
are the simple repetitions of items such as ouais
(up to nine times) or mh. There are also more
complex structures as exhibited in (2) that seem
to mix two kinds of items: base ones and mod-
ifiers (ah, euh). The base ones seem by default
to carry general purpose communicative functions
as described in (Bunt, 2009; Bunt, 2012) while the
others can also be produced alone but are generally
dealing specific dimension such as turn-taking, at-
titude expression or time management.
(2) a. ah ouais d?accord ok (ah yeah right
okay)
b. voila` oui non (that?s it yes no)
With regard to duration, the data is rather messy
concerning the very long items. There are extreme
lengthening on these units. Aside that and the filler
uh that exhibit a wide spread, the other items are
not produced with huge variations. Monosyllabic
remain well centered around 150-250 ms while di-
syllabic and repeated items are distributed in the
250-500 ms range. This is important for our next
step in which automatic acoustic analysis of these
items will be performed.
l
l
ll
l
l
lll
l
l
ll
l
l
l
l
l l
l
l l
l
l
l
l
ll
l
ll
l
l
l
ll
l
l
l
llll
l
l
l
ll lll
l
ll
l
l
l
l
ll ll
llll
l
ll
l
l
ll
l
l
ll
l
l
ll
l
l
l l
ah 
ah_
oua
is 
ah_
oui ben bon 
dacc
ord euh mh 
mh_
mh non
 
oua
is 
oua
is_o
uais
 
oui voila
 
0.0
0.5
1.0
1.5
2.0
Figure 2: Duration (in seconds) of each lexical
type
5 Inter-individual variability
Inter-individual variation is a big issue on the way
to the generalizability. We would like to under-
stand some of the feedback producing profiles.
Our intuitions coming from familiarity of the data
is that there are strong variation but they corre-
spond to a few different speaking styles. In fu-
ture work, we would like to see in a second step
whether we can identify and characterize these
styles.
l
150
200
250
300
350
400
450
Figure 3: Number of feedback items per speaker
Figure 3 illustrates the total figures of feedback
per speaker. As expected variation is huge, from
132 to 425 but with in fact with few outliers with
a nice batch of speaker in the 200 ? 300 range.
The wider spread of the distribution in the high
range comes from two factors. First of all, there
are participants producing a high quantity of feed-
89
back items. They produce a massive amount of
light backchannels (mh, ouais) compared to low-
quantity feedback producers. The later also pro-
duce feedback during the long pauses of the main
speaker but they produce much less overlapping
backchannels. This should be double checked
with a specific measure (adding overlapping as a
factor). However, a second effect seems important
for at least one speaker (the outlier): the amount
to time holding the floor. In fact the speaker pro-
ducing the most feedback did so because she was
rarely the main speaker.
In order to get a global idea of the different uses
of these items, Figure 5 represents the proportion
of each item per speaker. As expected, the varia-
tion is important but one can spot some tendencies.
For examples for the most frequent items, the rank
seems to preserved across speakers.
CID_
AB 
CID_
AC 
CID_
AG 
CID_
AP 
CID_
BX 
CID_
CM 
CID_
EB 
CID_
IM 
CID_
LJ 
CID_
LL 
CID_
MB 
CID_
MG 
CID_
ML 
CID_
NH 
CID_
SR 
CID_
YM 
voilaouiouais_ouaisouaisnonmh_mhmheuhdaccordbonbenah_ouiah_ouaisah
0.0
0.2
0.4
0.6
0.8
1.0
Figure 4: Distribution of the lexical items
Based on their feedback profile (proportion of
use of each items as illustrated in Figure 5), we
attempted to cluster the participants as showed in
5. While the lower parts of the dendrogram are
hard to interpret the higher part matches well with
the impression acquired by listening to the corpus
(no backchannels and rather formal feedback vs.
lots of backchannels and very colloquial style).
6 Current and Future Work
About this first batch of analyses, we will com-
plete the analysis of the evolution during the con-
versation. More precisely, we will go at the
individual level looking for time-based changes
AB
AG
IM ML
YM
BX
AP EB LJ NH
CM
SR
AC MB
LL MG0.1
0.2
0.3
0.4
0.5
Dendrogram of  diana(x = distSpForm)
diana (*, "NA")distSpForm
Heig
ht
Figure 5: Dendrogram of the participants cluster
based on their feedback profile
in their profiles as well as looking at the pairs
for tracking potential convergence effect either in
terms of distribution of lexical marker types or in
their duration.
In parallel to this work, we are launching in-
dependent prosodic and kinesic analyses of the
forms, as well as a discourse analysis of the func-
tions. Moreover the work is being extended by
adding two corpora in the study in order to allow
for a better situation generalisability: A French
MapTask; and a third corpus consisting in a less
cooperative situation. The idea is later to bring
together the observations from the different levels
in order to propose a multidimensional model for
feedback in French dialogues.
Those are steps toward more extensive studies
in the spirit of (Gravano et al, 2012) or (Neiberg
et al, 2013) on French language and in which we
hope to address more directly the issue of dis-
course situation generalisability.
Acknowledgment
This work has been realized with the support of
the ANR (Grant Number: ANR-12-JCJC-JSH2-
006-01) and exploited aligned data produced in
the framework of the ANR project (Grant Number
ANR-08-BLAN-0239). We would like to thank all
the members of these two projects.
90
References
J. Allwood, J. Nivre, and E. Ahlsen. 1992. On the se-
mantics and pragmatics of linguistic feedback. Jour-
nal of Semantics, 9.
R. Bertrand, G. Ferre?, P. Blache, R. Espesser, and
S. Rauzy. 2007. Backchannels revisited from a mul-
timodal perspective. In Proceedings of Auditory-
visual Speech Processing. Citeseer.
R. Bertrand, P. Blache, R. Espesser, G. Ferre?, C. Me-
unier, B. Priego-Valverde, and S. Rauzy. 2008.
Le cid-corpus of interactional data-annotation et ex-
ploitation multimodale de parole conversationnelle.
Traitement Automatique des Langues, 49(3):1?30.
B. Bigi. 2012. SPPAS: a tool for the phonetic segmen-
tation of speech. In Language Resource and Evalu-
ation Conference, pages 1748?1755, ISBN 978?2?
9517408?7?7, Istanbul (Turkey).
P. Blache, R. Bertrand, and G. Ferre?. 2009. Creating
and exploiting multimodal annotated corpora: the
toma project. Multimodal corpora, pages 38?53.
P. Blache, R. Bertrand, B. Bigi, E. Bruno, E. Cela,
R. Espesser, G. Ferre?, M. Guardiola, D. Hirst,
E. Muriasco, J.-C. Martin, C. Meunier, M.-A. Morel,
I. Nesterenko, P. Nocera, B. Palaud, L. Pre?vot,
B. Priego-Valverde, J. Seinturier, N. Tan, M. Tel-
lier, and S. Rauzy. 2010. Multimodal annotation
of conversational data. In Proceedings of Linguistic
Annotation Workshop.
C. Blanche-Benveniste and C. Jeanjean. 1987. Le
franc?ais parle?. Edition et transcription. Paris, Di-
dier Erudition.
A. Brun, C. Cerisara, D. Fohr, I. Illina, D. Langlois,
O. Mella, and K. Sma??li. 2004. Ants: le syste`me de
transcription automatique du loria. In Actes des XXV
Journe?es d?Etudes sur la Parole, Fe`s, Morocco.
H. Bunt. 2009. Multifunctionality and multidimen-
sional dialogue act annotation. In Proceedings of
DiaHolmia, SEMDIAL.
H. Bunt. 2012. The semantics of feedback. In
16th Workshop on the Semantics and Pragmatics of
Dialogue (SEMDIAL 2012), pages 118?127, Paris
(France).
L. Cerrato. 2007. Investigating Communicative Feed-
back Phenomena across Languages and Modalities.
Ph.D. thesis.
H.H. Clark. 1996. Using language. Cambridge: Cam-
bridge University Press.
A. Gravano, J. Hirschberg, and S?. Ben?us?. 2012. Af-
firmative cue words in task-oriented dialogue. Com-
putational Linguistics, 38(1):1?39.
Y. Kamiya, T. Ohno, and S. Matsubara. 2010. Coher-
ent back-channel feedback tagging of in-car spoken
dialogue corpus. In Proceedings of the 11th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 205?208. Association for Com-
putational Linguistics.
T. Misu, E. Mizukami, Y. Shiga, S. Kawamoto,
H. Kawai, and S. Nakamura. 2011. Toward con-
struction of spoken dialogue system that evokes
users? spontaneous backchannels. In Proceedings
of the SIGDIAL 2011 Conference, pages 259?265.
Association for Computational Linguistics.
P. Muller and L. Pre?vot. 2003. An empirical study
of acknowledgement structures. In Proceedings od
Diabruck, 7th workshop on semantics and pragmat-
ics of dialogue, Saarbrucken.
P. Muller and L. Pre?vot. 2009. Grounding information
in route explanation dialogues. In Spatial Language
and Dialogue. Oxford University Press.
D. Neiberg, G. Salvi, and J. Gustafson. 2013. Semi-
supervised methods for exploring the acoustics of
simple productive feedback. Speech Communica-
tion.
L. Pre?vot and R. Bertrand. 2012. Cofee-toward a mul-
tidimensional analysis of conversational feedback,
the case of french language. In Proceedings of the
Workshop on Feedback Behaviors. (poster).
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,
and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339?373.
91
