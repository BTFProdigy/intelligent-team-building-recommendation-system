Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 640?647,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Corpus Effects on the Evaluation of Automated Transliteration Systems
Sarvnaz Karimi Andrew Turpin Falk Scholer
School of Computer Science and Information Technology
RMIT University, GPO Box 2476V, Melbourne 3001, Australia
{sarvnaz,aht,fscholer}@cs.rmit.edu.au
Abstract
Most current machine transliteration sys-
tems employ a corpus of known source-
target word pairs to train their system, and
typically evaluate their systems on a similar
corpus. In this paper we explore the perfor-
mance of transliteration systems on corpora
that are varied in a controlled way. In partic-
ular, we control the number, and prior lan-
guage knowledge of human transliterators
used to construct the corpora, and the origin
of the source words that make up the cor-
pora. We find that the word accuracy of au-
tomated transliteration systems can vary by
up to 30% (in absolute terms) depending on
the corpus on which they are run. We con-
clude that at least four human transliterators
should be used to construct corpora for eval-
uating automated transliteration systems;
and that although absolute word accuracy
metrics may not translate across corpora, the
relative rankings of system performance re-
mains stable across differing corpora.
1 Introduction
Machine transliteration is the process of transform-
ing a word written in a source language into a word
in a target language without the aid of a bilingual
dictionary. Word pronunciation is preserved, as far
as possible, but the script used to render the target
word is different from that of the source language.
Transliteration is applied to proper nouns and out-
of-vocabulary terms as part of machine translation
and cross-lingual information retrieval (CLIR) (Ab-
dulJaleel and Larkey, 2003; Pirkola et al, 2006).
Several transliteration methods are reported in the
literature for a variety of languages, with their per-
formance being evaluated on multilingual corpora.
Source-target pairs are either extracted from bilin-
gual documents or dictionaries (AbdulJaleel and
Larkey, 2003; Bilac and Tanaka, 2005; Oh and Choi,
2006; Zelenko and Aone, 2006), or gathered ex-
plicitly from human transliterators (Al-Onaizan and
Knight, 2002; Zelenko and Aone, 2006). Some eval-
uations of transliteration methods depend on a single
unique transliteration for each source word, while
others take multiple target words for a single source
word into account. In their work on transliterating
English to Persian, Karimi et al (2006) observed
that the content of the corpus used for evaluating
systems could have dramatic affects on the reported
accuracy of methods.
The effects of corpus composition on the evalua-
tion of transliteration systems has not been specif-
ically studied, with only implicit experiments or
claims made in the literature such as introduc-
ing the effects of different transliteration mod-
els (AbdulJaleel and Larkey, 2003), language fam-
ilies (Linde?n, 2005) or application based (CLIR)
evaluation (Pirkola et al, 2006). In this paper, we re-
port our experiments designed to explicitly examine
the effect that varying the underlying corpus used in
both training and testing systems has on translitera-
tion accuracy. Specifically, we vary the number of
human transliterators that are used to construct the
corpus; and the origin of the English words used in
the corpus.
Our experiments show that the word accuracy of
automated transliteration systems can vary by up to
30% (in absolute terms), depending on the corpus
used. Despite the wide range of absolute values
640
in performance, the ranking of our two translitera-
tion systems was preserved on all corpora. We also
find that a human?s confidence in the language from
which they are transliterating can affect the corpus
in such a way that word accuracy rates are altered.
2 Background
Machine transliteration methods are divided into
grapheme-based (AbdulJaleel and Larkey, 2003;
Linde?n, 2005), phoneme-based (Jung et al, 2000;
Virga and Khudanpur, 2003) and combined tech-
niques (Bilac and Tanaka, 2005; Oh and Choi,
2006). Grapheme-based methods derive transforma-
tion rules for character combinations in the source
text from a training data set, while phoneme-based
methods use an intermediate phonetic transforma-
tion. In this paper, we use two grapheme-based
methods for English to Persian transliteration. Dur-
ing a training phase, both methods derive rules for
transforming character combinations (segments) in
the source language into character combinations in
the target language with some probability.
During transliteration, the source word si is seg-
mented and rules are chosen and applied to each seg-
ment according to heuristics. The probability of a
resulting word is the product of the probabilities of
the applied rules. The result is a list of target words
sorted by their associated probabilities, Li.
The first system we use (SYS-1) is an n-gram
approach that uses the last character of the previ-
ous source segment to condition the choice of the
rule for the current source segment. This system has
been shown to outperform other n-gram based meth-
ods for English to Persian transliteration (Karimi et
al., 2006).
The second system we employ (SYS-2) makes
use of some explicit knowledge of our chosen lan-
guage pair, English and Persian, and is also on
the collapsed-vowel scheme presented by Karimi et
al. (2006). In particular, it exploits the tendency for
runs of English vowels to be collapsed into a single
Persian character, or perhaps omitted from the Per-
sian altogether. As such, segments are chosen based
on surrounding consonants and vowels. The full de-
tails of this system are not important for this paper;
here we focus on the performance evaluation of sys-
tems, not the systems themselves.
2.1 System Evaluation
In order to evaluate the list Li of target words pro-
duced by a transliteration system for source word si,
a test corpus is constructed. The test corpus con-
sists of a source word, si, and a list of possible target
words {ti j}, where 1 ? j ? di, the number of dis-
tinct target words for source word si. Associated
with each ti j is a count ni j which is the number of
human transliterators who transliterated si into ti j.
Often the test corpus is a proportion of a larger
corpus, the remainder of which has been used for
training the system?s rule base. In this work we
adopt the standard ten-fold cross validation tech-
nique for all of our results, where 90% of a corpus
is used for training and 10% for testing. The pro-
cess is repeated ten times, and the mean result taken.
Forthwith, we use the term corpus to refer to the sin-
gle corpus from which both training and test sets are
drawn in this fashion.
Once the corpus is decided upon, a metric to mea-
sure the system?s accuracy is required. The appro-
priate metric depends on the scenario in which the
transliteration system is to be used. For example,
in a machine translation application where only one
target word can be inserted in the text to represent a
source word, it is important that the word at the top
of the system generated list of target words (by def-
inition the most probable) is one of the words gen-
erated by a human in the corpus. More formally,
the first word generated for source word si, Li1, must
be one of ti j,1 ? j ? di. It may even be desirable
that this is the target word most commonly used for
this source word; that is, Li1 = ti j such that ni j ? nik,
for all 1 ? k ? di. Alternately, in a CLIR appli-
cation, all variants of a source word might be re-
quired. For example, if a user searches for an En-
glish term ?Tom? in Persian documents, the search
engine should try and locate documents that contain
both ?AK? (3 letters: H- -) and ???'?(2 letters: H-),
two possible transliterations of ?Tom? that would be
generated by human transliterators. In this case, a
metric that counts the number of ti j that appear in
the top di elements of the system generated list, Li,
might be appropriate.
In this paper we focus on the ?Top-1? case, where
it is important for the most probable target word gen-
erated by the system, Li1 to be either the most pop-
641
ular ti j (labeled the Majority, with ties broken ar-
bitrarily), or just one of the ti j?s (labeled Uniform
because all possible transliterations are equally re-
warded). A third scheme (labeled Weighted) is also
possible where the reward for ti j appearing as Li1
is ni j/?dij=1 ni j; here, each target word is given a
weight proportional to how often a human translit-
erator chose that target word. Due to space consid-
erations, we focus on the first two variants only.
In general, there are two commonly used met-
rics for transliteration evaluation: word accuracy
(WA) and character accuracy (CA) (Hall and Dowl-
ing, 1980). In all of our experiments, CA based
metrics closely mirrored WA based metrics, and
so conclusions drawn from the data would be the
same whether WA metrics or CA metrics were used.
Hence we only discuss and report WA based metrics
in this paper.
For each source word in the test corpus of K
words, word accuracy calculates the percentage of
correctly transliterated terms. Hence for the major-
ity case, where every source word in the corpus only
has one target word, the word accuracy is defined as
MWA = |{si|Li1 = ti1,1 ? i ? K}|/K,
and for the Uniform case, where every target variant
is included with equal weight in the corpus, the word
accuracy is defined as
UWA = |{si|Li1 ? {ti j},1 ? i ? K,1 ? j ? di}|/K.
2.2 Human Evaluation
To evaluate the level of agreement between translit-
erators, we use an agreement measure based on Mun
and Eye (2004).
For any source word si, there are di different
transliterations made by the ni human translitera-
tors (ni = ?dij=1 ni j, where ni j is the number of times
source word si was transliterated into target word
ti j). When any two transliterators agree on the
same target word, there are two agreements being
made: transliterator one agrees with transliterator
two, and vice versa. In general, therefore, the to-
tal number of agreements made on source word si is
?dij=1 ni j(ni j ? 1). Hence the total number of actual
agreements made on the entire corpus of K words is
Aact =
K
?
i=1
di?
j=1
ni j(ni j ?1).
The total number of possible agreements (that is,
when all human transliterators agree on a single tar-
get word for each source word), is
Aposs =
K
?
i=1
ni(ni ?1).
The proportion of overall agreement is therefore
PA =
Aact
Aposs
.
2.3 Corpora
Seven transliterators (T1, T2, . . ., T7: all native Per-
sian speakers from Iran) were recruited to transliter-
ate 1500 proper names that we provided. The names
were taken from lists of names written in English on
English Web sites. Five hundred of these names also
appeared in lists of names on Arabic Web sites, and
five hundred on Dutch name lists. The transliterators
were not told of the origin of each word. The en-
tire corpus, therefore, was easily separated into three
sub-corpora of 500 words each based on the origin
of each word. To distinguish these collections, we
use E7, A7 and D7 to denote the English, Arabic and
Dutch sub-corpora, respectively. The whole 1500
word corpus is referred to as EDA7.
Dutch and Arabic were chosen with an assump-
tion that most Iranian Persian speakers have little
knowledge of Dutch, while their familiarity with
Arabic should be in the second rank after English.
All of the participants held at least a Bachelors de-
gree. Table 1 summarizes the information about
the transliterators and their perception of the given
task. Participants were asked to scale the difficulty
of the transliteration of each sub-corpus, indicated
as a scale from 1 (hard) to 3 (easy). Similarly, the
participants? confidence in performing the task was
rated from 1 (no confidence) to 3 (quite confident).
The level of familiarity with second languages was
also reported based on a scale of zero (not familiar)
to 3 (excellent knowledge).
The information provided by participants con-
firms our assumption of transliterators knowledge
of second languages: high familiarity with English,
some knowledge of Arabic, and little or no prior
knowledge of Dutch. Also, the majority of them
found the transliteration of English terms of medium
difficulty, Dutch was considered mostly hard, and
Arabic as easy to medium.
642
Second Language Knowledge Difficulty,Confidence
Transliterator English Dutch Arabic Other English Dutch Arabic
1 2 0 1 - 1,1 1,2 2,3
2 2 0 2 - 2,2 2,3 3,3
3 2 0 1 - 2,2 1,2 2,2
4 2 0 1 - 2,2 2,1 3,3
5 2 0 2 Turkish 2,2 1,1 3,2
6 2 0 1 - 2,2 1,1 3,3
7 2 0 1 - 2,2 1,1 2,2
Table 1: Transliterator?s language knowledge (0=not familiar to 3=excellent knowledge), perception of
difficulty (1=hard to 3=easy) and confidence (1=no confidence to 3=quite confident) in creating the corpus.
E7 D7 A7 EDA7
Corpus
0
20
40
60
80
100
W
or
d 
A
cc
ur
ac
y 
(%
) 
UWA (SYS-2)
UWA (SYS-1)
MWA (SYS-2)
MWA (SYS-1)
Figure 1: Comparison of the two evaluation metrics
using the two systems on four corpora. (Lines were
added for clarity, and do not represent data points.)
0 20 40 60 80 100
Corpus
0
20
40
60
80
100
W
or
d 
A
cc
ur
ac
y 
(%
) 
UWA (SYS-2)
UWA (SYS-1)
MWA (SYS-2)
MWA (SYS-1)
Figure 2: Comparison of the two evaluation metrics
using the two systems on 100 randomly generated
sub-corpora.
3 Results
Figure 1 shows the values of UWA and MWA for
E7, A7, D7 and EDA7 using the two transliteration
systems. Immediately obvious is that varying the
corpora (x-axis) results in different values for word
accuracy, whether by the UWA or MWA method. For
example, if you chose to evaluate SYS-2 with the
UWA metric on the D7 corpus, you would obtain a
result of 82%, but if you chose to evaluate it with the
A7 corpus you would receive a result of only 73%.
This makes comparing systems that report results
obtained on different corpora very difficult. Encour-
agingly, however, SYS-2 consistently outperforms
the SYS-1 on all corpora for both metrics except
MWA on E7. This implies that ranking system per-
formance on the same corpus most likely yields a
system ranking that is transferable to other corpora.
To further investigate this, we randomly extracted
100 corpora of 500 word pairs from EDA7 and ran
the two systems on them and evaluated the results
using both MWA and UWA. Both of the measures
ranked the systems consistently using all these cor-
pora (Figure 2).
As expected, the UWA metric is consistently
higher than the MWA metric; it allows for the top
transliteration to appear in any of the possible vari-
ants for that word in the corpus, unlike the MWA
metric which insists upon a single target word. For
example, for the E7 corpus using the SYS-2 ap-
proach, UWA is 76.4% and MWA is 47.0%.
Each of the three sub-corpora can be further di-
vided based on the seven individual transliterators,
in different combinations. That is, construct a sub-
corpus from T1?s transliterations, T2?s, and so on;
then take all combinations of two transliterators,
then three, and so on. In general we can construct
7Cr such corpora from r transliterators in this fash-
ion, all of which have 500 source words, but may
have between one to seven different transliterations
for each of those words.
Figure 3 shows the MWA for these sub-corpora.
The x-axis shows the number of transliterators used
to form the sub-corpora. For example, when x = 3,
the performance figures plotted are achieved on cor-
pora when taking all triples of the seven translitera-
tor?s transliterations.
From the boxplots it can be seen that performance
varies considerably when the number of transliter-
ators used to determine a majority vote is varied.
643
1 2 3 4 5 6 7
20
30
40
50
60
D7
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  




























                               
                               
                               
                               




1 2 3 4 5 6 7
20
30
40
50
60
Number of Transliterators
EDA7
1 2 3 4 5 6 7
20
30
40
50
60
W
or
d A
cc
ur
ac
y (
%)
E7
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  




























                             
                             
                             
                             




1 2 3 4 5 6 7
20
30
40
50
60
Number of Transliterators
W
or
d A
cc
ur
ac
y (
%)
A7
Figure 3: Performance on sub-corpora derived by combining the number of transliterators shown on the x-
axis. Boxes show the 25th and 75th percentile of the MWA for all 7Cx combinations of transliterators using
SYS-2, with whiskers showing extreme values.
However, the changes do not follow a fixed trend
across the languages. For E7, the range of accuracies
achieved is high when only two or three translitera-
tors are involved, ranging from 37.0% to 50.6% in
SYS-2 method and from 33.8% to 48.0% in SYS-1
(not shown) when only two transliterators? data are
available. When more than three transliterators are
used, the range of performance is noticeably smaller.
Hence if at least four transliterators are used, then it
is more likely that a system?s MWA will be stable.
This finding is supported by Papineni et al (2002)
who recommend that four people should be used for
collecting judgments for machine translation exper-
iments.
The corpora derived from A7 show consistent me-
dian increases as the number of transliterators in-
creases, but the median accuracy is lower than for
other languages. The D7 collection does not show
any stable results until at least six transliterator?s are
used.
The results indicate that creating a collection used
for the evaluation of transliteration systems, based
on a ?gold standard? created by only one human
transliterator may lead to word accuracy results that
could show a 10% absolute difference compared to
results on a corpus derived using a different translit-
E7 D7 A7 EDA7
Corpus
0
20
40
60
W
or
d 
A
cc
ur
ac
y 
(%
) 
T1
T2
T3
T4
T5
T6
T7
SYS-2
Figure 4: Word accuracy on the sub-corpora using
only a single transliterator?s transliterations.
erator. This is evidenced by the leftmost box in each
panel of the figure which has a wide range of results.
Figure 4 shows this box in more detail for each
collection, plotting the word accuracy for each
user for all sub-corpora for SYS-2. The accuracy
achieved varies significantly between translitera-
tors; for example, for E7 collections, word accuracy
varies from 37.2% for T1 to 50.0% for T5. This
variance is more obvious for the D7 dataset where
the difference ranges from 23.2% for T 1 to 56.2%
for T 3. Origin language also has an effect: accuracy
for the Arabic collection (A7) is generally less than
that of English (E7). The Dutch collection (D7),
shows an unstable trend across transliterators. In
other words, accuracy differs in a narrower range for
Arabic and English, but in wider range for Dutch.
644
This is likely due to the fact that most transliterators
found Dutch a difficult language to work with, as
reported in Table 1.
3.1 Transliterator Consistency
To investigate the effect of invididual transliterator
consistency on system accuracy, we consider the
number of Persian characters used by each transliter-
ator on each sub-corpus, and the average number of
rules generated by SYS-2 on the ten training sets de-
rived in the ten-fold cross validation process, which
are shown in Table 2. For example, when translit-
erating words from E7 into Persian, T3 only ever
used 21 out of 32 characters available in the Persian
alphabet; T7, on the other hand, used 24 different
Persian characters. It is expected that an increase in
number of characters or rules provides more ?noise?
for the automated system, hence may lead to lower
accuracy. Superficially the opposite seems true for
rules: the mean number of rules generated by SYS-
2 is much higher for the EDA7 corpus than for the A7
corpus, and yet Figure 1 shows that word accuracy
is higher on the EDA7 corpus. A correlation test,
however, reveals that there is no significant relation-
ship between either the number of characters used,
nor the number of rules generated, and the result-
ing word accuracy of SYS-2 (Spearman correlation,
p = 0.09 (characters) and p = 0.98 (rules)).
A better indication of ?noise? in the corpus may
be given by the consistency with which a translit-
erator applies a certain rule. For example, a large
number of rules generated from a particular translit-
erator?s corpus may not be problematic if many of
the rules get applied with a low probability. If, on
the other hand, there were many rules with approx-
imately equal probabilities, the system may have
difficulty distinguishing when to apply some rules,
and not others. One way to quantify this effect
is to compute the self entropy of the rule distribu-
tion for each segment in the corpus for an indi-
vidual. If pi j is the probability of applying rule
1 ? j ? m when confronted with source segment
i, then Hi = ??mj=1 pi j log2 pi j is the entropy of the
probability distribution for that rule. H is maximized
when the probabilities pi j are all equal, and mini-
mized when the probabilities are very skewed (Shan-
non, 1948). As an example, consider the rules:
t ?< H,0.5 >, t ?<?,0.3 > and t ?<X,0.2 >; for
which Ht = 0.79.
The expected entropy can be used to obtain a sin-
gle entropy value over the whole corpus,
E = ?
R
?
i=1
fi
S Hi,
where Hi is the entropy of the rule probabilities for
segment i, R is the total number of segments, fi is
the frequency with which segment i occurs at any
position in all source words in the corpus, and S is
the sum of all fi.
The expected entropy for each transliterator is
shown in Figure 5, separated by corpus. Compar-
ison of this graph with Figure 4 shows that gen-
erally transliterators that have used rules inconsis-
tently generate a corpus that leads to low accuracy
for the systems. For example, T1 who has the low-
est accuracy for all the collections in both methods,
also has the highest expected entropy of rules for
all the collections. For the E7 collection, the max-
imum accuracy of 50.0%, belongs to T 5 who has
the minimum expected entropy. The same applies
to the D7 collection, where the maximum accuracy
of 56.2% and the minimum expected entropy both
belong to T 3. These observations are confirmed
by a statistically significant Spearman correlation
between expected rule entropy and word accuracy
(r = ?0.54, p = 0.003). Therefore, the consistency
with which transliterators employ their own internal
rules in developing a corpus has a direct effect on
system performance measures.
3.2 Inter-Transliterator Agreement and
Perceived Difficulty
Here we present various agreement proportions (PA
from Section 2.2), which give a measure of consis-
tency in the corpora across all users, as opposed to
the entropy measure which gives a consistency mea-
sure for a single user. For E7, PA was 33.6%, for
A7 it was 33.3% and for D7, agreement was 15.5%.
In general, humans agree less than 33% of the time
when transliterating English to Persian.
In addition, we examined agreement among
transliterators based on their perception of the task
difficulty shown in Table 1. For A7, agreement
among those who found the task easy was higher
(22.3%) than those who found it in medium level
645
E7 D7 A7 EDA7
Char Rules Char Rules Char Rules Char Rules
T1 23 523 23 623 28 330 31 1075
T2 22 487 25 550 29 304 32 956
T3 21 466 20 500 28 280 31 870
T4 23 497 22 524 28 307 30 956
T5 21 492 22 508 28 296 29 896
T6 24 493 21 563 25 313 29 968
T7 24 495 21 529 28 299 30 952
Mean 23 493 22 542 28 304 30 953
Table 2: Number of characters used and rules generated using SYS-2, per transliterator.
(18.8%). PA is 12.0% for those who found the
D7 collection hard to transliterate; while the six
transliterators who found the E7 collection difficulty
medium had PA = 30.2%. Hence, the harder par-
ticipants rated the transliteration task, the lower the
agreement scores tend to be for the derived corpus.
Finally, in Table 3 we show word accuracy results
for the two systems on corpora derived from translit-
erators grouped by perceived level of difficulty on
A7. It is readily apparent that SYS-2 outperforms
SYS-1 on the corpus comprised of human translit-
erations from people who saw the task as easy with
both word accuracy metrics; the relative improve-
ment of over 50% is statistically significant (paired
t-test on ten-fold cross validation runs). However,
on the corpus composed of transliterations that were
perceived as more difficult, ?Medium?, the advan-
tage of SYS-2 is significantly eroded, but is still
statistically significant for UWA. Here again, using
only one transliteration, MWA, did not distinguish
the performance of each system.
4 Discussion
We have evaluated two English to Persian translit-
eration systems on a variety of controlled corpora
using evaluation metrics that appear in previous
transliteration studies. Varying the evaluation cor-
pus in a controlled fashion has revealed several in-
teresting facts.
We report that human agreement on the English
to Persian transliteration task is about 33%. The ef-
fect that this level of disagreement on the evalua-
tion of systems has, can be seen in Figure 4, where
word accuracy is computed on corpora derived from
single transliterators. Accuracy can vary by up to
30% in absolute terms depending on the translitera-
tor chosen. To our knowledge, this is the first paper
E7 D7 A7 EDA7
Corpus
0.0
0.2
0.4
0.6
En
tr
op
y 
T1
T2
T3
T4
T5
T6
T7
Figure 5: Entropy of the generated segments based
on the collections created by different transliterators.
to report human agreement, and examine its effects
on transliteration accuracy.
In order to alleviate some of these effects on the
stability of word accuracy measures across corpora,
we recommend that at least four transliterators are
used to construct a corpus. Figure 3 shows that con-
structing a corpus with four or more transliterators,
the range of possible word accuracies achieved is
less than that of using fewer transliterators.
Some past studies do not use more than a sin-
gle target word for every source word in the cor-
pus (Bilac and Tanaka, 2005; Oh and Choi, 2006).
Our results indicate that it is unlikely that these re-
sults would translate onto a corpus other than the
one used in these studies, except in rare cases where
human transliterators are in 100% agreement for a
given language pair.
Given the nature of the English language, an En-
glish corpus can contain English words from a vari-
ety of different origins. In this study we have used
English words from an Arabic and Dutch origin to
show that word accuracy of the systems can vary by
up to 25% (in absolute terms) depending on the ori-
gin of English words in the corpus, as demonstrated
in Figure 1.
In addition to computing agreement, we also in-
646
Relative
Perception SYS-1 SYS-2 Improvement (%)
UWA Easy 33.4 55.4 54.4 (p < 0.001)
Medium 44.6 48.4 8.52 (p < 0.001)
MWA Easy 23.2 36.2 56.0 (p < 0.001)
Medium 30.6 37.4 22.2 (p = 0.038)
Table 3: System performance when A7 is split into sub-corpora based on transliterators perception of the
task (Easy or Medium).
vestigated the transliterator?s perception of difficulty
of the transliteration task with the ensuing word ac-
curacy of the systems. Interestingly, when using cor-
pora built from transliterators that perceive the task
to be easy, there is a large difference in the word
accuracy between the two systems, but on corpora
built from transliterators who perceive the task to be
more difficult, the gap between the systems narrows.
Hence, a corpus applied for evaluation of transliter-
ation should either be made carefully with translit-
erators with a variety of backgrounds, or should be
large enough and be gathered from various sources
so as to simulate different expectations of its ex-
pected non-homogeneous users.
The self entropy of rule probability distributions
derived by the automated transliteration system can
be used to measure the consistency with which in-
dividual transliterators apply their own rules in con-
structing a corpus. It was demonstrated that when
systems are evaluated on corpora built by transliter-
ators who are less consistent in their application of
transliteration rules, word accuracy is reduced.
Given the large variations in system accuracy that
are demonstrated by the varying corpora used in this
study, we recommend that extreme care be taken
when constructing corpora for evaluating translitera-
tion systems. Studies should also give details of their
corpora that would allow any of the effects observed
in this paper to be taken into account.
Acknowledgments
This work was supported in part by the Australian
government IPRS program (SK).
References
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical
transliteration for English-Arabic cross-language informa-
tion retrieval. In Conference on Information and Knowledge
Management, pages 139?146.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine translit-
eration of names in Arabic text. In Proceedings of the ACL-
02 workshop on Computational approaches to semitic lan-
guages, pages 1?13.
Slaven Bilac and Hozumi Tanaka. 2005. Direct combination
of spelling and pronunciation information for robust back-
transliteration. In Conference on Computational Linguistics
and Intelligent Text Processing, pages 413?424.
Patrick A. V. Hall and Geoff R. Dowling. 1980. Approximate
string matching. ACM Computing Survey, 12(4):381?402.
Sung Young Jung, Sung Lim Hong, and Eunok Paek. 2000. An
English to Korean transliteration model of extended Markov
window. In Conference on Computational Linguistics, pages
383?389.
Sarvnaz Karimi, Andrew Turpin, and Falk Scholer. 2006. En-
glish to Persian transliteration. In String Processing and In-
formation Retrieval, pages 255?266.
Krister Linde?n. 2005. Multilingual modeling of cross-lingual
spelling variants. Information Retrieval, 9(3):295?310.
Eun Young Mun and Alexander Von Eye, 2004. Analyzing
Rater Agreement: Manifest Variable Methods. Lawrence
Erlbaum Associates.
Jong-Hoon Oh and Key-Sun Choi. 2006. An ensemble of
transliteration models for information retrieval. Information
Processing Management, 42(4):980?1002.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In The 40th Annual Meeting of Associ-
ation for Computational Linguistics, pages 311?318.
Ari Pirkola, Jarmo Toivonen, Heikki Keskustalo, and Kalervo
Ja?rvelin. 2006. FITE-TRT: a high quality translation tech-
nique for OOV words. In Proceedings of the 2006 ACM
Symposium on Applied Computing, pages 1043?1049.
Claude Elwood Shannon. 1948. A mathematical theory of
communication. Bell System Technical Journal, 27:379?
423.
Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of
proper names in cross-language applications. In ACM SIGIR
Conference on Research and Development on Information
Retrieval, pages 365?366.
Dmitry Zelenko and Chinatsu Aone. 2006. Discriminative
methods for transliteration. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language Process-
ing, pages 612?617.
647
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 648?655,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Collapsed Consonant and Vowel Models: New Approaches for
English-Persian Transliteration and Back-Transliteration
Sarvnaz Karimi Falk Scholer Andrew Turpin
School of Computer Science and Information Technology
RMIT University, GPO Box 2476V, Melbourne 3001, Australia
{sarvnaz,fscholer,aht}@cs.rmit.edu.au
Abstract
We propose a novel algorithm for English
to Persian transliteration. Previous meth-
ods proposed for this language pair apply
a word alignment tool for training. By
contrast, we introduce an alignment algo-
rithm particularly designed for translitera-
tion. Our new model improves the English
to Persian transliteration accuracy by 14%
over an n-gram baseline. We also propose
a novel back-transliteration method for this
language pair, a previously unstudied prob-
lem. Experimental results demonstrate that
our algorithm leads to an absolute improve-
ment of 25% over standard transliteration
approaches.
1 Introduction
Translation of a text from a source language to
a target language requires dealing with technical
terms and proper names. These occur in almost
any text, but rarely appear in bilingual dictionar-
ies. The solution is the transliteration of such out-of-
dictionary terms: a word from the source language
is transformed to a word in the target language, pre-
serving its pronunciation. Recovering the original
word from the transliterated target is called back-
transliteration. Automatic transliteration is impor-
tant for many different applications, including ma-
chine translation, cross-lingual information retrieval
and cross-lingual question answering.
Transliteration methods can be categorized into
grapheme-based (AbdulJaleel and Larkey, 2003; Li
et al, 2004), phoneme-based (Knight and Graehl,
1998; Jung et al, 2000), and combined (Bilac and
Tanaka, 2005) approaches. Grapheme-based meth-
ods perform a direct orthographical mapping be-
tween source and target words, while phoneme-
based approaches use an intermediate phonetic rep-
resentation. Both grapheme- or phoneme-based
methods usually begin by breaking the source word
into segments, and then use a source segment to tar-
get segment mapping to generate the target word.
The rules of this mapping are obtained by aligning
already available transliterated word pairs (training
data); alternatively, such rules can be handcrafted.
From this perspective, past work is roughly divided
into those methods which apply a word alignment
tool such as GIZA++ (Och and Ney, 2003), and ap-
proaches that combine the alignment step into their
main transliteration process.
Transliteration is language dependent, and meth-
ods that are effective for one language pair may
not work as well for another. In this paper, we
investigate the English-Persian transliteration prob-
lem. Persian (Farsi) is an Indo-European language,
written in Arabic script from right to left, but with
an extended alphabet and different pronunciation
from Arabic. Our previous approach to English-
Persian transliteration introduced the grapheme-
based collapsed-vowel method, employing GIZA++
for source to target algnment (Karimi et al, 2006).
We propose a new transliteration approach that ex-
tends the collapsed-vowel method. To meet Per-
sian language transliteration requirements, we also
propose a novel alignment algorithm in our training
stage, which makes use of statistical information of
648
the corpus, transliteration specifications, and simple
language properties. This approach handles possi-
ble consequences of elision (omission of sounds to
make the word easier to read) and epenthesis (adding
extra sounds to a word to make it fluent) in written
target words that happen due to the change of lan-
guage. Our method shows an absolute accuracy im-
provement of 14.2% over an n-gram baseline.
In addition, we investigate the problem of back-
transliteration from Persian to English. To our
knowledge, this is the first report of such a study.
There are two challenges in Persian to English
transliteration that makes it particularly difficult.
First, written Persian omits short vowels, while only
long vowels appear in texts. Second, monophthon-
gization (changing diphthongs to monophthongs) is
popular among Persian speakers when adapting for-
eign words into their language. To take these into
account, we propose a novel method to form trans-
formation rules by changing the normal segmenta-
tion algorithm. We find that this method signifi-
cantly improves the Persian to English translitera-
tion effectiveness, demonstrating an absolute perfor-
mance gain of 25.1% over standard transliteration
approaches.
2 Background
In general, transliteration consists of a training stage
(running on a bilingual training corpus), and a gen-
eration ? also called testing ? stage.
The training step of a transliteration develops
transformation rules mapping characters in the
source to characters in the target language using
knowledge of corresponding characters in translit-
erated pairs provided by an alignment. For example,
for the source-target word pair (pat,H



H), an align-
ment may map ?p? to ?H

? and ?a? to ? ?, and the
training stage may develop the rule pa ? , with ? ?
as the transliteration of ?a? in the context of ?pa?.
The generation stage applies these rules on a seg-
mented source word, transforming it to a word in
the target language.
Previous work on transliteration either employs a
word alignment tool (usually GIZA++), or develops
specific alignment strategies. Transliteration meth-
ods that use GIZA++ as their word pair aligner (Ab-
dulJaleel and Larkey, 2003; Virga and Khudanpur,
2003; Karimi et al, 2006) have based their work on
the assumption that the provided alignments are re-
liable. Gao et al (2004) argue that precise align-
ment can improve transliteration effectiveness, ex-
perimenting on English-Chinese data and compar-
ing IBM models (Brown et al, 1993) with phoneme-
based alignments using direct probabilities.
Other transliteration systems focus on alignment
for transliteration, for example the joint source-
channel model suggested by Li et al (2004). Their
method outperforms the noisy channel model in
direct orthographical mapping for English-Chinese
transliteration. Li et al also find that grapheme-
based methods that use the joint source-channel
model are more effective than phoneme-based meth-
ods due to removing the intermediate phonetic
transformation step. Alignment has also been in-
vestigated for transliteration by adopting Coving-
ton?s algorithm on cognate identification (Coving-
ton, 1996); this is a character alignment algorithm
based on matching or skipping of characters, with
a manually assigned cost of association. Coving-
ton considers consonant to consonant and vowel to
vowel correspondence more valid than consonant to
vowel. Kang and Choi (2000) revise this method for
transliteration where a skip is defined as inserting a
null in the target string when two characters do not
match based on their phonetic similarities or their
consonant and vowel nature. Oh and Choi (2002)
revise this method by introducing binding, in which
many to many correspondences are allowed. How-
ever, all of these approaches rely on the manually
assigned penalties that need to be defined for each
possible matching.
In addition, some recent studies investigate dis-
criminative transliteration methods (Klementiev and
Roth, 2006; Zelenko and Aone, 2006) in which each
segment of the source can be aligned to each seg-
ment of the target, where some restrictive conditions
based on the distance of the segments and phonetic
similarities are applied.
3 The Proposed Alignment Approach
We propose an alignment method based on segment
occurrence frequencies, thereby avoiding predefined
matching patterns and penalty assignments. We also
apply the observed tendency of aligning consonants
649
to consonants, and vowels to vowels, as a substi-
tute for phonetic similarities. Many to many, one to
many, one to null and many to one alignments can
be generated.
3.1 Formulation
Our alignment approach consists of two steps: the
first is based on the consonant and vowel nature
of the word?s letters, while the second uses a
frequency-based sequential search.
Definition 1 A bilingual corpus B is the set
{(S, T )}, where S = s1..s?, T = t1..tm, si is a
letter in the source language alphabet, and tj is a
letter in the target language alphabet.
Definition 2 Given some word, w, the consonant-
vowel sequence p = (C|V )+ for w is obtained
by replacing each consonant with C and each vowel
with V .
Definition 3 Given some consonant-vowel se-
quence, p, a reduced consonant-vowel sequence q
replaces all runs of C?s with C, and all runs of V ?s
with V; hence q = q?|q??, q? = V(CV)?(C|?)
and q?? = C(VC)?(V|?).
For each natural language word, we can determine
the consonant-vowel sequence (p) from which the
reduced consonant-vowel sequence (q) can be de-
rived, giving a common notation between two dif-
ferent languages, no matter which script either of
them use. To simplify, semi-vowels and approxi-
mants (sounds intermediate between consonants and
vowels, such as ?w? and ?y? in English) are treated
according to their target language counterparts.
In general, for all the word pairs (S, T ) in a corpus
B, an alignment can be achieved using the function
f : B ? A; (S, T ) 7? (S?, T? , r).
The function f maps the word pair (S, T ) ? B to
the triple (S?, T? , r) ? A where S? and T? are sub-
strings of S and T respectively. The frequency of
this correspondence is denoted by r. A represents a
set of substring alignments, and we use a per word
alignment notation of ae2p when aligning English to
Persian and ap2e for Persian to English.
3.2 Algorithm Details
Our algorithm consists of two steps.
Step 1 (Consonant-Vowel based)
For any word pair (S, T ) ? B, the corresponding
reduced consonant-vowel sequences, qS and qT , are
generated. If the sequences match, then the aligned
consonant clusters and vowel sequences are added
to the alignment set A. If qS does not match with
qT , the word pair remains unaligned in Step 1.
The assumption in this step is that transliteration
of each vowel sequence of the source is a vowel se-
quence in the target language, and similarly for con-
sonants. However, consonants do not always map to
consonants, or vowels to vowels (for example, the
English letter ?s? may be written as ??? in Persian
which consists of one vowel and one consonant). Al-
ternatively, they might be omitted altogether, which
can be specified as the null string, ?. We therefore
require a second step.
Step 2 (Frequency based)
For most natural languages, the maximum length
of corresponding phonemes of each grapheme is a
digraph (two letters) or at most a trigraph. Hence,
alignment can be defined as a search problem that
seeks for units with a maximum length of two or
three in both strings that need to be aligned. In our
approach, we search based on statistical occurrence
data available from Step 1.
In Step 2, only those words that remain unaligned
at the end of Step 1 need to be considered. For each
pair of words (S, T ), matching proceeds from left to
right, examining one of the three possible options of
transliteration: single letter to single letter, digraph
to single letter and single letter to digraph. Trigraphs
are unnecessary in alignment as they can be effec-
tively captured during transliteration generation, as
we explain below.
We define four different valid alignments for the
source (S = s1s2 . . . si . . . sl) and target (T =
t1t2 . . . tj . . . tm) strings: (si , tj , r), (sisi+1, tj , r),
(si , tj tj+1, r) and (si , ?, r). These four options are
considered as the only possible valid alignments,
and the most frequently occurring alignment (high-
est r) is chosen. These frequencies are dynamically
updated after successfully aligning a pair. For ex-
ceptional situations, where there is no character in
the target string to match with the source character
si , it is aligned with the empty string.
It is possible that none of the four valid alignment
650
options have occurred previously (that is, r = 0
for each). This situation can arise in two ways:
first, such a tuple may simply not have occurred in
the training data; and, second, the previous align-
ment in the current string pair may have been incor-
rect. To account for this second possibility, a par-
tial backtracking is considered. Most misalignments
are derived from the simultaneous comparison of
alignment possibilities, giving the highest priority to
the most frequent. For example if S=bbc, T=H
.
?
and A = {(b,H
.
,100),(bb,H
.
,40),(c,?,60)}, starting
from the initial position s1 and t1 , the first alignment
choice is (b,H
.
,101). However immediately after, we
face the problem of aligning the second ?b?. There
are two solutions: inserting ? and adding the triple
(b,?,1), or backtracking the previous alignment and
substituting that with the less frequent but possible
alignment of (bb,H
.
,41). The second solution is a
better choice as it adds less ambiguous alignments
containing ?. At the end, the alignment set is up-
dated as A = {(b,H
.
,100),(bb,H
.
,41),(c,?,61)}.
In case of equal frequencies, we check possible
subsequent alignments to decide on which align-
ment should be chosen. For example, if (b,H
.
,100)
and (bb,H
.
,100) both exist as possible options, we
consider if choosing the former leads to a subse-
quent ? insertion. If so, we opt for the latter.
At the end of a string, if just one character in the
target string remains unaligned while the last align-
ment is a ? insertion, that final alignment will be sub-
stituted for ?. This usually happens when the align-
ment of final characters is not yet registered in the
alignment set, mainly because Persian speakers tend
to transliterate the final vowels to consonants to pre-
serve their existence in the word. For example, in
the word ?Jose? the final ?e? might be transliterated
to ? ?? which is a consonant (?h?) and therefore is not
captured in Step 1.
Backparsing
The process of aligning words explained above
can handle words with already known components
in the alignment set A (the frequency of occurrence
is greater than zero). However, when this is not the
case, the system may repeatedly insert ? while part
or all of the target characters are left intact (unsuc-
cessful alignment). In such cases, processing the
source and target backwards helps to find the prob-
lematic substrings: backparsing.
The poorly aligned substrings of the source and
target are taken as new pairs of strings, which are
then reintroduced into the system as new entries.
Note that they themselves are not subject to back-
parsing. Most strings of repeating nulls can be bro-
ken up this way, and in the worst case will remain as
one tuple in the alignment set.
To clarify, consider the example given in Figure 1.
For the word pair (patricia,H


HP?

??), where an
association between ?c? and ? ?? is not yet regis-
tered. Forward parsing, as shown in the figure, does
not resolve all target characters; after the incorrect
alignment of ?c? with ???, subsequent characters are
also aligned with null, and the substring ? ??? re-
mains intact. Backward parsing, shown in the next
line of the figure, is also not successful. It is able to
correctly align the last two characters of the string,
before generating repeated null alignments. There-
fore, the central region ? substrings of the source
and target which remained unaligned plus one extra
aligned segment to the left and right ? is entered
as a new pair to the system (ici,? ??), as shown
in the line labelled Input 2 in the figure. This new
input meets Step 1 requirements, and is aligned suc-
cessfully. The resulting tuples are then merged with
the alignment set A.
An advantage of our backparsing strategy is that
it takes care of casual transliterations happening due
to elision and epenthesis (adding or removing ex-
tra sounds). It is not only in translation that people
may add extra words to make fluent target text; for
transliteration also, it is possible that spurious char-
acters are introduced for fluency. However, this of-
ten follows patterns, such as adding vowels to the
target form. These irregularities are consistently
covered in the backparsing strategy, where they re-
main connected to their previous character.
4 Transliteration Method
Transliteration algorithms use aligned data (the out-
put from the alignment process, ae2p or ap2e align-
ment tuples) for training to derive transformation
rules. These rules are then used to generate a tar-
get word T given a new input source word S.
651
Initial alignment set:
A = {(p,H

,42),(a, ,320),(a,?,99),(a, ?,10),(a,?,35),(r,P,200),(i,?,60),(i,?,5),(c,?,80),(c,h

,25),(t, H,51)}
Input: (patricia,H


HP?

??) qS = CVCVCV qT = CVCV
Step 1: qS 6= qT
Forward alignment: (p,H

,43), (a,?,100), (t, H,52), (r,P,201), (i,?,61), (c,?,1), (i,?,6), (a,?,100)
Backward alignment: (a, ,321), (i,?,61), (c,?,1), (i,?,6), (r,?,1), (t,?,1), (a,?,100), (p,?,1)
Input 2: (ici,? ??) qS = VCV qT = VCV
Step 1: (i,?,61),(c, ?,1), (i,?,61)
Final Alignment: ae2p = ((p,H ),(a,?),(t, H),((r,P),(i,?),(c, ?),(i,?),(a, ))
Updated alignment set:
A = {(p,H

,43),(a, ,321),(a,?,100),(a, ?,10),(a,?,35),(r,P,201),(i,?,62),(i,?,5),(c,?,80),(c,h

,25),(c, ?,1),(t, H,52)}
Figure 1: A backparsing example. Note middle tuples in forward and backward parsings are not merged in
A till the alignment is successfully completed.
Method Intermediate Sequence Segment(Pattern) Backoff
Bigram N/A #s, sh, he, el, ll, le, ey s,h,e,l,e,y
CV-MODEL1 CCVCCV sh(CC), hel(CVC), ll(CC), lley(CV) s(C), h(C), e(V), l(C), e(V), y(V)
CV-MODEL2 CCVCCV sh(CC), e(CVC), ll(CC), ey(CV) As Above.
CV-MODEL3 CVCV #sh(C), e(CVC), ll(C), ey(CV) sh(C), s(C), h(C), e(V), l(C), e(V), y(V)
Figure 2: An example of transliteration for the word pair (shelley, ???). Underlined characters are actually
transliterated for each segment.
4.1 Baseline
Most transliteration methods reported in the litera-
ture ? either grapheme- or phoneme-based ? use
n-grams (AbdulJaleel and Larkey, 2003; Jung et al,
2000). The n-gram-based methods differ mainly in
the way that words are segmented, both for train-
ing and transliteration generation. A simple n-
gram based method works only on single charac-
ters (unigram) and transformation rules are defined
as si ? tj , while an advanced method may take
the surrounding context into account (Jung et al,
2000). We found that using one past symbol (bigram
model) works better than other n-gram based meth-
ods for English to Persian transliteration (Karimi et
al., 2006).
Our collapsed-vowel methods consider language
knowledge to improve the string segmentation of
n-gram techniques (Karimi et al, 2006). The pro-
cess begins by generating the consonant-vowel se-
quence (Definition 2) of a source word. For ex-
ample, the word ?shelley? is represented by the se-
quence p = CCV CCV V . Then, following the col-
lapsed vowel concept (Definition 3), this sequence
becomes ?CCVCCV?. These approaches, which
we refer to as CV-MODEL1 and CV-MODEL2 re-
spectively, partition these sequences using basic pat-
terns (C and V) and main patterns (CC , CVC , VC
and CV). In the training phase, transliteration rules
are formed according to the boundaries of the de-
fined patterns and their aligned counterparts (based
on ae2p or ap2e) in the target language word T . Simi-
lar segmentation is applied during the transliteration
generation stage.
4.2 The Proposed Transliteration Approach
The restriction on the context length of consonants
imposed by CV-MODEL1 and CV-MODEL2 makes
the transliteration of consecutive consonants map-
ping to a particular character in the target language
difficult. For example, ?ght? in English maps to
only one character in Persian: ? H?. Dealing with
languages which have different alphabets, and for
which the number of characters in their alphabets
also differs (such as 26 and 32 for English and Per-
sian), increases the possibility of facing these cases,
especially when moving from the language with
smaller alphabet size to the one with a larger size.
To more effectively address this, we propose a col-
lapsed consonant and vowel method (CV-MODEL3)
which uses the full reduced sequence (Definition 3),
rather than simply reduced vowel sequences. Al-
though recognition of consonant segments is based
on the vowel positions, consonants are considered as
independent blocks in each string. Conversely, vow-
els are transliterated in the context of surrounding
652
consonants, as demonstrated in the example below.
A special symbol is used to indicate the start
and/or end of each word if the beginning and end
of the word is a consonant respectively. Therefore,
for the words starting or ending with consonants, the
symbol ?#? is added, which is treated as a consonant
and therefore grouped in the consonant segment.
An example of applying this technique is shown in
Figure 2 for the string ?shelley?. In this example,
?sh? and ?ll? are treated as two consonant segments,
where the transliteration of individual characters in-
side a segment is dependent on the other members
but not the surrounding segments. However, this is
not the case for vowel sequences which incorporate
a level of knowledge about any segment neighbours.
Therefore, for the example ?shelley?, the first seg-
ment is ?sh? which belongs to C pattern. During
transliteration, if ?#sh? does not appear in any ex-
isting rules, a backoff splits the segment to smaller
segments: ?#? and ?sh?, or ?s?and ?h?. The second
segment contains the vowel ?e?. Since this vowel
is surrounded by consonants, the segment pattern is
CVC. In this case, backoff only applies for vowels as
consonants are supposed to be part of their own in-
dependent segments. That is, if search in the rules of
pattern CVC was unsuccessful, it looks for ?e? in V
pattern. Similarly, segmentation for this word con-
tinues with ?ll? in C pattern and ?ey? in CV pattern
(?y? is an approximant, and therefore considered as
a vowel when transliterating English to Persian).
4.3 Rules for Back-Transliteration
Written Persian ignores short vowels, and only long
vowels appear in text. This causes most English
vowels to disappear when transliterating from En-
glish to Persian; hence, these vowels must be re-
stored during back-transliteration.
When the initial transliteration happens from En-
glish to Persian, the transliterator (whether hu-
man or machine) uses the rules of transliterat-
ing from English as the source language. There-
fore, transliterating back to the original language
should consider the original process, to avoid los-
ing essential information. In terms of segmenta-
tion in collapsed-vowel models, different patterns
define segment boundaries in which vowels are
necessary clues. Although we do not have most
of these vowels in the transliteration generation
phase, it is possible to benefit from their existence
in the training phase. For example, using CV-
MODEL3, the pair (P??,merkel) with qS =C and
ap2e=((,me),(P,r),(?,ke),(?,l)), produces just one
transformation rule ?P?? ? merkel? based on a
C pattern. That is, the Persian string contains no
vowel characters. If, during the transliteration gen-
eration phase, a source word ???Q?? (S=P??) is
entered, there would be one and only one output
of ?merkel?, while an alternative such as ?mercle?
might be required instead. To avoid overfitting the
system by long consonant clusters, we perform seg-
mentation based on the English q sequence, but cate-
gorise the rules based on their Persian segment coun-
terparts. That is, for the pair (P??,merkel) with
ae2p=((m,),(e,?),(r,P),(k,?),(e,?),(l,?)), these rules
are generated (with category patterns given in paren-
thesis):  ? m (C), P? ? rk (C), ? ? l (C), P?
? merk (C), P?? ? rkel (C). We call the suggested
training approach reverse segmentation.
Reverse segmentation avoids clustering all the
consonants in one rule, since many English words
might be transliterated to all-consonant Persian
words.
4.4 Transliteration Generation and Ranking
In the transliteration generation stage, the source
word is segmented following the same process of
segmenting words in training stage, and a probabil-
ity is computed for each generated target word:
P (T |S) =
|K|
Y
k=1
P (T?k|S?k),
where |K| is the number of distinct source seg-
ments. P (T?k|S?k) is the probability of the S?k?T?k
transformation rule, as obtained from the training
stage:
P (T?k|S?k) =
frequency of S?k ? T?k
frequency of S?k
,
where frequency of S?k is the number of its oc-
currence in the transformation rules. We apply a
tree structure, following Dijkstra?s ?-shortest path,
to generate the ? highest scoring (most probable)
transliterations, ranked based on their probabilities.
653
Corpus Baseline CV-MODEL3Bigram CV-MODEL1 CV-MODEL2 GIZA++ New Alignment
Small Corpus
TOP-1 58.0 (2.2) 61.7 (3.0) 60.0 (3.9) 67.4 (5.5) 72.2 (2.2)
TOP-5 85.6 (3.4) 80.9 (2.2) 86.0 (2.8) 90.9 (2.1) 92.9 (1.6)
TOP-10 89.4 (2.9) 82.0 (2.1) 91.2 (2.5) 93.8 (2.1) 93.5 (1.7)
Large Corpus
TOP-1 47.2 (1.0) 50.6 (2.5) 47.4 (1.0) 55.3 (0.8) 59.8 (1.1)
TOP-5 77.6 (1.4) 79.8 (3.4) 79.2 (1.0) 84.5 (0.7) 85.4 (0.8)
TOP-10 83.3 (1.5) 84.9 (3.1) 87.0 (0.9) 89.5 (0.4) 92.6 (0.7)
Table 1: Mean (standard deviation) word accuracy (%) for English to Persian transliteration.
5 Experiments
To investigate the effectiveness of CV-MODEL3 and
the new alignment approach on transliteration, we
first compare CV-MODEL3 with baseline systems,
employing GIZA++ for alignment generation during
system training. We then evaluate the same sys-
tems, using our new alignment approach. Back-
transliteration is also investigated, applying both
alignment systems and reverse segmentation. In all
our experiments, we used ten-fold cross-validation.
The statistical significance of different performance
levels are evaluated using a paired t-test. The no-
tation TOP-X indicates the first X transliterations
prodcued by the automatic methods.
We used two corpora of word pairs in English
and Persian: the first, called Large, contains 16,670
word pairs; the second, Small, contains 1,857 word
pairs, and are described fully in our previous paper
(Karimi et al, 2006).
The results of transliteration experiments are eval-
uated using word accuracy (Kang and Choi, 2000)
which measures the proportion of transliterations
that are correct out of the test corpus.
5.1 Accuracy of Transliteration Approaches
The results of our experiments for transliterating En-
glish to Persian, using GIZA++ for alignment gen-
eration, are shown in Table 1. CV-MODEL3 out-
performs all three baseline systems significantly in
TOP-1 and TOP-5 results, for both Persian corpora.
TOP-1 results were improved by 9.2% to 16.2%
(p<0.0001, paired t-test) relative to the baseline sys-
tems for the Small corpus. For the Large corpus,
CV-MODEL3 was 9.3% to 17.2% (p<0.0001) more
accurate relative to the baseline systems.
The results of applying our new alignment al-
gorithm are presented in the last column of Ta-
ble 1, comparing word accuracy of CV-MODEL3 us-
ing GIZA++ and the new alignment for English to
Persian transliteration. Transliteration accuracy in-
creases in TOP-1 for both corpora (a relative increase
of 7.1% (p=0.002) for the Small corpus and 8.1%
(p<0.0001) for the Large corpus). The TOP-10 re-
sults of the Large corpus again show a relative in-
crease of 3.5% (p=0.004). Although the new align-
ment also increases the performance for TOP-5 and
TOP-10 of the Small corpus, these increases are not
statistically significant.
5.2 Accuracy of Back-Transliteration
The results of back-transliteration are shown in Ta-
ble 2. We first consider performance improvements
gained from using CV-MODEL3: CV-MODEL3 using
GIZA++ outperforms Bigram, CV-MODEL1 and CV-
MODEL2 by 12.8% to 40.7% (p<0.0001) in TOP-
1 for the Small corpus. The corresponding im-
provement for the Large corpus is 12.8% to 74.2%
(p<0.0001).
The fifth column of the table shows the perfor-
mance increase when using CV-MODEL3 with the
new alignment algorithm: for the Large corpus, the
new alignment approach gives a relative increase in
accuracy of 15.5% for TOP-5 (p<0.0001) and 10%
for TOP-10 (p=0.005). The new alignment method
does not show a significant difference using CV-
MODEL3 for the Small corpus.
The final column of Table 2 shows the perfor-
mance of the CV-MODEL3 with the new reverse seg-
mentation approach. Reverse segmentation leads to
a significant improvement over the new alignment
approach in TOP-1 results for the Small corpus by
40.1% (p<0.0001), and 49.4% (p<0.0001) for the
Large corpus.
654
Corpus Bigram CV-MODEL1 CV-MODEL2 CV-MODEL3
GIZA++ New Alignment Reverse
Small Corpus
TOP-1 23.1 (2.0) 28.8 (4.6) 24.9 (2.8) 32.5 (3.6) 34.4 (3.8) 48.2 (2.9)
TOP-5 40.8 (3.1) 51.0 (4.8) 52.9 (3.4) 56.0 (3.5) 54.8 (3.7) 68.1 (4.9)
TOP-10 50.1 (4.1) 58.2 (5.3) 63.2 (3.1) 64.2 (3.2) 63.8 (3.6) 75.7 (4.2)
Large Corpus
TOP-1 10.1 (0.6) 15.6 (1.0) 12.0 (1.0) 17.6 (0.8) 18.0 (1.2) 26.9 (0.7)
TOP-5 20.6 (1.2) 31.7 (0.9) 28.0 (0.7) 36.2 (0.5) 41.8 (1.2) 41.3 (1.7)
TOP-10 27.2 (1.0) 40.1 (1.1) 37.4 (0.8) 46.0 (0.8) 50.6 (1.1) 49.3 (1.6)
Table 2: Comparison of mean (standard deviation) word accuracy (%) for Persian to English transliteration.
6 Conclusions
We have presented a new algorithm for English to
Persian transliteration, and a novel alignment al-
gorithm applicable for transliteration. Our new
transliteration method (CV-MODEL3) outperforms
the previous approaches for English to Persian, in-
creasing word accuracy by a relative 9.2% to 17.2%
(TOP-1), when using GIZA++ for alignment in train-
ing. This method shows further 7.1% to 8.1% in-
crease in word accuracy (TOP-1) with our new align-
ment algorithm.
Persian to English back-transliteration is also in-
vestigated, with CV-MODEL3 significantly outper-
forming other methods. Enriching this model with
a new reverse segmentation algorithm gives rise to
further accuracy gains in comparison to directly ap-
plying English to Persian methods.
In future work we will investigate whether pho-
netic information can help refine our CV-MODEL3,
and experiment with manually constructed rules as
a baseline system.
Acknowledgments
This work was supported in part by the Australian
government IPRS program (SK) and an ARC Dis-
covery Project Grant (AT).
References
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical
transliteration for English-Arabic cross language informa-
tion retrieval. In Conference on Information and Knowledge
Management, pages 139?146.
Slaven Bilac and Hozumi Tanaka. 2005. Direct combination
of spelling and pronunciation information for robust back-
transliteration. In Conferences on Computational Linguis-
tics and Intelligent Text Processing, pages 413?424.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statisti-
cal machine translation: Parameter estimation. Computional
Linguistics, 19(2):263?311.
Michael A. Covington. 1996. An algorithm to align
words for historical comparison. Computational Linguistics,
22(4):481?496.
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004. Improving
transliteration with precise alignment of phoneme chunks
and using contextual features. In Asia Information Retrieval
Symposium, pages 106?117.
Sung Young Jung, Sung Lim Hong, and Eunok Paek. 2000. An
English to Korean transliteration model of extended Markov
window. In Conference on Computational Linguistics, pages
383?389.
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic translit-
eration and back-transliteration by decision tree learning. In
Conference on Language Resources and Evaluation, pages
1135?1411.
Sarvnaz Karimi, Andrew Turpin, and Falk Scholer. 2006. En-
glish to Persian transliteration. In String Processing and In-
formation Retrieval, pages 255?266.
Alexandre Klementiev and Dan Roth. 2006. Weakly super-
vised named entity transliteration and discovery from mul-
tilingual comparable corpora. In Association for Computa-
tional Linguistics, pages 817?824.
Kevin Knight and Jonathan Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(4):599?612.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-
channel model for machine transliteration. In Association
for Computational Linguistics, pages 159?166.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-Korean
transliteration model using pronunciation and contextual
rules. In Conference on Computational Linguistics.
Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of
proper names in cross-language applications. In ACM SIGIR
Conference on Research and Development on Information
Retrieval, pages 365?366.
Dmitry Zelenko and Chinatsu Aone. 2006. Discriminative
methods for transliteration. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language Process-
ing., pages 612?617.
655
Coling 2010: Poster Volume, pages 605?613,
Beijing, August 2010
Best Topic Word Selection for Topic Labelling
Jey Han Lau,?? David Newman,?? Sarvnaz Karimi? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California
jhlau@csse.unimelb.edu.au, newman@uci.edu, skarimi@unimelb.edu.au, tb@ldwin.net
Abstract
This paper presents the novel task of best
topic word selection, that is the selection
of the topic word that is the best label for
a given topic, as a means of enhancing the
interpretation and visualisation of topic
models. We propose a number of features
intended to capture the best topic word,
and show that, in combination as inputs to
a reranking model, we are able to consis-
tently achieve results above the baseline of
simply selecting the highest-ranked topic
word. This is the case both when training
in-domain over other labelled topics for
that topic model, and cross-domain, us-
ing only labellings from independent topic
models learned over document collections
from different domains and genres.
1 Introduction
In the short time since its inception, topic mod-
elling (Blei et al, 2003) has become a main-
stream technique for tasks as diverse as multi-
document summarisation (Haghighi and Vander-
wende, 2009), word sense discrimination (Brody
and Lapata, 2009), sentiment analysis (Titov and
McDonald, 2008) and information retrieval (Wei
and Croft, 2006). For many of these tasks, the
multinomial topics learned by the topic model can
be interpreted natively as probabilities, or mapped
onto a pre-defined discrete class set. However,
for tasks where the learned topics are provided
to humans as a first-order output, e.g. for use in
document collection analysis/navigation, it can be
difficult for the end-user to interpret the rich sta-
tistical information encoded in the topics. This
research is concerned with making topics more
readily human interpretable, by selecting a single
term with which to label the topic.
Although topics are formally a multinomial dis-
tribution over terms, with every term having finite
probability in every topic, topics are usually dis-
played by printing the top-10 terms (i.e. the 10
most probable terms) in the topic. These top-10
terms typically account for about 30% of the topic
mass for reasonable setting of number of topics,
and usually provide sufficient information to de-
termine the subject area and interpretation of a
topic, and distinguish one topic from another.
Our research task can be illustrated via the top-
10 terms in the following topic, learned from a
book collection. Terms wi are presented in de-
scending order of P (wi|tj) for the topic tj :
trout fish fly fishing water angler stream rod
flies salmon
Clearly the topic relates to fishing, and indeed,
the fourth term fishing is an excellent label for the
topic. The task is thus termed best word or most
representative word selection, as we are selecting
the label from the closed set of the top-N topic
words in that topic.
Naturally, not all topics are equally coherent,
however, and the lower the topic coherence, the
more difficult the label selection task becomes.
For example:
oct sept nov aug dec july sun lite adv globe
appears to conflate months with newspaper
names, and no one of these topic words is able to
capture the topic accurately. As such, our method-
ology presupposes an automatic means of rating
topics for coherence. Fortunately, recent research
by Newman et al (2010) has shown that this is
achievable at levels approaching human perfor-
mance, meaning that this is not an unreasonable
assumption.
Labelling topics has applications across a di-
verse range of tasks. Our original interest in the
605
problem stems from work in document collection
visualisation/navigation, and the realisation that
presenting users with topics natively (e.g. as rep-
resented by the top-N terms) is ineffective, and
would be significantly enhanced if we could au-
tomatically predict succinct labels for each topic.
Another application area where labelling has been
shown to enhance the utility of topic models is se-
lectional preference learning via topic modelling
(Ritter et al, to appear). Here, topic labelling via
taxonomic classes (e.g. WordNet synsets) can lead
to better topic generalisation, in addition to better
human readability.
This paper is based around the assumption that
an appropriate label for a topic can be found
among the high-ranking (high probability) terms
in that topic. We assess the suitability of each term
by way of comparison with other high-ranking
terms in that same topic, using simple pointwise
mutual information and conditional probabilities.
We first experiment with a simple ranking method
based on the component scores, and then move
on to using those scores, along with features from
WordNet and from the original topic model, in a
ranking support vector regression (SVR) frame-
work. Our experiments demonstrate that we are
able to perform the task significantly better than
the baseline of selecting the topic word of high-
est marginal probability, including when training
the ranking model on labelled topics from other
document collections.
2 Related Work
Predictably, there has been significant work on in-
terpreting topics in the context of topic modelling.
Topic are conventionally interpreted via the top-
N words in each topic (Blei et al, 2003; Grif-
fiths and Steyvers, 2004), or alternatively by post-
hoc manual labelling of each topic based on do-
main knowledge and subjective interpretation of
each topic (Wang and McCallum, 2006; Mei et
al., 2006).
Mei et al (2007) proposed various approaches
for automatically suggesting phrasal labels for
topics, based on first extracting phrases from the
document collection, and subsequently ranking
the phrases based on KL divergence with a given
topic.
Magatti et al (2009) proposed a method for la-
belling topics induced by hierarchical topic mod-
elling, based on ontological alignment with the
Google Directory (gDir) hierarchy, and optionally
expanding topics based on a thesaurus or Word-
Net. Preliminary experiments suggest the method
has promise, but the method crucially relies on
both a hierarchical topic model and a pre-existing
ontology, so has limited applicability.
Over the general task of labelling a learned se-
mantic class, Pantel and Ravichandran (2004) pro-
posed the use of lexico-semantic patterns involv-
ing each member of that class to learn a (usu-
ally hypernym) label. The proposed method was
shown to perform well over the semantically ho-
mogeneous, fine-grained clusters learned by CBC
(Pantel and Lin, 2002), but for the coarse-grained,
heterogeneous topics learned by topic modelling,
it is questionable whether it would work as well.
The first works to report on human scoring of
topics were Chang et al (2009) and Newman et
al. (2010). The first study used a novel but syn-
thetic intruder detection task where humans eval-
uate both topics (that had an intruder word), and
assignment of topics to documents (that had an in-
truder topic). The second study had humans di-
rectly score topics learned by a topic model. This
latter work introduced the pointwise mutual infor-
mation (PMI) score to model human scoring. Fol-
lowing this work, we use PMI as features in the
ranking SVR model.
3 Methodology
Our task is to predict which words annotators
tend to select as most representative or best words
when presented with a list of ten words. Since
annotators are not generally unanimous in their
choice of best word, we formulate this as a rank-
ing task, and treat the top-1, 2 and 3 system-
ranked items as the best words, and compare that
to the top-1, 2 and 3 words chosen most frequently
by annotators. In this section, we describe the fea-
tures that may be useful for this ranking task. We
start with features motivated by word association.
An obvious idea is that the most representative
word should be readily evoked by other words
in the topic. For example, given a list of words
?space, earth, moon, nasa, mission?, which is a
606
Space Exploration topic, space could arguably be
the most representative word. This is because
it is natural to think about the word space after
seeing the words earth, moon and nasa individ-
ually. A good candidate for best word could be
the word that has high average conditional proba-
bility given each of the other words. To calculate
conditional probability, we use word counts from
the entire collection of English Wikipedia articles.
Conditional probability is defined as:
P (wi|wj) =
P (wi, wj)
P (wj)
,
where i 6= j and P (wi, wj) is the probability of
observing both wi and wj in the same sliding win-
dow, and P (wi) is the overall probability of word
wi in the corpus. In the above example, evoked by
means that space would fill the slot of wi. The av-
erage conditional probability for word wi is given
by:
avg-CP1(wi) = 19
?
j
P (wi|wj),
for j = 1 . . . 10, j 6= i (this range of indices ap-
plies to all following average quantities).
In other cases, we have the flip situation, where
the most representative word may evoke (rather
than be evoked by) other words in the list of ten
words. Imagine a NASCAR Racing topic, which
has a list of words ?race, car, nascar, driver, rac-
ing?. Given the word nascar, words from the list
such as race, car, racing and driver might come
to mind because nascar is heavily associated with
these words. Therefore, a good candidate, wi,
might also correlate with high P (wj |wi). As be-
fore, the average conditional probability (here de-
noted with CP2) for word wi is given by:
avg-CP2(wi) = 19
?
j
P (wj |wi).
Another approach to measuring word associa-
tion is by calculating pointwise mutual informa-
tion (PMI) between word pairs. Unlike condi-
tional probability, PMI is symmetric and thus the
order of words in a pair does not matter. We
calculate PMI using word counts from English
Wikipedia as follows:
PMI(wi, wj) = log P (wi, wj)P (wi)P (wj) .
The average PMI for word wi is given by:
avg-PMI(wi) = 19
?
j
PMI(wi, wj).
The topic model produces an ordered list of
words for each topic, and the ordering is given by
the marginal probability of each word given that
topic, P (wi|tj). The ranking of words based on
these probabilities indicates the importance of a
word in a topic, and it is also a feature that we use
for predicting the most representative word.
We also observe that sometimes the most repre-
sentative words are generalized concepts of other
words. As such, hypernym relations could be an-
other feature that may be relevant to predicting the
best word. To this end, we use WordNet to find
hypernym relations between pairs of words in a
topic and obtain a set of boolean-valued relation-
ships for each topic word.
Our last feature is the distributional similar-
ity scores of Pantel et al (2009), as trained over
Wikipedia.1 This takes the form of representing
the distributional similarity between each pairing
of terms sim(wi|wj); if wi is not in the top-200
most similar terms for a given wj , we assume it to
have a similarity of 0.
While the above features can be used alone
to get a ranking on the ten topic words, we can
also use various combinations of features in a
reranking model such as support vector regres-
sion (SVMrank: Joachims (2006)). Applying the
features described above ? conditional probabil-
ities, PMI, WordNet hypernym relations, the topic
model word rank, and Pantel?s distributional simi-
larity score ? as features for SVMrank, a ranking
of words is produced and candidates for the most
representative word are selected by choosing the
top-ranked words.
607
NEWS stock market investor fund trading investment firm exchange ...
police gun officer crime shooting death killed street victim ...
food restaurant chef recipe cooking meat meal kitchen eat...
patient doctor medical cancer hospital heart blood surgery ...
BOOKS loom cloth thread warp weaving machine wool cotton yarn ...
god worship religion sacred ancient image temple sun earth ...
crop land wheat corn cattle acre grain farmer manure plough ...
sentence verb noun adjective grammar speech pronoun ...
Figure 1: Selected topics from the two collections
(each line is one topic, with fewer than ten topic
words displayed because of limited space)
4 Datasets
We used two collections of text documents from
different genres for our experiments. The first col-
lection (NEWS) was created by selecting 55,000
news articles from the LDC Gigaword corpus.
The second collection (BOOKS) was 12,000 En-
glish language books selected from the Inter-
net Archive American Libraries collection. The
NEWS and BOOKS collections provide a diverse
range of content for topic modeling. In the first
case ? news articles from the past decade written
by journalists ? each article usually attempts to
clearly and concisely convey information to the
reader, and hence the learned topics tend to be
fairly interpretable. For BOOKS (with publication
dates spanning more than a century), the writing
style often uses lengthy and descriptive prose, so
one sees a different style to the learned topics.
The input to the topic model is a bag-of-words
representation of the collection of text documents,
where word counts are preserved, but word order
is lost. After performing fairly standard tokeniza-
tion and limited lemmatisation, and creating a vo-
cabulary of terms that occurred at least ten times,
each corpus was converted into its bag-of-words
representation. We learned topic models for the
two collections, choosing a setting of T = 200
topics for NEWS and T = 400 topics for BOOKS.
After computing the PMI-score for each topic (ac-
cording to Newman et al (2010)), we selected 60
topics with high PMI-score, and 60 topics with
low PMI-score, from both corpora, resulting in a
total of 240 topics for human evaluation.
The 240 topics selected for human scoring were
1Accessed from http://demo.patrickpantel.
com/Content/LexSem/thesaurus.htm.
Features Description
PMI Pointwise mutual information
CP1 Conditional probability P (wi|?)
CP2 Conditional probability P (?|wi)
TM Rank Original topic model word rank
Hypernym WordNet hypernym relationships
PDS Pantel distributional similarity score
Table 1: Description of feature sets
each evaluated by between 10 and 20 users. For
the two topic models, we used the conventional
approach of displaying each topic with its top-10
terms. In a typical survey, a user was asked to
evaluate anywhere from 60 to 120 topics. The in-
structions asked the user to perform the following
tasks, for each topic in the survey: (a) score the
topic for ?usefulness? or ?coherence? on a scale
of 1 to 3; and (b) select the single best word that
exemplifies the topic (when score=3).
From both NEWS and BOOKS, the 40 topics
with the highest average human scores had rela-
tively complete data for the ?best word? selection
task (i.e. every time a user gave a topics score=3,
they also selected a ?best word?). The remain-
der of this paper is concerned with the 40 NEWS
topics and 40 BOOKS topics where we had ?best
word? data from the annotators. Sample topics
from these two sets are given in Figure 1.
To measure presentational bias (i.e. the extent
to which annotators tend to choose a word seen
earlier rather than later, particularly when armed
with the knowledge that words are presented in or-
der of probability), we reissued a survey using the
40 NEWS topics to ten additional annotators, but
this time the top-10 topic words were presented
in random order. Again, these ten new annotators
were asked to select the best word.
5 Experiments
We used average PMI and conditional probabili-
ties, CP1 and CP2, to rank the ten words in each
topic. Candidates for the best words were selected
by choosing the top-1, 2 and 3 ranked words.
We used the following weighted scoring func-
tion for evaluation:
Best-N score =
?N
i=1 n(wrevi)?N
i=1 n(wi)
608
Features Best-1 Best-2 Best-3
Baseline 0.35 0.50 0.59
PMI 0.25 0.38 0.49
CP1 0.30 0.42 0.51
CP2 0.15 0.27 0.45
Upper bound 0.48 ? ?
Table 2: Best-1,2,3 scores for ranking with single
feature sets (PMI and both conditional probabili-
ties) for NEWS
Features Best-1 Best-2 Best-3
Baseline 0.38 0.48 0.60
PMI 0.25 0.38 0.49
CP1 0.30 0.38 0.47
CP2 0.15 0.30 0.49
Upper bound 0.64 ? ?
Table 3: Best-1,2,3 scores for ranking with single
feature sets (PMI and both conditional probabili-
ties) for BOOKS
where wrevi is the ith term ranked by the system
and wi is the ith most popular term selected by
annotators; revi gives the index of the word wi
in the annotator?s list; and n(w) is the number of
votes given by annotators for word w.
The baseline is obtained using the original word
rank produced by the topic model based on topic
word probabilities P (wi|tj). An upperbound is
calculated by evaluating the decision of an annota-
tor against others for each topic. This upperbound
signifies the maximum accuracy for human anno-
tators on average; since the annotators were asked
to pick a single best word in the survey, only the
Best-1 upperbound can be obtained.
The Best-1/2/3 results are summarized in Ta-
ble 2 for NEWS and Table 3 for BOOKS. These
Best-N scores are computed just using the single
feature of PMI, CP1 and CP2 (each in turn) to rank
the words in each topic. None of these features
alone produces a result that exceeds baseline per-
formance.
To make better use of all the features described
in Section 3, namely the PMI score, conditional
probabilities (both directions), topic model word
rank, WordNet Hypernym relationships and Pan-
tel?s distributional similarity score, we build a
ranking classifier using SVMrank and evaluating
Feature Set Best-1 Best-2 Best-3
Baseline 0.35 0.50 0.59
All Features 0.43 0.56 0.62
?PMI 0.45 (+0.02) 0.52 (?0.04) 0.62 (?0.00)
?CP1 0.35 (?0.08) 0.49 (?0.07) 0.57 (?0.05)
?CP2 0.40 (?0.03) 0.50 (?0.06) 0.61 (?0.01)
?TM Rank 0.40 (?0.03) 0.52 (?0.04) 0.57 (?0.05)
?Hypernym 0.43 (?0.00) 0.57 (+0.01) 0.62 (?0.00)
?PDS 0.43 (?0.00) 0.53 (?0.03) 0.62 (?0.00)
Upper bound 0.48 ? ?
Table 4: SVR-based best topic word results for
NEWS for all six feature types, and feature abla-
tion over each (numbers in brackets show the rel-
ative change over the full feature set)
Feature Set Best-1 Best-2 Best-3
Baseline 0.38 0.48 0.60
All Features 0.40 0.51 0.62
?PMI 0.38 (?0.02) 0.51 (?0.00) 0.63 (+0.01)
?CP1 0.33 (?0.07) 0.47 (?0.04) 0.56 (?0.06)
?CP2 0.40 (?0.00) 0.50 (?0.01) 0.64 (+0.02)
?TM Rank 0.35 (?0.05) 0.49 (?0.02) 0.63 (+0.01)
?Hypernym 0.40 (?0.00) 0.50 (?0.01) 0.61 (?0.01)
?PDS 0.45 (+0.05) 0.48 (?0.03) 0.67 (+0.05)
Upper bound 0.64 ? ?
Table 5: SVR-based best topic word results for
BOOKS for all six feature types, and feature abla-
tion over each (numbers in brackets show the rel-
ative change over the full feature set)
using 10-fold cross validation. Our first approach
is to use the entire set of features to train the clas-
sifier. Following this, we also measure the effect
of each feature by ablating (removing) one fea-
ture at a time. The drop in Best-N score indicates
which features are the strongest predictors of the
best words (a larger drop in score indicates that
feature is more important). The results for Best-1,
Best-2 and Best-3 scores are summarized in Ta-
ble 4 for NEWS, and Table 5 for BOOKS (averaged
across the 10 iterations of cross validation).
We then produced a condensed set of features,
consisting of the conditional probabilities, the
original topic model word rank and the WordNet
hypernym relationships. This ?best? set of fea-
tures is used to make predictions of best words.
Results are improved in most cases, and are sum-
marized in Table 6 for both NEWS and BOOKS.
609
Dataset Best-1 Best-2 Best-3
NEWS
Baseline 0.35 0.50 0.59
Best Feat. Set 0.45 0.50 0.65
Upper bound 0.48 ? ?
BOOKS
Baseline 0.38 0.48 0.60
Best Feat. Set 0.48 0.56 0.66
Upper bound 0.64 ? ?
Table 6: Results with the best feature set com-
pared to the baseline
Dataset Best-1 Best-2 Best-3
NEWS baseline 0.35 0.50 0.59
BOOKS ? NEWS 0.38 0.56 0.62
NEWS upper bound 0.48 ? ?
BOOKS baseline 0.38 0.48 0.60
NEWS ? BOOKS 0.48 0.56 0.65
BOOKS upper bound 0.64 ? ?
Table 7: Results for cross-domain learning
We also tested whether the SVM classifier
could be trained using data from one domain, and
run on data from another domain. Using our two
datasets as these different domains, we trained a
model using BOOKS data and made predictions
for NEWS, and then we trained a model using
NEWS data and made predictions for BOOKS.
The results, shown in Table 7, indicate that
we are still able to outperform the baseline, even
when the ranking classifier is trained on a differ-
ent domain. In fact, when we trained a model
using NEWS, we saw almost no drop in perfor-
mance for predicting best words for BOOKS, and
improvement is seen for Best-2 score from NEWS.
This implies that the SVM classifier generalizes
well across domains and suggests the possibility
of having a fixed training model to predict best
words for any data.
In these experiments, topic words are presented
in the original order that the topic model produces,
i.e. in descending order of probability of a word
under a topic P (wi|tj). We noticed that the first
words of the topics are frequently selected as the
best words by annotators, and suspected that this
was introducing a bias towards the first word. As
our baseline scores are derived from this topic
word ordering, such a bias could give rise to an
artificially high baseline.
To investigate this effect, we ran a second anno-
Word Order Best-1 Best-2 Best-3
Original 0.35 0.50 0.59
Randomized 0.23 0.33 0.46
Table 8: Reduction of baseline scores for NEWS
when words are presented in random order to an-
notators.
2 4 6 8 10
0.
0
0.
1
0.
2
0.
3
0.
4
Rank
Fr
ac
tio
n 
of
 h
um
an
 s
el
ec
te
d 
be
st
 w
or
d
ordered
random
Figure 2: Bias for humans selecting the best word,
when the topic words are presented in their origi-
nal ordering (ordered) or randomised (random)
tation exercise over the same set of topics (but dif-
ferent annotators), to obtain a new set of best word
annotations for NEWS, with the topic words pre-
sented in random order. In Figure 2, we plot the
cumulative proportion of words selected as best
word by the annotators across the topics, in the
case of the random topic word order, mapping the
topic words back onto their original ranks in the
topic model. A slight drop can be observed in the
proportion of first- and second-ranked topic words
being selected when we randomise the topic word
order. When we recalculate the baseline accuracy
for NEWS on the basis of the new set of annota-
tions, we observe an appreciable drop in the scores
(see Table 8).
6 Discussion
From the experiments in Section 5, perhaps the
first thing to observe is: (a) the high performance
of the baseline, and (b) the relatively low (Best-
1) upper bound accuracy for the task. The first is
perhaps unsurprising, given that it represents the
610
topic model?s own interpretation of the word(s)
which are most representative of that topic. In this
sense, we have set our sights high in attempting to
better the baseline. The upper bound accuracy is
a reflection of both the inter-annotator agreement,
and the best that we can meaningfully expect to
do for the task. That is, any result higher than this
would paradoxically suggest that we are able to do
better at a task than humans, where we are evalu-
ating ourselves relative to the labellings of those
humans. The upper bound for NEWS was slightly
less than 0.5, indicating that humans agree on the
best topic word only 50% of the time. To better
understand what is happening here, consider the
following topic from Figure 1:
health drug patient medical doctor hospital
care cancer treatment disease
This is clearly a coherent topic, but at least two
topic words suggest themselves as labels: health
and medical. By way of having between 10 and 20
annotators (uniquely) label a given topic, and in-
terpreting the multiple labellings probabilistically,
we are side-stepping the inter-annotator agree-
ment issue, but ultimately, for the Best-1 evalu-
ation, we are forced to select one term only, and
consider any alternative to be wrong. Because an-
notators selected only one best topic word, we un-
fortunately have no way of performing Best-2 or
Best-3 upper bound evaluation and deal with top-
ics such as this, but would expect the numbers to
rise appreciably.
Looking at the original feature rankings in Ta-
bles 2 and 3, no clear picture emerges as to which
of the three methods (PMI, CP1 and CP2) was
most successful, but there were certainly clear dif-
ferences in the relative numbers for each, point-
ing to possible complementarity in the scoring.
This expectation was born out in the results for
the reranking model in Tables 4 and 5, where the
combined feature set surpassed the baseline in all
cases, and feature ablation tended to lead to a drop
in results, with the single most effective feature set
being CP1 (P (wi|?)), followed by CP2 (P (?|wi))
and topic model rank. The lexical semantic fea-
tures of WordNet hypernymy and PDS (Pantel?s
distributional similarity) were the worst perform-
ers, often having no or negative impact on the re-
sults.
Comparing the best results for the SVR-based
reranking model and the upper bound Best-1
score, we approach the upper bound performance
for NEWS, but are still quite a way off with
BOOKS when training in-domain. This is encour-
aging, but a slightly artificial result in terms of the
broader applicability of this research, as what it
means in practical terms is that if we can access
multi-annotator best word labelling for the ma-
jority of topics in a given topic model, we can
use those annotations to predict the best word for
the remainder of the topics with reasonably suc-
cess. When we look to the cross-domain results,
however, we see that we almost perfectly replicate
the best-achieved Best-1, Best-2 and Best-3 in-
domain results for BOOKS by training on NEWS
(making no use of the annotations for BOOKS).
Applying the annotations for BOOKS to NEWS is
less successful in terms of Best-1 accuracy, but we
actually achieve higher Best-2, and largely mir-
ror the Best-3 results as compared to the best of
the in-domain results in Table 6. This leads to
the much more compelling conclusion that we can
take annotations from an independent topic model
(based on a completely unrelated document col-
lection), and apply them to successfully model the
best topic word for a new topic model, without
requiring any additional annotation. As we now
have two sets of topics multiply-annotated for best
words, this result suggests that we can perform the
best topic word selection task with high success
over novel topic models.
We carried out manual analysis of topics where
the model did particularly poorly, to get a sense
for how and where our model is being led astray.
One such example is the topic:
race car nascar driver racing cup winston team
gordon season
where the following topic words were selected by
our annotators: nascar (8 people), race (2 peo-
ple), and racing (2 people). First, we observe the
split between race and racing, where more judi-
cious lemmatisation/stemming would make both
the annotation easier and the evaluation cleaner.
The SVR model tends to select more common,
general terms, so in this case chose race as the
best word, and ranked nascar third. This is one
611
instance were nascar evokes all of the other words
effectively, but not conversely (racing is asso-
ciated with many events/sports beyond nascar,
e.g.).
Another topic where our model had difficulty
was:
window nave aisle transept chapel tower arch
pointed arches roof
where our best model selected nave, while the hu-
man annotators selected chapel (6 people), arch
(2 people), nave, roof , tower and transept (1 per-
son each). Clearly, our annotators struggled to
come up with a best word here, despite the topic
again being coherent. This is an obvious candi-
date for labelling with a hypernym/holonym of
the topic words (e.g. church or church architec-
ture), and points to the limitations of best word la-
belling ? there are certainly many topics where
best word labelling works, as our upper bound
analysis demonstrated, but there are equally many
topics where the most natural label is not found
in the top-ranked topic words. While this points
to slight naivety in the current task set up ? we
are forcing annotators to label words with topic
words, where we know that this is sub-optimal
for a significant number of topics ? we contend
that our numbers suggest that: (a) consistent best
topic word labelling is possible at least 50% of
the time; and (b) we have developed a method
which is highly adept at labelling these topics. As
a way forward, we intend to relax the constraint
on the topic label needing to be based on a topic
word, and explore the possibility of predicting
which topics are best labelled with topic words,
and which require independent labels. For topics
which can be labelled with topic words, we can
use the methodology developed here, and for top-
ics where this is predicted to be sub-optimal, we
intend to build on the work of Mei et al (2007),
Pantel and Ravichandran (2004) and others in se-
lecting phrasal/hypernym labels for topics. We are
also interested in applying the methodology pro-
posed herein to the closely-related task of intruder
word, or worst topic word, detection, as proposed
by Chang et al (2009).
Finally, looking to the question of the impact of
the presentation order of the topic words on best
word selection, it would appear that our baseline
is possibly an over-estimate (based on Table 8).
Having said that, the flipside of the bias is that it
leads to more consistency in the annotations, and
tends to help in tie-breaking of examples such as
race and racing from above, for example. In sup-
port of this claim, the upper bound Best-1 accu-
racy of the randomised annotations, relative to the
original gold-standard is 0.44, slightly below the
original upper bound for NEWS. More work is
needed to determine the real impact of this bias
on the overall task setup and evaluation.
7 Conclusion
This paper has presented the novel task of best
topic word selection, that is the selection of the
topic word that is the best label for a given topic.
We proposed a number of features intended to
capture the best topic word, and demonstrated
that, while they were relatively unsuccessful in
isolation, in combination as inputs to a rerank-
ing model, we were able to consistently achieve
results above the baseline of simply selecting the
highest-ranked topic word, both when training in-
domain over other labelled topics for that topic
model, and cross-domain, using only labellings
from independent topic models learned over docu-
ment collections from different domains and gen-
res.
Acknowledgements
NICTA is funded by the Australian government as repre-
sented by Department of Broadband, Communication and
Digital Economy, and the Australian Research Council
through the ICT centre of Excellence programme. DN has
also been supported by a grant from the Institute of Museum
and Library Services, and a Google Research Award.
References
Blei, D.M., A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Brody, S. and M. Lapata. 2009. Bayesian word sense
induction. In Proceedings of the 12th Conference
of the EACL (EACL 2009), pages 103?111, Athens,
Greece.
Chang, J., J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans
interpret topic models. In Proceedings of the 23rd
612
Annual Conference on Neural Information Process-
ing Systems (NIPS 2009), pages 288?296, Vancou-
ver, Canada.
Griffiths, T. and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy
of Sciences, 101:5228?5235.
Haghighi, A. and L. Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics ?
Human Language Technologies 2009 (NAACL HLT
2009), pages 362?370, Boulder, USA.
Joachims, T. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217?226, Philadelphia, USA.
Magatti, D., S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In Proceedings of the
International Conference on Intelligent Systems De-
sign and Applications, pages 1227?1232, Pisa, Italy.
Mei, Q., C. Liu, H. Su, and C. Zhai. 2006. A prob-
abilistic approach to spatiotemporal theme pattern
mining on weblogs. In Proceedings of the 15th
International World Wide Web Conference (WWW
2006), pages 533?542.
Mei, Q., X. Shen, and C. Zhai. 2007. Automatic la-
beling of multinomial topic models. In Proceedings
of the 13th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2007), pages 490?499, San Jose, USA.
Newman, D., J.H. Lau, K. Grieser, and T. Baldwin.
2010. Automatic evaluation of topic coherence.
In Proceedings of Human Language Technologies:
The 11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL HLT 2010), pages 100?108, Los
Angeles, USA.
Pantel, P. and D. Lin. 2002. Discovering word
senses from text. In Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 613?619, Ed-
monton, Canada.
Pantel, P. and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
the 4th International Conference on Human Lan-
guage Technology Research and 5th Annual Meet-
ing of the NAACL (HLT-NAACL 2004), pages 321?
328, Boston, USA.
Pantel, P., E. Crestan, A. Borkovsky, A-M. Popescu,
and V. Vyas. 2009. Web-scale distributional sim-
ilarity and entity set expansion. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2009), pages
938?947, Singapore.
Ritter, A, Mausam, and O Etzioni. to appear. A la-
tent Dirichlet alocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meeting
of the ACL (ACL 2010), Uppsala, Sweden.
Titov, I. and R. McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th International World Wide Web
Conference (WWW 2008), pages 111?120, Beijing,
China.
Wang, X. and A. McCallum. 2006. Topics over time:
A non-Markov continuous-time model of topical
trends. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD 2006), pages 424?433,
Philadelphia, USA.
Wei, S. and W.B. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of 29th
International ACM-SIGIR Conference on Research
and Development in Information Retrieval (SIGIR
2006), pages 178?185, Seattle, USA.
613
