Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 648?657,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Improving Web Search Relevance with Semantic Features
Yumao Lu Fuchun Peng Gilad Mishne Xing Wei Benoit Dumoulin
Yahoo! Inc.
701 First Avenue
Sunnyvale, CA, 94089
yumaol,fuchun,gilad,xwei,benoitd@yahoo-inc.com
Abstract
Most existing information retrieval (IR)
systems do not take much advantage of
natural language processing (NLP) tech-
niques due to the complexity and limited
observed effectiveness of applying NLP
to IR. In this paper, we demonstrate that
substantial gains can be obtained over a
strong baseline using NLP techniques, if
properly handled. We propose a frame-
work for deriving semantic text matching
features from named entities identified in
Web queries; we then utilize these features
in a supervised machine-learned ranking
approach, applying a set of emerging ma-
chine learning techniques. Our approach
is especially useful for queries that contain
multiple types of concepts. Comparing to
a major commercial Web search engine,
we observe a substantial 4% DCG5 gain
over the affected queries.
1 Introduction
Most existing IR models score documents pri-
marily based on various term statistics. In tra-
ditional models?from classic probabilistic mod-
els (Croft and Harper, 1979; Fuhr, 1992), through
vector space models (Salton et al, 1975; Narita
and Ogawa, 2000), to well studied statistical lan-
guage models (Ponte and Croft, 2000; Lafferty
and Zhai, 2001)?these term statistics have been
captured directly in the ranking formula. More re-
cently, learning to rank approaches to IR (Fried-
man, 2002) have become prominent; in these
frameworks, that aim at learning a ranking func-
tion from data, term statistics are often modeled
as term matching features in a machine learning
process.
Traditional text matching features are mainly
based on frequencies of n-grams of the user?s
query in a variety of document sections, such as
the document title, body text, anchor text, and so
on. Global information such as frequency of term
or term group in the corpus may also be used, as
well as its combination with local statistics ? pro-
ducing relative scores such as tf ? idf or BM25
scores (Robertson et al, 1995). Matching may
be restricted to certain window sizes to enforce
proximity, or may be more lenient, allowing un-
ordered sequences and nonconsecutive sequences
for a higher recall.
Even before machine learning was applied to
IR, NLP techniques such as Named Entity Recog-
nition (NER), Part-of-Speech (POS) tagging, and
parsing have been applied to both query model-
ing and document indexing (Smeaton and van Ri-
jsbergen, 1988; Narita and Ogawa, 2000; Sparck-
Jones, 1999). For example, statistical concept
language models generalize classic n-gram mod-
els to concept n-gram model by enforcing query
term proximity within each concept (Srikanth and
Srihari, 2003). However, researchers have of-
ten reported limited gains or even decreased per-
formance when applying NLP to IR (Voorhees,
1999).
Typically, concepts detected through NLP tech-
niques either in the query or in documents are
used as proximity constraints for text match-
ing (Sparck-Jones, 1999), ignoring the actual con-
cept type. The machine learned approach to docu-
ment ranking provides us with an opportunity to
revisit the manner in which NLP information is
used for ranking. Using knowledge gained from
NLP application as features rather than heuris-
tically allows us much greater flexibility in the
amount and variability of information used ? e.g.,
incorporating knowledge about the actual entity
types. This has several benefits: first, entity types
appearing in queries are an indicator of the user?s
intent. A query consisting of a business category
and a location (e.g., hotels Palo Alto) appears to be
648
informational, and perhaps is best answered with
a page containing a list of hotels in Palo Alto.
Queries containing a business name and a location
(e.g., Fuki Sushi Palo Alto) are more navigational
in nature ? for many users, the intent is finding the
home page of a specific business. Similarly, entity
types appearing in documents are an indicator of
the document type. For example, if ?Palo Alto?
appears ten times in document?s body text, it is
more likely to be a local listing page than a home
page. For the query hotels Palo Alto, a local listing
page may be a good page, while for the query Fuki
Sushi Palo Alto a listing page is not a good page.
In addition, knowledge of the particular entities
in queries allows us to incorporate external knowl-
edge about these entities, such as entity-specific
stopwords (?inc.? as in Yahoo Inc. or ?services?
as in kaiser medical service), and so on.
Finally, even when using named entities only
for deriving proximity-related features, we can
benefit from applying different levels of proxim-
ity for different entities. For example, for enti-
ties like cities (e.g., ?River Side?), the proximity
requirement is fairly strict: we should not allow
extra words between the original terms, and pre-
serve their order. For other entities the proximity
constraint can be relaxed?for example, for per-
son names, due to the middle name convention:
Hillary Clinton vs. Hillary R. Clinton.
In this paper, we propose a systematic approach
to modeling semantic features, incorporating con-
cept types extracted from query analysis. Ver-
tical attributes, such as city-state relationships,
metropolitan definition, or idf scores from a do-
main specific corpus, are extracted for each con-
cept type from vertical database. The vertical at-
tributes, together with the concept attributes, are
used to compose a set of semantic features for ma-
chine learning based IR models. A few machine
learning techniques are discussed to further im-
prove relevance for subclass of difficult queries
such as queries containing multiple types of con-
cepts. Figure 1 shows an overview of our ap-
proach; after discussing related work in Section 2,
we spend Sections 3 to 5 of the paper describing
the components of our system. We then evaluate
the effectiveness of our approach both using gen-
eral queries and with a set of ?difficult? queries;
our results show that the techniques are robust, and
particularly effective for this type of queries. We
conclude in Section 7.
Tagger1 Tagger2 Tagger n
Resolution Module
Query
with
Annotations
Query
Query
Linguistic
Analysis
Vertical AttributeLocation 
DB
Vertical AttributeBusiness
DB
Vertical Attribute
Semantic Text Matching
Document 
Index
Semantic Features
Specialized
Ranking
Module
Specialized
RankingModule
Specialized
RankingModule
...
DB
Name
...
......
Figure 1: Ranking with Semantic Features
2 Related Work
There is substantial body of work involving us-
age of NLP techniques to improve information re-
trieval (Brants, 2003; Strzalkowski et al, 1996).
Allan and Ragahavan (Allan and Raghavan, 2002)
use Part-of-Speech tagging to reduce ambiguity
of difficult queries by converting short queries
to questions. In other POS-tags work, Aram-
patzis et al (Arampatzis et al, 1990) observed
an improvement when using nouns only for re-
trieval. Croft et al (Croft et al, 1991) and Tong
et al (Buckley et al, 1993; Tong et al, 1996) ex-
plored phrases and structured queries and found
phrases are effective in improving retrieval per-
formance. Voorhees (Voohees, 1993) uses word
sense disambiguation to improve retrieval perfor-
mance. One IR domain that consistently benefits
from usage of various NLP techniques is question
answering, where queries are formed in natural
language format; e.g., (Peng et al, 2005).
In general, however, researchers often observe
limited gains or even degraded performance when
applying NLP to IR (Voorhees, 1999). Having
said this, most past studies use small datasets and
a modest baseline; it is unclear whether a similar
conclusion would be reached when using a state-
of-art system such as a commercial web search
engine as a baseline, and a full-web corpus ? as
we do in this paper. This leads to another differ-
ence between this work and existing work involv-
ing named entity recognition for retrieval. Most
649
previous research on usage of named entities in
IR combines entity detection in documents and
queries (Prager et al, 2000). Entity detection in
document has a high indexing cost that is often
overlooked, but cannot be ignored in the case of
commercial search engines. For this reason, we
restrict NLP processing to queries only ? although
we believe that document-side NLP processing
will provide additional useful information.
3 Query Analysis
We begin by briefly describing our approach to
named entity recognition in web queries, which
serves as the basis for deriving the semantic text
matching features.
Named entity recognition (NER) is the task of
identifying and classifying entities, such as per-
son names or locations, in text. The majority of
state-of-the-art NER methods utilize a statistical
approach, attempting to learn a mapping between
a sequence of observations (words) and a sequence
of tags (entity types). In these methods, the se-
quential nature of the data is often central to the
model, as named entities tend to appear in particu-
lar context in text. For example, for most types of
text, in the two sequences met with X and buy the
Y, the likelihood of X being a person name is sub-
stantially higher than the corresponding likelihood
of Y . Indeed, many named entity taggers perform
well when applied to grammatical text with suf-
ficient contexts, such as newswire text (Sang and
Meulder, 2003).
Web queries, however, tend to be short, with
most queries consisting of 1?3 words, and lack
context ? posing a particular challenge for iden-
tifying named entities in them. Existing work on
NER in web queries focuses on tailoring a solu-
tion for a particular entity type and its usage in
web search (Wang et al, 2005; Shen et al, 2008);
in contrast, we aim at identifying a large range
of possible entities in web queries, and using a
generic solution for all of them.
In web queries, different entity types may bene-
fit from different detection techniques. For exam-
ple, an entity type with a large variability among
instances as well as existence of external resources
like product name calls for an approach that can
make use of many features, such as a conditional
random field; for entity types that are more struc-
tured like person names, a grammar-based ap-
proach can be more effective (Shen et al, 2008).
To this end, we utilize multiple approaches for en-
tity detection and combine them into a single, co-
herent ?interpretation? of the query.
Given a query, we use several entity recogniz-
ers in parallel, one for each of the common en-
tity types found in web queries. The modeling
types may differ between the recognizers: some
are Markovian models, while others are just dic-
tionary lookups; the accuracy of each recognizer
is also different. We then have a machine-learned
disambiguation module that combines output from
different taggers, ranking the tagging sequences.
The details of scoring is out of the scope of this
paper, and we omit it for simplicity.
4 Semantic Text Matching Features
Our proposed semantic features operate at the
semantic type level rather than at the term level:
instead of matching a term (or set of terms) in doc-
uments, we match their semantic type. Given the
query San Francisco colleges and the annotation
[San Francisco]
CityName
[colleges]
BusinessCategory
,
the semantic text matching features would de-
scribe how relevant a document section is for a en-
tity of type CityName, for BusinessCategory,
and for their combination.
Concretely, we exploit a set of features that
attempts to capture proximity, general relevance,
and vertical relevance for each type of semantic
tag and for each section of the document. We now
review these feature by their broad types.
4.1 Semantic Proximity Features
Proximity features?features that capture the de-
gree to which search terms appear close to each
other in a document?are among the most impor-
tant feature sets in ranking functions. Traditional
proximity features are typically designed for all
query terms (Metzler and Croft, 2005) and may
suffer from wrong segmentations of the query. For
example, for the query New York city bus char-
ter, a traditional proximity feature may treat ?city
bus? similarly to ?York city.? But given detailed
information about the entities in the query in their
types, we can enforce proximity for ?New York
city? and ?bus charter? more accurately. Different
types of entities usually have different proximity
characteristics in relevant documents. Strongly-
bound entities such as city names typically have
very high proximity in relevant documents, while
entities such as business names may have much
650
lower proximity: a search for Kaiser medical of-
fice, for example, may be well-served with docu-
ments referring to Kaiser Permanente medical of-
fice, and as we mentioned before, person names
matches may also benefit from lenient proximity
enforcement. This is naturally addressed by treat-
ing each entity type differently.
We propose a set of semantic proximity fea-
tures that associate each semantic tag type with
generic proximity measures. We also consider tag-
ging confidence together with term group proxim-
ity; we discuss these two approaches next.
4.1.1 Semantic Minimum Coverage (SMC)
Minimum Coverage (MC) is a popular span based
proximity distance measure, which is defined as
the length of the shortest document segment that
cover the query term at least once in a docu-
ment (Tao and Zhai, 2007). We extend this mea-
sure to Semantic Minimum Coverage (SMC) for
each semantic type t in document section s and
define it as
SMC
t,s
=
1
|{k|T
k
= t}|
?
i?{k|T
k
=t}
w
i
MC
i,s
,
where w
i
is a weight for tagged term group i,
MC
i,s
is the the minimum coverage of term group
i in document section s, {k|T
k
= t} denotes the
set of all concepts having type t, and |{k|T
k
= t}|
is the size of the set. The definition of the weight
w is flexible. We list a few candidate weight-
ing schemes in this paper: uniform weights (wu),
weights based on idf scores (widf) and ?strength?-
based weight (ws), which we define as follows:
w
u
= 1;
w
idf
=
c
f
q
where c is a constant and f
q
is the frequency of the
term group in a large query log;
w
s
= min
l
MI
l
where MI
l
is the point-wise mutual information of
the l-th consecutive pair within the semantic tag.
We can also combine strength and idf scores such
that the weight reflects both relative importance
and constraints in proximity. In this paper, we use
w
si
= w
s
w
idf
.
In Section 6, we use all four weighting schemes
mentioned above in the semantic feature set.
4.1.2 Semantic Moving Average BM25
(SMABM25)
BM25, a commonly-used bag-of-words relevance
estimation method (Robertson et al, 1995), is de-
fined (when applied to document sections) as
BM25 =
?
j
idf
j
f
j,s
(c
1
+ 1)
f
i,s
+ c
1
(1? c
2
+ c
2
l
s
?
l
s
)
where f
j,s
is the frequency of term j in section s,
l
s
is the length of section s, ?l
s
is the average length
of document section s, c
1
, c
2
, c
3
are constants and
the idf score of term j is defined as
idf
j
= log
c
4
? d
j
+ c
5
d
j
+ c
5
,
where d
j
is the number of sections in all collec-
tions that contains term j and c
4
, c
5
are constants.
To characterize proximity, we could use a fixed
length sliding window and calculate the average
BM25. We further associate each sliding average
BM25 with each type of semantic term groups.
This results in a Semantic Moving Average BM25
(SMABM25) of type t, which we define as fol-
lows:
1
|{k|T
k
= t}|
?
i?{k|T
k
=t}
(1/M)
?
m
BM25
m
where m is a fixed length sliding window m and
M is the total number of sliding windows (that de-
pends on the length of the section window size).
4.2 Semantic Vertical Relevance Features
Vertical databases contain a large amount of struc-
tured domain knowledge typically discarded by
traditional web relevance features. Having access
to the semantic types in queries, we can tap into
that knowledge to improve accuracy. For exam-
ple, term frequencies in different corpora can as-
sist in determining relevance given an entity type.
As we mentioned in Section 1, we observe that
term frequency in a database of business names
provides an indication of the business brand, the
key part of the business name phrase. While both
?yahoo? and ?inc? are very common terms on the
web, in a database of businesses only ?inc? is com-
mon enough to be considered a stopword in the
context of business names.
We propose a Vertical Moving Average BM25
(VMABM25) as a feature aiming at quantifying
the vertical knowledge for web search. The ba-
sic idea here is to replace the idf score idf
j
of
651
SMABM25 with an idf score calculated from a
vertical database for type t, namely idft
j
:
1
|{k|T
k
= t}|
?
i?{k|T
k
=t}
(1/M)
?
m
BM25
m,t
where
BM25
m,t
=
?
j
idft
j
f
j,s
(c1 + 1)
f
i,s
+ c
1
(1? c
2
+ c
2
l
s
?
l
s
)
where the idft
j
is associated with the semantic type
t and calculated from the corpus associated with
that type.
VMABM25 links vertical knowledge, proxim-
ity, and page relevance together; we show later that
it is one of most salient features among all seman-
tic features.
4.3 Generalized Semantic Features
Finally, we develop a generalized feature based on
the previous features by removing tags. Semantic
features are often sparse, as many queries contain
one entity or no entities at all; generalized features
increase their coverage by combining the basic se-
mantic features. An entity without tag is essen-
tially a segment.
A segment feature x
i
for query i does not have
entity type and can be expressed as
x
i
=
1
K
i
K
i
?
k=1
x
T (k)
where K
i
is the number of segments in the query
and T (k) is the semantic type associated with kth
concept.
Although these features are less informative
than type-specific features, one advantage of using
them is that they have substantially higher cover-
age. In our experiments, more than 40% of the
queries have some identified entity. Another rel-
atively subtle advantage is that segment features
have no type related errors: the only possible error
is a mistake in entity boundaries.
5 Ranking Function Optimization
The ultimate goal of the machine learning ap-
proach to web search is to learn a ranking func-
tion h(x
i
), where x
i
is a feature vector of a query-
document pair i, such that the error
L(h) ?
N
?
i=1
(y
i
? h(x
i
))
2 (1)
is minimized. Here, y
i
is the actual relevance score
for the query-document pair i (typically assigned
by a human) and N is the number of training sam-
ples.
As mentioned in the previous Section, an inher-
ent issue with semantic features is their sparse-
ness. User queries are usually short, with an av-
erage length of less than 3 words. Text matching
features that are associated with the semantic type
of query term or term groups are clearly sparse
comparing with traditional, non-entity text match-
ing features ? that can be derived for any query.
When a feature is very sparse, it is unlikely that
it would play a very meaningful role in a machine
learned ranking function, since the error L would
largely depend on other samples that do not con-
tain the specific semantic features at all. To over-
come the spareness issue and take advantage of
semantic features, we suggested generalizing our
features; but we also exploit a few ranking func-
tion modeling techniques.
First, we use a ?divide-and-conquer? approach.
Long queries usually contain multiple concepts
and could be difficult to retrieve relevant docu-
ments. Semantic features, however, are rich in
this set of queries. We may train special models
to further optimize our ranking function for those
queries. The loss function over ranking function h
becomes
L
C
(h) ?
?
i?C
(y
i
? h(x
i
))
2 (2)
where C is the training set that falls into a pre-
defined subclass. For example, queries containing
both location and business name, queries contains
both location and business category, etc, are good
candidates to apply semantic features.
To this end, we first classify queries into several
classes, each of which has multiple types of enti-
ties. The semantic features of those types would
be dense for this subclass of queries. We then
train models that may rank the specific class of
queries well. This approach, however, may suf-
fer from significantly less training samples due to
training data partition resulted from the query clas-
sification. Increasing the modeling accuracy, then,
comes at a cost of reduced data available for train-
ing. We apply two techniques to address this is-
sue. The first approach is to over-weight subclass
training samples such that the subclass of queries
plays a more important role in modeling while still
652
keeping a large pool of the overall training sam-
ples. The second approach is model adaptation:
a generalized incremental learning method. Here,
instead of being over-weighted in a joint optimiza-
tion, the subclass of training data is used to mod-
ify an existing model such that the new model is
?adapted? to the subclass problem. We elaborate
on our approaches as follows.
5.1 Weighted Training Samples
To take advantage of both large a pool of training
samples and sparse related semantic features for
a subclass of queries, we could modify the loss
function as follows
L
w
C
(h) ? w
?
i?C
(y
i
? h(x
i
))
2
+
?
i?
?
C
(y
i
? h(x
i
))
2
,
(3)
where ?C is the complement of set C. Here, the
weight w is a compromise between loss function
(1) and (2). When w = 1, we have
L
1
C
(h) ? L(h);
when w? > ?
L
?
C
(h) ? L
C
(h).
A large weight may help optimize the training for
a special subclass of queries, and a small weight
may help to preserve good generality of the ranker.
We could use cross-validation to select the weight
w to optimize a the ranking function for a sub-
class of queries. In practice, a small w is desired
to avoid overfitting.
5.2 Model Adaptation
Model adaptation is an emerging machine learn-
ing technique that is used for information retrieval
applications with limited amount of training data.
In this paper, we apply Trada, proposed by Chen
et al (Chen et al, 2008), as our adaptation algo-
rithm.
The Trada algorithm aims at adapting tree-
based models. A popular tree based regression ap-
proach is Gradient Boosting Trees (GBT) , which
is an additive model h(x) =
?
K
k=1
?
k
h
k
(x),
where each regression tree h
k
is sequentially op-
timized with a hill-climbing procedure. As with
other decision trees, a binary regression tree h
k
(x)
consists of a set of decision nodes; each node is
associated with a feature variable and a splitting
value that partition the data into two parts, with the
corresponding predicted value defined in the leave
node. The basic idea of Trada is to apply piece-
wise linear transformation to the base model based
on the new training data. A set of linear transfor-
mations are applied to each decision node, either
predict or split point or both, such that the new pre-
dict or the split point of a node in a decision tree
satisfies
v = (1? p
C
)v? + p
C
v
C
where v? denotes predict or split point of that node
in the base mode and v
C
denotes predict or split
point of that node using new data set C, and
the weight p
C
depends on the number of origi-
nal training data and new training data that fall
through the node. For each node, the split or pre-
dict can be estimated by
p
C
=
?n
C
n+ ?n
C
,
where n is the number of training sample of the
base model that fall through the node, n
C
is the
number of new training sample that fall through
the node, and ? is a parameter that can be deter-
mined using cross validation. The parameter ?
is used to over-weight new training data, an ap-
proach that is very effective in practice. For new
features that are not included in the base model,
more trees are allowed to be added to incorporate
them.
6 Experiments
We now measure the effectiveness of our proposal,
and answer related questions, through extensive
experimental evaluation. We begin by examining
the effectiveness of features as well as the model-
ing approaches introduced in Section 5 on a par-
ticular class of queries?those with a local intent.
We proceed by evaluating whether if the type asso-
ciated with each entity really matters by compar-
ing results with type dependent semantic features
and segment features. Finally, we examine the ro-
bustness of our features by measuring the change
in the accuracy of our resulting ranking function
when the query analysis is wrong; we do this by
introducing simulated noise into the query analy-
sis results.
6.1 Dataset
Our training, validation and test sets are human-
labeled query-document pairs. Each item in the
653
sets consists of a feature vector x
i
represent-
ing the query and the document, and a judg-
ment score y
i
assigned by a human. There are
around 600 features in each vector, including both
the newly introduced semantic features and exist-
ing features; features are either query-dependent
ones, document-dependent ones, or query-and-
document-dependent features.
The training set is based on uniformly sampled
Web queries from our query log, and top ranked
documents returned by commercial search engines
for these queries; this set consists of 1.24M query-
document pairs.
We use two additional sets for validation and
testing. One set is based on uniformly sampled
Web queries, and contains 42790 validation sam-
ples and 70320 test samples. The second set is
based on uniformly sampled local queries. By lo-
cal queries, we mean queries that contain at least
two types of semantic tags: a location tag (such
as street, city or state name) and a business tag (a
business name or business category). We refer to
this class of queries ?local queries,? as users often
type this kind of queries in local vertical search.
The local query set consists of 11040 validation
samples and 39169 test samples. In the training
set we described above, there are 56299 training
samples out of the 1.24M total number of training
samples that satisfy the definition of local queries.
We call this set local training subset.
6.2 Evaluation Metrics
To evaluate the effectiveness of our semantic
features we use Discounted Cumulative Gain
(DCG) (Jarvelin and Kekalainen, 2000), a widely-
used metric for measuring Web search relevance.
(Jarvelin and Kekalainen, 2000). Given a query
and a ranked list of K documents (K = 5 in our
experiments), the DCG for this query is defined as
DCG(K) =
K
?
i=1
y
i
log
2
(1 + i)
. (4)
where y
i
? [0, 10] is a relevance score for the
document at position i, typically assigned by a hu-
man, where 10 is assigned to the most relevant
documents and 0 to the least relevant ones.
To measure statistical significance, we use the
Wilcoxon test (Wilcoxon, 1945); when the p-value
is below 0.01 we consider a difference to be statis-
tically significant and mark it with a bold font in
the result table.
6.3 Experimental Results
We use Stochastic Gradient Boosting Trees
(SGBT) (Friedman, 2002), a robust none linear
regression algorithm, for training ranking func-
tions and, as mentioned earlier, Trada (Chen et al,
2008) for model adaptation.
Training parameters are selected to optimize the
relevance on a separated validation set. The best
resulting is evaluated against the test set; all results
presented here use the test set for evaluation.
6.3.1 Feature Effectiveness with Ranking
Function Modeling
We apply the modeling approaches introduced in
Section 5 to improve feature effectiveness on ?dif-
ficult? queries?those more than one entity type;
we evaluate these approaches with the semantic-
feature-rich set, the local query test set. We split
training sets into two parts: one set belongs to the
local queries, the other is the rest. We first weight
the local queries and use the combined dataset as
training data to learn the ranking functions; we
train functions with and without the semantic fea-
tures. We evaluate these functions against the lo-
cal query test set. The results are summarized in
Table 1, where w denotes the weight assigned to
the local training set, bolded numbers are statis-
tically significant result compared to the baseline,
uniformly weighted training data without seman-
tic features (with superscript b). It is interesting
to observe that without semantic features, over-
weighted local training data does not have statis-
tically significant impact on the test performance;
with semantic features, a proper weight over train-
ing samples does improve test performance sub-
stantially.
Table 1: Evaluation of Ranking Models Trained
Against Over-weighted Local Queries with Se-
mantic Features on the Local Query Test Set
w/o semantic features w/ semantic features
Weight DCG(5) Impr. DCG(5) Impr.
w = 0 8.09
b
- 8.25 2.0%
w = 2 8.09 0.02% 8.26 2.1%
w = 4 8.13 0.49% 8.34 3.1%
w = 8 8.13 0.49% 8.42 4.1%
w = 16 8.13 0.49% 8.30 2.6%
w = 32 8.04 ?0.60% 8.27 2.2%
Next, we use the local query training set as ?new
data? in the tree adaptation approach. In tree adap-
tations, all parameters are set to optimize the per-
formance over the local validation set. We com-
654
pare two major adaptation approaches proposed
in (Chen et al, 2008): adapting predict only and
adapting both predict and split. We use the model
trained with the combined training and uniform
weights as the baseline; results are summarized in
Table 2.
Table 2: Trada Algorithms with Semantic Features
on Local Query Test Set
w/o semantic feat. w/ semantic feat.
Ada. Appr. DCG(5) Impr. DCG(5) Impr.
Combined data 8.09b - 8.25 2.0%
Ada. predict 8.02 ?0.1% 8.14 0.6%
Ada. predict 8.00 ?0.1% 8.17 1.0%
& split
Comparing Tables 1 and 2, note that using the
combined training data with local query training
samples over-weighted achieves better results than
tree adaption. The latter approach, however, has
the advantage of far less training time, since the
adaptation is over a much smaller local query
training set. With the same hardware, it takes
just a few minutes to train an adaptation model,
while it takes days to train a model over the entire,
combined training data. Considering that massive
model validation tasks are required to select good
training parameters, training many different mod-
els with over a million training samples becomes
prohibitly costly. Applying tree adaptation tech-
niques makes research and prototyping of these
models feasible.
6.3.2 Type Dependent Semantic Features vs.
Segment Features
Our next experiment compares type-dependent
features and segment features, evaluating models
trained with these features against the local query
test set. No special modeling approach is applied
here; results are summarized in Table 3. We ob-
serve that by using type-dependent semantic fea-
tures only, we can achieve as much as by using
all semantic features. Since segment features only
convey proximity information while the base fea-
ture set aleady contain a systematic set of prox-
imity measures, the improvement through segment
features is not as significant as the the type depen-
dent ones.
6.3.3 Robustness of Semantic Features
Our final set of experiments aims at evaluating the
robustness of our semantic features by introducing
Table 3: Type-dependent Semantic Features vs.
Segment Features
Feature set DCG(5)
base + type dependent semantic features 8.23
base + segment features 8.19
base + all semantic features 8.25
simulated errors to the output of our query analy-
sis. Concretely, we manipulate the precision and
the recall of a specific type of entity tagger, t, on
the training and test set. To decrease the recall of
type t, we uniformly remove a set of a% tags of
type t ? preserving precision. To decrease preci-
sion, we uniformly select a set of query segments
(viewing the entity detection as simple segmenta-
tion, as detailed earlier) and assign the semantic
type t to those segments. Since the newly added
term group are selected from query segmentation
results, the introduced errors are rather semantic
type error than boundary error or proximity error.
The total number of newly assigned type t tags are
b% of the original number of type t tags in the
training set. By doing this, we decrease the preci-
sion of type t while keeping the recall of it at the
same level.
Suppose the original tagger achieves precision
p and recall r. By removing a% of tags, we have
estimated precision p? and recall r? defined as fol-
lows:
r? =
100r ? ar
100
,
p? = p.
By adding b% more term group to this type, we
have estimated precision and recall as
p? =
100p
100 + bp
,
r? = r.
In the experiment reported here we use BUSI-
NESS NAME as the target semantic type for this ro-
bustness experiment. An editorial test shows that
our tagger achieves 74% precision and 66% recall
based on a random set of human labeled queries
for this entity type. We train ranking models with
various values of a and b. When we reduce the
estimated recall, we evaluate these models against
the local test set since other data are not affected.
The results are summarized in Table 4.
When we reduce the precision, we evaluate the
resulting models against the general test set as
655
Table 4: Relevance with simulated error on local
query test set
a b p? r? DCG(5) Impr.
0 0 0.74 0.66 8.25 ?
10 0 0.74 0.594 8.21 0.48%
20 0 0.74 0.528 8.19 0.72%
40 0 0.74 0.396 8.18 0.85%
Table 5: Search relevance with simulated error for
semantic features on general test set
a b p? r? DCG(5) Impr.
0 0 0.74 0.66 10.11 -
0 10 0.689 0.66 10.11 0.00%
0 20 0.645 0.66 10.12 0.10%
0 40 0.571 0.66 10.12 0.10%
0 60 0.513 0.66 10.12 0.10%
0 80 0.465 0.66 10.11 0.00%
0 100 0.425 0.66 10.10 ?0.10%
simulated errors would virtually affect any sam-
ples with certain probability. Results appear in
Table 5. The results are quite interesting: when
the recall of business name entity decreases, we
observe statistically significant relevance degrada-
tion: if less entities are discovered, search rele-
vance is hit. The experiments with simulated pre-
cision error, however, are less conclusive. One
may note the experiments are conducted over the
general test set. Therefore, it is not clear if the pre-
cision of the NER system really has insignificant
impact on the IR relevance or just the impact is
diluted in a larger test set.
6.4 Case Analysis
In this section, we take a close look at a few
cases where our new semantic features help
most and where they fail. For the query sil-
verado ranch in irving texas, with no semantic
features, the ranking function ranks a local
listing page for this business, http://local.
yahoo.com/info-28646193, as the top
document. With semantic features, the ranking
function ranks the business home page: http:
//www.silveradoranchparties.com/
as top URL. Examining the two documents, the
local listing page actually contains much more rel-
evant anchor text, which are the among the most
salient features in traditional ranking models. The
home page, however, contains almost no relevant
anchor text: for a small business home page, this
is not a rare situation. Looking at the semantic
features of these two pages, the highest resolution
of location, the city name ?Irving,? appears in the
document body text 19 times in the local listing
page body text, and only 2 times in the home page
body text. The training process learns, then, that
for a query for a local business name (rather than
a business category), home pages?even with
fewer location terms in them?are likely to be
more relevant than a local listing page that usually
contain high frequency location terms.
In some cases, however, our new features do
hurt performance. For the query pa treasur-
ers office, the ranking function with no seman-
tic features ranks the document http://www.
patreasury.org highest, while the one with
semantic features ranks the page http://www.
pikepa.org/treasurer.htm higher. The
latter page is somewhat relevant: it is a treasurer?s
office in Pennsylvania. However, it belongs to a
specific county, which makes it less relevant than
the former page. This is a classic error that we ob-
serve: a mismatch of the intended location area.
While users are looking for state level business,
we provide results of county level. To resolve
this type of error, query analysis and semantic text
matching are no longer enough: here, the rank-
ing function needs to know that Pike County is a
county in Pennsylvania, Milford is a city in Pike
County, and neither are referred to by the user.
Document-side entity recognition, however, may
provide this type of information, helping to ad-
dress this type of errors.
7 Conclusion and Future Research
In this paper, we investigate how semantic features
can improve search relevance in a large-scale in-
formation retrieval setting; to our knowledge, it is
the first study of this approach on a web scale. We
present a set of features that incorporate semantic
and vertical knowledge into the retrieval process,
propose techniques to handle the sparseness prob-
lem for these features, and describe how they fit
in the learning process. We demonstrate that these
carefully designed features significantly improve
relevance, particularly for difficult queries ? long
queries with multiple entities.
The work reported here focuses on query-side
processing, avoiding the indexing cost of docu-
ment processing. We are currently investigating
document-side analysis to complement the query-
side work, and believe that this will further boost
the retrieval accuracy; we hope to report on this in
a follow-up study.
656
References
J. Allan and H. Raghavan. 2002. Using Part-of-Speech
Patterns to Reduce Query Ambiguity. In Proceed-
ings of SIGIR.
A. T. Arampatzis, Th. P. Weide, C. H. A. Koster,
and P. Bommel. 1990. Text Filtering using
Linguistically-motivated Indexing Terms. Techni-
cal Report CSI-R9901, Computing Science Institute,
University of Nijmegen, Nijmegen,The Netherlands.
Thorsten Brants. 2003. Natural Language Processing
in Information Retrieval. In Proceedings of CLIN.
C. Buckley, J. Allan, and G. Salton. 1993. Auto-
matic Routing and Ad-hoc Retrieval Using SMART:
TREC 2. In Proceedings of TREC-2.
Keke Chen, Rongqing Lu, C.K. Wong, Gordon Sun,
Larry Heck, and Belle Tseng. 2008. Trada: tree
based ranking function adaptation. In Proceedings
of CIKM.
W.B. Croft and D.J. Harper. 1979. Using Probabilistic
Models of Document Retrieval without Relevance
Information. Journal of Documentation, 37:285?
295.
W. Bruce Croft, Howard R. Turtle, and David D. Lewis.
1991. The Use of Phrases and Structured queries in
Information Retrieval. In Proceedings of SIGIR.
J. H. Friedman. 2002. Stochastic Gradient Boost-
ing. Computational Statistics and Data Analysis,
38(4):367?378.
N. Fuhr. 1992. Probabilistic Models in Information
Retrieval. The Computer Journal, 35:243?255.
K. Jarvelin and J. Kekalainen. 2000. IR Evalua-
tion Methods for Retrieving Highly Relevant Doc-
uments. In Proceedings of SIGIR.
J. Lafferty and C. Zhai. 2001. Document Language
Models, Query Models and Risk Minimization for
Information Retrieval. In Proceedings of SIGIR.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In Pro-
ceedings of SIGIR.
M. Narita and Y. Ogawa. 2000. The Use of Phrases
from Query Texts in Information Retrieval. In Pro-
ceedings of SIGIR.
Fuchun Peng, Ralph Weischedel, Ana Licuanan, and
Jinxi Xu. 2005. Combining Deep Linguistics
Analysis and Surface Pattern Learning: A Hybrid
Approach to Chinese Definitional Question Answer-
ing. In In Proceedings of the HLT-EMNLP.
J. Ponte and W.B. Croft. 2000. A Language Modeling
Approach to Informaiton Retrieval. In Proceedings
of SIGIR.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by Predictive
Annotation. In Proceedings of SIGIR.
Stephen Robertson, Steve Walker, Susan Jones, Miche-
line Hancock-Beaulieu, and Mike Gatford. 1995.
Okapi at TREC-3. In Proceedings of TREC-3.
G. Salton, C. S. Yang, and C. T. Yu. 1975. A Theory of
Term Importance in Automatic Text Analysis. Jour-
nal of the Ameican Society of Information Science,
26:33?44.
Erik Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 Shared Task:
Language-Independent Named Entity Recognition.
In Proceedings of CoNLL.
Dou Shen, Toby Walkery, Zijian Zheng, Qiang Yang,
and Ying Li. 2008. Personal Name Classification in
Web Queries. In Proceedings of WSDM.
A.F. Smeaton and C.J. van Rijsbergen. 1988. Experi-
ments on Incorporating Syntactic Processing of User
Queries into a Document Retrieval Stragegy. In Pro-
ceedings of SIGIR.
K. Sparck-Jones, 1999. What is the Role of NLP in Text
Retrieval, pages 1?25. Kluwer.
M. Srikanth and R.K. Srihari. 2003. Incorporating
Query Term Dependencies in Language Models for
Document Retrieval. In Proceedings of SIGIR.
Tomek Strzalkowski, Jose Perez-Carballo, Jussi Karl-
gren, Anette Hulth, Pasi Tapanainen, and Timo
Lahtinen. 1996. Natural Language Information Re-
trieval: TREC-8 Report. In Proceedings of TREC-8.
Tao Tao and ChengXiang Zhai. 2007. An exploration
of proximity measures in information retrieval. In
Proceedings of SIGIR.
X. Tong, C. Zhai, N. Millic-Frayling, and D Evans.
1996. Evaluation of Syntactic Phrase Indexing ?
CLARIT NLP Track Report. In Proceedings of
TREC-5.
E. Voohees. 1993. Using WordNet to Disambiguate
Word Senses for Text Retrieval. In Proceedings of
SIGIR.
Ellen Voorhees. 1999. Natural Language Processing
and Information Retrieval. Lecture Notes in Com-
puter Science, 1714:32?48.
Lee Wang, Chuang Wang, Xing Xie, Josh Forman,
Yansheng Lu, Wei-Ying Ma, and Ying Li. 2005.
Detecting dominant locations from search queries.
In Proceedings of SIGIR.
F. Wilcoxon. 1945. Individual Comparisons by Rank-
ing Methods. Biometrics, 1:80?83.
657
Coling 2010: Poster Volume, pages 1318?1326,
Beijing, August 2010
Search with Synonyms: Problems and Solutions
Xing Wei, Fuchun Peng, Huishin Tseng, Yumao Lu, Xuerui Wang, Benoit Dumoulin
Yahoo! Labs	

{xwei,fuchun,huihui,yumaol,xuerui,benoitd}@yahoo-inc.com
Abstract
Search with synonyms is a challenging
problem for Web search, as it can eas-
ily cause intent drifting. In this paper,
we propose a practical solution to this is-
sue, based on co-clicked query analysis,
i.e., analyzing queries leading to clicking
the same documents. Evaluation results
on Web search queries show that syn-
onyms obtained from this approach con-
siderably outperform the thesaurus based
synonyms, such as WordNet, in terms of
keeping search intent.
1 Introduction
Synonym discovery has been an active topic in a
variety of language processing tasks (Baroni and
Bisi, 2004; Fellbaum, 1998; Lin, 1998; Pereira
et al, 1993; Sanchez and Moreno, 2005; Turney,
2001). However, due to the difficulties of syn-
onym judgment (either automatically or manu-
ally) and the uncertainty of applying synonyms
to specific applications, it is still unclear how
synonyms can help Web scale search task. Previ-
ous work in Information Retrieval (IR) has been
focusing mainly on related words (Bai et al,
2005; Wei and Croft, 2006; Riezler et al, 2008).
But Web scale data handling needs to be precise
and thus synonyms are more appropriate than re-
lated words for introducing less noise and alle-
viating the efficiency concern of query expan-
sion. In this paper, we explore both manually-
built thesaurus and automatic synonym discov-
ery, and apply a three-stage evaluation by sep-
arating synonym accuracy from relevance judg-
ment and user experience impact.
The main difficulties of discovering synonyms
for Web search are the following:
1. Synonym discovery is context sensitive.
Although there are quite a few manually built
thesauri available to provide high quality syn-
onyms (Fellbaum, 1998), most of these syn-
onyms have the same or nearly the same mean-
ing only in some senses. If we simply replace
them in search queries in all occurrences, it is
very easy to trigger search intent drifting. Thus,
Web search needs to understand different senses
encountered in different contexts. For example,
?baby? and ?infant? are treated as synonyms in
many thesauri, but ?Santa Baby? has nothing to
do with ?infant?. ?Santa Baby? is a song title,
and the meaning of ?baby? in this entity is dif-
ferent than the usual meaning of ?infant?.
2. Context can not only limit the use of syn-
onyms, but also broaden the traditional definition
of synonyms. For instance, ?dress? and ?attire?
sometimes have nearly the same meaning, even
though they are not associated with the same en-
try in many thesauri; ?free? and ?download? are
far from synonyms in traditional definition, but
?free cd rewriter? may carry the same query in-
tent as ?download cd rewriter?.
3. There are many new synonyms devel-
oped from the Web over time. ?Mp3? and
?mpeg3? were not synonyms twenty years ago;
?snp newspaper? and ?snp online? carry the
same query intent only after snponline.com was
published. Manually editing synonym list is pro-
hibitively expensive. Thus, we need an auto-
matic synonym discovery system that can learn
from huge amount of data and update the dictio-
nary frequently.
1318
In summary, synonym discovery for Web
search is different from traditional thesaurus
mining; it needs to be context sensitive and needs
to be updated timely. To address these prob-
lems, we conduct context based synonym dis-
covery from co-clicked queries, i.e., queries that
share similar document click distribution. To
show the effectiveness of our synonym discov-
ery method on Web search, we use several met-
rics to demonstrate significant improvements:
(1) synonym discovery accuracy that measures
how well it keeps the same search intent; (2)
relevance impact measured by Discounted Cu-
mulative Gain (DCG) (Jarvelin and Kekalainen.,
2002); and (3) user experience impact measured
by online experiment.
The rest of the paper is organized as follows.
In Section 2, we first discuss related work and
differentiate our work from existing work. Then
we present the details of our synonym discov-
ery approach in Section 3. In Section 4 we show
our query rewriting strategy to include synonyms
in Web search. We conduct experiments on ran-
domly sampled Web search queries and run the
three-stage evaluation in Section 5 and analyze
the results in Section 6. WordNet based syn-
onym reformulation and a current commercial
search engine are the baselines for the three-
stage evaluation respectively. Finally we con-
clude the paper in Section 7.
2 Related Works
Automatically discovering synonyms from large
corpora and dictionaries has been popular top-
ics in natural language processing (Sanchez and
Moreno, 2005; Senellart and Blondel, 2003; Tur-
ney, 2001; Blondel and Senellart, 2002; van der
Plas and Tiedemann, 2006), and hence, there has
been a fair amount of work in calculating word
similarity (Porzel and Malaka, 2004; Richardson
et al, 1998; Strube and Ponzetto, 2006; Bolle-
gala et al, 2007) for the purpose of discovering
synonyms, such as information gain on ontology
(Resnik, 1995) and distributional similarity (Lin,
1998; Lin et al, 2003). However, the definition
of synonym is application dependent and most
of the work has been applied to a specific task
(Turney, 2001) or restricted in one domain (Ba-
roni and Bisi, 2004). Synonyms extracted us-
ing these traditional approaches cannot be easily
adopted in Web search where keeping search in-
tent is critical.
Our work is also related to semantic matching
in IR: manual techniques such as using hand-
crafted thesauri and automatic techniques such
as query expansion and clustering all attempts to
provide a solution, with varying degrees of suc-
cess (Jones, 1971; van Rijsbergen, 1979; Deer-
wester et al, 1990; Liu and Croft, 2004; Bai
et al, 2005; Wei and Croft, 2006; Cao et al,
2007). These works focus mainly on adding in
loosely semantically related words to expand lit-
eral term matching. But related words may be
too coarse for Web search considering the mas-
sive data available.
3 Synonym Discovery based on
Co-clicked Queries
In this section, we discuss our approach to syn-
onym discovery based on co-clicked queries in
Web search in detail.
3.1 Co-clicked Query Clustering
Clustering has been extensively studied in many
applications, including query clustering (Wen et
al., 2002). One of the most successful tech-
niques for clustering is based on distributional
clustering (Lin, 1998; Pereira et al, 1993). We
adopt a similar approach to our co-clicked query
clustering. Each query is associated with a set
of clicked documents, which in turn associated
with the number of views and clicks. We then
compute the distance between a pair of queries
by calculating the Jensen-Shannon(JS) diver-
gence (Lin, 1991) between their clicked URL
distributions. We start with that every query
is a separate cluster, and merge clusters greed-
ily. After clusters are generated, pairs of queries
within the same cluster can be considered as
co-clicked/related queries with a similarity score
computed from their JS divergence.
Sim(qk|ql) = DJS(qk||ql) (1)
1319
3.2 Query Pair Alignment
To make sure that words are replacement for
each other in the co-clicked queries, we align
words in the co-clicked query pairs that have
the same length (number of terms), and have
the same terms for all positions except one.
This is a simplification for complicated aligning
processes. Previous work on machine transla-
tion (Brown et al, 1993) can be used when com-
plete alignment is needed for modeling. How-
ever, as we have tremendous amount of co-
clicked query data, our restricted version of
alignment is sufficient to obtain a reasonable
number of synonyms. In addition, this restricted
approach eliminates much noise introduced in
those complicated aligning processes.
3.2.1 Synonym Discovery from Co-clicked
Query Pair
Synonyms discovered from co-clicked queries
have two aspects of word meaning: (1) gen-
eral meaning in language and (2) specific mean-
ing in the query. These two aspects are related.
For example, if two words are more likely to
carry the same meaning in general, then they are
more likely to carry the same meaning in spe-
cific queries; on the other hand, if two words of-
ten carry the same meaning in a variety of spe-
cific queries, then we tend to believe that the two
words are synonyms in general language. How-
ever, neither of these two aspects can cover the
other. Synonyms in general language may not
be used to replace each other in a specific query.
For example, ?sea? and ?ocean? have nearly the
same meaning in language, but in the specific
query ?sea boss boat?, ?sea? and ?ocean? cannot
be treated as synonyms because ?sea boss? is a
brand; also, in the specific query ?women?s wed-
ding attire?, ?dress? can be viewed as a synonym
to ?attire?, but in general language, these two
words are not synonyms. Therefore, whether
two words are synonyms or not for a specific
query is a synthesis judgment based on both of
general meaning and specific context.
We develop a three-step process for synonym
discovery based on co-clicked queries, consider-
ing the above two aspects.
Step 1: Get al synonym candidates for word
wi in general meaning.
In this step, we would like to get al syn-
onym candidates for a word. This step corre-
sponds to Aspect (1) to catch the general mean-
ing of words in language. We consider all the
co-clicked queries with the word and sum over
them, as in Eq. 2
P (wj |wi) =
?
k simk(wi ? wj)?
wj
?
k sim(wi ? wj)
(2)
where simk(wi ? wj) represents the similarity
score (see Section 3.1) of a query qk that aligns
wi to wj . So intuitively, we aggregate scores of
all query pairs that align wi to wj , and normalize
it to a probability over the vocabulary.
Step 2: Get synonyms for word wi in query
qk.
In this step, we would like to get synonyms for
a word in a specific query. We define the prob-
ability of reformulating wi with wj for query qk
as the similarity score shown in Eq. 3.
P (wj |wi, qk) = simk(wi ? wj) (3)
Step 3: Combine the above two steps.
Now we have two sets of estimates for the syn-
onym probability, which is used to reformulate
wi with wj . One set of values are based on gen-
eral language information and another set of val-
ues are based on specific queries. We apply three
combination approaches to integrate the two sets
of values for a final decision of synonym dis-
covery: (1) two independent thresholds for each
probability, (2) linear combination with a coeffi-
cient, and (3) linear combination in log scale as
in Eq. 4, with ? as a mixture coefficient.
Pqk(wj |wi) ? ? log P (wj |wi)
+(1 ? ?) log P (wj |wi, qk) (4)
In experiments we found that there is no sig-
nificant difference with the results from different
combination methods by finely tuned parameter
setting.
3.2.2 Concept based Synonyms
The simple word alignment strategy we used
can only get the synonym mapping from single
1320
term to single term. But there are a lot of phrase-
to-phrase, term-to-phrase, or phrase-to-term syn-
onym mappings in language, such as ?babe in
arms? to ?infant?, and ?nyc? to ?new york city?.
We perform query segmentation on queries to
identify concept units from queries based on
an unsupervised segmentation model (Tan and
Peng, 2008). Each unit is a single word or sev-
eral consecutive words that represent a meaning-
ful concept.
4 Synonym Handling in Web Search
The automatic synonym discovery methods de-
scribed in Section 3 generate synonym pairs for
each query. A simple and straightforward way
to use the synonym pairs would be ?equalizing?
them in search, just like the ?OR? function in
most commercial search engines.
Another method would be to re-train the
whole ranking system using the synonym fea-
ture, but it is expensive and requires a large size
training set. We consider this to be future work.
Besides general equalization in all cases, we
also apply a restriction, specially, on whether or
not to allow synonyms to participate in document
selection. For the consideration of efficiency,
most Web search engines has a document selec-
tion step to pre-select a subset of documents for
full ranking. For the general equalization, the
synonym pair is treated as the same even in the
document selection round; in a conservative vari-
ation, we only use the original word for docu-
ment selection but use the synonyms in the sec-
ond phase finer ranking.
5 Experiments
In this section, we present the experimental re-
sults for our approaches with some in-depth dis-
cussion.
5.1 Evaluation Metrics
We have several metrics to evaluate the synonym
discovery system for Web search queries. They
corresponds to the three stages during the system
development. Each of them measures a different
aspect.
Stage 1: accuracy. Because we are more in-
terested in the application of reformulating Web
search queries, our guideline to the editorial
judgment focuses on the query intent change and
context-based synonyms. For example, ?trans-
porters? and ?movers? are good synonyms in
the context of ?boat? because ?boat transporters?
and ?boat movers? keep the same search intent,
but ?ocean? is not a good synonym to ?sea? in
the query of ?sea boss boats? because ?sea boss?
is a brand name and ?ocean boss? does not re-
fer to the same brand. Results are measured with
accuracy by the number of discovered synonyms
(which reflects coverage).
Stage 2: relevance. To evaluate the effec-
tiveness of our semantic features we use DCG,
a widely-used metric for measuring Web search
relevance.
Stage 3: user experience. In addition to the
search relevance, we also evaluate the practical
user experience after logging all the user search
behaviors during a two-week online experiment.
Web CTR: the Web click through rate (Sher-
man and Deighton, 2001; Lee et al, 2005) is de-
fined as
CTR = number of clicks
total page views
,
where a page view (PV) is one result page that a
search engine returns for a query.
Abandon rate: the percentage of queries that
are abandoned by user neither clicking a result
nor issuing a query refinement.
5.2 Data
A period of Web search query log with clicked
URLs are used to generate co-clicked query set.
After word alignment that extracts the co-clicked
query pairs with same number of units and with
only one different unit, we obtain 12.1M unseg-
mented query pairs and 11.9M segmented query
pairs.
Since we run a three-stage evaluation, there
are three independent evaluation set respectively:
1. accuracy test set. For the evaluation of syn-
onym discovery accuracy, we randomly sampled
42K queries from two weeks of query log, and
1321
evaluate the effectiveness of our synonym dis-
covery model with these queries. To test the syn-
onym discovery model built on the segmented
data, we segment the queries before using them
as evaluation set.
2. relevance test set. To evaluate the relevance
impact by the synonym discovery approach, we
run experiments on another two weeks of query
log and randomly sampled 1000 queries from the
affected queries (queries that have differences in
the top 5 results after synonym handling).
3. user experience test set. The user experi-
ence test is conducted online with a commercial
search engine.
5.3 Results of Synonym Discovery
Accuracy
Here we present the results of WordNet the-
saurus based query synonym discovery, co-
clicked based term-to-term query synonym dis-
covery, and co-click concept based query syn-
onym discovery.
5.3.1 Thesaurus-based Synonym
Replacement
The WordNet thesaurus-based synonym re-
placement is a baseline here. For any word that
has synonyms in the thesaurus, thesaurus-based
synonym replacement will rewrite the word with
synonyms from the thesaurus.
Although thesaurus often provides clean in-
formation, synonym replacement based on the-
saurus does not consider query context and in-
troduces too many errors and noise. Our exper-
iments show that only 46% of the discovered
synonyms are correct synonyms in query. The
accuracy is too low to be used for Web search
queries.
5.3.2 Co-clicked Query-based Context
Synonym Discovery
Here we present the results from our approach
based on co-clicked query data (in this section
the queries are all original queries without seg-
mentation). Figure 1 shows the accuracy of syn-
onyms by the number of discovered synonyms.
By applying different thresholds as cut-off lines
to Eq. 4, we get different numbers of synonyms
from the same test set. As we can see, loosening
the threshold can give us more synonym pairs,
but it could hurt the accuracy.
Figure 1: Accuracy versus number of synonyms
with term based synonym discovery
Figure 1 demonstrates how accuracy changes
with the number of synonyms. Y-axis repre-
sents the percentage of correctly discovered syn-
onyms, and X-axis represents the number of
discovered synonyms, including both of correct
ones and wrong ones. The three different lines
represents three different parameter settings of
mixture weights (? in Eq. 4, which is 0.2, 0.3,
or 0.4 in the figure). The figure shows accuracy
drops by increasing the number of synonyms.
More synonym pairs lead to lower accuracy.
From Figure 1 we can see: Firstly, three
curves with different thresholds almost over-
lap, which means the effectiveness of synonym
discovery is not very sensitive to the mixture
weight. Secondly, accuracy is monotonically de-
creasing as more synonyms are detected. By
getting more synonyms, the accuracy decreases
from 100% to less than 80% (we are not in-
terested in accuracies lower than 80% due to
the high precision requirement of Web search
tasks, so the graph contains only high-accuracy
results). This trend also confirms the effective-
ness of our approach (the accuracy for a random
approach would be a constant).
5.3.3 Concept based Context Synonym
Discovery
We present results from our model based on
segmented co-clicked query data in this section.
1322
Original Query New Query with Synonyms Intent
Examples of thesaurus-based based synonym replacement
basement window wells drainage basement window wells drain
billabong boardshorts sale billabong boardshorts sales event same
bigger stronger faster documentary larger stronger faster documentary
yahoo hayseed
maryland judiciary case search maryland judiciary pillowcase search different
free cell phone number lookup free cell earpiece number lookup
Examples of term-to-term synonym discovery
airlines jobs airlines careers
area code finder area code search same
acai berry acai fruit
acai berry acai juice
ace hardware different
crest toothpaste coupon crest whitestrips coupon
Examples of concept based synonym discovery
ae american eagle outfitters
apartments for rent apartment rentals same
arizona time zone arizona time
cortrust bank credit card cortrust bank mastercard
david beckham beckham different
dodge caliber dodge
Table 1: Examples of query synonym discovery: the first section is thesaurus based, second sec-
tion is co-clicked data based term-to-term synonym discovery, and the last section is concept based
synonym discovery.
The modeling part is the same as the one for
Section 5.3.2, and the only difference is that
the data were segmented. We have shown in
Section 5.3.2 that the mixture weight is not an
crucial factor within a reasonable range, so we
present only the result with one mixture weight
in Figure 2. As in Section 5.3.2, the figure shows
that the accuracy of synonym discovery is sensi-
tive to the threshold. It confirms that our model
is effective and setting threshold to Eq. 4 is a fea-
sible and sound way to discover not only single
term synonyms but also phrase synonyms.
Figure 2: Accuracy versus number of synonyms
with concept based synonym discovery
Table 1 shows some anecdotal examples of
query synonyms with the thesaurus-based syn-
onym replacement, context sensitive synonym
discovery, and concept based context sensitive
synonym discovery. In contrast, the upper part
of each section shows positive examples (query
intents remain the same after synonym replace-
ment) and the lower part shows negative ex-
amples (query intents change after synonym re-
placement).
5.4 Results of Relevance Impact
We run relevance test on 1000 randomly sampled
affected queries. With the automatic synonym
discovery approach we apply our synonym han-
dling method described in Section 4. Results of
DCG improvements by different thresholds and
synonym handling settings are presented in Ta-
ble 2. Thresholds are selected empirically from
the accuracy test in Section 5.3 (we run a small
size relevance test on the accuracy test set and
set the range of thresholds based on that). Note
that in our relevance experiments we use term-
to-term synonym pairs only. For the relevance
impact of concept-based synonym discovery, we
would like to study it in our future work.
1323
From Table 2 we can see that the automatic
synonym discovery approach we presented sig-
nificantly improves search relevance on various
settings, which confirms the effectiveness of our
synonym discovery for Web search queries. We
conjecture that avoiding synonym in document
selection is of help. This is because precision is
more important to Web search than recall for the
huge amount of data available on the Web.
Relevance impact with synonym handling
doc-selection
threshold1 threshold2 participation DCG
0.8 0.02 no +1.7%
0.8 0.02 yes +1.3%
0.8 0.05 no +1.8%
0.8 0.05 yes +1.4%
Table 2: Relevance impact with synonym han-
dling by different parameter settings. ?Thresh-
old1? is the threshold for context-based similar-
ity score?Eq. 3; ?threshold2? is the threshold
for general case similarity score?Eq. 2; ?doc-
selection participation? refers to whether or not
let synonym handling participate in document
selection. All improvements are statistically sig-
nificant by Wilcox significance test.
5.5 Results of User Experience Impact
In addition to the relevance impact, we also eval-
uated the practical user experience impact by
CTR and abandon rate (defined in Section 5.1)
through a two-week online run. Results show
that the synonym discovery method presented in
this paper improves Web CTR by 2%, and de-
creases abandon rate by 11.4%. All changes
are statistically significant, which indicates syn-
onyms are indeed beneficial to user experience.
6 Discussion and Error Analysis
From Table 1, we can see that our approach can
catch not only traditional synonyms, which are
the synonyms that can be found in manually-
built thesaurus, but also context-based syn-
onyms, which may not be treated as synonyms
in a standard dictionary or thesaurus. There are
a variety of synonyms our approach discovered:
1. Synonyms that are not considered as syn-
onyms in traditional thesaurus, such as ?berry?
and ?fruit? in the context of ?acai?. ?acai berry?
and ?acai fruit? refer to the same fruit.
2. Synonyms that have different part-of-
speech features than the corresponding original
words, such as ?finder? and ?search?. Users
searching ?area code finder? and users search-
ing ?area code search? are looking for the same
content. In the context of Web search queries,
part-of-speech is not an important factor as most
queries are not grammatically perfect.
3. Synonyms that show up in recent concepts,
such as ?webmail? and ?email? in the context
of ?cox?. The new concept of ?webmail? or
?email? has not been added to many thesauri yet.
4. Synonyms not limited by length, such as
?crossword puzzles? and ?crossword?, ?homes
for sale? and ?real estate?. The segmenter
helps our system discover synonyms in various
lengths.
With these many variations, the synonyms dis-
covered by our approach are not the ?synonyms?
in the traditional meaning. They are context sen-
sitive, Web data oriented and search effective
synonyms. These synonyms are discovered by
the statistical model we presented and based on
Web search queries and clicked data.
However, the click data themselves contain a
huge amount of noise. Although they can re-
flect the users? intents in some big picture, in
many specific cases synonyms discovered from
co-clicked data are biased by the click noise. In
our application?Web search query reformula-
tion with synonyms, accuracy is the most im-
portant thing and thus we are interested in er-
ror analysis. The errors that our model makes
in synonym discovery are mainly caused by the
following reasons:
(1) There are some concepts well accepted
such as ?cnn? means ?news? and ?amtrak?
means ?train?. And users searching ?news? tend
to click CNN Web site; users searching ?train?
tend to click Amtrak Web site. With our model,
?cnn? and ?news?, ?amtrak? and ?train? are dis-
covered to be synonyms, which may hurt the
search of ?news? or ?train? in general meaning.
1324
(2) Same clicks by different intents. Although
clicking on same documents generally indicates
same search intent, different intents could re-
sult in same or similar clicks, too. For exam-
ple, the queries of ?antique style wedding rings?
and ?antique style engagement rings? carry dif-
ferent intents, but very usually, these two differ-
ent intents lead to the clicks on the same Web
site. ?Booster seats? and ?car seats?, ?brighton
handbags? and ?brighton shoes? are other two
examples in the same case. For these examples,
clicking on Web URLs are not precise enough
to reflect the subtle difference of language con-
cepts.
(3) Bias from dominant user intents. Most
people searching ?apartment? are looking for an
apartment to rent. So ?apartment for rent? and
?apartment? have similar clicked URLs. But
these two are not synonyms in language. In these
cases, popular user intents dominate and bias the
meaning of language, which causes problems.
?Airline baggage restrictions? and ?airline travel
restrictions? is another example.
(4) Antonyms. Many context-based synonym
discovery methods suffer from the antonym
problem, because antonyms can have very simi-
lar contexts. In our model, the problem has been
reduced by integrating clicked-URLs. But still,
there are some examples, such as ?spyware? and
?antispyware?, resulting in similar clicks. To
learn how to ?protect a Web site?, a user often
needs to learn what are the main methods to ?at-
tack a Web site?, and these different-intent pairs
lead to the same clicks because different intents
do not have to mean different interests in many
specific cases.
Although these problems are not common, but
when they happen, they cause a bad user search
experience. We believe a solution to these prob-
lems might need more advanced linguistic anal-
ysis.
7 Conclusions
In this paper, we have developed a synonym dis-
covery approach based on co-clicked query data,
and improved search relevance and user experi-
ence significantly based on the approach.
For future work, we are investigating more
synonym handling methods to further improve
the synonym discovery accuracy, and to handle
the discovered synonyms in more ways than just
the query side.
References
Bai, J., D. Song, P. Bruza, J.Y. Nie, and G. Cao.
2005. Query Expansion using Term Relationships
in Language Models for Information Retrieval. In
Proceedings of the ACM 14th Conference on In-
formation and Knowledge Management.
Baroni, M. and S. Bisi. 2004. Using Cooccurrence
Statistics and the Web to Discover Synonyms in a
Technical Language. In LREC.
Blondel, V. and P. Senellart. 2002. Automatic Ex-
traction of Synonyms in a Dictionary. In Proc. of
the SIAM Workshop on Text Mining.
Bollegala, D., Y. Matsuo, and M. Ishizuka. 2007.
Measuring Semantic Similarity betweenWords us-
ing Web Search Engines. In Proceedings of the
16th international conference on World Wide Web
(WWW).
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statis-
tical Machine Translation: Parameter Estimation.
Computational Linguistics, 19(2):263.
Cao, G., J.Y. Nie, and J. Bai. 2007. Using Markov
Chains to Exploit Word Relationships in Informa-
tion Retrieval. In Proceedings of the 8th Confer-
ence on Large-Scale Semantic Access to Content.
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K.
Landauer, and R. Harshman. 1990. Indexing by
Latent Semantic Analysis. Journal of the Amer-
ican Society for Information Science, 41(6):391?
407.
Fellbaum, C., editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, Mass.
Jarvelin, K. and J. Kekalainen. 2002. Cumulated
Gain-Based Evaluation Evaluation of IR Tech-
niques. ACM TOIS, 20:422?446.
Jones, K. S., 1971. Automatic Keyword Classification
for Information Retrieval. London: Butterworths.
Lee, Uichin, Zhenyu Liu, and Junghoo Cho. 2005.
Automatic Identification of User Goals in Web
Search. In In the World-Wide Web Conference
(WWW).
1325
Lin, D., S. Zhao, L. Qin, and M. Zhou. 2003. Iden-
tifying Synonyms among Distributionally Similar
Words. In Proceedings of International Joint Con-
ference on Artificial Intelligence (IJCAI).
Lin, J. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Informa-
tion Theory, 37(1):145?151.
Lin, D. 1998. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of COLING/ACL-
98, pages 768?774.
Liu, X. and B. Croft. 2004. Cluster-based Retrieval
using LanguageModels. In Proceedings of SIGIR.
Pereira, F., N. Tishby, and L. Lee. 1993. Distribu-
tional Clustering of English Words. In Proceed-
ings of ACL, pages 183 ? 190.
Porzel, R. and R. Malaka. 2004. A Task-based Ap-
proach for Ontology Evaluation. In ECAI Work-
shop on Ontology Learning and Population.
Resnik, P. 1995. Using Information Content to Eval-
uate Semantic Similarity in a Taxonomy. In Pro-
ceedings of IJCAI-95, pages 448 ? 453.
Richardson, S., W. Dolan, and L. Vanderwende.
1998. MindNet: Acquiring and Structuring Se-
mantic Information from Text. In 36th Annual
meeting of the Association for Computational Lin-
guistics.
Riezler, Stefan, Yi Liu, and Alexander Vasserman.
2008. Translating Queries into Snippets for Im-
proved Query Expansion. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING?08).
Sanchez, D. and A. Moreno. 2005. Automatic Dis-
covery of Synonyms and Lexicalizations from the
Web. In Proceedings of the 8th Catalan Confer-
ence on Artificial Intelligence.
Senellart, P. and V. D. Blondel. 2003. Automatic
Discovery of Similar Words. In Berry, M., editor,
A Comprehensive Survey of Text Mining. Springer-
Verlag, New York.
Sherman, L. and J. Deighton. 2001. Banner ad-
vertising: Measuring effectiveness and optimiz-
ing placement. Journal of Interactive Marketing,
15(2):60?64.
Strube, M. and S. P. Ponzetto. 2006. WikiRe-
late! Computing Semantic Relatedness Using
Wikipedia. In Proceedings of AAAI.
Tan, B. and F. Peng. 2008. Unsupervised Query Seg-
mentation using Generative Language Models and
Wikipedia. In Proceedings of the 17th Interna-
tional World Wide Web Conference (WWW), pages
347?356.
Turney, P. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning.
van der Plas, Lonneke and Jorg Tiedemann. 2006.
Finding Synonyms using Automatic Word Align-
ment and Measures of Distributional Similarity.
In Proceedings of the COLING/ACL 2006, pages
866?873.
van Rijsbergen, C.J., 1979. Information Retrieval.
London: Butterworths.
Wei, X. and W. B. Croft. 2006. LDA-based Doc-
ument Models for Ad-hoc Retrieval. In Proceed-
ings of SIGIR, pages 178?185.
Wen, J.R., J.Y. Nie, and H.J. Zhang. 2002. Query
Clustering Using User Logs. ACM Transactions
on Information Systems, 20(1):59?81.
1326
