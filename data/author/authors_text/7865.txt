Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 566?574,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An effective Discourse Parser that uses Rich Linguistic Information
Rajen Subba ?
Display Advertising Sciences
Yahoo! Labs
Sunnyvale, CA, USA
rajen@yahoo-inc.com
Barbara Di Eugenio
Department of Computer Science
University of Illinois
Chicago, IL, USA
bdieugen@cs.uic.edu
Abstract
This paper presents a first-order logic learn-
ing approach to determine rhetorical relations
between discourse segments. Beyond lin-
guistic cues and lexical information, our ap-
proach exploits compositional semantics and
segment discourse structure data. We report
a statistically significant improvement in clas-
sifying relations over attribute-value learn-
ing paradigms such as Decision Trees, RIP-
PER and Naive Bayes. For discourse pars-
ing, our modified shift-reduce parsing model
that uses our relation classifier significantly
outperforms a right-branching majority-class
baseline.
1 Introduction
Many theories postulate a hierarchical structure for
discourse (Mann and Thompson, 1988; Moser et.
al., 1996; Polanyi et. al., 2004). Discourse struc-
ture is most often based on semantic / pragmatic re-
lationships between spans of text and results in a tree
structure, as that shown in Figure 1. Discourse
parsing, namely, deriving such tree structures and
the rhetorical relations labeling their inner nodes is
still a challenging and mostly unsolved problem in
NLP. It is linguistically plausible that such structures
are determined at least in part on the basis of the
meaning of the related chunks of texts, and of the
rhetorical intentions of their authors. However, such
knowledge is extremely difficult to capture. Hence,
previous work on discourse parsing (Wellner et. al.,
2006; Sporleder and Lascarides, 2005; Marcu, 2000;
Polanyi et. al., 2004; Soricut and Marcu, 2003;
?This work was done while the author was a student at the
University of Illinois at Chicago.
Baldridge and Lascarides, 2005) has relied only on
syntactic and lexical information, lexical chains and
shallow semantics.
We present an innovative discourse parser that
uses compositional semantics (when available) and
information on the structure of the segment being
built itself. Our discourse parser, based on a modi-
fied shift-reduce algorithm, crucially uses a rhetori-
cal relation classifier to determine the site of attach-
ment of a new incoming chunk together with the ap-
propriate relation label. Another novel aspect of our
work is the usage of Inductive Logic Programming
(ILP): ILP learns from first-order logic representa-
tions (FOL). The ILP-based relation classifier is
significantly more accurate than relation classifiers
that use competitive propositional ML algorithms
such as decision trees and Naive Bayes. In addi-
tion, it results in FOL rules that are linguistically
perspicuous. Our domain is that of instructional
how-to-do manuals, and we describe our corpus
in Section 2. In Section 3, we discuss the modified
shift-reduce parser we developed. The bulk of the
paper is devoted to the rhetorical relation classifier
in Section 4. Experimental results of both the rela-
tion classifier and the discourse parser in its entirety
are discussed in Section 5. Further details can be
found in (Subba, 2008).
2 Discourse Annotated Instructional
Corpus
Existing corpora annotated with rhetorical relations
(Carlson et. al., 2003; Wolf and Gibson, 2005;
Prasad et. al., 2008) focus primarily on news arti-
cles. However, for us the development of the dis-
course parser is parasitic on our ultimate goal: de-
veloping resources and algorithms for language in-
566
s1e1-s5e2
general:specific
dddd
dddd
dddd
dddd
dddd
dddd
dddd
dddd
dddd
d
ZZ
ZZ
ZZ
ZZ
ZZ
ZZ
ZZ
ZZ
ZZ
Z
s1e1 s2e1-s5e2
preparation:act
ggg
ggg
ggg
ggg
ggg
ggg
ggg
gg
WWW
WWW
WWW
WWW
WWW
WWW
WWW
WW
Another way ..
.. sheets.
s2e1-s4e1
preparation:act
RR
RR
RR
RR
RR
RR
RR
RR
ll
ll
ll
ll
ll
ll
ll
ll
s5e1-s5e2
act:goal
x
x
x
x
x
F
F
F
F
F
F
F
F
F
F
s2e1-s3e2
preparation:act
RR
RR
RR
RR
RR
RR
RR
RR
ll
ll
ll
ll
ll
ll
ll
ll
s4e1 s5e1 s5e2
s2e1-s2e2
reason:act
F
F
F
F
F
F
F
F
F
F
x
x
x
x
x
s3e1-s3e2
disjunction
F
F
F
F
F
F
F
F
F
F
x
x
x
x
x
x
x
x
x
x
Then lay
the sheet
.. the panel.
Using the ..
.. pattern,
mark the panel.
s2e1 s2e2 s3e1 s3e2
Because these
sheets ..
.. panels,
you can
tape one ..
.. panel.
Mark the
opening ..
.. sheet,
or cut it
.. blade.
Figure 1: Discourse Parse Tree of the Text in Example (1)
terfaces to instructional applications. Hence, we are
interested in working with instructional texts. We
worked with a corpus on home-repair that is about
5MB in size and is made up entirely of written En-
glish instructions,1 such as that shown in Exam-
ple (1). The text has been manually segmented
into Elementary Discourse Units (EDUs), the small-
est units of discourse. In total, our corpus contains
176 documents with an average of 32.6 EDUs for a
total of 5744 EDUs and 53,250 words. The structure
for Example (1) is shown in Figure 1.
(1) [Another way to measure and mark panels for
cutting is to make a template from the protec-
tive sheets.(s1e1)] [Because these sheets are the
same size as the panels,(s2e1)] [you can tape
one to the wall as though it were a panel.(s2e2)]
[Mark the opening on the sheet(s3e1)] [or cut
it out with a razor blade.(s3e2)] [Then lay the
sheet on the panel.(s4e1)] [Using the template
as a pattern,(s5e1)] [mark the panel.(s5e2)]
To explore our hypothesis, that rich linguistic in-
formation helps discourse parsing, and that the state
1The raw corpus was originally assembled at the Informa-
tion Technology Research Institute, University of Brighton.
of the art in machine learning supports such an
approach, we needed training data annotated with
both compositional semantics and rhetorical rela-
tions. We performed the first type of annotation al-
most completely automatically, and the second man-
ually, as we turn now to describing.
2.1 Compositional Semantics Derivation
The type of compositional semantics we are inter-
ested in is heavily rooted in verb semantics, which
is particularly appropriate for the instructional text
we are working with. Therefore, we used VerbNet
(Kipper et. al., 2000) as our verb lexicon. VerbNet
groups together verbs that undergo the same syn-
tactic alternations and share similar semantics. It
accounts for 4962 distinct verbs classified into 237
main classes. Each verb class is described by the-
matic roles, selectional restrictions on the arguments
and frames consisting of a syntactic description and
semantic predicates. Such semantic classification of
verbs can be helpful in making generalizations, es-
pecially when data is not abundant. Generalization
can also be achieved by means of the semantic pred-
icates. Although the verb classes of two verb in-
stances may differ, semantic predicates are shared
across verbs. To compositionally build verb based
567
semantic representations of our EDUs, we (Subba
et al, 2006) integrated a robust parser, LCFLEX
(Rose?, 2000), with a lexicon and ontology based
both on VerbNet and, for nouns, on CoreLex (Buite-
laar, 1998). The augmented parser was able to de-
rive complete semantic representations for 3257 of
the 5744 EDUs (56.7%). The only manual step was
to pick the correct parse from a forest of parse trees,
since the output of the parser can be ambiguous.
2.2 Rhetorical relation annotation
The discourse processing community has not yet
reached agreement on an inventory of rhetorical re-
lations. Among the many choices, our coding
scheme is a hybrid of (Moser et. al., 1996) and
(Marcu, 1999). We focused on what we call infor-
mational relations, namely, relations in the domain.
We used 26 relations, divided into 5 broad classes:
12 causal relations (e.g., preparation:act, goal:act,
cause:effect, step1:step2); 6 elaboration relations
(e.g., general:specific, set:member, object:attribute;
3 similarity relations (contrast1:contrast2, com-
parison, restatement); 2 temporal relations (co-
temp1:co-temp2, before:after); and 4 other rela-
tions, including joint and disjunction.
The annotation yielded 5172 relations, with rea-
sonable intercoder agreement. On 26% of the data,
we obtained ? = 0.66; ? rises to 0.78 when the two
most commonly confused relations, preparation:act
and step1:step2, are consolidated. We also anno-
tated the relata as nucleus (more important mem-
ber) and satellite (contributing member(s)) (Mann
and Thompson, 1988), with ? = 0.67.2 The most
frequent relation is preparation:act (24.46%), and in
general, causal relations are more frequently used in
our instructional corpus than in news corpora (Carl-
son et. al., 2003; Wolf and Gibson, 2005).
3 Shift-Reduce Discourse Parsing
Our discourse parser is a modified version of a shift-
reduce parser. The shift operation places the next
segment on top of the stack, TOP. The reduce oper-
ation will attach the text segment at TOP to the text
segment at TOP-1. (Marcu, 2000) also uses a shift-
reduce parser, though our parsing algorithm differs
2We don?t have space to explain why we annotate for nu-
cleus and satellite, even if (Moser et. al., 1996) argue that this
sort of distinction does not apply to informational relations.
in two respects: 1) we do not learn shift operations
and 2) in contrast to (Marcu, 2000), the attachment
of an incoming text segment to the emerging tree
may occur at any node on the right frontier. This al-
lows for the more sophisticated type of adjunction
operations required for discourse parsing as mod-
eled in D-LTAG (Webber, 2004). A reduce op-
eration is determined by the relation identification
component. We check if a relation exists between
the incoming text segment and the attachment points
on the right frontier. If more than one attachment
site exists, then the attachment site for which the rule
with the highest score fired (see below) is chosen for
the reduce operation. A reduce operation can fur-
ther trigger additional reduce operations if there is
more than one tree left in the stack after the first re-
duce operation. When no rules fire, a shift occurs.
In the event that all the segments in the input list
have been processed and a full DPT has not been
obtained, then we reduce TOP and TOP-1 using the
joint relation until a single DPT is built.
4 Classifying Rhetorical Relations
Identifying the informational relations between text
segments is central to our approach for building the
informational tree structure of text. We believe that
the use of a limited knowledge representation for-
malism, essentially propositional logic, is not ad-
equate and that a relational model that can handle
compositional semantics is necessary. We cast the
problem of determining informational relations as a
classification task. We used the ILP system Aleph
that is based on (Muggleton, 1995). Formulation
of any problem within the ILP framework consists
of background knowledge B and the set of exam-
ples E (E+? E?). In our ILP framework, positive
examples are ground clauses describing a relation
and its relata, e.g. relation(s5e1,s5e2,act:goal), or
relation(s2e1-s3e2,s4e1,preparation:act) from Fig-
ure 1. If e is a positive example of a relation r, then
it is also a negative example for all the other rela-
tions.
Background Knowledge (B) can be thought of as
features used by ILP to learn rules, as in traditional
attribute-value learning algorithms. We use the fol-
lowing information to learn rules for classifying re-
lations. Figure 2 shows a sample of the background
568
Verbs + Nouns: verb(?s5e2?,mark). noun(?s5e2?,panel).
Linguistic Cues: firstWordPOS(?s5e2?,?VB?). lastWordPOS(?s5e2?,?.?).
Similarity: segment sim score(?s5e1?,?s5e2?,0.0).
verbclass(?s5e2?,mark,?image impression-25.1?).
agent(?s5e2?,frame(mark),you).
Compositional Semantics: destination(?s5e2?,frame(mark),panel).
cause(?s5e2?,frame(mark),you,?s5e2-mark-e?).
prep(?s5e2?,frame(mark),end(?s5e2-mark-e?),mark,panel).
created image(?s5e2?,frame(mark),result(?s5e2-mark-e?),mark).
Structural Information: same sentence(?s5e1?,?s5e2?).
Figure 2: Example Background Knowledge
knowledge provided for EDU s5e2.
Verbs + Nouns: These features were derived by
tagging all the sentences in the corpus with a POS
tagger (Brill, 1995).
WordNet: For each noun in our data, we also use
information on hypernymy and meronymy relations
using WordNet. In a sense, this captures the domain
relations between objects in our data.
Linguistic Cues: Various cues can facilitate the
inference of informational relations, even if it is well
known that they are based solely on the content of
the text segments, various cues can facilitate the in-
ference of such relations. At the same time, it is
well known that relations are often non signalled:
in our corpus, only 43% of relations are signalled,
consistently with figures from the literature (44%
in (Williams and Reiter, 2003) and 45% in (Prasad
et. al., 2008)). Besides lexical cues such as but,
and and if, we also include modals, tense, compara-
tives and superlatives, and negation. E.g., wrong-act
in relations like prescribe-act:wrong-act is often ex-
pressed using a negation.
Similarity: For the two segments in question, we
compute the cosine similarity of the segments using
only nouns and verbs.
Compositional semantics: the semantic infor-
mation derived by our parser, as described in Sec-
tion 2.1. The semantic representation of segment
s5e2 from Example (1) is shown in Figure 2. Each
semantic predicate is a feature for the classifier.
Structural Information: For relations between
two EDUs, we use knowledge of whether the two
EDUs are intra-sentential or inter-sentential, since
some relations, e.g. criterion:act, are more likely to
be realized intra-sententially than inter-sententially.
For larger segments, we also encode the hierarchi-
cal representation of text segments that contain more
than one nucleus, the distance between the nuclei
of the two segments and any relations that exist be-
tween the smaller inner segments.
At this point, the attentive reader will be wonder-
ing how we encode compositional semantics for re-
lations relating text segments larger than one EDU.
Clearly we cannot just list the semantics of each
EDU that is dominated by the larger segment. We
follow the intuition that nuclei represent the most
important portions of segments (Mann and Thomp-
son, 1988). For segments such as s5e1-s5e2 that
contains a single nucleus, we simply reduce the se-
mantic content of the larger segment to that of its
nucleus:
s5e1-s5e2
verb(?s5e1-s5e2?,mark).
...
verbclass(?s5e1-s5e2?,..).
agent(?s5e1-s5e2?,..).
In this case, the semantics of the complex text seg-
ment is represented by the compositional semantics
of the single most important EDU.
For segments that contain more than one nu-
cleus, such as s3e1-s3e2, the discourse struc-
ture information of the segment is represented with
the additional predicates internal relation and par-
ent segment. These predicates can be used recur-
sively at every level of the tree to specify the relation
between the most important segments. In addition,
they also provide a means to represent the compo-
sitional semantics of the most important EDUs and
569
make them available to the relational learning algo-
rithm.
s3e1-s3e2
internal relation(s3e1,s3e2,?disjunction?).
parent segment(s3e1-s3e2,s3e1).
parent segment(s3e1-s3e2,s3e2).
LL
LL
LL
LL
LL
LL
LL
LL
LL
LL
L
rr
rr
rr
rr
rr
rr
rr
rr
rr
rr
r
verb(?s3e1?,mark).
noun(?s3e1?,opening).
...
verbclass(?s3e1?,..).
theme(?s3e1?,..).
verb(?s3e2?,cut).
noun(?s3e2?,opening).
...
noun(?s3e1?,blade).
4.1 Learning FOL Rules for Discourse Parsing
In Aleph, the hypothesis space is restricted to a set of
rules that conform to a predefined language L. This
is done with the use of mode declarations which, in
other words, introduces a language bias in the learn-
ing process. modeh declarations inform the learning
algorithm about what predicates to use as the head
of the rule and modeb specifies what predicates to
use in the body of the rule. Not all the information
in B needs to be included in the body of the rule.
This makes sense since we often learn definitions of
concepts based on more abstract higher level infor-
mation that is inferred from some other information
that is not part of our final definition. Mode decla-
rations are used by Aleph to build the most specific
clause (?) that can be learned for each example. ?
constrains the search for suitable hypotheses. ?i is
built by taking an example ei ? E+ and adding lit-
erals that are entailed by B and ei. We then have the
following property, whereHi is the hypothesis (rule)
we are trying to learn and is a generality operator:
  Hi  ?i
Finding the most specific clause (?) provides us
with a partially ordered set of clauses from which to
choose the best hypothesis based on some quantifi-
able qualitative criteria. This sub-lattice is bounded
by the most general clause (, the empty clause)
from the top and the most specific clause (?) at the
bottom. We use the heuristic search in Aleph that is
similar to the A*-like search strategy presented by
(Muggleton, 1995) to find the best hypothesis (rule).
A noise threshold on the number of negative exam-
ples that can be covered by a rule can be set. We
learn a model that learns perfect rules first and then
one that allows for at most 5 negative examples. A
backoff model that first uses the model trained with
noise = 0 and then noise = 5 if no classification
has been made is used. We use the evaluation func-
tion in Equation 1 to guide our search through the
tree of possible hypotheses. This evaluation func-
tion is also called the compression function since it
prefers simpler explanations to more complex ones
(Occam?s Razor). fs is the score for clause cs that
is being evaluated, ps is the number of positive ex-
amples, ns is the number of negative examples, ls is
the length of the clause (measured by the number of
clauses).
fs = ps ? (ns + (0.1? ls)) (1)
Classification in most ILP systems, including
Aleph, is restricted to binary classification (positive
vs. negative). In many applications with just two
classes, this is sufficient. However, we are faced
with a multi-classification problem. In order to per-
form multi-class classification, we use a decision
list. First, we build m binary classifiers for each
relation r ? R. Then, we form an ordered list of the
rules based on the following criterion:
1. Given two rules ri and rj , ri ,is ranked higher
than rj if (pi ? ni) > (pj ? nj).
2. if (pi?ni) = (pj ?nj), then ri is ranked higher
than rj if ( pipi+ni ) > (
pj
pj+nj ).
3. if (pi ? ni) = (pj ? nj) and ( pipi+ni ) = (
pj
pj+nj )then ri is ranked higher than rj if (li) > (lj).
4. default: random order
Classifying an unseen example is done by using
the first rule in the ordered list that satisfies it.
5 Experiments and Results
We report our results from experiments on both the
classification task and the discourse parsing task.
5.1 Relation Classification Results
For the classification task, we conducted exper-
iments using the stratified k-fold (k = 5) cross-
validation evaluation technique on our data. Unlike
570
(Wellner et. al., 2006; Sporleder and Lascarides,
2005), we do not assume that we know the order
of the relation in question. Instead we treat reversals
of non-commutative relations (e.g. preparation:act
and act:goal) as separate relations as well. We
compare our ILP model to RIPPER, Naive Bayes
and the Decision Tree algorithm. We should point
out that since attribute-value learning models can-
not handle first-order logic data, they have been pre-
sented with features that lose at least some of this
information. While this may then seem to result in
an unfair comparison, to the contrary, this is pre-
cisely the point: can we do better than very effec-
tive attribute-value approaches that however inher-
ently cannot take richer information into account?
All the statistical significance tests were performed
using the value of F-Score obtained from each of the
folds. We report performance on two sets of data
since we were not able to obtain compositional se-
mantic data for all the EDUs in our corpus:
? Set A: Examples for which semantic data was
available for all the nuclei of the segments
(1789 total). This allows us to have a better
idea of how much impact semantic data has on
the performance, if any.
? Set B: All examples regardless of whether or
not semantic data was available for the nuclei
of the segments (5475 total).
Model Semantics No Semantics
ILP 62.78 60.25
Decision Tree 56.29 55.45
RIPPER 58.02 56.96
Naive Bayes 35.83 34.66
Majority Class 31.63 31.63
Table 1: Classification Performance: Set A (F-Score)
Table 1 shows the results on Set A. ILP outper-
forms all the other models. Via ANOVA, we first
conclude that there is a statistically significant differ-
ence between the 8 models (p < 2.2e?16). To then
pinpoint where the difference precisely lies, pair-
wise comparisons using Student?s t-test show that
the difference between ILP (using semantics) and all
of the other learning models is statistically signifi-
cant at p < 0.05. Additionally, ILP with semantics
is significantly better than ILP without it (p < 0.05).
For Decision Tree, Naive Bayes and RIPPER, the
improvement in using semantics is not statistically
significant.
Model Semantics No Semantics
ILP 59.43 59.22
Decision Tree 53.84 53.69
RIPPER 51.1 51.36
Naive Bayes 49.69 51.62
Majority Class 22.01 22.01
Table 2: Classification Performance: Set B (F-Score)
In Table 2, we list the results on Set B. Once
again, our ILP model outperforms the other three
learning models. Naive Bayes is much more com-
petitive when using all the examples compared to
using only examples with semantic data. In the case
of the attribute-value machine learning models, the
use of semantic data seems to marginally hurt the
performance of the classifiers. However, this is in
contrast to the relational ILP model which always
performs better when using semantics. This result
suggests that the use of semantic data with loss of in-
formation may not be helpful, and in fact, it may ac-
tually hurt performance. Based on ANOVA, the dif-
ferences in these 8 models is statistically significant
with p < 6.95e?12. A pairwise t-test between ILP
(using semantics) and each of the other attribute-
value learning models shows that our results are sta-
tistically significant at p < 0.05.
In Table 3, we report the performance of the two
ILP models on each relation.3 In general, the models
perform better on relations that have the most exam-
ples.
The evaluation of work in discourse parsing is
hindered by the lack of a standard corpus or task.
Hence, our results cannot be directly compared
to (Marcu, 2000; Sporleder and Lascarides, 2005;
Wellner et. al., 2006), but neither can those works
be compared among themselves, because of differ-
ences in underlying corpora, the type and number of
relations used, and various assumptions. However,
we can still draw some general comparisons. Our
ILP-based models provide as much or significantly
3Due to space limitations, only relations with> 10 examples
are shown.
571
relation Semantics No Semantics
preparation:act 74.86 72.05
general:specific 31.74 28.24
joint 55.23 52
act:goal 86.12 83.85
criterion:act 77.37 75.32
goal:act 73.43 68.9
step1:step2 28.75 35.29
co-temp1:co-temp2 48.84 37.84
disjunction 83.33 80.81
act:criterion 54.29 54.79
contrast1:contrast2 22.22 5.0
act:preparation 65.31 70.59
act:reason 0 10.26
cause:effect 19.05 10.53
comparison 22.22 10.53
Table 3: Classification Performance (F-Score) by
Relation: ILP on Set A
more improvement over a majority-class baseline
when compared to these other works. This is the
case even though our work is based on less training
data, relatively more relations, relations both be-
tween just two EDUs and those involving larger text
segments, and we make no assumptions about the
order of the relations. Our results are comparable to
(Marcu, 2000), which reports an accuracy of about
61% for his classifier. His majority class baseline
performs at about 50% accuracy. (Wellner et. al.,
2006) reports an accuracy of up to 81%, with a ma-
jority class baseline performance of 45.7%. How-
ever, our task is more challenging than (Wellner et.
al., 2006). They use only 11 relations compared to
the 26 we use. They also assume the order of the
relation in the examples (i.e. examples for goal:act
would be treated as examples for act:goal by revers-
ing the order of the arguments) whereas we do not
make such assumptions. In addition, their training
data is almost twice as large as ours, based on re-
lation instances. (Sporleder and Lascarides, 2005)
also makes the same assumption on the ordering of
the relations as (Wellner et. al., 2006). They re-
port an accuracy of 57.75%. Their work, though,
was based on only 5 relations. Importantly, neither
(Wellner et. al., 2006; Sporleder and Lascarides,
2005) model examples with complex text segments
with more than one EDU.
5.2 How interesting are the rules?
Given that our ILP models learn first-order logic
rules, we can make some qualitative analysis of the
rules learned, such as those below, learnt by the ILP
model that uses semantics:
(2a) relation(A,B,?act:goal?) :-
firstWordPOS(A,?VBG?),
verbclass(A,D,?use-1?),
firstWordPOS(B,?VB?).
[pos cover = 23 neg cover = 1]
(2b) relation(A,B,?preparation:act?) :-
discourse cue(B,front,and),
cause(A,frame(C),D,E),
theme(B,frame(F),G), theme(A,frame(C),G).
[pos cover = 12 neg cover = 0]
(2c) relation(A,B,?preparation:act?) :-
discourse cue(B,front,then),
parent segment(A,C), parent segment(A,D),
internal relation(C,D,?preparation:act?).
[pos cover = 17 neg cover = 0]
(2a) is learned using examples such as
relation(s5e1,s5e2,?act:goal?) from Example (1).
(2b) uses relational semantic information. This rule
can be read as follows:
IF segment A contains a cause and a
theme, the same object that is the theme
in A is also the theme in segment B, and B
contains the discourse cue and at the front
THEN the relation between A and B is
preparation:act.
(2c) is a rule that makes use of the structural in-
formation about complex text segments. When us-
ing Set A, more than about 60% of the rules in-
duced include at least one semantic predicate in its
body. They occur more frequently in rules for re-
lations like preparation:act while less in rules for
general:specific and act:goal.
5.3 Discourse Parsing Results
In order to test our discourse parser, we used 151
documents for training and 25 for testing. We eval-
uated the performance of our parser on both the
discourse parse trees it builds at the sentence level
and at the document level. The test set contained
572
Sentence Level Document Level
model Semantics span nuclearity relation span nuclearity relation
SR-ILP yes 92.91 71.83 63.06 70.35 49.47 35.44
SR-ILP no 91.98 69.59 58.58 68.95 48.16 33.33
Baseline - 93.66 74.44 34.32 70.26 47.98 22.46
Table 4: Parsing Performance (F-Score): (Baseline = right-branching majority)
341 sentences out of which 180 sentences were seg-
mented into more than one EDU. We ran experi-
ments using our two ILP models for the relation
identifier, namely ILP with semantics and without
semantics. Our ILP based discourse parsing models
are named SR-ILP. We compare the performance of
our models against a right branching majority class
baseline. We used the sign-test to determine statis-
tical significance of the results. Using the automatic
evaluation methodology in (Marcu, 2000), preci-
sion, recall and F-Score measures are computed for
determining the hierarchical spans, nucleus-satellite
assignments and rhetorical relations. The perfor-
mance on labeling relations is the most important
measure since the results on nuclearity and hierar-
chical spans are by-products of the decisions made
to attach segments based on relations.
On labeling relations, the parser that uses all the
features (including compositional semantics) for de-
termining relations performs the best with an F-
Score of 63.06%. The difference of about 4.5% (be-
tween ILP with semantics and without semantics)
in F-Score is statistically significant at p = 0.006.
Our best model, SR-ILP (using semantics) beats the
baseline by about 28% in F-Score. Since the task at
the document level is much more challenging than
building the discourse structure at the sentence level,
we were not surprised to see a considerable drop in
performance. For our best model, the performance
on labeling relations drops to 35.44%. Clearly, the
mistakes made when attaching segments at lower
levels have quite an adverse effect on the overall
performance. A less greedy approach to parsing dis-
course structure is warranted.
While we would have hoped for a better perfor-
mance than 35.44%, to start with, (Forbes et. al.,
2001), (Polanyi et. al., 2004), and (Cristea, 2000) do
not report the performance of their discourse parsers
at all. (Marcu, 2000) reports precision and recall of
up to 63.2% and 59.8% on labeling relations using
manually segmented EDUs on three WSJ articles.
(Baldridge and Lascarides, 2005) reports 43.2% F-
Score on parsing 10 dialogues using a probabilistic
head-driven parsing model.
6 Conclusions
In conclusion, we have presented a relational ap-
proach for classifying informational relations and a
modified shift-reduce parsing algorithm for building
discourse parse trees based on informational rela-
tions. To our knowledge, this is the first attempt
at using a relational learning model for the task of
relation classification, or even discourse parsing in
general. Our approach is linguistically motivated.
Using ILP, we are able to account for rich composi-
tional semantic data of the EDUs based on VerbNet
as well as the structural relational properties of the
text segments. This is not possible using attribute-
value based models like Decision Trees and RIP-
PER and definitely not using probabilistic models
like Naive Bayes. Our experiments have shown that
semantics can be useful in classifying informational
relations. For parsing, our modified shift-reduce al-
gorithm using the ILP relation classifier outperforms
a right-branching baseline model significantly. Us-
ing semantics for parsing also yields a statistically
significant improvement. Our approach is also do-
main independent as the underlying model and data
are not domain specific.
Acknowledgments
This work is supported by the National Science Founda-
tion (IIS-0133123 and ALT-0536968) and the Office of
Naval Research (N000140010640).
573
References
Asher, N., and Lascarides, A.: Logics of Conversation.
Cambridge University Press, 2003.
Baldridge, J. and Lascarides, A.: Probabilistic Head-
Driven Parsing for Discourse Structure In Proceed-
ings of the Ninth Conference on Computational Natu-
ral Language Learning (CoNNL), Ann Arbor, 2005.
Brill, E.: Transformation-based error-driven learning
and natural language processing: A case study in
part-of-speech tagging. Computational Linguistics,
21(4):543565, 1995.
Buitelaar, P.: CoreLex: Systematic Polysemy and Un-
derspecification. Ph.D. Thesis, Brandies University,
1998.
Carlson, L. D. M. and Okurowski., M. E.: Building a
discourse-tagged corpus in the framework of rhetorical
structure theory. In Current Directions in Discourse
and Dialogue pages 85?112, 2003.
Cristea, D.: An Incremental Discourse Parser Architec-
ture. In D. Christodoulakis (Ed.) Proceedings of the
Second International Conference - Natural Language
Processing - Patras, Greece, June 2000.
Forbes, K., Miltsakaki, E., R. P. A. S. A. J. and Web-
ber., B.: D-ltag system - discourse parsing with a lexi-
calized tree adjoining grammar. Information Stucture,
Discourse Structure and Discourse Semantics, ESS-
LLI 2001.
Grosz, B. J. and Sidner, C. L.: Attention, intention and
the structure of discourse. Computational Linguistics
12:175?204, 1988.
Hobbs, J. R.: On the coherence and structure of dis-
course. In Polyani, Livia editor, The Structure of Dis-
course, 1985.
Kipper, K., H. T. D. and Palmer., M.: Class-based con-
struction of a verb lexicon. AAAI-2000, Proceedings
of the Seventeenth National Conference on Artificial
Intelligence, 2000.
Mann, W. and Thompson, S.: Rhetorical structure the-
ory: Toward a functional theory of text organization.
Text, 8(3):243?281, 1988.
Marcu, D.: Instructions for Manually Annotating the
Discourse Structures of Texts. Technical Report, Uni-
versity of Southern California, 1999.
Marcu, D.: The theory and practice of discourse parsing
and summarization. Cambridge, Massachusetts, Lon-
don, England, MIT Press, 2000.
Moser, M. G., Moore, J. D., and Glendening, E.: In-
structions for Coding Explanations: Identifying Seg-
ments, Relations and Minimal Units. University of
Pittsburgh, Department of Computer Science, 1996.
Muggleton, S. H.: Inverse entailment and progol.
In New Generation Computing Journal 13:245?286,
1995.
Polanyi, L., Culy, C., van den Berg, M. H. and Thione,
G. L.: A Rule Based Approach to Discourse Pars-
ing. Proceedings of the 5th SIGdial Workshop in Dis-
course And Dialogue. Cambridge, MA USA pp. 108-
117., May 1, 2004.
Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo,
L., Joshi, A., and Webber, B.: The Penn Discourse
Treebank 2.0. LREC, 2008.
Rose?, C. P.: A Syntactic Framework for Semantic In-
terpretation, Proceedings of the ESSLLI Workshop
on Linguistic Theory and Grammar Implementation,
2000.
Sporleder, C. and Lascarides., A.: Exploiting linguistic
cues to classify rhetorical relations. Recent Advances
in Natural Language Processing, 2005.
Soricut, R. and Marcu., D.: Sentence level discourse
parsing using syntactic and lexical information. Pro-
ceedings of the Human Language Technology and
North American Assiciation for Computational Lin-
guistics Conference, 2003.
Subba, R., Di Eugenio, B., E. T.: Building lexical re-
sources for princpar, a large coverage parser that gen-
erates principled semantic representations. LREC,
2006.
Subba, R.: Discourse Parsing: A Relational Learn-
ing Approach Ph.D. Thesis, University of Illinois
Chicago, December 2008.
Webber, B.: DLTAG: Extending Lexicalized TAG to Dis-
course. Cognitive Science 28:751-779, 2004.
Wellner, B., Pustejovsky, J., C. H. R. S. and Rumshisky.,
A.: Classification of discourse coherence rela-
tions: An exploratory study using multiple knowledge
sources. In Proceedings of the 7th SIGDIAL Work-
shop on Discourse and Dialogue, 2006.
Williams, S. and Reiter, E.: A corpus analysis of dis-
course relations for natural language generation. Pro-
ceedings of Corpus Linguistics, pages 899?908, 2003.
Wolf, F. and Gibson, E.: Representing discourse coher-
ence: A corpus-based analysis. Computational Lin-
guistics 31(2):249?287, 2005.
574
Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 21?24,
Rochester, April 2007. c?2007 Association for Computational Linguistics
Exploiting Event Semantics to Parse the Rhetorical Structure of
Natural Language Text
Rajen Subba
Department of Computer Science
University of Illinois at Chicago
Chicago, IL 60613
rsubba@cs.uic.edu
Abstract
Previous work on discourse parsing has
mostly relied on surface syntactic and lex-
ical features; the use of semantics is lim-
ited to shallow semantics. The goal of this
thesis is to exploit event semantics in order
to build discourse parse trees (DPT) based
on informational rhetorical relations. Our
work employs an Inductive Logic Pro-
gramming (ILP) based rhetorical relation
classifier, a Neural Network based dis-
course segmenter, a bottom-up sentence
level discourse parser and a shift-reduce
document level discourse parser.
1 Introduction
Discourse is a structurally organized set of coher-
ent text segments. The minimal unit of discourse is
called an elementary discourse unit (EDU). An EDU
or a span of EDUs constitute a segment. When we
read text, we automatically assign rhetorical (coher-
ence) relations to segments of text that we deem to
be related. Consider the segmented text below:
(Example 1) [Clean the walls thoroughly(1a)] [and allow them
to dry.(1b)] [If the walls are a dark color,(2a)] [apply
primer.(2b)] [Put a small amount of paste in the paint
tray;(3a)] [add enough water(4a)] [to thin the paste to
about the consistency of cream soup.(4b)]
It is plausible to state that the rhetorical relation
between (1a) and (1b) is preparation:act. We can
also posit that the relation act:goal holds between
(4a) and (4b). Figure 1 shows the complete annota-
tion of the full text. Now, if we were to reorder these
segments as [(1b), (4a), (2a), (4b), (3a), (2b), (1a)],
the text would not make much sense. Therefore, it
is imperative that the contiguous spans of discourse
be coherent for comprehension. Rhetorical relations
help make the text coherent.
Rhetorical relations based on the subject matter
of the segments are called informational relations.
A common understanding in discourse study is that
informational relations are based on the underlying
content of the text segments. However, previous
work (Marcu, 2000; Polanyi et al, 2004; Soricut
and Marcu, 2005; Sporleder and Lascarides, 2005)
in discourse parsing has relied on syntactic and lex-
ical information, and shallow semantics only.
The goal of this thesis is to build a computa-
tional model for parsing the informational structure
of instructional text that exploits ?deeper seman-
tics?, namely event semantics. Such discourse struc-
tures can be useful for applications such as informa-
tion extraction, question answering and intelligent
tutoring systems. Our approach makes use of a neu-
ral network discourse segmenter, a rhetorical rela-
tion classifier based on ILP and a discourse pars-
ing model that builds sentence level DPTs bottom-
up and document level DPTs using a shift-reduce
parser.
In section 2, we describe how we collected our
data. In section 3, we present our automatic dis-
course segmenter. Section 4 details our discourse
parsing model based on event semantics followed by
the conclusion in section 5.
21
Figure 1: Discourse Annotation for Example 1
2 Data Collection
Our work calls for the use of a supervised machine
learning approach. Therefore, we have manually an-
notated a corpus of instructional text with rhetorical
relations and event semantic information. We used
an existing corpus on home repair manuals (5Mb).1
2.1 Manual Discourse Annotation
In order to carry out the manual discourse anno-
tation, a coding scheme was developed based on
Marcu (1999) and RDA (Moser et al, 1996). The
annotated data consists of 5744 EDUs and 5131 re-
lations with a kappa value of 0.66 on about 26% of
the corpus. We analyzed a total of 1217 examples
to determine whether a cue phrase was present or
not. Only 523 examples (43%) were judged to be
signalled. Furthermore, discourse cues can be am-
biguous with regard to which relation they signal.
In order to account for cases where discourse cues
are not present and to resolve such ambiguities, we
intend to exploit event semantics.
2.2 Semi-Automatic Event Semantic
Annotation
Informational relations describe how the content of
two text segments are related. Therefore, it makes
intuitive sense that verb semantics can be useful in
determining these relations.2 In Subba et al (2006),
1The corpus was collected opportunistically off the internet
and from other sources, and originally assembled at the Infor-
mation Technology Research Institute, University of Brighton.
2Especially in instructional manuals where the meaning of
most sentences is centered on verbs.
we integrated LCFLEX (Rose and Lavie, 2000) with
VerbNet (Kipper et al, 2000) and CoreLex (Buite-
laar, 1998) to compositionally build verb based
event semantic representations of our EDUs.
VerbNet groups together verbs that undergo the
same syntactic alternations and share similar seman-
tics. It accounts for about 4962 distinct verbs clas-
sified into 237 main classes. The semantic infor-
mation is described in terms of an event that is de-
composed into four stages, namely start, during, end
and result. Semantic predicates like motion and to-
gether describe the participants of an event at var-
ious stages. CoreLex provides meaning represen-
tations for about 40,000 nouns that are compatible
with VerbNet.
The parser was used to semi-automatically anno-
tate both our training and test data. Since the output
of the parser can be ambiguous with respect to the
verb sense, we manually pick the correct sense.3
3 Automatic Discourse Segmentation
The task of the discourse segmenter is to segment
sentences into EDUs. In the past, the problem
of sentence level discourse segmentation has been
tackled using both symbolic methods (Polanyi et al,
2004; Huong et al, 2004) as well as statistical mod-
els (Soricut and Marcu, 2003; Marcu, 2000) that
have exploited syntactic and lexical features.
We have implemented a Neural Network model
3In addition, the parser generates semantic representations
for fragments of the sentence to handle ungrammatical sen-
tences, etc.
22
for sentence level discourse segmentation that uses
syntactic features and discourse cues. Our model
was trained and tested on RST-DT (2002) and
achieves a performance of up to 86.12% F-Score,
which is comparable to Soricut and Marcu (2003).
We plan to use this model on our corpus as well.
4 Discourse Parsing
Once the EDUs have been identified by the dis-
course segmenter, the entire discourse structure of
text needs to be constructed. This concerns deter-
mining which text segments are related and what re-
lation to assign to those segments. Our discourse
parsing model consists of a rhetorical relation clas-
sifier, a sentence level discourse parser and a docu-
ment level discourse parser.
4.1 Rhetorical Relation Classifier
In a preliminary investigation (Subba et al, 2006),
we modeled the problem of identifying rhetorical re-
lations as a classification problem using rich verb se-
mantics only.
Most of the work in NLP that involves learn-
ing has used more traditional machine learning
paradigms like decision-tree algorithms and SVMs.
However, we did not find them suitable for our data
which is represented in first order logic (FOL). We
found Progol (Muggleton, 1995), an ILP system, ap-
propriate for our needs. The general problem spec-
ification for Progol (ILP) is given by the following
posterior sufficiency property:
B ? H |= E
Given the background knowledge B and the ex-
amples E, Progol finds the simplest consistent hy-
pothesis H, such that B and H entails E. The rich
verb semantic representation of pairs of EDUs form
the background knowledge and the manually anno-
tated rhetorical relations between the pairs of EDUs
serve as the positive examples.4 An A*-like search
is used to search for the most probable hypothesis.
Given our model, we are able to learn rules such as
the ones given in Figure 2. Due to the lack of space
we only explain RULE1 here. RULE1 states that
4The output from the parser was further processed into def-
inite clauses. Positive examples are represented as ground unit
clauses.
RULE1:
relation(EDU1,EDU2,?before:after?) :- motion(EDU1,event0,during,C),
location(EDU2,event0,start,C,D).
RULE2:
relation(EDU1,EDU2,?act:goal?) :- cause(EDU1,C,event0),
together(EDU1,event0,end,physical,F,G),cause(EDU2,C,event0).
Figure 2: Examples of Rules learned by Progol
there is a theme (C) in motion during the event in
EDU1 (the first EDU) and that C is located in loca-
tion D at the start of the event in EDU2 (the second
EDU).
We trained our classifier on 423 examples and
tested it on 85 examples.5 A majority function base-
line performs at a 51.7 F-Score. Our model outper-
forms this baseline with an F-Score of 60.24.
Relation Precision Recall F-Score
goal:act 31.57 26.08 28.57
step1:step2 75 75 75
before:after 54.5 54.5 54.5
criterion:act 71.4 71.4 71.4
Total 61.7 58.8 60.24
Table 1: Rhetorical Relation Classifier Result
This study has shown that it is possible to learn
rules from FOL semantic representations using In-
ductive Logic Programming to classify rhetorical re-
lations. However, it is not yet clear how useful event
semantics is for discourse parsing. In the future, we
intend to extend our model to incorporate syntactic
and lexical information as well. Such an extension
will allow us to assess the contribution of event se-
mantics.
4.2 Building Discourse Parse Trees
In addition to extending the rhetorical relation clas-
sifier, our future work will involve building the dis-
course parse tree at the sentence level and at the doc-
ument level. At the document level, the input will
be the sentence level discourse parse trees and the
output will be the discourse structure of the entire
5For this preliminary experiment, we decided to use only
those relation sets that had more than 50 examples and those
that were classified as goal:act, step1:step2, criterion:act or be-
fore:after
23
document.
When combining two text segments, promotion
sets that approximate the most important EDUs of
the text segments will be used. As a starting point,
we propose to build sentence level DPTs bottom-up.
EDUs that are subsumed by the same syntactic con-
stituent (usually an S, S-Bar, VP) will be combined
together into a larger text segment recursively until
the the DPT at the root level has been constructed.
At the document level, the DPT will be built us-
ing a shift-reduce parser as in Marcu (2000). How-
ever, unlike Marcu (2000), there will only be one
shift and one reduce operation. The reduce oper-
ation will be determined by the rhetorical relation
classifier and an additional module that will deter-
mine all the possible attachment points for an in-
coming sentence level DPT. An incoming sentence
level DPT may be attached to any node on the right
frontier of the left DPT. Lexical cohesion will be
used to rank the possible attachment points. For both
sentence level discourse parsing and document level
discourse parsing, the rhetorical relation classifier
will be used to determine the informational relation
between the text segments.
5 Conclusion
In conclusion, this thesis will provide a computa-
tional model for parsing the discourse structure of
text based on informational relations. Our approach
exploits event semantic information of the EDUs.
Hence, it will provide a measurement of how helpful
event semantics can be in uncovering the discourse
structure of text. As a consequence, it will also shed
some light on the coverage of the lexical resources
we are using. Other contributions of our work in-
clude a parser that builds event semantic represen-
tations of sentences based on rich verb semantics
and noun semantics and a data driven automatic dis-
course segmenter that determines the minimal units
of discourse.
References
Buitelaar, P.: CoreLex: Systematic Polysemy and Under-
specification. Ph.D. thesis, Computer Science, Bran-
deis University, February 1998.
Huong Le Thanh, G. A. and Huyck., C.: Automated dis-
course segmentation by syntactic information and cue
phrases. International Conference on Artificial Intelli-
gence and Applications, 2004.
Kipper, K., H. T. D. and Palmer., M.: Class-based con-
struction of a verb lexicon. AAAI-2000, Proceedings
of the Seventeenth National Conference on Artificial
Intelligence, 2000.
Livia Polanyi, Christopher Culy, M. H. v. d. B. G. L. T.
and Ahn., D.: Sentential structure and discourse pars-
ing. ACL 2004, Workshop on Discourse Annotation,
2004.
Marcu, D.: Instructions for Manually Annotating the
Discourse Structures of Texts. Technical Report, Uni-
versity of Southern California, 1999.
Marcu, D.: The theory and practice of discourse parsing
and summarization. Cambridge, Massachusetts, Lon-
don, England, MIT Press, 2000.
Moser, M. G., Moore, J. D., and Glendening, E.: In-
structions for Coding Explanations: Identifying Seg-
ments, Relations and Minimal Units. University of
Pittsburgh, Department of Computer Science, 1996.
Muggleton., S. H.: Inverse entailment and progol.
In New Generation Computing Journal, 13:245?286,
1995.
Rose?, C. P. and Lavie., A.: Balancing robustness and ef-
ficiency in unification-augmented context-free parsers
for large practical applications. In Jean-Clause Junqua
and Gertjan van Noord, editors, Robustness in Lan-
guage and Speech Technology, 2000.
RST-DT.: Rst discourse treebank. Linguistic Data Con-
sortium., 2002.
Sporleder, C. and Lascarides., A.: Exploiting linguistic
cues to classify rhetorical relations. Recent Advances
in Natural Language Processing, 2005.
Soricut, R. and Marcu., D.: Sentence level discourse
parsing using syntactic and lexical information. Pro-
ceedings of the HLT and NAACL Conference, 2003.
Subba, R., Di Eugenio, B., E. T.: Building lexical
resources for princpar, a large coverage parser that
generates principled semantic representations. LREC
2006, 2006.
Subba, R., Di Eugenio, B., S. N. K.: Learning FOL
rules based on rich verb semantic representations to
automatically label rhetorical relations. EACL 2006,
Workshop on Learning Structured Information in Nat-
ural Language Applications, 2006.
Wellner, B., Pustejovsky, J., C. H. R. S. and Rumshisky.,
A.: Classification of discourse coherence rela-
tions: An exploratory study using multiple knowledge
sources. SIGDIAL Workshop on Discourse and Dia-
logue, 2006.
24
The problem of ontology alignment on the web: a first report
Davide Fossati and Gabriele Ghidoni and Barbara Di Eugenio
and Isabel Cruz and Huiyong Xiao and Rajen Subba
Computer Science
University of Illinois
Chicago, IL, USA
dfossa1@uic.edu, red.one.999@virgilio.it, bdieugen@cs.uic.edu
ifc@cs.uic.edu, hxiao2@uic.edu, rsubba@cs.uic.edu
Abstract
This paper presents a general architec-
ture and four algorithms that use Natu-
ral Language Processing for automatic on-
tology matching. The proposed approach
is purely instance based, i.e., only the
instance documents associated with the
nodes of ontologies are taken into account.
The four algorithms have been evaluated
using real world test data, taken from the
Google and LookSmart online directories.
The results show that NLP techniques ap-
plied to instance documents help the sys-
tem achieve higher performance.
1 Introduction
Many fundamental issues about the viability and
exploitation of the web as a linguistic corpus have
not been tackled yet. The web is a massive reposi-
tory of text and multimedia data. However, there is
not a systematic way of classifying and retrieving
these documents. Computational Linguists are of
course not the only ones looking at these issues;
research on the Semantic Web focuses on pro-
viding a semantic description of all the resources
on the web, resulting into a mesh of information
linked up in such a way as to be easily process-
able by machines, on a global scale. You can think
of it as being an efficient way of representing data
on the World Wide Web, or as a globally linked
database.1 The way the vision of the Semantic
Web will be achieved, is by describing each doc-
ument using languages such as RDF Schema and
OWL, which are capable of explicitly expressing
the meaning of terms in vocabularies and the rela-
tionships between those terms.
1http://infomesh.net/2001/swintro/
The issue we are focusing on in this paper is
that these languages are used to define ontologies
as well. If ultimately a single ontology were used
to describe all the documents on the web, sys-
tems would be able to exchange information in a
transparent way for the end user. The availability
of such a standard ontology would be extremely
helpful to NLP as well, e.g., it would make it far
easier to retrieve all documents on a certain topic.
However, until this vision becomes a reality, a plu-
rality of ontologies are being used to describe doc-
uments and their content. The task of automatic
ontology alignment ormatching (Hughes and Ash-
pole, 2005) then needs to be addressed.
The task of ontology matching has been typi-
cally carried out manually or semi-automatically,
for example through the use of graphical user in-
terfaces (Noy and Musen, 2000). Previous work
has been done to provide automated support to this
time consuming task (Rahm and Bernstein, 2001;
Cruz and Rajendran, 2003; Doan et al, 2003;
Cruz et al, 2004; Subba and Masud, 2004). The
various methods can be classified into two main
categories: schema based and instance based.
Schema based approaches try to infer the seman-
tic mappings by exploiting information related to
the structure of the ontologies to be matched, like
their topological properties, the labels or descrip-
tion of their nodes, and structural constraints de-
fined on the schemas of the ontologies. These
methods do not take into account the actual data
classified by the ontologies. On the other hand,
instance based approaches look at the information
contained in the instances of each element of the
schema. These methods try to infer the relation-
ships between the nodes of the ontologies from
the analysis of their instances. Finally, hybrid
approaches combine schema and instance based
51
methods into integrated systems.
Neither instance level information, nor NLP
techniques have been extensively explored in pre-
vious work on ontology matching. For exam-
ple, (Agirre et al, 2000) exploits documents (in-
stances) on the WWW to enrich WordNet (Miller
et al, 1990), i.e., to compute ?concept signatures,?
collection of words that significantly distinguish
one sense from another, however, not directly for
ontology matching. (Liu et al, 2005) uses doc-
uments retrieved via queries augmented with, for
example, synonyms that WordNet provides to im-
prove the accuracy of the queries themselves, but
not for ontology matching. NLP techniques such
as POS tagging, or parsing, have been used for
ontology matching, but on the names and defini-
tions in the ontology itself, for example, in (Hovy,
2002), hence with a schema based methodology.
In this paper, we describe the results we ob-
tained when using some simple but effective NLP
methods to align web ontologies, using an instance
based approach. As we will see, our results show
that more sophisticated methods do not necessar-
ily lead to better results.
2 General architecture
The instance based approach we propose uses
NLP techniques to compute matching scores
based on the documents classified under the nodes
of ontologies. There is no assumption on the struc-
tural properties of the ontologies to be compared:
they can be any kind of graph representable in
OWL. The instance documents are assumed to be
text documents (plain text or HTML).
The matching process starts from a pair of on-
tologies to be aligned. The two ontologies are
traversed and, for each node having at least one
instance, the system computes a signature based
on the instance documents. Then, the signatures
associated to the nodes of the two ontologies are
compared pairwise, and a similarity score for each
pair is generated. This score could then be used
to estimate the likelihood of a match between a
pair of nodes, under the assumption that the se-
mantics of a node corresponds to the semantics of
the instance documents classified under that node.
Figure 1 shows the architecture of our system.
The two main issues to be addressed are (1)
the representation of signatures and (2) the def-
inition of a suitable comparison metric between
signatures. For a long time, the Information Re-
Ontologiesdescription(OWL)Instancedocuments(HTML orplain text) NodeSignaturesCreation SignaturesComparison SimilarityScores
FileSystem WordNet
Figure 1: Ontology aligment architecture
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion Tokengroupingand counting
Figure 2: Baseline signature creation
trieval community has succesfully adopted a ?bag
of words? approach to effectively represent and
compare text documents. We start from there to
define a general signature structure and a metric to
compare signatures.
A signature is defined as a function S : K ?
R+, mapping a finite set of keys (which can be
complex objects) to positive real values. With a
signature of that form, we can use the cosine sim-
ilarity metric to score the similarity between two
signatures:
simil(S1, S2) =
?
p S1(kp)S2(kp)
?
?
i S1(ki)
2 ?
??
j S2(kj)
2
kp ? K1 ?K2, ki ? K1, kj ? K2
The cosine similarity formula produces a value
in the range [0, 1]. The meaning of that value de-
pends on the algorithm used to build the signa-
ture. In particular, there is no predefined thresh-
old that can be used to discriminate matches from
non-matches. However, such a threshold could be
computed a-posteriori from a statistical analysis of
experimental results.
2.1 Signature generation algorithms
For our experiments, we defined and implemented
four algorithms to generate signatures. The four
algorithms make use of text and language process-
ing techniques of increasing complexity.
2.1.1 Algorithm 1: Baseline signature
The baseline algorithm performs a very simple
sequence of text processing, schematically repre-
sented in Figure 2.
52
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion
Noungroupingand countingPOS tagging
Figure 3: Noun signature creation
HTML tags are first removed from the in-
stance documents. Then, the texts are tokenized
and punctuation is removed. Everything is then
converted to lowercase. Finally, the tokens are
grouped and counted. The final signature has the
form of a mapping table token ? frequency.
The main problem we expected with this
method is the presence of a lot of noise. In fact,
many ?irrelevant? words, like determiners, prepo-
sitions, and so on, are added to the final signature.
2.1.2 Algorithm 2: Noun signature
To cope with the problem of excessive noise,
people in IR often use fixed lists of stop words
to be removed from the texts. Instead, we intro-
duced a syntax based filter in our chain of pro-
cessing. The main assuption is that nouns are the
words that carry most of the meaning for our kind
of document comparison. Thus, we introduced
a part-of-speech tagger right after the tokeniza-
tion module (Figure 3). The results of the tagger
are used to discard everything but nouns from the
input documents. The part-of-speech tagger we
used ?QTAG 3.1 (Tufis and Mason, 1998), readily
available on the web as a Java library? is a Hidden
Markov Model based statistical tagger.
The problems we expected with this approach
are related to the high specialization of words in
natural language. Different nouns can bear simi-
lar meaning, but our system would treat them as if
they were completely unrelated words. For exam-
ple, the words ?apple? and ?orange? are semanti-
cally closer than ?apple? and ?chair,? but a purely
syntactic approach would not make any difference
between these two pairs. Also, the current method
does not include morphological processing, so dif-
ferent inflections of the same word, such as ?ap-
ple? and ?apples,? are treated as distinct words.
In further experiments, we also considered
verbs, another syntactic category of words bearing
a lot of semantics in natural language. We com-
puted signatures with verbs only, and with verbs
and nouns together. In both cases, however, the
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion
Synsetsgroupingand counting
POS tagging
Noun lookupon WordNet SynsetshierarchicalexpansionNoungroupingand counting
Figure 4: WordNet signature creation
performance of the system was worse. Thus, we
will not consider verbs in the rest of the paper.
2.1.3 Algorithm 3: WordNet signature
To address the limitations stated above, we used
the WordNet lexical resource (Miller et al, 1990).
WordNet is a dictionary where words are linked
together by semantic relationships. In Word-
Net, words are grouped into synsets, i.e., sets of
synonyms. Each synset can have links to other
synsets. These links represent semantic relation-
ships like hypernymy, hyponymy, and so on.
In our approach, after the extraction of nouns
and their grouping, each noun is looked up on
WordNet (Figure 4). The synsets to which the
noun belongs are added to the final signature in
place of the noun itself. The signature can also
be enriched with the hypernyms of these synsets,
up to a specified level. The final signature has the
form of a mapping synset ? value, where value is
a weighted sum of all the synsets found.
Two important parameters of this method are
related to the hypernym expansion process men-
tioned above. The first parameter is the maximum
level of hypernyms to be added to the signature
(hypernym level). A hypernym level value of 0
would make the algorithm add only the synsets of
a word, without any hypernym, to the signature. A
value of 1 would cause the algorithm to add also
their parents in the hypernym hierarchy to the sig-
nature. With higher values, all the ancestors up to
the specified level are added. The second parame-
ter, hypernym factor, specifies the damping of the
weight of the hypernyms in the expansion process.
Our algorithm exponentially dampens the hyper-
nyms, i.e., the weigth of a hypernym decreases ex-
ponentially as its level increases. The hypernym
factor is the base of the exponential function.
In general, a noun can have more than one
sense, e.g., ?apple? can be either a fruit or a tree.
This is reflected in WordNet by the fact that a
noun can belong to multiple synsets. With the
current approach, the system cannot decide which
53
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion
Synsetsgroupingand counting
POS tagging
Noun lookupon WordNet SynsetshierarchicalexpansionWord sensedisambigua-tion
Figure 5: Disambiguated signature creation
sense is the most appropriate, so all the senses
of a word are added to the final signature, with
a weight inversely proportional to the number of
possible senses of that word. This fact poten-
tially introduces semantic noise in the signature,
because many irrelevant senses might be added to
the signature itself.
Another limitation is that a portion of the nouns
in the source texts cannot be located in WordNet
(see Figure 6). Thus, we also tried a variation (al-
gorithm 3+2) that falls back on to the bare lexi-
cal form of a noun if it cannot be found in Word-
Net. This variation, however, resulted in a slight
decrease of performance.
2.1.4 Algorithm 4: Disambiguated signature
The problem of having multiple senses for each
word calls for the adoption of word sense dis-
ambiguation techniques. Thus, we implemented
a word sense disambiguator algorithm, and we
inserted it into the signature generation pipeline
(Figure 5). For each noun in the input documents,
the disambiguator takes into account a specified
number of context words, i.e., nouns preceding
and/or following the target word. The algorithm
computes a measure of the semantic distance be-
tween the possible senses of the target word and
the senses of each of its context words, pair-
wise. A sense for the target word is chosen such
that the total distance to its context is minimized.
The semantic distance between two synsets is de-
fined here as the minimum number of hops in
the WordNet hypernym hierarchy connecting the
two synsets. This definition allows for a rela-
tively straightforward computation of the seman-
tic distance using WordNet. Other more sophisti-
cated definitions of semantic distance can be found
in (Patwardhan et al, 2003). The word sense
disambiguation algorithm we implemented is cer-
tainly simpler than others proposed in the litera-
ture, but we used it to see whether a method that is
relatively simple to implement could still help.
The overall parameters for this signature cre-
ation algorithm are the same as the WordNet sig-
nature algorithm, plus two additional parameters
for the word sense disambiguator: left context
length and right context length. They represent re-
spectively how many nouns before and after the
target should be taken into account by the dis-
ambiguator. If those two parameters are both set
to zero, then no context is provided, and the first
possible sense is chosen. Notice that even in this
case the behaviour of this signature generation al-
gorithm is different from the previous one. In
a WordNet signature, every possible sense for a
word is inserted, whereas in a WordNet disam-
biguated signature only one sense is added.
3 Experimental setting
All the algorithms described in the previous sec-
tion have been fully implemented in a coherent
and extensible framework using the Java program-
ming language, and evaluation experiments have
been run. This section describes how the experi-
ments have been conducted.
3.1 Test data
The evaluation of ontology matching approaches
is usually made difficult by the scarceness of test
ontologies readily available in the community.
This problem is even worse for instance based ap-
proaches, because the test ontologies need also to
be ?filled? with instance documents. Also, we
wanted to test our algorithms with ?real world?
data, rather than toy examples.
We were able to collect suitable test data start-
ing from the ontologies published by the Ontology
Alignment Evaluation Initiative 2005 (Euzenat et
al., 2005). A section of their data contained an
OWL representation of fragments of the Google,
Yahoo, and LookSmart web directories. We ?re-
verse engineered? some of this fragments, in or-
der to reconstruct two consistent trees, one rep-
resenting part of the Google directory structure,
the other representing part of the LookSmart hi-
erarchy. The leaf nodes of these trees were filled
with instances downloaded from the web pages
classified by the appropriate directories. With this
method, we were able to fill 7 nodes of each ontol-
ogy with 10 documents per node, for a total of 140
documents. Each document came from a distinct
web page, so there was no overlap in the data to be
compared. A graphical representation of our two
test ontologies, source and target, is shown in Fig-
54
ure 6. The darker outlined nodes are those filled
with instance documents. For the sake of readabil-
ity, the names of the nodes corresponding to real
matches are the same. Of course, this informa-
tion is not used by our algorithms, which adopt a
purely instance based approach. Figure 6 also re-
ports the size of the instance documents associated
to each node: total number of words, noun tokens,
nouns, and nouns covered by WordNet.
3.2 Parameters
The experiments have been run with several com-
binations of the relevant parameters: number of
instance documents per node (5 or 10), algorithm
(1 to 4), extracted parts of speech (nouns, verbs, or
both), hypernym level (an integer value equal or
greater than zero), hypernym factor (a real num-
ber), and context length (an integer number equal
or greater than zero). Not all of the parameters are
applicable to every algorithm. The total number of
runs was 90.
4 Results
Each run of the system with our test ontologies
produced a set of 49 values, representing the
matching score of every pair of nodes containing
instances across the two ontologies. Selected ex-
amples of these results are shown in Tables 1, 2,
3, and 4. In the experiments shown in those ta-
bles, 10 instance documents for each node were
used to compute the signatures. Nodes that ac-
tually match (identified by the same label, e.g.,
?Canada? and ?Canada?) should show high sim-
ilarity scores, whereas nodes that do not match
(e.g., ?Canada? and ?Dendrochronology?), should
have low scores. Better algorithms would have
higher scores for matching nodes, and lower score
for non-matching ones. Notice that the two nodes
?Egypt? and ?Pyramid Theories,? although intu-
itively related, have documents that take different
perspectives on the subject. So, the algorithms
correctly identify the nodes as being different.
Looking at the results in this form makes it dif-
ficult to precisely assess the quality of the algo-
rithms. To do so, a statistical analysis has to be
performed. For each table of results, let us parti-
tion the scores in two distinct sets:
A = {simil(nodei, nodej) | real match = true}
B = {simil(nodei, nodej) | real match = false}
Target node
Canada
Canada 0.95 0.89 0.89 0.91 0.87 0.86 0.92
0.90 0.97 0.91 0.90 0.88 0.87 0.92
Egypt 0.86 0.89 0.91 0.87 0.86 0.88 0.90
Megaliths 0.90 0.91 0.99 0.93 0.95 0.94 0.93
Museums 0.89 0.88 0.90 0.93 0.88 0.87 0.90
0.88 0.88 0.95 0.91 0.99 0.93 0.91
0.87 0.87 0.86 0.88 0.82 0.82 0.96
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 1: Results ? Baseline signature algorithm
Target node
Canada
Canada 0.67 0.20 0.14 0.35 0.08 0.08 0.41
0.22 0.80 0.15 0.22 0.09 0.09 0.25
Egypt 0.13 0.23 0.26 0.22 0.17 0.24 0.25
Megaliths 0.28 0.20 0.85 0.37 0.22 0.27 0.33
Museums 0.30 0.19 0.18 0.58 0.08 0.14 0.27
0.13 0.12 0.26 0.18 0.96 0.14 0.17
0.42 0.20 0.17 0.26 0.09 0.11 0.80
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 2: Results ? Noun signature algorithm
Target node
Canada
Canada 0.79 0.19 0.19 0.38 0.15 0.06 0.56
0.26 0.83 0.18 0.20 0.16 0.07 0.24
Egypt 0.17 0.24 0.32 0.21 0.31 0.30 0.27
Megaliths 0.39 0.21 0.81 0.41 0.40 0.25 0.42
Museums 0.31 0.14 0.17 0.70 0.11 0.11 0.26
0.24 0.20 0.42 0.29 0.91 0.21 0.29
0.56 0.17 0.22 0.25 0.15 0.08 0.84
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 3: Results ? WordNet signature algorithm
(hypernym level=0)
55
TopGoogle
Science
Social_Sciences
Archaeology
Egypt
Alternative
Megaliths South_America
Nazca_Lines
Methodology
Dendrochronology
Museums Publications
Europe
UnitedKingdom
Organizations
NorthAmerica
Canada
TopLookSmart
Science_&_Health
Social_Science
Archaeology
Pyramid_Theories
Topics
Megaliths Nazca_Lines
Science
Dendrochronology
Museums Publications
UnitedKingdom
Associations
Canada
Seven_Wonders_of_the_World
Gyza_Pyramids
Source Target
44307; 16001;3957; 2220 15447; 4400;1660; 1428 18754; 5574; 1725; 1576
7362; 2377; 953; 823
2872; 949;499; 441
9620; 3257; 1233; 1001 3972; 1355;603; 541 3523; 1270;617; 555
23039; 7926; 1762; 1451
13705; 3958;1484; 1303
6171; 2333;943; 844
10721; 3280; 1099; 9887841; 2486; 869; 769
17196; 5529;1792; 1486
Figure 6: Ontologies used in the experiments. The numbers below the leaves indicate the size of instance
documents: # of words; # of noun tokens; # of nouns; # of nouns in WordNet
Target node
Canada
Canada 0.68 0.18 0.13 0.33 0.12 0.05 0.44
0.23 0.79 0.15 0.20 0.14 0.07 0.23
Egypt 0.15 0.23 0.28 0.22 0.27 0.31 0.27
Megaliths 0.30 0.18 0.84 0.37 0.34 0.27 0.33
Museums 0.29 0.16 0.15 0.60 0.11 0.10 0.24
0.20 0.17 0.38 0.26 0.89 0.21 0.26
0.45 0.17 0.18 0.24 0.15 0.08 0.80
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 4: Results ? Disambiguated signature al-
gorithm (hypernym level=0, left context=1, right
context=1)
With our test data, we would have 6 values in
set A and 43 values in set B. Then, let us com-
pute average and standard deviation of the values
included in each set. The average of A represents
the expected score that the system would assign
to a match; likewise, the average of B is the ex-
pected score of a non-match. We define the fol-
lowing measure to compare the performance of
our matching algorithms, inspired by ?effect size?
from (VanLehn et al, 2005):
discrimination size =
avg(A) ? avg(B)
stdev(A) + stdev(B)
Higher discrimination values mean that the
scores assigned to matches and non-matches are
more ?far away,? making it possible to use those
scores to make more reliable decisions about the
matching degree of pairs of nodes.
Table 5 shows the values of discrimination size
(last column) out of selected results from our ex-
periments. The algorithm used is reported in the
first column, and the values of the other relevant
parameters are indicated in other columns. We can
make the following observations.
? Algorithms 2, 3, and 4 generally outperform
the baseline (algorithm 1).
? Algorithm 2 (Noun signature), which still
uses a fairly simple and purely syntactical
technique, shows a substantial improvement.
Algorithm 3 (WordNet signature), which in-
troduces some additional level of semantics,
has even better performance.
? In algorithms 3 and 4, hypernym expansion
looks detrimental to performance. In fact, the
best results are obtained with hypernym level
equal to zero (no hypernym expansion).
? The word sense disambiguator implemented
in algorithm 4 does not help. Even though
disambiguating with some limited context
(1 word before and 1 word after) provides
slightly better results than choosing the first
available sense for a word (context length
equal to zero), the overall results are worse
than adding all the possible senses to the sig-
nature (algorithm 3).
? Using only 5 documents per node signifi-
cantly degrades the performance of all the al-
gorithms (see the last 5 lines of the table).
5 Conclusions and future work
The results of our experiments point out several
research questions and directions for future work,
56
Alg Docs POS Hyp lev Hyp fac L cont R cont Avg (A) Stdev (A) Avg (B) Stdev (B) Discrimination size1 10 0.96 0.02 0.89 0.03 1.372 10 noun 0.78 0.13 0.21 0.09 2.552 10 verb 0.64 0.20 0.31 0.11 1.042 10 nn+vb 0.77 0.14 0.21 0.09 2.483 10 noun 0 0.81 0.07 0.25 0.12 3.083 10 noun 1 1 0.85 0.07 0.41 0.12 2.353 10 noun 1 2 0.84 0.07 0.34 0.12 2.643 10 noun 1 3 0.83 0.07 0.31 0.12 2.803 10 noun 2 1 0.90 0.06 0.62 0.11 1.643 10 noun 2 2 0.86 0.07 0.45 0.12 2.183 10 noun 2 3 0.84 0.07 0.36 0.12 2.563 10 noun 3 1 0.95 0.04 0.78 0.08 1.443 10 noun 3 2 0.88 0.07 0.52 0.12 1.913 10 noun 3 3 0.85 0.07 0.38 0.12 2.453+2 10 noun 0 0 0.80 0.09 0.21 0.11 2.943+2 10 noun 1 2 0.83 0.08 0.30 0.11 2.733+2 10 noun 2 2 0.85 0.08 0.39 0.11 2.404 10 noun 0 0 0 0.80 0.12 0.24 0.10 2.644 10 noun 0 1 1 0.77 0.11 0.22 0.10 2.674 10 noun 0 2 2 0.77 0.11 0.23 0.10 2.594 10 noun 1 2 0 0 0.82 0.10 0.29 0.10 2.564 10 noun 1 2 1 1 0.80 0.10 0.34 0.10 2.274 10 noun 1 2 2 2 0.80 0.10 0.35 0.10 2.22
1 5 noun 0.93 0.05 0.86 0.04 0.882 5 noun 0.66 0.23 0.17 0.08 1.613 5 noun 0 0.70 0.17 0.21 0.11 1.764 5 noun 0 0 0 0.69 0.21 0.20 0.09 1.634 5 noun 0 1 1 0.64 0.21 0.18 0.08 1.58
Table 5: Results ? Discrimination size
some more specific and some more general. As
regards the more specific issues,
? Algorithm 2 does not perform morphological
processing, whereas Algorithm 3 does. How
much of the improved effectiveness of Algo-
rithm 3 is due to this fact? To answer this
question, Algorithm 2 could be enhanced to
include a morphological processor.
? The effectiveness of Algorithms 3 and 4 may
be hindered by the fact that many words are
not yet included in theWordNet database (see
Figure 6). Falling back on to Algorithm 2
proved not to be a solution. The impact of the
incompleteness of the lexical resource should
be investigated and assessed more precisely.
Another venue of research may be to exploit
different thesauri, such as the ones automati-
cally derived as in (Curran andMoens, 2002).
? The performance of Algorithm 4 might be
improved by using more sophisticated word
sense disambiguation methods. It would also
be interesting to explore the application of
the unsupervised method described in (Mc-
Carthy et al, 2004).
As regards our long term plans, first, structural
properties of the ontologies could potentially be
exploited for the computation of node signatures.
This kind of enhancement would make our system
move from a purely instance based approach to a
combined hybrid approach based on schema and
instances.
More fundamentally, we need to address the
lack of appropriate, domain specific resources that
can support the training of algorithms and models
appropriate for the task at hand. WordNet is a very
general lexicon that does not support domain spe-
cific vocabulary, such as that used in geosciences
or in medicine or simply that contained in a sub-
ontology that users may define according to their
interests. Of course, we do not want to develop
by hand domain specific resources that we have to
change each time a new domain arises.
The crucial research issue is how to exploit ex-
tremely scarce resources to build efficient and ef-
fective models. The issue of scarce resources
makes it impossible to use methods that are suc-
cesful at discriminating documents based on the
words they contain but that need large corpora
for training, for example Latent Semantic Anal-
ysis (Landauer et al, 1998). The experiments de-
scribed in this paper could be seen as providing
57
a bootstrapped model (Riloff and Jones, 1999; Ng
and Cardie, 2003)?in ML, bootstrapping requires
to seed the classifier with a small number of well
chosen target examples. We could develop a web
spider, based on the work described on this paper,
to automatically retrieve larger amounts of train-
ing and test data, that in turn could be processed
with more sophisticated NLP techniques.
Acknowledgements
This work was partially supported by NSF Awards
IIS?0133123, IIS?0326284, IIS?0513553, and
ONR Grant N00014?00?1?0640.
References
Eneko Agirre, Olatz Ansa, Eduard Hovy, and David
Martinez. 2000. Enriching very large ontologies
using the WWW. In ECAI Workshop on Ontology
Learning, Berlin, August.
Isabel F. Cruz and Afsheen Rajendran. 2003. Ex-
ploring a new approach to the alignment of ontolo-
gies. In Workshop on Semantic Web Technologies
for Searching and Retrieving Scientific Data, in co-
operation with the International Semantic Web Con-
ference.
Isabel F. Cruz, William Sunna, and Anjli Chaudhry.
2004. Semi-automatic ontology alignment for
geospatial data integration. GIScience, pages 51?
66.
James Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Work-
shop on Unsupervised Lexical Acquisition, pages
59?67, Philadelphia, PA, USA.
AnHai Doan, Jayant Madhavan, Robin Dhamankar, Pe-
dro Domingos, and Alon Halevy. 2003. Learning to
match ontologies on the semantic web. VLDB Jour-
nal, 12(4):303?319.
Je?ro?me Euzenat, Heiner Stuckenschmidt, and
Mikalai Yatskevich. 2005. Introduction
to the ontology alignment evaluation 2005.
http://oaei.inrialpes.fr/2005/results/oaei2005.pdf.
Eduard Hovy. 2002. Comparing sets of semantic rela-
tions in ontology. In R. Green, C. A. Bean, and S. H.
Myaeng, editors, Semantics of Relationships: An In-
terdisciplinary Perspective, pages 91?110. Kluwer.
T. C. Hughes and B. C. Ashpole. 2005. The semantics
of ontology alignment. Draft Paper, Lockheed Mar-
tin Advanced Technology Laboratories, Cherry Hill,
NJ. http://www.atl.lmco.com/projects/ontology/ pa-
pers/ SOA.pdf.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to Latent Semantic Analy-
sis. Discourse Processes, 25:259?284.
Shuang Liu, Clement Yu, and Weiyi Meng. 2005.
Word sense disambiguation in queries. In ACM
Conference on Information and Knowledge Man-
agement (CIKM2005), Bremen, Germany.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In 42nd Annual Meeting of the As-
sociation for Computational Linguistics, Barcelona,
Spain.
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. J. Miller. 1990. Introduction to wordnet: an on-
line lexical database. International Journal of Lexi-
cography, 3 (4):235?244.
Vincent Ng and Claire Cardie. 2003. Bootstrapping
coreference classifiers with multiple machine learn-
ing algorithms. In The 2003 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2003).
Natalya Fridman Noy and Mark A. Musen. 2000.
Prompt: Algorithm and tool for automated ontology
merging and alignment. In National Conference on
Artificial Intelligence (AAAI).
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using semantic relatedness for
word sense disambiguation. In Fourth International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CiCLING-03), Mexico City.
Erhard Rahm and Philip A. Bernstein. 2001. A sur-
vey of approaches to automatic schema matching.
VLDB Journal, 10(4):334?350.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI-99, Sixteenth National Con-
ference on Artificial Intelligence.
Rajen Subba and Sadia Masud. 2004. Automatic gen-
eration of a thesaurus using wordnet as a means to
map concepts. Tech report, University of Illinois at
Chicago.
Dan Tufis and Oliver Mason. 1998. Tagging romanian
texts: a case study for qtag, a language independent
probabilistic tagger. In First International Confer-
ence on Language Resources & Evaluation (LREC),
pages 589?596, Granada, Spain.
Kurt VanLehn, Collin Lynch, Kay Schulze, Joel
Shapiro, Robert Shelby, Linwood Taylor, Donald
Treacy, Anders Weinstein, and Mary Wintersgill.
2005. The andes physics tutoring system: Five years
of evaluations. In 12th International Conference on
Artificial Intelligence in Education, Amsterdam.
58
Discourse Parsing: Learning FOL Rules based on Rich Verb Semantic
Representations to automatically label Rhetorical Relations
Rajen Subba
Computer Science
University of Illinois
Chicago, IL, USA
rsubba@cs.uic.edu
Barbara Di Eugenio
Computer Science
University of Illinois
Chicago, IL, USA
bdieugen@cs.uic.edu
Su Nam Kim
Department of CSSE
University of Melbourne
Carlton, VIC, Australia
snkim@csse.unimelb.edu.au
Abstract
We report on our work to build a dis-
course parser (SemDP) that uses seman-
tic features of sentences. We use an In-
ductive Logic Programming (ILP) System
to exploit rich verb semantics of clauses
to induce rules for discourse parsing. We
demonstrate that ILP can be used to learn
from highly structured natural language
data and that the performance of a dis-
course parsing model that only uses se-
mantic information is comparable to that
of the state of the art syntactic discourse
parsers.
1 Introduction
The availability of corpora annotated with syntac-
tic information have facilitated the use of prob-
abilistic models on tasks such as syntactic pars-
ing. Current state of the art syntactic parsers
reach accuracies between 86% and 90%, as mea-
sured by different types of precision and recall
(for more details see (Collins, 2003)). Recent
semantic (Kingsbury and Palmer, 2002) and dis-
course (Carlson et al, 2003) annotation projects
are paving the way for developments in seman-
tic and discourse parsing as well. However unlike
syntactic parsing, significant development in dis-
course parsing remains at large.
Previous work on discourse parsing ((Soricut
and Marcu, 2003) and (Forbes et al, 2001))
have focused on syntactic and lexical features
only. However, discourse relations connect
clauses/sentences, hence, descriptions of events
and states. It makes linguistic sense that the
semantics of the two clauses ?generally built
around the semantics of the verbs, composed with
that of their arguments? affects the discourse re-
lation(s) connecting the clauses. This may be
even more evident in our instructional domain,
where relations derived from planning such as
Precondition-Act may relate clauses.
Of course, since semantic information is hard
to come by, it is not surprising that previous work
on discourse parsing did not use it, or only used
shallow word level ontological semantics as spec-
ified in WordNet (Polanyi et al, 2004). But when
rich sentence level semantics is available, it makes
sense to experiment with it for discourse parsing.
A second major difficulty with using such rich
verb semantic information, is that it is rep-
resented using complex data structures. Tradi-
tional Machine Learning methods cannot han-
dle highly structured data such as First Or-
der Logic (FOL), a representation that is suit-
ably used to represent sentence level seman-
tics. Such FOL representations cannot be reduced
to a vector of attribute/value pairs as the rela-
tions/interdependencies that exist among the pred-
icates would be lost.
Inductive Logic Programming (ILP) can learn
structured descriptions since it learns FOL de-
scriptions. In this paper, we present our first steps
using ILP to learn semantic descriptions of dis-
course relations. Also of relevance to the topic of
this workshop, is that discourse structure is inher-
ently highly structured, since discourse structure
is generally described in hierarchical terms: ba-
sic units of analysis, generally clauses, are related
by discourse relations, resulting in more complex
units, which in turn can be related via discourse re-
lations. At the moment, we do not yet address the
problem of parsing at higher levels of discourse.
We intend to build on the work we present in this
paper to achieve that goal.
The task of discourse parsing can be di-
vided into two disjoint sub-problems ((Soricut and
Marcu, 2003) and (Polanyi et al, 2004)). The two
sub-problems are automatic identification of seg-
ment boundaries and the labeling of rhetorical re-
lations. Though we consider the problem of auto-
matic segmentation to be an important part in dis-
course parsing, we have focused entirely on the
latter problem of automatically labeling rhetorical
33
Figure 1: SemDP System Architecture (Discourse Parser)
relations only. Our approach uses rich verb seman-
tics1 of elementary discourse units (EDUs)2 based
on VerbNet(Kipper et al, 2000) as background
knowledge and manually annotated rhetorical re-
lations as training examples. It is trained on a lot
fewer examples than the state of the art syntax-
based discourse parser (Soricut and Marcu, 2003).
Nevertheless, it achieves a comparable level of
performance with an F-Score of 60.24. Figure 1
shows a block diagram of SemDP?s system archi-
tecture. Segmentation, annotation of rhetorical re-
lations and parsing constitute the data collection
phase of the system. Learning is accomplished
using an ILP based system, Progol (Muggleton,
1995). As can be seen in Figure 1, Progol takes
as input both rich verb semantic information of
pairs of EDUs and the rhetorical relations between
them. The goal was to learn rules using the se-
mantic information from pairs of EDUs as in Ex-
ample 1:
(1) EDU1: ?Sometimes, you can add a liquid to the water
EDU2: ?to hasten the process?
relation(EDU1,EDU2,?Act:goal?).
to automatically label unseen examples with the
correct rhetorical relation.
The rest of the paper is organized as follows.
Section 2 describes our data collection methodol-
ogy. In section 3, Progol, the ILP system that we
1The semantic information we used is composed of Verb-
Net semantic predicates that capture event semantics as well
as thematic roles.
2EDUs are minimal discourse units produced as a result
of discourse segmentation.
used to induce rules for discourse parsing is de-
tailed. Evaluation results are presented in section
4 followed by the conclusion in section 5.
2 Data Collection
The lack of corpora annotated with both rhetorical
relations as well as sentence level semantic rep-
resentation led us to create our own corpus. Re-
sources such as (Kingsbury and Palmer, 2002) and
(Carlson et al, 2003) have been developed man-
ually. Since such efforts are time consuming and
costly, we decided to semi-automatically build our
annotated corpus. We used an existing corpus of
instructional text that is about 9MB in size and is
made up entirely of written English instructions.
The two largest components are home repair man-
uals (5Mb) and cooking recipes (1.7Mb). 3
Segmentation. The segmentation of the corpus
was done manually by a human coder. Our seg-
mentation rules are based on those defined in
(Mann and Thompson, 1988). For example, (as
shown in Example 2) we segment sentences in
which a conjunction is used with a clause at the
conjunction site.
(2) You can copy files (//) as well as cut messages.
(//) is the segmentation marker. Sentences are
segmented into EDUs. Not all the segmentation
3It was collected opportunistically off the internet and
from other sources, and originally assembled at the Informa-
tion Technology Research Institute, University of Brighton.
34
rules from (Mann and Thompson, 1988) are im-
ported into our coding scheme. For example, we
do not segment relative clauses. In total, our seg-
mentation resulted in 10,084 EDUs. The seg-
mented EDUs were then annotated with rhetorical
relations by the human coder4 and also forwarded
to the parser as they had to be annotated with se-
mantic information.
2.1 Parsing of Verb Semantics
We integrated LCFLEX (Rose? and Lavie, 2000),
a robust left-corner parser, with VerbNet (Kipper
et al, 2000) and CoreLex (Buitelaar, 1998). Our
interest in decompositional theories of lexical se-
mantics led us to base our semantic representation
on VerbNet.
VerbNet operationalizes Levin?s work and ac-
counts for 4962 distinct verbs classified into 237
main classes. Moreover, VerbNet?s strong syntac-
tic components allow it to be easily coupled with a
parser in order to automatically generate a seman-
tically annotated corpus.
To provide semantics for nouns, we use
CoreLex (Buitelaar, 1998), in turn based on the
generative lexicon(Pustejovsky, 1991). CoreLex
defines basic types such as art (artifact) or com
(communication). Nouns that share the same bun-
dle of basic types are grouped in the same System-
atic Polysemous Class (SPC). The resulting 126
SPCs cover about 40,000 nouns.
We modified and augmented LCFLEX?s exist-
ing lexicon to incorporate VerbNet and CoreLex.
The lexicon is based on COMLEX (Grishman et
al., 1994). Verb and noun entries in the lexicon
contain a link to a semantic type defined in the on-
tology. VerbNet classes (including subclasses and
frames) and CoreLex SPCs are realized as types in
the ontology. The deep syntactic roles are mapped
to the thematic roles, which are defined as vari-
ables in the ontology types. For more details on
the parser see (Terenzi and Di Eugenio, 2003).
Each of the 10,084 EDUs was parsed using the
parser. The parser generates both a syntactic tree
and the associated semantic representation ? for
the purpose of this paper, we only focus on the
latter. Figure 2 shows the semantic representation
generated for EDU1 from Example 1, ?sometimes,
you can add a liquid to the water?.
The semantic representation in Figure 2 is part
4Double annotation and segmentation is currently being
done to assess inter-annotator agreement using kappa.
(*SEM*
((AGENT YOU)
(VERBCLASS ((VNCLASS MIX-22.1-2))) (EVENT +)
(EVENT0
((END
((ARG1 (LIQUID))
(FRAME *TOGETHER) (ARG0 PHYSICAL)
(ARG2 (WATER)))))))
(EVENTSEM
((FRAME *CAUSE) (ARG1 E) (ARG0 (YOU)))))
(PATIENT1 LIQUID)
(PATIENT2 WATER)
(ROOT-VERB ADD))
Figure 2: Parser Output (Semantic Information)
of the F-Structure produced by the parser. The
verb add is parsed for a transitive frame with a PP
modifier that belongs to the verb class ?MIX-22.1-
2?. The sentence contains two PATIENTs, namely
liquid and water. you is identified as the AGENT
by the parser. *TOGETHER and *CAUSE are the
primitive semantic predicates used by VerbNet.
Verb Semantics in VerbNet are defined as events
that are decomposed into stages, namely start, end,
during and result. The semantic representation in
Figure 2 states that there is an event EVENT0 in
which the two PATIENTs are together at the end.
An independent evaluation on a set of 200 sen-
tences from our instructional corpus was con-
ducted. 5 It was able to generate complete parses
for 72.2% and partial parses for 10.9% of the verb
frames that we expected it to parse, given the re-
sources. The parser cannot parse those sentences
(or EDUs) that contain a verb that is not cov-
ered by VerbNet. This coverage issue, coupled
with parser errors, exacerbates the problem of data
sparseness. This is further worsened by the fact
that we require both the EDUs in a relation set
to be parsed for the Machine Learning part of our
work. Addressing data sparseness is an issue left
for future work.
2.2 Annotation of Rhetorical Relations
The annotation of rhetorical relations was done
manually by a human coder. Our coding scheme
builds on Relational Discourse Analysis (RDA)
(Moser and Moore, 1995), to which we made mi-
5The parser evaluation was not based on EDUs but rather
on unsegmented sentences. A sentence contained one or
more EDUs.
35
nor modifications; in turn, as far as discourse rela-
tions are concerned, RDA was inspired by Rhetor-
ical Structure Theory (RST) (Mann and Thomp-
son, 1988).
Rhetorical relations were categorized as infor-
mational, elaborational, temporal and others. In-
formational relations describe how contents in
two relata are related in the domain. These re-
lations are further subdivided into two groups;
causality and similarity. The former group con-
sists of relations between an action and other ac-
tions or between actions and their conditions or
effects. Relations like ?act:goal?, ?criterion:act?
fall under this group. The latter group con-
sists of relations between two EDUs according
to some notion of similarity such as ?restate-
ment? and ?contrast1:contrast2?. Elaborational
relations are interpropositional relations in which
a proposition(s) provides detail relating to some
aspect of another proposition (Mann and Thomp-
son, 1988). Relations like ?general:specific? and
?circumstance:situation? belong to this category.
Temporal relations like ?before:after? capture time
differences between two EDUs. Lastly, the cate-
gory others includes relations not covered by the
previous three categories such as ?joint? and ?inde-
terminate?.
Based on the modified coding scheme manual,
we segmented and annotated our instructional cor-
pus using the augmented RST tool from (Marcu et
al., 1999). The RST tool was modified to incor-
porate our relation set. Since we were only inter-
ested in rhetorical relations that spanned between
two adjacent EDUs 6, we obtained 3115 sets of
potential relations from the set of all relations that
we could use as training and testing data.
The parser was able to provide complete parses
for both EDUs in 908 of the 3115 relation sets.
These constitute the training and test set for Pro-
gol.
The semantic representation for the EDUs along
with the manually annotated rhetorical relations
were further processed (as shown in Figure 4) and
used by Progol as input.
3 The Inductive Logic Programming
Framework
We chose to use Progol, an Inductive Logic Pro-
gramming system (ILP), to learn rules based on
6At the moment, we are concerned with learning relations
between two EDUs at the base level of a Discourse Parse Tree
(DPT) and not at higher levels of the hierarchy.
the data we collected. ILP is an area of research
at the intersection of Machine Learning (ML) and
Logic Programming. The general problem speci-
fication in ILP is given by the following property:
B ?H |= E (3)
Given the background knowledge B and the ex-
amples E, ILP systems find the simplest consistent
hypothesis H, such that B and H entails E.
While most of the work in NLP that involves
learning has used more traditional ML paradigms
like decision-tree algorithms and SVMs, we did
not find them suitable for our data which is rep-
resented as Horn clauses. The requirement of us-
ing a ML system that could handle first order logic
data led us to explore ILP based systems of which
we found Progol most appropriate.
Progol combines Inverse Entailment with
general-to-specific search through a refinement
graph. A most specific clause is derived using
mode declarations along with Inverse Entailment.
All clauses that subsume the most specific clause
form the hypothesis space. An A*-like search
is used to search for the most probable theory
through the hypothesis space. Progol allows arbi-
trary programs as background knowledge and ar-
bitrary definite clauses as examples.
3.1 Learning from positive data only
One of the features we found appealing about Pro-
gol, besides being able to handle first order logic
data, is that it can learn from positive examples
alone.
Learning in natural language is a universal hu-
man process based on positive data only. How-
ever, the usual traditional learning models do not
work well without negative examples. On the
other hand, negative examples are not easy to ob-
tain. Moreover, we found learning from positive
data only to be a natural way to model the task of
discourse parsing.
To make the learning from positive data only
feasible, Progol uses a Bayesian framework. Pro-
gol learns logic programs with an arbitrarily low
expected error using only positive data. Of course,
we could have synthetically labeled examples of
relation sets (pairs of EDUs), that did not belong
to a particular relation, as negative examples. We
plan to explore this approach in the future.
A key issue in learning from positive data
only using a Bayesian framework is the ability
to learn complex logic programs. Without any
36
negative examples, the simplest rule or logic
program, which in our case would be a single
definite clause, would be assigned the highest
score as it captures the most number of examples.
In order to handle this problem, Progol?s scoring
function exercises a trade-off between the size of
the function and the generality of the hypothesis.
The score for a given hypothesis is calculated
according to formula 4.
ln p(H | E) = m ln
( 1
g(H)
)
?sz(H)+dm (4)
sz(H) and g(H) computes the size of the hy-
pothesis and the its generality respectively. The
size of a hypothesis is measured as the number
of atoms in the hypothesis whereas generality is
measured by the number of positive examples the
hypothesis covers. m is the number of examples
covered by the hypothesis and dm is a normaliz-
ing constant. The function ln p(H|E) decreases
with increases in sz(H) and g(H). As the number
of examples covered (m) grow, the requirements
on g(H) become even stricter. This property fa-
cilitates the ability to learn more complex rules
as they are supported by more positive examples.
For more information on Progol and the computa-
tion of Bayes? posterior estimation, please refer to
(Muggleton, 1995).
3.2 Discourse Parsing with Progol
We model the problem of assigning the correct
rhetorical relation as a classification task within
the ILP framework. The rich verb semantic repre-
sentation of pairs of EDUs, as shown in Figure 3 7,
form the background knowledge and the manually
annotated rhetorical relations between the pairs of
EDUs, as shown in Figure 4, serve as the positive
examples in our learning framework. The num-
bers in the definite clauses are ids used to identify
the EDUs.
Progol constructs logic programs based on the
background knowledge and the examples in Fig-
ures 3 and 4. Mode declarations in the Progol in-
put file determines which clause to be used as the
head (i.e. modeh) and which ones to be used in
the body (i.e. modeb) of the hypotheses. Figure 5
shows an abridged set of our mode declarations.
7The output from the parser was further processed into
definite clauses.
...
agent(97,you).
together(97,event0,end,physical,liquid,water).
cause(97,you,e).
patient1(97,liquid).
patient2(97,water).
theme(98,process).
rushed(98,event0,during,process).
cause(98,AGENT98,e).
...
Figure 3: Background Knowledge for Example 1
...
relation(18,19,?Act:goal?).
relation(97,98,?Act:goal?).
relation(1279,1280,?Step1:step2?).
relation(1300,1301,?Step1:step2?).
relation(1310,1311,?Step1:step2?).
relation(412,413,?Before:after?).
relation(441,442,?Before:after?).
...
Figure 4: Positive Examples
Our mode declarations dictate that the predicate
relation be used as the head and the other pred-
icates (has possession, transfer and visible) form
the body of the hypotheses. ?*? indicates that the
number of hypotheses to learn for a given relation
is unlimited. ?+? and ?-? signs indicate variables
within the predicates of which the former is an in-
put variable and the latter an output variable. ?#?
is used to denote a constant. Each argument of the
predicate is a type, whether a constant or a vari-
able. Types are defined as a single definite clause.
Our goal is to learn rules where the LHS of the
rule contains the relation that we wish to learn and
:- modeh(*,relation(+edu,+edu,#relationtype))?
:- modeb(*,has possession(+edu,#event,
#eventstage,+verbarg,+verbarg))?
:- modeb(*,has possession(+edu,#event,
#eventstage,+verbarg,-verbarg))?
:- modeb(*,transfer(+edu,#event,#eventstage,-verbarg))?
:- modeb(*,visible(+edu,#event,#eventstage,+verbarg))?
:- modeb(*,together(+edu,#event,
#eventstage,+verbarg,+verbarg,+verbarg))?
:- modeb(*,rushed(+edu,#event,#eventstage,+verbarg))?
Figure 5: Mode Declarations
37
RULE1:
relation(EDU1,EDU2,?Act:goal?) :-
degradation material integrity(EDU1,event0,result,C),
allow(EDU2,event0,during,C,D).
RULE2:
relation(EDU1,EDU2,?Act:goal?) :-
cause(EDU1,C,D),
together(EDU1,event0,end,E,F,G),
cause(EDU2,C,D).
RULE3:
relation(EDU1,EDU2,?Step1:step2?) :-
together(EDU2,event0,end,C,D,E),
has possession(EDU1,event0,during,C,F).
RULE4:
relation(EDU1,EDU2,?Before:after?) :-
motion(EDU1,event0,during,C),
location(EDU2,event0,start,C,D).
RULE6:
relation(EDU1,EDU2,?Act:goal?) :-
motion(EDU1,event0,during,C).
Figure 6: Rules Learned
the RHS is a CNF of the semantic predicates de-
fined in VerbNet with their arguments. Given the
amount of training data we have, the nature of the
data itself and the Bayesian framework used, Pro-
gol learns simple rules that contain just one or two
clauses on the RHS. 6 of the 68 rules that Progol
manages to learn are shown in Figure 6. RULE4
states that there is a theme in motion during the
event in EDU A (which is the first EDU) and that
the theme is located in location D at the start of
the event in EDU B (the second EDU). RULE2 is
learned from pairs of EDUs such as in Example
1. The simple rules in Figure 6 may not readily
appeal to our intuitive notion of what such rules
should include. It is not clear at this point as to
how elaborate these rules should be, in order to
correctly identify the relation in question. One
of the reasons why more complex rules are not
learned by Progol is that there aren?t enough train-
ing examples. As we add more training data in the
future, we will see if rules that are more elaborate
than the ones in Figure 6 are learned .
4 Evaluation of the Discourse Parser
Table 1 shows the sets of relations for which we
managed to obtain semantic representations (i.e.
for both the EDUs).
Relations like Preparation:act did not yield any
Relation Total Train Test
Set Set
Step1:step2: 232 188 44
Joint: 190
Goal:act: 170 147 23
General:specific: 77
Criterion:act: 53 46 7
Before:after: 53 42 11
Act:side-effect: 38
Co-temp1:co-temp2: 22
Cause:effect: 19
Prescribe-act:wrong-act: 14
Obstacle:situation: 11
Reason:act: 9
Restatement: 6
Contrast1:contrast2: 6
Circumstance:situation: 3
Act:constraint: 2
Criterion:wrong-act: 2
Set:member: 1
Act:justification: 0
Comparison: 0
Preparation:act: 0
Object:attribute: 0
Part:whole: 0
Same-unit: 0
Indeterminate: 0
908 423 85
Table 1: Relation Set Count (Total Counts include ex-
amples that yielded semantic representations for both EDUs)
examples that could potentially be used. For a
number of relations, the total number of examples
we could use were less than 50. For the time being,
we decided to use only those relation sets that had
more than 50 examples. In addition, we chose not
to use Joint and General:specific relations. They
will be included in the future. Hence, our training
and testing data consisted of the following four re-
lations: Goal:act, Step1:step2, Criterion:act and
Before:after. The total number of examples we
used was 508 of which 423 were used for training
and 85 were used for testing.
Table 2, Table 3 and Table 4 show the results
from running the system on our test data. A total
of 85 positive examples were used for testing the
system.
Table 2 evaluates our SemDP system against a
baseline. Our baseline is the majority function,
which performs at a 51.7 F-Score. SemDP outper-
forms the baseline by almost 10 percentage points
38
Discourse Precision Recall F-Score
Parser
SemDP 61.7 58.8 60.24
Baseline* 51.7 51.7 51.7
Table 2: Evaluation vs Baseline (* our baseline is
the majority function)
Relation Precision Recall F-Score
Goal:act 31.57 26.08 28.57
Step1:step2 75 75 75
Before:after 54.5 54.5 54.5
Criterion:act 71.4 71.4 71.4
Total 61.7 58.8 60.24
Table 3: Test Results for SemDP
with an F-Score of 60.24. To the best of our
knowledge, we are also not aware of any work that
uses rich semantic information for discourse pars-
ing. (Polanyi et al, 2004) do not provide any eval-
uation results at all. (Soricut and Marcu, 2003) re-
port that their SynDP parser achieved up to 63.8 F-
Score on human-segmented test data. Our result of
60.24 F-Score shows that a Discourse Parser based
purely on semantics can perform as well. How-
ever, since the corpus, the size of training data and
the set of rhetorical relations we have used differ
from (Soricut and Marcu, 2003), a direct compar-
ison cannot be made.
Table 3 breaks down the results in detail for
each of the four rhetorical relations we tested on.
Since we are learning from positive data only and
the rules we learn depend heavily on the amount
of training data we have, we expected the system
to be more accurate with the relations that have
more training examples. As expected, SemDP did
very well in labeling Step1:step2 relations. Sur-
prisingly though, it did not perform as well with
Goal:act, even though it had the second highest
number of training examples (147 in total). In fact,
SemDP misclassified more positive test examples
for Goal:act than Before:after or Criterion:act, re-
lations which had almost one third the number of
Relation Goal:act Step1:step2 Before:after Criterion:act
Goal:act 6 8 5 0
Step1:step2 6 33 5 0
Before:after 0 4 6 1
Criterion:act 0 0 2 5
Table 4: Confusion Matrix for SemDP Test Result
training examples. Overall SemDP achieved a pre-
cision of 61.7 and a Recall of 58.8.
In order to find out how the positive test exam-
ples were misclassified, we investigated the dis-
tribution of the relations classified by SemDP. Ta-
ble 4 is the confusion matrix that highlights this
issue. A majority of the actual Goal:act relations
are incorrectly classified as Step1:step1 and Be-
fore:after. Likewise, most of the misclassification
of actual Step1:step1 seems to labeled as Goal:act
or Before:after. Such misclassification occurs be-
cause the simple rules learned by SemDP are not
able to accurately distinguish cases where positive
examples of two different relations share similar
semantic predicates. Moreover, since we are learn-
ing using positive examples only, it is possible that
a positive example may satisfy two or more rules
for different relations. In such cases, the rule that
has the highest score (as calculated by formula 4)
is used to label the unseen example.
5 Conclusions and Future Work
We have shown that it is possible to learn First Or-
der Logic rules from complex semantic data us-
ing an ILP based methodology. These rules can
be used to automatically label rhetorical relations.
Moreover, our results show that a Discourse Parser
that uses only semantic information can perform
as well as the state of the art Discourse Parsers
based on syntactic and lexical information.
Future work will involve the use of syntactic in-
formation as well. We also plan to run a more thor-
ough evaluation on the complete set of relations
that we have used in our coding scheme. It is also
important that the manual segmentation and an-
notation of rhetorical relations be subject to inter-
annotator agreement. A second human annotator
is currently annotating a sample of the annotated
corpus. Upon completion, the annotated corpus
will be checked for reliability.
Data sparseness is a well known problem inMa-
chine Learning. Like most paradigms, our learn-
ing model is also affected by it. We also plan to
explore techniques to deal with this issue.
39
Lastly, we have not tackled the problem of dis-
course parsing at higher levels of the DPT and seg-
mentation in this paper. Our ultimate goal is to
build a Discourse Parser that will automatically
segment a full text as well as annotate it with
rhetorical relations at every level of the DPT using
semantic as well as syntactic information. Much
work needs to be done but we are excited to see
what the aforesaid future work will yield.
Acknowledgments
This work is supported by award 0133123 from the National
Science Foundation. Thanks to C.P. Rose? for LCFLEX, M.
Palmer and K. Kipper for VerbNet, C. Buitelaar for CoreLex,
and Stephen Muggleton for Progol.
References
Paul Buitelaar. 1998. CoreLex: Systematic Polysemy
and Underspecification. Ph.D. thesis, Computer Science,
Brandeis University, February.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the frame-
work of Rhetorical Structure Theory. In Current Direc-
tions in Discourse and Dialogue, pp. 85-112, Jan van Kup-
pevelt and Ronnie Smith eds., Kluwer Academic Publish-
ers.
Michael Collins. 2003. Head-driven statistical methods for
natural language parsing. Computational Linguistics, 29.
Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad, Anoop
Sarkar, Aravind Joshi and Bonnie Webber. 2001. D-
LTAG System - Discourse Parsing with a Lexicalized Tree
Adjoining Grammar. Information Stucture, Discourse
Structure and Discourse Semantics, ESSLLI, 2001.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
1994. COMLEX syntax: Building a computational lex-
icon. In COLING 94, Proceedings of the 15th Interna-
tional Conference on Computational Linguistics, pages
472?477, Kyoto, Japan, August.
Paul Kingsbury and Martha Palmer. 2000. From Treebank
to Propbank. In Third International Conference on Lan-
guage Resources and Evaluation, LREC-02, Las Palmas,
Canary Islands, Spain, May 28 - June 3, 2002.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000.
Class-based construction of a verb lexicon. In AAAI-2000,
Proceedings of the Seventeenth National Conference on
Artificial Intelligence, Austin, TX.
Beth Levin and Malka Rappaport Hovav. 1992. Wiping the
slate clean: a lexical semantic exploration. In Beth Levin
and Steven Pinker, editors, Lexical and Conceptual Se-
mantics, Special Issue of Cognition: International Journal
of Cognitive Science. Blackwell Publishers.
William C. Mann and Sandra Thompson. 1988. Rhetorical
Structure Theory: toward a Functional Theory of Text Or-
ganization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An unsuper-
vised approach to recognizing discourse relations. In Pro-
ceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL-2002), Philadelphia, PA,
July.
Daniel Marcu, Magdalena Romera and Estibaliz Amorrortu.
1999. Experiments in Constructing a Corpus of Discourse
Trees: Problems, Annotation Choices, Issues. In The
Workshop on Levels of Representation in Discourse, pages
71-78, Edinburgh, Scotland, July.
M. G. Moser, and J. D. Moore. 1995. Using Discourse
Analysis and Automatic Text Generation to Study Dis-
course Cue Usage. In AAAI Spring Symposium on Empir-
ical Methods in Discourse Interpretation and Generation,
1995.
Stephen H. Muggleton. 1995. Inverse Entailment and Pro-
gol. In New Generation Computing Journal, Vol. 13, pp.
245-286, 1995.
Martha Palmer, Daniel Gildea and, Paul Kingsbury. 2005.
The Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?105.
Livia Polanyi, Christopher Culy, Martin H. van den Berg,
Gian Lorenzo Thione, and David Ahn. 2004. Senten-
tial Structure and Discourse Parsing. Proceedings of the
ACL2004 Workshop on Discourse Annotation, Barcelona,
Spain, July 25, 2004.
James Pustejovsky. 1991. The generative lexicon. Computa-
tional Linguistics, 17(4):409?441.
Carolyn Penstein Rose? and Alon Lavie. 2000. Balancing ro-
bustness and efficiency in unification-augmented context-
free parsers for large practical applications. In Jean-
Clause Junqua and Gertjan van Noord, editors, Robustness
in Language and Speech Technology. Kluwer Academic
Press.
Radu Soricut and Daniel Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Information.
In Proceedings of the Human Language Technology and
North American Assiciation for Computational Linguis-
tics Conference (HLT/NAACL-2003), Edmonton, Canada,
May-June.
Elena Terenzi and Barbara Di Eugenio. 2003. Building lex-
ical semantic representations for natural language instruc-
tions. In HLT-NAACL03, 2003 Human Language Tech-
nology Conference, pages 100?102, Edmonton, Canada,
May. (Short Paper).
40
