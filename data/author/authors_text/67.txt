A word-grammar based morl)hoh)gieal nalyzer 
for agglutinative languages 
Aduriz 1.+, Agirre E., Aldezabal I., Alegria I., Arregi X., Arriohl J. M., Artola X., Gojenola K., 
Marilxalar A., Sarasola K., Urkia M.+ 
l)ept, of Colllptiier 1Aulgtlages and Systems, University of lhe Basqtlo Cotlnlry, 64.9 P. K., 
E-20080 1)onostia, Basque Counh'y 
tUZEI, Aldapeta 20, E-20009 1)onostia, Basque Country 
+Universidad de Barcelona, Grin Vfii de Isis Cortes CalaiallaS, 585, E-08007 Flarcelona 
j ipgogak @ si.elm, es. 
Abst rac l  
Agglutinative languages presenl rich 
morphology and for sonic applications 
they lleed deep analysis at word level. 
Tile work here presenled proposes a 
model for designing a full nlorpho- 
logical analyzer. 
The model integrates lhe two-level 
fornlalisnl alld a ullificalion-I)asod 
fornialisni. In contrast to other works, 
we propose to separate the treatment of 
sequential and non-sequetTtial mou)ho- 
lactic constraints. Sequential constraints 
are applied in lhe seglllenlalion phase, 
and non-seqtlontial OlleS ill the filial 
feature-combination phase. Early appli- 
cation of sequential nlorpholactic 
coilsli'aiills during tile segnloillaiioi/ 
process nlakes feasible :,ill officienl 
iinplenleilialion of tile full morpho- 
logical analyzer. 
The result of lhis research has been tile 
design and imi)len~entation of a full 
nlorphosynlactic analysis procedure for 
each word in unrestricted Basque texts. 
I n t roduct ion  
Morphological analysis of woMs is a basic 
tool for automatic language processing, and 
indispensable when dealing willl highly 
agglutinative languages like Basque (Aduriz el 
al., 98b). In lhis conlext, some applications, 
like spelling corfeclion, do ilOI need illOl'e lhan 
the seglllOlltation of each word inlo its 
different COlllponenl nlorphellles alollg with 
their morphological information, ltowever, 
there are oiher applications such as lemnializa- 
tion, lagging, phrase recognition, and 
delernlinaiion of clause boundaries (Aduriz el 
al., 95), which need an additional global 
morphological i)arsing j of the whole word. 
Such a complete nlorphological analyzer has 
lo consider three main aspects (l~,ilchie et al, 
92; Sproal, 92): 
1 Morl)hographenfics (also called morpho- 
phonology). This ternl covers orthographic 
variations that occur when linking 
I l lOfphellleS. 
2) morpholactics. Specil'ication of which 
nlorphenles can or cannot combine with 
each other lo form wflid words. 
3) Feature-combination. Specification of how 
these lnorphemes can be grouped and how 
their nlorphosyntactic features can be 
comlfined. 
The system here presented adopts, oil the one 
hand, tile lwo-level fornlalisnl to deal with 
morphogralfilemics and sequential morl)ho- 
lactics (Alegria el al., 96) and, on the other 
hand, a unification-based woM-grammar 2 to 
combine the grammatical information defined 
in nlorphemes and to  tackle complex 
nlorphotactics. This design allowed us to 
develop a full coverage analyzer that processes 
efl'iciently unrestricted texts in Basque. 
The remainder of tills paper is organized sis 
follows. After a brief' description of Basque 
nlorphology, section 2 describes tile 
architecture for morphological processing, 
where the morphosynlactic omponent is 
included. Section 3 specifies tile plaenomena 
covered by the analyzer, explains its desigi~ 
criteria, alld presents implementation and 
ewthialion details. Section d compares file 
I This has also been called mo*7)hOSh,ntactic 
parsitlg. When we use lhc \[(fill #11017~\]lOSyltl~/X WC 
will always refer to il~c lficrarchical structure at 
woM level, conlbining morphology and synlax. 
2 '\]'\]lt3 \[IDl'll\] WOl'd-gF(lllllllUl" should not be confused 
with the synlaclic lilcory presented in (Hudson, 84). 
system with previous works. Finally, the paper 
ends with some concluding renmrks. 
1 Brief description of Basque 
morphology 
These are the most important features of 
Basque morphology (Alegria et al, 96): 
? As prepositional functions are realized by 
case suffixes inside word-fornls, Basque 
presents a relatively high power to generate 
inflected word-forms. For instance, froth a 
single noun a minimum of 135 inflected 
forms can be generated. Therefore, the 
number of simple word-forms covered by 
the current 70,000 dictionary entries woukl 
not be less than 10 million. 
? 77 of the inflected forms are simple 
combinations of number, determination, 
and case marks, not capable of further 
inflection, but the other 58 word-forms 
ending in one of the two possible genitives 
(possessive and locative) can be further 
inflected with the 135 morphemes. This 
kind of recursive construction reveals a 
noun ellipsis inside a noun phrase and 
could be theoretically exteuded ad 
infinitum; however, in practice it is not 
usual to fiud more than two levels of this 
kind of recursion in a word-form. Taking 
into account a single level of noun ellipsis, 
the number of word-forum coukl be 
estimated over half a billion. 
? Verbs offer a lot of grammatical 
information. A verb tbrln conveys informa- 
tion about the subject, the two objects, as 
well as the tense and aspect. For example: 
diotsut (Eng.: 1 am telling you something). 
o Word-formation is very productive in 
Basque. It is very usual to create new 
compounds as well as derivatives. 
As a result of this wealth of infornmtion 
contained within word-forms, complex struc- 
tures have to be built to represent complete 
morphological information at word level. 
2 An architecture for the full 
morphological ana lyzer  
The framework we propose for the 
morphological treatment is shown in Figure 1. 
The morphological nalyzer is the fiont-end to 
all present applications for the processing of 
Basque texts. It is composed of two modules: 
the segmentation module and the 
morphosyntactic analyzer. 
conformant .................. ~ U~atabas N TEZ-conf~ 
\[Segmentation module 
____~| HorphograDhemics 
Morphotactics I 
TEI-FS .............. ~ ~ ~ ~  ~ - p ~  
conformant Cegmented TexN 
Morphosyntactic 
analyzer 
Feature- combination 
Morphotactics II 
TEI-FS \] .............. ~ actically 
Lermnatization, linguistic Analysis tagging tools 
Figure 1. Architecture 1"o1" morphological processing. 
The segmentation ,nodule was previously 
implemented in (Alegria et al, 96). This 
system applies two-level morphology 
(Koskenniemi, 83) for the morphological 
description and obtains, for each word, its 
possible segmentations (one or many) into 
component morphemes. The two-level system 
has the following components: 
? A set of 24 morphograf~hemic rules, 
compiled into transducers (Karttunen, 94). 
? A lexicon made up of around 70,000 items, 
grouped into 120 sublexicons and stored in 
a general lexical database (Aduriz et al, 
98a). 
This module has full coverage of free-running 
texts in Basque, giving an average number of 
2.63 different analyses per word. The result is 
the set of possible morphological segmenta- 
tions of a word, where each morpheme is 
associated with its corresponding features in 
the lexicon: part of speech (POS), 
subcategory, declension case, number, 
definiteness, as well as syntactic function and 
some semantic features. Therefore, the output 
of the segmeutation phase is very rich, as 
shown in Figure 2 with the word amarengan 
(Eng.: on the mother). 
grammar 
mother) 
POS noun) 
subc~t common 
:count: +) 
(an imate  +) 
(nleasurable "-) 
aren 
(of life) 
(POS decl-suffix) 
(definite +) 
(number sing) 
(case genitive) 
(synt-f @nouncomp) 
J gan \] 
(o.1 / 
(POS decl-suf fix) I 
(case inossivo) \] 
(synt-f @adverbial)I 
=> 
amarengan 
(o. the mother) 
POS noun) 
subcat common) 
number sing) 
definite +) 
case inessive) 
count +) 
animate +) 
measurable -) 
synt-f @adverbial) 
iq:e, ure 2. Morphosynlactic analysis eof (unureugun (l{ng.: (m 
The architecture is a modular envhoument that 
allows different ypes of output depending on 
the desired level of analysis. The foundation of 
the architecture lies in the fact lhat TEI- 
confommnt SGML has been adopted for the 
comnmnication allloIlg modules (Ide and 
VCFOIIiS, 95). l~'eature shucluleS coded 
accoMing TIU are used to represent linguistic 
information, illcluding tile input mM outl)ut of 
the morplaological analyzer. This reprcscnta- 
tion rambles the use of SGML-aware parsers 
and tools, and Call he easily filtered into 
different formats (Artola et ill., 00). 
3 Word level morl)hosyntactic analysis 
This section Hrst presents the l~henomena lhat 
must be covered by the morphosyntactic 
analyzer, then explains ils design criteria, and 
finally shows implementation and ewfluation 
details. 
3.1 Phenomena covered by the analyzer 
There are several features that emphasized the 
need of morphosyntactic almlysis in order to 
build up word level information: 
I) Multiplicity of values for the same feature 
in successive morphemes. In the analysis 
of Figure 2 there are two different values 
for the POS (noun and declension suffix), 
two for the case (genitive and inessive), 
and two for the syntactic function 
(@nouncomp and @adverbial). Multiple 
values at moq~hemc-level will have to be 
merged to obtain the word level infer 
mation. 
2) Words with phrase structure. Although the 
segmentation is done for isolated words, 
independently of context, in several cases 
3 l?calurc wtlues starling with the "@" character 
correspond to syntactic functions, like @noullcomp 
(norm complement) or @adverbial. 
the mother) 
tile resulting structure is oquiwflent o the 
aualysis of a phrase, as can be seen i, 
Figure 2. 111 this case, although there are 
two different cases (genitive and inessive), 
lhe case of the full word-form is simply 
inessive. 
3) Noun ellipsis inside word-lbrms. A noun 
ellipsis can occur withi, the word 
(oceasi(mally more than once). This 
information must be made explicit in the 
resulting analysis. For example, Figure 3 
shows the analysis of a single word-forln 
like diotsudumtrel&z (Eng.: with what I am 
lelling you). The first line shows its 
segmentation into four morphemes 
(die tsut+en+ 0 +arekin). The feature 
compl ill tile final analysis conveys the 
information for the verb (l um lelliHg you), 
that carries information about pc'rson, 
number and case o1' subject, object and 
indirect object. The feature comp2 
represents an elided noun and its 
declension stfffix (with). 
4) l)erivation and composition are productive 
in Basque. There arc more than 80 deri- 
w/tion morphemes (especially suffixes) 
intensively used in word-fornlatioll. 
3.2 Design of the word-grammar 
The need to impose hierarchical structure upon 
sequences of morphemes and to build complex 
constructions from them forced us to choose a 
unil'ication mechanism. This task is currently 
unsolwlble using finite-state techniques, clue to 
the growth in size of the resulting network 
(Beesley, 98). We have developed a unifica- 
tion based word-grammar, where each rule 
combines information flom different 
mot+lJlemes giving as a result a feature 
structure for each interpretation of a word- 
fol'nl, treating the previously mentioned cases. 
3 
diotsut 
I am tellh,g you) 
POS verb) 
(tense present) 
(pers-ergative is)\[ 
(pets-dative 2s) 
(pers-absol 3s) 
en 
(what) 
(POS relation) 
(subcat subord) 
(relator relative 
(synt-f @rel-clause 
0 
() 
(POS ellipsis) 
arekin 
(wire) 
(POS declension-suffix)) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
=> diotsudanarekin (wi~ what lamtel l ingyou) 
(POS verb-noun_ellipsis) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
(compl (POS verb) 
(subcat subord) 
(relator relative) 
(synt-f @tel-clause) 
(tense present) 
(pers-ergative is) 
(pets-dative 2s) 
(pers-absol 3s)) 
(comp2 (POS noun) 
(subcat common) 
(number sing) 
(definite+) 
(synt-f @adverbial)) 
Figure 3. Morphosyntactic analysis of diotxudanarekin (Eng.: with what I am tellittg you) 
As a consequence of the rich naorphology of 
Basque we decided to control morphotactic 
phenomena, as much as possible, in the 
morphological segmentation phase. Alterna- 
tively, a model with minimal morphotactic 
treatment (Ritchie et al, 92) would produce 
too many possible analyses after segmentation, 
which should be reiected in a second phase. 
Therefore, we propose to separate sequential 
morphotactics (i.e., which sequences of 
morphemes can or cannot combine with each 
other to form valid words), which will be 
recognized by the two-level system by means 
of continuation classes, and non-sequential 
morphotactics like long-distance dependencies 
that will be controlled by the word-gmnunar. 
The general linguistic principles used to define 
unification equations in the word-grannnar 
rules are the following: 
1) Information risen from the lemma. The 
POS and semantic features are risen flom 
the lemnm. This principle is applied to 
common nouns, adjectives and adverbs. 
The lemma also gives the mnnber in 
proper nouns, pronouns and determiners 
(see Figure 2). 
2) lnfornmtion risen from case suffixes. 
Simple case suffixes provide information 
on declension case, number and syntactic 
function. For example, tile singular 
genitive case is given by the suffix -tell in 
ama+ren (Eng.: of the mother). For 
compound case suffixes the number and 
determination are taken from the first 
suffix and the case from the second one. 
First, both suffixes are joined and after 
that they are attached to the lemma. 
3) Noun ellipsis. When an ellipsis occurs, the 
POS of the whole word-form is expressed 
by a compound, which indicates both the 
presence of the ellipsis (always a noun) 
and the main POS of the word. 
For instance, the resulting POS is 
verb-noun_e l l ips is  when a noun- 
ellipsis occurs after a verb. All the 
information corresponding to both units, 
the explicit lemma and the elided one, is 
stored (see Figure 3). 
4) Subordination morl~hemes. When a 
subordination morpheme is attached to a 
verb, the verb POS and its featm'es are 
risen as well as the subordhmte relation 
and the syntactic fnnction conveyed by the 
naorpheme. 
5) Degree morphemes attached to adjectives, 
past participles and adverbs. The POS and 
diotsudan 
(diotsut + en) 
(POS verb) 
(tense present) 
(relator relative) 
/ \ / 
diotsut 
(POS verb) 
(tense present 
diotsudanarekin 
(diotsut + en -I 0 + arekin) 
(POS verb-noun_ell ipsis) 
(case sociative) 
arekin 
(0 + arekin) 
(POS noun ellipsis) 
(case sociative) 
en 
(pos 
? . . 
o 
(POS e l l ips i s  re la t ion)  
arekin 
(case sociative) 
Figure 4. Parse tree for diotmuhmarekitl (Eng.: with what I am lellittg yott) 
main features arc taken from the lemma 
and the features corresponding to the 
degrees of comparison (comparative, 
supcrhttive) aft taken from the degree 
morphemes. 
6) l)efiwttion. 1)miwttion suffixes select tile 
POS of the base-form to create the deriw> 
tive anti in most cases to change its POS. 
For instance, the suffix -garri (Eng.: -able) 
is applied to verbs and the derived word is 
an adjective. When the derived form is 
obtained by means o1' a prefix, it does not 
change the POS of the base-form. In both 
cases the morphosyntactic rules add a new 
feature representing the structure of tile 
word as a derivative (root and affixes). 
7) Composition. At the moment, we only 
treat the most freqttent kind of 
composition (noun-noun). Since Basque is 
syntactically characterized as a right-head 
hmguage, the main information of the 
compound is taken from the second 
element. 
8) Order of application of the mofphosyn- 
tactic phenomena. When several morpho- 
syntactic phenomena are applied to the 
same leml l la ,  so as to eliminate 
nonsensical readings, the natural order to 
consider them in Basque is the following: 
lemmas, derbation prefixes, deriwltion 
suffixes, composition and inflection (see 
Figure 4). 
9) Morl)hotactic constraints. Elimination of 
illegal sequences of morphemes, such as 
those due to long-distance dependencies, 
which are difficult to restrict by means of 
conti.uation classes. 
The first and second principles are defined lo 
combine information of previously recognized 
mOrl~hemcs, but all the other principles arc 
related to both feature-combination a d non- 
sequential moq~hotactics. 
3.3 Implementation 
We have chosen the PATR formalism 
(Shiebcr, 86) for the definition of the moqflm- 
syntactic rules. There were two main reasons 
for this choice: 
? The formalism is based o.  unification. 
Unification is adequate for the treatment of 
complex phenomena (e.g., agreement of 
conslituents in case, tmmber and definite- 
hess) and complex linguistic structures. 
? Simplicity. The grammar is not linked to a 
linguistic theory, e.g. GPSG in (Ritchie et 
al., 92)? The fact that PATR is simpler than 
more sophisticated formalisms will allow 
that in @e future the grammar could be 
adapted to any of them. 
25 rules have been defined, distributed in the 
following way: 
? 11 rules for the merging of declension 
morphemes and their combination with the 
main categories, 
? 9 rules for the description of verbal 
subordination morphenles, 
? 2 general fulcs for derivation, 
? 1 rule for each of the following 
phenomeml: ellipsis, degree of COlnpavison 
of adjectives (comparative and SUl)erlative) 
and noun composition. 
3.4 Evaluat ion 
As a cousequence of the size of the lexical 
database and tile extensive treatment of 
nlorphosyntax, the resulting analyzer offers 
full coverage when applied to real texts, 
capable of treating unknown words and non- 
standard forms (dialectal wtriants and typical 
errors). 
We performed four experilnents to ewtluate 
tile efficiency of the implemented analyzer 
(see Table 1). A 10,832-word text was 
randomly selected from newspapers. We 
measured tile number of words per second 
analyzed by the morphosyntactic analyzer and 
also by the whole morphological analyzer 
(results taken on a Sun Ultra 10). Ill the first 
experiment all tile word-t'ornls were analyzed 
one-by-one; while ill tile other three experi- 
ments words with more than one occurrence 
were analyzed only once. Ill the last two 
experimeuts a memory with the analysis of tile 
most frequent word-forms (MFW) in Basque 
was used, so that only word-forms not found 
in the MFW were analyzed. 
Test 
description 
All 
word forms 
Diffcrent 
word forms 
MFW 
10,000 words 
(I 5 Mb) 
MFW 
50,000 words 
(75 mb) 
# words/scc 
analyzed Morphosynt. 
words analyzer 
10,832 
3,692 
1,483 
533 
15,13 
44 40 
111 95 
308 270 
words/see 
Full 
morphological 
analyzer 
13,5 
Table 1. Evaluation results. 
Even when our language is agglutinative, and 
its morphological phenomena need more 
computational resources to build complex and 
deep structures, the results prove tile feasibility 
of implementiug efficiently a fifll 
morphological analyzer, although efficiency 
was not the main concern of our 
implementation. The system is currently being 
applied to unrestricted texts in real-time 
applications. 
4 Related work 
(Koskeniemmi, 83) defined the formalism 
named two-level morphology. Its main 
contributiou was the treatment of 
morl)hographemics and morphotactics. The 
formalisnl has been stmcessfully applied to a 
wide wlriety ot' languages. 
(Karttunen, 94) speeds the two-level model 
compiling two-level rules into lexical 
transducers, also increasing the expressiveness 
of the model 
The morphological analyzer created by 
(Ritchie et al, 92) does not adopt finite state 
mechanisms to control morphotactic 
phenomena. Their two-level implementation 
incorporates a straightforward morphotactics, 
reducing tile number of sublexicons to the 
indispensable (prefixes, lemmas and suffixes). 
This approximation would be highly 
inefficient for agglutinative languages, as it 
would create lnany nonsensical interpretatiolas 
that should be rejected by tile unification 
phase. They use the word-grammar for both 
morphotactics and feature-conlbination. 
ill a similar way, (Trost, 90) make a proposal 
to combine two-level morphology and non- 
sequential morphotactics. 
The PC-Kimmo-V2 system (Antworth, 94) 
presents an architecture similar to ours applied 
to English, using a finite-state segmentation 
phase before applying a unification-based 
grammar. 
(Pr6szdky and Kis, 99) describe a morpho- 
syntactic analyzer for Hungarian, an agglu- 
tinative language. The system clots not use the 
two-level model for segmentation, precom- 
piling suffix-sequences to improve efficiency. 
They claim the need of a word-grammar, 
giving a first outline of its design, although 
they do not describe it in detail. 
(Oflazer, 99) presents a different approach for 
the treatment of Turkish, an agglutinative 
language, applying directly a dependency 
parsing scheme to morpheme groups, that is, 
merging morphosyntax and syntax. Although 
we are currently using a similar model to 
Basque, there are several applications that are 
word-based and need full morphological 
parsing of each word-t'orm, like the word- 
oriented Constraint Graminar formalism for 
disambiguation (Karlsson et aI., 95). 
Conc lus ion  
We propose a model for fllll morphological 
analysis iutegrating two different components. 
On tile one hand, the two-level formalism 
deals with morphographenfics and sequential 
morphotactics and, on the other hand, a 
unil\]cation-based word-grammar combines lhe 
granlll-iatical in\['ornlatioli defined in illoi'- 
phelllOS alld also handles COlllplcx illori)ho- 
tactics. 
Early application of sCqtloniial I/lOrl)hotactic 
conslraints dtu-ing the segmentation process 
avoids all excessive laUlllber of nleaningless 
segmentation possibilities before the 
coulputationally lllOlO expensive unification 
process. Unification permits lhe resohition of a 
wide variety of morl)hological phenonlena, 
like ellipsis, thal force the definition of: 
complex and deep structures Io roprosenl the 
output of the analyzer. 
This design allowed us io develop a full 
coverage allalyzor that processes efficiently 
unrestricted loxis in Basque, a strongly 
agglulinafive langttage. 
The anaiyzcl" has bccll integrated ill a gCllOl'al 
franlework for the l)lOCessing of l~asquc, with 
all the linguistic inodulos communicating by 
l l leallS O\[: foattll'C stltlClll l 'eS ill accord  {o the 
principles of ihe Text Encoding Initiative. 
Acknowledgements  
This research was partially supported by the 
Basque Government, the University of the 
\]71aS(lUe Cotlntry {/lid the CICYq' (Cotllisidn 
lntcrministorial de Ciencia y Tecnologfil). 
References 
Aduriz 1., Aldczabal I., Ansa ()., Arlola X., I)faz de 
Ilarraza A., Insau.~li .I.M. (1998a) EI)BL: a 
Mttlli-l~ttrposed Lexica/ Sttl)l)c;rl .lot the 
Treatment of Ba,s'que. Proceedings of the l;irst 
Inlernational Confcncncc on l Auiguagc Resources 
and Ewduation, Granada. 
Aduriz I., Agirre E., Aldczabal 1., Alegria 1., Ansa 
O., Arrcgi X., Arriola J.M., ArtolaX., I)faz de 
lhu'raza A., Ezciza N., Gqicnola K., Maritxahu" 
A., Maritxalar M., Oronoz M., Sarasola K., 
Soroa A., Urizar R., Urkia M. (1998b) A 
Framework .for the Automatic Pmce.vsi#~g (if" 
Basqtte. Proceedings o1 the First Ii~ternational 
Con \[elel i te on Lall.gtlagc Resources turf 
Evaluation, Granada. 
Aduriz I., Alcgria I., Arriohl J.M., Artola X., l)faz 
do Ilarraza A., Ecciza N., Gojcnola K., 
Maritxalar M. (1995) Di\[.ferelt! Issues in the 
Design qf a lemmatizer/Tagger fo Ba,s'qtte. From 
Tcxls to Tags: Issues in Mullilingual Language 
Analysis. ACL SIGI)AT Workshop, l)ublin. 
Alcgria 1., Art(Ha X., Sarasoht K., Urkia M. (1996) 
Automatic moqdzological analysis of Basque. 
IAtcrary and IAnguistic Computing, 11 (4): 193- 
203. Oxford University. 
Aniworlh E. I.. (1994) Morphological Par, ffng with 
a lhl(fication-ba,s'ed Word Grcmmutr. Norlh 
Te, xas Natural l~anguage Processing Workshop, 
Texas. 
Arlola X., Dfaz de \]larraza A., Ezciza N., Oo.icnohi 
K., Marilxahu' A., Soma A. (2000) A proposal 
for the integration of NLP tools using SGML- 
lagged documeHls. Proceedings of ll~e Second 
Cotfforence or1 Language Resources and 
Evaltmfion (IA~,EC 2000). Athens, Greece 2000. 
Bcesl%, K. (1998)AraDic Morphological Analysis 
(m the lnlernet, l'rocccdings of the International 
Conference on Mulii-IAngual Computing (Arabic 
& lhlglish), Cambridge. 
Hudson R. (1990) English Word Grammmar. 
Oxford: Basil Blackwcll. 
ldc N., Vcronis J. K. (1995) Text-Ettcoding hHtia- 
tire, Bac:kgmtmd and Context. Kluwcr Academic 
Publishers. 
Karlsson F., Voulilaincn A., Heikkiht J., Anltila A. 
(1995) Constrai, t Gnmmmr: A lxm,?tmge- 
i#ldcpcndent System Jor Pm:ffng Um'estricled 
Text, Mouton do Gruyicr ed.. 
Kartmnen 1,. (1994) Con,s'tructin~ l,e.vical 
7)'ansdttcers. Proc. of CO13NG'94, 406-411. 
Koskcnniemi, K, (1983) Two-level Mc;qdlo\[ogy: A 
ge,eral Comptttational Model ./br Word-Form 
Recognition and Pmduclioth University of 
Ilclsinki, l)clmrtmcnt of General IAnguisiics. 
l~ublications " 11. 
()flazcr K (1999) l)epetMe/t O' Parsing, with a, 
E.rtended I:inite State Approac\]t. ACL'99, 
Maryland. 
Pr6sz6ky G., Kis B (1999)A Unificati(m-hascd 
Apl~roach to Moqdto-syntactic I'arsitl<~ of 
Agghttinative and Other (Highly) lnjlectional 
Languages. ACtd99, Ma,yhmd. 
Ritchie G., Pulhnan S. G., FJlack A. W., Russcl G. 
J. (1992) Comlmtational Moudu)logy: Practical 
Mechanism,s'.fi)r the l#lglish l,exico,. ACL-MIT 
Series on Natural Language Processing, MIT 
Press. 
Shicbcr S. M. (1986) At/ lntroductiotz to 
Unification-Based Approaches to Grammar. 
CSLI, Slanford. 
Sproat R. (1992) Morphology anU Computcaion. 
ACL-MIT Press series in Natural Language 
Processing. 
Trost It. (1990) The application of two-level 
morldzo/ogy to rzon-concatenative German 
moqgtology. COIANG'90, Hclsinki. 
7 
Syntactic features for high precision Word Sense Disambiguation 
 
David Mart?nez, Eneko Agirre  
IxA NLP Group 
University of the Basque Country 
Donostia, Spain 
{jibmaird,eneko}@si.ehu.es 
Llu?s M?rquez 
TALP Research Center 
Polytechnical University of Catalonia 
Barcelona, Spain 
lluism@lsi.upc.es 
 
Abstract 
This paper explores the contribution 
of a broad range of syntactic features 
to WSD: grammatical relations coded 
as the presence of adjuncts/arguments 
in isolation or as subcategorization 
frames, and instantiated grammatical 
relations between words. We have 
tested the performance of syntactic 
features using two different ML 
algorithms (Decision Lists and 
AdaBoost) on the Senseval-2 data. 
Adding syntactic features to a basic 
set of traditional features improves 
performance, especially for AdaBoost. 
In addition, several methods to build 
arbitrarily high accuracy WSD 
systems are also tried, showing that 
syntactic features allow for a precision 
of 86% and a coverage of 26% or 95% 
precision and 8% coverage.  
1. Introduction 
Supervised learning has become the most 
successful paradigm for Word Sense 
Disambiguation (WSD). This kind of algorithms 
follows a two-step process: 
1. Choosing the representation as a set of 
features for the context of occurrence of the 
target word senses.  
2. Applying a Machine Learning (ML) 
algorithm to train on the extracted features 
and tag the target word in the test examples.  
Current WSD systems attain high performances 
for coarse word sense differences (two or three 
senses) if enough training material is available. 
In contrast, the performance for finer-grained 
sense differences (e.g. WordNet senses as used 
in Senseval 2 (Preiss & Yarowsky, 2001)) is far 
from application needs. Nevertheless, recent 
work (Agirre and Martinez, 2001a) shows that it 
is possible to exploit the precision-coverage 
trade-off and build a high precision WSD system 
that tags a limited number of target words with a 
predefined precision.  
This paper explores the contribution of a 
broad set of syntactically motivated features that 
ranges from the presence of complements and 
adjuncts, and the detection of subcategorization 
frames, up to grammatical relations instantiated 
with specific words. The performance of the 
syntactic features is measured in isolation and in 
combination with a basic set of local and topical 
features (as defined in the literature), and using 
two ML algorithms: Decision Lists (Dlist) and 
AdaBoost (Boost). While Dlist does not attempt 
to combine the features, i.e. it takes the strongest 
feature only, Boost tries combinations of 
features and also uses negative evidence, i.e. the 
absence of features. 
Additionally, the role of syntactic features in 
a high-precision WSD system based on the 
precision-coverage trade-off is also investigated.  
The paper is structured as follows. Section 2 
reviews the features previously used in the 
literature. Section 3 defines a basic feature set 
based on the preceding review. Section 4 
presents the syntactic features as defined in our 
work, alongside the parser used. In section 5 the 
two ML algorithms are presented, as well as the 
strategies for the precision-coverage trade-off. 
Section 6 shows the experimental setting and the 
results. Finally section 7 draws the conclusions 
and summarizes further work. 
2. Previous work. 
Yarowsky (1994) defined a basic set of features 
that has been widely used (with some variations) 
by other WSD systems. It consisted on words 
appearing in a window of ?k positions around 
the target and bigrams and trigrams constructed 
with the target word. He used words, lemmas, 
coarse part-of-speech tags and special classes of 
words, such as ?Weekday?. These features have 
been used by other approaches, with variations 
such as the size of the window, the distinction 
between open class/closed class words, or the 
pre-selection of significative words to look up in 
the context of the target word.  
Ng (1996) uses a basic set of features similar 
to those defined by Yarowsky, but they also use 
syntactic information: verb-object and subject-
verb relations. The results obtained by the 
syntactic features are poor, and no analysis of 
the features or any reason for the low 
performance is given. 
Stetina et al (1998) achieve good results with 
syntactic relations as features. They use a 
measure of semantic distance based on WordNet 
to find similar features. The features are 
extracted using a statistical parser (Collins, 
1996), and consist of the head and modifiers of 
each phrase. Unfortunately, they do not provide 
a comparison with a baseline system that would 
only use basic features.  
The Senseval-2 workshop was held in 
Toulouse in July 2001 (Preiss & Yarowsky, 
2001). Most of the supervised systems used only 
a basic set of local and topical features to train 
their ML systems. Regarding syntactic 
information, in the Japanese tasks, several 
groups relied on dependency trees to extract 
features that were used by different models 
(SVM, Bayes, or vector space models). For the 
English tasks, the team from the University of 
Sussex extracted selectional preferences based 
on subject-verb and verb-object relations. The 
John Hopkins team applied syntactic features 
obtained using simple heuristic patterns and 
regular expressions. Finally, WASP-bench used 
finite-state techniques to create a grammatical 
relation database, which was later used in the 
disambiguation process. The papers in the 
proceedings do not provide specific evaluation 
of the syntactic features, and it is difficult to 
derive whether they were really useful or not.  
3. Basic feature set 
We have taken a basic feature set widely used in 
the literature, divided in topical features and 
local features (Agirre & Martinez, 2001b). 
Topical features correspond to open-class 
lemmas that appear in windows of different sizes 
around the target word. In this experiment, we 
used two different window-sizes: 4 lemmas 
around the target (coded as win_lem_4w), and 
the lemmas in the sentence plus the 2 previous 
and 2 following sentences (win_lem_2s). 
Local features include bigrams and trigrams 
(coded as big_, trig_ respectively) that contain 
the target word. An index (+1, -1, 0) is used to 
indicate the position of the target in the bigram 
or trigram, which can be formed by part of 
speech, lemmas or word forms (wf, lem, 
pos). We used TnT (Brants, 2000) for PoS 
tagging.  
For instance, we could extract the following 
features for the target word known from the 
sample sentence below: word form ?whole? 
occurring in a 2 sentence window (win_wf_2s), 
the bigram  ?known widely? where target is the 
last word (big_wf_+1) and the trigram ?RB RB N? 
formed by the two PoS before the target word 
(trig_pos_+1). 
 
?There is nothing in the whole range of human 
experience more widely known and universally ?? 
4. Set of Syntactic Features. 
In order to extract syntactic features from the 
tagged examples, we needed a parser that would 
meet the following requirements: free for 
research, able to provide the whole structure 
with named syntactic relations (in contrast to 
shallow parsers), positively evaluated on well-
established corpora, domain independent, and 
fast enough. 
Three parsers fulfilled all the requirements: 
Link Grammar (Sleator and Temperley, 1993), 
Minipar (Lin, 1993) and (Carroll & Briscoe, 
2001). We installed the first two parsers, and 
performed a set of small experiments (John 
Carroll helped out running his own parser). 
Unfortunately, we did not have a comparative 
evaluation to help choosing the best. We 
performed a little comparative test, and all 
parsers looked similar. At this point we chose 
Minipar mainly because it was fast, easy to 
install and the output could be easily processed. 
The choice of the parser did not condition the 
design of the experiments (cf. section 7). 
From the output of the parser, we extracted 
different sets of features. First, we distinguish 
between direct relations (words linked directly 
in the parse tree) and indirect relations (words 
that are two or more dependencies apart in the 
syntax tree, e.g. heads of prepositional modifiers 
of a verb). For example, from ?Henry was listed 
on the petition as the mayor's attorney? a direct 
verb-object relation is extracted between listed 
and Henry and the indirect relation ?head of a 
modifier prepositional phrase? between listed 
and petition. For each relation we store also its 
inverse. The relations are coded according to the 
Minipar codes (cf. Appendix): 
 
[Henry obj_word listed] 
[listed objI_word Henry] 
[petition mod_Prep_pcomp-n_N_word listed] 
[listed mod_Prep_pcomp-n_NI_word petition] 
 
For instance, in the last relation above, mod_Prep 
indicates that listed has some prepositional 
phrase attached, pcomp-n_N indicates that petition 
is the head of the prepositional phrase, I 
indicates that it is an inverse relation, and word 
that the relation is between words (as opposed to 
relations between lemmas).  
We distinguished two different kinds of 
syntactic relations: instantiated grammatical 
relations (IGR) and grammatical relations (GR). 
4.1. Instantiated Grammatical Relations 
IGRs are coded as [wordsense relation value] 
triples, where the value can be either the word 
form or the lemma. Some examples for the 
target noun ?church? are shown below. In the 
first example, a direct relation is extracted for 
the ?building? sense, and in the second example 
an indirect relation for the ?group of Christians? 
sense. 
 
Example 1: ?...Anglican churches have been 
demolished...? 
[Church#2 obj_lem  demolish] 
 
Example 2: ?...to whip men into a surrender to a 
particular churh...? 
[Church#1 mod_Prep_pcomp-n_N_lem surrender] 
4.2. Grammatical relations 
This kind of features refers to the grammatical 
relation themselves. In this case, we collect 
bigrams [wordsense relation] and also n-grams 
[wordsense relation1 relation2 relation3 ...]. The 
relations can refer to any argument, adjunct or 
modifier. N-grams are similar to verbal 
subcategorization frames. At present, they have 
been used only for verbs. Minipar provides 
simple subcategorization information in the PoS 
itself, e.g. V_N_N for a verb taking two 
arguments. We have defined 3 types of n-grams: 
? Ngram1: The subcategorization information 
included in the PoS data given by Minipar, 
e.g. V_N_N.  
? Ngram2: The subcategorization information 
in ngram1, filtered by the arguments that 
actually occur in the sentence. 
? Ngram3: Which includes all dependencies in 
the parse tree.  
The three types have been explored in order to 
account for the argument/adjunct distinction, 
which Minipar does not always assign correctly. 
In the first case, Minipar?s judgment is taken 
from the PoS. In the second case the PoS and the 
relations deemed as arguments are combined 
(adjuncts are hopefully filtered out, but some 
arguments might be also discarded). In the third, 
all relations (including adjuncts and arguments) 
are considered. 
In the example below, the ngram1 feature 
indicates that the verb has two arguments (i.e. it 
is transitive), which is an error of Minipar 
probably caused by a gap in the lexicon. The 
ngram2 feature indicates simply that it has a 
subject and no object, and the ngram3 feature 
denotes the presence of the adverbial modifier 
?still?. Ngram2 and ngram3 try to repair possible 
gaps in Minipar?s lexicon. 
 
Example: ?His mother was nudging him, but he 
was still falling? 
[Fall#1 ngram1 V_N_N] 
[Fall#1 ngram2 subj] 
[Fall#1 ngram3 amodstill+subj] 
5. ML algorithms. 
In order to measure the contribution of syntactic 
relations, we wanted to test them on several ML 
algorithms. At present we have chosen one 
algorithm which does not combine features 
(Decision Lists) and another which does 
combine features (AdaBoost).  
Despite their simplicity, Decision Lists (Dlist 
for short) as defined in Yarowsky (1994) have 
been shown to be very effective for WSD 
(Kilgarriff & Palmer, 2000). Features are 
weighted with a log-likelihood measure, and 
arranged in an ordered list according to their 
weight. In our case the probabilities have been 
estimated using the maximum likelihood 
estimate, smoothed adding a small constant (0.1) 
when probabilities are zero. Decisions taken 
with negative values were discarded (Agirre & 
Martinez, 2001b).  
AdaBoost (Boost for short) is a general 
method for obtaining a highly accurate 
classification rule by linearly combining many 
weak classifiers, each of which may be only 
moderately accurate (Freund, 1997). In these 
experiments, a generalized version of the Boost 
algorithm has been used, (Schapire, 1999), 
which works with very simple domain 
partitioning weak hypotheses (decision stumps) 
with confidence rated predictions. This 
particular boosting algorithm is able to work 
efficiently in very high dimensional feature 
spaces, and has been applied, with significant 
success, to a number of NLP disambiguation 
tasks, including word sense disambiguation 
(Escudero et al, 2000). Regarding 
parametrization, the smoothing parameter has 
been set to the default value (Schapire, 1999), 
and Boost has been run for a fixed number of 
rounds (200) for each word. No optimization of 
these parameters has been done at a word level. 
When testing, the sense with the highest 
prediction is assigned. 
5.1. Precision vs. coverage trade-off. 
A high-precision WSD system can be obtained 
at the cost of low coverage, preventing the 
system to return an answer in the lowest 
confidence cases. We have tried two methods on 
Dlists, and one method on Boost. 
The first method is based on a decision-
threshold (Dagan and Itai, 1994): the algorithm 
rejects decisions taken when the difference of 
the maximum likelihood among the competing 
senses is not big enough. For this purpose, a 
one-tailed confidence interval was created so we 
could state with confidence 1 - ? that the true 
value of the difference measure was bigger than 
a given threshold (named ?). As in (Dagan and 
Itai, 1994), we adjusted the measure to the 
amount of evidence. Different values of ? were 
tested, using a 60% confidence interval. The 
values of ? range from 0 to 4. For more details 
check (Agirre and Martinez, 2001b). 
The second method is based on feature 
selection (Agirre and Martinez, 2001a). Ten-
fold cross validation on the training data for 
each word was used to measure the precision of 
each feature in isolation. Thus, the ML 
algorithm would be used only on the features 
with precision exceeding a given threshold. This 
method has the advantage of being able to set 
the desired precision of the final system.  
In the case of Boost, there was no 
straightforward way to apply the first method. 
The application of the second method did not 
yield satisfactory results, so we turned to 
directly use the support value returned for each 
decision being made. We first applied a 
threshold directly on this support value, i.e. 
discarding decisions made with low support 
values. A second approximation, which is the 
one reported here, applies a threshold over the 
difference in the support for the winning sense 
and the second winning sense. Still, further work 
is needed in order to investigate how Boost 
could discard less-confident results. 
6. Experimental setting and results. 
We used the Senseval-2 data (73 nouns, verbs 
and adjectives), keeping the original training and 
testing sets. In order to measure the contribution 
of syntactic features the following experiments 
were devised (not all ML algorithms were used 
in all experiments, as specified): contribution of 
IGR-type and GR-type relations (Dlist), 
contribution of syntactic features over a 
combination of local and topical features (Dlist, 
Boost), and contribution of syntactic features in 
a high precision system (Dlist, Boost). 
Performance is measured as precision and 
coverage (following the definitions given in 
Senseval-2). We also consider F11 to compare 
the overall performance as it gives the harmonic 
average between precision and recall (where 
recall is in this case precision times the 
coverage). F1 can be used to select the best 
precision/coverage combination (cf. section 6.3). 
6.1. Results for different sets of syntactic 
features (Dlist). 
Table 1 shows the precision, coverage and F1 
figures for each of the grammatical feature sets 
as used by the decision list algorithm. 
Instantiated Grammatical Relations provide very 
good precision, but low coverage. The only 
exceptions are verbs, which get very similar 
precision for both kinds of syntactic relations. 
Grammatical Relations provide lower precision 
but higher coverage. A combination of both 
attains best F1, and is the feature set used in 
subsequent experiments.  
                                                     
1 F1=2*precision*recall/(precision+recall). In this 
case we use recall=precision*coverage. 
6.2. Results for different combinations of 
features (Dlist, Boost) 
Both ML algorithms were used on syntactic 
features, local features, a combination of 
local+topical features (also called basic), and a 
combination of all features (basic+syntax) in 
turn. Table 2 shows the F1 figures for each 
algorithm, feature set and PoS.  
All in all, Boost is able to outperform Dlist in 
all cases, except for local features. Syntactic 
features get worse results than local features. 
Regarding the contribution of syntactic 
features to the basic set, the last two columns in 
Table 2 show a "+" whenever the difference in 
the precision over the basic feature set is 
significant (McNemar's test). Dlist is able to 
scarcely profit from the additional syntactic 
features (only significant for verbs). Boost 
attains significant improvement, showing that 
basic and syntactic features are complementary.  
The difference 
algorithms could be 
Dlist is a conservati
that it only uses the 
by the first feature tha
(abstaining if none o
using a combination o
single-feature classife
negative evidence) 
positive predictions t
Dlist. Since the feat
covered and given th
accurate, Boost achie
it is a significant
approaching a 100% c
6.3. Precision vs. coverage: high precision 
systems (Dlist, Boost)  
Figure 1 shows the results for the three methods 
to exploit the precision/coverage trade-off in 
order to obtain a high-precision system. For each 
method two sets of features have been used: the 
basic set alne and the combination of both 
basic and syntactic features.  
The figure reveals an interesting behavior for 
different coverage ranges. In the high coverage 
range, Boost on basic+syntactic features attains 
the best performance. In the medium coverage 
area, the feature selection method for Dlist 
obtains the best results, also for basic+syntactic 
features. Finally, in the low coverage and high 
precision area the decision-threshold method for 
Dlist is able to reach precisions in the high 90?s, 
with no profit from syntactic features. 
The two methods to raise precision for Dlists 
are very effective. The decision-threshold 
e in performance 
 coverage. The 
s 86% precision 
ctic features, but 
.  
obtain extremely 
 of low coverage) 
most predictive 
ave had problems 
 algorithm for 
ions. 
r coverage over 
ures consistently 
ing that syntactic 
IGR GR All-syntax 
PoS Prec. Cov. F1 Prec. Cov. F1 Prec. Cov. F1 
A 81,6 21,8 29,2 70,1 65,4 55,4 70,7 68,9 57,7 
N 74,6 36,0 38,5 65,4 57,6 47,8 67,6 62,5 52,0 
V 68,6 32,2 33,4 67,3 41,2 39,2 66,3 52,7 45,4 
Ov. 72,9 31,9 35,2 67,1 52,1 46,0 67,7 59,5 50,4 
Table 1: precision and coverage for different sets of syntactic features (percentage). 
 
  Syntax Local Local+Topical (Basic) Basic + Syntax 
PoS MFS Dlist Boost Dlist Boost Dlist Boost Dlist Boost 
A 59,0 57,7 62,6 66,3 67,5 65,3 66,2 65,4     67,7 
N 57,1 52,0 60,0 63,6 65,3 63,2 67,9 63,3 69,3+ 
V 40,3 45,4 48,5 51,6 50,1 51,0 51,6   51,2+ 53,9+ 
Ov. 48,2 50,4 55,2 59,4 59,3 58,5 60,7 58,7 62,5+ 
Table 2: F1 results (perc.) for different feature sets. ?+? indicates statistical significance over Basic. between the two ML 
explained by the fact that 
ve algorithm in the sense 
positive information given 
t holds in the test example 
f them are applicable). By 
f the predictions of several 
rs (using both positive and 
Boost is able to assign 
o more test examples than 
ure space is more widely 
at the classifiers are quite 
ves better recall levels and 
ly better algorithm for 
method obtains constant increas
up to 93% precision with 7%
feature selection method attain
with 26% coverage using synta
there is no further improvement
In this case Dlist is able to 
good accuracy rates (at the cost
restricting to the use of the 
features. On the contrary, we h
in adjusting the AdaBoost
obtaining high precision predict
The figure also shows, fo
20%, that the syntactic feat
allow for better results, confirmoverage WSD system. features improve the results of the basic set. 
7. Conclusions and further work. 
This paper shows that syntactic features 
effectively contribute to WSD precision. We 
have extracted syntactic relations using the 
Minipar parser, but the results should be also 
applicable to other parsers with similar 
performance. Two kinds of syntactic features are 
defined: Instantiated Grammatical Relations  
(IGR) between words, and Grammatical 
Relations (GR) coded as the presence of 
adjuncts / arguments in isolation or as 
subcategorization frames.  
The experimental results were tried on the 
Senseval-2 data, comparing two different ML 
algorithms (Dlist and Boost) trained both on a 
basic set of widely used features alone, and on a 
combination of basic and syntactic features. The 
main conclusions are the following: 
? IGR get better precision than GR, but the 
best precision/coverage combination 
(measured with F1) is attained by the 
combination of both. 
? Boost is able to profit from the addition of 
syntactic features, obtaining better results 
than Dlist. This proves that syntactic 
features contain information that is not 
present in other traditional features.  
? Overall the improvement is around two 
points for Boost, with highest increase for 
verbs.  
Several methods to exploit the precision-
coverage trade-off where also tried: 
? The results show that syntactic features 
consistently improve the results on all data 
points except in the very low coverage 
range, confirming the contribution of syntax. 
? The results also show that Dlist are suited to 
build a system with high precision: either a 
precision of 86% and a coverage of 26%, or 
95% precision and 8% coverage. 
Regarding future work, a thorough analysis of 
the quality of each of the syntactic relations 
extracted should be performed. In addition, a 
word-by-word analysis would be interesting, as 
some words might profit from specific syntactic 
features, while others might not. A preliminary 
analysis has been performed in (Agirre & 
Martinez, 2001b). 
Other parsers rather than Minipar could be 
used. In particular, we found out that Minipar 
always returns unambiguous trees, often making 
erroneous attachment decisions. A parser 
returning ambiguous output could be more 
desirable. The results of this paper do not 
depend on the parser used, only on the quality of 
the output, which should be at least as good as 
Minipar. 
Concerning the performance of the algorithm 
as compared to other Senseval 2 systems, it is 
not the best. Getting the best results was not the 
objective of this paper, but to show that syntactic 
features are worth including. We plan to 
improve the pre-processing of our systems, the 
detection of multiword lexical entries, etc. which 
could improve greatly the results. In addition 
there can be a number of factors that could 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: prec./cov. curve for three high precision methods on basic and basic+syntactic features. 
0,50
0,55
0,60
0,65
0,70
0,75
0,80
0,85
0,90
0,95
1,00
0 0,2 0,4 0,6 0,8 1coverage
pr
ec
isi
on
dlist threshold basic dlist feat.sel. basic boost basic
dlist threshold basic+synt dlist feat.sel. basic+synt boost basic+synt
diminish or disguise the improvement in the 
results: hand-tagging errors, word senses 
missing from training or testing data, biased 
sense distributions, errors in syntactic relations, 
etc. Factor out this ?noise? could show the real 
extent of the contribution of syntactic features. 
On the other hand, we are using a high 
number of features. It is well known that many 
ML algorithms have problems to scale to high 
dimensional feature spaces, especially when the 
number of training examples is relatively low (as 
it is the case for Senseval-2 word senses). 
Researching on more careful feature selection 
(which is dependent of the ML algorithm) could 
also improve the contribution of syntactic 
features, and WSD results in general. In 
addition, alternative methods to produce a high 
precision method based on Boost need to be 
explored. 
Finally, the results on high precision WSD 
open the avenue for acquiring further examples 
in a bootstrapping framework.  
Acknowledgements 
This research has been partially funded by McyT 
(Hermes project TIC-2000-0335-C03-03). David 
Martinez was funded by the Basque 
Government, grant AE-BFI:01.245). 
References 
Agirre, E. and D. Martinez. 2001a. Decision Lists for 
English and Basque. Proceedings of the 
SENSEVAL-2 Workshop. In conjunction with 
ACL'2001/EACL'2001. Toulouse, France. 
Agirre, E. and D. Martinez. 2001b. Analysis of 
supervised word sense disambiguation systems. Int. 
report LSI 11-2001, available from the authors. 
Brants, T. 2000. TnT - A Statistical Part-of-Speech 
Tagger. In Proc. of the Sixth Applied Natural 
Language Processing Conference, Seattle, WA. 
Carroll, J. and E. Briscoe (2001) `High precision 
extraction of grammatical relations'. In Proceedings 
of the 7th ACL/SIGPARSE International Workshop 
on Parsing Technologies, Beijing, China. 78-89.  
Collins M. 1996. A new statistical parser based on 
bigram lexical dependencies. In Proceedings of the 
34th Annual Meeting of the ACL, pages 184-191. 
Dagan I., and A. Itai. 1994. Word Sense 
Disambiguation Using a Second Language 
Monolingual Corpus. Computational Linguistics 
20:4, pp. 563--596. 
Freund Y. and R. E. Schapire. 1997. A Decision-
Theoretic Generalization of On-line Learning and 
an Application to Boosting. Journal of Computer 
and System Sciences, 55(1):119--139. 
Escudero G., L. M?rquez, G. Rigau. 2000. Boosting 
Applied to Word Sense Disambiguation. 
Proceedings of the 12th European Conference on 
Machine Learning, ECML 2000. Barcelona, Spain. 
Kilgarriff, A. and M. Palmer. (eds). 2000. Special 
issue on SENSEVAL. Computer and the 
Humanities, 34 (1-2). 
Lin, D. 1993. Principle Based parsing without 
Overgeneration. In 31st Annual Meeting of the 
Association for Computational Linguistics. 
Columbus, Ohio. pp 112-120.  
Ng, H. T. and H. B. Lee. 1996. Integrating Multiple 
Knowledge Sources to Disambiguate Word Sense: 
An Exemplar-based Approach. Proceedings of the 
34th Annual Meeting of the Association for 
Computational Linguistics. 
Preiss, J. and D. Yarowsky. 2001. Proc. of the 
Second Intl. Workshop on Evaluating Word Sense 
Disambiguation Systems (Senseval 2). In conj. with 
ACL'2001/EACL'2001. Toulouse, France. 
Schapire, R. E. and Y. Singer. 1999. Improved 
Boosting Algorithms Using Confidence-rated 
Predictions. Machine Learning, 37(3):297--336. 
Sleator, D. and D. Temperley. 1993. Parsing English 
with a Link Grammar. Third International 
Workshop on Parsing Technologies. 
Stetina J., S. Kurohashi, M. Nagao. 1998. General 
Word Sense Disambiguation Method Based on a 
Full Sentential Context. In Usage of WordNet in 
Natural Language Processing , Proceedings of 
COLING-ACL Workshop. Montreal (Canada).  
Yarowsky, D. 1994. Decision Lists for Lexical 
Ambiguity Resolution: Application to Accent 
Restoration in Spanish and French. Proceedings of 
the 32nd Annual Meeting of the Association for 
Computational Linguistics, pp. 88--95.  
Appendix: main Minipar relations. 
Relation Direct Indirect Description 
by-subj X  Subj. with passives 
C  X clausal complement 
Cn  X nominalized clause 
comp1 X  complement (PP, inf/fin clause) of noun 
Desc X  description  
Fc X  finite complement 
I  X see c and fc, dep. between clause and main verb 
Mod X  Modifier 
Obj X  Object 
pcomp-c X  clause of pp 
Pcomp-n X  nominal head of pp 
Pnmod X  postnominal modifier. 
Pred X  predicative (can be A or N) 
Sc X  sentential complement 
Subj X  subject 
Vrel X  passive verb modifier of nouns 
For each relation the acronym, whether it is used as a 
direct relation or to construct indirect relations, and a 
short description are provided. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 17?24
Manchester, August 2008
On Robustness and Domain Adaptation using SVD
for Word Sense Disambiguation
Eneko Agirre and Oier Lopez de Lacalle
Informatika Fakultatea, University of the Basque Country
20018, Donostia, Basque Country
{e.agirre,oier.lopezdelacalle}@ehu.es
Abstract
In this paper we explore robustness and
domain adaptation issues for Word Sense
Disambiguation (WSD) using Singular
Value Decomposition (SVD) and unlabeled
data. We focus on the semi-supervised do-
main adaptation scenario, where we train
on the source corpus and test on the tar-
get corpus, and try to improve results us-
ing unlabeled data. Our method yields
up to 16.3% error reduction compared to
state-of-the-art systems, being the first to
report successful semi-supervised domain
adaptation. Surprisingly the improvement
comes from the use of unlabeled data from
the source corpus, and not from the target
corpora, meaning that we get robustness
rather than domain adaptation. In addition,
we study the behavior of our system on the
target domain.
1 Introduction
In many Natural Language Processing (NLP)
tasks we find that a large collection of manually-
annotated text is used to train and test supervised
machine learning models. While these models
have been shown to perform very well when tested
on the text collection related to the training data
(what we call the source domain), the performance
drops considerably when testing on text from other
domains (called target domains).
In order to build models that perform well in
new (target) domains we usually find two settings
(Daum?e III, 2007): In the semi-supervised setting
the goal is to improve the system trained on the
source domain using unlabeled data from the tar-
get domain, and the baseline is that of the system
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
trained on the source domain. In the supervised
setting, training data from both source and tar-
get domains are used, and the baseline is provided
by the system trained on the target domain. The
semi-supervised setting is the most attractive, as it
would save developers the need to hand-annotate
target corpora every time a new domain is to be
processed.
The main goal of this paper is to use unlabeled
data in order to get better domain-adaptation re-
sults for Word Sense Disambiguation (WSD) in
the semi-supervised setting. Singular Value De-
composition (SVD) has been shown to find corre-
lations between terms which are helpful to over-
come the scarcity of training data in WSD (Gliozzo
et al, 2005). This paper explores how this ability
of SVD can be applied to the domain-adaptation of
WSD systems, and we show that SVD and unla-
beled data improve the results of two state-of-the-
art WSD systems (k-NN and SVM). For the sake of
this paper we call this set of experiments the do-
main adaptation scenario.
In addition, we also perform some related exper-
iments on just the target domain. We use unlabeled
data in order to improve the results of a system
trained and tested in the target domain. These re-
sults are complementary to the domain adaptation
experiments, and also provide an upperbound for
semi-supervised domain adaptation. We call these
experiments the target domain scenario. Note
that both scenarios are semi-supervised, in that our
focus is on the use of unlabeled data in addition to
the available labeled data.
The experiments were performed on a publicly
available corpus which was designed to study the
effect of domain in WSD (Koeling et al, 2005). It
comprises 41 nouns closely related to the SPORTS
and FINANCES domains with 300 examples for
each. The 300 examples were drawn from the
British National Corpus (Leech, 1992) (BNC), the
SPORTS section of the Reuters corpus (Leech,
17
1992), and the FINANCES section of Reuters in
equal number.
The paper is structured as follows. Section 2 re-
views prior work in the area. Section 3 presents the
datasets used, and Section 4 the learning methods,
including the application of SVD. The experimen-
tal results are presented in Section 5, for the semi-
supervised domain adaptation scenario, and Sec-
tion 6, for the target scenario. Section 7 presents
the discussion and Section 8 the conclusions and
future work.
2 Prior Work
Domain adaptation is a subject attracting more
and more attention. In the semi-supervised set-
ting, Blitzer et al (2006) use Structural Corre-
spondence Learning and unlabeled data to adapt
a Part-of-Speech tagger. They carefully select so-
called ?pivot features? to learn linear predictors,
perform SVD on the weights learned by the pre-
dictor, and thus learn correspondences among fea-
tures in both source and target domains. Our tech-
nique also uses SVD, but we directly apply it to all
features, and thus avoid the need to define pivot
features. In preliminary work we unsuccessfully
tried to carry along the idea of pivot features to
WSD. Zelikovitz and Hirsh (2001) use unlabeled
data (so-called background knowledge) with La-
tent Semantic Indexing (also based on SVD) on a
Text Classification task with positive results. They
use related unlabeled text and include it in the
term-by-document matrix to expand it and capture
better the interesting properties of the data. Their
approach is similar to our SMA method in Section
4.2).
In the supervised setting, a recent paper by
Daum?e III (2007) shows that, using a very simple
feature augmentation method coupled with Sup-
port Vector Machines, he is able to effectively
use both labeled target and source data to pro-
vide the best results in a number of NLP tasks.
His method improves or equals over previously ex-
plored more sophisticated methods (Daum?e III and
Marcu, 2006; Chelba and Acero, 2004).
Regarding WSD, some initial works made ba-
sic analysis of the particular issues. Escudero et
al. (2000) tested the supervised adaptation set-
ting on the DSO corpus, which had examples from
the Brown corpus and Wall Street Journal cor-
pus. They found that the source corpus did not
help when tagging the target corpus, showing that
tagged corpora from each domain would suffice,
and concluding that hand tagging a large general
corpus would not guarantee robust broad-coverage
WSD. Agirre and Mart??nez (2000) also used the
DSO corpus in the supervised setting to show that
training on a subset of the source corpora that is
topically related to the target corpus does allow
for some domain adaptation. Their work used the
fact that the genre tags of Brown allowed to detect
which parts of the corpus were related to the target
corpus.
More recently, Koeling et al (2005) presented
an unsupervised system to learn the predominant
senses of particular domains. Their system was
based on the use of a similarity thesaurus induced
from the domain corpus and WordNet. They used
the same dataset as in this paper for evaluation.
Chan and Ng (2007) performed supervised domain
adaptation on a manually selected subset of 21
nouns from the DSO corpus. They used active
learning, count-merging, and predominant sense
estimation in order to save target annotation ef-
fort. They showed that adding just 30% of the tar-
get data to the source examples the same precision
as the full combination of target and source data
could be achieved. They also showed that using
the source corpus allowed to significantly improve
results when only 10%-30% of the target corpus
was used for training. No data was given about the
use of both tagged corpora.
Though not addressing domain adaptation, other
works on WSD also used SVD and are closely re-
lated to the present paper. Gliozzo et al (2005)
used SVD to reduce the space of the term-to-
document matrix, and then computed the similarity
between train and test instances using a mapping
to the reduced space (similar to our SMA method
in Section 4.2). They combined other knowledge
sources into a complex kernel using SVM. They
report improved performance on a number of lan-
guages in the Senseval-3 lexical sample dataset.
Our present paper differs from theirs in that we
propose an additional method to use SVD (the OMT
method, Section 4.2), and that we evaluate the con-
tribution of unlabeled data and SVD in isolation,
leaving combination for future work.
Ando (2006) used Alternative Structured Op-
timization, which is closely related to Structural
Learning (cited above). He first trained one lin-
ear predictor for each target word, and then per-
formed SVD on 7 carefully selected submatrices
18
of the feature-to-predictor matrix of weights. The
system attained small but consistent improvements
(no significance data was given) on the Senseval-
3 lexical sample datasets using SVD and unlabeled
data.
We have previously shown (Agirre et al, 2005;
Agirre and Lopez de Lacalle, 2007) that perform-
ing SVD on the feature-to-documents matrix is a
simple technique that allows to improve perfor-
mance with and without unlabeled data. The use
of several k-NN classifiers trained on a number of
reduced and original spaces was shown to rank first
in the Senseval-3 dataset and second in the Se-
mEval 2007 competition. The present work ex-
tends our own in that we present a comprehensive
study on a domain adaptation dataset, producing
additional insight on our method and the relation
between SVD, features and unlabeled data.
3 Data sets
The dataset we use was designed for domain-
related WSD experiments by Koeling et al (2005),
and is publicly available. The examples come
from the BNC (Leech, 1992) and the SPORTS and
FINANCES sections of the Reuters corpus (Rose
et al, 2002), comprising around 300 examples
(roughly 100 from each of those corpora) for each
of the 41 nouns. The nouns were selected be-
cause they were salient in either the SPORTS or
FINANCES domains, or because they had senses
linked to those domains. The occurrences were
hand-tagged with the senses from WordNet (WN)
version 1.7.1 (Fellbaum, 1998).
Compared to the DSO corpus used in prior work
(cf. Section 2) this corpus has been explicitly cre-
ated for domain adaptation studies. DSO con-
tains texts coming from the Brown corpus and the
Wall Street Journal, but the texts are not classi-
fied according to specific domains (e.g. Sports, Fi-
nances), which make DSO less suitable to study
domain adaptation.
In addition to the labeled data, we also use
unlabeled data coming from the three sources
used in the labeled corpus: the ?written? part of
the BNC (89.7M words), the FINANCES part of
Reuters (117,734 documents, 32.5M words), and
the SPORTS part (35,317 documents, 9.1M words).
4 Learning features and methods
In this section, we review the learning features, the
two methods to apply SVD, and the two learning
algorithms used in the experiments.
4.1 Learning features
We relied on the usual features used in previous
WSD work, grouped in three main sets. Local
collocations comprise the bigrams and trigrams
formed around the target word (using either lem-
mas, word-forms, and PoS tags
1
), those formed
with the previous/posterior lemma/word-form in
the sentence, and the content words in a ?4-word
window around the target. Syntactic dependen-
cies
2
use the object, subject, noun-modifier, prepo-
sition, and sibling lemmas, when available. Fi-
nally, Bag-of-words features are the lemmas of
the content words in the whole context, plus the
salient bigrams in the context (Pedersen, 2001).
4.2 Features from the reduced space
Apart from the original space of features, we have
the so called SVD features, obtained from the
projection of the feature vectors into the reduced
space (Deerwester et al, 1990). Basically, we set
a term-by-document or feature-by-example matrix
M from the corpus (see section below for more
details). SVD decomposes it into three matrices,
M = U?V
T
. If the desired number of dimensions
in the reduced space is p, we select p rows from ?
and V , yielding ?
p
and V
p
respectively. We can
map any feature vector
~
t (which represents either a
train or test example) into the p-dimensional space
as follows:
~
t
p
=
~
t
T
V
p
?
?1
p
. Those mapped vectors
have p dimensions, and each of the dimensions is
what we call a SVD feature. We can now use the
mapped vectors (
~
t
p
) to train and test any learning
method, as usual. We have explored two different
variants in order to build the reduced matrix and
obtain the SVD features, as follows.
Single Matrix for All target words (SVD-
SMA). The method comprises the following steps:
(i) extract bag-of-word features (terms in this case)
from unlabeled corpora, (ii) build the term-by-
document matrix, (iii) decompose it with SVD, and
(iv) project the labeled data (train/test). This tech-
nique is very similar to previous work on SVD
(Gliozzo et al, 2005; Zelikovitz and Hirsh, 2001).
The dimensionality reduction is performed once,
over the whole unlabeled corpus, and it is then ap-
plied to the labeled data of each word. The reduced
1
The PoS tagging was performed with the fnTBL toolkit
(Ngai and Florian, 2001)
2
This software was kindly provided by David Yarowsky?s
group, from Johns Hopkins University.
19
space is constructed only with terms, which corre-
spond to bag-of-words features, and thus discards
the rest of the features. Given that the WSD litera-
ture has shown that all features, including local and
syntactic features, are necessary for optimal per-
formance (Pradhan et al, 2007), we propose the
following alternative to construct the matrix.
One Matrix per Target word (SVD-OMT). For
each word: (i) construct a corpus with its occur-
rences in the labeled and, if desired, unlabeled cor-
pora, (ii) extract all features, (iii) build the feature-
by-example matrix, (iv) decompose it with SVD,
and (v) project all the labeled training and test data
for the word. Note that this variant performs one
SVD process for each target word separately, hence
its name. We proposed this technique in (Agirre et
al., 2005).
An important parameter when doing SVD is the
number of dimensions in the reduced space (p).
We tried two different values for p (25 and 200) in
the BNC domain, and the results were consistent
in that 25 performed better for SVD-OMT and 200
better for SVD-SMA. Those values were chosen for
testing in the SPORTS and FINANCES domains, i.e.
25 for SVD-OMT and 200 for SVD-SMA.
4.3 Building Matrices
The methods in the previous section can be applied
to the following matrices M :
? TRAIN: The matrix comprises features from
labeled train examples alone. This matrix can
only be used to obtain OMT features.
? TRAIN ? BNC: In addition to TRAIN, we
matrix also includes unlabeled examples from
the source corpus (BNC). Both OMT and SMA
features can be obtained.
? TRAIN ? {SPORTS,FINANCES}: Like the
previous, but using unlabeled examples from
one of the target corpora (FINANCES or
SPORTS) instead. Both OMT and SMA feature
can be obtained.
Based on previous work (Agirre et al, 2005), we
used 50% of the respective unlabeled corpora for
OMT features, and the whole corpora for SMA.
4.4 Learning methods
We used two well known classifiers, Support Vec-
tor Machines (SVM) and k-Nearest Neighbors (k-
NN). Regarding SVM we used linear kernels imple-
mented in SVM-Light (Joachims, 1999). We esti-
mated the soft margin (C) for each feature space
and each word using a greedy process in a prelim-
inary experiment on the source training data using
cross-validation. The same C value was used in the
rest of the settings.
k-NN is a memory based learning method,
where the neighbors are the k most similar la-
beled examples to the test example. The similarity
among instances is measured by the cosine of their
vectors. The test instance is labeled with the sense
obtaining the maximum the sum of the weighted
vote of the k most similar contexts. We set k to
5 based on previous results (Agirre and Lopez de
Lacalle, 2007).
5 Domain adaptation scenario
In this scenario we try to adapt a general purpose
supervised WSD system trained on the source cor-
pus (BNC) to a target corpus (either SPORTS or FI-
NANCES) using unlabeled corpora only.
5.1 Experimental results
Table 1 shows the precision results for this sce-
nario. Note that all methods have full coverage,
i.e. they return a sense for all test examples, and
therefore precision suffices to compare among sys-
tems. We have computed significance ranges for
all results in this paper using bootstrap resam-
pling (Noreen, 1989). F
1
scores outside of these
intervals are assumed to be significantly different
from the related F
1
score (p < 0.05).
The table has two main parts, each regarding
to one of the target domains, SPORTS and FI-
NANCES. The use of two target domains allows to
test whether the methods behave similarly in both
domains. The columns denote the classifier and
SVD method used: the MFS column corresponds
to the most frequent sense, k-NN-ORIG (SVM-
ORIG) corresponds to performing k-NN (SVM) on
the original feature space, k-NN-OMT (SVM-OMT)
corresponds to k-NN (SVM) on the reduced dimen-
sions of the OMT strategy, and k-NN-SMA (SVM-
SMA) corresponds to k-NN (SVM) on the reduced
dimensions of the SMA strategy (cf. Section 4.2).
The rows correspond to the matrix used for SVD
(cf. Section 4.3). Note that some of the cells have
no result, because that combination is not applica-
ble, e.g. using the TRAIN ? BNC in the original
space.
In the first row (TRAIN) of Table 1 we can
see that in both domains SVM on the original
space outperforms k-NN with statistical signifi-
20
BNC? SPORTS
matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA
TRAIN 39.0?1.3 51.7?1.3 53.0?1.6 - 53.9?1.3 47.4?1.5 -
TRAIN ? SPORTS - - 47.8?1.5 49.7?1.5 - 51.8?1.5 53.8?1.5
TRAIN ? BNC - - 61.4?1.4 57.1?1.5 - 57.1?1.6 57.2?1.5
BNC? FINANCES
matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA
TRAIN 51.2?1.6 60.4?1.6 62.5?1.4 - 62.9?1.6 59.4?1.5 -
TRAIN ? FINANCES - - 57.4?1.9 60.6?1.5 - 60.4?1.4 62.7?1.4
TRAIN ? BNC - - 65.9?1.5 68.3?1.4 - 67.0?1.3 66.8?1.5
Table 1: Precision for the domain adaptation scenario: training on labeled source corpus, plus unlabeled
corpora.
cance. Those are the baseline systems. On the
same row, working on the reduced space of the
TRAIN matrix with OMT allows to improve the re-
sults of k-NN, but not for SVM.
Contrary to our expectations, adding target unla-
beled corpora (TRAIN ? SPORTS and TRAIN ? FI-
NANCES rows respectively) does not improve the
results over the baseline. But using the source un-
labeled data (TRAIN ? BNC), we find that for both
domains and in all four columns the results are sig-
nificantly better than for the best baseline in both
SPORTS and FINANCES corpora.
The best results on the TRAIN ? BNC row de-
pend on the domain corpus. While k-NN-OMT ob-
tains the best results for SPORTS, in FINANCES
k-NN-SMA is best. k-NN, in principle a weaker
method that SVM, is able to attain the same or
superior performance than SVM on the reduced
spaces.
Table 3 summarizes the main results, and also
shows the error reduction figures, which range be-
tween 6.9% and 16.3%. As the most important
conclusion, we want to stress that, in this sce-
nario, we are able to build a very robust system
just adding unlabeled source material, and that we
fail to adapt to the domain using the target cor-
pus. These results are relevant to improve a generic
WSD system to be more robust when ported to new
domains.
5.2 Controlling size
In the original experiments reported in the previ-
ous sections, the size of the unlabeled corpora was
not balanced. Due to the importance of the amount
of unlabeled data, we performed two control ex-
periments for the OMT and SMA matrices on the
domain adaptation scenario, focusing on the k-NN
method. Regarding OMT, we used the minimum
number of instances per word between BNC and
each of the target domains. The system obtained
60.0 of precision using unlabeled data from BNC
and 49.5 for SPORTS data (compared to 61.4 and
47.8 in table 1, respectively). We did the same in
the FINANCES domain, and we obtained 65.6 of
precision for BNC and 54.4 for FINANCES (com-
pared to 65.7 and 57.4 in table 1, respectively). Al-
though the contribution of BNC unlabeled data is
slightly lower in this experiment, due to the smaller
amount of data, it still outperforms the target unla-
beled data by a large margin.
In the case of the SMA matrix, we used 25% of
the BNC, which is comparable to the SPORTS and
FINANCES sizes. The results, 56.9 of precision in
SPORTS domain and 68.1 in FINANCES (compared
to 57.1 and 68.3 in table 1, respectively), confirm
that the size is not an important factor for SMA ei-
ther.
6 Target scenario
In this second scenario we focus on the target do-
main. We train and test on the target domain, and
use unlabeled data in order to improve the result.
The goal of these experiments is to check the be-
havior of our method when applied to the target
domain, in order to better understand the results on
the domain adaptation scenario. They also provide
an upperbound for semi-supervised domain adap-
tation.
6.1 Experimental results
The results are presented in table 2. All experi-
ments in this section have been performed using
3-fold cross-validation. Again, we have full cover-
age in all cases, and the significance ranges corre-
spond to the 95% confidence level. The table has
two main parts, each regarding to one of the target
domains, SPORTS and FINANCES. As in Table 1,
the columns specify the classifier and SVD method
used, and the rows correspond to the matrices used
21
SPORTS? SPORTS (xval)
matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA
TRAIN 77.8?1.2 84.5?1.0 85.0?1.1 - 85.1?1.0 81.0?1.5 -
TRAIN ? SPORTS - - 86.1?0.9 82.7?1.1 - 85.1?1.1 80.3?1.5
TRAIN ? BNC - - 84.4?1.0 80.4?1.5 - 84.3?0.9 79.8?1.2
FINANCES? FINANCES (xval)
matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMA
TRAIN 82.3?1.3 87.1?1.0 87.4?1.0 - 87.0?1.0 85.5?1.1 -
TRAIN ? SPORTS - - 87.8?0.8 84.3?1.4 - 86.4?0.9 82.9?1.1
TRAIN ? BNC - - 87.4?1.2 83.5?1.2 - 85.7?0.9 84.3?1.1
Table 2: Precision for the target scenario: training on labeled target corpora, plus unlabeled corpora.
to obtain the features.
Table 2 shows that k-NN-OMT using the tar-
get corpus (SPORTS and FINANCES, respectively)
slightly improves over the k-NN-ORIG and SVM-
ORIG classifiers, with significant difference in the
SPORTS domain. Contrary to the results on the
previous section, the source unlabeled corpus de-
grades performance, but the target corpus does al-
low for small improvements. Note that, in this sce-
nario, both SVM and k-NN perform similarly in the
original space, but only k-NN is able to profit from
the reduced space. Table 3 summarizes the best
result, alongside the error reduction.
The results of these experiments allow to con-
trast both scenarios, and to get deeper insight about
the relation between the labeled and unlabeled data
when performing SVD, as we will examine in the
next section.
7 Discussion
The main contribution of this paper is to show
that we obtain robustness when faced with do-
main shifts using a semi-supervised strategy. We
show that we can obtain it using a large, general,
unlabeled corpus. Note that our semi-supervised
method to attain robustness for domain shifts is
very cost-effective, as it does not require costly
hand-tagged material nor even large numbers of
unlabeled data from each target domain. These
results are more valuable given the lack of sub-
stantial positive results on the literature on semi-
supervised or supervised domain adaptation for
WSD (Escudero et al, 2000; Mart??nez and Agirre,
2000; Chan and Ng, 2007).
Compared to other settings, our semi-supervised
results improve over the completely unsupervised
system in (Koeling et al, 2005), which had 43.7%
and 49.9% precision for the SPORTS and FI-
NANCES domains respectively, but lag well behind
the target domain scenario, showing that there is
still room for improvement in the semi-supervised
setting.
While these results are based on a lexical sam-
ple, and thus not directly generalizable to an all-
words corpus, we think that they reflect the main
trends for nouns, as the 41 nouns where selected
among those exhibiting domain dependence (Koel-
ing et al, 2005). We can assume, though it would
be needed to be explored empirically, that other
nouns exhibiting domain independence would de-
grade less when moving to other domains, and thus
corroborate the robustness effect we have discov-
ered.
The fact that we attain robustness rather than do-
main adaptation proper deserves some analysis. In
the domain adaptation scenario only source unla-
beled data helped, but the results on the target sce-
nario show that it is the target unlabeled data which
is helping, and not the source one. Given that
SVD basically finds correlations among features,
it seems that constructing the term-by-document
(or feature-by-example) matrix with the training
data and the unlabeled corpus related to the train-
ing data is the key factor in play here.
The reasons for this can be traced back as fol-
lows. Our source corpus is the BNC, which is a
balanced corpus containing a variety of genres and
domains. The 100 examples for each word that
have been hand-tagged were gathered at random,
and thus cover several domains. For instance, the
OMT strategy for building the matrix extracts hun-
dreds of other examples from the BNC, and when
SVD collapses the features into a reduced space,
it effectively captures the most important corre-
lations in the feature-by-example matrix. When
faced with examples from a new domain, the re-
duced matrix is able to map some of the features
found in the test example to those in the train ex-
ample. Such overlap is more difficult if only 100
examples from the source domain are available.
22
SPORTS FINANCES sign. E.R (%) method
53.9?1.3 62.9?1.6 - - labeled source (SVM-ORIG: baseline )
57.1?1.5 68.3?1.4 ++ 6.9/14.5 labeled source + SVD on unlabeled source (k-NN-SMA)
61.4?1.4 65.9?1.5 ++ 16.3/8.1 labeled source + SVD on unlabeled source (k-NN-OMT)
85.1?1.0 87.0?1.0 - - labeled target (SVM-ORIG: baseline)
86.1?0.9 87.8?0.8 + 6.7/6.1 labeled target + SVD on unlabeled target (k-NN-OMT)
Table 3: Summary with the most important results for the two scenarios (best results for each in bold).
The significance column shows significance over baselines: ++ (significant in both target domains),
+ (significant in a single domain). The E.R column shows the error reduction in percentages over the
baseline methods.
The unlabeled data and SVD process allow to cap-
ture correlations among the features occurring in
the test data and those in the training data.
On the other hand, we are discarding all original
features, as we focus on the features from the re-
duced space alone. The newly found correlations
come at the price of possibly ignoring effective
original features, causing information loss. Only
when the correlations found in the reduced space
outweigh this information loss do we get better
performance on the reduced space than in the orig-
inal space. The experiment in Section 6 is impor-
tant in that it shows that the improvement is much
smaller and only significant in the target domain
scenario, which is in accordance with the hypothe-
sis above. This information loss is a motivation for
the combination of the features from the reduced
space with the original features, which will be the
focus of our future work.
Regarding the learning method and the two
strategies to apply SVD, the results show that k-
NN profits from the reduced spaces more than
SVM, even if its baseline performance is lower
than SVM. Regarding the matrix building system,
in the domain adaptation scenario, k-NN-OMT ob-
tains the best results (with statistical significance)
in the SPORTS corpus, and k-NN-SMA yields the
best results (with statistical significance) in the FI-
NANCES domain. Averaging over both domains,
k-NN-OMT is best. The target scenario results con-
firm this trend, as k-NN-OMT is superior to k-NN-
SMA in both domains. These results are in ac-
cordance with our previous experience on WSD
(Agirre et al, 2005), where our OMT method got
better results than SMA and those of (Gliozzo et
al., 2005) (who also use a method similar to SMA)
on the Senseval-3 lexical sample. While OMT re-
duces the feature-by-example matrix of each tar-
get word, SMA reduces a single term-by-document
matrix. SMA is able to find important correlations
among similar terms in the corpus, but it misses the
rich feature set used by WSD systems, as it focuses
on bag-of-words alone. OMT on the other hand is
able to find correlations between all features which
are relevant to the target word only.
8 Conclusions and Future Work
In this paper we explore robustness and domain
adaptation issues for Word Sense Disambiguation
using SVD and unlabeled data. We focus on the
semi-supervised scenario, where we train on the
source corpus (BNC), test on two target corpora
(SPORTS and FINANCES sections of Reuters), and
improve the results using unlabeled data.
Our method yields up to 16.3% error reduction
compared to SVM and k-NN on the labeled data
alone, showing the first positive results on domain
adaptation for WSD. In fact, we show that our re-
sults are due to the use of a large, general, unla-
beled corpus, and rather than domain-adaptation
proper we show robustness in face of a domain
shift. This kind of robustness is even more cost-
effective than semi-supervised domain adaptation,
as it does not require large unlabeled corpora and
repeating the computations for each new target do-
main.
This paper shows that the OMT technique to ap-
ply SVD that we proposed in (Agirre et al, 2005)
compares favorably to SMA, which has been previ-
ously used in (Gliozzo et al, 2005), and that k-NN
excels SVM on the features from the reduced space.
We also show that the unlabeled data needs to be
related to the training data, and that the benefits of
our method are larger when faced with a domain
shift (compared to test data coming from the same
domain as the training data).
In the future, we plan to combine the features
from the reduced space with the rest of features,
either using a combination of k-NN classifiers
(Agirre et al, 2005; Agirre and Lopez de Lacalle,
2007) or a complex kernel (Gliozzo et al, 2005).
23
A natural extension of our work would be to apply
our techniques to the supervised domain adapta-
tion scenario.
Acknowledgments
We wish to thank Diana McCarthy and Rob Koel-
ing for kindly providing us the Reuters tagged cor-
pora, David Mart??nez for helping us with the learn-
ing features, and Walter Daelemans for his ad-
vice on domain adaptation. Oier Lopez de La-
calle has a PhD grant from the Basque Govern-
ment. This work is partially funded by the Educa-
tion Ministry (KNOW TIN2006-15049, OpenMT
TIN2006-15307-C03-02) and the Basque Country
University (IT-397-07).
References
Agirre, E. and O. Lopez de Lacalle. 2007. UBC-ALM:
Combining k-NN with SVD for WSD. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007). Association for
Computational Linguistics.
Agirre, E., O. Lopez de Lacalle, and D. Mart??nez. 2005.
Exploring feature spaces with svd and unlabeled data
for Word Sense Disambiguation. In Proceedings of
the Conference on Recent Advances on Natural Lan-
guage Processing (RANLP?05).
Ando, R. Kubota. 2006. Applying alternating structure
optimization to word sense disambiguation. In Pro-
ceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL).
Blitzer, J., R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing.
Chan, Yee Seng and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics.
Chelba, C. and A. Acero. 2004. Adaptation of maxi-
mum entropy classifier: Little data can help a lot. In
Proceedings of of th Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Daum?e III, H. and D. Marcu. 2006. Domain adaptation
for statistical classifiers. Journal of Artificial Intelli-
gence Research, 26:101?126.
Daum?e III, H. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics.
Deerwester, S., S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society for Infor-
mation Science.
Escudero, G., L. M?arquez, and G. Rigau. 2000. An
Empirical Study of the Domain Dependence of Su-
pervised Word Sense Didanbiguation Systems. Pro-
ceedings of the joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora, EMNLP/VLC.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Gliozzo, A. M., C. Giuliano, and C. Strapparava. 2005.
Domain Kernels for Word Sense Disambiguation.
43nd Annual Meeting of the Association for Com-
putational Linguistics. (ACL-05).
Joachims, T. 1999. Making Large?Scale SVM Learn-
ing Practical. Advances in Kernel Methods ? Sup-
port Vector Learning, Cambridge, MA. MIT Press.
Koeling, R., D. McCarthy, and J. Carroll. 2005.
Domain-specific sense distributions and predomi-
nant sense acquisition. In Proceedings of the Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Process-
ing. HLT/EMNLP.
Leech, G. 1992. 100 million words of English: the
British National Corpus. Language Research.
Mart??nez, D. and E. Agirre. 2000. One Sense per Col-
location and Genre/Topic Variations. Conference on
Empirical Method in Natural Language.
Ngai, G. and R. Florian. 2001. Transformation-Based
Learning in the Fast Lane. Proceedings of the Sec-
ond Conference of the North American Chapter of
the Association for Computational Linguistics.
Noreen, E. W. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
Pedersen, T. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01).
Pradhan, S., E. Loper, D. Dligach, and M. Palmer.
2007. Semeval-2007 task-17: English lexical sam-
ple, srl and all words. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007).
Rose, T. G., M. Stevenson, and M. Whitehead. 2002.
The reuters corpus volumen 1 from yesterday?s news
to tomorrow?s language resources. In Proceedings
of the Third International Conference on Language
Resources and Evaluation (LREC-2002).
Zelikovitz, S. and H. Hirsh. 2001. Using LSI for text
classification in the presence of background text. In
Proceedings of CIKM-01, 10th ACM International
Conference on Information and Knowledge Manage-
ment. US.
24
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 33?41,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Personalizing PageRank for Word Sense Disambiguation
Eneko Agirre and Aitor Soroa
IXA NLP Group
University of the Basque Country
Donostia, Basque Contry
{e.agirre,a.soroa}@ehu.es
Abstract
In this paper we propose a new graph-
based method that uses the knowledge in
a LKB (based on WordNet) in order to
perform unsupervised Word Sense Disam-
biguation. Our algorithm uses the full
graph of the LKB efficiently, performing
better than previous approaches in English
all-words datasets. We also show that the
algorithm can be easily ported to other lan-
guages with good results, with the only re-
quirement of having a wordnet. In addi-
tion, we make an analysis of the perfor-
mance of the algorithm, showing that it is
efficient and that it could be tuned to be
faster.
1 Introduction
Word Sense Disambiguation (WSD) is a key
enabling-technology that automatically chooses
the intended sense of a word in context. Super-
vised WSD systems are the best performing in
public evaluations (Palmer et al, 2001; Snyder
and Palmer, 2004; Pradhan et al, 2007) but they
need large amounts of hand-tagged data, which is
typically very expensive to build. Given the rela-
tively small amount of training data available, cur-
rent state-of-the-art systems only beat the simple
most frequent sense (MFS) baseline1 by a small
margin. As an alternative to supervised systems,
knowledge-based WSD systems exploit the infor-
mation present in a lexical knowledge base (LKB)
to perform WSD, without using any further corpus
evidence.
1This baseline consists of tagging all occurrences in the
test data with the sense of the word that occurs more often in
the training data
Traditional knowledge-based WSD systems as-
sign a sense to an ambiguous word by comparing
each of its senses with those of the surrounding
context. Typically, some semantic similarity met-
ric is used for calculating the relatedness among
senses (Lesk, 1986; McCarthy et al, 2004). One
of the major drawbacks of these approaches stems
from the fact that senses are compared in a pair-
wise fashion and thus the number of computa-
tions can grow exponentially with the number of
words. Although alternatives like simulated an-
nealing (Cowie et al, 1992) and conceptual den-
sity (Agirre and Rigau, 1996) were tried, most of
past knowledge based WSD was done in a subop-
timal word-by-word process, i.e., disambiguating
words one at a time.
Recently, graph-based methods for knowledge-
based WSD have gained much attention in the
NLP community (Sinha and Mihalcea, 2007; Nav-
igli and Lapata, 2007; Mihalcea, 2005; Agirre
and Soroa, 2008). These methods use well-known
graph-based techniques to find and exploit the
structural properties of the graph underlying a par-
ticular LKB. Because the graph is analyzed as a
whole, these techniques have the remarkable prop-
erty of being able to find globally optimal solu-
tions, given the relations between entities. Graph-
based WSD methods are particularly suited for
disambiguating word sequences, and they man-
age to exploit the interrelations among the senses
in the given context. In this sense, they provide
a principled solution to the exponential explosion
problem, with excellent performance.
Graph-based WSD is performed over a graph
composed by senses (nodes) and relations between
pairs of senses (edges). The relations may be of
several types (lexico-semantic, coocurrence rela-
tions, etc.) and may have some weight attached to
33
them. The disambiguation is typically performed
by applying a ranking algorithm over the graph,
and then assigning the concepts with highest rank
to the corresponding words. Given the compu-
tational cost of using large graphs like WordNet,
many researchers use smaller subgraphs built on-
line for each target context.
In this paper we present a novel graph-based
WSD algorithm which uses the full graph of
WordNet efficiently, performing significantly bet-
ter that previously published approaches in En-
glish all-words datasets. We also show that the
algorithm can be easily ported to other languages
with good results, with the only requirement of
having a wordnet. The algorithm is publicly avail-
able2 and can be applied easily to sense invento-
ries and knowledge bases different from WordNet.
Our analysis shows that our algorithm is efficient
compared to previously proposed alternatives, and
that a good choice of WordNet versions and rela-
tions is fundamental for good performance.
The paper is structured as follows. We first de-
scribe the PageRank and Personalized PageRank
algorithms. Section 3 introduces the graph based
methods used for WSD. Section 4 shows the ex-
perimental setting and the main results, and Sec-
tion 5 compares our methods with related exper-
iments on graph-based WSD systems. Section 6
shows the results of the method when applied to
a Spanish dataset. Section 7 analyzes the perfor-
mance of the algorithm. Finally, we draw some
conclusions in Section 8.
2 PageRank and Personalized PageRank
The celebrated PageRank algorithm (Brin and
Page, 1998) is a method for ranking the vertices
in a graph according to their relative structural
importance. The main idea of PageRank is that
whenever a link from vi to vj exists in a graph, a
vote from node i to node j is produced, and hence
the rank of node j increases. Besides, the strength
of the vote from i to j also depends on the rank
of node i: the more important node i is, the more
strength its votes will have. Alternatively, PageR-
ank can also be viewed as the result of a random
walk process, where the final rank of node i rep-
resents the probability of a random walk over the
graph ending on node i, at a sufficiently large time.
Let G be a graph with N vertices v1, . . . , vN
and di be the outdegree of node i; let M be a
2http://ixa2.si.ehu.es/ukb
N?N transition probability matrix, where Mji =
1
di
if a link from i to j exists, and zero otherwise.
Then, the calculation of the PageRank vector Pr
over G is equivalent to resolving Equation (1).
Pr = cMPr + (1 ? c)v (1)
In the equation, v is a N ? 1 vector whose ele-
ments are 1N and c is the so called damping factor,
a scalar value between 0 and 1. The first term of
the sum on the equation models the voting scheme
described in the beginning of the section. The sec-
ond term represents, loosely speaking, the proba-
bility of a surfer randomly jumping to any node,
e.g. without following any paths on the graph.
The damping factor, usually set in the [0.85..0.95]
range, models the way in which these two terms
are combined at each step.
The second term on Eq. (1) can also be seen as
a smoothing factor that makes any graph fulfill the
property of being aperiodic and irreducible, and
thus guarantees that PageRank calculation con-
verges to a unique stationary distribution.
In the traditional PageRank formulation the vec-
tor v is a stochastic normalized vector whose ele-
ment values are all 1N , thus assigning equal proba-
bilities to all nodes in the graph in case of random
jumps. However, as pointed out by (Haveliwala,
2002), the vector v can be non-uniform and assign
stronger probabilities to certain kinds of nodes, ef-
fectively biasing the resulting PageRank vector to
prefer these nodes. For example, if we concen-
trate all the probability mass on a unique node i,
all random jumps on the walk will return to i and
thus its rank will be high; moreover, the high rank
of i will make all the nodes in its vicinity also re-
ceive a high rank. Thus, the importance of node i
given by the initial distribution of v spreads along
the graph on successive iterations of the algorithm.
In this paper, we will use traditional PageRank
to refer to the case when a uniform v vector is used
in Eq. (1); and whenever a modified v is used, we
will call it Personalized PageRank. The next sec-
tion shows how we define a modified v.
PageRank is actually calculated by applying an
iterative algorithm which computes Eq. (1) suc-
cessively until convergence below a given thresh-
old is achieved, or, more typically, until a fixed
number of iterations are executed.
Regarding PageRank implementation details,
we chose a damping value of 0.85 and finish the
calculation after 30 iterations. We did not try other
34
damping factors. Some preliminary experiments
with higher iteration counts showed that although
sometimes the node ranks varied, the relative order
among particular word synsets remained stable af-
ter the initial iterations (cf. Section 7 for further
details). Note that, in order to discard the effect
of dangling nodes (i.e. nodes without outlinks) we
slightly modified Eq. (1). For the sake of brevity
we omit the details, which the interested reader
can check in (Langville and Meyer, 2003).
3 Using PageRank for WSD
In this section we present the application of
PageRank to WSD. If we were to apply the tra-
ditional PageRank over the whole WordNet we
would get a context-independent ranking of word
senses, which is not what we want. Given an input
piece of text (typically one sentence, or a small set
of contiguous sentences), we want to disambiguate
all open-class words in the input taken the rest as
context. In this framework, we need to rank the
senses of the target words according to the other
words in the context. Theare two main alternatives
to achieve this:
? To create a subgraph of WordNet which con-
nects the senses of the words in the input text,
and then apply traditional PageRank over the
subgraph.
? To use Personalized PageRank, initializing v
with the senses of the words in the input text
The first method has been explored in the lit-
erature (cf. Section 5), and we also presented a
variant in (Agirre and Soroa, 2008) but the second
method is novel in WSD. In both cases, the algo-
rithms return a list of ranked senses for each target
word in the context. We will see each of them in
turn, but first we will present some notation and a
preliminary step.
3.1 Preliminary step
A LKB is formed by a set of concepts and relations
among them, and a dictionary, i.e., a list of words
(typically, word lemmas) each of them linked to
at least one concept of the LKB. Given any such
LKB, we build an undirected graph G = (V, E)
where nodes represent LKB concepts (vi), and
each relation between concepts vi and vj is rep-
resented by an undirected edge ei,j .
In our experiments we have tried our algorithms
using three different LKBs:
? MCR16 + Xwn: The Multilingual Central
Repository (Atserias et al, 2004b) is a lexical
knowledge base built within the MEANING
project3. This LKB comprises the original
WordNet 1.6 synsets and relations, plus some
relations from other WordNet versions auto-
matically mapped4 into version 1.6: WordNet
2.0 relations and eXtended WordNet relations
(Mihalcea and Moldovan, 2001) (gold, silver
and normal relations). The resulting graph
has 99, 632 vertices and 637, 290 relations.
? WNet17 + Xwn: WordNet 1.7 synset and
relations and eXtended WordNet relations.
The graph has 109, 359 vertices and 620, 396
edges
? WNet30 + gloss: WordNet 3.0 synset and
relations, including manually disambiguated
glosses . The graph has 117, 522 vertices and
525, 356 relations.
Given an input text, we extract the list Wi i =
1 . . .m of content words (i.e. nouns, verbs, ad-
jectives and adverbs) which have an entry in the
dictionary, and thus can be related to LKB con-
cepts. Let Concepts i = {v1, . . . , vim} be the
im associated concepts of word Wi in the LKB
graph. Note that monosemous words will be re-
lated to just one concept, whereas polysemous
words may be attached to several. As a result
of the disambiguation process, every concept in
Concepts i, i = 1, . . . , m receives a score. Then,
for each target word to be disambiguated, we just
choose its associated concept in G with maximal
score.
In our experiments we build a context of at least
20 content words for each sentence to be disam-
biguated, taking the sentences immediately before
and after it in the case that the original sentence
was too short.
3.2 Traditional PageRank over Subgraph
(Spr)
We follow the algorithm presented in (Agirre and
Soroa, 2008), which we explain here for complete-
ness. The main idea of the subgraph method is to
extract the subgraph of GKB whose vertices and
relations are particularly relevant for a given input
3http://nipadio.lsi.upc.es/nlp/meaning
4We use the freely available WordNet mappings from
http://www.lsi.upc.es/?nlp/tools/download-map.php
35
context. Such a subgraph is called a ?disambigua-
tion subgraph? GD, and it is built in the following
way. For each word Wi in the input context and
each concept vi ? Concepts i, a standard breath-
first search (BFS) over GKB is performed, start-
ing at node vi. Each run of the BFS calculates the
minimum distance paths between vi and the rest of
concepts of GKB . In particular, we are interested
in the minimum distance paths between vi and the
concepts associated to the rest of the words in the
context, vj ?
?
j 6=i Conceptsj . Let mdpvi be the
set of these shortest paths.
This BFS computation is repeated for every
concept of every word in the input context, stor-
ing mdpvi accordingly. At the end, we obtain a
set of minimum length paths each of them hav-
ing a different concept as a source. The disam-
biguation graph GD is then just the union of the
vertices and edges of the shortest paths, GD =
?m
i=1{mdpvj/vj ? Concepts i}.
The disambiguation graph GD is thus a sub-
graph of the original GKB graph obtained by com-
puting the shortest paths between the concepts of
the words co-occurring in the context. Thus, we
hypothesize that it captures the most relevant con-
cepts and relations in the knowledge base for the
particular input context.
Once the GD graph is built, we compute the tra-
ditional PageRank algorithm over it. The intuition
behind this step is that the vertices representing
the correct concepts will be more relevant in GD
than the rest of the possible concepts of the context
words, which should have less relations on average
and be more isolated.
As usual, the disambiguation step is performed
by assigning to each word Wi the associated con-
cept in Concepts i which has maximum rank. In
case of ties we assign all the concepts with maxi-
mum rank. Note that the standard evaluation script
provided in the Senseval competitions treats mul-
tiple senses as if one was chosen at random, i.e.
for evaluation purposes our method is equivalent
to breaking ties at random.
3.3 Personalized PageRank (Ppr and
Ppr w2w)
As mentioned before, personalized PageRank al-
lows us to use the full LKB. We first insert the
context words into the graph G as nodes, and link
them with directed edges to their respective con-
cepts. Then, we compute the personalized PageR-
ank of the graph G by concentrating the initial
probability mass uniformly over the newly intro-
duced word nodes. As the words are linked to
the concepts by directed edges, they act as source
nodes injecting mass into the concepts they are as-
sociated with, which thus become relevant nodes,
and spread their mass over the LKB graph. There-
fore, the resulting personalized PageRank vector
can be seen as a measure of the structural rele-
vance of LKB concepts in the presence of the input
context.
One problem with Personalized PageRank is
that if one of the target words has two senses
which are related by semantic relations, those
senses reinforce each other, and could thus
dampen the effect of the other senses in the con-
text. With this observation in mind we devised
a variant (dubbed Ppr w2w), where we build the
graph for each target word in the context: for each
target word Wi, we concentrate the initial proba-
bility mass in the senses of the words surrounding
Wi, but not in the senses of the target word itself,
so that context words increase its relative impor-
tance in the graph. The main idea of this approach
is to avoid biasing the initial score of concepts as-
sociated to target word Wi, and let the surround-
ing words decide which concept associated to Wi
has more relevance. Contrary to the other two ap-
proaches, Ppr w2w does not disambiguate all tar-
get words of the context in a single run, which
makes it less efficient (cf. Section 7).
4 Evaluation framework and results
In this paper we will use two datasets for com-
paring graph-based WSD methods, namely, the
Senseval-2 (S2AW) and Senseval-3 (S3AW) all
words datasets (Snyder and Palmer, 2004; Palmer
et al, 2001), which are both labeled with WordNet
1.7 tags. We did not use the Semeval dataset, for
the sake of comparing our results to related work,
none of which used Semeval data. Table 1 shows
the results as recall of the graph-based WSD sys-
tem over these datasets on the different LKBs. We
detail overall results, as well as results per PoS,
and the confidence interval for the overall results.
The interval was computed using bootstrap resam-
pling with 95% confidence.
The table shows that Ppr w2w is consistently
the best method in both datasets and for all LKBs.
Ppr and Spr obtain comparable results, which is
remarkable, given the simplicity of the Ppr algo-
36
Senseval-2 All Words dataset
LKB Method All N V Adj. Adv. Conf. interval
MCR16 + Xwn Ppr 51.1 64.9 38.1 57.4 47.5 [49.3, 52.6]
MCR16 + Xwn Ppr w2w 53.3 64.5 38.6 58.3 48.1 [52.0, 55.0]
MCR16 + Xwn Spr 52.7 64.8 35.3 56.8 50.2 [51.3, 54.4]
WNet17 + Xwn Ppr 56.8 71.1 33.4 55.9 67.1 [55.0, 58.7]
WNet17 + Xwn Ppr w2w 58.6 70.4 38.9 58.3 70.1 [56.7, 60.3]
WNet17 + Xwn Spr 56.7 66.8 37.7 57.6 70.8 [55.0, 58.2]
WNet30 + gloss Ppr 53.5 70.0 28.6 53.9 55.1 [51.8, 55.2]
WNet30 + gloss Ppr w2w 55.8 71.9 34.4 53.8 57.5 [54.1, 57.8]
WNet30 + gloss Spr 54.8 68.9 35.1 55.2 56.5 [53.2, 56.3]
MFS 60.1 71.2 39.0 61.1 75.4 [58.6, 61.9]
SMUaw 68.6 78.0 52.9 69.9 81.7
Senseval-3 All Words dataset
LKB Method All N V Adj. Adv.
MCR16 + Xwn Ppr 54.3 60.9 45.4 56.5 92.9 [52.3, 56.1]
MCR16 + Xwn Ppr w2w 55.8 63.2 46.2 57.5 92.9 [53.7, 57.7]
MCR16 + Xwn Static 53.7 59.5 45.0 57.8 92.9 [51.8, 55.7]
WNet17 + Xwn Ppr 56.1 62.6 46.0 60.8 92.9 [54.0, 58.1]
WNet17 + Xwn Ppr w2w 57.4 64.1 46.9 62.6 92.9 [55.5, 59.3]
WNet17 + Xwn Spr 56.20 61.6 47.3 61.8 92.9 [54.8, 58.2]
WNet30 + gloss Ppr 48.5 52.2 41.5 54.2 78.6 [46.7, 50.6]
WNet30 + gloss Ppr w2w 51.6 59.0 40.2 57.2 78.6 [49.9, 53.3]
WNet30 + gloss Spr 45.4 54.1 31.4 52.5 78.6 [43.7, 47.4]
MFS 62.3 69.3 53.6 63.7 92.9 [60.2, 64.0]
GAMBL 65.2 70.8 59.3 65.3 100
Table 1: Results (as recall) on Senseval-2 and Senseval-3 all words tasks. We also include the MFS
baseline and the best results of supervised systems at competition time (SMUaw,GAMBL).
rithm, compared to the more elaborate algorithm
to construct the graph. The differences between
methods are not statistically significant, which is a
common problem on this relatively small datasets
(Snyder and Palmer, 2004; Palmer et al, 2001).
Regarding LKBs, the best results are obtained
using WordNet 1.7 and eXtended WordNet. Here
the differences are in many cases significant.
These results are surprising, as we would ex-
pect that the manually disambiguated gloss re-
lations from WordNet 3.0 would lead to bet-
ter results, compared to the automatically disam-
biguated gloss relations from the eXtended Word-
Net (linked to version 1.7). The lower perfor-
mance of WNet30+gloss can be due to the fact
that the Senseval all words data set is tagged using
WordNet 1.7 synsets. When using a different LKB
for WSD, a mapping to WordNet 1.7 is required.
Although the mapping is cited as having a correct-
ness on the high 90s (Daude et al, 2000), it could
have introduced sufficient noise to counteract the
benefits of the hand-disambiguated glosses.
Table 1 also shows the most frequent sense
(MFS), as well as the best supervised sys-
tems (Snyder and Palmer, 2004; Palmer et
al., 2001) that participated in each competition
(SMUaw and GAMBL, respectively). The MFS is
a baseline for supervised systems, but it is consid-
ered a difficult competitor for unsupervised sys-
tems, which rarely come close to it. In this case
the MFS baseline was computed using previously
availabel training data like SemCor. Our best re-
sults are close to the MFS in both Senseval-2 and
Senseval-3 datasets. The results for the supervised
system are given for reference, and we can see that
the gap is relatively small, specially for Senseval-
3.
5 Comparison to Related work
In this section we will briefly describe some
graph-based methods for knowledge-based WSD.
The methods here presented cope with the prob-
lem of sequence-labeling, i.e., they disambiguate
all the words coocurring in a sequence (typically,
all content words of a sentence). All the meth-
ods rely on the information represented on some
LKB, which typically is some version of Word-
Net, sometimes enriched with proprietary rela-
tions. The results on our datasets, when available,
are shown in Table 2. The table also shows the
performance of supervised systems.
The TexRank algorithm (Mihalcea, 2005) for
WSD creates a complete weighted graph (e.g. a
graph where every pair of distinct vertices is con-
nected by a weighted edge) formed by the synsets
of the words in the input context. The weight
37
Senseval-2 All Words dataset
System All N V Adj. Adv.
Mih05 54.2 57.5 36.5 56.7 70.9
Sihna07 56.4 65.6 32.3 61.4 60.2
Tsatsa07 49.2 ? ? ? ?
Spr 56.6 66.7 37.5 57.6 70.8
Ppr 56.8 71.1 33.4 55.9 67.1
Ppr w2w 58.6 70.4 38.9 58.3 70.1
MFS 60.1 71.2 39.0 61.1 75.4
Senseval-3 All Words dataset
System All N V Adj. Adv.
Mih05 52.2 - - - -
Sihna07 52.4 60.5 40.6 54.1 100.0
Nav07 - 61.9 36.1 62.8 -
Spr 56.2 61.6 47.3 61.8 92.9
Ppr 56.1 62.6 46.0 60.8 92.9
Ppr w2w 57.4 64.1 46.9 62.6 92.9
MFS 62.3 69.3 53.6 63.7 92.9
Nav05 60.4 - - - -
Table 2: Comparison with related work. Note that
Nav05 uses the MFS.
of the links joining two synsets is calculated by
executing Lesk?s algorithm (Lesk, 1986) between
them, i.e., by calculating the overlap between the
words in the glosses of the correspongind senses.
Once the complete graph is built, the PageRank al-
gorithm is executed over it and words are assigned
to the most relevant synset. In this sense, PageR-
ank is used an alternative to simulated annealing
to find the optimal pairwise combinations. The
method was evaluated on the Senseval-3 dataset,
as shown in row Mih05 on Table 2.
(Sinha and Mihalcea, 2007) extends their pre-
vious work by using a collection of semantic sim-
ilarity measures when assigning a weight to the
links across synsets. They also compare differ-
ent graph-based centrality algorithms to rank the
vertices of the complete graph. They use differ-
ent similarity metrics for different POS types and
a voting scheme among the centrality algorithm
ranks. Here, the Senseval-3 corpus was used as
a development data set, and we can thus see those
results as the upper-bound of their method.
We can see in Table 2 that the methods pre-
sented in this paper clearly outperform both Mih05
and Sin07. This result suggests that analyzing the
LKB structure as a whole is preferable than com-
puting pairwise similarity measures over synsets.
The results of various in-house made experiments
replicating (Mihalcea, 2005) also confirm this ob-
servation. Note also that our methods are simpler
than the combination strategy used in (Sinha and
Mihalcea, 2007), and that we did not perform any
parameter tuning as they did.
In (Navigli and Velardi, 2005) the authors de-
velop a knowledge-based WSD method based on
lexical chains called structural semantic intercon-
nections (SSI). Although the system was first de-
signed to find the meaning of the words in Word-
Net glosses, the authors also apply the method for
labeling text sequences. Given a text sequence,
SSI first identifies monosemous words and assigns
the corresponding synset to them. Then, it iter-
atively disambiguates the rest of terms by select-
ing the senses that get the strongest interconnec-
tion with the synsets selected so far. The inter-
connection is calculated by searching for paths on
the LKB, constrained by some hand-made rules of
possible semantic patterns. The method was eval-
uated on the Senseval-3 dataset, as shown in row
Nav05 on Table 2. Note that the method labels
an instance with the most frequent sense of the
word if the algorithm produces no output for that
instance, which makes comparison to our system
unfair, specially given the fact that the MFS per-
forms better than SSI. In fact it is not possible to
separate the effect of SSI from that of the MFS.
For this reason we place this method close to the
MFS baseline in Table 2.
In (Navigli and Lapata, 2007), the authors per-
form a two-stage process for WSD. Given an input
context, the method first explores the whole LKB
in order to find a subgraph which is particularly
relevant for the words of the context. Then, they
study different graph-based centrality algorithms
for deciding the relevance of the nodes on the sub-
graph. As a result, every word of the context is
attached to the highest ranking concept among its
possible senses. The Spr method is very similar
to (Navigli and Lapata, 2007), the main differ-
ence lying on the initial method for extracting the
context subgraph. Whereas (Navigli and Lapata,
2007) apply a depth-first search algorithm over the
LKB graph ?and restrict the depth of the subtree
to a value of 3?, Spr relies on shortest paths be-
tween word synsets. Navigli and Lapata don?t re-
port overall results and therefore, we can?t directly
compare our results with theirs. However, we can
see that on a PoS-basis evaluation our results are
consistently better for nouns and verbs (especially
the Ppr w2w method) and rather similar for adjec-
tives.
(Tsatsaronis et al, 2007) is another example of
a two-stage process, the first one consisting on
finding a relevant subgraph by performing a BFS
38
Spanish Semeval07
LKB Method Acc.
Spanish Wnet + Xnet? Ppr 78.4
Spanish Wnet + Xnet? Ppr w2w 79.3
? MFS 84.6
? Supervised 85.10
Table 3: Results (accuracy) on Spanish Semeval07
dataset, including MFS and the best supervised
system in the competition.
search over the LKB. The authors apply a spread-
ing activation algorithm over the subgraph for
node ranking. Edges of the subgraph are weighted
according to its type, following a tf.idf like ap-
proach. The results show that our methods clearly
outperform Tsatsa07. The fact that the Spr method
works better suggests that the traditional PageR-
ank algorithm is a superior method for ranking the
subgraph nodes.
As stated before, all methods presented here
use some LKB for performing WSD. (Mihalcea,
2005) and (Sinha and Mihalcea, 2007) use Word-
Net relations as a knowledge source, but neither
of them specify which particular version did they
use. (Tsatsaronis et al, 2007) uses WordNet 1.7
enriched with eXtended WordNet relations, just
as we do. Both (Navigli and Velardi, 2005; Nav-
igli and Lapata, 2007) use WordNet 2.0 as the un-
derlying LKB, albeit enriched with several new
relations, which are manually created. Unfor-
tunately, those manual relations are not publicly
available, so we can?t directly compare their re-
sults with the rest of the methods. In (Agirre and
Soroa, 2008) we experiment with different LKBs
formed by combining relations of different MCR
versions along with relations extracted from Sem-
Cor, which we call supervised and unsupervised
relations, respectively. The unsupervised relations
that yielded bests results are also used in this paper
(c.f Section 3.1).
6 Experiments on Spanish
Our WSD algorithm can be applied over non-
english texts, provided that a LKB for this partic-
ular language exists. We have tested the graph-
algorithms proposed in this paper on a Spanish
dataset, using the Spanish WordNet as knowledge
source (Atserias et al, 2004a).
We used the Semeval-2007 Task 09 dataset as
evaluation gold standard (Ma`rquez et al, 2007).
The dataset contains examples of the 150 most
frequent nouns in the CESS-ECE corpus, manu-
Method Time
Ppr 26m46
Spr 119m7
Ppr w2w 164m4
Table 4: Elapsed time (in minutes) of the algo-
rithms when applied to the Senseval-2 dataset.
ally annotated with Spanish WordNet synsets. It
is split into a train and test part, and has an ?all
words? shape i.e. input consists on sentences,
each one having at least one occurrence of a tar-
get noun. We ran the experiment over the test part
(792 instances), and used the train part for cal-
culating the MFS baseline. We used the Span-
ish WordNet as LKB, enriched with eXtended
WordNet relations. It contains 105, 501 nodes and
623, 316 relations. The results in Table 3 are con-
sistent with those for English, with our algorithm
approaching MFS performance. Note that for this
dataset the supervised algorithm could barely im-
prove over the MFS, suggesting that for this par-
ticular dataset MFS is particularly strong.
7 Performance analysis
Table 4 shows the time spent by the different al-
gorithms when applied to the Senseval-2 all words
dataset, using the WNet17 + Xwn as LKB. The
dataset consists on 2473 word instances appear-
ing on 476 different sentences. The experiments
were done on a computer with four 2.66 Ghz pro-
cessors and 16 Gb memory. The table shows that
the time elapsed by the algorithms varies between
30 minutes for the Ppr method (which thus dis-
ambiguates circa 82 instances per minute) to al-
most 3 hours spent by the Ppr w2w method (circa
15 instances per minute). The Spr method lies
in between, requiring 2 hours for completing the
task, but its overall performance is well below the
PageRank based Ppr w2w method. Note that the
algorithm is coded in C++ for greater efficiency,
and uses the Boost Graph Library.
Regarding PageRank calculation, we have tried
different numbers of iterations, and analyze the
rate of convergence of the algorithm. Figure 1 de-
picts the performance of the Ppr w2w method for
different iterations of the algorithm. As before, the
algorithm is applied over the MCR17 + Xwn LKB,
and evaluated on the Senseval-2 all words dataset.
The algorithm converges very quickly: one sole it-
eration suffices for achieving a relatively high per-
39
57
57.2
57.4
57.6
57.8
58
58.2
58.4
58.6
0 5 10 15 20 25 30
R
ec
al
l
Iterations
Rate of convergence
3
3
3
3 3
3 3 3
Figure 1: Rate of convergence of PageRank algo-
rithm over the MCR17 + Xwn LKB.
formance, and 20 iterations are enough for achiev-
ing convergence. The figure shows that, depend-
ing on the LKB complexity, the user can tune the
algorithm and lower the number of iterations, thus
considerably reducing the time required for disam-
biguation.
8 Conclusions
In this paper we propose a new graph-based
method that uses the knowledge in a LKB (based
on WordNet) in order to perform unsupervised
Word Sense Disambuation. Our algorithm uses the
full graph of the LKB efficiently, performing bet-
ter than previous approaches in English all-words
datasets. We also show that the algorithm can be
easily ported to other languages with good results,
with the only requirement of having a wordnet.
Both for Spanish and English the algorithm attains
performances close to the MFS.
The algorithm is publicly available5 and can be
applied easily to sense inventories and knowledge
bases different from WordNet. Our analysis shows
that our algorithm is efficient compared to previ-
ously proposed alternatives, and that a good choice
of WordNet versions and relations is fundamental
for good performance.
Acknowledgments
This work has been partially funded by the EU Commission
(project KYOTO ICT-2007-211423) and Spanish Research
Department (project KNOW TIN2006-15049-C03-01).
References
E. Agirre and G. Rigau. 1996. Word sense disam-
biguation using conceptual density. In In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics, pages 16?22.
5http://ixa2.si.ehu.es/ukb
E. Agirre and A. Soroa. 2008. Using the multilin-
gual central repository for graph-based word sense
disambiguation. In Proceedings of LREC ?08, Mar-
rakesh, Morocco.
J. Atserias, G. Rigau, and L. Villarejo. 2004a. Span-
ish wordnet 1.6: Porting the spanish wordnet across
princeton versions. In In Proceedings of LREC ?04.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004b. The meaning
multilingual central repository. In In Proceedings of
GWC, Brno, Czech Republic.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1-7).
J. Cowie, J. Guthrie, and L. Guthrie. 1992. Lexical
disambiguation using simulated annealing. In HLT
?91: Proceedings of the workshop on Speech and
Natural Language, pages 238?242, Morristown, NJ,
USA.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping
WordNets using structural information. In Proceed-
ings of ACL?2000, Hong Kong.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02: Proceedings of the 11th international
conference on World Wide Web, pages 517?526,
New York, NY, USA. ACM.
A. N. Langville and C. D. Meyer. 2003. Deeper inside
pagerank. Internet Mathematics, 1(3):335?380.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference
on Systems documentation, pages 24?26, New York,
NY, USA. ACM.
L. Ma`rquez, L. Villarejo, M. A. Mart??, and M. Taule?.
2007. Semeval-2007 task 09: Multilevel semantic
annotation of catalan and spanish. In Proceedings
of SemEval-2007, pages 42?47, Prague, Czech Re-
public, June.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant word senses in untagged
text. In ACL ?04: Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, page 279, Morristown, NJ, USA. Association
for Computational Linguistics.
R. Mihalcea and D. I. Moldovan. 2001. eXtended
WordNet: Progress report. In in Proceedings of
NAACL Workshop on WordNet and Other Lexical
Resources, pages 95?100.
R. Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
HLT05, Morristown, NJ, USA.
40
R. Navigli and M. Lapata. 2007. Graph connectivity
measures for unsupervised word sense disambigua-
tion. In IJCAI.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: A knowledge-based approach
to word sense disambiguation. IEEE Trans. Pattern
Anal. Mach. Intell., 27(7):1075?1086.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and H.T.
Dang. 2001. English tasks: All-words and verb
lexical sample. In Proc. of SENSEVAL-2: Second
International Workshop on Evaluating Word Sense
Disambiguation Systems, Tolouse, France, July.
S. Pradhan, E. Loper, D. Dligach, and M.Palmer. 2007.
Semeval-2007 task-17: English lexical sample srl
and all words. In Proceedings of SemEval-2007,
pages 87?92, Prague, Czech Republic, June.
R. Sinha and R. Mihalcea. 2007. Unsupervised graph-
based word sense disambiguation using measures
of word semantic similarity. In Proceedings of the
IEEE International Conference on Semantic Com-
puting (ICSC 2007), Irvine, CA, USA.
B. Snyder and M. Palmer. 2004. The English all-words
task. In ACL 2004 Senseval-3 Workshop, Barcelona,
Spain, July.
G. Tsatsaronis, M. Vazirgiannis, and I. Androutsopou-
los. 2007. Word sense disambiguation with spread-
ing activation networks generated from thesauri. In
IJCAI.
41
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 42?50,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Supervised Domain Adaption for WSD
Eneko Agirre and Oier Lopez de Lacalle
IXA NLP Group
University of the Basque Country
Donostia, Basque Contry
{e.agirre,oier.lopezdelacalle}@ehu.es
Abstract
The lack of positive results on super-
vised domain adaptation for WSD have
cast some doubts on the utility of hand-
tagging general corpora and thus devel-
oping generic supervised WSD systems.
In this paper we show for the first time
that our WSD system trained on a general
source corpus (BNC) and the target corpus,
obtains up to 22% error reduction when
compared to a system trained on the tar-
get corpus alone. In addition, we show
that as little as 40% of the target corpus
(when supplemented with the source cor-
pus) is sufficient to obtain the same results
as training on the full target data. The key
for success is the use of unlabeled data
with SVD, a combination of kernels and
SVM.
1 Introduction
In many Natural Language Processing (NLP)
tasks we find that a large collection of manually-
annotated text is used to train and test supervised
machine learning models. While these models
have been shown to perform very well when tested
on the text collection related to the training data
(what we call the source domain), the perfor-
mance drops considerably when testing on text
from other domains (called target domains).
In order to build models that perform well in
new (target) domains we usually find two settings
(Daume? III, 2007). In the semi-supervised setting,
the training hand-annotated text from the source
domain is supplemented with unlabeled data from
the target domain. In the supervised setting, we
use training data from both the source and target
domains to test on the target domain.
In (Agirre and Lopez de Lacalle, 2008) we
studied semi-supervised Word Sense Disambigua-
tion (WSD) adaptation, and in this paper we fo-
cus on supervised WSD adaptation. We compare
the performance of similar supervised WSD sys-
tems on three different scenarios. In the source
to target scenario the WSD system is trained on
the source domain and tested on the target do-
main. In the target scenario the WSD system
is trained and tested on the target domain (using
cross-validation). In the adaptation scenario the
WSD system is trained on both source and target
domain and tested in the target domain (also using
cross-validation over the target data). The source
to target scenario represents a weak baseline for
domain adaptation, as it does not use any exam-
ples from the target domain. The target scenario
represents the hard baseline, and in fact, if the do-
main adaptation scenario does not yield better re-
sults, the adaptation would have failed, as it would
mean that the source examples are not useful when
we do have hand-labeled target examples.
Previous work shows that current state-of-the-
art WSD systems are not able to obtain better re-
sults on the adaptation scenario compared to the
target scenario (Escudero et al, 2000; Agirre and
Mart??nez, 2004; Chan and Ng, 2007). This would
mean that if a user of a generic WSD system (i.e.
based on hand-annotated examples from a generic
corpus) would need to adapt it to a specific do-
main, he would be better off throwing away the
generic examples and hand-tagging domain exam-
ples directly. This paper will show that domain
adaptation is feasible, even for difficult domain-
related words, in the sense that generic corpora
can be reused when deploying WSD systems in
specific domains. We will also show that, given
the source corpus, our technique can save up to
60% of effort when tagging domain-related occur-
rences.
We performed on a publicly available corpus
which was designed to study the effect of domains
in WSD (Koeling et al, 2005). It comprises 41
42
nouns which are highly relevant in the SPORTS
and FINANCES domains, with 300 examples for
each. The use of two target domains strengthens
the conclusions of this paper.
Our system uses Singular Value Decomposi-
tion (SVD) in order to find correlations between
terms, which are helpful to overcome the scarcity
of training data in WSD (Gliozzo et al, 2005).
This work explores how this ability of SVD and
a combination of the resulting feature spaces im-
proves domain adaptation. We present two ways
to combine the reduced spaces: kernel combina-
tion with Support Vector Machines (SVM), and k
Nearest-Neighbors (k-NN) combination.
The paper is structured as follows. Section 2 re-
views prior work in the area. Section 3 presents
the data sets used. In Section 4 we describe
the learning features, including the application of
SVD, and in Section 5 the learning methods and
the combination. The experimental results are pre-
sented in Section 6. Section 7 presents the discus-
sion and some analysis of this paper and finally
Section 8 draws the conclusions.
2 Prior work
Domain adaptation is a practical problem attract-
ing more and more attention. In the supervised
setting, a recent paper by Daume? III (2007) shows
that a simple feature augmentation method for
SVM is able to effectively use both labeled tar-
get and source data to provide the best domain-
adaptation results in a number of NLP tasks. His
method improves or equals over previously ex-
plored more sophisticated methods (Daume? III
and Marcu, 2006; Chelba and Acero, 2004). The
feature augmentation consists in making three ver-
sion of the original features: a general, a source-
specific and a target-specific versions. That way
the augmented source contains the general and
source-specific version and the augmented target
data general and specific versions. The idea be-
hind this is that target domain data has twice the
influence as the source when making predictions
about test target data. We reimplemented this
method and show that our results are better.
Regarding WSD, some initial works made a ba-
sic analysis of domain adaptation issues. Escud-
ero et al (2000) tested the supervised adaptation
scenario on the DSO corpus, which had examples
from the Brown corpus and Wall Street Journal
corpus. They found that the source corpus did
not help when tagging the target corpus, show-
ing that tagged corpora from each domain would
suffice, and concluding that hand tagging a large
general corpus would not guarantee robust broad-
coverage WSD. Agirre and Mart??nez (2000) used
the DSO corpus in the supervised scenario to show
that training on a subset of the source corpora that
is topically related to the target corpus does allow
for some domain adaptation.
More recently, Chan and Ng (2007) performed
supervised domain adaptation on a manually se-
lected subset of 21 nouns from the DSO corpus.
They used active learning, count-merging, and
predominant sense estimation in order to save tar-
get annotation effort. They showed that adding
just 30% of the target data to the source exam-
ples the same precision as the full combination of
target and source data could be achieved. They
also showed that using the source corpus allowed
to significantly improve results when only 10%-
30% of the target corpus was used for training.
Unfortunately, no data was given about the target
corpus results, thus failing to show that domain-
adaptation succeeded. In followup work (Zhong et
al., 2008), the feature augmentation approach was
combined with active learning and tested on the
OntoNotes corpus, on a large domain-adaptation
experiment. They reduced significantly the ef-
fort of hand-tagging, but only obtained domain-
adaptation for smaller fractions of the source and
target corpus. Similarly to these works we show
that we can save annotation effort on the target
corpus, but, in contrast, we do get domain adap-
tation when using the full dataset. In a way our
approach is complementary, and we could also ap-
ply active learning to further reduce the number of
target examples to be tagged.
Though not addressing domain adaptation,
other works on WSD also used SVD and are
closely related to the present paper. Ando (2006)
used Alternative Structured Optimization. She
first trained one linear predictor for each target
word, and then performed SVD on 7 carefully se-
lected submatrices of the feature-to-predictor ma-
trix of weights. The system attained small but
consistent improvements (no significance data was
given) on the Senseval-3 lexical sample datasets
using SVD and unlabeled data.
Gliozzo et al (2005) used SVD to reduce the
space of the term-to-document matrix, and then
computed the similarity between train and test
43
instances using a mapping to the reduced space
(similar to our SMA method in Section 4.2). They
combined other knowledge sources into a complex
kernel using SVM. They report improved perfor-
mance on a number of languages in the Senseval-
3 lexical sample dataset. Our present paper dif-
fers from theirs in that we propose an additional
method to use SVD (the OMT method), and that
we focus on domain adaptation.
In the semi-supervised setting, Blitzer et al
(2006) used Structural Correspondence Learning
and unlabeled data to adapt a Part-of-Speech tag-
ger. They carefully select so-called ?pivot fea-
tures? to learn linear predictors, perform SVD on
the weights learned by the predictor, and thus learn
correspondences among features in both source
and target domains. Our technique also uses SVD,
but we directly apply it to all features, and thus
avoid the need to define pivot features. In prelim-
inary work we unsuccessfully tried to carry along
the idea of pivot features to WSD. On the contrary,
in (Agirre and Lopez de Lacalle, 2008) we show
that methods closely related to those presented in
this paper produce positive semi-supervised do-
main adaptation results for WSD.
The methods used in this paper originated in
(Agirre et al, 2005; Agirre and Lopez de Lacalle,
2007), where SVD over a feature-to-documents
matrix improved WSD performance with and
without unlabeled data. The use of several k-
NN classifiers trained on a number of reduced and
original spaces was shown to get the best results
in the Senseval-3 dataset and ranked second in the
SemEval 2007 competition. The present paper ex-
tends this work and applies it to domain adapta-
tion.
3 Data sets
The dataset we use was designed for domain-
relatedWSD experiments by Koeling et al (2005),
and is publicly available. The examples come
from the BNC (Leech, 1992) and the SPORTS and
FINANCES sections of the Reuters corpus (Rose
et al, 2002), comprising around 300 examples
(roughly 100 from each of those corpora) for each
of the 41 nouns. The nouns were selected be-
cause they were salient in either the SPORTS or
FINANCES domains, or because they had senses
linked to those domains. The occurrences were
hand-tagged with the senses from WordNet (WN)
version 1.7.1 (Fellbaum, 1998). In our experi-
ments the BNC examples play the role of general
source corpora, and the FINANCES and SPORTS
examples the role of two specific domain target
corpora.
Compared to the DSO corpus used in prior work
(cf. Section 2) this corpus has been explicitly cre-
ated for domain adaptation studies. DSO con-
tains texts coming from the Brown corpus and the
Wall Street Journal, but the texts are not classi-
fied according to specific domains (e.g. Sports,
Finances), which make DSO less suitable to study
domain adaptation. The fact that the selected
nouns are related to the target domain makes
the (Koeling et al, 2005) corpus more demanding
than the DSO corpus, because one would expect
the performance of a generic WSD system to drop
when moving to the domain corpus for domain-
related words (cf. Table 1), while the performance
would be similar for generic words.
In addition to the labeled data, we also use
unlabeled data coming from the three sources
used in the labeled corpus: the ?written? part
of the BNC (89.7M words), the FINANCES part
of Reuters (32.5M words), and the SPORTS part
(9.1M words).
4 Original and SVD features
In this section, we review the features and two
methods to apply SVD over the features.
4.1 Features
We relied on the usual features used in previous
WSD work, grouped in three main sets. Local
collocations comprise the bigrams and trigrams
formed around the target word (using either lem-
mas, word-forms, or PoS tags) , those formed
with the previous/posterior lemma/word-form in
the sentence, and the content words in a ?4-word
window around the target. Syntactic dependen-
cies use the object, subject, noun-modifier, prepo-
sition, and sibling lemmas, when available. Fi-
nally, Bag-of-words features are the lemmas of
the content words in the whole context, plus the
salient bigrams in the context (Pedersen, 2001).
We refer to these features as original features.
4.2 SVD features
Apart from the original space of features, we have
used the so called SVD features, obtained from
the projection of the feature vectors into the re-
duced space (Deerwester et al, 1990). Basically,
44
we set a term-by-document or feature-by-example
matrix M from the corpus (see section below for
more details). SVD decomposes M into three ma-
trices, M = U?V T . If the desired number of
dimensions in the reduced space is p, we select p
rows from ? and V , yielding ?p and Vp respec-
tively. We can map any feature vector ~t (which
represents either a train or test example) into the
p-dimensional space as follows: ~tp = ~tTVp??1p .
Those mapped vectors have p dimensions, and
each of the dimensions is what we call a SVD fea-
ture. We have explored two different variants in
order to build the reduced matrix and obtain the
SVD features, as follows.
Single Matrix for All target words (SVD-
SMA). The method comprises the following steps:
(i) extract bag-of-word features (terms in this case)
from unlabeled corpora, (ii) build the term-by-
document matrix, (iii) decompose it with SVD, and
(iv) map the labeled data (train/test). This tech-
nique is very similar to previous work on SVD
(Gliozzo et al, 2005; Zelikovitz and Hirsh, 2001).
The dimensionality reduction is performed once,
over the whole unlabeled corpus, and it is then ap-
plied to the labeled data of each word. The re-
duced space is constructed only with terms, which
correspond to bag-of-words features, and thus dis-
cards the rest of the features. Given that the WSD
literature shows that all features are necessary for
optimal performance (Pradhan et al, 2007), we
propose the following alternative to construct the
matrix.
OneMatrix per Target word (SVD-OMT). For
each word: (i) construct a corpus with its occur-
rences in the labeled and, if desired, unlabeled cor-
pora, (ii) extract all features, (iii) build the feature-
by-example matrix, (iv) decompose it with SVD,
and (v) map all the labeled training and test data
for the word. Note that this variant performs one
SVD process for each target word separately, hence
its name.
When building the SVD-OMT matrices we can
use only the training data (TRAIN) or both the train
and unlabeled data (+UNLAB). When building the
SVD-SMA matrices, given the small size of the in-
dividual word matrices, we always use both the
train and unlabeled data (+UNLAB). Regarding the
amount of data, based also on previous work, we
used 50% of the available data for OMT, and the
whole corpora for SMA. An important parameter
when doing SVD is the number of dimensions in
the reduced space (p). We tried two different val-
ues for p (25 and 200) in the BNC domain, and
set a dimension for each classifier/matrix combi-
nation.
4.3 Motivation
The motivation behind our method is that although
the train and test feature vectors overlap suffi-
ciently in the usual WSD task, the domain dif-
ference makes such overlap more scarce. SVD
implicitly finds correlations among features, as it
maps related features into nearby regions in the re-
duced space. In the case of SMA, SVD is applied
over the joint term-by-document matrix of labeled
(and possibly unlabeled corpora), and it thus can
find correlations among closely related words (e.g.
cat and dog). These correlations can help reduce
the gap among bag-of-words features from the
source and target examples. In the case of OMT,
SVD over the joint feature-by-example matrix of
labeled and unlabeled examples of a word allows
to find correlations among features that show sim-
ilar occurrence patterns in the source and target
corpora for the target word.
5 Learning methods
k-NN is a memory based learning method, where
the neighbors are the k most similar labeled exam-
ples to the test example. The similarity among in-
stances is measured by the cosine of their vectors.
The test instance is labeled with the sense obtain-
ing the maximum sum of the weighted vote of the
k most similar contexts. We set k to 5 based on
previous results published in (Agirre and Lopez de
Lacalle, 2007).
Regarding SVM, we used linear kernels, but
also purpose-built kernels for the reduced spaces
and the combinations (cf. Section 5.2). We used
the default soft margin (C=0). In previous ex-
periments we learnt that C is very dependent on
the feature set and training data used. As we
will experiment with different features and train-
ing datasets, it did not make sense to optimize it
across all settings.
We will now detail how we combined the origi-
nal and SVD features in each of the machine learn-
ing methods.
5.1 k-NN combinations
Our k-NN combination method (Agirre et al,
2005; Agirre and Lopez de Lacalle, 2007) takes
45
advantage of the properties of k-NN classifiers and
exploit the fact that a classifier can be seen as
k points (number of nearest neighbor) each cast-
ing one vote. This makes easy to combine sev-
eral classifiers, one for each feature space. For in-
stance, taking two k-NN classifiers of k = 5, C1
andC2, we can combine them into a single k = 10
classifier, where five votes come from C1 and five
from C2. This allows to smoothly combine classi-
fiers from different feature spaces.
In this work we built three single k-NN classi-
fiers trained on OMT, SMA and the original fea-
tures, respectively. In order to combine them we
weight each vote by the inverse ratio of its position
in the rank of the single classifier, (k ? ri + 1)/k,
where ri is the rank.
5.2 Kernel combination
The basic idea of kernel methods is to find a suit-
able mapping function (?) in order to get a better
generalization. Instead of doing this mapping ex-
plicitly, kernels give the chance to do it inside the
algorithm. We will formalize it as follows. First,
we define the mapping function ? : X ? F . Once
the function is defined, we can use it in the kernel
function in order to become an implicit function
K(x, z) = ??(x) ? ?(z)?, where ??? denotes a in-
ner product between vectors in the feature space.
This way, we can very easily define mappings
representing different information sources and use
this mappings in several machine learning algo-
rithm. In our work we use SVM.
We defined three individual kernels (OMT, SMA
and original features) and the combined kernel.
The original feature kernel (KOrig) is given by
the identity function over the features ? : X ? X ,
defining the following kernel:
KOrig(xi,xj) =
?xi ? xj?
?
?xi ? xi? ?xj ? xj?
where the denominator is used to normalize and
avoid any kind of bias in the combination.
The OMT kernel (KOmt) and SMA kernel
(KSma) are defined using OMT and SMA projec-
tion matrices, respectively (cf. Section 4.2). Given
the OMT function mapping ?omt : Rm ? Rp,
where m is the number of the original features
and p the reduced dimensionality, then we define
KOmt(xi,xj) as follows (KSma is defined simi-
larly):
??omt(xi) ? ?omt(xj)?
?
??omt(xi) ? ?omt(xi)? ??omt(xj) ? ?omt(xj)?
BNC ? X SPORTS FINANCES
MFS 39.0 51.2
k-NN 51.7 60.4
SVM 53.9 62.9
Table 1: Source to target results: Train on BNC,
test on SPORTS and FINANCES.
Finally, we define the kernel combination:
KComb(xi,xj) =
n?
l=1
Kl(xi,xj)
?
Kl(xi,xi)Kl(xj,xj)
where n is the number of single kernels explained
above, and l the index for the kernel type.
6 Domain adaptation experiments
In this section we present the results in our two ref-
erence scenarios (source to target, target) and our
reference scenario (domain adaptation). Note that
all methods presented here have full coverage, i.e.
they return a sense for all test examples, and there-
fore precision equals recall, and suffices to com-
pare among systems.
6.1 Source to target scenario: BNC ? X
In this scenario our supervised WSD systems are
trained on the general source corpus (BNC) and
tested on the specific target domains separately
(SPORTS and FINANCES). We do not perform any
kind of adaptation, and therefore the results are
those expected for a generic WSD system when
applied to domain-specific texts.
Table 1 shows the results for k-NN and SVM
trained with the original features on the BNC. In
addition, we also show the results for the Most
Frequent Sense baseline (MFS) taken from the
BNC. The second column denotes the accuracies
obtained when testing on SPORTS, and the third
column the accuracies for FINANCES. The low ac-
curacy obtained with MFS, e.g. 39.0 of precision
in SPORTS, shows the difficulty of this task. Both
classifiers improve over MFS. These classifiers are
weak baselines for the domain adaptation system.
6.2 Target scenario X ? X
In this scenario we lay the harder baseline which
the domain adaptation experiments should im-
prove on (cf. next section). The WSD systems
are trained and tested on each of the target cor-
pora (SPORTS and FINANCES) using 3-fold cross-
validation.
46
SPORTS FINANCES
X ? X TRAIN +UNLAB TRAIN +UNLAB
MFS 77.8 - 82.3 -
k-NN 84.5 - 87.1 -
SVM 85.1 - 87.0 -
k-NN-OMT 85.0 86.1 87.3 87.6
SVM-OMT 82.9 85.1 85.3 86.4
k-NN-SMA - 81.1 - 83.2
SVM-SMA - 81.3 - 84.1
k-NN-COMB 86. 0 86.7 87.9 88.6
SVM-COMB - 86.5 - 88.5
Table 2: Target results: train and test on SPORTS,
train and test on FINANCES, using 3-fold cross-
validation.
Table 2 summarizes the results for this scenario.
TRAIN denotes that only tagged data was used to
train, +UNLAB denotes that we added unlabeled
data related to the source corpus when computing
SVD. The rows denote the classifier and the feature
spaces used, which are organized in four sections.
On the top rows we show the three baseline clas-
sifiers on the original features. The two sections
below show the results of those classifiers on the
reduced dimensions, OMT and SMA (cf. Section
4.2). Finally, the last rows show the results of the
combination strategies (cf. Sections 5.1 and 5.2).
Note that some of the cells have no result, because
that combination is not applicable (e.g. using the
train and unlabeled data in the original space).
First of all note that the results for the base-
lines (MFS, SVM, k-NN) are much larger than
those in Table 1, showing that this dataset is spe-
cially demanding for supervised WSD, and partic-
ularly difficult for domain adaptation experiments.
These results seem to indicate that the examples
from the source general corpus could be of little
use when tagging the target corpora. Note spe-
cially the difference in MFS performance. The pri-
ors of the senses are very different in the source
and target corpora, which is a well-known short-
coming for supervised systems. Note the high re-
sults of the baseline classifiers, which leave small
room for improvement.
The results for the more sophisticated methods
show that SVD and unlabeled data helps slightly,
except for k-NN-OMT on SPORTS. SMA de-
creases the performance compared to the classi-
fiers trained on original features. The best im-
provements come when the three strategies are
combined in one, as both the kernel and k-NN
combinations obtain improvements over the re-
spective single classifiers. Note that both the k-NN
BNC + X SPORTS FINANCES
? X TRAIN + UNLAB TRAIN + UNLAB
BNC ? X 53.9 - 62.9 -
X ? X 86.0 86.7 87.9 88.5
MFS 68.2 - 73.1 -
k-NN 81.3 - 86.0 -
SVM 84.7 - 87.5 -
k-NN-OMT 84.0 84.7 87.5 86.0
SVM-OMT 85.1 84.7 84.2 85.5
k-NN-SMA - 77.1 - 81.6
SVM-SMA - 78.1 - 80.7
k-NN-COMB 84.5 87.2 88.1 88.7
SVM-COMB - 88.4 - 89.7
SVM-AUG 85.9 - 88.1 -
Table 3: Domain adaptation results: Train on
BNC and SPORTS, test on SPORTS (same for FI-
NANCES).
and SVM combinations perform similarly.
In the combination strategy we show that unla-
beled data helps slightly, because instead of only
combining OMT and original features we have the
opportunity to introduce SMA. Note that it was not
our aim to improve the results of the basic classi-
fiers on this scenario, but given the fact that we are
going to apply all these techniques in the domain
adaptation scenario, we need to show these results
as baselines. That is, in the next section we will try
to obtain results which improve significantly over
the best results in this section.
6.3 Domain adaptation scenario
BNC + X ? X
In this last scenario we try to show that our WSD
system trained on both source (BNC) and tar-
get (SPORTS and FINANCES) data performs better
than the one trained on the target data alone. We
also use 3-fold cross-validation for the target data,
but the entire source data is used in each turn. The
unlabeled data here refers to the combination of
unlabeled source and target data.
The results are presented in table 3. Again, the
columns denote if unlabeled data has been used in
the learning process. The rows correspond to clas-
sifiers and the feature spaces involved. The first
rows report the best results in the previous scenar-
ios: BNC ? X for the source to target scenario,
and X ? X for the target scenario. The rest
of the table corresponds to the domain adaptation
scenario. The rows below correspond to MFS and
the baseline classifiers, followed by the OMT and
SMA results, and the combination results. The last
row shows the results for the feature augmentation
algorithm (Daume? III, 2007).
47
SPORTS FINANCES
BNC ? X
MFS 39.0 51.2
SVM 53.9 62.9
X ? X
MFS 77.8 82.3
SVM 85.1 87.0
k-NN-COMB (+UNLAB) 86.7 88.6
BNC +X ? X
MFS 68.2 73.1
SVM 84.7 87.5
SVM-AUG 85.9 88.1
SVM-COMB (+UNLAB) 88.4 89.7
Table 4: The most important results in each sce-
nario.
Focusing on the results, the table shows that
MFS decreases with respect to the target scenario
(cf. Table 2) when the source data is added, prob-
ably caused by the different sense distributions in
BNC and the target corpora. The baseline classi-
fiers (k-NN and SVM) are not able to improve over
the baseline classifiers on the target data alone,
which is coherent with past research, and shows
that straightforward domain adaptation does not
work.
The following rows show that our reduction
methods on themselves (OMT, SMA used by k-
NN and SVM) also fail to perform better than in
the target scenario, but the combinations using
unlabeled data (k-NN-COMB and specially SVM-
COMB) do manage to improve the best results for
the target scenario, showing that we were able to
attain domain adaptation. The feature augmenta-
tion approach (SVM-AUG) does improve slightly
over SVM in the target scenario, but not over the
best results in the target scenario, showing the dif-
ficulty of domain adaptation for WSD, at least on
this dataset.
7 Discussion and analysis
Table 4 summarizes the most important results.
The kernel combination method with unlabeled
data on the adaptation scenario reduces the error
on 22.1% and 17.6% over the baseline SVM on
the target scenario (SPORTS and FINANCES re-
spectively), and 12.7% and 9.0% over the k-NN
combination method on the target scenario. These
gains are remarkable given the already high base-
line, specially taking into consideration that the
41 nouns are closely related to the domains. The
differences, including SVM-AUG, are statistically
significant according to the Wilcoxon test with
%25 %32 %50 %62 %75 %82 %100sports (%)
80
82
84
86
88
accu
racy 
(%)
SVM-COMB (+UNLAB, BNC + SPORTS -> SPORTS)SVM-AUG (BNC + SPORTS -> SPORTS)SVM-ORIG (SPORTS -> SPORTS)y=85.1
Figure 1: Learning curves for SPORTS. The X
axis denotes the amount of SPORTS data and the
Y axis corresponds to accuracy.
%25 %32 %50 %62 %75 %82 %100finances (%)
84
86
88
90
accu
racy 
(%)
SVM-COMB (+UNLAB, BNC + FIN. -> FIN.)SVM-AUG (BNC + FIN. -> FIN.)SVM-ORIG (FIN. -> FIN.)y=87.0
Figure 2: Learning curves for FINANCES. The X
axis denotes the amount of FINANCES data and Y
axis corresponds to the accuracy.
p < 0.01.
In addition, we carried extra experiments to ex-
amine the learning curves, and to check, given
the source examples, how many additional ex-
amples from the target corpus are needed to ob-
tain the same results as in the target scenario us-
ing all available examples. We fixed the source
data and used increasing amounts of target data.
We show the original SVM on the target scenario,
and SVM-COMB (+UNLAB) and SVM-AUG as the
domain adaptation approaches. The results are
shown in figure 1 for SPORTS and figure 2 for FI-
NANCES. The horizontal line corresponds to the
performance of SVM on the target domain. The
point where the learning curves cross the horizon-
tal line show that our domain adaptation method
needs only around 40% of the target data in order
to get the same performance as the baseline SVM
on the target data. The learning curves also shows
48
that the domain adaptation kernel combination ap-
proach, no matter the amount of target data, is al-
ways above the rest of the classifiers, showing the
robustness of our approach.
8 Conclusion and future work
In this paper we explore supervised domain adap-
tation for WSD with positive results, that is,
whether hand-labeling general domain (source)
text is worth the effort when training WSD sys-
tems that are to be applied to specific domains (tar-
gets). We performed several experiments in three
scenarios. In the first scenario (source to target
scenario), the classifiers were trained on source
domain data (the BNC) and tested on the target do-
mains, composed by the SPORTS and FINANCES
sections of Reuters. In the second scenario (tar-
get scenario) we set the main baseline for our do-
main adaptation experiment, training and testing
our classifiers on the target domain data. In the last
scenario (domain adaptation scenario), we com-
bine both source and target data for training, and
test on the target data.
We report results in each scenario for k-NN and
SVM classifiers, for reduced features obtained us-
ing SVD over the training data, for the use of un-
labeled data, and for k-NN and SVM combinations
of all.
Our results show that our best domain adap-
tation strategy (using kernel combination of SVD
features and unlabeled data related to the training
data) yields statistically significant improvements:
up to 22% error reduction compared to SVM on
the target domain data alone. We also show that
our domain adaptation method only needs 40% of
the target data (in addition to the source data) in
order to get the same results as SVM on the target
alone.
We obtain coherent results in two target scenar-
ios, and consistent improvement at all levels of
the learning curves, showing the robustness or our
findings. We think that our dataset, which com-
prises examples for 41 nouns that are closely re-
lated to the target domains, is specially demand-
ing, as one would expect the performance of a
generic WSD system to drop when moving to
the domain corpus, specially on domain-related
words, while we could expect the performance to
be similar for generic or unrelated words.
In the future we would like to evaluate
our method on other datasets (e.g. DSO or
OntoNotes), to test whether the positive results are
confirmed. We would also like to study word-by-
word behaviour, in order to assess whether target
examples are really necessary for words which are
less related to the domain.
Acknowledgments
This work has been partially funded by the EU Commission
(project KYOTO ICT-2007-211423) and Spanish Research
Department (project KNOW TIN2006-15049-C03-01). Oier
Lopez de Lacalle has a PhD grant from the Basque Govern-
ment.
References
Eneko Agirre and Oier Lopez de Lacalle. 2007. Ubc-
alm: Combining k-nn with svd for wsd. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 342?
345, Prague, Czech Republic, June. Association for
Computational Linguistics.
Eneko Agirre and Oier Lopez de Lacalle. 2008. On
robustness and domain adaptation using SVD for
word sense disambiguation. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 17?24, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Eneko Agirre and David Mart??nez. 2004. The effect
of bias on an automatically-built word sense corpus.
Proceedings of the 4rd International Conference on
Languages Resources and Evaluations (LREC).
E. Agirre, O.Lopez de Lacalle, and David Mart??nez.
2005. Exploring feature spaces with svd and un-
labeled data for Word Sense Disambiguation. In
Proceedings of the Conference on Recent Advances
on Natural Language Processing (RANLP?05),
Borovets, Bulgaria.
Rie Kubota Ando. 2006. Applying alternating struc-
ture optimization to word sense disambiguation. In
Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL), pages
77?84, New York City.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120?128, Sydney, Australia, July.
Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2007. Do-
main adaptation with active learning for word sense
disambiguation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 49?56, Prague, Czech Republic,
June. Association for Computational Linguistics.
49
Ciprian Chelba and Alex Acero. 2004. Adaptation
of maximum entropy classifier: Little data can help
a lot. In Proceedings of of th Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Barcelona, Spain.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Scott Deerwester, Susan Dumais, Goerge Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal
of the American Society for Information Science,
41(6):391?407.
Gerard Escudero, Lluiz Ma?rquez, and German Rigau.
2000. An Empirical Study of the Domain Depen-
dence of Supervised Word Sense Didanbiguation
Systems. Proceedings of the joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora, EMNLP/VLC.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and
Carlo Strapparava. 2005. Domain Kernels for Word
Sense Disambiguation. 43nd Annual Meeting of the
Association for Computational Linguistics. (ACL-
05).
R. Koeling, D. McCarthy, and J. Carroll. 2005.
Domain-specific sense distributions and predomi-
nant sense acquisition. In Proceedings of the Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing. HLT/EMNLP, pages 419?426, Ann Ar-
bor, Michigan.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
David Mart??nez and Eneko Agirre. 2000. One Sense
per Collocation and Genre/Topic Variations. Con-
ference on Empirical Method in Natural Language.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), Pittsburgh, PA.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, srl and all words. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 87?92,
Prague, Czech Republic.
Tony G. Rose, Mark Stevenson, and Miles Whitehead.
2002. The reuters corpus volumen 1 from yester-
day?s news to tomorrow?s language resources. In
Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC-
2002), pages 827?832, Las Palmas, Canary Islands.
Sarah Zelikovitz and Haym Hirsh. 2001. Using LSI
for text classification in the presence of background
text. In Henrique Paques, Ling Liu, and David
Grossman, editors, Proceedings of CIKM-01, 10th
ACM International Conference on Information and
Knowledge Management, pages 113?118, Atlanta,
US. ACM Press, New York, US.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An
empirical study. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1002?1010, Honolulu, Hawaii,
October. Association for Computational Linguistics.
50
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Study on Similarity and Relatedness
Using Distributional and WordNet-based Approaches
Eneko Agirre? Enrique Alfonseca? Keith Hall? Jana Kravalova?? Marius Pas?ca? Aitor Soroa?
? IXA NLP Group, University of the Basque Country
? Google Inc.
? Institute of Formal and Applied Linguistics, Charles University in Prague
{e.agirre,a.soroa}@ehu.es {ealfonseca,kbhall,mars}@google.com
kravalova@ufal.mff.cuni.cz
Abstract
This paper presents and compares WordNet-
based and distributional similarity approaches.
The strengths and weaknesses of each ap-
proach regarding similarity and relatedness
tasks are discussed, and a combination is pre-
sented. Each of our methods independently
provide the best results in their class on the
RG and WordSim353 datasets, and a super-
vised combination of them yields the best pub-
lished results on all datasets. Finally, we pio-
neer cross-lingual similarity, showing that our
methods are easily adapted for a cross-lingual
task with minor losses.
1 Introduction
Measuring semantic similarity and relatedness be-
tween terms is an important problem in lexical se-
mantics. It has applications in many natural lan-
guage processing tasks, such as Textual Entailment,
Word Sense Disambiguation or Information Extrac-
tion, and other related areas like Information Re-
trieval. The techniques used to solve this problem
can be roughly classified into two main categories:
those relying on pre-existing knowledge resources
(thesauri, semantic networks, taxonomies or ency-
clopedias) (Alvarez and Lim, 2007; Yang and Pow-
ers, 2005; Hughes and Ramage, 2007) and those in-
ducing distributional properties of words from cor-
pora (Sahami and Heilman, 2006; Chen et al, 2006;
Bollegala et al, 2007).
In this paper, we explore both families. For the
first one we apply graph based algorithms to Word-
Net, and for the second we induce distributional
similarities collected from a 1.6 Terabyte Web cor-
pus. Previous work suggests that distributional sim-
ilarities suffer from certain limitations, which make
them less useful than knowledge resources for se-
mantic similarity. For example, Lin (1998b) finds
similar phrases like captive-westerner which made
sense only in the context of the corpus used, and
Budanitsky and Hirst (2006) highlight other prob-
lems that stem from the imbalance and sparseness of
the corpora. Comparatively, the experiments in this
paper demonstrate that distributional similarities can
perform as well as the knowledge-based approaches,
and a combination of the two can exceed the per-
formance of results previously reported on the same
datasets. An application to cross-lingual (CL) sim-
ilarity identification is also described, with applica-
tions such as CL Information Retrieval or CL spon-
sored search. A discussion on the differences be-
tween learning similarity and relatedness scores is
provided.
The paper is structured as follows. We first
present the WordNet-based method, followed by the
distributional methods. Section 4 is devoted to the
evaluation and results on the monolingual and cross-
lingual tasks. Section 5 presents some analysis, in-
cluding learning curves for distributional methods,
the use of distributional similarity to improve Word-
Net similarity, the contrast between similarity and
relatedness, and the combination of methods. Sec-
tion 6 presents related work, and finally, Section 7
draws the conclusions and mentions future work.
2 WordNet-based method
WordNet (Fellbaum, 1998) is a lexical database of
English, which groups nouns, verbs, adjectives and
adverbs into sets of synonyms (synsets), each ex-
pressing a distinct concept. Synsets are interlinked
with conceptual-semantic and lexical relations, in-
cluding hypernymy, meronymy, causality, etc.
Given a pair of words and a graph-based repre-
sentation of WordNet, our method has basically two
19
steps: We first compute the personalized PageR-
ank over WordNet separately for each of the words,
producing a probability distribution over WordNet
synsets. We then compare how similar these two dis-
crete probability distributions are by encoding them
as vectors and computing the cosine between the
vectors.
We represent WordNet as a graph G = (V,E) as
follows: graph nodes represent WordNet concepts
(synsets) and dictionary words; relations among
synsets are represented by undirected edges; and
dictionary words are linked to the synsets associated
to them by directed edges.
For each word in the pair we first compute a per-
sonalized PageRank vector of graph G (Haveliwala,
2002). Basically, personalized PageRank is com-
puted by modifying the random jump distribution
vector in the traditional PageRank equation. In our
case, we concentrate all probability mass in the tar-
get word.
Regarding PageRank implementation details, we
chose a damping value of 0.85 and finish the calcula-
tion after 30 iterations. These are default values, and
we did not optimize them. Our similarity method is
similar, but simpler, to that used by (Hughes and Ra-
mage, 2007), which report very good results on sim-
ilarity datasets. More details of our algorithm can be
found in (Agirre and Soroa, 2009). The algorithm
and needed resouces are publicly available1.
2.1 WordNet relations and versions
The WordNet versions that we use in this work are
the Multilingual Central Repository or MCR (At-
serias et al, 2004) (which includes English Word-
Net version 1.6 and wordnets for several other lan-
guages like Spanish, Italian, Catalan and Basque),
and WordNet version 3.02. We used all the rela-
tions in MCR (except cooccurrence relations and se-
lectional preference relations) and in WordNet 3.0.
Given the recent availability of the disambiguated
gloss relations for WordNet 3.03, we also used a
version which incorporates these relations. We will
refer to the three versions as MCR16, WN30 and
WN30g, respectively. Our choice was mainly moti-
vated by the fact that MCR contains tightly aligned
1http://http://ixa2.si.ehu.es/ukb/
2Available from http://http://wordnet.princeton.edu/
3http://wordnet.princeton.edu/glosstag
wordnets of several languages (see below).
2.2 Cross-linguality
MCR follows the EuroWordNet design (Vossen,
1998), which specifies an InterLingual Index (ILI)
that links the concepts across wordnets of differ-
ent languages. The wordnets for other languages in
MCR use the English WordNet synset numbers as
ILIs. This design allows a decoupling of the rela-
tions between concepts (which can be taken to be
language independent) and the links from each con-
tent word to its corresponding concepts (which is
language dependent).
As our WordNet-based method uses the graph of
the concepts and relations, we can easily compute
the similarity between words from different lan-
guages. For example, consider a English-Spanish
pair like car ? coche. Given that the Spanish Word-
Net is included in MCR we can use MCR as the
common knowledge-base for the relations. We can
then compute the personalized PageRank for each
of car and coche on the same underlying graph, and
then compare the similarity between both probabil-
ity distributions.
As an alternative, we also tried to use pub-
licly available mappings for wordnets (Daude et al,
2000)4 in order to create a 3.0 version of the Span-
ish WordNet. The mapping was used to link Spanish
variants to 3.0 synsets. We used the English Word-
Net 3.0, including glosses, to construct the graph.
The two Spanish WordNet versions are referred to
as MCR16 and WN30g.
3 Context-based methods
In this section, we describe the distributional meth-
ods used for calculating similarities between words,
and profiting from the use of a large Web-based cor-
pus.
This work is motivated by previous studies that
make use of search engines in order to collect co-
occurrence statistics between words. Turney (2001)
uses the number of hits returned by a Web search
engine to calculate the Pointwise Mutual Informa-
tion (PMI) between terms, as an indicator of syn-
onymy. Bollegala et al (2007) calculate a number
of popular relatedness metrics based on page counts,
4http://www.lsi.upc.es/?nlp/tools/download-map.php.
20
like PMI, the Jaccard coefficient, the Simpson co-
efficient and the Dice coefficient, which are com-
bined with lexico-syntactic patterns as model fea-
tures. The model parameters are trained using Sup-
port Vector Machines (SVM) in order to later rank
pairs of words. A different approach is the one taken
by Sahami and Heilman (2006), who collect snip-
pets from the results of a search engine and repre-
sent each snippet as a vector, weighted with the tf?idf
score. The semantic similarity between two queries
is calculated as the inner product between the cen-
troids of the respective sets of vectors.
To calculate the similarity of two words w1 and
w2, Ruiz-Casado et al (2005) collect snippets con-
taining w1 from a Web search engine, extract a con-
text around it, replace it with w2 and check for the
existence of that modified context in the Web.
Using a search engine to calculate similarities be-
tween words has the drawback that the data used will
always be truncated. So, for example, the numbers
of hits returned by search engines nowadays are al-
ways approximate and rounded up. The systems that
rely on collecting snippets are also limited by the
maximum number of documents returned per query,
typically around a thousand. We hypothesize that
by crawling a large corpus from the Web and doing
standard corpus analysis to collect precise statistics
for the terms we should improve over other unsu-
pervised systems that are based on search engine
results, and should yield results that are competi-
tive even when compared to knowledge-based ap-
proaches.
In order to calculate the semantic similarity be-
tween the words in a set, we have used a vector space
model, with the following three variations:
In the bag-of-words approach, for each word w
in the dataset we collect every term t that appears in
a window centered in w, and add them to the vector
together with its frequency.
In the context window approach, for each word
w in the dataset we collect every window W cen-
tered in w (removing the central word), and add it
to the vector together with its frequency (the total
number of times we saw windowW around w in the
whole corpus). In this case, all punctuation symbols
are replaced with a special token, to unify patterns
like , the <term> said to and ? the <term> said to.
Throughout the paper, when we mention a context
window of size N it means N words at each side of
the phrase of interest.
In the syntactic dependency approach, we parse
the entire corpus using an implementation of an In-
ductive Dependency parser as described in Nivre
(2006). For each word w we collect a template of
the syntactic context. We consider sequences of gov-
erning words (e.g. the parent, grand-parent, etc.) as
well as collections of descendants (e.g., immediate
children, grandchildren, etc.). This information is
then encoded as a contextual template. For example,
the context template cooks <term> delicious could
be contexts for nouns such as food, meals, pasta, etc.
This captures both syntactic preferences as well as
selectional preferences. Contrary to Pado and Lap-
ata (2007), we do not use the labels of the syntactic
dependencies.
Once the vectors have been obtained, the fre-
quency for each dimension in every vector is
weighted using the other vectors as contrast set, with
the ?2 test, and finally the cosine similarity between
vectors is used to calculate the similarity between
each pair of terms.
Except for the syntactic dependency approach,
where closed-class words are needed by the parser,
in the other cases we have removed stopwords (pro-
nouns, prepositions, determiners and modal and
auxiliary verbs).
3.1 Corpus used
We have used a corpus of four billion documents,
crawled from the Web in August 2008. An HTML
parser is used to extract text, the language of each
document is identified, and non-English documents
are discarded. The final corpus remaining at the end
of this process contains roughly 1.6 Terawords. All
calculations are done in parallel sharding by dimen-
sion, and it is possible to calculate all pairwise sim-
ilarities of the words in the test sets very quickly
on this corpus using the MapReduce infrastructure.
A complete run takes around 15 minutes on 2,000
cores.
3.2 Cross-linguality
In order to calculate similarities in a cross-lingual
setting, where some of the words are in a language l
other than English, the following algorithm is used:
21
Method Window size RG dataset WordSim353 dataset
MCR16 0.83 [0.73, 0.89] 0.53 (0.56) [0.45, 0.60]
WN30 0.79 [0.67, 0.86] 0.56 (0.58) [0.48, 0.63]
WN30g 0.83 [0.73, 0.89] 0.66 (0.69) [0.59, 0.71]
CW 1 0.83 [0.73, 0.89] 0.63 [0.57, 0.69]
2 0.83 [0.74, 0.90] 0.60 [0.53, 0.66]
3 0.85 [0.76, 0.91] 0.59 [0.52, 0.65]
4 0.89 [0.82, 0.93] 0.60 [0.53, 0.66]
5 0.80 [0.70, 0.88] 0.58 [0.51, 0.65]
6 0.75 [0.62, 0.84] 0.58 [0.50, 0.64]
7 0.72 [0.58, 0.82] 0.57 [0.49, 0.63]
BoW 1 0.81 [0.70, 0.88] 0.64 [0.57, 0.70]
2 0.80 [0.69, 0.87] 0.64 [0.58, 0.70]
3 0.79 [0.67, 0.86] 0.64 [0.58, 0.70]
4 0.78 [0.66, 0.86] 0.65 [0.58, 0.70]
5 0.77 [0.64, 0.85] 0.64 [0.58, 0.70]
6 0.76 [0.63, 0.85] 0.65 [0.58, 0.70]
7 0.75 [0.62, 0.84] 0.64 [0.58, 0.70]
Syn G1,D0 0.81 [0.70, 0.88] 0.62 [0.55, 0.68]
G2,D0 0.82 [0.72, 0.89] 0.55 [0.48, 0.62]
G3,D0 0.81 [0.71, 0.88] 0.62 [0.56, 0.68]
G1,D1 0.82 [0.72, 0.89] 0.62 [0.55, 0.68]
G2,D1 0.82 [0.73, 0.89] 0.62 [0.55, 0.68]
G3,D1 0.82 [0.72, 0.88] 0.62 [0.55, 0.68]
CW+ 4; G1,D0 0.88 [0.81, 0.93] 0.66 [0.59, 0.71]
Syn 4; G2,D0 0.87 [0.80, 0.92] 0.64 [0.57, 0.70]
4; G3,D0 0.86 [0.77, 0.91] 0.63 [0.56, 0.69]
4; G1,D1 0.83 [0.73, 0.89] 0.48 [0.40, 0.56]
4; G2,D1 0.83 [0.73, 0.89] 0.49 [0.40, 0.56]
4; G3,D1 0.82 [0.72, 0.89] 0.48 [0.40, 0.56]
Table 1: Spearman correlation results for the various WordNet-based
models and distributional models. CW=Context Windows, BoW=bag
of words, Syn=syntactic vectors. For Syn, the window size is actually
the tree-depth for the governors and descendants. For examples, G1
indicates that the contexts include the parents and D2 indicates that both
the children and grandchildren make up the contexts. The final grouping
includes both contextual windows (at width 4) and syntactic contexts in
the template vectors. Max scores are bolded.
1. Replace each non-English word in the dataset
with its 5-best translations into English using
state-of-the-art machine translation technology.
2. The vector corresponding to each Spanish word
is calculated by collecting features from all the
contexts of any of its translations.
3. Once the vectors are generated, the similarities
are calculated in the same way as before.
4 Experimental results
4.1 Gold-standard datasets
We have used two standard datasets. The first
one, RG, consists of 65 pairs of words collected by
Rubenstein and Goodenough (1965), who had them
judged by 51 human subjects in a scale from 0.0 to
4.0 according to their similarity, but ignoring any
other possible semantic relationships that might ap-
pear between the terms. The second dataset, Word-
Sim3535 (Finkelstein et al, 2002) contains 353 word
pairs, each associated with an average of 13 to 16 hu-
man judgements. In this case, both similarity and re-
5Available at http://www.cs.technion.ac.il/
?gabr/resources/data/wordsim353/wordsim353.html
Context RG terms and frequencies
ll never forget the * on his face when grin,2,smile,10
he had a giant * on his face and grin,3,smile,2
room with a huge * on her face and grin,2,smile,6
the state of every * will be updated every automobile,2,car,3
repair or replace the * if it is stolen automobile,2,car,2
located on the north * of the Bay of shore,14,coast,2
areas on the eastern * of the Adriatic Sea shore,3,coast,2
Thesaurus of Current English * The Oxford Pocket Thesaurus slave,3,boy,5,shore,3,string,2
wizard,4,glass,4,crane,5,smile,5
implement,5,oracle,2,lad,2
food,3,car,2,madhouse,3,jewel,3
asylum,4,tool,8,journey,6,etc.
be understood that the * 10 may be designed crane,3,tool,3
a fight between a * and a snake and bird,3,crane,5
Table 2: Sample of context windows for the terms in the RG dataset.
latedness are annotated without any distinction. Sev-
eral studies indicate that the human scores consis-
tently have very high correlations with each other
(Miller and Charles, 1991; Resnik, 1995), thus val-
idating the use of these datasets for evaluating se-
mantic similarity.
For the cross-lingual evaluation, the two datasets
were modified by translating the second word in
each pair into Spanish. Two humans translated
simultaneously both datasets, with an inter-tagger
agreement of 72% for RG and 84% for Word-
Sim353.
4.2 Results
Table 1 shows the Spearman correlation obtained on
the RG and WordSim353 datasets, including the in-
terval at 0.95 of confidence6.
Overall the distributional context-window ap-
proach performs best in the RG, reaching 0.89 corre-
lation, and both WN30g and the combination of con-
text windows and syntactic context perform best on
WordSim353. Note that the confidence intervals are
quite large in both RG and WordSim353, and few of
the pairwise differences are statistically significant.
Regarding WordNet-based approaches, the use of
the glosses and WordNet 3.0 (WN30g) yields the
best results in both datasets. While MCR16 is close
to WN30g for the RG dataset, it lags well behind
on WordSim353. This discrepancy is further ana-
lyzed is Section 5.3. Note that the performance of
WordNet in the WordSim353 dataset suffers from
unknown words. In fact, there are nine pairs which
returned null similarity for this reason. The num-
6To calculate the Spearman correlations values are trans-
formed into ranks, and we calculate the Pearson correlation on
them. The confidence intervals refer to the Pearson correlations
of the rank vectors.
22
Figure 1: Effect of the size of the training corpus, for the best distributional similarity model in each dataset. Left: WordSim353 with bag-of-words,
Right: RG with context windows.
Dataset Method overall ? interval
RG MCR16 0.78 -0.05 [0.66, 0.86]
WN30g 0.74 -0.09 [0.61, 0.84]
Bag of words 0.68 -0.23 [0.53, 0.79]
Context windows 0.83 -0.05 [0.73, 0.89]
WS353 MCR16 0.42 (0.53) -0.11 (-0.03) [0.34, 0.51]
WN30g 0.58 (0.67) -0.07 (-0.02) [0.51, 0.64]
Bag of words 0.53 -0.12 [0.45, 0.61]
Context windows 0.52 -0.11 [0.44, 0.59]
Table 3: Results obtained by the different methods on the Span-
ish/English cross-lingual datasets. The ? column shows the perfor-
mance difference with respect to the results on the original dataset.
ber in parenthesis in Table 1 for WordSim353 shows
the results for the 344 remaining pairs. Section 5.2
shows a proposal to overcome this limitation.
The bag-of-words approach tends to group to-
gether terms that can have a similar distribution of
contextual terms. Therefore, terms that are topically
related can appear in the same textual passages and
will get high values using this model. We see this
as an explanation why this model performed better
than the context window approach for WordSim353,
where annotators were instructed to provide high
ratings to related terms. On the contrary, the con-
text window approach tends to group together words
that are exchangeable in exactly the same context,
preserving order. Table 2 illustrates a few exam-
ples of context collected. Therefore, true synonyms
and hyponyms/hyperonyms will receive high simi-
larities, whereas terms related topically or based on
any other semantic relation (e.g. movie and star) will
have lower scores. This explains why this method
performed better for the RG dataset. Section 5.3
confirms these observations.
4.3 Cross-lingual similarity
Table 3 shows the results for the English-Spanish
cross-lingual datasets. For RG, MCR16 and the
context windows methods drop only 5 percentage
points, showing that cross-lingual similarity is feasi-
ble, and that both cross-lingual strategies are robust.
The results for WordSim353 show that WN30g is
the best for this dataset, with the rest of the meth-
ods falling over 10 percentage points relative to the
monolingual experiment. A closer look at the Word-
Net results showed that most of the drop in perfor-
mance was caused by out-of-vocabulary words, due
to the smaller vocabulary of the Spanish WordNet.
Though not totally comparable, if we compute the
correlation over pairs covered in WordNet alne, the
correlation would drop only 2 percentage points. In
the case of the distributional approaches, the fall in
performance was caused by the translations, as only
61% of the words were translated into the original
word in the English datasets.
5 Detailed analysis and system
combination
In this section we present some analysis, including
learning curves for distributional methods, the use
of distributional similarity to improve WordNet sim-
ilarity, the contrast between similarity and related-
ness, and the combination of methods.
5.1 Learning curves for distributional methods
Figure 1 shows that the correlation improves with
the size of the corpus, as expected. For the re-
sults using the WordSim353 corpus, we show the
results of the bag-of-words approach with context
size 10. Results improve from 0.5 Spearman correla-
tion up to 0.65 when increasing the corpus size three
orders of magnitude, although the effect decays at
the end, which indicates that we might not get fur-
23
Method Without similar words With similar words
WN30 0.56 (0.58) [0.48, 0.63] 0.58 [0.51, 0.65]
WN30g 0.66 (0.69) [0.59, 0.71] 0.68 [0.62, 0.73]
Table 4: Results obtained replacing unknown words with their most
similar three words (WordSim353 dataset).
Method overall Similarity Relatedness
MCR16 0.53 [0.45, 0.60] 0.65 [0.56, 0.72] 0.33 [0.21, 0.43]
WN30 0.56 [0.48, 0.63] 0.73 [0.65, 0.79] 0.38 [0.27, 0.48]
WN30g 0.66 [0.59, 0.71] 0.72 [0.64, 0.78] 0.56 [0.46, 0.64]
BoW 0.65 [0.59, 0.71] 0.70 [0.63, 0.77] 0.62 [0.53, 0.69]
CW 0.60 [0.53, 0.66] 0.77 [0.71, 0.82] 0.46 [0.36, 0.55]
Table 5: Results obtained on the WordSim353 dataset and on the two
similarity and relatedness subsets.
ther gains going beyond the current size of the cor-
pus. With respect to results for the RG dataset, we
used a context-window approach with context radius
4. Here, results improve even more with data size,
probably due to the sparse data problem collecting
8-word context windows if the corpus is not large
enough. Correlation improves linearly right to the
end, where results stabilize around 0.89.
5.2 Combining both approaches: dealing with
unknown words in WordNet
Although the vocabulary of WordNet is very ex-
tensive, applications are bound to need the similar-
ity between words which are not included in Word-
Net. This is exemplified in the WordSim353 dataset,
where 9 pairs contain words which are unknown to
WordNet. In order to overcome this shortcoming,
we could use similar words instead, as provided by
the distributional thesaurus. We used the distribu-
tional thesaurus defined in Section 3, using context
windows of width 4, to provide three similar words
for each of the unknown words in WordNet. Results
improve for both WN30 and WN30g, as shown in
Table 4, attaining our best results for WordSim353.
5.3 Similarity vs. relatedness
We mentioned above that the annotation guidelines
of WordSim353 did not distinguish between simi-
lar and related pairs. As the results in Section 4
show, different techniques are more appropriate to
calculate either similarity or relatedness. In order to
study this effect, ideally, we would have two ver-
sions of the dataset, where annotators were given
precise instructions to distinguish similarity in one
case, and relatedness in the other. Given the lack
of such datasets, we devised a simpler approach in
order to reuse the existing human judgements. We
manually split the dataset in two parts, as follows.
First, two humans classified all pairs as be-
ing synonyms of each other, antonyms, iden-
tical, hyperonym-hyponym, hyponym-hyperonym,
holonym-meronym, meronym-holonym, and none-
of-the-above. The inter-tagger agreement rate was
0.80, with a Kappa score of 0.77. This anno-
tation was used to group the pairs in three cate-
gories: similar pairs (those classified as synonyms,
antonyms, identical, or hyponym-hyperonym), re-
lated pairs (those classified as meronym-holonym,
and pairs classified as none-of-the-above, with a hu-
man average similarity greater than 5), and unrelated
pairs (those classified as none-of-the-above that had
average similarity less than or equal to 5). We then
created two new gold-standard datasets: similarity
(the union of similar and unrelated pairs), and relat-
edness (the union of related and unrelated)7.
Table 5 shows the results on the relatedness and
similarity subsets of WordSim353 for the different
methods. Regarding WordNet methods, both WN30
and WN30g perform similarly on the similarity sub-
set, but WN30g obtains the best results by far on
the relatedness data. These results are congruent
with our expectations: two words are similar if their
synsets are in close places in the WordNet hierarchy,
and two words are related if there is a connection
between them. Most of the relations in WordNet
are of hierarchical nature, and although other rela-
tions exist, they are far less numerous, thus explain-
ing the good results for both WN30 and WN30g on
similarity, but the bad results of WN30 on related-
ness. The disambiguated glosses help find connec-
tions among related concepts, and allow our method
to better model relatedness with respect to WN30.
The low results for MCR16 also deserve some
comments. Given the fact that MCR16 performed
very well on the RG dataset, it comes as a surprise
that it performs so poorly for the similarity subset
of WordSim353. In an additional evaluation, we at-
tested that MCR16 does indeed perform as well as
MCR30g on the similar pairs subset. We believe
that this deviation could be due to the method used to
construct the similarity dataset, which includes some
pairs of loosely related pairs labeled as unrelated.
7Available at http://alfonseca.org/eng/research/wordsim353.html
24
Methods combined in the SVM RG dataset WordSim353 dataset WordSim353 similarity WordSim353 relatedness
WN30g, bag of words 0.88 [0.82, 0.93] 0.78 [0.73, 0.81] 0.81 [0.76, 0.86] 0.72 [0.65, 0.77]
WN30g, context windows 0.90 [0.84, 0.94] 0.73 [0.68, 0.79] 0.83 [0.78, 0.87] 0.64 [0.56, 0.71]
WN30g, syntax 0.89 [0.83, 0.93] 0.75 [0.70, 0.79] 0.83 [0.78, 0.87] 0.67 [0.60, 0.74]
WN30g, bag of words, context windows, syntax 0.96 [0.93, 0.97] 0.78 [0.73, 0.82] 0.83 [0.78, 0.87] 0.71 [0.65, 0.77]
Table 6: Results using a supervised combination of several systems. Max values are bolded for each dataset.
Concerning the techniques based on distributional
similarities, the method based on context windows
provides the best results for similarity, and the bag-
of-words representation outperforms most of the
other techniques for relatedness.
5.4 Supervised combination
In order to gain an insight on which would be the up-
per bound that we could obtain when combining our
methods, we took the output of three systems (bag
of words with window size 10, context window with
size 4, and the WN30g run). Each of these outputs is
a ranking of word pairs, and we implemented an or-
acle that chooses, for each pair, the rank that is most
similar to the rank of the pair in the gold-standard.
The outputs of the oracle have a Spearman correla-
tion of 0.97 for RG and 0.92 for WordSim353, which
gives as an indication of the correlations that could
be achieved by choosing for each pair the rank out-
put by the best classifier for that pair.
The previous results motivated the use of a su-
pervised approach to combine the output of the
different systems. We created a training cor-
pus containing pairs of pairs of words from the
datasets, having as features the similarity and rank
of each pair involved as given by the differ-
ent unsupervised systems. A classifier is trained
to decide whether the first pair is more simi-
lar than the second one. For example, a train-
ing instance using two unsupervised classifiers is
0.001364, 31, 0.327515, 64, 0.084805, 57, 0.109061, 59, negative
meaning that the similarities given by the first clas-
sifier to the two pairs were 0.001364 and 0.327515
respectively, which ranked them in positions 31 and
64. The second classifier gave them similarities of
0.084805 and 0.109061 respectively, which ranked
them in positions 57 and 59. The class negative in-
dicates that in the gold-standard the first pair has a
lower score than the second pair.
We have trained a SVM to classify pairs of pairs,
and use its output to rank the entries in both datasets.
It uses a polynomial kernel with degree 4. We did
Method Source Spearman (MC) Pearson (MC)
(Sahami et al, 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78]
(Chen et al, 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85]
(Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89]
(Leacock et al, 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91]
(Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90]
(Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92]
(Bollegala et al, 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92]
(Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93]
(Jarmasz, 2003) Roget?s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94]
(Patwardhan et al, 2006) WordNet n/a 0.91
(Alvarez and Lim, 2007) WordNet n/a 0.91
(Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96]
(Hughes et al, 2007) WordNet 0.90 n/a
Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a
Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93]
Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95]
Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0.51, 0.87]
SVM Web, WN 0.92 [0.84, 0.96] 0.93 [0.85, 0.97]
Table 7: Comparison with previous approaches for MC.
not have a held-out set, so we used the standard set-
tings of Weka, without trying to modify parameters,
e.g. C. Each word pair is scored with the number
of pairs that were considered to have less similar-
ity using the SVM. The results using 10-fold cross-
validation are shown in Table 6. A combination of
all methods produces the best results reported so far
for both datasets, statistically significant for RG.
6 Related work
Contrary to the WordSim353 dataset, common prac-
tice with the RG dataset has been to perform the
evaluation with Pearson correlation. In our believe
Pearson is less informative, as the Pearson correla-
tion suffers much when the scores of two systems are
not linearly correlated, something which happens
often given due to the different nature of the tech-
niques applied. Some authors, e.g. Alvarez and Lim
(2007), use a non-linear function to map the system
outputs into new values distributed more similarly
to the values in the gold-standard. In their case, the
mapping function was exp (?x4 ), which was chosenempirically. Finding such a function is dependent
on the dataset used, and involves an extra step in the
similarity calculations. Alternatively, the Spearman
correlation provides an evaluation metric that is in-
dependent of such data-dependent transformations.
Most similarity researchers have published their
25
Word pair M&C SVM Word pair M&C SVM
automobile, car 3.92 62 crane, implement 1.68 26
journey, voyage 3.84 54 brother, lad 1.66 39
gem, jewel 3.84 61 car, journey 1.16 37
boy, lad 3.76 57 monk, oracle 1.1 32
coast, shore 3.7 53 food, rooster 0.89 3
asylum, madhouse 3.61 45 coast, hill 0.87 34
magician, wizard 3.5 49 forest, graveyard 0.84 27
midday, noon 3.42 61 monk, slave 0.55 17
furnace, stove 3.11 50 lad, wizard 0.42 13
food, fruit 3.08 47 coast, forest 0.42 18
bird, cock 3.05 46 cord, smile 0.13 5
bird, crane 2.97 38 glass, magician 0.11 10
implement, tool 2.95 55 rooster, voyage 0.08 1
brother, monk 2.82 42 noon, string 0.08 5
Table 8: Our best results for the MC dataset.
Method Source Spearman
(Strube and Ponzetto, 2006) Wikipedia 0.19?0.48
(Jarmasz, 2003) WordNet 0.33?0.35
(Jarmasz, 2003) Roget?s 0.55
(Hughes and Ramage, 2007) WordNet 0.55
(Finkelstein et al, 2002) Web corpus, WN 0.56
(Gabrilovich and Markovitch, 2007) ODP 0.65
(Gabrilovich and Markovitch, 2007) Wikipedia 0.75
SVM Web corpus, WN 0.78
Table 9: Comparison with previous work for WordSim353.
complete results on a smaller subset of the RG
dataset containing 30 word pairs (Miller and
Charles, 1991), usually referred to as MC, making it
possible to compare different systems using differ-
ent correlation. Table 7 shows the results of related
work on MC that was available to us, including our
own. For the authors that did not provide the de-
tailed data we include only the Pearson correlation
with no confidence intervals.
Among the unsupervised methods introduced in
this paper, the context window produced the best re-
ported Spearman correlation, although the 0.95 con-
fidence intervals are too large to allow us to accept
the hypothesis that it is better than all others meth-
ods. The supervised combination produces the best
results reported so far. For the benefit of future re-
search, our results for the MC subset are displayed
in Table 8.
Comparison on the WordSim353 dataset is eas-
ier, as all researchers have used Spearman. The
figures in Table 9) show that our WordNet-based
method outperforms all previously published Word-
Net methods. We want to note that our WordNet-
based method outperforms that of Hughes and Ram-
age (2007), which uses a similar method. Although
there are some differences in the method, we think
that the main performance gain comes from the use
of the disambiguated glosses, which they did not
use. Our distributional methods also outperform all
other corpus-based methods. The most similar ap-
proach to our distributional technique is Finkelstein
et al (2002), who combined distributional similar-
ities from Web documents with a similarity from
WordNet. Their results are probably worse due to
the smaller data size (they used 270,000 documents)
and the differences in the calculation of the simi-
larities. The only method which outperforms our
non-supervised methods is that of (Gabrilovich and
Markovitch, 2007) when based on Wikipedia, prob-
ably because of the dense, manually distilled knowl-
edge contained in Wikipedia. All in all, our super-
vised combination gets the best published results on
this dataset.
7 Conclusions and future work
This paper has presented two state-of-the-art dis-
tributional and WordNet-based similarity measures,
with a study of several parameters, including per-
formance on similarity and relatedness data. We
show that the use of disambiguated glosses allows
for the best published results for WordNet-based
systems on the WordSim353 dataset, mainly due to
the better modeling of relatedness (as opposed to
similarity). Distributional similarities have proven
to be competitive when compared to knowledge-
based methods, with context windows being better
for similarity and bag of words for relatedness. Dis-
tributional similarity was effectively used to cover
out-of-vocabulary items in the WordNet-based mea-
sure providing our best unsupervised results. The
complementarity of our methods was exploited by
a supervised learner, producing the best results so
far for RG and WordSim353. Our results include
confidence values, which, surprisingly, were not in-
cluded in most previous work, and show that many
results over RG and WordSim353 are indistinguish-
able. The algorithm for WordNet-base similarity
and the necessary resources are publicly available8.
This work pioneers cross-lingual extension and
evaluation of both distributional and WordNet-based
measures. We have shown that closely aligned
wordnets provide a natural and effective way to
compute cross-lingual similarity with minor losses.
A simple translation strategy also yields good results
for distributional methods.
8http://ixa2.si.ehu.es/ukb/
26
References
E. Agirre and A. Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proc. of EACL
2009, Athens, Greece.
M.A. Alvarez and S.J. Lim. 2007. A Graph Modeling
of Semantic Similarity between Words. Proc. of the
Conference on Semantic Computing, pages 355?362.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The meaning multi-
lingual central repository. In Proc. of Global WordNet
Conference, Brno, Czech Republic.
D. Bollegala, Matsuo Y., and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proceedings of WWW?2007.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based Measures of Lexical Semantic Relatedness.
Computational Linguistics, 32(1):13?47.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proceedings of COCLING/ACL 2006.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets using structural information. In Proceedings of
ACL?2000, Hong Kong.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press,
Cambridge, Mass.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing Search in Context: The Concept Revisited. ACM
Transactions on Information Systems, 20(1):116?131.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Explicit
Semantic Analysis. Proc of IJCAI, pages 6?12.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02: Proceedings of the 11th international con-
ference on World Wide Web, pages 517?526.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings of
EMNLP-CoNLL-2007, pages 581?589.
M. Jarmasz. 2003. Roget?s Thesuarus as a lexical re-
source for Natural Language Processing.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of International Conference on Research
in Computational Linguistics, volume 33. Taiwan.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense iden-
tification. WordNet: An Electronic Lexical Database,
49(2):265?283.
D. Lin. 1998a. An information-theoretic definition of
similarity. In Proc. of ICML, pages 296?304, Wiscon-
sin, USA.
D. Lin. 1998b. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of ACL-98.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
J. Nivre. 2006. Inductive Dependency Parsing, vol-
ume 34 of Text, Speech and Language Technology.
Springer.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Re-
latedness of Concepts. In Proceedings of the EACL
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Pycholinguistics Together,
pages 1?8, Trento, Italy.
P. Resnik. 1995. Using Information Content to Evaluate
Semantic Similarity in a Taxonomy. Proc. of IJCAI,
14:448?453.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
M Ruiz-Casado, E. Alfonseca, and P. Castells. 2005.
Using context-window overlapping in Synonym Dis-
covery and Ontology Extension. In Proceedings of
RANLP-2005, Borovets, Bulgaria,.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. Proc. of WWW, pages 377?386.
M. Strube and S.P. Ponzetto. 2006. WikiRelate! Com-
puting Semantic Relatedness Using Wikipedia. In
Proceedings of the AAAI-2006, pages 1419?1424.
P.D. Turney. 2001. Mining the Web for Synonyms: PMI-
IR versus LSA on TOEFL. Lecture Notes in Computer
Science, 2167:491?502.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publishers.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proc. of ACL, pages 133?138, Las
Cruces, New Mexico.
D. Yang and D.M.W. Powers. 2005. Measuring semantic
similarity in the taxonomy of WordNet. Proceedings
of the Australasian conference on Computer Science.
27
Proceedings of ACL-08: HLT, pages 317?325,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving Parsing and PP attachment Performance with Sense Information
Eneko Agirre
IXA NLP Group
University of the Basque Country
Donostia, Basque Country
e.agirre@ehu.es
Timothy Baldwin
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
tim@csse.unimelb.edu.au
David Martinez
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
davidm@csse.unimelb.edu.au
Abstract
To date, parsers have made limited use of se-
mantic information, but there is evidence to
suggest that semantic features can enhance
parse disambiguation. This paper shows that
semantic classes help to obtain significant im-
provement in both parsing and PP attachment
tasks. We devise a gold-standard sense- and
parse tree-annotated dataset based on the in-
tersection of the Penn Treebank and SemCor,
and experiment with different approaches to
both semantic representation and disambigua-
tion. For the Bikel parser, we achieved a
maximal error reduction rate over the base-
line parser of 6.9% and 20.5%, for parsing and
PP-attachment respectively, using an unsuper-
vised WSD strategy. This demonstrates that
word sense information can indeed enhance
the performance of syntactic disambiguation.
1 Introduction
Traditionally, parse disambiguation has relied on
structural features extracted from syntactic parse
trees, and made only limited use of semantic in-
formation. There is both empirical evidence and
linguistic intuition to indicate that semantic fea-
tures can enhance parse disambiguation perfor-
mance, however. For example, a number of different
parsers have been shown to benefit from lexicalisa-
tion, that is, the conditioning of structural features
on the lexical head of the given constituent (Mager-
man, 1995; Collins, 1996; Charniak, 1997; Char-
niak, 2000; Collins, 2003). As an example of lexi-
calisation, we may observe in our training data that
knife often occurs as the manner adjunct of open in
prepositional phrases headed by with (c.f. open with
a knife), which would provide strong evidence for
with (a) knife attaching to open and not box in open
the box with a knife. It would not, however, pro-
vide any insight into the correct attachment of with
scissors in open the box with scissors, as the disam-
biguation model would not be able to predict that
knife and scissors are semantically similar and thus
likely to have the same attachment preferences.
In order to deal with this limitation, we propose to
integrate directly the semantic classes of words into
the process of training the parser. This is done by
substituting the original words with semantic codes
that reflect semantic classes. For example, in the
above example we could substitute both knife and
scissors with the semantic class TOOL, thus relating
the training and test instances directly. We explore
several models for semantic representation, based
around WordNet (Fellbaum, 1998).
Our approach to exploring the impact of lexical
semantics on parsing performance is to take two
state-of-the-art statistical treebank parsers and pre-
process the inputs variously. This simple method
allows us to incorporate semantic information into
the parser without having to reimplement a full sta-
tistical parser, and also allows for maximum compa-
rability with existing results in the treebank parsing
community. We test the parsers over both a PP at-
tachment and full parsing task.
In experimenting with different semantic repre-
sentations, we require some strategy to disambiguate
the semantic class of polysemous words in context
(e.g. determining for each instance of crane whether
it refers to an animal or a lifting device). We explore
a number of disambiguation strategies, including the
use of hand-annotated (gold-standard) senses, the
317
use of the most frequent sense, and an unsupervised
word sense disambiguation (WSD) system.
This paper shows that semantic classes help to
obtain significant improvements for both PP attach-
ment and parsing. We attain a 20.5% error reduction
for PP attachment, and 6.9% for parsing. These re-
sults are achieved using most frequent sense infor-
mation, which surprisingly outperforms both gold-
standard senses and automatic WSD.
The results are notable in demonstrating that very
simple preprocessing of the parser input facilitates
significant improvements in parser performance. We
provide the first definitive results that word sense
information can enhance Penn Treebank parser per-
formance, building on earlier results of Bikel (2000)
and Xiong et al (2005). Given our simple procedure
for incorporating lexical semantics into the parsing
process, our hope is that this research will open the
door to further gains using more sophisticated pars-
ing models and richer semantic options.
2 Background
This research is focused on applying lexical seman-
tics in parsing and PP attachment tasks. Below, we
outline these tasks.
Parsing
As our baseline parsers, we use two state-of-the-
art lexicalised parsing models, namely the Bikel
parser (Bikel, 2004) and Charniak parser (Charniak,
2000). While a detailed description of the respective
parsing models is beyond the scope of this paper, it
is worth noting that both parsers induce a context
free grammar as well as a generative parsing model
from a training set of parse trees, and use a devel-
opment set to tune internal parameters. Tradition-
ally, the two parsers have been trained and evaluated
over the WSJ portion of the Penn Treebank (PTB:
Marcus et al (1993)). We diverge from this norm in
focusing exclusively on a sense-annotated subset of
the Brown Corpus portion of the Penn Treebank, in
order to investigate the upper bound performance of
the models given gold-standard sense information.
PP attachment in a parsing context
Prepositional phrase attachment (PP attachment)
is the problem of determining the correct attachment
site for a PP, conventionally in the form of the noun
or verb in a V NP PP structure (Ratnaparkhi et al,
1994; Mitchell, 2004). For instance, in I ate a pizza
with anchovies, the PP with anchovies could attach
either to the verb (c.f. ate with anchovies) or to the
noun (c.f. pizza with anchovies), of which the noun
is the correct attachment site. With I ate a pizza with
friends, on the other hand, the verb is the correct at-
tachment site. PP attachment is a structural ambigu-
ity problem, and as such, a subproblem of parsing.
Traditionally the so-called RRR data (Ratna-
parkhi et al, 1994) has been used to evaluate PP
attachment algorithms. RRR consists of 20,081
training and 3,097 test quadruples of the form
(v,n1,p,n2), where the attachment decision is
either v or n1. The best published results over RRR
are those of Stetina and Nagao (1997), who em-
ploy WordNet sense predictions from an unsuper-
vised WSD method within a decision tree classifier.
Their work is particularly inspiring in that it signifi-
cantly outperformed the plethora of lexicalised prob-
abilistic models that had been proposed to that point,
and has not been beaten in later attempts.
In a recent paper, Atterer and Schu?tze (2007) crit-
icised the RRR dataset because it assumes that an
oracle parser provides the two hypothesised struc-
tures to choose between. This is needed to derive the
fact that there are two possible attachment sites, as
well as information about the lexical phrases, which
are typically extracted heuristically from gold stan-
dard parses. Atterer and Schu?tze argue that the only
meaningful setting for PP attachment is within a
parser, and go on to demonstrate that in a parser set-
ting, the Bikel parser is competitive with the best-
performing dedicated PP attachment methods. Any
improvement in PP attachment performance over the
baseline Bikel parser thus represents an advance-
ment in state-of-the-art performance.
That we specifically present results for PP attach-
ment in a parsing context is a combination of us sup-
porting the new research direction for PP attachment
established by Atterer and Schu?tze, and us wishing
to reinforce the findings of Stetina and Nagao that
word sense information significantly enhances PP
attachment performance in this new setting.
Lexical semantics in parsing
There have been a number of attempts to incorpo-
rate word sense information into parsing tasks. The
318
most closely related research is that of Bikel (2000),
who merged the Brown portion of the Penn Tree-
bank with SemCor (similarly to our approach in Sec-
tion 4.1), and used this as the basis for evaluation of
a generative bilexical model for joint WSD and pars-
ing. He evaluated his proposed model in a parsing
context both with and without WordNet-based sense
information, and found that the introduction of sense
information either had no impact or degraded parse
performance.
The only successful applications of word sense in-
formation to parsing that we are aware of are Xiong
et al (2005) and Fujita et al (2007). Xiong et al
(2005) experimented with first-sense and hypernym
features from HowNet and CiLin (both WordNets
for Chinese) in a generative parse model applied
to the Chinese Penn Treebank. The combination
of word sense and first-level hypernyms produced
a significant improvement over their basic model.
Fujita et al (2007) extended this work in imple-
menting a discriminative parse selection model in-
corporating word sense information mapped onto
upper-level ontologies of differing depths. Based
on gold-standard sense information, they achieved
large-scale improvements over a basic parse selec-
tion model in the context of the Hinoki treebank.
Other notable examples of the successful incorpo-
ration of lexical semantics into parsing, not through
word sense information but indirectly via selectional
preferences, are Dowding et al (1994) and Hektoen
(1997). For a broader review of WSD in NLP appli-
cations, see Resnik (2006).
3 Integrating Semantics into Parsing
Our approach to providing the parsers with sense
information is to make available the semantic de-
notation of each word in the form of a semantic
class. This is done simply by substituting the origi-
nal words with semantic codes. For example, in the
earlier example of open with a knife we could sub-
stitute both knife and scissors with the class TOOL,
and thus directly facilitate semantic generalisation
within the parser. There are three main aspects that
we have to consider in this process: (i) the seman-
tic representation, (ii) semantic disambiguation, and
(iii) morphology.
There are many ways to represent semantic re-
lationships between words. In this research we
opt for a class-based representation that will map
semantically-related words into a common semantic
category. Our choice for this work was the WordNet
2.1 lexical database, in which synonyms are grouped
into synsets, which are then linked via an IS-A hi-
erarchy. WordNet contains other types of relations
such as meronymy, but we did not use them in this
research. With any lexical semantic resource, we
have to be careful to choose the appropriate level of
granularity for a given task: if we limit ourselves to
synsets we will not be able to capture broader gen-
eralisations, such as the one between knife and scis-
sors;1 on the other hand by grouping words related at
a higher level in the hierarchy we could find that we
make overly coarse groupings (e.g. mallet, square
and steel-wool pad are also descendants of TOOL in
WordNet, none of which would conventionally be
used as the manner adjunct of cut). We will test dif-
ferent levels of granularity in this work.
The second problem we face is semantic disam-
biguation. The more fine-grained our semantic rep-
resentation, the higher the average polysemy and the
greater the need to distinguish between these senses.
For instance, if we find the word crane in a con-
text such as demolish a house with the crane, the
ability to discern that this corresponds to the DE-
VICE and not ANIMAL sense of word will allow us
to avoid erroneous generalisations. This problem of
identifying the correct sense of a word in context is
known as word sense disambiguation (WSD: Agirre
and Edmonds (2006)). Disambiguating each word
relative to its context of use becomes increasingly
difficult for fine-grained representations (Palmer et
al., 2006). We experiment with different ways of
tackling WSD, using both gold-standard data and
automatic methods.
Finally, when substituting words with semantic
tags we have to decide how to treat different word
forms of a given lemma. In the case of English, this
pertains most notably to verb inflection and noun
number, a distinction which we lose if we opt to
map all word forms onto semantic classes. For our
current purposes we choose to substitute all word
1In WordNet 2.1, knife and scissors are sister synsets, both
of which have TOOL as their 4th hypernym. Only by mapping
them onto their 1st hypernym or higher would we be able to
capture the semantic generalisation alluded to above.
319
forms, but we plan to look at alternative represen-
tations in the future.
4 Experimental setting
We evaluate the performance of our approach in two
settings: (1) full parsing, and (2) PP attachment
within a full parsing context. Below, we outline the
dataset used in this research and the parser evalu-
ation methodology, explain the methodology used
to perform PP attachment, present the different op-
tions for semantic representation, and finally detail
the disambiguation methods.
4.1 Dataset and parser evaluation
One of the main requirements for our dataset is the
availability of gold-standard sense and parse tree an-
notations. The gold-standard sense annotations al-
low us to perform upper bound evaluation of the rel-
ative impact of a given semantic representation on
parsing and PP attachment performance, to contrast
with the performance in more realistic semantic dis-
ambiguation settings. The gold-standard parse tree
annotations are required in order to carry out evalu-
ation of parser and PP attachment performance.
The only publicly-available resource with these
two characteristics at the time of this work was the
subset of the Brown Corpus that is included in both
SemCor (Landes et al, 1998) and the Penn Tree-
bank (PTB).2 This provided the basis of our dataset.
After sentence- and word-aligning the SemCor and
PTB data (discarding sentences where there was a
difference in tokenisation), we were left with a total
of 8,669 sentences containing 151,928 words. Note
that this dataset is smaller than the one described by
Bikel (2000) in a similar exercise, the reason being
our simple and conservative approach taken when
merging the resources.
We relied on this dataset alne for all the exper-
iments in this paper. In order to maximise repro-
ducibility and encourage further experimentation in
the direction pioneered in this research, we parti-
tioned the data into 3 sets: 80% training, 10% devel-
opment and 10% test data. This dataset is available
on request to the research community.
2OntoNotes (Hovy et al, 2006) includes large-scale tree-
bank and (selective) sense data, which we plan to use for future
experiments when it becomes fully available.
We evaluate the parsers via labelled bracketing re-
call (R), precision (P) and F-score (F1). We use
Bikel?s randomized parsing evaluation comparator3
(with p < 0.05 throughout) to test the statistical sig-
nificance of the results using word sense informa-
tion, relative to the respective baseline parser using
only lexical features.
4.2 PP attachment task
Following Atterer and Schu?tze (2007), we wrote
a script that, given a parse tree, identifies in-
stances of PP attachment ambiguity and outputs the
(v,n1,p,n2) quadruple involved and the attach-
ment decision. This extraction system uses Collins?
rules (based on TREEP (Chiang and Bikel, 2002))
to locate the heads of phrases. Over the combined
gold-standard parsing dataset, our script extracted a
total of 2,541 PP attachment quadruples. As with
the parsing data, we partitioned the data into 3 sets:
80% training, 10% development and 10% test data.
Once again, this dataset and the script used to ex-
tract the quadruples are available on request to the
research community.
In order to evaluate the PP attachment perfor-
mance of a parser, we run our extraction script over
the parser output in the same manner as for the gold-
standard data, and compare the extracted quadru-
ples to the gold-standard ones. Note that there is
no guarantee of agreement in the quadruple mem-
bership between the extraction script and the gold
standard, as the parser may have produced a parse
which is incompatible with either attachment possi-
bility. A quadruple is deemed correct if: (1) it exists
in the gold standard, and (2) the attachment deci-
sion is correct. Conversely, it is deemed incorrect if:
(1) it exists in the gold standard, and (2) the attach-
ment decision is incorrect. Quadruples not found in
the gold standard are discarded. Precision was mea-
sured as the number of correct quadruples divided by
the total number of correct and incorrect quadruples
(i.e. all quadruples which are not discarded), and re-
call as the number of correct quadruples divided by
the total number of gold-standard quadruples in the
test set. This evaluation methodology coincides with
that of Atterer and Schu?tze (2007).
Statistical significance was calculated based on
3www.cis.upenn.edu/?dbikel/software.html
320
a modified version of the Bikel comparator (see
above), once again with p < 0.05.
4.3 Semantic representation
We experimented with a range of semantic represen-
tations, all of which are based on WordNet 2.1. As
mentioned above, words in WordNet are organised
into sets of synonyms, called synsets. Each synset
in turn belongs to a unique semantic file (SF). There
are a total of 45 SFs (1 for adverbs, 3 for adjectives,
15 for verbs, and 26 for nouns), based on syntactic
and semantic categories. A selection of SFs is pre-
sented in Table 1 for illustration purposes.
We experiment with both full synsets and SFs as
instances of fine-grained and coarse-grained seman-
tic representation, respectively. As an example of
the difference in these two representations, knife in
its tool sense is in the EDGE TOOL USED AS A CUT-
TING INSTRUMENT singleton synset, and also in the
ARTIFACT SF along with thousands of other words
including cutter. Note that these are the two ex-
tremes of semantic granularity in WordNet, and we
plan to experiment with intermediate representation
levels in future research (c.f. Li and Abe (1998), Mc-
Carthy and Carroll (2003), Xiong et al (2005), Fu-
jita et al (2007)).
As a hybrid representation, we tested the effect
of merging words with their corresponding SF (e.g.
knife+ARTIFACT ). This is a form of semantic spe-
cialisation rather than generalisation, and allows the
parser to discriminate between the different senses
of each word, but not generalise across words.
For each of these three semantic representations,
we experimented with substituting each of: (1) all
open-class POSs (nouns, verbs, adjectives and ad-
verbs), (2) nouns only, and (3) verbs only. There are
thus a total of 9 combinations of representation type
and target POS.
4.4 Disambiguation methods
For a given semantic representation, we need some
form of WSD to determine the semantics of each
token occurrence of a target word. We experimented
with three options:
1. Gold-standard: Gold-standard annotations
from SemCor. This gives us the upper bound
performance of the semantic representation.
SF ID DEFINITION
adj.all all adjective clusters
adj.pert relational adjectives (pertainyms)
adj.ppl participial adjectives
adv.all all adverbs
noun.act nouns denoting acts or actions
noun.animal nouns denoting animals
noun.artifact nouns denoting man-made objects
...
verb.consumption verbs of eating and drinking
verb.emotion verbs of feeling
verb.perception verbs of seeing, hearing, feeling
...
Table 1: A selection of WordNet SFs
2. First Sense (1ST): All token instances of a
given word are tagged with their most fre-
quent sense in WordNet.4 Note that the first
sense predictions are based largely on the same
dataset as we use in our evaluation, such that
the predictions are tuned to our dataset and not
fully unsupervised.
3. Automatic Sense Ranking (ASR): First sense
tagging as for First Sense above, except that an
unsupervised system is used to automatically
predict the most frequent sense for each word
based on an independent corpus. The method
we use to predict the first sense is that of Mc-
Carthy et al (2004), which was obtained us-
ing a thesaurus automatically created from the
British National Corpus (BNC) applying the
method of Lin (1998), coupled with WordNet-
based similarity measures. This method is fully
unsupervised and completely unreliant on any
annotations from our dataset.
In the case of SFs, we perform full synset WSD
based on one of the above options, and then map the
prediction onto the corresponding (unique) SF.
5 Results
We present the results for each disambiguation ap-
proach in turn, analysing the results for parsing and
PP attachment separately.
4There are some differences with the most frequent sense in
SemCor, due to extra corpora used in WordNet development,
and also changes in WordNet from the original version used for
the SemCor tagging.
321
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .857 .808 .832 .837 .845 .841
SF .855 .809 .831 .847? .854? .850?
SFn .860 .808 .833 .847? .853? .850?
SFv .861 .811 .835 .847? .856? .851?
word + SF .865? .814? .839? .837 .846 .842
word + SFn .862 .809 .835 .841? .850? .846?
word + SFv .862 .810 .835 .840 .851 .845
Syn .863? .812 .837 .845? .853? .849?
Synn .860 .807 .832 .841 .849 .845
Synv .863? .813? .837? .843? .851? .847?
Table 2: Parsing results with gold-standard senses (? in-
dicates that the recall or precision is significantly better
than baseline; the best performing method in each col-
umn is shown in bold)
5.1 Gold standard
We disambiguated each token instance in our cor-
pus according to the gold-standard sense data, and
trained both the Charniak and Bikel parsers over
each semantic representation. We evaluated the
parsers in full parsing and PP attachment contexts.
The results for parsing are given in Table 2. The
rows represent the three semantic representations
(including whether we substitute only nouns, only
verbs or all POS). We can see that in almost all
cases the semantically-enriched representations im-
prove over the baseline parsers. These results are
statistically significant in some cases (as indicated
by ?). The SFv representation produces the best re-
sults for Bikel (F-score 0.010 above baseline), while
for Charniak the best performance is obtained with
word+SF (F-score 0.007 above baseline). Compar-
ing the two baseline parsers, Bikel achieves better
precision and Charniak better recall. Overall, Bikel
obtains a superior F-score in all configurations.
The results for the PP attachment experiments us-
ing gold-standard senses are given in Table 3, both
for the Charniak and Bikel parsers. Again, the F-
score for the semantic representations is better than
the baseline in all cases. We see that the improve-
ment is significant for recall in most cases (particu-
larly when using verbs), but not for precision (only
Charniak over Synv and word+SFv for Bikel). For
both parsers the best results are achieved with SFv,
which was also the best configuration for parsing
with Bikel. The performance gain obtained here is
larger than in parsing, which is in accordance with
the findings of Stetina and Nagao that lexical se-
mantics has a considerable effect on PP attachment
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .667 .798 .727 .659 .820 .730
SF .710 .808 .756 .714? .809 .758
SFn .671 .792 .726 .706 .818 .758
SFv .729? .823 .773? .733? .827 .778?
word + SF .710? .801 .753 .706? .837 .766?
word + SFn .698? .813 .751 .706? .829 .763?
word + SFv .714? .805 .757? .706? .837? .766?
Syn .722? .814 .765? .702? .825 .758
Synn .678 .805 .736 .690 .822 .751
Synv .702? .817? .755? .690? .834 .755?
Table 3: PP attachment results with gold-standard senses
(? indicates that the recall or precision is significantly bet-
ter than baseline; the best performing method in each col-
umn is shown in bold)
performance. As in full-parsing, Bikel outperforms
Charniak, but in this case the difference in the base-
lines is not statistically significant.
5.2 First sense (1ST)
For this experiment, we use the first sense data from
WordNet for disambiguation. The results for full
parsing are given in Table 4. Again, the perfor-
mance is significantly better than baseline in most
cases, and surprisingly the results are even better
than gold-standard in some cases. We hypothesise
that this is due to the avoidance of excessive frag-
mentation, as occurs with fine-grained senses. The
results are significantly better for nouns, with SFn
performing best. Verbs seem to suffer from lack of
disambiguation precision, especially for Bikel. Here
again, Charniak trails behind Bikel.
The results for the PP attachment task are shown
in Table 5. The behaviour is slightly different here,
with Charniak obtaining better results than Bikel in
most cases. As was the case for parsing, the per-
formance with 1ST reaches and in many instances
surpasses gold-standard levels, achieving statistical
significance over the baseline in places. Compar-
ing the semantic representations, the best results are
achieved with SFv, as we saw in the gold-standard
PP-attachment case.
5.3 Automatic sense ranking (ASR)
The final option for WSD is automatic sense rank-
ing, which indicates how well our method performs
in a completely unsupervised setting.
The parsing results are given in Table 6. We can
see that the scores are very similar to those from
322
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .857 .807 .832 .837 .845 .841
SF .851 .804 .827 .843 .850 .846
SFn .863? .813 .837? .850? .854? .852?
SFv .857 .808 .832 .843 .853? .848
word + SF .859 .810 .834 .833 .841 .837
word + SFn .862? .811 .836 .844? .851? .848?
word + SFv .857 .808 .832 .831 .839 .835
Syn .857 .810 .833 .837 .844 .840
Synn .863? .812 .837? .844? .851? .848?
Synv .860 .810 .834 .836 .844 .840
Table 4: Parsing results with 1ST (? indicates that the
recall or precision is significantly better than baseline; the
best performing method in each column is shown in bold)
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .667 .798 .727 .659 .820 .730
SF .710 .808 .756 .702 .806 .751
SFn .671 .781 .722 .702 .829 .760
SFv .737? .836? .783? .718? .821 .766?
word + SF .706 .811 .755 .694 .823 .753
word + SFn .690 .815 .747 .667 .810 .731
word + SFv .714? .805 .757? .710? .819 .761?
Syn .725? .833? .776? .698 .828 .757
Synn .698 .828? .757? .667 .817 .734
Synv .722? .811 .763? .706? .818 .758?
Table 5: PP attachment results with 1ST (? indicates that
the recall or precision is significantly better than baseline;
the best performing method in each column is shown in
bold)
1ST, with improvements in some cases, particularly
for Charniak. Again, the results are better for nouns,
except for the case of SFv with Bikel. Bikel outper-
forms Charniak in terms of F-score in all cases.
The PP attachment results are given in Table 7.
The results are similar to 1ST, with significant im-
provements for verbs. In this case, synsets slightly
outperform SF. Charniak performs better than Bikel,
and the results for Synv are higher than the best ob-
tained using gold-standard senses.
6 Discussion
The results of the previous section show that the im-
provements in parsing results are small but signifi-
cant, for all three word sense disambiguation strate-
gies (gold-standard, 1ST and ASR). Table 8 sum-
marises the results, showing that the error reduction
rate (ERR) over the parsing F-score is up to 6.9%,
which is remarkable given the relatively superficial
strategy for incorporating sense information into the
parser. Note also that our baseline results for the
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .857 .807 .832 .837 .845 .841
SF .863 .815? .838 .845? .852 .849
SFn .862 .810 .835 .845? .850 .847?
SFv .859 .810 .833 .846? .856? .851?
word + SF .859 .810 .834 .836 .844 .840
word + SFn .865? .813? .838? .844? .852? .848?
word + SFv .856 .806 .830 .832 .839 .836
Syn .856 .807 .831 .840 .847 .843
Synn .864? .813? .838? .844? .851? .847?
Synv .857 .806 .831 .837 .845 .841
Table 6: Parsing results with ASR (? indicates that the
recall or precision is significantly better than baseline; the
best performing method in each column is shown in bold)
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .667 .798 .727 .659 .820 .730
SF .733? .824 .776? .698 .805 .748
SFn .682 .791 .733 .671 .807 .732
SFv .733? .813 .771? .710? .812 .757?
word + SF .714? .798 .754 .675 .800 .732
word + SFn .690 .807 .744 .659 .804 .724
word + SFv .706? .800 .750 .702? .814 .754?
Syn .733? .827 .778? .694 .805 .745
Synn .686 .810 .743 .667 .806 .730
Synv .714? .816 .762? .714? .816 .762?
Table 7: PP attachment results with ASR (? indicates that
the recall or precision is significantly better than baseline;
the best performance in each column is shown in bold)
dataset are almost the same as previous work pars-
ing the Brown corpus with similar models (Gildea,
2001), which suggests that our dataset is representa-
tive of this corpus.
The improvement in PP attachment was larger
(20.5% ERR), and also statistically significant. The
results for PP attachment are especially important,
as we demonstrate that the sense information has
high utility when embedded within a parser, where
the parser needs to first identify the ambiguity and
heads correctly. Note that Atterer and Schu?tze
(2007) have shown that the Bikel parser performs as
well as the state-of-the-art in PP attachment, which
suggests our method improves over the current state-
of-the-art. The fact that the improvement is larger
for PP attachment than for full parsing is suggestive
of PP attachment being a parsing subtask where lex-
ical semantic information is particularly important,
supporting the findings of Stetina and Nagao (1997)
over a standalone PP attachment task. We also ob-
served that while better PP-attachment usually im-
proves parsing, there is some small variation. This
323
WSD TASK PAR BASE SEM ERR BEST
Pars.
C .832 .839? 4.2% word+SF
Gold- B .841 .851? 6.3% SFv
standard
PP
C .727 .773? 16.9% SFv
B .730 .778? 17.8% SFv
Pars.
C .832 .837? 3.0% SFn, Synn
1ST
B .841 .852? 6.9% SFn
PP
C .727 .783? 20.5% SFv
B .730 .766? 13.3% SFv
Pars.
C .832 .838? 3.6% SF, word+SFn, Synn
ASR
B .841 .851? 6.3% SFv
PP
C .727 .778? 18.7% Syn
B .730 .762? 11.9% Synv
Table 8: Summary of F-score results with error reduc-
tion rates and the best semantic representation(s) for each
setting (C = Charniak, B = Bikel)
means that the best configuration for PP-attachment
does not always produce the best results for parsing
One surprising finding was the strong perfor-
mance of the automatic WSD systems, actually
outperforming the gold-standard annotation overall.
Our interpretation of this result is that the approach
of annotating all occurrences of the same word with
the same sense allows the model to avoid the data
sparseness associated with the gold-standard distinc-
tions, as well as supporting the merging of differ-
ent words into single semantic classes. While the
results for gold-standard senses were intended as
an upper bound for WordNet-based sense informa-
tion, in practice there was very little difference be-
tween gold-standard senses and automatic WSD in
all cases barring the Bikel parser and PP attachment.
Comparing the two parsers, Charniak performs
better than Bikel on PP attachment when automatic
WSD is used, while Bikel performs better on parsing
overall. Regarding the choice of WSD system, the
results for both approaches are very similar, show-
ing that ASR performs well, even if it does not re-
quire sense frequency information.
The analysis of performance according to the se-
mantic representation is not so clear cut. Gener-
alising only verbs to semantic files (SFv) was the
best option in most of the experiments, particularly
for PP-attachment. This could indicate that seman-
tic generalisation is particularly important for verbs,
more so than nouns.
Our hope is that this paper serves as the bridge-
head for a new line of research into the impact of
lexical semantics on parsing. Notably, more could
be done to fine-tune the semantic representation be-
tween the two extremes of full synsets and SFs.
One could also imagine that the appropriate level of
generalisation differs across POS and even the rel-
ative syntactic role, e.g. finer-grained semantics are
needed for the objects than subjects of verbs.
On the other hand, the parsing strategy is very
simple, as we just substitute words by their semantic
class and then train statistical parsers on the trans-
formed input. The semantic class should be an in-
formation source that the parsers take into account in
addition to analysing the actual words used. Tighter
integration of semantics into the parsing models,
possibly in the form of discriminative reranking
models (Collins and Koo, 2005; Charniak and John-
son, 2005; McClosky et al, 2006), is a promising
way forward in this regard.
7 Conclusions
In this work we have trained two state-of-the-art
statistical parsers on semantically-enriched input,
where content words have been substituted with
their semantic classes. This simple method allows
us to incorporate lexical semantic information into
the parser, without having to reimplement a full sta-
tistical parser. We tested the two parsers in both a
full parsing and a PP attachment context.
This paper shows that semantic classes achieve
significant improvement both on full parsing and
PP attachment tasks relative to the baseline parsers.
PP attachment achieves a 20.5% ERR, and parsing
6.9% without requiring hand-tagged data.
The results are highly significant in demonstrating
that a simplistic approach to incorporating lexical
semantics into a parser significantly improves parser
performance. As far as we know, these are the first
results over both WordNet and the Penn Treebank to
show that semantic processing helps parsing.
Acknowledgements
We wish to thank Diana McCarthy for providing us
with the sense rank for the target words. This work
was partially funded by the Education Ministry (project
KNOW TIN2006-15049), the Basque Government (IT-
397-07), and the Australian Research Council (grant no.
DP0663879). Eneko Agirre participated in this research
while visiting the University of Melbourne, based on joint
funding from the Basque Government and HCSNet.
324
References
Eneko Agirre and Philip Edmonds, editors. 2006. Word Sense
Disambiguation: Algorithms and Applications. Springer,
Dordrecht, Netherlands.
Michaela Atterer and Hinrich Schu?tze. 2007. Prepositional
phrase attachment without oracles. Computational Linguis-
tics, 33(4):469?476.
Daniel M. Bikel. 2000. A statistical model for parsing and
word-sense disambiguation. In Proc. of the Joint SIGDAT
Conference on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora (EMNLP/VLC-2000), pages
155?63, Hong Kong, China.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In Proc.
of the 43rd Annual Meeting of the ACL, pages 173?80, Ann
Arbor, USA.
Eugene Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In Proc. of the 15th Annual
Conference on Artificial Intelligence (AAAI-97), pages 598?
603, Stanford, USA.
Eugene Charniak. 2000. A maximum entropy-based parser.
In Proc. of the 1st Annual Meeting of the North Ameri-
can Chapter of Association for Computational Linguistics
(NAACL2000), Seattle, USA.
David Chiang and David M. Bikel. 2002. Recovering latent
information in treebanks. In Proc. of the 19th International
Conference on Computational Linguistics (COLING 2002),
pages 183?9, Taipei, Taiwan.
Michael Collins and Terry Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational Linguistics,
31(1):25?69.
Michael J. Collins. 1996. A new statistical parser based on
lexical dependencies. In Proc. of the 34th Annual Meeting
of the ACL, pages 184?91, Santa Cruz, USA.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):589?637.
John Dowding, Robert Moore, Franc?ois Andry, and Douglas
Moran. 1994. Interleaving syntax and semantics in an effi-
cient bottom-up parser. In Proc. of the 32nd Annual Meeting
of the ACL, pages 110?6, Las Cruces, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki
Tanaka. 2007. Exploiting semantic information for HPSG
parse selection. In Proc. of the ACL 2007 Workshop on Deep
Linguistic Processing, pages 25?32, Prague, Czech Repub-
lic.
Daniel Gildea. 2001. Corpus variation and parser performance.
In Proc. of the 6th Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2001), pages 167?202,
Pittsburgh, USA.
Erik Hektoen. 1997. Probabilistic parse selection based
on semantic cooccurrences. In Proc. of the 5th Inter-
national Workshop on Parsing Technologies (IWPT-1997),
pages 113?122, Boston, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proc. of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume: Short
Papers, pages 57?60, New York City, USA.
Shari Landes, Claudia Leacock, and Randee I. Tengi. 1998.
Building semantic concordances. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database. MIT
Press, Cambridge, USA.
Hang Li and Naoki Abe. 1998. Generalising case frames using
a thesaurus and the MDL principle. Computational Linguis-
tics, 24(2):217?44.
Dekang Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of the 36th Annual Meeting of the
ACL and 17th International Conference on Computational
Linguistics: COLING/ACL-98, pages 768?774, Montreal,
Canada.
David M. Magerman. 1995. Statistical decision-tree models
for parsing. In Proc. of the 33rd Annual Meeting of the ACL,
pages 276?83, Cambridge, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguistics,
19(2):313?30.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically ac-
quired selectional preferences. Computational Linguistics,
29(4):639?654.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.
2004. Finding predominant senses in untagged text. In
Proc. of the 42nd Annual Meeting of the ACL, pages 280?
7, Barcelona, Spain.
David McClosky, Eugene Charniak, and Mark Johnson. 2006.
Effective self-training for parsing. In Proc. of the Hu-
man Language Technology Conference of the NAACL
(NAACL2006), pages 152?159, New York City, USA.
Brian Mitchell. 2004. Prepositional Phrase Attachment using
Machine Learning Algorithms. Ph.D. thesis, University of
Sheffield.
Martha Palmer, Hoa Dang, and Christiane Fellbaum. 2006.
Making fine-grained and coarse-grained sense distinctions,
both manually and automatically. Natural Language Engi-
neering, 13(2):137?63.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994.
A maximum entropy model for prepositional phrase attach-
ment. In HLT ?94: Proceedings of the Workshop on Human
Language Technology, pages 250?255, Plainsboro, USA.
Philip Resnik. 2006. WSD in NLP applications. In Eneko
Agirre and Philip Edmonds, editors, Word Sense Disam-
biguation: Algorithms and Applications, chapter 11, pages
303?40. Springer, Dordrecht, Netherlands.
Jiri Stetina and Makoto Nagao. 1997. Corpus based PP attach-
ment ambiguity resolution with a semantic dictionary. In
Proc. of the 5th Annual Workshop on Very Large Corpora,
pages 66?80, Hong Kong, China.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the Penn Chinese Tree-
bank with semantic knowledge. In Proc. of the 2nd Inter-
national Joint Conference on Natural Language Processing
(IJCNLP-05), pages 70?81, Jeju Island, Korea.
325
Proceedings of ACL-08: HLT, pages 550?558,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Robustness and Generalization of Role Sets: PropBank vs. VerbNet
Ben?at Zapirain and Eneko Agirre
IXA NLP Group
University of the Basque Country
{benat.zapirain,e.agirre}@ehu.es
Llu??s Ma`rquez
TALP Research Center
Technical University of Catalonia
lluism@lsi.upc.edu
Abstract
This paper presents an empirical study on the
robustness and generalization of two alterna-
tive role sets for semantic role labeling: Prop-
Bank numbered roles and VerbNet thematic
roles. By testing a state?of?the?art SRL sys-
tem with the two alternative role annotations,
we show that the PropBank role set is more
robust to the lack of verb?specific semantic
information and generalizes better to infre-
quent and unseen predicates. Keeping in mind
that thematic roles are better for application
needs, we also tested the best way to generate
VerbNet annotation. We conclude that tagging
first PropBank roles and mapping into Verb-
Net roles is as effective as training and tagging
directly on VerbNet, and more robust for do-
main shifts.
1 Introduction
Semantic Role Labeling is the problem of analyzing
clause predicates in open text by identifying argu-
ments and tagging them with semantic labels indi-
cating the role they play with respect to the verb.
Such sentence?level semantic analysis allows to de-
termine ?who? did ?what? to ?whom?, ?when? and
?where?, and, thus, characterize the participants and
properties of the events established by the predi-
cates. This kind of semantic analysis is very inter-
esting for a broad spectrum of NLP applications (in-
formation extraction, summarization, question an-
swering, machine translation, etc.), since it opens
the door to exploit the semantic relations among lin-
guistic constituents.
The properties of the semantically annotated cor-
pora available have conditioned the type of research
and systems that have been developed so far. Prop-
Bank (Palmer et al, 2005) is the most widely used
corpus for training SRL systems, probably because
it contains running text from the Penn Treebank cor-
pus with annotations on all verbal predicates. Also,
a few evaluation exercises on SRL have been con-
ducted on this corpus in the CoNLL-2004 and 2005
conferences. However, a serious criticisms to the
PropBank corpus refers to the role set it uses, which
consists of a set of numbered core arguments, whose
semantic translation is verb-dependent. While Arg0
and Arg1 are intended to indicate the general roles
of Agent and Theme, other argument numbers do
not generalize across verbs and do not correspond
to general semantic roles. This fact might compro-
mise generalization and portability of SRL systems,
especially when the training corpus is small.
More recently, a mapping from PropBank num-
bered arguments into VerbNet thematic roles has
been developed and a version of the PropBank cor-
pus with thematic roles has been released (Loper et
al., 2007). Thematic roles represent a compact set of
verb-independent general roles widely used in lin-
guistic theory (e.g., Agent, Theme, Patient, Recipi-
ent, Cause, etc.). We foresee two advantages of us-
ing such thematic roles. On the one hand, statisti-
cal SRL systems trained from them could generalize
better and, therefore, be more robust and portable,
as suggested in (Yi et al, 2007). On the other hand,
roles in a paradigm like VerbNet would allow for in-
ferences over the assigned roles, which is only pos-
sible in a more limited way with PropBank.
In a previous paper (Zapirain et al, 2008), we pre-
sented a first comparison between the two previous
role sets on the SemEval-2007 Task 17 corpus (Prad-
han et al, 2007). The SemEval-2007 corpus only
550
comprised examples about 50 different verbs. The
results of that paper were, thus, considered prelim-
inary, as they could depend on the small amount of
data (both in training data and number of verbs) or
the specific set of verbs being used. Now, we ex-
tend those experiments to the entire PropBank cor-
pus, and we include two extra experiments on do-
main shifts (using the Brown corpus as test set) and
on grouping VerbNet labels. More concretely, this
paper explores two aspects of the problem. First,
having in mind the claim that general thematic roles
should be more robust to changing domains and
unseen predicates, we study the performance of a
state-of-the-art SRL system trained on either codi-
fication of roles and some specific settings, i.e. in-
cluding/excluding verb-specific information, label-
ing unseen verb predicates, or domain shifts. Sec-
ond, assuming that application scenarios would pre-
fer dealing with general thematic role labels, we ex-
plore the best way to label a text with thematic roles,
namely, by training directly on VerbNet roles or by
using the PropBank SRL system and perform a pos-
terior mapping into thematic roles.
The results confirm our preliminary findings (Za-
pirain et al, 2008). We observe that the PropBank
roles are more robust in all tested experimental con-
ditions, i.e., the performance decrease is more se-
vere for VerbNet. Besides, tagging first PropBank
roles and then mapping into VerbNet roles is as ef-
fective as training and tagging directly on VerbNet,
and more robust for domain shifts.
The rest of the paper is organized as follows: Sec-
tion 2 contains some background on PropBank and
VerbNet role sets. Section 3 presents the experimen-
tal setting and the base SRL system used for the role
set comparisons. In Section 4 the main compara-
tive experiments on robustness are described. Sec-
tion 5 is devoted to analyze the posterior mapping of
PropBank outputs into VerbNet thematic roles, and
includes results on domain?shift experiments using
Brown as test set. Finally, Sections 6 and 7 contain
a discussion of the results.
2 Corpora and Semantic Role Sets
The PropBank corpus is the result of adding a se-
mantic layer to the syntactic structures of Penn Tree-
bank II (Palmer et al, 2005). Specifically, it pro-
vides information about predicate-argument struc-
tures to all verbal predicates of the Wall Street Jour-
nal section of the treebank. The role set is theory?
neutral and consists of a set of numbered core ar-
guments (Arg0, Arg1, ..., Arg5). Each verb has a
frameset listing its allowed role labels and mapping
each numbered role to an English-language descrip-
tion of its semantics.
Different senses for a polysemous verb have dif-
ferent framesets, but the argument labels are seman-
tically consistent in all syntactic alternations of the
same verb?sense. For instance in ?Kevin broke [the
window]Arg1 ? and in ?[The door]Arg1 broke into a
million pieces?, for the verb broke.01, both Arg1 ar-
guments have the same semantic meaning, that is
?broken entity?. Nevertheless, argument labels are
not necessarily consistent across different verbs (or
verb senses). For instance, the same Arg2 label is
used to identify the Destination argument of a propo-
sition governed by the verb send and the Beneficiary
argument of the verb compose. This fact might com-
promise generalization of systems trained on Prop-
Bank, which might be focusing too much on verb?
specific knowledge. It is worth noting that the two
most frequent arguments, Arg0 and Arg1, are in-
tended to indicate the general roles of Agent and
Theme and are usually consistent across different
verbs. However, this correspondence is not total.
According to the study by (Yi et al, 2007), Arg0
corresponds to Agent 85.4% of the time, but also
to Experiencer (7.2%), Theme (2.1%), and Cause
(1.9%). Similarly, Arg1 corresponds to Theme in
47.0% of the occurrences but also to Topic (23.0%),
Patient (10.8%), and Product (2.9%), among others.
Contrary to core arguments, adjuncts (Temporal and
Location markers, etc.) are annotated with a closed
set of general and verb-independent labels.
VerbNet (Kipper et al, 2000) is a computational
verb lexicon in which verbs are organized hier-
archically into classes depending on their syntac-
tic/semantic linking behavior. The classes are based
on Levin?s verb classes (Levin, 1993) and each con-
tains a list of member verbs and a correspondence
between the shared syntactic frames and the se-
mantic information, such as thematic roles and se-
lectional constraints. There are 23 thematic roles
(Agent, Patient, Theme, Experiencer, Source, Ben-
eficiary, Instrument, etc.) which, unlike the Prop-
551
Bank numbered arguments, are considered as gen-
eral verb-independent roles.
This level of abstraction makes them, in princi-
ple, better suited (compared to PropBank numbered
arguments) for being directly exploited by general
NLP applications. But, VerbNet by itself is not an
appropriate resource to train SRL systems. As op-
posed to PropBank, the number of tagged examples
is far more limited in VerbNet. Fortunately, in the
last years a twofold effort has been made in order
to generate a large corpus fully annotated with the-
matic roles. Firstly, the SemLink1 resource (Loper
et al, 2007) established a mapping between Prop-
Bank framesets and VerbNet thematic roles. Sec-
ondly, the SemLink mapping was applied to a repre-
sentative portion of the PropBank corpus and man-
ually disambiguated (Loper et al, 2007). The re-
sulting corpus is currently available for the research
community and makes possible comparative studies
between role sets.
3 Experimental Setting
3.1 Datasets
The data used in this work is the benchmark corpus
provided by the SRL shared task of CoNLL-2005
(Carreras and Ma`rquez, 2005). The dataset, of over
1 million tokens, comprises PropBank sections 02?
21 for training, and sections 24 and 23 for develop-
ment and test, respectively. From the input informa-
tion, we used part of speech tags and full parse trees
(generated using Charniak?s parser) and discarded
named entities. Also, we used the publicly avail-
able SemLink mapping from PropBank into Verb-
Net roles (Loper et al, 2007) to generate a replicate
of the CoNLL-2005 corpus containing also the Verb-
Net annotation of roles.
Unfortunately, SemLink version 1.0 does not
cover all propositions and arguments in the Prop-
Bank corpus. In order to have an homogeneous cor-
pus and not to bias experimental evaluation, we de-
cided to discard all incomplete examples and keep
only those propositions that were 100% mapped into
VerbNet roles. The resulting corpus contains 56% of
the original propositions, that is, over 50,000 propo-
sitions in the training set. This subcorpus is much
larger than the SemEval-2007 Task 17 dataset used
1http://verbs.colorado.edu/semlink/
in our previous experimental work (Zapirain et al,
2008). The difference is especially noticeable in
the diversity of predicates represented. In this case,
there are 1,709 different verbs (1,505 lemmas) com-
pared to the 50 verbs of the SemEval corpus. We
believe that the size and richness of this corpus is
enough to test and extract reliable conclusions on
the robustness and generalization across verbs of the
role sets under study.
In order to study the behavior of both role sets
in out?of?domain data, we made use of the Prop-
Banked Brown corpus (Marcus et al, 1994) for test-
ing, as it is also mapped into VerbNet thematic roles
in the SemLink resource. Again, we discarded those
propositions that were not entirely mapped into the-
matic roles (45%).
3.2 SRL System
Our basic Semantic Role Labeling system represents
the tagging problem as a Maximum Entropy Markov
Model (MEMM). The system uses full syntactic
information to select a sequence of constituents
from the input text and tags these tokens with Be-
gin/Inside/Outside (BIO) labels, using state-of-the-
art classifiers and features. The system achieves very
good performance in the CoNLL-2005 shared task
dataset and in the SRL subtask of the SemEval-2007
English lexical sample task (Zapirain et al, 2007).
Check this paper for a complete description of the
system.
When searching for the most likely state se-
quence, the following constraints are observed2:
1. No duplicate argument classes for Arg0?Arg5
PropBank (or VerbNet) roles are allowed.
2. If there is a R-X argument (reference), then
there has to be a X argument before (referent).
3. If there is a C-X argument (continuation), then
there has to be a X argument before.
4. Before a I-X token, there has to be a B-X or I-X
token.
5. Given a predicate, only the arguments de-
scribed in its PropBank (or VerbNet) lexical en-
try (i.e., the verbal frameset) are allowed.
2Note that some of the constraints are dependent of the role
set used, i.e., PropBank or VerbNet
552
Regarding the last constraint, the lexical entries
of the verbs were constructed from the training data
itself. For instance, the verb build appears with
four different PropBank core roles (Arg0?3) and five
VerbNet roles (Product, Material, Asset, Attribute,
Theme), which are the only ones allowed for that
verb at test time. Note that in the cases where the
verb sense was known we could constraint the pos-
sible arguments to those that appear in the lexical en-
try of that sense, as opposed of using the arguments
that appear in all senses.
4 On the Generalization of Role Sets
We first seek a basic reference of the comparative
performance of the classifier on each role set. We
devised two settings based on our dataset. In the
first setting (?SemEval?) we use all the available in-
formation provided in the corpus, including the verb
senses in PropBank and VerbNet. This information
was available both in the training and test, and was
thus used as an additional feature by the classifier
and to constrain further the possible arguments when
searching for the most probable Viterbi path. We call
this setting ?SemEval? because the SemEval-2007
competition (Pradhan et al, 2007) was performed
using this configuration.
Being aware that, in a real scenario, the sense in-
formation will not be available, we devised the sec-
ond setting (?CoNLL?), where the hand-annotated
verb sense information was discarded. This is the
setting used in the CoNLL 2005 shared task (Car-
reras and Ma`rquez, 2005).
The results for the first setting are shown in the
?SemEval setting? rows of Table 1. The correct,
excess, missed, precision, recall and F1 measures
are reported, as customary. The significance inter-
vals for F1 are also reported. They have been ob-
tained with bootstrap resampling (Noreen, 1989).
F1 scores outside of these intervals are assumed to
be significantly different from the related F1 score
(p < 0.05). The results for PropBank are slightly
better, which is reasonable, as the number of labels
that the classifier has to learn in the case of VerbNet
should make the task harder. In fact, given the small
difference, one could think that VerbNet labels, be-
ing more numerous, are easier to learn, perhaps be-
cause they are more consistent across verbs.
In the second setting (?CoNLL setting? row in
the same table) the PropBank classifier degrades
slightly, but the difference is not statistically signif-
icant. On the contrary, the drop of 1.6 points for
VerbNet is significant, and shows greater sensitivity
to the absence of the sense information for verbs.
One possible reason could be that the VerbNet clas-
sifier is more dependant on the argument filter (i.e.,
the 5th constraint in Section 3.2, which only allows
roles that occur in the verbal frameset) used in the
Viterbi search, and lacking the sense information
makes the filter less useful. In fact, we have attested
that the 5th constrain discard more than 60% of the
possible candidates for VerbNet, making the task of
the classifier easier.
In order to test this hypothesis, we run the CoNLL
setting with the 5th constraint disabled (that is, al-
lowing any argument). The results in the ?CoNLL
setting (no 5th)? rows of Table 1 show that the drop
for PropBank is negligible and not significant, while
the drop for VerbNet is more important, and statisti-
cally significant.
Another view of the data is obtained if we com-
pute the F1 scores for core arguments and adjuncts
separately (last two columns in Table 1). The per-
formance drop for PropBank in the first three rows
is equally distributed on both core arguments and ad-
juncts. On the contrary, the drop for VerbNet roles
is more acute in core arguments (3.7 points), while
adjuncts with the 5th constraint disabled get results
close to the SemEval setting. These results confirm
that the information in the verbal frameset is more
important in VerbNet than in PropBank, as only core
arguments are constrained in the verbal framesets.
The explanation could stem from the fact that cur-
rent SRL systems rely more on syntactic information
than pure semantic knowledge. While PropBank ar-
guments Arg0?5 are easier to distinguish on syntac-
tic grounds alone, it seems quite difficult to distin-
guish among roles like Theme and Topic unless we
have access to the specific verbal frameset. This cor-
responds nicely with the performance drop for Verb-
Net when there is less information about the verb in
the algorithm (i.e., sense or frameset).
We further analyzed the results by looking at each
of the individual core arguments and adjuncts. Ta-
ble 2 shows these results on the CoNLL setting. The
performance for the most frequent roles is similar
553
PropBank
Experiment correct excess missed precision recall F1 F1 core F1 adj.
SemEval setting 6,022 1,378 1,722 81.38 77.76 79.53 ?0.9 82.25 72.48
CoNLL setting 5,977 1,424 1,767 80.76 77.18 78.93 ?0.9 81.64 71.90
CoNLL setting (no 5th) 5,972 1,434 1,772 80.64 77.12 78.84 ?0.9 81.49 71.50
No verbal features 5,557 1,828 2,187 75.25 71.76 73.46 ?1.0 74.87 70.11
Unseen verbs 267 89 106 75.00 71.58 73.25 ?4.0 76.21 64.92
VerbNet
Experiment correct excess missed precision recall F1 F1 core F1 adj.
SemEval setting 5,927 1,409 1,817 80.79 76.54 78.61 ?0.9 81.28 71.83
CoNLL setting 5,816 1,548 1,928 78.98 75.10 76.99 ?0.9 79.44 70.20
CoNLL setting (no 5th) 5,746 1,669 1,998 77.49 74.20 75.81 ?0.9 77.60 71.67
No verbal features 4,679 2,724 3,065 63.20 60.42 61.78 ?0.9 59.19 69.95
Unseen verbs 207 136 166 60.35 55.50 57.82 ?4.3 55.04 63.41
Table 1: Basic results using PropBank (top) and VerbNet (bottom) role sets on different settings.
for both. Arg0 gets 88.49, while Agent and Expe-
riencer get 87.31 and 87.76 respectively. Arg2 gets
79.91, but there is more variation on Theme, Topic
and Patient (which get 75.46, 85.70 and 78.64 re-
spectively).
Finally, we grouped the results according to the
frequency of the verbs in the training data. Table 3
shows that both PropBank and VerbNet get decreas-
ing results for less frequent verbs. PropBank gets
better results in all frequency ranges, except for the
most frequent, which contains a single verb (say).
Overall, the results on this section point out at the
weaknesses of the VerbNet role set regarding robust-
ness and generalization. The next sections examine
further its behavior.
4.1 Generalization to Unseen Predicates
In principle, the PropBank core roles (Arg0?4) get
a different interpretation depending of the verb, that
is, the meaning of each of the roles is described sepa-
rately for each verb in the PropBank framesets. Still,
the annotation criteria used with PropBank tried to
make the two main roles (Arg0 and Arg1, which ac-
count for most of the occurrences) consistent across
verbs. On the contrary, in VerbNet al roles are com-
pletely independent of the verb, in the sense that the
interpretation of the role does not vary across verbs.
But, at the same time, each verbal entry lists the pos-
sible roles it accepts, and the combinations allowed.
This experiment tests the sensitivity of the two ap-
proaches when the SRL system encounters a verb
which does not occur in the training data. In prin-
ciple, we would expect the VerbNet semantic la-
bels, which are more independent across verbs, to be
more robust at tagging new predicates. It is worth
noting that this is a realistic scenario, even for the
verb-specific PropBank labels. Predicates which do
not occur in the training data, but do have a Prop-
Bank lexicon entry, could appear quite often in the
text to be analyzed.
For this experiment, we artificially created a test
set for unseen verbs. We chose 50 verbs at random,
and split them into 40 verbs for training and 10 for
testing (yielding 13,146 occurrences for training and
2,723 occurrences for testing; see Table 4).
The results obtained after training and testing the
classifier are shown in the last rows in Table 1. Note
that they are not directly comparable to the other re-
sults mentioned so far, as the train and test sets are
smaller. Figures indicate that the performance of the
PropBank argument classifier is considerably higher
than the VerbNet classifier, with a ?15 point gap.
This experiment shows that lacking any informa-
tion about verbal head, the classifier has a hard time
to distinguish among VerbNet roles. In order to con-
firm this, we performed the following experiment.
4.2 Sensitivity to Verb-dependent Features
In this experiment we want to test the sensitivity of
the role sets when the classifier does not have any in-
formation of the verb predicate. We removed from
the training and testing data all the features which
make any reference to the verb, including, among
others: the surface form, lemma and POS of the
verb, and all the combined features that include the
verb form (please, refer to (Zapirain et al, 2007) for
a complete description of the feature set).
The results are shown in the ?No verbal features?
554
CoNLL setting No verb features
PBank VNet PBank VNet
corr. F1 corr. F1 F1 F1
Overall 5977 78.93 5816 76.99 73.46 61.78
Arg0 1919 88.49 84.02
Arg1 2240 79.81 73.29
Arg2 303 65.44 48.58
Arg3 10 52.63 14.29
Actor1 44 85.44 0.00
Actor2 10 71.43 25.00
Agent 1603 87.31 77.21
Attribut. 25 71.43 50.79
Cause 51 62.20 5.61
Experien. 215 87.76 86.69
Location 31 64.58 25.00
Patient1 38 67.86 5.71
Patient 208 78.64 25.06
Patient2 21 67.74 43.33
Predicate 83 62.88 28.69
Product 44 61.97 2.44
Recipient 85 79.81 62.73
Source 29 60.42 30.95
Stimulus 39 63.93 13.70
Theme 1021 75.46 52.14
Theme1 20 57.14 4.44
Theme2 21 70.00 23.53
Topic 683 85.70 73.58
ADV 132 53.44 129 52.12 52.67 53.31
CAU 13 53.06 13 52.00 53.06 45.83
DIR 22 53.01 27 56.84 40.00 46.34
DIS 133 77.78 137 79.42 77.25 78.34
LOC 126 61.76 126 61.02 59.56 57.34
MNR 109 58.29 111 54.81 52.99 51.49
MOD 249 96.14 248 95.75 96.12 95.57
NEG 124 98.41 124 98.80 98.41 98.01
PNC 26 44.07 29 44.62 38.33 41.79
TMP 453 75.00 450 73.71 73.06 73.89
Table 2: Detailed results on the CoNLL setting. Refer-
ence arguments and verbs have been omitted for brevity,
as well as those with less than 10 occ. The last two
columns refer to the results on the CoNLL setting with
no verb features.
Freq. PBank VNet Freq. PBank VNet
0-50 74,21 71,11 500-900 77,97 75,77
50-100 74,79 71,83 > 900 91,83 92,23
100-500 77,16 75,41
Table 3: F1 results split according to the frequency of the
verb in the training data.
Train affect, announce, ask, attempt, avoid, believe, build, care,
cause, claim, complain, complete, contribute, describe,
disclose, enjoy, estimate, examine, exist, explain, express,
feel, fix, grant, hope, join, maintain, negotiate, occur,
prepare, promise, propose, purchase, recall, receive,
regard, remember, remove, replace, say
Test allow, approve, buy, find, improve, kill, produce, prove,
report, rush
Table 4: Verbs used in the unseen verb experiment
rows of Table 1. The performance drops more than
5 points in PropBank, but the drop for VerbNet is
dramatic, with more than 15 points.
A closer look at the detailed role-by-role perfor-
mances can be done if we compare the F1 rows in the
CoNLL setting and in the ?no verb features? setting
in Table 2. Those results show that both Arg0 and
Arg1 are quite robust to the lack of target verb in-
formation, while Arg2 and Arg3 get more affected.
Given the relatively low number of Arg2 and Arg3
arguments, their performance drop does not affect
so much the overall PropBank performance. In the
case of VerbNet, the picture is very different. Focus-
ing on the most frequent roles first, while the perfor-
mance drop for Experiencer, Agent and Topic is of
1, 10 and 12 points respectively, the other roles get
very heavy losses (e.g. Theme and Patient drop 23
and 50 points), and the rest of roles are barely found.
It is worth noting that the adjunct labels get very
similar performances in both PropBank and Verb-
Net cases. In fact, Table 1 in the last two rows shows
very clearly that the performance drop is caused by
the core arguments.
The better robustness of the PropBank roles can
be explained by the fact that, when creating Prop-
Bank, the human PropBank annotators tried to be
consistent when tagging Arg0 and Arg1 across
verbs. We also think that both Arg0 and Arg1 can
be detected quite well relying on unlexicalized syn-
tactic features only, that is, not knowing which are
the verbal and nominal heads. On the other hand,
distinguishing between Arg2?4 is more dependant
on the subcategorization frame of the verb, and thus
more sensitive to the lack of verbal information.
In the case of VerbNet, the more fine-grained dis-
tinction among roles seems to depend more on the
meaning of the predicate. For instance, distinguish-
ing between Agent?Experiencer, or Theme?Topic?
Patient. The lack of the verbal head makes it much
more difficult to distinguish among those roles. The
same phenomena can be observed among the roles
not typically realized as Subject or Object such as
Recipient, Source, Product, or Stimulus.
5 Mapping into VerbNet Thematic Roles
As mentioned in the introduction, the interpretation
of PropBank roles depends on the verb, and that
555
Test on WSJ all core adj.
PropBank to VerbNet (hand) 79.17 ?0.9 81.77 72.50
VerbNet (SemEval setting) 78.61 ?0.9 81.28 71.84
PropBank to VerbNet (MF) 77.15 ?0.9 79.09 71.90
VerbNet (CoNLL setting) 76.99 ?0.9 79.44 70.88
Test on Brown
PropBank to VerbNet (MF) 64.79 ?1.0 68.93 55.94
VerbNet (CoNLL setting) 62.87 ?1.0 67.07 54.69
Table 5: Results on VerbNet roles using two different
strategies. Topmost 4 rows for the usual test set (WSJ),
and the 2 rows below for the Brown test set.
makes them less suitable for NLP applications. On
the other hand, VerbNet roles have a direct inter-
pretation. In this section, we test the performance
of two different approaches to tag input sentences
with VerbNet roles: (1) train on corpora tagged with
VerbNet, and tag the input directly; (2) train on cor-
pora tagged with PropBank, tag the input with Prop-
Bank roles, and use a PropBank to VerbNet mapping
to output VerbNet roles.
The results for the first approach are already avail-
able (cf. Table 1). For the second approach, we
just need to map PropBank roles into VerbNet roles
using SemLink (Loper et al, 2007). We devised
two experiments. In the first one we use the hand-
annotated verb class in the test set. For each predi-
cate we translate PropBank roles into VerbNet roles
making use of the SemLink mapping information
corresponding to that verb lemma and its verbal
class.
For instance, consider an occurrence of allow in a
test sentence. If the occurrence has been manually
annotated with the VerbNet class 29.5, we can use
the following entry in SemLink to add the VerbNet
role Predicate to the argument labeled with Arg1,
and Agent to the Arg0 argument.
<predicate lemma="allow">
<argmap pb-roleset="allow.01" vn-class="29.5">
<role pb-arg="1" vn-theta="Predicate" />
<role pb-arg="0" vn-theta="Agent" />
</argmap>
</predicate>
The results obtained using the hand-annotated
VerbNet classes (and the SemEval setting for Prop-
Bank), are shown in the first row of Table 5. If we
compare these results to those obtained by VerbNet
in the SemEval setting (second row of Table 5), they
are 0.5 points better, but the difference is not statis-
tically significant.
experiment corr. F1
Grouped (CoNLL Setting) 5,951 78.11?0.9
PropBank to VerbNet to Grouped 5,970 78.21?0.9
Table 6: Results for VerbNet grouping experiments.
In a second experiment, we discarded the sense
annotations from the dataset, and tried to predict the
VerbNet class of the target verb using the most fre-
quent class for the verb in the training data. Sur-
prisingly, the accuracy of choosing the most fre-
quent class is 97%. In the case of allow the most
frequent class is 29.5, so we would use the same
SemLink entry as above. The third row in Table 5
shows the results using the most frequent VerbNet
class (and the CoNLL setting for PropBank). The
performance drop compared to the use of the hand-
annotated VerbNet class is of 2 points and statisti-
cally significant, and 0.2 points above the results ob-
tained using VerbNet directly on the same conditions
(fourth row of the same Table).
The last two rows in table 5 show the results when
testing on the the Brown Corpus. In this case, the
difference is larger, 1.9 points, and statistically sig-
nificant in favor of the mapping approach. These
results show that VerbNet roles are less robust to
domain shifts. The performance drop when mov-
ing to an out?of?domain corpus is consistent with
previously published results (Carreras and Ma`rquez,
2005).
5.1 Grouping experiments
VerbNet roles are more numerous than PropBank
roles, and that, in itself, could cause a drop in per-
formance. Motivated by the results in (Yi et al,
2007), we grouped the 23 VerbNet roles in 7 coarser
role groups. Note that their groupings are focused
on the roles which map to PropBank Arg2. In our
case we are interested in a more general grouping
which covers all VerbNet roles, so we added two
additional groups (Agent-Experiencer and Theme-
Topic-Patient). We re-tagged the roles in the datasets
with those groups, and then trained and tested our
SRL system on those grouped labels. The results
are shown in the first row of Table 6. In order to
judge if our groupings are easier to learn, we can
see that he performance gain with respect to the un-
grouped roles (fourth row of Table 5) is small (76.99
556
vs. 78.11) but significant. But if we compare them
to the results of the PropBank to VerbNet mapping,
where we simply substitute the fine-grained roles by
their corresponding groups, we see that they still lag
behind (second row in Table 6).
Although one could argue that better motivated
groupings could be proposed, these results indicate
that the larger number of VerbNet roles does not ex-
plain in itself the performance difference when com-
pared to PropBank.
6 Related Work
As far as we know, there are only two other works
performing comparisons of alternative role sets on
a common test data. Gildea and Jurafsky (2002)
mapped FrameNet frame elements into a set of ab-
stract thematic roles (i.e., more general roles such as
Agent, Theme, Location), and concluded that their
system could use these thematic roles without degra-
dation in performance.
(Yi et al, 2007) is a closely related work. They
also compare PropBank and VerbNet role sets, but
they focus on the performance of Arg2. They show
that splitting Arg2 instances into subgroups based on
VerbNet thematic roles improves the performance of
the PropBank-based classifier. Their claim is that
since VerbNet uses argument labels that are more
consistent across verbs, they would provide more
consistent training instances which would general-
ize better, especially to new verbs and genres. In fact
they get small improvements in PropBank (WSJ)
and a large improvement when testing on Brown.
An important remark is that Yi et al use a com-
bination of grouped VerbNet roles (for Arg2) and
PropBank roles (for the rest of arguments). In con-
trast, our study compares both role sets as they stand,
without modifications or mixing. Another difference
is that they compare the systems based on the Prop-
Bank roles ?by mapping the output VerbNet labels
back to PropBank Arg2? while in our case we de-
cided to do just the contrary (i.e., mapping PropBank
output into VerbNet labels and compare there). As
we already said, we think that VerbNet?based labels
can be more useful for NLP applications, so our tar-
get is to have a SRL system that provides VerbNet
annotations. While not in direct contradiction, both
studies show different angles of the complex relation
between the two role sets.
7 Conclusion and Future work
In this paper we have presented a study of the per-
formance of a state-of-the-art SRL system trained
on two alternative codifications of roles (PropBank
and VerbNet) and some particular settings, e.g., in-
cluding/excluding verb?specific information in fea-
tures, labeling of infrequent and unseen verb pred-
icates, and domain shifts. We observed that Prop-
Bank labeling is more robust in all previous experi-
mental conditions, showing less performance drops
than VerbNet labels.
Assuming that application-based scenarios would
prefer dealing with general thematic role labels, we
explore the best way to label a text with VerbNet
thematic roles, namely, by training directly on Verb-
Net roles or by using the PropBank SRL system
and performing a posterior mapping into thematic
roles. While results are similar and not statistically
significant in the WSJ test set, when testing on the
Brown out?of?domain test set the difference in favor
of PropBank plus mapping step is statistically signif-
icant. We also tried to map the fine-grained VerbNet
roles into coarser roles, but it did not yield better re-
sults than the mapping from PropBank roles. As a
side-product, we show that a simple most frequent
sense disambiguation strategy for verbs is sufficient
to provide excellent results in the PropBank to Verb-
Net mapping.
Regarding future work, we would like to explore
ways to improve the performance on VerbNet roles,
perhaps using selectional preferences. We also want
to work on the adaptation to new domains of both
roles sets.
Acknowledgements
We are grateful to Martha Palmer and Edward Loper
for kindly providing us with the SemLink map-
pings. This work has been partially funded by
the Basque Government (IT-397-07) and by the
Ministry of Education (KNOW TIN2006-15049,
OpenMT TIN2006-15307-C03-02). Ben?at is sup-
ported by a PhD grant from the University of the
Basque Country.
557
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Ido Dagan and Daniel Gildea, editors, Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning (CoNLL-2005), pages 152?
164, Ann Arbor, Michigan, USA, June. Association
for Computational Linguistics.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class based construction of a verb lexicon. In
Proceedings of the 17th National Conference on Arti-
ficial Intelligence (AAAI-2000), Austin, TX, July.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press, Chicago.
Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between prop-
bank and verbnet. In Proceedings of the 7th In-
ternational Workshop on Computational Linguistics,
Tilburg, the Netherlands.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: annotating predicate argument structure. In
HLT ?94: Proceedings of the workshop on Human
Language Technology, pages 114?119, Morristown,
NJ, USA. Association for Computational Linguistics.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
105.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, SRL and all words. In Proceed-
ings of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 87?92, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Szu-Ting Yi, Edward Loper, and Martha Palmer. 2007.
Can semantic roles generalize across genres? In Pro-
ceedings of the Human Language Technology Con-
ferences/North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT/NAACL-2007).
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez. 2007.
Sequential SRL Using Selectional Preferences. An
Approach with Maximum Entropy Markov Models. In
Proceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 354?357.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez. 2008.
A Preliminary Study on the Robustness and General-
ization of Role Sets for Semantic Role Labeling. In
Proceedings of the 9th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing-2008), pages 219?230, Haifa, Israel,
February.
558
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 73?76,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Generalizing over Lexical Features:
Selectional Preferences for Semantic Role Classification
Be
?
nat Zapirain, Eneko Agirre
Ixa Taldea
University of the Basque Country
Donostia, Basque Country
{benat.zapirain,e.agirre}@ehu.es
Llu??s M
`
arquez
TALP Research Center
Technical University of Catalonia
Barcelona, Catalonia
lluism@lsi.upc.edu
Abstract
This paper explores methods to allevi-
ate the effect of lexical sparseness in the
classification of verbal arguments. We
show how automatically generated selec-
tional preferences are able to generalize
and perform better than lexical features in
a large dataset for semantic role classifi-
cation. The best results are obtained with
a novel second-order distributional simi-
larity measure, and the positive effect is
specially relevant for out-of-domain data.
Our findings suggest that selectional pref-
erences have potential for improving a full
system for Semantic Role Labeling.
1 Introduction
Semantic Role Labeling (SRL) systems usually
approach the problem as a sequence of two sub-
tasks: argument identification and classification.
While the former is mostly a syntactic task, the
latter requires semantic knowledge to be taken
into account. Current systems capture semantics
through lexicalized features on the predicate and
the head word of the argument to be classified.
Since lexical features tend to be sparse (especially
when the training corpus is small) SRL systems
are prone to overfit the training data and general-
ize poorly to new corpora.
This work explores the usefulness of selectional
preferences to alleviate the lexical dependence of
SRL systems. Selectional preferences introduce
semantic generalizations on the type of arguments
preferred by the predicates. Therefore, they are
expected to improve generalization on infrequent
and unknown words, and increase the discrimina-
tive power of the argument classifiers.
For instance, consider these two sentences:
JFK was assassinated (in Dallas)
Location
JFK was assassinated (in November)
Temporal
Both share syntactic and argument structure, so
the lexical features (i.e., the words ?Dallas? and
?November?) represent the most important knowl-
edge to discriminate between the two different ad-
junct roles. The problem is that, in new text,
one may encounter similar expressions with new
words like Texas or Autumn.
We propose a concrete classification problem as
our main evaluation setting for the acquired selec-
tional preferences: given a verb occurrence and
a nominal head word of a constituent dependant
on that verb, assign the most plausible role to the
head word according to the selectional preference
model. This problem is directly connected to ar-
gument classification in SRL, but we have iso-
lated the evaluation from the complete SRL task.
This first step allows us to analyze the potential
of selectional preferences as a source of seman-
tic knowledge for discriminating among different
role labels. Ongoing work is devoted to the inte-
gration of selectional preference?derived features
in a complete SRL system.
2 Related Work
Automatic acquisition of selectional preferences
is a relatively old topic, and will mention the
most relevant references. Resnik (1993) proposed
to model selectional preferences using semantic
classes from WordNet in order to tackle ambiguity
issues in syntax (noun-compounds, coordination,
PP-attachment).
Brockman and Lapata (2003) compared sev-
eral class-based models (including Resnik?s se-
lectional preferences) on a syntactic plausibility
judgement task for German. The models re-
turn weights for (verb, syntactic function, noun)
triples, and the correlation with human plausibil-
ity judgement is used for evaluation. Resnik?s
selectional preference scored best among class-
based methods, but it performed equal to a simple,
purely lexical, conditional probability model.
73
Distributional similarity has also been used to
tackle syntactic ambiguity. Pantel and Lin (2000)
obtained very good results using the distributional
similarity measure defined by Lin (1998).
The application of selectional preferences to se-
mantic roles (as opposed to syntactic functions)
is more recent. Gildea and Jurafsky (2002) is
the only one applying selectional preferences in
a real SRL task. They used distributional clus-
tering and WordNet-based techniques on a SRL
task on FrameNet roles. They report a very small
improvement of the overall performance when us-
ing distributional clustering techniques. In this pa-
per we present complementary experiments, with
a different role set and annotated corpus (Prop-
Bank), a wider range of selectional preference
models, and the analysis of out-of-domain results.
Other papers applying semantic preferences
in the context of semantic roles, rely on the
evaluation on pseudo tasks or human plausibil-
ity judgments. In (Erk, 2007) a distributional
similarity?based model for selectional preferences
is introduced, reminiscent of that of Pantel and
Lin (2000). The results over 100 frame-specific
roles showed that distributional similarities get
smaller error rates than Resnik and EM, with Lin?s
formula having the smallest error rate. Moreover,
coverage of distributional similarities and Resnik
are rather low. Our distributional model for selec-
tional preferences follows her formalization.
Currently, there are several models of distri-
butional similarity that could be used for selec-
tional preferences. More recently, Pad?o and Lap-
ata (2007) presented a study of several parameters
that define a broad family of distributional similar-
ity models, including publicly available software.
Our paper tests similar techniques to those pre-
sented above, but we evaluate selectional prefer-
ence models in a setting directly related to SR
classification, i.e., given a selectional preference
model for a verb we find the role which fits best
for a given head word. The problem is indeed
qualitatively different: we do not have to choose
among the head words competing for a role (as
in the papers above) but among selectional prefer-
ences competing for a head word.
3 Selectional Preference Models
In this section we present all the variants for ac-
quiring selectional preferences used in our study,
and how we apply them to the SR classification.
WordNet-based SP models: we use Resnik?s se-
lectional preference model.
Distributional SP models: Given the availabil-
ity of publicly available resources for distribu-
tional similarity, we used 1) a ready-made the-
saurus (Lin, 1998), and 2) software (Pad?o and La-
pata, 2007) which we run on the British National
Corpus (BNC).
In the first case, Lin constructed his thesaurus
based on his own similarity formula run over a
large parsed corpus comprising journalism texts.
The thesaurus lists, for each word, the most sim-
ilar words, with their weight. In order to get the
similarity for two words, we could check the entry
in the thesaurus for either word. But given that
the thesaurus is not symmetric, we take the av-
erage of both similarities. We will refer to this
similarity measure as sim
th
lin
. Another option is
to use second-order similarity, where we compute
the similarity of two words using the entries in the
thesaurus, either using the cosine or Jaccard mea-
sures. We will refer to these similarity measures
as sim
th2
jac
and sim
th2
cos
hereinafter.
For the second case, we tried the optimal pa-
rameters as described in (Pad?o and Lapata, 2007,
p. 179): word-based space, medium context, log-
likelihood association, and 2,000 basis elements.
We tested Jaccard, cosine and Lin?s measure (Lin,
1998) for similarity, yielding sim
jac
, sim
cos
and
sim
lin
, respectively.
3.1 Role Classification with SP Models
Given a target sentence where a predicate and sev-
eral potential argument and adjunct head words
occur, the goal is to assign a role label to each of
the head words. The classification of candidate
head words is performed independently of each
other.
Since we want to evaluate the ability of selec-
tional preference models to discriminate among
different roles, this is the only knowledge that will
be used to perform classification (avoiding the in-
clusion of any other feature commonly used in
SRL). Thus, for each head word, we will simply
select the role (r) of the predicate (p) which fits
best the head word (w). This selection rule is for-
malized as:
R(p, w) = argmax
r?Roles(p)
S(p, r, w)
being S(p, r, w) the prediction of the selectional
preference model, which can be instantiated with
all the variants mentioned above.
74
For the sake of comparison we also define a lex-
ical baseline model, which will determine the con-
tribution of lexical features in argument classifica-
tion. For a test pair (p, w) the model returns the
role under which the head word occurred most of-
ten in the training data given the predicate.
4 Experimental Setting
The data used in this work is the benchmark cor-
pus provided by the CoNLL-2005 shared task on
SRL (Carreras and M`arquez, 2005). The dataset,
of over 1 million tokens, comprises PropBank sec-
tions 02-21 for training, and sections 24 and 23 for
development and test, respectively. In these ex-
periments, NEG, DIS and MOD arguments have
been discarded because, apart from not being con-
sidered ?pure? adjunct roles, the selectional pref-
erences implemented in this study are not able to
deal with non-nominal argument heads.
The predicate?rol?head (p, r, w) triples for gen-
eralizing the selectional preferences are extracted
from the arguments of the training set, yield-
ing 71,240 triples, from which 5,587 different
predicate-role selectional preferences (p, r) are
derived by instantiating the different models in
Section 3.
Selectional preferences are then used, to predict
the corresponding roles of the (p, w) pairs from
the test corpora. The test set contains 4,134 pairs
(covering 505 different predicates) to be classified
into the appropriate role label. In order to study
the behavior on out-of-domain data, we also tested
on the PropBanked part of the Brown corpus. This
corpus contains 2,932 (p, w) pairs covering 491
different predicates.
The performance of each selectional preference
model is evaluated by calculating the standard pre-
cision, recall and F
1
measures. It is worth men-
tioning that none of the models is able to predict
the role when facing an unknown head word. This
happens more often with WordNet based models,
which have a lower word coverage compared to
distributional similarity?based models.
5 Results and Discussion
The results are presented in Table 1. The lexi-
cal row corresponds to the baseline lexical match
method. The following row corresponds to the
WordNet-based selectional preference model. The
distributional models follow, including the results
obtained by the three similarity formulas on the
prec. rec. F
1
prec. recall F
1
lexical .779 .349 .482 .663 .059 .108
res .589 .495 .537 .505 .379 .433
sim
Jac
.573 .564 .569 .481 .452 .466
sim
cos
.607 .598 .602 .507 .476 .491
sim
Lin
.580 .560 .570 .500 .470 .485
sim
th
Lin
.635 .625 .630 .494 .464 .478
sim
th2
Jac
.657 .646 .651 .531 .499 .515
sim
th2
cos
.654 .644 .649 .531 .499 .515
Table 1: Results for WSJ test (left), and Brown
test (right)
co-occurrences extracted from the BNC (sim
Jac
,
sim
cos
sim
Lin
), and the results obtained when
using Lin?s thesaurus directly (sim
th
Lin
) and as a
second-order vector (sim
th2
Jac
and sim
th2
cos
).
As expected, the lexical baseline attains very
high precision in all datasets, which underscores
the importance of the lexical head word features
in argument classification. The recall is quite
low, specially in Brown, confirming and extend-
ing (Pradhan et al, 2008), which also reports sim-
ilar performance drops when doing argument clas-
sification on out-of-domain data.
One of the main goals of our experiments is to
overcome the data sparseness of lexical features
both on in-domain and out-of-domain data. All
our selectional preference models improve over
the lexical matching baseline in recall, up to 30
absolute percentage points in the WSJ test dataset
and 44 absolute percentage points in the Brown
corpus. This comes at the cost of reduced preci-
sion, but the overall F-score shows that all selec-
tional preference models improve over the base-
line, with up to 17 absolute percentage points
on the WSJ datasets and 41 absolute percentage
points on the Brown dataset. The results, thus,
show that selectional preferences are indeed alle-
viating the lexical sparseness problem.
As an example, consider the following head
words of potential arguments of the verb wear
found in the test set: doctor, men, tie, shoe. None
of these nouns occurred as heads of arguments of
wear in the training data, and thus the lexical fea-
ture would be unable to predict any role for them.
Using selectional preferences, we successfully as-
signed the Arg0 role to doctor and men, and the
Arg1 role to tie and shoe.
Regarding the selectional preference variants,
WordNet-based and first-order distributional sim-
ilarity models attain similar levels of precision,
but the former are clearly worse on recall and F
1
.
75
The performance loss on recall can be explained
by the worse lexical coverage of WordNet when
compared to automatically generated thesauri. Ex-
amples of words missing in WordNet include ab-
breviations (e.g., Inc., Corp.) and brand names
(e.g., Texaco, Sony). The second-order distribu-
tional similarity measures perform best overall,
both in precision and recall. As far as we know,
it is the first time that these models are applied to
selectional preference modeling, and they prove to
be a strong alternative to first-order models. The
relative performance of the methods is consistent
across the two datasets, stressing the robustness of
all methods used.
Regarding the use of similarity software (Pad?o
and Lapata, 2007) on the BNC vs. the use of
Lin?s ready-made thesaurus, both seem to perform
similarly, as exemplified by the similar results of
sim
Lin
and sim
th
Lin
. The fact that the former per-
formed better on the Brown data, and worse on the
WSJ data could be related to the different corpora
used to compute the co-occurrence, balanced cor-
pus and journalism texts respectively. This could
be an indication of the potential of distributional
thesauri to adapt to the target domain.
Regarding the similarity metrics, the cosine
seems to perform consistently better for first-order
distributional similarity, while Jaccard provided
slightly better results for second-order similarity.
The best overall performance was for second-
order similarity, also using the cosine. Given
the computational complexity involved in build-
ing a complete thesaurus based on the similarity
software, we used the ready-made thesaurus of
Lin, but could not try the second-order version on
BNC.
6 Conclusions and Future Work
We have empirically shown how automatically
generated selectional preferences, using WordNet
and distributional similarity measures, are able to
effectively generalize lexical features and, thus,
improve classification performance in a large-
scale argument classification task on the CoNLL-
2005 dataset. The experiments show substantial
gains on recall and F
1
compared to lexical match-
ing, both on the in-domain WSJ test and, espe-
cially, on the out-of-domain Brown test.
Alternative selectional models were studied and
compared. WordNet-based models attain good
levels of precision but lower recall than distribu-
tional similarity methods. A new second-order
similarity method proposed in this paper attains
the best results overall in all datasets.
The evidence gathered in this paper suggests
that using semantic knowledge in the form of se-
lectional preferences has a high potential for im-
proving the results of a full system for SRL, spe-
cially when training data is scarce or when applied
to out-of-domain corpora.
Current efforts are devoted to study the integra-
tion of the selectional preference models presented
in this paper in a in-house SRL system. We are
particularly interested in domain adaptation, and
whether distributional similarities can profit from
domain corpora for better performance.
Acknowledgments
This work has been partially funded by the EU Commis-
sion (project KYOTO ICT-2007-211423) and Spanish Re-
search Department (project KNOW TIN2006-15049-C03-
01). Be?nat enjoys a PhD grant from the University of the
Basque Country.
References
Carsten Brockmann and Mirella Lapata. 2003. Evaluating
and combining approaches to selectional preference ac-
quisition. In Proceedings of the 10th Conference of the
European Chapter of the ACL, pages 27?34.
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic role labeling. In
Proceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005), pages 152?
164, Ann Arbor, MI, USA.
Katrin Erk. 2007. A simple, similarity-based model for se-
lectional preferences. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguistics,
pages 216?223, Prague, Czech Republic.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?288.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL, pages 768?774.
Sebastian Pad?o and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Computa-
tional Linguistics, 33(2):161?199, June.
Patrick Pantel and Dekang Lin. 2000. An unsupervised ap-
proach to prepositional phrase attachment using contex-
tually similar words. In Proceedings of the 38th Annual
Conference of the ACL, pages 101?108.
S. Pradhan, W. Ward, and J. H. Martin. 2008. Towards robust
semantic role labeling. Computational Linguistics, 34(2).
Philip Resnik. 1993. Semantic classes and syntactic ambigu-
ity. In Proceedings of the workshop on Human Language
Technology, pages 278?283, Morristown, NJ, USA.
76
One Sense per Collocation and Genre/Topic Variations 
David Martinez 
IXA NLP Group 
University of the Basque Country 
649 pk. 20.080 
Donostia. Spain 
jibmaird@si.ehu.es 
Eneko Agirre 
IXA NLP Group 
University of the Basque Country 
649 pk. 20.080 
Donostia. Spain 
eneko@si.ehu.es 
Abstract 
This paper revisits the one sense per 
collocation hypothesis using fine-grained 
sense distinctions and two different corpora. 
We show that the hypothesis i  weaker for 
fine-grained sense distinctions (70% vs. 
99% reported earlier on 2-way ambiguities). 
We also show that one sense per collocation 
does hold across corpora, but that 
collocations vary from one corpus to the 
other, following genre and topic variations. 
This explains the low results when 
performing word sense disambiguation 
across corpora. In fact, we demonstrate hat 
when two independent corpora share a 
related genre/topic, the word sense 
disambiguation results would be better. 
Future work on word sense disambiguation 
will have to take into account genre and 
topic as important parameters on their 
models. 
Introduction 
In the early nineties two famous papers claimed 
that the behavior of word senses in texts adhered 
to two principles: one sense per discourse (Gale 
et al, 1992) and one sense per collocation 
(Yarowsky, 1993). 
These hypotheses were shown to hold for 
some particular corpora (totaling 380 Mwords) 
on words with 2-way ambiguity. The word 
sense distinctions came from different sources 
(translations into French, homophones, 
homographs, pseudo-words, etc.), but no 
dictionary or lexical resource was linked to 
them. In the case of the one sense per 
collocation paper, several corpora were used, 
but nothing is said on whether the collocations 
hold across corpora. 
Since the papers were published, word sense 
disambiguation has moved to deal with fine- 
grained sense distinctions from widely 
recognized semantic lexical resources; 
ontologies like Sensus, Cyc, EDR, WordNet, 
EuroWordNet, etc. or machine-readable 
dictionaries like OALDC, Webster's, LDOCE, 
etc. This is due, in part, to the availability of 
public hand-tagged material, e.g. SemCor 
(Miller et al, 1993) and the DSO collection (Ng 
& Lee, 1996). We think that the old hypotheses 
should be tested under the conditions of this 
newly available data. This paper focuses on the 
DSO collection, which was tagged with 
WordNet senses (Miller et al 1990) and 
comprises sentences extracted from two 
different corpora: the balanced Brown Corpus 
and the Wall Street Journal corpus. 
Krovetz (1998) has shown that the one sense 
per discourse hypothesis does not hold for fine- 
grained senses in SemCor and DSO. His results 
have been confirmed in our own experiments. 
We will therefore concentrate on the one sense 
per collocation hypothesis, considering these 
two questions: 
? Does the collocation hypothesis hold across 
corpora, that is, across genre and topic 
variations (compared to a single corpus, 
probably with little genre and topic 
variations)? 
? Does the collocation hypothesis hold for free- 
grained sense distinctions (compared to 
homograph level granularity)? 
The experimental tools to test the hypothesis 
will be decision lists based on various kinds of 
collocational information. We will compare the 
performance across several corpora (the Brown 
Corpus and Wall Street Journal parts of the 
DSO collection), and also across different 
sections of the Brown Corpus, selected 
according to the genre and topics covered. We 
will also perform a direct comparison, using 
agreement statistics, of the collocations used 
and of the results obtained. 
207 
This study has special significance at this 
point of word sense disambiguation research. A
recent study (Agirre & Martinez, 2000) 
concludes that, for currently available hand- 
tagged data, the precision is limited to around 
70% when tagging all words in a running text. 
In the course of extending available data, the 
efforts to use corpora tagged by independent 
teams of researchers have been shown to fail 
(Ng et al, 1999), as have failed some tuning 
experiments (Escudero et al, 2000), and an 
attempt to use examples automatically acquired 
from the Internet (Agirre & Martinez, 2000). All 
these studies obviated the fact that the examples 
come from different genre and topics. Future 
work that takes into account he conclusions 
drawn in this paper will perhaps be able to 
automatically extend the number of examples 
available and tackle the acquisition problem. 
The paper is organized as follows. The 
resources used and the experimental settings are 
presented first. Section 3 presents the 
collocations considered and Section 4 explains 
how decision lists have been adapted to n-way 
ambiguities. Sections 5 and 6 show the in- 
corpus and cross-corpora experiments, 
respectively. Section 7 discusses the effect of 
drawing training and testing data from the same 
documents. Section 8 evaluates the impact of 
genre and topic variations, which is fiarther 
discussed in Section 9. Finally, Section 10 
presents some conclusions. 
1 Resources used 
The DSO collection (Ng and Lee, 1996) focuses 
on 191 frequent and polysemous words (nouns 
and verbs), and contains around 1,000 sentences 
per word. Overall, there are 112,800 sentences, 
where 192,874 occurrences of the target words 
were hand-tagged with WordNet senses (Miller 
et al, 1990). 
The DSO collection was built with examples 
from the Wall Street Journal (WSJ) and 
Brown Corpus (BC). The Brown Corpus is 
balanced, and the texts are classified according 
some predefined categories (el. Table 1). The 
examples from the Brown Corpus comprise 
78,080 occurrences of word senses, and the 
examples from the WSJ 114,794 occurrences. 
The sentences in the DSO collection were 
tagged with parts of speech using TnT (Brants, 
2000) trained on the Brown Corpus itself. 
A. Press: Reportage 
B. Press: Editorial 
C. Press: Reviews (theatre, books, music, dance) 
D. Religion 
E. Skills and Hobbies 
F. Popular Lore 
G. Belles Lettres, Biography, Memoirs, etc. 
H. Miscellaneous 
J. Learned 
K. General Fiction 
L. Mystery and Detective Fiction 
M. Science Fiction 
N. Adventure and Western Fiction 
P. Romance and Love Story 
R. Humor 
Table 1: List of categories of texts from the 
Brown Corpus, divided into informative prose 
(top) and imaginative prose (bottom). 
1.1 Categories in the Brown Corpus 
and genre/topic variation 
The Brown Corpus manual (Francis & Kucera, 
1964) does not detail the criteria followed to set 
the categories in Table 1: 
The samples represent a wide range of  styles 
and varieties of  prose... The list of main 
categories and their subdivisions was drawn 
up at a conference held at Brown University 
in February 1963. 
These categories have been previously used in 
genre detection experiments (Karlgrcn & 
Cutting, 1994), where each category was used 
as a genre. We think that the categories not only 
reflect genre variations but also topic variations 
(e.g. the Religion category follows topic 
distinctions rather than genre). Nevertheless we 
are aware that some topics can be covered in 
more than one category. Unfortunately there are 
no topically tagged corpus which also have 
word sense tags. We thus speak of genre and 
topic variation, knowing that further analysis 
would be needed to measure the effect of each 
of them. 
2 Experimental setting 
In order to analyze and compare the behavior of 
several kinds of collocations (cf. Section 3), 
Yarowsky (1993) used a measure of entropy as 
well as the results obtained when tagging held- 
out data with the collocations organized as 
decision lists (el. Section 4) .  As Yarowsky 
shows, both measures correlate closely, so we 
208 
only used the experimental results of  decision Word PoS #Senses #Ex. BC #Ex. WSJ 
lists. Age N 5 243 248 
When comparing the performance on Art N 4 200 194 
decision lists trained on two different corpora Body N 9 296 110 
(or sub-corpora) we always take an equal Car N 5 357 1093 
amount of examples per word from each Child N 6 577 484 
corpora. This is done to discard the amount-of- Cost N 3 317 1143 
data factor. Head N 28 432 434 
As usual, we use 10-fold cross-validation Interest N 8 364 1115 
Line N 28 453 880 
when training and testing on the same corpus. Point N 20 442 249 
No significance tests could be found for our State N 6 757 706 
comparison, as training and test sets differ. Thing N 11 621 805 
Because of the large amount of experiments Work N 6 596 825 
involved, we focused on 21 verbs and nouns (el. Become V 4 763 736 
Table 2), selected from previous works (Agirre Fall V 17 221 1227 
& Martinez, 2000; Escudero et al, 2000). Grow V 8 243 731 
Lose V 10 245 935 
Set V 20 925 355 
Speak V 5 210 307 
Strike V 17 159 95 
Tell V 8 740 744 
3 Collocations considered 
For the sake of this work we take a broad 
definition of collocations, which were classified 
in three subsets: local content word collocations, 
local part-of-speech and function-word 
collocations, and global content-word 
collocations. If a more strict linguistic 
perspective was taken, rather than collocations 
we should speak about co-occurrence r lations. 
In fact, only local content word collocations 
would adhere to this narrower view. 
We only considered those collocations that 
could be easily exlracted form a part of speech 
tagged corpus, like word to left, word to right, 
etc. Local content word collocations comprise 
bigrams (word to left, word to right) and 
trigrams (two words to left, two words to right 
and both words to right and left). At least one of 
those words needs to be a content word. Local 
function-word collocations comprise also all 
kinds of bigrams and trigrams, as before, but the 
words need to be function words. Local PoS 
collocations take the Part of Speech of the 
words in the bigrams and trigrams. Finally 
global content word collocations comprise the 
content words around the target word in two 
different contexts: a window of 4 words around 
the target word, and all the words in the 
sentence. Table 3 summarizes the collocations 
used. These collocations have been used in other 
word sense disambiguation research and are also 
referred to as features (Gale et al, 1993; Ng & 
Lee, 1996; Escudero et al, 2000). 
Compared to Yarowsky (1993), who also 
took into account grammatical relations, we 
only share the content-word-to-left and the 
content-word-to-right collocations. 
Table 2: Data for selected words. Part of 
speech, number of senses and number of 
examples m BC and WSJ are shown. 
Local content word collocations 
Word-to-left Content Word 
Word-to-right Content Word 
Two-words-to-left At least one 
Two-words-to-right Content Word 
Word-to-right-and-left 
Local PoS and function word collocations 
Word-to-left PoS Function Word 
Word-to-right PoS Function Word 
Two-words-to-left PoS Both Function Two-words-to-fight PoS Words Word-to-fight-and-left PoS 
Global content word collocations 
Word in Window of 4 Content Word Word in sentence 
Table 3: Kinds of collocations considered 
We did not lemmatize content words, and we 
therefore do take into account he form of the 
target word. For instance, governing body and 
governing bodies are different collocations for 
the sake of this paper. 
4 Adaptation of decision lists to n-way 
ambiguities 
Decision lists as defined in (Yarowsky, 1993; 
1994) are simple means to solve ambiguity 
problems. They have been successfully applied 
to accent restoration, word" sense disambiguation 
209 
and homograph disambiguation (Yarowsky, 
1994; 1995; 1996). In order to build decision 
lists the training examples are processed to 
extract he features (each feature corresponds to
a kind of collocation), which are weighted with 
a log-likelihood measure. The list of all features 
ordered by log-likelihood values constitutes the 
decision list. We adapted the original formula in 
order to accommodate ambiguities higher than 
two: 
. , Pr(sense i I features)  , weight(sensei ,  feature , )  = ~ogt-  ) 
Pr(sense~ l feature , )  
,i=i 
When testing, the decision list is checked in 
order and the feature with highest weight hat is 
present in the test sentence selects the winning 
word sense. For this work we also considered 
negative weights, which were not possible on 
two-way ambiguities. 
The probabilities have been estimated using 
the maximum likelihood estimate, smoothed 
using a simple method: when the denominator 
in the formula is 0 we replace it with 0.1. It is 
not clear how the smoothing technique proposed 
in (Yarowsky, 1993) could be extended to n- 
way ambiguities. 
More details of the implementation can be 
found in (Agirre & Martinez, 2000). 
5 In-corpus experiments: 
collocations are weak  (80%) 
We extracted the collocations in the Brown 
Corpus section of the DSO corpus and, using 
10-fold cross-validation, tagged the same 
corpus. Training and testing examples were thus 
from the same corpus. The same procedure was 
followed for the WSJ part. The results are 
shown in Tables 4 and 5. We can observe the 
following: 
? The best kinds of collocations are local 
content word collocations, especially if two 
words from the context are taken into 
consideration, but the coverage is low. 
Function words to right and left also attain 
remarkable precision. 
? Collocations are stronger in the WSJ, surely 
due to the fact that the BC is balanced, and 
therefore includes more genres and topics. 
This is a first indicator than genre and topic 
variations have to be taken into account. 
? Collocations for fine-gained word-senses are 
sensibly weaker than those reported by 
Yarowsky (1993) for two-way ambiguous 
words. Yarowsky reports 99% precision, 
N V Overall 
Collocations Pr. Cov. Pr. Cov. Pr. Coy. 
Word-to-righ~ .768.254.529.264 1680.258 
Word-to-left .724.185.867.182.775.184 
Two-words-to-righ1.784.191 .623.113.744.163 
Two-words-to-left. 811 . 160.862.179.830.166 
Word-to-right-and-left.820.169.728.129.793.155 
Word-to-righ1.600.457.527.370.577.426 
Word-to-left .545.609.629.472.570.560 
Two-words-to-righ1.638.133.687.084.650.116 
Two-words-to-left .600.140.657.108.617.128 
Word-to-right-and-left.721.220.694.138.714.191 
PoS-to-righ1.490.993.488.993.489.993 
PoS -to-left .465.991 .584.994.508.992 
Two- PoS -to-righ1.526.918.534.879.529.904 
Two- PoS -to-left .518.822.614.912.555.854 
PoS -to-right-and-left .555.918.634.891 .583.908 
O~daii~ib:~al;P~g,~.Fiifii~ !622 7o6 i64b:i~00 i629:Ii60 
Word in sentence .611 1.00.572 1.00.597 1.00 
Word in Window of 4.627.979.611.975.622.977 
OVERAM.; : i::/::: i:~ .661i,L00,635I'.00.652:11200 
Table 4: Train on WSJ, tag WSJ. 
N V Overall 
Collocations Pr. Coy. Pr. Cov. Pr. Cov. 
Word-to-right,644.203 4 2.230 .562.212 
Word-to-left,626.124 770.139 .681.129 
Two-words-to-right,657.146 500.103 ,613.131 
Two-words-to-left,740.092 ,819.122 ,774.103 
Word-to-right-and-left.647.088 686.114 .663.098 
Word-to-right 480.503 452.406 ,471.468 
Word-to-leA 414.639 572.527 :,464.599 
Two-words-to-right,520.183 624.113 ,547.158 
Two-words-to-left ,420.131 648.173 ,516.146 
Word-to-right-and-leg 549.238 654.160 ,577.210 
PoS4o-righ~ 340.992 356.992 i,346.992 
PoS -to-left,350.994 483.992 ,398.993 
Two- PoS -to-righ' 406.923 422.876 ,412.906 
Two- PoS -to-lef 396.792 539.897 i,452.829 
PoS -to-right-and-lef ,416.921 545.885 ,461.908 
Word in sentence 545 1.00 !.492 1.00 ,526 1.00 
Word in Window of 4 550.972 1.525.951 ,541.964 
Table 5: Train on BC, tag BC. 
while our highest results do not reach 80%. 
It has to be noted that the test and training 
examples come from the same corpus, which 
means that, for some test cases, there are 
training examples from the same document. In 
somesense we can say that one sense  per  
d i scourse  comes into play. This point will be 
further explored in Section 7. 
210 
1. state -- (the group of people comprising the government ofa sovereign) 
2. state, province 
-- (the territory occupied by one of the constituent administrative districts of a nation) 
3. state, nation, country, land, commonwealth, res publica, body politic 
-- (a politically organized body of people under a single government) 
4. state -- (the way something iswith respect o its main attributes) 
5. Department of  State, State Department, State 
-- (the federal department that sets and maintains foreign policies) 
6. country, state, land, nation -- (the territory occupied by a nation) 
F igure  1: Word senses for state in WordNet 1.6 (6 out of  8 are shown) 
In the rest o f  this paper, only the overall 
results for each subset of  the collocations will be 
shown. We will pay special attention to local- 
content collocations, as they are the strongest, 
and also closer to strict definitions of  
collocation. 
As an example of  the learned collocations 
Table 6 shows some strong local content word 
col locat ions for the noun state, and Figure 1 
shows the word senses of  state (6 out of  the 8 
senses are shown as the rest were not present in 
the corpora). 
6 Cross-corpora experiments: 
one sense per col location in doubt. 
In these experiments we train on the Brown 
Corpus and tag the WSJ corpus and vice versa. 
Tables 7 and 8, when compared to Tables 4 and 
5 show a significant drop in performance (both 
precision and coverage) for all kind of  
collocations (we only show the results for each 
subset of  collocations). For instance, Table 7 
shows a drop in .16 in precision for local 
content collocations when compared to Table 4. 
These results confirm those by (Escudero et 
al. 2000) who conclude that the information 
learned in one corpus is not useful to tag the 
other. 
In order to analyze the reason of  this 
performance degradation, we compared the 
local content-word collocations extracted from 
one corpus and the other. Table 9 shows the 
amount of  collocations extracted from each 
corpus, how many of  the collocations are shared 
on average and how many of  the shared 
collocations are in contradiction. The low 
amount of  collocations shared between both 
corpora could explain the poor figures, but fo r  
some words (e.g. point) there is a worrying 
proportion of  contradicting collocations. 
We inspected some of  the contradicting 
collocations and saw that m all the cases they 
were caused by errors (or at least differing 
Senses 
Collocations Log #1 #2 #3 #4 #5 #6 
State government 3.68 - - 4 
six states 3.68 - - 4 
State's largest 3.68 - - 4 
State of emergency 3.68 - 4 
Federal, state 3.68 - - 4 
State, including 3.68 - - 4 
Current state of 3.40 - 3 - 
State aid 3.40 - 3 
State where Farmers 3.40 3 
State of rnind 3.40 3 
Current state 3.40 3 
State thrift 3.40 - 3 
Distributable state aid 3.40 - 3 
State judges 3.40 3 
a state court 3.40 3 - 
said the state 3.40 3 
Several states 3.40 - 3 
State monopolies 3.40 - 3 
State laws 3.40 3 
State aid bonds 3.40 - 3 - 
Distributable state 3.40 - 3 
State and local 2.01 1 1 15 
Federal and state 1.60 1 5 - 
State court 1.38 - 12 3 - 
Other state. 1.38 4 1 - 
State$overnments 1.09 1 3 - 
Table  6: Local content-word collocations for 
State in WSJ  
Collocations Pr. 
Overall ocal content .597 
Overall ocal PoS&Fun .478 
Overall global content .442 
OVERALL .485 
N V \[Overall 
Cov. Pr. Cov. Pr. Cov. 
.338 591 .356 595 .344 
.999 ,491 .997 483 .998 
1.00:455 .999 .447 1.00 
1.00 497 1.00 489 1.00 
Tab le  7: Train on BC, tag WSJ 
N V i Overall 
Collocations Pr. Cov. Pr. Cov. i Pr. Cov. 
Overall ocal content 512 .273 .556 .336 530 .295 
Overall local PoS&Fun 421 1.00 .486 1.00 44.4 1.00 
Overall global content !.392 1.00 .423 1.00 403 1.00 
OVERALL 429 1.00 .483 1.00 448 1.00 
Tab le  8: Train on WSJ, tag BC 
211 
criteria) of the hand-taggers when dealing with 
words with difficult sense: distinctions. For 
instance, Table 10 shows some collocations of 
point which receive contradictory senses in the 
BC and the WSJ. The collocation important 
point, for instance, is assigned the second sense I 
in all 3 occurrences in the 13C, and the fourth 
sense 2in all 2 occurrences in the WSJ. 
We can therefore conclude that the one sense 
per collocation holds across corpora, as the 
contradictions found were due to tagging errors. 
The low amount of collocations in common 
would explain in itself the low figures on cross- 
corpora tagging. 
But yet, we wanted to further study the 
reasons of the low number of collocations in 
common, which causes the low cross-corpora 
performance. We thought of several factors that 
could come into play: 
a) As noted earlier, the training and test 
examples from the in-corpus experiments are 
taken at random, and they could be drawn 
from the same document. This could make 
the results appear better for in-corpora 
experiments. On the contrary, in the cross- 
corpora experiments training and testing 
example come from different documents. 
b) The genre and topic changes caused by the 
shift from one corpus to the other. 
c) Corpora have intrinsic features that carmot 
be captured by sole genre and topic 
variations. 
d) The size of the data, being small, would 
account for the low amount of collocations 
shared. 
We explore a) in Section 7 mad b) in Section 8. 
c) and d) are commented in Section 8. 
7 Drawing training and testing 
examples from the same documents 
affects performance 
In order to test whether drawing training and 
testing examples from the same document or not 
explains the different performance in in-corpora 
and cross-corpora tagging, low cross-corpora 
results, we performed the following experiment. 
Instead of organizing the 10 random subsets for 
cross-validation on the examples, we choose 10 
subsets of the documents (also at random). This 
i The second sense of point is defined as the precise 
location of something; a spatially limited location. 
2 Defined as an isolated fact that is considered 
separately from the whole. 
# Coll. # Coll. % Coil % Coll. Word PoS BC WSJ Shared Contradict. 
Age N 45 60 27 0 
Art N 24 35 34 20 
Body N 12 20 12 0 
Car N 92 99 17 0 
Child N 77 111 40 05 
Cost N 88 88 32 0 
Head N 77 95 07 33 
Interest N 80 141 32 33 
Line N 110 145 20 38 
Point N 44 44 32 86 
State N 196 214 28 48 
Thing N 197 183 66 52 
Work N 112 149 46 63 
Become V 182 225 51 15 
Fall V 36 68 19 60 
Grow V 61 71 36 33 
Lose V 63 56 47 43 
Set V 94 113 54 43 
Speak V 34 38 28 0 
Strike V 12 17 14 0 
Tell V 137 190 45 57 
Table 9: Collocations hared and m 
contradiction between BC and WSJ. 
BC WSJ Collocation #2 #4 Other #2 #4 Other 
important point 3 0 0 0 2 0 
pointofview 1 13 1 19 0 0 
Table 10: Contradictory senses of point 
way, the testing examples and training examples 
are guaranteed to come from different 
documents. We also think that this experiment 
would show more realistic performance figures, 
as a real application can not expect to find 
examples from the documents used for training. 
Unfortunately, there are not any explicit 
document boundaries, neither in the BC nor in 
the WSJ. 
In the BC, we took files as documents, even 
if files might contain more than one excerpt 
from different documents. This guarantees that 
document boundaries are not crossed. It has to 
be noted that following this organization, the 
target examples would share fewer examples 
from the same topic. The 168 files from the BC 
were divided in 10 subsets at random: we took 8 
subsets with 17 files and 2 subsets with 16 files. 
For the WSJ, the only cue was the directory 
organization. In this case we were unsure about 
the meaning of this organization, but hand 
inspection showed that document boundaries 
were not crossing discourse boundaries. The 61 
directories were divided in 9 subsets with 6 
directories and 1 subset with 7. 
212 
Again, 10-fold cross-validation was used, on 
these subsets and the results in Tables 11 and 12 
were obtained. The ,5 column shows the change 
in precision with respect to Tables 5 and 6. 
Table 12 shows that, for the BC, precision 
and coverage, compared to Table 5, are 
degraded significantly. On the contrary results 
for the WSJ are nearly the same (el. Tables 11 
and 4). 
The results for WSJ indicate that drawing 
training and testing data from the same or 
different documents in itself does not affect so 
much the results. On the other hand, the results 
for BC do degrade significantly. This could be 
explained by the greater variation in topic and 
genre between the files in the BC corpus. This 
will be further studied in Section 8. 
Table 13 summarizes the overall results on 
WSJ and BC for each of the different 
experiments performed. The figures show that 
drawing training and testing data from the same 
or different documents would not in any case 
explain the low figures in cross-corpora t gging. 
8 Genre and topic variation affects 
performance 
Trying to shed some light on this issue we 
observed that the category press:reportage, is
related to the genre/topics of the WSJ. We 
therefore designed the following experiment: we 
tagged each category in the BC with the 
decision lists trained on the WSJ, and also with 
the decision lists trained on the rest of the 
categories in the BC. 
Table 14 shows that the local content-word 
collocations trained in the WSJ attain the best 
precision and coverage for press:reportage, 
both compared to the results for the other 
categories, and to the results attained by the rest 
of the BC on press:reportage. That is: 
? From all the categories, the collocations from 
press:reportage are the most similar to those 
of WSJ. 
? WSJ contains collocations which are closer 
to those of press:reportage, than those from 
the rest of the BC. 
In other words, having related genre/topics help 
having common collocations, and therefore, 
warrant better word sense disambiguation 
performance. 
Overall Localcontent 
pr. coy. Apr. pr. cov. Apr. 
N .650 1.00 -.011 .762 .486 -.002 
V .634 1.00 -.001 .697 .494 -.040 
Overall .644 1.00 -.011 .738 .489 -.017 
Table 11: Train on WSJ, tag WSJ, 
crossvalidation according to files 
Overall Local content 
pr. cov. Apr. pr. cov. Apr. 
N .499 1.00 -.078 .573 .307 -.102 
V .543 1.00 -.021 .608 .379 -.027 
Overall .514 1.00 -.058 .587 .333 -.074 
Table 12: Train on BC, tag BC, 
crossvalidation according to files 
Overall (prec.) 
In-corpora In-corpora 
(examples) (files) Cross-corpora 
WSJ .652 .644 .489 
BC .572 .514 .448 
Table 13: Overall results in different 
experiments 
Category 
WSJ Rest of BC 
local content local content 
pr. coy. pr. cov. 
Press: Reportage .625 .330 .541 .285 
Press: Editorial .504 .283 .593 .334 
Press: Reviews .438 .268 .488 .404 
Religion .409 .306 .537 .326 
Skills and Hobbies .569 .296 .571 .302 
Popular Lore .488 .304 .563 .353 
Belles Lettres . . . . .  516 .272 .524 .314 
Miscellaneous .534 .321 .534 .304 
Learned .518 .257 .563 .280 
General Fiction .525 .239 .605 .321 
Mystery and . . . .  523 .243 .618 .369 
Science Fiction .459 .211 .586 .307 
Adventure and . . . .  551 .223 .702 .312 
Romance and . . . .  561 .271 .595 .340 
Humor .516 .321 .524 .337 
Table 14: Tagging different categories in BC. 
Best precision results are shown in bold. 
9 Reasons for cross-corpor a degradation 
The goal of sections 7 and 8 was to explore the 
possible causes for the low number of 
collocations in common between BC and WSJ. 
Section 7 concludes that drawing the examples 
from different files is not the main reason for 
the degradation. This is specially true when the 
corpus has low genre/topic variation (e.g. WSJ). 
Section 8 shows that sharing enre/topic s a key 
factor; as the WSJ corpus attains better esults 
on the press:reportage category than the rest of 
213 
the categories on the BC itself. Texts on the 
same genre/topic share more collocations than 
texts on disparate genre/topics, even if they 
come from different corpora. 
This seems to also rule out explanation c) 
(cf. Section 6), as a good measure of topic/genre 
similarity would help overcome cross-corpora 
problems. 
That only leaves the low amount of data 
available for this study (explanation d). It is true 
that data-scarcity can affect the number of 
collocations hared across corpora. We think 
that larger amounts will make', this number grow, 
especially if the corpus draws texts from 
different genres and topics. Nevertheless, the 
figures in Table 14 indicate that even in those 
conditions genre/topic relatedness would help to 
find common collocations. 
10 -Conclusions 
This paper shows that the one sense per 
collocation hypothesis is weaker for fine- 
grained word sense distinctions (e.g. those in 
WordNet): from the 99% precision mentioned 
for 2-way ambiguities in (Yarowsky, 1993) we 
drop to 70% figures. These figures could 
perhaps be improved using more available data. 
We also show that one sense per collocation 
does hold across corpora, but that collocations 
vary from one corpus to other, following genre 
and topic variations. This explains the low 
results when performing word sense 
disambiguation across corpora. In fact, we 
demonstrated that when two independent 
corpora share a related genre/topic, the word 
sense disambiguation results would be better. 
This has considerable impact in future work 
on word sense disambiguation, as genre and 
topic are shown to be crucial parameters. A 
system trained on a specific genre/topic would 
have difficulties to adapt to new genre/topics. 
Besides, methods that try to extend 
automatically the amount of examples for 
training need also to account for genre and topic 
variations. 
As a side effect, we have shown that the 
results on usual WSD exercises, which mix 
training and test data drawn from the same 
documents, are higher than those from a more 
realistic setting. 
We also discovered several hand-tagging 
errors, which distorted extracted collocations. 
We did not evaluate the extent of these errors, 
but they certainly affected the performance on 
cross-corpora t gging. 
Further work will focus on evaluating the 
separate weight of genre and topic in word sense 
disambiguation performance, and on studying 
the behavior of each particular word and 
features through genre and topic variations. We 
plan to devise ways to integrate genre/topic 
parameters into the word sense disambiguation 
models, and to apply them on a system to 
acquire training examples automatically. 
References 
Agirre, E. and D. Martinez. Exploring automatic 
word sense disambiguation with decision lists and 
the Web. Proceedings of the COLING Workshop 
on Semantic Annotation and Intelligent Content. 
Saarbrticken, Germany. 2000. 
Brants, T. TnT- A Statistical Part-of-Speech Tagger. 
In Proceedings of the Sixth Applied Natural 
Language Processing Conference, Seattle, WA. 
2000. 
Escudero, G. , L. Mhrquez and G. Rigau. On the 
Portability and Tuning of Supervised Word Sense 
Disambiguation Systems. In Proceedings of the 
Joint Sigdat Conference on Empirical Methods in 
Natural Language Processing and Very Large 
Corpora, Hong Kong. 2000. 
Francis, W. M. and H. Kucera. Brown Corpus 
Manual oflnformation. Department ofLinguistics, 
Brown University. Also available at 
http://khnt.hit.uib.no/icame/manuals/brown/. 1964. 
Gale, W., K. W. Church, and D. Yarowsky. A 
Method for Disambiguating Word Senses in a 
Large Corpus, Computers and the Humanities, 26, 
415--439, 1993. 
Ide, N. and J. Veronis. Introduction to the Special 
Issue on Word Sense Disambiguation: The State of 
the Art. Computational Linguistics, 24(1), 1--40, 
1998. 
Karlgren, J. and D. Cutting. Recognizing Text Genres 
with Simple Metrics Using Discriminant Analysis. 
Proceedings of the International Conference on 
Computational Linguistics. 1994 
Krovetz, R. More Than One Sense Per Discourse, 
Proceedings of SENSEVAL and the Lexicography 
Loop Workshop. http://www.itri.brighton.ac.uk/ 
events/senseval/PROCEEDINGS/. 1998 
Leacock, C., M. Chodorow, and G. A. Miller. Using 
Corpus Statistics and WordNet Relations for Sense 
Identification. Computational Linguistics, 24(1), 
147--166, 1998. 
Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, 
and K. Miller. Five Papers on WordNet. Special 
Issue of International Journal of Lexicography, 
3(4), 1990. 
214 
Miller, G. A., C. Leacock, R. Tengi, and R. T. 
Bunker, A Semantic Concordance. Proceedings of 
the ARPA Workshop on Human Language 
Technology, 1993. 
Ng, H. T. and H. B. Lee. Integrating Multiple 
Knowledge Sources to Disambiguate Word Sense: 
An Exemplar-based Approach. Proceedings of the 
34th Annual Meeting of the Association for 
Computational Linguistics. 1996. 
Ng, H. T., C. Y. Lira and S. K. Foo. A Case Study on 
Inter-Annotator Agreement for Word Sense 
Disambiguation. Proceedings of the Siglex-ACL 
Workshop on Standarizing Lexical Resources. 
1999. 
Yarowsky, D. One Sense per Collocation. Proc. of 
the 5th DARPA Speech and Natural Language 
Workshop. 1993 
Yarowsky, D. Decision Lists for Lexical Ambiguity 
Resolution: Application to Accent Restoration in 
Spanish and French. Proceedings of the 32rid 
Annual Meeting of the Association for 
Computational Linguistics, pp. 88--95. 1994. 
Yarowsky, D. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics. 
Cambridge, MA, pp. 189-196, 1995. 
Yarowsky, D. Homograph Disambiguation in Text- 
to-speech Synthesis. J Hirschburg, R. Sproat and J. 
Van Santen (eds.) Progress in Speech Synthesis, 
Springer-Vorlag, pp. 159-175. 1996. 
215 
Learning class-to-class selectional preferences 
Eneko Agirre 
IXA NLP Group 
University of the Basque Country 
649 pk. 20.080 
 Donostia. Spain. 
eneko@si.ehu.es 
David Martinez 
IXA NLP Group 
University of the Basque Country 
649 pk. 20.080 
 Donostia. Spain. 
jibmaird@si.ehu.es 
Abstract 
Selectional preference learning 
methods have usually focused on word-
to-class relations, e.g., a verb selects as 
its subject a given nominal class. This 
papers extends previous statistical 
models to class-to-class preferences, 
and presents a model that learns 
selectional preferences for classes of 
verbs. The motivation is twofold: 
different senses of a verb may have 
different preferences, and some classes 
of verbs can share preferences. The 
model is tested on a word sense 
disambiguation task which uses 
subject-verb and object-verb 
relationships extracted from a small 
sense-disambiguated corpus.  
1 Introduction 
Previous literature on selectional preference has 
usually learned preferences for words in the 
form of classes, e.g., the object of eat is an 
edible entity. This paper extends previous 
statistical models to classes of verbs, yielding a 
relation between classes in a hierarchy, as 
opposed to a relation between a word and a 
class.  
The model is trained using subject-verb and 
object-verb associations extracted from Semcor, 
a corpus (Miller et al, 1993) tagged with 
WordNet word-senses (Miller et al, 1990). The 
syntactic relations were extracted using the 
Minipar parser (Lin, 1993). A peculiarity of this 
exercise is the use of a small sense-
disambiguated corpus, in contrast to using a 
large corpus of ambiguous words. We think that 
two factors can help alleviate the scarcity of 
data: the fact that using disambiguated words 
provides purer data, and the ability to use classes 
of verbs in the preferences. Nevertheless, the 
approach can be easily extended to larger, non-
disambiguated corpora.  
We have defined a word sense 
disambiguation exercise in order to evaluate the 
extracted preferences, using a sample of words 
and a sample of documents, both from Semcor. 
Following this short introduction, section 2 
reviews selectional restriction acquisition. 
Section 3 explains our approach, which is 
formalized in sections 4 and 5. Next, section 6 
shows the results on the WSD experiment. Some 
of the acquired preferences are analysed in 
section 7. Finally, some conclusions are drawn 
and future work is outlined. 
2 Selectional preference learning 
Selectional preferences try to capture the fact 
that linguistic elements prefer arguments of a 
certain semantic class, e.g. a verb like ?eat? 
prefers as object edible things, and as subject 
animate entities, as in, (1) ?She was eating an 
apple?. Selectional preferences get more 
complex than it might seem: (2) ?The acid ate 
the metal?, (3) ?This car eats a lot of gas?, (4) 
?We ate our savings?, etc.   
Corpus-based approaches for selectional 
preference learning extract a number of  (e.g. 
verb/subject) relations from large corpora and 
use an algorithm to generalize from the set of 
nouns for each verb separately. Usually, nouns 
are generalized using classes (concepts) from a 
lexical knowledge base (e.g. WordNet).  
Resnik (1992, 1997) defines an information-
theoretic measure of the association between a 
verb and nominal WordNet classes: selectional 
association. He uses verb-argument pairs from 
Brown. Evaluation is performed applying 
intuition and WSD. Our measure follows in part 
from his formalization.  
Abe and Li (1995) follow a similar approach, 
but they employ a different information-
theoretic measure (the minimum description 
length principle) to select the set of concepts in a 
hierarchy that generalize best the selectional 
preferences for a verb. The argument pairs are 
extracted from the WSJ corpus, and evaluation 
is performed using intuition and PP-attachment 
resolution.  
Stetina et al (1998) extract word-arg-word 
triples for all possible combinations, and use a 
measure of  ?relational probability? based on 
frequency and similarity. They provide an 
algorithm to disambiguate all words in a 
sentence. It is directly applied to WSD with 
good results.  
3 Our approach 
The model explored in this paper emerges as a 
result of the following observations:  
? Distinguishing verb senses can be useful. 
The examples for eat above are taken from 
WordNet, and each corresponds to a 
different word sense1: example (1) is from 
the ?take in solid food? sense of eat, (2) 
from the ?cause to rust? sense, and 
examples (3) and (4) from the ?use up? 
sense.  
? If the word senses of a set of verbs are 
similar (e.g. word senses of ingestion verbs 
like eat, devour, ingest, etc.) they can have 
related selectional preferences, and we can 
generalize and say that a class of verbs has a 
particular selectional preference.  
Our formalization thus distinguishes among verb 
senses, that is, we treat each verb sense as a 
1                                                           
1 A note is in order to introduce the terminology used in the 
paper. We use concept and class indistinguishably, and 
they refer to the so-called synsets in WordNet. Concepts in 
WordNet are represented as sets of synonyms, e.g. <food, 
nutrient>. A word sense in WordNet is a word-concept 
pairing, e.g. given the concepts a=<chicken, poulet, 
volaille> and b=<wimp, chicken, crybaby> we can say 
that chicken has at least two word senses, the pair chicken-
a and the pair chicken-b. In fact the former is sense 1 of 
chicken, and the later is sense 3 of chicken. For the sake of 
simplicity we also talk about <chicken, poulet, volaille> 
being a word sense of chicken. 
different unit that has a particular selectional 
preference. From the selectional preferences of 
single verb word senses, we also infer 
selectional preferences for classes of verbs. 
Contrary to other methods (e.g. Li and 
Abe?s), we don?t try to find the classes which 
generalize best the selectional preferences. All 
possibilities, even the very low probability ones, 
are stored. 
The method stands as follows: we collect 
[noun-word-sense relation verb-word-sense] 
triples from Semcor, where the relation is either 
subject or object. As word senses refer to 
concepts, we also collect the triple for each 
possible combination of concepts that subsume 
the word senses in the triple. Direct frequencies 
and estimates of frequencies for classes are then 
used to compute probabilities for the triples.  
These probabilities could be used to 
disambiguate either nouns, verbs or both at the 
same time. For the time being, we have chosen 
to disambiguate nouns only, and therefore we 
compute the probability for a nominal concept, 
given that it is the subject/object of a particular 
verb. Note that when disambiguating we ignore 
the particular sense in which the governing verb 
occurs. 
4 Formalization 
As mentioned in the previous sections we are 
interested in modelling the probability of a 
nominal concept given that it is the 
subject/object of a particular verb: 
)|( vrelcnP i  (1) 
Before providing the formalization for our 
approach we present a model based on words 
and a model based on nominal-classes. Our 
class-to-class model is an extension of the 
second2. The estimation of the frequencies of 
classes are presented in the following section. 
1                                                           
2 Notation: v stands for a verb, cn (cv) stand for nominal 
(verbal) concept, cni (cvi ) stands for the concept linked to 
the i-th sense of the given noun (verb), rel could be any 
grammatical relation (in our case object or subject), ? 
stands for the subsumption relation, fr stands for frequency 
and rf? .for the estimation of the frequencies of classes. 
4.1 Word-to-word model: eat chickeni 
At this stage we do not use information of class 
subsumption. The probability of the first sense 
of chicken being an object of eat depends on 
how often does the concept linked to chicken1 
appear as object of the word eat, divided by the 
number of occurrences of eat with an object.  
)(
)()|( vrelfr
vrelcnfrvrelcnP ii =   (2) 
Note that instead of )|( vrelsenseP i  we use 
)|( vrelcnP i , as we count occurrences of 
concepts rather than word senses. This means 
that synonyms also count, e.g. poulet as 
synonyms of the first sense of chicken.  
4.2 word-to-class model:  
eat <food, nutrient> 
The probability of eat chicken1 depends on the 
probabilities of the concepts subsumed by and 
subsuming chicken1 being objects of eat. For 
instance, if chicken1 never appears as an object 
of eat, but other word senses under <food, 
nutrient> do, the probability of chicken1 will not 
be 0.  
Formula (3) shows that for all concepts 
subsuming cni the probability of cni given the 
more general concept times the probability of 
the more general concept being a subject/object 
of the verb is added. The first probability is 
estimated dividing the class frequencies of cni 
with the class frequencies of the more general 
concept. The second probability is estimated as 
in 4.1.  
4.3 class-to-class model:  
<ingest, take in, ?> <food, nutrient> 
The probability of eat chicken1 depends on the 
probabilities of all concepts above chicken1 
being objects of all concepts above the possible 
senses of eat. For instance, if devour never 
appeared on the training corpus, the model could 
infer its selectional preference from that of its 
??
??
?=?=
icncnicncn
vrelfr
vrelcnrf
cnrf
cncnrfvrelcnPcncnPvrelcnP iii )(
)(?
)(?
),(?)|()|()|(  (3) 
? ?
? ?
? ?
? ?
??=
??=
icncn cvcvvsenseofcv
icncn cvcvvsenseofcv
j
ji
j
j
jiji
cvrelfr
cvrelcnrf
cvrf
cvcvrf
cnrf
cncnrf
cvrelcnPcvcvPcncnPvrelcnP
max
max
)(
)(?
)(?
),(?
)(?
),(?
)|()|()|()|(
 (4) 
?
?
?=
cnicn
i
i
cnfrcnclassescnrf )()(
1)(?   (5) 
??
??
? ??
=
?
?
otherwise
cncnifcnfrcnclassescncnrf iij jji cncn
0
)()(
1
),(?  (6) 
?
?
?=
cnicn
vrelcnfrcnclassesvrelcnrf ii
)()(
1)(?  (7) 
? ?
? ?
??=
cnicn cnicv
ii
ii
cvrelcnfrcvclassescnclassescvrelcnrf )()(
1
)(
1)(?  (8) 
superclass <ingest, take in, ...>. As the verb can 
be polysemous, the sense with maximum 
probability is selected.  
Formula (4) shows that the maximum 
probability for the possible senses (cvj) of the 
verb is taken. For each possible verb concept 
(cv) and noun concept (cn) subsuming the target 
concepts (cni,cvj), the probability of the target 
concept given the subsuming concept (this is 
done twice, once for the verb, once for the noun) 
times the probability the nominal concept being 
subject/object of the verbal concept is added.  
5 Estimation of class frequencies 
Frequencies for classes can be counted directly 
from the corpus when the class is linked to a 
word sense that actually appears in the corpus, 
written as fr(cni). Otherwise they have to be 
estimated using the direct counts for all 
subsumed concepts, written as )(? icnrf . Formula 
(5) shows that all the counts for the subsumed 
concepts (cni) are added, but divided by the 
number of classes for which ci is a subclass (that 
is, all ancestors in the hierarchy). This is 
necessary to guarantee the following: 
?
? icncn
cncnP i )|( = 1. 
Formula (6) shows the estimated frequency 
of a concept given another concept. In the case 
of the first concept subsuming the second it is 0, 
otherwise the frequency is estimated as in (5). 
Formula (7) estimates the counts for 
[nominal-concept relation verb] triples for all 
possible nominal-concepts, which is based on 
the counts for the triples that actually occur in 
the corpus. All the counts for subsumed 
concepts are added, divided by the number of 
classes in order to guarantee the following: 
?
cn
vsubjcnP )|( =1 
Finally, formula (8) extends formula (7) to 
[nominal-concept relation verbal-concept] in a 
similar way. 
6 Training and testing on a WSD 
exercise 
For training we used the sense-disambiguated 
part of Brown, Semcor, which comprises around 
250.000 words tagged with WordNet word 
senses. The parser we used is Minipar. For this 
current experiment we only extracted verb-
object and verb-subject pairs. Overall 14.471 
verb-object pairs and 12.242 verb-subject pairs 
w
st
cl
fo
ex
no
co
W
co
T
ch
le
ra
m
of
th
1
3 
M
Noun # sens # occ 
# occ.  
as obj 
# occ.  
as subj 
account 10 27 8 3 
age 5 104 10 9 
church 3 128 19 10 
duty 3 25 8 1 
head 30 179 58 16 
interest 7 140 31 13 
member 5 74 13 11 
people 4 282 41 83 
Overall  67 959 188 146 
Table 1. Data for the selected nouns. 
 
 Prec.  
Obj 
Cov.  Rec Prec.  
Subj 
Cov. Rec. 
Random .192 1.00 .192 .192 1.00 .192 
MFS .690 1.00 .690 .690 1.00 .690 
Word2word .959 .260 .249 .742 .243 .180 
Word2class .669 .867 .580 .562 .834 .468 
Class2class .666 .973 .648 .540 .995 .537 
Table 2. Average results for the 8 nouns. ere extracted. For the sake of efficiency, we 
ored all possible class-to-class relations and 
ass frequencies at this point, as defined in 
rmulas (5) to (8).  
The acquired data was tested on a WSD 
ercise. The goal was to disambiguate all 
uns occurring as subjects and objects, but it 
uld be also used to disambiguate verbs. The 
SD algorithm just gets the frequencies and 
mputes the probabilities as they are needed. 
he word sense with the highest probability is 
osen.  
Two experiments were performed: on the 
xical sample we selected a set of 8 nouns at 
ndom3 and applied 10fold crossvalidation to 
ake use of all available examples. In the case 
 whole documents, they were withdrawn from 
e training corpus and tested in turn.  
                                                           
This set was also used on a previous paper (Agirre & 
artinez, 2000). 
Table 1 shows the data for the set of nouns. 
Note that only 19% (15%) of the occurrences of 
the nouns are objects (subjects) of any verb. 
Table 2 shows the average results using subject 
and object relations for each possible 
formalization. Each column shows respectively, 
the precision, the coverage over the occurrences 
with the given relation, and the recall. Random 
and most frequent baselines are also shown. 
Word-to-word gets the highest precision of all 
three, but it can only be applied on a few 
instances. Word-to-class gets slightly better 
precision than class-to-class, but class-to-class is 
near complete coverage and thus gets the best 
recall of all three. All are well above the random 
baseline, but slightly below the most frequent 
sense.  
On the all-nouns experiment, we 
disambiguated the nouns appearing in four files 
extracted from Semcor. We observed that not 
many nouns were related to a verb as object or 
subject (e.g. in the file br-a01 only 40% (16%) 
of the polisemous nouns were tagged as object 
(subject)). Table 3 illustrates the results on this 
task. Again, word-to-word obtains the best 
precision in all cases, but because of the lack of 
data the recall is low. Class-to-class attains the 
best recall.  
We think that given the small corpus 
available, the results are good. Note that there is 
no smoothing or cut-off value involved, and 
some decisions are taken with very little points 
of data. Sure enough both smoothing and cut-off 
values will allow to improve the precision. On 
the contrary, literature has shown that the most 
frequent sense baseline needs less training data.  
7 Analysis of the acquired selectional 
preferences 
In order to analyze the acquired selectional 
preferences, we wanted a word that did not 
occur too often and which had clearly 
distinguishable senses. The goal is to study the 
preferences that were applied in the 
disambiguation for all occurrences, and check 
what is the difference among each of the 
models.  
The selected word was church, which has three 
senses in WordNet, and occurs 19 times. Figure 
1 shows the three word senses and the 
corresponding subsuming concepts. Table 4 
shows the results of the disambiguation 
algorithm for church.  
Object Subject 
File Rand. MFS word2word word2class class2class Rand. MFS word2word word2class class2class 
br-a01 .286 .746 .138 .447 .542 .313 .884 .312 .640 .749 
br-b20 .233 .776 .093 .418 .487 .292 .780 .354 .580 .677 
br-j09 .254 .645 .071 .429 .399 .256 .761 .200 .500 .499 
br-r05 .269 .639 .126 .394 .577 .294 .720 .144 .601 .710 
Table 3. Average recall for the nouns in the four Semcor files. 
 
Sense 1 
church, Christian church, Christianity 
       => religion, faith 
           => institution, establishment 
               => organization, organisation 
                   => social group 
                       => group, grouping 
 
Sense 2 
church, church building 
       => place of worship, house of prayer,  
            house of God, house of worship 
           => building, edifice 
               => structure, construction 
                   => artifact, artefact 
                       => object, physical object 
                           => entity, something 
 
Sense 3 
church service, church 
       => service, religious service, divine service 
           => religious ceremony, religious ritual
               => ceremony 
                   => activity 
                       => act, human action, human activity 
Figure 1. Word senses and superclasses for church
In the word-to-word model, the model is 
unable to tag any of the examples4 (all the verbs 
related to ?church? were different). For church 
as object, both class-to-class and word-to-class 
have similar recall, but word-to-class has better 
precision. Notice that the majority of the 
examples with church as object were not tagged 
with the most frequent sense in Semcor, and 
therefore the MFS precision is remarkably low 
(21%). For church as subject, the class-to-class 
model has both better precision and coverage.  
We will now study in more detail each of the 
examples.  
7.1 Church as object 
There were 19 examples with church as object 
(15 tagged in Semcor with sense 2 and 4 with 
sense 1). Using the word-to-class model, 12 
were tagged correctly, 5 incorrectly and 2 had 
not enough data to answer. In the class-to-class 
model 12 examples were tagged correctly and 7 
incorrectly. Therefore there was no gain in 
recall.  
First, we will analyze the results of the 
word-to-class model. From the 12 hits, 10 
corresponded to sense 2 and the other 2 to sense 
1. Here we show the 12 verbs and the 
superconcept of the senses of church that gets 
the highest selectional preference probability, 
and thus selects the winning sense, in this case, 
correctly.  
? Tagged with sense 2: 
 look: <buil
 have: <buil
1                                 
4 Note that we applied 10fo
not able to tag anything b
samples do not appear in t
the verbs governing church 
 demolish: <building, edifice> 
 move:  <structure, construction> 
 support:  <structure, construction> 
 build:  <structure, construction> 
 enter:  <structure, construction> 
 sell:  <artifact, artefact> 
 abandon:  <artifact, artefact> 
 see:  <artifact, artefact> 
? Tagged with sense 1 
 strengthen:  <organization, organisation> 
 turn_to:  <organization, organisation> 
The five examples where the model failed 
revealed different types of errors. We will check 
each of the verbs in turn. 
1. Attend (Semcor 2, word-to-class 1)5: We 
quote the whole sentence:  
From many sides come remarks that 
Protestant churches are badly attended and 
the large medieval cathedrals look all but 
empty during services . 
We think that the correct sense should be 3 ( 
?church services? are attended, not the 
buildings). In any case, the class that gets the 
higher weight is <institution, establishment>, 
pointing to sense 1 of church and beating the 
more appropriate class <religious ceremony, 
religious ritual> because of the lack of 
examples in the training.  
2. Join (Semcor 1, word-to-class 2): It seems 
that this verb should be a good clue for sense 1.  
But among the few occurrences of join in the 
training set there were ?join-obj-temple? and 
oth temple and 
 have organization-
et and they were thus 
 under <building, 
                  
se in Semcor (the correct 
igned by the model. 
 #occ OK KO No ansr Prec. Cov. Rec. 
obj MFS 19 4 15 0 .210 1.00 .210 
obj word-to-word 19 0 0 19 .000 .000 .000 
obj word-to-class 19 12 5 2 .705 .894 .631 
obj class-to-class 19 12 7 0 .631 1.00 .631 
subj MFS 10 8 2 0 .800 1.00 .800 
subj word-to-word 10 0 0 10 .000 .000 .000 
subj word-to-class 10 4 3 3 .571 .700 .400 
subj class-to-class 10 6 4 0 .600 1.00 .600 
Table 4: Results disambiguating the word church. ding, edifice> 
ding, edifice> 
                          
ld crossvalidation. The model is 
ecause the verbs in the testing 
he training samples. In fact all  
occur only once. 
?join-obj-synagogue?. B
synagogue have do not
related concepts in WordN
tagged with a concept
1                                         
5 For each verb we list the sen
reference sense) and the sense ass
edifice>. This implies that <place of worship, 
house of prayer, house of God, house of 
worship> gets most credit and the answer is 
sense 2.  
3. Imprison (Semcor 1, word-to-class 3): The 
scarcity of training examples is very evident 
here. There are only 2 examples of imprison 
with an object, one of them wrongly selected by 
Minipar (imprison-obj-trip) that falls under 
<act, human action, human activity> and points 
to sense 3.  
4. Empty (Semcor 2, word-to-class 1): The 
different senses of empty introduce misleading 
examples. The best credit is given to <group, 
grouping> (following an sense of empty which 
is not appropriate here) which selects the sense 1 
of church. The correct sense of empty in this 
context relates with <object, physical object>, 
and would thus select the correct sense, but does 
not have enough credit.  
5. Advance (Semcor 2, word-to-class 3): the 
misleading senses of ?advance? and the low 
number of examples point to sense 3.  
We thus identified 4 sources of error in the 
word-to-class model: 
A. Incorrect Semcor tag 
B. Wrongly extracted verb-object relations 
C. Scarcity of data 
D. Misleading verb senses 
The class-to-class model should help to 
mitigate the effects of errors type C and D. We 
would specially hope for the class-to-class 
model to discard misleading verb senses. We 
now turn to analyze the results of this model.  
From the 12 correct examples tagged using 
word-to-class, we observed that 3 were 
mistagged using class-to-class. The reason was 
that the class-to-class introduces new examples 
from verbs that are superclasses of the target 
verb, and these introduced noise. For example, 
we examined the verb turn_to (tagged in Semcor 
with sense 1):  
1. turn-to (Semcor 1, word-to-class 1): there are 
fewer training examples than in the class-to-
class model and they get more credit. The 
relation ?turn_to-obj-platoon? gives weight to 
the class <organization, organisation>.  
2. turn-to (Semcor 1, class-to-class 2): the 
relations ?take_up-obj-position? and ?call_on-
obj-esprit_de_corps? introduce noise and point 
to the class <artifact, artefact>. As a result, the 
sense 2 is wrongly selected.  
From the 5 mistagged examples in class-to-
class, only ?empty? was tagged correctly using 
classes (in this case the class-to-class model is 
able to select the correct sense of the verb, 
discarding the misleading senses of empty):  
1. Attend, Join, Advance: they had errors of 
type A and B (incorrect Semcor tag/ misleading 
verb-object relations) and we can not expect the 
?class-to-class? model to handle them.  
2. Imprison: still has not enough information to 
make a good choice.  
3. Empty (Semcor 2, class-to-class 2): new 
examples associated to the appropriate sense of 
empty give credit to the classes <place of 
worship, house of prayer, house of God, house 
of worship> and <church, church building>. 
With the weight of these classes the correct 
sense 2 is correctly chosen.  
Finally, the 2 examples that received no 
answer in the ?word-to-class? model were 
tagged correctly:  
1. Flurry (Semcor 2, class-to-class 2): the 
answer is correct although the choice is made 
with few data. The strongest class is <structure, 
construction>. 
2. Rebuild (Semcor 2, class-to-class 2): the new 
information points to the appropriate sense.  
7.2 Church as subject 
The class2class model showed a better behavior 
with the examples in which church appeared as 
subject. There were only 10 examples, 8 tagged 
with sense 1 and 2 with sense 2.  
In this case, the class-to-class model tagged 
in the same way the examples tagged by the 
class-to-word model, but it also tagged the 3 
occurrences that had not been tagged by the 
word-to-class model (2 correctly and 1 
incorrectly).  
8 Conclusions 
We presented a statistical model that extends 
selectional preference to classes of verbs, 
yielding a relation between classes in a 
hierarchy, as opposed to a relation between a 
word and a class. The motivation is twofold: 
different senses of a verb may have different 
preferences, and some classes of verbs can share 
preferences. 
The model is trained using subject-verb and 
object-verb relations extracted from a sense-
disambiguated corpus using Minipar. A 
peculiarity of this exercise is the use of a small 
sense-disambiguated corpus, in contrast to using 
a large corpus of ambiguous words.  
Contrary to other methods we do not try to 
find the classes which generalize best the 
selectional preferences. All possibilities, even 
the ones with very low probability, are stored. 
Evaluation is based on a word sense 
disambiguation exercise for a sample of words 
and a sample of documents from Semcor. The 
proposed model gets similar results on precision 
but significantly better recall than the classical 
word-to-class model.  
We plan to train the model on a large 
untagged corpus, in order to compare the quality 
of the acquired selectional preferences with 
those extracted from this small tagged corpora. 
The model can easily be extended to 
disambiguate other relations and POS. At 
present we are also integrating the model on a 
supervised WSD algorithm that uses decision 
lists.  
References 
Abe, H. & Li, N. 1996. Learning Word Association 
Norms Using Tree Cut Pair Models. In 
Proceedings of the 13th International Conference 
on Machine Learning ICML.  
Agirre E. and Martinez D. 2000. Decision lists and 
automatic word sense disambiguation. COLING 
2000, Workshop on Semantic Annotation and 
Intelligent Content. Luxembourg.  
Lin, D. 1993. Principle Based parsing without 
Overgeneration. In 31st Annual Meeting of the 
Association for Computational Linguistics. 
Columbus, Ohio. pp 112-120.  
Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, 
and K. Miller. 1990. Five Papers on WordNet. 
Special Issue of the International Journal of 
Lexicography, 3(4).  
Miller, G. A., C. Leacock, R. Tengi, and R. T. 
Bunker. 1993. A Semantic Concordance. 
Proceedings of the ARPA Workshop on Human 
Language Technology.  
Resnik, P. 1992. A class-based approach to lexical 
discovery. In Proceedings of the Proceedings of 
the 30th Annual Meeting of the Association for 
Computational Linguists., . 327-329.  
Resnik,P. 1997. Selectional Preference and Sense 
Disambiguation.. In Proceedings of the ANLP 
Workshop ``Tagging Text with Lexical 
Semantics: Why What and How?''., Washington, 
DC.  
Stetina J., Kurohashi S., Nagao M. 1998. General 
Word Sense Disambiguation Method Based on a 
Full Sentential Context. In Usage of WordNet in 
Natural Language Processing , Proceedings of 
COLING-ACL Workshop. Montreal (C Canada).  
 
 
A Multilingual Approach to Disambiguate Prepositions  
and Case Suffixes 
Eneko Agirre, Mikel Lersundi, David Martinez 
 
IxA NLP group 
University of the Basque Country 
649 pk. - 20.080 Donostia (Spain) 
{eneko, jialeaym, jibmaird}@si.ehu.es 
 
Abstract 
This paper presents preliminary 
experiments in the use of translation 
equivalences to disambiguate 
prepositions or case suffixes. The core 
of the method is to find translations of 
the occurrence of the target preposition 
or case suffix, and assign the 
intersection of their set of 
interpretations. Given a table with 
prepositions and their possible 
interpretations, the method is fully 
automatic. We have tested this method 
on the occurrences of the Basque 
instrumental case -z in the definitions of 
a Basque dictionary, looking for the 
translations in the definitions from 3 
Spanish and 3 English dictionaries. The 
results have been that we are able to 
disambiguate with 94.5% accuracy 
2.3% of those occurrences (up to 91). 
The ambiguity is reduced from 7 
readings down to 3.1. The results are 
very encouraging given the simple 
techniques used, and show great 
potential for improvement. 
1 Introduction 
This paper presents some preliminary experiments 
in the use of translation equivalences to 
disambiguate the interpretations of case suffixes in 
Basque. Basque is an agglutinative language, and 
its case suffixes are more or less equivalent to 
prepositions, but are also used to mark the subject 
and objects of verbs. The method is general, and 
could be as easily applied to prepositions in any 
other language. The core of the method is to find a 
preposition in the translation of an occurrence of 
the target case suffix, and select the 
interpretation(s) in the intersection of both as the 
valid interpretation(s). At this point, we have not 
used additional sources for the disambiguation, 
e.g. governing verbs, nouns, etc., but they could 
complement the technique here presented. 
In this particular experiment, the method was 
tested on the definitions of a Basque monolingual 
dictionary, using the -z instrumental as the target 
case suffix. The main reason is that we are in the 
process of building a Lexical Knowledge Base out 
of dictionary definitions, and the disambiguation 
of case suffixes and other semantic dependencies 
is of great interest. 
The method searches for the respective 
definitions in English and Spanish monolingual 
dictionaries and tries to find a preposition that is 
the translation of the target case suffix. Once the 
preposition is found, the intersection of the set of 
interpretations of both the source case suffix and 
the translated preposition is taken, and the 
outcome is stored. 
The resources needed to perform this task are 
the following: lemmatizers, bilingual dictionaries 
and monolingual dictionaries, as well as a table of 
possible interpretations of prepositions and case 
suffixes. In our case, we have used Basque, 
English and Spanish lemmatizers, Basque/English 
and Basque/Spanish bilingual dictionaries, a target 
Basque monolingual dictionary, 3 Spanish and 3 
English monolingual dictionaries.  
The method is fully automatic; the Spanish and 
English monolingual dictionaries are accessed 
from the Internet, and the rest are local, installed 
in our machines. The manual work has been to 
build the table with possible interpretations of the 
prepositions and case suffixes. 
                       July 2002, pp. 1-8.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
The paper is structured as follows. Section 2 
presents the method for disambiguation in detail. 
Section 3 introduces the interpretations for the 
case suffix and the prepositions. The results are 
shown in Section 4, which are further discussed in 
Section 5. Finally, section 6 presents the 
conclusions and future work.  
2 Method for disambiguation 
The goal of the method is to disambiguate 
between the possible interpretations of a case 
suffix appearing in any text. We have taken as the 
target text the definitions from a monolingual 
Basque dictionary Euskal Hiztegia, EH in short 
(Sarasola, 1996). The method consists on five 
steps:  
? Extraction of the definitions in EH where the 
target case suffix occurs.  
? Search of on-line Spanish and English 
dictionaries to obtain the translation 
equivalent of the definitions.  
? Extraction of the target preposition from the 
translation definitions.  
? Disambiguation based on the intersection of 
the interpretations of case suffix and 
prepositions.  
We will explain each step in turn. 
2.1 Extraction of relations from EH 
Given a case suffix, in this step we will search the 
EH dictionary for occurrences of the case suffix. 
We first lemmatize and perform morphological 
analysis of the definitions (Aduriz et. al, 1996). 
The definitions that contain the target case suffix 
in a morphological analysis are extracted, storing 
the following information: the Basque dictionary 
entry of the definition, the lemma that has the case 
suffix, the case suffix, and the following lemma.  
Below we can see a sample definition, its 
lemmatized version, and the two triples extracted 
from this definition. The occurrences of the 
instrumental -z are shown in bold. 
 
Ildo iz. A1 Goldeaz lurra irauliz 
egiten den irekidura luzea1  
                                                     
1 The literal translation of the definition is the 
following : furrow, a long trench produced turning 
 
/<@@lema ildo>/<ID>/ 
/<@@Adiera_string A1.>/<ID>/ 
/<@@Kategoria iz. >/<ID>/ 
"<Goldeaz>" 
  "golde"  IZE ARR DEK INS NUMS MUGM  
"<lurra>" 
  "lur"  IZE ARR DEK ABS NUMS MUGM  
"<irauliz>" 
  "irauli"  ADI SIN AMM PART DEK INS MG  
"<egiten>" 
  "egin"  ADI SIN AMM ADOIN ASP EZBU  
"<den>" 
  "izan"  ADL A1 NOR NR_HU ERL MEN ERLT  
"<irekidura>" 
  "irekidura"  IZE ARR DEK ABS MG  
"<luzea>" 
  "luze"  ADJ IZO DEK ABS NUMS MUGM  
"<$.>" 
  PUNT_PUNT 
 
golde#INS#lur2 
irauli#INS#egin 
 
Extracting lemma-suffix-lemma triples in this 
simple way leads to some errors (cf. section 5.1). 
For instance, the first triple should rather be the 
dependency golde#INS#irauli (plow#with#turn, to 
be read in reverse order). We will see that even in 
this case we will be able to obtain correct 
translations and disambiguate the preposition 
correctly. Nevertheless, in the future we plan to 
use a syntactic parser to identify better the lemmas 
that are related by the case suffix.  
2.2 Search for Spanish/English 
translations 
After we have a list of entries in the Basque 
dictionary that contain the lemma-suffix-lemma 
triple, we search for their equivalent definitions in 
Spanish and English. We first look up the entry in 
the bilingual dictionary, and then retrieve the 
                                                                                  
over the ground with a plow.  
2 The translation of the first triple is plow#with#ground, 
to be read on reverse. The translation of the second is 
turn#NULL#produce, to be also read on reverse. In this 
second triple the instrumental case suffix is not 
translated explicitly by a preposition, but by a syntactic 
construct. 
definitions for each of the possible translations 
from the monolingual dictionaries. 
We use two bilingual and 6 monolingual 
Machine Readable Dictionaries: Morris 
Basque/English dictionary (Morris, 1998) Elhuyar 
Basque/Spanish dictionary (Elhuyar, 1996); 
English monolingual on-line dictionaries are: 
Cambridge (online), Heritage (online), and 
Wordsmyth (online); and Spanish monolingual 
on-line dictionaries are: Colmex (online), Rae 
(online), and Vox (online). The Basque dictionary 
and the bilingual dictionaries are stored in a local 
server, while the monolingual dictionaries are 
accessed from the Internet using a wrapper. 
The incomplete list of the translation of ildo 
(furrow in English, surco in Spanish) is shown 
below. Note that we got two different definitions 
for surco, coming from different Spanish 
dictionaries. 
 
furrow#A long , narrow , shallow 
trench made in the ground by a 
plow  
 
surco#Excavaci?n alargada , angosta y 
poco profunda que se hace 
paralelamente en la tierra con el 
arado , para sembrarla despu?s  
 
surco#Hendedura que se hace en la 
tierra con el arado  
2.3 Extraction of Spanish/English 
equivalent relations 
Given a list of definitions in Spanish and English, 
we search in the definition the translation of the 
Basque triple found in step 2.1, that is, we look for 
a triple of consecutive words where the first word 
is the translation of the last word in the Basque 
triple, the second word is a preposition (which 
corresponds to the Basque suffix) and the third 
word is the translation of the first word in the 
Basque triple. Between the preposition and the last 
word in the triple we allow for the presence of a 
determiner or an adjective in the text. More 
complex patterns could be allowed, up to full 
syntactic analyses, but at this point we follow this 
simple scheme. 
Below we can find the triples for 
golde#INS#lur, obtained from the three definitions 
above. One triple is obtained twice from two 
different definitions. 
 
furrow#ground#by#plow 
surco#tierra#con#arado 
surco#tierra#con#arado 
 
Definitions that do not have a matching triple 
are discarded, leaving Basque triples without 
matching triple ambiguous. For instance we could 
not find triples for irauli#INS#egin(cf. example in 
section 2.1). The instrumental suffix is sometimes 
translated without prepositions (in this case ?? 
made turning ??). 
Looking up the bilingual dictionaries for 
translation requires lemmatization and Part of 
Speech tagging. For English we use the TnT PoS 
tagger (Brants, 2000) and WordNet for 
lemmatization (Miller et al, 1990). For Spanish 
we use (Atserias et al, 1998).  
2.4 Disambiguation 
For each Basque case suffix, Spanish preposition 
and English preposition we have a list of 
interpretations (cf. Table 1). We assign the 
interpretations of the preposition to each 
Spanish/English triple. The intersection of all the 
interpretations is assigned to it. 
Continuing with out example, we can see that 
the intersection between the interpretations of the 
English by preposition (three interpretations) and 
the interpretations of the Spanish con preposition 
(four interpretations) are manner and instrument. 
Therefore, we can say that the Basque 
instrumental case interpretation in this case will 
be manner or instrument. 
 
furrow#ground#by a#plow# 
manner instrument during-time 
surco#tierra#con el#arado# 
manner instrument cause containing 
 
golde#INS#lur#instrument manner 
3 Interpretations for the 
instrumental case suffix and 
equivalent prepositions 
The method explained in the previous section is 
fully automatic, and it only requires the list of 
interpretations for each case suffix and 
preposition. In this work, we want to evaluate if 
the overall approach is feasible, so we selected 
Basque as the target language and a single case 
suffix, -z the instrumental case. Table 1 shows the 
list of possible interpretations and Table 2 and 3 
examples for each interpretation. 
The sources for the interpretations of the 
instrumental case have been a grammar of Basque 
(Euskaltzaindia, 1985) and a bilingual dictionary 
(Elhuyar, 1996). Possible interpretations for 
Spanish and English prepositions have been taken 
from an English dictionary (Cambridge, online), a 
Spanish dictionary (Vox, online) and a Spanish 
grammar (Bosque & Demonte, 1999).  
For this work we have taken a descriptive 
approach, but other more theoretically committed 
approaches are also possible. The overall method 
is independent of the set of interpretations, as it 
only needs a table of possible interpretations in the 
style of Table 1. Section 5.4 further discusses 
other alternatives. 
In order to disambiguate the occurrences of the 
instrumental case suffix we have taken the 
Spanish and English translations for this case 
suffix. The list of possible translations is 
preliminary and covers what we found necessary 
to make this experiment. Table 1 shows the list of 
prepositions and interpretations for Spanish and 
English. Examples of the interpretations can be 
found in Table 2. The Spanish preposition de had 
the same interpretations as the instrumental case 
suffix (cf. Table1), so it was discarded. 
4 Results 
The instrumental case occurs in 4,004 different 
definitions in the EH dictionary. The algorithm in 
Section 2 was applied to all these definitions, 
yielding a result for 125 triples, 3.1% of the total. 
The triples for which we had an answer were 
tagged by hand independently, i.e. not consulting 
the results output by the algorithm. The hand-
tagged set constitutes what we call the gold 
standard. 
A single linguist made the tagging, consulting 
other teammates when in doubt. Apart from 
marking the interpretation, there were some other 
special cases. 
1. In some of the examples, the instrumental 
case was part of a more complex scheme, and 
was tagged accordingly: 
? Part of a postposition (XPOST), e.g. -en 
bidez (by means of) or -en ordez (instead 
of). 
? Part of a conjunction (XLOK), e.g. batez 
ere (specially). 
? Part of a compounded suffix ?zko 
(XZKO), which results from the 
aggregation of the instrumental ?z  with 
the location genitive -ko. 
2. There were three errors in the lemmatization 
process (XLEM), due to lexicalized items, e.g. 
gizonezko (meaning male person).  
3. Finally, the relation in the definition was 
sometimes wrongly retrieved, e.g.  
? The triple would contain the determiner or 
an adjective instead of the dependencies. 
We thought that the algorithm would be 
able to work well even with those cases, 
so we decided to keep them. 
? The triple contains a conjunction (X): 
these were tagged as incorrect. 
Table 4 shows the amount of such cases, 
alongside the frequency of each interpretation. 
The most frequent interpretation is instrument. In 
seven examples, the linguist decided to keep two 
interpretations: instrument and manner. In a single 
example, the linguist was unable to select an 
interpretation, so this example was discarded. 
The output of the algorithm was compared 
with the gold standard, yielding the accuracy 
figures in Table 5. An output was considered 
correct if it yielded at least one interpretation in 
common with the gold standard. The accuracy is 
given for each dictionary in isolation, or merging 
all the results (as mentioned in section 2, when 
two dictionaries propose interpretations for the 
same triple, their intersection is taken). The 
remaining ambiguity is 3.1 overall. 
  Basque English Spanish 
 -z (ins.) of by with in de con a en 
theme x x   x x  x  
during-time x x x   x    
instrument x  x x x x x  x 
manner x  x  x x x x x 
cause x x  x x x x   
containing x x  x x x x   
matter x x    x    
Table 1: interpretations for the instrumental case in Basque and its equivalents in English and Spanish. 
 
 Basque English 
theme Seguru nago horretaz 
Matematikaz asko daki 
I?m sure of that 
He?s an expert in maths 
during-time Arratsaldez lasai egon nahi dut 
Gauez egin dut 
I like to relax of an evening 
I did it by night 
instrument Autobusez etorri naiz 
Belarra segaz moztu 
Euskaraz hitz egin 
I have come by bus 
To cut grass with a scythe 
To speak in Basque 
manner Animali baten hestea betez egindako haragia
 
Ahots ozen batez 
A meat preparation made by filling an 
animal intestine 
In a loud voice 
cause Haren aitzakiez nekatuta nago  
Beldurrez zurbildu 
Kanpoan lan egitea baztertu zuenez, lan-
aukera ederra galdu zuen 
Sick of his excuses 
To turn white of fear 
In refusing to work abroad, she missed an 
excellent job opportunity 
containing Edalontzia ardoz beteta dago 
Txapelaz dagoen gizona 
Ilez estalia 
The glass is full of wine 
The man with the beret on 
Cover in hair 
matter Armairua egurrez egina dago The wardrobe is made of wood 
Table 2: examples in Basque and English for the set of possible interpretations. 
 
 Basque Spanish 
theme Mariaz aritu dira 
Honetaz ziur naiz 
Han mencionado a  Maria 
Estoy seguro de esto 
during-time Gauez egin dut Lo he hecho de noche 
instrument Belarra segaz moztu 
Euskaraz hitz egin 
Hiria harresiz inguratu dute 
Cortar la hierba con la guada?a 
Hablar en vasco 
Han cubierto la ciudad de murallas 
manner Oinez etorri zen 
Ahots ozen batez 
Bere familiaren laguntzaz erosi zuen 
Berdez margotzen ari dira 
Vino a pie 
En voz alta 
Lo compr? con la ayuda de su familia 
Lo estan pintando de verde 
cause Beldurrez zurbildu 
Maitasunez hil 
Con el miedo me qued? p?lido 
Morir de amor 
containing Edalontzia ardoz beteta dago 
Txapelaz dagoen gizona ikusi dut 
El baso esta lleno de vino 
He visto a un hombre con boina 
matter Armairua egurrez egina dago El armario est? hecho de madera 
Table 3: examples in Basque and Spanish for the set of possible interpretations. 
Table 4 also shows the most frequent baseline 
(MF), constructed as follows: for each occurrence 
of the suffix, the three most frequent 
interpretations are chosen. The accuracy of this 
baseline is practically equal to that of the 
algorithm. Note that the frequency is computed on 
the same sample where it is applied, yielding 
better results than it should. 
5 Discussion 
The obtained results show a very good accuracy, 
leaving a remaining ambiguity of 3.1 results per 
example. This means that we were able to discard 
an average of 4 readings for each of the examples, 
introducing only 5.5% of error. The results are 
practically equal to the most frequent baseline, 
which is usually hard to beat using knowledge-
based techniques. 
Coverage of the method is very low, only 
2.3%, but this was not an issue for us, as we plan 
to couple this method with other Machine 
Learning techniques in a bootstrapping 
framework. Nevertheless, we are still interested in 
increasing the coverage, in order to obtain more 
training data. 
Next, we will analyze more in depth the causes 
of the low coverage, the sources of the errors and 
ambiguity and the interpretations of case suffixes 
and prepositions. 
5.1 Sources of low coverage 
As soon as we started devising this method, it was 
clear to us that the coverage will be rather low. 
The main reason is that different dictionaries tend 
to give different details in their definitions, or use 
differing paraphrases. This fact is intrinsic to our 
method, and accounts for the large majority of 
missing answers. 
On the other hand, the simple method used to 
find triples means that a change in the order of the 
complements will cause our method to fail 
looking for a translation triple. Syntactic analysis, 
even shallow parsing methods, will help increase 
the coverage. 
Another source of discarded triples are the 
cases where the suffix is not translated by a 
preposition, e.g. the relation is carried out by a 
subject or direct object. When syntactic analysis is 
performed, we
interpretations o
5.2 Sources
Only five errors
were caused 
especially whe
determiner inste
- xixta/pric
needle 
- luma/feed
a submarine
There errors
parser. Other 
# interpretation 
   8 XPOST 
1 XLOK 
12 XZKO 
   3 XLEM 
   9 X 
1 No interpretation 
34 Total discarded 
  37 instrument 
  35 containing 
   7 instrument manner 
6 manner 
5 theme 
   1 cause 
0 matter 
0 during-time 
Table 4: fre
 
Dictionary 
cambridge 
Am. heritage
wordsmith 
Colmex 
vox_ya 
Rae 
overall  
MF baseline 
Table 5: result  
combination fo also plan to incorporate the 
f the other syntactic relations. 
 of error  
 we made by the algorithm, which 
by the wrong triple pairings, 
n the Basque triple contained a 
ad of the related word. Examples: 
k: punta batez osatua/made by a 
le: odi batez osatua/wake made by 
 
 could be avoided using a syntactic 
wrong pairings were caused by 
91 Total kept 
quency of tags in gold standard. 
total correct accur. ambig.
16 15 0.938 4.0
34 32 0.941 3.2
26 26 1.000 3.7
10 9 0.900 2.6
7 7 1.000 2.8
26 25 0.962 2.8
91 86 0.945 3.1
91 85 0.934 3.0
s for each of the dictionaries, overall
r all and the most frequent baseline. 
errors in the English PoS tagger, or chance made 
the algorithm find an unrelated definition.  
5.3 Remaining ambiguity 
The amount of readings left by our method in this 
experiment is rather high, around 3.1 readings 
compared to 7 possible readings for the 
instrumental. This is a strong reduction but we 
would like to make it even smaller. 
We plan to study which is the source of the 
residual ambiguity. Alternative sets of 
interpretations (cf. Section 5.4) with coarser 
grained differences and smaller ambiguity, could 
yield better results. Another alternative is to 
explore more infrequent translations of the case 
suffixes, which might yield a narrower overlap. 
This is the case for the instrumental case suffix 
being translated with from, up, etc. 
5.4 Interpretations of case suffixes and 
prepositions 
Different authors give differing interpretations for 
prepositions. It has been our choice to take a 
descriptive list of possible interpretations from a 
set of sources, mainly dictionaries and grammar 
books. 
This work covers only the instrumental case 
suffix and its translations to English and Spanish. 
If tables for all case suffixes and prepositions were 
built, the method could be applied to all case 
suffixes and prepositions, yielding disambiguated 
relations in all three languages. 
More theoretically committed lists of 
interpretations (Dorr et al, 1998; Civit et al, 
2000; Sowa, 2000) should also be considered, but 
unfortunately we have not found a full account for 
all prepositions. If such a full table of 
interpretations existed, it could be very easy to 
apply our method, and obtain the outcome in 
terms of these other interpretations. 
6 Conclusion and further work 
This paper presents preliminary experiments in the 
use of translation equivalences to disambiguate 
prepositions or case suffixes. The core of the 
method is to find translations of the occurrence of 
the target preposition or case suffix, and assign the 
intersection of their set of interpretations. The 
method is fully automatic, given a table with 
prepositions and their possible interpretations.  
We have tested this method on the occurrences 
of the Basque instrumental case -z in the 
definitions of a Basque dictionary. We have 
searched the translations in the definitions from 3 
Spanish and 3 English dictionaries.  
The results have been that we are able to 
disambiguate with 94.5% accuracy 2.3% of those 
occurrences (up to 91). The ambiguity is reduced 
from 7 readings down to 3.1. We think that these 
are very good results, especially seeing that there 
is room for improvement.  
More specifically, we plan to apply surface 
syntactic analysis to better extract the dependency 
relations, which is the main source of errors. We 
would like to study other inventories of 
preposition interpretations, both in order to have 
better theoretical foundations as well as to 
investigate whether coarser grained distinctions 
would lead to a reduction in the ambiguity.  
In the future, we plan to explore the possibility 
to feed a Machine Learning algorithm with the 
automatically disambiguated examples, in order to 
construct a full-fledged disambiguation algorithm 
following a bootstrapping approach. On the other 
hand, we would like to apply the method to the set 
of all prepositions and case suffixes, and beyond 
that to all syntactic dependencies. The results will 
be directly loaded in a Lexical Knowledge Base 
extracted from the Basque dictionary (Ansa et al, 
in prep.). 
We also plan to explore whether this method 
can be applied to free running text, removing the 
constraint that the translations have to be 
definitions of the equivalent word. 
Finally, this technique could be coupled with 
techniques that make use of the semantic types of 
the words in the context. 
Overall, we found the results are very 
encouraging given the simple techniques used, 
and we think that it shows great potential for 
improvement and interesting avenues for research. 
Acknowledgments 
Mikel Lersundi and David Martinez were 
supported by Basque Government grants AE-
BFI:98.217 and AE-BFI:01.2485. This work was 
partially funded by the MCYT HERMES project 
(TIC-2000-0335) and the EC MEANING project 
(IST-2001-34460). 
References 
Aduriz I., Aldezabal I., Alegria I., Artola X., 
Ezeiza N., Urizar R., 1996, "EUSLEM: A 
Lemmatiser / Tagger for Basque" Proc. Of 
EURALEX'96, G?teborg (Sweden) Part 1, 17-26. 
Ansa O., Arregi X., Lersundi M., ?A 
Conceptual Schema for a Basque Lexical-
Semantic Framework? (in preparation) 
Bosque, I., Demonte, V., 1999, Gramatica 
descriptiva de la lengua Espa?ola, Espasa, 
Madrid. 
Brants, T. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth 
Applied Natural Language Processing 
Conference, Seattle, WA. 
Cambridge, online. Cambridge 
International Dictionary of English 
http://dictionary.cambridge.org/ 
Civit, M., Castell?n, I., Mart?, M.A. and Taul?, 
M., 2000,  ?LEXPIR: a verb lexicon for Spanish? 
Cuadernos de Filolog?a Inglesa, Vol. 9.1. Corpus-
based Research in English Language and 
Linguistics, University of Granada. 
Colmex, online. Diccionario del espa?ol usual 
en M?xico (Colmex) http://mezcal.colmex.mx 
(also accessible from 
http://www.foreignword.com 
Dorr, Bonnie J., Nizar Habash, and David 
Traum, 1998, ?A Thematic Hierarchy for Efficient 
Generation from Lexical-Conceptual Structure,? 
in Proceedings of the Third Conference of the 
Association for MT in the America's, Langhorne, 
PA, pp. 333--343 
Elhuyar, 1996, Elhuyar Hiztegia, Elhuyar K.E., 
Usurbil. 
Euskaltzaindia, 1985, Euskal Gramatika Lehen 
Urratsak-I (EGLU-I), Euskaltzaindia, Bilbo. 
Heritage, online. The American Heritage? 
Dictionary of the English Language. 
http://www.bartleby.com/61 
J. Atserias, J. Carmona, I. Castellon, S. 
Cervell, M. Civit, L. Marquez, M.A. Marti, L. 
Padro, R.Placer, H. Rodriguez, M. Taule & J. 
Turmo ?Morphosyntactic Analysis and Parsing of 
Unrestricted Spanish Text? First International 
Conference on Language Resources and 
Evaluation (LREC'98). Granada, Spain, 1998. 
Miller, G. A., R. Beckwith, C. Fellbaum, D. 
Gross, and K. Miller. 1990. Five Papers on 
WordNet. Special Issue of International Journal of 
Lexicography, 3(4). 
Morris M., 1998, Morris Student dictionary, 
Klaudio Harluxet Fundazioa, Donostia. 
Rae, online. Diccionario de la Real Academia 
de la Lengua http://buscon.rae.es/drae/drae.htm 
Sarasola, I., 1996, Euskal Hiztegia, 
Gipuzkoako Kutxa, Donostia. 
John F. Sowa, 2000, Knowledge 
Representation: Logical, Philosophical, and 
Computational Foundations, Brooks Cole 
Publishing Co., Pacific Grove, CA 
John F. Sowa, ed. (1992) Knowledge-Based 
Systems, Special Issue on Conceptual Graphs, vol. 
5, no. 3, September 1992 
Vox, online. Diccionario General de la lengua 
espa?ola VOX http://www.vox.es/consultar.html 
Wordsmyth, online. The Wordsmyth 
Educational Dictionary-Thesaurus 
http://www.wordsmyth.net 
MEANING: a Roadmap to Knowledge Technologies 
 
German Rigau. TALP Research Center. UPC. Barcelona. rigau@lsi.upc.es 
Bernardo Magnini. ITC-IRST. Povo-Trento. magnini@itc.it 
Eneko Agirre. IXA group. EHU. Donostia. eneko@si.ehu.es  
Piek Vossen. Irion Technologies. Delft. Piek.Vossen@irion.nl  
John Carroll. COGS. U. Sussex. Brighton. johnca@cogs.susx.ac.uk 
 
Abstract  
Knowledge Technologies need to extract 
knowledge from existing texts, which 
calls for advanced Human Language 
Technologies (HLT). Progress is being 
made in Natural Language Processing but 
there is still a long way towards Natural 
Language Understanding. An important 
step towards this goal is the development 
of technologies and resources that deal 
with concepts rather than words. The 
MEANING project argues that we need to 
solve two complementary and 
intermediate tasks to enable the next 
generation of intelligent open domain 
HLT application systems: Word Sense 
Disambiguation and large-scale 
enrichment of Lexical Knowledge Bases. 
Innovations in this area will lead to HLT 
with deeper understanding of texts, and 
immediate progress in real applications of 
Knowledge Technologies. 
Introduction 
The field of Information Society Technologies 
(IST) is one of the main thematic priorities of 
the European Commission for the 6th Framework 
programme. In this field, Knowledge 
Technologies (KT) aim to provide meaning to 
the petabytes of information content our 
societies will generate in the near future. 
Information and knowledge management 
systems need to evolve accordingly, to enable 
the next generation of intelligent open domain 
Human Language Technologies (HLT) that will 
deal with the growing potential of the 
knowledge-rich and multilingual society. 
In order to develop a trustable semantic web 
infrastructure and a multilingual ontology 
framework to support knowledge management a 
wide range of techniques are required to 
progressively automate the knowledge lifecycle. 
In particular, this involves extracting high-level 
meaning from the large collections of content 
data and its representation and management in a 
common knowledge base. 
Even now, building large and rich knowledge 
bases takes a great deal of expensive manual 
effort; this has severely hampered Knowledge-
Technologies and HLT application development. 
For example, dozens of person-years have been 
invest into the development of wordnets1 for 
various languages, but the data in these 
resources is still not sufficiently rich to support 
advanced concept-based HLT applications 
directly. Furthermore, resources produced by 
introspection usually fail to register what really 
occurs in texts. Applications will not scale up to 
working in the open domain without more 
detailed and rich general-purpose, which should 
perhaps include domain-specific linguistic 
knowledge.  
The MEANING project identifies two 
complementary intermediate tasks which we 
think are crucial in order to enable the next 
generation of intelligent open domain HLT 
application systems: Word Sense 
Disambiguation (WSD) and large-scale 
enrichment of Lexical Knowledge Bases.  
                                                     
1 A wordnet is a conceptually structured knowledge 
base of word senses. The English WordNet (Miller 
90, Fellbaum 98) has been developed at Princeton 
University over the past 14 years. EuroWordNet 
(Vossen 1998) is a multilingual database with 
wordnets for several European languages (Dutch, 
Italian, Spanish, German, French, Czech and 
Estonian). Balkanet is building wordnets for the 
Balkan languages following the EuroWordNet 
design. 
The advance in these two areas will allow for 
large-scale extractions of shallow meaning from 
texts, in the form of relations among concepts. 
WSD provides the technology to convert 
relations between words into relations between 
concepts. Rich and large-scale Lexical 
Knowledge Bases will have be the repositories 
of extracted relations and other linguistic 
knowledge.  
However, progress is difficult due to the 
following interdependence: 
? In order to achieve accurate WSD, we need 
far more linguistic and semantic knowledge 
than is available in current lexical 
knowledge bases (e.g. current wordnets).  
? In order to enrich Lexical Knowledge Bases 
we need to acquire information from 
corpora, which have been accurately tagged 
with word senses.  
Providing innovative technology to solve this 
problem will be one of the main challenges to 
access KTs.  
Following this introduction section 1 presents 
the major research goals in HLT. Section 2 
presents the MEANING roadmap. Finally, 
section 4 draws the conclusions. 
1 Major research goals in HLT 
In order to extend the state-of-the-art in human 
language technologies (HLT) future research 
must devise: (1) innovative processes and tools 
for automatic acquisition of lexical knowledge 
from large-scale document collections; (2) novel 
techniques for accurately selecting the sense of 
open-class words in a large number of 
languages; (3) ways to enrich existing 
multilingual linguistic knowledge resources with 
new kinds of lexical information by 
automatically mapping information across 
languages. We present each one in turn. 
1.1 Dealing with knowledge acquisition 
The acquisition of linguistic knowledge from 
corpora has been a very successful line of 
research. Research in the acquisition of 
subcategorization information, selectional 
preferences, in thematic role assignments and 
diathesis alternations (Agirre and Mart?nez 
2001, 2002, McCarthy and Korhonen, 1998; 
Korhonen et al, 2000; McCarthy 2001), domain 
information (Magnini and Cavagli? 2000), topic 
signatures (Agirre et al 2001b), lexico-semantic 
relations between words (Agirre et al 2002) etc. 
has obtained encouraging results. The 
acquisition process usually involves large bodies 
of text, which have been previously processed 
with shallow language processors.  
Much of the use of the acquired knowledge 
has been hampered by the fact that the texts are 
not sense-disambiguated, and therefore, only 
knowledge for words can be acquired, that is, 
subcategorization for words, selectional 
preferences for words, etc. It is a well 
established fact that much of the linguistic 
behavior of words can be better explained if it is 
keyed to word senses.  
For instance, the subcategorization frames of 
verbs are highly dependent of the sense of the 
verb. Some senses of a given verb allow for a 
particular combination of complements, while 
others do not (McCarthy, 2001). The same is 
applicable to selectional preferences; traditional 
approaches that learn selectional preferences for 
a verb, tend to mix e.g. all subjects for differents 
senses, even if verbs can have different 
selectional preferences for each word sense 
(Agirre & Martinez, 2002). 
Having texts automatically sense-tagged with 
high accuracy will produce significantly better 
acquired knowledge at a sense level, including 
subcategorization frequencies, domain 
information, topic signatures, selectional 
preferences, specific lexico-semantic relations, 
thematic role assignments and diathesis 
alternations. It will also facilitate the 
investigation on automatic methods for dealing 
with new senses not present in current wordnets 
and clustering of word senses. Furthermore, 
linguistic information keyed to word senses that 
are linked to interlingual concepts (as proposed 
in the EuroWordNet model), can be easily 
integrated in a multilingual Lexical Knowledge 
Base (cf. section 2.3) 
2.2 Dealing with WSD 
Word Sense Disambiguation (WSD) is the task 
of assigning the appropriate meaning (sense) to a 
given word in a text or discourse. Ide and 
Veronis (1998) argue that word sense ambiguity 
is a central problem for many established HLT 
applications (for example Machine Translation, 
Information Extraction and Information 
Retrieval). This is also the case for associated 
sub-tasks (i.e. reference resolution and parsing). 
For this reason many international research 
groups are working on WSD, using a wide range 
of approaches. However, no large-scale broad-
coverage accurate WSD system has been built 
up to date2. With current state-of-the-art 
accuracy in the range 60-70%, WSD is one of 
the most important open problems in Natural 
Language Processing. 
A promising current line of research uses 
semantically annotated corpora to train Machine 
Learning (ML) algorithms to decide which word 
sense to choose in which contexts. The words in 
these annotated corpora are tagged manually 
with semantic classes taken from a particular 
lexical semantic resource (most commonly 
WordNet). Many standard ML techniques have 
been tried, such as Bayesian learning, Exemplar 
based learning, Decision Lists, and recently 
margin-based classifiers like Boosting and 
Support Vector Machines (Escudero et al, 
2000a, 2000b, 2000c, 2000d, 2001; Mart?nez 
and Agirre, 2000). These approaches are termed 
"supervised" because they learn from previously 
sense annotated data and therefore they require a 
large amount of human intervention to annotate 
the training data. 
Supervised WSD systems are data hungry. 
They suffer from the "knowledge acquisition 
bottleneck", it takes them mere seconds to digest 
all of the processed corpus contained in training 
materials that take months to annotate manually. 
So, although Machine Learning classifiers are 
undeniably effective, they are not feasible until 
we can obtain reliable unsupervised training 
data. Ng (1997) estimates that the manual 
annotation effort necessary to build a broad 
coverage word-sense annotated English corpus 
is about 16 person-years; and this effort would 
have to be replicated for each different language. 
Unfortunately, many people think that Ng?s 
estimate might fell short, as the annotated corpus 
thus produced is not guaranteed to enable high 
accuracy WSD.  
Some recent work is focusing on reducing 
the acquisition cost and the need for supervision 
                                                     
2 See the conclusions of the SENSEVAL-2 
competition: http://www.sle.sharp.co.uk/senseval2/ 
in corpus-based methods for WSD. Leacock et 
al. (1998) and Mihalcea and Moldovan (1999) 
automatically generate arbitrarily large corpora 
for unsupervised WSD training, using the 
synonyms or definitions of word senses 
provided in WordNet to formulate search engine 
queries over the Web. In another line of 
research, (Yarowsky, 1995) and (Blum and 
Mitchell, 1998) have shown that it is possible to 
reduce the need for supervision with the help of 
large amounts of unannotated data. Applying 
these ideas, (Agirre and Mart?nez, 2000) has 
developed knowledge-based prototypes for 
obtaining accurate examples from the web for 
specific WordNet synsets, as well as, large 
quantities of unannotated examples. 
But in order to make significant advances in 
WSD system accuracy, systems need to be able 
to use types of lexical knowledge that are not 
currently available in wide-coverage lexical 
knowledge bases: for example subcategorisation 
frequencies for predicates (particularly verbs) 
rely on word senses, selectional preferences of 
predicates for classes of arguments, amongst 
others (Carroll and McCarthy, 2000; McCarthy 
et al, 2001; Agirre and Mart?nez, 2002;).  
2.3 Dealing with multilingualism  
Language diversity is at the same time a 
valuable cultural heritage worth preserving, and 
an obstacle to achieving a more cohesive social 
and economic development. This situation has 
been further stressed as a major challenge in IST 
research lines. Improving language 
communication capabilities is a prerequisite for 
increasing industrial competitiveness, this way 
leading to a sound growth in key economic 
sectors.  
However, this obstacle can be helpful 
because all languages realize the meaning in 
different ways. We can benefit from this fact 
using a novel multilingual mapping process that 
exploits the EuroWordNet architecture. In 
EuroWordNet local wordnets are linked via an 
Inter-Lingual-Index (ILI) allowing the 
connection from words in one language to 
translation equivalent words in any of the other 
languages. In that way, technological advances 
in one language can help the other.  
For instance, for Basque, being an 
agglutinative language with very rich 
morphological-syntactic information, it is 
possible to extract semantic relations that would 
be more difficult to capture in other languages. 
Below we can see an example of the relation 
betwewen silversmith and silver, extracted from 
the Basque words zilargile ? zilar respectively. 
This relation has been disambiguated into the 
?maker_of? lexico-semantic relation (Agirre & 
Lersundi, 2000).  
On the contrary, Basque is not largely present 
in the web as the others. Using this approach it is 
possible to balance both gaps.  
Although the technology to provide 
compatibility across wordnets exits (Daud? et al 
1999, 2000, 2001), new research is needed for 
porting and uploading the various types of 
knowledge across languages, and new ways to 
test the validity of the ported knowledge in the 
target languages.  
3. The MEANING Roadmap 
The improvements mentioned above have been 
explored separately with relative success. In 
fact, no research group in isolation has tried to 
combine all this aforementioned factors. We 
designed the MEANING project3 convinced that 
only a combination of all relevant knowledge 
and resources will be able to produce significant 
advances in this crucial research area.  
MEANING will treat the web as a (huge) 
corpus to learn information from, since even the 
largest conventional corpora available (e.g. the 
Reuters corpus, the British National Corpus) are 
not large enough to be able to acquire reliable 
information in sufficient detail about language 
behaviour. Moreover, most languages do not 
have large or diverse enough corpora available. 
MEANING proposes an innovative 
bootstrapping process to deal with the inter-
dependency between WSD and knowledge 
acquisition: 
1. Train accurate WSD systems and apply 
them to very large corpora by coupling 
knowledge-based techniques on the existing 
EuroWordNet (e.g. to populate it with 
domain labels, to induce automatically 
                                                     
                                                     3 Started in March 2002, MEANING IST-2001-
34460 "Developing Multilingual Web-scale 
Language Technologies" is a three years research 
project funded by the EC. 
training examples) with ML techniques that 
combine very large amounts of labeled and 
unlabeled data. When ready, use also the 
knowledge acquired in 2. 
2. Use the obtained accurate WSD data in 
conjunction with shallow parsing techniques 
and domain tagging to extract new linguistic 
knowledge to incorporate into 
EuroWordNet. 
This method will be able to break this 
interdependency in a series of cycles thanks to 
the fact that the WSD system will be based on 
all domain information, sophisticated linguistic 
knowledge, large numbers of automatically 
tagged examples from the web, and a 
combination of annotated and unannotated data. 
The first WSD system will have weaker 
linguistic knowledge, but the sole combination 
of the rest of the factors will produce significant 
performance gains. Besides, some of the 
required linguistic knowledge can be acquired 
from unnanotated data, and can therefore be 
acquired without using any WSD system. Once 
acceptable WSD is available, the acquired 
knowledge will be of a higher quality, and will 
allow for better WSD performance. 
Multilingualism will be also helpful for 
MEANING. The idiosyncratic way the meaning 
is realised in a particular language will be 
captured and ported to the rest of languages 
involved in the project4 using EuroWordNet as a 
Multilingual Central Repository in three 
consecutive phases (see figure 1). 
For instance, selectional preferences acquired 
for verb senses based on the English corpora, 
can be uploaded into the Multilingual Central 
Repository. As the selectional prefenrece 
relation is keyed to concepts in the repository, 
this knowledge can be ported to the other 
languages. Of course, the ported knowledge 
needs to be checked in order to evaluate the 
validity of this approach.  
Below, we can see the selectional preference 
for the first sense of know from (Agirre & 
martinez, 2002). The first sense of know is 
univocally linked to <know, cognize,
cognise>, which in EuroWordNet is linked to 
4 MEANING will work with three major European 
languages (English, Spanish and Italian) and two 
minority languages (Catalan and Basque).  
w
S
a
B
s
0
0
0
0
0
4
W
s
m
p
m
c
e
Multilingual Central Repository 
EANING is going to constitute 
wledge resource for a number of 
sses that need large amounts of 
to be effective tools (e.g. web 
P tools and software of the next 
l benefit from the MEANING 
Multilingual
Central Repository
Italian
EWN
Basque
EWN
Spanish
EWN
English
EWN
Basque
Web Corpus
Italian
Web Corpus
English
Web Corpus
Catalan
EWN
Spanish
Web Corpus
Catalan
Web Corpus
ACQ
ACQACQ
ACQ
UPLOADUPLOAD
UPLOADUPLOAD
PORT
PORT
PORT
PORT
WSD
WSD
WSD
WSD
 access applications are based on 
NG will open the way for access 
gual web based on concepts, 
lications with capabilities that 
ceed those currently available. 
ill facilitate development of 
pen domain Internet applications 
tion/Answering, Cross Lingual 
etrieval, Summarisation, Text 
Event Tracking, Information 
achine Translation, etc.). 
EANING will supply a common 
cture to Internet documents, thus 
owledge management of web 
ommon conceptual structure is a ord senses conocer_1 and saber_1 in 
panish, con?ixer_1 and saber_1 in Catalan 
nd antzeman_1, jakin_2 and ezagutu_1 in 
asque.  
ense 1: know, cognize -- (be
cognizant or aware of a fact or a
specific piece of information;
possess knowledge or information
about;
,1128 <communication> 
,0615 <measure quantity amount quantum> 
,0535 <attribute> 
,0389 <object physical_object> 
,0307 <cognition knowledge> 
 Conclusions 
here the acquisition of knowledge  from large-
cale document collections will be  one of the 
ajor challenge for the next generation of text 
rocessing applications, MEANING emphasises 
ultilingual  content-based access to web 
ontent. Moreover, it can provide a keystone 
nabling technologies for the semantic web. In 
particular, the 
produced by M
the natural kno
semantic proce
linguistic data 
ontologies). NL
generation wil
outcomes.  
Figure 1: MEANING data flow. 
Current web
words; MEANI
to the multilin
providing app
significantly ex
MEANING w
concept-based o
(such as Ques
Information R
Categorisation, 
Extraction, M
Furthermore, M
conceptual stru
facilitating kn
content. This c
decisive enabling technology for allowing the 
semantic web. 
Acknowledgements 
The MEANING project is funded by the 
European Commission (IST-2001-34460). 
References 
Agirre E. and Lersundi M. Extracci?n de relaciones 
l?xico-sem?nticas a partir de palabras derivadas 
usando patrones de definici?n. Proceedings of the 
Annual SEPLN meeting. Spain, 2000. 
Agirre E., Lersundi M. and Mart?nez D. A 
Multilingual Approach to Disambiguate 
Prepositions and Case Suffixes. Proceeding of the 
Workshop ?Word Sense Disambiguation: Recent 
Successes and Future Directions? organized by 
ACL 2002.  
Agirre E. and Mart?nez D. Exploring automatic word 
sense disambiguation with decision lists and the 
Web.  Proceedings of the Workshop ?Semantic 
Annotation And Intelligent Annotation? organized 
by COLING 2000. Luxembourg. 2000.  
Agirre E. and Martinez D. Learning class-to-class 
selectional preferences. Proceedings of the 
Workshop "Computational Natural Language 
Learning" (CoNLL-2001). In conjunction with 
ACL'2001/EACL'2001. Toulouse. 2001. 
Agirre E., Ansa O., Mart?nez D. and Hovy E. 
Enriching WordNet concepts with topic signatures. 
Proceedings of the NAACL workshop on WordNet 
and Other lexical Resources: Applications, 
Extensions and Customizations. Pittsburg. 2001. 
Agirre E. and Martinez D. Integrating selectional 
preferences in WordNet. Proceedings of the first 
International WordNet Conference. Mysore, India, 
2002. 
Blum A. and Mitchel T. Combining labelled and 
unlabeled data with co-training. In Proceedings of 
the 11th Annual Conference on Computational 
Learning Theory. 1998. 
Carroll, J. and McCarthy, D. Word sense 
disambiguation using automatically acquired 
verbal preferences. Computers and the Humanities. 
Senseval Special Issue, Vol. 34, No 1-2. 2000. 
Daud? J., Padr? L. and Rigau G., Mapping 
Multilingual Hierarchies using Relaxation 
Labelling, Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora (EMNLP/VLC'99). Maryland, 
1999.  
Daud? J., Padr? L. and Rigau G., Mapping WordNets 
Using Structural Information , 38th Anual Meeting 
of the ACL. Hong Kong, 2000.  
Daud? J., Padr? L. and Rigau G., A Complete WN1.5 
to WN1.6 Mapping, Proceedings of NAACL 
Workshop "WordNet and Other Lexical Resources: 
Applications, Extensions and Customizations". 
Pittsburg, PA, 2001. 
Escudero G., M?rquez L. and Rigau G., Boosting 
Applied to Word Sense Disambiguation. 
Proceedings of the 11th European Conference on 
Machine Learning. Barcelona. 2000.  
Escudero G., M?rquez L. and Rigau G., Naive Bayes 
and Exemplar-Based approaches to Word Sense 
Disambiguation Revisited.  Proceedings of the 14th 
European Conference on Artificial Intelligence, 
Berlin. 2000.  
Escudero G., M?rquez L. and Rigau G., A 
Comparison between Supervised Learning 
Algorithms for Word Sense Disambiguation. 
Proceedings of Fourth Computational Natural 
Language Learning Workshop. Lisbon. 2000.  
Escudero G., M?rquez L. and Rigau G., An Empirical 
Study of the Domain Dependence of Supervised 
Word Sense Disambiguation Systems. Proceedings 
of Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora. Hong Kong. 2000. 
Escudero G., M?rquez L. and Rigau G., Using 
LazyBoosting for Word Sense Disambiguation. 
Proceedings of 2nd International Workshop 
?Evaluating Word Sense Disambiguation 
Systems?, SENSEVAL-2. Toulouse. 2001. 
Fellbaum C. editor. WordNet An Electronic Lexical 
Database. The MIT Press. 1998. 
Ide, N. and V?ronis, J. Introduction to the special 
issue on word sense disambiguation: The state of 
the art. Computational Linguistics, 24 (1), 1998. 
Korhonen A., Gorrell, G. and McCarthy D. Statistical 
Filtering and Subcategorization Frame 
Acquisition. In Proceedings of the Joint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora. 
Hong Kong. 2000. 
Leacock, C. Chodorow, M. and Miller, G.A. Using 
Corpus Statistics and WordNet Relations for Sense 
Identication, Computational Linguistics, 24(1), 
1998. 
Magnini B. and Cavagli? G., Integrating subject field 
codes into WordNet. In Proceedings of the 2nd 
International Conference on Language Resources 
and Evaluation, Athens. 2000. 
Mart?nez D. and Agirre E. One Sense per Collocation 
and Genre/Topic Variations. Proceedings of the 
Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora. Hong Kong, 2000. 
McCarthy, D. and Korhonen, A. Detecting verbal 
participation in diathesis alternations. Proceedings 
of the 17th International Conference on 
Computational Linguistics and 36th Annual 
Meeting of the Association for Computational 
Linguistics COLING-ACL'98. Montreal. 1998.  
McCarthy D., Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, 
Subcategorization Frames and Selectional 
Preferences. Ph.D. thesis, University of Sussex. 
2001. 
McCarthy D., Carroll J. and Preiss J. Disambiguating 
noun and verb senses using automatically acquired 
selectional preferences. Proceedings of the 
SENSEVAL-2 Workshop at ACL/EACL'01, 
Toulouse. 2001. 
Mihalcea R. and Moldovan D. An automatic method 
for generating sense tagged corpora. In 
Proceedings of American Association for Artificial 
Intelligence. 1999. 
Miller G. Five papers on WordNet, Special Issue of 
International Journal of Lexicogrphy 3(4). 1990. 
Ng. H. T. Getting Serious about Word Sense 
Disambiguation. In Proceedings of Workshop 
?Tagging Text with Lexical Semantics: Why, what 
and how??, Washington, 1997. 
Vossen P. EuroWordNet: A Multilingual Database 
with Lexical Semantic Networks, Kluwer Academic 
Publishers, Dordrecht. 1998. 
Yarowsky D., Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics. 1995. 
The Basque lexical-sample task  
Eneko Agirre, Itziar Aldabe, Mikel Lersundi, David Martinez, Eli Pociello, Larraitz Uria(*) 
IxA NLP group, Basque Country University  
649 pk. 20.080 Donostia, Spain 
eneko@si.ehu.es 
 
Abstract 
In this paper we describe the Senseval 3 
Basque lexical sample task. The task 
comprised 40 words (15 nouns, 15 verbs and 
10 adjectives) selected from the Basque 
WordNet. 10 of the words were chosen in 
coordination with other lexical-sample tasks. 
The examples were taken from newspapers, an 
in-house balanced corpus and Internet texts. 
We additionally included a large set of 
untagged examples, and a lemmatised version 
of the data including lemma, PoS and case 
information. The method used to hand-tag the 
examples produced an inter-tagger agreement 
of 78.2% before arbitration. The eight 
competing systems attained results well above 
the most frequent baseline and the best system 
from Swarthmore College scored 70.4% 
recall. 
1 Introduction 
This paper reviews the Basque lexical-sample task 
organized for Senseval 3. Each participant was 
provided with a relatively small set of labelled 
examples (2/3 of 75+15*senses+7*multiwords) 
and a comparatively large set of unlabelled 
examples (roughly ten times more when possible) 
for around 40 words. The larger number of 
unlabelled data was released with the purpose to 
enable the exploration of semi-supervised systems. 
The test set comprised 1/3 of the tagged examples. 
The sense inventory was taken from the Basque 
WordNet, which is linked to WordNet version 1.6 
(Fellbaum, 1998). The examples came mainly from 
newspaper texts, although we also used a balanced 
in-house corpus and texts from Internet. The words 
selected for this task were coordinated with other 
lexical-sample tasks (such as Catalan, English, 
Italian, Romanian and Spanish) in order to share 
around 10 of the target words.  
The following steps were taken in order to carry 
out the task: 
                                                     
(*) Authors listed in alphabetic order. 
1. set the exercise  
a. choose sense inventory from a pre-existing 
resource 
b. choose target corpora 
c. choose target words  
d. lemmatize the corpus automatically 
e. select examples from the corpus 
2. hand-tagging 
a. define the procedure 
b. revise the sense inventory 
c. tag 
d. analyze the inter-tagger agreement 
e. arbitrate 
This paper is organized as follows: The 
following section presents the setting of the 
exercise. Section 3 reviews the hand-tagging, and 
Section 4 the details of the final release. Section 5 
shows the results of the participant systems. 
Section 6 discusses some main issues and finally, 
Section 7 draws the conclusions. 
2 Setting of the exercise  
In this section we present the setting of the 
Basque lexical-sample exercise. 
2.1 Basque 
As Basque is an agglutinative language, the 
dictionary entry takes each of the elements 
necessary to form the different functions. More 
specifically, the affixes corresponding to the 
determinant, number and declension case are taken 
in this order and independently of each other (deep 
morphological structure). For instance, ?etxekoari 
emaiozu? can be roughly translated as ?[to the one 
in the house] [give it]? where the underlined 
sequence of suffixes in Basque corresponds to ?to 
the one in the?.  
2.2 Sense inventory 
We chose the Basque WordNet, linked to 
WordNet 1.6, for the sense inventory. This way, 
the hand tagging enabled us to check the sense 
coverage and overall quality of the Basque 
WordNet, which is under construction. The Basque 
WordNet is available at http://ixa3.si.ehu.es/ 
wei3.html. 
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2.3 Corpora used 
Being Basque a minority language it is not easy 
to find the required number of occurrences for each 
word. We wanted to have both balanced and 
newspaper examples, but we also had to include 
texts extracted from the web, specially for the 
untagged corpus. The procedure to find examples 
from the web was the following: for each target 
word all possible morphological declensions were 
automatically generated, searched in a search-
engine, documents retrieved, automatically 
lemmatized (Aduriz et al 2000), filtered using 
some heuristics to ensure quality of context, and 
finally filtered for PoS mismatches. Table 1 shows 
the number of examples from each source. 
2.4 Words chosen 
Basically, the words employed in this task are 
the same words used in Senseval 2 (40 words, 15 
nouns, 15 verbs and 10 adjectives), only the sense 
inventory changed. Besides, in Senseval 3 we 
replaced 5 verbs with new ones. The reason for this 
is that in the context of the MEANING project1 we 
are exploring multilingual lexical acquisition, and 
there are ongoing experiments that focus on those 
verbs. (Agirre et al 2004; Atserias et al 2004). 
In fact, 10 words in the English lexical-sample 
have translations in the Basque, Catalan, Italian, 
Romanian and Spanish lexical tasks: channel, 
crown, letter, program, party (nouns), simple 
(adjective), play, win, lose, decide (verbs).  
2.5 Selection of examples from corpora 
The minimum number of examples for each 
word according to the task specifications was 
calculated as follows: 
 
N=75+15*senses+7*multiwords  
 
As the number of senses in WordNet is very high, 
we decided to first estimate the number of senses 
and multiwords that really occur in the corpus. The 
taggers were provided with a sufficient number of 
examples, but they did not have to tag all. After 
they had tagged around 100 examples, they would 
count the number of senses and multiwords that 
had occurred and computed the N according to 
those counts.  
The context is constituted of 5 sentences, 
including the sentence with the target word 
appearing in the middle. Links were kept to the 
source corpus, document, and to the newspaper 
section when applicable.  
The occurrences were split at random in training 
set (2/3 of all occurrences) and test set (1/3).  
                                                     
1 http://www.lsi.upc.es/~nlp/meaning/meaning.html 
 Total (N) (B) (I)
# words 40  
# senses 316  
# number of tagged examples 7362 5695 924 743
# number of untagged examples 62498 - - 62498
# tags  9887  
Table 1: Some figures regarding the task. N, B and I 
correspond to the source of the examples: newspaper, 
balanced corpus and Internet respectively. 
3 Hand tagging 
Three persons, graduate linguistics students, 
took part in the tagging. They are familiar with 
word senses, as they are involved in the 
development of the Basque WordNet. The 
following procedure was defined in the tagging of 
each word. 
? Before tagging, one of the linguists (the editor) 
revised the 40 words in the Basque WordNet. 
She had to delete and add senses to the words, 
specially for adjectives and verbs, and was 
allowed to check the examples in the corpus.  
? The three taggers would meet, read the glosses 
and examples given in the Basque WordNet 
and discuss the meaning of each synset. They 
tried to agree and clarify the meaning 
differences among the synsets. For each word 
two hand-taggers and a referee is assigned by 
chance. 
? The number of senses of a word in the Basque 
WordNet might change during this meeting; 
that is, linguists could agree that one of the 
word?s senses was missing, or that a synset did 
not fit with a word. This was done prior to 
looking at the corpus. Then, the editor would 
update the Basque WordNet according to those 
decisions before giving the taggers the final 
synset list. Overall (including first bullet 
above), 143 senses were deleted and 92 senses 
added, leaving a total of 316 senses. This 
reflects the current situation of the Basque 
WordNet, which is still under construction. 
? Two taggers independently tagged all 
examples for the word. No communication was 
allowed while tagging the word. 
? Multiple synset tags were allowed, as well as 
the following tags: the lemma (in the case of 
multiword terms), U (unassignable), P (proper 
noun), and X (incorrectly lemmatized). Those 
with an X were removed from the final release. 
In the case of proper nouns and multiword 
terms no synset tag was assigned. Sometimes 
the U tag was used for word senses which are 
not in the Basque WordNet. For instance, the 
sense of kanal corresponding to TV channel, 
which is the most frequent sense in the 
examples, is not present in the Basque 
WordNet (it was not included in WordNet 1.6).  
? A program was used to compute agreement 
rates and to output those occurrences where 
there was disagreement. Those occurrences 
were  grouped by the senses assigned. 
? A third tagger, the referee, reviewed the 
disagreements and decided which one was the 
correct sense (or senses).  
The taggers were allowed to return more than one 
sense, and they returned 9887 tags (1.34 per 
occurrence). Overall, the two taggers agreed in at 
least one tag 78.2% of the time. Some words 
attained an agreement rate above 95% (e.g. nouns 
kanal or tentsio), but others like herri ?
town/people/nation? attained only 52% agreement. 
On average, the whole tagging task took 54 
seconds per occurrence for the tagger, and 20 
seconds for the referee. However, this average 
does not include the time the taggers and the 
referee spent in the meetings they did to 
understand the meaning of each synset. The 
comprehension of a word with all its synsets 
required 45.5 minutes on average. 
4 Final release 
Table 1 includes the total amount of hand-tagged 
and untagged examples that were released. In 
addition to the usual release, the training and 
testing data were also provided in a lemmatized 
version (Aduriz et al 2000) which included 
lemma, PoS and case information. The motivation 
was twofold: 
? to make participation of the teams easier, 
considering the deep inflection of Basque. 
? to factor out the impact of different 
lemmatizers and PoS taggers in the system 
comparison.  
5 Participants and Results 
5 teams took part in this task: Swarthmore 
College (swat), Basque Country University 
(BCU), Instituto per la Ricerca Scientifica e 
Tecnologica (IRST), University of Minnesota 
Duluth (Duluth) and University of Maryland 
(UMD). All the teams presented supervised systems 
which only used the tagged training data, and no 
other external resource. In particular, no system 
used the pointers to the full texts, or the additional 
untagged texts. All the systems used the lemma, 
PoS and case information provided, except the 
BCU team, which had additional access to number, 
determiner and ellipsis information directly from 
the analyzer. This extra information was not 
provided publicly because of representation issues.  
 
 Prec. Rec. Attempted
basque-swat_hk-bo 71.1  70.4  99.04 %
BCU_Basque_svm 69.9  69.9  100.00 %
BCU_-_Basque_Comb 69.5  69.5  100.00 %
swat-hk-basque 67.0  67.0  100.00 %
IRST-Kernels-bas 65.5  65.5  100.00 %
swat-basque 64.6  64.6  100.00 %
Duluth-BLSS 60.8  60.8  100.00 %
UMD_SST1 65.6  58.7  89.42 %
MFS 55.8  55.8  100.00 %
Table 2: Results of systems and MFS baseline, ordered 
according to Recall. 
We want to note that due to a bug, a few examples 
were provided without lemmas.  
The results for the fine-grained scoring are 
shown in Table 2, including the Most Frequent 
Sense baseline (MFS). We will briefly describe 
each of the systems presented by each team in 
order of best recall.  
? Swat presented three systems based in the 
same set of features: the best one was based on 
Adaboost, the second on a combination of five 
learners (Adaboost, maximum entropy, 
clustering system based on cosine similarity, 
decision lists, and na?ve bayes, combined by 
majority voting), and the third on a 
combination of three systems (the last three).  
? BCU presented two systems: the first one based 
on Support Vector Machines (SVM) and the 
second on a majority-voting combination of 
SVM, cosine based vectors and na?ve bayes.  
? IRST participated with a kernel-based method. 
? Duluth participated with a system that votes 
among three bagged decision trees. 
? UMD presented a system based on SVM. 
The winning system is the one using Adaboost 
from Swat, followed closely by the BCU system 
using SVM. 
6 Discussion 
These are the main issues we think are 
interesting for further discussion. 
Sense inventory. Using the Basque WordNet 
presented some difficulties to the taggers. The 
Basque WordNet has been built using the 
translation approach, that is, the English synsets 
have been ?translated? into Basque. The taggers 
had some difficulties to comprehend synsets, and 
especially, to realize what makes a synset different 
from another. In some cases the taggers decided to 
group some of the senses, for instance, in herri ?
town/people/nation? they grouped 6 senses. This 
explains the relatively high number of tags per 
occurrence (1.34). The taggers think that the 
tagging would be much more satisfactory if they 
had defined the word senses directly from the 
corpus.  
Basque WordNet quality. There was a 
mismatch between the Basque WordNet and the 
corpus: most of the examples were linked to a 
specific genre, and this resulted in i) having a 
handful of senses in the Basque WordNet that did 
not appear in our corpus and ii) having some 
senses that were not included in the Basque 
WordNet. Fortunately, we already predicted this 
and we had a preparation phase where the editor 
enriched WordNet accordingly. Most of the 
deletions in the preliminary part were due to the 
semi-automatic method to construct the Basque 
WordNet. All in all, we think that tagging corpora 
is the best way to ensure the quality of the 
WordNets and we plan to pursue this extensively 
for the improvement of the Basque WordNet.  
7 Conclusions and future work 
5 teams participated in the Basque lexical-
sample task with 8 systems. All of the participants 
presented supervised systems which used lemma, 
PoS and case information provided, but none used 
the large amount of untagged senses provided by 
the organizers. The winning system attained 70.4 
recall. Regarding the organization of the task, we 
found that the taggers were more comfortable 
grouping some of the senses in the Basque 
WordNet. We also found that tagging word senses 
is essential for enriching and quality checking of 
the Basque WordNet. 
Acknowledgements 
The work has been partially funded by the 
European Commission (MEANING project IST-
2001-34460). Eli Pociello has a PhD grant from 
the Basque Government.  
References  
I. Aduriz, E. Agirre, I. Aldezabal, I. Alegria, X. 
Arregi, J.M. Arriola, X. Artola, K. Gojenola, A. 
Maritxalar, K. Sarasola, M. Urkia. 2000. A 
Word-grammar Based Morphological Analyzer 
for Agglutinative Languages. In Proceedings of 
the International Conference on Computational 
Linguistics (COLING). Saarbrucken, Germany.  
E. Agirre, A. Atutxa, K. Gojenola, K. Sarasola. 
2004. Exploring portability of syntactic 
information from English to Basque. In 
Proceedings of the 4rd International Conference 
on Languages Resources and Evaluations 
(LREC). Lisbon, Portugal. 
J. Atserias, B. Magnini, O. Popescu, E. Agirre, A. 
Atutxa, G. Rigau, J. Carroll and R. Koeling 
2004. Cross-Language Acquisition of Semantic 
Models for Verbal Predicates. In Proceedings of 
the 4rd International Conference on Languages 
Resources and Evaluations (LREC). Lisbon, 
Portugal. 
C. Fellbaum. 1998. WordNet: An electronic 
Lexical Database. The MIT Press, Cambridge, 
Massachusetts.  
The Basque Country University system: English and Basque tasks
Eneko Agirre
IXA NLP Group
Basque Country University
Donostia, Spain
eneko@si.ehu.es
David Martinez
IXA NLP Group
Basque Country University
Donostia, Spain
davidm@si.ehu.es
Abstract
Our group participated in the Basque and En-
glish lexical sample tasks in Senseval-3. A
language-specific feature set was defined for
Basque. Four different learning algorithms were
applied, and also a method that combined their
outputs. Before submission, the performance
of the methods was tested for each task on the
Senseval-3 training data using cross validation.
Finally, two systems were submitted for each
language: the best single algorithm and the best
ensemble.
1 Introduction
Our group (BCU, Basque Country University),
participated in the Basque and English lexical
sample tasks in Senseval-3. We applied 4 differ-
ent learning algorithms (Decision Lists, Naive
Bayes, Vector Space Model, and Support Vector
Machines), and also a method that combined
their outputs. These algorithms were previously
tested and tuned on the Senseval-2 data for En-
glish. Before submission, the performance of
the methods was tested for each task on the
Senseval-3 training data using 10 fold cross val-
idation. Finally, two systems were submitted
for each language, the best single algorithm and
the best ensemble in cross-validation.
The main difference between the Basque and
English systems was the feature set. A rich
set of features was used for English, includ-
ing syntactic dependencies and domain infor-
mation, extracted with different tools, and also
from external resources like WordNet Domains
(Magnini and Cavaglia?, 2000). The features for
Basque were different, as Basque is an agglu-
tinative language, and syntactic information is
given by inflectional suffixes. We tried to rep-
resent this information in local features, relying
on the analysis of a deep morphological analyzer
developed in our group (Aduriz et al, 2000).
In order to improve the performance of the al-
gorithms, different smoothing techniques were
tested on the English Senseval-2 lexical sam-
ple data (Agirre and Martinez, 2004), and ap-
plied to Senseval-3. These methods helped to
obtain better estimations for the features, and
to avoid the problem of 0 counts Decision Lists
and Naive Bayes.
This paper is organized as follows. The learn-
ing algorithms are first introduced in Section 2,
and Section 3 describes the features applied to
each task. In Section 4, we present the exper-
iments performed on training data before sub-
mission; this section also covers the final config-
uration of each algorithm, and the performance
obtained on training data. Finally, the official
results in Senseval-3 are presented and discussed
in Section 5.
2 Learning Algorithms
The algorithms presented in this section rely on
features extracted from the context of the target
word to make their decisions.
The Decision List (DL) algorithm is de-
scribed in (Yarowsky, 1995b). In this algorithm
the sense with the highest weighted feature is se-
lected, as shown below. We can avoid undeter-
mined values by discarding features that have a
0 probability in the divisor. More sophisticated
smoothing techniques have also been tried (cf.
Section 4).
arg max
k
w(s
k
, f
i
) = log(
Pr(s
k
|f
i
)
?
j =k
Pr(s
j
|f
i
)
)
The Naive Bayes (NB) algorithm is based
on the conditional probability of each sense
given the features in the context. It also re-
quires smoothing.
arg max
k
P (s
k
)
?
m
i=1
P (f
i
|s
k
)
For the Vector Space Model (V) algo-
rithm, we represent each occurrence context as
a vector, where each feature will have a 1 or 0
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
value to indicate the occurrence/absence of the
feature. For each sense in training, one cen-
troid vector is obtained. These centroids are
compared with the vectors that represent test-
ing examples, by means of the cosine similarity
function. The closest centroid is used to assign
its sense to the testing example. No smooth-
ing is required to apply this algorithm, but it is
possible to use smoothed values.
Regarding Support Vector Machines
(SVM) we utilized SVM-Light (Joachims,
1999), a public distribution of SVM. Linear ker-
nels were applied, and the soft margin (C) was
estimated per each word (cf. Section 4).
3 Features
3.1 Features for English
We relied on an extensive set of features of
different types, obtained by means of different
tools and resources. The features used can be
grouped in four groups:
Local collocations: bigrams and trigrams
formed with the words around the target. These
features are constituted with lemmas, word-
forms, or PoS tags1. Other local features
are those formed with the previous/posterior
lemma/word-form in the context.
Syntactic dependencies: syntactic depen-
dencies were extracted using heuristic patterns,
and regular expressions defined with the PoS
tags around the target2. The following rela-
tions were used: object, subject, noun-modifier,
preposition, and sibling.
Bag-of-words features: we extract the
lemmas of the content words in the whole con-
text, and in a ?4-word window around the tar-
get. We also obtain salient bigrams in the con-
text, with the methods and the software de-
scribed in (Pedersen, 2001).
Domain features: The WordNet Domains
resource was used to identify the most relevant
domains in the context. Following the relevance
formula presented in (Magnini and Cavaglia?,
2000), we defined 2 feature types: (1) the most
relevant domain, and (2) a list of domains above
a predefined threshold3. Other experiments us-
ing domains from SUMO, the EuroWordNet
1The PoS tagging was performed with the fnTBL
toolkit (Ngai and Florian, 2001).
2This software was kindly provided by David
Yarowsky?s group, from Johns Hopkins University.
3The software to obtain the relevant domains was
kindly provided by Gerard Escudero?s group, from Uni-
versitat Politecnica de Catalunya
top-ontology, and WordNet?s Semantic Fields
were performed, but these features were dis-
carded from the final set.
3.2 Features for Basque
Basque is an agglutinative language, and syn-
tactic information is given by inflectional suf-
fixes. The morphological analysis of the text is
a necessary previous step in order to select in-
formative features. The data provided by the
task organization includes information about
the lemma, declension case, and PoS for the par-
ticipating systems. Our group used directly the
output of the parser (Aduriz et al, 2000), which
includes some additional features: number, de-
terminer mark, ambiguous analyses and elliptic
words. For a few examples, the morphological
analysis was not available, due to parsing errors.
In Basque, the determiner, the number and
the declension case are appended to the last el-
ement of the phrase. When defining our fea-
ture set for Basque, we tried to introduce the
same knowledge that is represented by features
that work well for English. We will describe
our feature set with an example: for the phrase
?elizaren arduradunei? (which means ?to the
directors of the church?) we get the following
analysis from our analyzer:
eliza |-ren |arduradun |-ei
church |of the |director |to the +pl.
The order of the words is the inverse in En-
glish. We extract the following information for
each word:
elizaren:
Lemma: eliza (church)
PoS: noun
Declension Case: genitive (of)
Number: singular
Determiner mark: yes
arduradunei:
Lemma: arduradun (director)
PoS: noun
Declension Case: dative (to)
Number: plural
Determiner mark: yes
We will assume that eliza (church) is the
target word. Words and lemmas are shown
in lowercase and the other information in up-
percase. As local features we defined different
types of unigrams, bigrams, trigrams and a
window of ?4 words. The unigrams were con-
structed combining word forms, lemmas, case,
number, and determiner mark. We defined 4
kinds of unigrams:
Uni wf0 elizaren
Uni wf1 eliza SING+DET
Uni wf2 eliza GENITIVE
Uni wf3 eliza SING+DET GENITIVE
As for English, we defined bigrams based on
word forms, lemmas and parts-of-speech. But
in order to simulate the bigrams and trigrams
used for English, we defined different kinds of
features. For word forms, we distinguished two
cases: using the text string (Big wf0), or using
the tags from the analysis (Big wf1). The word
form bigrams for the example are shown below.
In the case of the feature type ?Big wf1?, the
information is split in three features:
Big wf0 elizaren arduradunei
Big wf1 eliza GENITIVE
Big wf1 GENITIVE arduradun PLUR+DET
Big wf1 arduradun PLUR+DET DATIVE
Similarly, depending on the use of the de-
clension case, we defined three kinds of bigrams
based on lemmas:
Big lem0 eliza arduradun
Big lem1 eliza GENITIVE
Big lem1 GENITIVE arduradun
Big lem1 arduradun DATIVE
Big lem2 eliza GENITIVE
Big lem2 arduradun DATIVE
The bigrams constructed using Part-of-
speech are illustrated below. We included the
declension case as if it was another PoS:
Big pos -1 NOUN GENITIVE
Big pos -1 GENITIVE NOUN
Big pos -1 NOUN DATIVE
Trigrams are built similarly, by combining the
information from three consecutive words. We
also used as local features all the content words
in a window of ?4 words around the target. Fi-
nally, as global features we took all the con-
tent lemmas appearing in the context, which
was constituted by the target sentence and the
two previous and posterior sentences.
One difficult case to model in Basque is the el-
lipsis. For example, the word ?elizakoa? means
?the one from the church?. We were able to
extract this information from our analyzer and
we represented it in the features, using a special
symbol in place of the elliptic word.
4 Experiments on training data
The algorithms that we applied were first tested
on the Senseval-2 lexical sample task for En-
glish. The best versions were then evaluated by
10 fold cross-validation on the Senseval-3 data,
both for Basque and English. We also used the
training data in cross-validation to tune the pa-
rameters, such as the smoothed frequencies, or
the soft margin for SVM. In this section we will
describe first the parameters of each method
(including the smoothing procedure), and then
the cross-validation results on the Senseval-3
training data.
4.1 Methods and Parameters
DL: On Senseval-2 data, we observed that
DL improved significantly its performance with
a smoothing technique based on (Yarowsky,
1995a). For our implementation, the smoothed
probabilities were obtained by grouping the ob-
servations by raw frequencies and feature types.
As this method seems sensitive to the feature
types and the amount of examples, we tested
3 DL versions: DL smooth (using smoothed
probabilities), DL fixed (replacing 0 counts with
0.1), and DL discard (discarding features ap-
pearing with only one sense).
NB: We applied a simple smoothing method
presented in (Ng, 1997), where zero counts are
replaced by the probability of the given sense
divided by the number of examples.
V: The same smoothing method used for NB
was applied for vectors. For Basque, two ver-
sions were tested: as the Basque parser can re-
turn ambiguous analyses, partial weights are as-
signed to the features in the context, and we can
chose to use these partial weights (p), or assign
the full weight to all features (f).
SVM: No smoothing was applied. We esti-
mated the soft margin using a greedy process in
cross-validation on the training data per each
word.
Combination: Single voting was used,
where each system voted for its best ranked
sense, and the most voted sense was chosen.
More sophisticate schemes like ranked voting,
were tried on Senseval-2 data, but the results
did not improve. We tested combinations of
the 4 algorithms, leaving one out, and the two
best. The best results were obtained combining
3 methods (leave one out).
Method Recall
vector 73,9
SVM 73,5
DL smooth 69,4
NB 69,4
DL fixed 65,6
DL discard 65,4
MFS 57,1
Table 1: Single systems (English) in cross-
validation, sorted by recall.
Combination Recall
SVM-vector-DL smooth-NB 73,2
SVM-vector-DL fixed-NB 72,7
SVM-vector-DL smooth 74,0
SVM-vector-DL fixed 73,8
SVM-vector-NB 73,6
SVM-DL smooth-NB 72,4
SVM-DL fixed-NB 71,3
SVM-vector 73,1
Table 2: Combined systems (English) in cross-
validation, best recall in bold.
Method Recall
SVM 71,1
NB 68,5
vector(f) 66,8
DL smooth 65,9
DL fixed 65,2
vector(p) 65,0
DL discard 60,7
MFS 53,0
Table 3: Single systems (Basque) in cross-
validation, sorted by recall.
Combination Recall
SVM-vector-DL smooth-NB 70,6
SVM-vector-DL fixed-NB 71,1
SVM-vector-DL smooth 70,6
SVM-vector-DL fixed 70,8
SVM-vector-NB 71,1
SVM-DL smooth-NB 70,2
SVM-DL fixed-NB 70,5
SVM-vector 69,0
SVM-NB 69,8
Table 4: Combined systems (Basque) in cross-
validation, best recall in bold. Only vector(f)
was used for combination.
4.2 Results on English Training Data
The results using cross-validation on the
Senseval-3 data are shown in Table 1 for single
systems, and in Table 2 for combined methods.
All the algorithms have full-coverage (for En-
glish and Basque), therefore the recall and the
precision are the same. The most frequent sense
(MFS) baseline is also provided, and it is easily
beaten by all the algorithms.
We have to note that these figures are consis-
tent with the performance we observed in the
Senseval-2 data, where the vector method is
the best performing single system, and the best
combination is SVM-vector-DL smooth. There
is a small gain when combining 3 systems, which
we expected would be higher. We submitted the
best single system, and the best combination for
this task.
4.3 Results on Basque Training Data
The performance on the Senseval-3 Basque
training data is given in Table 1 for single sys-
tems, and in Table 2 for combined methods. In
this case, the vector method, and DL smooth
obtain lower performance in relation to other
methods. This can be due to the type of fea-
tures used, which have not been tested as ex-
tensively as for English. In fact, it could hap-
pen that some features contribute mostly noise.
Also, the domain tag of the examples, which
could provide useful information, was not used.
There is no improvement when combining dif-
ferent systems, and the result of the combina-
tion of 4 systems is unusually high in relation
to the English experiments. We also submit-
ted two systems for this task: the best single
method in cross-validation (SVM), and the best
3-method combination (SVM-vector-NB).
5 Results and Conclusions
Table 5 shows the performance obtained by our
systems and the winning system in the Senseval-
3 evaluation. We can see that we are very close
to the best algorithms in both languages.
The recall of our systems is 1.2%-1.9% lower
than cross-validation for every system and task,
which is not surprising when we change the set-
ting. The combination of methods is useful for
English, where we improve the recall in 0.3%,
reaching 72.3%. The difference is statistically
significant according to McNemar?s test.
However, the combination of methods does
not improve the results in the the Basque task,
where the SVM method alone provides better
Task Code Method Rec.
Eng. Senseval-3 Best ? 72,9
Eng. BCU comb
SVM-vector-
DL smooth 72,3
Eng. BCU-english vector 72,0
Basq. Senseval-3 Best ? 70,4
Basq. BCU-basque SVM 69,9
Basq. BCU-Basque comb SVM-vector-
NB
69,5
Table 5: Official results for the English and
Basque lexical tasks (recall).
results (69.9% recall). In this case the difference
is not significant applying McNemar?s test.
Our disambiguation procedure shows a sim-
ilar behavior on the Senseval-2 and Senseval-3
data for English (both in cross-validation and
in the testing part), where the ensemble works
best, followed by the vector model. This did
not apply to the Basque dataset, where some
algorithms seem to perform below the expecta-
tions. For future work, we plan to study better
the Basque feature set and include new features,
such as domain tags.
Overall, the ensemble of algorithms provides
a more robust system for WSD, and is able to
achieve state-of-the-art performance.
6 Acknowledgements
We wish to thank both David Yarowsky?s group,
from Johns Hopkins University, and Gerard Es-
cudero?s group, from Universitat Politecnica de
Catalunya, for providing us software for the ac-
quisition of features. This research has been
partially funded by the European Commission
(MEANING IST-2001-34460).
References
I. Aduriz, E. Agirre, I. Aldezabal, I. Alegria,
X. Arregi, J. Arriola, X. Artola, K. Gojenola,
A. Maritxalar, K. Sarasola, and M. Urkia.
2000. A word-grammar based morphological
analyzer for agglutinative languages. In Pro-
ceedings of the International Conference on
Computational Linguistics COLING, Saar-
brucken, Germany.
Eneko Agirre and David Martinez. 2004.
Smoothing and word sense disambiguation.
(submitted).
T. Joachims. 1999. Making large?scale SVM
learning practical. In Advances in Kernel
Methods ? Support Vector Learning, pages
169?184, Cambridge, MA. MIT Press.
Bernardo Magnini and Gabriela Cavaglia?. 2000.
Integrating subject field codes into WordNet.
In Proceedings of the Second International
LREC Conference, Athens, Greece.
Hwee Tou Ng. 1997. Exemplar-based word
sense disambiguation: Some recent improve-
ments. In Proceedings of the Second EMNLP
Conference. ACL, Somerset, New Jersey.
Grace Ngai and Radu Florian. 2001.
Transformation-based learning in the fast
lane. Proceedings of the Second Conference
of the NAACL, Pittsburgh, PA, USA.
Ted Pedersen. 2001. A decision tree of bi-
grams is an accurate predictor of word sense.
Proceedings of the Second Meeting of the
NAACL, Pittsburgh, PA.
David Yarowsky. 1995a. Three machine learn-
ing algorithms for lexical ambiguity resolu-
tion. In PhD thesis, Department of Com-
puter and Information Sciences, University of
Pennsylvania.
David Yarowsky. 1995b. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 189?196,
Cambridge, MA.
The ?Meaning? System on the English Allwords Task
L. Villarejo   , L. Ma`rquez   , E. Agirre  , D. Mart??nez  , , B. Magnini  ,
C. Strapparava  , D. McCarthy  , A. Montoyo  , and A. Sua?rez 
 
TALP Research Center, Universitat Polite`cnica de Catalunya,  luisv,lluism  @lsi.upc.es
 IXA Group, University of the Basque Country,  eneko,davidm  @si.ehu.es
 ITC-irst (Istituto per la Ricerca Scientifica e Tecnologica),  magnini,strappa  @itc.it
 University of Sussex, dianam@sussex.ac.uk
 LSI, University of Alicante, montoyo@dlsi.ua.es,armando.suarez@ua.es
1 Introduction
The ?Meaning? system has been developed within
the framework of the Meaning European research
project1 . It is a combined system, which integrates
several supervised machine learning word sense
disambiguation modules, and several knowledge?
based (unsupervised) modules. See section 2 for de-
tails. The supervised modules have been trained ex-
clusively on the SemCor corpus, while the unsuper-
vised modules use WordNet-based lexico?semantic
resources integrated in the Multilingual Central
Repository (MCR) of the Meaning project (Atserias
et al, 2004).
The architecture of the system is quite simple.
Raw text is passed through a pipeline of linguis-
tic processors (tokenizers, POS tagging, named en-
tity extraction, and parsing) and then a Feature Ex-
traction module codifies examples with features ex-
tracted from the linguistic annotation and MCR.
The supervised modules have priority over the un-
supervised and they are combined using a weighted
voting scheme. For the words lacking training ex-
amples, the unsupervised modules are applied in a
cascade sorted by decreasing precision. The tuning
of the combination setting has been performed on
the Senseval-2 allwords corpus.
Several research groups have been providers of
resources and tools, namely: IXA group from the
University of the Basque Country, ITC-irst (?Is-
tituto per la Ricerca Scientifica e Tecnologica?),
University of Sussex (UoS), University of Alicante
(UoA), and TALP research center at the Technical
University of Catalonia. The integration was carried
out by the TALP group.
2 The WSD Modules
We have used up to seven supervised learning sys-
tems and five unsupervised WSD modules. Some
of them have also been applied individually to the
1Meaning, Developing Multilingual Web-scale Lan-
guage Technologies (European Project IST-2001-34460):
http://www.lsi.upc.es/  nlp/meaning/meaning.html.
Senseval-3 lexical sample and allwords tasks.
 Naive Bayes (NB) is the well?known Bayesian
algorithm that classifies an example by choos-
ing the class that maximizes the product, over
all features, of the conditional probability of
the class given the feature. The provider of this
module is IXA. Conditional probabilities were
smoothed by Laplace correction.
 Decision List (DL) are lists of weighted clas-
sification rules involving the evaluation of one
single feature. At classification time, the algo-
rithm applies the rule with the highest weight
that matches the test example (Yarowsky,
1994). The provider is IXA and they also ap-
plied smoothing to generate more robust deci-
sion lists.
 In the Vector Space Model method (cosVSM),
each example is treated as a binary-valued fea-
ture vector. For each sense, one centroid vec-
tor is obtained from training. Centroids are
compared with the vectors representing test ex-
amples, using the cosine similarity function,
and the closest centroid is used to classify the
example. No smoothing is required for this
method provided by IXA.
 Support Vector Machines (SVM) find the hy-
perplane (in a high dimensional feature space)
that separates with maximal distance the pos-
itive examples from the negatives, i.e., the
maximal margin hyperplane. Providers are
TALP (SVM  ) and IXA (SVM 	 ) groups. Both
used the freely available implementation by
(Joachims, 1999), linear kernels, and one?vs?
all binarization, but with different parameter
tuning and feature filtering.
 Maximum Entropy (ME) are exponential
conditional models parametrized by a flexible
set of features. When training, an iterative opti-
mization procedure finds the probability distri-
bution over feature coefficients that maximizes
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
the entropy on the training data. This system is
provided by UoA.
 AdaBoost (AB) is a method for learning an en-
semble of weak classifiers and combine them
into a strong global classification rule. We
have used the implementation described in
(Schapire and Singer, 1999) with decision trees
of depth fixed to 3. The provider of this system
is TALP.
 Domain Driven Disambiguation (DDD) is an
unsupervised method that makes use of do-
main information in order to solve lexical am-
biguity. The disambiguation of a word in
its context is mainly a process of compari-
son between the domain of the context and
the domains of the word?s senses (Magnini
et al, 2002). ITC-irst provided two variants
of the system DDD   and DDD  , aiming at
maximizing precision and F  score, respec-
tively. The UoA group also provided another
domain?based unsupervised classifier (DOM).
Their approach exploits information contained
in glosses of WordNet Domains and introduces
a new lexical resource ?Relevant Domains? ob-
tained from Association Ratio over glosses of
WordNet Domains.
 Automatic Predominant Sense (autoPS) pro-
vide an unsupervised first sense heuristic for
the polysemous words in WordNet. This
is produced by UoS automatically from the
BNC (McCarthy et al, 2004). The method
uses automatically acquired thesauruses for the
main PoS categories. The nearest neighbors
for each word are related to its WordNet senses
using a WordNet similarity measure.
 We also used a Most Frequent Sense tagger,
according to the WordNet ranking of senses
(MFS).
3 Evaluation of Individual Modules
For simplicity, and also due to time constraints, the
supervised modules were trained exclusively on the
SemCor-1.6 corpus, intentionally avoiding the use
of other sources of potential training examples, e.g,
other corpora, WordNet examples and glosses, sim-
ilar/substitutable examples extracted from the same
Semcor-1.6, etc. An independent training set was
generated for each polysemous word (of a certain
part?of?speech) with 10 or more examples in the
SemCor-1.6 corpus. This makes a total of 2,440 in-
dependent learning problems, on which all super-
vised WSD systems were trained.
The feature representation of the training exam-
ples was shared between all learning modules. It
consists of a rich feature representation obtained
using the Feature Extraction module of the TALP
team in the Senseval-3 English lexical sample task.
The feature set includes the classic window?based
pattern features extracted from a local context and
the ?bag?of?words? type of features taken from a
broader context. It also contains a set of features
representing the syntactic relations involving the
target word, and semantic features of the surround-
ing words extracted from the MCR of the Meaning
project. See (Escudero et al, 2004) for more details
on the set of features used.
The validation corpus for these classifiers was the
Senseval-2 allwords dataset, which contains 2,473
target word occurrences. From those, 2,239 occur-
rences correspond to polysemous words. We will
refer to this subcorpus as S2-pol. Only 1,254 words
from S2-pol were actually covered by the classifiers
trained on the SemCor-1.6 corpus. We will refer to
this subset of words as the S2-pol-sup corpus. The
conversion between WordNet-1.6 synsets (SemCor-
1.6) and WordNet-1.7 (Senseval-2) was performed
on the output of the classifiers by applying an auto-
matically derived mapping provided by TALP2.
Table 1 shows the results (precision and cover-
age) obtained by the individual supervised modules
on the S2-pol-sup subcorpus, and by the unsuper-
vised modules on the S2-pol subcorpus (i.e., we
exclude from evaluation the monosemous words).
Support Vector Machines and AdaBoost are the best
performing methods, though all of them perform in
a small accuracy range from 53.4% to 59.5%.
Regarding the unsupervised methods, DDD is
clearly the best performing method, achieving a re-
markable precision of 61.9% with the DDD   vari-
ant, at a cost of a lower coverage. The DDD  ap-
pears to be the best system for augmenting the cov-
erage of the former. Note that the autoPS heuristic
for ranking senses is a more precise estimator than
the WordNet most?frequent?sense (MFS).
4 Integration of WSD modules
All the individual modules have to be integrated in
order to construct a complete allwords WSD sys-
tem. Following the architecture described in section
1, we decided to apply the unsupervised modules
only to the subset of the corpus not covered by the
training examples. Some efforts on applying the
unsupervised modules jointly with the supervised
failed at improving accuracy. See an example in ta-
ble 3.
2http://www.lsi.upc.es/  nlp/tools/mapping.html
supervised, S2-pol-sup corpus unsupervised, S2-pol corpus
SVM   AB cosVSM SVM  ME NB DL DDD  DDD  autoPS MFS DOM
prec. 59.5 59.1 57.8 57.1 56.3 54.6 53.4 61.9 50.2 45.2 32.5 23.8
cov. 100.0 100.0 100.0 100.0 100.0 100.0 100.0 48.8 99.6 89.6 98.0 49.1
Table 1: Results of individual supervised and unsupervised WSD modules
As a first approach, we devised three baseline
systems (Base-1, Base-2, and Base-3), which use
the best modules available in both subsets. Base-1
applies the SVM  supervised method and the MFS
for the non supervised part. Base-2 applies also the
SVM  supervised method and the cascade DDD   ?
MFS for the non supervised part (MFS is used in the
cases in which DDD   abstains). Base-3 shares the
same approach but uses a third unsupervised mod-
ule: DDD   ?DDD  ?MFS.
The precision results of the baselines systems can
be found in the right hand side of table 3. As it can
be observed, the positive contribution of the DDD  
module is very significant since Base-2 performs
2.2 points higher than Base-1. The addition of the
third unsupervised module (DDD   ) makes Base-3
to gain 0.4 extra precision points.
As simple combination schemes we considered
majority voting and weighted voting. More sophis-
ticated combination schemes are difficult to tune
due to the extreme data sparseness on the valida-
tion set. In the case of unsupervised systems, these
combination schemes degraded accuracy because
the least accurate systems perform much worse that
the best ones. Thus, we simply decided to apply a
cascade of unsupervised modules sorted by preci-
sion on the Senseval-2 corpus.
In the case of the supervised classifiers there is a
chance of improving the global performance, since
there are several modules performing almost as well
as the best. Previous to the experiments, we cal-
culated the agreement rates on the outputs of each
pair of systems (low agreements increase the prob-
ability of uncorrelatedness between errors of differ-
ent systems). We obtained an average agreement of
83.17%, with values between 64.7% (AB vs SVM 	 )
and 88.4% (SVM 	 vs cosVSM).
The ensembles were obtained by incrementally
aggregating, to the best performing classifier, the
classifiers from a list sorted by decreasing accu-
racy. The ranking of classifiers can be performed
by evaluating them at different levels of granular-
ity: from particular words to the overall accuracy
on the whole validation set. The level of granularity
defines a tradeoff between classifier specialization
and risk of overfitting to the tuning corpus. We de-
cided to take an intermediate level of granularity,
and sorted the classifiers according to their perfor-
mance on word sets based on the number of training
examples available3 .
Table 2 contains the results of the ranking exper-
iment, by considering five word-sets of increasing
number of training examples: between 10 and 20,
between 21 and 40, between 41 and 80, etc. At each
cell, the accuracy value is accompanied by the rel-
ative position the system achieves in that particu-
lar subset. Note that the resulting orderings, though
highly correlated, are quite different from the one
derived from the overall results.
(10,20) (21,40) (41,80) (81,160)  160
SVM  60.9-1 59.1-1 64.2-2 61.1-2 56.4-1
AB 60.9-1 56.6-2 60.0-7 64.7-1 56.1-2
c-VSM 59.9-2 56.6-2 62.6-3 57.0-4 55.8-3
SVM  50.8-5 55.1-4 61.6-4 57.4-3 53.1-5
ME 56.7-3 55.3-3 65.3-1 53.3-5 53.8-4
NB 59.9-2 54.6-5 61.1-5 49.2-6 51.5-7
DL 56.4-4 49.9-6 60.5-6 47.2-7 52.5-6
Table 2: Results on frequency?based word sets
Table 3 shows the precision results4 of the Mean-
ing system obtained on the whole Senseval-2 corpus
by combining from 1 to 7 supervised classifiers ac-
cording to the classifier orderings of table 2 for each
subset of words. The unsupervised classifiers are
all applied in a cascade sorted by precision. M-Vot
stands for a majority voting scheme, while W-Vot
refers to the weighted voting scheme. The weights
for the classifiers are simply the accuracy values on
the validation corpus. As an additional example,
the column M-Vot+ shows the results of the vot-
ing scheme when the unsupervised DDD   module
is also included in the ensemble. The table also in-
cludes the baseline results.
Unfortunately, the ensembles of classifiers did
not provide significant improvements on the final
precision. Only in the case of weighted voting a
slight improvement is observed when adding up to
3 classifiers. From the fourth classifier performance
also degrades. The addition of unsupervised sys-
tems to the supervised ensemble systematically de-
graded performance.
As a reference, the best result (67.5% precision
3One of the factors that differentiates between learning al-
gorithms is the amount of training examples needed to learn.
4Coverage of the combined systems is 98% in all cases.
M-Vot W-Vot M-Vot+ Base-1 Base-2 Base-3
1 67.3 67.3 66.4 ? ? ?
2 ? 67.4 66.3 ? ? ?
3 67.2 67.5 67.1 ? ? ?
4 ? 67.1 66.9 ? ? ?
5 66.5 66.5 66.7 ? ? ?
6 ? 66.3 66.3 ? ? ?
7 65.7 65.9 66.0 ? ? ?
best 67.3 67.5 67.1 64.8 67.0 67.4
Table 3: Results of the combination of systems
System prec. recall F 
Meaning-c 61.1% 61.0% 61.05
Meaning-wv 62.5% 62.3% 62.40
Table 4: Results on the Senseval-3 test corpus
and 98.0% coverage) would have put our combined
system in second place in the Senseval-2 allwords
task.
5 Evaluation on the Senseval-3 Corpus
The Senseval-3 test set contains 2,081 target words,
1,851 of them polysemous. The subset covered by
the SemCor-1.6 training contains 1,211 target words
(65.42%, compared to the 56.0% of the Senseval-2
corpus). We submitted the outputs of two different
configurations of the Meaning system: Meaning-
c and Meaning-wv. These systems correspond to
Base-3 and W-Vot (in the best configuration) from
table 3, respectively. The results from the official
evaluation are given in table 4. Again, we applied an
automatic mapping from WordNet-1.6 to WordNet-
1.7.1 synset labels. However, there are senses in
1.7.1 that do not exist in 1.6, thus our system sim-
ply cannot assign them.
It can be observed that, even though on the tun-
ing corpus both variants obtained very similar pre-
cision (67.4 and 67.5), on the test set the weighted
voting scheme is clearly better than the baseline sys-
tem, probably due to the robustness achieved by the
ensemble. The performance decrease observed on
the test set with respect to the Senseval-2 corpus is
very significant (   5 points). Given that the baseline
system performs worse than the voted approach, it
seems unlikely that there is overfitting during the
ensemble tuning. However, we plan to repeat the
tuning experiments directly on the Senseval-3 cor-
pus to see if the same behavior and conclusions
are observed. Probably, the decrease in perfor-
mance is due to the differences between the train-
ing and test corpora. We intend to investigate the
differences between SemCor-1.6, Senseval-2, and
Senseval-3 corpora at different levels of linguistic
information in order to check the appropriateness of
using SemCor-1.6 as the main information source.
6 Acknowledgements
This research has been possible thanks to the sup-
port of European and Spanish research projects:
IST-2001-34460 (Meaning), TIC2000-0335-C03-
02 (Hermes). The authors would like to thank also
Gerard Escudero for letting us use the Feature Ex-
traction module and German Rigau for helpful sug-
gestions and comments.
References
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and P. Vossen. 2004. The
Meaning multilingual central repository. In Pro-
ceedings of the Second International WordNet
Conference.
G. Escudero, L. Ma`rquez, and G. Rigau. 2004.
TALP system for the english lexical sample task.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
T. Joachims. 1999. Making large?scale SVM learn-
ing practical. In B. Scho?lkopf, C. J. C. Burges,
and A. J. Smola, editors, Advances in Kernel
Methods ? Support Vector Learning, pages 169?
184. MIT Press, Cambridge, MA.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
D. McCarthy, R. Koeling, J. Weeds, and J. Car-
roll. 2004. Using automatically acquired pre-
dominant senses for word sense disambiguation.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
R. Schapire and Y. Singer. 1999. Improved boost-
ing algorithms using confidence?rated predic-
tions. Machine Learning, 37(3):297?336.
David Yarowsky. 1994. Decision lists for lexi-
cal ambiguity resolution: Application to accent
restoration in Spanish and French. In Proceed-
ings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 88?95,
Las Cruces, NM.
Unsupervised WSD based on automatically retrieved examples:
The importance of bias
Eneko Agirre
IXA NLP Group
University of the Basque Country
Donostia, Spain
eneko@si.ehu.es
David Martinez
IXA NLP Group
University of the Basque Country
Donostia, Spain
davidm@si.ehu.es
Abstract
This paper explores the large-scale acquisition of
sense-tagged examples for Word Sense Disam-
biguation (WSD). We have applied the ?WordNet
monosemous relatives? method to construct auto-
matically a web corpus that we have used to train
disambiguation systems. The corpus-building pro-
cess has highlighted important factors, such as the
distribution of senses (bias). The corpus has been
used to train WSD algorithms that include super-
vised methods (combining automatic and manually-
tagged examples), minimally supervised (requiring
sense bias information from hand-tagged corpora),
and fully unsupervised. These methods were tested
on the Senseval-2 lexical sample test set, and com-
pared successfully to other systems with minimum
or no supervision.
1 Introduction
The results of recent WSD exercises, e.g. Senseval-
21 (Edmonds and Cotton, 2001) show clearly that
WSD methods based on hand-tagged examples are
the ones performing best. However, the main draw-
back for supervised WSD is the knowledge acqui-
sition bottleneck: the systems need large amounts
of costly hand-tagged data. The situation is more
dramatic for lesser studied languages. In order to
overcome this problem, different research lines have
been explored: automatic acquisition of training ex-
amples (Mihalcea, 2002), bootstrapping techniques
(Yarowsky, 1995), or active learning (Argamon-
Engelson and Dagan, 1999). In this work, we have
focused on the automatic acquisition of examples.
When supervised systems have no specific train-
ing examples for a target word, they need to rely on
publicly available all-words sense-tagged corpora
like Semcor (Miller et al, 1993), which is tagged
with WordNet word senses. The systems perform-
ing best in the English all-words task in Senseval-2
were basically supervised systems trained on Sem-
cor. Unfortunately, for most of the words, this cor-
1http://www.senseval.org.
pus only provides a handful of tagged examples. In
fact, only a few systems could overcome the Most
Frequent Sense (MFS) baseline, which would tag
each word with the sense occurring most frequently
in Semcor. In our approach, we will also rely on
Semcor as the basic resource, both for training ex-
amples and as an indicator of the distribution of the
senses of the target word.
The goal of our experiment is to evaluate up to
which point we can automatically acquire examples
for word senses and train accurate supervised WSD
systems on them. This is a very promising line of
research, but one which remains relatively under-
studied (cf. Section 2). The method we applied
is based on the monosemous relatives of the target
words (Leacock et al, 1998), and we studied some
parameters that affect the quality of the acquired
corpus, such as the distribution of the number of
training instances per each word sense (bias), and
the type of features used for disambiguation (local
vs. topical).
Basically, we built three systems, one fully su-
pervised (using examples from both Semcor and au-
tomatically acquired examples), one minimally su-
pervised (using the distribution of senses in Semcor
and automatically acquired examples) and another
fully unsupervised (using an automatically acquired
sense rank (McCarthy et al, 2004) and automati-
cally acquired examples).
This paper is structured as follows. First, Section
2 describes previous work on the field. Section 3 in-
troduces the experimental setting for evaluating the
acquired corpus. Section 4 is devoted to the process
of building the corpus, which is evaluated in Section
5. Finally, the conclusions are given in Section 6.
2 Previous work
As we have already mentioned, there is little work
on this very promising area. In (Leacock et al,
1998), the method to obtain sense-tagged examples
using monosemous relatives is presented. In this
work, they retrieve the same number of examples
per each sense, and they give preference to monose-
mous relatives that consist in a multiword contain-
ing the target word. Their experiment is evaluated
on 3 words (a noun, a verb, and an adjective) with
coarse sense-granularity and few senses. The results
showed that the monosemous corpus provided pre-
cision comparable to hand-tagged data.
In another related work, (Mihalcea, 2002) gener-
ated a sense tagged corpus (GenCor) by using a set
of seeds consisting of sense-tagged examples from
four sources: SemCor, WordNet, examples created
using the method above, and hand-tagged examples
from other sources (e.g., the Senseval-2 corpus). By
means of an iterative process, the system obtained
new seeds from the retrieved examples. An exper-
iment in the lexical-sample task showed that the
method was useful for a subset of the Senseval-2
testing words (results for 5 words are provided).
3 Experimental Setting for Evaluation
In this section we will present the Decision List
method, the features used to represent the context,
the two hand-tagged corpora used in the experiment
and the word-set used for evaluation.
3.1 Decision Lists
The learning method used to measure the quality of
the corpus is Decision Lists (DL). This algorithm is
described in (Yarowsky, 1994). In this method, the
sense s
k
with the highest weighted feature f
i
is se-
lected, according to its log-likelihood (see Formula
1). For our implementation, we applied a simple
smoothing method: the cases where the denomina-
tor is zero are smoothed by the constant 0.1 .
weight(s
k
, f
i
) = log(
Pr(s
k
|f
i
)
?
j =k
Pr(s
j
|f
i
)
) (1)
3.2 Features
In order to represent the context, we used a basic set
of features frequently used in the literature for WSD
tasks (Agirre and Martinez, 2000). We distinguish
two types of features:
 Local features: Bigrams and trigrams, formed
by the word-form, lemma, and part-of-speech2
of the surrounding words. Also the content
lemmas in a ?4 word window around the tar-
get.
 Topical features: All the content lemmas in the
context.
2The PoS tagging was performed using TnT (Brants, 2000)
We have analyzed the results using local and top-
ical features separately, and also using both types
together (combination).
3.3 Hand-tagged corpora
Semcor was used as training data for our supervised
system. This corpus offers tagged examples for
many words, and has been widely used for WSD.
It was necessary to use an automatic mapping be-
tween the WordNet 1.6 senses in Semcor and the
WordNet 1.7 senses in testing (Daude et al, 2000).
For evaluation, the test part of the Senseval-2 En-
glish lexical-sample task was chosen. The advan-
tage of this corpus was that we could focus on a
word-set with enough examples for testing. Be-
sides, it is a different corpus, so the evaluation is
more realistic than that made using cross-validation.
The test examples whose senses were multiwords
or phrasal verbs were removed, because they can be
efficiently detected with other methods in a prepro-
cess.
It is important to note that the training part of
Senseval-2 lexical-sample was not used in the con-
struction of the systems, as our goal was to test
the performance we could achieve with minimal re-
sources (i.e. those available for any word). We only
relied on the Senseval-2 training bias in preliminary
experiments on local/topical features (cf. Table 4),
and to serve as a reference for unsupervised perfor-
mance (cf. Table 5).
3.4 Word-set
The experiments were performed on the 29 nouns
available for the Senseval-2 lexical-sample task. We
separated these nouns in 2 sets, depending on the
number of examples they have in Semcor: Set A
contained the 16 nouns with more than 10 examples
in Semcor, and Set B the remaining low-frequency
words.
4 Building the monosemous relatives web
corpus
In order to build this corpus3, we have acquired
1000 Google snippets for each monosemous word
in WordNet 1.7. Then, for each word sense of the
ambiguous words, we gathered the examples of its
monosemous relatives (see below). This method is
inspired in (Leacock et al, 1998), and has shown to
be effective in experiments of topic signature acqui-
sition (Agirre and Lopez, 2004). This last paper also
shows that it is possible to gather examples based on
3The automatically acquired corpus will be referred indis-
tinctly as web-corpus, or monosemous-corpus
monosemous relatives for nearly all noun senses in
WordNet4.
The basic assumption is that for a given word
sense of the target word, if we had a monosemous
synonym of the word sense, then the examples of
the synonym should be very similar to the target
word sense, and could therefore be used to train a
classifier of the target word sense. The same, but
in a lesser extent, can be applied to other monose-
mous relatives, such as direct hyponyms, direct hy-
pernyms, siblings, indirect hyponyms, etc. The ex-
pected reliability decreases with the distance in the
hierarchy from the monosemous relative to the tar-
get word sense.
The monosemous-corpus was built using the sim-
plest technique: we collected examples from the
web for each of the monosemous relatives. The rel-
atives have an associated number (type), which cor-
relates roughly with the distance to the target word,
and indicates their relevance: the higher the type,
the less reliable the relative. A sample of monose-
mous relatives for different senses of church, to-
gether with its sense inventory in WordNet 1.7 is
shown in Figure 1.
Distant hyponyms receive a type number equal
to the distance to the target sense. Note that we
assigned a higher type value to direct hypernyms
than to direct hyponyms, as the latter are more use-
ful for disambiguation. We also decided to include
siblings, but with a high type value (3).
In the following subsections we will describe step
by step the method to construct the corpus. First we
will explain the acquisition of the highest possible
amount of examples per sense; then we will explain
different ways to limit the number of examples per
sense for a better performance; finally we will see
the effect of training on local or topical features on
this kind of corpora.
4.1 Collecting the examples
The examples are collected following these steps
1: We query Google5 with the monosemous rel-
atives for each sense, and we extract the snippets as
returned by the search engine. All snippets returned
by Google are used (up to 1000). The list of snippets
is sorted in reverse order. This is done because the
top hits usually are titles and incomplete sentences
that are not so useful.
2: We extract the sentences (or fragments of sen-
tences) around the target search term. Some of the
4All the examples in this work are publicly available in
http://ixa2.si.ehu.es/pub/sensecorpus
5We use the offline XML interface kindly provided by
Google for research.
Sense 0 1 2 3 Total Semcor
church#1 0 476 524 0 1000 60
church#2 306 100 561 0 967 58
church#3 147 0 20 0 167 10
Overall 453 576 1105 0 2134 128
Table 1: Examples per type (0,1,...) that are ac-
quired from the web for the three senses of church
following the Semcor bias, and total examples in
Semcor.
sentences are discarded, according to the following
criteria: length shorter than 6 words, having more
non-alphanumeric characters than words divided by
two, or having more words in uppercase than in low-
ercase.
3: The automatically acquired examples contain
a monosemous relative of the target word. In or-
der to use these examples to train the classifiers,
the monosemous relative (which can be a multi-
word term) is substituted by the target word. In
the case of the monosemous relative being a mul-
tiword that contains the target word (e.g. Protestant
Church for church) we can choose not to substitute,
because Protestant, for instance, can be a useful fea-
ture for the first sense of church. In these cases, we
decided not to substitute and keep the original sen-
tence, as our preliminary experiments on this corpus
suggested (although the differences were not signif-
icant).
4: For a given word sense, we collect the desired
number of examples (see following section) in or-
der of type: we first retrieve all examples of type
0, then type 1, etc. up to type 3 until the necessary
examples are obtained. We did not collect exam-
ples from type 4 upwards. We did not make any
distinctions between the relatives from each type.
(Leacock et al, 1998) give preference to multiword
relatives containing the target word, which could be
an improvement in future work.
On average, we have acquired roughly 24,000 ex-
amples for each of the target words used in this ex-
periment.
4.2 Number of examples per sense (bias)
Previous work (Agirre and Martinez, 2000) has re-
ported that the distribution of the number of exam-
ples per word sense (bias for short) has a strong
influence in the quality of the results. That is, the
results degrade significantly whenever the training
and testing samples have different distributions of
the senses.
As we are extracting examples automatically, we
have to decide how many examples we will use for
Sense 1
church, Christian church, Christianity -- (a group of Christians; any group professing
Christian doctrine or belief)
Sense 2
church, church building -- (a place for public (especially Christian) worship)
Sense 3
church service, church -- (a service conducted in a church)
Monosemous relatives for different senses of church
Synonyms (Type 0): church building (sense 2), church service (sense 3) ...
Direct hyponyms (Type 1): Protestant Church (sense 1), Coptic Church (sense 1) ...
Direct hypernyms (Type 2): house of prayer (sense 2), religious service (sense 3) ...
Distant hyponyms (Type 2,3,4...): Greek Church (sense 1), Western Church (sense 1)...
Siblings (Type 3): Hebraism (sense 2), synagogue (sense 2) ...
Figure 1: Sense inventory and some monosemous relatives in WordNet 1.7 for church.
Web corpus
Sense
Semcor Web bias Semcor Pr Semcor MR Automatic MR Senseval test
# ex % # ex % # ex % # ex % # ex % # ex %
authority#1 18 60 338 0.5 338 33.7 324 59.9 138 19.3 37 37.4
authority#2 5 16.7 44932 66.4 277 27.6 90 16.6 75 10.5 17 17.2
authority#3 3 10 10798 16 166 16.6 54 10.0 93 13.0 1 1.0
authority#4 2 6.7 886 1.3 111 11.1 36 6.7 67 9.4 0 0
authority#5 1 3.3 6526 9.6 55 5.5 18 3.3 205 28.6 34 34.3
authority#6 1 3.3 71 0.1 55 5.5 18 3.3 71 9.9 10 10.1
authority#7 0 0 4106 6.1 1 0.1 1 0.2 67 9.4 0 0
Overall 30 100 67657 100 1003 100 541 100 716 100 99 100
Table 2: Distribution of examples for the senses of authority in different corpora. Pr (proportional) and MR
(minimum ratio) columns correspond to different ways to apply Semcor bias.
each sense. In order to test the impact of bias, dif-
ferent settings have been tried:
 No bias: we take an equal amount of examples
for each sense.
 Web bias: we take all examples gathered from
the web.
 Automatic ranking: the number of examples
is given by a ranking obtained following the
method described in (McCarthy et al, 2004).
They used a thesaurus automatically created
from the BNC corpus with the method from
(Lin, 1998), coupled with WordNet-based sim-
ilarity measures.
 Semcor bias: we take a number of examples
proportional to the bias of the word senses in
Semcor.
For example, Table 1 shows the number of exam-
ples per type (0,1,...) that are acquired for church
following the Semcor bias. The last column gives
the number of examples in Semcor.
We have to note that the 3 first methods do not
require any hand-labeled data, and that the fourth
relies in Semcor.
The way to apply the bias is not straightforward
in some cases. In our first approach for Semcor-
bias, we assigned 1,000 examples to the major sense
in Semcor, and gave the other senses their propor-
tion of examples (when available). But in some
cases the distribution of the Semcor bias and that
of the actual examples in the web would not fit. The
problem is caused when there are not enough exam-
ples in the web to fill the expectations of a certain
word sense.
We therefore tried another distribution. We com-
puted, for each word, the minimum ratio of exam-
ples that were available for a given target bias and a
given number of examples extracted from the web.
We observed that this last approach would reflect
better the original bias, at the cost of having less ex-
amples.
Table 2 presents the different distributions of
examples for authority. There we can see the
Senseval-testing and Semcor distributions, together
with the total number of examples in the web; the
Semcor proportional distribution (Pr) and minimum
ratio (MR); and the automatic distribution. The
table illustrates how the proportional Semcor bias
produces a corpus where the percentage of some of
Word Web bias Semcor bias Automatic bias
art 15,387 10,656 2,610
authority 67,657 541 716
bar 50,925 16,627 5,329
bum 17,244 2,555 4,745
chair 24,625 8,512 2,111
channel 31,582 3,235 10,015
child 47,619 3,504 791
church 8,704 5,376 6,355
circuit 21,977 3,588 5,095
day 84,448 9,690 3,660
detention 2,650 1,510 511
dyke 4,210 1,367 843
facility 11,049 8,578 1,196
fatigue 6,237 3,438 5,477
feeling 9,601 1,160 945
grip 20,874 2,209 277
hearth 6,682 1,531 2,730
holiday 16,714 1,248 1,846
lady 12,161 2,959 884
material 100,109 7,855 6,385
mouth 648 287 464
nation 608 594 608
nature 32,553 24,746 9,813
post 34,968 4,264 8,005
restraint 33,055 2,152 2,877
sense 10,315 2,059 2,176
spade 5,361 2,458 2,657
stress 10,356 2,175 3,081
yew 10,767 2,000 8,013
Average 24,137 4,719 3,455
Total 699,086 136,874 100,215
Table 3: Number of examples following different
sense distributions. Minimum-ratio is applied for
the Semcor and automatic bias.
the senses is different from that in Semcor, e.g. the
first sense only gets 33.7% of the examples, in con-
trast to the 60% it had in Semcor.
We can also see how the distributions of senses
in Semcor and Senseval-test have important differ-
ences, although the main sense is the same. For the
web and automatic distributions, the first sense is
different; and in the case of the web distribution, the
first hand-tagged sense only accounts for 0.5% of
the examples retrieved from the web. Similar distri-
bution discrepancies can be observed for most of the
words in the test set. The Semcor MR column shows
how using minimum ratio we get a better reflection
of the proportion of examples in Semcor, compared
to the simpler proportional approach (Semcor Pr) .
For the automatic bias we only used the minimum
ratio.
To conclude this section, Table 3 shows the num-
ber of examples acquired automatically following
the web bias, the Semcor bias with minimum ratio,
and the Automatic bias with minimum ratio.
4.3 Local vs. topical features
Previous work on automatic acquisition of examples
(Leacock et al, 1998) has reported lower perfor-
mance when using local collocations formed by PoS
tags or closed-class words. We performed an early
experiment comparing the results using local fea-
tures, topical features, and a combination of both.
In this case we used the web corpus with Senseval
training bias, distributed according to the MR ap-
proach, and always substituting the target word. The
recall (per word and overall) is given in Table 4.
In this setting, we observed that local collocations
achieved the best precision overall, but the combina-
tion of all features obtained the best recall. The table
does not show the precision/coverage figures due to
space constraints, but local features achieve 58.5%
precision for 96.7% coverage overall, while topical
and combination of features have full-coverage.
There were clear differences in the results per
word, showing that estimating the best feature-set
per word would improve the performance. For the
corpus-evaluation experiments, we chose to work
with the combination of all features.
5 Evaluation
In all experiments, the recall of the systems is pre-
sented as evaluation measure. There is total cover-
age (because of the high overlap of topical features)
and the recall and precision are the same6.
In order to evaluate the acquired corpus, our first
task was to analyze the impact of bias. The results
are shown in Table 5. There are 2 figures for each
distribution: (1) simply assign the first ranked sense,
and (2) use the monosemous corpus following the
predetermined bias. As we described in Section 3,
the testing part of the Senseval-2 lexical sample data
was used for evaluation. We also include the results
using Senseval2 bias, which is taken from the train-
ing part. The recall per word for some distributions
can be seen in Table 4.
The results show clearly that when bias informa-
tion from a hand-tagged corpora is used the recall
improves significantly, even when the bias comes
from a corpus -Semcor- different from the target
corpus -Senseval-. The bias is useful by itself, and
we see that the higher the performance of the 1st
ranked sense heuristic, the lower the gain using the
monosemous corpus. We want to note that in fully
unsupervised mode we attain a recall of 43.2% with
the automatic ranking. Using the minimally su-
pervised information of bias, we get 49.8% if we
have the bias from an external corpus (Semcor) and
6Except for the experiment in Section 4.3, where using local
features the coverage is only partial.
Senseval bias Semcor Autom.
Word Loc. Top. Comb. bias bias
art 54.2 45.6 47.0 55.6 45.6
authority 47.8 43.2 46.2 41.8 40.0
bar 52.1 55.9 57.2 51.6 26.4
bum 81.2 87.5 85.0 5.0 57.5
chair 88.7 88.7 88.7 88.7 69.4
channel 39.7 53.7 55.9 16.2 30.9
child 56.5 55.6 56.5 54.0 34.7
church 67.7 51.6 54.8 48.4 49.7
circuit 45.3 54.2 56.1 41.5 49.1
day 59.4 54.7 56.8 48.0 12.5
detention 87.5 87.5 87.5 52.1 87.5
dyke 89.3 89.3 89.3 92.9 80.4
facility 28.6 21.4 21.4 26.8 22.0
fatigue 82.5 82.5 82.5 82.5 75.0
feeling 55.1 60.2 60.2 60.2 42.5
grip 19.0 38.0 39.0 16.0 28.2
hearth 73.4 75.0 75.0 75.0 60.4
holiday 96.3 96.3 96.3 96.3 72.2
lady 80.4 73.9 73.9 80.4 23.9
material 43.2 44.2 43.8 54.2 52.3
mouth 36.8 38.6 39.5 54.4 46.5
nation 80.6 80.6 80.6 80.6 80.6
nature 44.4 39.3 40.7 46.7 34.1
post 43.9 40.5 40.5 34.2 47.4
restraint 29.5 37.5 37.1 27.3 31.4
sense 58.1 37.2 38.4 47.7 41.9
spade 74.2 72.6 74.2 67.7 85.5
stress 53.9 46.1 48.7 2.6 27.6
yew 81.5 81.5 81.5 66.7 77.8
Overall 56.5 56.0 57.0 49.8 43.2
Table 4: Recall for all the nouns using the monose-
mous corpus with Senseval-2 training bias (MR, and
substitution), Semcor bias, and Automatic bias. The
Senseval-2 results are given by feature type.
57.5% if we have access to the bias of the target
corpus (Senseval7). This results show clearly that
the acquired corpus has useful information about the
word senses, and that bias is extremely important.
We will present two further experiments per-
formed with the monosemous corpus resource. The
goal of the first will be to measure the WSD per-
formance that we achieve using Semcor as the only
supervised data source. In our second experiment,
we will compare the performance of our totally un-
supervised approach (monosemous corpus and au-
tomatic bias) with other unsupervised approaches in
the Senseval-2 English lexical task.
5.1 Monosemous corpus and Semcor bias
In this experiment we compared the performance
using the monosemous corpus (with Semcor bias
and minimum ratio), and the examples from Sem-
cor. We noted that there were clear differences
depending on the number of training examples for
7Bias obtained from the training-set.
each word, therefore we studied each word-set de-
scribed in Section 3.4 separately. The results per
word-set are shown in Table 6. The figures cor-
respond to the recall training in Semcor, the web-
corpus, and the combination of both.
If we focus on set B (words with less than 10 ex-
amples in Semcor), we see that the MFS figure is
very low (40.1%). There are some words that do not
have any occurrence in Semcor, and thus the sense
is chosen at random. It made no sense to train the
DL for this set, therefore this result is not in the ta-
ble. For this set, the bias information from Semcor
is also scarce, but the DLs trained on the web-corpus
raise the performance to 47.8%.
For set A, the average number of examples is
higher, and this raises the results for Semcor MFS
(51.9%). We see that the recall for DL training
in Semcor is lower that the MFS baseline (50.5%).
The main reasons for these low results are the dif-
ferences between the training and testing corpora
(Semcor and Senseval). There have been previous
works on portability of hand-tagged corpora that
show how some constraints, like the genre or topic
of the corpus, affect heavily the results (Martinez
and Agirre, 2000). If we train on the web-corpus
the results improve, and the best results are ob-
tained with the combination of both corpora, reach-
ing 51.6%. We need to note, however, that this is
still lower than the Semcor MFS.
Finally, we will examine the results for the whole
set of nouns in the Senseval-2 lexical-sample (last
row in Table 6), where we see that the best approach
relies on the web-corpus. In order to disambiguate
the 29 nouns using only Semcor, we apply MFS
when there are less than 10 examples (set B), and
train the DLs for the rest.
The results in Table 6 show that the web-corpus
raises recall, and the best results are obtained com-
bining the Semcor data and the web examples
(50.3%). As we noted, the web-corpus is specially
useful when there are few examples in Semcor (set
B), therefore we made another test, using the web-
corpus only for set B, and applying MFS for set A.
The recall was slightly better (50.5%), as is shown
in the last column.
5.2 Monosemous corpus and Automatic bias
(unsupervised method)
In this experiment we compared the performance
of our unsupervised system with other approaches.
For this goal, we used the resources available from
the Senseval-2 competition8, where the answers of
the participating systems in the different tasks were
8http://www.senseval.org.
Bias Type 1stsense
Train
exam. Diff.
no bias 18.3 38.0 +19.7
web bias unsuperv. 33.3 39.8 +6.5
autom. ranking 36.1 43.2 +7.1
Semcor bias minimally- 47.8 49.8 +2.0
Senseval2 bias supervised 55.6 57.5 +1.9
Table 5: Performance (recall) on Senseval-2 lexical-
sample, using different bias to create the corpus.
The type column shows the kind of system.
Word-set MFS Semcor Web Semcor
+ Web
MFS &
Web
set A (> 10) 51.9 50.5 50.9 51.6 51.9
set B (< 10) 40.1 - 47.7 47.8 47.8
all words 47.8 47.4 49.8 50.3 50.5
Table 6: Recall training in Semcor, the acquired
web corpus (Semcor bias), and a combination of
both, compared to that of the Semcor MFS.
available. This made possible to compare our re-
sults and those of other systems deemed unsuper-
vised by the organizers on the same test data and set
of nouns.
From the 5 unsupervised systems presented in
the Senseval-2 lexical-sample task as unsupervised,
the WASP-Bench system relied on lexicographers
to hand-code information semi-automatically (Tug-
well and Kilgarriff, 2001). This system does not
use the training data, but as it uses manually coded
knowledge we think it falls clearly in the supervised
category.
The results for the other 4 systems and our own
are shown in Table 7. We show the results for the
totally unsupervised system and the minimally un-
supervised system (Semcor bias). We classified the
UNED system (Fernandez-Amoros et al, 2001) as
minimally supervised. It does not use hand-tagged
examples for training, but some of the heuristics that
are applied by the system rely on the bias informa-
tion available in Semcor. The distribution of senses
is used to discard low-frequency senses, and also to
choose the first sense as a back-off strategy. On the
same conditions, our minimally supervised system
attains 49.8 recall, nearly 5 points more.
The rest of the systems are fully unsupervised,
and they perform significantly worse than our sys-
tem.
6 Conclusions and Future Work
This paper explores the large-scale acquisition of
sense-tagged examples for WSD, which is a very
Method Type Recall
Web corpus (Semcor bias) minimally- 49.8
UNED supervised 45.1
Web corpus (Autom. bias) 43.3
Kenneth Litkowski-clr-ls unsupervised 35.8
Haynes-IIT2 27.9
Haynes-IIT1 26.4
Table 7: Our minimally supervised and fully unsu-
pervised systems compared to the unsupervised sys-
tems (marked in bold) in the 29 noun subset of the
Senseval-2 Lexical Sample.
promising line of research, but remains relatively
under-studied. We have applied the ?monosemous
relatives? method to construct automatically a web
corpus which we have used to train three systems
based on Decision Lists: one fully supervised (ap-
plying examples from Semcor and the web corpus),
one minimally supervised (relying on the distribu-
tion of senses in Semcor and the web corpus) and
another fully unsupervised (using an automatically
acquired sense rank and the web corpus). Those
systems were tested on the Senseval-2 lexical sam-
ple test set.
We have shown that the fully supervised system
combining our web corpus with the examples in
Semcor improves over the same system trained on
Semcor alone. This improvement is specially no-
ticeable in the nouns that have less than 10 examples
in Semcor. Regarding the minimally supervised
and fully unsupervised systems, we have shown
that they perform well better than the other systems
of the same category presented in the Senseval-2
lexical-sample competition.
The system can be trained for all nouns
in WordNet, using the data available at
http://ixa2.si.ehu.es/pub/sensecorpus.
The research also highlights the importance of
bias. Knowing how many examples are to be fed
into the machine learning system is a key issue. We
have explored several possibilities, and shown that
the learning system (DL) is able to learn from the
web corpus in all the cases, beating the respective
heuristic for sense distribution.
We think that this research opens the opportu-
nity for further improvements. We have to note that
the MFS heuristic and the supervised systems based
on the Senseval-2 training data are well ahead of
our results, and our research aims at investigating
ideas to close this gap. Some experiments on the
line of adding automatically retrieved examples to
available hand-tagged data (Semcor and Senseval-
2) have been explored. The preliminary results indi-
cate that this process has to be performed carefully,
taking into account the bias of the senses and apply-
ing a quality-check of the examples before they are
included in the training data.
For the future we also want to test the perfor-
mance of more powerful Machine Learning meth-
ods, explore feature selection methods for each in-
dividual word, and more sophisticated ways to com-
bine the examples from the web corpus with those
of Semcor or Senseval. Now that the monosemous
corpus is available for all nouns, we would also like
to test the system on the all-words task. In addition,
we will give preference to multiwords that contain
the target word when choosing the relatives. Finally,
more sophisticated methods to acquire examples are
now available, like ExRetriever (Fernandez et al,
2004), and they could open the way to better exam-
ples and performance.
7 Acknowledgments
We wish to thank Diana McCarthy, from the Univer-
sity of Sussex, for providing us the sense rank for
the target nouns. This research has been partially
funded by the European Commission (MEANING
IST-2001-34460).
References
E. Agirre and O. Lopez. 2004. Publicly available
topic signatures for all wordnet nominal senses.
In Proceedings of the 4rd International Con-
ference on Language Resources and Evaluation
(LREC), Lisbon, Portugal.
E. Agirre and D. Martinez. 2000. Exploring auto-
matic word sense disambiguation with decision
lists and the web. In Procedings of the COLING
2000 Workshop on Semantic Annotation and In-
telligent Content, Luxembourg.
S. Argamon-Engelson and I. Dagan. 1999.
Committee-based sample selection for proba-
bilistic classifiers. In Journal of Artificial Intel-
ligence Research, volume 11, pages 335?360.
T. Brants. 2000. Tnt - a statistical part-of-speech
tagger. In Proceedings of the Sixth Applied Nat-
ural Language Processing Conference, Seattle,
WA.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping
wordnets using structural information. In 38th
Anual Meeting of the Association for Computa-
tional Linguistics (ACL?2000), Hong Kong.
P. Edmonds and S. Cotton. 2001. Senseval-2:
Overview. In Proceedings of the Second Interna-
tional Workshop on evaluating Word Sense Dis-
ambiguation Systems, Toulouse, France.
D. Fernandez-Amoros, J. Gonzalo, and F. Verdejo.
2001. The uned systems at senseval-2. In Pro-
ceedings of the SENSEVAL-2 Workshop. In con-
junction with ACL, Toulouse, France.
J. Fernandez, M. Castillo, G. Rigau, J. Atserias, and
J. Turmo. 2004. Automatic acquisition of sense
examples using exretriever. In Proceedings of the
4rd International Conference on Language Re-
sources and Evaluation (LREC), Lisbon, Portu-
gal.
C. Leacock, M. Chodorow, and G. A. Miller. 1998.
Using corpus statistics and WordNet relations for
sense identification. In Computational Linguis-
tics, volume 24, pages 147?165.
D. Lin. 1998. Automatic retrieval and clustering
of similar words. In In Proceedings of COLING-
ACL, Montreal, Canada.
D. Martinez and E. Agirre. 2000. One sense per
collocation and genre/topic variations. In Pro-
ceedings of the Joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing
and Very Large Corpora, Hong Kong.
D. McCarthy, R. Koeling, J. Weeds, and J. Car-
roll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL) (to appear), Barcelona,
Spain.
R. Mihalcea. 2002. Bootstrapping large sense
tagged corpora. In Proceedings of the 3rd Inter-
national Conference on Language Resources and
Evaluation (LREC), Las Palmas, Spain.
G. A. Miller, C. Leacock, R. Tengi, and R. Bunker.
1993. A semantic concordance. In Proceedings
of the ARPA Human Language Technology Work-
shop, pages 303?308, Princeton, NJ.
D. Tugwell and A. Kilgarriff. 2001. Wasp-bench:
a lexicographic tool supporting word sense dis-
ambiguation. In Proceedings of the SENSEVAL-2
Workshop. In conjunction with ACL-2001/EACL-
2001, Toulouse, France.
D. Yarowsky. 1994. Decision lists for lexical am-
biguity resolution: Application to accent restora-
tion in spanish and french. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, Las Cruces, NM.
D. Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics
(ACL), Cambridge, MA.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 585?593,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Two graph-based algorithms for state-of-the-art WSD
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle and Aitor Soroa
IXA NLP Group
University of the Basque Country
Donostia, Basque Contry
a.soroa@si.ehu.es
Abstract
This paper explores the use of two graph
algorithms for unsupervised induction and
tagging of nominal word senses based on
corpora. Our main contribution is the op-
timization of the free parameters of those
algorithms and its evaluation against pub-
licly available gold standards. We present
a thorough evaluation comprising super-
vised and unsupervised modes, and both
lexical-sample and all-words tasks. The
results show that, in spite of the infor-
mation loss inherent to mapping the in-
duced senses to the gold-standard, the
optimization of parameters based on a
small sample of nouns carries over to all
nouns, performing close to supervised sys-
tems in the lexical sample task and yield-
ing the second-best WSD systems for the
Senseval-3 all-words task.
1 Introduction
Word sense disambiguation (WSD) is a key
enabling-technology. Supervised WSD tech-
niques are the best performing in public evalu-
ations, but need large amounts of hand-tagged
data. Existing hand-annotated corpora like Sem-
Cor (Miller et al, 1993), which is annotated with
WordNet senses (Fellbaum, 1998) allow for a
small improvement over the simple most frequent
sense heuristic, as attested in the all-words track of
the last Senseval competition (Snyder and Palmer,
2004). In theory, larger amounts of training data
(SemCor has approx. 700K words) would improve
the performance of supervised WSD, but no cur-
rent project exists to provide such an expensive re-
source.
Supervised WSD is based on the ?fixed-list of
senses? paradigm, where the senses for a tar-
get word are a closed list coming from a dic-
tionary or lexicon. Lexicographers and seman-
ticists have long warned about the problems of
such an approach, where senses are listed sepa-
rately as discrete entities, and have argued in fa-
vor of more complex representations, where, for
instance, senses are dense regions in a contin-
uum (Cruse, 2000).
Unsupervised WSD has followed this line of
thinking, and tries to induce word senses directly
from the corpus. Typical unsupervised WSD sys-
tems involve clustering techniques, which group
together similar examples. Given a set of induced
clusters (which represent word uses or senses1),
each new occurrence of the target word will be
compared to the clusters and the most similar clus-
ter will be selected as its sense.
Most of the unsupervised WSD work has been
based on the vector space model, where each
example is represented by a vector of features
(e.g. the words occurring in the context), and
the induced senses are either clusters of ex-
amples (Schu?tze, 1998; Purandare and Peder-
sen, 2004) or clusters of words (Pantel and Lin,
2002). Recently, Ve?ronis (Ve?ronis, 2004) has pro-
posed HyperLex, an application of graph models
to WSD based on the small-world properties of
cooccurrence graphs. Graph-based methods have
gained attention in several areas of NLP, including
knowledge-based WSD (Mihalcea, 2005; Navigli
and Velardi, 2005) and summarization (Erkan and
Radev, 2004; Mihalcea and Tarau, 2004).
The HyperLex algorithm presented in (Ve?ronis,
2004) is entirely corpus-based. It builds a cooccur-
rence graph for all pairs of words cooccurring in
the context of the target word. Ve?ronis shows that
this kind of graph fulfills the properties of small
world graphs, and thus possesses highly connected
1Unsupervised WSD approaches prefer the term ?word
uses? to ?word senses?. In this paper we use them inter-
changeably to refer to both the induced clusters, and to the
word senses from some reference lexicon.
585
components (hubs) in the graph. These hubs even-
tually identify the main word uses (senses) of the
target word, and can be used to perform word
sense disambiguation. These hubs are used as a
representation of the senses induced by the sys-
tem, the same way that clusters of examples are
used to represent senses in clustering approaches
to WSD (Purandare and Pedersen, 2004).
One of the problems of unsupervised systems
is that of managing to do a fair evaluation.
Most of current unsupervised systems are evalu-
ated in-house, with a brief comparison to a re-
implementation of a former system, leading to a
proliferation of unsupervised systems with little
ground to compare among them.
In preliminary work (Agirre et al, 2006), we
have shown that HyperLex compares favorably
to other unsupervised systems. We defined a
semi-supervised setting for optimizing the free-
parameters of HyperLex on the Senseval-2 En-
glish Lexical Sample task (S2LS), which con-
sisted on mapping the induced senses onto the
official sense inventory using the training part of
S2LS. The best parameters were then used on the
Senseval-3 English Lexical Sample task (S3LS),
where a similar semi-supervised method was used
to output the official sense inventory.
This paper extends the previous work in sev-
eral aspects. First of all, we adapted the PageR-
ank graph-based method (Brin and Page, 1998)
for WSD and compared it with HyperLex.
We also extend the previous evaluation scheme,
using measures in the clustering community which
only require a gold standard clustering and no
mapping step. This allows for having a purely
unsupervised WSD system, and at the same time
comparing supervised and unsupervised systems
according to clustering criteria.
We also include the Senseval-3 English All-
words testbed (S3AW), where, in principle, unsu-
pervised and semi-supervised systems have an ad-
vantage over purely supervised systems due to the
scarcity of training data. We show that our sys-
tem is competitive with supervised systems, rank-
ing second.
This paper is structured as follows. We first
present two graph-based algorithms, HyperLex
and PageRank. Section 3 presents the two evalu-
ation frameworks. Section 4 introduces parameter
optimization. Section 5 shows the experimental
setting and results. Section 6 analyzes the results
and presents related work. Finally, we draw the
conclusions and advance future work.
2 A graph algorithm for corpus-based
WSD
The basic steps for our implementation of Hyper-
Lex and its variant using PageRank are common.
We first build the cooccurrence graph, then we se-
lect the hubs that are going to represent the senses
using two different strategies inspired by Hyper-
Lex and PageRank. We are then ready to use the
induced senses to do word sense disambiguation.
2.1 Building cooccurrence graphs
For each word to be disambiguated, a text corpus
is collected, consisting of the paragraphs where
the word occurs. From this corpus, a cooccur-
rence graph for the target word is built. Vertices
in the graph correspond to words2 in the text (ex-
cept the target word itself). Two words appear-
ing in the same paragraph are said to cooccur, and
are connected with edges. Each edge is assigned
a weight which measures the relative frequency of
the two words cooccurring. Specifically, let wij be
the weight of the edge3 connecting nodes i and j,
then wij = 1 ? max[P (i | j), P (j | i)], where
P (i | j) = freqijfreqj and P (j | i) =
freqij
freqi .The weight of an edge measures how tightly
connected the two words are. Words which always
occur together receive a weight of 0. Words rarely
cooccurring receive weights close to 1.
2.2 Selecting hubs: HyperLex vs. PageRank
Once the cooccurrence graph is built, Ve?ronis pro-
poses a simple iterative algorithm to obtain its
hubs. At each step, the algorithm finds the ver-
tex with highest relative frequency4 in the graph,
and, if it meets some criteria, it is selected as a hub.
These criteria are determined by a set of heuristic
parameters, that will be explained later in Section
4. After a vertex is selected to be a hub, its neigh-
bors are no longer eligible as hub candidates. At
any time, if the next vertex candidate has a relative
frequency below a certain threshold, the algorithm
stops.
Another alternative is to use the PageRank algo-
rithm (Brin and Page, 1998) for finding hubs in the
2Following Ve?ronis, we only work on nouns.
3The cooccurrence graph is undirected, i.e. wij = wji
4In cooccurrence graphs, the relative frequency of a vertex
and its degree are linearly related, and it is therefore possible
to avoid the costly computation of the degree.
586
coocurrence graph. PageRank is an iterative algo-
rithm that ranks all the vertices according to their
relative importance within the graph following a
random-walk model. In this model, a link between
vertices v1 and v2 means that v1 recommends v2.
The more vertices recommend v2, the higher the
rank of v2 will be. Furthermore, the rank of a ver-
tex depends not only on how many vertices point
to it, but on the rank of these vertices as well.
Although PageRank was initially designed to
work with directed graphs, and with no weights
in links, the algorithm can be easily extended
to model undirected graphs whose edges are
weighted. Specifically, let G = (V, E) be an undi-
rected graph with the set of vertices V and set of
edges E. For a given vertex vi, let In(vi) be the set
of vertices pointing to it5. The rank of vi is defined
as:
P (vi) = (1? d) + d
?
j?In(vi)
wji
?
k?In(vj) wjk
P (vj)
where wij is the weight of the link between ver-
tices vi and vj , and 0 ? d ? 1. d is called the
damping factor and models the probability of a
web surfer standing at a vertex to follow a link
from this vertex (probability d) or to jump to a ran-
dom vertex in the graph (probability 1 ? d). The
factor is usually set at 0.85.
The algorithm initializes the ranks of the ver-
tices with a fixed value (usually 1N for a graph with
N vertices) and iterates until convergence below a
given threshold is achieved, or, more typically, un-
til a fixed number of iterations are executed. Note
that the convergence of the algorithms doesn?t de-
pend on the initial value of the ranks.
After running the algorithm, the vertices of the
graph are ordered in decreasing order according to
its rank, and a number of them are chosen as the
main hubs of the word. The hubs finally selected
depend again of some heuristics and will be de-
scribed in section 4.
2.3 Using hubs for WSD
Once the hubs that represent the senses of the word
are selected (following any of the methods pre-
sented in the last section), each of them is linked
to the target word with edges weighting 0, and
the Minimum Spanning Tree (MST) of the whole
graph is calculated and stored.
5As G is undirected, the in-degree of a vertex v is equal
to its out-degree.
The MST is then used to perform word sense
disambiguation, in the following way. For every
instance of the target word, the words surrounding
it are examined and looked up in the MST. By con-
struction of the MST, words in it are placed under
exactly one hub. Each word in the context receives
a set of scores s, with one score per hub, where all
scores are 0 except the one corresponding to the
hub where it is placed. If the scores are organized
in a score vector, all values are 0, except, say, the
i-th component, which receives a score d(hi, v),
which is the distance between the hub hi and the
node representing the word v. Thus, d(hi, v) as-
signs a score of 1 to hubs and the score decreases
as the nodes move away from the hub in the tree.
For a given occurrence of the target word, the
score vectors of all the words in the context are
added, and the hub that receives the maximum
score is chosen.
3 Evaluating unsupervised WSD systems
All unsupervised WSD algorithms need some ad-
dition in order to be evaluated. One alternative, as
in (Ve?ronis, 2004), is to manually decide the cor-
rectness of the hubs assigned to each occurrence
of the words. This approach has two main disad-
vantages. First, it is expensive to manually verify
each occurrence of the word, and different runs of
the algorithm need to be evaluated in turn. Sec-
ond, it is not an easy task to manually decide if
an occurrence of a word effectively corresponds
with the use of the word the assigned hub refers
to, specially considering that the person is given a
short list of words linked to the hub. Besides, it is
widely acknowledged that people are leaned not to
contradict the proposed answer.
A second alternative is to evaluate the system
according to some performance in an application,
e.g. information retrieval (Schu?tze, 1998). This is
a very attractive idea, but requires expensive sys-
tem development and it is sometimes difficult to
separate the reasons for the good (or bad) perfor-
mance.
A third alternative would be to devise a method
to map the hubs (clusters) returned by the system
to the senses in a lexicon. Pantel and Lin (2002)
automatically mapped the senses to WordNet, and
then measured the quality of the mapping. More
recently, tagged corpora have been used to map
the induced senses, and then compare the sys-
tems over publicly available benchmarks (Puran-
587
dare and Pedersen, 2004; Niu et al, 2005; Agirre
et al, 2006), which offers the advantage of com-
paring to other systems, but converts the whole
system into semi-supervised. See Section 5 for
more details on these systems. Note that the map-
ping introduces noise and information loss, which
is a disadvantage when comparing to other sys-
tems that rely on the gold-standard senses.
Yet another possibility is to evaluate the induced
senses against a gold standard as a clustering task.
Induced senses are clusters, gold standard senses
are classes, and measures from the clustering lit-
erature like entropy or purity can be used. In this
case the manually tagged corpus is taken to be the
gold standard, where a class is the set of examples
tagged with a sense.
We decided to adopt the last two alternatives,
since they allow for comparison over publicly
available systems of any kind.
3.1 Evaluation of clustering: hubs as clusters
In this setting the selected hubs are treated as
clusters of examples and gold standard senses are
classes. In order to compare the clusters with the
classes, hand annotated corpora are needed (for in-
stance Senseval). The test set is first tagged with
the induced senses. A perfect clustering solution
will be the one where each cluster has exactly the
same examples as one of the classes, and vice
versa. The evaluation is completely unsupervised.
Following standard cluster evaluation prac-
tice (Zhao and Karypis, 2005), we consider three
measures: entropy, purity and Fscore. The entropy
measure considers how the various classes of ob-
jects are distributed within each cluster. In gen-
eral, the smaller the entropy value, the better the
clustering algorithm performs. The purity mea-
sure considers the extent to which each cluster
contained objects from primarily one class. The
larger the values of purity, the better the cluster-
ing algorithm performs. The Fscore is used in a
similar fashion to Information Retrieval exercises,
with precision and recall defined as the percent-
age of correctly ?retrieved? examples for a clus-
ter (divided by total cluster size), and recall as the
percentage of correctly ?retrieved? examples for a
cluster (divided by total class size). For a formal
definition refer to (Zhao and Karypis, 2005). If the
clustering is identical to the original classes in the
datasets, FScore will be equal to one which means
that the higher the FScore, the better the clustering
is.
3.2 Evaluation as supervised WSD: mapping
hubs to senses
(Agirre et al, 2006) presents a straightforward
framework that uses hand-tagged material in or-
der to map the induced senses into the senses used
in a gold standard . The WSD system first tags the
training part of some hand-annotated corpus with
the induced hubs. The hand labels are then used
to construct a matrix relating assigned hubs to ex-
isting senses, simply counting the times an occur-
rence with sense sj has been assigned hub hi. In
the testing step we apply the WSD algorithm over
the test corpus, using the hubs-to-senses matrix to
select the sense with highest weights. See (Agirre
et al, 2006) for further details.
4 Tuning the parameters
The behavior of the original HyperLex algorithm
was influenced by a set of heuristic parameters,
which were set by Ve?ronis following his intuition.
In (Agirre et al, 2006) we tuned the parameters us-
ing the mapping strategy for evaluation. We set a
range for each of the parameters, and evaluated the
algorithm for each combination of the parameters
on a fixed set of words (S2LS), which was differ-
ent from the final test sets (S3LS and S3AW). This
ensures that the chosen parameter set can be used
for any noun, and is not overfitted to a small set of
nouns.
In this paper, we perform the parameter tuning
according to four different criteria, i.e., best su-
pervised performance and best unsupervised en-
tropy/purity/FScore performance. At the end, we
have four sets of parameters (those that obtained
the best results in S2LS for each criterion), and
each set is then selected to be run against the S3LS
and S3AW datasets.
The parameters of the graph-based algorithm
can be divided in two sets: those that affect how
the cooccurrence graph is built (p1?p4 below), and
those that control the way the hubs are extracted
from it (p5?p8 below).
p1 Minimum frequency of edges (occurrences)
p2 Minimum frequency of vertices (words)
p3 Edges with weights above this value are removed
p4 Context containing fewer words are not processed
p5 Minimum number of adjacent vertices in a hub
p6 Max. mean weight of the adjacent vertices of a hub
p7 Minimum frequency of hubs
p8 Number of selected hubs
588
Vr opt Pr fr (p7) and Pr fx (p8)
Vr Range Best Range Best
p1 5 1-3 1 1-3 2
p2 10 2-4 3 2-4 3
p3 .9 .3-.7 .4 .4-.5 .5
p4 4 4 4 4 4
p5 6 1-7 1 ? ?
p6 .8 .6-.95 .95 ? ?
p7 .001 .0009-.003 .001 .0015-.0025 .0016
p8 ? ? ? 50-65 55
Table 1: Parameters of the HyperLex algorithm
Both strategies to select hubs from the coocur-
rence graph (cf. Sect. 2.2) share parameters p1?
p4. The algorithm proposed by Ve?ronis uses p5?
p6 as requirements for hubs, and p7 as the thresh-
old to stop looking for more hubs: candidates with
frequency below p7 are not eligible to be hubs.
Regarding PageRank the original formulation
does not have any provision for determining which
are hubs and which not, it just returns a weighted
list of vertices. We have experimented with two
methods: a threshold for the frequency of the hubs
(as before, p7), and a fixed number of hubs for ev-
ery target word (p8). For a shorthand we use Vr for
Veronis? original formulation with default param-
eters, Vr opt for optimized parameters, and Pr fr
and Pr fx respectively for the two ways of using
PageRank.
Table 1 lists the parameters of the HyperLex al-
gorithm, with the default values proposed for them
in the original work (second column), the ranges
that we explored, and the optimal values according
to the supervised recall evaluation (cf. Sect. 3.1).
For Vr opt we tried 6700 combinations. PageRank
has less parameters, and we also used the previous
optimization of Vr opt to limit the range of p4, so
Pr fr and Pr fx get respectively 180 and 288 com-
binations.
5 Experiment setting and results
To evaluate the HyperLex algorithm in a standard
benchmark, we will first focus on a more exten-
sive evaluation of S3LS and then see the results
in S3AW (cf. Sec. 5.4). Following the design
for evaluation explained in Section 3, we use the
standard train-test split for the supervised evalua-
tion, while the unsupervised evaluation only uses
the test part.
Table 2 shows the results of the 4 variants of
our algorithm. Vr stands for the original Vero-
nis algorithm with default parameters, Vr opt to
our optimized version, and Pr fr and Pr fx to the
Sup. Unsupervised
Rec. Entr. Pur. FS
Vr 59.9 50.3 58.2 44.1
Vr opt 64.6 18.3 78.5 35.0
Pr fr 64.5 18.7 77.2 34.3
Pr fx 62.2 25.4 72.2 33.3
1ex-1hub 40.1 0.0 100.0 14.5
MFS 54.5 53.2 52.8 28.3
S3LS-best 72.9 19.9 67.3 63.8
kNN-all 70.6 21.2 64.0 60.6
kNN-BoW 63.5 22.6 61.1 57.1
Cymfony (10%-S3LS) 57.9 25.0 55.7 52.0
Prob0 (MFS-S3) 54.2 28.8 49.3 46.0
clr04 (MFS-Sc) 48.8 25.8 52.5 46.2
Ciaosenso (MFS-Sc) 48.7 28.0 50.3 48.8
duluth-senserelate 47.5 27.2 51.1 44.9
Table 2: Results for the nouns in S3LS using the 4 meth-
ods (Vr, Vr opt, Pr fr and Pr fx). Each of the methods was
optimized in S2LS using the 4 evaluation criteria (Supervised
recall, Entropy, Purity and Fscore) and evaluated on S3LS ac-
cording to the respective evaluation criteria (in the columns).
Two baselines, plus 3 supervised and 5 unsupervised systems
are also shown. Bold is used for best results in each category.
two variants of PageRank. In the columns we find
the evaluation results according to our 4 criteria.
For supervised evaluation we indicate only recall,
which in our case equals precision, as the cover-
age is 100% in all cases (values returned by the
official Senseval scorer). We also include 2 base-
lines, a system returning a single cluster (that of
the most frequent sense, MFS), and another re-
turning one cluster for each example (1ex-1hub).
The last rows list the results for 3 supervised and
5 unsupervised systems (see Sect. 5.1). We will
comment on the result of this table from different
perspectives.
5.1 Supervised evaluation
In this subsection we will focus in the first four
evaluation rows in Table 2. All variants of the al-
gorithm outperform by an ample margin the MFS
and the 1ex-1hub baselines when evaluated on
S3LS recall. This means that the method is able
to learn useful hubs. Note that we perform this su-
pervised evaluation just for comparison with other
systems, and to prove that we are able to provide
high performance WSD.
The default parameter setting (Vr) gets the
worst results, followed by the fixed-hub imple-
mentation of PageRank (Pr fx). Pagerank with
frequency threshold (Pr fr) and the optimized
Veronis (Vr opt) obtain a 10 point improvement
over the MFS baseline with very similar results
(the difference is not statistically significant ac-
cording to McNemar?s test at 95% confidence
589
level).
Table 2 also shows the results of three super-
vised systems. These results (and those of the
other unsupervised systems in the table) where ob-
tained from the Senseval website, and the only
processing we did was to filter nouns. S3LS-best
stands for the the winner of S3LS (Mihalcea et al,
2004), which is 8.3 points over our method. We
also include the results of two of our in-house sys-
tems. kNN-all is a state-of-the-art system (Agirre
et al, 2005) using wide range of local and top-
ical features, and only 2.3 points below the best
S3LS system. kNN-BoW which is the same super-
vised system, but restricted to bag-of-words fea-
tures only, which are the ones used by our graph-
based systems. The table shows that Vr opt and
Pr fr are one single point from kNN-BoW, which
is an impressive result if we take into account the
information loss of the mapping step and that we
tuned our parameters on a different set of words.
The last 5 rows of Table 2 show several un-
supervised systems, all of which except Cym-
fony (Niu et al, 2005) and (Purandare and Ped-
ersen, 2004) participated in S3LS (check (Mihal-
cea et al, 2004) for further details on the systems).
We classify them according to the amount of ?su-
pervision? they have: some have access to most-
frequent information (MFS-S3 if counted over
S3LS, MFS-Sc if counted over SemCor), some use
10% of the S3LS training part for mapping (10%-
S3LS). Only one system (Duluth) did not use in
any way hand-tagged corpora.
The table shows that Vr opt and Pr fr are more
than 6 points above the other unsupervised sys-
tems, but given the different typology of unsuper-
vised systems, it?s unfair to draw definitive con-
clusions from a raw comparison of results. The
system coming closer to ours is that described in
(Niu et al, 2005). They use hand tagged corpora
which does not need to include the target word to
tune the parameters of a rather complex clustering
method which does use local features. They do use
the S3LS training corpus for mapping. For every
sense of the target word, three of its contexts in
the train corpus are gathered (around 10% of the
training data) and tagged. Each cluster is then re-
lated with its most frequent sense. The mapping
method is similar to ours, but we use all the avail-
able training data and allow for different hubs to
be assigned to the same sense.
Another system similar to ours is (Purandare
and Pedersen, 2004), which unfortunately was
evaluated on Senseval 2 data and is not included
in the table. The authors use first and second or-
der bag-of-word context features to represent each
instance of the corpus. They apply several cluster-
ing algorithms based on the vector space model,
limiting the number of clusters to 7. They also
use all available training data for mapping, but
given their small number of clusters they opt for a
one-to-one mapping which maximizes the assign-
ment and discards the less frequent clusters. They
also discard some difficult cases, like senses and
words with low frequencies (10% of total occur-
rences and 90, respectively). The different test set
and mapping system make the comparison diffi-
cult, but the fact that the best of their combina-
tions beats MFS by 1 point on average (47.6% vs.
46.4%) for the selected nouns and senses make us
think that our results are more robust (nearly 10%
over MFS).
5.2 Clustering evaluation
The three columns corresponding to fully unsu-
pervised evaluation in Table 2 show that all our
3 optimized variants easily outperform the MFS
baseline. The best results are in this case for the
optimized Veronis, followed closely by Pagerank
with frequency threshold.
The comparison with the supervised and unsu-
pervised systems shows that our system gets better
entropy and purity values, but worse FScore. This
can be explained by the bias of entropy and purity
towards smaller and more numerous clusters. In
fact the 1ex-1hub baseline obtains the best entropy
and purity scores. Our graph-based system tends
to induce a large number of senses (with averages
of 60 to 70 senses). On the other hand FScore pe-
nalizes the systems inducing a different number of
clusters. As the supervised and unsupervised sys-
tems were designed to return the same (or similar)
number of senses as in the gold standard, they at-
tain higher FScores. This motivated us to compare
the results of the best parameters across evaluation
methods.
5.3 Comparison across evaluation methods
Table 3 shows all 16 evaluation possibilities for
each variant of the algorithm, depending of the
evaluation criteria used in S2LS (in the rows)
and the evaluation criteria used in S3LS (in the
columns). This table shows that the best results (in
bold for each variant) tend to be in the diagonal,
590
that is, when the same evaluation criterion is used
for optimization and test, but it is not decisive. If
we take the first row (supervised evaluation) as the
most credible criterion, we can see that optimiz-
ing according to entropy and purity get similar and
sometimes better result (Pr fr and Pr fx). On the
contrary the Fscore yields worse results by far.
This indicates that a purely unsupervised sys-
tem evaluated according to the gold standard
(based on entropy or purity) yields optimal param-
eters similar to the supervised (mapped) version.
This is an important result, as it shows that the
quality in performance does not come from the
mapping step, but from the algorithm and opti-
mal parameter setting. The table shows that op-
timization on purity and entropy criteria do corre-
late with good performance in the supervised eval-
uation.
The failure of FScore based optimization, in our
opinion, indicates that our clustering algorithm
prefers smaller and more numerous clusters, com-
pared to the gold standard. FScore prefers cluster-
ing solutions that have a similar number of clusters
to that of the gold standard, but it is unable to drive
the optimization or our algorithm towards good re-
sults in the supervised evaluation.
All in all, the best results are attained with
smaller and more numerous hubs, a kind of micro-
senses. This effect is the same for all three vari-
ants tried and all evaluation criteria, with Fscore
yielding less clusters. At first we were uncom-
fortable with this behavior, so we checked whether
HyperLex was degenerating into a trivial solution.
This was the main reason to include the 1ex-1hub
baseline, which simulates a clustering algorithm
returning one hub per example, and its precision
was 40.1, well below the MFS baseline. We also
realized that our results are in accordance with
some theories of word meaning, e.g. the ?indef-
initely large set of prototypes-within-prototypes?
envisioned in (Cruse, 2000). Ted Pedersen has
also observed a similar behaviour in his vector-
space model clustering experiments (PC). We now
think that the idea of having many micro-senses
is attractive for further exploration, specially if we
are able to organize them into coarser hubs in fu-
ture work.
5.4 S3AW task
In the Senseval-3 all-words task (Snyder and
Palmer, 2004) all words in three document ex-
Sup. Unsupervised
Alg. Opt. Rec. Entr. Pur. FS
Vr Sup 64.6 18.4 77.9 30.0
Ent 64.6 18.3 78.3 29.1
Pur 63.7 19.0 78.5 30.8
Fsc 60.4 38.2 63.5 35.0
Pr fr Sup 64.5 20.8 76.1 28.6
Ent 64.6 18.7 77.7 27.2
Pur 64.7 19.3 77.2 27.6
Fsc 61.2 36.0 65.2 34.3
Pr fx Sup 62.2 28.2 69.3 29.5
Ent 63.1 25.4 72.2 28.4
Pur 63.1 25.4 72.2 28.4
Fsc 54.5 32.9 66.5 33.3
Table 3: Cross-evaluation comparison. In the rows the eval-
uation method for optmizing over S2LS is shown, and in the
columns the result over S3LS according to the different eval-
uation methods.
recall
kuaw 70.9
Pr fr 70.7
Vr opt 70.1
GAMBL 70.1
MFS 69.9
LCCaw 68.6
Table 4: Results for the nouns in S3AW, compared to the
most frequent baseline and the top three supervised systems
cerpts need to be disambiguated. Given the
scarce amount of training data available in Sem-
cor (Miller et al, 1993), supervised systems barely
improve upon the simple most frequent heuris-
tic. In this setting the unsupervised evaluation
schemes are not feasible, as many of the target
words occur only once, so we used the map-
ping strategy with Semcor to produce the required
WordNet senses in the output.
Table 4 shows the results for our systems with
the best parameters according to the supervised
criterion on S2LS, plus the top three S3AW super-
vised systems and the most frequent sense heuris-
tic. In order to focus the comparison, we only kept
noun occurrences of all systems and filtered out
multiwords, target words with two different lem-
mas and unknown tags, leaving a total of 857 oc-
currences of nouns. We can see that Pr fr is only
0.2 from the S3AW winning system, demonstrat-
ing that our unsupervised graph-based systems
that use Semcor for mapping are nearly equivalent
to the most powerful supervised systems to date.
In fact, the differences in performance for the sys-
tems are not statistically significant (McNemar?s
test at 95% significance level).
591
6 Conclusions and further work
This paper has explored the use of two graph algo-
rithms for corpus-based disambiguation of nomi-
nal senses. We have shown that the parameter op-
timization learnt over a small set of nouns signifi-
cantly improves the performance for all nouns, and
produces a system which (1) in a lexical-sample
setting (Senseval 3 dataset) is 10 points over the
Most-Frequent-Sense baseline, 1 point over a su-
pervised system using the same kind of informa-
tion (i.e. bag-of-words features), and 8 points be-
low the best supervised system, and (2) in the all-
words setting is a` la par the best supervised sys-
tem. The performance of PageRank is statistically
the same as that of HyperLex, with the advantage
of PageRank of using less parameters.
In order to compete on the same test set as su-
pervised systems, we do use hand-tagged data, but
only to do the mapping from the induced senses
into the gold standard senses. In fact, we believe
that using our WSD system as a purely unsuper-
vised system (i.e. returning just hubs), the per-
fomance would be higher, as we would avoid the
information loss in the mapping. We would like
to test this on Information Retrieval, perhaps on a
setting similar to that of (Schu?tze, 1998), which
would allow for an indirect evaluation of the qual-
ity and a comparison with supervised WSD system
on the same grounds.
We have also shown that the optimization ac-
cording to purity and entropy values (which does
not need the supervised mapping step) yields very
good parameters, comparable to those obtained in
the supervised optimization strategy. This indi-
cates that we are able to optimize the algorithm
in a completely unsupervised fashion for a small
number of words, and then carry over to tag new
text with the induced senses.
Regarding efficiency, our implementation of
HyperLex is extremely fast. Trying the 6700 com-
binations of parameters takes 5 hours in a 2 AMD
Opteron processors at 2GHz and 3Gb RAM. A
single run (building the MST, mapping and tag-
ging the test sentences) takes only 16 sec. For this
reason, even if an on-line version would be in prin-
ciple desirable, we think that this batch version is
readily usable as a standalone word sense disam-
biguation system.
Both graph-based methods and vector-based
clustering methods rely on local information, typ-
ically obtained by the occurrences of neighbor
words in context. The advantage of graph-
based techniques over over vector-based cluster-
ing might come from the fact that the former are
able to measure the relative importance of a vertex
in the whole graph, and thus combine both local
and global cooccurrence information.
For the future, we would like to look more
closely the micro-senses induced by HyperLex,
and see if we can group them into coarser clus-
ters. We would also like to integrate different
kinds of information, specially the local or syn-
tactic features so successfully used by supervised
systems, but also more heterogeneous information
from knowledge bases.
Graph models have been very successful in
some settings (e.g. the PageRank algorithm of
Google), and have been rediscovered recently
for natural language tasks like knowledge-based
WSD, textual entailment, summarization and de-
pendency parsing. Now that we have set a ro-
bust optimization and evaluation framework we
would like to test other such algorithms (e.g.
HITS (Kleinberg, 1999)) in the same conditions.
Acknowledgements
Oier Lopez de Lacalle enjoys a PhD grant from the
Basque Government. We thank the comments of
the three anonymous reviewers.
References
E. Agirre, O. Lopez de Lacalle, and D. Martinez. 2005.
Exploring feature spaces with svd and unlabeled
data for word sense disambiguation. In Proc. of
RANLP.
E. Agirre, O. Lopez de Lacalle, D. Martinez, and
A. Soroa. 2006. Evaluating and optimizing the pa-
rameters of an unsupervised graph-based wsd algo-
rithm. In Proc. of the NAACL Texgraphs workshop.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1-7).
D. A. Cruse, 2000. Polysemy: Theoretical and Compu-
tational Approaches, chapter Aspects of the Micro-
structure of Word Meanings, pages 31?51. OUP.
G Erkan and D. R. Radev. 2004. Lexrank: Graph-
based centrality as salience in text summarization.
Journal of Artificial Intelligence Research (JAIR).
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
592
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604?632.
R. Mihalcea and P Tarau. 2004. Textrank: Bringing
order into texts. In Proc. of EMNLP2004.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The senseval-3 english lexical sample task. In
R. Mihalcea and P. Edmonds, editors, Senseval-3
proccedings, pages 25?28. ACL, July.
R. Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proc. of
EMNLP2005.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker.
1993. A semantic concordance. In Proc. of the
ARPA HLT workshop.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: a knowledge-based approach
to word sense disambiguation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
7(27):1063?1074, June.
C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word in-
dependent context pair classification model for word
sense disambiguation. In Proc. of CoNLL-2005.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD02.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proc. of CoNLL-2004, pages 41?
48.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proc. of SENSEVAL.
J. Ve?ronis. 2004. Hyperlex: lexical cartography for in-
formation retrieval. Computer Speech & Language,
18(3):223?252.
Y Zhao and G Karypis. 2005. Hierarchical clustering
algorithms for document datasets. Data Mining and
Knowledge Discovery, 10(2):141?168.
593
Workshop on TextGraphs, at HLT-NAACL 2006, pages 89?96,
New York City, June 2006. c?2006 Association for Computational Linguistics
Evaluating and optimizing the parameters
of an unsupervised graph-based WSD algorithm
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle and Aitor Soroa
IXA NLP Group
University of Basque Country
Donostia, Basque Contry
a.soroa@ehu.es
Abstract
Ve?ronis (2004) has recently proposed
an innovative unsupervised algorithm for
word sense disambiguation based on
small-world graphs called HyperLex. This
paper explores two sides of the algorithm.
First, we extend Ve?ronis? work by opti-
mizing the free parameters (on a set of
words which is different to the target set).
Second, given that the empirical compar-
ison among unsupervised systems (and
with respect to supervised systems) is sel-
dom made, we used hand-tagged corpora
to map the induced senses to a standard
lexicon (WordNet) and a publicly avail-
able gold standard (Senseval 3 English
Lexical Sample). Our results for nouns
show that thanks to the optimization of
parameters and the mapping method, Hy-
perLex obtains results close to supervised
systems using the same kind of bag-of-
words features. Given the information
loss inherent in any mapping step and the
fact that the parameters were tuned for an-
other set of words, these are very interest-
ing results.
1 Introduction
Word sense disambiguation (WSD) is a key en-
abling technology. Supervised WSD techniques are
the best performing in public evaluations, but need
large amounts of hand-tagging data. Existing hand-
annotated corpora like SemCor (Miller et al, 1993),
which is annotated with WordNet senses (Fellbaum,
1998) allow for a small improvement over the simple
most frequent sense heuristic, as attested in the all-
words track of the last Senseval competition (Sny-
der and Palmer, 2004). In theory, larger amounts
of training data (SemCor has approx. 500M words)
would improve the performance of supervised WSD,
but no current project exists to provide such an ex-
pensive resource.
Supervised WSD is based on the ?fixed-list of
senses? paradigm, where the senses for a target word
are a closed list coming from a dictionary or lex-
icon. Lexicographers and semanticists have long
warned about the problems of such an approach,
where senses are listed separately as discrete enti-
ties, and have argued in favor of more complex rep-
resentations, where, for instance, senses are dense
regions in a continuum (Cruse, 2000).
Unsupervised WSD has followed this line of
thinking, and tries to induce word senses directly
from the corpus. Typical unsupervised WSD sys-
tems involve clustering techniques, which group to-
gether similar examples. Given a set of induced
clusters (which represent word uses or senses1),
each new occurrence of the target word will be com-
pared to the clusters and the most similar cluster will
be selected as its sense.
Most of the unsupervised WSD work has been
based on the vector space model (Schu?tze, 1998;
Pantel and Lin, 2002; Purandare and Pedersen,
2004), where each example is represented by a vec-
tor of features (e.g. the words occurring in the
context). Recently, Ve?ronis (Ve?ronis, 2004) has
1Unsupervised WSD approaches prefer the term ?word uses?
to ?word senses?. In this paper we use them interchangeably to
refer to both the induced clusters, and to the word senses from
some reference lexicon.
89
proposed HyperLex, an application of graph mod-
els to WSD based on the small-world properties
of cooccurrence graphs. Hand inspection of the
clusters (called hubs in this setting) by the author
was very positive, with hubs capturing the main
senses of the words. Besides, hand inspection of the
disambiguated occurrences yielded precisions over
95% (compared to a most frequent baseline of 73%)
which is an outstanding figure for WSD systems.
We noticed that HyperLex had some free param-
eters and had not been evaluated against a public
gold standard. Besides, we were struck by the few
works where supervised and unsupervised systems
were evaluated on the same test data. In this pa-
per we use an automatic method to map the induced
senses to WordNet using hand-tagged corpora, en-
abling the automatic evaluation against available
gold standards (Senseval 3 English Lexical Sam-
ple S3LS (Mihalcea et al, 2004)) and the automatic
optimization of the free parameters of the method.
The use of hand-tagged corpora for tagging makes
this algorithm a mixture of unsupervised and super-
vised: the method to induce senses in completely
unsupervised, but the mapping is supervised (albeit
very straightforward).
This paper is structured as follows. We first
present the graph-based algorithm as proposed by
Ve?ronis, reviewing briefly the features of small-
world graphs. Section 3 presents our framework for
mapping and evaluating the induced hubs. Section 4
introduces parameter optimization. Section 5 shows
the experiment setting and results. Section 6 ana-
lyzes the results and presents related work. Finally,
we draw the conclusions and advance future work.
2 HyperLex
Before presenting the HyperLex algorithm itself, we
briefly introduce small-world graphs.
2.1 Small world graphs
The small-world nature of a graph can be explained
in terms of its clustering coefficient and characteris-
tic path length. The clustering coefficient of a graph
shows the extent to which nodes tend to form con-
nected groups that have many edges connecting each
other in the group, and few edges leading out of
the group. On the other side, the characteristic path
length represents ?closeness? in a graph. See (Watts
and Strogatz, 1998) for further details on these char-
acteristics.
Randomly built graphs exhibit low clustering co-
efficients and are believed to represent something
very close to the minimal possible average path
length, at least in expectation. Perfectly ordered
graphs, on the other side, show high clustering coef-
ficients but also high average path length. According
to Watts and Strogatz (1998), small-world graphs lie
between these two extremes: they exhibit high clus-
tering coefficients, but short average path lengths.
Barabasi and Albert (1999) use the term ?scale-
free? to graphs whose degree probability follow a
power-law2. Specifically, scale free graphs follow
the property that the probability P (k) that a vertex
in the graph interacts with k other vertices decays as
a power-law, following P (k) ? k??. It turns out
that in this kind of graphs there exist nodes centrally
located and highly connected, called hubs.
2.2 The HyperLex algorithm for WSD
The HyperLex algorithm builds a cooccurrence
graph for all pairs of words cooccurring in the con-
text of the target word. Ve?ronis shows that this kind
of graph fulfills the properties of small world graphs,
and thus possess highly connected components in
the graph. The centers or prototypes of these com-
ponents, called hubs, eventually identify the main
word uses (senses) of the target word.
We will briefly introduce the algorithm here,
check (Ve?ronis, 2004) for further details. For each
word to be disambiguated, a text corpus is collected,
consisting of the paragraphs where the word occurs.
From this corpus, a cooccurrence graph for the tar-
get word is built. Nodes in the graph correspond to
the words3 in the text (except the target word itself).
Two words appearing in the same paragraph are said
to cooccur, and are connected with edges. Each edge
is assigned with a weight which measures the rela-
tive frequency of the two words cooccurring. Specif-
ically, let wij be the weight of the edge4 connecting
2Although scale-free graphs are not necessarily small
worlds, a lot of real world networks are both scale-free and
small worlds.
3Following Ve?ronis, we only work on nouns for the time
being.
4Note that the cooccurrence graph is undirected, i.e. wij =
wji
90
nodes i and j, then
wij = 1? max[P (i | j), P (j | i)]
P (i | j) =
freqij
freqj
and P (j | i) =
freqij
freqi
The weight of an edge measures how tightly con-
nected the two words are. Words which always oc-
cur together receive a weight of 0. Words rarely
cooccurring receive weights close to 1.
Once the cooccurrence graph is built, a simple it-
erative algorithm is executed to obtain its hubs. At
each step, the algorithm finds the vertex with high-
est relative frequency5 in the graph, and, if it meets
some criteria, it is selected as a hub. These criteria
are determined by a set of heuristic parameters, that
will be explained later in Section 4. After a vertex is
selected to be a hub, its neighbors are no longer eli-
gible as hub candidates. At any time, if the next ver-
tex candidate has a relative frequency below a cer-
tain threshold, the algorithm stops.
Once the hubs are selected, each of them is linked
to the target word with edges weighting 0, and the
Minimum Spanning Tree (MST) of the whole graph
is calculated and stored.
The MST is then used to perform word sense dis-
ambiguation, in the following way. For every in-
stance of the target word, the words surrounding it
are examined and confronted with the MST. By con-
struction of the MST, words in it are placed under
exactly one hub. Each word in the context receives
a set of scores s, with one score per hub, where all
scores are 0 except the one corresponding to the hub
where it is placed. If the scores are organized in a
score vector, all values are 0, except, say, the i-th
component, which receives a score d(hi, v), which
is the distance between the hub hi and the node rep-
resenting the word v. Thus, d(hi, v) assigns a score
of 1 to hubs and the score decreases as the nodes
move away from the hub in the tree.
For a given occurrence of the target word, the
score vectors of all the words in the context are
added, and the hub that receives the maximum score
is chosen.
5In cooccurrence graphs, the relative frequency of a vertex
and its degree are linearly related, and it is therefore possible to
avoid the costly computation of the degree.
Base
corpus
hyperLex_wsd hyperLex_wsd
hyperLex
Evaluator
Tagged
corpus
Test
corpus
Mapping
corpus
MST
matrix
Mapping
Figure 1: Design for the automatic mapping and evaluation
of HyperLex algorithm against a gold standard (test corpora).
3 Evaluating unsupervised WSD systems
All unsupervised WSD algorithms need some addi-
tion in order to be evaluated. One alternative, as in
(Ve?ronis, 2004), is to manually decide the correct-
ness of the hubs assigned to each occurrence of the
words. This approach has two main disadvantages.
First, it is expensive to manually verify each occur-
rence of the word, and different runs of the algo-
rithm need to be evaluated in turn. Second, it is not
an easy task to manually decide if an occurrence of
a word effectively corresponds with the use of the
word the assigned hub refers to, especially consid-
ering that the person is given a short list of words
linked to the hub. We also think that instead of judg-
ing whether the hub returned by the algorithm is cor-
rect, the person should have independently tagged
the occurrence with hubs, which should have been
then compared to the hub returned by the system.
A second alternative is to evaluate the system ac-
cording to some performance in an application, e.g.
information retrieval (Schu?tze, 1998). This is a very
attractive idea, but requires expensive system devel-
opment and it is sometimes difficult to separate the
reasons for the good (or bad) performance.
A third alternative would be to devise a method
to map the hubs (clusters) returned by the system
to the senses in a lexicon. Pantel and Lin (2002)
automatically map the senses to WordNet, and then
measure the quality of the mapping. More recently,
the mapping has been used to test the system on
publicly available benchmarks (Purandare and Ped-
91
Default p180 p1800 p6700
value Range Best Range Best Range Best
p1 5 2-3 2 1-3 2 1-3 1
p2 10 3-4 3 2-4 3 2-4 3
p3 0.9 0.7-0.9 0.7 0.5-0.7 0.5 0.3-0.7 0.4
p4 4 4 4 4 4 4 4
p5 6 6-7 6 3-7 3 1-7 1
p6 0.8 0.5-0.8 0.6 0.4-0.8 0.7 0.6-0.95 0.95
p7 0.001 0.0005-0.001 0.0009 0.0005-0.001 0.0009 0.0009-0.003 0.001
Table 1: Parameters of the HyperLex algorithm
ersen, 2004; Niu et al, 2005). See Section 6 for
more details on these systems.
Yet another possibility is to evaluate the induced
senses against a gold standard as a clustering task.
Induced senses are clusters, gold standard senses are
classes, and measures from the clustering literature
like entropy or purity can be used. As we wanted to
focus on the comparison against a standard data-set,
we decided to leave aside this otherwise interesting
option.
In this section we present a framework for au-
tomatically evaluating unsupervised WSD systems
against publicly available hand-tagged corpora. The
framework uses three data sets, called Base corpus,
Mapping corpus and Test corpus:
? The Base Corpus: a collection of examples of
the target word. The corpus is not annotated.
? The Mapping Corpus: a collection of examples
of the target word, where each corpus has been
manually annotated with its sense.
? The Test Corpus: a separate collection, also an-
notated with senses.
The evaluation framework is depicted in Figure 1.
The first step is to execute the HyperLex algorithm
over the Base corpus in order to obtain the hubs of
a target word, and the generated MST is stored. As
stated before, the Base Corpus is not tagged, so the
building of the MST is completely unsupervised.
In a second step (left part in Figure 1), we assign a
hub score vector to each of the occurrences of target
word in the Mapping corpus, using the MST calcu-
lated in the previous step (following the WSD al-
gorithm in Section 2.2). Using the hand-annotated
sense information, we can compute a mapping ma-
trix M that relates hubs and senses in the following
way. Suppose there are m hubs and n senses for the
target word. Then, M = {mij} 1 ? i ? m, 1 ?
j ? n, and each mij = P (sj |hi), that is, mij is the
probability of a word having sense j given that it has
been assigned hub i. This probability can be com-
puted counting the times an occurrence with sense
sj has been assigned hub hi.
This mapping matrix will be used to transform
any hub score vector h? = (h1, . . . , hm) returned
by the WSD algorithm into a sense score vector
s? = (s1, . . . , sn). It suffices to multiply the score
vector by M , i.e., s? = h?M .
In the last step (right part in Figure 1), we apply
the WSD algorithm over the Test corpus, using again
the MST generated in the first step, and returning a
hub score vector for each occurrence of the target
word in the test corpus. We then run the Evaluator,
which uses the M mapping matrix in order to con-
vert the hub score vector into a sense score vector.
The Evaluator then compares the sense with high-
est weight in the sense score vector to the sense that
was manually assigned, and outputs the precision
figures.
Preliminary experiments showed that, similar to
other unsupervised systems, HyperLex performs
better if it sees the test examples when building the
graph. We therefore decided to include a copy of the
training and test corpora in the base corpus (discard-
ing all hand-tagged sense information, of course).
Given the high efficiency of the algorithm this poses
no practical problem (see efficiency figures in Sec-
tion 6).
4 Tuning the parameters
As stated before, the behavior of the HyperLex algo-
rithm is influenced by a set of heuristic parameters,
that affect the way the cooccurrence graph is built,
the number of induced hubs, and the way they are
extracted from the graph. There are 7 parameters in
total:
p1 Minimum frequency of edges (occurrences)
p2 Minimum frequency of vertices (words)
p3 Edges with weights above this value are removed
p4 Context containing fewer words are not processed
92
word train test MFS default p180 p1800 p6700
argument 221 111 51.4 51.4 51.4 51.4 51.4
arm 266 133 82.0 82.0 80.5 82.0 82.7
atmosphere 161 81 66.7 67.9 70.4 70.4 67.9
audience 200 100 67.0 69.0 71.0 74.0 77.0
bank 262 132 67.4 69.7 75.0 76.5 75.0
degree 256 128 60.9 60.9 60.9 62.5 63.3
difference 226 114 40.4 40.4 41.2 46.5 49.1
difficulty 46 23 17.4 30.4 30.4 39.1 26.1
disc 200 100 38.0 66.0 75.0 70.0 76.0
image 146 74 36.5 63.5 62.2 67.6 64.9
interest 185 93 41.9 49.5 41.9 47.3 51.6
judgment 62 32 28.1 28.1 28.1 53.1 50.0
organization 112 56 73.2 73.2 73.2 71.4 73.2
paper 232 117 25.6 42.7 39.3 47.9 53.8
party 230 116 62.1 67.2 64.7 65.5 67.2
performance 172 87 32.2 44.8 46.0 54.0 59.8
plan 166 84 82.1 81.0 79.8 81.0 83.3
shelter 196 98 44.9 45.9 49.0 48.0 54.1
sort 190 96 65.6 64.6 64.6 65.6 64.6
source 64 32 65.6 59.4 56.2 62.5 62.5
Average: 54.5 59.9 60.3 63.0 64.6
(Over S2LS) 51.9 56.2 57.5 58.7 60.0
Table 2: Precision figures for nouns over the test corpus (S3LS). The second and third columns show the number of occurrences
in the train and test splits. The MFS column corresponds to the most frequent sense. The rest of columns correspond to different
parameter settings: default for the default setting, p180 for the best combination over 180, etc.. The last rows show the micro-
average over the S3LS run, and we also add the results on the S2LS dataset (different sets of nouns) to confirm that the same trends
hold in both datasets.
p5 Minimum number of adjacent vertices a hub must have
p6 Max. mean weight of the adjacent vertices of a hub
p7 Minimum frequency of hubs
Table 1 lists the parameters of the HyperLex al-
gorithm, and the default values proposed for them in
the original work (second column).
Given that we have devised a method to efficiently
evaluate the performance of HyperLex, we are able
to tune the parameters against the gold standard. We
first set a range for each of the parameters, and eval-
uated the algorithm for each combination of the pa-
rameters on a collection of examples of different
words (Senseval 2 English lexical-sample, S2LS).
This ensures that the chosen parameter set is valid
for any noun, and is not overfitted to a small set of
nouns.6 The set of parameters that obtained the best
results in the S2LS run is then selected to be run
against the S3LS dataset.
We first devised ranges for parameters amounting
to 180 possible combinations (p180 column in Ta-
ble 2), and then extended the ranges to amount to
1800 and 6700 combinations (columns p1800 and
p6700).
6In fact, previous experiments showed that optimizing the
parameters for each word did not yield better results.
5 Experiment setting and results
To evaluate the HyperLex algorithm in a standard
benchmark, we applied it to the 20 nouns in S3LS.
We use the standard training-test split. Following
the design in Section 3, we used both the training
and test sets as the Base Corpus (ignoring the sense
tags, of course). The Mapping Corpus comprised
the training split only, and the Test corpus the test
split only. The parameter tuning was done in a simi-
lar fashion, but on the S2LS dataset.
In Table 2 we can see the number of examples
of each word in the different corpus and the results
of the algorithm. We indicate only precision, as the
coverage is 100% in all cases. The left column,
named MFS, shows the precision when always as-
signing the most frequent sense (relative to the train
split). This is the baseline of our algorithm as our
algorithm does see the tags in the mapping step (see
Section 6 for further comments on this issue).
The default column shows the results for the Hy-
perLex algorithm with the default parameters as set
by Ve?ronis, except for the minimum frequency of
the vertices (p2 in Table 1), which according to some
preliminary experiments we set to 3. As we can see,
the algorithm with the default settings outperforms
93
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0.62
 0.5  0.55  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1
Pr
ec
is
io
n
Similarity
Parameter space
Best fitting line
Figure 2: Dispersion plot of the parameter space for 6700
combinations. The horizontal axis shows the similarity of a pa-
rameter set w.r.t. the best parameter set using the cosine. The
vertical axis shows the precision in S2LS. The best fitting line
is also depicted.
the MFS baseline by 5.4 points average, and in al-
most all words (except plan, sort and source).
The results for the best of 180 combinations of the
parameters improve the default setting (0.4 overall),
Extending the parameter space to 1800 and 6700 im-
proves the precision up to 63.0 and 64.6, 10.1 over
the MFS (MFS only outperforms HyperLex in the
best setting for two words). The same trend can be
seen on the S2LS dataset, where the gain was more
modest (note that the parameters were optimized for
S2LS).
6 Discussion and related work
We first comment the results, doing some analysis,
and then compare our results to those of Ve?ronis. Fi-
nally we overview some relevant work and review
the results of unsupervised systems on the S3LS
benchmark.
6.1 Comments on the results
The results show clearly that our exploration of the
parameter space was successful, with the widest pa-
rameter space showing the best results.
In order to analyze whether the search in the pa-
rameter space was making any sense, we drew a dis-
persion plot (see Figure 2). In the top right-hand cor-
ner we have the point corresponding to the best per-
forming parameter set. If the parameters were not
conditioning the good results, then we would have
expected a random cloud of points. On the contrary,
we can see that there is a clear tendency for those
default p180 p1800 p6700
hubs defined 9.2 ?3.8 15.3 ?5.7 38.6 ?11.8 77.7?18.7
used 8.4 ?3.5 14.4 ?5.3 30.4 ?9.3 45.2?13.3
senses defined 5.4 ?1.5 5.4 ?1.5 5.4 ?1.5 5.4 ?1.5
used 2.6 ?1.2 2.5 ?1 3.1 ?1.1 3.2?1.2
senses in test 5.1 ?1.3 - - -
Table 3: Average number of hubs and senses (along with the
standard deviation) for three parameter settings. Defined means
the number of hubs induced, and used means the ones actually
returned by HyperLex when disambiguating the test set. The
same applies for senses, that is, defined means total number of
senses (equal for all columns), and used means the senses that
were actually used by HyperLex in the test set. The last row
shows the actual number of senses used by the hand-annotators
in the test set.
parameter sets most similar to the best one to obtain
better results, and in fact the best fitting line shows a
clearly ascending slope.
Regarding efficiency, our implementation of Hy-
perLex is extremely fast. Doing the 1800 combina-
tions takes 2 hours in a 2 AMD Opteron processors
at 2GHz and 3Gb RAM. A single run (building the
MST, mapping and tagging the test sentences) takes
only 16 sec. For this reason, even if an on-line ver-
sion would be in principle desirable, we think that
this batch version is readily usable.
6.2 Comparison to (Ve?ronis, 2004)
Compared to Ve?ronis we are inducing larger num-
bers of hubs (with different parameters), using less
examples to build the graphs and obtaining more
modest results (far from the 90?s). Regarding the lat-
ter, our results are in the range of other S3LS WSD
systems (see below), and the discrepancy can be ex-
plained by the way Ve?ronis performed his evaluation
(see Section 3).
Table 3 shows the average number of hubs for
the four parameter settings. The average number
of hubs for the default setting is larger than that of
Ve?ronis (which ranges between 4 and 9 per word),
but quite close to the average number of senses. The
exploration of the parameter space prefers parame-
ter settings with even larger number of hubs, and the
figures shows that most of them are actually used
for disambiguation. The table also shows that, after
the mapping, less than half of the senses are actu-
ally used, which seems to indicate that the mapping
tends to favor the most frequent senses.
Regarding the actual values of the parameters
used (c.f. Table 1), we had to reduce the value
94
of some parameters (e.g. the minimum frequency
of vertices) due to the smaller number of of exam-
ples (Ve?ronis used from 1900 to 8700 examples per
word). In theory, we could explore larger parame-
ter spaces, but Table 1 shoes that the best setting for
the 6700 combinations has no parameter in a range
boundary (except p5, which cannot be further re-
duced).
All in all, the best results are attained with smaller
and more numerous hubs, a kind of micro-senses.
A possible explanation for this discrepancy with
Ve?ronis could be that he was inspecting by hand
the hubs that he got, and perhaps was biased by the
fact that he wanted the hubs to look more like stan-
dard senses. At first we were uncomfortable with
this behavior, so we checked whether HyperLex was
degenerating into a trivial solution. We simulated
a clustering algorithm returning one hub per exam-
ple, and its precision was 40.1, well below the MFS
baseline. We also realized that our results are in
accordance with some theories of word meaning,
e.g. the ?indefinitely large set of prototypes-within-
prototypes? envisioned in (Cruse, 2000). We now
think that the idea of having many micro-senses is
very attractive for further exploration, especially if
we are able to organize them into coarser hubs.
6.3 Comparison to related work
Table 4 shows the performance of different systems
on the nouns of the S3LS benchmark. When not re-
ported separately, we obtained the results for nouns
running the official scorer program on the filtered
results, as available in the S3LS web page. The sec-
ond column shows the type of system (supervised,
unsupervised).
We include three supervised systems, the winner
of S3LS (Mihalcea et al, 2004), an in-house system
(kNN-all, CITATION OMITTED) which uses opti-
mized kNN, and the same in-house system restricted
to bag-of-words features only (kNN-bow), i.e. dis-
carding other local features like bigrams or trigrams
(which is what most unsupervised systems do). The
table shows that we are one point from the bag-of-
words classifier kNN-bow, which is an impressive
result if we take into account the information loss of
the mapping step and that we tuned our parameters
on a different set of words. The full kNN system is
state-of-the-art, only 4 points below the S3LS win-
System Type Prec. Cov.
S3LS-best Sup. 74.9 0.99
kNN-all Sup. 70.3 1.0
kNN-bow Sup. 65.7 1.0
HyperLex Unsup(S3LS) 64.6 1.0
Cymfony Unsup(10%-S3LS) 57.9 1.0
Prob0 Unsup. (MFS-S3) 55.0 0.98
MFS - 51.5 1.0
Ciaosenso Unsup (MFS-Sc) 53.95 0.90
clr04 Unsup (MFS-Sc) 48.86 1.0
duluth-senserelate Unsup 47.48 1.0
(Purandare and
Pedersen, 2004)
Unsup (S2LS) - -
Table 4: Comparison of HyperLex and MFS baseline to S3LS
systems for nouns. The last system was evaluated on S2LS.
ner.
Table 4 also shows several unsupervised systems,
all of which except Cymfony and (Purandare and
Pedersen, 2004) participated in S3LS (check (Mi-
halcea et al, 2004) for further details on the sys-
tems). We classify them according to the amount of
?supervision? they have: some have have access to
most-frequent information (MFS-S3 if counted over
S3LS, MFS-Sc if counted over SemCor), some use
10% of the S3LS training part for mapping (10%-
S3LS), and some use the full amount of S3LS train-
ing for mapping (S3LS). Only one system (Duluth)
did not use in any way hand-tagged corpora.
Given the different typology of unsupervised sys-
tems, it?s unfair to draw definitive conclusions from
a raw comparison of results. The system coming
closer to ours is that described in (Niu et al, 2005).
They use hand tagged corpora which does not need
to include the target word to tune the parameters of
a rather complex clustering method which does use
local information (an exception to the rule of unsu-
pervised systems). They do use the S3LS training
corpus for mapping. For every sense the target word,
three of its contexts in the train corpus are gathered
(around 10% of the training data) and tagged. Each
cluster is then related with its most frequent sense.
Only one cluster may be related to a specific sense,
so if two or more clusters map to the same sense,
only the largest of them is retained. The mapping
method is similar to ours, but we use all the avail-
able training data and allow for different hubs to be
assigned to the same sense.
Another system similar to ours is (Purandare and
Pedersen, 2004), which unfortunately was evaluated
on Senseval 2 data. The authors use first and second
95
order bag-of-word context features to represent each
instance of the corpus. They apply several clustering
algorithms based on the vector space model, limiting
the number of clusters to 7. They also use all avail-
able training data for mapping, but given their small
number of clusters they opt for a one-to-one map-
ping which maximizes the assignment and discards
the less frequent clusters. They also discard some
difficult cases, like senses and words with low fre-
quencies (10% of total occurrences and 90, respec-
tively). The different test set and mapping system
make the comparison difficult, but the fact that the
best of their combinations beats MFS by 1 point on
average (47.6% vs. 46.4%) for the selected nouns
and senses make us think that our results are more
robust (nearly 10% over MFS).
7 Conclusions and further work
This paper has explored two sides of HyperLex: the
optimization of the free parameters, and the empir-
ical comparison on a standard benchmark against
other WSD systems. We use hand-tagged corpora
to map the induced senses to WordNet senses.
Regarding the optimization of parameters, we
used a another testbed (S2LS) comprising different
words to select the best parameter. We consistently
improve the results of the parameters by Ve?ronis,
which is not perhaps so surprising, but the method
allows to fine-tune the parameters automatically to a
given corpus given a small test set.
Comparing unsupervised systems against super-
vised systems is seldom done. Our results indicate
that HyperLex with the supervised mapping is on
par with a state-of-the-art system which uses bag-
of-words features only. Given the information loss
inherent to any mapping, this is an impressive re-
sult. The comparison to other unsupervised systems
is difficult, as each one uses a different mapping
strategy and a different amount of supervision.
For the future, we would like to look more closely
the micro-senses induced by HyperLex, and see if
we can group them into coarser clusters. We also
plan to apply the parameters to the Senseval 3 all-
words task, which seems well fit for HyperLex: the
best supervised system only outperforms MFS by
a few points in this setting, and the training cor-
pora used (Semcor) is not related to the test corpora
(mainly Wall Street Journal texts).
Graph models have been very successful in some
settings (e.g. the PageRank algorithm of Google),
and have been rediscovered recently for natural lan-
guage tasks like knowledge-based WSD, textual en-
tailment, summarization and dependency parsing.
We would like to test other such algorithms in the
same conditions, and explore their potential to inte-
grate different kinds of information, especially the
local or syntactic features so successfully used by
supervised systems, but also more heterogeneous in-
formation from knowledge bases.
References
A. L. Barabasi and R. Albert. 1999. Emergence of scal-
ing in random networks. Science, 286(5439):509?512,
October.
D. A. Cruse, 2000. Polysemy: Theoretical and Com-
putational Approaches, chapter Aspects of the Micro-
structure of Word Meanings. OUP.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
senseval-3 english lexical sample task. In R. Mihal-
cea and P. Edmonds, editors, Senseval-3 proceedings,
pages 25?28. ACL, July.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A semantic concordance. In Proc. of the ARPA HLT
workshop.
C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word
independent context pair classification model for word
sense disambiguation. In Proc. of CoNLL-2005.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD02.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Proc. of CoNLL-2004.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proc. of SENSEVAL.
J. Ve?ronis. 2004. HyperLex: lexical cartography for in-
formation retrieval. Computer Speech & Language,
18(3):223?252.
D. J. Watts and S. H. Strogatz. 1998. Collec-
tive dynamics of ?small-world? networks. Nature,
393(6684):440?442, June.
96
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 1?6,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 01: Evaluating WSD
on Cross-Language Information Retrieval
Eneko Agirre
IXA NLP group
University of the Basque Country
Donostia, Basque Counntry
e.agirre@ehu.es
Oier Lopez de Lacalle
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibloleo@ehu.es
German Rigau
IXA NLP group
University of the Basque Country
Donostia, Basque Country
german.rigau@ehu.es
Bernardo Magnini
ITC-IRST
Trento, Italy
magnini@itc.it
Arantxa Otegi
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibotusa@ehu.es
Piek Vossen
Irion Technologies
Delftechpark 26
2628XH Delft, Netherlands
Piek.Vossen@irion.nl
Abstract
This paper presents a first attempt of an
application-driven evaluation exercise of
WSD. We used a CLIR testbed from the
Cross Lingual Evaluation Forum. The ex-
pansion, indexing and retrieval strategies
where fixed by the organizers. The par-
ticipants had to return both the topics and
documents tagged with WordNet 1.6 word
senses. The organization provided training
data in the form of a pre-processed Semcor
which could be readily used by participants.
The task had two participants, and the orga-
nizer also provide an in-house WSD system
for comparison.
1 Introduction
Since the start of Senseval, the evaluation of Word
Sense Disambiguation (WSD) as a separate task is a
mature field, with both lexical-sample and all-words
tasks. In the first case the participants need to tag the
occurrences of a few words, for which hand-tagged
data has already been provided. In the all-words task
all the occurrences of open-class words occurring in
two or three documents (a few thousand words) need
to be disambiguated.
The community has long mentioned the neces-
sity of evaluating WSD in an application, in order
to check which WSD strategy is best, and more im-
portant, to try to show that WSD can make a differ-
ence in applications. The use of WSD in Machine
Translation has been the subject of some recent pa-
pers, but less attention has been paid to Information
Retrieval (IR).
With this proposal we want to make a first try to
define a task where WSD is evaluated with respect
to an Information Retrieval and Cross-Lingual Infor-
mation Retrieval (CLIR) exercise. From the WSD
perspective, this task will evaluate all-words WSD
systems indirectly on a real task. From the CLIR
perspective, this task will evaluate which WSD sys-
tems and strategies work best.
We are conscious that the number of possible con-
figurations for such an exercise is very large (in-
cluding sense inventory choice, using word sense in-
duction instead of disambiguation, query expansion,
WSD strategies, IR strategies, etc.), so this first edi-
tion focuses on the following:
? The IR/CLIR system is fixed.
? The expansion / translation strategy is fixed.
? The participants can choose the best WSD
strategy.
1
? The IR system is used as the upperbound for
the CLIR systems.
We think that it is important to start doing this
kind of application-driven evaluations, which might
shed light to the intricacies in the interaction be-
tween WSD and IR strategies. We see this as the
first of a series of exercises, and one outcome of this
task should be that both WSD and CLIR communi-
ties discuss together future evaluation possibilities.
This task has been organized in collabora-
tion with the Cross-Language Evaluation Forum
(CLEF1). The results will be analyzed in the CLEF-
2007 workshop, and a special track will be pro-
posed for CLEF-2008, where CLIR systems will
have the opportunity to use the annotated data
produced as a result of the Semeval-2007 task.
The task has a webpage with all the details at
http://ixa2.si.ehu.es/semeval-clir.
This paper is organized as follows. Section 2
describes the task with all the details regarding
datasets, expansion/translation, the IR/CLIR system
used, and steps for participation. Section 3 presents
the evaluation performed and the results obtained by
the participants. Finally, Section 4 draws the con-
clusions and mention the future work.
2 Description of the task
This is an application-driven task, where the appli-
cation is a fixed CLIR system. Participants disam-
biguate text by assigning WordNet 1.6 synsets and
the system will do the expansion to other languages,
index the expanded documents and run the retrieval
for all the languages in batch. The retrieval results
are taken as the measure for fitness of the disam-
biguation. The modules and rules for the expansion
and the retrieval will be exactly the same for all par-
ticipants.
We proposed two specific subtasks:
1. Participants disambiguate the corpus, the cor-
pus is expanded to synonyms/translations and
we measure the effects on IR/CLIR. Topics2 are
not processed.
1http://www.clef-campaign.org
2In IR topics are the short texts which are used by the sys-
tems to produce the queries. They usually provide extensive
information about the text to be searched, which can be used
both by the search engine and the human evaluators.
2. Participants disambiguate the topics per lan-
guage, we expand the queries to syn-
onyms/translations and we measure the effects
on IR/CLIR. Documents are not processed
The corpora and topics were obtained from the
ad-hoc CLEF tasks. The supported languages in the
topics are English and Spanish, but in order to limit
the scope of the exercise we decided to only use En-
glish documents. The participants only had to dis-
ambiguate the English topics and documents. Note
that most WSD systems only run on English text.
Due to these limitations, we had the following
evaluation settings:
IR with WSD of topics , where the participants
disambiguate the documents, the disam-
biguated documents are expanded to syn-
onyms, and the original topics are used for
querying. All documents and topics are in En-
glish.
IR with WSD of documents , where the partic-
ipants disambiguate the topics, the disam-
biguated topics are expanded and used for
querying the original documents. All docu-
ments and topics are in English.
CLIR with WSD of documents , where the partic-
ipants disambiguate the documents, the dis-
ambiguated documents are translated, and the
original topics in Spanish are used for query-
ing. The documents are in English and the top-
ics are in Spanish.
We decided to focus on CLIR for evaluation,
given the difficulty of improving IR. The IR results
are given as illustration, and as an upperbound of
the CLIR task. This use of IR results as a reference
for CLIR systems is customary in the CLIR commu-
nity (Harman, 2005).
2.1 Datasets
The English CLEF data from years 2000-2005 com-
prises corpora from ?Los Angeles Times? (year
1994) and ?Glasgow Herald? (year 1995) amounting
to 169,477 documents (579 MB of raw text, 4.8GB
in the XML format provided to participants, see Sec-
tion 2.3) and 300 topics in English and Spanish (the
topics are human translations of each other). The
relevance judgments were taken from CLEF. This
2
might have the disadvantage of having been pro-
duced by pooling the results of CLEF participants,
and might bias the results towards systems not using
WSD, specially for monolingual English retrieval.
We are considering the realization of a post-hoc
analysis of the participants results in order to ana-
lyze the effect on the lack of pooling.
Due to the size of the document collection, we de-
cided that the limited time available in the competi-
tion was too short to disambiguate the whole collec-
tion. We thus chose to take a sixth part of the corpus
at random, comprising 29,375 documents (874MB
in the XML format distributed to participants). Not
all topics had relevant documents in this 17% sam-
ple, and therefore only 201 topics were effectively
used for evaluation. All in all, we reused 21,797
relevance judgements that contained one of the doc-
uments in the 17% sample, from which 923 are pos-
itive3. For the future we would like to use the whole
collection.
2.2 Expansion and translation
For expansion and translation we used the publicly
available Multilingual Central Repository (MCR)
from the MEANING project (Atserias et al, 2004).
The MCR follows the EuroWordNet design, and
currently includes English, Spanish, Italian, Basque
and Catalan wordnets tightly connected through the
Interlingual Index (based on WordNet 1.6, but linked
to all other WordNet versions).
We only expanded (translated) the senses returned
by the WSD systems. That is, given a word like
?car?, it will be expanded to ?automobile? or ?railcar?
(and translated to ?auto? or ?vago?n? respectively) de-
pending on the sense in WN 1.6. If the systems re-
turns more than one sense, we choose the sense with
maximum weight. In case of ties, we expand (trans-
late) all. The participants could thus implicitly affect
the expansion results, for instance, when no sense
could be selected for a target noun, the participants
could either return nothing (or NOSENSE, which
would be equivalent), or all senses with 0 score. In
the first case no expansion would be performed, in
the second all senses would be expanded, which is
equivalent to full expansion. This fact will be men-
tioned again in Section 3.5.
3The overall figures are 125,556 relevance judgements for
the 300 topics, from which 5700 are positive
Note that in all cases we never delete any of the
words in the original text.
In addition to the expansion strategy used with the
participants, we tested other expansion strategies as
baselines:
noexp no expansion, original text
fullexp expansion (translation in the case of English
to Spanish expansion) to all synonyms of all
senses
wsd50 expansion to the best 50% senses as returned
by the WSD system. This expansion was tried
over the in-house WSD system of the organizer
only.
2.3 IR/CLIR system
The retrieval engine is an adaptation of the Twenty-
One search system (Hiemstra and Kraaij, 1998) that
was developed during the 90?s by the TNO research
institute at Delft (The Netherlands) getting good re-
sults on IR and CLIR exercises in TREC (Harman,
2005). It is now further developed by Irion technolo-
gies as a cross-lingual retrieval system (Vossen et al,
). For indexing, the TwentyOne system takes Noun
Phrases as an input. Noun Phases (NPs) are detected
using a chunker and a word form with POS lexicon.
Phrases outside the NPs are not indexed, as well as
non-content words (determiners, prepositions, etc.)
within the phrase.
The Irion TwentyOne system uses a two-stage re-
trieval process where relevant documents are first
extracted using a vector space matching and sec-
ondly phrases are matched with specific queries.
Likewise, the system is optimized for high-precision
phrase retrieval with short queries (1 up 5 words
with a phrasal structure as well). The system can be
stripped down to a basic vector space retrieval sys-
tem with an tf.idf metrics that returns documents for
topics up to a length of 30 words. The stripped-down
version was used for this task to make the retrieval
results compatible with the TREC/CLEF system.
The Irion system was also used for pre-
processing. The CLEF corpus and topics were con-
verted to the TwentyOne XML format, normalized,
and named-entities and phrasal structured detected.
Each of the target tokens was identified by an unique
identifier.
2.4 Participation
The participants were provided with the following:
3
1. the document collection in Irion XML format
2. the topics in Irion XML format
In addition, the organizers also provided some of
the widely used WSD features in a word-to-word
fashion4 (Agirre et al, 2006) in order to make partic-
ipation easier. These features were available for both
topics and documents as well as for all the words
with frequency above 10 in SemCor 1.6 (which can
be taken as the training data for supervised WSD
systems). The Semcor data is publicly available 5.
For the rest of the data, participants had to sign and
end user agreement.
The participants had to return the input files en-
riched with WordNet 1.6 sense tags in the required
XML format:
1. for all the documents in the collection
2. for all the topics
Scripts to produce the desired output from word-
to-word files and the input files were provided by
organizers, as well as DTD?s and software to check
that the results were conformant to the respective
DTD?s.
3 Evaluation and results
For each of the settings presented in Section 2 we
present the results of the participants, as well as
those of an in-house system presented by the orga-
nizers. Please refer to the system description papers
for a more complete description. We also provide
some baselines and alternative expansion (transla-
tion) strategies. All systems are evaluated accord-
ing to their Mean Average Precision 6 (MAP) as
computed by the trec eval software on the pre-
existing CLEF relevance-assessments.
3.1 Participants
The two systems that registered sent the results on
time.
PUTOP They extend on McCarthy?s predominant
sense method to create an unsupervised method
of word sense disambiguation that uses auto-
matically derived topics using Latent Dirichlet
4Each target word gets a file with all the occurrences, and
each occurrence gets the occurrence identifier, the sense tag (if
in training), and the list of features that apply to the occurrence.
5http://ixa2.si.ehu.es/semeval-clir/
6http://en.wikipedia.org/wiki/
Information retrieval
Allocation. Using topic-specific synset similar-
ity measures, they create predictions for each
word in each document using only word fre-
quency information. The disambiguation pro-
cess took aprox. 12 hours on a cluster of 48 ma-
chines (dual Xeons with 4GB of RAM). Note
that contrary to the specifications, this team
returned WordNet 2.1 senses, so we had to
map automatically to 1.6 senses (Daude et al,
2000).
UNIBA This team uses a a knowledge-based WSD
system that attempts to disambiguate all words
in a text by exploiting WordNet relations. The
main assumption is that a specific strategy for
each Part-Of-Speech (POS) is better than a sin-
gle strategy. Nouns are disambiguated basi-
cally using hypernymy links. Verbs are dis-
ambiguated according to the nouns surrounding
them, and adjectives and adverbs use glosses.
ORGANIZERS In addition to the regular partic-
ipants, and out of the competition, the orga-
nizers run a regular supervised WSD system
trained on Semcor. The system is based on
a single k-NN classifier using the features de-
scribed in (Agirre et al, 2006) and made avail-
able at the task website (cf. Section 2.4).
In addition to those we also present some com-
mon IR/CLIR baselines, baseline WSD systems, and
an alternative expansion:
noexp a non-expansion IR/CLIR baseline of the
documents or topics.
fullexp a full-expansion IR/CLIR baseline of the
documents or topics.
wsdrand a WSD baseline system which chooses a
sense at random. The usual expansion is ap-
plied.
1st a WSD baseline system which returns the sense
numbered as 1 in WordNet. The usual expan-
sion is applied.
wsd50 the organizer?s WSD system, where the 50%
senses of the word ranking according to the
WSD system are expanded. That is, instead of
expanding the single best sense, it expands the
best 50% senses.
3.2 IR Results
This section present the results obtained by the par-
ticipants and baselines in the two IR settings. The
4
IRtops IRdocs CLIR
no expansion 0.3599 0.3599 0.1446
full expansion 0.1610 0.1410 0.2676
UNIBA 0.3030 0.1521 0.1373
PUTOP 0.3036 0.1482 0.1734
wsdrand 0.2673 0.1482 0.2617
1st sense 0.2862 0.1172 0.2637
ORGANIZERS 0.2886 0.1587 0.2664
wsd50 0.2651 0.1479 0.2640
Table 1: Retrieval results given as MAP. IRtops
stands for English IR with topic expansion. IR-
docs stands for English IR with document expan-
sion. CLIR stands for CLIR results for translated
documents.
second and third columns of Table 1 present the re-
sults when disambiguating the topics and the docu-
ments respectively. Non of the expansion techniques
improves over the baseline (no expansion).
Note that due to the limitation of the search en-
gine, long queries were truncated at 50 words, which
might explain the very low results of the full expan-
sion.
3.3 CLIR results
The last column of Table 1 shows the CLIR results
when expanding (translating) the disambiguated
documents. None of the WSD systems attains the
performance of full expansion, which would be the
baseline CLIR system, but the WSD of the organizer
gets close.
3.4 WSD results
In addition to the IR and CLIR results we also pro-
vide the WSD performance of the participants on
the Senseval 2 and 3 all-words task. The documents
from those tasks were included alongside the CLEF
documents, in the same formats, so they are treated
as any other document. In order to evaluate, we had
to map automatically all WSD results to the respec-
tive WordNet version (using the mappings in (Daude
et al, 2000) which are publicly available).
The results are presented in Table 2, where we can
see that the best results are attained by the organizers
WSD system.
3.5 Discussion
First of all, we would like to mention that the WSD
and expansion strategy, which is very simplistic, de-
grades the IR performance. This was rather ex-
Senseval-2 all words
precision recall coverage
ORGANIZERS 0.584 0.577 93.61%
UNIBA 0.498 0.375 75.39%
PUTOP 0.388 0.240 61.92%
Senseval-3 all words
precision recall coverage
ORGANIZERS 0.591 0.566 95.76%
UNIBA 0.484 0.338 69.98%
PUTOP 0.334 0.186 55.68%
Table 2: English WSD results in the Senseval-2 and
Senseval-3 all-words datasets.
pected, as the IR experiments had an illustration
goal, and are used for comparison with the CLIR
experiments. In monolingual IR, expanding the top-
ics is much less harmful than expanding the docu-
ments. Unfortunately the limitation to 50 words in
the queries might have limited the expansion of the
topics, which make the results rather unreliable. We
plan to fix this for future evaluations.
Regarding CLIR results, even if none of the WSD
systems were able to beat the full-expansion base-
line, the organizers system was very close, which is
quite encouraging due to the very simplistic expan-
sion, indexing and retrieval strategies used.
In order to better interpret the results, Table 3
shows the amount of words after the expansion in
each case. This data is very important in order to un-
derstand the behavior of each of the systems. Note
that UNIBA returns 3 synsets at most, and therefore
the wsd50 strategy (select the 50% senses with best
score) leaves a single synset, which is the same as
taking the single best system (wsdbest). Regarding
PUTOP, this system returned a single synset, and
therefore the wsd50 figures are the same as the ws-
dbest figures.
Comparing the amount of words for the two par-
ticipant systems, we see that UNIBA has the least
words, closely followed by PUTOP. The organizers
WSD system gets far more expanded words. The
explanation is that when the synsets returned by a
WSD system all have 0 weights, the wsdbest expan-
sion strategy expands them all. This was not explicit
in the rules for participation, and might have affected
the results.
A cross analysis of the result tables and the num-
ber of words is interesting. For instance, in the IR
exercise, when we expand documents, the results in
5
English Spanish
No WSD noexp 9,900,818 9,900,818fullexp 93,551,450 58,491,767
UNIBA
wsdbest 19,436,374 17,226,104
wsd50 19,436,374 17,226,104
PUTOP wsdbest 20,101,627 16,591,485wsd50 20,101,627 16,591,485
Baseline 1st 24,842,800 20,261,081
WSD wsdrand 24,904,717 19,137,981
ORG. wsdbest 26,403,913 21,086,649wsd50 36,128,121 27,528,723
Table 3: Number of words in the document col-
lection after expansion for the WSD system and all
baselines. wsdbest stands for the expansion strategy
used with participants.
the third column of Table 1 show that the ranking for
the non-informed baselines is the following: best for
no expansion, second for random WSD, and third
for full expansion. These results can be explained
because of the amount of expansion: the more ex-
pansion the worst results. When more informed
WSD is performed, documents with more expansion
can get better results, and in fact the WSD system of
the organizers is the second best result from all sys-
tem and baselines, and has more words than the rest
(with exception of wsd50 and full expansion). Still,
the no expansion baseline is far from the WSD re-
sults.
Regarding the CLIR result, the situation is in-
verted, with the best results for the most productive
expansions (full expansion, random WSD and no ex-
pansion, in this order). For the more informed WSD
methods, the best results are again for the organizers
WSD system, which is very close to the full expan-
sion baseline. Even if wsd50 has more expanded
words wsdbest is more effective. Note the very high
results attained by random. These high results can
be explained by the fact that many senses get the
same translation, and thus for many words with few
translation, the random translation might be valid.
Still the wsdbest, 1st sense and wsd50 results get
better results.
4 Conclusions and future work
This paper presents the results of a preliminary at-
tempt of an application-driven evaluation exercise
of WSD in CLIR. The expansion, indexing and re-
trieval strategies proved too simplistic, and none of
the two participant systems and the organizers sys-
tem were able to beat the full-expansion baseline.
Due to efficiency reasons, the IRION system had
some of its features turned off. Still the results are
encouraging, as the organizers system was able to
get very close to the full expansion strategy with
much less expansion (translation).
For the future, a special track of CLEF-2008 will
leave the avenue open for more sophisticated CLIR
techniques. We plan to extend the WSD annotation
to all words in the CLEF English document collec-
tion, and we also plan to contact the best performing
systems of the SemEval all-words tasks to have bet-
ter quality annotations.
Acknowledgements
We wish to thank CLEF for allowing us to use their data, and the
CLEF coordinator, Carol Peters, for her help and collaboration.
This work has been partially funded by the Spanish education
ministry (project KNOW)
References
E. Agirre, O. Lopez de Lacalle, and D. Martinez. 2006.
Exploring feature set combinations for WSD. In Proc.
of the SEPLN.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The MEANING
Multilingual Central Repository. In Proceedings of the
2.nd Global WordNet Conference, GWC 2004, pages
23?30. Masaryk University, Brno, Czech Republic.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets Using Structural Information. In Proc. of ACL,
Hong Kong.
D. Harman. 2005. Beyond English. In E. M. Voorhees
and D. Harman, editors, TREC: Experiment and Eval-
uation in Information Retrieval, pages 153?181. MIT
press.
D. Hiemstra and W. Kraaij. 1998. Twenty-One in ad-hoc
and CLIR. In E.M. Voorhees and D. K. Harman, ed-
itors, Proc. of TREC-7, pages 500?540. NIST Special
Publication.
P. Vossen, G. Rigau, I. Alegria, E. Agirre, D. Farwell,
and M. Fuentes. Meaningful results for Information
Retrieval in the MEANING project. In Proc. of the
3rd Global Wordnet Conference.
6
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 7?12,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semeval-2007 Task 02:
Evaluating Word Sense Induction and Discrimination Systems
Eneko Agirre
IXA NLP Group
Univ. of the Basque Country
Donostia, Basque Country
e.agirre@ehu.es
Aitor Soroa
IXA NLP Group
Univ. of the Basque Country
Donostia, Basque Country
a.soroa@ehu.es
Abstract
The goal of this task is to allow for com-
parison across sense-induction and discrim-
ination systems, and also to compare these
systems to other supervised and knowledge-
based systems. In total there were 6 partic-
ipating systems. We reused the SemEval-
2007 English lexical sample subtask of task
17, and set up both clustering-style unsuper-
vised evaluation (using OntoNotes senses as
gold-standard) and a supervised evaluation
(using the part of the dataset for mapping).
We provide a comparison to the results of
the systems participating in the lexical sam-
ple subtask of task 17.
1 Introduction
Word Sense Disambiguation (WSD) is a key
enabling-technology. Supervised WSD techniques
are the best performing in public evaluations, but
need large amounts of hand-tagging data. Exist-
ing hand-annotated corpora like SemCor (Miller
et al, 1993), which is annotated with WordNet
senses (Fellbaum, 1998) allow for a small improve-
ment over the simple most frequent sense heuristic,
as attested in the all-words track of the last Sense-
val competition (Snyder and Palmer, 2004). In the-
ory, larger amounts of training data (SemCor has
approx. 500M words) would improve the perfor-
mance of supervised WSD, but no current project
exists to provide such an expensive resource. An-
other problem of the supervised approach is that the
inventory and distribution of senses changes dra-
matically from one domain to the other, requiring
additional hand-tagging of corpora (Mart??nez and
Agirre, 2000; Koeling et al, 2005).
Supervised WSD is based on the ?fixed-list of
senses? paradigm, where the senses for a target word
are a closed list coming from a dictionary or lex-
icon. Lexicographers and semanticists have long
warned about the problems of such an approach,
where senses are listed separately as discrete enti-
ties, and have argued in favor of more complex rep-
resentations, where, for instance, senses are dense
regions in a continuum (Cruse, 2000).
Unsupervised Word Sense Induction and Dis-
crimination (WSID, also known as corpus-based un-
supervised systems) has followed this line of think-
ing, and tries to induce word senses directly from
the corpus. Typical WSID systems involve cluster-
ing techniques, which group together similar exam-
ples. Given a set of induced clusters (which repre-
sent word uses or senses1), each new occurrence of
the target word will be compared to the clusters and
the most similar cluster will be selected as its sense.
One of the problems of unsupervised systems is
that of managing to do a fair evaluation. Most of cur-
rent unsupervised systems are evaluated in-house,
with a brief comparison to a re-implementation of a
former system, leading to a proliferation of unsuper-
vised systems with little ground to compare among
them. The goal of this task is to allow for compar-
ison across sense-induction and discrimination sys-
tems, and also to compare these systems to other su-
pervised and knowledge-based systems.
The paper is organized as follows. Section 2
presents the evaluation framework used in this task.
Section 3 presents the systems that participated in
1WSID approaches prefer the term ?word uses? to ?word
senses?. In this paper we use them interchangeably to refer to
both the induced clusters, and to the word senses from some
reference lexicon.
7
the task, and the official results. Finally, Section 5
draws the conclusions.
2 Evaluating WSID systems
All WSID algorithms need some addition in order
to be evaluated. One alternative is to manually de-
cide the correctness of the clusters assigned to each
occurrence of the words. This approach has two
main disadvantages. First, it is expensive to man-
ually verify each occurrence of the word, and dif-
ferent runs of the algorithm need to be evaluated
in turn. Second, it is not an easy task to manu-
ally decide if an occurrence of a word effectively
corresponds with the use of the word the assigned
cluster refers to, especially considering that the per-
son is given a short list of words linked to the clus-
ter. We also think that instead of judging whether
the cluster returned by the algorithm is correct, the
person should have independently tagged the occur-
rence with his own senses, which should have been
then compared to the cluster returned by the system.
This is paramount to compare a corpus which has
been hand-tagged with some reference senses (also
known as the gold-standard) with the clustering re-
sult. The gold standard tags are taken to be the def-
inition of the classes, and standard measures from
the clustering literature can be used to evaluate the
clusters against the classes.
A second alternative would be to devise a method
to map the clusters returned by the systems to the
senses in a lexicon. Pantel and Lin (2002) automat-
ically map the senses to WordNet, and then mea-
sure the quality of the mapping. More recently, the
mapping has been used to test the system on pub-
licly available benchmarks (Purandare and Peder-
sen, 2004; Niu et al, 2005).
A third alternative is to evaluate the systems ac-
cording to some performance in an application, e.g.
information retrieval (Schu?tze, 1998). This is a very
attractive idea, but requires expensive system devel-
opment and it is sometimes difficult to separate the
reasons for the good (or bad) performance.
In this task we decided to adopt the first two alter-
natives, since they allow for comparison over pub-
licly available systems of any kind. With this goal on
mind we gave all the participants an unlabeled cor-
pus, and asked them to induce the senses and create
a clustering solution on it. We evaluate the results
according to the following types of evaluation:
1. Evaluate the induced senses as clusters of ex-
amples. The induced clusters are compared to
the sets of examples tagged with the given gold
standard word senses (classes), and evaluated
using the FScore measure for clusters. We will
call this evaluation unsupervised.
2. Map the induced senses to gold standard
senses, and use the mapping to tag the test cor-
pus with gold standard tags. The mapping is
automatically produced by the organizers, and
the resulting results evaluated according to the
usual precision and recall measures for super-
vised word sense disambiguation systems. We
call this evaluation supervised.
We will see each of them in turn.
2.1 Unsupervised evaluation
In this setting the results of the systems are treated
as clusters of examples and gold standard senses are
classes. In order to compare the clusters with the
classes, hand annotated corpora is needed. The test
set is first tagged with the induced senses. A per-
fect clustering solution will be the one where each
cluster has exactly the same examples as one of the
classes, and vice versa.
Following standard cluster evaluation prac-
tice (Zhao and Karypis, 2005), we consider the FS-
core measure for measuring the performance of the
systems. The FScore is used in a similar fashion
to Information Retrieval exercises, with precision
and recall defined as the percentage of correctly ?re-
trieved? examples for a cluster (divided by total clus-
ter size), and recall as the percentage of correctly
?retrieved? examples for a cluster (divided by total
class size).
Given a particular class sr of size nr and a cluster
hi of size ni, suppose nir examples in the class sr
belong to hi. The F value of this class and cluster is
defined to be:
f(sr, hi) =
2P (sr, hi)R(sr, hi)
P (sr, hi) + R(sr, hi)
where P (sr, hi) =
nir
nr
is the precision value and
R(sr, hi) =
nir
ni
is the recall value defined for class
sr and cluster hi. The FScore of class sr is the max-
imum F value attained at any cluster, that is,
8
F (sr) = max
hi
f(sr, hi)
and the FScore of the entire clustering solution is:
FScore =
c?
r=1
nr
n
F (sr)
where q is the number of classes and n is the size
of the clustering solution. If the clustering is the
identical to the original classes in the datasets, FS-
core will be equal to one which means that the higher
the FScore, the better the clustering is.
For the sake of completeness we also include the
standard entropy and purity measures in the unsu-
pervised evaluation. The entropy measure consid-
ers how the various classes of objects are distributed
within each cluster. In general, the smaller the en-
tropy value, the better the clustering algorithm per-
forms. The purity measure considers the extent to
which each cluster contained objects from primarily
one class. The larger the values of purity, the bet-
ter the clustering algorithm performs. For a formal
definition refer to (Zhao and Karypis, 2005).
2.2 Supervised evaluation
We have followed the supervised evaluation frame-
work for evaluating WSID systems as described in
(Agirre et al, 2006). First, we split the corpus into
a train/test part. Using the hand-annotated sense in-
formation in the train part, we compute a mapping
matrix M that relates clusters and senses in the fol-
lowing way. Suppose there are m clusters and n
senses for the target word. Then, M = {mij} 1 ?
i ? m, 1 ? j ? n, and each mij = P (sj |hi), that
is, mij is the probability of a word having sense j
given that it has been assigned cluster i. This proba-
bility can be computed counting the times an occur-
rence with sense sj has been assigned cluster hi in
the train corpus.
The mapping matrix is used to transform any
cluster score vector h? = (h1, . . . , hm) returned by
the WSID algorithm into a sense score vector s? =
(s1, . . . , sn). It suffices to multiply the score vector
by M , i.e., s? = h?M .
We use the M mapping matrix in order to convert
the cluster score vector of each test corpus instance
into a sense score vector, and assign the sense with
All Nouns Verbs
train 22281 14746 9773
test 4851 2903 2427
all 27132 17649 12200
Table 1: Number of occurrences for the 100 target words in
the corpus following the train/test split.
maximum score to that instance. Finally, the result-
ing test corpus is evaluated according to the usual
precision and recall measures for supervised word
sense disambiguation systems.
3 Results
In this section we will introduce the gold standard
and corpus used, the description of the systems and
the results obtained. Finally we provide some mate-
rial for discussion.
Gold Standard
The data used for the actual evaluation was bor-
rowed from the SemEval-2007 ?English lexical
sample subtask? of task 17. The texts come from the
Wall Street Journal corpus, and were hand-annotated
with OntoNotes senses (Hovy et al, 2006). Note
that OntoNotes senses are coarser than WordNet
senses, and thus the number of senses to be induced
is smaller in this case.
Participants were provided with information
about 100 target words (65 verbs and 35 nouns),
each target word having a set of contexts where the
word appears. After removing the sense tags from
the train corpus, the train and test parts were joined
into the official corpus and given to the participants.
Participants had to tag with the induced senses all
the examples in this corpus. Table 1 summarizes the
size of the corpus.
Participant systems
In total there were 6 participant systems. One of
them (UoFL) was not a sense induction system, but
rather a knowledge-based WSD system. We include
their data in the results section below for coherence
with the official results submitted to participants, but
we will not mention it here.
I2R: This team used a cluster validation method
to estimate the number of senses of a target word in
untagged data, and then grouped the instances of this
target word into the estimated number of clusters us-
ing the sequential Information Bottleneck algorithm.
9
UBC-AS: A two stage graph-based clustering
where a co-occurrence graph is used to compute
similarities against contexts. The context similarity
matrix is pruned and the resulting associated graph
is clustered by means of a random-walk type al-
gorithm. The parameters of the system are tuned
against the Senseval-3 lexical sample dataset, and
some manual tuning is performed in order to reduce
the overall number of induced senses. Note that this
system was submitted by the organizers. The orga-
nizers took great care in order to participate under
the same conditions as the rest of participants.
UMND2: A system which clusters the second or-
der co-occurrence vectors associated with each word
in a context. Clustering is done using k-means and
the number of clusters was automatically discovered
using the Adapted Gap Statistic. No parameter tun-
ing is performed.
upv si: A self-term expansion method based on
co-ocurrence, where the terms of the corpus are ex-
panded by its best co-ocurrence terms in the same
corpus. The clustering is done using one implemen-
tation of the KStar method where the stop criterion
has been modified. The trial data was used for de-
termining the corpus structure. No further tuning is
performed.
UOY: A graph based system which creates a co-
occurrence hypergraph model. The hypergraph is
filtered and weighted according to some associa-
tion rules. The clustering is performed by selecting
the nodes of higher degree until a stop criterion is
reached. WSD is performed by assigning to each in-
duced cluster a score equal to the sum of weights of
hyperedges found in the local context of the target
word. The system was tested and tuned on 10 nouns
of Senseval-3 lexical-sample.
Official Results
Participants were required to induce the senses of
the target words and cluster all target word contexts
accordingly2. Table 2 summarizes the average num-
ber of induced senses as well as the real senses in
the gold standard.
2They were allowed to label each context with a weighted
score vector, assigning a weight to each induced sense. In the
unsupervised evaluation only the sense with maximum weight
was considered, but for the supervised one the whole score vec-
tor was used. However, none of the participating systems la-
beled any instance with more than one sense.
system All nouns verbs
I2R 3.08 3.11 3.06
UBC-AS? 1.32 1.63 1.15
UMND2 1.36 1.71 1.17
upv si 5.57 7.2 4.69
UOY 9.28 11.28 8.2
Gold standard
test 2.87 2.86 2.86
train 3.6 3.91 3.43
all 3.68 3.94 3.54
Table 2: Average number of clusters as returned by the par-
ticipants, and number of classes in the gold standard. Note that
UBC-AS? is the system submitted by the organizers of the task.
System R. All Nouns Verbs
FSc. Pur. Entr. FSc. FSc.
1c1word 1 78.9 79.8 45.4 80.7 76.8
UBC-AS? 2 78.7 80.5 43.8 80.8 76.3
upv si 3 66.3 83.8 33.2 69.9 62.2
UMND2 4 66.1 81.7 40.5 67.1 65.0
I2R 5 63.9 84.0 32.8 68.0 59.3
UofL?? 6 61.5 82.2 37.8 62.3 60.5
UOY 7 56.1 86.1 27.1 65.8 45.1
Random 8 37.9 86.1 27.7 38.1 37.7
1c1inst 9 9.5 100 0 6.6 12.7
Table 3: Unsupervised evaluation on the test corpus (FScore),
including 3 baselines. Purity and entropy are also provided.
UBC-AS? was submitted by the organizers. UofL?? is not a
sense induction system.
System Rank Supervised evaluation
All Nouns Verbs
I2R 1 81.6 86.8 75.7
UMND2 2 80.6 84.5 76.2
upv si 3 79.1 82.5 75.3
MFS 4 78.7 80.9 76.2
UBC-AS? 5 78.5 80.7 76.0
UOY 6 77.7 81.6 73.3
UofL?? 7 77.1 80.5 73.3
Table 4: Supervised evaluation as recall. UBC-AS? was sub-
mitted by the organizers. UofL?? is not a sense induction sys-
tem.
Table 3 shows the unsupervised evaluation of
the systems on the test corpus. We also include
three baselines: the ?one cluster per word? baseline
(1c1word), which groups all instances of a word into
a single cluster, the ?one cluster per instance? base-
line (1c1inst), where each instance is a distinct clus-
ter, and a random baseline, where the induced word
senses and their associated weights have been ran-
domly produced. The random baseline figures in this
paper are averages over 10 runs.
As shown in Table 3, no system outperforms the
1c1word baseline, which indicates that this baseline
10
is quite strong, perhaps due the relatively small num-
ber of classes in the gold standard. However, all
systems outperform by far the random and 1c1inst
baselines, meaning that the systems are able to in-
duce correct senses. Note that the purity and entropy
measures are not very indicative in this setting. For
completeness, we also computed the FScore using
the complete corpus (both train and test). The re-
sults are similar and the ranking is the same. We
omit them for brevity.
The results of the supervised evaluation can be
seen in Table 4. The evaluation is also performed
over the test corpus. Apart from participants, we
also show the most frequent sense (MFS), which
tags every test instance with the sense that occurred
most often in the training part. Note that the su-
pervised evaluation combines the information in the
clustering solution implicitly with the MFS infor-
mation via the mapping in the training part. Pre-
vious Senseval evaluation exercises have shown that
the MFS baseline is very hard to beat by unsuper-
vised systems. In fact, only three of the participant
systems are above the MFS baseline, which shows
that the clustering information carries over the map-
ping successfully for these systems. Note that the
1c1word baseline is equivalent to MFS in this set-
ting. We will review the random baseline in the dis-
cussion section below.
Further Results
Table 5 shows the results of the best systems from
the lexical sample subtask of task 17. The best sense
induction system is only 6.9 percentage points below
the best supervised, and 3.5 percentage points be-
low the best (and only) semi-supervised system. If
the sense induction system had participated, it would
be deemed as semi-supervised, as it uses, albeit in a
shallow way, the training data for mapping the clus-
ters into senses. In this sense, our supervised evalu-
ation does not seek to optimize the available training
data.
After the official evaluation, we realized that con-
trary to previous lexical sample evaluation exercises
task 17 organizers did not follow a random train/test
split. We decided to produce a random train/test
split following the same 82/18 proportion as the of-
ficial split, and re-evaluated the systems. The results
are presented in Table 6, where we can see that all
System Supervised evaluation
best supervised 88.7
best semi-supervised 85.1
best induction (semi-sup.) 81.6
MFS 78.7
best unsupervised 53.8
Table 5: Comparing the best induction system in this task with
those of task 17.
System Supervised evaluation
I2R 82.2
UOY 81.3
UMND2 80.1
upv si 79.9
UBC-AS 79.0
MFS 78.4
Table 6: Supervised evaluation as recall using a random
train/test split.
participants are above the MFS baseline, showing
that all of them learned useful clustering informa-
tion. Note that UOY was specially affected by the
original split. The distribution of senses in this split
did not vary (cf. Table 2).
Finally, we also studied the supervised evalua-
tion of several random clustering algorithms, which
can attain performances close to MFS, thanks to the
mapping information. This is due to the fact that the
random clusters would be mapped to the most fre-
quent senses. Table 7 shows the results of random
solutions using varying numbers of clusters (e.g.
random2 is a random choice between two clusters).
Random2 is only 0.1 below MFS, but as the number
of clusters increases some clusters don?t get mapped,
and the recall of the random baselines decrease.
4 Discussion
The evaluation of clustering solutions is not straight-
forward. All measures have some bias towards cer-
tain clustering strategy, and this is one of the reasons
of adding the supervised evaluation as a complemen-
tary information to the more standard unsupervised
evaluation.
In our case, we noticed that the FScore penal-
ized the systems with a high number of clusters,
and favored those that induce less senses. Given
the fact that FScore tries to balance precision (higher
for large numbers of clusters) and recall (higher for
small numbers of clusters), this was not expected.
We were also surprised to see that no system could
11
System Supervised evaluation
random2 78.6
random10 77.6
ramdom100 64.2
random1000 31.8
Table 7: Supervised evaluation of several random baselines.
beat the ?one cluster one word? baseline. An expla-
nation might lay in that the gold-standard was based
on the coarse-grained OntoNotes senses. We also
noticed that some words had hundreds of instances
and only a single sense. We suspect that the partic-
ipating systems would have beaten all baselines if a
fine-grained sense inventory like WordNet had been
used, as was customary in previous WSD evaluation
exercises.
Supervised evaluation seems to be more neutral
regarding the number of clusters, as the ranking of
systems according to this measure include diverse
cluster averages. Each of the induced clusters is
mapped into a weighted vector of senses, and thus
inducing a number of clusters similar to the number
of senses is not a requirement for good results. With
this measure some of the systems3 are able to beat
all baselines.
5 Conclusions
We have presented the design and results of the
SemEval-2007 task 02 on evaluating word sense in-
duction and discrimination systems. 6 systems par-
ticipated, but one of them was not a sense induc-
tion system. We reused the data from the SemEval-
2007 English lexical sample subtask of task 17, and
set up both clustering-style unsupervised evaluation
(using OntoNotes senses as gold-standard) and a su-
pervised evaluation (using the training part of the
dataset for mapping). We also provide a compari-
son to the results of the systems participating in the
lexical sample subtask of task 17.
Evaluating clustering solutions is not straightfor-
ward. The unsupervised evaluation seems to be
sensitive to the number of senses in the gold stan-
dard, and the coarse grained sense inventory used
in the gold standard had a great impact in the re-
sults. The supervised evaluation introduces a map-
ping step which interacts with the clustering solu-
tion. In fact, the ranking of the participating systems
3All systems in the case of a random train/test split
varies according to the evaluation method used. We
think the two evaluation results should be taken to be
complementary regarding the information learned
by the clustering systems, and that the evaluation
of word sense induction and discrimination systems
needs further developments, perhaps linked to a cer-
tain application or purpose.
Acknowledgments
We want too thank the organizers of SemEval-2007 task 17 for
kindly letting us use their corpus. We are also grateful to Ted
Pedersen for his comments on the evaluation results. This work
has been partially funded by the Spanish education ministry
(project KNOW) and by the regional government of Gipuzkoa
(project DAHAD).
References
E. Agirre, D. Mart??nez, O. Lo?pez de Lacalle, and
A. Soroa. 2006. Evaluating and optimizing the param-
eters of an unsupervised graph-based wsd algorithm.
In Proceedings of the NAACL TextGraphs workshop,
pages 89?96, New York City, June.
D. A. Cruse, 2000. Polysemy: Theoretical and Com-
putational Approaches, chapter Aspects of the Micro-
structure of Word Meanings, pages 31?51. OUP.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solution.
In Proceedings of HLT/NAACL.
R. Koeling, D. McCarthy, and J.D. Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition.
D. Mart??nez and E. Agirre. 2000. One sense per colloca-
tion and genre/topic variations.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A semantic concordance. In Proc. of the ARPA HLT
workshop.
C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word
independent context pair classification model for word
sense disambiguation. In Proc. of CoNLL-2005.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD02.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Proc. of CoNLL-2004, pages 41?48.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proc. of SENSEVAL.
Y Zhao and G Karypis. 2005. Hierarchical clustering
algorithms for document datasets. Data Mining and
Knowledge Discovery, 10(2):141?168.
12
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 342?345,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-ALM: Combining k-NN with SVD for WSD
Eneko Agirre and Oier Lopez de Lacalle
IXA NLP Group
University of the Basque Country
Donostia, Basque Country
{e.agirre,jibloleo}@ehu.es
Abstract
This work describes the University of the
Basque Country system (UBC-ALM) for
lexical sample and all-words WSD subtasks
of SemEval-2007 task 17, where it per-
formed in the second and fifth positions re-
spectively. The system is based on a com-
bination of k-Nearest Neighbor classifiers,
with each classifier learning from a distinct
set of features: local features (syntactic, col-
locations features), topical features (bag-of-
words, domain information) and latent fea-
tures learned from a reduced space using
Singular Value Decomposition.
1 Introduction
Our group (UBC-ALM) participated in the lexical
sample and all-words WSD subtasks of SemEval-
2007 task 17. We applied a combination of different
k-Nearest Neighbor (k-NN) classifiers. Each clas-
sifier manages different information sources (fea-
tures), making the combination a powerful solution.
This algorithm was previously tested on the datasets
from previous editions of Senseval (Agirre et al,
2005; Agirre et al, 2006). Before submission, the
performance of the system was tested on the Se-
mEval lexical sample training data. For learning we
use a rich set of features, including latent features
obtained from a reduced space using Singular Value
Decomposition (SVD).
This paper is organized as follows. The learning
features are presented in section 2, and the learning
algorithm and the combinations of single k-NNs are
given in section 3. Section 4 focuses on the tuning
experiments. Finally, section 5 summarizes the offi-
cial results and some conclusions.
2 Feature set
We relied on an extensive set of features of differ-
ent types, obtained by means of different tools and
resources. We defined two main groups: the origi-
nal features extracted directly from the text, and the
SVD features obtained after applying SVD decom-
position and projecting the original features into the
new semantic space (Agirre et al, 2005).
2.1 Original features
Local collocations: bigrams and trigrams formed
with the words around the target. These features are
constituted by lemmas, word-forms, or PoS tags1.
Other local features are those formed with the previ-
ous/posterior lemma/word-form in the context.
Syntactic dependencies: syntactic dependencies
were extracted using heuristic patterns, and regular
expressions defined with the PoS tags around the tar-
get2. The following relations were used: object, sub-
ject, noun-modifier, preposition, and sibling.
Bag-of-words features: we extract the lemmas
of the content words in the whole context, and in a
?4-word window around the target. We also obtain
salient bigrams in the context, with the methods and
the software described in (Pedersen, 2001).
Domain features: The WordNet Domains re-
source was used to identify the most relevant do-
mains in the context. Following the relevance for-
mula presented in (Magnini and Cavaglia?, 2000), we
defined 2 feature types: (1) the most relevant do-
main, and (2) a list of domains above a predefined
threshold3 .
1The PoS tagging was performed with the fnTBL toolkit
(Ngai and Florian, 2001).
2This software was kindly provided by David Yarowsky?s
group, from Johns Hopkins University.
3The software to obtain the relevant domains was kindly
provided by Gerard Escudero?s group, from Universitat Politec-
342
2.2 SVD features
Singular Value Decomposition (SVD) is an interest-
ing solution to the sparse data problem. This tech-
nique reduces the dimensions of the vectorial space
finding correlations and collapsing features. It also
gives the chance to use unlabeled data as an addi-
tional source of correlations.
M ? Rm?n, a matrix of features-by-document is
built from the training corpus and decomposed into
three matrices, as shown in Eq. (1). U and V , row
and column matrix, respectively, have orthonormal
columns and ? is a diagonal matrix which contains
k eigenvalues in descending order.
M = U?V T =
k=min{m,n}
?
i=1
?iuiviT (1)
We used the singular value matrix (?) and the
column matrix (U ) to create a projection matrix,
which is used to project the data (represented in fea-
tures vectors) from the original space to a reduced
space. Prior to that we selected the first p columns
from the ? and U matrices (p < k): ~tp = ~tTUp??1p
We have explored two different variants in order
to build a matrix, and obtain the SVD features:
SVD One Matrix per Target word (SVD-
OMT). For each word (i) we extracted all the fea-
tures from the given training (test) corpus, (ii) built
the feature-by-document matrix from training cor-
pus, (iii) decomposed it with SVD, and (iv) project
all the training (test) data. Note that this variant has
been only used in the lexical sample task due to its
costly computational requirements.
SVD Single Matrix for All target words (SVD-
SMA): (i) we extracted bag-of-words features from
the British National Corpus (BNC) (Leech, 1992),
(ii) built the feature-by-document matrix, (iii) de-
compose it with SVD, and (iv) project all the data
(train/test).
3 Learning Algorithm
The machine learning (ML) algorithm presented in
this section rely on the previously described fea-
tures. Each occurrence or instance is represented by
the features found in the context (fi). Given an oc-
currence of a word, the ML method below returns a
nica de Catalunya
weight for each sense (weight(sk)). The sense with
maximum weight will be selected.
We use a set of combination of the k-Nearest
Neighbor (k-NN) to tag the target words in both the
lexical sample and all-words tasks.
3.1 k-Nearest Neighbor
k-NN is a memory-based learning method, where
the neighbors are the k most similar contexts, repre-
sented by feature vectors (~ci), of the test vector (~f ).
The similarity among instances is measured by the
cosine of their vectors. The test instance is labeled
with the sense obtaining the maximum sum of the
weighted votes of the k most similar contexts. The
vote is weighted depending on its (neighbor) posi-
tion in the ordered rank, with the closest being first.
Eq. (2) formalizes k-NN, where Ci corresponds to
the sense label of the i-th closest neighbor.
arg max
Sj
=
k
?
i=1
{
1
i if Ci = Sj
0 otherwise (2)
3.2 k-NN combinations and feature splits
As seen in section 2 we use a variety of heteroge-
neous sets of features. Our previous experience has
shown that splitting the problem up into more co-
herent spaces, training different classifiers in each
feature space, and then combining them into a sin-
gle classifier is a good way to improve the results
(Agirre et al, 2005; Agirre et al, 2006). Depend-
ing on the feature type (original features or features
extracted from SVD projection) we split different
sets of feature spaces. In total we tried 10 features
spaces.
For the original features:
? all feats: Extracted all original features.
? all notdom: All original features except do-
main features.
? local: All the original features except domain
and bag-of-words features.
? topic: The sum of bag-of-words and domain
features.
? bow: Bag-of-word features.
? dom: Domain features.
343
Combination accuracy
all feats+topic+local+SVD-OMT[all feats]+SVD-OMT[topic]+SVD-OMT[local] 88.8
all feats+all notdom+topic+local+SVD-SMA+SVD-OMT[all feats]+SVD-OMT[topic]+SVD-OMT[local] 88.7
all feats+topic+local+SVD-SMA+SVD-OMT[all feats]+SVD-OMT[topic]+SVD-OMT[local] 88.5
all notdom+topic+local+SVD-SMA+SVD-OMT[all feats]+SVD-OMT[topic]+SVD-OMT[local] 88.5
all feats+all notdom+topic+local 88.4
all notdom+local+SVD-SMA 88.3
all feats+all notdom+local+SVD-SMA 88.2
all notdom+topic+local 88.1
all feats+topic+local 88.1
word-by-word optimization 89.5
Table 1: Result for the best k-NN combinations in 3 fold cross-validation SemEval lexical sample.
For the SVD features:
? SVD-OMT[all feats]: OMT matrix applied to
all original features.
? SVD-OMT[local]: OMT matrix to the local
original features.
? SVD-OMT[topic]: OMT matrix to the topic
original features.
? SVD-SMA: Features obtained from the projec-
tion of bow features with the SMA matrix.
Depending on the ML method one can try differ-
ent approaches to combine classifiers. In this work,
we exploited the fact that a k-NN classifier can be
seen as k points casting each one vote. The votes
are weigthed by the inverse ratio of its position in
the rank (k ? ri + 1)/k, where ri is the rank. Each
of the k-NN classifiers is trained on a different fea-
ture space and then combined.
4 Experiments on training data
We optimized and tuned the system differently for
each kind of tasks. We will examine each in turn.
4.1 Optimization for the lexical sample task
For the lexical sample task we only use the train-
ing data provided. We tuned the classifiers using 3
fold cross-validation on the SemEval lexical sample
training data. We tried to optimize several param-
eters: number of neighbors, SVD dimensions and
best combination of the single k-NNs. We set k as
one of 1, 3, 5 and 7, and the SVD dimension (d) as
one of 50, 100, 200 and 300. We also fixed the best
combination. This is the optimization procedure we
followed:
1. For each single classifier and feature set (see
section 2), check each parameter combination.
2. Fix the parameters for each single classifier. In
our case, k = 5 and k = 7 had similar results,
so we postponed the decision. d = 200 was the
best dimension for all classifiers, except SVD-
OMT[topic] which was d = 50.
3. For the best parameter settings (k = 5; k = 7
and d = 200; d = 50 when SVD-OMT[topic])
make a priori meaningful combinations (due
to CPU requirements, not all combination were
feasible).
4. Choose the x best combination overall, and op-
timize word by word among these combination.
We set x = 8 for this work, k was fixed in 5,
and d = 200 (except with SVD-OMT[topic]
which was d = 50).
Table 1 shows the best results for 3 fold cross-
validation in SemEval lexical sample training cor-
pus. The figures show that optimizing each word the
performance increases 0.7 percentage points over
the best combination.
4.2 Optimization for the all-words task
To train the classifiers for the all-words task we just
used Semcor (Miller et al, 1993). In (Agirre et
al., 2006) we already tested our approach on the
Senseval-3 all-words task. The best performance
for the Senseval-3 all-words task was obtained with
k = 5 and d = 200, but we decided to to perform
further experiments to search for the best combina-
tion. We tested the performance of the combination
of single k-NN training on Semcor and testing both
on the Senseval-3 all-words data (cf. Table 2) and on
the training data from SemEval-2007 lexical sample
(cf. Table 3).
Note that tables 2 and 3 show contradictory re-
sults. Given that in SemEval-2007 lexical sample
344
Combination rec. prec.
all feats+local+notbow 0.685 0.685
all feats+local+SVD-SMA 0.679 0.679
all feats+topic+local+SVD-SMA 0.689 0.689
Table 2: Results for the best k-NN combinations in
Senseval-3 all-words, using Semcor as training cor-
pus.
Combination rec. prec.
all feats+SVD-SMA 0.666 0.666
all feats+local+SVD-SMA 0.661 0.661
all feats+topic+local+SVD-SMA 0.664 0.664
Table 3: Results for the best k-NN combinations in
training part of SemEval lexical sample, using Sem-
cor as training corpus.
Task Method Rank rec. prec.
LS Best 1 0.887 0.887
LS UBC-ALM 2 0.869 0.869
LS Baseline - 0.780 0.780
AW Best 1 0.591 0.591
AW k-NN combination 5 0.544 0.544
AW Baseline - 0.514 0.514
Table 4: Official results for SemEval-2007 task 17
lexical sample and all-words subtasks.
the senses are more coarse grained, we decided to
take the best combination on Senseval-3 all-words
for the final submission.
5 Results and conclusions
Table 4 shows the performance obtained by our sys-
tem and the winning systems in the SemEval lexi-
cal sample and all-words evaluation. On the lexical
sample evaluation our system is 2.6 lower than the
cross-validation evaluation. This can be a sign of a
slight overfitting on the training data. All in all we
ranked second over 13 systems.
Our all-words system did not perform so well.
Our system is around 4.7 points below the winning
system, ranking 5th from a total of 14, and 3 points
above the baseline given by the organizers. This is
a disappointing result when compared to our previ-
ous work on Senseval-3 all-words where we were
able to beat the best official results (Agirre et al,
2006). Note that the test set was rather small, with
465 occurrences only, which might indicate that the
performance differences are not statistically signifi-
cant. We plan to further investigate the reasons for
our results.
Acknowledgments
We wish to thank to David Mart??nez for helping us
extracting learning features. This work has been
partially funded by the Spanish education ministry
(project KNOW). Oier Lopez de Lacalle is sup-
ported by a PhD grant from the Basque Government.
References
E. Agirre, O.Lopez de Lacalle, and David Mart??nez.
2005. Exploring feature spaces with svd and unlabeled
data for Word Sense Disambiguation. In Proceedings
of the Conference on Recent Advances on Natural Lan-
guage Processing (RANLP?05), Borovets, Bulgaria.
E. Agirre, O. Lopez de Lacalle, and D. Mart??nez. 2006.
Exploring feature spaces with svd and unlabeled data
for Word Sense Disambiguation. In Proceedings
of the XXII Conference of Sociedad Espaola para
el Procesamiento del Lenguaje Natural (SEPLN?06),
Zaragoza, Spain.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
B. Magnini and G. Cavaglia?. 2000. Integrating subject
field codes into WordNet. In Proceedings of the Sec-
ond International LREC Conference, Athens, Greece.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A Semantic Concordance. In Proceedings of the
ARPA Human Language Technology Workshop. Dis-
tributed as Human Language Technology by San Ma-
teo, CA: Morgan Kaufmann Publishers., pages 303?
308, Princeton, NJ.
G. Ngai and R. Florian. 2001. Transformation-Based
Learning in the Fast Lane. Proceedings of the Second
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 40-47,
Pittsburgh, PA, USA.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), Pittsburgh, PA.
345
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 346?349,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-AS: A Graph Based Unsupervised System
for Induction and Classification
Eneko Agirre and Aitor Soroa
IXA NLP Group
UBC
Donostia, Basque Contry
{e.agirre,a.soroa}@si.ehu.es
Abstract
This paper describes a graph-based unsu-
pervised system for induction and clas-
sification. The system performs a two
stage graph based clustering where a co-
occurrence graph is first clustered to com-
pute similarities against contexts. The con-
text similarity matrix is pruned and the re-
sulting associated graph is clustered again
by means of a random-walk type algorithm.
The system relies on a set of parameters that
have been tuned to fit the corpus data. The
system has participated in tasks 2 and 13
of the SemEval-2007 competition, on word
sense induction and Web people search, re-
spectively, with mixed results.
1 Introduction
This paper describes a graph-based unsupervised
system for induction and classification. Given a set
of data to be classified, the system first induces the
possible clusters and then clusters the data accord-
ingly. The paper is organized as follows. Section 2
gives an description of the general framework of our
system. Sections 3 and 4 presents in more detail the
implementation of the framework for the Semeval-
2007 WEPS task (Artiles et al, 2007) and Semeval-
2007 sense induction task (Agirre and Soroa, 2007),
respectively. Section 5 presents the results obtained
in both tasks, and Section 6 draws some conclusions.
2 A graph based system for unsupervised
classification
The system performs a two stage graph based clus-
tering where a co-occurrence graph is first clustered
to compute similarities against contexts. The context
similarity matrix is pruned and the resulting associ-
ated graph is clustered again by means of a random-
walk type algorithm. We will see both steps in turn.
First step: calculating hub score vectors
In a first step, and for each entity to be clustered, a
graph consisting on context word co-occurrences is
built. Vertices in the co-occurrence graph are words
and two vertices share an edge whenever they co-
occur in the same context. Besides, each edge re-
ceives a weight, which indicates how strong the in-
cident vertices relate each other.
As shown in (Ve?ronis, 2004), co-occurrence
graphs exhibit the so called small world structure
(Watts and Strogatz, 1998) and, thus, they contain
highly dense subgraphs which will represent the dif-
ferent clusters the entity may have. For identifying
these clusters we have implemented two algorithms
based on the notion of centrality of the vertices,
where some highly dense vertices, called ?hubs?, are
chosen as representatives of each cluster. The algo-
rithms are the HyperLex algorithm (Ve?ronis, 2004)
and the HITS algorithm (Kleinberg, 1999).
Once the hubs are identified, the minimum span-
ning tree (MST) of the co-occurrence graph is com-
puted. The root elements of the MST are precisely
the induced hubs and each vertex of the original
graph ?and, thus, each word of the corpus? is at-
tached to exactly one of these hubs, at a certain dis-
tance. Note that the MST can be considered as a
single link clustering over the co-occurrence graph.
The original contexts are then taken one by one
and scored according to the MST in the following
way: each word in the context receives a set of score
vectors, with one score per hub, where all scores are
346
0 except for the one corresponding to the hub where
it is placed1, which will receive a socre d(hi, v),
which is the distance between the hub hi and the
node representing the word v in the MST. Thus,
d(hi, v) assigns a score of 1 to hubs and the score
decreases as the nodes move away from the hub in
the MST. As a consequence, each context receives a
hub score vector, which is just the sum of the score
vectors of all the words in the context.
At this point we can use the hub score vectors
to create clusters of contexts, just assigning to each
context the hub with maximum score. This process
is thoroughly explained in (Agirre et al, 2006b).
One of the problems of such an approach comes
from the tendency of the system to produce a high
number of hubs, somehow favouring small micro-
clusters over coarse ones. Knowing in advance that
the number of clusters in the tasks we will partici-
pate in would not be very high, we decided to per-
form a second stage and re-cluster again the results
obtained in the first step, using a different graph-
based technique. Re-clustering also gives us the op-
portunity to feed the system with additional data, as
will be explained below.
Second step: clustering via MCL
In this second stage, we compute a square ma-
trix with as many rows/columns as contexts, and
where each element represents the relatedness be-
tween two contexts, just computing the cosine dis-
tance of its (normalized) hub score vectors obtained
in the first step. We prune each row in the matrix
and keep only the element with maximum values, so
that the percentage of the kept elements? sum respect
the total is below a given threshold. The resulting
matrix M represents the adjacency matrix of a di-
rected weighted graph, where vertices are contexts
and edges represent the similarity between them. We
can feed the matrixM with external information just
by calculating another dissimilarity matrix between
contexts and lineally interpolating the matrices with
a factor.
Finally, we apply the Markov Clustering (MCL)
algorithm (van Dongen, 2000) over the graph M
for calculating the final clusters. MCL is a graph-
clustering algorithm based on simulation of stochas-
1Note that each word will be attached to exactly one hub in
the MST.
tic flows in graphs, its main idea being that random
walks within the graph will tend to stay in the same
cluster rather than jump between clusters. MCL has
the remarkable property that there is no need to a-
priori decide how many clusters it must find. How-
ever, it has some parameters which will influence the
granularity of the clusters.
In fact, the behavior of the whole process relies
on a number of parameters, which can be divided in
several groups:
? Parameters for calculating the hubs
? Parameters for merging the hubs information
with external information in the matrix M (?)
? The threshold for pruning the graph (?)
? Parameters of the MCL algorithm (I , inflation
parameter)
In sections 3 and 4 we describe the parameters
we actually used for the final experiments, as well
as how the tuning of these parameters has been per-
formed for the two tasks.
3 Web People Search task
In this section we will explain in more detail how
we implemented the general schema described in
the previous section to the ?Web People Search?
task (Artiles et al, 2007). The task consist on dis-
ambiguating person names in a web searching sce-
nario. The input consists on web pages retrieved
from a web searching engine using person names as
a query. The aim is to determine how many ref-
erents (people with the same name) exist for that
person name, and classify each document with its
corresponding referent. There is a train set con-
sisting on 49 names and 100 documents per name.
The test setting consist on 30 unrelated names, with
100 document per name. The evaluation is per-
formed following the ?purity? and ?inverse purity?
measures. Roughly speaking, purity measures how
many classes they are in each cluster (like the pre-
cision measure). If a cluster fits into one class, the
purity equals to 1. On the other side, inverse purity
measures how many clusters they are in each class
(recall). The final figure is obtained by combining
purity and inverse purity by means of the standard
F-Measure with ? = 0.5.
The parameters of the system were tuned using
the train part of the corpus as a development set. As
usual, the parameters that yielded best results were
used on the test part.
347
We first apply a home-made wrapper over the
html files for retrieving the text chunks of the pages,
which is usually mixed with html tags, javascript
code, etc. The text is split into sentences and parsed
using the FreeLing parser (Atserias et al, 2006).
Only the lemmas of nouns are retained. We filter the
nouns and keep only back those words whose fre-
quency, according to the British National Corpus, is
greater than 4. Next, we search for the person name
across the sentences, and when such a sentence is
found we build a context consisting on its four pre-
decessor and four successors, i.e., contexts consists
on 9 sentences. At the end, each document is rep-
resented as a set of contexts containing the person
name. Finally, the person names are removed from
the contexts.
For inducing the hubs we apply the HyperLex al-
gorithm (Ve?ronis, 2004). Then, the MST is calcu-
lated and every context is assigned with a hub score
vector. We calculate the hub score vector of the
whole document by averaging the score vectors of
its contexts. The M matrix of pairwise similarities
between documents is then computed and pruned
with a threshold of 0.2, as described in section 2.
We feed the system with additional data about
the topology of the pages over the web. For each
document di to be classified we retrieve the set of
documents Pi which link to di. We use the pub-
licly available API for Microsoft Search. Then, for
each pair of documents di and dj we calculate the
number of overlapping documents linking to them,
i.e., lij = #{Pi ? Pj} with the intuition that, the
more pages point to the two documents, the more
probably is that they both refer to the same per-
son. The resulting matrix, ML is combined with
the original matrix M to give a final matrix M ?, by
means of a linear interpolation with factor of 0.2, i.e.
M ? = 0.2M + 0.8ML. Finally, the MCL algorithm
is run over M ? with an inflation parameter of 5.
4 Word Sense Induction and
Discrimination task
The goal of this task is to allow for comparison
across sense-induction and discrimination systems,
and also to compare these systems to other super-
vised and knowledge-based systems. The input con-
sist on 100 target words (65 verbs and 35 nouns),
each target word having a set of contexts where the
word appears. The goal is to automatically induce
the senses each word has, and cluster the contexts
accordingly. Two evaluation measures are provided:
and unsupervised evaluation (FScore measure) and
a supervised evaluation, where the organizers auto-
matically map the induced clusters onto senses. See
(Agirre and Soroa, 2007) for more details.
In order to improve the overall performance, we
have clustered the 35 nouns and the 65 verbs sepa-
rately. In the case of nouns, we have filtered the orig-
inal contexts and kept only noun lemmas, whereas
for verbs lemmas of nouns, verbs and adjectives
were hold.
The algorithm for inducing the hubs is also dif-
ferent among nouns and verbs. Nouns hubs are in-
duced with the usual HyperLex algorithm (just like
in section 3) but for identifying verb hubs we used
the HITS algorithm (Kleinberg, 1999), based on pre-
liminary experiments.
The co-occurrence relatedness is also measured
differently for verbs: instead of using the original
conditional probabilities, the ?2 measure between
words is used. The reason behind is that condi-
tional probabilities, as used in (Ve?ronis, 2004), per-
form poorly in presence of words which occur in
nearly all contexts, giving them an extraordinary
high weight in the graph. Very few nouns hap-
pen to occur in many contexts, but they are verbs
which certainly do (be, use, etc). On the other
hand, ?2 measures to what extent the observed co-
occurrences diverge from those expected by chance,
so weights of edges incident with very common,
non-informant words will be low.
Parameter tuning for both nouns and verbs was
performed over the senseval-3 testbed, and the best
parameter combination were applied over the sense
induction corpus. However, there is a factor we have
taken into account in tuning directly over the sense
induction corpus, i.e., that the granularity?and thus
the number of classes? of senses in OntoNotes (the
inventory used in the gold standard) is considerably
coarser than in senseval-3. Therefore, we have man-
ually tuned the inflation parameter of the MCL al-
gorithm in order to achieve numbers of clusters be-
tween 1 and 4.
A threshold of 0.6was used when pruning the dis-
similarity matrix M for both nouns and verbs. We
have tried to feed the system with additional data
348
System All Nouns Verbs
Best 78.7 80.8 76.3
Worst 56.1 62.3 45.1
Average 65.4 69.0 61.4
UBC-AS 78.7 80.8 76.3
Table 1: Results of Semeval-2007 Task 2. Unsuper-
vised evaluation (FScore).
System All Nouns Verbs
Best 81.6 86.8 76.2
Worst 77.1 80.5 73.3
Average 79.1 82.8 75.0
UBC-AS 78.5 80.7 76.0
Table 2: Results of Semeval-2007 Task 2. Super-
vised evaluation as recall.
(mostly local and domain features of the context
words) but, although the system performed slightly
better, we decided that the little gain (which prob-
ably was not statistically significant) was no worth
the effort.
5 Results
Table 1 shows the results of the unsupervised evalu-
ation in task 2, where our system got the best results
in this setting. Table 2 shows the supervised evalua-
tion on the same task, where our system got a rank-
ing of 4, performing slightly worse than the average
of the systems.
In Table 3 we can see the results of Semeval-2007
Task 13. As can be seen, our system didn?t manage
to capture the structure of the corpus, and it got the
worst result, far below the average of the systems.
6 Conclusions
We have presented graph-based unsupervised sys-
tem for induction and classification. The system per-
forms a two stage graph based clustering where a co-
occurrence graph is first clustered to compute simi-
larities against contexts. The context similarity ma-
trix is pruned and the resulting associated graph is
clustered again by means of a random-walk type al-
gorithm. The system has participated in tasks 2 and
13 of the SemEval-2007 competition, on word sense
induction and Web people search, respectively, with
mixed results. We did not have time to perform
an in-depth analysis of the reasons causing such a
different performance. One of the reasons for the
failure in the WePS task could be the fact that we
System F?=0.5
Best 78.0
Worst 40.0
Average 60.0
UBC-AS 40.0
Table 3: Results of Semeval-2007 Task 13
were first-comers, with very little time to develop
the system, and we used a very basic and coarse pre-
processing of the HTML files. Another factor could
be that we intentionally made our clustering algo-
rithm return few clusters. We were mislead by the
training data provided, as the final test data had more
classes on average.
Acknowledgements
This work has been partially funded by the Spanish
education ministry (project KNOW) and by the re-
gional government of Gipuzkoa (project DAHAD).
References
E. Agirre and A. Soroa. 2007. Semeval-2007 task 2:evaluating
word sense induction and discrimination systems. In Pro-
ceedings of Semeval 2007, Association for Computational
Linguistics.
E. Agirre, D. Mart??nez, O. Lo?pez de Lacalle, and A. Soroa.
2006a. Evaluating and optimizing the parameters of an un-
supervised graph-based wsd algorithm. In Proceedings of
TextGraphsWorkshop. NAACL06., pages 89?96. Association
for Computational Linguistics, June.
E. Agirre, D. Mart??nez, O. Lo?pez de Lacalle, and A. Soroa.
2006b. Two graph-based algorithms for state-of-the-art wsd.
In Proceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 585?593. Asso-
ciation for Computational Linguistics, July.
J. Artiles, J. Gonzalo, and S. Sekine. 2007. Establishing a
benchmark for the web people search task: The semeval
2007 weps track. In Proceedings of Semeval 2007, Asso-
ciation for Computational Linguistics.
J. Atserias, B. Casas, E. Comelles, M. Gonza?lez, L. Padro?, and
M. Padro?. 2006. Freeling 1.3: Syntactic and semantic ser-
vices in an open-source NLP library. In Proceedings of the
5th International Conference on Language Resources and
Evaluation (LREC?06), pages 48?55.
Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked
environment. Journal of the ACM, 46(5):604?632.
Stijn van Dongen. 2000. A cluster algorithm for graphs.
Technical Report INS-R0010, National Research Institute
for Mathematics and Computer Science in the Netherlands,
Amsterdam, May.
J. Ve?ronis. 2004. Hyperlex: lexical cartography for informa-
tion retrieval. Computer Speech & Language, 18(3):223?
252.
D. J. Watts and S. H. Strogatz. 1998. Collective dynamics of
?small-world? networks. Nature, 393(6684):440?442, June.
349
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 350?353,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-UMB: Combining unsupervised and supervised systems for all-words
WSD
David Martinez,Timothy Baldwin
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
{davidm,tim}@csse.unimelb.edu.au
Eneko Agirre, Oier Lopez de Lacalle
IXA NLP Group
Univ. of the Basque Country
Donostia, Basque Country
{e.agirre,jibloleo}@ehu.es
Abstract
This paper describes the joint submission
of two systems to the all-words WSD sub-
task of SemEval-2007 task 17. The main
goal of this work was to build a competitive
unsupervised system by combining hetero-
geneous algorithms. As a secondary goal,
we explored the integration of unsupervised
predictions into a supervised system by dif-
ferent means.
1 Introduction
This paper describes the joint submission of two sys-
tems to the all-words WSD subtask of SemEval-
2007 task 17. The systems were developed by the
University of the Basque Country (UBC), and the
University of Melbourne (UMB). The main goal of
this work was to build a competitive unsupervised
system by combining heterogeneous algorithms. As
a secondary goal, we explored the integration of
this method into a supervised system by different
means. Thus, this paper describes both the unsu-
pervised system (UBC-UMB-1), and the combined
supervised system (UBC-UMB-2) submitted to the
all-words task.
Our motivation in building unsupervised systems
comes from the difficulty of creating hand-tagged
data for all words and all languages, which is col-
loquially known as the knowledge acquisition bot-
tleneck. There have also been promising results in
recent work on the combination of unsupervised ap-
proaches that suggest the gap with respect to super-
vised systems is narrowing (Brody et al, 2006).
The remainder of the paper is organized as fol-
lows. First we describe the disambiguation algo-
rithms in Section 2. Next, the development exper-
iments are presented in Section 3, and our final sub-
missions and results in Section 4. Finally, we sum-
marize our conclusions in Section 5.
2 Algorithms
In this section, we will describe the standalone algo-
rithms (three unsupervised and one supervised) and
the combination schemes we explored. The unsu-
pervised methods are based on different intuitions
for disambiguation (topical features, local context,
and WordNet relations), which is a desirable charac-
teristic for combining algorithms.
2.1 Topic Signatures (TS)
Topic signatures (Agirre and de Lacalle, 2004) are
lists of words related to a particular sense. They can
be built from a variety of sources, and be used di-
rectly to perform WSD. Cuadros and Rigau (2006)
present a detailed evaluation of topic signatures built
from a variety of knowledge sources. In this work
we built those coming from the following:
? the relations in the Multilingual Central Repos-
itory (TS-MCR)
? the relations in the Extended WordNet (TS-
XWN)
In order to apply this resource for WSD, we sim-
ply measured the word-overlap between the target
context and each of the senses of the target word.
The sense with highest overlap is chosen as the cor-
rect sense.
350
2.2 Relatives in Context (RIC)
This is an unsupervised method presented in Mar-
tinez et al (2006). This algorithm makes use of
the WordNet relatives of the target word for disam-
biguation. The process is carried out in these steps:
(i) obtain a set of close relatives from WordNet for
each sense (the relatives can be polysemous); (ii) for
each test instance define all possible word sequences
that include the target word; (iii) for each word se-
quence, substitute the target word with each relative
and query a web search engine; (iv) rank queries ac-
cording to the following factors: length of the query,
distance of the relative to the target word, and num-
ber of hits; and (v) select the sense associated with
the highest ranked query.
The intuition behind this system is that we can
find related words that can be substituted for the tar-
get word in a given context, which are indicative of
its sense. The close relatives that can form more
common phrases from the target context determine
the target sense.
2.3 Relative Number (RNB)
This heuristic has been motivated as a way of identi-
fying rare senses of a word. An important disadvan-
tage of unsupervised systems is that rare senses can
be over-represented in the models, while supervised
systems are able to discard them because they have
access to token-level word sense distributions.
This simple algorithm relies on the number of
close relatives found in WordNet for each sense of
the word. The senses are ranked according to the
number of synonyms, direct hypernyms, and di-
rect hyponyms they have in WordNet. The highest
ranked sense is taken to be the most important for the
target word, and all occurrences of the target word
are tagged with that sense.
2.4 k-Nearest Neighbours (kNN)
As our supervised system, we relied on kNN. This is
a memory-based learning method where the neigh-
bours are the k most similar contexts, represented by
feature vectors (~ci) of the test vector (~f ). The sim-
ilarity among instances is measured by the cosine
of their vectors. The test instance is labeled with the
sense that obtains the maximum sum of the weighted
votes of the k most similar contexts. Each vote is
weighted depending on its (neighbour) position in
the ordered rank, with the closest being first. Equa-
tion 1 formalizes kNN, where Ci corresponds to the
sense label of the i-th closest neighbour.
arg max
Sj
=
k
?
i=1
{
1
i if Ci = Sj
0 otherwise (1)
The UBC group used a combination of kNN clas-
sifiers trained over a large set of features, and en-
hanced this method using Singular Value Decompo-
sition (SVD) for their supervised submission (UBC-
ALM) to the lexical-sample and all-words subtasks
(Agirre and Lopez de Lacalle, 2007). However, we
only used the basic implementation in this work, due
to time constraints.
2.5 Combination of systems
We explored two approaches to combine the stan-
dalone systems. The first consisted simply of adding
up the normalized weights that each system would
give to each sense. We tested this voting approach
both for the unsupervised and supervised settings.
The second method could only be applied in com-
bination with the supervised kNN system. The
idea was to include the unsupervised predictions as
weighted features for the supervised system. We re-
fer to this method as ?stacking?, and it has been pre-
viously used to integrate heterogeneous knowledge
sources for WSD (Stevenson and Wilks, 2001).
3 Development experiments
We tested the single algorithms and their combina-
tion over both Semcor and the training distribution
of the SemEval-2007 lexical-sample subtask of task
17 (S07LS for short). The goal of these experiments
was to obtain an estimate of the expected perfor-
mance, and submit the most promising configura-
tion. We present first the tests on the unsupervised
setting, and then the supervised setting. It is im-
portant to note that the hand-tagged corpora was not
used to fine-tune the parameters of the unsupervised
algorithms.
3.1 Unsupervised systems
For the first evaluation of our unsupervised systems,
we relied on Semcor, and tagged 43,063 instances
of the 329 word types occurring in SemEval-2007
351
System Recall
RNB 30.6
TS-MCR 57.5
TS-XWN 47.0
TS-MCR & TS-XWN 57.3
RBN & TS-MCR & TS-XWN 53.6
Table 1: Evaluation of standalone and combined
unsupervised systems over 43,063 instances from
Semcor
System Recall
TS-MCR 60.1
TS-XWN 54.3
TS-MCR & TS-XWN 61.1
TS-MCR & TS-XWN & RIC* 61.2
Table 2: Evaluation of standalone and combined
unsupervised systems over 8,518 instances from
S07LS training
all-words. Due to time constraints, we were not able
to test the RIC algorithm on this dataset. The re-
sults are shown in Table 1. We can see that the RNB
heuristic performs poorly, and that the best configu-
ration consists of applying the single TS-MCR algo-
rithm. From this experiment, we decided to remove
the RNB heuristic and focus on the topic signatures
and RIC.
We also used S07LS for extra experiments in
the unsupervised setting. From the training part of
the S07LS dataset, we extracted 8,518 instances of
words also occurring in SemEval-2007 all-words.
As S07LS used senses from OntoNotes, we relied
on the mapping provided by the task organisers to
link them to WordNet senses. We left RNB out of
this experiment due to its low performance in Sem-
cor, and regarding RIC, we only evaluated a sample
of 68 instances. Results are shown in Table 2. The
best scores are achieved when combining both sets
of topic signatures. The few cases that have been
disambiguated with RIC improve the overall perfor-
mance slightly.
3.2 Combined system
We could not rely on Semcor in the supervised set-
ting (we used it for training), and therefore tried to
use as much data as possible from the training com-
ponent of S07LS, wherein all the instances avail-
able (22,281) were disambiguated. We tested first
System Recall
kNN 87.4
kNN & TS-MCR 86.8
kNN & TS-XWN 86.4
kNN & TS-MCR & TS-XWN 86.0
Table 3: Evaluation of voting supervised systems in
22,281 instances from S07LS training
System Recall
kNN 71.7
kNN & TS-MCR & TS-XWN 71.8
Table 4: Evaluation of ?stacking? the unsupervised
systems on kNN over 8,518 instances from S07LS
training
the voting combination by adding the normalized
weights from the output of each system. Due to
time constraints we only evaluated the combination
of kNN with TS-MCR and TS-XWN. Results are
shown in Table 3, where we can see that combin-
ing the unsupervised systems with voting hurts the
performance of the kNN method.
Finally, we applied the second combination ap-
proach, consisting of including the predictions of the
unsupervised systems as features for kNN (?stack-
ing?). We performed this experiment on the training
part of S07LS, but only for the 8,518 instances of
the words occurring on the all-words dataset. The
results of this experiment are given in Table 4. We
observed a slight improvement in this case.
4 Final systems
For our final submissions, we chose the combination
?TS-MCR& TS-XWN&RIC? for the unsupervised
system (UBC-UMB-1), and the combination ?kNN
& TS-MCR & TS-XWN? via ?stacking? for our su-
pervised system (UBC-UMB-2). The results of all
the systems are given in Table 5.
We can see that our unsupervised system ranked
10th. Unfortunately, we do not know at the time of
writing which other systems are unsupervised, and
therefore are unable to compare to other unsuper-
vised systems.
Our ?stacking? supervised system performs
slightly lower than the kNN supervised systems by
UBC-ALM (which ranks 7th), showing that our sys-
tem was not able to profit from information from
352
System Precision Recall
1. 0.537 0.537
2. 0.527 0.527
3. 0.524 0.524
4. 0.522 0.486
5. 0.518 0.518
6. 0.514 0.514
7. 0.493 0.492
8. UBC-UMB-2 0.485 0.484
9. 0.420 0.420
10. UBC-UMB-1 0.362 0.362
11. 0.355 0.355
12. 0.337 0.337
13. 0.298 0.298
14. 0.120 0.118
Table 5: Official results for all systems in task #17
of SemEval-2007. Our systems are shown in bold.
UBC-UMB-1 stands for TS-MCR & TS-XWN &
RIC, and UBC-UMB-2 for kNN & TS-MCR & TS-
XWN.
System Precision Recall
TS-MCR 36.7 36.5
TS-XWN 33.1 32.9
RIC 30.6 30.4
TS-MCR & TS-XWN 37.5 37.3
TS-MCR & TS-XWN & RIC 36.2 36.2
Table 6: Our unsupervised systems in the SemEval-
2007 all words test data
the unsupervised systems. However, we cannot at-
tribute the decrease only to the unsupervised fea-
tures, as the kNN implementations were different
(UBC-ALM relied on SVD).
After the gold-standard data was released, we
were able to test the contribution of each of the un-
supervised systems in the ensemble, as well as two
additional combinations. The results are given in
Table 6. We can see that TS-MCR is the best per-
forming method, confirming our development ex-
periments (cf. Tables 1 and 2). In contrast, in-
cluding RIC decreased the performance by 0.7 per-
cent points, and had we used only TS-MCR and TS-
XWN our results would have been better.
5 Conclusions
In this submission we combined heterogeneous un-
supervised algorithms to obtain competitive perfor-
mance without relying on training data. However,
due to time constraints, we were only able to submit
a preliminary system, and some of the unsupervised
methods were not properly developed and tested.
For future work we plan to properly test these
methods, and deploy other unsupervised algorithms.
We also plan to explore more sophisticated combina-
tion strategies, using meta-learning to try to predict
which features of each word make a certain WSD
system succeed (or fail).
Acknowledgements
The first and second authors were supported by Aus-
tralian Research Council grant no. DP0663879. We
want to thank German Rigau from the University of
the Basque Country for kindly providing access to
the MCR.
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proceedings of the 4rd International
Conference on Language Resources and Evaluations
(LREC), pages 1123?6, Lisbon, Portugal.
Eneko Agirre and Oier Lopez de Lacalle. 2007. UBC-
ALM: Lexical-Sample and All-Words tasks. In
Proceedings of SemEval-2007 (forthcoming), Prague,
Czech Republic.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised WSD. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the ACL, pages 97?104, Sydney, Australia.
Montse Cuadros and German Rigau. 2006. Quality as-
sessment of large scale knowledge resources. In Pro-
ceedings of the International Conference on Empirical
Methods in Natural Language Processing (EMNLP-
06), pages 534?41, Sydney, Australia.
David Martinez, Eneko Agirre, and Xinglong Wang.
2006. Word relatives in context for word sense dis-
ambiguation. In Proceedings of the 2006 Australasian
Language Technology Workshop, pages 42?50, Syd-
ney, Australia.
Mark Stevenson and YorickWilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321?49.
353
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 354?357,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-UPC: Sequential SRL Using Selectional Preferences.
An aproach with Maximum Entropy Markov Models
Ben?at Zapirain, Eneko Agirre
IXA NLP Group
University of the Basque Country
Donostia, Basque Country
{benat.zapirain,e.agirre}@ehu.es
Llu??s Ma`rquez
TALP Research Center
Technical University of Catalonia
Barcelona, Catalonia
lluism@lsi.upc.edu
Abstract
We present a sequential Semantic Role La-
beling system that describes the tagging
problem as a Maximum Entropy Markov
Model. The system uses full syntactic in-
formation to select BIO-tokens from input
data, and classifies them sequentially us-
ing state-of-the-art features, with the addi-
tion of Selectional Preference features. The
system presented achieves competitive per-
formance in the CoNLL-2005 shared task
dataset and it ranks first in the SRL subtask
of the Semeval-2007 task 17.
1 Introduction
In Semantic Role Labeling (SRL) the goal is to iden-
tify word sequences or arguments accompanying the
predicate and assign them labels depending on their
semantic relation. In this task we disambiguate ar-
gument structures in two ways: predicting VerbNet
(Kipper et al, 2000) thematic roles and PropBank
(Palmer et al, 2005) numbered arguments, as well
as adjunct arguments.
In this paper we describe our system for the SRL
subtask of the Semeval2007 task 17. It is based on
the architecture and features of the system named
?model 2? of (Surdeanu et al, forthcoming), but it
introduces two changes: we use Maximum Entropy
for learning instead of AdaBoost and we enlarge the
feature set with combined features and other seman-
tic features.
Traditionally, most of the features used in SRL
are extracted from automatically generated syntac-
tic and lexical annotations. In this task, we also ex-
periment with provided hand labeled semantic infor-
mation for each verb occurrence such as the Prop-
Bank predicate sense and the Levin class. In addi-
tion, we use automatically learnt Selectional Prefer-
ences based on WordNet to generate a new kind of
semantic based features.
We participated in both the ?close? and the ?open?
tracks of Semeval2007 with the same system, mak-
ing use, in the second case, of the larger CoNLL-
2005 training set.
2 System Description
2.1 Data Representation
In order to make learning and labeling easier, we
change the input data representation by navigating
through provided syntactic structures and by extract-
ing BIO-tokens from each of the propositions to be
annotated as shown in (Surdeanu et al, forthcom-
ing). These sequential tokens are selected by ex-
ploring the sentence spans or regions defined by the
clause boundaries, and they are labeled with BIO
tags depending on the location of the token: at the
beginning, inside, or outside of a verb argument. Af-
ter this data pre-processing step, we obtain a more
compact and easier to process data representation,
making also impossible overlapping and embedded
argument predictions.
2.2 Feature Representation
Apart from Selectional Preferences (cf. Section 3)
and those extracted from provided semantic infor-
mation, most of the features we used are borrowed
from the existing literature (Gildea and Jurafsky,
2002; Xue and Palmer, 2004; Surdeanu et al, forth-
coming).
354
On the verb predicate:
? Form; Lemma; POS tag; Chunk type and Type
of verb phrase; Verb voice; Binary flag indicat-
ing if the verb is a start/end of a clause.
? Subcategorization, i.e., the phrase structure rule
expanding the verb parent node.
? VerbNet class of the verb (in the ?close? track
only).
On the focus constituent:
? Type; Head;
? First and last words and POS tags of the con-
stituent.
? POS sequence.
? Bag-of-words of nouns, adjectives, and adverbs
in the constituent.
? TOP sequence: right-hand side of the rule ex-
panding the constituent node; 2/3/4-grams of
the TOP sequence.
? Governing category as described in (Gildea
and Jurafsky, 2002).
Context of the focus constituent:
? Previous and following words and POS tags of
the constituent.
? The same features characterizing focus con-
stituents are extracted for the two previous and
following tokens, provided they are inside the
clause boundaries of the codified region.
Relation between predicate and constituent:
? Relative position; Distance in words and
chunks; Level of embedding with respect to the
constituent: in number of clauses.
? Binary position; if the argument is after or be-
fore the predicate.
? Constituent path as described in (Gildea and
Jurafsky, 2002); All 3/4/5-grams of path con-
stituents beginning at the verb predicate or end-
ing at the constituent.
? Partial parsing path as described in (Carreras
et al, 2004)); All 3/4/5-grams of path elements
beginning at the verb predicate or ending at the
constituent.
? Syntactic frame as described by Xue and
Palmer (2004)
Combination Features
? Predicate and Phrase Type
? Predicate and binary position
? Head Word and Predicate
? Predicate and PropBank frame sense
? Predicate, PropBank frame sense, VerbNet
class (in the ?close? track only)
2.3 Maximum Entropy Markov Models
Maximum Entropy Markov Models are a discrimi-
native model for sequential tagging that models the
local probability P (sn | sn?1, o), where o is the
context of the observation.
Given a MEMM, the most likely state sequence is
the one that maximizes the following
S = argmax
n?
i=1
P (si | si?1, o)
Translating the problem to SRL, we have
role/argument labels connected to each state in the
sequence (or proposition), and the observations are
the features extracted in these points (token fea-
tures). We get the most likely label sequence finding
out the most likely state sequence (Viterbi).
All the conditional probabilities are given by the
Maximum Entropy classifier with a tunable Gaus-
sian prior from the Mallet Toolkit1.
Some restrictions are considered when we search
the most likely sequence2:
1. No duplicate argument classes for A0-A5 and
thematic roles.
2. If there is a R-X argument (reference), then
there has to be a X argument before (refer-
enced).
3. If there is a C-X argument (continuation), then
there has to be a X argument before.
4. Before a I-X token, there has to be a B-X or I-X
token (because of the BIO encoding).
5. Given a predicate and its PropBank sense, only
some arguments are allowed (e.g. not all the
verbs support A2 argument).
6. Given a predicate and its Verbnet class, only
some thematic roles are allowed.
3 Including Selectional Preferences
Selectional Preferences (SP) try to capture the fact
that linguistic elements prefer arguments of a cer-
tain semantic class, e.g. a verb like ?eat? prefers as
subject edible things, and as subject animate entities,
as in ?She was eating an apple? They can be learned
from corpora, generalizing from the observed argu-
ment heads (e.g. ?apple?, ?biscuit?, etc.) into ab-
stract classes (e.g. edible things). In our case we
1http://mallet.cs.umass.edu
2Restriction 5 applies to PropBank output. Restriction 6 ap-
plies to VerbNet output
355
follow (Agirre and Martinez, 2001) and use Word-
Net (Fellbaum, 1998) as the generalization classes
(the concept <food,nutrient>).
The aim of using Selectional Preferences (SP) in
SRL is to generalize from the argument heads in
the training instances into general word classes. In
theory, using word classes might overcome the data
sparseness problem for the head-based features, but
at the cost of introducing some noise.
More specifically, given a verb, we study the oc-
currences of the target verb in a training corpus (e.g.
the PropBank corpus), and learn a set of SPs for
each argument and adjunct of that verb. For in-
stance, given the verb ?kill? we would have 2 SPs
for each argument type, and 4 SPs for some of the
observed adjuncts: kill A0, kill A1, kill AM-
LOC, kill AM-MNR, kill AM-PNC and kill AM-
TMP.
Rather than coding the SPs directly as features,
we code the predictions instead, i.e. for each propo-
sition in the training and testing set, we check the
SPs for all the argument (and adjunct) headwords,
and the SP which best fits the headword (see below)
is the one that is selected. We codify the predicted
argument (or adjunct) label as features, and we insert
them among the corresponding argument features.
For instance, let?s assume that the word ?railway?
appears as the headword of a candidate argument of
?kill?. WordNet 1.6 yields the following hypernyms
for ?railway? (from most general to most specific, we
include the WordNet 1.6 concept numbers preceded
by their specifity level);
1 00001740 1 00017954
2 00009457 2 05962976
3 00011937 3 05997592
4 03600463 4 06004580
5 03243979 5 06008236
6 03526208 6 06005839
7 03208595 7 02927599
8 03209020
Note that we do not care about the sense ambigu-
ity and the explosion of concepts that it carries. Our
algorithm will check each of the hypernyms of rail-
way and match them with the concepts in the SPs of
?kill?, giving preference to the most specific concept.
In case that equally specific concepts match different
SPs, we will choose the SP that has the concept that
ranks highest in the SP, and code the SP feature with
the label of the SP where the match succeeds. In the
example, these are the most specific matches:
AM-LOC Con:03243979 Level:5 Ranking:32
A0 Con:06008236 Level:5 Ranking:209
There is a tie in the level, so we choose the one
with the highest rank. All in all, this means that ac-
cording to the learnt SPs we would predict that ?rail-
way? is a location feature for ?kill?, and we would
therefore insert the ?SP:AM-LOC? feature among
the argument features.
If ?railway? appears as the headword of other
verbs, the predicted argument might be different.
See for instance, the following verbs:
destroy:A1 Con:03243979 Level:5 Ranking:43
go:A0 Con:02927599 Level:7 Ranking:131
go:A2 Con:02927599 Level:7 Ranking:721
build:A1 Con:03209020 Level:8 Ranking:294
Note that our training examples did not contain
?railway? as an argument of any of these verbs, but
due to the SPs we are able to code into a feature that
?railway? belongs to a concrete semantic class which
contains conceptually similar headwords.
We decided to code the prediction of the SPs,
rather than the SPs themselves, in order to be more
robust to noise.
There is a further subtlety with our SP system. In
order to label training and testing sets in similar con-
ditions and avoid overfitting problems as much as
possible, we split the training set into five folds and
tagged each one with SPs learnt from the other four.
For extracting SP features from test set examples,
we use SPs learnt in the whole training set.
4 Experiments and Results
We participated in the ?close? and the ?open? tracks
with the same classification model, but using dif-
ferent training sets in each one. In the close track
we only use the provided training set, and in the
open, the CoNLL-2005 training set (without Verb-
Net classes or thematic roles).
Before our participation, we tested the system in
the CoNLL-2005 close track setting and it achieved
competitive performance in comparison to the state-
of-the-art results published in that challenge.
4.1 Semeval2007 setting
The data provided in the close track consists of the
propositions of 50 different verb lemmas from Prop-
Bank (sections 02-21). The data for the CoNLL-
2005 is also a subset of the PropBank data, but it
356
Track Label rank prec. rec. F1
Close VerbNet 1st 85.31 82.08 83.66
Close PropBank 1st 85.04 82.07 83.52
Open PropBank 1st 84.51 82.24 83.36
Table 1: Results in the SRL subtask of SemEval-
2007 task 17
includes all the propositions in sections 02-21 and
no VerbNet classes nor thematic roles for learning.
There is a total of 21 argument types for Prop-
Bank and 47 roles for VerbNet, which amounts to
21 ? 2 + 1 = 43 BIO-labels for PropBank predic-
tions and 47 ? 2 + 1 = 95 for VerbNet. We filtered
the less frequent (<5).
We trained the Maximum Entropy classifiers with
114,380 examples for the close track, and with
828,811 for the open track. We tuned the classifier
by setting the Exponential Gaussian prior in 0.1
4.2 Results
In the close track we trained two classifiers, one
to label PropBank numbered arguments and a sec-
ond to label VerbNet thematic roles. Due to lack
of time, we only trained the PropBank labels in the
open track. Table 1 shows the results obtained in the
SRL subtask. We ranked first in all of them, out of
two participants.
4.3 Discussion
The results indicate that in the close track the system
performs similarly on both PropBank arguments and
VerbNet roles. The absence of VerbNet class-based
features in the CoNLL-2005 training data could
cause the loss of performance in the open track. We
plan to perform the experiment on VerbNet roles for
the open track to check the ability of the classifier to
generalize across verbs.
Regarding the use of SP features, nowadays, we
have not obtained relevant improvements in the pre-
dictions of the classifiers. It is our first approach to
these kind of semantic features and there are more
sophisticated but evident extraction variants which
we are exploring.
Although the general performance is very simi-
lar without SP features, using them our system ob-
tains better results in ARG3 core arguments and in
the most frequent adjuncts such as location (LOC),
general-purpose (ADV) and temporal (TMP).
We reproduced this improvements in experiments
realized with CoNLL-2005 larger test sets. In that
case, we improved ARG3-ARG4 core arguments as
well as the mentioned adjuncts. There were more
examples to be classified and we get better overall
performance, but we need further experiments to be
more conclusive.
5 Conclusions
We have presented a sequential semantic role la-
beling system for the Semeval-2007 task 17 (SRL).
Based on Maximum Entropy Markov Models, it ob-
tains competitive and promising results. We also
have introduced semantic features extracted from
Selectional Restrictions but we only have prelimi-
nary evidence of their usefulness.
Acknowledgements
We thank David Martinez for kindly providing the
software that learnt the selectional preferences. This
work has been partially funded by the Spanish ed-
ucation ministry (KNOW). Ben?at is supported by a
PhD grant from the University of the Basque Coun-
try.
References
E. Agirre and D. Martinez. 2001. Learning class-to-class
selectional preferences. In Proceedings of CoNLL-
2001, Toulouse, France.
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hi-
erarchical recognition of propositional arguments with
perceptrons. In Proceedings of CoNLL 2004.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics , 28(3).
K. Kipper, Hoa Trang Dang, and M. Palmer. 2000.
Class-based construction of a verb lexicon. In Pro-
ceedings of AAAI-2000 Seventeenth National Confer-
ence on Artificial Intellingence, Austin, TX .
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics , 31(1).
M. Surdeanu, L. Ma`rquez, X. Carreras, and P. Comas.
(forthcoming). Combination strategies for semantic
role labeling. In Journal of Artificial Intelligence Re-
search.
N. Xue and M. Palmer. 2004. Calibrating features for se-
mantic role labeling. In Proceedings of EMNLP-2004 .
357
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 123?128,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 17: All-words Word Sense Disambiguation
on a Specific Domain
Eneko Agirre
IXA NLP group
UBC
Donostia, Basque Country
e.agirre@ehu.es
Oier Lopez de Lacalle
IXA NLP group
UBC
Donostia, Basque Country
oier.lopezdelacalle@ehu.es
Christiane Fellbaum
Department of Computer Science
Princeton University
Princeton, USA
fellbaum@princeton.edu
Andrea Marchetti
IIT
CNR
Pisa, Italy
andrea.marchetti@iit.cnr.it
Antonio Toral
ILC
CNR
Pisa, Italy
antonio.toral@ilc.cnr.it
Piek Vossen
Faculteit der Letteren
Vrije Universiteit Amsterdam
Amsterdam, Netherlands
p.vossen@let.vu.nl
Abstract
Domain portability and adaptation of NLP
components and Word Sense Disambiguation
systems present new challenges. The diffi-
culties found by supervised systems to adapt
might change the way we assess the strengths
and weaknesses of supervised and knowledge-
based WSD systems. Unfortunately, all ex-
isting evaluation datasets for specific domains
are lexical-sample corpora. With this paper
we want to motivate the creation of an all-
words test dataset for WSD on the environ-
ment domain in several languages, and present
the overall design of this SemEval task.
1 Introduction
Word Sense Disambiguation (WSD) competitions
have focused on general domain texts, as attested
in the last Senseval and Semeval competitions (Kil-
garriff, 2001; Mihalcea et al, 2004; Pradhan et al,
2007). Specific domains pose fresh challenges to
WSD systems: the context in which the senses occur
might change, distributions and predominant senses
vary, some words tend to occur in fewer senses in
specific domains, and new senses and terms might
be involved. Both supervised and knowledge-based
systems are affected by these issues: while the first
suffer from different context and sense priors, the
later suffer from lack of coverage of domain-related
words and information.
Domain adaptation of supervised techniques is a
hot issue in Natural Language Processing, includ-
ing Word Sense Disambiguation. Supervised Word
Sense Disambiguation systems trained on general
corpora are known to perform worse when applied
to specific domains (Escudero et al, 2000; Mart??nez
and Agirre, 2000), and domain adaptation tech-
niques have been proposed as a solution to this prob-
lem with mixed results.
Current research on applying WSD to specific do-
mains has been evaluated on three available lexical-
sample datasets (Ng and Lee, 1996; Weeber et al,
2001; Koeling et al, 2005). This kind of dataset
contains hand-labeled examples for a handful of se-
lected target words. As the systems are evaluated on
a few words, the actual performance of the systems
over complete texts can not be measured. Differ-
ences in behavior of WSD systems when applied to
lexical-sample and all-words datasets have been ob-
served on previous Senseval and Semeval competi-
tions (Kilgarriff, 2001; Mihalcea et al, 2004; Prad-
han et al, 2007): supervised systems attain results
on the high 80?s and beat the most frequent base-
line by a large margin for lexical-sample datasets,
but results on the all-words datasets were much more
modest, on the low 70?s, and a few points above the
most frequent baseline.
Thus, the behaviour of WSD systems on domain-
specific texts is largely unknown. While some words
could be supposed to behave in similar ways, and
thus be amenable to be properly treated by a generic
123
WSD algorithm, other words have senses closely
linked to the domain, and might be disambiguated
using purpose-built domain adaptation strategies (cf.
Section 4). While it seems that domain-specific
WSD might be a tougher problem than generic
WSD, it might well be that domain-related words
are easier to disambiguate.
The main goal of this task is to provide a mul-
tilingual testbed to evaluate WSD systems when
faced with full-texts from a specific domain, that of
environment-related texts. The paper is structured
as follows. The next section presents current lexi-
cal sample datasets for domain-specific WSD. Sec-
tion 3 presents some possible settings for domain
adaptation. Section 4 reviews the state-of-the art in
domain-specific WSD. Section 5 presents the design
of our task, and finally, Section 6 draws some con-
clusions.
2 Specific domain datasets available
We will briefly present the three existing datasets
for domain-related studies in WSD, which are all
lexical-sample.
The most commonly used dataset is the Defense
Science Organization (DSO) corpus (Ng and Lee,
1996), which comprises sentences from two differ-
ent corpora. The first is the Wall Street Journal
(WSJ), which belongs to the financial domain, and
the second is the Brown Corpus (BC) which is a bal-
anced corpora of English usage. 191 polysemous
words (nouns and verbs) of high frequency in WSJ
and BC were selected and a total of 192,800 occur-
rences of these words were tagged with WordNet 1.5
senses, more than 1,000 instances per word in aver-
age. The examples from BC comprise 78,080 oc-
currences of word senses, and examples from WSJ
consist on 114,794 occurrences. In domain adapta-
tion experiments, the Brown Corpus examples play
the role of general corpora, and the examples from
the WSJ play the role of domain-specific examples.
Koeling et al (2005) present a corpus were the
examples are drawn from the balanced BNC cor-
pus (Leech, 1992) and the SPORTS and FINANCES
sections of the newswire Reuters corpus (Rose et al,
2002), comprising around 300 examples (roughly
100 from each of those corpora) for each of the 41
nouns. The nouns were selected because they were
salient in either the SPORTS or FINANCES domains,
or because they had senses linked to those domains.
The occurrences were hand-tagged with the senses
from WordNet version 1.7.1 (Fellbaum, 1998). In
domain adaptation experiments the BNC examples
play the role of general corpora, and the FINANCES
and SPORTS examples the role of two specific do-
main corpora.
Finally, a dataset for biomedicine was developed
by Weeber et al (2001), and has been used as
a benchmark by many independent groups. The
UMLS Metathesaurus was used to provide a set of
possible meanings for terms in biomedical text. 50
ambiguous terms which occur frequently in MED-
LINE were chosen for inclusion in the test set. 100
instances of each term were selected from citations
added to the MEDLINE database in 1998 and man-
ually disambiguated by 11 annotators. Twelve terms
were flagged as ?problematic? due to substantial dis-
agreement between the annotators. In addition to the
meanings defined in UMLS, annotators had the op-
tion of assigning a special tag (?none?) when none
of the UMLS meanings seemed appropriate.
Although these three corpora are useful for WSD
research, it is difficult to infer which would be the
performance of a WSD system on full texts. The
corpus of Koeling et al, for instance, only includes
words which where salient for the target domains,
but the behavior of WSD systems on other words
cannot be explored. We would also like to note that
while the biomedicine corpus tackles scholarly text
of a very specific domain, the WSJ part of the DSO
includes texts from a financially oriented newspaper,
but also includes news of general interest which have
no strict relation to the finance domain.
3 Possible settings for domain adaptation
When performing supervised WSD on specific do-
mains the first setting is to train on a general domain
data set and to test on the specific domain (source
setting). If performance would be optimal, this
would be the ideal solution, as it would show that a
generic WSD system is robust enough to tackle texts
from new domains, and domain adaptation would
not be necessary.
The second setting (target setting) would be to
train the WSD systems only using examples from
124
the target domain. If this would be the optimal set-
ting, it would show that there is no cost-effective
method for domain adaptation. WSD systems would
need fresh examples every time they were deployed
in new domains, and examples from general do-
mains could be discarded.
In the third setting, the WSD system is trained
with examples coming from both the general domain
and the specific domain. Good results in this setting
would show that supervised domain adaptation is
working, and that generic WSD systems can be sup-
plemented with hand-tagged examples from the tar-
get domain.
There is an additional setting, where a generic
WSD system is supplemented with untagged exam-
ples from the domain. Good results in this setting
would show that semi-supervised domain adapta-
tion works, and that generic WSD systems can be
supplemented with untagged examples from the tar-
get domain in order to improve their results.
Most of current all-words generic supervised
WSD systems take SemCor (Miller et al, 1993) as
their source corpus, i.e. they are trained on SemCor
examples and then applied to new examples. Sem-
Cor is the largest publicly available annotated cor-
pus. It?s mainly a subset of the Brown Corpus, plus
the novel The Red Badge of Courage. The Brown
corpus is balanced, yet not from the general domain,
as it comprises 500 documents drawn from differ-
ent domains, each approximately 2000 words long.
Although the Brown corpus is balanced, SemCor is
not, as the documents were not chosen at random.
4 State-of-the-art in WSD for specific
domains
Initial work on domain adaptation for WSD sys-
tems showed that WSD systems were not able to
obtain better results on the source or adaptation set-
tings compared to the target settings (Escudero et
al., 2000), showing that a generic WSD system (i.e.
based on hand-annotated examples from a generic
corpus) would not be useful when moved to new do-
mains.
Escudero et al (2000) tested the supervised adap-
tation scenario on the DSO corpus, which had exam-
ples from the Brown Corpus and Wall Street Journal
corpus. They found that the source corpus did not
help when tagging the target corpus, showing that
tagged corpora from each domain would suffice, and
concluding that hand tagging a large general corpus
would not guarantee robust broad-coverage WSD.
Agirre and Mart??nez (2000) used the same DSO cor-
pus and showed that training on the subset of the
source corpus that is topically related to the target
corpus does allow for domain adaptation, obtaining
better results than training on the target data alone.
In (Agirre and Lopez de Lacalle, 2008), the au-
thors also show that state-of-the-art WSD systems
are not able to adapt to the domains in the context
of the Koeling et al (2005) dataset. While WSD
systems trained on the target domain obtained 85.1
and 87.0 of precision on the sports and finances do-
mains, respectively, the same systems trained on the
BNC corpus (considered as a general domain cor-
pus) obtained 53.9 and 62.9 of precision on sports
and finances, respectively. Training on both source
and target was inferior that using the target examples
alone.
Supervised adaptation
Supervised adaptation for other NLP tasks has been
widely reported. For instance, (Daume? III, 2007)
shows that a simple feature augmentation method
for SVM is able to effectively use both labeled tar-
get and source data to provide the best domain-
adaptation results in a number of NLP tasks. His
method improves or equals over previously explored
more sophisticated methods (Daume? III and Marcu,
2006; Chelba and Acero, 2004). In contrast, (Agirre
and Lopez de Lacalle, 2009) reimplemented this
method and showed that the improvement on WSD
in the (Koeling et al, 2005) data was marginal.
Better results have been obtained using purpose-
built adaptation methods. Chan and Ng (2007) per-
formed supervised domain adaptation on a manu-
ally selected subset of 21 nouns from the DSO cor-
pus. They used active learning, count-merging, and
predominant sense estimation in order to save tar-
get annotation effort. They showed that adding just
30% of the target data to the source examples the
same precision as the full combination of target and
source data could be achieved. They also showed
that using the source corpus significantly improved
results when only 10%-30% of the target corpus
was used for training. In followup work (Zhong et
125
Projections for 2100 suggest that temperature in Europe will have risen by between 2 to 6.3 C above 1990
levels. The sea level is projected to rise, and a greater frequency and intensity of extreme weather events are
expected. Even if emissions of greenhouse gases stop today, these changes would continue for many decades
and in the case of sea level for centuries. This is due to the historical build up of the gases in the atmosphere
and time lags in the response of climatic and oceanic systems to changes in the atmospheric concentration
of the gases.
Figure 1: Sample text from the environment domain.
al., 2008), the feature augmentation approach was
combined with active learning and tested on the
OntoNotes corpus, on a large domain-adaptation ex-
periment. They significantly reduced the effort of
hand-tagging, but only obtained positive domain-
adaptation results for smaller fractions of the target
corpus.
In (Agirre and Lopez de Lacalle, 2009) the au-
thors report successful adaptation on the (Koeling
et al, 2005) dataset on supervised setting. Their
method is based on the use of unlabeled data, re-
ducing the feature space with SVD, and combina-
tion of features using an ensemble of kernel meth-
ods. They report 22% error reduction when using
both source and target data compared to a classifier
trained on target the target data alone, even when the
full dataset is used.
Semi-supervised adaptation
There are less works on semi-supervised domain
adaptation in NLP tasks, and fewer in WSD task.
Blitzer et al (2006) used Structural Correspondence
Learning and unlabeled data to adapt a Part-of-
Speech tagger. They carefully select so-called pivot
features to learn linear predictors, perform SVD on
the weights learned by the predictor, and thus learn
correspondences among features in both source and
target domains. Agirre and Lopez de Lacalle (2008)
show that methods based on SVD with unlabeled
data and combination of distinct feature spaces pro-
duce positive semi-supervised domain adaptation re-
sults for WSD.
Unsupervised adaptation
In this context, we take unsupervised to mean
Knowledge-Based methods which do not require
hand-tagged corpora. The predominant sense acqui-
sition method was succesfully applied to specific do-
mains in (Koeling et al, 2005). The methos has two
steps: In the first, a corpus of untagged text from the
target domain is used to construct a thesaurus of sim-
ilar words. In the second, each target word is disam-
biguated using pairwise WordNet-based similarity
measures, taking as pairs the target word and each of
the most related words according to the thesaurus up
to a certain threshold. This method aims to obtain,
for each target word, the sense which is the most
predominant for the target corpus. When a general
corpus is used, the most predominant sense in gen-
eral is obtained, and when a domain-specific corpus
is used, the most predominant sense for that corpus
is obtained (Koeling et al, 2005). The main motiva-
tion of the authors is that the most frequent sense is a
very powerful baseline, but it is one which requires
hand-tagging text, while their method yields simi-
lar information automatically. The results show that
they are able to obtain good results. In related work,
(Agirre et al, 2009) report improved results using
the same strategy but applying a graph-based WSD
method, and highlight the domain-adaptation poten-
tial of unsupervised knowledge-based WSD systems
compared to supervised WSD.
5 Design of the WSD-domain task
This task was designed in the context of Ky-
oto (Piek Vossen and VanGent, 2008)1, an Asian-
European project that develops a community plat-
form for modeling knowledge and finding facts
across languages and cultures. The platform op-
erates as a Wiki system with an ontological sup-
port that social communities can use to agree on the
meaning of terms in specific domains of their inter-
est. Kyoto will focus on the environmental domain
because it poses interesting challenges for informa-
tion sharing, but the techniques and platforms will
be independent of the application domain. Kyoto
1http://www.kyoto-project.eu/
126
will make use of semantic technologies based on
ontologies and WSD in order to extract and repre-
sent relevant information for the domain, and is thus
interested on measuring the performance of WSD
techniques on this domain.
The WSD-domain task will comprise comparable
all-words test corpora on the environment domain.
Texts from the European Center for Nature Con-
servation2 and Worldwide Wildlife Forum3 will be
used in order to build domain specific test corpora.
We will select documents that are written for a gen-
eral but interested public and that involve specific
terms from the domain. The document content will
be comparable across languages. Figure 1 shows an
example in English related to global warming.
The data will be available in a number of lan-
guages: English, Dutch, Italian and Chinese. The
sense inventories will be based on wordnets of the
respective languages, which will be updated to in-
clude new vocabulary and senses. The test data will
comprise three documents of around 2000 words
each for each language. The annotation procedure
will involve double-blind annotation plus adjudica-
tion, and inter-tagger agreement data will be pro-
vided. The formats and scoring software will fol-
low those of Senseval-34 and SemEval-20075 En-
glish all-words tasks.
There will not be training data available, but par-
ticipants are free to use existing hand-tagged cor-
pora and lexical resources (e.g. SemCor and pre-
vious Senseval and SemEval data). We plan to make
available a corpus of documents from the same do-
main as the selected documents, as well as wordnets
updated to include the terms and senses in the se-
lected documents.
6 Conclusions
Domain portability and adaptation of NLP com-
ponents and Word Sense Disambiguation systems
present new challenges. The difficulties found by
supervised systems to adapt might change the way
we assess the strengths and weaknesses of super-
vised and knowledge-based WSD systems. Unfor-
tunately, all existing evaluation datasets for specific
2http://www.ecnc.org
3http://www.wwf.org
4http://www.senseval.org/senseval3
5http://nlp.cs.swarthmore.edu/semeval/
domains are lexical-sample corpora. With this paper
we have motivated the creation of an all-words test
dataset for WSD on the environment domain in sev-
eral languages, and presented the overall design of
this SemEval task.
Further details can be obtained from the Semeval-
20106 website, our task website7, and in our distri-
bution list8
7 Acknowledgments
The organization of the task is partially funded
by the European Commission (KYOTO FP7 ICT-
2007-211423) and the Spanish Research Depart-
ment (KNOW TIN2006-15049-C03-01).
References
Eneko Agirre and Oier Lopez de Lacalle. 2008. On ro-
bustness and domain adaptation using SVD for word
sense disambiguation. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 17?24, Manchester, UK, August.
Coling 2008 Organizing Committee.
Eneko Agirre and Oier Lopez de Lacalle. 2009. Super-
vised domain adaptation for wsd. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-09).
E. Agirre, O. Lopez de Lacalle, and A. Soroa. 2009.
Knowledge-based WSD and specific domains: Per-
forming over supervised WSD. In Proceedings of IJ-
CAI, Pasadena, USA.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
49?56, Prague, Czech Republic, June. Association for
Computational Linguistics.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy classifier: Little data can help a
lot. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Barcelona, Spain.
6http://semeval2.fbk.eu/
7http://xmlgroup.iit.cnr.it/SemEval2010/
8http://groups.google.com/groups/wsd-domain
127
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Gerard Escudero, Lluiz Ma?rquez, and German Rigau.
2000. An Empirical Study of the Domain Dependence
of Supervised Word Sense Disambiguation Systems.
Proceedings of the joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora, EMNLP/VLC.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
A. Kilgarriff. 2001. English Lexical Sample Task De-
scription. In Proceedings of the Second International
Workshop on evaluating Word Sense Disambiguation
Systems, Toulouse, France.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense
acquisition. In Proceedings of the Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing.
HLT/EMNLP, pages 419?426, Ann Arbor, Michigan.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
David Mart??nez and Eneko Agirre. 2000. One Sense per
Collocation and Genre/Topic Variations. Conference
on Empirical Method in Natural Language.
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004.
The Senseval-3 English lexical sample task. In Pro-
ceedings of the 3rd ACL workshop on the Evaluation
of Systems for the Semantic Analysis of Text (SENSE-
VAL), Barcelona, Spain.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A Semantic Concordance. In Proceedings of the
ARPA Human Language Technology Workshop. Dis-
tributed as Human Language Technology by San Ma-
teo, CA: Morgan Kaufmann Publishers., pages 303?
308, Princeton, NJ.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the Association for
Computationla Linguistics (ACL), pages 40?47.
Nicoletta Calzolari Christiane Fellbaum Shu-kai Hsieh
Chu-Ren Huang Hitoshi Isahara Kyoko Kanzaki An-
drea Marchetti Monica Monachini Federico Neri
Remo Raffaelli German Rigau Maurizio Tescon
Piek Vossen, Eneko Agirre and Joop VanGent. 2008.
Kyoto: a system for mining, structuring and distribut-
ing knowledge across languages and cultures. In
European Language Resources Association (ELRA),
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, may.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of
the Fourth International Workshop on Semantic Eval-
uations (SemEval-2007), pages 87?92, Prague, Czech
Republic.
Tony G. Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volumen 1: from Yes-
terday?s News to Tomorrow?s Language Resources.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC-2002),
pages 827?832, Las Palmas, Canary Islands.
Marc Weeber, James G. Mork, and Alan R. Aronson.
2001. Developing a test collection for biomedical
word sense disambiguation. In Proceedings of the
AMAI Symposium, pages 746?750, Washington, DC.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An em-
pirical study. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1002?1010, Honolulu, Hawaii, October.
Association for Computational Linguistics.
128
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 41?49,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
WikiWalk: Random walks on Wikipedia for Semantic Relatedness
Eric Yeh, Daniel Ramage,
Christopher D. Manning
Computer Science Department,
Stanford University
Stanford, CA, USA
{yeh1,dramage,manning}@cs.stanford.edu
Eneko Agirre, Aitor Soroa
Ixa Taldea
University of the Basque Country
Donostia, Basque Country
{e.agirre,a.soroa}@ehu.es
Abstract
Computing semantic relatedness of natural
language texts is a key component of tasks
such as information retrieval and sum-
marization, and often depends on knowl-
edge of a broad range of real-world con-
cepts and relationships. We address this
knowledge integration issue by comput-
ing semantic relatedness using person-
alized PageRank (random walks) on a
graph derived from Wikipedia. This pa-
per evaluates methods for building the
graph, including link selection strategies,
and two methods for representing input
texts as distributions over the graph nodes:
one based on a dictionary lookup, the
other based on Explicit Semantic Analy-
sis. We evaluate our techniques on stan-
dard word relatedness and text similarity
datasets, finding that they capture similar-
ity information complementary to existing
Wikipedia-based relatedness measures, re-
sulting in small improvements on a state-
of-the-art measure.
1 Introduction
Many problems in NLP call for numerical mea-
sures of semantic relatedness, including document
summarization, information retrieval, and textual
entailment. Often, measuring the relatedness of
words or text passages requires world knowledge
about entities and concepts that are beyond the
scope of any single word in the document. Con-
sider, for instance, the following pair:
1. Emancipation Proclamation
2. Gettysburg Address
To correctly assess that these examples are re-
lated requires knowledge of the United States Civil
War found neither in the examples themselves nor
in traditional lexical resources such as WordNet
(Fellbaum, 1998). Fortunately, a massive collabo-
ratively constructed knowledge resource is avail-
able that has specific articles dedicated to both.
Wikipedia is an online encyclopedia containing
around one million articles on a wide variety of
topics maintained by over one hundred thousand
volunteer editors with quality comparable to that
of traditional encyclopedias.
Recent work has shown that Wikipedia can be
used as the basis of successful measures of se-
mantic relatedness between words or text pas-
sages (Strube and Ponzetto, 2006; Gabrilovich and
Markovitch, 2007; Milne and Witten, 2008). The
most successful measure, Explicit Semantic Anal-
ysis (ESA) (Gabrilovich and Markovitch, 2007),
treats each article as its own dimension in a vec-
tor space. Texts are compared by first projecting
them into the space of Wikipedia articles and then
comparing the resulting vectors.
In addition to article text, Wikipedia stores a
great deal of information about the relationships
between the articles in the form of hyperlinks, info
boxes, and category pages. Despite a long his-
tory of research demonstrating the effectiveness
of incorporating link information into relatedness
measures based on the WordNet graph (Budanit-
sky and Hirst, 2006), previous work on Wikipedia
has made limited use of this relationship infor-
mation, using only category links (Bunescu and
Pasca, 2006) or just the actual links in a page
(Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008).
In this work, we combine previous approaches
by converting Wikipedia into a graph, mapping in-
put texts into the graph, and performing random
walks based on Personalized PageRank (Haveli-
wala, 2002) to obtain stationary distributions that
characterize each text. Semantic relatedness be-
tween two texts is computed by comparing their
distributions. In contrast to previous work, we
explore the use of all these link types when con-
41
structing the Wikipedia graph, the intuition being
these links, or some combination of them, con-
tain additional information that would allow a gain
over methods that use only just the article text. We
also discuss two methods for performing the initial
mapping of input texts to the graph, using tech-
niques from previous studies that utilized Word-
Net graphs and Wikipedia article text.
We find that performance is signficantly af-
fected by the strategy used to initialize the graph
walk, as well as the links selected when con-
structing the Wikipedia graph. Our best system
combines an ESA-initialized vector with random
walks, improving on state-of-the-art results over
the (Lee et al, 2005) dataset. An analysis of
the output demonstrates that, while the gains are
small, the random walk adds complementary re-
latedness information not present in the page text.
2 Preliminaries
A wide range of different methods, from corpus-
based distributional similarity methods, such as
Latent Semantic Analysis (Landauer et al, 1998),
to knowledge-based ones that employ structured
sources such as WordNet,
1
have been developed
to score semantic relatedness and similarity. We
now review two leading techniques which we use
as starting points for our approach: those that per-
form random walks over WordNet?s graph struc-
ture, and those that utilize Wikipedia as an under-
lying data source.
2.1 Random Graph Walks for Semantic
Relatedness
Some of the best performing WordNet-based al-
gorithms for computing semantic relatedness are
based on the popular Personalized PageRank al-
gorithm (Hughes and Ramage, 2007; Agirre and
Soroa, 2009). These approaches start by taking
WordNet as a graph of concepts G = (V,E) with
a set of vertices V derived from WordNet synsets
and a set of edges E representing relations be-
tween synsets. Both algorithms can be viewed
as random walk processes that postulate the ex-
istence of a particle that randomly traverses the
graph, but at any time may jump, or teleport, to
a new vertex with a given teleport probability. In
standard PageRank (Brin and Page, 1998), this tar-
get is chosen uniformly, whereas for Personalized
1
See (Budanitsky and Hirst, 2006) for a survey.
PageRank it is chosen from a nonuniform distribu-
tion of nodes, specified by a teleport vector.
The final weight of node i represents the propor-
tion of time the random particle spends visiting it
after a sufficiently long time, and corresponds to
that node?s structural importance in the graph. Be-
cause the resulting vector is the stationary distri-
bution of a Markov chain, it is unique for a par-
ticular walk formulation. As the teleport vector
is nonuniform, the stationary distribution will be
biased towards specific parts of the graph. In the
case of (Hughes and Ramage, 2007) and (Agirre
and Soroa, 2009), the teleport vector is used to re-
flect the input texts to be compared, by biasing the
stationary distribution towards the neighborhood
of each word?s mapping.
The computation of relatedness for a word pair
can be summarized in three steps: First, each input
word is mapped with to its respective synsets in
the graph, creating its teleport vector. In the case
words with multiple synsets (senses), the synsets
are weighted uniformly. Personalized PageRank is
then executed to compute the stationary distribu-
tion for each word, using their respective teleport
vectors. Finally, the stationary distributions for
each word pair are scored with a measure of vector
similarity, such as cosine similarity. The method
to compute relatedness for text pairs is analogous,
with the only difference being in the first step all
words are considered, and thus the stationary dis-
tribution is biased towards all synsets of the words
in the text.
2.2 Wikipedia as a Semantic Resource
Recent Wikipedia-based lexical semantic related-
ness approaches have been found to outperform
measures based on the WordNet graph. Two such
methods stand out: Wikipedia Link-based Mea-
sure (WLM) (Milne and Witten, 2008), and Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007).
WLM uses the anchors found in the body of
Wikipedia articles, treating them as links to other
articles. Each article is represented by a list of
its incoming and outgoing links. For word relat-
edness, the set of articles are first identified by
matching the word to the text in the anchors, and
the score is derived using several weighting strate-
gies applied to the overlap score of the articles?
links. WLM does not make further use of the link
graph, nor does it attempt to differentiate the links.
42
In contrast to WLM, Explicit Semantic Analy-
sis (ESA) is a vector space comparison algorithm
that does not use the link structure, relying solely
on the Wikipedia article text. Unlike Latent Se-
mantic Analysis (LSA), the underlying concept
space is not computationally derived, but is instead
based on Wikipedia articles. For a candidate text,
each dimension in its ESA vector corresponds to
a Wikipedia article, with the score being the sim-
ilarity of the text with the article text, subject to
TF-IDF weighting. The relatedness of two texts
is computed as the cosine similarity of their ESA
vectors.
Although ESA reports the best results to date
on both the WordSim-353 dataset as well as the
Lee sentence similarity dataset, it does not utilize
the link structure, which motivated a combined ap-
proach as follows.
2.3 A Combined Approach
In this work, we base our random walk algorithms
after the ones described in (Hughes and Ramage,
2007) and (Agirre et al, 2009), but use Wikipedia-
based methods to construct the graph. As in previ-
ous studies, we obtain a relatedness score between
a pair of texts by performing random walks over
a graph to compute a stationary distribution for
each text. For our evaluations, the score is simply
the cosine similarity between the distributions. In
the following sections, we describe how we built
graphs from Wikipedia, and how input texts are
initially mapped into these structures.
3 Building a Wikipedia Graph
In order to obtain the graph structure of Wikipedia,
we simply treat the articles as vertices, and
the links between articles as the edges. There
are several sources of pre-processed Wikipedia
dumps which could be used to extract the arti-
cles and links between articles, including DBpe-
dia (Auer et al, 2008), which provides a rela-
tional database representation of Wikipedia, and
Wikipedia-Miner
2
, which produces similar infor-
mation from Wikipedia dumps directly. In this
work we used a combination of Wikipedia-Miner
and custom processing scripts. The dump used in
this work is from mid 2008.
As in (Milne and Witten, 2008), anchors in
Wikipedia articles are used to define links between
2
http://wikipedia-miner.sourceforge.net
articles. Because of different distributional proper-
ties, we explicitly distinguish three types of links,
in order to explore their impact on the graph walk.
Infobox links are anchors found in the infobox
section of Wikipedia articles. Article in-
foboxes, when present, often enumerate
defining attributes and characteristics for that
article?s topic.
Categorical links reference articles whose titles
belong in the Wiki namespace ?Category,?
as well as those with titles beginning with
?List of.? These pages are often just lists of
anchors to other articles, which may be use-
ful for capturing categorical information that
roughly contains a mixture of hyponymy and
meronymy relations between articles.
Content links are those that are not already clas-
sified as infobox nor categorical, and are in-
tended to represent the set of miscellaneous
anchors found solely in the article body.
These may include links already found in the
categorical and infobox categories.
Links can be further factored out according to
generality, a concept introduced in (Gabrilovich
and Markovitch, 2009). We say that one article
is more general than another when the number of
inlinks is larger. Although only a rough heuris-
tic, the intuition is that articles on general top-
ics will receive many links, whereas specific ar-
ticles will receive fewer. We will use +k notation
for links which point to more general articles, i.e.,
where the difference in generality between source
s and target t is #inlink(t)/#inlink(s) ? k.
We will use ?k for links to less general articles,
i.e., #inlink(s)/#inlink(t) ? k. Finally we
use =k when the generality is in the same order
of magnitude, i.e., when the link is neither +k
nor ?k. The original notion of generality from
(Gabrilovich and Markovitch, 2009) restricts con-
sideration to only more general articles by one or-
der of magnitude (+10), without reference to the
link types introduced above.
Given the size of the Wikipedia graph, we ex-
plored further methods inspired by (Gabrilovich
and Markovitch, 2009) to make the graph smaller.
We discarded articles with fewer than 2,000 non-
stop words and articles with fewer than 5 outgoing
and incoming links. We will refer to the complete
43
graph as full and to this reduced graph as reduced.
3
4 Initializing a Wikipedia Graph Walk
In order to apply Personalized PageRank to a
given passage of text or word, we need to con-
struct a custom teleport vector, representing the
initial distribution of mass over the article nodes.
In this section we introduce two such methods,
one based on constructing a direct mapping from
individual words to Wikipedia articles (which we
call dictionary-based initialization), and the other
based directly on the results of ESA. We will see
each technique in turn.
4.1 Dictionary based initialization
Given a target word, we would like to define
its teleport vector using the set of articles in
Wikipedia to which the word refers. This is analo-
gous to a dictionary, where an entry lists the set of
meanings pertaining to the entry.
We explored several methods for building such
a dictionary. The first method constructed the dic-
tionary using the article title directly, while also
including redirection pages and disambiguation
pages for additional ways to refer to the article. In
addition, we can use the anchor text to refer to arti-
cles, and we turned to Wikipedia-Miner to extract
this information. Anchors are indeed a rich source
of information, as they help to relate similar words
to Wikipedia articles. For instance, links to page
Monk are created by using textual anchors such as
lama, brothers, monastery, etc. As a result, the
dictionary entries for those words will have a link
to the Monk page. This information turned out to
be very valuable, so all experiments have been car-
ried out using anchors.
An additional difficulty was that any of these
methods yielded dictionaries where the entries
could refer to tens, even hundreds of articles. In
most of the cases we could see that relevant arti-
cles were followed by a long tail of loosely related
articles. We tried two methods to prune the dic-
tionary. The first, coarse, method was to eliminate
all articles whose title contains a space. The mo-
tivation was that our lexical semantic relatedness
datasets (cf. Section 5) do not contain multiword
entries (e.g., United States). In the second method,
we pruned articles from the dictionary which ac-
3
In order to keep category and infobox links, the 2,000
non-stop word filter was not applied to categories and lists of
pages.
Graphs
Graph # Vertices # Edges
Full 2,483,041 49,602,752
Reduced 1,002,411 30,939,288
Dictionaries
Dictionary # Entries Avg. Articles
all 6,660,315 1.31
1% 6,660,306 1.12
1% noent 1,058,471 1.04
Table 1: Graph and dictionary sizes. Avg. Articles
column details the average number of articles per
entry.
counted for less than 1% or 10% of the occur-
rences of that anchor word, as suggested by (Milne
and Witten, 2008).
In short, for this method of initialization, we ex-
plored the use of the following variants: all, all ar-
ticles are introduced in the dictionary; noent, arti-
cles with space characters are omitted; 1% (10%),
anchors that account for less than 1% (10%) of the
total number of anchors for that entry are omitted.
We did not use stemming. If a target word has no
matching Wikipedia article in the dictionary, then
it is ignored.
Table 1 shows the numbers for some graph and
dictionary versions. Although the average number
of articles per entry in the dictionary might seem
low, it is actually quite high for the words in the
datasets: for MC it?s 5.92, and for wordsim353 it?s
42.14. If we keep the articles accounting for 10%
of all occurrences, the numbers drops drastically
to 1.85 and 1.64 respectively.
As we will see in the results section, smaller
graphs and dictionaries are able to attain higher
results, but at the cost of losing information for
some words. That is, we observed that some fac-
tored, smaller graphs contained less noise, but that
meant that some articles and words are isolated in
the graph, and therefore we are not able to com-
pute relatedness for them. As a solution, we de-
vised an alternative way to initialize the random
walk. Instead of initializing it according to the ar-
ticles in the dictionary, we initialized it with the
vector weights returned by ESA, as explained in
the next section.
44
4.2 Initialization with ESA
In addition to the dictionary based approach, we
also explored the use of ESA to construct the tele-
port vector. In contrast to dictionary initialization,
ESA uses the text of the article body instead of an-
chor text or the article titles. Because ESA maps
query text to a weighted vector of Wikipedia arti-
cles, it can be naturally adapted as a teleport vector
for a random walk with a simple L
1
normaliza-
tion. We used Apache Lucene
4
to implement both
ESA?s repository of Wikipedia articles, and to re-
turn vectors for queries. Each article is indexed as
its own document, with page text preprocessed to
strip out Wiki markup.
Although we followed the steps outlined in
(Gabrilovich and Markovitch, 2007), we had to
add an extension to the algorithm: for a return
vector from ESA, we order the articles by score,
and retain only the scores for the top-n articles,
setting the scores of the remaining articles to 0.
Without this modification, our performance results
were will below the reported numbers, but with a
cutoff at 625 (determined by a basic grid search),
we obtained a correlation of 0.76 on the Lee sen-
tence similarity dataset, over the previously pub-
lished score of 0.72.
4.3 Teleport Probability
For this work, we used a value of 0.15 as the prob-
ability of returning to the teleport distribution at
any given step. The walk terminates when the vec-
tor converges with an L
1
error of 0.0001 (circa 30
iterations). Some preliminary experiments on a re-
lated Word Sense Disambiguation task indicated
that in this context, our algorithm is quite robust to
these values, and we did not optimize them. How-
ever, we will discuss using different return param-
eters in Section 6.1.
5 Experiments
In this section, we compare the two methods of
initialization as well as several types of edges. For
a set of pairs, system performance is evaluated by
how well the generated scores correlate with the
gold scores. Gold scores for each pair are the av-
erage of human judgments for that pair. In order to
compare against previous results obtained on the
datasets, we use the Spearman correlation coeffi-
cient on the Miller Charles (MC) and WordSim-
353 word-pair datasets, and the Pearson correla-
4
http://lucene.apache.org
Dictionary Graph MC
all full 0.369
1% full 0.610
1%, noent full 0.565 (0.824)
1% reduced 0.563
1% reduced +2 0.530
1% reduced +4 0.601
1% reduced +8 0.512
1% reduced +10 0.491 (0.522)
10% full 0.604 (0.750)
10% reduced 0.605 (0.751)
10% reduced +2 0.491 (0.540)
10% reduced +4 0.476 (0.519)
10% reduced +8 0.474 (0.506)
10% reduced +10 0.430 (0.484)
WordNet 0.90 / 0.89
WLM 0.70
ESA 0.72
Table 2: Spearman correlation on the MC dataset
with dictionary-based initialization. Refer to Sec-
tion 3 for explanation of dictionary and graph
building methods. Between parenthesis, results
excluding pairs which had a word with an empty
dictionary entry.
tion coefficient on the (Lee et al, 2005) document-
pair dataset.
5.1 Dictionary-based Initialization
Given the smaller size of the MC dataset, we
explored the effect of the different variants to
build the graph and dictionary on this dataset.
Some selected results are shown in Table 2, along-
side those of related work, where we used Word-
Net for (Hughes and Ramage, 2007) and (Agirre
et al, 2009) (separated by ?/? in the results),
WLM for (Milne and Witten, 2008) and ESA for
(Gabrilovich and Markovitch, 2007).
We can observe that using the full graph and
dictionaries yields very low results. Reducing the
dictionary (removing articles with less than 1% or
10% of the total occurrences) produces higher re-
sults, but reducing the graph does not provide any
improvement. On a closer look, we realized that
pruning the dictionary to 10% or removing multi-
words (noent) caused some words to not get any
link to articles (e.g., magician). If we evaluate
only over pairs where both words get a Personal-
ized PageRank vector, the results raise up to 0.751
and 0.824, respectively, placing our method close
45
Dictionary Graph WordSim-353
1% full 0.449
1%, noent full 0.440 (0.634)
1% reduced 0.485
WordNet 0.55 / 0.66
WLM 0.69
ESA 0.75
WikiRelate 0.50
Table 3: Spearman correlation on the WordSim-
353 dataset with dictionary-based initialization.
Refer to Section 3 for explanation of dictionary
and graph building methods. Between parenthe-
sis, results excluding pairs which had a word with
an empty dictionary entry.
Dictionary Graph (Lee et al, 2005)
1%, noent Full 0.308
1% Reduced +4 0.269
ESA 0.72
Table 4: Pearson correlation on (Lee et al, 2005)
with dictionary-based initialization. Refer to Sec-
tion 3 for explanation of dictionary and graph
building methods.
to the best results on the MC dataset. This came
at the cost of not being able to judge the related-
ness of 3 and 5 pairs, respectively. We think that
removing multiwords (noent) is probably too dras-
tic, but the positive effect is congruent with (Milne
and Witten, 2008), who suggested that the cover-
age of certain words in Wikipedia is not adequate.
The results in Table 3 show the Spearman cor-
relation for some selected runs over the WordSim-
353 dataset. Again we see that a restrictive dic-
tionary allows for better results on the pairs which
do get a dictionary entry, up to 0.63. WikiRelate
refers to the results in (Strube and Ponzetto, 2006).
We only tested a few combinations over (Lee et
al., 2005), with results given in Table 4. These are
well below state-of-the-art, and show that initial-
izing the random walk with all words in the doc-
ument does not characterize the documents well,
resulting in low correlation.
5.2 ESA-based initialization
While the results using a dictionary based ap-
proach were encouraging, they did not come close
to the state-of-the-art results achieved by ESA.
Here, we explore combining ESA and random
Method Text Sim
ESA@625 0.766
ESA@625+Walk All 0.556
ESA@625+Walk Categories 0.410
ESA@625+Walk Content 0.536
ESA@625+Walk Infobox 0.710
Table 5: Pearson correlation on the (Lee et al,
2005) dataset when walking on various types of
links. Note that walking tends to hurt performance
overall, with Infobox links by far the least harm-
ful.
walks, by using ESA to initialize the teleport vec-
tor. Following section 4.2, we used a top-n cutoff
of 625.
Table 5 displays the results of our ESA im-
plementation followed by a walk from that ESA
distribution. Walking on any link type actually
depresses performance below the baseline ESA
value, although the Infobox links seem the least
harmful.
However, as mentioned in Section 3, links be-
tween articles represent many different types of
relationships beyond the few well-defined links
present in lexical resources like WordNet. This
also extends to where the link is found, and the ar-
ticle it is pointing to. As such, not all links are cre-
ated equal, and we expect that some types of links
at different levels of generality will perform bet-
ter or worse than others. Table 6 presents a sam-
ple grid search across the category links choosing
more general, less general, or similar generality at
several factors of k, showing that there is a consis-
tent pattern across multiple link types. Note that
the best value indeed improves upon the score of
the ESA distribution, albeit modestly.
We performed a similar analysis across all link
types and found that the best link types were Cat-
egory links at +6 and Infobox links at =2. Intu-
itively, these link types make sense: for seman-
tic relatedness, it seem reasonable to expect more
general pages within the same category to help.
And for Infobox links, much rarer and much more
common pages can both introduce their own kind
of noise. While the improvement from each type
of edge walk is small, they are additive?the best
results on the sentence similarity dataset was from
walking across both link types. Our final Pearson
correlation coefficient of .772 is to our knowledge
the highest number reported in the literature, al-
46
Generality of Category links
+k -k =k
k = 2 0.760 0.685 0.462
k = 4 0.766 0.699 0.356
k = 6 0.771 0.729 0.334
k = 8 0.768 0.729 0.352
k = 10 0.768 0.720 0.352
Table 6: Pearson correlation on the (Lee et al,
2005) with random walks over only a subset of
the edges in the Category link information (scores
.410 when taking all edges). Note that factoring
the graph by link generality can be very helpful to
the walk.
Method Text Sim
ESA@625 0.766
ESA@625+Walk Cat@+6 0.770
ESA@625+Walk Cat@+6 Inf@=2 0.772
Bag of words (Lee et al, 2005) 0.5
LDA (Lee et al, 2005) 0.60
ESA* 0.72
Table 7: Pearson correlation on the (Lee et al,
2005) dataset for our best sytems compared to pre-
viously reported numbers. ESA* is the score for
raw ESA as reported number in (Gabrilovich and
Markovitch, 2007).
beit only a small improvement over our ESA@625
score.
Despite the results obtained for text similarity,
the best settings found for the Lee dataset did not
translate to consistent improvements over the ESA
baseline for Spearman rank correlation on the lex-
ical similarity datasets. While our scores on the
MC dataset of 30 word pairs did improve with the
walk in roughly the same way as in Lee, no such
improvements were found on the larger WordSim-
353 data. On WordSim-353, our implementa-
tion of ESA scored 0.709 (versus Gabrilovich?s
reported ESA score of 0.75), and our walk on
Cat@+6 showing no gain or loss. In contrast to
the text similarity dataset, Infobox links were no
longer helpful, bringing the correlation down to
.699. We believe this is because Infobox links
helped the most with entities, which are very rare
in the WordSim-353 data, but are more common
in the Lee dataset.
6 Discussion
Our results suggest that even with a simple
dictionary-based approach, the graph of Wikipedia
links can act as an effective resource for comput-
ing semantic relatedness. However, the dictio-
nary approach alone was unable to reach the re-
sults of state-of-the-art models using Wikipedia
(Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008) or using the same technique on
WordNet (Hughes and Ramage, 2007; Agirre
et al, 2009). Thus, it seems that the text of
Wikipedia provides a stronger signal than the link
structure. However, a pruned dictionary can im-
prove the results of the dictionary based initial-
ization, which indicates that some links are in-
formative for semantic relatedness while others
are not. The careful pruning, disambiguation and
weighting functions presented in (Milne and Wit-
ten, 2008) are directions for future work.
The use of WordNet as a graph provided ex-
cellent results (Hughes and Ramage, 2007), close
to those of ESA. In contrast with our dictionary-
based initialization on Wikipedia, no pruning of
dictionary or graph seem necessary to obtain high
results with WordNet. One straightforward expla-
nation is that Wikipedia is a noisy source of link
information. In fact, both ESA and (Milne and
Witten, 2008) use ad-hoc pruning strategies in or-
der to obtain good results.
6.1 ESA and Walk Comparison
By using ESA to generate the teleport distribu-
tion, we were able to introduce small gains us-
ing the random walk. Because these gains were
small, it is plausible that the walk introduces only
modest changes from the initial ESA teleport dis-
tributions. To evaluate this, we examined the dif-
ferences between the vector returned by ESA and
distribution over the equivalent nodes in the graph
after performing a random walk starting with that
ESA vector.
For this analysis, we took all of the text entries
used in this study, and generated two distributions
over the Wikipedia graph, one using ESA@625,
the other the result of performing a random walk
starting at ESA@625. We generated a list of the
concept nodes for both distributions, sorted in de-
creasing order by their associated scores. Start-
ing from the beginning of both lists, we then
counted the number of matched nodes until they
disagreed on ordering, giving a simple view of
47
Walk Type Avg Std Max
MC Cat@+6 12.1 7.73 35
Cat@+6 Inf@=2 5.39 5.81 20
WordSim Cat@+6 12.0 10.6 70
Cat@+6 Inf@=2 5.74 7.78 54
Lee Cat@+6 28.3 89.7 625
Cat@+6 Inf@=2 4.24 14.8 103
Table 8: Statistics for first concept match length,
by run and walk type.
how the walk perturbed the strongest factors in the
graphs. We performed this for both the best per-
forming walk models (ESA@625+Walk Cat@+6
and ESA@625+Walk Cat@+6 Inf@=2) against
ESA@625. Results are given in Table 8.
As expected, adding edges to the random walk
increases the amount of change from the graph,
as initialized by ESA. A cursory examination of
the distributions also revealed a number of outliers
with extremely high match lengths: these were
likely due to the fact that the selected edge types
were already extremely specialized. Thus for a
number of concept nodes, it is likely they did not
have any outbound edges at all.
Having established that the random walk does
indeed have an impact on the ESA vectors, the
next question is if changes via graph walk are
consistently helpful. To answer this, we com-
pared the performance of the walk on the (Lee et
al., 2005) dataset for probabilities at selected val-
ues, using the best link pruned Wikipedia graph
(ESA@625+Walk Cat@+6 Inf@=2), and using all
of the available edges in the graph for compari-
son. Here, a lower probability means the distribu-
tion spreads out further into the graph, compared
to higher values, where the distribution varies only
slightly from the ESA vector. Results are given in
Table 9. Performance for the pruned graph im-
proves as the return probability decreases, with
larger changes introduced by the graph walk re-
sulting in better scores, whereas using all available
links decreases performance. This reinforces the
notion that Wikipedia links are indeed noisy, but
that within a selected edge subset, making use of
all information via the random walk indeed results
in gains.
7 Conclusion
This paper has demonstrated that performing ran-
dom walks with Personalized PageRank over the
Prob Corr (Pruned) Corr (All)
0.01 0.772 0.246
0.10 0.773 0.500
0.15 0.772 0.556
0.30 0.771 0.682
0.45 0.769 0.737
0.60 0.767 0.758
0.90 0.766 0.766
0.99 0.766 0.766
Table 9: Return probability vs. correlation, on tex-
tual similarity data (Lee et al, 2005).
Wikipedia graph is a feasible and potentially fruit-
ful means of computing semantic relatedness for
words and texts. We have explored two methods of
initializing the teleport vector: a dictionary-based
method and a method based on ESA, the cur-
rent state-of-the-art technique. Our results show
the importance of pruning the dictionary, and for
Wikipedia link structure, the importance of both
categorizing by anchor type and comparative gen-
erality. We report small improvements over the
state-of-the-art on (Lee et al, 2005) using ESA as
a teleport vector and a limited set of links from
Wikipedia category pages and infoboxes.
In future work, we plan to explore new ways
to construct nodes, edges, and dictionary entries
when constructing the Wikipedia graph and dic-
tionary. We believe that finer grained methods of
graph construction promise to improve the value
of the Wikipedia link structure. We also plan to
further investigate the differences between Word-
Net and Wikipedia and how these may be com-
bined, from the perspective of graph and random
walk techniques. A public distribution of software
used for these experiments will also be made avail-
able.
5
Acknowledgements
The authors would like to thank Michael D. Lee
and Brandon Pincombe for access to their textual
similarity dataset, and the reviewers for their help-
ful comments. Eneko Agirre performed part of
the work while visiting Stanford, thanks to a grant
from the Science Ministry of Spain.
5
Please see http://nlp.stanford.edu/
software and http://ixa2.si.ehu.es/ukb
48
References
E. Agirre and A. Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proceedings
of 14th Conference of the European Chapter of the
Association for Computational Linguistics, Athens,
Greece.
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pasc?a, and A. Soroa. 2009. A study on similarity
and relatedness using distributional and WordNet-
based approaches. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies,
Boulder, USA.
S Auer, C Bizer, G Kobilarov, J Lehmann, R Cyganiak,
and Z Ives. 2008. Dbpedia: A nucleus for a web
of open data. In Proceedings of 6th International
Semantic Web Conference, 2nd Asian Semantic Web
Conference (ISWC+ASWC 2007), pages 722?735.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1-7).
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic relat-
edness. Computational Linguistics, 32(1):13?47.
R. C. Bunescu and M. Pasca. 2006. Using encyclo-
pedic knowledge for named entity disambiguation.
In Proceedings of 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. MIT Press.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-07).
E. Gabrilovich and S. Markovitch. 2009. Wikipedia-
based semantic interpretation. Journal of Artificial
Intelligence Research, 34:443?498.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02, pages 517?526, New York, NY, USA.
ACM.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings
of EMNLP-CoNLL, pages 581?589.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. An
introduction to latent semantic analysis. Discourse
Processes, 25(2-3):259?284.
M. D. Lee, B. Pincombe, and M. Welsh. 2005. An em-
pirical evaluation of models of text document sim-
ilarity. In Proceedings of the 27th Annual Confer-
ence of the Cognitive Science Society, pages 1254?
1259, Mahwah, NJ. Erlbaum.
D. Milne and I.H. Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
Wikipedia links. In Proceedings of the first AAAI
Workshop on Wikipedia and Artifical Intellegence
(WIKIAI?08), Chicago, I.L.
M. Strube and S.P. Ponzetto. 2006. Wikirelate! com-
puting semantic relatedness using Wikipedia. In
Proceedings of the 21st National Conference on Ar-
tificial Intelligence, pages 1419?1424.
49
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 37?45,
Beijing, August 2010
Plagiarism Detection across Distant Language Pairs
Alberto Barro?n-Ceden?o Paolo Rosso
Natural Language Engineering Lab. - ELiRF
Universidad Polite?cnica de Valencia
{lbarron, prosso}@dsic.upv.es
Eneko Agirre Gorka Labaka
IXA NLP Group
Basque Country University
{e.agirre, gorka.labaka}@ehu.es
Abstract
Plagiarism, the unacknowledged reuse of
text, does not end at language boundaries.
Cross-language plagiarism occurs if a text
is translated from a fragment written in a
different language and no proper citation
is provided. Regardless of the change of
language, the contents and, in particular,
the ideas remain the same. Whereas dif-
ferent methods for the detection of mono-
lingual plagiarism have been developed,
less attention has been paid to the cross-
language case.
In this paper we compare two recently
proposed cross-language plagiarism de-
tection methods (CL-CNG, based on char-
acter n-grams and CL-ASA, based on sta-
tistical translation), to a novel approach
to this problem, based on machine trans-
lation and monolingual similarity analy-
sis (T+MA). We explore the effectiveness
of the three approaches for less related
languages. CL-CNG shows not be ap-
propriate for this kind of language pairs,
whereas T+MA performs better than the
previously proposed models.
1 Introduction
Plagiarism is a problem in many scientific and cul-
tural fields. Text plagiarism may imply differ-
ent operations: from a simple cut-and-paste, to
the insertion, deletion and substitution of words,
up to an entire process of paraphrasing. Differ-
ent models approach the detection of monolin-
gual plagiarism (Shivakumar and Garc??a-Molina,
1995; Hoad and Zobel, 2003; Maurer et al, 2006).
Each of these models is appropriate only in those
cases where all the implied documents are written
in the same language.
Nevertheless, the problem does not end at lan-
guage boundaries. Plagiarism is also committed if
the reused text is translated from a fragment writ-
ten in a different language and no citation is pro-
vided. When plagiarism is generated by a transla-
tion process, it is known as cross-language plagia-
rism (CLP).
Less attention has been paid to the detection of
this kind of plagiarism due to its enhanced diffi-
culty (Ceska et al, 2008; Barro?n-Ceden?o et al,
2008; Potthast et al, 2010). In fact, in the recently
held 1st International Competition on Plagiarism
Detection (Potthast et al, 2009), no participants
tried to approach it.
In order to describe the prototypical process of
automatic plagiarism detection, we establish the
following notation. Let dq be a plagiarism suspect
document. Let D be a representative collection
of reference documents. D presumably includes
the source of the potentially plagiarised fragments
in dq . Stein et al, (2007) divide the process into
three stages1:
1. heuristic retrieval of potential source doc-
uments: given dq, retrieving an appropri-
ate number of its potential source documents
D? ? D such that |D?|? |D|;
2. exhaustive comparison of texts: comparing
the text from dq and d ? D? in order to
identify reused fragments and their potential
1This schema was formerly proposed for monolingual
plagiarism detection. Nevertheless, it can be applied with-
out further modifications to the cross-language case.
37
sources; and
3. knowledge-based post-processing: those de-
tected fragments with proper citation are dis-
carded as they are not plagiarised.
The result is offered to the human expert to take
the final decision. In the case of cross-language
plagiarism detection (CLPD), the texts are written
in different languages: dq ? L and d? ? L?.
In this research we focus on step 2: cross-
language exhaustive comparison of texts, ap-
proaching it as an Information Retrieval problem
of cross-language text similarity. Step 1, heuristic
retrieval, may be approached by different CLIR
techniques, such as those proposed by Dumais et
al. (1997) and Pouliquen et al (2003).
Cross-language similarity between texts,
?(dq, d?), has been previously estimated on
the basis of different models: multilingual
thesauri (Steinberger et al, 2002; Ceska et
al., 2008), comparable corpora ?CL-Explicit
Semantic Analysis CL-ESA? (Potthast et
al., 2008), machine translation techniques
?CL-Alignment-based Similarity Analysis CL-
ASA? (Barro?n-Ceden?o et al, 2008; Pinto et al,
2009) and n-grams comparison ?CL-Character
n-Grams CL-CNG? (Mcnamee and Mayfield,
2004).
A comparison of CL-ASA, CL-ESA, and CL-
CNG was carried out recently by Potthast et
al. (2010). The authors report that in general,
despite its simplicity, CL-CNG outperformed the
other two models. Additionally, CL-ESA showed
good results in the cross-language retrieval of
topic-related texts, whereas CL-ASA obtained
better results in exact (human) translations.
However, most of the language pairs used in the
reported experiments (English-{German, Span-
ish, French, Dutch, Polish}) are related, whether
because they have common predecessors or be-
cause a large proportion of their vocabularies
share common roots. In fact, the lower syntactical
relation between the English-Polish pair caused
a performance degradation for CL-CNG, and for
CL-ASA to a lesser extent. In order to confirm
whether the closeness among languages is an im-
portant factor, this paper works with more dis-
tant language pairs: English-Basque and Spanish-
Basque.
The rest of the paper is structured as follows.
Section 2 describes the motivation for working
on this research topic, stressing the situation of
cross-language plagiarism among writers in less
resourced languages. A brief overview of the few
works on CLPD is included. The three similar-
ity estimation models compared in this research
work are presented in Section 3. The experimental
framework and the obtained results are included
in Section 4. Finally, Section 5 draws conclusions
and discusses further work.
2 Motivation
Cases of CLP are common nowadays because in-
formation in multiple languages is available on the
Web, but people still write in their own language.
This special kind of plagiarism occurs more often
when the target language is a less resourced one2,
as is the case of Basque.
Basque is a pre-indoeuropean language with
less than a million speakers in the world and
no known relatives in the language families
(Wikipedia, 2010a). Still, Basque shares a portion
of its vocabulary with its contact languages (Span-
ish and French). Therefore, we decided to work
with two language pairs: Basque with Spanish,
its contact language, and with English, perhaps
the language with major influence over the rest of
languages in the world. Although the considered
pairs share most of their alphabet, the vocabulary
and language typologies are very different. For
instance Basque is an agglutinative language.
In order to illustrate the relations among these
languages, Fig. 1 includes extracts from the En-
glish (en), Spanish (es) and Basque (eu) versions
of the same Wikipedia article. The fragments are
a sample of the lexical and syntactic distance be-
tween Basque and the other two languages. In
fact, these sentences are completely co-derived
and the corresponding entire articles are a sample
of the typical imbalance in text available in the dif-
ferent languages (around 2, 000, 1, 300, and only
2Less resourced language is that with a low degree of rep-
resentation on the Web (Alegria et al, 2009). Whereas the
available text for German, French or Spanish is less than for
English, the difference is more dramatic with other languages
such as Basque.
38
The Party of European Socialists (PES) is
a European political party comprising thirty-two
socialist, social democratic and labour parties
from each European Union member state and
Norway.
El Partido Socialista Europeo (PSE) es un
partido pol??tico pan-europeo cuyos miembros
son de partidos socialdemo?cratas, socialistas y
laboristas de estados miembros de la Unio?n Eu-
ropea, as?? como de Noruega.
Europako Alderdi Sozialista Europar Bata-
suneko herrialdeetako eta Norvegiako hogeita
hamahiru alderdi sozialista, sozialdemokrata eta
laborista biltzen dituen alderdia da.
Figure 1: First sentences from the Wikipedia arti-
cles ?Party of European Socialists? (en),?Partido
Socialista Europeo? (es), and ?Europako Alderdi
Sozialista? (eu) (Wikipedia, 2010b).
100 words are contained in the en, es and eu arti-
cles, respectively).
Of high relevance is that the two corpora used
in this work were manually constructed by trans-
lating English and Spanish text into Basque. In the
experiments carried out by Potthast et al (2010),
which inspired our work, texts from the JCR-
Acquis corpus (Steinberger et al, 2006) and
Wikipedia were used. The first one is a multilin-
gual corpus with no clear definition of source and
target languages, whereas in Wikipedia no spe-
cific relationship exists between the different lan-
guages in which a topic may be broached. In some
cases (cf. Fig. 1) they are clearly co-derived, but
in others they are completely independent.
CLPD has been investigated just recently,
mainly by adapting models formerly proposed
for cross-language information retrieval. This
is the case of cross-language explicit seman-
tic analysis (CL-ESA), proposed by Potthast et
al. (2008). In this case the comparison be-
tween texts is not carried out directly. Instead,
a comparable corpus CL,L? is required, contain-
ing documents on multiple topics in the two im-
plied languages. One of the biggest corpora
of this nature is Wikipedia. The similarity be-
tween dq ? L and every document c ? CL
is computed based on the cosine measure. The
same process is made for L?. This step gener-
ates two vectors [cos(dq, c1), . . . , cos(dq, c|CL|)]
and [cos(d?, c?1), . . . , cos(d?, c?|CL? |)], where each
dimension is comparable between the two vectors.
Therefore, the cosine between such vectors can be
estimated in order to ?indirectly? estimate how
similar dq and d? are. The authors suggest that this
model can be used for CLPD.
Another recent model is MLPlag, proposed by
Ceska et al (2008). It exploits the EuroWord-
Net Thesaurus3, that includes sets of synonyms in
multiple European languages, with common iden-
tifiers across languages. The authors report ex-
periments over a subset of documents of the En-
glish and Czech sections of the JRC-Acquis cor-
pus as well as a corpus of simplified vocabulary4 .
The main difficulty they faced was the amount of
words in the documents not included in the the-
saurus (approximately 50% of the vocabulary).
This is a very similar approach to that pro-
posed by Pouliquen et al (2003) for the identi-
fication of document translations. In fact, both
approaches have something in common: transla-
tions are searched at document level. It is assumed
that an entire document has been reused (trans-
lated). Nevertheless, a writer is free to plagiarise
text fragments from different sources, and com-
pose a mixture of original and reused text.
A third model is the cross-language alignment-
based similarity analysis (CL-ASA), proposed by
Barro?n-Ceden?o et al (2008), which is based on
statistical machine translation technology. This
model was proposed to detect plagiarised text
fragments (similar models have been proposed for
extraction of parallel sentences from comparable
corpora (Munteanu et al, 2004)). The authors
report experiments over a short set of texts from
which simulated plagiarism was created from En-
glish to Spanish. Human as well as automatic ma-
chine translations were included in the collection.
Further descriptions of this model are included in
Section 3, as it is one of those being assessed in
this research work.
To the best of our knowledge, no work (in-
cluding the three previously mentioned) has been
done considering less resourced languages. In this
research work we approach the not uncommon
problem of CLPD in Basque, with source texts
written in Spanish (the co-official language of the
3http://www.illc.uva.nl/EuroWordNet/
4The authors do not mention the origin of the documents.
39
low tok pd bd sd lem
T+MA   
CL-ASA   
CL-CNG    
Table 1: Text preprocessing operations re-
quired for the different models. low=lowercasing,
tok=tokenization, pd=punctuation marks deletion, bd=blank
space deletion, sd=symbols deletion, lem=lematization.
Basque Country) and English (the language with
most available texts in the world).
We compare three cross-language similarity
analysis methods: T+MA (translation followed
by monolingual analysis), a novel method based
on machine translation followed by a monolin-
gual similarity estimation; CL-CNG, a character
n-gram based comparison model; and CL-ASA
a model that combines translation and similarity
estimation in a single step. Neither MLPlag nor
CL-ESA are included in the comparison. On the
one hand, we are interested in plagiarism at sen-
tence level, and MLPlag is designed to compare
entire documents. On the other hand, in previous
experiments over exact translations, CL-ASA has
shown to outperform it on language pairs whose
alphabet or syntax are unrelated (Potthast et al,
2010). This is precisely the case of en-eu and
es-eu language pairs. Additionally, the amount
of Wikipedia articles in Basque available for the
construction of the required comparable corpus is
insufficient for the CL-ESA data requirements.
3 Definition of Models
In this section, we describe the three cross-
language similarity models we compare. For ex-
perimental purposes (cf. Section 4) we consider
dq to be a suspicious sentence written in L and
D? to be a collection of potential source sentences
written in L? (L 6= L?). The text pre-processing
required by the different models is summarised
in Table 1. Examples illustrating how the models
work are included in Section 4.3.
3.1 Translation + Monolingual Analysis
dq ? L is translated into L? on the basis of
the Giza++ (Och and Ney, 2003), Moses (Koehn
et al, 2007) and SRILM (Stolcke, 2002) tools,
generating d?q . The translation system uses a
log-linear combination of state-of-the-art features,
such as translation probabilities and lexical trans-
lation models on both directions and a target lan-
guage model. After translation, d?q and d? are
lexically related, making possible a monolingual
comparison.
Multiple translations from dq into d?q are pos-
sible. Therefore, performing a monolingual sim-
ilarity analysis based on ?traditional? techniques,
such as those based on word n-grams compari-
son (Broder, 1997) or hash collisions (Schleimer
et al, 2003), is not an option. Instead, we take the
approach of the bag-of-words, which has shown
good results in the estimation of monolingual text
similarity (Barro?n-Ceden?o et al, 2009). Words in
d?q and d? are weighted by the standard tf -idf , and
the similarity between them is estimated by the
cosine similarity measure.
3.2 CL-Alignment-based Similarity Analysis
In this model an estimation of how likely is that d?
is a translation of dq is performed. It is based on
the adaptation of the Bayes rule for MT:
p(d? | dq) = p(d
?) p(dq | d?)
p(dq)
. (1)
As p(dq) does not depend on d?, it is neglected.
From an MT point of view, the conditional prob-
ability p(dq | d?) is known as translation model
probability and is computed on the basis of a sta-
tistical bilingual dictionary. p(d?) is known as lan-
guage model probability; it describes the target
language L? in order to obtain grammatically ac-
ceptable translations (Brown et al, 1993).
Translating dq into L? is not the concern of
this method, rather it focuses on retrieving texts
written in L? which are potential translations of
dq . Therefore, Barro?n-Ceden?o et al (2008) pro-
posed replacing the language model (the one used
in T+MA) by that known as length model. This
model depends on text?s character lengths instead
of language structures.
Multiple translations from d into L? are possi-
ble, and it is uncommon to find a pair of translated
texts d and d? such that |d| = |d?|. Nevertheless,
the length of such translations is closely related
to a translation length factor. In accordance with
Pouliquen et al (2003), the length model is de-
fined as:
40
?(d?) = e
?0.5
0
B
@
|d?|
|dq| ??
?
1
C
A
2
, (2)
where ? and ? are the mean and the standard devi-
ation of the character lengths between translations
of texts from L into L?. If the length of d? is not the
expected given dq, it receives a low qualification.
The translation model probability is defined as:
p(d | d?) =
Y
x?d
X
y?d?
p(x, y), (3)
where p(x, y), a statistical bilingual dictionary,
represents the likelihood that x is a valid transla-
tion of y. After estimating p(x, y) from a parallel
corpus, on the basis of the IBM statistical trans-
lation models (Brown et al, 1993), we consider,
for each word x, only the k best translations y
(those with the highest probabilities) up to a min-
imum probability mass of 0.4. This threshold was
empirically selected as it eliminated noisy entries
without discarding an important amount of rele-
vant pairs.
The similarity estimation based on CL-ASA is
finally computed as:
?(dq, d?) = ?(d?) p(dq | d?). (4)
3.3 CL-Character n-Gram Analysis
This model, the simplest of those compared in this
research, has been used in (monolingual) Author-
ship Attribution (Keselj et al, 2003) as well as
cross-language Information Retrieval (Mcnamee
and Mayfield, 2004). The simplified alphabet con-
sidered is ? = {a, . . . , z, 0, . . . , 9}; any other
symbol is discarded (cf. Table 1). The resulting
text strings are codified into character 3-grams,
which are weighted by the standard tf -idf (con-
sidering this n has previously shown to produce
the best results). The similarity between such rep-
resentations of dq and d? is estimated by the cosine
similarity measure.
4 Experiments
The objective of our experiments is to compare
the performance of the three similarity estimation
models. Section 4.1 introduces the corpora we
have exploited. The experimental framework is
described in Section 4.2. Section 4.3 illustrates
how the models work, and the obtained results are
presented and discussed in Section 4.4.
4.1 Corpora
In other Information Retrieval tasks a plethora of
corpora is available for experimental and compar-
ison purposes. However, plagiarism implies an
ethical infringement and, to the best of our knowl-
edge, there is no corpora of actual cases available,
other than some seminal efforts on creating cor-
pora of text reuse (Clough et al, 2002), artificial
plagiarism (Potthast et al, 2009), and simulated
plagiarism (Clough and Stevenson, 2010). The
problem is worse for cross-language plagiarism.
Therefore, in our experiments we use two
parallel corpora: Software, an en-eu translation
memory of software manuals generously supplied
by Elhuyar Fundazioa5; and Consumer, a cor-
pus extracted from a consumer oriented mag-
azine that includes articles written in Spanish
along with their Basque, Catalan, and Galician
translations6 (Alca?zar, 2006). Software includes
288, 000 parallel sentences; 8.66 (6.83) words per
sentence in the English (Basque) section. Con-
sumer contains 58, 202 sentences; 19.77 (15.20)
words per sentence in Spanish (Basque). These
corpora also reflect the imbalance of text available
in the different languages.
4.2 Experimental Framework
We consider Dq and D? to be two entire docu-
ments from which plagiarised sentences and their
source are to be detected. We work at this level
of granularity, and not entire documents, for two
main reasons: (i) we are focused on the exhaus-
tive comparison stage of the plagiarism detection
process (cf. Section 1); and (ii) even a single sen-
tence could be considered a case of plagiarism,
as it transmits a complete idea. However, a pla-
giarised sentence is usually not enough to auto-
matically negate the validity of an entire docu-
ment. This decision is left to the human expert,
which can examine the documents where several
plagiarised sentences occur. Note that the task be-
comes computationally more expensive as, for ev-
ery sentence, we are looking through thousands
5http://www.elhuyar.org
6http://revista.consumer.es
41
es-eu en-eu
? ? ? ?
f1 1.1567 0.2346 1.0561 0.5497
f2 1.1569 0.2349 1.0568 0.5510
f3 1.1571 0.2349 1.0566 0.5433
f4 1.1565 0.2363 1.0553 0.5352
f5 1.1571 0.2348 1.0553 0.5467
avg. 1.1569 0.2351 1.0560 0.5452
Table 2: Length models estimated for each train-
ing partition f1,...,5. The values describe a normal distri-
bution centred in ? ? ?, representing the expected length of
the source text given the suspicious one.
0
0.2
0.4
0.6
0.8
1
0 50 100 150 200 250
Pr
o
ba
bi
lit
y
di
st
rib
u
tio
n
Length of the sentences
eu
es
en
Figure 2: Example length factor for a sentence
written in Basque (eu) dq , such that |dq| = 90.
The normal distributions represent the expected lengths for
the translation d?, either in Spanish (es) or English (en).
of topically-related sentences that are potential
sources of dq, and not only those of a specific doc-
ument.
CLPD is considered a ranking problem. Let
dq ? Dq be a plagiarism suspicious sentence and
d? ? D? be its source sentence. We consider that
the result of the process is correct if, given dq, d?
is properly retrieved. A 5-fold cross validation for
both en-eu and es-eu was performed. Bilingual
dictionaries, language and length models were es-
timated with the corresponding training partitions.
The computed values for ? and ? are those in-
cluded in Table 2. The values for the different
partitions are very similar, showing the low vari-
ability in the translation lengths. On the basis of
these estimated parameters, an example of length
factor for a specific sentence is plotted in Fig. 2.
In the test partitions, for each suspicious sen-
tence dq , 11, 640 source candidate sentences exist
for es-eu and 57, 290 for en-eu. This results in
more than 135 million and 3 billion comparisons
carried out for es-eu and en-eu respectively.
xeu yen p(x, y) xeu yen p(x, y)
beste another 0.288 beste other 0.348
dokumentu document 0.681 batzu some 0.422
makro macro 0.558 ezin not 0.179
ezin cannot 0.279 izan is 0.241
izan the 0.162 atzi access 0.591
. . 0.981
Table 3: Entries in the bilingual dictionary for the
words in dq. Relevant entries for the example are in bold.
4.3 Illustration of Models
In order to clarify how the different models work,
consider the following sentence pair, a suspicious
sentence dq written in Basque and its source d?
written in English (sentences are short for illustra-
tive purposes):
dq beste dokumentu batzuetako makroak ezin dira atzitu.
d? macros from other documents are not accessible.
CL-CNG Example
In this case, symbols and spaces are discarded.
Sentences become:
dq bestedokumentubatzuetakomakroakezindiraatzitu
d? macrosfromotherdocumentsarenotaccessible
Only three 3-grams appear in both sentences
(ume, men, ent). In order to keep the example sim-
ple, the 3-grams are weighted by tf only (in the
actual experiments, tf -idf is used), resulting in a
dot product of 3. The corresponding vectors mag-
nitudes are |dq| = 6.70 and |d?| = 5.65. There-
fore, the estimated similarity is ?(dq, d?) = 0.079.
CL-ASA Example
In this case, the text must be tokenised and lem-
matised, resulting in the following string:
dq beste dokumentu batzu makro ezin izan atzi .
d? macro from other document be not accessible .
The sentences? lengths are |dq| = 38 and |d?| =
39. Therefore, on the basis of Eq. 2, the length
factor between them is ?(dq, d?) = 0.998.
The relevant entries of the previously estimated
dictionary are included in Table 3. Such entries
are substituted in Eq. 3, and the overall process
results in a similarity ?(dq , d?) = 2.74. Whereas
not a stochastic value, this is a weight used when
ranking all the potential source sentences in D?.
T+MA Example
In this case, the same pre-processing than
in CL-ASA is performed. In T+MA dq is
translated into L?, resulting in the new pair:
d?q other document macro cannot be access .
d? macro from other document be not accessible .
42
Note that d?q is a valid translation of dq . Never-
theless, it has few syntactic relation to d?. There-
fore, applying more sophisticated codifications
than the cosine measure over bag-of-words is not
an option. The example is again simplified by
weighting the words based on tf . Five words ap-
pear in both sentences, resulting in a dot product
of 5. The vectors magnitudes are |d?q| = |d?| =?
7. The estimation by T+MA is ?(dq, d?) =
0.71, a high similarity level.
4.4 Results and Discussion
For evaluation we consider a standard measure:
Recall. More specifically Recall after n texts have
been retrieved (n = [1 . . . , 50]). Figure 3 plots the
average Recall value obtained in the 5-folds with
respect to the rank position (n).
In both language pairs, CL-CNG obtained
worse results than those reported for English-
Polish by Potthast et al (2010): R@50 = 0.68
vs. R@50 = 0.53 for es-eu and 0.28 for en-eu.
This is due to the fact that neither the vocabulary
nor its corresponding roots keep important rela-
tions. Therefore, when language pairs have a low
syntactical relationship, CL-CNG is not an op-
tion. Still, CL-CNG performs better with es-eu
than with en-eu because the first pair is composed
of contact languages (cf. Section 1).
About CL-ASA, the results obtained with es-
eu and en-eu are quite different: R@50 = 0.68
for en-eu and R@50 = 0.53 for es-eu. Whereas
in the first case they are comparable to those of
CL-CNG, in the second one CL-ASA completely
outperforms it. The improvement of CL-ASA ob-
tained for en-eu is due to the size of the training
corpus available in this case (approximately five
times the number of sentences available for es-
eu). This shows the sensitivity of the model with
respect to the size of the available resources.
Lastly, although T+MA is a simple approach
that reduces the cross-language similarity estima-
tion to a translation followed by a monolingual
process, it obtained a good performance (R@50=
0.77 for en-eu and R@50=0.89 for es-eu). More-
over, this method proved to be less sensitive than
CL-ASA to the lack of resources. This could
be due to the fact that it considers both direc-
tions of the translation model (e[n|s]-eu and eu-
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 10 20 50
R
ec
al
l
rank
CL-ASA
CL-CNG
T+MA
(a) es-eu
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 10 20 50
R
ec
al
l
rank
(b) en-eu
Figure 3: Evaluation of the cross-language rank-
ing. Results plotted as rank versus Recall for the three eval-
uated models and the two language pairs (R@[1, . . . , 50]).
e[n|s]). Additionally, the language model, applied
in order to compose syntactically correct transla-
tions, reduces the amount of wrong translations
and, indirectly, includes more syntactic informa-
tion in the process. On the contrary, CL-ASA
only considers one direction translation model eu-
e[n|s] and completely disregards syntactical rela-
tions between the texts.
Note that the better results come at the cost
of higher computational demand. CL-CNG only
requires easy to compute string comparisons.
CL-ASA requires translation probabilities from
aligned corpora, but once the probabilities are es-
timated, cross-language similarity can be com-
puted very fast. T+MA requires the previous
translation of all the texts, which can be very
costly for large collections.
5 Conclusions and Further Work
In a society where information in multiple lan-
guages is available on the Web, cross-language
43
plagiarism is occurring every day with increasing
frequency. Still, cross-language plagiarism de-
tection has not been approached sufficiently due
to its intrinsic complexity. Though few attempts
have been made, even less work has been made to
tackle this problem for less resourced languages,
and to explore distant language pairs.
We investigated the case of Basque, a lan-
guage where, due to the lack of resources, cross-
language plagiarism is often committed from texts
in Spanish and English. Basque has no known rel-
atives in the language family. However, it shares
some of its vocabulary with Spanish.
Two state-of-the-art methods based on trans-
lation probabilities and n-gram overlapping, and
a novel technique based on statistical machine
translation were evaluated. The novel technique
obtains the best results in both language pairs,
with the n-gram overlap technique performing
worst. In this sense, our results complement those
of Potthast et al (2010), which includes closely
related language pairs as well.
Our results also show that better results come at
the cost of more expensive processing time. For
the future, we would like to investigate such per-
formance trade-offs in more demanding datasets.
For future work we consider that exploring se-
mantic text features across languages could im-
prove the results. It could be interesting to fur-
ther analyse how the reordering of words through
translations might be relevant for this task. Addi-
tionally, working with languages even more dis-
tant from each other, such as Arabic or Hindi,
seems to be a challenging and interesting task.
Acknowledgements
The research work of the first two authors is partially funded
by CONACYT-Mexico and the MICINN project TEXT-
ENTERPRISE 2.0 TIN2009-13391-C04-03 (Plan I+D+i).
The research work of the last two authors is partially funded
by the MICINN projects OPENMT-2 TIN2009-14675-C03-
01 and KNOW2 TIN2009-14715-C04-01.
References
Alca?zar, Asier. 2006. Towards Linguistically Search-
able Text. In Proceedings of the BIDE 2005, Bilbao,
Basque Country.
Alegria, In?aki, Mikel L. Forcada, and Kepa Sara-
sola, editors. 2009. Proceedings of the SEPLN
2009 Workshop on Information Retrieval and Infor-
mation Extraction for Less Resourced Languages,
Donostia, Basque Country. University of the Basque
Country.
Barro?n-Ceden?o, Alberto, Paolo Rosso, David Pinto,
and Alfons Juan. 2008. On Cross-lingual Plagia-
rism Analysis Using a Statistical Model. In Stein,
Stamatatos, and Koppel, editors, ECAI 2008 Work-
shop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2008), pages 9?13,
Patras, Greece. CEUR-WS.org.
Barro?n-Ceden?o, Alberto, Andreas Eiselt, and Paolo
Rosso. 2009. Monolingual Text Similarity Mea-
sures: A Comparison of Models over Wikipedia Ar-
ticles Revisions. In Sharma, Verma, and Sangal, ed-
itors, ICON 2009, pages 29?38, Hyderabad, India.
Macmillan Publishers.
Broder, Andrei Z. 1997. On the Resemblance and
Containment of Documents. In Compression and
Complexity of Sequences (SEQUENCES?97), pages
21?29. IEEE Computer Society.
Brown, Peter F., Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311.
Ceska, Zdenek, Michal Toman, and Karel Jezek. 2008.
Multilingual Plagiarism Detection. In Proceedings
of the 13th International Conference on Artificial
Intelligence, pages 83?92. Springer Verlag Berlin
Heidelberg.
Clough, Paul and Mark Stevenson. 2010. Developing
a Corpus of Plagiarised Short Answers. Language
Resources and Evaluation: Special Issue on Plagia-
rism and Authorship Analysis.
Clough, Paul, Robert Gaizauskas, and Scott Piao.
2002. Building and Annotating a Corpus for the
Study of Journalistic Text Reuse. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation (LREC 2002), volume V,
pages 1678?1691, Las Palmas, Spain.
Dumais, Susan T., Todd A. Letsche, Michael L.
Littman, and Thomas K. Landauer. 1997. Auto-
matic Cross-Language Retrieval Using Latent Se-
mantic Indexing. In AAAI-97 Spring Symposium
Series: Cross-Language Text and Speech Retrieval,
pages 24?26. Stanford University.
Hoad, Timothy C. and Justin Zobel. 2003. Meth-
ods for Identifying Versioned and Plagiarized Doc-
uments. Journal of the American Society for Infor-
mation Science and Technology, 54(3):203?215.
44
Keselj, Vlado, Fuchun Peng, Nick Cercone, and Calvin
Thomas. 2003. N-gram-based Author Profiles
for Authorship Attribution. In Proceedings of the
Conference Pacific Association for Computational
Linguistics, PACLING?03, pages 255?264, Halifax,
Canada.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), demonstra-
tion session, Prague, Czech Republic.
Maurer, Hermann, Frank Kappe, and Bilal Zaka. 2006.
Plagiarism - A Survey. Journal of Universal Com-
puter Science, 12(8):1050?1084.
Mcnamee, Paul and James Mayfield. 2004. Character
N-Gram Tokenization for European Language Text
Retrieval. Information Retrieval, 7(1-2):73?97.
Munteanu, Dragos S., Alexander Fraser, and Daniel
Marcu. 2004. Improved Machine Translation
Performace via Parallel Sentence Extraction from
Comparable Corpora. In Proceedings of the Hu-
man Language Technology and North American As-
sociation for Computational Linguistics Conference
(HLT/NAACL 2004), Boston, MA.
Och, Frank Josef and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
See also http://www.fjoch.com/GIZA++.html.
Pinto, David, Jorge Civera, Alberto Barro?n-Ceden?o,
Alfons Juan, and Paolo Rosso. 2009. A Statistical
Approach to Crosslingual Natural Language Tasks.
Journal of Algorithms, 64(1):51?60.
Potthast, Martin, Benno Stein, and Maik Anderka.
2008. A Wikipedia-Based Multilingual Retrieval
Model. In Macdonald, Ounis, Plachouras, Ruthven,
and White, editors, 30th European Conference on
IR Research, ECIR 2008, Glasgow, volume 4956
LNCS of Lecture Notes in Computer Science, pages
522?530, Berlin Heidelberg New York. Springer.
Potthast, Martin, Benno Stein, Andreas Eiselt, Alberto
Barro?n-Ceden?o, and Paolo Rosso. 2009. Overview
of the 1st International Competition on Plagiarism
Detection. In Stein, Rosso, Stamatatos, Koppel, and
Agirre, editors, SEPLN 2009 Workshop on Uncov-
ering Plagiarism, Authorship, and Social Software
Misuse (PAN 09), pages 1?9, San Sebastian, Spain.
CEUS-WS.org.
Potthast, Martin, Alberto Barro?n-Ceden?o, Benno
Stein, and Paolo Rosso. 2010. Cross-Language Pla-
giarism Detection. Language Resources and Eval-
uation, Special Issue on Plagiarism and Authorship
Analysis.
Pouliquen, Bruno, Ralf Steinberger, and Camelia Ig-
nat. 2003. Automatic Identification of Docu-
ment Translations in Large Multilingual Document
Collections. In Proceedings of the International
Conference on Recent Advances in Natural Lan-
guage Processing (RANLP-2003), pages 401?408,
Borovets, Bulgaria.
Schleimer, Saul, Daniel S. Wilkerson, and Alex Aiken.
2003. Winnowing: Local Algorithms for Document
Fingerprinting. In Proceedings of the 2003 ACM
SIGMOD International Conference on Management
of Data, New York, NY. ACM.
Shivakumar, Narayanan and Hector Garc??a-Molina.
1995. SCAM: A Copy Detection Mechanism for
Digital Documents. In Proceedings of the 2nd An-
nual Conference on the Theory and Practice of Dig-
ital Libraries.
Stein, Benno, Sven Meyer zu Eissen, and Martin Pot-
thast. 2007. Strategies for Retrieving Plagiarized
Documents. In Clarke, Fuhr, Kando, Kraaij, and de
Vries, editors, Proceedings of the 30th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 825?
826, Amsterdam, The Netherlands. ACM.
Steinberger, Ralf, Bruno Pouliquen, and Johan Hag-
man. 2002. Cross-lingual Document Similarity
Calculation Using the Multilingual Thesaurus EU-
ROVOC. Computational Linguistics and Intelligent
Text Processing. Proceedings of the CICLing 2002,
2276:415?424.
Steinberger, Ralf, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Da?niel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006),
volume 9, Genoa, Italy.
Stolcke, Andreas. 2002. SRILM - An Extensible Lan-
guage Modeling toolkit. In Intl. Conference on Spo-
ken Language Processing, Denver, Colorado.
Wikipedia. 2010a. Basque language. [Online; ac-
cessed 5-February-2010].
Wikipedia. 2010b. Party of European Socialists | Par-
tido Socialista Europeo | Europako Alderdi Sozial-
ista . [Online; accessed 10-February-2010].
45
Coling 2010: Poster Volume, pages 9?17,
Beijing, August 2010
Document Expansion Based on WordNet
for Robust IR
Eneko Agirre
IXA NLP Group
Univ. of the Basque Country
e.agirre@ehu.es
Xabier Arregi
IXA NLP Group
Univ. of the Basque Country
xabier.arregi@ehu.es
Arantxa Otegi
IXA NLP Group
Univ. of the Basque Country
arantza.otegi@ehu.es
Abstract
The use of semantic information to im-
prove IR is a long-standing goal. This pa-
per presents a novel Document Expansion
method based on a WordNet-based system
to find related concepts and words. Ex-
pansion words are indexed separately, and
when combined with the regular index,
they improve the results in three datasets
over a state-of-the-art IR engine. Consid-
ering that many IR systems are not robust
in the sense that they need careful fine-
tuning and optimization of their parame-
ters, we explored some parameter settings.
The results show that our method is spe-
cially effective for realistic, non-optimal
settings, adding robustness to the IR en-
gine. We also explored the effect of doc-
ument length, and show that our method
is specially successful with shorter docu-
ments.
1 Introduction
Since the earliest days of IR, researchers noted
the potential pitfalls of keyword retrieval, such
as synonymy, polysemy, hyponymy or anaphora.
Although in principle these linguistic phenom-
ena should be taken into account in order to ob-
tain high retrieval relevance, the lack of algo-
rithmic models prohibited any systematic study
of the effect of this phenomena in retrieval. In-
stead, researchers resorted to distributional se-
mantic models to try to improve retrieval rele-
vance, and overcome the brittleness of keyword
matches. Most research concentrated on Query
Expansion (QE) methods, which typically ana-
lyze term co-occurrence statistics in the corpus
and in the highest scored documents for the orig-
inal query in order to select terms for expanding
the query terms (Manning et al, 2009). Docu-
ment expansion (DE) is a natural alternative to
QE, but surprisingly it was not investigated un-
til very recently. Several researchers have used
distributional methods from similar documents in
the collection in order to expand the documents
with related terms that do not actually occur in the
document (Liu and Croft, 2004; Kurland and Lee,
2004; Tao et al, 2006; Mei et al, 2008; Huang
et al, 2009). The work presented here is com-
plementary, in that we also explore DE, but use
WordNet instead of distributional methods.
Lexical semantic resources such as WordNet
(Fellbaum, 1998) might provide a principled and
explicit remedy for the brittleness of keyword
matches. WordNet has been used with success
in psycholinguistic datasets of word similarity and
relatedness, where it often surpasses distributional
methods based on keyword matches (Agirre et al,
2009b). WordNet has been applied to IR before.
Some authors extended the query with related
terms (Voorhees, 1994; Liu et al, 2005), while
others have explicitly represented and indexed
word senses after performing word sense disam-
biguation (WSD) (Gonzalo et al, 1998; Stokoe
et al, 2003; Kim et al, 2004). More recently,
a CLEF task was organized (Agirre et al, 2008;
Agirre et al, 2009a) where queries and docu-
ments were semantically disambiguated, and par-
ticipants reported mixed results.
This paper proposes to use WordNet for docu-
ment expansion, proposing a new method: given
9
a full document, a random walk algorithm over
the WordNet graph ranks concepts closely related
to the words in the document. This is in con-
trast to previous WordNet-based work which fo-
cused on WSD to replace or supplement words
with their senses. Our method discovers impor-
tant concepts, even if they are not explicitly men-
tioned in the document. For instance, given a doc-
ument mentioning virus, software and DSL, our
method suggests related concepts and associated
words such us digital subscriber line, phone com-
pany and computer. Those expansion words are
indexed separately, and when combined with the
regular index, we show that they improve the re-
sults in three datasets over a state-of-the-art IR en-
gine (Boldi and Vigna, 2005). The three datasets
used in this study are ResPubliQA (Pen?as et al,
2009), Yahoo! Answers (Surdeanu et al, 2008)
and CLEF-Robust (Agirre et al, 2009a).
Considering that many IR systems are not ro-
bust in the sense that they need careful fine-tuning
and optimization of their parameters, we decided
to study the robustness of our method, explor-
ing some alternative settings, including default pa-
rameters, parameters optimized in development
data, and parameters optimized in other datasets.
The study reveals that the additional semantic ex-
pansion terms provide robustness in most cases.
We also hypothesized that semantic document
expansion could be most profitable when docu-
ments are shorter, and our algorithm would be
most effective for collections of short documents.
We artificially trimmed documents in the Robust
dataset. The results, together with the analysis of
document lengths of the three datasets, show that
document expansion is specially effective for very
short documents, but other factors could also play
a role.
The paper is structured as follows. We first in-
troduce the document expansion technique. Sec-
tion 3 introduces the method to include the expan-
sions in a retrieval system. Section 4 presents the
experimental setup. Section 5 shows our main re-
sults. Sections 6 and 7 analyze the robustness and
relation to document length. Section 8 compares
to related work. Finally, the conclusions and fu-
ture work are mentioned.
2 Document Expansion Using WordNet
Our key insight is to expand the document with
related words according to the background infor-
mation in WordNet (Fellbaum, 1998), which pro-
vides generic information about general vocabu-
lary terms. WordNet groups nouns, verbs, adjec-
tives and adverbs into sets of synonyms (synsets),
each expressing a distinct concept. Synsets are in-
terlinked with conceptual-semantic and lexical re-
lations, including hypernymy, meronymy, causal-
ity, etc.
In contrast with previous work, we select those
concepts that are most closely related to the doc-
ument as a whole. For that, we use a technique
based on random walks over the graph represen-
tation of WordNet concepts and relations.
We represent WordNet as a graph as fol-
lows: graph nodes represent WordNet concepts
(synsets) and dictionary words; relations among
synsets are represented by undirected edges; and
dictionary words are linked to the synsets asso-
ciated to them by directed edges. We used ver-
sion 3.0, with all relations provided, including the
gloss relations. This was the setting obtaining the
best results in a word similarity dataset as reported
by Agirre et al (2009b).
Given a document and the graph-based repre-
sentation of WordNet, we obtain a ranked list of
WordNet concepts as follows:
1. We first pre-process the document to obtain
the lemmas and parts of speech of the open
category words.
2. We then assign a uniform probability distri-
bution to the terms found in the document.
The rest of nodes are initialized to zero.
3. We compute personalized PageR-
ank (Haveliwala, 2002) over the graph,
using the previous distribution as the reset
distribution, and producing a probability
distribution over WordNet concepts The
higher the probability for a concept, the
more related it is to the given document.
Basically, personalized PageRank is computed
by modifying the random jump distribution vec-
tor in the traditional PageRank equation. In our
case, we concentrate all probability mass in the
concepts corresponding to the words in the docu-
10
ment.
Let G be a graph with N vertices v1, . . . , vN
and di be the outdegree of node i; let M be a N ?
N transition probability matrix, where Mji = 1diif a link from i to j exists, and zero otherwise.
Then, the calculation of the PageRank vector Pr
over G is equivalent to resolving Equation (1).
Pr = cMPr + (1? c)v (1)
In the equation, v is a N ? 1 vector and c is the
so called damping factor, a scalar value between
0 and 1. The first term of the sum on the equa-
tion models the voting scheme described in the
beginning of the section. The second term repre-
sents, loosely speaking, the probability of a surfer
randomly jumping to any node, e.g. without fol-
lowing any paths on the graph. The damping fac-
tor, usually set in the [0.85..0.95] range, models
the way in which these two terms are combined at
each step.
The second term on Eq. (1) can also be seen as a
smoothing factor that makes any graph fulfill the
property of being aperiodic and irreducible, and
thus guarantees that PageRank calculation con-
verges to a unique stationary distribution.
In the traditional PageRank formulation the
vector v is a stochastic normalized vector whose
element values are all 1N , thus assigning equalprobabilities to all nodes in the graph in case of
random jumps. In the case of personalized PageR-
ank as used here, v is initialized with uniform
probabilities for the terms in the document, and
0 for the rest of terms.
PageRank is actually calculated by applying an
iterative algorithm which computes Eq. (1) suc-
cessively until a fixed number of iterations are
executed. In our case, we used a publicly avail-
able implementation1, with default values for the
damping value (0.85) and the number of iterations
(30). In order to select the expansion terms, we
chose the 100 highest scoring concepts, and get
all the words that lexicalize the given concept.
Figure 1 exemplifies the expansion. Given the
short document from Yahoo! Answers (cf. Sec-
tion 4) shown in the top, our algorithm produces
the set of related concepts and words shown in the
1http://ixa2.si.ehu.es/ukb/
bottom. Note that the expansion produces syn-
onyms, but also other words related to concepts
that are not mentioned in the document.
3 Including Expansions in a Retrieval
System
Once we have the list of words for document ex-
pansion, we create one index for the words in the
original documents and another index with the ex-
pansion terms. This way, we are able to use the
original words only, or to also include the expan-
sion words during the retrieval.
The retrieval system was implemented using
MG4J (Boldi and Vigna, 2005), as it provides
state-of-the-art results and allows to combine sev-
eral indices over the same document collection.
We conducted different runs, by using only the in-
dex made of original words (baseline) and also by
using the index with the expansion terms of the
related concepts.
BM25 was the scoring function of choice. It is
one of the most relevant and robust scoring func-
tions available (Robertson and Zaragoza, 2009).
wBM25Dt := (2)
tfDt
k1
(
(1? b) + b dlDavdlD
)
+ tfDt
idft
where tfDt is the term frequency of term t in doc-
ument D, dlD is the document length, idft is the
inverted document frequency (or more specifically
the RSJ weight, (Robertson and Zaragoza, 2009)),
and k1 and b are free parameters.
The two indices were combined linearly, as fol-
lows (Robertson and Zaragoza, 2009):
score(d, e, q) := (3)
?
t?q?d
wBM25Dt + ?
?
t?q?e
wBM25Et
where D and E are the original and expanded in-
dices, d, e and q are the original document, the
expansion of the document and the query respec-
tively, t is a term, and ? is a free parameter for the
relative weight of the expanded index.
11
You should only need to turn off virus and anti-spy not uninstall. And that?s
done within each of the softwares themselves. Then turn them back on later after
installing any DSL softwares.
06566077-n? computer software, package, software, software package, software program, software system
03196990-n? digital subscriber line, dsl
01569566-v? instal, install, put in, set up
04402057-n? line, phone line, suscriber line, telephone circuit, telephone line
08186221-n? phone company, phone service, telco, telephone company, telephone service
03082979-n? computer, computing device, computing machine, data processor, electronic computer
Figure 1: Example of a document expansion, with original document on top, and some of the relevant
WordNet concepts identified by our algorithm, together with the words that lexicalize them. Words in
the original document are shown in bold, synonyms in italics, and other related words underlined.
4 Experimental Setup
We chose three data collections. The first is based
on a traditional news collection. DE could be
specially interesting for datasets with short docu-
ments, which lead our choice of the other datasets:
the second was chosen because it contains shorter
documents, and the third is a passage retrieval task
which works on even shorter paragraphs. Table 1
shows some statistics about the datasets.
One of the collections is the English dataset
of the Robust task at CLEF 2009 (Agirre et al,
2009a). The documents are news collections from
LA Times 94 and Glasgow Herald 95. The top-
ics are statements representing information needs,
consisting of three parts: a brief title statement; a
one-sentence description; a more complex narra-
tive describing the relevance assessment criteria.
We use only the title and the description parts of
the topics in our experiments.
The Yahoo! Answers corpus is a subset of a
dump of the Yahoo! Answers web site2 (Surdeanu
et al, 2008), where people post questions and
answers, all of which are public to any web user
willing to browse them. The dataset is a small
subset of the questions, selected for their linguis-
tic properties (for example they all start with ?how
{to?do?did?does?can?would?could?should}?).
Additionally, questions and answers of obvious
low quality were removed. The document set was
created with the best answer of each question
(only one for each question).
2Yahoo! Webscope dataset ?ydata-yanswers-manner-
questions-v1 0? http://webscope.sandbox.yahoo.com/
docs length q. train q. test
Robust 166,754 532 150 160
Yahoo! 89610 104 1000 88610
ResPubliQA 1,379,011 20 100 500
Table 1: Number of documents, average docu-
ment length, number of queries for train and test
in each collection.
The other collection is the English dataset of
ResPubliQA exercise at the Multilingual Ques-
tion Answering Track at CLEF 2009 (Pen?as et al,
2009). The exercise is aimed at retrieving para-
graphs that contain answers to a set of 500 natu-
ral language questions. The document collection
is a subset of the JRC-Acquis Multilingual Paral-
lel Corpus, and consists of 21,426 documents for
English which are aligned to a similar number of
documents in other languages3. For evaluation,
we used the gold standard released by the orga-
nizers, which contains a single correct passage for
each query. As the retrieval unit is the passage,
we split the document collection into paragraphs.
We applied the expansion strategy only to pas-
sages which had more than 10 words (half of the
passages), for two reasons: the first one was that
most of these passages were found not to contain
relevant information for the task (e.g. ?Article 2?
or ?Having regard to the proposal from the Com-
mission?), and the second was that we thus saved
some computation time.
In order to evaluate the quality of our expansion
in practical retrieval settings, the next Section re-
3Note that Table 1 shows the number of paragraphs,
which conform the units we indexed.
12
base. expa. ?
Robust MAP .3781 .3835*** 1.43%
Yahoo! MRR .2900 .2950*** 1.72%P@1 .2142 .2183*** 1.91%
ResPubliQA MRR .3931 .4077*** 3.72%P@1 .2860 .3000** 4.90%
Table 2: Results using default parameters.
port results with respect to several parameter set-
tings. Parameter optimization is often neglected
in retrieval with linguistic features, but we think it
is crucial since it can have a large effect on rele-
vance performance and therefore invalidate claims
of improvements over the baseline. In each setting
we assign different values to the free parameters in
the previous section, k1, b and ?.
5 Results
The main evaluation measure for Robust is mean
Average Precision (MAP), as customary. In two of
the datasets (Yahoo! and ResPubliQA) there is a
single correct answer per query, and therefore we
use Mean Reciprocal Rank (MRR) and Mean Pre-
cision at rank 1 (P@1) for evaluation. Note that in
this setting MAP is identical to MRR. Statistical
significance was computed using Paired Random-
ization Test (Smucker et al, 2007). In the tables
throughout the paper, we use * to indicate statis-
tical significance at 90% confidence level, ** for
95% and *** for 99%. Unless noted otherwise,
base. refers to MG4J with the standard index, and
expa. refers to MG4J using both indices. Best
results per row are in bold when significant. ? re-
ports relative improvement respect to the baseline.
5.1 Default Parameter Setting
The values for k1 and b are the default values as
provided in the wBM25 implementation of MG4J,
1.2 and 0.5 respectively. We could not think of a
straightforward value for ?. A value of 1 would
mean that we are assigning equal importance to
original and expanded terms, which seemed an
overestimation, so we used 0.1. Table 2 shows
the results when using the default setting of pa-
rameters. The use of expansion is beneficial in all
datasets, with relative improvements ranging from
1.43% to 4.90%.
base. expa. ?
Robust MAP .3740 .3823** 2.20%
Yahoo! MRR .3070 .3100*** 0.98%P@1 .2293 .2317* 1.05%
ResPubliQA MRR .4970 .4942 -0.56%P@1 .3980 .3940 -1.01%
Table 3: Results using optimized parameters.
Setting System k1 b ?
Default base. 1.20 0.50 -expa. 1.20 0.50 0.100
Robust base. 1.80 0.64 -expa. 1.66 0.55 0.075
Yahoo! basel. 0.99 0.82 -expa. 0.84 0.87 0.146
ResPubliQA base. 0.09 0.56 -expa. 0.13 0.65 0.090
Table 4: Parameters as in the default setting or as
optimized in each dataset. The ? parameter is not
used in the baseline systems.
5.2 Optimized Parameter Setting
We next optimized all three parameters using the
train part of each collection. The optimization of
the parameters followed a greedy method called
?promising directions? (Robertson and Zaragoza,
2009). The comparison between the baseline and
expansion systems in Table 3 shows that expan-
sion helps in Yahoo! and Robust, with statistical
significance. The differences in ResPubliQA are
not significant, and indicate that expansion terms
were not helpful in this setting.
Note that the optimization of the parameters
yields interesting effects in the baseline for each
of the datasets. If we compare the results of the
baseline with default settings (Table 2) and with
optimized setting (Table 3), the baseline improves
MRR dramatically in ResPubliQA (26% relative
improvement), significantly in Yahoo! (5.8%) and
decreases MAP in Robust (-0.01%). This dis-
parity of effects could be explained by the fact
that the default values are often approximated us-
ing TREC-style news collections, which is exactly
the genre of the Robust documents, while Yahoo
uses shorter documents, and ResPubliQA has the
shortest documents.
Table 4 summarizes the values of the parame-
ters in both default and optimized settings. For k1,
the optimization yields very different values. In
Robust the value is similar to the default value, but
13
base. expa. ? ?
Rob MAP .3781 .3881*** 2.64% 0.18
Y! MRR .2900 .2980*** 2.76% 0.27P@1 .2142 .2212*** 3.27%
ResP. MRR .3931 .4221*** 7.39% 0.61P@1 .2860 .3180** 11.19%
Table 5: Results obtained using the ? optimized
setting, including actual values of ?.
in ResPubliQA the optimization pushes it down
below the typical values cited in the literature
(Robertson and Zaragoza, 2009), which might ex-
plain the boost in performance for the baseline in
the case of ResPubliQA. When all three param-
eters are optimized together, the values ? in the
table range from 0.075 to 0.146. The values of the
optimized ? can be seem as an indication of the
usefulness of the expanded terms, so we explored
this farther.
5.3 Exploring ?
As an additional analysis experiment, we wanted
to know the effect of varying ? keeping k1 and b
constant at their default values. Table 5 shows the
best values in each dataset, which that the weight
of the expanded terms and the relative improve-
ment are highly correlated.
5.4 Exploring Number of Expansion
Concepts
One of the free parameters of our system is the
number of concepts to be included in the docu-
ment expansion. We have performed a limited
study with the default parameter setting on the
Robust setting, using 100, 500 and 750 concepts,
but the variations were not statistically significant.
Note that with 100 concepts we were actually ex-
panding with 268 words, with 500 concepts we
add 1247 words and with 750 concepts we add
1831 words.
6 Robustness
The results in the previous section indicate that
optimization is very important, but unfortunately
real applications usually lack training data. In this
Section we wanted to study whether the param-
eters can be carried over from one dataset to the
other, and if not, whether the extra terms found by
train base. expa. ?
Rob.
def. MAP .3781 .3835*** 1.43%
Rob. MAP .3740 .3823** 2.20%
Y! MAP .3786 .3759 -0.72%
Res. MAP .3146 .3346*** 6.35%
Y!
def. MRR .2900 .2950*** 1.72%
Rob. MRR .2920 .2920 0.0%
Y! MRR .3070 .3100** 0.98%
Res. MRR .2600 .2750*** 5.77%
ResP.
def. MRR .3931 .4077*** 3.72%
Rob. MRR .3066 .3655*** 19.22%
Y! MRR .3010 .3459*** 14.93%
Res. MRR .4970 .4942 -0.56%
Table 6: Results optimizing parameters with train-
ing from other datasets. We also include default
and optimization on the same dataset for compar-
ison. Only MRR and MAP results are given.
DE would make the system more robust to those
sub-optimal parameters.
Table 6 includes a range of parameter set-
tings, including defaults, and optimized parame-
ters coming from the same and different datasets.
The values of the parameters are those in Table
4. The results show that when the parameters are
optimized in other datasets, DE provides improve-
ment with statistical significance in all cases, ex-
cept for the Robust dataset when using parameters
optimized from Yahoo! and vice-versa.
Overall, the table shows that our DE method ei-
ther improves the results significantly or does not
affect performance, and that it provides robustness
across different parameter settings, even with sub-
optimal values.
7 Exploring Document Length
The results in Table 6 show that the perfor-
mance improvements are best in the collection
with shortest documents (ResPubliQA). But the
results for Robust and Yahoo! do not show any re-
lation to document length. We thus decided to do
an additional experiment artificially shrinking the
document in Robust to a certain percentage of its
original length. We create new pseudo-collection
with the shrinkage factors of 2.5%, 10%, 20% and
50%, keeping the first N% words in the document
and discarding the rest. In all cases we used the
same parameters, as optimized for Robust.
Table 7 shows the results (MAP), with some
clear indication that the best improvements are ob-
14
tained for the shortest documents.
length base. expa. ?
2.5% 13 .0794 .0851 7.18%
10% 53 .1757 .1833 4.33%
20% 107 .2292 .2329 1.61%
50% 266 .3063 .3098 1.14%
100% 531 .3740 .3823 2.22%
Table 7: Results (MAP) on Robust when arti-
ficially shrinking documents to a percentage of
their length. In addition to the shrinking rate we
show the average lengths of documents.
8 Related Work
Given the brittleness of keyword matches, most
research has concentrated on Query Expansion
(QE) methods. These methods analyze the user
query terms and select automatically new related
query terms. Most QE methods use statistical
(or distributional) techniques to select terms for
expansion. They do this by analyzing term co-
occurrence statistics in the corpus and in the high-
est scored documents of the original query (Man-
ning et al, 2009). These methods seemed to im-
prove slightly retrieval relevance on average, but
at the cost of greatly decreasing the relevance of
difficult queries. But more recent studies seem
to overcome some of these problems (Collins-
Thompson, 2009).
An alternative to QE is to perform the expan-
sion in the document. Document Expansion (DE)
was first proposed in the speech retrieval commu-
nity (Singhal and Pereira, 1999), where the task
is to retrieve speech transcriptions which are quite
noisy. Singhal and Pereira propose to enhance the
representation of a noisy document by adding to
the document vector a linearly weighted mixture
of related documents. In order to determine re-
lated documents, the original document is used as
a query into the collection, and the ten most rele-
vant documents are selected.
Two related papers (Liu and Croft, 2004; Kur-
land and Lee, 2004) followed a similar approach
on the TREC ad-hoc document retrieval task.
They use document clustering to determine simi-
lar documents, and document expansion is carried
out with respect to these. Both papers report sig-
nificant improvements over non-expanded base-
lines. Instead of clustering, more recent work (Tao
et al, 2006; Mei et al, 2008; Huang et al, 2009)
use language models and graph representations of
the similarity between documents in the collec-
tion to smooth language models with some suc-
cess. The work presented here is complementary,
in that we also explore DE, but use WordNet in-
stead of distributional methods. They use a tighter
integration of their expansion model (compared to
our simple two-index model), which coupled with
our expansion method could help improve results
further. We plan to explore this in the future.
An alternative to statistical expansion methods
is to use lexical semantic knowledge bases such as
WordNet. Most of the work has focused on query
expansion and the use of synonyms from Word-
Net after performing word sense disambiguation
(WSD) with some success (Voorhees, 1994; Liu
et al, 2005). The short context available in
the query when performing WSD is an impor-
tant problems of these techniques. In contrast,
we use full document context, and related words
beyond synonyms. Another strand of WordNet
based work has explicitly represented and indexed
word senses after performing WSD (Gonzalo et
al., 1998; Stokoe et al, 2003; Kim et al, 2004).
The word senses conform a different space for
document representation, but contrary to us, these
works incorporate concepts for all words in the
documents, and are not able to incorporate con-
cepts that are not explicitly mentioned in the doc-
ument. More recently, a CLEF task was orga-
nized (Agirre et al, 2009a) where terms were se-
mantically disambiguated to see the improvement
that this would have on retrieval; the conclusions
were mixed, with some participants slightly im-
proving results with information from WordNet.
To the best of our knowledge our paper is the first
on the topic of document expansion using lexical-
semantic resources.
We would like to also compare our performance
to those of other systems as tested on the same
datasets. The systems which performed best in
the Robust evaluation campaign (Agirre et al,
2009a) report 0.4509 MAP, but note that they de-
ployed a complex system combining probabilis-
tic and monolingual translation-based models. In
ResPubliQA (Pen?as et al, 2009), the official eval-
15
uation included manual assessment, and we can-
not therefore reproduce those results. Fortunately,
the organizers released all runs, but only the first
ranked document for each query was included, so
we could only compute P@1. The P@1 of best
run was 0.40. Finally (Surdeanu et al, 2008) re-
port MRR figure around 0.68, but they evaluate
only in the questions where the correct answer
is retrieved by answer retrieval in the top 50 an-
swers, and is thus not comparable to our setting.
Regarding the WordNet expansion technique
we use here, it is implemented on top of publicly
available software4, which has been successfully
used in word similarity (Agirre et al, 2009b) and
word sense disambiguation (Agirre and Soroa,
2009). In the first work, a single word was in-
put to the random walk algorithm, obtaining the
probability distribution over all WordNet synsets.
The similarity of two words was computed as the
similarity of the distribution of each word, obtain-
ing the best results for WordNet-based systems on
the word similarity dataset, and comparable to the
results of a distributional similarity method which
used a crawl of the entire web. Agirre et al (2009)
used the context of occurrence of a target word to
start the random walk, and obtained very good re-
sults for WordNet WSD methods.
9 Conclusions and Future Work
This paper presents a novel Document Expan-
sion method based on a WordNet-based system
to find related concepts and words. The docu-
ments in three datasets were thus expanded with
related words, which were fed into a separate in-
dex. When combined with the regular index we
report improvements over MG4J usingwBM25 for
those three datasets across several parameter set-
tings, including default values, optimized param-
eters and parameters optimized in other datasets.
In most of the cases the improvements are sta-
tistically significant, indicating that the informa-
tion in the document expansion is useful. Similar
to other expansion methods, parameter optimiza-
tion has a stronger effect than our expansion strat-
egy. The problem with parameter optimization is
that in most real cases there is no tuning dataset
4http://ixa2.si.ehu.es/ukb
available. Our analysis shows that our expansion
method is more effective for sub-optimal param-
eter settings, which is the case for most real-live
IR applications. A comparison across the three
datasets and using artificially trimmed documents
indicates that our method is particularly effective
for short documents.
As document expansion is done at indexing
time, it avoids any overhead at query time. It
also has the advantage of leveraging full document
context, in contrast to query expansion methods,
which use the scarce information present in the
much shorter queries. Compared to WSD-based
methods, our method has the advantage of not
having to disambiguate all words in the document.
Besides, our algorithm picks the most relevant
concepts, and thus is able to expand to concepts
which are not explicitly mentioned in the docu-
ment. The successful use of background informa-
tion such as the one in WordNet could help close
the gap between semantic web technologies and
IR, and opens the possibility to include other re-
sources like Wikipedia or domain ontologies like
those in the Unified Medical Language System.
Our method to integrate expanded terms using
an additional index is simple and straightforward,
and there is still ample room for improvement.
A tighter integration of the document expansion
technique in the retrieval model should yield bet-
ter results, and the smoothed language models of
(Mei et al, 2008; Huang et al, 2009) seem a
natural choice. We would also like to compare
with other existing query and document expan-
sion techniques and study whether our technique
is complementary to query expansion approaches.
Acknowledgments
This work has been supported by KNOW2
(TIN2009-14715-C04-01) and KYOTO (ICT-
2007-211423) projects. Arantxa Otegi?s work is
funded by a PhD grant from the Basque Govern-
ment. Part of this work was done while Arantxa
Otegi was visiting Yahoo! Research Barcelona.
References
Agirre, E. and A. Soroa. 2009. Personalizing PageR-
ank for Word Sense Disambiguation. In Proc. of
16
EACL 2009, Athens, Greece.
Agirre, E., G. M. Di Nunzio, N. Ferro, T. Mandl,
and C. Peters. 2008. CLEF 2008: Ad-Hoc Track
Overview. In Working Notes of the Cross-Lingual
Evaluation Forum.
Agirre, E., G. M. Di Nunzio, T. Mandl, and A. Otegi.
2009a. CLEF 2009 Ad Hoc Track Overview: Ro-
bust - WSD Task. In Working Notes of the Cross-
Lingual Evaluation Forum.
Agirre, E., A. Soroa, E. Alfonseca, K. Hall, J. Kraval-
ova, and M. Pasca. 2009b. A Study on Similarity
and Relatedness Using Distributional and WordNet-
based Approaches. In Proc. of NAACL, Boulder,
USA.
Boldi, P. and S. Vigna. 2005. MG4J at TREC 2005.
In The Fourteenth Text REtrieval Conference (TREC
2005) Proceedings, number SP 500-266 in Special
Publications. NIST.
Collins-Thompson, Kevyn. 2009. Reducing the risk
of query expansion via robust constrained optimiza-
tion. In Proceedings of CIKM ?09, pages 837?846.
Fellbaum, C., editor. 1998. WordNet: An Elec-
tronic Lexical Database and Some of its Applica-
tions. MIT Press, Cambridge, Mass.
Gonzalo, J., F. Verdejo, I. Chugur, and J. Cigarran.
1998. Indexing with WordNet synsets can improve
text retrieval. In Proceedings ACL/COLING Work-
shop on Usage of WordNet for Natural Language
Processing.
Haveliwala, T. H. 2002. Topic-sensitive PageRank. In
Proceedings of WWW ?02, pages 517?526.
Huang, Yunping, Le Sun, and Jian-Yun Nie. 2009.
Smoothing document language model with local
word graph. In Proceedings of CIKM ?09, pages
1943?1946.
Kim, S. B., H. C. Seo, and H. C. Rim. 2004. Informa-
tion retrieval using word senses: root sense tagging
approach. In Proceedings of SIGIR ?04, pages 258?
265.
Kurland, O. and L. Lee. 2004. Corpus structure, lan-
guage models, and ad hoc information retrieval. In
Proceedings of SIGIR ?04, pages 194?201.
Liu, X. and W. B. Croft. 2004. Cluster-based retrieval
using language models. In Proceedings of SIGIR
?04, pages 186?193.
Liu, S., C. Yu, and W. Meng. 2005. Word sense dis-
ambiguation in queries. In Proceedings of CIKM
?05, pages 525?532.
Manning, C. D., P. Raghavan, and H. Schu?tze. 2009.
An introduction to information retrieval. Cam-
bridge University Press, UK.
Mei, Qiaozhu, Duo Zhang, and ChengXiang Zhai.
2008. A general optimization framework for
smoothing language models on graph structures. In
Proceedings of SIGIR ?08, pages 611?618.
Pen?as, A., P. Forner, R. Sutcliffe, A. Rodrigo,
C. Fora?scu, I. Alegria, D. Giampiccolo, N. Moreau,
and P. Osenova. 2009. Overview of ResPubliQA
2009: Question Answering Evaluation over Euro-
pean Legislation. In Working Notes of the Cross-
Lingual Evaluation Forum.
Robertson, S. and H. Zaragoza. 2009. The Proba-
bilistic Relevance Framework: BM25 and Beyond.
Foundations and Trends in Information Retrieval,
3(4):333?389.
Singhal, A. and F. Pereira. 1999. Document expansion
for speech retrieval. In Proceedings of SIGIR ?99,
pages 34?41, New York, NY, USA. ACM.
Smucker, M. D., J. Allan, and B. Carterette. 2007. A
comparison of statistical significance tests for infor-
mation retrieval evaluation. In Proc. of CIKM 2007,
Lisboa, Portugal.
Stokoe, C., M. P. Oakes, and J. Tait. 2003. Word sense
disambiguation in information retrieval revisited. In
Proceedings of SIGIR ?03, page 166.
Surdeanu, M., M. Ciaramita, and H. Zaragoza. 2008.
Learning to Rank Answers on Large Online QA
Collections. In Proceedings of ACL 2008.
Tao, T., X. Wang, Q. Mei, and C. Zhai. 2006. Lan-
guage model information retrieval with document
expansion. In Proceedings of HLT/NAACL, pages
407?414, June.
Voorhees, E. M. 1994. Query expansion using lexical-
semantic relations. In Proceedings of SIGIR ?94,
page 69.
17
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2260?2269, Dublin, Ireland, August 23-29 2014.
?One Entity per Discourse? and ?One Entity per Collocation?
Improve Named-Entity Disambiguation
Ander Barrena*, Eneko Agirre*, Bernardo Cabaleiro**, Anselmo Pe
?
nas**, Aitor Soroa*
*IXA NLP Group / University of the Basque Country, Basque Country
abarrena014@ikasle.ehu.es, e.agirre@ehu.es, a.soroa@ehu.es
**UNED NLP & IR Group, Madrid
anselmo@lsi.uned.es, bcabaleiro@lsi.uned.es
Abstract
The ?one sense per discourse? (OSPD) and ?one sense per collocation? (OSPC) hypotheses have
been very influential in Word Sense Disambiguation. The goal of this paper is twofold: (i) to
explore whether these hypotheses hold for entities, that is, whether several mentions in the same
discourse (or the same collocation) tend to refer to the same entity or not, and (ii) test their impact
in Named-Entity Disambiguation (NED). Our experiments show consistent results on different
collections and three state-of-the-art NED system. OSPD hypothesis holds in around 96%-98%
of documents whereas OSPC hypothesis holds in 91%-98% of collocations. Furthermore, a
simple NED post-processing in which the majority entity is promoted, produces a gain in perfor-
mance in all cases, reaching up to 8 absolute points of improvement in F-measure. These results
show that NED systems would benefit of considering these hypotheses into their implementation.
1 Introduction
The ?one sense per discourse? (OSPD) hypothesis was introduced by Gale et al. (1992), and stated that a
word tends to preserve its meaning when occurring multiple times in a discourse. They estimated that the
probability of two occurrences of the same polysemous noun drawn from one document having the same
sense to be around 94% for documents from Grolier encyclopedia, and 96% for documents from Brown,
based on word senses from the Oxford Advanced Learner?s Dictionary and a handful of examples. A few
years later, Krovetz (1998) reported 66% on larger corpora (SemCor and DSO) annotated with WordNet
senses by third parties, but, unfortunately, he only reported how many polysemous nouns occurred with
a single sense in all documents, not in each document. In the context of statistical machine translation,
Carpuat (2009) reported that, 80% of the time, words occurring multiple times in a source document are
translated into a single word in the target language.
In the case of entities, OSPD is closely related to coreference, where the task is to find whether two
different mentions (perhaps using different surface strings like ?John? and ?he?) in a document refer
to the same entity or not. For instance, the coreference system presented by (Lee et al., 2013), uses a
heuristic which links mentions in a document that share the same surface string: ?This sieve [heuristic]
accounts for approximately 16 CoNLL F1 points improvement, which proves that a signicant percentage
of mentions in text are indeed repetitions of previously seen concepts?. Our paper actually quantifies the
amount of those repetitions for entities, providing additional evidence for the heuristic.
The ?one sense per collocation? (OSPC) hypothesis was introduced by Yarowsky (1993), stating that
a word tends to preserve its meaning when occurring with the same collocate. Yarowsky tested his
hypothesis for several definitions of collocate, including positional collocates (word to left or right)
and syntactic collocations (governing verb of object, governing verb of subject, modifying adjective).
He reported entropy on train data, as well as disambiguation performance on unseen data, with the
precision ranging between 90% and 99% for a handful of words with two distinct homograph senses,
like, e.g. ?bass? or ?colon?. In larger-scale research, Martinez and Agirre (2000) measured the precision
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2260
Abbott Beefs Up Litigation Reserves NORTH CHICAGO, Ill. (AP) Abbott Laboratories Inc., bracing
for a costly settlement in a federal investigation involving the prostate-cancer drug Lupron, said Friday
it was increasing litigation reserves by $344 million. As part of the announcement, Abbott said it had
restated its quarterly results and is now reporting a loss of $319.9 million for the first three months
of this year rather than a profit. The move comes amid long-running negotiations between the U.S.
Department of Justice and TAP Pharmaceutical Products, the 50-50 joint venture between Abbott and
Takeda Chemical Industries of Japan that made Lupron. Abbott said in January ...
Figure 1: Example of OSPD for entities. All occurrences of ?Abbott? refer to ?Abbott Laboratories?.
of similar collocations on corpora (Semcor and DSO) annotated by third parties with finer-grained senses
from WordNet, reporting lower figures around 70%.
In this paper, we take a collocation to be a word (or multiword term) that co-occurs with the target
named-entity more often than would be expected by chance. In our case we use syntactic dependencies
to extract co-occurring terms.
These two hypotheses have been very influential, and have inspired multiple heuristics and methods
in Word Sense Disambiguation research (Agirre and Edmonds, 2007, Chapters 5,7,10,11). In this work
we are going to show that both hypotheses hold for named-entities as well, and that the hypotheses can
be used to post-process the output of any Named-Entity Disambiguation system (NED) to improve its
performance. NED, also known as Entity Linking, takes as input a named-entity mention in context and
assigns it a specific entity from a given entity repository (Hachey et al., 2012; Daiber et al., 2013).
In the first part of this work we are going to test whether the two hypotheses hold for entity mentions
with respect to a repository of entities extracted from Wikipedia. For instance, do all occurrences of
mention ?Abbott? in a document refer to the same entity? Do all occurrences of mention ?CPI? as
subject of verb ?rise? refer to the same entity? Do all occurrences of ?CDU? in relation to ?Merkel? refer
to the same entity? The examples in Figures 1 and 2 show evidence that this is indeed the case. The
experiments aim at quantifying in which degree OSPD and OSPC hypotheses hold for entities
1
.
In the second part of the paper, we will explore a simple method to incorporate OSPD and OSPC
hypotheses to any existing NED system, showing their potential. After running the NED system, we take
its output and observe, for each mention string, which is the entity returned most often for a given docu-
ment (or collocation), assigning to all occurrences the majority entity. We tested the improvements with a
freely available NED system (Daiber et al., 2013), a reimplementation of a strong Bayesian NED system
(Han and Sun, 2011) and an in-house graph-based system. We got statistically significant improvements
for all systems and ?one sense? hypotheses that we tested, with a couple exceptions.
In order to check the OSPD and OSPC hypotheses for entities, we first looked into existing datasets.
AIDA (Hoffart et al., 2011)
2
is a publicly available hand-tagged corpus based on the CoNLL named-
entity recognition and disambiguation task dataset. AIDA contains links of all entity mentions in full
documents, so it is a natural fit for OSPD. We estimated OSPD based on more than 4,000 mentions that
occur multiple times in a document. For completeness, we also estimated OSPD at the collection level.
OSPD and OSPC are independent of each other, as one is applied at the document level and the other
at the corpus level, focusing on the entities that occur with a specific collocation. Multiple occurrences
of a target string in a document usually occur with different collocations, and conversely, multiple occur-
rences of a target string with a specific collocation typically occur in different documents. Note also that
singletons (entities that are only mentioned once in a document) are not affected by OSPD, but could be
affected by OSPC.
In order to estimate OSPC, no available corpus existed, so we decided to base our dataset on the TAC
KBP 2009 Entity Linking dataset
3
(TAC2009 for short) (Ji et al., 2010). The TAC2009 dataset involves
138 mention strings, which have been annotated in several documents drawn primarily from Gigaword
4
.
1
For the sake of clarity we will also refer to OSPD and OSPC for entities as OSPD and OSPC.
2
http://www.mpi-inf.mpg.de/yago-naga/aida/downloads.html
3
http://www.nist.gov/tac/2013/KBP/EntityLinking/index.html
4
http://catalog.ldc.upenn.edu/LDC2003T05
2261
CPI subject-of rise:
China?s consumer price index, or CPI, rose 2.8 percent last December.
In the 10 months to October, the CPI rose 1.35 percent, the core price index grew 1.13 percent ...
Measured on a month-on-month basis, March CPI rose 2.3 percent from February, ...
... still lower than in China, Hong Kong and Singapore, whose CPIs have rised 8.0 percent, ...
The core CPI rose 0.2 percent, in line with Wall Street expectations.
Angela Merkel has CDU:
... who share power with Merkel?s CDU nationally in an uneasy ? grand coalition ? ...
Economy Minister Michael Glos, also from the CSU, the sister party to Merkel?s CDU ...
In the past Merkel?s CDU had been able to rely on the CSU?s strength in Bavaria ...
... but while her conservative CDU wanted new legal tools to do so, ...
The new development has put a further strain on Merkel?s CDU ...
Figure 2: Examples of OSPC for entities, showing five examples for a syntactic collocation (top row)
and fie examples for a more specific proposition (bottom row). ?CPI? might refer to ?Comunist Party
of India? or ?Consumer Price Index?, among others, but refers to the second in all cases. ?CDU? can
refer to the German ?Christian Democratic Union? or ?Catholic Distance University?, among others, but
refers to the first in all cases.
We extracted several syntactic collocations for those 138 mention strings from Gigaword, and hand-
annotated them, yielding an estimate for the OSPC. Note that TAC2009 only provides the annotation for
a specific mention in a document, so we had to annotate by hand the rest of occurrences in the documents.
For instance, we analyzed examples of ?CPI? as subject of the verb ?rise? (cf. Figure 2). Some of the
syntactic collocations like the subjects of verb ?has? seemed very uninformative, so we decided to also
check the OSPC hypothesis on more specific collocations, involving more complete argument structures.
For instance, we checked ?ABC? occurring as subject of ?has? with object ?radio?. We call this more
specific collocations propositions (Pe?nas and Hovy, 2010).
The paper is structured as follows. We will first present the resources used in this study. Section 3
presents the results of OSPD. Section 3.1 extends OSPD when, instead of documents, we take the com-
plete collection. Section 4 presents the study of OSPC both for syntactic dependencies and propositions.
Section 5 presents the experiments where OSPD and OSPC are used to improve the performance of
existing systems. Finally, we draw the conclusions and future work.
2 Resources used
AIDA is based on the corpus used in the CONLL named-entity recognition and classification task, where
all entities in full documents had been linked to the referred Wikipedia articles (using the 2010 Wikipedia
dump). We use the full AIDA dataset, with 1,393 documents, 34,140 disambiguated entity mentions,
where 27,240 are linked to a Wikipedia article. All in all there are 6,877 distinct mention strings (types)
which are linked at least once to a Wikipedia article. The rest refer to articles not in Wikipedia (NIL
instances), and were discarded. This corpus covers news from a sample of a few days spanning from
1996-05-28 to 1996-12-07.
In order to prepare our dataset for OSPC, we chose the dataset of the TAC KBP 2009 Entity Linking
competition, as this dataset have been extensively used in Entity Linking evaluation. In addition, the cor-
pus used in the task was very large, allowing us to mine relevant collocations (see below). We manually
annotated the occurrences in the extracted collocations, producing two datasets, one for each kind of col-
location (cf. Section 4). Note that the TAC KBP organizers only annotated one specific mention in each
target document. For completeness, we also tagged the rest of the occurrences of the target mentions in
the documents, thus allowing us to provide OSPD estimated based on TAC2009 data as well. This is
the third dataset that we annotated by hand. The hand-annotation was performed by a single person, and
later reviewed by the rest of the authors. The three annotation datasets are publicly available
5
. Hand-
5
http://ixa2.si.ehu.es/OEPDC
2262
NHasN ?U.S. dollar?
NPN ?condition of anonymity?
NVN ?official tells AFP?
NVNPN ?article maintains interest within layout?
NVPN ?others steal from input?
VNPN ?includes link to website?
Table 1: List of the six patterns used to extract propositions, with some examples.
tagging is costly, so we tagged around 250 examples of syntactic collocations and around 250 examples
of propositions.
Note that both AIDA and TAC2009 contain mentions that were not linked to a Wikipedia article
because the mention referred to an entity which was not listed in the entity inventory. We ignored all
those cases (called NIL cases), as we would need to investigate, for each NIL, which actual entity they
refer to.
The collocations were extracted from the TAC KBP collection (Ji et al., 2010), comprising 1.7 mil-
lion documents, 1.3 millions from newswire and 0.5 millions from the web. We have parsed them with
the Stanford CoreNLP software (Klein and Manning, 2003), obtaining around 650 million dependen-
cies (De Marneffe and Manning, 2008). We selected subject, object, prepositional complements and
adjectival modifiers as the source for syntactic collocations. In order to provide more specific collo-
cations, we implemented the syntactic patterns proposed in (Pe?nas and Hovy, 2010), which produce
so-called propositions. The result is a database with 16 million distinct propositions. Table 1 shows the
six patterns used in this work, together with some examples.
In order to know whether a mention is ambiguous, we built a dictionary based on Wikipedia which
lists, for each string mention, which entities it can refer to. We followed the construction method of
(Spitkovsky and Chang, 2012), which checked article titles, redirects, disambiguation pages and hyper-
links to find mention strings that can be used to refer to entities. Contrary to them, we could not access
hyperlinks in the web, so we could use only those in Wikipedia. According to our dictionary, the am-
biguity of the mentions that we are studying is very high, 26.4 entities on average for the mentions in
AIDA, and 62.6 entities on average for the mentions in TAC2009.
3 One entity per discourse
In order to estimate OSPD we divided the number of times a mention string referred to different entities
in the document with the number of times a mention string occurred multiple times in the document. In
the denominator and numerator we count each mention-document pair once.
Regarding AIDA, we found 12,084 occurrences of mentions which occurred more than once in a
document, making 4,265 unique mention-document pairs
6
(cf. Table 2). In the vast majority of the
cases those mentions refer to a single entity in the document, and only in 170 cases the mentions in the
document refer to several entities. The last row in Table 2 shows the ratio between those values, 96.01%,
showing that OSPD is strong in this dataset.
We also checked OSPD in the TAC2009 dataset. Out of the 138 distinct mention strings used in the
task, we discarded those only linked to NIL (that is, no corresponding Wikipedia article existed) and
those which were not ambiguous (that is, they had only one entity in the dictionary, cf. Section 2). That
leaves 105 mention strings, occurring 1,776 times in 918 different documents, which we annotated by
hand. The 105 strings occurred 1,776 times in 918 documents. Removing the cases where the mention
occurred only once, we were left with 1,173 occurrences, which make 334 unique mention-document
pairs, of which only 6 occurred with more than one sense (rightmost row in Table 2). This yields an
estimate for OSPD of 98.2%.
6
By unique mention-document pairs we mean that we only count once for a mention occurring multiple times in a document.
For instance if mention Smith occurs 10 times in the whole corpus, 8 times in document A and 2 times in document B, we count
two unique mention-document pairs.
2263
AIDA TAC2009
Mention-document pairs 4,265 334
Ambiguous pairs 170 6
OSPD 96.0% 98.2%
Table 2: One entity per discourse: per document statistics in AIDA and TAC2009 datasets. Pairs stand
for the number of unique mention-document pairs. The 4,265 pairs in AIDA correspond to 12,084
occurrences of mentions, and the 334 pairs in TAC2009 correspond to 1,173 occurrences.
All mentions First mention
AIDA TAC2009 AIDA TAC2009
Mention types 3,363 105 2,731 105
Ambiguous types 475 26 454 25
OSPD (collections) 85.9% 75.2% 83.4% 76.2%
Table 3: One entity per collection: statistics in AIDA and TAC2009. In the first two columns (?All
mentions?) we consider all mention types (3, 363 types in AIDA correspond to 23, 726 occurrences of
mentions, and 105 types in TAC2009 correspond to 1, 776 occurrences). In the second two columns
(?First mention?) we leave only the first mention of each document (in this case, there are 2, 731 mention
types in AIDA which correspond to 15, 275 occurrences, and 105 types in TAC2009 corresponding to
941 occurrences).
Finally, we also thought about measuring OSPD on the Wikipedia articles, where many mentions
have been manually linked to their respective article. Unfortunately, we noted that Wikipedia guidelines
explicitly prevent authors linking a mention multiple times: Generally, a link should appear only once
in an article, but if helpful for readers, links may be repeated in infoboxes, tables, image captions,
footnotes, and at the first occurrence after the lead
7
. The fact that Wikipedia editors did not explicitly
state exceptions to the above rule (e.g. for cases where the word or phrase is used to refer to two different
articles, thus breaking the OSPD hypothesis) is remarkable, and might indicate that Wikipedia editors
had not felt the need to challenge the OSPD hypothesis.
3.1 One entity per collection
We took the opportunity to also explore ?one entity per collection?, which gives an idea of what is
the spread of entities for whole document collections. In this case, there is no need to count mention-
document pairs, as there is one single document, the collection, so we estimate the hypothesis according
to mention types. The first two columns in table 3 shows that, overall, mentions which occurred more
than once in the collection tend to refer to the same entity 85.9% of the time in AIDA, and 75.2% of the
time in TAC2009.
As we know that multiple mentions in a document tend to refer to one entity, the second two columns
in table 3 offers the statistics when factoring out multiple occurrences of mention in a document, that is,
leaving the first mention in each document. The statistics are very similar, with minor variations.
We think that the lower estimate for TAC2009 is an artifact of how the TAC KBP organizers set up the
dataset, as they were explicitly looking for cases where the target string would refer to different entities,
making the task more challenging for NED systems. This fact does not affect OSPD for documents, as
those strings still tend to refer to a single entity per document, but given the need to find occurrences
for different entities, the organizers (Ji et al., 2010) did focus on strings occurring with different entities
across the document collection. This is in contrast with AIDA, where they tagged all named-entities
occurring in the target documents. Had the organizers of TAC2009 focused on a random choice of
strings and documents, the one entity per collection would also hold to the high degree exhibited in
AIDA, as the genre of most of the documents is also news (as in AIDA).
7
http://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking#What_generally_
should_be_linked
2264
Syn. coll. Propositions
Mention-collocation pairs 58 61
Ambiguous pairs 5 1
OSPC 91.4% 98.4%
Table 4: One entity per collocation: statistics for syntactic collocations and propositions. The 58
mention-collocation pairs correspond to 262 occurrences, and the 61 mention-proposition pairs to 279.
4 One entity per collocation
In order to estimate OSPC for syntactic collocations, we manually annotated several occurrences of the
138 mention strings of the TAC2009 dataset. Hand-tagging mention entities is a costly process, so we
chose (at random) one syntactic dependency relation for each of the 138 mention strings that occurred
more than five times in the corpus. We then hand-tagged at random five occurrences of each collocation
(cf. Figure 2). This method would provide a maximum of 5 examples for each of the 138 mentions, but
after checking the minimum frequency of the collocations, the quality of the context, repeated sentences,
mentions that are not ambiguous in the dictionary, and whether the mention could be attached to an
entity in the database, the actual number was lower. All in all we found 58 mention-collocation pairs
(262 occurrences) for syntactic collocations (cf. middle column in Table 4). Only 5 mentions referred to
more than one entity per collocation, yielding that OSPC for syntactic collocation is around 91.4%.
To gather the dataset for propositions, we followed the same method as for the syntactic collocations,
that is, we chose (at random) one propositions involving one of the 138 mention strings that occurred
more than five times in the corpus, and hand-tagged at random five occurrences of each proposition
(cf. Figure 2. As with syntactic collocations, we also found a limited number of mentions filling the
desired properties. That left 61 mention-collocation pairs (279 occurrences) for propositions (cf. right
column in Table 4). Only 1 mention referred to more than one entity per proposition, yielding OSPC
for propositions around 98.4%. This shows that the more specific the context is, the stronger is the link
between mention and entity.
5 Improving performance
In order to check whether any of the ?one sense? hypothesis above could improve the performance of
a NED system, we followed a simple procedure: After running the NED system, we take its output
and observe, for each mention string, which is the entity returned most often for a given document (or
collocation), assigning to all occurrences the majority entity. In case of ties, we return the entity with the
highest support from the NED system. We tested the improvements on three NED systems: the freely
available DBpedia Spotlight, a reimplementation of a strong Bayesian NED system and a graph-based
system.
DBpedia Spotlight is a freely available NED system (Daiber et al., 2013), based on a generative proba-
bilistic model (Han and Sun, 2011). Nowadays it is one of the most widely used NED systems and attains
performances close to state-of-the-art (Daiber et al., 2013)We used the default values of the parameters
for all the experiments in this paper.
We also tested an in-house reimplementation of the generative probabilistic model presented in (Han
and Sun, 2011). This is a state-of-the-art system which got the same accuracy as the best participant
(72.0) when evaluated in the non-NIL subset of TAC2013.
UKB is a freely-available system for performing Word Sense Disambiguation and Similarity based
on random walks on graphs (Agirre and Soroa, 2009). Instead of using it on WordNet, we represented
Wikipedia as a graph, where vertices are the wikipedia articles and edges represents bidirectional hy-
perlinks among Wikipedia pages, effectively implementing a NED system. We used a Wikipedia dump
from 2013 in our experiments. UKB is a competitive, state-of-the-art system which attained a score of
69.0 when evaluated in the non-NIL subset of the TAC2013 dataset.
The input of the systems is the context of each mention to be disambiguated, in the form of a 100
token window centered in the target mention. In NED, the identification of the correct mention to be
2265
Mention in context Entity
Abbott Beefs Up Litigation ... Abbot Kinney
Abbott Laboratories Inc., bracing ... Abbott Laboratories
Abbott said it had restated ... Abbott Laboratories
venture between Abbott and Takeda ... Abbott Laboratories
Abbott said in January ... Abbott Laboratories
Figure 3: Applying OSPD: Each of the five occurrences of Abbott in the document in Figure 1 has been
tagged independently by a NED systems, which return the correct entity in all but one case (precision
80%). Applying OSPD would return the correct entity (Abbott Laboratories) in all cases, improving
precision to 100%.
AIDA TAC 2009
Prec. Recall F1 Prec. Recall F1
Spotlight 83.24 63.90 72.30 64.48 46.44 53.99
+ OSPD Discourse 84.17 70.01 76.44 64.65 48.50 55.42
+ OSPD Collection 84.02 74.64 79.05 56.24 47.98 51.78
UKB 70.09 69.03 69.55 67.70 67.64 67.67
+ OSPD Discourse 71.30 70.23 70.76 70.21 70.21 70.21
+ OSPD Collection 75.79 74.64 75.21 68.84 68.84 68.84
(Han and Sun, 2011) 65.71 65.11 65.41 65.49 65.49 65.49
+ OSPD Discourse 67.77 67.37 67.57 66.27 66.27 66.27
+ OSPD Collection 74.29 73.89 74.09 68.24 68.24 68.24
Table 5: Applying OSPD: NED performance on AIDA and TAC2009 OSPD datasets, including each of
the three NED systems, and the results after applying OSPD at the document and collections levels. Bold
marks best result for each system.
disambiguated is part of the problem. AIDA does provide gold mentions, but TAC2009 only provides a
query string which might be just a substring of the real mention in the document. We treated both corpus
in the same way. In the case of DBpedia Spotlight we use the built-in mention spotter. In the case of our
in-house implementations, we use the longest string that matches a valid entity mention in the system, as
given by the dictionary (cf. Section 3).
Some of the NED systems do not return an entity for all mentions, so we evaluate precision, recall and
the harmonic mean (F1 measure). Statistical significance has been estimated using Wilcoxon. We reused
the same corpora as in the previous sections for the evaluation, and also removed all NIL mentions (i.e.
mentions which refer to an entity not in Wikipedia).
5.1 One entity per discourse
We report the improvements using OSPD for both document and collection levels. At the document
level, we relabel mentions that occur multiple times in a document using the entity returned most times
by the NED system in that document. Figure 3 illustrates the idea for a NED system on the same sample
document as in Figure 1. At the collection level, we relabel mentions using the entity returned most
times by the NED systems in the whole collection.
Table 5 reports the results of the performance as evaluated on mentions occurring multiple times in
the AIDA and TAC2009 datasets. The numbers in the left part of the table correspond to the perfor-
mance as evaluated on mentions occurring multiple times in AIDA documents. Note that the number of
occurrences where OSPD at the collection level can be applied is larger (a superset of those for OSPD
at the document level), as, for instance, a mention string occurring once in three different documents
won?t be affected by OSPD at the document level, but it could be relabeled at the collection level. We
were especially interested in making the numbers between OSPD at the document and collection levels
2266
CPI subject-of rise Angela Merkel has CDU:
Consumer price index Christian Democratic Union (Germany)
Consumer price index Catholic Distance University
Communist Party of India Christian Democratic Union (Germany)
Communist Party of India Christian Democratic Union (Germany)
Consumer price index Christian Democratic Union (Germany)
Figure 4: Applying OSPC: A NED system system tagged each example in Figure 2 independently. For
CPI, the precision is 60%, but after relabeling with OSPC it would be 100%. For CDU, the improvement
is from 80% to 100%.
Syntactic collocations Propositions
prec. recall F1 prec. recall F1
Spotlight 82.46 66.41 73.57 74.67 60.22 66.67
+ OSPC 82.63 67.18 74.11 74.79 62.72 68.23
UKB 75.86 75.57 75.72 67.87 67.38 67.63
+ OSPC 78.54 78.24 78.39 68.59 68.10 68.35
(Han and Sun, 2011) 75.57 75.57 75.57 71.33 71.33 71.33
+ OSPC 78.24 78.24 78.24 73.12 73.12 73.12
Table 6: Applying OSPC: NED performance on TAC2009, including each of the three NED systems,
and the results after applying OSPC for syntactic collocations and propositions. Bold is used for best
results for each system.
directly comparable, and therefore report the results on the same occurrences, that is, the occurrences
where OSPD at the document level can be applied.
The results show a small but consistent improvement for OSPD at the document level in precision,
recall and F1 for the three NED systems, around 1 or 2 absolute points. The improvements when applying
OSPD at the collection level are also consistent, but remarkably larger, between 5 and 9 absolute points.
All improvements are statistically significant (p-value below 0.01).
Table 5 also reports the results after applying OSPD to TAC2009 instances which occurred more
than once in a document. Results for OSPD at document level and collection level follow the same
methodology as for AIDA. The improvement at the collection level is not so consistent, with a loss in
performance for Spotlight, a small improvement for UKB, and a larger improvement for (Han and Sun,
2011). All differences across the table are statistically significant (p-value below 0.01).
While the OSPD at the document level is strong in both corpora, Section 3.1 showed that the OSPD
at the collection level is only strong in AIDA, with a much lower estimate in TAC2009. This fact
would explain why the improvement with OSPD at the collection level is not consistent. Following
the rationale in Section 3.1, we think that had the organizers of the task chosen strings and documents
at random, the improvement in TAC 2009 at the collection level would be also as high as in AIDA. The
high improvement in AIDA at the collection level compared to the more modest improvement at the
document level, despite having a lower OSPD estimate (cf. Section 3.1), could be caused by the fact that
there are more occurrences and evidence in favor of the majority entity.
5.2 One entity per collocation
Figure 4 shows the application of OSPC to the output of a NED system to two sample collocations in our
dataset. In this case, the application of OSPC would increase precision to 100%. The actual result on the
datasets produced in Section 4 for syntactic collocations and propositions is reported on table 6.
Regarding syntactic collocations, table 6 shows that the improvement is small but consistent for the
three systems on precision, recall and F1, ranging from 0.5 to 2.5 absolute points in F1 score. The results
for propositions also show the same trend, with consistent improvements across the table. All differences
2267
in the two tables are statistically significant (p-value < 0.01), except for UKB.
6 Conclusions and future work
Our study shows that OSPD holds for 96%-98% (in the AIDA and TAC2009 datasets, respectively)
of the mentions that occur multiple times in documents. We also measured OSPD at the collection
level (86% and 75%, respectively). OSPC holds for 91% of the mentions that occur multiple times in
the syntactic collocations that we studied, and 98% of the mentions that occur multiple times in more
specific collocations. We reused the publicly available AIDA dataset for estimating OSPD. In addition,
we created a dataset to study OSPC based on the TAC KBP Entity Linking 2009 task dataset, which is
publicly available
8
.
We carefully chose to estimate both OPSD and OSPC on TAC2009, in order to make the numbers
between OSPD and OSPC comparable. The OSPD numbers for AIDA are very similar to those obtained
on TAC2009, providing complementary evidence. Although the high estimate of OSPD for entities was
somehow expected, the high estimate of OSPC for the syntactic collocations, especially the propositions,
was somehow unexpected, given the high ambiguity rate of the discussed strings, and the fact that the
ambiguity included similar entities, like for instance ?ABC? which can refer, among other 190 entities,
to the American Broadcasting Company or the Australian Broadcasting Corporation.
Our results also show that a simple application of the OSPD and OSPC hypotheses to the output of
three different NED systems improves the results in all cases. Remarkably, the highest performance gain,
8 absolute points, was for OSPD at the collection level in the AIDA corpus.
The results presented here could be largely dependent on the domain and genre of the documents,
as well as the definition of collocation. Our work is a strong basis for claiming that OSPD and OSPC
hold for entities, but the evidence could be further extended exploring alternative operationalization of
collocations and a larger breadth of genres and domains.
For the future we would like to check whether these hypotheses can be further used to improve current
NED systems. The OSPD hypothesis can be used to jointly disambiguate all occurrences of a mention
in a document. The OSPC hypothesis could be used to acquire important disambiguation features, or to
perform large-scale joint entity linking. The OSPD for whole collections could be useful for documents
on specific domains, and for domain adaptation scenarios.
Acknowledgements
This work was partially funded by MINECO (CHIST-ERA READERS project ? PCIN-2013-002- C02-
01) and the European Commission (QTLEAP ? FP7-ICT-2013.4.1-610516, OPENER ? FP7-ICT-2011-
SME-DCL-296451). Ander Barrena is supported by a PhD grant from the University of the Basque
Country.
References
Eneko Agirre and Philip Edmonds. 2007. Word Sense Disambiguation: Algorithms and Applications. Springer
Publishing Company, Incorporated, 1st edition.
Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings
of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ?09,
pages 33?41.
Marine Carpuat. 2009. One translation per discourse. In Proceedings of the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions, DEW ?09, pages 19?27, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Joachim Daiber, Max Jakob, Chris Hokamp, and Pablo N. Mendes. 2013. Improving efficiency and accuracy
in multilingual entity extraction. In Proceedings of the 9th International Conference on Semantic Systems,
I-SEMANTICS ?13, pages 121?124, New York, NY, USA. ACM.
8
http://ixa2.si.ehu.es/OEPDC
2268
Marie-Catherine De Marneffe and Christopher D. Manning. 2008. Stanford typed dependencies manual. URL
http://nlp. stanford. edu/software/dependencies manual. pdf.
William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of
the workshop on Speech and Natural Language, HLT ?91, page 233237, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2012. Evaluating Entity
Linking with Wikipedia. Artif. Intell., 194:130?150, January.
Xianpei Han and Le Sun. 2011. A Generative Entity-mention Model for Linking Entities with Knowledge Base.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, volume 1, pages 945?954.
Johannes Hoffart, Mohamed A. Yosef, Ilaria Bordino, Hagen F?urstenau, Manfred Pinkal, Marc Spaniol, Bilyana
Taneva, Stephan Thater, and Gerdhard Weikum. 2011. Robust Disambiguation of Named Entities in Text.
In Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland, United Kingdom
2011, pages 782?792.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Griffitt, and Joe Ellis. 2010. Overview of the tac 2010 knowledge
base population track. In Third Text Analysis Conference (TAC 2010).
Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics-Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
Robert Krovetz. 1998. More than one sense per discourse. In NEC Princeton NJ Labs., Research Memorandum.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics,
39(4).
David Martinez and Eneko Agirre. 2000. One sense per collocation and genre/topic variations. In Proceedings
of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large
corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics -
Volume 13, EMNLP ?00, page 207215, Stroudsburg, PA, USA. Association for Computational Linguistics.
Anselmo Pe?nas and Eduard Hovy. 2010. Filling knowledge gaps in text for machine reading. In Proceedings
of the 23rd International Conference on Computational Linguistics: Posters, pages 979?987. Association for
Computational Linguistics.
Valentin I. Spitkovsky and Angel X. Chang. 2012. A Cross-lingual Dictionary for English Wikipedia Concepts.
Eighth International Conference on Language Resources and Evaluation (LREC 2012).
David Yarowsky. 1993. One sense per collocation. In Proceedings of the workshop on Human Language Tech-
nology, HLT ?93, page 266271, Stroudsburg, PA, USA. Association for Computational Linguistics.
2269
Selectional Preferences for
Semantic Role Classification
Ben?at Zapirain?
University of the Basque Country
Eneko Agirre??
University of the Basque Country
Llu??s Ma`rquez?
Universitat Polite`cnica de Catalunya
Mihai Surdeanu?
University of Arizona
This paper focuses on a well-known open issue in Semantic Role Classification (SRC) research:
the limited influence and sparseness of lexical features. We mitigate this problem using models
that integrate automatically learned selectional preferences (SP). We explore a range of models
based on WordNet and distributional-similarity SPs. Furthermore, we demonstrate that the SRC
task is better modeled by SP models centered on both verbs and prepositions, rather than verbs
alone. Our experiments with SP-based models in isolation indicate that they outperform a lexical
baseline with 20 F1 points in domain and almost 40 F1 points out of domain. Furthermore, we
show that a state-of-the-art SRC system extended with features based on selectional preferences
performs significantly better, both in domain (17% error reduction) and out of domain (13%
error reduction). Finally, we show that in an end-to-end semantic role labeling system we obtain
small but statistically significant improvements, even though our modified SRC model affects
only approximately 4% of the argument candidates. Our post hoc error analysis indicates that
the SP-based features help mostly in situations where syntactic information is either incorrect or
insufficient to disambiguate the correct role.
? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.
E-mail: benat.zapirain@ehu.es.
?? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.
E-mail: e.agirre@ehu.es.
? UPC Campus Nord (Omega building), Jordi Girona 1?3, 08034 Barcelona, Catalonia.
E-mail: lluism@lsi.upc.edu.
? 1040 E. 4th Street, Tucson, AZ 85721. E-mail: msurdeanu@arizona.edu.
Submission received: 14 November 2011; revised submission received: 31 May 2012; accepted for publication:
15 August 2012.
doi:10.1162/COLI a 00145
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
1. Introduction
Semantic Role Labeling (SRL) is the problem of analyzing clause predicates in text by
identifying arguments and tagging them with semantic labels indicating the role they
play with respect to the predicate. Such sentence-level semantic analysis allows the
determination of who did what to whom, when and where, and thus characterizes the
participants and properties of the events established by the predicates. For instance,
consider the following sentence, in which the arguments of the predicate to send have
been annotated with their respective semantic roles.1
(1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal.
Recognizing these event structures has been shown to be important for a broad
spectrum of NLP applications. Information extraction, summarization, question
answering, machine translation, among others, can benefit from this shallow semantic
analysis at sentence level, which opens the door for exploiting the semantic relations
among arguments (Boas 2002; Surdeanu et al 2003; Narayanan and Harabagiu 2004;
Melli et al 2005; Moschitti et al 2007; Higashinaka and Isozaki 2008; Surdeanu,
Ciaramita, and Zaragoza 2011). In Ma`rquez et al (2008) the reader can find a broad
introduction to SRL, covering several historical and definitional aspects of the problem,
including also references to the main resources and systems.
State-of-the-art systems leverage existing hand-tagged corpora (Fillmore,
Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised
machine learning systems, and typically perform SRL in two sequential steps:
argument identification and argument classification. Whereas the former is mostly a
syntactic recognition task, the latter usually requires semantic knowledge to be taken
into account. The semantic knowledge that most current systems capture from text is
basically limited to the predicates and the lexical units contained in their arguments,
including the argument head. These ?lexical features? tend to be sparse, especially
when the training corpus is small, and thus SRL systems are prone to overfit the
training data and generalize poorly to new corpora (Pradhan, Ward, and Martin 2008).
As a simplified example of the effect of sparsity, consider the following sentences
occurring in an imaginary training data set for SRL:
(2) [JFK]Patient was assassinated [in Dallas]Location
(3) [John Lennon]Patient was assassinated [in New York]Location
(4) [JFK]Patient was assassinated [in November]Temporal
(5) [John Lennon]Patient was assassinated [in winter]Temporal
All four sentences share the same syntactic structure, so the lexical features (i.e., the
words Dallas, New York, November, and winter) represent the most relevant knowledge
for discriminating between the Location and Temporal adjunct labels in learning.
1 For simplicity, in this paper we talk about arguments in the most general sense. Unless noted otherwise,
argument will refer to both core-arguments (Agent, Patient, Instrument, etc.) and adjuncts (Manner,
Temporal, Location, etc.).
632
Zapirain et al Selectional Preferences for Semantic Role Classification
The problem is that, as in the following sentences, for the same predicate, one may
encounter similar expressions with new words like Texas or December, which the
classifiers cannot match with the lexical features seen during training, and thus become
useless for classification:
(6) [Smith] was assassinated [in Texas]
(7) [Smith] was assassinated [in December]
This problem is exacerbated when SRL systems are applied to texts coming from
new domains where the number of new predicates and argument heads increases
considerably. The CoNLL-2004 and 2005 evaluation exercises on semantic role labeling
(Carreras and Ma`rquez 2004, 2005) reported a significant performance degradation
of around 10 F1 points when applied to out-of-domain texts from the Brown corpus.
Pradhan, Ward, and Martin (2008) showed that this performance degradation is
essentially caused by the argument classification subtask, and suggested the lexical
data sparseness as one of the main reasons.
In this work, we will focus on Semantic Role Classification (SRC), and we will show
that selectional preferences (SP) are useful for generalizing lexical features, helping
fight sparseness and domain shifts, and improving SRC results. Selectional preferences
try to model the kind of words that can fill a specific argument of a predicate, and
have been widely used in computational linguistics since the early days (Wilks 1975).
Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) and
distributional similarity based on corpora (Pantel and Lin 2000) have been successfully
used for acquiring selectional preferences, and in this work we have used several of
those models.
The contributions of this work to the field of SRL are the following:
1. We formalize and implement a method that applies several selectional
preference models to Semantic Role Classification, introducing for the first
time the use of selectional preferences for prepositions, in addition to
selectional preferences for verbs.
2. We show that the selectional preference models are able to generalize
lexical features and improve role classification performance in a controlled
experiment disconnected from a complete SRL system. The positive effect
is consistently observed in all variants of WordNet and distributional
similarity measures and is especially relevant for out-of-domain data. The
separate learning of SPs for verbs and prepositions contributes
significantly to the improvement of the results.
3. We integrate the information of several SP models in a state-of-the-art SRL
system (SwiRL)2 and obtain significant improvements in semantic role
classification and, as a consequence, in the end-to-end SRL task. The key
for the improvement lies in the combination of the predictions provided
by SwiRL and the several role classification models based on selectional
preferences.
2 http://surdeanu.info/mihai/swirl/.
633
Computational Linguistics Volume 39, Number 3
4. We present a manual analysis of the output of the combined role
classification system. By observing a set of real examples, we categorized
and quantified the situations in which SP models tend to help role
classification. By inspecting also a set of negative cases, this analysis also
sheds light on the limitations of the current approach and identifies
opportunities for further improvements.
The use of selectional preferences for improving role classification was first pre-
sented in Zapirain, Agirre, and Ma`rquez (2009), and later extended in Zapirain et al
(2010) to a full-fledged SRC system. In the current paper, we provide more detailed
background information and details of the selectional preference models, as well as
complementary experiments on the integration in a full-fledged system. More impor-
tantly, we incorporate a detailed analysis of the output of the system, comparing it with
that of a state-of-the-art SRC system not using SPs.
The rest of the paper is organized as follows. Section 2 provides background on the
automatic acquisition of selectional preference, and its recent relation to the semantic
role labeling problem. In Section 3, the SP models investigated in this paper are ex-
plained in all their variants. The results of the SP models in laboratory conditions are
presented in Section 4. Section 5 describes the method for integrating the SP models in a
state-of-the-art SRL system and discusses the results obtained. In Section 6 the qualita-
tive analysis of the system output is presented, including a detailed discussion of several
examples. Finally, Section 7 concludes and outlines some directions for future research.
2. Background
The simplest model for generating selectional preferences would be to collect all heads
filling each role of the target predicate. This is akin to the lexical features used by current
SRL systems, and we refer to this model as the lexical model. More concretely, the
lexical model for verb-role selectional preferences consists of the list of words appearing
as heads of the role arguments of the predicate verb. This model can be extracted
automatically from the SRL training corpus using straightforward techniques. When
using this model for role classification, it suffices to check whether the head word of
the argument matches any of the words in the lexical model. The lexical model is the
baseline for our other SP models, all of which build on that model.
In order to generalize the lexical model, semantic classes can be used. Although in
principle any lexical resource listing semantic classes for nouns could be applied, most
of the literature has focused on the use of WordNet (Resnik 1993b). In the WordNet-
based model, the words occurring in the lexical model are projected over the semantic
hierarchy of WordNet, and the semantic classes which represent best those words are
selected. Given a new example, the SRC system has to check whether the new word
matches any of those semantic classes. For instance, in example sentences (2)?(5), the
semantic class <time period> covers both training examples for Temporal (i.e., November
and winter), and <geographical area> covers the examples for Location. When test
words Texas and December occur in Examples (6) and (7), the semantic classes to which
they belong can be used to tag the first as Location and the second as Temporal.
As an alternative to the use of WordNet, one can also apply automatically acquired
distributional similarity thesauri. Distributional similarity methods analyze the co-
occurrence patterns of words and are able to capture, for instance, that December is more
closely related to November than to Dallas (Grefenstette 1992). Distributional similarity is
typically used on-line (i.e., given a pair of words, their similarity is computed on the go),
634
Zapirain et al Selectional Preferences for Semantic Role Classification
but, in order to speed up its use, it has also been used to produce off-line a full thesauri,
storing, for every word, the weighted list of all outstanding similar words (Lin 1998).
In the Distributional similarity model, when test item Texas in Example (6) is to be
labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity
to November and winter, would be used to label the argument with the Location role.
The automatic acquisition of selectional preferences is a well-studied topic in NLP.
Many methods using semantic classes and selectional preferences have been proposed
and applied to a variety of syntactic?semantic ambiguity problems, including syntactic
parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez
2008; Koo, Carreras, and Collins 2008; Agirre et al 2011), word sense disambiguation
(Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun res-
olution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and
Roth 2009). In addition, selectional preferences have been shown to be effective to
improve the quality of inference and information extraction rules (Pantel et al 2007;
Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not
mention selectional preferences, but all of them use some notion of preferring certain
semantic types over others in order to accomplish their respective task.
In fact, one could use different notions of semantic types. In one extreme, we would
have a small set of coarse semantic classes. For instance, some authors have used the
26 so-called ?semantic fields? used to classify all nouns in WordNet (Agirre, Baldwin,
and Martinez 2008; Agirre et al 2011). The classification could be more fine-grained, as
defined by the WordNet hierarchy (Resnik 1993b; Agirre and Martinez 2001; McCarthy
and Carroll 2003), and other lexical resources could be used as well. Other authors have
used automatically induced hierarchical word classes, clustered according to occurrence
information from corpora (Koo, Carreras, and Collins 2008; Ratinov and Roth 2009).
On the other extreme, each word would be its own semantic class, as in the lexical
model, but one could also model selectional preference using distributional similarity
(Grefenstette 1992; Lin 1998; Pantel and Lin 2000; Erk 2007; Bergsma, Lin, and Goebel
2008). In this paper we will focus on WordNet-based models that use the whole hierarchy
and on distributional similarity models, and we will use the lexical model as baseline.
2.1 WordNet-Based Models
Resnik (1993b) proposed the modeling of selectional preferences using semantic classes
from WordNet and applied the model to tackle some ambiguity issues in syntax, such
as noun-compounds, coordination, and prepositional phrase attachment. Given two
alternative structures, Resnik used selectional preferences to choose the attachment
maximizing the fitness of the head to the selectional preferences of the attachment
points. This is similar to our task, but in our case we compare the target head to the selec-
tional preference models for each possible role label (i.e., given a verb and the head of an
argument, we need to find the role with the selectional preference that fits the head best).
In Resnik?s model, he first characterizes the restrictiveness of the selectional pref-
erence of an argument position r of a governing predicate p, noted as R(p, r). For that,
given a set of classes C from the WordNet nominal hierarchies, he takes the relative en-
tropy or Kullback-Leibler distance between the prior distribution P(C) and the posterior
distribution P(C|p, r):
R(p, r) =
?
c?C
P(c|p, r)logP(c|p, r)
P(c)
(1)
635
Computational Linguistics Volume 39, Number 3
The priors can be computed from any corpora, computing frequencies of classes
and using maximum likelihood estimates. The frequencies for classes cannot be directly
observed, but they can be estimated from the lexical frequencies of the nouns under
the class, as in Equation (2). Note that in WordNet, hypernyms (?hyp? for short)
correspond to superclass relations, and therefore hyp(n) returns all superclasses of
noun n.
freq(c) =
?
{n|c?hyp(n)}
freq(n) (2)
A complication arises because of the polysemy of nouns. If each occurrence of a
noun counted once in all classes that its senses belong to, polysemous nouns would
account for more probability mass than monosemous nouns, even if they occurred the
same number of times. As a solution, the frequency of polysemous nouns is split among
its senses uniformly. For instance, the probability of the class <time period> can be
estimated according to the frequencies of nouns like November, spring, and the rest of
nouns under it. November has a single sense, so every occurrence counts as 1, but spring
has six different senses, so each occurrence should only count as 0.16. Note that with
this method we are implicitly dealing with the word sense ambiguity problem. When
encountering a polysemous noun as an argument of a verb, we record the occurrence
of all of its senses. Given enough occurrences of nouns, the classes generalizing the
intended sense of the nouns will gather more counts than competing classes. In the
example, <time period> would have 1.16 compared with 0.16 <tool> (i.e., for the metal
elastic device meaning of spring). Researchers have used this fact to perform Word Sense
Disambiguation using selectional preferences (Resnik 1993a; Agirre and Martinez 2001;
McCarthy and Carroll 2003).
The posterior probability can be computed similarly, but it takes into account occur-
rences of the nouns in the required argument position of the predicate, and thus requires
a corpus annotated with roles.
The selectional preference of a predicate p and role r for a head w0 of any potential
argument, noted as SPRes(p, r, w0), is formulated as follows:3
SPRes(p, r, w0) = max
c0?hyp(w0 )
P(c0|p, r)log P(c0|p,r)P(c0)
R(p, r)
(3)
The numerator formalizes the goodness of fit for the best semantic class c0 that
contains w0. The hypernym (i.e., superclass) of w0 yielding the maximum value is
chosen. The denominator models how restrictive the selectional preference is for p and
r, as modeled in Equation (1).
Variations of Resnik?s idea to find a suitable level of generalization have been
explored in later years. Li and Abe (1998) applied the minimum-description length
principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a
class should be preferred rather than its children.
Brockmann and Lapata (2003) compared several class-based models (including
Resnik?s selectional preferences) on a syntactic plausibility judgment task for German.
3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented
in this paper.
636
Zapirain et al Selectional Preferences for Semantic Role Classification
The models return weights for (verb, syntactic function, noun) triples, and correla-
tion with human plausibility judgment is used for evaluation. Resnik?s selectional
preference scored best among WordNet-based methods (Li and Abe 1998; Clark and
Weir 2002). Despite its earlier publication, Resnik?s method is still the most popular
representative among WordNet-based methods (Pado?, Pado?, and Erk 2007; Erk, Pado?,
and Pado? 2010; Baroni and Lenci 2010). We also chose to use Resnik?s model in this
paper.
One of the disadvantages of the WordNet-based models, compared with the distri-
butional similarity models, is that they require that the heads are present in WordNet.
This limitation can negatively influence the coverage of the model, and also its general-
ization ability.
2.2 Distributional Similarity Models
Distributional similarity models assume that a word is characterized by the words it
co-occurs with. In the simplest model, co-occurring words are taken from a fixed-size
context window. Each word w would be represented by the set of words that co-occur
with it, T(w). In a more elaborate model, each word w would be represented as a vector
of words T(w) with weights, where Ti(w) corresponds to the weight of the ith word in
the vector. The weights can be calculated following a simple frequency of co-occurrence,
or using some other formula.
Then, given two words w and w0, their similarity can be computed using any simi-
larity measure between their co-occurrence sets or vectors. For instance, early work by
Grefenstette (1992) used the Jaccard similarity coefficient of the two sets T(w) and T(w0)
(cf. Equation (4) in Figure 1). Lee (1999) reviews a wide range of similarity functions,
including Jaccard and the cosine between two vectors T(w) and T(w0) (cf. Equation (5)
in Figure 1).
In the context of lexical semantics, the similarity measure defined by Lin (1998)
has been very successful. This measure (cf. Equation (6) in Figure 1) takes into account
syntactic dependencies (d) in its co-occurrence model. In this case, the set T(w) of co-
occurrences of w contains pairs (d,v) of dependencies and words, representing the fact
simJac(w, w0) =
|T(w) ? T(w0)|
|T(w) ? T(w0)|
(4)
simcos(w, w0) =
?n
i=1
Ti(w)Ti(w0)
?
?n
i=1
Ti(w)2
?
?n
i=1
Ti(w0)2
(5)
simLin(w, w0) =
?
(d,v)?T(w)?T(w0 )(I(w, d, v) + I(w0, d, v))
?
(d,v)?T(w) I(w, d, v) +
?
(d,v)?T(w0 ) I(w0, d, v)
(6)
Figure 1
Similarity measures used in the paper. Jac and cos stand for Jaccard and cosine similarity metrics.
T(w) is the set of words co-occurring with w, Ti(w) is the weight of the ith element of the vector
of words co-occurring with w, and I(w, d, v) is the mutual information between w and d, v.
637
Computational Linguistics Volume 39, Number 3
that the corpus contains an occurrence of w having dependency d with v. For instance,
if the corpus contains John loves Mary, then the pair (ncsubj, love) would be in the set
T for John. The measure uses information-theoretic principles, and I(w, d, v) represents
the information content of the triple (Lin 1998).
Although the use of co-occurrence vectors for words to compute similarity has been
standard practice, some authors have argued for more complex uses. Schu?tze (1998)
builds vectors for each context of occurrence of a word, combining the co-occurrence
vectors for each word in the context. The vectors for contexts were used to induce
senses and to improve information retrieval results. Edmonds (1997) built a lexical co-
occurrence network, and applied it to a lexical choice task. Chakraborti et al (2007)
used transitivity over co-occurrence relations, with good results on several classification
tasks. Note that all these works use second order and higher order to refer to their method.
In this paper, we will also use second order to refer to a new method which goes beyond
the usual co-occurrence vectors (cf. Section 3.3).
A full review of distributional models is out of the scope of this paper, as we are in-
terested in showing that some of those models can be used successfully to improve SRC.
Pado? and Lapata (2007) present a review of distributional models for word similarity,
and a study of several parameters that define a broad family of distributional similarity
models, including Jaccard and Lin. They provide publicly available software,4 which
we have used in this paper, as explained in the next section. Baroni and Lenci (2010)
present a framework for extracting distributional information from corpora that can be
used to build models for different tasks.
Distributional similarity models were first used to tackle syntactic ambiguity. For
instance, Pantel and Lin (2000) obtained very good results on PP-attachment using the
distributional similarity measure defined by Lin (1998). Distributional similarity was
used to overcome sparsity problems: Alongside the counts in the training data of the
target words, the counts of words similar to the target ones were also used. Although
not made explicit, Lin was actually using a distributional similarity model of selectional
preferences.
The application of distributional selectional preferences to semantic roles (as op-
posed to syntactic functions) is more recent. Gildea and Jurafsky (2002) are the only ones
applying selectional preferences in a real SRL task. They used distributional clustering
and WordNet-based techniques on a SRL task on FrameNet roles. They report a very
small improvement of the overall performance when using distributional clustering
techniques. In this paper we present complementary experiments, with a different role
set and annotated corpus (PropBank), a wider range of selectional preference models,
and the analysis of out-of-domain results.
Other papers applying semantic preferences in the context of semantic roles rely on
the evaluation of artificial tasks or human plausibility judgments. Erk (2007) introduced
a distributional similarity?based model for selectional preferences, reminiscent of that
of Pantel and Lin (2000). Her approach models the selectional preference SPsim(p, r, w0)
of an argument position r of governing predicate p for a possible head-word w0 as
follows:
SPsim(p, r, w0) =
?
w?Seen(p,r)
sim(w0, w) ? weight(p, r, w) (7)
4 http://www.coli.uni-saarland.de/?pado/dv/dv.html.
638
Zapirain et al Selectional Preferences for Semantic Role Classification
where sim(w0, w) is the similarity between the seen and potential heads, Seen(p, r) is the
set of heads of role r for predicate p seen in the training data set (as in the lexical model),
and weight(p, r, w) is the weight of the seen head word w. Our distributional model for
selectional preferences follows her formalization.
Erk instantiated the basic model with several corpus-based distributional similarity
measures, including Lin?s similarity, Jaccard, and cosine (Figure 1) among others, and
several implementations of the weight function such as the frequency. The quality of
each model instantiation, alongside Resnik?s model and an expectation maximization
(EM)-based clustering model, was tested in a pseudo-disambiguation task where the
goal was to distinguish an attested filler of the role and a randomly chosen word. The
results over 100 frame-specific roles showed that distributional similarities attain similar
error rates to Resnik?s model but better than EM-based clustering, with Lin?s formula
having the smallest error rate. Moreover, the coverage of distributional similarity mea-
sures was much better than Resnik?s. In a more recent paper, Erk, Pado?, and Pado? (2010)
extend the aforementioned work, including evaluation to human plausibility judgments
and a model for inverse selectional preferences.
In this paper we test similar techniques to those presented here, but we evaluate
selectional preference models in a setting directly related to semantic role classification,
namely, given a selectional preference model for a verb we find the role which fits
best the given head word. The problem is indeed qualitatively different from previous
work in that we do not have to choose among the head words competing for a role but
among selectional preferences of roles competing for a head word.
More recent work on distributional selectional preference has explored the use of
discriminative models (Bergsma, Lin, and Goebel 2008) and topical models (O? Se?aghdha
2010; Ritter, Mausam, and Etzioni 2010). These models would be a nice addition to those
implemented in this paper, and if effective, they would improve further our results with
respect to the baselines which don?t use selectional preferences.
Contrary to WordNet-based models, distributional preferences do not rely on a
hand-built resource. Their coverage and generalization ability depend on the corpus
from which the distributional similarity model was computed. This fact makes this
approach more versatile in domain adaptation scenarios, as more specific and test-set
focused generalization corpora could be used to modify, enrich, or even replace the
original corpus.
2.3 PropBank
In this work we use the semantic roles defined in PropBank. The Proposition Bank
(Palmer, Gildea, and Kingsbury 2005) emerged as a primary resource for research in
SRL. It provides semantic role annotation for all verbs in the Penn Treebank corpus.
PropBank takes a ?theory-neutral? approach to the designation of core semantic roles.
Each verb has a frameset listing its allowed role labelings in which the arguments are
designated by number (starting from 0). Each numbered argument is provided with an
English language description specific to that verb. The most frequent roles are Arg0 and
Arg1 and, generally, Arg0 stands for the prototypical agent and Arg1 corresponds to the
prototypical patient or theme of the proposition. The rest of arguments (Arg2 to Arg5)
do not generalize across verbs, that is, they have verb specific interpretations.
Apart from the core numbered roles, there are 13 labels to designate adjuncts:
AM-ADV (general-purpose), AM-CAU (cause), AM-DIR (direction), AM-DIS (dis-
course marker), AM-EXT (extent), AM-LOC (location), AM-MNR (manner), AM-MOD
639
Computational Linguistics Volume 39, Number 3
Table 1
Example of verb-role lexical SP models for write, listed in alphabetical order. Number of heads
indicates the number of head words attested, Unique heads indicates the number of distinct
head words attested, and Examples lists some of the heads in alphabetical order.
Verb-role Number of Unique Examples
heads heads
write-Arg0 98 84 Angrist anyone baker ball bank Barlow Bates ...
write-Arg1 97 69 abstract act analysis article asset bill book ...
write-Arg2 7 7 bank commander hundred jaguar Kemp member ...
write-AM-LOC 2 2 paper space
write-AM-TMP 1 1 month
(modal verb), AM-NEG (negation marker), AM-PNC (purpose), AM-PRD (predication),
AM-REC (reciprocal), and AM-TMP (temporal).
3. Selectional Preference Models for Argument Classification
Our approach for applying selectional preferences to semantic role classification is
discriminative. That is, the SP-based models provide a score for every possible role
label given a verb (or preposition), the head word of the argument, and the selectional
preferences for the verb (or preposition). These scores can be used to directly assign the
most probable role or to codify new features to train enriched semantic role classifiers.
In this section we first present all the variants for acquiring selectional preferences
used in our study, and then present the method to apply them to semantic role classifi-
cation. We selected several variants that have been successful in some previous works.
3.1 Lexical SP Model
In order to implement the lexical model we gathered all heads w of arguments filling
a role r of a predicate p and obtained freq(p, r, w) from the corresponding training data
(cf. Section 4.1). Table 1 shows a sample of the heads of arguments attested in the
corpus for the verb write. The lexical SP model can be simply formalized as follows:
SPlex(p, r, w0) = freq(p, r, w0) (8)
3.2 WordNet-Based SP Models
We instantiated the model based on (Resnik 1993b) presented in the previous sec-
tion (SPRes, cf. Equation (3)) using the implementation of Agirre and Martinez (2001).
Tables 2 and 3 show the synsets5 that generalize best the head words in Table 1
for write-Arg0 and write-Arg1, according to the weight assigned to those synsets by
Equation (1). According to this model, and following basic intuition, the words attested
as being Arg0s of write are best generalized by semantic classes such as living things,
5 The WordNet terminology for concepts is synset. In this paper we use concept, synset, and semantic class
interchangeably.
640
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 2
Excerpt from the selectional preferences for write-Arg0 according to SPRes, showing the synsets
that generalize best the head words in Table 1. Weight lists the weight assigned to those synsets
by Equation (1). Description includes the words and glosses in the synset.
Synset Weight Description
n#00002086 5.875 life form organism being living thing any living entity
n#00001740 5.737 entity something anything having existence (living or nonliving)
n#00009457 4.782 object physical object a physical (tangible and visible) entity;
n#00004123 4.351 person individual someone somebody mortal human soul
a human being;
Table 3
Excerpt from the selectional preferences for write-Arg1 according to SPRes, showing the synsets
that generalize best the head words in Table 1. Weight lists the weight assigned to those synsets
by Equation (1). Description includes the words and glosses in the synset.
Synset Weight Description
n#00019671 7.956 communication something that is communicated between people
or groups
n#04949838 4.257 message content subject matter substance what a communication
that . . .
n#00018916 3.848 relation an abstraction belonging to or characteristic of two entities
n#00013018 3.574 abstraction a concept formed by extracting common features
from examples
entities, physical objects, and human beings, whereas Arg1s by communication, mes-
sage, relation, and abstraction.
Resnik?s method performs well among Wordnet-based methods, but we realized
that it tends to overgeneralize. For instance, in Table 2, the concept for ?entity? (one of
the unique beginners of the WordNet hierarchy) has a high weight. This means that a
head like ?grant? would be assigned Arg0. In fact, any noun which is under concept
n#00001740 (entity) but not under n#04949838 (message) would be assigned Arg0. This
observation led us to speculate on an alternative method which would try to generalize
as little as possible.
Our intuition is that general synsets can fit several selectional preferences at the
same time. For instance, the <entity> class, as a superclass of most words, would be a
correct generalization for the selectional preferences of all agent, patient, and instrument
roles of a predicate like break. On the contrary, specific concepts are usually more useful
for characterizing selectional preferences, as in the <tool> class for the instrument role
of break. The priority of using specific synsets over more general ones is, thus, justified
in the sense that they may better represent the most relevant semantic characteristics of
the selectional preferences.
The alternative method (SPwn) is based on the depth of the concepts in the WordNet
hierarchy and the frequency of the nouns. The use of the depth in hierarchies to model
the specificity of concepts (the deeper the more specific) is not new (Rada et al 1989;
Sussna 1993; Agirre and Rigau 1996). Our method tries to be conservative with respect
to generalization: When we check which SP is a better fit for a given target head, we
always prefer the SP that contains the most specific generalization for the target head
(the lowest synset which is a hypernym of the target word).
641
Computational Linguistics Volume 39, Number 3
Table 4
Excerpt from the selectional preferences for write-Arg0 according to SPwn, showing from deeper
to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists
the depth of synsets in WordNet. Description includes the words and glosses in the synset.
Synset Depth Freq. Description
n#01967203 9 1 humanoid human being any living or extinct member of the . . .
n#07603319 8 1 spy undercover agent a secret agent hired by a state to . . .
n#07151308 8 1 woman a human female who does housework
n#06183656 8 1 Federal Reserve the central bank of the US
Table 5
Excerpt from the selectional preferences for write-Arg1 according to SPwn, showing from deeper
to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists
the depth of synsets in WordNet. Description includes the words and glosses in the synset.
Synset Depth Freq. Description
n#05403815 13 1 information formal accusation of a crime
n#05401516 12 1 accusation accusal a formal charge of wrongdoing brought . . .
n#04925620 11 1 charge complaint a pleading describing some wrong or offense
n#04891230 11 1 memoir an account of the author?s personal experiences
More concretely, we model selectional preferences as a multiset6 of synsets, storing
all hypernyms of the heads seen in the training data for a certain role of a given
predicate, that is:
Smul(p, r) =
?
w?Seen(p,r)
hyp(w) (9)
where Seen(p, r) are all the argument heads for predicate p and role r, and hyp(w) returns
all the synsets and hypernyms of w, including hypernyms of hypernyms recursively up
to the top synsets.
For any given synset s, let d(s) be the depth of the synset in the WordNet hierarchy,
and let 1Smul(p,r)(s) be the multiplicity function which returns how many times s is con-
tained in the multiset Smul(p, r). We define a partial order among synsets a, b ? Smul(p, r)
as follows: ord(a) > ord(b) iff d(a) > d(b) or d(a) = d(b) ? 1Smul(p,r)(a) > 1Smul(p,r)(b).
Tables 4 and 5 show the most specific synsets (according to their depth) for write-Arg0
and write-Arg1.
We can then measure the goodness of fit of the selectional preference for a word as
the rank in the partial order of the first hypernym of the head that is also present in the
selectional preference. For that, we introduce SPwn(p, r, w), which following the previous
notation is defined as:
SPwn(p, r, w) = arg max
s?hyp(w)?Smul(p,r)
ord(s) (10)
6 Multisets are similar to sets, but allow for repeated members.
642
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 6
Most similar words for Texas and December according to Lin (1998).
Texas Florida 0.249, Arizona 0.236, California 0.231, Georgia 0.221, Kansas 0.217,
Minnesota 0.214, Missouri 0.214, Michigan 0.213, Colorado 0.208, North
Carolina 0.207, Oklahoma 0.207, Arkansas 0.205, Alabama 0.205, Nebraska
0.201, Tennessee 0.197, New Jersey 0.194, Illinois 0.189, Virginia 0.188,
Kentucky 0.188, Wisconsin 0.188, Massachusetts 0.184, New York 0.183
December June 0.341, October 0.340, November 0.333, April 0.330, February 0.329,
September 0.328, July 0.323, January 0.322, August 0.317, may 0.305, March
0.250, Spring 0.147, first quarter 0.135, mid-December 0.131, month 0.130,
second quarter 0.129, mid-November 0.128, fall 0.125, summer 0.125,
mid-October 0.121, autumn 0.121, year 0.121, third quarter 0.119
In case of ties, the role coming first in alphabetical order would be returned. Note that,
similar to the Resnik model (cf. Section 2.1), this model implicitly deals with the word
ambiguity problem.
As with any other approximation to measure specificity of concepts, the use of
depth has some issues, as some deeply rooted stray synsets would take priority. For
instance, Table 4 shows that synset n#01967203 for human being is the deepest synset. In
practice, when we search the synsets of a target word in the SPwn models following Eq.
(10), the most specific synsets (specially stray synsets) are not found, and synsets higher
in the hierarchy are used.
3.3 Distributional SP Models
All our distributional SP models are based on Equation (7). We have used several vari-
ants for sim(w0, w), as presented subsequently, but in all cases, we used the frequency
freq(p, r, w) as the weight in the equation. Given the availability of public resources for
distributional similarity, rather than implementing sim(w0, w) afresh we used (1) the pre-
compiled similarity measures by Lin (1998),7 and (2) the software for semantic spaces
by Pado? and Lapata (2007).
In the first case, Lin computed the similarity numbers for an extensive vocabulary
based on his own similarity formula (cf. Equation (6) in Figure 1) run over a large
parsed corpus comprising journalism texts from different sources: WSJ (24 million
words), San Jose Mercury (21 million words) and AP Newswire (19 million words).
The resource includes, for each word in the vocabulary, its most similar words with
the similarity weight. In order to get the similarity for two words, we can check the
entry in the thesaurus for either word. We will refer to this similarity measure as
simpreLin. Table 6 shows the most similar words for Texas and December according to this
resource.
For the second case, we applied the software to the British National Corpus to
extract co-occurrences, using the optimal parameters as described in Pado? and Lapata
(2007, page 179): word-based space, medium context, log-likelihood association, and
7 http://www.cs.ualberta.ca/?lindek/downloads.htm.
643
Computational Linguistics Volume 39, Number 3
Table 7
Summary of distributional similarity measures used in this work.
Similarity measure Source
simcos cosine BNC
simJac Jaccard BNC
simLin Lin BNC
simpreLin Lin Pre-computed
simpreLin?cos cosine (2nd order) Pre-computed
simpreLin?Jac Jaccard (2nd order) Pre-computed
2,000 basis elements. We tested Jaccard, cosine, and Lin?s measure for similarity, yielding
simJac, simcos, and simLin, respectively.
In addition to measuring the similarity of two words directly, that is, using the co-
occurrence vectors of each word as in Section 2, we also tried a variant which we will
call second-order similarity. In this case each word is represented by a vector which
contains all similar words with weights, where those weights come from first order
similarity. That is, in order to obtain the second-order vector for word w, we need to
compute its first order similarity with all other words in the vocabulary. The second-
order similarity of two words is then computed according to those vectors. For this, we
just need to change the definition of T and T in the similarity formulas in Figure 1: Now
T(w) would return the list of words which are taken to be similar to w, and T(w) would
return the same list but as a vector with weights.
This approximation is computationally expensive, as we need to compute the
square matrix of similarities for all word pairs in the vocabulary, which is highly time-
consuming. Fortunately, the pre-computed similarity scores of Lin (1998) (which use
simLin) are readily available, and thus the second-order similarity vectors can be easily
computed. We used Jaccard and cosine to compute the similarity of the vectors, and we
will refer to these similarity measures as simpreLin?Jac and sim
pre
Lin?cos hereinafter. Due to the
computational complexity, we did not compute second order similarity for the semantic
space software of Pado? and Lapata (2007).
Table 7 summarizes all similarity measures used in this study, and the corpus or
pre-computed similarity list used to build them.
3.4 Selectional Preferences for Prepositions
All the previously described models have been typically applied to verb-role selectional
preferences for NP arguments. Applying them to general semantic role labeling may
not be straightforward, however, and may require some extensions and adaptations.
For instance, not all argument candidates are noun phrases. Common arguments with
other syntactic types include prepositional, adjectival, adverbial, and verb phrases. Any
candidate argument without a nominal head cannot be directly treated by the models
described so far.
644
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 8
Example of prep-role lexical models for the preposition from, listed in alphabetical order.
Prep-role Number of Unique Examples
heads heads
from-Arg0 32 30 Abramson agency association barrier cut ...
from-Arg1 173 118 accident ad agency appraisal arbitrage ...
from-Arg2 708 457 academy account acquisition activity ad ...
from-Arg3 396 165 activity advertising agenda airport ...
from-Arg4 5 5 europe Golenbock system Vizcaya west
from-AM-ADV 19 17 action air air conception datum everyone ...
from-AM-CAU 5 4 air air design experience exposure
from-AM-DIR 79 71 agency alberta amendment america arson ...
from-AM-LOC 20 17 agency area asia body bureau orlando ...
from-AM-MNR 29 28 agency Carey company earnings floor ...
from-AM-TMP 33 21 april august beginning bell day dec. half ...
A particularly interesting case is that of prepositional phrases.8 Prepositions define
relations between the preposition attachment point and the preposition complement.
Prepositions are ambiguous with respect to these relations, which allows us to talk
about preposition senses. The Preposition Project (Litkowski and Hargraves 2005, 2006)
is an effort that produced a detailed sense inventory for English prepositions, which
was later used in a preposition sense disambiguation task at SemEval-2007 (Litkowski
and Hargraves 2007). Sense labels are defined as semantic relations, similar to those of
semantic role labels. In a more recent work, Srikumar and Roth (2011) presented a joint
model for extended semantic role labeling in which they show that determining the
sense of the preposition is mutually related to the task of labeling the argument role of
the prepositional phrase. Following the previous work, we also think that prepositions
define implicit selectional preferences, and thus decided to explore the use of preposi-
tional preferences with the aim of improving the selection of the appropriate semantic
roles. Addressing other arguments with non-nominal heads has been intentionally left
for further work.
The most straightforward way of including prepositional information in SP models
would be to add the preposition as an extra parameter of the SP. Initial experiments
revealed sparseness problems with collecting the ?verb, preposition, NP-head, role?
4-tuples from the training set. A simpler approach consists of completely disregarding
the verb information while collecting the prepositional preferences. That is, the selec-
tional preference for a preposition p and role r is defined as the union of all nouns w
found as heads of noun phrases embedded in prepositional phrases headed by p and
labeled with semantic role r. Then, one can apply any of the variants described in the
previous sections to calculate SP(p, r, w). Table 8 shows a sample of the lexical model for
the preposition from, organized according to the roles it plays.
These simple prep-role preferences largely avoided the sparseness problem while
still being able to capture relevant information to distinguish the appropriate roles in
many PP arguments. In particular, they proved to be relevant to distinguish between
adjuncts of the type ?[in New York]Location? vs. ?[in Winter]Temporal.? Nonetheless, we
8 Prepositional phrase is the second most frequent type of syntactic constituent for semantic arguments
(13%), after noun phrases (45%).
645
Computational Linguistics Volume 39, Number 3
are aware that not taking into account verb information also introduces some lim-
itations. In particular, the simplification could damage the performance on PP core
arguments, which are verb-dependent.9 For instance, our prepositional preferences
would not be able to suggest appropriate roles for the following two PP arguments:
?increase [ from seven cents a share]Arg3? and ?receive [ from the funds]Arg2,? because
the two head nouns (cents and funds) are semantically very similar. Assigning the
correct roles in these cases clearly depends on the information carried by the verbs.
Arg3 is the starting point for the predicate increase, whereas Arg2 refers to the source for
receive.
Our perspective on making this simple definition of prep-role SPs was practical and
just a starting point to play with the argument preferences introduced by prepositions.
A more complex model, distinguishing between prepositional phrases in adjunct and
core argument positions, should be able to model the linguistics better yet aleviate the
sparseness problem, and would hopefully produce better results.
The combination scheme for applying verb-role and prep-role is also very simple.
Depending on the syntactic type of the argument we apply one or the other model, both
in learning and testing:
 When the argument is a noun phrase, we use verb-role selectional
preferences.
 When the argument is a prepositional phrase, we use prep-role
selectional preferences.
We thus use a straightforward method to combine both kinds of SPs. More complex
possibilities like doing mixtures of both SPs are left for future work.
3.5 Role Classification with SP Models
Selectional preference models can be directly used to perform role classification. Given
a target predicate p and noun phrase candidate argument with head w, we simply select
the role r of the predicate which best fits the head according to the SP model. This
selection rule is formalized as:
ROLE(p, w) = arg max
r?Roles(p)
SP(p, r, w) (11)
with Roles(p) being the set of all roles applicable to the predicate p, and SP(p, r, w)
the goodness of fit of the selectional preference model for the head w, which can be
instantiated with all the variants mentioned in the previous subsections, including
the lexical model (Equation (8)) WordNet-based SP models (Equations (3) and (10)),
and distributional SP models (Equation (7)), using different similarity models as in
Table 7. Ties were broken returning the role coming first according to alphabetical
order. Note that in the case of SPwn (Equation 10) we need to use arg min rather than
arg max.
9 The percentage of prepositional phrases in core argument position is 48%, slightly lower than in adjunct
position (52%).
646
Zapirain et al Selectional Preferences for Semantic Role Classification
Note that if the candidate argument is a prepositional phrase with preposition p?
and embedded NP head word w, the classification rule uses the prep-role SP model,
that is:
ROLE(p, p?, w) = arg max
r?Roles(p? )
SP(p?, r, w)
4. Experiments with Selectional Preferences in Isolation
In this section we evaluate the ability of selectional preference models to discriminate
among different roles. For that, SP models will be used in isolation, according to the clas-
sification rule in Equation (11), to predict role labels for a set of (predicate, argument-head)
pairs. That is, we are interested in the discriminative power of the semantic information
carried by the SPs, factoring out any other feature commonly used by the state-of-the-
art SRL systems. The data sets used and the experimental results are presented in the
following.
4.1 Data Sets
The data used in this work are the benchmark corpus provided by the CoNLL-2005
shared task on SRL (Carreras and Ma`rquez 2005). The data set, of over 1 million tokens,
comprises PropBank Sections 02?21 for training, and Sections 24 and 23 for develop-
ment and testing, respectively. The Selectional Preferences implemented in this study
are not able to deal with non-nominal argument heads, such us those of NEG, DIS,
MOD (i.e., SPs never predict NEG, DIS, or MOD roles); but, in order to replicate the
same evaluation conditions of typical PropBank-based SRL experiments all arguments
are evaluated. That is, our SP models don?t return any prediction for those, and the
evaluation penalizes them accordingly.
The predicate?role?head triples (p, r, w) for generalizing the selectional preferences
are extracted from the arguments of the training set, yielding 71,240 triples, from which
5,587 different predicate-role selectional preferences (p, r) are derived by instantiating
the different models in Section 3. Tables 9 and 10 show additional statistics about some
of the most (and least) frequent verbs and prepositions in these tuples.
The test set contains 4,134 pairs (covering 505 different predicates) to be classified
into the appropriate role label. In order to study the behavior on out-of-domain data,
we also tested on the PropBanked part of the Brown corpus (Marcus et al 1994). This
corpus contains 2,932 (p, w) pairs covering 491 different predicates.
4.2 Results
The performance of each selectional preference model is evaluated by calculating
the customary precision (P), recall (R), and F1 measures.10 For all experiments re-
ported in this paper, we checked for statistical significance using bootstrap resampling
(100 samples) coupled with one-tailed paired t-test (Noreen 1989). We consider a result
significantly better than another if it passes this test at the 99% confidence interval.
10 P = Correct/Predicted ? 100, R = Correct/Gold ? 100, where Correct is the number of correct predictions,
Predicted is the number of predictions, and Gold is the total number of gold annotations.
F1 = 2PR/(P + R) is the harmonic mean of P and R.
647
Computational Linguistics Volume 39, Number 3
Table 9
Statistics of the three most and least frequent verbs in the training set. Role frame lists the types
of arguments seen in training for each verb; Heads indicates the total number of arguments for
the verb; Heads per role shows the average number of head words for each role; and Unique
heads per role lists the average number of unique head words for each verb?s role.
Verb Role frame Heads Heads Unique heads
per role per role
say Arg0,Arg1,Arg3,AM-ADV, AM-LOC, 7,488 1,069 371
AM-MNR, AM-TMP, AM-LOC,AM-MNR
have Arg0,Arg1,AM-ADV,AM-LOC 3,487 498 189
AM-MNR,AM-NEG,AM-TMP
make Arg0,Arg1,Arg2,AM-ADV 2,207 315 143
AM-LOC,AM-MNR,AM-TMP
... ... ... ... ...
accrete Arg1 1 1 1
accede Arg0 1 1 1
absolve Arg0 1 1 1
Table 10
Statistics of the three most and least frequent prepositions in the training set. Role frame lists
the types of arguments seen in training for each preposition; Heads indicates the total number
of arguments for the preposition; Heads per role shows the average number of head words for
each role; and Unique heads per role lists the average number of unique head words for each
preposition?s role.
Preposition Role frame Heads Heads Unique heads
per role per role
in Arg0,Arg1,Arg2,Arg3,Arg4,Arg5 6,859 403 81
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-EXT,AM-LOC,AM-MNR,AM-NEG,
AM-PNC,AM-PRD,AM-TMP
to Arg0,Arg1,Arg2,Arg3,Arg4, 3,495 233 94
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-EXT,AM-LOC,AM-MNR,AM-PNC,
AM-PRD,AM-TMP
for Arg0,Arg1,Arg2,Arg3,Arg4, 2,935 225 74
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-LOC,AM-MNR,AM-PNC,AM-TMP
... ... ... ... ...
beside Arg2, AM-LOC 2 1 1
atop Arg2, AM-DIR 2 1 1
aboard AM-LOC 1 1 1
Tables 11 and 12 list the results of the various selectional preference models in
isolation. Table 11 shows the results for verb-role SPs, and Table 12 lists the results
for the combination of verb-role and preposition-role SPs as described in Section 3.4.11
It is worth noting that the results of Tables 11 and 12 are calculated over exactly the
11 Note that the results reported here are not identical to those we reported in Zapirain, Agirre, and
Ma`rquez (2009). The differences are two-fold: (a) in our previous experiments we discarded roles such
as MOD, DIS, and NEG, whereas here we evaluate on all roles, and (b) our previous work used only the
subset of the data that could be mapped to VerbNet (around 50%), whereas here we inspect all tuples.
648
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 11
Results for verb-role SPs in the development partition of WSJ, the test partition of WSJ, and the
Brown corpus. For each experiment, we show precision (P), recall (R), and F1. Values in boldface
font are the highest in the corresponding column. F1 values marked with ? are significantly
lower than the highest F1 score in the same column.
Verb-role SPs
Development WSJ Test Brown Test
P R F1 P R F1 P R F1
lexical 73.94 21.81 33.69? 70.75 26.66 39.43? 59.39 05.51 10.08?
SPRes 43.65 35.70 39.28? 45.07 37.11 40.71? 36.34 27.58 31.33?
SPwn 53.09 43.35 47.73? 55.44 45.58 50.03? 41.76 31.58 35.96?
SPsimLin 53.88 44.35 48.65? 52.27 45.13 48.66? 48.30 32.08 38.56?
SPsimJac 48.40 45.53 46.92? 48.85 46.38 47.58? 42.10 34.34 37.82?
SPsimcos 52.37 49.26 50.77? 53.13 50.44 51.75? 43.24 35.27 38.85?
SPsimpreLin
60.29 59.54 59.91 59.93 59.38 59.65 50.79 48.39 49.56
SPsimpreLin?Jac
60.56 56.97 58.71 61.76 58.63 60.16 51.97 42.39 46.69?
SPsimpreLin?cos
60.22 56.64 58.37 61.12 58.12 59.63 51.92 42.35 46.65?
Table 12
Results for combined verb-role and prep-role SPs in the development partition of WSJ, the test
partition of WSJ, and the Brown corpus. For each experiment, we show precision (P), recall (R),
and F1. Values in boldface font are the highest in the corresponding column. F1 values marked
with ? are significantly lower from the highest F1 score in the same column.
Preposition-role and Verb-role SPs
Development WSJ Test Brown Test
P R F1 P R F1 P R F1
lexical 82.05 39.17 53.02? 82.98 43.77 57.31? 68.47 13.60 22.69?
SPRes 63.72 53.09 57.93? 63.47 53.24 57.91? 55.12 44.15 49.03?
SPwn 71.72 59.68 65.15? 65.70 63.88 64.78? 60.08 48.10 53.43?
SPsimLin 63.84 54.58 58.85? 63.75 56.40 59.85? 54.27 39.96 46.04?
SPsimJac 61.75 61.13 61.44? 61.83 61.40 61.61? 55.42 53.45 54.42?
SPsimcos 64.81 64.17 64.49? 64.67 64.22 64.44? 56.56 54.54 55.53?
SPsimpreLin
67.78 67.10 67.44? 68.34 67.87 68.10? 58.43 56.35 57.37?
SPsimpreLin?Jac
69.90 69.20 69.55 70.82 70.33 70.57 62.37 60.15 61.24
SPsimpreLin?cos
69.47 68.78 69.12 70.28 69.80 70.04 62.36 60.14 61.23
same example set. PP arguments are treated by the verb-role SPs by just ignoring the
preposition and considering the head noun of the NP immediately embedded in the PP.
It is worth mentioning that none of the SP models is able to predict the role when
facing a head word missing from the model. This is especially noticeable in the lexical
model, which can only return predictions for words seen in the training data and is
649
Computational Linguistics Volume 39, Number 3
penalized in recall. WordNet based models, which have a lower word coverage com-
pared to distributional similarity?based models, are also penalized in recall.
In both tables, the lexical row corresponds to the baseline lexical match method.
The following rows correspond to the WordNet-based selectional preference models.
The distributional models follow, including the results obtained by the three similarity
formulas on the co-occurrences extracted from the BNC (simJac, simcos simLin), and the
results obtained when using Lin?s pre-computed similarities directly (simpreLin) and as a
second-order vector (simpreLin?Jac and sim
pre
Lin?cos).
First and foremost, this experiment proves that splitting SPs into verb- and
preposition-role SPs yields better results. The comparison of Tables 11 and 12 shows
that the improvements are seen for both precision and recall, but especially remarkable
for recall. The overall F1 improvement is of up to 10 points. Unless stated otherwise, the
rest of the analysis will focus on Table 12.
As expected, the lexical baseline attains a very high precision in all data sets, which
underscores the importance of the lexical head word features in argument classification.
Its recall is quite low, however, especially in Brown, confirming and extending Pradhan,
Ward, and Martin (2008), who also report a similar performance drop for argument
classification on out-of-domain data. All our selectional preference models improve
over the lexical matching baseline in recall, with up to 24 absolute percentage points
in the WSJ test data set and 47 absolute percentage points in the Brown corpus. This
comes at the cost of reduced precision, but the overall F-score shows that all selectional
preference models are well above the baseline, with up to 13 absolute percentage
points on the WSJ data sets and 39 absolute percentage points on the Brown data set.
The results, thus, show that selectional preferences are indeed alleviating the lexical
sparseness problem.12
As an example, consider the following head words of potential arguments of the
verb wear found in the test set: doctor, men, tie, shoe. None of these nouns occurred as
heads of arguments of wear in the training data, and thus the lexical feature would
be unable to predict any role for them. Using selectional preferences, we successfully
assigned the A0 role to doctor and men, and the A1 role to tie and shoe.
Regarding the selectional preference variants, WordNet-based and first-order distri-
butional similarity models attain similar levels of precision, but the former have lower
recall and F1. The performance loss on recall can be explained by the limited lexical
coverage of WordNet when compared with automatically generated thesauri. Examples
of words missing in WordNet include abbreviations (e.g., Inc., Corp.) and brand names
(e.g., Texaco, Sony).
The comparison of the WordNet-based models indicates that our proposal for a
lighter method of WordNet-based selectional preference was successful, as our simpler
variant performs better than Resnik?s method. In manual analysis, we realized that
Resnik?s model tends to always predict the most frequent roles whereas our model
covers a wider role selection. Resnik?s tendency to overgeneralize makes more frequent
roles cover all the vocabulary, and the weighting system penalizes roles with fewer
occurrences.
12 We verified that the lexical model shows higher classification accuracy than all the more elaborate SP
models on the subset of cases covered by both the lexical and the SP models. In this situation, if we aimed
at constructing the best role classifier with SPs alone we could devise a back-off strategy, in the style of
Chambers and Jurafsky (2010), which uses the predictions of the lexical model when present and one of
the SP models if not. As presented in Section 5, however, our main goal is to integrate these SP models
in a real end-to-end SRL system, so we keep their analysis as independent predictors for the moment.
650
Zapirain et al Selectional Preferences for Semantic Role Classification
The results for distributional models indicate that the SPs using Lin?s ready-made
thesaurus (simpreLin) outperforms Pado? and Lapata?s distributional similarity model (Pado?
and Lapata 2007) calculated over the BNC (simLin) in both Tables 11 and 12. This might
be due to the larger size of the corpus used by Lin, but also by the fact that Lin used a
newspaper corpus, compared with the balanced BNC corpus. Further work would be
needed to be more conclusive, and, if successful, could improve further the results of
some SP models.
Among the three similarity metrics using Pado? and Lapata?s software, the cosine
seems to perform consistently better. Regarding the comparison between first-order and
second-order using pre-computed similarity models, the results indicate that second-
order is best when using both the verb-role and prep-role models (cf. Table 12), although
the results for verb-roles are mixed (cf. Table 11). Jaccard seems to provide slightly better
results than cosine for second-order vectors.
In summary, the use of separate verb-role and prep-role models produces the best
results, and second-order similarity is highly competitive. As far as we know, this is
the first time that prep-role models and second-order models are applied to selectional
preference modeling.
5. Semantic Role Classification Experiments
In this section we advance the use of SP in SRL one step further and show that selec-
tional preferences are able to effectively improve performance of a state-of-the-art SRL
system. More concretely, we integrate the information of selectional preference models
in a SRL system and show significant improvements in role classification, especially
when applied to out-of-domain corpora.13
We will use some of the selectional preference models presented in the previous
section. We will focus on the combination of verb-role and prep-role models. Regarding
the similarity models, we will choose the best two performing models from each of
the three families that we tried, namely, the two WordNet models, the two best models
based on the BNC corpus (simJac,simcos), and the two best models based on Lin?s precom-
puted similarity metrics (sim2Jac,sim
2
cos). We left the exploration of other combinations for
future work.
5.1 Integrating Selectional Preferences in Role Classification
For these experiments, we modified the SwiRL SRL system, a state-of-the-art semantic
role labeling system (Surdeanu et al 2007). SwiRL ranked second among the systems
that did not implement model combination at the CoNLL-2005 shared task and fifth
overall (Carreras and Ma`rquez 2005). Because the focus of this section is on role classi-
fication, we modified the SRC component of SwiRL to use gold argument boundaries,
that is, we assume that semantic role identification works perfectly. Nevertheless, for a
realistic evaluation, all the features in the role classification model are generated using
actual syntactic trees generated by the Charniak parser (Charniak 2000).
The key idea behind our approach is model combination: We generate a battery of
base models using all resources available and we combine their outputs using multi-
ple strategies. Our pool of base models contains 13 different models: The first is the
13 The data sets used for the experiments reported in this section are exactly the ones described in
Section 4.1.
651
Computational Linguistics Volume 39, Number 3
unmodified SwiRL SRC, the next six are the selected SP models from the previous
section, and the last six are variants of SwiRL SRC. In each variant, the feature set of
the unmodified SwiRL SRC model is extended with a single feature that models the
choice of a given SP, for example, SRC+SPres contains an extra feature that indicates the
choice of Resnik?s SP model.14
We combine the outputs of these base models using two different strategies: (a)
majority voting, which selects the label predicted by most models, and (b) meta-
classification, which uses a supervised model to learn the strengths of each base model.
For the meta-classification model, we opted for a binary classification approach: First,
for each constituent we generate n data points, one for each distinct role label proposed
by the pool of base models; then we use a binary meta-classifier to label each candidate
role as either correct or incorrect. We trained the meta-classifier on the usual PropBank
training partition, using 10-fold cross-validation to generate outputs for the base
models that require the same training material. At prediction time, for each candidate
constituent we selected the role label that was classified as correct with the highest
confidence.
The binary meta-classifier uses the following set of features:
 Labels proposed by the base models, for example, the feature SRC+SPres=Arg0
indicates that the SRC+SPres base model proposed the Arg0 label. We add
13 such features, one for each base model. Intuitively, this feature allows
the meta-classifier to learn the strengths of each base model with respect
to role labels: SRC+SPres should be trusted for the Arg0 role, and so on.
 Boolean value indicating agreement with the majority vote, for example, the
feature Majority=true indicates that the majority of the base models
proposed the same label as the one currently considered by the
meta-classifier.
 Number of base models that proposed this data point?s label. To reduce sparsity,
for each number of base models, N, we generate N distinct features
indicating that the number of base models that proposed this label is
larger than k, where k ? [0, N). For example, if two base models proposed
the label under consideration, we generate the following two features:
BaseModelNumber>0 and BaseModelNumber>1. This feature provides finer
control over the number of votes received by a label than the majority
voter, for example, the meta-classifier can learn to trust a label if more
than two base models proposed it, even if the majority vote disagrees.
 List of actual base models that proposed this data point?s label. We store a
distinct feature for each base model that proposed the current label, and
also a concatenation of all these base model names. The latter feature is
designed to allow the meta-classifier to learn preferences for certain
combinations of base models. For example, if two base models, SPres and
SPwn, proposed the label under consideration, we generate three features:
Base=SPres, Base=SPwn, and Base=SPres+SPwn.
14 Adding more than one SP output as a feature in SwiRL?s SRC model did not improve performance in
development over the single-SP SRC model. Our conjecture is that the large number of features in SRC
has the potential to drown the SP-based features. This may be accentuated when there are more SP-based
features because their signal is divided among them due to their overlap. We have also tried to add the
input features of the SP models directly to the SRC model but this also proved to be unsuccessful during
development.
652
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 13
Results for the combination approaches. Accuracy shows the overall results. Core and Adj
contain F1 results restricted to the core numbered roles and adjuncts, respectively. SRC is
SwiRL?s standalone SRC model; +SPx stands for the SRC model extended with a feature given by
the corresponding SP model. Values in boldface font are the highest in the corresponding
column. Accuracy values marked with ? are significantly lower than the highest accuracy score
in the same column.
WSJ test Brown test
Acc. Core F1 Adj. F1 Acc. Core F1 Adj. F1
SRC 90.83? 93.25 81.31 79.52 84.42 57.76
+SPRes 90.76? 93.17 81.08 79.86? 84.52 59.24
+SPwn 90.56? 92.88 81.11 79.73? 84.26 59.69
+SPsimJac 90.86? 93.37 80.30 79.83? 84.43 59.54
+SPsimcos 90.87? 93.33 80.92 80.50? 85.14 60.16
+SPsimpreLin?Jac
90.95? 93.03 82.75 80.75? 85.62 59.63
+SPsimpreLin?cos
91.23? 93.78 80.56 80.48? 84.95 61.01
Meta-classifier 92.43 94.62 84.00 81.94 86.25 63.36
Voting 92.36 94.57 83.68 82.15 86.37 63.78
5.2 Results for Semantic Role Classification
Table 13 compares the performance of both combination approaches against the stand-
alone SRC model. In the table, the SRC+SP? models stand for SRC classifiers enhanced
with one feature from the corresponding SP. The meta-classifier shown in the table com-
bines the output of all the 13 base models introduced previously. We implemented the
meta-classifier using Support Vector Machines (SVMs)15 with a quadratic polynomial
kernel, and C = 0.01 (tuned in the development set).16 Lastly, Table 13 shows the results
of the voting strategy, over the same set of base models.
In the columns we show overall classification accuracy and F1 results for both core
arguments (Core) and adjunct arguments (Adj.). Note that for the overall SRC scores, we
report classification accuracy, defined as ratio of correct predictions over total number
of arguments to be classified. The reason for this is that the models in this section always
return a label for all arguments to be classified, and thus accuracy, precision, recall, and
F1 are all equal.
Table 13 indicates that four out of the six SRC+SP? models perform better than the
standalone SRC model in domain (WSJ), and all of them outperform SRC out of domain
(Brown). The improvements are small, however, and, generally, not statistically signifi-
cant. On the other hand, the meta-classifier outperforms the original SRC model both
in domain (17.4% relative error reduction; 1.60 points of accuracy improvement) and
out of domain (13.4% relative error reduction; 2.42 points of accuracy improvement),
and the differences are statistically significant. This experiment proves our claim that
SPs can be successfully used to improve semantic role classification. It also underscores
the fact that combining SRC and SPs is not trivial, however. Our hypothesis is that this
15 http://svmlight.joachims.org.
16 We have also trained the meta-classifier with other learning algorithms (e.g., logistic regression with
L2 regularization) and we obtained similar but slightly lower results.
653
Computational Linguistics Volume 39, Number 3
is caused by the large performance disparity (20 F1 points in domain and 18 out of
domain) between the original SRC model and the standalone SP methods.
Interestingly, the meta-classifier performs only marginally better than the voting ap-
proach in domain and slightly worse out of domain. We believe that this is another effect
of the above observation: Given the weaker SP-based features, the meta-classifier does
not learn much beyond a majority vote, which is exactly what the simpler, unsuper-
vised voting method models. Nevertheless, regardless of the combination method, this
experiment emphasizes that infusing SP information in the SRC task is beneficial.
Table 13 also shows that our approach yields consistent improvements for both
core and adjunct arguments. Out of domain, we see a bigger accuracy improvement
for adjunct arguments (6.02 absolute points) vs. core arguments (1.83 points, for the
voting model). This is to be expected, as most core arguments fall under the Arg0 and
Arg1 classes, which can typically be disambiguated based on syntactic information (i.e.,
subject vs. object). On the other hand, there are no syntactic hints for adjunct arguments,
so the system learns to rely more on SP information in this case.
Regarding the performance of individual combinations of SRC and SP methods
(e.g., SRC+SPRes), the differences among SP models in Table 13 are much smaller
than in Table 12. SPsimpreLin?cos and SPsim
pre
Lin?Jac
yield the best results in both cases, and
distributional methods are slightly stronger than WordNet-based methods. SPRes and
SPwn perform similarly when combined, with a small lead for Resnik?s method. The
smaller differences and changes in the rank among SP methods are due to the complex
interactions when combining SP models with the SRC system.
Table 14
Precision (P), recall (R), and F1 results per argument type for the standalone SRC model and
the meta-classifier, in the two test data sets (WSJ and Brown). Due to space limitations, the
AM- prefix has been dropped from the labels of all adjuncts. When classifying all arguments
(last row), the F1 score is an accuracy score because in this scenario P = R = F1. We checked for
statistical significance for the overall F1 scores (All row). Values in boldface font indicate the
highest F1 score in the corresponding row and block. F1 values marked with ? are significantly
lower than the corresponding highest F1 score.
WSJ test Brown test
SRC Meta-classifier SRC Meta-classifier
P R F1 P R F1 P R F1 P R F1
Arg0 93.6 96.7 95.1 95.1 97.4 96.2 87.6 89.3 88.4 89.4 91.0 90.2
Arg1 93.3 94.5 93.9 94.2 95.7 95.0 84.3 90.6 87.3 86.2 91.9 89.0
Arg2 86.0 82.6 84.3 87.8 87.4 87.6 52.7 56.8 54.7 55.9 59.9 57.8
Arg3 77.6 63.4 69.8 82.4 68.3 74.7 36.4 19.0 25.0 45.8 26.2 33.3
Arg4 86.8 78.6 82.5 89.5 81.0 85.0 59.4 34.5 43.7 67.9 34.5 45.8
Core 92.9 93.6 93.3 94.2 95.1 94.6 82.6 86.3 84.4 84.6 87.9 86.3
ADV 58.5 51.4 54.7 64.4 52.3 57.7 45.1 24.3 31.6 51.9 25.7 34.4
CAU 61.1 71.0 65.7 80.0 77.4 78.7 64.7 45.8 53.7 84.6 45.8 59.5
DIR 46.2 25.0 32.4 68.8 45.8 55.0 64.7 45.8 53.7 73.9 44.5 55.6
DIS 84.3 82.7 83.5 95.6 82.7 88.7 52.6 27.0 35.7 54.5 32.4 40.7
EXT 50.0 12.5 20.0 50.0 12.5 20.0 0.0 0.0 0.0 0.0 0.0 0.0
LOC 85.2 80.9 83.0 85.0 84.7 84.8 67.8 61.2 64.3 68.3 68.7 68.5
MNR 55.8 54.1 55.0 68.9 61.7 65.1 47.4 38.9 42.7 59.2 49.3 53.8
PNC 51.9 37.8 43.8 62.5 40.5 49.2 51.7 39.5 44.8 53.3 42.1 47.1
TMP 93.6 95.9 94.7 92.8 95.9 94.4 79.0 78.1 78.5 84.1 83.2 83.7
Adj 83.1 79.6 81.3 86.2 81.9 84.0 64.9 52.1 57.8 69.8 58.0 63.4
All ? ? 90.8? ? ? 92.4 ? ? 79.5? ? ? 81.9
654
Zapirain et al Selectional Preferences for Semantic Role Classification
Lastly, Table 14 shows a breakdown of the results by argument type for the orig-
inal SRC model and the meta-classifier (results are also presented over all numbered
arguments, Core, adjuncts, and Adj). This comparison emphasizes the previous obser-
vation that SPs are more useful for arguments that are independent of syntax than for
arguments that are usually tied to certain syntactic constructs (i.e., Arg0 and Arg1). For
example, in domain the meta-classifier improves Arg0 classification with 1.1 F1 points,
but it boosts the classification performance for causative arguments (AM-CAU) with 13
absolute points. A similar behavior is observed out of domain. For example, whereas
Arg0 classification is improved with 1.7 points, the classification of manner arguments
(AM-MNR) is improved by 11 points. All in all, with two exceptions, selectional prefer-
ences improve classification accuracy for all argument types, both in and out of domain.
The previous experiments showed that a meta-classifier (and a voting approach)
over a battery of base models improves over the performance of each individual clas-
sifier. Given that half of our base models are all relatively minor changes of the same
original classifier (SwiRL), however, it would be desirable to ensure that the overall
performance gain of the meta-classification system is due to the infusion of semantic
information that is missing in the baseline SRC, and not to a regularization effect coming
from the ensemble of classifiers. The qualitative analysis presented in Section 6 will
reinforce this hypothesis.
5.3 Results for End-to-End Semantic Role Labeling
Lastly, we investigate the contribution of SPs in an end-to-end SRL system. As discussed
before, our approach focuses on argument classification, a subtask of complete SRL,
because this component suffers in the presence of lexical data sparseness (Pradhan,
Ward, and Martin 2008). To understand the impact of SPs on the complete SRL task we
compared two SwiRL models: one that uses the original classification model (the SRC
line in Table 13) and another that uses our meta-classifier model (the Meta-classifier
line in Table 13). To implement this experiment we had to modify the publicly down-
loadable SwiRL model, which performs identification and classification jointly, using a
single multi-class model. We changed this framework to a pipeline model, which first
performs argument identification (i.e., is this constituent an argument or not?), followed
by argument classification (i.e., knowing that this constituent is an argument, what is
its label?).17 We used the same set of features as the original SwiRL system and the
original model to identify argument boundaries. This pipeline model allowed us to
easily plug in different classification models, which offers a simple platform to evaluate
the contribution of SPs in an end-to-end SRL system.
Table 15 compares the original SwiRL pipeline (SwiRL in the table) with the pipeline
model where the classification component was replaced with the meta-classifier previ-
ously introduced (SwiRL w/ meta). The latter model backs off to the original classifi-
cation model for candidates that are not covered by our current selectional preferences
(i.e., are not noun phrases or prepositional phrases containing a noun phrase as the
second child). We report results for the test partitions of WSJ and Brown in the same
table. Note that these results are not directly comparable with the results in Tables 13
and 14, because in those initial experiments we used gold argument boundaries whereas
17 This pipeline model performs slightly worse than the original SwiRL on the WSJ data and slightly better
on Brown.
655
Computational Linguistics Volume 39, Number 3
Table 15
Precision (P), recall (R), and F1 results per argument for the end-to-end semantic role labeling
task. We compared two models: the original SwiRL model and the one where the classification
component was replaced with the meta-classifier introduced at the beginning of the section. We
used the official CoNLL-2005 shared-task scorer to produce these results. We checked for
statistical significance for the overall F1 scores (All row). Values in boldface font indicate the
highest F1 score in the corresponding row and block. F1 values marked with ? are significantly
lower than the corresponding highest F1 score.
WSJ test Brown test
SwiRL SwiRL w/ meta SwiRL SwiRL w/ meta
P R F1 P R F1 P R F1 P R F1
Arg0 87.0 81.6 84.2 87.8 81.9 84.8 86.6 81.3 83.9 87.3 81.7 84.4
Arg1 79.1 71.8 75.3 79.4 72.1 75.6 70.2 64.6 67.3 71.1 65.2 68.0
Arg2 70.0 56.6 62.6 69.2 58.3 63.3 41.8 42.7 42.2 42.3 44.6 43.4
Arg3 72.4 43.9 54.7 72.6 44.5 55.2 36.4 12.9 19.0 34.6 14.5 20.5
Arg4 73.3 61.8 67.0 73.8 60.8 66.7 48.8 25.6 33.6 44.4 25.6 32.5
ADV 59.4 50.6 54.6 59.5 50.0 54.4 49.0 38.2 42.9 49.9 38.5 43.5
CAU 61.5 43.8 51.2 66.0 45.2 53.7 58.7 35.5 44.3 59.1 34.2 43.3
DIR 44.7 20.0 27.6 50.0 22.6 30.9 59.0 27.2 37.2 61.3 25.9 36.5
DIS 76.1 63.8 69.4 77.0 63.8 69.7 58.8 41.0 48.3 59.7 41.3 48.9
EXT 72.7 50.0 59.3 72.7 50.0 59.3 20.0 8.1 11.5 21.4 8.1 11.8
LOC 64.7 52.9 58.2 64.8 55.4 59.7 48.3 37.7 42.3 46.8 40.5 43.5
MNR 59.1 52.0 55.3 61.4 51.7 56.2 53.8 47.3 50.3 55.9 48.3 51.8
PNC 47.1 34.8 40.0 46.4 33.9 39.2 51.8 26.4 35.0 52.4 26.7 35.1
TMP 78.7 71.4 74.9 78.4 71.5 73.8 59.7 60.6 60.2 61.0 61.2 61.1
All 79.7 70.9 75.0? 80.0 71.3 75.4 71.8 64.2 67.8? 72.4 64.6 68.4
Table 15 shows results for an end-to-end model, which includes predicted argument
boundaries.
Table 15 shows that the use of selectional preferences improves overall results when
using predicted argument boundaries as well. Selectional preferences improve F1 scores
for four out of five core arguments in both WSJ and Brown, for six out of nine modifier
arguments in WSJ, and for seven out of nine modifier arguments in Brown. Notably, the
SPs improve results for the most common argument types (Arg0 and Arg1). All in all,
SPs yield a 0.4 F1 point improvement in WSJ and 0.6 F1 point improvement in Brown.
These improvements are small but they are statistically significant. We consider these re-
sults encouraging, especially considering that only a small percentage of arguments are
actually inspected by selectional preferences. This analysis is summarized in Table 16,
which lists how many argument candidates are inspected by the system in its different
stages. The table indicates that the vast majority of argument candidates are filtered
out by the argument identification component, which does not use SPs. Because of this,
even though approximately 50% of the role classification decisions can be reinforced
with SPs, only 4.5% and 3.6% of the total number of argument candidates in WSJ and
Brown, respectively, are actually inspected by the classification model that uses SPs.
6. Analysis and Discussion
We conducted a complementary manual analysis to further verify the usefulness of the
semantic information provided by the selectional preferences. We manually inspected
100 randomly selected classification cases, 50 examples in which the meta-classifier is
656
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 16
Counts for argument candidates for the two test partitions on the end-to-end semantic role
labeling task. The Predicted non-arguments line indicates how many candidate arguments are
classified as non-arguments by the argument identification classifier. The Incompatible with SPs
line indicates how many candidates were classified as arguments but cannot be modeled by our
current SPs (i.e., they are not noun phrases or prepositional phrases containing a noun phrase as
the second child). Lastly, the Compatible with SPs line lists how many candidates were both
classified as likely arguments and can be modeled by the SPs.
WSJ test Brown test
Predicted non-arguments 158,310 184,958
Incompatible with SPs 5,739 11,167
Compatible with SPs 7,691 7,867
Total 171,740 203,992
correct and the baseline SRC (SwiRL) is wrong, and 50 where the meta-classifier chooses
the incorrect classifier and the SRC is right. Interestingly, we observed that the majority
of cases have a clear linguistic interpretation, shedding light on the reasons why the
meta-classifier using SP information manages to correct some erroneous predictions of
the original SRC model, but also on the limitations of selectional preferences.
Regarding the success of the meta-classifier, the studied cases generally correspond
to low frequency verb?argument head pairs, in which the baseline SRC might have
had problems with generalization. In 29 of the cases (?58%), the syntactic information
is not enough to disambiguate the proper role, tends to indicate a wrong role label,
or it confuses the SRC because it contains errors. Most of the semantically based SP
predictions are correct, however, so the meta-classifier does select the correct role label.
In another 15 cases (?30%) the source of the baseline SRC error is not clear, but still,
several SP models suggest the correct role, giving the opportunity to the meta-classifier
to make the right choice. Finally, in the remaining six cases (?12%) a ?chance effect? is
observed: The failure of the baseline SRC model does not have a clear interpretation and,
moreover, most SP predictions are actually wrong. In these situations, several labels are
predicted with the same confidence, and the meta-classifier selects the correct one by
chance.
Figure 2 shows four real examples in which we see the importance of the infor-
mation provided by the selectional preferences. In example (a), the verb flash never
occurs in training with the argument head word news. The syntactic structure alone
strongly suggests Arg0, because the argument is an NP just to the left of a verb in active
form. This is probably why the baseline SRC incorrectly predicts Arg0. Some semantic
information is needed to know that the word news is not the agent of the predicate
(Arg0), but rather the theme (thing shining, Arg1). Selectional preferences make this
work perfectly, because all variants predict the correct label by signaling that news is
much more compatible with flash in Arg1 position rather than Arg0.
In example (b), the predicate promise expects a person as Arg1 (person promised to,
Recipient) and an action as Arg2 (promised action, Theme). Moreover, the presence of
Arg2 is obligatory. The syntactic structure is correct but does not provide the semantic
(Arg1 should be a person) or structural information (the assignment of Arg1 would have
required an additional Arg2) needed to select the appropriate role. SwiRL does not have
it either, and it assigns the incorrect Arg1 label. Most SP models correctly predict that
investigation is more similar to the heads of Arg2 arguments of promise than to the
heads of Arg1 arguments, however.
657
Computational Linguistics Volume 39, Number 3
(a) Several traders could be seen shaking their heads when (([the news]Arg0?Arg1)NP
( flashed)VP)S .
(b) Italian President Francesco Cossiga (promised ([a quick investigation into
whether Olivetti broke Cocom rules]Arg1?Arg2)NP)VP.
(c) Annual payments (will more than double ([from (a year ago)NP]TMP?Arg3)PP to
about $240 million ? ? ? )VP ? ? ?
(d) Procter & Gamble Co. plans to (begin ((testing (next month)NP)VP)S ([a superco.
detergent that ? ? ? washload]Arg0?Arg1)NP)VP .
Figure 2
Examples of incorrect SwiRL role assignments fixed by the meta-classifier. In each sentence, the
verb is emphasized in italics and the head word for the selectional preferences is boldfaced. The
argument under focus is marked within square brackets. x ? y means that the incorrect label x
assigned by the baseline SwiRL model is corrected into role label y by the combined system.
Finally, examples also contain simplified syntactic annotations from the test set predicted
syntactic layer, which are used for the discussion in the text.
In example (c) we see the application of prep-role selectional preferences. In that
sentence, the baseline SRC is likely confused by the content word feature of the PP
?from a year ago? (Surdeanu et al 2003). In PropBank, ?year? is a strong indicator
of a temporal adjunct (AM-TMP). The predicate double, however, describes the Arg3
argument as ?starting point? of the action and it is usually introduced by the preposition
from. This is very common also for other motion verbs (go, rise, etc.), resulting in the
from-Arg3 selectional preference containing a number of heads of temporal expressions,
in particular many more instances of the word year than the from-AM-TMP selectional
preference. As a consequence, the majority of SP models predict the correct Arg3 label.
Finally, example (d) highlights that selectional preferences increase robustness in
front of parsing errors. In this example, the NP ?a superco. detergent? is incorrectly
attached to ?begin? instead of the predicate testing by the syntactic parser. This produces
many incorrect features derived from syntax (syntactic frame, path, etc.) that may con-
fuse the baseline SRC model, which ends up producing an incorrect Arg0 assignment.
Most of the SP models, however, predict that detergent is not a plausible Agent for test
(?examiner?), but instead it fits best with the Arg1 position (?examined?).
Nevertheless, selectional preferences have a significant limitation: They do not
model syntactic structures, which often give strong hints for classification. In fact, the
vast majority of the situations where the meta-classifier performs worse than the origi-
nal SRC model are cases that are syntax-driven, hence situations that are incompletely
addressed by the current SP models. Even though the SRC and the SRC+SP models
have features that model syntax, they can be overwhelmed by the SP features and
standalone models, which leads to incorrect meta-classification results. Figure 3 shows a
few representative examples in this category. In the first example in the figure, the meta-
classifier changes the correctly assigned label Arg2 to Arg1, because most SP models
favor the Arg1 label for the argument ?test.? In the PropBank training corpus, however,
the argument following the verb fail is labeled Arg2 in 79% of the cases. Because the
SP models do not take into account syntax or positional information, this syntactic
preference is lost. Similarly, SPs do not model the fact that the verb buy is seldom
preceded by an Arg1 argument, or the argument immediately following the verb precede
tends to be Arg1, hence the incorrect classifications in Figure 3 (b) and (c). All these
658
Zapirain et al Selectional Preferences for Semantic Role Classification
(a) Some ?circuit breakers? installed after the October 1987 crash (failed ([their first
test ]Arg2?Arg1)NP)VP...
(b) Many fund managers argue that now?s ([the time]TMP?Arg1)NP (to buy)VP)S .
(c) Telephone volume was up sharply, but it was still at just half the level of the
weekend (preceding ([Black Monday ]Arg1?TMP)NP)VP .
Figure 3
Examples of incorrect assignments by the meta-classifier. In each sentence, the verb is
emphasized in italics and the head word for the selectional preferences is boldfaced. The
argument under focus is marked within square brackets. x ? y means that the correct
x label assigned by the baseline model is wrongly converted into y by the meta-classifier.
As in Figure 2, examples also contain simplified syntactic annotations taken from the test
set predicted syntactic layer.
examples are strong motivation for SP models that model both lexical and syntactic
preferences. We will address such models in future work.
7. Conclusions
Current systems usually perform SRL in two pipelined steps: argument identification
and argument classification. Whereas identification is mostly syntactic, classification
requires semantic knowledge to be taken into account. In this article we have shown
that the lexical heads seen in training data are too sparse to assign the correct role,
and that selectional preferences are able to generalize those lexical heads. In fact, we
show for the first time that the combination of the predictions of several selectional
preference models with a state-of-the-art SRC system yields significant improvements in
both in-domain and out-of-domain test sets. These improvements to role classification
translate into small but statistically significant improvements in an end-to-end semantic
role labeling system. We find these results encouraging considering that in the complete
semantic role labeling task only a small percentage of argument candidates are affected
by our modified role classification model. The experiments were carried out over the
well-known CoNLL-2005 data set, based on PropBank.
We applied several selectional preference models, based on WordNet and distribu-
tional similarity. Our experiments show that all models outperform the pure lexical
matching approach, with distributional methods performing better that WordNet-based
methods, and second-order similarity models being the best. In addition to the tradi-
tional selectional preferences for verbs, we introduce the use of selectional preferences
for prepositions, which are applied to classifying prepositional phrases. The combi-
nation of both types of selectional preferences improves over the use of selectional
preferences for verbs alone.
The analysis performed over the cases where the base SRC system and the com-
bined system differed showed that the selectional preferences are specially helpful when
syntactic information is either incorrect or insufficient to disambiguate the correct role.
The analysis also highlighted that the limitations of selectional preferences for modeling
syntactic structures introduce some errors in the combined model. Those errors could
be addressed if the SP models included some syntactic information.
Our research leaves the door open for tighter integration of semantic and syntactic
information for Semantic Role Labeling. We introduced selectional preferences in the
SRC system as simple features, but models which extend syntactic structures with
659
Computational Linguistics Volume 39, Number 3
selectional preferences (or vice versa) could overcome some of the errors that our system
introduced. Extending the use of selectional preferences to other syntactic types beyond
noun phrases and prepositional phrases would be also of interest. In addition, the
method for combining selectional preferences for verbs and prepositions was naive,
and we expect that a joint model of verb and preposition preferences for prepositional
phrases would improve results further. Finally, individual selectional preference meth-
ods could be improved and newer methods incorporated, which could further improve
the results.
Acknowledgments
The authors would like to thank the three
anonymous reviewers for their detailed
and insightful comments on the submitted
version of this manuscript, which helped
us to improve it significantly in this revision.
This work was partially funded by
the Spanish Ministry of Science and
Innovation through the projects OpenMT-2
(TIN2009-14675-C03) and KNOW2
(TIN2009-14715-C04-04). It also received
financial support from the Seventh
Framework Programme of the EU
(FP7/2007- 2013) under grant agreements
247762 (FAUST) and 247914 (MOLTO).
Mihai Surdeanu was supported by the Air
Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion
or recommendations expressed in this
material are those of the authors and do
not necessarily reflect the view of the
Air Force Research Laboratory (AFRL).
References
Agirre, Eneko, Timothy Baldwin, and
David Martinez. 2008. Improving
parsing and PP attachment performance
with sense information. In Proceedings
of ACL-08: HLT, pages 317?325,
Columbus, OH.
Agirre, Eneko, Kepa Bengoetxea, Koldo
Gojenola, and Joakim Nivre. 2011.
Improving dependency parsing with
semantic classes. In Proceedings of the
49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 699?703,
Portland, OR.
Agirre, Eneko and David Martinez. 2001.
Learning class-to-class selectional
preferences. In Proceedings of the 2001
Workshop on Computational Natural
Language Learning (CoNLL-2001),
pages 1?8, Toulouse.
Agirre, Eneko and German Rigau. 1996.
Word sense disambiguation using
conceptual density. In Proceedings of the
16th Conference on Computational
Linguistics - Volume 1, COLING ?96,
pages 16?22, Stroudsburg, PA.
Baroni, Marco and Alessandro Lenci.
2010. Distributional memory: A general
framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of EMNLP, pages 59?68,
Honolulu, HI.
Boas, H. C. 2002. Bilingual framenet
dictionaries for machine translation.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC), pages 1,364?1,371,
Las Palmas de Gran Canaria.
Brockmann, Carsten and Mirella Lapata.
2003. Evaluating and combining
approaches to selectional preference
acquisition. In Proceedings of the 10th
Conference of the European Chapter of the
Association of Computational Linguistics
(EACL-2003), pages 27?34, Budapest.
Carreras, X. and L. Ma`rquez. 2004.
Introduction to the CoNLL-2004
Shared Task: Semantic Role Labeling.
In Proceedings of the Eighth Conference
on Computational Natural Language
Learning (CoNLL-2004), pages 89?97,
Boston, MA.
Carreras, X. and L. Ma`rquez. 2005.
Introduction to the CoNLL-2005
Shared Task: Semantic Role Labeling.
In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 152?164,
Ann Arbor, MI.
Chakraborti, Sutanu, Nirmalie Wiratunga,
Robert Lothian, and Stuart Watt. 2007.
Acquiring word similarities with higher
order association mining. In Proceedings
of the 7th International Conference on
Case-Based Reasoning: Case-Based Reasoning
Research and Development, ICCBR ?07,
pages 61?76, Berlin.
660
Zapirain et al Selectional Preferences for Semantic Role Classification
Chambers, Nathanael and Daniel Jurafsky.
2010. Improving the use of pseudo-words
for evaluating selectional preferences. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics,
pages 445?453, Uppsala, Sweden.
Charniak, E. 2000. A maximum-entropy
inspired parser. In Proceedings of the 1st
Meeting of the North American Chapter
of the Association for Computational
Linguistics (NAACL-2000), pages 132?139,
Seattle, WA.
Clark, Stephen and Stephen Weir. 2002.
Class-based probability estimation
using a semantic hierarchy. Computational
Linguistics, 28(2):187?206.
Edmonds, Philip. 1997. Choosing the word
most typical in context using a lexical
co-occurrence network. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics and Eighth
Conference of the European Chapter of the
Association for Computational Linguistics,
ACL ?98, pages 507?509, Stroudsburg, PA.
Erk, Katrin. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics
(ACL-2007), pages 216?223, Prague.
Erk, Katrin, Sebastian Pado?, and Ulrike Pado?.
2010. A flexible, corpus-driven model of
regular and inverse selectional preferences.
Computational Linguistics, 36(4):723?763.
Fillmore, C. J., J. Ruppenhofer, and C. F.
Baker. 2004. FrameNet and representing
the link between semantic and syntactic
relations. In Frontiers in Linguistics,
volume I of Language and Linguistics
Monograph Series B. Institute of Linguistics,
Academia Sinica, Taipei, pages 19?59.
Gildea, D. and D. Jurafsky. 2002. Automatic
labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Grefenstette, Gregory. 1992. Sextant:
Exploring unexplored contexts for
semantic extraction from syntactic
analysis. In ACL?92, pages 324?326,
Newark, DE.
Higashinaka, Ryuichiro and Hideki Isozaki.
2008. Corpus-based question answering
for why-questions. In Proceedings of the
Third International Joint Conference on
Natural Language Processing (IJCNLP),
pages 418?425, Hyderabad.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics
(ACL-1990), pages 268?275, Pittsburgh, PA.
Koo, Terry, Xavier Carreras, and Michael
Collins. 2008. Simple semi-supervised
dependency parsing. In Proceedings
of ACL-08: HLT, pages 595?603,
Columbus, OH.
Lee, Lillian. 1999. Measures of distributional
similarity. In 37th Annual Meeting of the
Association for Computational Linguistics,
pages 25?32, College Park, MD.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Lin, Dekang. 1998. Automatic retrieval
and clustering of similar words. In
Proceedings of the 36th Annual Meeting
of the Association for Computational
Linguistics and the 17th International
Conference on Computational Linguistics
(COLING-ACL-1998), pages 768?774,
Montreal.
Litkowski, K. C. and O. Hargraves. 2005. The
preposition project. In Proceedings of the
ACL-SIGSEM Workshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistic Formalisms and
Applications, pages 171?179, Colchester.
Litkowski, K. C. and O. Hargraves. 2007.
SemEval-2007 Task 06: Word-sense
disambiguation of prepositions.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 24?29, Prague.
Litkowski, Ken and Orin Hargraves.
2006. Coverage and inheritance in the
preposition project. In Prepositions ?06:
Proceedings of the Third ACL-SIGSEM
Workshop on Prepositions, pages 37?44,
Trento.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument
structure. In Proceedings of the Workshop on
Human Language Technology (HLT-94),
pages 114?119, Plainsboro, NJ.
Ma`rquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008.
Semantic role labeling: An introduction to
the special issue. Computational Linguistics,
34(2):145?159.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguisties, 29:639?654.
Melli, Gabor, Yang Wang, Yudong Liu,
Mehdi M. Kashani, Zhongmin Shi,
661
Computational Linguistics Volume 39, Number 3
Baohua Gu, Anoop Sarkar, and Fred
Popowich. 2005. Description of SQUASH,
the SFU question answering summary
handler for the DUC-2005 summarization
task. In Proceedings of Document
Understanding Workshop, HLT/EMNLP
Annual Meeting, Vancouver.
Moschitti, Alessandro, Silvia Quarteroni,
Roberto Basili, and Suresh Manandhar.
2007. Exploiting syntactic and shallow
semantic kernels for question/answer
classification. In Proceedings of the
45th Conference of the Association for
Computational Linguistics (ACL),
pages 776?783, Prague.
Narayanan, S. and S. Harabagiu. 2004.
Question answering based on semantic
structures. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING), pages 693?701,
Geneva.
Noreen, E. W. 1989. Computer-Intensive
Methods for Testing Hypotheses:
An Introduction, Wiley.
O? Se?aghdha, Diarmuid. 2010. Latent
variable models of selectional preference.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 435?444, Uppsala.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pado?, Sebastian, Ulrike Pado?, and Katrin Erk.
2007. Flexible, corpus-based modelling
of human plausibility judgements. In
Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL-2007),
pages 400?409, Prague.
Palmer, M., D. Gildea, and P. Kingsbury.
2005. The proposition bank: An annotated
corpus of semantic roles. Computational
Linguistics, 31(1):71?105.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: Learning
inferential selectional preferences.
In Human Language Technologies 2007:
The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main
Conference, pages 564?571, Rochester, NY.
Pantel, Patrick and Dekang Lin. 2000. An
unsupervised approach to prepositional
phrase attachment using contextually
similar words. In Proceedings of the 38th
Annual Conference of the Association of
Computational Linguistics (ACL-2000),
pages 101?108, Hong Kong.
Pradhan, S., W. Ward, and J. H. Martin. 2008.
Towards robust semantic role labeling.
Computational Linguistics, 34(2):289?310.
Rada, R., H. Mili, E. Bicknell, and M. Blettner.
1989. Development and application of a
metric on semantic nets. IEEE Transactions
on Systems, Man, and Cybernetics,
19(1):17?30.
Ratinov, Lev and Dan Roth. 2009. Design
challenges and misconceptions in named
entity recognition. In Proceedings of the
Thirteenth Conference on Computational
Natural Language Learning (CoNLL-2009),
pages 147?155, Boulder, CO.
Resnik, Philip. 1993a. Selection and
Information: A Class-Based Approach to
Lexical Relationships. Ph.D. thesis,
University of Pennsylvania.
Resnik, Philip. 1993b. Semantic classes and
syntactic ambiguity. In Proceedings of the
Workshop on Human Language Technology,
pages 278?283, Morristown, NJ.
Ritter, Alan, Mausam, and Oren Etzioni.
2010. A latent Dirichlet alocation method
for selectional preferences. In Proceedings of
the 48th Annual Meeting of the Association for
Computational Linguistics, pages 424?434,
Uppsala.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?123.
Srikumar, V. and D. Roth. 2011. A joint
model for extended semantic role labeling.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 129?139, Edinburgh.
Surdeanu, M., S. Harabagiu, J. Williams,
and P. Aarseth. 2003. Using predicate-
argument structures for information
extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics (ACL-2003), pages 8?15,
Sapporo.
Surdeanu, Mihai, Massimiliano Ciaramita,
and Hugo Zaragoza. 2011. Learning to
rank answers to non-factoid questions
from Web collections. Computational
Linguistics, 37(2):351?383.
Surdeanu, Mihai, Llu??s Ma`rquez, Xavier
Carreras, and Pere R. Comas. 2007.
Combination strategies for semantic role
labeling. Journal of Artificial Intelligence
Research (JAIR), 29:105?151.
Sussna, Michael. 1993. Word sense
disambiguation for free-text indexing
using a massive semantic network. In
Proceedings of the Second International
662
Zapirain et al Selectional Preferences for Semantic Role Classification
Conference on Information and Knowledge
Management, CIKM ?93, pages 67?74,
New York, NY.
Wilks, Yorick. 1975. Preference semantics.
In E. L. Kaenan, editor, Formal Semantics of
Natural Language. Cambridge University
Press, Cambridge, MA, pages 329?348.
Zapirain, Ben?at, Eneko Agirre, and Llu??s
Ma`rquez. 2009. Generalizing over lexical
features: Selectional preferences for
semantic role classification. In Proceedings
of the Joint Conference of the 47th Annual
Meeting of the Association for Computational
Linguistics and the 4th International Joint
Conference on Natural Language Processing
(ACL-IJCNLP-2009), pages 73?76, Suntec.
Zapirain, Ben?at, Eneko Agirre, Llu??s
Ma`rquez, and Mihai Surdeanu. 2010.
Improving semantic role classification
with selectional preferences. In Proceedings
of the 11th Annual Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL HLT
2010), pages 373?376, Los Angeles, CA.
663

Random Walks for Knowledge-Based
Word Sense Disambiguation
Eneko Agirre?
IXA NLP group
University of the Basque Country
Oier Lo?pez de Lacalle??
University of Edinburgh
IKERBASQUE
Basque Foundation for Science
Aitor Soroa?
IXA NLP group
University of the Basque Country
Word Sense Disambiguation (WSD) systems automatically choose the intended meaning of a
word in context. In this article we present a WSD algorithm based on random walks over large
Lexical Knowledge Bases (LKB). We show that our algorithm performs better than other graph-
based methods when run on a graph built from WordNet and eXtended WordNet. Our algorithm
and LKB combination compares favorably to other knowledge-based approaches in the literature
that use similar knowledge on a variety of English data sets and a data set on Spanish. We include
a detailed analysis of the factors that affect the algorithm. The algorithm and the LKBs used are
publicly available, and the results easily reproducible.
1. Introduction
Word Sense Disambiguation (WSD) is a key enabling technology that automatically
chooses the intended sense of a word in context. It has been the focus of intensive
research since the beginning of Natural Language Processing (NLP), and more recently
it has been shown to be useful in several tasks such as parsing (Agirre, Baldwin, and
Martinez 2008; Agirre et al. 2011), machine translation (Carpuat and Wu 2007; Chan,
Ng, and Chiang 2007), information retrieval (Pe?rez-Agu?era and Zaragoza 2008; Zhong
and Ng 2012), question answering (Surdeanu, Ciaramita, and Zaragoza 2008), and
? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: e.agirre@ehu.es.
?? IKERBASQUE, Basque Foundation for Science, 48011, Bilbao, Basque Country.
E-mail: oier.lopezdelacalle@gmail.com.
? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: a.soroa@ehu.es.
Submission received: 30 July 2011; Revised submission received: 15 November 2012; Accepted for publication:
17 February 2013.
doi:10.1162/COLI a 00164
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
summarization (Barzilay and Elhadad 1997). WSD is considered to be a key step in
order to approach language understanding beyond keyword matching.
The best performing WSD systems are currently those based on supervised learn-
ing, as attested in public evaluation exercises (Snyder and Palmer 2004; Pradhan et al.
2007), but they need large amounts of hand-tagged data, which is typically very ex-
pensive to produce. Contrary to lexical-sample exercises (where plenty of training and
testing examples for a handful of words are provided), all-words exercises (which
comprise all words occurring in a running text, and where training data is more scarce)
show that only a few systems beat the most frequent sense (MFS) heuristic, with small
differences. For instance, the best system in SensEval-3 scored 65.2 F1, compared to 62.4
(Snyder and Palmer 2004). The best current state-of-the-art WSD system (Zhong and Ng
2010), outperforms the MFS heuristic by 5% to 8% in absolute F1 scores on the SensEval
and SemEval fine-grained English all words tasks.
The causes of the small improvement over the MFS heuristic can be found in the
relatively small amount of training data available (sparseness) and the problems that
arise when the supervised systems are applied to different corpora from that used to
train the system (corpus mismatch) (Ng 1997; Escudero, Ma?rquez, and Rigau 2000).
Note that most of the supervised systems for English are trained over SemCor (Miller
et al. 1993), a half-a-million word subset of the Brown Corpus made available from the
WordNet team, and DSO (Ng and Lee 1996), comprising 192,800 word occurrences from
the Brown and WSJ corpora corresponding to the 191 most frequent nouns and verbs.
Several researchers have explored solutions to sparseness. For instance, Chan and Ng
(2005) present an unsupervised method to obtain training examples from bilingual data,
which was used together with SemCor and DSO to train one of the best performing
supervised systems to date (Zhong and Ng 2010).
In view of the problems of supervised systems, knowledge-based WSD is emerging
as a powerful alternative. Knowledge-based WSD systems exploit the information in
a lexical knowledge base (LKB) to perform WSD. They currently perform below su-
pervised systems on general domain data, but are attaining performance close or above
MFS without access to hand-tagged data (Ponzetto and Navigli 2010). In this sense, they
provide a complementary strand of research which could be combined with supervised
methods, as shown for instance in Navigli (2008). In addition, Agirre, Lo?pez de Lacalle,
and Soroa (2009) show that knowledge-based WSD systems can outperform supervised
systems in a domain-specific data set, where MFS from general domains also fails. In
this article, we will focus our attention on knowledge-based methods.
Early work for knowledge-based WSD was based on measures of similarity
between pairs of concepts. In order to maximize pairwise similarity for a sequence of
n words where each has up to k senses, the algorithms had to consider up to kn sense
sequences. Greedy methods were often used to avoid the combinatorial explosion
(Patwardhan, Banerjee, and Pedersen 2003). As an alternative, graph-based methods
are able to exploit the structural properties of the graph underlying a particular LKB.
These methods are able to consider all possible combinations of occurring senses on a
particular context, and thus offer a way to analyze efficiently the inter-relations among
them, gaining much attention in the NLP community (Mihalcea 2005; Navigli and
Lapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata 2010).
The nodes in the graph represent the concepts (word senses) in the LKB, and edges
in the graph represent relations between them, such as subclass and part-of. Network
analysis techniques based on random walks like PageRank (Brin and Page 1998) can
then be used to choose the senses that are most relevant in the graph, and thus output
those senses.
58
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
In order to deal with large knowledge bases containing more than 100,000 con-
cepts (Fellbaum 1998), previous algorithms had to extract subsets of the LKB (Navigli
and Lapata 2007, 2010) or construct ad hoc graphs for each context to be dis-
ambiguated (Mihalcea 2005; Sinha and Mihalcea 2007). An additional reason for the
use of custom-built subsets of ad hoc graphs for each context is that if we were using
a centrality algorithm like PageRank over the whole graph, it would choose the most
important senses in the LKB regardless of context, limiting the applicability of the
algorithm. For instance, the word coach is ambiguous at least between the ?sports
coach? and the ?transport service? meanings, as shown in the following examples:
(1) Nadal is sharing a house with his uncle and coach, Toni, and his physical trainer,
Rafael Maymo.
(2) Our fleet comprises coaches from 35 to 58 seats.
If we were to run a centrality algorithm over the whole LKB, with no context, then
we would always assign coach to the same concept, and we would thus fail to correctly
disambiguate either one of the given examples.
The contributions of this article are the following: (1) A WSD method based on ran-
dom walks over large LKBs. The algorithm outperforms other graph-based algorithms
when using a LKB built from WordNet and eXtended WordNet. The algorithm and
LKB combination compares favorably to the state-of-the-art in knowledge-based WSD
on a wide variety of data sets, including four English and one Spanish data set. (2) A
detailed analysis of the factors that affect the algorithm. (3) The algorithm together with
the corresponding graphs are publicly available1 and can be applied easily to sense
inventories and knowledge bases different from WordNet.
The algorithm for WSD was first presented in Agirre and Soroa (2009). In this article,
we present further evaluation on two more recent data sets, analyze the parameters and
options of the system, compare it to the state of the art, and discuss the relation of our
algorithm with PageRank and the MFS heuristic.
2. Related Work
Traditional knowledge-based WSD systems assign a sense to an ambiguous word by
comparing each of its senses with those of the surrounding context. Typically, some se-
mantic similarity metric is used for calculating the relatedness among senses (Lesk 1986;
Patwardhan, Banerjee, and Pedersen 2003). The metric varies between counting word
overlaps between definitions of the words (Lesk 1986) to finding distances between
concepts following the structure of the LKB (Patwardhan, Banerjee, and Pedersen 2003).
Usually the distances are calculated using only hierarchical relations on the LKB (Sussna
1993; Agirre and Rigau 1996). Combining both intuitions, Jiang and Conrath (1997)
present a metric that combines statistics from corpus and a lexical taxonomy structure.
One of the major drawbacks of these approaches stems from the fact that senses are
compared in a pairwise fashion and thus the number of computations grows exponen-
tially with the number of words?that is, for a sequence of n words where each has
up to k senses they need to consider up to kn sense sequences. Although alternatives
like simulated annealing (Cowie, Guthrie, and Guthrie 1992) and conceptual density
1 http://ixa2.si.ehu.es/ukb.
59
Computational Linguistics Volume 40, Number 1
(Agirre and Rigau 1996) were tried, most of the knowledge-based WSD at the time was
done in a suboptimal word-by-word greedy process, namely, disambiguating words
one at a time (Patwardhan, Banerjee, and Pedersen 2003). Still, some recent work on
finding predominant senses in domains has applied such similarity-based techniques
with success (McCarthy et al. 2007).
Recently, graph-based methods for knowledge-based WSD have gained much at-
tention in the NLP community (Mihalcea 2005; Navigli and Velardi 2005; Navigli and
Lapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata
2010). These methods use well-known graph-based techniques to find and exploit the
structural properties of the graph underlying a particular LKB. Graph-based techniques
consider all the sense combinations of the words occurring on a particular context at
once, and thus offer a way to analyze the relations among them with respect to the
whole graph. They are particularly suited for disambiguating words in the sequence,
and they manage to exploit the interrelations among the senses in the given context.
In this sense, they provide a principled solution to the exponential explosion problem
mentioned before, with excellent performance.
Graph-based WSD is performed over a graph composed of senses (nodes) and
relations between pairs of senses (edges). The relations may be of several types (lexico-
semantic, cooccurrence relations, etc.) and may have some weight attached to them. All
the methods reviewed in this section use some version of WordNet as a LKB. Apart
from relations in WordNet, some authors have used semi-automatic and fully auto-
matic methods to enrich WordNet with additional relations. Mihalcea and Moldovan
(2001) disambiguated WordNet glosses in a resource called eXtended WordNet. The
disambiguated glosses have been shown to improve results of a graph-based system
(Agirre and Soroa 2008), and we have also used them in our experiments. Navigli and
Velardi (2005) enriched WordNet with cooccurrence relations semi-automatically and
showed that those relations are effective in a number of graph-based WSD systems
(Navigli and Velardi 2005; Navigli and Lapata 2007, 2010). More recently, Cuadros and
Rigau (2006, 2007, 2008) learned automatically so-called KnowNets, and showed that
the new provided relations improved WSD performance when plugged into a simple
vector-based WSD system. Finally, Ponzetto and Navigli (2010) have acquired relations
automatically from Wikipedia, released as WordNet++, and have shown that they are
beneficial in a graph-based WSD algorithm. All of these relations are publicly available
with the exception of Navigli and Velardi (2005), but note that the system is available
on-line.2
Disambiguation is typically performed by applying a ranking algorithm over the
graph, and then assigning the concepts with highest rank to the corresponding words.
Given the computational cost of using large graphs like WordNet, most researchers use
smaller subgraphs built on-line for each target context. The main idea of the subgraph
method is to extract the subgraph whose vertices and relations are particularly relevant
for the set of senses from a given input context. The subgraph is then analyzed and the
most relevant vertices are chosen as the correct senses of the words.
The TextRank algorithm for WSD (Mihalcea 2005) creates a complete weighted
graph (e.g., a graph in which every pair of distinct vertices is connected by a weighted
edge) formed by the synsets of the words in the input context. The weight of the links
joining two synsets is calculated by executing Lesk?s algorithm (Lesk 1986) between
them?that is, by calculating the overlap between the words in the glosses of the
2 http://lcl.uniroma1.it/ssi.
60
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
corresponding senses. Once the complete graph is built, a random walk algorithm
(PageRank) is executed over it and words are assigned to the most relevant synset.
In this sense, PageRank is used as an alternative to simulated annealing to find the
optimal pairwise combinations. This work is extended in Sinha and Mihalcea (2007),
using a collection of semantic similarity measures when assigning a weight to the links
across synsets. They also compare different graph-based centrality algorithms to rank
the vertices of the complete graph. They use different similarity metrics for different
POS types and a voting scheme among the centrality algorithm ranks.
In Navigli and Velardi (2005), the authors develop a knowledge-based WSD method
based on lexical chains called structural semantic interconnections (SSI). Although the
system was first designed to find the meaning of the words in WordNet glosses, the
authors also apply the method for labeling each word in a text sequence. Given a
text sequence, SSI first identifies monosemous words and assigns the corresponding
synset to them. Then, it iteratively disambiguates the rest of the terms by selecting
the senses that get the strongest interconnection with the synsets selected so far. The
interconnection is calculated by searching for paths on the LKB, constrained by some
hand-made rules of possible semantic patterns.
In Navigli and Lapata (2007, 2010), the authors perform a two-stage process for
WSD. Given an input context, the method first explores the whole LKB in order to find
a subgraph that is particularly relevant for the words of the context. The subgraph is
calculated by applying a depth-first search algorithm over the LKB graph for every
word sense occurring in a context. Then, they study different graph-based centrality
algorithms for deciding the relevance of the nodes on the subgraph. As a result, every
word of the context is attached to the highest ranking concept among its possible senses.
The best results were obtained by a simple algorithm like choosing the concept for each
word with the largest degree (number of edges) and by PageRank (Brin and Page 1998).
We reimplemented their best methods in order to compare our algorithm with theirs on
the same setting (cf. Section 6.3). In later work (Ponzetto and Navigli 2010) the authors
apply a subset of their methods to an enriched WordNet with additional relations from
Wikipedia, improving their results for nouns.
Tsatsaronis, Vazirgiannis, and Androutsopoulos (2007) and Agirre and Soroa (2008)
also use such a two-stage process. They build the graph as before, but using breadth-
first search. The first authors apply a spreading activation algorithm over the subgraph
for node ranking, while the second use PageRank. In later work (Tsatsaronis, Varlamis,
and N?rva?g 2010) spreading activation is compared with PageRank and other centrality
measures like HITS (Kleinberg 1998), obtaining better results than in their previous
work.
This work departs from earlier work in its use of the full graph, and its ability to
infuse context information when computing the importance of nodes in the graph. For
this, we resort to an extension of the PageRank algorithm (Brin and Page 1998), called
Personalized PageRank (Haveliwala 2002), which tries to bias PageRank using a set of
representative topics and thus capture more accurately the notion of importance with
respect to a particular topic. In our case, we initialize the random walk with the words
in the context of the target word, and thus we obtain a context-dependent PageRank.
We will show that this method is indeed effective for WSD. Note that in order to use
other centrality algorithms (e.g., HITS [Kleinberg 1998]), previous authors had to build
a subgraph first. In principle, those algorithms could be made context-dependent when
using the full graph and altering their formulae, but we are not aware of such variations.
Random walks over WordNet using Personalized PageRank have been also used
to measure semantic similarity between two words (Hughes and Ramage 2007; Agirre
61
Computational Linguistics Volume 40, Number 1
et al. 2009). In those papers, the random walks are initialized with a single word,
whereas we use all content words in the context. The results obtained by the authors,
especially in the latter paper, are well above other WordNet-based methods.
Most previous work on knowledge-based WSD has presented results on one or two
general domain corpora for English. We present our results on four general domain
data sets for English and a Spanish data set (Ma`rquez et al. 2007). Alternatively, some
researchers have applied knowledge-based WSD to specific domains, using different
methods to adapt the method to the particular test domain. In Agirre, Lo?pez de Lacalle,
and Soroa (2009) and Navigli et al. (2011), the authors apply our Personalized PageRank
method to a domain-specific corpus with good results. Ponzetto and Navigli (2010) also
apply graph-based algorithms to the same domain-specific corpus.
3. WordNet
Most WSD work uses WordNet as the sense inventory of choice. WordNet (Fellbaum
1998) is a freely available3 lexical database of English, which groups nouns, verbs,
adjectives, and adverbs into sets of synonyms, each expressing a distinct concept (called
synset in WordNet parlance). For instance, coach has five nominal senses and two verbal
senses, which correspond to the following synsets:
<coach#n1, manager#n2, handler#n3>
<coach#n2, private instructor#n1, tutor#n1>
<coach#n3, passenger car#n1, carriage#n1>
<coach#n4, four-in-hand#n2, coach-and-four#n1>
<coach#n5, bus#n1, autobus#n1, charabanc#n1,double-decker#n1,jitney#n1 . . .>
<coach#v1, train#v7>
<coach#v2>
In these synsets coach#n1 corresponds to the first nominal sense of coach, coach#v1 corre-
sponds to the first verbal sense, and so on. Each of the senses of coach corresponds to a
different synset, and each synset contains several words with different sense numbers.
For instance, the first nominal sense of coach has two synonyms: manager in its second
sense and handler in its third sense. As a synset can be identified by any of its words
in a particular sense number, we will use a word and sense number to represent the
full concept. Each synset has a descriptive gloss (e.g., a carriage pulled by four horses with
one driver for coach#n4, or drive a coach for coach#v2). The examples correspond to the
current version of WordNet (3.1), but the sense differences have varied across different
versions. There exist automatic mappings across versions (Daude, Padro, and Rigau
2000), but they contain small errors. In this article we will focus on WordNet versions
1.7 and 2.1, which have been used to tag the evaluation data sets used in this article
(cf. Section 6).
The synsets in WordNet are interlinked with conceptual-semantic and lexical rela-
tions. Examples of conceptual-semantic relations are hypernymy, which corresponds to
the superclass or is-a relation, and holonymy, the part-of relation. Figure 1 shows two
small regions of the graph around three synsets of the word coach, including several
conceptual-semantic relations and lexical relations. For example, the figure shows that
concept trainer#n1 is a coach#n1 (hypernymy relation), and that seat#n1 is a part of
coach#n5 (holonymy relation). The figure only shows a small subset of the relations
3 http://wordnet.princeton.edu.
62
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
Figure 1
Example showing three senses of coach, with links to related concepts.
for three synsets of coach. If we were to show the relations of the rest of the synsets
in WordNet we would end up with a densely connected graph, where one can go from
one synset to another following the semantic relations. In addition to purely conceptual-
semantic relations which hold between synsets, there are also lexical relations which
hold between specific senses. For instance, angry#a2 is the antonym of calm#a2 and a
derivation relation exists between handler#n3 and handle#v6, meaning that handler is a
derived form of handle and that the third nominal sense of handler is related to the sixth
verbal sense of handle. Although lexical relations hold only between two senses, we gen-
eralize to the whole synset. This generalization captures the notion that if handler#n3 is
related by derivation to handle#v6, then coach#n1 is also semantically related to handle#v6
(as shown in Figure 1).
In addition to these relations, we also use the relation between each synset and
the words in the glosses. Most of the words in the glosses have been manually asso-
ciated with their corresponding senses, and we can thus produce a link between the
synset being glossed, and the synsets of each of the words in the gloss. For instance,
following one of the given glosses, a gloss relation would be added between coach#v2
and drive#v2. The gloss relations were not available prior to WordNet 3.0, and we
thus used automatically disambiguated glosses for WordNet 1.7 and WordNet 2.1, as
made freely available in the eXtended WordNet (Mihalcea and Moldovan 2001). Note
also that the eXtended WordNet provided about 550,000 relations, whereas the disam-
biguated glosses made available with WordNet 3.0 provide around 339,000 relations.
We compare the performance of XWN relations and WordNet 3.0 gloss relations in
Section 6.4.4.
Table 1 summarizes the most relevant relations (with less frequent relations
grouped as ?other?). The table also lists how we grouped the relations, and the overall
counts. Note that inverse relations are not counted, as their numbers equal those of the
original relation. In Section 6.4.5 we report the impact of the relations in the behavior
of the system. Overall, the graph for WordNet 1.7 has 109, 359 vertices (concepts) and
620, 396 edges (relations between concepts). Note that there is some overlap between
XWN and other types of relations. For instance, the hypernym of coach#n4 is carriage#n2,
which is also present in its gloss. Note that most of the relation types relate concepts
from the same part of speech, with the exception of derivation and XWN.
63
Computational Linguistics Volume 40, Number 1
Table 1
Relations and their inverses in WordNet 1.7, how we grouped them, and overall counts. XWN
refers to relations from the disambiguated glosses in eXtended WordNet.
relation inverse group counts
hypernymy hyponymy TAX 89,078
derivation derivation REL 28,866
holonymy meronymy MER 21,260
antonymy antonymy ANT 7,558
other other REL 3,134
xwn xwn?1 XWN 551,551
Finally, we have also used the Spanish WordNet (Atserias, Rigau, and Villarejo
2004). In addition to the native relations, we also added relations from the eXtended
WordNet. All in all, it contains 105, 501 vertices and 623, 316 relations.
3.1 Representing WordNet as a Graph
An LKB such as WordNet can be seen as a set of concepts and relations among them,
plus a dictionary, which contains the list of words (typically word lemmas) linked to
the corresponding concepts (senses). WordNet can be thus represented as a graph G =
(V, E). V is the set of nodes, where each node represents one concept (vi ? V), and E
is the set of edges. Each relation between concepts vi and vj is represented by an edge
ei,j ? E. We ignore the relation type of the edges. If two WordNet relations exist between
two nodes, we only represent one edge, and ignore the type of the relation. We chose to
use undirected relations between concepts, because most of the relations are symmetric
and have their inverse counterpart (cf. Section 3), and in preliminary work we failed to
see any effect using directed relations.
In addition, we also add vertices for the dictionary words, which are linked to
their corresponding concepts by directed edges (cf. Figure 1). Note that monosemous
words will be related to just one concept, whereas polysemous words may be attached
to several. Section 5.2 explains the reason for using directed edges, and also mentions
an alternative to avoid introducing these vertices.
4. PageRank and Personalized PageRank
The PageRank random walk algorithm (Brin and Page 1998) is a method for ranking
the vertices in a graph according to their relative structural importance. The main idea
of PageRank is that whenever a link from vi to vj exists in a graph, a vote from node i
to node j is produced, and hence the rank of node j increases. In addition, the strength
of the vote from i to j also depends on the rank of node i: The more important node
i is, the more strength its votes will have. Alternatively, PageRank can also be viewed
as the result of a random walk process, where the final rank of node i represents the
probability of a random walk over the graph ending on node i, at a sufficiently large
time.
Let G be a graph with N vertices v1, . . . , vN and di be the outdegree of node i; let M
be a N ? N transition probability matrix, where Mji = 1di if a link from i to j exists, and
64
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
zero otherwise. Then, the calculation of the PageRank Vector P over G is equivalent to
resolving Equation (1).
P = cMP+ (1 ? c)v (1)
In the equation, v is a N ? 1 stochastic vector and c is the so-called damping
factor, a scalar value between 0 and 1. The first term of the sum on the equation
models the voting scheme described in the beginning of the section. The second term
represents, loosely speaking, the probability of a surfer randomly jumping to any node
(e.g., without following any paths on the graph). The damping factor, usually set in
the [0.85..0.95] range, models the way in which these two terms are combined at
each step.
The second term in Equation (1) can also be seen as a smoothing factor that makes
any graph fulfill the property of being aperiodic and irreducible, and thus guarantees
that the PageRank calculation converges to a unique stationary distribution.
In the traditional PageRank formulation the vector v is a stochastic normalized
vector whose element values are all 1N , thus assigning equal probabilities to all nodes
in the graph in the case of random jumps. However, as pointed out by Haveliwala
(2002), the vector v can be non-uniform and assign stronger probabilities to certain
kinds of nodes, effectively biasing the resulting PageRank vector to prefer these nodes.
For example, if we concentrate all the probability mass on a unique node i, all random
jumps on the walk will return to i and thus its rank will be high; moreover, the high rank
of i will make all the nodes in its vicinity also receive a high rank. Thus, the importance
of node i given by the initial distribution of v spreads along the graph on successive
iterations of the algorithm. As a consequence, the P vector can be seen as representing
the relevance of every node in the graph from the perspective of node i.
In this article, we will use Static PageRank to refer to the case when a uniform
v vector is used in Equation (1); and whenever a modified v is used, we will call it
Personalized PageRank. The next section shows how we define a modified v.
PageRank is actually calculated by applying an iterative algorithm that computes
Equation (1) successively until convergence below a given threshold is achieved, or
until a fixed number of iterations are executed. Following usual practice, we used a
damping value of 0.85 and finish the calculations after 30 iterations (Haveliwala 2002;
Langville and Meyer 2003; Mihalcea 2005). Some preliminary experiments with higher
iteration counts showed that although sometimes the node ranks varied, the relative
order among particular word synsets remained stable after the initial iterations (cf.
Section 6.4 for further details). Note that, in order to discard the effect of dangling
nodes (i.e., nodes without outlinks) one would need to slightly modify Equation (1)
following Langville and Meyer (2003).4 This modification is not necessary for WordNet,
as it does not have dangling nodes.
5. Random Walks for WSD
We tested two different methods to apply random walks to WSD.
4 The equation becomes P = cMP + (ca + (1 ? c)e)v, where ai = 1 if node i is a dangling node,
and 0 otherwise, and e is a vector of all ones.
65
Computational Linguistics Volume 40, Number 1
5.1 Static PageRank, No Context
If we apply traditional PageRank over the whole WordNet, we get a context-
independent ranking of word senses. All concepts in WordNet get ranked according
to their PageRank value. Given a target word, it suffices to check which is the relative
ranking of its senses, and the WSD system would output the one ranking highest. We
call this application of PageRank to WSD Static PageRank STATIC for short, as it does
not change with the context, and we use it as a baseline.
As the PageRank measure over undirected graphs for a node is closely related
to the degree of the node, the Static PageRank returns the most predominant sense
according to the number of relations the senses have. We think that this is closely related
to the Most Frequent Sense attested in general corpora, as the lexicon builders would
tend to assign more relations to the most predominant sense. In fact, our results (cf.
Section 6.4.5) show that this is indeed the case for the English WordNet.
5.2 Personalized PageRank, Using Context
Static PageRank is independent of context, but this is not what we want in a WSD
system. Given an input piece of text we want to disambiguate all content words in
the input according to the relationships among them. For this we can use Personalized
PageRank (PPR for short) over the whole WordNet graph.
Given an input text (e.g., a sentence), we extract the list Wi i = 1 . . .m of content
words (i.e., nouns, verbs, adjectives, and adverbs) that have an entry in the dictionary,
and thus can be related to LKB concepts. As a result of the disambiguation process,
every LKB concept receives a score. Then, for each target word to be disambiguated, we
just choose its associated concept in G with maximum score.
In order to apply Personalized PageRank over the LKB graph, the context words
are first inserted into the graph G as nodes, and linked with directed edges to their
respective concepts. Then, the Personalized PageRank of the graph G is computed by
concentrating the initial probability mass uniformly over the newly introduced word
nodes. As the words are linked to the concepts by directed edges, they act as source
nodes injecting mass into the concepts they are associated with, which thus become
relevant nodes, and spread their mass over the LKB graph. Therefore, the resulting
Personalized PageRank vector can be seen as a measure of the structural relevance of
LKB concepts in the presence of the input context.
Making the edges from words to concepts directed is important, as the use of
undirected edges will move part of the probability mass in the concepts to the word
nodes. Note the contrast with the edges representing relations between concepts, which
are undirected (cf. Section 3.1).
Alternatively, we could do without the word nodes, concentrating the initial prob-
ability mass on the senses of the words under consideration. Such an initialization over
the graph with undirected edges between synset nodes is equivalent to initializing
the walk on the words in a graph with undirected edges between synset nodes and
directed nodes from words to synsets. We experimentally checked that the results
of both alternatives are indistinguishable. Although the alternative without nodes is
marginally more efficient, we keep the word nodes as they provide a more intuitive and
appealing formalization.
One problem with Personalized PageRank is that if one of the target words has
two senses that are related by semantic relations, those senses reinforce each other, and
could thus dampen the effect of the other senses in the context. Although one could
66
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
remove direct edges between competing senses from the graph, it is quite rare that those
senses are directly linked, and usually a path with several edges is involved. With this
observation in mind we devised a variant called word-to-word heuristic (PPRw2w for
short), where we run Personalized PageRank separately for each target word in the
context, that is, for each target word Wi, we concentrate the initial probability mass
in the senses of the rest of the words in the context of Wi, but not in the senses of
the target word itself, so that context words increase their relative importance in the
graph. The main idea of this approach is to avoid biasing the initial score of concepts
associated with target word Wi, and let the surrounding words decide which concept
associated with Wi has more relevance. Contrary to the previous approach, PPRw2w does
not disambiguate all target words of the context in a single run, which makes it less
efficient (cf. Section 6.4).
Figure 2 illustrates the disambiguation of a sample sentence. The STATIC method
(not shown in the figure) would choose the synset coach#n1 for the word coach because
it is related to more concepts than other senses, and because those senses are related
to concepts that have a high degree (for instance, sport#1). The PPR method (left side
of Figure 2) concentrates the initial mass on the content words in the example. After
running the iterative algorithm, the system would return coach#n1 as the result for the
target word coach. Although the words in the sentence clearly indicate that the correct
synset in this sentence corresponds to coach#n5, the fact that teacher#n1 is related to
trainer#n1 in WordNet causes both coach#n2 and coach#n1 to reinforce each other, and
make their pagerank higher. The right side of Figure 2 depicts the PPRw2w method,
where the word coach is not activated. Thus, there is no reinforcement between coach
senses, and the method would correctly choose coach#n5 as the proper synset.
6. Evaluation
WSD literature has used several measures for evaluation. Precision is the percentage of
correctly disambiguated instances divided by the number of instances disambiguated.
Some systems don?t disambiguate all instances, and thus the precision can be high
even if the system disambiguates a handful of instances. In our case, when a word has
Figure 2
Portion of WordNet to illustrate the disambiguation of coach in the sentence Our fleet comprises
coaches from 35 to 58 seats. Each word in the sentence (shown partially) is linked to all its synsets.
The path between trainer#n1 and teacher#1 is omitted for brevity (see Figure 1). The left part
shows the PPR method, and the right part shows the PPRw2w method.
67
Computational Linguistics Volume 40, Number 1
two senses with the same PageRank value, our algorithm does not return anything,
because it abstains from returning a sense in the case of ties. In contrast, recall measures
the percentage of correctly disambiguated instances divided by the total number of
instances to be disambiguated. This measure penalizes systems that are unable to return
a solution for all instances. Finally, the harmonic mean between precision and recall
(F1) combines both measures. F1 is our main measure of evaluation, as it provides a
balanced measure between the two extremes. Note that a system that returns a solution
for all instances would have equal precision, recall, and F1 measures.
In our experiments we build a context of at least 20 content words for each sentence
to be disambiguated, taking the sentences immediately before and after it in the case that
the original sentence was too short. The parameters for the PageRank algorithm were
set to 0.85 and 30 iterations following standard practice (Haveliwala 2002; Langville
and Meyer 2003; Mihalcea 2005). The post hoc impact of those and other parameters
has been studied in Section 6.4.
The general domain data sets used in this work are the SensEval-2 (S2AW) (Snyder
and Palmer 2004), SensEval-3 (S3AW) (Palmer et al. 2001), and SemEval-2007 fine-
grained (S07AW) (Palmer et al. 2001; Snyder and Palmer 2004; Pradhan et al. 2007)
and coarse grained all-words data sets (S07CG) (Navigli, Litkowski, and Hargraves
2007). All data sets have been produced similarly: A few documents were selected for
tagging, at least two annotators tagged nouns, verbs, adjectives, and adverbs, inter-
tagger agreement was measured, and the discrepancies between taggers were solved.
The first two data sets are labeled with WordNet 1.7 tags, the third uses WordNet
2.1 tags, and the last one uses coarse-grained senses that group WordNet 2.1 senses.
We run our system using WordNet 1.7 relations and senses for the first two data sets,
and WordNet 2.1 for the other two. Section 6.4.3 explores the use of WordNet 3.0 and
compares the performance with the use of other versions.
Regarding the coarse senses in S07CG, we used the mapping from WordNet 2.1
senses made available by the authors of the data set. In order to return coarse grained-
senses, we run our algorithm on fine-grained senses, and aggregate the scores for
all senses that map to the same coarse-grained sense. We finally choose the coarse-
grained sense with the highest score.
The data sets used in this article contain polysemous and monosemous words,
as customary; the percentage of monosemous word occurrences in the S2AW, S3AW,
S07AW, and S07CG data sets are 20.7%, 16.9%, 14.4%, and 29.9%, respectively.
6.1 Results
Table 2 shows the results as F1 of our random walk WSD systems over these data
sets. We detail overall results, as well as results per part of speech, and whether there
is any statistical difference with respect to the best result on each column. Statistical
significance is obtained using the paired bootstrap resampling method (Noreen 1989),
p < 0.01.
The table shows that PPRw2w is consistently the best method in three data sets. All
in all the differences are small, and in one data set STATIC obtains the best results. The
differences with respect to the best system overall are always statistically significant.
In fact, it is remarkable that a simple non-contextual measure like STATIC performs so
well, without the need for building subgraphs or any other manipulation. Section 6.4.6
will show that in some circumstances the performance of STATIC is much lower and
analyzes the reasons for this drop. Regarding the use of the word-to-word heuristic, it
consistently provides slightly better results than PPR in all four data sets. An analysis of
68
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
Table 2
Results on English data sets (F1). Best results in each column in bold. ? Statistically significant
with respect to the best result in each column.
S2AW - SensEval-2 All-Words
Method All N V Adj. Adv.
PPR 58.7? 71.8 35.0 58.9 69.8
PPRw2w 59.7 70.3 40.3 59.8 72.9
STATIC 58.0? 66.5 40.2 59.8 72.5
S3AW - SensEval-3 All-Words
Method All N V Adj. Adv.
PPR 57.3? 63.7 47.5 61.3 96.3
PPRw2w 57.9 65.3 47.2 63.6 96.3
STATIC 56.5? 62.5 47.1 62.8 96.3
S07AW - SemEval 2007 All-Words
Method All N V Adj. Adv.
PPR 39.7? 51.6 34.6 ? ?
PPRw2w 41.7? 56.0 35.3 ? ?
STATIC 43.0 56.0 37.3 ? ?
S07CG - SemEval 2007 Coarse-grained All-Words
Method All N V Adj. Adv.
PPR 78.1? 78.3 73.8 84.0 78.4
PPRw2w 80.1 83.6 71.1 83.1 82.3
STATIC 79.2? 81.0 72.4 82.9 82.8
the performance according to the POS shows that PPRw2w performs better particularly
on nouns, but there does not seem to be a clear pattern for the rest. In the rest of the
article, we will only show the overall results, omitting those for all POS, in order not to
clutter the result tables.
Our algorithms do not always return an answer, and thus the precision is higher
than the F1 measure. For instance, in S2AW the percentage of instances that get an
answer ranges between 95.4% and 95.6% for PPR, PPRw2w, and STATIC. The precision
for PPRw2w in S2AW is 61.1%, the recall is 58.4%, and F1 is 59.7%. This pattern of slightly
higher values for precisions, lower values for recall, and F1 in between is repeated for
all data sets, POS, and data sets. The percentage of instances that get an answer for the
other data sets is higher, ranging between 98.1% in S3AW and 99.9% in S07CG.
6.2 Comparison to State-of-the-Art Systems
In this section we compare our results with the WSD systems described in Section 2, as
well as the top performing supervised systems at competition time and other unsuper-
vised systems that improved on them. Note that we do not mention all unsupervised
systems participating in the competitions, but we do select the top performing ones. All
results in Table 3 are given as overall F1 for all Parts of Speech, but we also report F1 for
nouns in the case of S07CG, where Ponz10 (Ponzetto and Navigli 2010) reported very
69
Computational Linguistics Volume 40, Number 1
high results, but only for nouns. Note that the systems reported here and our system
might use different context sizes.
For easier reference, Table 3 uses a shorthand for each system, whereas the text in
this section includes the shorthand and the full reference the first time the shorthand is
used. The shorthand uses the first letters of the first author followed by the year of the
paper, except for systems which participated in SensEval and SemEval, where we use
their acronym. Most systems in the table have been presented in Section 2, with a few
exceptions that will be presented this section.
The results in Table 3 confirm that our system (PPRw2w) performs on the state-of-the-
art of knowledge-based and unsupervised systems, with two exceptions:
(1) Nav10 (Navigli and Lapata 2010) obtained better results on S07AW.
We will compare both systems in more detail below, and also include
a reimplementation in the next subsection which shows that, when
using the same LKB, our method obtains better results.
(2) Although not reported in the table, an unsupervised system using
automatically acquired training examples from bilingual data (Chan and
Ng 2005) obtained very good results on S2AW nouns (77.2 F1, compared
with our 70.3 F1 in Table 2). The automatically acquired training examples
are used in addition to hand-annotated data in Zhong10 (Zhong and Ng
2010), also reported in the table (see below).
We report the best unsupervised systems in S07AW and S07CG on the same row.
JU-SKNSB (Naskar and Bandyopadhyay 2007) is a system based on an extended version
Table 3
Comparison with state-of-the-art results (F1). The top rows report knowledge-based and
unsupervised systems, followed by our system (PPRw2w). Below we report systems that use
annotated data to some degree: (1) MFS or counts from hand-annotated corpora, (2) fully
supervised systems, including the best supervised participants in each exercise. Best result
among unsupervised systems in each column is shown in bold. Please see text for references
of each system.
System S2AW S3AW S07AW S07CG (N)
Mih05 54.2 52.2
Sinha07 57.6 53.6
Tsatsa10 58.8 57.4
Agirre08 56.8
Nav10 52.9 43.1
JU-SKNSB / TKB-UO 40.2 70.2 (70.8)
Ponz10 (79.4)
PPRw2w 59.7 57.9 41.7 80.1 (83.6)
MFS(1) 60.1 62.3 51.4 78.9 (77.4)
IRST-DDD-00(1) 58.3
Nav05(1) / UOR-SSI(1) 60.4 83.2 (84.1)
BESTsup (2) 68.6 65.2 59.1 82.5 (82.3)
Zhong10(2) 68.2 67.6 58.3 82.6
70
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
of the Lesk algorithm (Lesk 1986), evaluated on S07AW. TKB-UO (Anaya-Sa?nchez,
Pons-Porrata, and Berlanga-Llavori 2007), which was evaluated in S07CG, clusters
WordNet senses and uses so-called topic signatures based on WordNet information for
disambiguation. IRST-DDD-00 (Strapparava, Gliozzo, and Giuliano 2004) is a system
based on WordNet domains which leverages on large unannotated corpora. They
obtained excellent results, but their calculation of scores takes into account synset
probabilities from SemCor, and the system can thus be considered to use some degree
of supervision. We consider that systems which make use of information derived from
hand-annotated corpora need to be singled out as having some degree of supervision.
This includes systems using the MFS heuristic, as it is derived from hand-annotated
corpora. In the case of the English WordNet, the use of the first sense also falls in this
category, as the order of senses in WordNet is based on sense counts in hand-annotated
corpora. Note that for wordnets in other languages, hand-annotated corpus is scarce,
and thus our main results do not use this information. Section 6.4.7 analyzes the results
of our system when combined with this information.
Among supervised systems, the best supervised systems at competition time are
reported in a single row (Mihalcea 2002; Decadt et al. 2004; Chan, Ng, and Zhong
2007; Tratz et al. 2007). We also report Zhong10 (Zhong and Ng 2010), which is a freely
available supervised system giving some of the strongest results in WSD.
We will now discuss in detail the systems that are most similar to our own. We first
review the WordNet versions and relations used by each system. Mih05 (Mihalcea 2005)
and Sinha07 (Sinha and Mihalcea 2007) apply several similarity methods, which use
WordNet information from versions 1.7.1 and 2.0, respectively, including all relations
and the text in the glosses.5 Tsatsa10 (Tsatsaronis, Varlamis, and N?rva?g 2010) uses
WordNet 2.0. Agirre08 (Agirre and Soroa 2008) experimented with several LKBs formed
by combining relations from different sources and versions, including WordNet 1.7 and
eXtended WordNet. Nav05 and Nav10 (Navigli and Velardi 2005; Navigli and Lapata
2010) use WordNet 2.0, enriched with manually added co-occurrence relations which
are not publicly available.
We can see in Table 3 that the combination of Personalized PageRank and LKB
presented in this article outperforms both Mih05 and Sinha07. In order to factor
out the difference in the WordNet version, we performed experiments using WN2.1
and eXtended WordNet, yielding 58.7 and 56.5 F1 for S2AW and S3AW, respectively.
Although a head-to-head comparison is not possible, the systems use similar informa-
tion: Although they use glosses, our algorithm cannot directly use the glosses, and thus
we use disambiguated glosses as delivered in eXtended WordNet. All in all the results
suggest that analyzing the LKB structure as a graph is preferable to computing pairwise
similarity measures over synsets to build a custom graph and then applying graph
measures. The results of various in-house experiments replicating Mih05 also confirmed
this observation. Note also that our methods are simpler than the combination strategy
used in Sinha07.
Nav05 (Navigli and Velardi 2005) uses a knowledge-based WSD method based on
lexical chains called structural semantic interconnections (SSI). The SSI method was
evaluated on the SensEval-3 data set, as shown in row Nav05 in Table 3. Note that
the method labels an instance with the MFS of the word if the algorithm produces
no output for that instance, which makes comparison to our system unfair, especially
given the fact that the MFS performs better than SSI. In fact, it is not possible to separate
5 Personal communication.
71
Computational Linguistics Volume 40, Number 1
the effect of SSI from that of the MFS, and we thus report it as using some degree of
supervision in the table. A variant of the algorithm called UOR-SSI (Navigli, Litkowski,
and Hargraves 2007) (reported in the same row) used a manually added set of 70,000
relations and obtained the best results in S07CG out-of-competition,6 even better than
the best supervised method. Reimplementing SSI is not trivial, so we did not check the
performance of a variant of SSI that does not use MFS and that uses the same LKB as
our method. Section 6.4.7 analyzes the results of our system when combined with MFS
information.
Agirre08 (Agirre and Soroa 2008) uses breadth-first search to extract subgraphs of
the WordNet graph for each context to be disambiguated, and then applies PageRank.
Our better results seem to indicate that using the full graph instead of those subgraphs
would perform better. In order to check whether the better results are due to differences
in the information used, the next subsection presents the results of our reimplementa-
tion of the systems using the same information as our full-graph algorithm.
Tsatsa10 (Tsatsaronis, Vazirgiannis, and Androutsopoulos 2007; Tsatsaronis,
Varlamis, and N?rva?g 2010) also builds the graph using breadth-first search, but
weighting each type of edge differently, and using graph-based measures that take into
account those weights. This is in contrast to the experiments performed in this article
where edges have no weight, and is an interesting avenue for future work.
Nav10 (Navigli and Lapata 2010) first builds a subgraph of WordNet composed of
paths between synsets using depth-first search and then applies a set of graph centrality
algorithms. The best results are obtained using the degree of the nodes, and they present
two variants, depending on how they treat ties: Either they return a sense at random, or
they return the most frequent sense. For fair comparison to our system (which does not
use MFS as a back-off), Table 3 reports the former variant as Nav10. This system is better
than ours in one data set and worse in another. They use 60,000 relations that are not
publicly available, but they do not use eXtended WordNet relations. In order to check
whether the difference in performance is due to the relations used or the algorithm,
the next subsection presents a reimplementation of their best graph-based algorithms
using the same LKB as we do. In earlier work (Navigli and Lapata 2007) they test a
similar system on S3AW, but report results only for nouns, verbs, and adjectives (F1 of
61.9, 36.1, and 62.8, respectively), all of which are below the results of our system (cf.
Table 2).
In Ponz10 (Ponzetto and Navigli 2010) the authors apply the same techniques as in
Nav10 to a new resource called WordNet++. They report results for nouns using degree
on subgraphs for the S07CG data set, as shown in Table 3. Their F1 on nouns is 79.4,
lower than our results using our LKB.
6.3 Comparison with Related Algorithms
The previous section shows that our algorithm when applied to a LKB built from
WordNet and eXtended WordNet outperforms other knowledge-based systems in all
cases but one system in one data set. In this section we factor out algorithm and LKB,
and present the results of other graph-based methods for WSD using the same WordNet
versions and relations as in the previous section. As we mentioned in Section 2, ours
is the only method using the full WordNet graph. Navigli and Lapata (2010) and
Ponzetto and Navigli (2010) build a custom graph based on the relations in WordNet
6 The task was co-organized by the authors.
72
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
as follows: For each sense si of each word in the context, a depth-first search (DFS for
short) is conducted through the WordNet graph starting in si until another sense sj of a
word in the context is found or maximum distance is reached. The maximum distance
was set by the authors to 6. All nodes and edges between si and sj, inclusive, are added to
the subgraph. Graph-based measures are then used to select the output senses for each
target word, with degree and PageRank yielding the best results. In closely related work,
Agirre and Soroa (2008) and Tsatsaronis, Varlamis, and N?rva?g (2010) use breadth-first
search (BFS) over the whole graph, and keep all paths connecting senses. Note that
unlike the dfs approach, bfs does not require any threshold. The subgraphs obtained
by each of these methods are slightly different.
We reimplemented both strategies, namely, DFS with threshold 6 and BFS with no
threshold. Table 4 shows the overall results of degree and PageRank for both kinds of
subgraphs. DFS yields slightly better results than BFS but PPRw2w is best in all four data
sets, with statistical significance.
In addition, we run PPR and PPRw2w on DFS and BFS subgraphs, and obtained better
results than degree and PageRank in all data sets. DFS with PPR and DFS with PPRw2w
are best in S3AW and S07AW, respectively, although the differences with PPRw2w are not
statistically significant. PPRw2w on the full graph is best in two data sets, with statistical
significance.
From these results we can conclude that PPR and PPRw2w yield the best results
also for subgraphs. Regarding the use of the full graph with respect to DFS or BFS,
the performances for PPRw2w are very similar, but using the full graph gives a small
advantage. Section 6.4.5 provides an analysis of efficiency.
6.4 Analysis of Performance Factors
The behavior of the WSD system is influenced by a set of parameters that can yield
different results. In our main experiments we did not perform any parameter tuning;
we just used some default values which were found to be useful according to previous
work. In this section we perform a post hoc analysis of several parameters on the general
performance of the system, reporting F1 on a single data set, S2AW.
Table 4
Results for subgraph methods compared with our method (F1). In the Reference column we
mention the reference system that we reimplemented. Best results in each column in bold. ?
Statistically significant with respect to the best result in each column. 0 No significant difference.
Reference S2AW S3AW S07AW S07CG
DFSdegree Nav10, Ponz10 58.4? 56.4? 40.3? 79.4?
BFSdegree 57.9? 56.5? 39.9? 79.2?
DFSPageRank Nav10 58.2? 56.4? 39.9? 79.6?
BFSPageRank Agirre08 57.7? 56.7? 39.7? 79.4?
DFSPPR 59.3? 58.2 41.40 78.1?
BFSPPR 58.80 57.50 41.20 78.8?
DFSPPRw2w 58.7
0 58.00 41.2? 79.7?
BFSPPRw2w 58.1
? 57.90 41.9 79.5?
PPRw2w 59.7 57.90 41.70 80.1
73
Computational Linguistics Volume 40, Number 1
55
56
57
58
59
60
0 5 10 15 20 25 30
Iterations



  
Figure 3
Convergence according to number of PageRank iterations (F1 on S2AW).
6.4.1 PageRank Parameters. The PageRank algorithm has two main parameters, the
so-called damping factor and the number of iterations (or, conversely, the convergence
threshold), which we set as 0.85 and 30, respectively (cf. Section 4). Figure 3 depicts
the effect of varying the number of iterations. It shows that the algorithm converges
very quickly: One sole iteration yields relatively high performance, and 20 iterations
are enough to achieve convergence. Note also that the performance is in the [58.0, 58.5]
range for iterations over 5. Note that we use the same range of F1 for the y axis of Figures
3, 4, and 5 for easier comparison.
Figure 4 shows the effect of varying the damping factor. Note that a damping
factor of zero means that the PageRank value coincides with the initial probability
distribution. Given the way we initialize the distribution (c.f. Section 5.2), it would mean
that the algorithm is not able to disambiguate the target words. Thus, the initial value
on Figure 4 corresponds to a damping factor of 0.001. On the other hand, a damping
factor of 1 yields to the same results as the STATIC method (c.f. Section 5.1). The best
value is attained with 0.90, with similar values around it (less than 0.5 absolute points in
55
56
57
58
59
60
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Damping factor









Figure 4
Varying the damping factor (F1 on S2AW).
74
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
55
56
57
58
59
60
5 10 15 20 25 30 35 40 45 50
Context size

 


 

Figure 5
Varying the context size (F1 on S2AW).
variation), in agreement with previous results which preferred values in the 0.85...0.95
interval (Haveliwala 2002; Langville and Meyer 2003; Mihalcea 2005).
6.4.2 Size of Context Window. Figure 5 shows the performance of the system when trying
different context windows for the target words. The best context size is for windows
of 20 content words, with less than 0.5 absolute point losses for windows in the [5, 25]
range.
6.4.3 Using Different WordNet Versions. There has been little research on the best strat-
egy to use when dealing with data sets and resources attached to different WordNet
versions. Table 5 shows the results for the four data sets used in this study when using
different WordNet versions. Two of the data sets (S2AW and S3AW) were tagged with
senses from version 1.7, S07AW with senses from version 2.1, and S07CG with coarse
senses built on version 2.1 senses.
Given the fact that WordNet 3.0 is a more recent version that includes more rela-
tions, one would hope that using it would provide the best results (Cuadros and Rigau
2008; Navigli and Lapata 2010). We built a graph analogous to the ones for versions
1.7 and 2.1, but using the hand-disambiguated glosses instead of eXtended WordNet
glosses. We used freely available mappings (Daude, Padro, and Rigau 2000)7 to convert
our eXtended WordNet relations to 3.0, and then the WordNet 3.0 sense results to the
corresponding version. In addition, we also tested WN1.7 on S07AW and S07CG, and
WN2.1 on S2AW and S3AW, also using the mappings from Daude, Padro, and Rigau
(2000).
Table 5 shows that the best results are obtained using our algorithm on the same
WordNet version as used in the respective data set. When testing on data sets tagged
with WordNet 1.7, similar results are obtained using 2.1 or 3.0. When testing on data sets
based on 2.1, 3.0 has a small lead over 1.7. In any case, the differences are small ranging
from 1.4 absolute points to 0.5 points. All in all, it seems that the changes introduced
7 http://nlp.lsi.upc.edu/tools/download-map.php.
75
Computational Linguistics Volume 40, Number 1
Table 5
Comparing WordNet versions. Best result in each row in bold.
Data set version 1.7 + xwn 2.1 + xwn 3.0 + xwn
S2AW 1.7 59.7 58.7 58.4
S3AW 1.7 57.9 56.5 56.8
S07AW 2.1 40.7 41.7 40.9
S07CG 2.1 coarse 79.6 80.1 79.6
by different versions slightly deteriorate the results, and the best strategy is to use the
same WordNet version as was used for tagging.
6.4.4 Using xwn vs. WN3.0 Gloss Relations. WordNet 3.0 was released with an accom-
panying data set comprising glosses where some of the words had been manually
disambiguated. In Table 6 we present the results of using these glosses with the WN3.0
graph, showing that the results are lower than using XWN relations. We also checked
the use of WN3.0 gloss relation with other WordNet versions, and the results using
XWN were always slightly better. We hypothesize that the better results for XWN are
due to the amount of relations, with XWN holding 61% more relations than WN3.0
glosses. Still, the best relations are obtained with the combination of both kinds of gloss
relations.
6.4.5 Analysis of Relations. Previous results were obtained using all the relations
of WordNet and taking eXtended WordNet relations into account. In this section
we analyze the effect of the relation types on the whole process, following the
relation groups presented in Table 1. Table 7 shows the results when using different
combinations over relation types. The eXtended WordNet XWN relations appear the
most valuable when performing random walk WSD, as their performance is as good
as when using the whole graph, and they produce a large drop when ablated from the
graph. Ignoring antonymy relations produces a small improvement, but the differences
between using all the relations, eliminating antonyms, and using XWN relations only
are too small to draw any further conclusions. It seems that given the XWN relations
(the most numerous, cf. Section 3.1), our algorithm is fairly robust to the addition or
deletion of other kinds of relations (less numerous).
6.4.6 Behavior with Respect to STATIC and MFS. The high results of the very simple STATIC
method (PageRank with no context) seems to imply that there is no need to use context
for disambiguation. Our intuition was that the synsets which correspond to the most
Table 6
Comparing XWN and WN3.0 gloss relations, separately and combined. Best result in each row in
bold.
Data set 3.0 + XWN 3.0 + gloss 3.0 + XWN + gloss
S2AW 58.4 58.1 58.8
S3AW 56.8 51.7 56.1
S07AW 40.9 38.8 42.2
S07CG 79.6 78.9 80.2
76
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
Table 7
Analysis of relation types. The first column shows the performance using just that relation type.
The second shows the combination of TAX and each type. The last column shows all relations
except the corresponding type.
relation single + TAX ablation
TAX 37.4 ? 59.9
ANT 19.1 42.1 59.9
MER 23.4 36.4 59.6
REL 35.4 46.1 59.6
XWN 59.9 59.8 47.1
Reference system (all relations) 59.7
frequent senses would get more relations. We thus computed the correlation between
systems, gold tags, and MFS. In order to make the correlation results comparable to
the figures used on evaluation, we use the number of times both sets of results agree,
divided by the number of results returned by the first system. Table 8 shows such a
matrix of pairwise correlations. If we take the row of gold tags, the results reflect the
performance of each system (the precision). In the case of MFS, the column shows that
STATIC has a slightly larger correlation with the MFS than the other two methods. The
matrix also shows that all our three methods agree more than 80% of the time, with PPR
and STATIC having a relatively smaller agreement.
In contrast, related work using the same techniques over domain-specific words
(Agirre, Lo?pez de Lacalle, and Soroa 2009) shows that the results of our Personalized
PageRank models departs significantly from MFS and STATIC. Table 9 shows the results
of the three techniques on the three subcorpora that constitute the evaluation data set
published in Koeling, McCarthy, and Carroll (2005). This data set consists of examples
retrieved from the Sports and Finance sections of the Reuters corpus, and also from the
balanced British National Corpus (BNC), which is used as a general domain contrast
corpus.
Applying PageRank over the entire WordNet graph yields low results, very similar
to those of MFS, and below those of Personalized PageRank. This confirms that STATIC
PageRank is closely related to MFS, as we hypothesized in Section 5.1 and showed in
Table 8 for the other general domain data sets. Whereas the results of PPRw2w are very
similar in the general-domain BNC, PPRw2w departs from STATIC and MFS with 30 and
20 points of difference in the domain-specific Sports and Finance corpora. These results
are highly relevant, because they show that PPR is able to effectively use contextual
information, and depart from the MFS and STATIC baselines.
Table 8
Correlation between systems, gold tags, and MFS.
Gold MFS PPR PPRw2w STATIC
Gold 100.0 61.3 58.6 59.7 57.8
MFS 60.1 100.0 79.8 79.0 81.3
PPR 57.4 79.8 100.0 86.8 82.8
PPRw2w 58.4 79.0 86.8 100.0 86.4
STATIC 56.7 81.4 82.8 86.4 100.0
77
Computational Linguistics Volume 40, Number 1
Table 9
Results on three subcorpora as reported in Agirre, Lo?pez de Lacalle, and Soroa (2009),
where Sports and Finance are domain-specific. Best results on each column in bold.
System BNC Sports Finance
MFS 34.9 19.6 37.1
STATIC 36.6 20.1 39.6
PPRw2w 37.7 51.5 59.3
Table 10
Combination with MFS (F1). The first two rows correspond to our system with and without
information from MFS. Below that we report systems that also use MFS. Best results in each
column in bold.
System S2AW S3AW S07AW S07CG (N)
PPRw2w 59.7 57.9 41.7 80.1 (83.6)
PPRw2w MFS 62.6 63.0 48.6 81.4 (82.1)
MFS 60.1 62.3 51.4 78.9 (77.4)
IRST-DDD-00 58.3
Nav05 / UOR-SSI 60.4 83.2 (84.1)
Ponz10 81.7 (85.5)
6.4.7 Combination with MFS. As mentioned in Section 6.2, we have avoided using any
information regarding sense frequencies from annotated corpora, as this information
is not always available for all wordnets. In this section we report the results of our
algorithm when taking into account prior probabilities of senses taken from sense
counts. We used the sense counts provided with WordNet in the index.sense file.8 In
this setting, the edges linking words and their respective senses are weighted according
to the prior probabilities of those senses, instead of uniform weights as in Section 5.2.
Table 10 shows that results when using priors from MFS improve over the results
of the original PPRw2w in all data sets. The improvement varies across parts of speech,
and, for instance, the results for nouns in S07CG are worse (shown in rightmost column
of Table 10). In addition, the results for PPRw2w when using MFS information improve
over MFS in all cases except for S07AW.
The table also reports the best systems that do use MFS (see Section 6.3 for detailed
explanations). For S2AW and S07AW we do not have references to related systems.
For S3AW we can see that our system performs best. In the case of S07CG, UOR-SSI
reports better results than our system. Finally, the final row reports their system when
combined with MFS information as back-off (Ponzetto and Navigli 2010), which also
attains better results than our system. We tried to use a combination method similar to
theirs, but did not manage to improve results.
6.4.8 Efficiency of Full Graphs vs. Subgraphs. Given the very close results of our algorithm
when using full graphs and subgraphs (cf. Section 6.3), we studied the efficiency of each.
We benchmarked several graph-based methods on the S2AW data set, which comprises
8 http://wordnet.princeton.edu/wordnet/man/senseidx.5WN.html.
78
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
2,473 instances to be disambiguated. All tests were done on a multicore computer with
16 GB of memory using a single 2.66 GHz processor. When using the full graph PPR
disambiguates full sentences in one go at 684 instances per minute, whereas PPRw2w
disambiguates one word at a time, 70 instances per minute. The DFS subgraphs provide
better performance than PPRw2w, 228 instances per minute when using degree, with
marginally slower performance when using PPRw2w (210 instances per minute). The
BFS subgraph is slowest, with around 20 instances per minute. The memory footprint of
using the full graph algorithm is small, just 270 MB, so several processes can be run on
a multiprocessor machine easily.
All in all, there is a tradeoff in performance and speed, with PPRw2w on the full
graph providing better results at the cost of some speed, and PPR on the full graph
providing the best speed at the cost of worse performance. Using DFS with PPRw2w
lays in between and is also a good alternative, and its speed can be improved using
pre-indexed paths.
6.5 Experiments on Spanish
Our WSD algorithm can be applied over non-English texts, provided that a LKB for this
particular language exists. We have applied our random walk algorithms to the Spanish
WordNet (Atserias, Rigau, and Villarejo 2004), using the SemEval-2007 Task 09 data set
as evaluation gold standard (Ma`rquez et al. 2007). The data set contains examples of the
150 most frequent nouns in the CESS-ECE corpus, manually annotated with Spanish
WordNet synsets. It is split into a train and test part, and has an ?all words? shape
(i.e., input consists of sentences, each one having at least one occurrence of a target
noun). We ran the experiment over the test part (792 instances), and used the train part
for calculating the MFS heuristic. The results in Table 11 are consistent with those for
English, with our algorithms approaching MFS performance, and PPRw2w yielding the
best results. Note that for this data set the supervised algorithm could barely improve
over the MFS, which performs very well, suggesting that in this particular data set the
sense distributions are highly skewed.
Finally, we also show results for the first sense in the Spanish WordNet. In the
Spanish WordNet the order of the senses of a word has been assigned directly by
the lexicographer (Atserias, Rigau, and Villarejo 2004), as there is no information
of sense frequency from hand-annotated corpora. This is in contrast to the English
WordNet, where the senses are ordered according to their frequency in annotated
Table 11
Results as F1 on the Spanish SemEval07 data set, including first sense, MFS, and the best
supervised system in the competition. ? Statistically significant difference with respect to
the best of our results (in bold).
Method Acc.
PPR 78.4?
PPRw2w 79.3
STATIC 76.5?
First sense 66.4?
MFS 84.6?
BEST 85.1?
79
Computational Linguistics Volume 40, Number 1
corpora (Fellbaum 1998), and reflects the status on most other wordnets. In this case,
our algorithm clearly improves over the first sense in the dictionary.
7. Conclusions
In this article we present a method for knowledge-based Word Sense Disambiguation
based on random walks over relations in a LKB. Our algorithm uses the full graph
of WordNet efficiently, and performs better than PageRank or degree on subgraphs
(Navigli and Lapata 2007; Agirre and Soroa 2008; Navigli and Lapata 2010; Ponzetto
and Navigli 2010). We also show that our combination of method and LKB built from
WordNet and eXtended WordNet compares favorably to other knowledge-based
systems using similar information sources (Mihalcea 2005; Sinha and Mihalcea
2007; Tsatsaronis, Vazirgiannis, and Androutsopoulos 2007; Tsatsaronis, Varlamis, and
N?rva?g 2010). Our analysis shows that Personalized PageRank yields similar results
when using subgraphs and the full graph, with a trade-off between speed and perfor-
mance, where Personalized PageRank over the full graph is fastest, its word-to-word
variant slowest, and Personalized PageRank over the subgraph lies in between.
We also show that the algorithm can be easily ported to other languages with good
results, with the only requirement of having a wordnet. Our results improve over the
first sense of the Spanish dictionary. This is particularly relevant for wordnets other
than English. For the English WordNet the senses of a word are ordered according to the
frequency of the senses in hand-annotated corpora, and thus the first sense is equivalent
to the Most Frequent Sense, but this information is not always available for languages
that lack large-scale hand-annotated corpora.
We have performed an extensive analysis, showing the behavior according to the
parameters of PageRank, and studying the impact of different relations and WordNet
versions. We have also analyzed the relation between our PPR algorithm, MFS, and
STATIC PageRank. In general domain corpora they get similar results, close to the
performance of the MFS learned from SemCor, but the results reported on domain-
specific data sets (Agirre, Lo?pez de Lacalle, and Soroa 2009) show that PPR is able to
move away from the MFS and STATIC and improve over them, indicating that PPR is able
to effectively use contextual information, and depart from MFS and STATIC PageRank.
The experiments in this study are readily reproducible, as the algorithm and the
LKBs are publicly available.9 The system can be applied easily to sense inventories and
knowledge bases different from WordNet.
In the future we would like to explore methods to incorporate global weights of the
edges in the random walk calculations (Tsatsaronis, Varlamis, and N?rva?g 2010). Given
the complementarity of the WordNet++ resource (Ponzetto and Navigli 2010) and our
algorithm, it would be very interesting to explore the combination of both, as well as
the contribution of other WordNet related resources (Cuadros and Rigau 2008).
Acknowledgments
We are deeply indebted to the reviewers,
who greatly helped to improve the
article. Our thanks to Rob Koeling and
Diana McCarthy for kindly providing
the data set, thesauri, and assistance,
and to Roberto Navigli and Simone
Ponzetto for clarifying the method to
map to coarse-grained senses. This
work has been partially funded by the
Education Ministry (project KNOW2
TIN2009-15049-C03-01).
9 http://ixa2.si.ehu.es/ukb.
80
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
References
Agirre, E., E. Alfonseca, K. Hall, J. Kravalova,
M. Pasca, and A. Soroa. 2009. A study
on similarity and relatedness using
distributional and WordNet-based
approaches. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics - Human Language
Technologies conference (NAACL/HLT?09),
pages 19?27, Boulder, CO.
Agirre, E., T. Baldwin, and D. Martinez.
2008. Improving parsing and PP
attachment performance with sense
information. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics (ACL/HLT?08),
pages 317?325, Columbus, OH.
Agirre, E., K. Bengoetxea, K. Gojenola,
and J. Nivre. 2011. Improving
dependency parsing with semantic
classes. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies
(ACL/HLT?11), pages 699?703,
Portland, OR.
Agirre, E., O. Lo?pez de Lacalle, and A. Soroa.
2009. Knowledge-based WSD on specific
domains: Performing better than generic
supervised WSD. In Proceedings of the
19th International Joint Conference on
Artificial Intelligence (IJCAI?09),
pages 1,501?1,506, Pasadena, CA.
Agirre, E. and G. Rigau. 1996. Word sense
disambiguation using conceptual density.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING?96), pages 16?22, Copenhagen.
Agirre, E. and A. Soroa. 2008. Using the
multilingual central repository for
graph-based word sense disambiguation.
In Proceedings of the 6th Conference on
Language Resources and Evaluation
(LREC ?08), pages 1,388?1,392, Marrakesh.
Agirre, E. and A. Soroa. 2009. Personalizing
PageRank for word sense disambiguation.
In Proceedings of the 12th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL?09),
pages 33?41, Athens.
Anaya-Sa?nchez, H., A. Pons-Porrata, and
R. Berlanga-Llavori. 2007. TKB-UO: Using
sense clustering for WSD. In Proceedings of
the 4th International Workshop on Semantic
Evaluations (SemEval-2007) in conjunction
with ACL, pages 322?325, Prague.
Atserias, J., G. Rigau, and L. Villarejo. 2004.
Spanish wordnet 1.6: Porting the Spanish
wordnet across Princeton versions.
In Proceedings of the 4th Conference on
Language Resources and Evaluation
(LREC?04), pages 161?164, Lisbon.
Barzilay, R. and M. Elhadad. 1997. Using
lexical chains for text summarization.
In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization,
pages 10?17, New York, NY.
Brin, S. and L. Page. 1998. The anatomy
of a large-scale hypertextual Web search
engine. Computer Networks and ISDN
Systems, 30(1-7):107?117.
Carpuat, M. and D. Wu. 2007. Improving
statistical machine translation using word
sense disambiguation. In Proceedings of
the Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP/CoNLL?07), pages 61?72, Prague.
Chan, Y. S. and H. T. Ng. 2005. Scaling
up word sense disambiguation via
parallel texts. In Proceedings of the
20th National Conference on Artificial
Intelligence (AAAI?05), pages 1,037?1,042,
Pittsburgh, PA.
Chan, Y. S., H. T. Ng, and D. Chiang.
2007. Word sense disambiguation
improves statistical machine translation.
In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics
(ACL?07), pages 33?40, Prague.
Chan, Y. S., H. T. Ng, and Z. Zhong. 2007.
NUS-PT: Exploiting parallel texts for
word sense disambiguation in the English
all-words tasks. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007) in conjunction
with ACL, pages 253?256, Prague.
Cowie, J., J. Guthrie, and L. Guthrie. 1992.
Lexical disambiguation using simulated
annealing. In Proceedings of the Workshop
on Speech and Natural Language (HLT?91),
pages 238?242, Morristown, NJ.
Cuadros, M. and G. Rigau. 2006. Quality
assessment of large scale knowledge
resources. In Proceedings of Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing (EMNLP?06),
pages 534?541, Sydney.
Cuadros, M. and G. Rigau. 2007.
Semeval-2007 task 16: Evaluation of
wide coverage knowledge resources.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007) in conjunction with ACL,
pages 81?86, Prague.
Cuadros, M. and G. Rigau. 2008. Knownet:
Using topic signatures acquired from the
Web for building automatically highly
dense knowledge bases. In Proceedings
81
Computational Linguistics Volume 40, Number 1
of the 22nd International Conference on
Computational Linguistics (COLING?08),
pages 71?84, Manchester.
Daude, J., L. Padro, and G. Rigau. 2000.
Mapping WordNets using structural
information. In Proceedings of the 38th
Annual Meeting of the Association for
Computational Linguistics (ACL?00),
pages 504?511, Hong Kong.
Decadt, B., V. Hoste, W. Daelemans, and
A. Van Den Bosch. 2004. GAMBL, genetic
algorithm optimization of memory-based
WSD. In Proceedings of SENSEVAL-3 Third
International Workshop on Evaluation of
Systems for the Semantic Analysis of Text,
pages 108?112, Barcelona.
Escudero, G., L. Ma?rquez, and G. Rigau.
2000. An empirical study of the domain
dependence of supervised word sense
disambiguation systems. Proceedings of
the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing
and Very Large Corpora (EMNLP/VLC?00),
pages 172?180, Hong Kong.
Fellbaum, C., editor. 1998. WordNet: An
Electronic Lexical Database and Some of Its
Applications. MIT Press, Cambridge, MA.
Haveliwala, T. H. 2002. Topic-sensitive
PageRank. In Proceedings of the
11th International Conference on World
Wide Web (WWW?02), pages 517?526,
New York, NY.
Hughes, T. and D. Ramage. 2007. Lexical
semantic relatedness with random graph
walks. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP/CoNLL?07),
pages 581?589, Prague.
Jiang, J. J. and D. W. Conrath. 1997. Semantic
similarity based on corpus statistics and
lexical taxonomy. In Proceedings of the
International Conference on Research in
Computational Linguistics, pages 19?33,
Taiwan.
Kleinberg., J. M. 1998. Authoritative
sources in a hyperlinked environment.
In Proceedings of the Ninth Annual
ACM-SIAM Symposium on Discrete
Algorithms (SODA?98), pages 668?677,
Philadelphia, PA.
Koeling, R., D. McCarthy, and J. Carroll.
2005. Domain-specific sense distributions
and predominant sense acquisition.
In Proceedings of the Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing (HLT/EMNLP?05),
pages 419?426, Ann Arbor, MI.
Langville, A. N. and C. D. Meyer. 2003.
Deeper inside PageRank. Internet
Mathematics, 1(3):335?380.
Lesk, M. 1986. Automatic sense
disambiguation using machine readable
dictionaries: How to tell a pine cone from
an ice cream cone. In Proceedings of the
5th Annual International Conference on
Systems Documentation (SIGDOC?86),
pages 24?26, New York, NY.
Ma`rquez, L., M. A. Villarejo, T. Mart??, and
M. Taule?. 2007. SemEval-2007 Task 09:
Multilevel semantic annotation of
Catalan and Spanish. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007) in conjunction
with ACL, pages 42?47, Prague.
McCarthy, D., R. Koeling, J. Weeds,
and J. Carroll. 2007. Unsupervised
acquisition of predominant word senses.
Computational Linguistics, 33(4):553?590.
Mihalcea, R. 2002. Word sense
disambiguation with pattern learning
and automatic feature selection. Natural
Language Engineering, 8:343?358.
Mihalcea, R. 2005. Unsupervised
Large-vocabulary word sense
disambiguation with graph-based
algorithms for sequence data labeling.
In Proceedings of the Conference on
Human Language Technology and
Empirical Methods in Natural Language
Processing (HLT?05), pages 411?418,
Morristown, NJ.
Mihalcea, R. and D. I. Moldovan. 2001.
eXtended WordNet: Progress report.
In Proceedings of the NAACL Workshop
on WordNet and Other Lexical Resources,
pages 95?100, Pittsburgh, PA.
Miller, G. A., C. Leacock, R. Tengi, and R.
Bunker. 1993. A semantic concordance.
In Proceedings of the Workshop on
Human Language Technology (HLT?93),
pages 303?308, Plainsboro, NJ.
Naskar, S. K. and S. Bandyopadhyay.
2007. JU-SKNSB: Extended WordNet
based WSD on the English all-words
task at SemEval-1. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007) in conjunction
with ACL, pages 203?206, Prague.
Navigli, R. 2008. A structural approach
to the automatic adjudication of word
sense disagreements. Natural Language
Engineering, 14(4):547?573.
Navigli, R. and M. Lapata. 2007. Graph
connectivity measures for unsupervised
word sense disambiguation. In Proceedings
of the 17th International Joint Conference
82
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
on Artificial Intelligence (IJCAI?07),
pages 1,683?1,688, Hyderabad.
Navigli, R. and M. Lapata. 2010.
An experimental study of graph
connectivity for unsupervised word
sense disambiguation. IEEE Transactions
on Pattern Analysis and Machine
Intelligence, 32(4):678?692.
Navigli, R., K. C. Litkowski, and
O. Hargraves. 2007. SemEval-2007 Task 07:
Coarse-grained English all-words task.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007) in conjunction with ACL,
pages 30?35, Prague.
Navigli, R. and P. Velardi. 2005. Structural
semantic interconnections: A knowledge-
based approach to word sense
disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
27(7):1,075?1,086.
Navigli, Roberto, Stefano Faralli, Aitor Soroa,
Oier Lo?pez de Lacalle, and Eneko Agirre.
2011. Two birds with one stone: Learning
semantic models for text categorization
and word sense disambiguation.
In Proceedings of CIKM, pages 2,317?2,320,
Glasgow.
Ng, H. T. and H. B. Lee. 1996. Integrating
multiple knowledge sources to
disambiguate word sense: An exemplar-
based approach. In Proceedings of the
34th Annual Meeting of the Association for
Computational Linguistics, ACL ?96,
pages 40?47, Stroudsburg, PA.
Ng, T. H. 1997. Getting serious about
word sense disambiguation.
In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical
Semantics: Why, What, and How?,
pages 1?7, Washington, DC.
Noreen, E. W. 1989. Computer-Intensive
Methods for Testing Hypotheses.
John Wiley & Sons.
Palmer, M., C. Fellbaum, S. Cotton, L. Delfs,
and H. T. Dang. 2001. English tasks:
All-words and verb lexical sample.
In Proceedings of SENSEVAL-2: Second
International Workshop on Evaluating
Word Sense Disambiguation Systems,
pages 21?24, Toulouse.
Patwardhan, S., S. Banerjee, and
T. Pedersen. 2003. Using measures of
semantic relatedness for word sense
disambiguation. In Proceedings of the
Fourth International Conference on
Intelligent Text Processing and
Computational Linguistics (CICLING-03),
pages 241?257, Mexico City.
Pe?rez-Agu?era, J. R. and H. Zaragoza. 2008.
UCM-Y!R at CLEF 2008 Robust and WSD
tasks. In Proceedings of the 9th Cross
Language Evaluation Forum Workshop
(CLEF?08), pages 138?145, Aarhus.
Ponzetto, S. P. and R. Navigli. 2010.
Knowledge-rich word sense
disambiguation rivaling supervised
system. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics (ACL?10), pages 1,522?1,531,
Uppsala.
Pradhan, S., E. Loper, D. Dligach, and
M. Palmer. 2007. SemEval-2007 Task-17:
English lexical sample SRL and all words.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007) in conjunction with ACL,
pages 87?92, Prague.
Sinha, R. and R. Mihalcea. 2007.
Unsupervised graph-based word sense
disambiguation using measures of word
semantic similarity. In Proceedings of the
IEEE International Conference on Semantic
Computing (ICSC 2007), pages 363?369,
Irvine, CA.
Snyder, B. and M. Palmer. 2004. The
English all-words task. In Proceedings of
SENSEVAL-3 Third International Workshop
on Evaluation of Systems for the Semantic
Analysis of Text, pages 41?43, Barcelona.
Strapparava, C., A. Gliozzo, and C. Giuliano.
2004. Pattern abstraction and term
similarity for word sense disambiguation:
IRST at SENSEVAL-3. In Proceedings of
SENSEVAL-3 Third International Workshop
on Evaluation of Systems for the Semantic
Analysis of Text, pages 229?234, Barcelona.
Surdeanu, M., M. Ciaramita, and
H. Zaragoza. 2008. Learning to rank
answers on large online QA collections.
In Proceedings of the 46th Annual Meeting
of the Association for Computational
Linguistics (ACL/HLT?08), pages 719?727,
Columbus, OH.
Sussna, M. 1993. Word sense disambiguation
for free-text indexing using a massive
semantic network. In Proceedings of the
Second International Conference on
Information and Knowledge Management
(CIKM?93), pages 67?74, New York, NY.
Tratz, S., A. Sanfilippo, M. Gregory,
A. Chappell, C. Posse, and P. Whitney.
2007. PNNL: A supervised maximum
entropy approach to word sense
disambiguation. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007) in conjunction
with ACL, pages 264?267, Prague.
83
Computational Linguistics Volume 40, Number 1
Tsatsaronis, G., I. Varlamis, and K. N?rva?g.
2010. An experimental study on
unsupervised graph-based word
sense disambiguation. In Proceedings
of the 11th International Conference on
Computational Linguistics and Intelligent
Text Processing (CICLing?10),
pages 184?198, Iasi.
Tsatsaronis, G., M. Vazirgiannis, and
I. Androutsopoulos. 2007. Word sense
disambiguation with spreading activation
networks generated from thesauri.
In Proceedings of the 17th International
Joint Conference on Artificial Intelligence
(IJCAI?07), pages 1,725?1,730, Hyderabad.
Zhong, Z. and H. T. Ng. 2010. It makes
sense: A wide-coverage word sense
disambiguation system for free text.
In Proceedings of the ACL 2010 System
Demonstrations, pages 78?83, Uppsala.
Zhong, Z. and H. T. Ng. 2012. Word sense
disambiguation improves information
retrieval. In Proceedings of the 50th Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers),
pages 273?282, Jeju Island.
84
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373?376,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Semantic Role Classification with Selectional Preferences
Ben?at Zapirain, Eneko Agirre
IXA NLP Group
Basque Country Univ.
{benat.zapirain,e.agirre}@ehu.es
Llu??s Ma`rquez
TALP Research Center
Technical Univ. of Catalonia
lluism@lsi.upc.edu
Mihai Surdeanu
Stanford NLP Group
Stanford Univ.
mihais@stanford.edu
Abstract
This work incorporates Selectional Prefer-
ences (SP) into a Semantic Role (SR) Clas-
sification system. We learn separate selec-
tional preferences for noun phrases and prepo-
sitional phrases and we integrate them in a
state-of-the-art SR classification system both
in the form of features and individual class
predictors. We show that the inclusion of the
refined SPs yields statistically significant im-
provements on both in domain and out of do-
main data (14.07% and 11.67% error reduc-
tion, respectively). The key factor for success
is the combination of several SP methods with
the original classification model using meta-
classification.
1 Introduction
Semantic Role Labeling (SRL) is the process of
extracting simple event structures, i.e., ?who? did
?what? to ?whom?, ?when? and ?where?. Current
systems usually perform SRL in two pipelined steps:
argument identification and argument classification.
While identification is mostly syntactic, classifica-
tion requires semantic knowledge to be taken into
account. Semantic information is usually captured
through lexicalized features on the predicate and the
head?word of the argument to be classified. Since
lexical features tend to be sparse, SRL systems are
prone to overfit the training data and generalize
poorly to new corpora.
Indeed, the SRL evaluation exercises at CoNLL-
2004 and 2005 (Carreras and Ma`rquez, 2005) ob-
served that all systems showed a significant perfor-
mance degradation (?10 F1 points) when applied to
test data from a different genre of that of the training
set. Pradhan et al (2008) showed that this perfor-
mance degradation is essentially caused by the argu-
ment classification subtask, and suggested the lexi-
cal data sparseness as one of the main reasons. The
same authors studied the contribution of the different
feature types in SRL and concluded that the lexical
features were the most salient features in argument
classification (Pradhan et al, 2007).
In recent work, we showed (Zapirain et al, 2009)
how automatically generated selectional preferences
(SP) for verbs were able to perform better than pure
lexical features in a role classification experiment,
disconnected from a full-fledged SRL system. SPs
introduce semantic generalizations on the type of ar-
guments preferred by the predicates and, thus, they
are expected to improve results on infrequent and
unknown words. The positive effect was especially
relevant for out-of-domain data. In this paper we ad-
vance (Zapirain et al, 2009) in two directions:
(1) We learn separate SPs for prepositions and verbs,
showing improvement over using SPs for verbs
alone.
(2) We integrate the information of several SP mod-
els in a state-of-the-art SRL system (SwiRL1) and
show significant improvements in SR classifica-
tion. The key for the improvement lies in a meta-
classifier, trained to select among the predictions
provided by several role classification models.
2 SPs for SR Classification
SPs have been widely believed to be an impor-
tant knowledge source when parsing and perform-
ing SRL, especially role classification. Still, present
parsers and SRL systems use just lexical features,
which can be seen as the most simple form of SP,
1http://www.surdeanu.name/mihai/swirl/
373
where the headword needs to be seen in the training
data, and otherwise the SP is not satisfied. Gildea
and Jurafsky (2002) showed barely significant im-
provements in semantic role classification of NPs
for FrameNet roles using distributional clusters. In
(Erk, 2007) a number of SP models are tested in
a pseudo-task related to SRL. More recently, we
showed (Zapirain et al, 2009) that several methods
to automatically generate SPs generalize well and
outperform lexical match in a large dataset for se-
mantic role classification, but the impact on a full
system was not explored.
In this work we apply a subset of the SP meth-
ods proposed in (Zapirain et al, 2009). These meth-
ods can be split in two main families, depending on
the resource used to compute similarity: WordNet-
based methods and distributional methods. Both
families define a similarity score between a word
(the headword of the argument to be classified) and a
set of words (the headwords of arguments of a given
role).
WordNet-based similarity: One of the models
that we used is based on Resnik?s similarity mea-
sure (1993), referring to it as res. The other model is
an in-house method (Zapirain et al, 2009), referred
as wn, which only takes into account the depth of
the most common ancestor, and returns SPs that are
as specific as possible.
Distributional similarity: Following (Zapirain et
al., 2009) we considered both first order and second
order similarity. In first order similarity, the simi-
larity of two words was computed using the cosine
(or Jaccard measure) of the co-occurrence vectors of
the two words. Co-occurrence vectors where con-
structed using freely available software (Pado? and
Lapata, 2007) run over the British National Corpus.
We used the optimal parameters (Pado? and Lapata,
2007, p. 179). We will refer to these similarities as
simcos and simJac, respectively. In contrast, sec-
ond order similarity uses vectors of similar words,
i.e., the similarity of two words was computed us-
ing the cosine (or Jaccard measure) between the
thesaurus entries of those words in Lin?s thesaurus
(Lin, 1998). We refer to these as sim2cos and sim
2
Jac.
Given a target sentence with a verb and its argu-
ments, the task of SR classification is to assign the
correct role to each of the arguments. When using
SPs alone, we only use the headwords of the ar-
guments, and each argument is classified indepen-
dently of the rest. For each headword, we select the
role (r) of the verb (c) which fits best the head word
(w), where the goodness of fit (SPsim(v, r, w)) is
modeled using one of the similarity models above,
between the headword w and the headwords seen in
training data for role r of verb v. This selection rule
is formalized as follows:
Rsim(v, w) = arg max
r?Roles(v)
SPsim(v, r, w) (1)
In our previous work (Zapirain et al, 2009), we
modelled SPs for pairs of predicates (verbs) and ar-
guments, independently of the fact that the argu-
ment is a core argument (typically a noun) or an
adjunct argument (typically a prepositional phrase).
In contrast, (Litkowski and Hargraves, 2005) show
that prepositions have SPs of their own, especially
when functioning as adjuncts. We therefore decided
to split SPs according to whether the potential argu-
ment is a Prepositional Phrase (PP) or a Noun Phrase
(NP). For NPs, which tend to be core arguments2,
we use the SPs of the verb (as formalized above).
For PPs, which have an even distribution between
core and adjunct arguments, we use the SPs of the
prepositions alone, ignoring the verbs. Implementa-
tion wise, this means that in Eq. (1), we change v
for p, where p is the preposition heading the PP.
3 Experiments with SPs in isolation
In this section we evaluate the use of SPs for classi-
fication in isolation, i.e., we use formula 1, and no
other information. In addition we contrast the use
of both verb-role and preposition-role SPs, as com-
pared to the use of verb-role SPs alone.
The dataset used in these experiments (and in Sec-
tion 4) is the same as provided by the CoNLL-2005
shared task on SRL (Carreras and Ma`rquez, 2005).
This dataset comprises several sections of the Prop-
Bank corpus (news from the WSJ) as well as an ex-
tract of the Brown Corpus. Sections 02-21 are used
for generating the SPs and training, Section 00 for
development, and Section 23 for testing, as custom-
ary. The Brown Corpus is used for out-of-domain
testing, but due to the limited size of the provided
section, we extended it with instances from Sem-
Link3. Since the focus of this work is on argument
2In our training data, NPs are adjuncts only 5% of the times
3http://verbs.colorado.edu/semlink/
374
Verb-Role SPs Preposition-Role and Verb-Role SPs
WSJ-test Brown WSJ-test Brown
prec. rec. F1 prec. rec. F1 prec. rec. F1 prec. rec. F1
lexical 70.75 26.66 39.43 59.39 05.51 10.08 82.98 43.77 57.31 68.47 13.60 22.69
SPres 45.07 37.11 40.71 36.34 27.58 31.33 63.47 53.24 57.91 55.12 44.15 49.03
SPwn 55.44 45.58 50.03 41.76 31.58 35.96 65.70 63.88 64.78 60.08 48.10 53.43
SPsimJac 48.85 46.38 47.58 42.10 34.34 37.82 61.83 61.40 61.61 55.42 53.45 54.42
SPsimcos 53.13 50.44 51.75 43.24 35.27 38.85 64.67 64.22 64.44 56.56 54.54 55.53
SPsim2
Jac
61.76 58.63 60.16 51.97 42.39 46.69 70.82 70.33 70.57 62.37 60.15 61.24
SPsim2cos 61.12 58.12 59.63 51.92 42.35 46.65 70.28 69.80 70.04 62.36 60.14 61.23
Table 1: Results for SPs in isolation, left for verb SPs, and right both preposition and verb SPs.
Labels proposed by the base models
Number of base models that proposed this datum?s label
List of actual base models that proposed this datum?s label
Table 2: Features of the binary meta-classifier.
classification, we use the gold PropBank data to
identify argument boundaries. Considering that SPs
can handle only nominal arguments, in these exper-
iments we used only arguments mapped to NPs and
PPs containing a nominal head. From the training
sections, we extracted over 140K such arguments for
the supervised generation of SPs. The development
and test sections contain over 5K and 8K examples,
respectively, and the portion of the Brown Corpus
comprises an amount of 8.1K examples.
Table 1 lists the results of the different SPs in iso-
lation. The results reported in the left part of Table
1 are comparable to those we reported in (Zapirain
et al, 2009). The differences are due to the fact that
we do not discard roles like MOD, DIS, NEG and
that our previous work used only the subset of the
data that could be mapped to VerbNet (around 50%).
All in all, the table shows that splitting SPs into verb
and preposition SPs yields better results, both in pre-
cision and recall, improving F1 up to 10 points in
some cases.
4 Integrating SPs in a SRL system
For these experiments we modified SwiRL (Sur-
deanu et al, 2007): (a) we matched the gold bound-
aries against syntactic constituents predicted inter-
nally using the Charniak parser (Charniak, 2000);
and (b) we classified these constituents with their
semantic role using a modified version of SwiRL?s
feature set.
We explored two different strategies for integrat-
ing SPs in SwiRL. The first, obvious method is to
extend SwiRL?s feature set with features that model
the preferences of the SPs, i.e., for each SP model
SPi we add a feature whose value is Ri. The second
method combines SwiRL?s classification model and
our SP models using meta-classification. We opted
for a binary classification approach: first, for each
constituent we generate n datums, one for each dis-
tinct role label proposed by the pool of base models;
then we use a binary meta-classifier to label each
candidate role as correct or incorrect. Table 2 lists
the features of the meta-classifier. We trained the
meta-classifier on the usual PropBank training par-
tition, using cross-validation to generate outputs for
the base models that require the same training ma-
terial. At prediction time, for each candidate con-
stituent we selected the role label that was classified
as correct with the highest confidence.
Table 3 compares the performance of both
combination approaches against the standalone
SwiRL classifier. We show results for both core
arguments (Core), adjunct arguments (Arg) and
all arguments combined (All). In the table, the
SwiRL+SP? models stand for SwiRL classifiers
enhanced with one feature from the correspond-
ing SP. Adding more than one SP-based feature to
SwiRL did not improve results. Our conjecture
is that the SwiRL classifier enhanced with SP-
based features does not learn relevant weights for
these features because their signal is ?drowned? by
SwiRL?s large initial feature set and the correlation
between the different SPs. This observation moti-
vated the development of the meta-classifier. The
meta-classifier shown in the table combines the out-
put of the SwiRL+SP? models with the predictions
of SP models used in isolation. We implemented
the meta-classifier using Support Vector Machines
(SVM)4 with a quadratic polynomial kernel, and
4http://svmlight.joachims.org
375
WSJ-test Brown
Core Adj All Core Adj All
SwiRL 93.25 81.31 90.83 84.42 57.76 79.52
+SPRes 93.17 81.08 90.76 84.52 59.24 79.86
+SPwn 92.88 81.11 90.56 84.26 59.69 79.73
+SPsimJac 93.37 80.30 90.86 84.43 59.54 79.83
+SPsimcos 93.33 80.92 90.87 85.14 60.16 80.50
+SPsim2
Jac
93.03 82.75 90.95 85.62 59.63 80.75
+SPsim2cos 93.78 80.56 91.23 84.95 61.01 80.48
Meta 94.37 83.40 92.12 86.20 63.40 81.91
Table 3: Classification accuracy for the combination ap-
proaches. +SPx stands for SwiRL plus each SP model.
C = 0.01 (tuned in development).
Table 3 indicates that four out of the six
SwiRL+SP? models perform better than SwiRL in
domain (WSJ-test), and all of them outperform
SwiRL out of domain (Brown). However, the im-
provements are small and, generally, not statistically
significant. On the other hand, the meta-classifier
outperforms SwiRL both in domain (14.07% error
reduction) and out of domain (11.67% error reduc-
tion), and the differences are statistically signifi-
cant (measured using two-tailed paired t-test at 99%
confidence interval on 100 samples generated us-
ing bootstrap resampling). We also implemented
two unsupervised voting baselines, one unweighted
(each base model has the same weight) and one
weighted (each base model is weighted by its accu-
racy in development). However, none of these base-
lines outperformed the standalone SwiRL classifier.
This is further proof that, for SR classification, meta-
classification is crucial because it can learn the dis-
tinct specializations of the various base models.
Finally, Table 3 shows that our approach yields
consistent improvements for both core and adjunct
arguments. Out of domain, we see a bigger accuracy
improvement for adjunct arguments (5.64 absolute
points) vs. core arguments (1.78 points). This is
to be expected, as most core arguments fall under
the Arg0 and Arg1 classes, which can typically be
disambiguated based on syntactic information, i.e.,
subject vs. object. On the other hand, there are no
syntactic hints for adjunct arguments, so the system
learns to rely more on SP information in this case.
5 Conclusions
This paper is the first work to show that SPs improve
a state-of-the-art SR classification system. Sev-
eral decisions were crucial for success: (a) we de-
ployed separate SP models for verbs and preposi-
tions, which in conjunction outperform SP models
for verbs alone; (b) we incorporated SPs into SR
classification using a meta-classification approach
that combines eight base models, developed from
variants of a state-of-the-art SRL system and the
above SP models. We show that the resulting system
outperforms the original SR classification system for
arguments mapped to nominal or prepositional con-
stituents. The improvements are statistically sig-
nificant both on in-domain and out-of-domain data
sets.
Acknowledgments
This work was partially supported by projects KNOW-
2 (TIN2009-14715-C04-01 / 04), KYOTO (ICT-2007-
211423) and OpenMT-2 (TIN2009-14675C03)
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic role labeling. In
Proc. of CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
K. Erk. 2007. A simple, similarity-based model for se-
lectional preferences. In Proc. of ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING-ACL.
K. Litkowski and O. Hargraves. 2005. The preposi-
tion project. In Proceedings of the Workshop on The
Linguistic Dimensions of Prepositions and their Use
in Computational Linguistic Formalisms and Applica-
tions.
S. Pado? and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2).
S. Pradhan, W. Ward, and J. Martin. 2007. Towards ro-
bust semantic role labeling. In Proc. of NAACL-HLT.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards ro-
bust semantic role labeling. Computational Linguis-
tics, 34(2).
P. Resnik. 1993. Semantic classes and syntactic ambigu-
ity. In Proc. of HLT.
M. Surdeanu, L. Ma`rquez, X. Carreras, and P.R. Comas.
2007. Combination strategies for semantic role label-
ing. Journal of Artificial Intelligence Research, 29.
B. Zapirain, E. Agirre, and L. Ma`rquez. 2009. General-
izing over lexical features: Selectional preferences for
semantic role classification. In Proc. of ACL-IJCNLP.
376
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 699?703,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Improving Dependency Parsing with Semantic Classes 
Eneko Agirre*, Kepa Bengoetxea*, Koldo Gojenola*, Joakim Nivre+ 
* Department of Computer Languages and Systems, University of the Basque Country 
UPV/EHU 
+ Department of Linguistics and Philosophy, Uppsala University 
{e.agirre, kepa.bengoetxea, koldo.gojenola}@ehu.es joakim.nivre@lingfil.uu.se 
 
 
 
 
Abstract  
This paper presents the introduction of 
WordNet semantic classes in a dependency 
parser, obtaining improvements on the full 
Penn Treebank for the first time. We tried 
different combinations of some basic se-
mantic classes and word sense disambigua-
tion algorithms. Our experiments show that 
selecting the adequate combination of se-
mantic features on development data is key 
for success. Given the basic nature of the 
semantic classes and word sense disam-
biguation algorithms used, we think there is 
ample room for future improvements. 
1 Introduction 
Using semantic information to improve parsing 
performance has been an interesting research ave-
nue since the early days of NLP, and several re-
search works have tried to test the intuition that 
semantics should help parsing, as can be exempli-
fied by the classical PP attachment experiments 
(Ratnaparkhi, 1994). Although there have been 
some significant results (see Section 2), this issue 
continues to be elusive. In principle, dependency 
parsing offers good prospects for experimenting 
with word-to-word-semantic relationships. 
We present a set of experiments using semantic 
classes in dependency parsing of the Penn Tree-
bank (PTB). We extend the tests made in Agirre et 
al. (2008), who used different types of semantic 
information, obtaining significant improvements in 
two constituency parsers, showing how semantic 
information helps in constituency parsing.  
As our baseline parser, we use MaltParser 
(Nivre, 2006). We will evaluate the parser on both 
the full PTB (Marcus et al 1993) and on a sense-
annotated subset of the Brown Corpus portion of 
PTB, in order to investigate the upper bound per-
formance of the models given gold-standard sense 
information, as in Agirre et al (2008). 
2 Related Work 
Agirre et al (2008) trained two state-of-the-art sta-
tistical parsers (Charniak, 2000; Bikel, 2004) on 
semantically-enriched input, where content words 
had been substituted with their semantic classes. 
This was done trying to overcome the limitations 
of lexicalized approaches to parsing (Magerman, 
1995; Collins, 1996; Charniak, 1997; Collins, 
2003), where related words, like scissors and knife 
cannot be generalized. This simple method allowed 
incorporating lexical semantic information into the 
parser. They tested the parsers in both a full pars-
ing and a PP attachment context. The experiments 
showed that semantic classes gave significant im-
provement relative to the baseline, demonstrating 
that a simplistic approach to incorporating lexical 
semantics into a parser significantly improves its 
performance. This work presented the first results 
over both WordNet and the Penn Treebank to show 
that semantic processing helps parsing.  
Collins (2000) tested a combined parsing/word 
sense disambiguation model based in WordNet 
which did not obtain improvements in parsing. 
Koo et al (2008) presented a semisupervised 
method for training dependency parsers, using 
word clusters derived from a large unannotated 
corpus as features. They demonstrate the effective-
ness of the approach in a series of dependency 
parsing experiments on PTB and the Prague De-
pendency Treebank, showing that the cluster-based 
features yield substantial gains in performance 
across a wide range of conditions. Suzuki et al 
(2009) also experiment with the same method 
combined with semi-supervised learning. 
699
Ciaramita and Attardi (2007) show that adding 
semantic features extracted by a named entity tag-
ger (such as PERSON or MONEY) improves the 
accuracy of a dependency parser, yielding a 5.8% 
relative error reduction on the full PTB. 
Candito and Seddah (2010) performed experi-
ments in statistical parsing of French, where termi-
nal forms were replaced by more general symbols, 
particularly clusters of words obtained through 
unsupervised clustering. The results showed that 
word clusters had a positive effect. 
Regarding dependency parsing of the English 
PTB, currently Koo and Collins (2010) and Zhang 
and Nivre (2011) hold the best results, with 93.0 
and 92.9 unlabeled attachment score, respectively. 
Both works used the Penn2Malt constituency-to-
dependency converter, while we will make use of 
PennConverter (Johansson and Nugues, 2007). 
Apart from these, there have been other attempts 
to make use of semantic information in different 
frameworks and languages, as in (Hektoen 1997; 
Xiong et al 2005; Fujita et al 2007). 
3 Experimental Framework 
In this section we will briefly describe the data-
driven parser used for the experiments (subsection 
3.1), followed by the PTB-based datasets (subsec-
tion 3.2). Finally, we will describe the types of se-
mantic representation used in the experiments. 
3.1 MaltParser 
MaltParser (Nivre et al 2006) is a trainable de-
pendency parser that has been successfully applied 
to typologically different languages and treebanks. 
We will use one of its standard versions (version 
1.4). The parser obtains deterministically a de-
pendency tree in linear-time in a single pass over 
the input using two main data structures: a stack of 
partially analyzed items and the remaining input 
sequence. To determine the best action at each 
step, the parser uses history-based feature models 
and SVM classifiers. One of the main reasons for 
using MaltParser for our experiments is that it eas-
ily allows the introduction of semantic informa-
tion, adding new features, and incorporating them 
in the training model. 
3.2 Dataset 
We used two different datasets: the full PTB and 
the Semcor/PTB intersection (Agirre et al 2008). 
The full PTB allows for comparison with the state-
of-the-art, and we followed the usual train-test 
split. The Semcor/PTB intersection contains both 
gold-standard sense and parse tree annotations, and 
allows to set an upper bound of the relative impact 
of a given semantic representation on parsing. We 
use the same train-test split of Agirre et al (2008), 
with a total of 8,669 sentences containing 151,928 
words partitioned into 3 sets: 80% training, 10% 
development and 10% test data. This dataset is 
available on request to the research community. 
We will evaluate the parser via Labeled Attach-
ment Score (LAS). We will use Bikel?s random-
ized parsing evaluation comparator to test the 
statistical significance of the results using word 
sense information, relative to the respective base-
line parser using only standard features.  
We used PennConverter (Johansson and 
Nugues, 2007) to convert constituent trees in the 
Penn Treebank annotation style into dependency 
trees. Although in general the results from parsing 
Pennconverter?s output are lower than with other 
conversions, Johansson and Nugues (2007) claim 
that this conversion is better suited for semantic 
processing, with a richer structure and a more fine-
grained set of dependency labels. For the experi-
ments, we used the best configuration for English 
at the CoNLL 2007 Shared Task on Dependency 
Parsing (Nivre et al, 2007) as our baseline.  
3.3 Semantic representation and disambigua-
tion methods 
We will experiment with the range of semantic 
representations used in Agirre et al (2008), all of 
which are based on WordNet 2.1. Words in Word-
Net (Fellbaum, 1998) are organized into sets of 
synonyms, called synsets (SS). Each synset in turn 
belongs to a unique semantic file (SF). There are a 
total of 45 SFs (1 for adverbs, 3 for adjectives, 15 
for verbs, and 26 for nouns), based on syntactic 
and semantic categories. For example, noun se-
mantic files (SF_N) differentiate nouns denoting 
acts or actions, and nouns denoting animals, 
among others. We experiment with both full syn-
sets and SFs as instances of fine-grained and 
coarse-grained semantic representation, respec-
tively. As an example of the difference in these 
two representations, knife in its tool sense is in the 
EDGE TOOL USED AS A CUTTING 
INSTRUMENT singleton synset, and also in the 
ARTIFACT SF along with thousands of other 
700
words including cutter. Note that these are the two 
extremes of semantic granularity in WordNet. 
As a hybrid representation, we also tested the ef-
fect of merging words with their corresponding SF 
(e.g. knife+ARTIFACT). This is a form of seman-
tic specialization rather than generalization, and 
allows the parser to discriminate between the dif-
ferent senses of each word, but not generalize 
across words. For each of these three semantic rep-
resentations, we experimented with using each of: 
(1) all open-class POSs (nouns, verbs, adjectives 
and adverbs), (2) nouns only, and (3) verbs only. 
There are thus a total of 9 combinations of repre-
sentation type and target POS: SS (synset), SS_N 
(noun synsets), SS_V (verb synsets), SF (semantic 
file), SF_N (noun semantic files), SF_V (verb se-
mantic files), WSF (wordform+SF), WSF_N 
(wordform+SF for nouns) and WSF_V (for verbs).  
For a given semantic representation, we need 
some form of WSD to determine the semantics of 
each token occurrence of a target word. We ex-
perimented with three options: a) gold-standard 
(GOLD) annotations from SemCor, which gives 
the upper bound performance of the semantic rep-
resentation, b) first Sense (1ST), where all token 
instances of a given word are tagged with their 
most frequent sense in WordNet, and c) automatic 
Sense Ranking (ASR) which uses the sense re-
turned by an unsupervised system based on an in-
dependent corpus (McCarthy et al 2004). For the 
full Penn Treebank experiments, we only had ac-
cess to the first sense, taken from Wordnet 1.7. 
4 Results 
In the following two subsections, we will first pre-
sent the results in the SemCor/PTB intersection, 
with the option of using gold, 1st sense and auto-
matic sense information (subsection 4.1) and the 
next subsection (4.2) will show the results on the 
full PTB, using 1st sense information. All results 
are shown as labelled attachment score (LAS). 
4.1 Semcor/PTB (GOLD/1ST/ASR) 
We conducted a series of experiments testing: 
? Each individual semantic feature, which 
gives 9 possibilities, also testing different 
learning configurations for each one. 
? Combinations of semantic features, for in-
stance, SF+SS_N+WSF would combine the 
semantic file with noun synsets and word-
form+semantic file. 
Although there were hundreds of combinations, 
we took the best combination of semantic features 
on the development set for the final test. For that 
reason, the table only presents 10 results for each 
disambiguation method, 9 for the individual fea-
tures and one for the best combination. 
Table 1 presents the results obtained for each of 
the disambiguation methods (gold standard sense 
information, 1st sense, and automatic sense rank-
ing) and individual semantic feature. In all cases 
except two, the use of semantic classes is benefi-
 System            LAS 
Baseline  81.10  
SS 81.18 +0.08 
SS_N 81.40 +0.30 
SS_V *81.58 +0.48 
SF **82.05 +0.95 
SF_N
 81.51 +0.41 
SF_V 81.51 +0.41 
WSF 81.51 +0.41 
WSF_N 81.43 +0.33 
WSF_V *81.51 +0.41 
 
 
Gold 
SF+SF_N+SF_V+SS+WSF_N *81.74 +0.64 
SS 81.30 +0.20 
SS_N *81.56 +0.46 
SS_V *81.49 +0.39 
SF 81.00 -0.10 
SF_N
 80.97 -0.13 
SF_V **81.66 +0.56 
WSF 81.32 +0.22 
WSF_N *81.62 +0.52 
WSF_V **81.72 +0.62 
 
 
ASR 
SF_V+SS_V 81.41 +0.31 
SS 81.40 +0.30 
SS_N 81.39 +0.29 
SS_V *81.48 +0.38 
SF *81.59 +0.49 
SF_N
 81.38 +0.28 
SF_V *81.52 +0.42 
WSF *81.57 +0.46 
WSF_N 81.40 +0.30 
WSF_V 81.42 +0.32 
 
 
1ST 
SF+SS_V+WSF_N **81.92 +0.81 
Table 1. Evaluation results on the test set for the 
Semcor-Penn intersection. Individual semantic 
features and best combination. 
(**: statistically significant, p < 0.005; *: p < 0.05) 
 
701
cial albeit small. Regarding individual features, the 
SF feature using GOLD senses gives the best im-
provement. However, GOLD does not seem to 
clearly improve over 1ST and ASR on the rest of 
the features. Comparing the automatically obtained 
classes, 1ST and ASR, there is no evident clue 
about one of them being superior to the other. 
Regarding the best combination as selected in 
the training data, each WSD method yields a dif-
ferent combination, with best results for 1ST. The 
improvement is statistically significant for both 
1ST and GOLD. In general, the results in Table 1 
do not show any winning feature across all WSD 
algorithms. The best results are obtained when us-
ing the first sense heuristic, but the difference is 
not statistically significant. This shows that perfect 
WSD is not needed to obtain improvements, but it 
also shows that we reached the upperbound of our 
generalization and learning method. 
4.2 Penn Treebank and 1st sense 
We only had 1st sense information available for 
the full PTB. We tested MaltParser on the best 
configuration obtained for the reduced Sem-
cor/PTB on the full treebank, taking sections 2-21 
for training and section 23 for the final test. Table 
2 presents the results, showing that several of the 
individual features and the best combination give 
significant improvements. To our knowledge, this 
is the first time that WordNet semantic classes help 
to obtain improvements on the full Penn Treebank. 
It is interesting to mention that, although not 
shown on the tables, using lemmatization to assign 
semantic classes to wordforms gave a slight in-
crease for all the tests (0.1 absolute point approxi-
mately), as it helped to avoid data sparseness. We 
applied Schmid?s (1994) TreeTagger. This can be 
seen as an argument in favour of performing mor-
phological analysis, an aspect that is many times 
neglected when processing morphologically poor 
languages as English. 
We also did some preliminary experiments us-
ing Koo et al?s (2008) word clusters, both inde-
pendently and also combined with the WordNet-
based features, without noticeable improvements. 
5 Conclusions 
We tested the inclusion of several types of seman-
tic information, in the form of WordNet semantic 
classes in a dependency parser, showing that: 
? Semantic information gives an improvement 
on a transition-based deterministic depend-
ency parsing. 
? Feature combinations give an improvement 
over using a single feature. Agirre et al 
(2008) used a simple method of substituting 
wordforms with semantic information, 
which only allowed using a single semantic 
feature. MaltParser allows the combination 
of several semantic features together with 
other features such as wordform, lemma or 
part of speech. Although tables 1 and 2 only 
show the best combination for each type of 
semantic information, this can be appreci-
ated on GOLD and 1ST in Table 1. Due to 
space reasons, we only have showed the best 
combination, but we can say that in general 
combining features gives significant in-
creases over using a single semantic feature. 
? The present work presents a statistically sig-
nificant improvement for the full treebank 
using WordNet-based semantic information 
for the first time. Our results extend those of 
Agirre et al (2008), which showed im-
provements on a subset of the PTB. 
Given the basic nature of the semantic classes 
and WSD algorithms, we think there is room for 
future improvements, incorporating new kinds of 
semantic information, such as WordNet base con-
cepts, Wikipedia concepts, or similarity measures. 
 
 System            LAS 
Baseline  86.27  
SS *86.53 +0.26 
SS_N 86.33 +0.06 
SS_V *86.48 +0.21 
SF **86.63 +0.36 
SF_N
 *86.56 +0.29 
SF_V 86.34 +0.07 
WSF *86.50 +0.23 
WSF_N 86.25 -0.02 
WSF_V *86.51 +0.24 
 
 
1ST 
SF+SS_V+WSF_N *86.60 +0.33 
 
Table 1. Evaluation results (LAS) on the test 
set for the full PTB. Individual features and 
best combination. 
(**: statistically, p < 0.005; *: p < 0.05) 
 
702
References  
Eneko Agirre, Timothy Baldwin, and David Martinez. 
2008. Improving parsing and PP attachment perform-
ance with sense information. In Proceedings of ACL-
08: HLT, pages 317?325, Columbus, Ohio. 
Daniel M. Bikel. 2004. Intricacies of Collins? parsing 
model. Computational Linguistics, 30(4):479?511. 
Candito, M. and D. Seddah. 2010. Parsing word clus-
ters. In Proceedings of the NAACL HLT 2010 First 
Workshop on Statistical Parsing of Morphologically-
Rich Language, Los Angeles, USA. 
M. Ciaramita and G. Attardi. 2007. Dependency Parsing 
with Second-Order Feature Maps and Annotated Se-
mantic Information, In Proceedings of the 10th In-
ternational Conference on Parsing Technology.  
Eugene Charniak. 1997. Statistical parsing with a con-
text-free grammar and word statistics. In Proc. of the 
15th Annual Conference on Artificial Intelligence 
(AAAI-97), pages 598?603, Stanford, USA. 
Eugene Charniak. 2000. A maximum entropy-based 
parser. In Proc. of the 1st Annual Meeting of the 
North American Chapter of Association for Compu-
tational Linguistics (NAACL2000), Seattle, USA. 
Michael J. Collins. 1996. A new statistical parser based 
on lexical dependencies. In Proc. of the 34th Annual 
Meeting of the ACL, pages 184?91, USA. 
Michael Collins. 2000. A Statistical Model for Parsing 
and Word-Sense Disambiguation. Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora. 
Michael Collins. 2003. Head-driven statistical models 
for natural language parsing. Computational Linguis-
tics, 29(4):589?637. 
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge. 
Sanae Fujita, Francis Bond, Stephan Oepen, and Taka-
aki Tanaka. 2007. Exploiting semantic information 
for HPSG parse selection. In Proc. of the ACL 2007 
Workshop on Deep Linguistic Processing. 
Richard Johansson and Pierre Nugues. 2007. Extended 
Constituent-to-dependency Conversion for English. 
In Proceedings of NODALIDA 2007, Tartu, Estonia. 
Erik Hektoen. 1997. Probabilistic parse selection based 
on semantic cooccurrences. In Proc. of the 5th Inter-
national Workshop on Parsing Technologies. 
Terry Koo, Xavier Carreras, and Michael Collins. 2008. 
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08, pages 595?603, USA. 
Terry Koo, and Michael Collins. 2008. Efficient Third-
order Dependency Parsers. In Proceedings of ACL-
2010, pages 1?11, Uppsala, Sweden. 
Shari Landes, Claudia Leacock, and Randee I. Tengi. 
1998. Building semantic concordances. In Christiane 
Fellbaum, editor, WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, USA. 
David M. Magerman. 1995. Statistical decision-tree 
models for parsing. In Proc. of the 33rd Annual 
Meeting of the ACL, pages 276?83, USA. 
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn treebank. Computational 
Linguistics, 19(2):313?30. 
Diana McCarthy, Rob Koeling, Julie Weeds, and John 
Carroll. 2004. Finding predominant senses in 
untagged text. In Proc. of the 42nd Annual Meeting 
of the ACL, pages 280?7, Barcelona, Spain. 
Joakim Nivre. 2006. Inductive Dependency Parsing. 
Text, Speech and Language Technology series, 
Springer. 2006, XI, ISBN: 978-1-4020-4888-3. 
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson,  Sebastian Riedel and 
Deniz Yuret. 2007b. The CoNLL 2007 Shared Task 
on Dependency Parsing. Proceedings of EMNLP-
CoNLL. Prague, Czech Republic. 
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 
1994. A maximum entropy model for prepositional 
phrase attachment. In HLT ?94: Proceedings of the 
Workshop on Human Language Technology, USA. 
Helmut Schmid. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing. September 1994 
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Mi-
chael Collins. 2009. An Empirical Study of Semi-
supervised Structured Conditional Models for De-
pendency Parsing. In Proceedings of EMNLP, pages 
551?560. Association for Computational Linguistics. 
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and 
Yueliang Qian. 2005. Parsing the Penn Chinese 
Treebank with semantic knowledge. In Proc. of the 
2nd International Joint Conference on Natural Lan-
guage Processing (IJCNLP-05), Korea. 
Yue Zhang, and Joakim Nivre. 2011. Transition-Based 
Parsing with Rich Non-Local Features. In Proceed-
ings of the 49th Annual Meeting of the Association 
for Computational Linguistics. 
703
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 151?156,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PATHS: A System for Accessing Cultural Heritage Collections
Eneko Agirre?, Nikolaos Aletras?, Paul Clough?, Samuel Fernando?,
Paula Goodale?, Mark Hall?, Aitor Soroa? and Mark Stevenson?
(?) IXA NLP Group, University of the Basque Country
Manuel Lardizabal, 1, 20.018 Donostia, Basque Country
(?) Department of Computer Science, Sheffield University
211 Portobello, Sheffield S1 4DP, United Kingdom
Abstract
This paper describes a system for navigat-
ing large collections of information about
cultural heritage which is applied to Eu-
ropeana, the European Library. Euro-
peana contains over 20 million artefacts
with meta-data in a wide range of Euro-
pean languages. The system currently pro-
vides access to Europeana content with
meta-data in English and Spanish. The pa-
per describes how Natural Language Pro-
cessing is used to enrich and organise this
meta-data to assist navigation through Eu-
ropeana and shows how this information is
used within the system.
1 Introduction
Significant amounts of information about cultural
heritage has been digitised in recent years and is
now easily available through online portals. How-
ever, this vast amount of material can also be over-
whelming for many users since they are provided
with little or no guidance on how to find and inter-
pret this information. Potentially useful and rel-
evant content is hidden from the users who are
typically offered simple keyword-based searching
functionality as the entry point into a cultural her-
itage collection. The situation is very different
within traditional mechanisms for viewing cultural
heritage (e.g. museums) where artefacts are or-
ganised thematically and users guided through the
collection.
This paper describes a system that allows users
to explore large cultural heritage collections. Nav-
igation is based around the metaphor of pathways
(or trails) through the collection, an approach that
has been widely explored as an alternative to stan-
dard keyword-based search (Furuta et al, 1997;
Reich et al, 1999; Shipman et al, 2000; White and
Huang, 2010). Pathways are sets of artefacts or-
ganised around a theme which form access points
to the collection.
Pathways are a useful way to access informa-
tion about cultural heritage. Users accessing these
collections are often unfamiliar with their content,
making keyword-based search unsuitable since
they are unable to formulate appropriate queries
(Wilson et al, 2010). Non-keyword-based search
interfaces have been shown to be suitable for ex-
ploratory search (Marchionini, 2006). Pathways
support this exploration by echoing the organised
galleries and guided tours found in museums.
2 Related Work
Heitzman et al (1997) describe the ILEX system
which acts as a guide through the jewellery col-
lection of the National Museum of Scotland. The
user explores the collection through a set of web
pages which provide descriptions of each artefact
that are personalised for each user. The system
makes use of information about the artefacts the
user has viewed to build up a model of their in-
terests and uses this to customise the descriptions
of each artefact and provide recommendations for
further artefacts in which they may be interested.
Grieser et al (2007) also explore providing rec-
ommendations based on the artefacts a user has
viewed so far. They make use of a range of tech-
niques including language modelling, geospatial
modelling and analysis of previous visitors? be-
haviour to provide recommendations to visitors to
the Melbourne Museum.
Grieser et al (2011) explore methods for de-
termining the similarity between museum arte-
facts, commenting that this is useful for navigation
through these collections and important for per-
sonalisation (Bowen and Filippini-Fantoni, 2004;
O?Donnell et al, 2001), recommendation (Bohn-
ert et al, 2009; Trant, 2009) and automatic tour
generation (Finkelstein et al, 2002; Roes et al,
2009). They also use exhibits from Melbourne
151
Museum and apply a range of approaches to deter-
mine the similarity between them, including com-
paring descriptions and measuring physical dis-
tance between them in the museum.
These approaches, like many of the systems
that have been developed for online access to cul-
tural heritage (e.g. (Hage et al, 2010)), are based
around virtual access to a concrete physical space
(i.e. a museum). They often provide tours which
are constrained by the physical layout of the mu-
seum, such as virtual museum visits. However,
these approaches are less suitable for unstructured
collections such as databases of cultural heritage
artefacts collected from multiple institutions or
artefacts not connected with existing physical pre-
sentation (e.g. in a museum). The PATHS sys-
tem is designed for these types of collections and
makes use of natural language analysis to sup-
port navigation. In particular, similarity between
artefacts is computed automatically (see Section
4.1), background information automatically added
to artefact descriptions (see Section 4.2) and a hi-
erarchy of artefacts generated (see Section 4.3).
3 Cultural Heritage Data
The PATHS system has been applied to data from
Europeana1. This is a web-portal to collections
of cultural heritage artefacts provided by a wide
range of European institutions. Europeana cur-
rently provides access to over 20 million artefacts
including paintings, films, books, archival records
and museum objects. The artefacts are provided
by around 1,500 institutions which range from
major institutions, including the Rijksmuseum in
Amsterdam, the British Library and the Louvre,
to smaller organisations such as local museums.
It therefore contains an aggregation of digital con-
tent from several sources and is not connected with
any one physical museum.
The PATHS system makes use of three collec-
tions from Europeana. The first of these con-
tains artefacts from content providers in the United
Kingdom which has meta-data in English. The
artefacts in the remaining two collections come
from institutions in Spain and have meta-data in
Spanish.
CultureGrid Culture Grid2 is a digital content
provider service from the Collection Trust3.
1http://www.europeana.eu
2http://www.culturegrid.org.uk
3http://www.collectionstrust.org.uk
It contains information about over one mil-
lion artefacts from 40 different UK content
providers such as national and regional mu-
seums and libraries.
Cervantes Biblioteca Virtual Miguel De Cer-
vantes4 contains digitalised Spanish text in
various formats. In total, the online library
contains about 75,000 works from a range of
periods in Spanish history.
Hispana The Biblioteca Nacional de Espan?a5
contains information about a diverse set of
content including text and drawings. The ma-
terial is collected from different providers in
Spain including museums and libraries.
Europeana stores metadata for each artefact in
an XML-based format which includes information
such as its title, the digital format, the collection,
the year of creation and also a short description of
each artefact. However, this meta-data is created
by the content providers and varies significantly
across artefacts. Many of the artefacts have only
limited information associated with them, for ex-
ample a single word title. In addition, the content
providers that contribute to Europeana use differ-
ent hierarchical structures to organise their collec-
tions (e.g. Library of Congress Subject Headings6
and the Art and Architecture Thesaurus7), or do
not organise their content into any structure. Con-
sequently the various hierarchies that are used in
Europeana only cover some of the artefacts and
are not compatible with each other.
3.1 Filtering Data
Analysis of the artefacts in these three collections
revealed that many have short and uninformative
titles or lack a description. This forms a challenge
to language processing techniques since the arte-
fact?s meta-data does not contain enough informa-
tion to model it accurately.
The collections were filtered by removing any
artefacts that have no description and have either
fewer than four words in their title or have a title
that is repeated more than 100 times in the col-
lection. Table 1 shows the number of artefacts
in each of the Europeana collections before and
4http://www.cervantesvirtual.com
5http://www.bne.es
6http://authorities.loc.gov/
7http://www.getty.edu/research/tools/
vocabularies/aat/
152
after this filter has been applied. Applying the
heuristic leads to the removal of around 31% of the
artefacts, although the number varies significantly
across the collections with 61% of the artefacts in
CultureGrid being removed and only 1% of those
in Hispana.
Collection Lang. Total Filtered
CultureGrid Eng. 1,207,781 466,958
Hispana Sp. 1,235,133 1,219,731
Cervantes Sp. 19,278 14,983
2,462,192 1,701,672
Table 1: Number of artefacts in Europeana collec-
tions before and after filtering
4 Data Processing
A range of pre-preprocessing steps were carried
out on these collections to provide additional in-
formation to support navigation in the PATHS sys-
tem.
4.1 Artefact Similarity
We begin by computing the similarity between
the various artefacts in the Europeana collections.
This information is useful for navigation and rec-
ommendation but is not available in the Europeana
collections since they are drawn from a diverse
range of sources.
Similarity is computed using an approach de-
scribed by Aletras et al (2012). in which the top-
ics generated from each artefact?s metadata using
a topic model are compared. Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) is a widely used
type of topic model in which documents can be
viewed as probability distributions over topics, ?.
The similarity between a pair of documents can be
estimated by comparing their topic distributions.
This is achieved by viewing each distribution as
a vector of probabilities and then computing the
cosine of the angle between them:
sim(a, b) =
~?a.~?b
|~?a| ? | ~?b|
(1)
where ~?a is the vector created from the probability
distribution generated by LDA for document a.
This approach is evaluated using a set of 295
pairs of artefacts for which human judgements
of similarity were obtained using crowdsourcing
(Aletras et al, 2012). Pearson correlation between
the similarity scores and human judgements was
0.53.
The similarity between all the artefacts in the
collection is computed in a pairwise fashion. The
25 artefacts with the highest score are retained for
each artefact.
4.2 Background Links
The metadata associated with Europeana artefacts
is often very limited. Consequently links to rele-
vant articles in Wikipedia were added to each the
meta-data of each artefact using Wikipedia Miner
(Milne and Witten, 2008) to provide background
information. In addition to the link, Wikipedia
Miner returns a confidence value between 0 and
1 for each link based on the context of the item.
The accuracy of the links added by Wikipedia
Miner were evaluated using the meta-data associ-
ated with 21 randomly selected artefacts. Three
annotators analysed the links added and found that
a confidence value of 0.5 represented a good bal-
ance between accuracy and coverage. See Fer-
nando and Stevenson (2012) for further details.
4.3 Hierarchies
The range of hierarchies used by the various col-
lections that comprise the Europeana collection
make navigation difficult (see Section 3). Con-
sequently, the Wikipedia links added to the arte-
fact meta-data were used to automatically gener-
ate hierarchies that the cover the entire collection.
These hierarchies are used by the PATHS system
to assist browsing and exploration.
Two approaches are used to generate hierarchies
of Europeana artefacts (WikiFreq and WikiTax).
These are combined to generate the WikiMerge hi-
erarchy which is used in the PATHS system.
WikiFreq uses link frequencies across the en-
tire collection to organise the artefacts. The first
stage in the hierarchy generation process is to
compute the frequency with which each linked
Wikipedia article appears in the collection. The
links in each artefact are these analysed to con-
struct a hierarchy consisting of Wikipedia articles.
The links in the meta-data associated with each
artefact are ordered based on their frequency in the
entire collection and that set of links then inserted
into the hierarchy. For example, if the set of or-
dered links for an artefact is a1, a2, a3 ? ? ? an then
the artefact is then inserted into the hierarchy un-
der the branch a1 ? a2 ? a3 ? ? ? ? an, with
a1 at the top level in the tree and the artefact ap-
pearing under the node an. If this branch does not
already exist in the tree then it is created.
153
The hierarchy is pruned to removing nodes with
fewer than 20 artefacts in them. In addition, if a
node has more than 20 child nodes, only the 20
most frequent are used.
WikiTax uses the Wikipedia Taxonomy
(Ponzetto and Strube, 2011), a taxonomy derived
from Wikipedia categories. Europeana artefacts
are inserted into this taxonomy using the links
added by Wikipedia Miner with each artefact
being added to the taxonomy for all categories
listed in the links. This leads to a taxonomy in
which artefacts can occur in multiple locations.
Each approach was used to generate hierarchies
from the Europeana collections. The resulting hi-
erarchies were evaluated via online surveys, see
Fernando et al (2012) for further details. It was
found that WikiFreq performed well at placing
items into the correct location in the taxonomy and
grouping together similar items under the same
node. However, the overall structure of WikiTax
was judged to be more coherent and comprehensi-
ble.
WikiMerge combines combines WikiFreq and
WikiTax. WikiFreq is used to link each artefact
to Wikipedia articles a1 . . . an, but only the link
to the most specific article, an, is retained. The
an articles are linked to their parent WikiTax top-
ics based on the Wikipedia categories the articles
belong to. The resulting hierarchy is pruned re-
moving all WikiTax topics that do not have a Wik-
iFreq child or have only one child topic. Finally
top-level topics in the combined hierarchy are then
linked to their respective Wikipedia root node.
The resulting WikiMerge hierarchy has Wik-
iFreq topics as its leaves and WikiTax topics as
its interior and root nodes. Experiments showed
that this approach was successful in combining
the strengths of the two methods (Fernando et al,
2012).
5 The PATHS System
The PATHS system provides access to the Euro-
peana collections described in Section 3 by mak-
ing use of the additional information generated us-
ing the approaches described in Section 4. The in-
terface of the PATHS system has three main areas:
Paths enables users to navigate via pathways (see
Section 5.1).
Search supports discovery of both collection arte-
facts and pathways through keyword search
(see Section 5.2).
Explore enables users to explore the collections
using a variety of types of overview (see Sec-
tion 5.3).
5.1 Paths Area
This area provides users with access to Europeana
through pathways or trails. These are manually
generated sets of artefacts organised into a tree
structure which are designed to showcase the con-
tent available to the user in an organised way.
These can be created by users and can be pub-
lished for others to follow. An example path-
way on the topic ?railways? is shown in Figure
1. A short description of the pathway?s content is
shown towards the top of the figure and a graphical
overview of its contents at the bottom.
Figure 1: Example pathway on the topic ?rail-
ways?
Figure 2 shows as example artefact as displayed
in the system. The example artefact is a portrait
of Catherine the Great. The left side of the figure
shows information extracted directly from the Eu-
ropeana meta-data for this artefact. The title and
textual description are shown towards the top left
together with a thumbnail image of the artefact.
Other information from the meta-data is shown be-
neath the ?About this item? heading. The right
side of the figure shows additional information
Figure 2: Example artefact displayed in system in-
terface. Related artefacts and background links are
displayed on right hand side
154
Figure 3: Example visualisations of hierarchy: thesaurus view (top left), tag cloud (top right), map views
(bottom)
about the artefact generated using the approaches
described in Sections 4.1 and 4.2. Related arte-
facts are shown to the user one at a time, click-
ing on the thumbnail image leads to the equivalent
page for the related artefact. Below this are links
to the Wikipedia articles that are identified in the
text of the article?s title and description.
5.2 Search Area
This area allows users to search for artefacts and
pathways using standard keyword search imple-
mented using Lucene (McCandless et al, 2010).
5.3 Explore Area
The system provides a variety of ways to view
the hierarchies generated using the approach de-
scribed in Section 4.3. Figure 3 shows how these
are displayed for a section of the hierarchy with
the label ?Society?. The simplest view (shown in
the top left of Figure 3) is a thesaurus type view
in which levels of the hierarchy are represented by
indentation. The system also allows levels of the
hierarchy to be viewed as a tag cloud (top right of
Figure 3). The final representation of the hierar-
chy is as a map, shown in the bottom of Figure 3.
In this visualisation categories in the hierarchy are
represented as ?islands? on the map. Zooming in
on the map provides more detail about that area of
the hierarchy.
6 Summary and Future Developments
This paper describes a system for navigating Eu-
ropeana, an aggregation of collections of cultural
heritage artefacts. NLP analysis is used to organ-
ise the collection and provide additional informa-
tion. The results of this analysis are provided to
the user through an online interface which pro-
vides access to English and Spanish content in Eu-
ropeana.
Planned future development of this system in-
cludes providing recommendations and more per-
sonalised access. Similarity information (Sec-
tion 4.1) can be used to provide information from
which the recommendations can be made. Person-
alised access will make use of information about
individual users (e.g. from their browsing be-
haviour or information they provide about their
preferences) to generate individual views of Eu-
ropeana.
155
Online Demo
The PATHS system is available at
http://explorer.paths-project.eu/
Acknowledgments
The research leading to these results was
carried out as part of the PATHS project
(http://paths-project.eu) funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 270082
References
N. Aletras, M. Stevenson, and P. Clough. 2012. Com-
puting similarity between items in a digital library
of cultural heritage. Journal of Computing and Cul-
tural Heritage, 5(4):no. 16.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
F. Bohnert, D. Schmidt, and I. Zuckerman. 2009. Spa-
tial Process for Recommender Systems. In Proc. of
IJCAI 2009, pages 2022?2027, Pasadena, CA.
J. Bowen and S. Filippini-Fantoni. 2004. Personaliza-
tion and the Web from a Museum Perspective. In
Proc. of Museums and the Web 2004, pages 63?78.
Samuel Fernando and Mark Stevenson. 2012. Adapt-
ing Wikification to Cultural Heritage. In Proceed-
ings of the 6th Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Human-
ities, pages 101?106, Avignon, France.
Samuel Fernando, Mark Hall, Eneko Agirre, Aitor
Soroa, Paul Clough, and Mark Stevenson. 2012.
Comparing taxonomies for organising collections of
documents. In Proc. of COLING 2012, pages 879?
894, Mumbai, India.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing Search in Context: The Concept Revisited. ACM
Trans. on Information Systems, 20(1):116?131.
R. Furuta, F.. Shipman, C. Marshall, D. Brenner, and
H. Hsieh. 1997. Hypertext paths and the World-
Wide Web: experiences with Walden?s Paths. In
Proc. of the Eighth ACM conference on Hypertext,
pages 167?176, New York, NY.
K. Grieser, T. Baldwin, and S. Bird. 2007. Dynamic
Path Prediction and Recommendation in a Museum
Environment. In Proc. of the Workshop on Lan-
guage Technology for Cultural Heritage Data (LaT-
eCH 2007), pages 49?56, Prague, Czech Republic.
K. Grieser, T. Baldwin, F. Bohnert, and L. Sonenberg.
2011. Using Ontological and Document Similarity
to Estimate Museum Exhibit Relatedness. Journal
of Computing and Cultural Heritage, 3(3):1?20.
W.R. van Hage, N. Stash, Y. Wang, and L.M. Aroyo.
2010. Finding your way through the Rijksmuseum
with an adaptive mobile museum guide. In Proc. of
ESWC 2010, pages 46?59.
J. Heitzman, C. Mellish, and J. Oberlander. 1997. Dy-
namic Generation of Museum Web Pages: The In-
telligent Labelling Explorer. Archives and Museum
Informatics, 11(2):117?125.
G. Marchionini. 2006. Exploratory Search: from Find-
ing to Understanding. Comm. ACM, 49(1):41?46.
M. McCandless, E. Hatcher, and O. Gospodnetic.
2010. Lucene in Action. Manning Publications.
D. Milne and I. Witten. 2008. Learning to Link with
Wikipedia. In Proc. of CIKM 2008, Napa Valley,
California.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: An architecture for a dynamic hy-
pertext generation system. Natural Language En-
gineering, 7:225?250.
S.P. Ponzetto and M. Strube. 2011. Taxonomy in-
duction based on a collaboratively built knowledge
repository. Artificial Intelligence, 175(9-10):1737?
1756.
S. Reich, L. Carr, D. De Roure, and W. Hall. 1999.
Where have you been from here? Trails in hypertext
systems. ACM Computing Surveys, 31.
I. Roes, N. Stash, Y. Wang, and L. Aroyo. 2009. A
personalized walk through the museum: the CHIP
interactive tour guide. In Proc. of the 27th Interna-
tional Conference on Human Factors in Computing
Systems, pages 3317?3322, Boston, MA.
F. Shipman, R. Furuta, D. Brenner, C. Chung, and
H. Hsieh. 2000. Guided paths through web-based
collections: Design, experiences, and adaptations.
Journal of the American Society for Information Sci-
ence, 51(3):260?272.
J. Trant. 2009. Tagging, folksonomies and art mu-
seums: Early experiments and ongoing research.
Journal of Digital Information, 10(1).
R. White and J. Huang. 2010. Assessing the scenic
route: measuring the value of search trails in web
logs. In Proc. of SIGIR 2010, pages 587?594.
M. Wilson, Kulesm B., M. Schraefel, and B. Schnei-
derman. 2010. From keyword search to explo-
ration: Designing future search interfaces for the
web. Foundations and Trends in Web Science,
2(1):1?97.
156
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 649?655,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
On WordNet Semantic Classes and Dependency Parsing
Kepa Bengoetxea?, Eneko Agirre?, Joakim Nivre?,
Yue Zhang*, Koldo Gojenola?
?University of the Basque Country UPV/EHU / IXA NLP Group
?Uppsala University / Department of Linguistics and Philology
? Singapore University of Technology and Design
kepa.bengoetxea@ehu.es, e.agirre@ehu.es,
joakim.nivre@lingfil.uu.se, yue zhang@sutd.edu.sg,
koldo.gojenola@ehu.es
Abstract
This paper presents experiments with
WordNet semantic classes to improve de-
pendency parsing. We study the effect
of semantic classes in three dependency
parsers, using two types of constituency-
to-dependency conversions of the English
Penn Treebank. Overall, we can say that
the improvements are small and not sig-
nificant using automatic POS tags, con-
trary to previously published results using
gold POS tags (Agirre et al, 2011). In
addition, we explore parser combinations,
showing that the semantically enhanced
parsers yield a small significant gain only
on the more semantically oriented LTH
treebank conversion.
1 Introduction
This work presents a set of experiments to investi-
gate the use of lexical semantic information in de-
pendency parsing of English. Whether semantics
improve parsing is one interesting research topic
both on parsing and lexical semantics. Broadly
speaking, we can classify the methods to incor-
porate semantic information into parsers in two:
systems using static lexical semantic repositories,
such as WordNet or similar ontologies (Agirre et
al., 2008; Agirre et al, 2011; Fujita et al, 2010),
and systems using dynamic semantic clusters au-
tomatically acquired from corpora (Koo et al,
2008; Suzuki et al, 2009).
Our main objective will be to determine
whether static semantic knowledge can help pars-
ing. We will apply different types of semantic in-
formation to three dependency parsers. Specifi-
cally, we will test the following questions:
? Does semantic information in WordNet help
dependency parsing? Agirre et al (2011)
found improvements in dependency parsing
using MaltParser on gold POS tags. In this
work, we will investigate the effect of seman-
tic information using predicted POS tags.
? Is the type of semantic information related
to the type of parser? We will test three
different parsers representative of successful
paradigms in dependency parsing.
? How does the semantic information relate to
the style of dependency annotation? Most ex-
periments for English were evaluated on the
Penn2Malt conversion of the constituency-
based Penn Treebank. We will also examine
the LTH conversion, with richer structure and
an extended set of dependency labels.
? How does WordNet compare to automati-
cally obtained information? For the sake of
comparison, we will also perform the experi-
ments using syntactic/semantic clusters auto-
matically acquired from corpora.
? Does parser combination benefit from seman-
tic information? Different parsers can use se-
mantic information in diverse ways. For ex-
ample, while MaltParser can use the semantic
information in local contexts, MST can in-
corporate them in global contexts. We will
run parser combination experiments with and
without semantic information, to determine
whether it is useful in the combined parsers.
After introducing related work in section 2, sec-
tion 3 describes the treebank conversions, parsers
and semantic features. Section 4 presents the re-
sults and section 5 draws the main conclusions.
2 Related work
Broadly speaking, we can classify the attempts to
add external knowledge to a parser in two sets:
using large semantic repositories such as Word-
Net and approaches that use information automat-
ically acquired from corpora. In the first group,
Agirre et al (2008) trained two state-of-the-art
constituency-based statistical parsers (Charniak,
649
2000; Bikel, 2004) on semantically-enriched in-
put, substituting content words with their seman-
tic classes, trying to overcome the limitations of
lexicalized approaches to parsing (Collins, 2003)
where related words, like scissors and knife, can-
not be generalized. The results showed a signi-
cant improvement, giving the first results over both
WordNet and the Penn Treebank (PTB) to show
that semantics helps parsing. Later, Agirre et al
(2011) successfully introduced WordNet classes in
a dependency parser, obtaining improvements on
the full PTB using gold POS tags, trying different
combinations of semantic classes. MacKinlay et
al. (2012) investigate the addition of semantic an-
notations in the form of word sense hypernyms, in
HPSG parse ranking, reducing error rate in depen-
dency F-score by 1%, while some methods pro-
duce substantial decreases in performance. Fu-
jita et al (2010) showed that fully disambiguated
sense-based features smoothed using ontological
information are effective for parse selection.
On the second group, Koo et al (2008) pre-
sented a semisupervised method for training de-
pendency parsers, introducing features that incor-
porate word clusters automatically acquired from
a large unannotated corpus. The clusters include
strongly semantic associations like {apple, pear}
or {Apple, IBM} and also syntactic clusters like
{of, in}. They demonstrated its effectiveness in
dependency parsing experiments on the PTB and
the Prague Dependency Treebank. Suzuki et al
(2009), Sagae and Gordon (2009) and Candito
and Seddah (2010) also experiment with the same
cluster method. Recently, T?ackstr?om et al (2012)
tested the incorporation of cluster features from
unlabeled corpora in a multilingual setting, giving
an algorithm for inducing cross-lingual clusters.
3 Experimental Framework
In this section we will briefly describe the PTB-
based datasets (subsection 3.1), followed by the
data-driven parsers used for the experiments (sub-
section 3.2). Finally, we will describe the different
types of semantic representation that were used.
3.1 Treebank conversions
Penn2Malt
1
performs a simple and direct conver-
sion from the constituency-based PTB to a depen-
dency treebank. It obtains projective trees and has
been used in several works, which allows us to
1
http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
compare our results with related experiments (Koo
et al, 2008; Suzuki et al, 2009; Koo and Collins,
2010). We extracted dependencies using standard
head rules (Yamada and Matsumoto, 2003), and a
reduced set of 12 general dependency tags.
LTH
2
(Johansson and Nugues, 2007) presents
a conversion better suited for semantic process-
ing, with a richer structure and a more fine-grained
set of dependency labels (42 different dependency
labels), including links to handle long-distance
phenomena, giving a 6.17% of nonprojective sen-
tences. The results from parsing the LTH output
are lower than those for Penn2Malt conversions.
3.2 Parsers
We have made use of three parsers representative
of successful paradigms in dependency parsing.
MaltParser (Nivre et al, 2007) is a determinis-
tic transition-based dependency parser that obtains
a dependency tree in linear-time in a single pass
over the input using a stack of partially analyzed
items and the remaining input sequence, by means
of history-based feature models. We added two
features that inspect the semantic feature at the top
of the stack and the next input token.
MST
3
represents global, exhaustive graph-
based parsing (McDonald et al, 2005; McDon-
ald et al, 2006) that finds the highest scoring di-
rected spanning tree in a graph. The learning pro-
cedure is global since model parameters are set
relative to classifying the entire dependency graph,
in contrast to the local but richer contexts used
by transition-based parsers. The system can be
trained using first or second order models. The
second order projective algorithm performed best
on both conversions, and we used it in the rest of
the evaluations. We modified the system in or-
der to add semantic features, combining them with
wordforms and POS tags, on the parent and child
nodes of each arc.
ZPar
4
(Zhang and Clark, 2008; Zhang and
Nivre, 2011) performs transition-based depen-
dency parsing with a stack of partial analysis
and a queue of remaining inputs. In contrast to
MaltParser (local model and greedy deterministic
search) ZPar applies global discriminative learn-
ing and beam search. We extend the feature set of
ZPar to include semantic features. Each set of se-
mantic information is represented by two atomic
2
http://nlp.cs.lth.se/software/treebank converter
3
http://mstparser.sourceforge.net
4
www.sourceforge.net/projects/zpar
650
Base WordNet WordNet Clusters
line SF SS
Malt 88.46 88.49 (+0.03) 88.42 (-0.04) 88.59 (+0.13)
MST 90.55 90.70 (+0.15) 90.47 (-0.08) 90.88 (+0.33)?
ZPar 91.52 91.65 (+0.13) 91.70 (+0.18)? 91.74 (+0.22)
Table 1: LAS results with several parsing algo-
rithms, Penn2Malt conversion (?: p <0.05, ?: p
<0.005). In parenthesis, difference with baseline.
feature templates, associated with the top of the
stack and the head of the queue, respectively. ZPar
was directly trained on the Penn2Malt conversion,
while we applied the pseudo-projective transfor-
mation (Nilsson et al, 2008) on LTH, in order to
deal with non-projective arcs.
3.3 Semantic information
Our aim was to experiment with different types of
WordNet-related semantic information. For com-
parison with automatically acquired information,
we will also experiment with bit clusters.
WordNet. We will experiment with the seman-
tic representations used in Agirre et al (2008) and
Agirre et al (2011), based on WordNet 2.1. Word-
Net is organized into sets of synonyms, called
synsets (SS). Each synset in turn belongs to a
unique semantic file (SF). There are a total of 45
SFs (1 for adverbs, 3 for adjectives, 15 for verbs,
and 26 for nouns), based on syntactic and seman-
tic categories. For example, noun SFs differen-
tiate nouns denoting acts or actions, and nouns
denoting animals, among others. We experiment
with both full SSs and SFs as instances of fine-
grained and coarse-grained semantic representa-
tion, respectively. As an example, knife in its
tool sense is in the EDGE TOOL USED AS A
CUTTING INSTRUMENT singleton synset, and
also in the ARTIFACT SF along with thousands
of words including cutter. These are the two ex-
tremes of semantic granularity in WordNet. For
each semantic representation, we need to deter-
mine the semantics of each occurrence of a target
word. Agirre et al (2011) used i) gold-standard
annotations from SemCor, a subset of the PTB, to
give an upper bound performance of the semantic
representation, ii) first sense, where all instances
of a word were tagged with their most frequent
sense, and iii) automatic sense ranking, predicting
the most frequent sense for each word (McCarthy
et al, 2004). As we will make use of the full PTB,
we only have access to the first sense information.
Clusters. Koo et al (2008) describe a semi-
Base WordNet WordNet Clusters
line SF SS
Malt 84.95 85.12 (+0.17) 85.08 (+0.16) 85.13 (+0.18)
MST 85.06 85.35 (+0.29)? 84.99 (-0.07) 86.18 (+1.12)?
ZPar 89.15 89.33 (+0.18) 89.19 (+0.04) 89.17 (+0.02)
Table 2: LAS results with several parsing algo-
rithms in the LTH conversion (?: p <0.05, ?: p
<0.005). In parenthesis, difference with baseline.
supervised approach that makes use of cluster fea-
tures induced from unlabeled data, providing sig-
nificant performance improvements for supervised
dependency parsers on the Penn Treebank for En-
glish and the Prague Dependency Treebank for
Czech. The process defines a hierarchical cluster-
ing of the words, which can be represented as a
binary tree where each node is associated to a bit-
string, from the more general (root of the tree) to
the more specific (leaves). Using prefixes of vari-
ous lengths, it can produce clusterings of different
granularities. It can be seen as a representation of
syntactic-semantic information acquired from cor-
pora. They use short strings of 4-6 bits to represent
parts of speech and the full strings for wordforms.
4 Results
In all the experiments we employed a baseline fea-
ture set using word forms and parts of speech, and
an enriched feature set (WordNet or clusters). We
firstly tested the addition of each individual se-
mantic feature to each parser, evaluating its contri-
bution to the parser?s performance. For the combi-
nations, instead of feature-engineering each parser
with the wide array of different possibilities for
features, as in Agirre et al (2011), we adopted
the simpler approach of combining the outputs of
the individual parsers by voting (Sagae and Lavie,
2006). We will use Labeled Attachment Score
(LAS) as our main evaluation criteria. As in pre-
vious work, we exclude punctuation marks. For
all the tests, we used a perceptron POS-tagger
(Collins, 2002), trained on WSJ sections 2?21, to
assign POS tags automatically to both the training
(using 10-way jackknifing) and test data, obtaining
a POS tagging accuracy of 97.32% on the test data.
We will make use of Bikel?s randomized parsing
evaluation comparator to test the statistical signi-
cance of the results. In all of the experiments the
parsers were trained on sections 2-21 of the PTB
and evaluated on the development set (section 22).
Finally, the best performing system was evaluated
on the test set (section 23).
651
Parsers LAS UAS
Best baseline (ZPar) 91.52 92.57
Best single parser (ZPar + Clusters) 91.74 (+0.22) 92.63
Best combination (3 baseline parsers) 91.90 (+0.38) 93.01
Best combination of 3 parsers:
3 baselines + 3 SF extensions 91.93 (+0.41) 92.95
Best combination of 3 parsers:
3 baselines + 3 SS extensions 91.87 (+0.35) 92.92
Best combination of 3 parsers:
3 baselines + 3 cluster extensions 91.90 (+0.38) 92.90
Table 3: Parser combinations on Penn2Malt.
Parsers LAS UAS
Best baseline (ZPar) 89.15 91.81
Best single parser (ZPar + SF) 89.33 (+0.15) 92.01
Best combination (3 baseline parsers) 89.15 (+0.00) 91.81
Best combination of 3 parsers:
3 baselines + 3 SF extensions 89.56 (+0.41)? 92.23
Best combination of 3 parsers:
3 baselines + 3 SS extensions 89.43 (+0.28) 93.12
Best combination of 3 parsers:
3 baselines + 3 cluster extensions 89.52 (+0.37)? 92.19
Table 4: Parser combinations on LTH (?: p <0.05,
?: p <0.005).
4.1 Single Parsers
We run a series of experiments testing each indi-
vidual semantic feature, also trying different learn-
ing configurations for each one. Regarding the
WordNet information, there were 2 different fea-
tures to experiment with (SF and SS). For the bit
clusters, there are different possibilities, depend-
ing on the number of bits used. For Malt and MST,
all the different lengths of bit strings were used.
Given the computational requirements and the pre-
vious results on Malt and MST, we only tested all
bits in ZPar. Tables 1 and 2 show the results.
Penn2Malt. Table 1 shows that the only signifi-
cant increase over the baseline is for ZPar with SS
and for MST with clusters.
LTH. Looking at table 2, we can say that the dif-
ferences in baseline parser performance are accen-
tuated when using the LTH treebank conversion,
as ZPar clearly outperforms the other two parsers
by more than 4 absolute points. We can see that
SF helps all parsers, although it is only significant
for MST. Bit clusters improve significantly MST,
with the highest increase across the table.
Overall, we see that the small improvements
do not confirm the previous results on Penn2Malt,
MaltParser and gold POS tags. We can also con-
clude that automatically acquired clusters are spe-
cially effective with the MST parser in both tree-
bank conversions, which suggests that the type of
semantic information has a direct relation to the
parsing algorithm. Section 4.3 will look at the de-
tails by each knowledge type.
4.2 Combinations
Subsection 4.1 presented the results of the base al-
gorithms and their extensions based on semantic
features. Sagae and Lavie (2006) report improve-
ments over the best single parser when combining
three transition-based models and one graph-based
model. The same technique was also used by the
winning team of the CoNLL 2007 Shared Task
(Hall et al, 2007), combining six transition-based
parsers. We used MaltBlender
5
, a tool for merging
the output of several dependency parsers, using the
Chu-Liu/Edmonds directed MST algorithm. After
several tests we noticed that weighted voting by
each parser?s labeled accuracy gave good results,
using it in the rest of the experiments. We trained
different types of combination:
? Base algorithms. This set includes the 3 base-
line algorithms, MaltParser, MST, and ZPar.
? Extended parsers, adding semantic informa-
tion to the baselines. We include the three
base algorithms and their semantic exten-
sions (SF, SS, and clusters). It is known (Sur-
deanu and Manning, 2010) that adding more
parsers to an ensemble usually improves ac-
curacy, as long as they add to the diver-
sity (and almost regardless of their accuracy
level). So, for the comparison to be fair, we
will compare ensembles of 3 parsers, taken
from sets of 6 parsers (3 baselines + 3 SF,
SS, and cluster extensions, respectively).
In each experiment, we took the best combina-
tion of individual parsers on the development set
for the final test. Tables 3 and 4 show the results.
Penn2Malt. Table 3 shows that the combina-
tion of the baselines, without any semantic infor-
mation, considerably improves the best baseline.
Adding semantics does not give a noticeable in-
crease with respect to combining the baselines.
LTH (table 4). Combining the 3 baselines does
not give an improvement over the best baseline, as
ZPar clearly outperforms the other parsers. How-
ever, adding the semantic parsers gives an increase
with respect to the best single parser (ZPar + SF),
which is small but significant for SF and clusters.
4.3 Analysis
In this section we analyze the data trying to under-
stand where and how semantic information helps
most. One of the obstacles of automatic parsers
is the presence of incorrect POS tags due to auto-
5
http://w3.msi.vxu.se/users/jni/blend/
652
LAS on sentences LAS on sentences
POS tags Parser LAS test set without POS errors with POS errors
Gold ZPar 90.45 91.68 89.14
Automatic ZPar 89.15 91.62 86.51
Automatic Best combination of 3 parsers: 89.56 (+0.41) 91.90 (+0.28) 87.06 (+0.55)
3 baselines + 3 SF extensions
Automatic Best combination of 3 parsers: 89.43 (+0.28) 91.95 (+0.33) 86.75 (+0.24)
3 baselines + 3 SS extensions
Automatic Best combination of 3 parsers: 89.52 (+0.37) 91.92 (+0.30) 86.96 (+0.45)
3 baselines + 3 cluster extensions
Table 5: Differences in LAS (LTH) for baseline and extended parsers with sentences having cor-
rect/incorrect POS tags (the parentheses show the difference w.r.t ZPar with automatic POS tags).
matic tagging. For example, ZPar?s LAS score on
the LTH conversion drops from 90.45% with gold
POS tags to 89.12% with automatic POS tags. We
will examine the influence of each type of seman-
tic information on sentences that contain or not
POS errors, and this will clarify whether the incre-
ments obtained when using semantic information
are useful for correcting the negative influence of
POS errors or they are orthogonal and constitute
a source of new information independent of POS
tags. With this objective in mind, we analyzed the
performance on the subset of the test corpus con-
taining the sentences which had POS errors (1,025
sentences and 27,300 tokens) and the subset where
the sentences had (automatically assigned) correct
POS tags (1,391 sentences and 29,386 tokens).
Table 5 presents the results of the best single
parser on the LTH conversion (ZPar) with gold
and automatic POS tags in the first two rows. The
LAS scores are particularized for sentences that
contain or not POS errors. The following three
rows present the enhanced (combined) parsers
that make use of semantic information. As the
combination of the three baseline parsers did not
give any improvement over the best single parser
(ZPar), we can hypothesize that the gain coming
from the parser combinations comes mostly from
the addition of semantic information. Table 5 sug-
gests that the improvements coming from Word-
Net?s semantic file (SF) are unevenly distributed
between the sentences that contain POS errors and
those that do not (an increase of 0.28 for sentences
without POS errors and 0.55 for those with er-
rors). This could mean that a big part of the in-
formation contained in SF helps to alleviate the
errors performed by the automatic POS tagger. On
the other hand, the increments are more evenly
distributed for SS and clusters, and this can be
due to the fact that the semantic information is
orthogonal to the POS, giving similar improve-
ments for sentences that contain or not POS errors.
We independently tested this fact for the individ-
ual parsers. For example, with MST and SF the
gains almost doubled for sentences with incorrect
POS tags (+0.37 with respect to +0.21 for sen-
tences with correct POS tags) while the gains of
adding clusters? information for sentences without
and with POS errors were similar (0.91 and 1.33,
repectively). This aspect deserves further inves-
tigation, as the improvements seem to be related
to both the type of semantic information and the
parsing algorithm.We did an initial exploration but
it did not give any clear indication of the types of
improvements that could be expected using each
parser and semantic data.
5 Conclusions
This work has tried to shed light on the contribu-
tion of semantic information to dependency pars-
ing. The experiments were thorough, testing two
treebank conversions and three parsing paradigms
on automatically predicted POS tags. Compared
to (Agirre et al, 2011), which used MaltParser on
the LTH conversion and gold POS tags, our results
can be seen as a negative outcome, as the improve-
ments are very small and non-significant in most
of the cases. For parser combination, WordNet
semantic file information does give a small sig-
nificant increment in the more fine-grained LTH
representation. In addition we show that the im-
provement of automatic clusters is also weak. For
the future, we think tdifferent parsers, eitherhat a
more elaborate scheme is needed for word classes,
requiring to explore different levels of generaliza-
tion in the WordNet (or alternative) hierarchies.
Acknowledgments
This research was supported by the the Basque
Government (IT344- 10, S PE11UN114), the Uni-
versity of the Basque Country (GIU09/19) and
the Spanish Ministry of Science and Innovation
(MICINN, TIN2010-20218).
653
References
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving parsing and PP attachment per-
formance with sense information. In Proceedings
of ACL-08: HLT, pages 317?325, Columbus, Ohio,
June. Association for Computational Linguistics.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency pars-
ing with semantic classes. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 699?703, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4):479?511.
Marie Candito and Djam?e Seddah. 2010. Pars-
ing word clusters. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?84, Los
Angeles, CA, USA, June. Association for Computa-
tional Linguistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, NAACL 2000, pages
132?139, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589?637, December.
Sanae Fujita, Francis Bond, Stephan Oepen, and
Takaaki Tanaka. 2010. Exploiting semantic infor-
mation for hpsg parse selection. Research on Lan-
guage and Computation, 8(1):122.
Johan Hall, Jens Nilsson, Joakim Nivre, Glsen Eryigit,
Beta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multi-
lingual parser optimization. In Proceedings of the
CoNLL Shared Task EMNLP-CoNLL.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1?11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Andrew MacKinlay, Rebecca Dridan, Diana McCarthy,
and Timothy Baldwin. 2012. The effects of seman-
tic annotations on precision parse ranking. In First
Joint Conference on Lexical and Computational Se-
mantics (*SEM), page 228236, Montreal, Canada,
June. Association for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of CoNLL 2006.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2008.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th Con-
ference of the ACL.
Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A.,
Glsen Eryiit, Sandra Kbler, Marinov S., and Edwin
Marsi. 2007. Maltparser: A language-independent
system for data-driven dependency parsing. Natural
Language Engineering.
Kenji Sagae and Andrew Gordon. 2009. Clustering
words by syntactic similarity improves dependency
parsing of predicate-argument structures. In Pro-
ceedings of the Eleventh International Conference
on Parsing Technologies.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics Conference (NAACL-2010), Los Ange-
les, CA, June.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 551?560, Singapore, August. As-
sociation for Computational Linguistics.
654
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 477?
487, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th IWPT, pages 195?
206. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
655
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75?80,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 17: All-words Word Sense Disambiguation
on a Specific Domain
Eneko Agirre, Oier Lopez de Lacalle
IXA NLP group
UBC
Donostia, Basque Country
{e.agirre,oier.lopezdelacalle}@ehu.es
Christiane Fellbaum
Department of Computer Science
Princeton University
Princeton, USA
fellbaum@princeton.edu
Shu-Kai Hsieh
Department of English
National Taiwan Normal University
Taipei, Taiwan
shukai@ntnu.edu.tw
Maurizio Tesconi
IIT
CNR
Pisa, Italy
maurizio.tesconi@iit.cnr.it
Monica Monachini
ILC
CNR
Pisa, Italy
monica.monachini@ilc.cnr.it
Piek Vossen, Roxanne Segers
Faculteit der Letteren
Vrije Universiteit Amsterdam
Amsterdam, Netherlands
p.vossen@let.vu.nl,roxane.segers@gmail.com
Abstract
Domain portability and adaptation of NLP
components and Word Sense Disambigua-
tion systems present new challenges. The
difficulties found by supervised systems to
adapt might change the way we assess the
strengths and weaknesses of supervised
and knowledge-based WSD systems. Un-
fortunately, all existing evaluation datasets
for specific domains are lexical-sample
corpora. This task presented all-words
datasets on the environment domain for
WSD in four languages (Chinese, Dutch,
English, Italian). 11 teams participated,
with supervised and knowledge-based sys-
tems, mainly in the English dataset. The
results show that in all languages the par-
ticipants where able to beat the most fre-
quent sense heuristic as estimated from
general corpora. The most successful ap-
proaches used some sort of supervision in
the form of hand-tagged examples from
the domain.
1 Introduction
Word Sense Disambiguation (WSD) competitions
have focused on general domain texts, as attested
in previous Senseval and SemEval competitions
(Kilgarriff, 2001; Mihalcea et al, 2004; Snyder
and Palmer, 2004; Pradhan et al, 2007). Spe-
cific domains pose fresh challenges to WSD sys-
tems: the context in which the senses occur might
change, different domains involve different sense
distributions and predominant senses, some words
tend to occur in fewer senses in specific domains,
the context of the senses might change, and new
senses and terms might be involved. Both super-
vised and knowledge-based systems are affected
by these issues: while the first suffer from differ-
ent context and sense priors, the later suffer from
lack of coverage of domain-related words and in-
formation.
The main goal of this task is to provide a mul-
tilingual testbed to evaluate WSD systems when
faced with full-texts from a specific domain. All
datasets and related information are publicly avail-
able from the task websites
1
.
This task was designed in the context of Ky-
oto (Vossen et al, 2008)
2
, an Asian-European
project that develops a community platform for
modeling knowledge and finding facts across lan-
guages and cultures. The platform operates as a
Wiki system with an ontological support that so-
cial communities can use to agree on the mean-
ing of terms in specific domains of their interest.
Kyoto focuses on the environmental domain be-
cause it poses interesting challenges for informa-
tion sharing, but the techniques and platforms are
1
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
2
http://www.kyoto-project.eu/
75
independent of the application domain.
The paper is structured as follows. We first
present the preparation of the data. Section 3 re-
views participant systems and Section 4 the re-
sults. Finally, Section 5 presents the conclusions.
2 Data preparation
The data made available to the participants in-
cluded the test set proper, and background texts.
Participants had one week to work on the test set,
but the background texts where provided months
earlier.
2.1 Test datasets
The WSD-domain comprises comparable all-
words test corpora on the environment domain.
Three texts were compiled for each language by
the European Center for Nature Conservation
3
and
Worldwide Wildlife Forum
4
. They are documents
written for a general but interested public and in-
volve specific terms from the domain. The docu-
ment content is comparable across languages. Ta-
ble 1 shows the numbers for the datasets.
Although the original plan was to annotate mul-
tiword terms, and domain terminology, due to time
constraints we focused on single-word nouns and
verbs. The test set clearly marked which were
the words to be annotated. In the case of Dutch,
we also marked components of single-word com-
pounds. The format of the test set followed that of
previous all-word exercises, which we extended to
accommodate Dutch compounds. For further de-
tails check the datasets in the task website.
The sense inventory was based on publicly
available wordnets of the respective languages
(see task website for details). The annotation pro-
cedure involved double-blind annotation by ex-
perts plus adjudication, which allowed us to also
provide Inter Annotator Agreement (IAA) figures
for the dataset. The procedure was carried out us-
ing KAFnotator tool (Tesconi et al, 2010). Due
to limitations in resources and time, the English
dataset was annotated by a single expert annota-
tor. For the rest of languages, the agreement was
very good, as reported in Table 1.
Table 1 includes the results of the random base-
line, as an indication of the polysemy in each
dataset. Average polysemy is highest for English,
and lowest for Dutch.
3
http://www.ecnc.org
4
http://www.wwf.org
Total Noun Verb IAA Random
Chinese 3989 754 450 0.96 0.321
Dutch 8157 997 635 0.90 0.328
English 5342 1032 366 n/a 0.232
Italian 8560 1340 513 0.72 0.294
Table 1: Dataset numbers, including number of
tokens, nouns and verbs to be tagged, Inter-
Annotator Agreement (IAA) and precision of ran-
dom baseline.
Documents Words
Chinese 58 455359
Dutch 98 21089
English 113 2737202
Italian 27 240158
Table 2: Size of the background data.
2.2 Background data
In addition to the test datasets proper, we also pro-
vided additional documents on related subjects,
kindly provided by ECNC and WWF. Table 2
shows the number of documents and words made
available for each language. The full list with the
urls of the documents are available from the task
website, together with the background documents.
3 Participants
Eleven participants submitted more than thirty
runs (cf. Table 3). The authors classified their runs
into supervised (S in the tables, three runs), weakly
supervised (WS, four runs), unsupervised (no runs)
and knowledge-based (KB, the rest of runs)
5
. Only
one group used hand-tagged data from the domain,
which they produced on their own. We will briefly
review each of the participant groups, ordered fol-
lowing the rank obtained for English. They all par-
ticipated on the English task, with one exception
as noted below, so we report their rank in the En-
glish task. Please refer to their respective paper in
these proceedings for more details.
CFILT: They participated with a domain-
specific knowledge-based method based on Hop-
field networks (Khapra et al, 2010). They first
identify domain-dependant words using the back-
ground texts, use a graph based on hyponyms in
WordNet, and a breadth-first search to select the
most representative synsets within domain. In ad-
dition they added manually disambiguated around
one hundred examples from the domain as seeds.
5
Note that boundaries are slippery. We show the classifi-
cations as reported by the authors.
76
English
Rank Participant System ID Type P R R nouns R verbs
1 Anup Kulkarni CFILT-2 WS 0.570 0.555 ?0.024 0.594 ?0.028 0.445 ?0.047
2 Anup Kulkarni CFILT-1 WS 0.554 0.540 ?0.021 0.580 ?0.025 0.426 ?0.043
3 Siva Reddy IIITH1-d.l.ppr.05 WS 0.534 0.528 ?0.027 0.553 ?0.023 0.456 ?0.041
4 Abhilash Inumella IIITH2-d.r.l.ppr.05 WS 0.522 0.516 ?0.023 0.529 ?0.027 0.478 ?0.041
5 Ruben Izquierdo BLC20SemcorBackground S 0.513 0.513 ?0.022 0.534 ?0.026 0.454 ?0.044
- - Most Frequent Sense - 0.505 0.505 ?0.023 0.519 ?0.026 0.464 ?0.043
6 Ruben Izquierdo BLC20Semcor S 0.505 0.505 ?0.025 0.527 ?0.031 0.443 ?0.045
7 Anup Kulkarni CFILT-3 KB 0.512 0.495 ?0.023 0.516 ?0.027 0.434 ?0.048
8 Andrew Tran Treematch KB 0.506 0.493 ?0.021 0.516 ?0.028 0.426 ?0.046
9 Andrew Tran Treematch-2 KB 0.504 0.491 ?0.021 0.515 ?0.030 0.425 ?0.044
10 Aitor Soroa kyoto-2 KB 0.481 0.481 ?0.022 0.487 ?0.025 0.462 ?0.039
11 Andrew Tran Treematch-3 KB 0.492 0.479 ?0.022 0.494 ?0.028 0.434 ?0.039
12 Radu Ion RACAI-MFS KB 0.461 0.460 ?0.022 0.458 ?0.025 0.464 ?0.046
13 Hansen A. Schwartz UCF-WS KB 0.447 0.441 ?0.022 0.440 ?0.025 0.445 ?0.043
14 Yuhang Guo HIT-CIR-DMFS-1.ans KB 0.436 0.435 ?0.023 0.428 ?0.027 0.454 ?0.043
15 Hansen A. Schwartz UCF-WS-domain KB 0.440 0.434 ?0.024 0.434 ?0.029 0.434 ?0.044
16 Abhilash Inumella IIITH2-d.r.l.baseline.05 KB 0.496 0.433 ?0.024 0.452 ?0.023 0.390 ?0.044
17 Siva Reddy IIITH1-d.l.baseline.05 KB 0.498 0.432 ?0.021 0.463 ?0.026 0.344 ?0.038
18 Radu Ion RACAI-2MFS KB 0.433 0.431 ?0.022 0.434 ?0.027 0.399 ?0.049
19 Siva Reddy IIITH1-d.l.ppv.05 KB 0.426 0.425 ?0.026 0.434 ?0.028 0.399 ?0.043
20 Abhilash Inumella IIITH2-d.r.l.ppv.05 KB 0.424 0.422 ?0.023 0.456 ?0.025 0.325 ?0.044
21 Hansen A. Schwartz UCF-WS-domain.noPropers KB 0.437 0.392 ?0.025 0.377 ?0.025 0.434 ?0.043
22 Aitor Soroa kyoto-1 KB 0.384 0.384 ?0.022 0.382 ?0.024 0.391 ?0.047
23 Ruben Izquierdo BLC20Background S 0.380 0.380 ?0.022 0.385 ?0.026 0.366 ?0.037
24 Davide Buscaldi NLEL-WSD-PDB WS 0.381 0.356 ?0.022 0.357 ?0.027 0.352 ?0.049
25 Radu Ion RACAI-Lexical-Chains KB 0.351 0.350 ?0.015 0.344 ?0.017 0.368 ?0.030
26 Davide Buscaldi NLEL-WSD WS 0.370 0.345 ?0.022 0.352 ?0.027 0.328 ?0.037
27 Yoan Gutierrez Relevant Semantic Trees KB 0.328 0.322 ?0.022 0.335 ?0.026 0.284 ?0.044
28 Yoan Gutierrez Relevant Semantic Trees-2 KB 0.321 0.315 ?0.022 0.327 ?0.024 0.281 ?0.040
29 Yoan Gutierrez Relevant Cliques KB 0.312 0.303 ?0.021 0.304 ?0.024 0.301 ?0.041
- - Random baseline - 0.232 0.232 0.253 0.172
Chinese
Rank Participant System ID Type P R R nouns R verbs
- - Most Frequent Sense - 0.562 0.562 ?0.026 0.589 ?0.027 0.518 ?0.039
1 Meng-Hsien Shih HR KB 0.559 0.559 ?0.024 0.615 ?0.026 0.464 ?0.039
2 Meng-Hsien Shih GHR KB 0.517 0.517 ?0.024 0.533 ?0.035 0.491 ?0.038
- - Random baseline - 0.321 0.321 0.326 0.312
4 Aitor Soroa kyoto-3 KB 0.322 0.296 ?0.022 0.257 ?0.027 0.360 ?0.038
3 Aitor Soroa kyoto-2 KB 0.342 0.285 ?0.021 0.251 ?0.026 0.342 ?0.040
5 Aitor Soroa kyoto-1 KB 0.310 0.258 ?0.023 0.256 ?0.029 0.261 ?0.031
Dutch
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.526 0.526 ?0.022 0.575 ?0.029 0.450 ?0.034
2 Aitor Soroa kyoto-2 KB 0.519 0.519 ?0.022 0.561 ?0.027 0.454 ?0.034
- - Most Frequent Sense - 0.480 0.480 ?0.022 0.600 ?0.027 0.291 ?0.025
3 Aitor Soroa kyoto-1 KB 0.465 0.465 ?0.021 0.505 ?0.026 0.403 ?0.033
- - Random baseline - 0.328 0.328 0.350 0.293
Italian
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.529 0.529 ?0.021 0.530 ?0.024 0.528 ?0.038
2 Aitor Soroa kyoto-2 KB 0.521 0.521 ?0.018 0.522 ?0.023 0.519 ?0.035
3 Aitor Soroa kyoto-1 KB 0.496 0.496 ?0.019 0.507 ?0.020 0.468 ?0.037
- - Most Frequent Sense - 0.462 0.462 ?0.020 0.472 ?0.024 0.437 ?0.035
- - Random baseline - 0.294 0.294 0.308 0.257
Table 3: Overall results for the domain WSD datasets, ordered by recall.
This is the only group using hand-tagged data
from the target domain. Their best run ranked 1st.
IIITTH: They presented a personalized PageR-
ank algorithm over a graph constructed from
WordNet similar to (Agirre and Soroa, 2009),
with two variants. In the first (IIITH1), the vertices
of the graph are initialized following the rank-
ing scores obtained from predominant senses as in
(McCarthy et al, 2007). In the second (IIITH2),
the graph is initialized with keyness values as in
77
0.3 0.35 0.4 0.45 0.5 0.55
Rel. Cliques
Rel. Sem. Trees-2
Rel. Sem. Trees
NLEL-WSD
RACAI-Lexical-Chains
NLEL-WSD-PDB
BLC20BG
Kyoto-1
UCF-WS-domain.noPropers
IIITH2-d.r.l.ppv.05
IIITH1-d.l.ppv.05
RACAI-2MFS-BOW
IIITH1-d.l.baseline.05
IIITH2-d.r.l.baseline.05
UCF-WS-domain
HIT-CIR-DMFS
UCF-WS
RACAI-MFS
Treematch-3
Kyoto-2
Treematch-2
Treematch
CFILT-3
BLC20SC
BLC20SCBG
IIITH2-d.l.ppr.05
IIITH1-d.l.ppr.05
CFILT-1
CFILT-2
MFS
Figure 1: Plot for all the systems which participated in English domain WSD. Each point correspond
to one system (denoted in axis Y) according each recall and confidence interval (axis X ). Systems are
ordered depending on their rank.
(Rayson and Garside, 2000). Some of the runs
use sense statistics from SemCor, and have been
classified as weakly supervised. They submitted a
total of six runs, with the best run ranking 3rd.
BLC20(SC/BG/SCBG): This system is super-
vised. A Support Vector Machine was trained us-
ing the usual set of features extracted from con-
text and the most frequent class of the target word.
Semantic class-based classifiers were built from
SemCor (Izquierdo et al, 2009), where the classes
were automatically obtained exploiting the struc-
tural properties of WordNet. Their best run ranked
5th.
Treematch: This system uses a knowledge-
based disambiguation method that requires a dic-
tionary and untagged text as input. A previously
developed system (Chen et al, 2009) was adapted
to handle domain specific WSD. They built a
domain-specific corpus using words mined from
relevant web sites (e.g. WWF and ECNC) as
seeds. Once parsed the corpus, the used the de-
pendency knowledge to build a nodeset that was
used for WSD. The background documents pro-
vided by the organizers were only used to test how
exhaustive the initial seeds were. Their best run
ranked 8th.
Kyoto: This system participated in all four
languages, with a free reimplementation of
the domain-specific knowledge-based method for
WSD presented in (Agirre et al, 2009). It
uses a module to construct a distributional the-
saurus, which was run on the background text, and
a disambiguation module based on Personalized
PageRank over wordnet graphs. Different Word-
Net were used as the LKB depending on the lan-
guage. Their best run ranked 10th. Note that this
team includes some of the organizers of the task.
A strict separation was kept, in order to keep the
test dataset hidden from the actual developers of
the system.
RACAI: This participant submitted three differ-
ent knowledge-based systems. In the first, they use
the mapping to domains of WordNet (version 2.0)
in order to constraint the domains of the content
words of the test text. In the second, they choose
among senses using lexical chains (Ion and Ste-
fanescu, 2009). The third system combines the
previous two. Their best system ranked 12th.
HIT-CIR: They presented a knowledge-based
system which estimates predominant sense from
raw test. The predominant senses were calculated
with the frequency information in the provided
background text, and automatically constructed
78
thesauri from bilingual parallel corpora. The sys-
tem ranked 14.
UCFWS: This knowledge-based WSD system
was based on an algorithm originally described in
(Schwartz and Gomez, 2008), in which selectors
are acquired from the Web via searching with lo-
cal context of a given word. The sense is cho-
sen based on the similarity or relatedness between
the senses of the target word and various types
of selectors. In some runs they include predom-
inant senses(McCarthy et al, 2007). The best run
ranked 13th.
NLEL-WSD(-PDB): The system used for the
participation is based on an ensemble of different
methods using fuzzy-Borda voting. A similar sys-
tem was proposed in SemEval-2007 task-7 (Bus-
caldi and Rosso, 2007). In this case, the com-
ponent method used where the following ones:
1) Most Frequent Sense from SemCor; 2) Con-
ceptual Density ; 3) Supervised Domain Relative
Entropy classifier based on WordNet Domains;
4) Supervised Bayesian classifier based on Word-
Net Domains probabilities; and 5) Unsupervised
Knownet-20 classifiers. The best run ranked 24th.
UMCC-DLSI (Relevant): The team submitted
three different runs using a knowledge-based sys-
tem. The first two runs use domain vectors and
the third is based on cliques, which measure how
much a concept is correlated to the sentence by
obtaining Relevant Semantic Trees. Their best run
ranked 27th.
(G)HR: They presented a Knowledge-based
WSD system, which make use of two heuristic
rules (Li et al, 1995). The system enriched the
Chinese WordNet by adding semantic relations for
English domain specific words (e.g. ecology, en-
vironment). When in-domain senses are not avail-
able, the system relies on the first sense in the Chi-
nese WordNet. In addition, they also use sense
definitions. They only participated in the Chinese
task, with their best system ranking 1st.
4 Results
The evaluation has been carried out using the stan-
dard Senseval/SemEval scorer scorer2 as in-
cluded in the trial dataset, which computes preci-
sion and recall. Table 3 shows the results in each
dataset. Note that the main evaluation measure is
recall (R). In addition we also report precision (P)
and the recall for nouns and verbs. Recall mea-
sures are accompanied by a 95% confidence in-
terval calculated using bootstrap resampling pro-
cedure (Noreen, 1989). The difference between
two systems is deemed to be statistically signifi-
cant if there is no overlap between the confidence
intervals. We show graphically the results in Fig-
ure 1. For instance, the differences between the
highest scoring system and the following four sys-
tems are not statistically significant. Note that this
method of estimating statistical significance might
be more strict than other pairwise methods.
We also include the results of two baselines.
The random baseline was calculated analytically.
The first sense baseline for each language was
taken from each wordnet. The first sense baseline
in English and Chinese corresponds to the most
frequent sense, as estimated from out-of-domain
corpora. In Dutch and Italian, it followed the in-
tuitions of the lexicographer. Note that we don?t
have the most frequent sense baseline from the do-
main texts, which would surely show higher re-
sults (Koeling et al, 2005).
5 Conclusions
Domain portability and adaptation of NLP com-
ponents and Word Sense Disambiguation systems
present new challenges. The difficulties found by
supervised systems to adapt might change the way
we assess the strengths and weaknesses of super-
vised and knowledge-based WSD systems. With
this paper we have motivated the creation of an
all-words test dataset for WSD on the environ-
ment domain in several languages, and presented
the overall design of this SemEval task.
One of the goals of the exercise was to show
that WSD systems could make use of unannotated
background corpora to adapt to the domain and
improve their results. Although it?s early to reach
hard conclusions, the results show that in each of
the datasets, knowledge-based systems are able to
improve their results using background text, and
in two datasets the adaptation of knowledge-based
systems leads to results over the MFS baseline.
The evidence of domain adaptation of supervised
systems is weaker, as only one team tried, and the
differences with respect to MFS are very small.
The best results for English are obtained by a sys-
tem that combines a knowledge-based system with
some targeted hand-tagging. Regarding the tech-
niques used, graph-based methods over WordNet
and distributional thesaurus acquisition methods
have been used by several teams.
79
All datasets and related information are publicly
available from the task websites
6
.
Acknowledgments
We thank the collaboration of Lawrence Jones-Walters, Amor
Torre-Marin (ECNC) and Karin de Boom (WWF), com-
piling the test and background documents. This work
task is partially funded by the European Commission (KY-
OTO ICT-2007-211423), the Spanish Research Department
(KNOW-2 TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proceedings of the
12th Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL09), pages 33?
41. Association for Computational Linguistics.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
Davide Buscaldi and Paolo Rosso. 2007. Upv-wsd : Com-
bining different wsd methods by means of fuzzy borda
voting. In Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-2007), pages
434?437.
P. Chen, W. Ding, and D. Brown. 2009. A fully unsupervised
word sense disambiguation method and its evaluation on
coarse-grained all-words task. In Proceeding of the North
American Chapter of the Association for Computational
Linguistics (NAACL09).
Radu Ion and Dan Stefanescu. 2009. Unsupervised word
sense disambiguation with lexical chains and graph-based
context formalization. In Proceedings of the 4th Language
and Technology Conference: Human Language Technolo-
gies as a Challenge for Computer Science and Linguistics,
pages 190?194.
Rub?en Izquierdo, Armando Su?arez, and German Rigau.
2009. An empirical study on class-based word sense dis-
ambiguation. In EACL ?09: Proceedings of the 12th Con-
ference of the European Chapter of the Association for
Computational Linguistics, pages 389?397, Morristown,
NJ, USA. Association for Computational Linguistics.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense dis-
ambiguation combining corpus based and wordnet based
parameters. In Proceedings of the 5th International Con-
ference on Global Wordnet (GWC2010).
A. Kilgarriff. 2001. English Lexical Sample Task Descrip-
tion. In Proceedings of the Second International Work-
shop on evaluating Word Sense Disambiguation Systems,
Toulouse, France.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense acqui-
sition. In Proceedings of the Human Language Technol-
ogy Conference and Conference on Empirical Methods in
6
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
Natural Language Processing. HLT/EMNLP, pages 419?
426, Ann Arbor, Michigan.
Xiaobin Li, Stan Szpakowicz, and Stan Matwin. 1995. A
wordnet-based algorithm for word sense disambiguation.
In Proceedings of The 14th International Joint Conference
on Artificial Intelligence (IJCAI95).
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2007. Unsupervised acquisition of predominant
word senses. Computational Linguistics, 33(4).
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004. The
Senseval-3 English lexical sample task. In Proceedings of
the 3rd ACL workshop on the Evaluation of Systems for the
Semantic Analysis of Text (SENSEVAL), Barcelona, Spain.
Eric W. Noreen. 1989. Computer-Intensive Methods for Test-
ing Hypotheses. John Wiley & Sons.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of the
Fourth International Workshop on Semantic Evaluations
(SemEval-2007), pages 87?92, Prague, Czech Republic.
Paul Rayson and Roger Garside. 2000. Comparing corpora
using frequency profiling. In Proceedings of the workshop
on Comparing corpora, pages 1?6.
Hansen A. Schwartz and Fernando Gomez. 2008. Acquir-
ing knowledge from the web to be used as selectors for
noun sense disambiguation. In Proceedings of the Twelfth
Conference on Computational Natural Language Learn-
ing (CONLL08).
B. Snyder and M. Palmer. 2004. The English all-words task.
In Proceedings of the 3rd ACL workshop on the Evalua-
tion of Systems for the Semantic Analysis of Text (SENSE-
VAL), Barcelona, Spain.
M. Tesconi, F. Ronzano, S. Minutoli, C. Aliprandi, and
A. Marchetti. 2010. Kafnotator: a multilingual seman-
tic text annotation tool. In In Proceedings of the Second
International Conference on Global Interoperability for
Language Resources.
Piek Vossen, Eneko Agirre, Nicoletta Calzolari, Christiane
Fellbaum, Shu kai Hsieh, Chu-Ren Huang, Hitoshi Isa-
hara, Kyoko Kanzaki, Andrea Marchetti, Monica Mona-
chini, Federico Neri, Remo Raffaelli, German Rigau,
Maurizio Tescon, and Joop VanGent. 2008. Kyoto: a
system for mining, structuring and distributing knowl-
edge across languages and cultures. In Proceedings of the
Sixth International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European Lan-
guage Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
80
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 417?420,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
Kyoto: An Integrated System for Specific Domain WSD
Aitor Soroa, Eneko Agirre, Oier Lopez de Lacalle
University of the Basque Country
a.soroa@ehu.es
Monica Monachini
Istituto di Linguistica Computazionale
monica.monachini@ilc.cnr.it
Jessie Lo, Shu-Kai Hsieh
National Taiwan Normal University
shukai@ntnu.edu.tw
Wauter Bosma, Piek Vossen
Vrije Universiteit
{p.vossen,w.bosma}@let.vu.nl
Abstract
This document describes the prelimi-
nary release of the integrated Kyoto sys-
tem for specific domain WSD. The sys-
tem uses concept miners (Tybots) to ex-
tract domain-related terms and produces
a domain-related thesaurus, followed by
knowledge-based WSD based on word-
net graphs (UKB). The resulting system
can be applied to any language with a
lexical knowledge base, and is based on
publicly available software and resources.
Our participation in Semeval task #17 fo-
cused on producing running systems for
all languages in the task, and we attained
good results in all except Chinese. Due
to the pressure of the time-constraints in
the competition, the system is still under
development, and we expect results to im-
prove in the near future.
1 Introduction
In this paper we describe the participation of the
integrated Kyoto system on the ?SemEval-2010
task #17: All-words Word Sense Disambigua-
tion on a Specific Domain? task (Agirre et al,
2010). The goal of our participation was to eval-
uate the preliminary release of the integrated sys-
tem for specific domain WSD developed for the
Kyoto project
1
. Besides, we wanted to test the
performance of our domain specific WSD system
(Agirre et al, 2009) on this test set, and to inte-
grate the thesaurus construction software (Tybots)
developed for the project. The system can be run
for any language and domain if provided with a
lexical knowledge base and some background doc-
uments on the domain.
We will first present the components of our sys-
tem, followed by the experimental design and the
1
http://www.kyoto-project.eu
results. Finally, the conclusions are presented.
2 The Kyoto System for Domain Specific
WSD
We will present in turn UKB, the Tybots, and the
lexical knowledge-bases used.
2.1 UKB
UKB is a knowledge-based unsupervised WSD
system which exploits the structure of an under-
lying Language Knowledge Base (LKB) and finds
the most relevant concepts given an input con-
text (Agirre and Soroa, 2009). UKB starts by tak-
ing the LKB as a graph of concepts G = (V,E)
with a set of vertices V derived from LKB con-
cepts and a set of edges E representing relations
among them. Giving an input context, UKB ap-
plies the so called Personalized PageRank (Haveli-
wala, 2002) over it to obtain the most representa-
tive senses for the context.
PageRank (Brin and Page, 1998) is a method
for scoring the vertices V of a graph according
to each node?s structural importance. The algo-
rithm can be viewed as random walk process that
postulate the existence of a particle that randomly
traverses the graph, but at any time may jump to
a new vertex with a given damping factor (also
called teleport probability). After PageRank cal-
culation, the final weight of node i represents the
proportion of time that a random particle spends
visiting node i after a sufficiently long time. In
standard PageRank, the teleport vector is chosen
uniformly, whereas for Personalized PageRank it
is chosen from a nonuniform distribution of nodes,
specified by a teleport vector.
UKB concentrates the initial probability mass
of the teleport vector in the words occurring in
the context of the target word, causing all random
jumps on the walk to return to these words and
thus assigning a higher rank to the senses linked to
these words. Moreover, the high rank of the words
417
spreads through the links in the graph and make
all the nodes in its vicinity also receive high ranks.
Given a target word, the system checks which is
the relative ranking of its senses, and the WSD
system would output the one ranking highest.
UKB is very flexible and can be use to perform
WSD on different settings, depending on the con-
text used for disambiguating a word instance. In
this paper we use it to perform general and do-
main specific WSD, as shown in section 3. PageR-
ank is calculated by applying an iterative algo-
rithm until convergence below a given threshold
is achieved. Following usual practice, we used a
damping value of 0.85 and set the threshold value
at 0.001. We did not optimize these parameters.
2.2 Tybots
Tybots (Term Yielding Robots) are text mining
software that mine domain terms from corpus
(e.g. web pages), organizing them in a hierar-
chical structure, connecting them to wordnets and
ontologies to create a semantic model for the do-
main (Bosma and Vossen, 2010). The software is
freely available using Subversion
2
. Tybots try to
establish a view on the terminology of the domain
which is as complete as possible, discovering rela-
tions between terms and ranking terms by domain
relevance.
Preceding term extraction, we perform tok-
enization, part-of-speech tagging and lemmatiza-
tion, which is stored in Kyoto Annotation For-
mat (KAF) (Bosma et al, 2009). Tybots work
through KAF documents, acquire domain relevant
terms based on the syntactic features, gather co-
occurrence statistics to decide which terms are sig-
nificant in the domain and produce a thesaurus
with sets of related words. Section 3.3 describes
the specific settings that we used.
2.3 Lexical Knowledge bases
We used the following wordnets, as suggested by
the organizers:
WN30g: English WordNet 3.0 with gloss relations
(Fellbaum, 1998).
Dutch: The Dutch LKB is part of the Cor-
netto database version 1.3 (Vossen et al, 2008).
The Cornetto database can be obtained from
the Dutch/Flanders Taalunie
3
. Cornetto com-
prises taxonomic relations and equivalence rela-
2
http://kyoto.let.vu.nl/svn/kyoto/trunk
3
http://www.inl.nl/nl/lexica/780
#entries #synsets #rels. #WN30g
Monolingual
Chinese 8,186 14,243 20,433 20,584
Dutch 83,812 70,024 224,493 83,669
Italian 46,724 49,513 65,567 52,524
WN30g 147,306 117,522 525,351 n/a
Bilingual
Chinese-eng 8,186 141,561 566,368
Dutch-eng 83,812 188,511 833,513
Italian-eng 46,724 167,094 643,442
Table 1: Wordnets and their sizes (entries, synsets,
relations and links to WN30g).
tions from both WordNet 2.0 and 3.0. Cornetto
concepts are mapped to English WordNet 3.0.
Italian: Italwordnet (Roventini et al, 2003) was
created in the framework of the EuroWordNet,
employs the same set of semantic relations used
in EuroWordNet, and includes links to WordNet
3.0 synsets.
Chinese: The Chinese WordNet (Version 1.6) is
now partially open to the public
4
(Tsai et al,
2001). The Chinese WordNet is also mapped to
WordNet 3.0.
Table 1 shows the sizes of the graphs created
using each LKB as a source. The upper part shows
the number of lexical entries, synsets and relations
of each LKB. It also depicts the number of links to
English WordNet 3.0 synsets.
In addition, we also created bilingual graphs for
Dutch, Italian and Chinese, comprising the orig-
inal monolingual LKB, the links to WordNet 3.0
and WordNet 3.0 itself. We expected this richer
graphs to perform better performance. The sizes
of the bilingual graphs are shown in the lower side
of Table 1.
3 Experimental setting
All test documents were lemmatized and PoS-
tagged using the linguistic processors available
within the Kyoto project. In this section we de-
scribe the submitted runs.
3.1 UKB parameters
We use UKB with the default parameters. In par-
ticular, we don?t use dictionary weights, which in
the case of English come from annotated corpora.
This is done in order to make the system fully un-
supervised. It?s also worth mentioning that in the
default setting parts of speech were not used.
4
http://cwn.ling.sinica.edu.tw
418
RANK RUN P R R-NOUN R-VERB
Chinese
- 1sense 0.562 0.562 0.589 0.518
1 Best 0.559 0.559 - -
- Random 0.321 0.321 0.326 0.312
4 kyoto-3 0.322 0.296 0.257 0.360
3 kyoto-2 0.342 0.285 0.251 0.342
5 kyoto-1 0.310 0.258 0.256 0.261
Dutch
1 kyoto-3 0.526 0.526 0.575 0.450
2 kyoto-2 0.519 0.519 0.561 0.454
- 1sense 0.480 0.480 0.600 0.291
3 kyoto-1 0.465 0.465 0.505 0.403
- Random 0.328 0.328 0.350 0.293
English
1 Best 0.570 0.555 - -
- 1sense 0.505 0.505 0.519 0.454
10 kyoto-2 0.481 0.481 0.487 0.462
22 kyoto-1 0.384 0.384 0.382 0.391
- Random 0.232 0.232 0.253 0.172
Italian
1 kyoto-3 0.529 0.529 0.530 0.528
2 kyoto-2 0.521 0.521 0.522 0.519
3 kyoto-1 0.496 0.496 0.507 0.468
- 1sense 0.462 0.462 0.472 0.437
- Random 0.294 0.294 0.308 0.257
Table 2: Overall results of our runs, including pre-
cision (P) and recall (R), overall and for each PoS.
We include the First Sense (1sense) and random
baselines, as well as the best run, as provided by
the organizers.
3.2 Run1: UKB using context
The first run is an application of the UKB tool in
the standard setting, as described in (Agirre and
Soroa, 2009). Given the input text, we split it in
sentences, and we disambiguate each sentence at a
time. We extract the lemmas which have an entry
in the LKB and then apply Personalized PageR-
ank over all of them, obtaining a score for every
concept of the LKB. To disambiguate the words in
the sentence we just choose its associated concept
(sense) with maximum score.
In our experiments we build a context of at least
20 content words for each sentence to be disam-
biguated, taking the sentences immediately before
when necessary. UKB allows two main methods
of disambiguation, namely ppr and ppr w2w. We
used the latter method, as it has been shown to per-
form best.
In this setting we used the monolingual graphs
for each language (cf. section 2.3). Note that
in this run there is no domain adaptation, it thus
serves us as a baseline for assessing the benefits of
applying domain adaptation techniques.
3.3 Run2: UKB using related words
Instead of disambiguating words using their con-
text of occurrence, we follow the method de-
scribed in (Agirre et al, 2009). The idea is to first
obtain a list of related words for each of the tar-
get words, as collected from a domain corpus. On
a second step each target word is disambiguated
using the N most related words as context (see
below). For instance, in order to disambiguate
the word environment, we would not take into
account the context of occurrence (as in Section
3.2), but we would use the list of most related
words in the thesaurus (e.g. ?biodiversity, agri-
culture, ecosystem, nature, life, climate, . . .?). Us-
ing UKB over these contexts we obtain the most
predominant sense for each target word in the do-
main(McCarthy et al, 2007), which is used to la-
bel all occurrences of the target word in the test
dataset.
In order to build the thesaurus with the lists of
related words, we used Tybots (c.f. section 2.2),
one for each corpus of the evaluation dataset, i.e.
Chinese, Dutch, English, and Italian. We used the
background documents provided by the organiz-
ers, which we processed using the linguistic pro-
cessors of the project to obtain the documents in
KAF. We used the Tybots with the following set-
tings. We discarded co-occurring words with fre-
quencies below 10
5
. Distributional similarity was
computed using (Lin, 1998). Finally, we used up
to 50 related words for each target word.
As in run1, we used the monolingual graphs for
the LKBs in each language.
3.4 Run3: UKB using related words and
bilingual graphs
The third run is exactly the same as run2, except
that we used bilingual graphs instead of monolin-
gual ones for all languages other than English (cf.
section 2.3). There is no run3 for English.
4 Results
Table 2 shows the results of our system on the
different languages. We will analyze different as-
pects of the results in turn.
Domain adaptation: Using Personalized Pager-
ank over related words (run2 and run3) con-
sistently outperforms the standard setting (run1)
in all languages. This result is consistent with
5
In the case of Dutch we did not use any threshold due to
the small size of the background corpus.
419
our previous work on English (Agirre et al,
2009), and shows that domain adaptation works
for knowledge-based systems.
Monolingual vs. Bilingual graphs: As ex-
pected, we obtained better results using the bilin-
gual graphs (run3) than with monolingual graphs
(run2), showing that the English WordNet has a
richer set of relations, and that those relations can
be successfully ported to other languages. This
confirms that aligning different wordnets at the
synset level is highly beneficial.
Overall results: the results of our runs are highly
satisfactory. In two languages (Dutch and Ital-
ian) our best runs perform better than the first
sense baseline, which is typically hard to beat for
knowledge-based systems. In English, our system
performs close but below the first sense baseline,
and in Chinese our method performed below the
random baseline.
The poor results obtained for Chinese can be
due the LKB topology; an analysis over the graph
shows that it is formed by a large number of
small components, unrelated with each other. This
?flat? structure heavily penalizes the graph based
method, which is many times unable to discrimi-
nate among the concepts of a word. We are cur-
rently inspecting the results, and we don?t discard
bugs, due to the preliminary status of our software.
In particular, we need to re-examine the output of
the Tybot for Chinese.
5 Conclusions
This paper describes the results of the prelimi-
nary release of he integrated Kyoto system for do-
main specific WSD. It comprises Tybots to con-
struct a domain-related thesaurus, and UKB for
knowledge-based WSD based on wordnet graphs.
We applied our system to all languages in the
dataset, obtaining good results. In fact, our sys-
tem can be applied to any language with a lexical
knowledge base, and is based on publicly available
software and resources. We used the wordnets and
background texts provided by the organizers of the
task.
Our results show that we were succesful in
adapting our system to the domain, as we man-
aged to beat the first sense baseline in two lan-
guages. Our results also show that adding the En-
glish WordNet to the other language wordnets via
the available links is beneficial.
Our participation focused on producing running
systems for all languages in the task, and we at-
tained good results in all except Chinese. Due to
the pressure and the time-constraints in the com-
petition, the system is still under development. We
are currently revising our system for bugs and fine-
tuning it.
Acknowledgments
This work task is partially funded by the Eu-
ropean Commission (KYOTO ICT-2007-211423),
the Spanish Research Department (KNOW-2
TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
E. Agirre and A. Soroa. 2009. Personalizing pagerank for
word sense disambiguation. In Proceedings of EACL09,
pages 33?41. Association for Computational Linguistics.
E. Agirre, O. L?opez de Lacalle, and A. Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
E. Agirre, O. L?opez de Lacalle, C. Fellbaum, S.K. Hsieh,
M. Tesconi, M. Monachini, P. Vossen, and R. Segers.
2010. Semeval-2010 task 17: All-words word sense dis-
ambiguation on a specific domain. In Same volume.
W. E. Bosma and P. Vossen. 2010. Bootstrapping language
neutral term extraction. In Proceedings of LREC2010,
May.
W. E. Bosma, P. Vossen, A. Soroa, G. Rigau, M. Tesconi,
A. Marchetti, M. Monachini, and C. Aliprandi. 2009.
KAF: a generic semantic annotation format. In Proceed-
ings of the GL2009 Workshop on Semantic Annotation.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks and
ISDN Systems, 30(1-7).
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database. The MIT Press, Cambridge, MA.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW
?02: Proceedings of the 11th international conference on
WWW, pages 517?526, New York, NY, USA. ACM.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of ACL98, Montreal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2007.
Unsupervised acquisition of predominant word senses.
Computational Linguistics, 33(4).
A. Roventini, A. Alonge, F. Bertagna, N. Calzolari, J. Can-
cila, C. Girardi, B. Magnini, R. Marinelli, M. Speranza,
and A. Zampolli. 2003. Italwordnet: building a large
semantic database for the automatic treatment of Italian.
Linguistica Computazionale, Special Issue (XVIII-XIX),
pages 745?791.
B.S. Tsai, C.R. Huang, S.c. Tseng, J.Y. Lin, K.J. Chen, and
Y.S. Chuang. 2001. Definition and tests for lexical se-
mantic relations in Chinese. In Proceedings CLSW 2001.
P. Vossen, I. Maks, R. Segers, H. van der Vliet, and H. van
Zutphen. 2008. The cornetto database: the architecture
and alignment issues. In Proceedings GWC 2008, pages
485?506.
420
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 385?393,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity
Eneko Agirre
University of the Basque Country
Donostia, 20018, Basque Country
e.agirre@ehu.es
Daniel Cer
Stanford University
Stanford, CA 94305, USA
danielcer@stanford.edu
Mona Diab
Center for Computational Learning Systems
Columbia University
mdiab@ccls.columbia.edu
Aitor Gonzalez-Agirre
University of the Basque Country
Donostia, 20018, Basque Country
agonzalez278@ikasle.ehu.es
Abstract
Semantic Textual Similarity (STS) measures
the degree of semantic equivalence between
two texts. This paper presents the results of
the STS pilot task in Semeval. The training
data contained 2000 sentence pairs from pre-
viously existing paraphrase datasets and ma-
chine translation evaluation resources. The
test data also comprised 2000 sentences pairs
for those datasets, plus two surprise datasets
with 400 pairs from a different machine trans-
lation evaluation corpus and 750 pairs from a
lexical resource mapping exercise. The sim-
ilarity of pairs of sentences was rated on a
0-5 scale (low to high similarity) by human
judges using Amazon Mechanical Turk, with
high Pearson correlation scores, around 90%.
35 teams participated in the task, submitting
88 runs. The best results scored a Pearson
correlation>80%, well above a simple lexical
baseline that only scored a 31% correlation.
This pilot task opens an exciting way ahead,
although there are still open issues, specially
the evaluation metric.
1 Introduction
Semantic Textual Similarity (STS) measures the
degree of semantic equivalence between two sen-
tences. STS is related to both Textual Entailment
(TE) and Paraphrase (PARA). STS is more directly
applicable in a number of NLP tasks than TE and
PARA such as Machine Translation and evaluation,
Summarization, Machine Reading, Deep Question
Answering, etc. STS differs from TE in as much as
it assumes symmetric graded equivalence between
the pair of textual snippets. In the case of TE the
equivalence is directional, e.g. a car is a vehicle, but
a vehicle is not necessarily a car. Additionally, STS
differs from both TE and PARA in that, rather than
being a binary yes/no decision (e.g. a vehicle is not a
car), STS incorporates the notion of graded semantic
similarity (e.g. a vehicle and a car are more similar
than a wave and a car).
STS provides a unified framework that allows for
an extrinsic evaluation of multiple semantic compo-
nents that otherwise have tended to be evaluated in-
dependently and without broad characterization of
their impact on NLP applications. Such components
include word sense disambiguation and induction,
lexical substitution, semantic role labeling, multi-
word expression detection and handling, anaphora
and coreference resolution, time and date resolution,
named-entity handling, underspecification, hedging,
semantic scoping and discourse analysis. Though
not in the scope of the current pilot task, we plan to
explore building an open source toolkit for integrat-
ing and applying diverse linguistic analysis modules
to the STS task.
While the characterization of STS is still prelim-
inary, we observed that there was no comparable
existing dataset extensively annotated for pairwise
semantic sentence similarity. We approached the
construction of the first STS dataset with the fol-
lowing goals: (1) To set a definition of STS as a
graded notion which can be easily communicated to
non-expert annotators beyond the likert-scale; (2) To
gather a substantial amount of sentence pairs from
diverse datasets, and to annotate them with high
quality; (3) To explore evaluation measures for STS;
(4) To explore the relation of STS to PARA and Ma-
chine Translation Evaluation exercises.
385
In the next section we present the various sources
of the STS data and the annotation procedure used.
Section 4 investigates the evaluation of STS sys-
tems. Section 5 summarizes the resources and tools
used by participant systems. Finally, Section 6
draws the conclusions.
2 Source Datasets
Datasets for STS are scarce. Existing datasets in-
clude (Li et al, 2006) and (Lee et al, 2005). The
first dataset includes 65 sentence pairs which cor-
respond to the dictionary definitions for the 65
word pairs in Similarity(Rubenstein and Goode-
nough, 1965). The authors asked human informants
to assess the meaning of the sentence pairs on a
scale from 0.0 (minimum similarity) to 4.0 (maxi-
mum similarity). While the dataset is very relevant
to STS, it is too small to train, develop and test typ-
ical machine learning based systems. The second
dataset comprises 50 documents on news, ranging
from 51 to 126 words. Subjects were asked to judge
the similarity of document pairs on a five-point scale
(with 1.0 indicating ?highly unrelated? and 5.0 indi-
cating ?highly related?). This second dataset com-
prises a larger number of document pairs, but it goes
beyond sentence similarity into textual similarity.
When constructing our datasets, gathering natu-
rally occurring pairs of sentences with different de-
grees of semantic equivalence was a challenge in it-
self. If we took pairs of sentences at random, the
vast majority of them would be totally unrelated, and
only a very small fragment would show some sort of
semantic equivalence. Accordingly, we investigated
reusing a collection of existing datasets from tasks
that are related to STS.
We first studied the pairs of text from the Recog-
nizing TE challenge. The first editions of the chal-
lenge included pairs of sentences as the following:
T: The Christian Science Monitor named a US
journalist kidnapped in Iraq as freelancer Jill
Carroll.
H: Jill Carroll was abducted in Iraq.
The first sentence is the text, and the second is
the hypothesis. The organizers of the challenge an-
notated several pairs with a binary tag, indicating
whether the hypothesis could be entailed from the
text. Although these pairs of text are interesting we
decided to discard them from this pilot because the
length of the hypothesis was typically much shorter
than the text, and we did not want to bias the STS
task in this respect. We may, however, explore using
TE pairs for STS in the future.
Microsoft Research (MSR) has pioneered the ac-
quisition of paraphrases with two manually anno-
tated datasets. The first, called MSR Paraphrase
(MSRpar for short) has been widely used to evaluate
text similarity algorithms. It contains 5801 pairs of
sentences gleaned over a period of 18 months from
thousands of news sources on the web (Dolan et
al., 2004). 67% of the pairs were tagged as para-
phrases. The inter annotator agreement is between
82% and 84%. Complete meaning equivalence is
not required, and the annotation guidelines allowed
for some relaxation. The pairs which were anno-
tated as not being paraphrases ranged from com-
pletely unrelated semantically, to partially overlap-
ping, to those that were almost-but-not-quite seman-
tically equivalent. In this sense our graded annota-
tions enrich the dataset with more nuanced tags, as
we will see in the following section. We followed
the original split of 70% for training and 30% for
testing. A sample pair from the dataset follows:
The Senate Select Committee on Intelligence
is preparing a blistering report on prewar
intelligence on Iraq.
American intelligence leading up to the
war on Iraq will be criticized by a powerful
US Congressional committee due to report
soon, officials said today.
In order to construct a dataset which would reflect
a uniform distribution of similarity ranges, we sam-
pled the MSRpar dataset at certain ranks of string
similarity. We used the implementation readily ac-
cessible at CPAN1 of a well-known metric (Ukko-
nen, 1985). We sampled equal numbers of pairs
from five bands of similarity in the [0.4 .. 0.8] range
separately from the paraphrase and non-paraphrase
pairs. We sampled 1500 pairs overall, which we split
50% for training and 50% for testing.
The second dataset from MSR is the MSR Video
Paraphrase Corpus (MSRvid for short). The authors
showed brief video segments to Annotators from
Amazon Mechanical Turk (AMT) and were asked
1http://search.cpan.org/?mlehmann/
String-Similarity-1.04/Similarity.pm
386
Figure 1: Video and corresponding descriptions from
MSRvid
Figure 2: Definition and instructions for annotation
to provide a one-sentence description of the main ac-
tion or event in the video (Chen and Dolan, 2011).
Nearly 120 thousand sentences were collected for
2000 videos. The sentences can be taken to be
roughly parallel descriptions, and they included sen-
tences for many languages. Figure 1 shows a video
and corresponding descriptions.
The sampling procedure from this dataset is sim-
ilar to that for MSRpar. We construct two bags of
data to draw samples. The first includes all possible
pairs for the same video, and the second includes
pairs taken from different videos. Note that not all
sentences from the same video were equivalent, as
some descriptions were contradictory or unrelated.
Conversely, not all sentences coming from different
videos were necessarily unrelated, as many videos
were on similar topics. We took an equal number of
samples from each of these two sets, in an attempt to
provide a balanced dataset between equivalent and
non-equivalent pairs. The sampling was also done
according to string similarity, but in four bands in the
[0.5 .. 0.8] range, as sentences from the same video
had a usually higher string similarity than those in
the MSRpar dataset. We sampled 1500 pairs overall,
which we split 50% for training and 50% for testing.
Given the strong connection between STS sys-
tems and Machine Translation evaluation metrics,
we also sampled pairs of segments that had been
part of human evaluation exercises. Those pairs in-
cluded a reference translation and a automatic Ma-
chine Translation system submission, as follows:
The only instance in which no tax is levied is
when the supplier is in a non-EU country and
the recipient is in a Member State of the EU.
The only case for which no tax is still
perceived ?is an example of supply in the
European Community from a third country.
We selected pairs from the translation shared task
of the 2007 and 2008 ACL Workshops on Statistical
Machine Translation (WMT) (Callison-Burch et al,
2007; Callison-Burch et al, 2008). For consistency,
we only used French to English system submissions.
The training data includes all of the Europarl human
ranked fr-en system submissions from WMT 2007,
with each machine translation being paired with the
correct reference translation. This resulted in 729
unique training pairs. The test data is comprised of
all Europarl human evaluated fr-en pairs from WMT
2008 that contain 16 white space delimited tokens or
less.
In addition, we selected two other datasets that
were used as out-of-domain testing. One of them
comprised of all the human ranked fr-en system
submissions from the WMT 2007 news conversa-
tion test set, resulting in 351 unique system refer-
ence pairs.2 The second set is radically different as
it comprised 750 pairs of glosses from OntoNotes
4.0 (Hovy et al, 2006) and WordNet 3.1 (Fellbaum,
1998) senses. The mapping of the senses of both re-
sources comprised 110K sense pairs. The similarity
between the sense pairs was generated using simple
word overlap. 50% of the pairs were sampled from
senses which were deemed as equivalent senses, the
rest from senses which did not map to one another.
3 Annotation
In this first dataset we defined a straightforward lik-
ert scale ranging from 5 to 0, but we decided to pro-
vide definitions for each value in the scale (cf. Fig-
ure 2). We first did pilot annotations of 200 pairs se-
2At the time of the shared task, this data set contained dupli-
cates resulting in 399 sentence pairs.
387
lected at random from the three main datasets in the
training set. We did the annotation, and the pairwise
Pearson ranged from 84% to 87% among ourselves.
The agreement of each annotator with the average
scores of the other was between 87% and 89%.
In the future, we would like to explore whether
the definitions improve the consistency of the tag-
ging with respect to a likert scale without defini-
tions. Note also that in the assessment of the qual-
ity and evaluation of the systems performances, we
just took the resulting SS scores and their averages.
Using the qualitative descriptions for each score in
analysis and evaluation is left for future work.
Given the good results of the pilot we decided to
deploy the task in Amazon Mechanical Turk (AMT)
in order to crowd source the annotation task. The
turkers were required to have achieved a 95% of ap-
proval rating in their previous HITs, and had to pass
a qualification task which included 6 example pairs.
Each HIT included 5 pairs of sentences, and was
paid at 0.20$ each. We collected 5 annotations per
HIT. In the latest data collection, each HIT required
114.9 second for completion.
In order to ensure the quality, we also performed
post-hoc validation. Each HIT contained one pair
from our pilot. After the tagging was completed
we checked the correlation of each individual turker
with our scores, and removed annotations of turkers
which had low correlations (below 50%). Given the
high quality of the annotations among the turkers,
we could alternatively use the correlation between
the turkers itself to detect poor quality annotators.
4 Systems Evaluation
Given two sentences, s1 and s2, an STS system
would need to return a similarity score. Participants
can also provide a confidence score indicating their
confidence level for the result returned for each pair,
but this confidence is not used for the main results.
The output of the systems performance is evaluated
using the Pearson product-moment correlation co-
efficient between the system scores and the human
scores, as customary in text similarity (Rubenstein
and Goodenough, 1965). We calculated Pearson for
each evaluation dataset separately.
In order to have a single Pearson measure for each
system we concatenated the gold standard (and sys-
tem outputs) for all 5 datasets into a single gold stan-
dard file (and single system output). The first ver-
sion of the results were published using this method,
but the overall score did not correspond well to the
individual scores in the datasets, and participants
proposed two additional evaluation metrics, both of
them based on Pearson correlation. The organizers
of the task decided that it was more informative, and
on the benefit of the community, to also adopt those
evaluation metrics, and the idea of having a single
main evaluation metric was dropped. This decision
was not without controversy, but the organizers gave
more priority to openness and inclusiveness and to
the involvement of participants. The final result ta-
ble thus included three evaluation metrics. For the
future we plan to analyze the evaluation metrics, in-
cluding non-parametric metrics like Spearman.
4.1 Evaluation metrics
The first evaluation metric is the Pearson correla-
tion for the concatenation of all five datasets, as de-
scribed above. We will use overall Pearson or sim-
ply ALL to refer to this measure.
The second evaluation metric normalizes the out-
put for each dataset separately, using the linear least
squares method. We concatenated the system results
for five datasets and then computed a single Pear-
son correlation. Given Y = {yi} and X = {xi}
(the gold standard scores and the system scores,
respectively), we transform the system scores into
X ? = {x?i} in order to minimize the squared error?
i (yi ? x
?
i)
2. The linear transformation is given by
x?i = xi ? ?1 + ?2, where ?1 and ?2 are found an-
alytically. We refer to this measure as Normalized
Pearson or simply ALLnorm. This metric was sug-
gested by one of the participants, Sergio Jimenez.
The third evaluation metric is the weighted mean
of the Pearson correlations on individual datasets.
The Pearson returned for each dataset is weighted
according to the number of sentence pairs in that
dataset. Given ri the five Pearson scores for
each dataset, and ni the number of pairs in each
dataset, the weighted mean is given as
?
i=1..5(ri ?
ni)/
?
i=1..5 ni We refer to this measure as weighted
mean of Pearson or Mean for short.
4.2 Using confidence scores
Participants were allowed to include a confidence
score between 1 and 100 for each of their scores.
We used weighted Pearson to use those confidence
388
scores3. Table 2 includes the list of systems which
provided a non-uniform confidence. The results
show that some systems were able to improve their
correlation, showing promise for the usefulness of
confidence in applications.
4.3 The Baseline System
We produced scores using a simple word overlap
baseline system. We tokenized the input sentences
splitting at white spaces, and then represented each
sentence as a vector in the multidimensional to-
ken space. Each dimension had 1 if the token was
present in the sentence, 0 otherwise. Similarity of
vectors was computed using cosine similarity.
We also run a random baseline several times,
yielding close to 0 correlations in all datasets, as ex-
pected. We will refer to the random baseline again
in Section 4.5.
4.4 Participation
Participants could send a maximum of three system
runs. After downloading the test datasets, they had
a maximum of 120 hours to upload the results. 35
teams participated, submitting 88 system runs (cf.
first column of Table 1). Due to lack of space we
can?t detail the full names of authors and institutions
that participated. The interested reader can use the
name of the runs to find the relevant paper in these
proceedings.
There were several issues in the submissions. The
submission software did not ensure that the nam-
ing conventions were appropriately used, and this
caused some submissions to be missed, and in two
cases the results were wrongly assigned. Some par-
ticipants returned Not-a-Number as a score, and the
organizers had to request whether those where to be
taken as a 0 or as a 5.
Finally, one team submitted past the 120 hour
deadline and some teams sent missing files after the
deadline. All those are explicitly marked in Table 1.
The teams that included one of the organizers are
also explicitly marked. We want to stress that in
these teams the organizers did not allow the devel-
opers of the system to access any data or informa-
tion which was not available for the rest of partic-
ipants. One exception is weiwei, as they generated
3http://en.wikipedia.org/wiki/Pearson_
product-moment_correlation_coefficient#
Calculating_a_weighted_correlation
the 110K OntoNotes-WordNet dataset from which
the other organizers sampled the surprise data set.
After the submission deadline expired, the orga-
nizers published the gold standard in the task web-
site, in order to ensure a transparent evaluation pro-
cess.
4.5 Results
Table 1 shows the results for each run in alphabetic
order. Each result is followed by the rank of the sys-
tem according to the given evaluation measure. To
the right, the Pearson score for each dataset is given.
In boldface, the three best results in each column.
First of all we want to stress that the large majority
of the systems are well above the simple baseline,
although the baseline would rank 70 on the Mean
measure, improving over 19 runs.
The correlation for the non-MT datasets were re-
ally high: the highest correlation was obtained was
for MSRvid (0.88 r), followed by MSRpar (0.73 r)
and On-WN (0.73 r). The results for the MT evalu-
ation data are lower, (0.57 r) for SMT-eur and (0.61
r) for SMT-News. The simple token overlap base-
line, on the contrary, obtained the highest results
for On-WN (0.59 r), with (0.43 r) on MSRpar and
(0.40 r) on MSRvid. The results for MT evaluation
data are also reversed, with (0.40 r) for SMT-eur and
(0.45 r) for SMT-News.
The ALLnorm measure yields the highest corre-
lations. This comes at no surprise, as it involves a
normalization which transforms the system outputs
using the gold standard. In fact, a random base-
line which gets Pearson correlations close to 0 in all
datasets would attain Pearson of 0.58914.
Although not included in the results table for lack
of space, we also performed an analysis of confi-
dence intervals. For instance, the best run according
to ALL (r = .8239) has a 95% confidence interval of
[.8123,.8349] and the second a confidence interval
of [.8016,.8254], meaning that the differences are
not statistically different.
5 Tools and resources used
The organizers asked participants to submit a de-
scription file, special emphasis on the tools and re-
sources that they used. Table 3 shows in a simpli-
4We run the random baseline 10 times. The mean is reported
here. The standard deviation is 0.0005
389
Run ALL Rank ALLnrm Rank Mean Rank MSRpar MSRvid SMT-eur On-WN SMT-news
00-baseline/task6-baseline .3110 87 .6732 85 .4356 70 .4334 .2996 .4542 .5864 .3908
aca08ls/task6-University Of Sheffield-Hybrid .6485 34 .8238 15 .6100 18 .5166 .8187 .4859 .6676 .4280
aca08ls/task6-University Of Sheffield-Machine Learning .7241 17 .8169 18 .5750 38 .5166 .8187 .4859 .6390 .2089
aca08ls/task6-University Of Sheffield-Vector Space .6054 48 .7946 44 .5943 27 .5460 .7241 .4858 .6676 .4280
acaputo/task6-UNIBA-DEPRI .6141 46 .8027 38 .5891 31 .4542 .7673 .5126 .6593 .4636
acaputo/task6-UNIBA-LSARI .6221 44 .8079 30 .5728 40 .3886 .7908 .4679 .6826 .4238
acaputo/task6-UNIBA-RI .6285 41 .7951 43 .5651 45 .4128 .7612 .4531 .6306 .4887
baer/task6-UKP-run1 .8117 4 .8559 4 .6708 4 .6821 .8708 .5118 .6649 .4672
baer/task6-UKP-run2 plus postprocessing smt twsi .8239 1 .8579 2 .6773 1 .6830 .8739 .5280 .6641 .4937
baer/task6-UKP-run3 plus random .7790 8 .8166 19 .4320 71 .6830 .8739 .5280 -.0620 -.0520
croce/task6-UNITOR-1 REGRESSION BEST FEATURES .7474 13 .8292 12 .6316 10 .5695 .8217 .5168 .6591 .4713
croce/task6-UNITOR-2 REGRESSION ALL FEATURES .7475 12 .8297 11 .6323 9 .5763 .8217 .5102 .6591 .4713
croce/task6-UNITOR-3 REGRESSION ALL FEATURES ALL DOMAINS .6289 40 .8150 21 .5939 28 .4686 .8027 .4574 .6591 .4713
csjxu/task6-PolyUCOMP-RUN1 .6528 31 .7642 59 .5492 51 .4728 .6593 .4835 .6196 .4290
danielcer/stanford fsa? .6354 38 .7212 70 .4848 66 .3795 .5350 .4377 .6052 .4164
danielcer/stanford pdaAll? .4229 77 .7160 72 .5044 62 .4409 .4698 .4558 .6468 .4769
danielcer/stanford rte? .5589 55 .7807 55 .4674 67 .4374 .8037 .3533 .3077 .3235
davide buscaldi/task6-IRIT-pg1 .4280 76 .7379 65 .5009 63 .4295 .6125 .4952 .5387 .3614
davide buscaldi/task6-IRIT-pg3 .4813 68 .7569 61 .5202 58 .4171 .6728 .5179 .5526 .3693
davide buscaldi/task6-IRIT-wu .4064 81 .7287 69 .4898 65 .4326 .5833 .4856 .5317 .3480
demetrios glinos/task6-ATA-BASE .3454 83 .6990 81 .2772 87 .1684 .6256 .2244 .1648 .0988
demetrios glinos/task6-ATA-CHNK .4976 64 .7160 73 .3215 86 .2312 .6595 .1504 .2735 .1426
demetrios glinos/task6-ATA-STAT .4165 79 .7129 75 .3312 85 .1887 .6482 .2769 .2950 .1336
desouza/task6-FBK-run1 .5633 54 .7127 76 .3628 82 .2494 .6117 .1495 .4212 .2439
desouza/task6-FBK-run2 .6438 35 .8080 29 .5888 32 .5128 .7807 .3796 .6228 .5474
desouza/task6-FBK-run3 .6517 32 .8106 25 .6077 20 .5169 .7773 .4419 .6298 .6085
dvilarinoayala/task6-BUAP-RUN-1 .4997 63 .7568 62 .5260 56 .4037 .6532 .4521 .6050 .4537
dvilarinoayala/task6-BUAP-RUN-2 -.0260 89 .5933 89 .1016 89 .1109 .0057 .0348 .1788 .1964
dvilarinoayala/task6-BUAP-RUN-3 .6630 25 .7474 64 .5105 59 .4018 .6378 .4758 .5691 .4057
enrique/task6-UNED-H34measures .4381 75 .7518 63 .5577 48 .5328 .5788 .4785 .6692 .4465
enrique/task6-UNED-HallMeasures .2791 88 .6694 87 .4286 72 .3861 .2570 .4086 .6006 .5305
enrique/task6-UNED-SP INIST .4680 69 .7625 60 .5615 47 .5166 .6303 .4625 .6442 .4753
georgiana dinu/task6-SAARLAND-ALIGN VSSIM .4952 65 .7871 50 .5065 60 .4043 .7718 .2686 .5721 .3505
georgiana dinu/task6-SAARLAND-MIXT VSSIM .4548 71 .8258 13 .5662 43 .6310 .8312 .1391 .5966 .3806
jan snajder/task6-takelab-simple .8133 3 .8635 1 .6753 2 .7343 .8803 .4771 .6797 .3989
jan snajder/task6-takelab-syntax .8138 2 .8569 3 .6601 5 .6985 .8620 .3612 .7049 .4683
janardhan/task6-janardhan-UNL matching .3431 84 .6878 84 .3481 83 .1936 .5504 .3755 .2888 .3387
jhasneha/task6-Penn-ELReg .6622 27 .8048 34 .5654 44 .5480 .7844 .3513 .6040 .3607
jhasneha/task6-Penn-ERReg .6573 28 .8083 28 .5755 37 .5610 .7857 .3568 .6214 .3732
jhasneha/task6-Penn-LReg .6497 33 .8043 36 .5699 41 .5460 .7818 .3547 .5969 .4137
jotacastillo/task6-SAGAN-RUN1 .5522 57 .7904 47 .5906 29 .5659 .7113 .4739 .6542 .4253
jotacastillo/task6-SAGAN-RUN2 .6272 42 .8032 37 .5838 34 .5538 .7706 .4480 .6135 .3894
jotacastillo/task6-SAGAN-RUN3 .6311 39 .7943 45 .5649 46 .5394 .7560 .4181 .5904 .3746
Konstantin Z/task6-ABBYY-General .5636 53 .8052 33 .5759 36 .4797 .7821 .4576 .6488 .3682
M Rios/task6-UOW-LEX PARA .6397 36 .7187 71 .3825 80 .3628 .6426 .3074 .2806 .2082
M Rios/task6-UOW-LEX PARA SEM .5981 49 .6955 82 .3473 84 .3529 .5724 .3066 .2643 .1164
M Rios/task6-UOW-SEM .5361 59 .6287 88 .2567 88 .2995 .2910 .1611 .2571 .2212
mheilman/task6-ETS-PERP .7808 7 .8064 32 .6305 11 .6211 .7210 .4722 .7080 .5149
mheilman/task6-ETS-PERPphrases .7834 6 .8089 27 .6399 7 .6397 .7200 .4850 .7124 .5312
mheilman/task6-ETS-TERp .4477 73 .7291 68 .5253 57 .5049 .5217 .4748 .6169 .4566
nitish aggarwal/task6-aggarwal-run1? .5777 52 .8158 20 .5466 52 .3675 .8427 .3534 .6030 .4430
nitish aggarwal/task6-aggarwal-run2? .5833 51 .8183 17 .5683 42 .3720 .8330 .4238 .6513 .4499
nitish aggarwal/task6-aggarwal-run3 .4911 67 .7696 57 .5377 53 .5320 .6874 .4514 .5827 .2818
nmalandrakis/task6-DeepPurple-DeepPurple hierarchical .6228 43 .8100 26 .5979 23 .5984 .7717 .4292 .6480 .3702
nmalandrakis/task6-DeepPurple-DeepPurple sigmoid .5540 56 .7997 41 .5558 50 .5960 .7616 .2628 .6016 .3446
nmalandrakis/task6-DeepPurple-DeepPurple single .4918 66 .7646 58 .5061 61 .4989 .7092 .4437 .4879 .2441
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach? .3880 82 .6706 86 .4111 76 .3427 .3549 .4271 .5298 .4034
rada/task6-UNT-CombinedRegression .7418 14 .8406 7 .6159 14 .5032 .8695 .4797 .6715 .4033
rada/task6-UNT-IndividualDecTree .7677 9 .8389 9 .5947 25 .5693 .8688 .4203 .6491 .2256
rada/task6-UNT-IndividualRegression .7846 5 .8440 6 .6162 13 .5353 .8750 .4203 .6715 .4033
sbdlrhmn/task6-sbdlrhmn-Run1 .6663 23 .7842 53 .5376 54 .5440 .7335 .3830 .5860 .2445
sbdlrhmn/task6-sbdlrhmn-Run2 .4169 78 .7104 77 .4986 64 .4617 .4489 .4719 .6353 .4353
sgjimenezv/task6-SOFT-CARDINALITY .7331 15 .8526 5 .6708 3 .6405 .8562 .5152 .7109 .4833
sgjimenezv/task6-SOFT-CARDINALITY-ONE-FUNCTION .7107 19 .8397 8 .6486 6 .6316 .8237 .4320 .7109 .4833
siva/task6-DSS-alignheuristic .5253 60 .7962 42 .6030 21 .5735 .7123 .4781 .6984 .4177
siva/task6-DSS-average .5490 58 .8047 35 .5943 26 .5020 .7645 .4875 .6677 .4324
siva/task6-DSS-wordsim .5130 61 .7895 49 .5287 55 .3765 .7761 .4161 .5728 .3964
skamler /task6-EHU-RUN1v2?? .3129 86 .6935 83 .3889 79 .3605 .5187 .2259 .4098 .3465
sokolov/task6-LIMSI-cosprod .6392 37 .7344 67 .3940 78 .3948 .6597 .0143 .4157 .2889
sokolov/task6-LIMSI-gradtree .6789 22 .7377 66 .4118 75 .4848 .6636 .0934 .3706 .2455
sokolov/task6-LIMSI-sumdiff .6196 45 .7101 78 .4131 74 .4295 .5724 .2842 .3989 .2575
spirin2/task6-UIUC-MLNLP-Blend .4592 70 .7800 56 .5782 35 .6523 .6691 .3566 .6117 .4603
spirin2/task6-UIUC-MLNLP-CCM .7269 16 .8217 16 .6104 17 .5769 .8203 .4667 .5835 .4945
spirin2/task6-UIUC-MLNLP-Puzzle .3216 85 .7857 51 .4376 69 .5635 .8056 .0630 .2774 .2409
sranjans/task6-sranjans-1 .6529 30 .8018 39 .6249 12 .6124 .7240 .5581 .6703 .4533
sranjans/task6-sranjans-2 .6651 24 .8128 22 .6366 8 .6254 .7538 .5328 .6649 .5036
sranjans/task6-sranjans-3 .5045 62 .7846 52 .5905 30 .6167 .7061 .5666 .5664 .3968
tiantianzhu7/task6-tiantianzhu7-1 .4533 72 .7134 74 .4192 73 .4184 .5630 .2083 .4822 .2745
tiantianzhu7/task6-tiantianzhu7-2 .4157 80 .7099 79 .3960 77 .4260 .5628 .1546 .4552 .1923
tiantianzhu7/task6-tiantianzhu7-3 .4446 74 .7097 80 .3740 81 .3411 .5946 .1868 .4029 .1823
weiwei/task6-weiwei-run1?? .6946 20 .8303 10 .6081 19 .4106 .8351 .5128 .7273 .4383
yeh/task6-SRIUBC-SYSTEM1? .7513 11 .8017 40 .5997 22 .6084 .7458 .4688 .6315 .3994
yeh/task6-SRIUBC-SYSTEM2? .7562 10 .8111 24 .5858 33 .6050 .7939 .4294 .5871 .3366
yeh/task6-SRIUBC-SYSTEM3? .6876 21 .7812 54 .4668 68 .4791 .7901 .2159 .3843 .2801
ygutierrez/task6-UMCC DLSI-MultiLex .6630 26 .7922 46 .5560 49 .6022 .7709 .4435 .4327 .4264
ygutierrez/task6-UMCC DLSI-MultiSem .6529 29 .8115 23 .6116 16 .5269 .7756 .4688 .6539 .5470
ygutierrez/task6-UMCC DLSI-MultiSemLex .7213 18 .8239 14 .6158 15 .6205 .8104 .4325 .6256 .4340
yrkakde/task6-yrkakde-DiceWordnet .5977 50 .7902 48 .5742 39 .5294 .7470 .5531 .5698 .3659
yrkakde/task6-yrkakde-JaccNERPenalty .6067 47 .8078 31 .5955 24 .5757 .7765 .4989 .6257 .3468
Table 1: The first row corresponds to the baseline. ALL for overall Pearson, ALLnorm for Pearson after normaliza-
tion, and Mean for mean of Pearsons. We also show the ranks for each measure. Rightmost columns show Pearson for
each individual dataset. Note: ? system submitted past the 120 hour window, ? post-deadline fixes, ? team involving
one of the organizers.
390
Run ALL ALLw MSRpar MSRparw MSRvid MSRvidw SMT-eur SMT-eurw On-WN On-WNw SMT-news SMT-newsw
davide buscaldi/task6-IRIT-pg1 .4280 .4946 .4295 .4082 .6125 .6593 .4952 .5273 .5387 .5574 .3614 .4674
davide buscaldi/task6-IRIT-pg3 .4813 .5503 .4171 .4033 .6728 .7048 .5179 .5529 .5526 .5950 .3693 .4648
davide buscaldi/task6-IRIT-wu .4064 .4682 .4326 .4035 .5833 .6253 .4856 .5138 .5317 .5189 .3480 .4482
enrique/task6-UNED-H34measures .4381 .2615 .5328 .4494 .5788 .4913 .4785 .4660 .6692 .6440 .4465 .3632
enrique/task6-UNED-HallMeasures .2791 .2002 .3861 .3802 .2570 .2343 .4086 .4212 .6006 .5947 .5305 .4858
enrique/task6-UNED-SP INIST .4680 .3754 .5166 .5082 .6303 .5588 .4625 .4801 .6442 .5761 .4753 .4143
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach .3880 .3636 .3427 .3498 .3549 .3353 .4271 .3989 .5298 .4619 .4034 .3228
tiantianzhu7/task6-tiantianzhu7-1 .4533 .5442 .4184 .4241 .5630 .5630 .2083 .4220 .4822 .5031 .2745 .3536
tiantianzhu7/task6-tiantianzhu7-2 .4157 .5249 .4260 .4340 .5628 .5758 .1546 .4776 .4552 .4926 .1923 .3362
tiantianzhu7/task6-tiantianzhu7-3 .4446 .5229 .3411 .3611 .5946 .5899 .1868 .4769 .4029 .4365 .1823 .4014
Table 2: Results according to weighted correlation for the systems that provided non-uniform confidence alongside
their scores.
fied way the tools and resources used by those par-
ticipants that did submit a valid description file. In
the last row, the totals show that WordNet was the
most used resource, followed by monolingual cor-
pora and Wikipedia. Acronyms, dictionaries, mul-
tilingual corpora, stopword lists and tables of para-
phrases were also used.
Generic NLP tools like lemmatization and PoS
tagging were widely used, and to a lesser extent,
parsing, word sense disambiguation, semantic role
labeling and time and date resolution (in this or-
der). Knowledge-based and distributional methods
got used nearly equally, and to a lesser extent, align-
ment and/or statistical machine translation software,
lexical substitution, string similarity, textual entail-
ment and machine translation evaluation software.
Machine learning was widely used to combine and
tune components. Several less used tools were also
listed but were used by three or less systems.
The top scoring systems tended to use most of
the resources and tools listed (UKP, Takelab), with
some notable exceptions like Sgjimenez which was
based on string similarity. For a more detailed anal-
ysis, the reader is directed to the papers of the par-
ticipants in this volume.
6 Conclusions and Future Work
This paper presents the SemEval 2012 pilot eval-
uation exercise on Semantic Textual Similarity. A
simple definition of STS beyond the likert-scale was
set up, and a wealth of annotated data was pro-
duced. The similarity of pairs of sentences was
rated on a 0-5 scale (low to high similarity) by hu-
man judges using Amazon Mechanical Turk. The
dataset includes 1500 sentence pairs from MSRpar
and MSRvid (each), ca. 1500 pairs from WMT,
and 750 sentence pairs from a mapping between
OntoNotes and WordNet senses. The correlation be-
tween non-expert annotators and annotations from
the authors is very high, showing the high quality of
the dataset. The dataset was split 50% as train and
test, with the exception of the surprise test datasets:
a subset of WMT from a different domain and the
OntoNotes-WordNet mapping. All datasets are pub-
licly available.5
The exercise was very successful in participation
and results. 35 teams participated, submitting 88
runs. The best results scored a Pearson correlation
over 80%, well beyond a simple lexical baseline
with 31% of correlation. The metric for evaluation
was not completely satisfactory, and three evalua-
tion metrics were finally published. We discuss the
shortcomings of those measures.
There are several tasks ahead in order to make
STS a mature field. The first is to find a satisfac-
tory evaluation metric. The second is to analyze the
definition of the task itself, with a thorough analysis
of the definitions in the likert scale.
We would also like to analyze the relation be-
tween the STS scores and the paraphrase judgements
in MSR, as well as the human evaluations in WMT.
Finally, we would also like to set up an open frame-
work where NLP components and similarity algo-
rithms can be combined by the community. All in
all, we would like this dataset to be the focus of the
community working on algorithmic approaches for
semantic processing and inference at large.
Acknowledgements
We would like to thank all participants, specially (in al-
phabetic order) Yoan Gutierrez, Michael Heilman, Ser-
gio Jimenez, Nitin Madnami, Diana McCarthy and Shru-
tiranjan Satpathy for their contributions on evaluation
metrics. Eneko Agirre was partially funded by the
5http://www.cs.york.ac.uk/semeval-2012/
task6/
391
A
cr
on
ym
s
D
ic
ti
on
ar
ie
s
D
is
tr
ib
ut
io
na
lt
he
sa
ur
us
M
on
ol
in
gu
al
co
rp
or
a
M
ul
ti
li
ng
ua
lc
or
po
ra
S
to
p
w
or
ds
Ta
bl
es
of
pa
ra
ph
ra
se
s
W
ik
ip
ed
ia
W
or
dN
et
A
li
gn
m
en
t
D
is
tr
ib
ut
io
na
ls
im
il
ar
it
y
K
B
S
im
il
ar
it
y
L
em
m
at
iz
er
L
ex
ic
al
S
ub
st
it
ut
io
n
M
ac
hi
ne
L
ea
rn
in
g
M
T
ev
al
ua
ti
on
M
W
E
N
am
ed
E
nt
it
y
re
co
gn
it
io
n
P
O
S
ta
gg
er
S
em
an
ti
c
R
ol
e
L
ab
el
in
g
S
M
T
S
tr
in
g
si
m
il
ar
it
y
S
yn
ta
x
Te
xt
ua
le
nt
ai
lm
en
t
T
im
e
an
d
da
te
re
so
lu
ti
on
W
or
d
S
en
se
D
is
am
bi
gu
at
io
n
O
th
er
aca08ls/task6-University Of Sheffield-Hybrid x x x x x x x
aca08ls/task6-University Of Sheffield-Machine Learning x x x x x x x
aca08ls/task6-University Of Sheffield-Vector Space x x x x x
baer/task6-UKP-run1 x x x x x x x x x x x x x x
baer/task6-UKP-run2 plus postprocessing smt twsi x x x x x x x x x x x x x x
baer/task6-UKP-run3 plus random x x x x x x x x x x x x x x
croce/task6-UNITOR-1 REGRESSION BEST FEATURES x x x x x x
croce/task6-UNITOR-2 REGRESSION ALL FEATURES x x x x x x
croce/task6-UNITOR-3 REGRESSION ALL FEATURES ALL DOMAINS x x x x x x
csjxu/task6-PolyUCOMP-RUN x x x x
danielcer/stanford fsa x x x x x x x
danielcer/stanford pdaAll x x x x x x x
danielcer/stanford rte x x x x x x x x
davide buscaldi/task6-IRIT-pg1 x x x x x
davide buscaldi/task6-IRIT-pg3 x x x x x
davide buscaldi/task6-IRIT-wu x x x x x
demetrios glinos/task6-ATA-BASE x x x x x x x
demetrios glinos/task6-ATA-CHNK x x x x x x x
demetrios glinos/task6-ATA-STAT x x x x x x x
desouza/task6-FBK-run1 x x x x x x x x x x x x x
desouza/task6-FBK-run2 x x x x x x x x
desouza/task6-FBK-run3 x x x x x x
dvilarinoayala/task6-BUAP-RUN-1 x x
dvilarinoayala/task6-BUAP-RUN-2 x
dvilarinoayala/task6-BUAP-RUN-3 x x
jan snajder/task6-takelab-simple x x x x x x x x x x x x x
jan snajder/task6-takelab-syntax x x x x x x x x x
janardhan/task6-janardhan-UNL matching x x x x x x
jotacastillo/task6-SAGAN-RUN1 x x x x x x x x
jotacastillo/task6-SAGAN-RUN2 x x x x x x x x
jotacastillo/task6-SAGAN-RUN3 x x x x x x x x
Konstantin Z/task6-ABBYY-General
M Rios/task6-UOW-LEX PARA x x x x x x x x
M Rios/task6-UOW-LEX PARA SEM x x x x x x x x
M Rios/task6-UOW-SEM x x x x x x x
mheilman/task6-ETS-PERP x x x x x x x
mheilman/task6-ETS-PERPphrases x x x x x x x x x
mheilman/task6-ETS-TERp x x x x x x x
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach x x x x x x x x x x
rada/task6-UNT-CombinedRegression x x x x x x x x x
rada/task6-UNT-IndividualDecTree x x x x x x x x x
rada/task6-UNT-IndividualRegression x x x x x x x x x
sgjimenezv/task6-SOFT-CARDINALITY x x x
sgjimenezv/task6-SOFT-CARDINALITY-ONE-FUNCTION x x x
skamler /task6-EHU-RUN1v2 x x x x x
sokolov/task6-LIMSI-cosprod x x x x
sokolov/task6-LIMSI-gradtree x x x x
sokolov/task6-LIMSI-sumdiff x x x x
spirin2/task6-UIUC-MLNLP-Blend x x x x x x x x x x x
spirin2/task6-UIUC-MLNLP-CCM x x x x x x x x x x x
spirin2/task6-UIUC-MLNLP-Puzzle x x x x x x x x x x x
sranjans/task6-sranjans-1 x x x x x x x x
sranjans/task6-sranjans-2 x x x x x x x x x x x
sranjans/task6-sranjans-3 x x x x x x x x x x x
tiantianzhu7/task6-tiantianzhu7-1 x x x x
tiantianzhu7/task6-tiantianzhu7-2 x x x
tiantianzhu7/task6-tiantianzhu7-3 x x x x
weiwei/task6-weiwei-run1 x x x x x x
yeh/task6-SRIUBC-SYSTEM1 x x x x x x x
yeh/task6-SRIUBC-SYSTEM2 x x x x x x x
yeh/task6-SRIUBC-SYSTEM3 x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiLex x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiSem x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiSemLex x x x x x x x x
yrkakde/task6-yrkakde-DiceWordnet x x x
Total 8 6 10 33 5 5 9 20 47 7 31 37 49 13 13 4 7 12 43 9 4 13 17 10 5 15 25
Table 3: Resources and tools used by the systems that submitted a description file. Leftmost columns correspond to
the resources, and rightmost to tools, in alphabetic order.
392
European Communitys Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 270082
(PATHS project) and the Ministry of Economy under
grant TIN2009-14715-C04-01 (KNOW2 project). Daniel
Cer gratefully acknowledges the support of the Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181 and
the support of the DARPA Broad Operational Language
Translation (BOLT) program through IBM. The STS an-
notations were funded by an extension to DARPA GALE
subcontract to IBM # W0853748 4911021461.0 to Mona
Diab. Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of the
author(s) and do not necessarily reflect the view of the
DARPA, AFRL, or the US government.
References
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, StatMT ?07, pages 136?158.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, StatMT ?08, pages 70?106.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meetings of the Asso-
ciation for Computational Linguistics (ACL).
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In COLING
04: Proceedings of the 20th international conference
on Computational Linguistics, page 350.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
Michael D. Lee, Brandon Pincombe, and Matthew Welsh.
2005. An empirical evaluation of models of text doc-
ument similarity. In Proceedings of the 27th Annual
Conference of the Cognitive Science Society, pages
1254?1259, Mahwah, NJ.
Y. Li, D. McLean, Z. A. Bandar, J. D. O?Shea, and
K. Crockett. 2006. Sentence similarity based on se-
mantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138?
1150, August.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627?633, October.
E. Ukkonen. 1985. Algorithms for approximate string
matching. Information and Contro, 64:110?118.
393
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 617?623,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SRIUBC: Simple Similarity Features for Semantic Textual Similarity
Eric Yeh
SRI International
Menlo Park, CA USA
yeh@ai.sri.com
Eneko Agirre
University of the Basque Country
Donostia, Basque Country
e.agirre@ehu.es
Abstract
We describe the systems submitted by SRI In-
ternational and the University of the Basque
Country for the Semantic Textual Similarity
(STS) SemEval-2012 task. Our systems fo-
cused on using a simple set of features, fea-
turing a mix of semantic similarity resources,
lexical match heuristics, and part of speech
(POS) information. We also incorporate pre-
cision focused scores over lexical and POS in-
formation derived from the BLEU measure,
and lexical and POS features computed over
split-bigrams from the ROUGE-S measure.
These were used to train support vector re-
gressors over the pairs in the training data.
From the three systems we submitted, two per-
formed well in the overall ranking, with split-
bigrams improving performance over pairs
drawn from the MSR Research Video De-
scription Corpus. Our third system maintained
three separate regressors, each trained specif-
ically for the STS dataset they were drawn
from. It used a multinomial classifier to pre-
dict which dataset regressor would be most ap-
propriate to score a given pair, and used it to
score that pair. This system underperformed,
primarily due to errors in the dataset predictor.
1 Introduction
Previous semantic similarity tasks, such as para-
phrase identification or recognizing textual entail-
ment, have focused on performing binary decisions.
These problems are usually framed in terms of iden-
tifying whether a pair of texts exhibit the needed
similarity or entailment relationship or not. In many
cases, such as producing a ranking over similarity
scores, a soft measure of similarity between a pair
of texts would be more desirable.
We contributed three systems for the 2012 Se-
mantic Textual Similarity (STS) task (Agirre et al,
2012). These are:
1. System 1, which used a combination of seman-
tic similarity, lexical similarity, and precision
focused part-of-speech (POS) features.
2. System 2, which used features from System
1, with the addition of skip-bigram features
derived from the ROUGE-S (Lin, 2004) mea-
sure. POS variants of skip-bigrams were incor-
porated as well.
3. System 3, used the features from above to first
classify the dataset the pair was drawn from,
and then applied regressors trained for that
dataset.
Our systems characterize sentence pairs as feature
vectors, populated by a variety of scorers that will be
described below. During training, we used support
vector regression (SVR) to train regressors against
these vectors and their associated similarity scores.
The STS training data is divided into three
datasets, reflecting their origin: Microsoft Research
Paraphrase Corpus (MSRpar), MSR Research Video
Description Corpus (MSRvid), and WMT2008 De-
velopment dataset (SMTeuroparl). We trained indi-
vidual regressors for each of these datasets, and ap-
plied them to their counterparts in the testing set.
Both Systems 1 and 2 used the following types of
features:
617
1. Resource based word to word semantic similar-
ities.
2. Cosine-based lexical similarity measure.
3. Bilingual Evaluation Understudy (BLEU) (Pa-
pineni et al, 2002) lexical overlap.
4. Precision focused Part of Speech (POS) fea-
tures.
System 2 added the following features:
1. Lexically motivated skip-bigram overlap.
2. Precision focused skip-bigram POS features.
One of the primary motivations for our the choice
of features was to use relatively simple and fast fea-
tures, which can be scaled up to large datasets, given
appropriate caching and pre-generated lookups. As
the test phase included surprise datasets, whose ori-
gin was not disclosed, we also trained a fourth model
using all of the training data from all three datasets.
Systems 1 and 2 employed this strategy for the sur-
prise data.
Since the statistics for each of the training datasets
varied, directly pooling them together may not be
the best strategy when scoring the surprise data,
whose origins were unknown. To account for this,
System 3 treated this as a gated regression problem,
where pairs are considered to originate strictly from
one dataset, and to score using a model specifically
tailored for that dataset. We first trained regressors
on each of the datasets separately. Then we trained
a classifier to predict which dataset a given pair is
likeliest to have been drawn from, and then applied
the matching trained regressor to obtain its score.
This team included one of the organizers. We
want to stress that we took all measures to make our
participation on the same conditions as the rest of
participants. In particular, the organizer did not al-
low the other member of the team to access any data
or information which was not already available for
the rest of participants.
For the rest of this system description, we first
outline the scorers used to populate the feature vec-
tors used for Systems 1 and 2. We then describe
the setup for performing the regression. We follow
with an explanation of our strategies for dealing with
the surprise data, including a description of System
3. We then summarize performance over the the
datasets, and discuss future avenues of investigation.
2 Resource Based Similarity
Our system uses several resources for assessing the
word to word similarity between a pair of sentences.
In order to pool together the similarity scores for a
given pair, we employed the Semantic Matrix (Fer-
nando and Stevenson, 2008) framework. To gen-
erate the scores, we used several resources, princi-
pally those derived from the relation graph of Word-
Net (Fellbaum, 1998), and those derived from distri-
butional resources, namely Explicit Semantic Anal-
ysis (Gabrilovich and Markovitch, 2009), and the
Dekang Lin Proximity-based Thesaurus 1. We now
describe the Semantic Matrix method, and follow
with descriptions of each of the resources used.
2.1 Semantic Matrix
The Semantic Matrix is a method for pooling all
of the pairwise similarity scores between the to-
kens found in two input strings. In order to score
the similarity between a pair of strings s1 and s2
we first identify all of the unique vocabulary words
from these strings to derive their corresponding oc-
currence vectors v1 and v2. Each dimension of
these vectors corresponds to a unique vocabulary
word, and binary values were used, corresponding
to whether that word was observed. The similarity
score for pair, sim(s1, s2), is given by Formula 1.
sim(s1, s2) =
vT1 Wv2
?v1? ?v2?
(1)
with W being the symmetric matrix marking the
similarity between pairs of words in the vocabulary.
We note that this is similar to the Mahalanobis dis-
tance, except adjusted to produce a similarity. For
this experiment, we normalized matrix entries so all
values lay in the 0-1 range.
As named entities and other words encountered
may not appear in one or more of the resources used,
we applied the identity to W. This is equivalent to
adding a strict lexical match fallback on top of the
similarity measure.
1http://webdocs.cs.ualberta.ca/ lindek/downloads.htm
618
Per (Fernando and Stevenson, 2008), a filter was
applied over the values of W. Any entries that fell
below a given threshold value were flattened to zero,
in order to prevent low scoring similarities from
overwhelming the score. From previous studies over
MSRpar, we applied a threshold of 0.9.
For our experiments, each of the word to word
similarity scorers described below were used to gen-
erate a corresponding word similarity matrix W,
with scores generated using the Semantic Matrix.
2.2 WordNet Similarity
We used several methods to obtain word to word
similarities from WordNet. WordNet is a lexical-
semantic resource that describes typed relationships
between synsets, semantic categories a word may
belong to. Similarity scoring methods identify the
synsets associated with a pair of words, and then use
this relationship graph to generate a score.
The first set of scorers were generated from the
Leacock-Chodorow, Lin, and Wu-Palmer measures
from the WordNet Similarity package (Pedersen et
al., 2004). For each of these measures, we averaged
across all of the possible synsets between a given
pair of words.
Another scorer we used was Personalized PageR-
ank (PPR) (Agirre et al, 2010), a topic sensitive
variant of the PageRank algorithm (Page et al,
1999) that uses a random walk process to identify
the significant nodes of a graph given its link struc-
ture. We first derived a graph G from WordNet,
treating synsets as the vertices and the relationships
between synsets as the edges. To obtain a signature
for a given word, we apply topic sensitive PageRank
(Haveliwala, 2002) over G, using the synsets asso-
ciated with the word as the initial distribution. At
convergence, we convert the stationary distribution
into a vector. The similarity between two words is
the cosine similarity between their vectors.
2.3 Distributional Resources
In contrast with the structure based WordNet based
methods, distributional methods use statistical prop-
erties of corpora to derive similarity scores. We gen-
erated two scorers, one based on Explicit Seman-
tic Analysis (ESA), and the other on the Dekang
Lin Proximity-based Thesaurus. For a given word,
ESA generates a concept vector, where the con-
cepts are Wikipedia articles, and the score measures
how closely associated that word is with the textual
content of the article. To score the similarity be-
tween two words, we computed the cosine similar-
ity of their concept vectors. This method proved to
give state-of-the-art performance on the WordSim-
353 word pair relatedness dataset (Finkelstein et al,
2002).
The Lin Proximity-based Thesaurus identifies
the neighborhood around words encountered in the
Reuters and Text Retrieval Conference (TREC). For
a given word, the Thesaurus identifies the top 200
words with the most similar neighborhoods, listing
the score based on these matches. For our experi-
ments, we treated these as feature vectors, with the
intuition being similar words should share similar
neighbors. Again, the similarity score between two
words was scored using the cosine similarity of their
vectors.
3 Cosine Similarity
Another scorer we used was the cosine similarity
over the lemmas found in the sentences in a pair.
For generating the vectors used in the cosine simi-
larity computation, we used the term frequency of
the lemmas.
4 BLEU Features
BLEU is a measure developed to automatically as-
sess how closely sentences generated by machine
translation systems match reference human gener-
ated texts. BLEU is a directional measurement, and
works on the assumption that the more lexically sim-
ilar a system generated sentence is to a reference sen-
tence, a human generated translation, the better the
system sentence is. This can also be seen as a stand-
in for the semantic similarity of the pairs, as was
shown when BLEU was applied to the paraphrase
identification identification problem in (Finch et al,
2005).
The BLEU score for a given system sentence and
reference sentence of order N is computed using
Formula 2.
BLEU(sys, ref) = B ? exp
N?
n=1
1
N
log(pn) (2)
619
B is a brevity penalty used to prevent degenerate
translations. Given this has little bearing on our ex-
periments, we set its value to 1 for our experiments.
Following (Papineni et al, 2002), we give each order
n equal weight in the geometric mean. The proba-
bility of an order n-gram from the system sentence
being found in the reference, pn, is given in Formula
3.
pn =
?
ngram?sys countsys?ref (ngram)
?
ngram?sys countsys(ngram)
(3)
countsys(ngram) is frequency of oc-
currence for the given n-gram in the sys-
tem sentence. The numerator term is
computed as countsys?ref (ngram) =
min(countsys(ngram), countref (ngram)) where
countref (ngram) is the frequency of occurrence
of that n-gram in the reference sentence. This
is equivalent to having each n-gram have a 1-1
mapping with a matching n-gram in the reference
(if any), and counting the number of mappings.
As there is a risk of higher order system n-grams
having no matches in the reference, we apply Lapla-
cian smoothing to the n-gram counts.
BLEU is considered to be a precision focused
measure, as it only measures how much of the sys-
tem sentence matches a reference sentence. Follow-
ing (Finch et al, 2005), we obtain a modified BLEU
score for strings s1 and s2 of a pair by averaging the
BLEU scores where each takes a turn as the system
sentence, as given in Formula 4.
Score(s1, s2) =
1
2
BLEU(s1, s2) ? BLEU(s2, s1)
(4)
For our experiments, we used BLEU scores of or-
der N = 1..4, over n-grams formed over the sen-
tence lemmas, and used these as features for charac-
terizing a given pair.
4.1 Precision Focused POS Features
From past experiments with paraphrase identifica-
tion over the MSR Paraphrase Corpus, we have
found including POS information to be beneficial.
To this capture this kind of information, we gen-
erated precision focused POS features, which mea-
sures the following between the sentences in a prob-
lem pair:
1. The overlap in POS tags.
2. The mismatch in POS tags.
We follow the formulation for POS vectors given
in (Finch et al, 2005). For a given sentence pair,
we identify the set of words whose lemmas were
matched in both the system and reference sentences,
Wmatch and those with no matches, Wmiss. Using
the directional notion of system and reference sen-
tences from BLEU, for each word w ?Wmatch,
POSMatch(t, sys, ref) =
?
w?Wmatch
countt(w)
|sys|
(5)
where countt is 1 if wordw has the matching POS
tag, and 0 otherwise. |sys| is the token count of the
system sentence. This is deemed to be precision-
focused, as this computation is done over candidates
found in the system sentence.
To generate the score for missing POS tags, we
perform a similar computation,
POSMiss(t, sys, ref) =
?
w?Wmiss
countt(w)
|sys|
(6)
To score the POS match and misses between a
pair, we follow Formula 4 and average the scores
for each POS tag, where the sentences in a given
pair swap positions as the system and reference sen-
tences.
5 Split-Bigram Features
System 2 added split-bigram features, which were
derived from the ROUGE-S measure. Like bigrams,
split-bigrams consist of an ordered pair of distinct
tokens drawn from a source sentence. Unlike bi-
grams, split-bigrams allow for a number of inter-
vening tokens to appear between the split-bigram to-
kens. For example, ?The cat ate fish.? would gen-
erate the following split-bigrams the?cat, the?ate,
the?fish, cat?ate, cat?fish, and ate?fish. The in-
tent of split-bigrams is to quickly capture long range
620
dependencies, without requiring a parse of the sen-
tence.
Similar to ROUGE-S, we used lexical overlap
of the split-bigrams as an approximation of seman-
tic similarity. As our pairs are bidirectional, we
used the same framework (Formula 2) for obtain-
ing BLEU scores to generate split-bigram overlap
scores for our pairs. Here, counts are obtained over
split-bigrams found in the system and reference sen-
tences, and the order was set to 1.
For generating the skip-bigram overlap score for
a pair, we used a maximum distance of three.
5.1 Skip-Bigram POS Features
In the same vein as the precision focused POS
features, we used the POS tags of matched split-
bigrams as features, where the frequency of the
POS tags in split-bigrams, t ? t?, were used.
Here, Bmatch represents the split-bigrams which
were found in both the system and reference sen-
tences, matched on lexical content.
SBMatch(t? t?, sys, ref) =
?
b?Bmatch
countt?t?(b)
|sys|
(7)
Due to sparsity, we only considered scores from
split-bigram matches between the system and ref-
erence sentences, and do not model split-bigram
misses. As before, we generate scores for each split-
bigram tag sequence by averaging the scores where
both sentences in a pair have swapped positions. For
our experiments, we only considered split-bigram
POS features of up to distance 3. In our initial exper-
iments we found split-bigram POS features helped
only in the case of shorter sentence pairs, so we only
generated features if both the sentences in a given
pair contained ten tokens or less.
6 Experimental Setup
For all three systems, we used the Stanford
CoreNLP (Toutanova et al, 2003) package to per-
form lemmatization and POS tagging of the in-
put sentences. For regressors, we used LibSVM?s
(Chang and Lin, 2011) support vector regression ca-
pability, using radial basis kernels. Based off of tun-
ing on the training set, we set ? = 1 and the default
Dataset Mean Std.Dev
MSRpar 3.322 0.9294
MSRvid 2.135 1.595
SMTeur 4.307 0.7114
Table 1: Means and standard deviations of similarity
scores for each of the training datasets.
slack value.
From previous experience with paraphrase iden-
tification over the MSR Paraphrase Corpus, we re-
tained stop words in all of our experiments.
7 Dealing with Surprise Data
As the STS training data was broken into three sep-
arate datasets, each with their own distinct statistics,
we developed three regressors trained individually
on each of these datasets. This presented a problem
when dealing with surprise datasets, whose statistics
were not known.
The approach taken by Systems 1 and 2 was sim-
ply to pool together all three training datasets into a
single dataset and train a single regressor on that uni-
fied model. We then applied that regressor against
the two surprise datasets, OnWN and SMTnews.
Analysis of the similarity score statistics showed
that they varied greatly between each of the train-
ing sets, as given in Table 1. Thus combining the
datasets blindly, as with Systems 1 and 2, may prove
to be a suboptimal strategy. The approach taken by
System 3 was to consider the feature vectors them-
selves as capturing information about which dataset
they were drawn from, and to use a classifier to pre-
dict that dataset. We then emit the score from the
regressor trained on just that matching dataset. We
used the Stanford Classifier?s (Manning and Klein,
2003) multinomial logistic regression as our dataset
predictor, using the feature vectors from System 2.
Five-fold cross validation over the training data
showed the dataset predictor to have an overall ac-
curacy of 91.75%.
In order to assess performance over the known
datasets at test time, System 3 also applied the same
strategy for the MSRpar, MSRvid, and SMTeuroparl
test sets.
621
Sys All Allnorm Mean MSRpar MSRvid SMTeur OnWN SMTnews
1 0.7513 / 11 0.8017 / 40 0.5997 / 22 0.6084 0.7458 0.4688 0.6315 0.3994
2 0.7562 / 10 0.8111 / 24 0.5858 / 33 0.6050 0.7939 0.4294 0.5871 0.3366
3 0.6876 / 21 0.7812 / 54 0.4668 / 68 0.4791 0.7901 0.2159 0.3843 0.2801
Table 2: Pearson correlation of described systems against test data, by dataset. Overall measures are All indicates the
combined Pearson, Allnorm the normalized variant, and Mean the macro average of Pearson correlations. Rank for
the system in the overall measure is given after the slash.
Guess/Gold MSRpar MSRvid SMTeur
MSRpar 664 7 75
MSRvid 7 737 10
SMTeur 79 6 649
Table 3: Confusion for the dataset predictor, used to pre-
dict which dataset a pair was drawn from. This was
ddrawn using five-fold cross validation over the training
set, with columns representing golds and guesses as rows.
8 Results and Discussion
Results on the test data for each of the systems
against the individual datasets, are given in Table
2, given in Pearson linear correlation with the gold
standard scores. Overall measures for the systems
are given, along with their overall ranking.
The split-bigram features in System 2 contributed
primarily to performance over the MSRvid dataset,
while degrading performance on the other datasets
slightly. This is likely a result of increasing spar-
sity in the feature space, but overall this system per-
formed well. System 3 underperformed on most
datasets, asides from its performance on MSRvid.
The confusion generated over five-fold cross vali-
dation over the training set is given in Table 3, and
precision, recall, and F1 scores by dataset label from
five-fold cross validation over the training set are
given in Table 4. As these show, predictor errors lay
primarily in confusing MSRpar for SMTeuroparl,
and vice versa. This error was significant enough to
reduce performance on both the MSRpar and SM-
Teuroparl test sets. This proved to be enough to re-
duce the scores between these two datasets.
9 Conclusion and Future Work
Our STS systems have shown that relatively sim-
ple syntax free methods can be employed to the
STS task. Future avenues of investigation would
Dataset Prec Rec F1
MSRpar 0.8901 0.8853 0.8877
MSRvid 0.9775 0.9827 0.9801
SMTeur 0.8842 0.8842 0.8842
Table 4: Results on classifying pairs by source dataset,
using five-fold cross validation over training data.
be to include the use of syntactic information, in
order to obtain better predicate-argument informa-
tion. Syntactic information has proven useful for
the paraphrase identification task over MSRpar, as
demonstrated in studies such as (Das and Smith,
2009) and (Socher et al, 2011). Furthermore, a
qualitative assessment of the pairs across different
datasets showed relatively significant differences,
which would strengthen the argument for develop-
ing features and methods specific to each dataset.
Another improvement would be to develop a bet-
ter dataset predictor for System 3. Also recognizing
there may be ways to normalize and rescale scores
across datasets so the regression models used do not
have to account for differing means and standard de-
viations.
Finally, there are other bodies of source data that
may be adapted for use with the STS task, such as
the paraphrasing pairs of the Recognizing Textual
Entailment challenges, human generated reference
translations for machine translation evaluation, and
human generated summaries used for summariza-
tion evaluations. Although these are gold decisions,
at the very least they could provide a source of high
similarity pairs, from which one could manufacture
lower scoring variants.
Acknowledgments
The authors would like to thank the Semantic Tex-
tual Similarity organizers for all of their hard work
622
and effort.
One of the authors was supported by the
Intelligence Advanced Research Projects Activ-
ity (IARPA) via Air Force Research Laboratory
(AFRL) contract number FA8650-10-C-7058. The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
AFRL, or the U.S. Government.
Eneko Agirre was partially funded by the
European Communitys Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 270082 (PATHS project) and the Ministry
of Economy under grant TIN2009-14715-C04-01
(KNOW2 project)
References
Eneko Agirre, Montse Cuadros, German Rigau, and Aitor
Soroa. 2010. Exploring knowledge bases for similar-
ity. In Proceedings of the International Conference on
Language Resources and Evaluation 2010.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In In Proceedings of the Joint Confer-
ence of the Annual Meeting of the Association for
Computational Linguistics and the International Joint
Conference on Natural Language Processing(ACL
2009), pages 468?476, Singapore.
Christine Fellbaum. 1998. WordNet - An Electronic Lex-
ical Database. MIT Press.
Samuel Fernando and Mark Stevenson. 2008. A se-
mantic similarity approach to paraphrase detection. In
Computational Linguistics UK (CLUK 2008) 11th An-
nual Research Colloqium.
Andrew Finch, Young-Sook Hwang, and Eiichio Sumita.
2005. Using machine translation evaluation tech-
niques to determine sentence-level semantic equiva-
lence. In Proceedings of the Third International Work-
shop on Paraphrasing (IWP 2005), pages 17?24, Jeju
Island, South Korea.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Evgeniy Gabrilovich and Shaul Markovitch. 2009.
Wikipedia-based semantic interpretation. Journal of
Artificial Intelligence Research, 34:443?498.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank.
In WWW ?02, pages 517?526, New York, NY, USA.
ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. pages 74?81, Barcelona,
Spain, jul. Association for Computational Linguistics.
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, maxent models, and conditional estimation with-
out magic. In Proceedings of the 2003 Conference
of the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology: Tutorials - Volume 5, NAACL-Tutorials ?03,
pages 8?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number =
SIDL-WP-1999-0120.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In Proceedings of the Nine-
teenth National Conference on Artificial Intelligence
(Intelligent Systems Demonstrations), pages 1024?
1025, San Jose, CA, July.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems 24.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
623
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 32?43, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
*SEM 2013 shared task: Semantic Textual Similarity
Eneko Agirre
University of the Basque Country
e.agirre@ehu.es
Daniel Cer
Stanford University
danielcer@stanford.edu
Mona Diab
George Washington University
mtdiab@gwu.edu
Aitor Gonzalez-Agirre
University of the Basque Country
agonzalez278@ikasle.ehu.es
Weiwei Guo
Columbia University
weiwei@cs.columbia.edu
Abstract
In Semantic Textual Similarity (STS), sys-
tems rate the degree of semantic equivalence,
on a graded scale from 0 to 5, with 5 be-
ing the most similar. This year we set up
two tasks: (i) a core task (CORE), and (ii)
a typed-similarity task (TYPED). CORE is
similar in set up to SemEval STS 2012 task
with pairs of sentences from sources related
to those of 2012, yet different in genre from
the 2012 set, namely, this year we included
newswire headlines, machine translation eval-
uation datasets and multiple lexical resource
glossed sets. TYPED, on the other hand, is
novel and tries to characterize why two items
are deemed similar, using cultural heritage
items which are described with metadata such
as title, author or description. Several types of
similarity have been defined, including simi-
lar author, similar time period or similar lo-
cation. The annotation for both tasks lever-
ages crowdsourcing, with relative high inter-
annotator correlation, ranging from 62% to
87%. The CORE task attracted 34 participants
with 89 runs, and the TYPED task attracted 6
teams with 14 runs.
1 Introduction
Given two snippets of text, Semantic Textual Simi-
larity (STS) captures the notion that some texts are
more similar than others, measuring the degree of
semantic equivalence. Textual similarity can range
from exact semantic equivalence to complete un-
relatedness, corresponding to quantified values be-
tween 5 and 0. The graded similarity intuitively cap-
tures the notion of intermediate shades of similarity
such as pairs of text differ only in some minor nu-
anced aspects of meaning only, to relatively impor-
tant differences in meaning, to sharing only some
details, or to simply being related to the same topic,
as shown in Figure 1.
One of the goals of the STS task is to create a
unified framework for combining several semantic
components that otherwise have historically tended
to be evaluated independently and without character-
ization of impact on NLP applications. By providing
such a framework, STS will allow for an extrinsic
evaluation for these modules. Moreover, this STS
framework itself could in turn be evaluated intrin-
sically and extrinsically as a grey/black box within
various NLP applications such as Machine Trans-
lation (MT), Summarization, Generation, Question
Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of NLP
tasks. STS is different from TE inasmuch as it as-
sumes bidirectional graded equivalence between the
pair of textual snippets. In the case of TE the equiv-
alence is directional, e.g. a car is a vehicle, but a ve-
hicle is not necessarily a car. STS also differs from
both TE and Paraphrasing (in as far as both tasks
have been defined to date in the literature) in that,
rather than being a binary yes/no decision (e.g. a ve-
hicle is not a car), we define STS to be a graded sim-
ilarity notion (e.g. a vehicle and a car are more sim-
ilar than a wave and a car). A quantifiable graded
bidirectional notion of textual similarity is useful for
a myriad of NLP tasks such as MT evaluation, infor-
mation extraction, question answering, summariza-
tion, etc.
32
? (5) The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
? (4) The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade Kabul.
The US army invaded Kabul on May 7th last year, 2010.
? (3) The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a suspect.
?He is not a suspect anymore.? John said.
? (2) The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
? (1) The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
? (0) The two sentences are on different topics.
John went horse back riding at dawn with a whole group of friends.
Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.
Figure 1: Annotation values with explanations and examples for the core STS task.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al, 2012). In addition, we held
a DARPA sponsored workshop at Columbia Uni-
versity1. In 2013, STS was selected as the official
Shared Task of the *SEM 2013 conference. Ac-
cordingly, in STS 2013, we set up two tasks: The
core task CORE, which is similar to the 2012 task;
and a pilot task on typed-similarity TYPED between
semi-structured records.
For CORE, we provided all the STS 2012 data
as training data, and the test data was drawn from
related but different datasets. This is in contrast
to the STS 2012 task where the train/test data
were drawn from the same datasets. The 2012
datasets comprised the following: pairs of sentences
from paraphrase datasets from news and video elic-
itation (MSRpar and MSRvid), machine transla-
tion evaluation data (SMTeuroparl, SMTnews) and
pairs of glosses (OnWN). The current STS 2013
dataset comprises the following: pairs of news head-
lines, SMT evaluation sentences (SMT) and pairs of
glosses (OnWN and FNWN).
The typed-similarity pilot task TYPED attempts
1http://www.cs.columbia.edu/?weiwei/
workshop/
to characterize, for the first time, the reason and/or
type of similarity. STS reduces the problem of judg-
ing similarity to a single number, but, in some appli-
cations, it is important to characterize why and how
two items are deemed similar, hence the added nu-
ance. The dataset comprises pairs of Cultural Her-
itage items from Europeana,2 a single access point
to millions of books, paintings, films, museum ob-
jects and archival records that have been digitized
throughout Europe. It is an authoritative source of
information coming from European cultural and sci-
entific institutions. Typically, the items comprise
meta-data describing a cultural heritage item and,
sometimes, a thumbnail of the item itself.
Participating systems in the TYPED task need to
compute the similarity between items, using the tex-
tual meta-data. In addition to general similarity, par-
ticipants need to score specific kinds of similarity,
like similar author, similar time period, etc. (cf. Fig-
ure 3).
The paper is structured as follows. Section 2 re-
ports the sources of the texts used in the two tasks.
Section 3 details the annotation procedure. Section
4 presents the evaluation of the systems, followed
by the results of CORE and TYPED tasks. Section 6
draws on some conclusions and forward projections.
2http://www.europeana.eu/
33
Figure 2: Annotation instructions for CORE task
year dataset pairs source
2012 MSRpar 1500 news
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 news
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2013 TYPED 1500 Cultural Heritage items
Table 1: Summary of STS 2012 and 2013 datasets.
2 Source Datasets
Table 1 summarizes the 2012 and 2013 datasets.
2.1 CORE task
The CORE dataset comprises pairs of news head-
lines (HDL), MT evaluation sentences (SMT) and
pairs of glosses (OnWN and FNWN).
For HDL, we used naturally occurring news head-
lines gathered by the Europe Media Monitor (EMM)
engine (Best et al, 2005) from several different news
sources. EMM clusters together related news. Our
goal was to generate a balanced data set across the
different similarity ranges, hence we built two sets
of headline pairs: (i) a set where the pairs come
from the same EMM cluster, (ii) and another set
where the headlines come from a different EMM
cluster, then we computed the string similarity be-
tween those pairs. Accordingly, we sampled 375
headline pairs of headlines that occur in the same
EMM cluster, aiming for pairs equally distributed
between minimal and maximal similarity using sim-
ple string similarity. We sample another 375 pairs
from the different EMM cluster in the same manner.
The SMT dataset comprises pairs of sentences
used in machine translation evaluation. We have two
different sets based on the evaluation metric used:
an HTER set, and a HYTER set. Both metrics use
the TER metric (Snover et al, 2006) to measure the
similarity of pairs. HTER typically relies on several
(1-4) reference translations. HYTER, on the other
hand, leverages millions of translations. The HTER
set comprises 150 pairs, where one sentence is ma-
chine translation output and the corresponding sen-
tence is a human post-edited translation. We sam-
ple the data from the dataset used in the DARPA
GALE project with an HTER score ranging from 0
to 120. The HYTER set has 600 pairs from 3 sub-
sets (each subset contains 200 pairs): a. reference
34
Figure 3: Annotation instructions for TYPED task
vs. machine translation. b. reference vs. Finite State
Transducer (FST) generated translation (Dreyer and
Marcu, 2012). c. machine translation vs. FST gen-
erated translation. The HYTER data set is used in
(Dreyer and Marcu, 2012).
The OnWN/FnWN dataset contains gloss pairs
from two sources: OntoNotes-WordNet (OnWN)
and FrameNet-WordNet (FnWN). These pairs are
sampled based on the string similarity ranging from
0.4 to 0.9. String similarity is used to measure the
similarity between a pair of glosses. The OnWN
subset comprises 561 gloss pairs from OntoNotes
4.0 (Hovy et al, 2006) and WordNet 3.0 (Fellbaum,
1998). 370 out of the 561 pairs are sampled from the
110K sense-mapped pairs as made available from
the authors. The rest, 291 pairs, are sampled from
unmapped sense pairs with a string similarity rang-
ing from 0.5 to 0.9. The FnWN subset has 189
manually mapped pairs of senses from FrameNet 1.5
(Baker et al, 1998) to WordNet 3.1. They are ran-
domly selected from 426 mapped pairs. In combi-
nation, both datasets comprise 750 pairs of glosses.
2.2 Typed-similarity TYPED task
This task is devised in the context of the PATHS
project,3 which aims to assist users in accessing
digital libraries looking for items. The project
tests methods that offer suggestions about items that
might be useful to recommend, to assist in the inter-
pretation of the items, and to support the user in the
discovery and exploration of the collections. Hence
the task is about comparing pairs of items. The pairs
are generated in the Europeana project.
A study in the PATHS project suggested that users
would be interested in knowing why the system is
suggesting related items. The study suggested seven
similarity types: similar author or creator, similar
people involved, similar time period, similar loca-
3http://www.paths-project.eu
35
Figure 4: TYPED pair on our survey. Only general and author similarity types are shown.
tion, similar event or action, similar subject and sim-
ilar description. In addition, we also include general
similarity. Figure 3 shows the definition of each sim-
ilarity type as provided to the annotators.
The dataset is generated in semi-automatically.
First, members of the project manually select 25
pairs of items for each of the 7 similarity types (ex-
cluding general similarity), totalling 175 manually
selected pairs. After removing duplicates and clean-
ing the dataset, we got 163 pairs. Second, we use
these manually selected pairs as seeds to automat-
ically select new pairs as follows: Starting from
those seeds, we use the Europeana API to get similar
items, and we repeat this process 5 times in order to
diverge from the original items (we stored the vis-
ited items to avoid looping). Once removed from
the seed set, we select the new pairs following two
approaches:
? Distance 1: Current item and similar item.
? Distance 2: Current item and an item that is
similar to a similar item (twice removed dis-
tance wise)
This yields 892 pairs for Distance 1 and 445 of
Distance 2. We then divide the data into train and
test, preserving the ratios. The train data contains
82 manually selected pairs, 446 pairs with similarity
distance 1 and 222 pairs with similarity distance 2.
The test data follows a similar distribution.
Europeana items cannot be redistributed, so we
provide their urls and a script which uses the official
36
Europeana API to access and extract the correspond-
ing metadata in JSON format and a thumbnail. In
addition, the textual fields which are relevant for the
task are made accessible in text files, as follows:
? dcTitle: title of the item
? dcSubject: list of subject terms (from some vo-
cabulary)
? dcDescription: textual description of the item
? dcCreator: creator(s) of the item
? dcDate: date(s) of the item
? dcSource: source of the item
3 Annotation
3.1 CORE task
Figure 1 shows the explanations and values for
each score between 5 and 0. We use the Crowd-
Flower crowd-sourcing service to annotate the
CORE dataset. Annotators are presented with the
detailed instructions given in Figure 2 and are asked
to label each STS sentence pair on our 6 point scale
using a dropdown box. Five sentence pairs at a time
are presented to annotators. Annotators are paid
0.20 cents per set of 5 annotations and we collect
5 separate annotations per sentence pair. Annota-
tors are restricted to people from the following coun-
tries: Australia, Canada, India, New Zealand, UK,
and US.
To obtain high quality annotations, we create a
representative gold dataset of 105 pairs that are man-
ually annotated by the task organizers. During an-
notation, one gold pair is included in each set of 5
sentence pairs. Crowd annotators are required to
rate 4 of the gold pairs correct to qualify to work
on the task. Gold pairs are not distinguished in any
way from the non-gold pairs. If the gold pairs are
annotated incorrectly, annotators are told what the
correct annotation is and they are given an explana-
tion of why. CrowdFlower automatically stops low
performing annotators ? those with too many incor-
rectly labeled gold pairs ? from working on the task.
The distribution of scores in the headlines HDL
dataset is uniform, as in FNWN and OnWN, al-
though the scores are slightly lower in FNWN and
slightly higher in OnWN. The scores for SMT are
not uniform, with most of the scores uniformly dis-
tributed between 3.5 and 5, a few pairs between 2
and 3.5, and nearly no pairs with values below 2.
3.2 TYPED task
The dataset is annotated using crowdsourcing. The
survey contains the 1500 pairs of the dataset (750 for
train and 750 for test), plus 20 gold pairs for quality
control. Each participant is shown 4 training gold
questions at the beginning, and then one gold every
2 or 4 questions depending on the accuracy. If accu-
racy dropped to less than 66.7% percent the survey
is stopped and the answers from that particular an-
notator are discarded. Each annotator is allowed to
rate a maximum of 20 pairs to avoid getting answers
from people that are either tired or bored. To ensure
a good comprehension of the items, the task is re-
stricted to only accept annotators from some English
speaking countries: UK, USA, Australia, Canada
and New Zealand.
Participants are asked to rate the similarity be-
tween pairs of cultural heritage items from rang-
ing from 5 to 0, following the instructions shown
in Figure 3. We also add a ?Not Applicable? choice
for cases in which annotators are not sure or didn?t
know. For those cases, we calculate the similarity
score using the values of the rest of the annotators (if
none, we convert it to 0). The instructions given to
the annotators are the ones shown in Figure 3. Fig-
ure 4 shows a pair from the dataset, as presented to
annotators.
The similarity scores for the pairs follow a similar
distribution in all types. Most of the pairs have a
score between 4 and 5, which can amount to as much
as 50% of all pairs in some types.
3.3 Quality of annotation
In order to assess the annotation quality, we measure
the correlation of each annotator with the average of
the rest of the annotators. We then averaged all the
correlations. This method to estimate the quality is
identical to the method used for evaluation (see Sec-
tion 4.1) and it can be thus used as the upper bound
for the systems. The inter-tagger correlation in the
CORE dataset for each of dataset is as follows:
? HDL: 85.0%
? FNWN: 69.9%
? OnWN: 87.2%
? SMT: 65.8%
For the TYPED dataset, the inter-tagger correla-
tion values for each type of similarity is as follows:
? General: 77.0%
37
? Author: 73.1%
? People Involved: 62.5%
? Time period: 72.0%
? Location: 74.3%
? Event or Action: 63.9%
? Subject: 74.5%
? Description: 74.9%
In both datasets, the correlation figures are high,
confirming that the task is well designed. The weak-
est correlations in the CORE task are SMT and
FNWN. The first might reflect the fact that some
automatically produced translations are confusing
or difficult to understand, and the second could be
caused by the special style used to gloss FrameNet
concepts. In the TYPED task the weakest correla-
tions are for the People Involved and Event or Action
types, as they might be the most difficult to spot.
4 Systems Evaluation
4.1 Evaluation metrics
Evaluation of STS is still an open issue. STS ex-
periments have traditionally used Pearson product-
moment correlation, or, alternatively, Spearman
rank order correlation. In addition, we also need a
method to aggregate the results from each dataset
into an overall score. The analysis performed in
(Agirre and Amigo?, In prep) shows that Pearson and
averaging across datasets are the best suited com-
bination in general. In particular, Pearson is more
informative than Spearman, in that Spearman only
takes the rank differences into account, while Pear-
son does account for value differences as well. The
study also showed that other alternatives need to be
considered, depending on the requirements of the
target application.
We leave application-dependent evaluations for
future work, and focus on average weighted Pear-
son correlation. When averaging, we weight each
individual correlation by the size of the dataset.
In addition, participants in the CORE task are al-
lowed to provide a confidence score between 1 and
100 for each of their scores. The evaluation script
down-weights the pairs with low confidence, follow-
ing weighted Pearson.4 In order to compute sta-
tistical significance among system results, we use
4http://en.wikipedia.org/wiki/Pearson_
product-moment_correlation_coefficient#
Calculating_a_weighted_correlation
a one-tailed parametric test based on Fisher?s z-
transformation (Press et al, 2002, equation 14.5.10).
4.2 The Baseline Systems
For the CORE dataset, we produce scores using a
simple word overlap baseline system. We tokenize
the input sentences splitting at white spaces, and
then represent each sentence as a vector in the mul-
tidimensional token space. Each dimension has 1
if the token is present in the sentence, 0 otherwise.
Vector similarity is computed using the cosine sim-
ilarity metric. We also run two freely available sys-
tems, DKPro (Bar et al, 2012) and TakeLab (S?aric? et
al., 2012) from STS 2012,5 and evaluate them on the
CORE dataset. They serve as two strong contenders
since they ranked 1st (DKPro) and 2nd (TakeLab) in
last year?s STS task.
For the TYPED dataset, we first produce XML
files for each of the items, using the fields as pro-
vided to participants. Then we run named entity
recognition and classification (NERC) and date de-
tection using Stanford CoreNLP. This is followed by
calculating the similarity score for each of the types
as follows.
? General: cosine similarity of TF-IDF vectors of
tokens from all fields.
? Author: cosine similarity of TF-IDF vectors for
dc:Creator field.
? People involved, time period and location:
cosine similarity of TF-IDF vectors of loca-
tion/date/people recognized by NERC in all
fields.
? Events: cosine similarity of TF-IDF vectors of
verbs in all fields.
? Subject and description: cosine similarity of
TF-IDF vectors of respective fields.
IDF values are calculated from a subset of the
Europeana collection (Culture Grid collection). We
also run a random baseline several times, yielding
close to 0 correlations in all datasets, as expected.
4.3 Participation
Participants could send a maximum of three system
runs. After downloading the test datasets, they had
a maximum of 120 hours to upload the results. 34
teams participated in the CORE task, submitting 89
5Code is available at http://www-nlp.stanford.
edu/wiki/STS
38
Team and run Head. OnWN FNWN SMT Mean # Team and run Head. OnWN FNWN SMT Mean #
baseline-tokencos .5399 .2828 .2146 .2861 .3639 73 KnCe2013-all .3475 .3505 .1073 .1551 .2639 86
DKPro .7347 .7345 .3405 .3256 .5652 - KnCe2013-diff .4028 .3537 .1284 .1804 .2934 84
TakeLab-best .6559 .6334 .4052 .3389 .5221 - KnCe2013-set .0462 -.1526 .0376 -.0605 -.0397 90
TakeLab-sts12 .4858 .6334 .2693 .2787 .4340 - LCL Sapienza-ADW1 .6943 .4661 .3571 .3311 .4880 43
aolney-w3c3 .5248 .4701 .1777 .2744 .3986 67 LCL Sapienza-ADW2 .6520 .5280 .3598 .3681 .5019 32
BGU-1 .5075 .3252 .0768 .1843 .3181 81 LCL Sapienza-ADW3 .6205 .5108 .4462 .3838 .4996 34
BGU-2 .3608 .3777 -.0173 .0698 .2363 88 LIPN-tAll .7063 .6937 .4037 .3005 .5425 16
BGU-3 .3591 .3360 .0072 .2122 .2748 85 LIPN-tSp .5791 .7199 .3522 .3721 .5261 24
BUAP-RUN1 .5005 .2579 .1766 .2322 .3234 78 MayoClinicNLP-r1wtCDT .6584 .7775 .3735 .3605 .5649 6
BUAP-RUN2 .4860 .2872 .2082 .2117 .3216 79 MayoClinicNLP-r2CDT .6827 .6612 .3960 .3946 .5572 8
BUAP-RUN3 .4817 .2711 .2511 .1990 .3156 82 MayoClinicNLP-r3wtCD .6440 .8295 .3202 .3561 .5671 5
CFILT-1 .5336 .2381 .2261 .2906 .3531 75 NTNU-RUN1 .7279 .5952 .3215 .4015 .5519 9
CLaC-RUN1 .6774 .7667 .3793 .3068 .5511 10 NTNU-RUN2 .5909 .1634 .3650 .3786 .3946 68
CLaC-RUN2 .6921 .7366 .3793 .3375 .5587 7 NTNU-RUN3 .7274 .5882 .3115 .4035 .5498 12
CLaC-RUN3 .5276 .6495 .4158 .3082 .4755 47 PolyUCOMP-RUN1 .5176 .1517 .2496 .2914 .3284 77
CNGL-LPSSVR .6510 .6971 .1180 .2861 .4961 36 SOFTCARDINALITY-run1 .6410 .7360 .3442 .3035 .5273 23
CNGL-LPSSVRTL .6385 .6756 .1823 .3098 .4998 33 SOFTCARDINALITY-run2 .6713 .7412 .3838 .2981 .5402 18
CNGL-LSSVR .6552 .6943 .2016 .3005 .5086 30 SOFTCARDINALITY-run3 .6603 .7401 .3347 .2900 .5294 22
CPN-combined.RandSubSpace .6771 .5135 .3314 .3369 .4939 39 sriubc-System1? .6083 .2915 .2790 .3065 .4011 66
CPN-combined.SVM .6685 .5096 .3621 .3408 .4939 38 sriubc-System2? .6359 .3664 .2713 .3476 .4420 57
CPN-individual.RandSubSpace .6771 .5484 .3314 .2769 .4826 45 sriubc-System3? .5443 .2843 .2705 .3275 .3842 70
DeepPurple-length .6542 .5105 .2507 .2803 .4598 56 SXUCFN-run1 .6806 .5355 .3181 .3980 .5198 27
DeepPurple-linear .6878 .5105 .2693 .2787 .4721 50 SXUCFN-run2 .4881 .6146 .4237 .3844 .4797 46
DeepPurple-lineara .6227 .5105 .3265 .2952 .4607 55 SXUCFN-run3 .6761 .6481 .3025 .4003 .5458 14
deft-baseline .6532 .8431 .5083 .3265 .5795 3 SXULLL-1 .4840 .7146 .0415 .1543 .3944 69
deft-baseline2 .5706 .8111 .5503 .3325 .5495 13 UCam-A .5510 .3099 .2385 .1171 .3200 80
DLS@CU-char .3867 .2386 .3726 .3337 .3309 76 UCam-B .6399 .4440 .3995 .3400 .4709 53
DLS@CU-charSemantic .4669 .4165 .3859 .3411 .4056 64 UCam-C .4962 .5639 .1724 .3006 .4207 62
DLS@CU-charWordSemantic .4921 .3769 .4647 .3492 .4135 63 UCSP-NC? .1736 .0853 .1151 .1658 .1441 89
ECNUCS-Run1 .5656 .2083 .1725 .2949 .3533 74 UMBC EBIQUITY-galactus .7428 .7053 .5444 .3705 .5927 2
ECNUCS-Run2 .7120 .5388 .2013 .2504 .4720 51 UMBC EBIQUITY-ParingWords .7642 .7529 .5818 .3804 .6181 1
ECNUCS-Run3 .6799 .5284 .2203 .3595 .4967 35 UMBC EBIQUITY-saiyan .7838 .5593 .5815 .3563 .5683 4
HENRY-run1 .7601 .4631 .3516 .2801 .4917 41 UMCC DLSI-1 .5841 .4847 .2917 .2855 .4352 58
HENRY-run2 .7645 .4631 .3905 .3593 .5229 26 UMCC DLSI-2 .6168 .5557 .3045 .3407 .4833 44
HENRY-run3 .7103 .3934 .3364 .3308 .4734 48 UMCC DLSI-3 .3846 .1342 -.0065 .2736 .2523 87
IBM EG-run2 .7217 .6110 .3364 .3460 .5365 19 UNIBA-2STEPSML .4255 .4801 .1832 .2710 .3673 71
IBM EG-run5 .7410 .5987 .4133 .3426 .5452 15 UNIBA-DSM PERM .6319 .4910 .2717 .3155 .4610 54
IBM EG-run6 .7447 .6257 .4381 .3275 .5502 11 UNIBA-STACKING .6275 .4658 .2111 .2588 .4293 61
ikernels-sys1 .7352 .5432 .3842 .3180 .5188 28 Unimelb NLP-bahar .7119 .3490 .3813 .3507 .4733 49
ikernels-sys2 .7465 .5572 .3875 .3409 .5339 21 Unimelb NLP-concat .7085 .6790 .3374 .3230 .5415 17
ikernels-sys3 .7395 .4228 .3596 .3294 .4919 40 Unimelb NLP-stacking .7064 .6140 .1865 .3144 .5091 29
INAOE-UPV-run1 .6392 .3249 .2711 .3491 .4332 59 Unitor-SVRegressor run1 .6353 .5744 .3521 .3285 .4941 37
INAOE-UPV-run2 .6390 .3260 .2662 .3457 .4319 60 Unitor-SVRegressor run2 .6511 .5610 .3580 .3096 .4902 42
INAOE-UPV-run3 .6468 .6295 .4090 .3047 .5085 31 Unitor-SVRegressor run3 .6027 .5489 .3269 .3192 .4716 52
KLUE-approach 1 .6521 .6507 .3996 .3367 .5254 25 UPC-AE .6092 .5679 -.1268 .2090 .4037 65
KLUE-approach 2 .6510 .6869 .4189 .3360 .5355 20 UPC-AED .4136 .4770 -.0852 .1662 .3050 83
UPC-AED T .5119 .6386 -.0464 .1235 .3671 72
Table 2: Results on the CORE task. The first rows on the left correspond to the baseline and to two publicly available
systems, see text for details. Note: ? signals team involving one of the organizers, ? for systems submitting past the
120 hour window.
system runs. For the TYPED task, 6 teams partici-
pated, submitting 14 system runs.6
Some submissions had minor issues: one team
had a confidence score of 0 for all items (we re-
placed them by 100), and another team had a few
Not-a-Number scores for the SMT dataset, which
we replaced by 5. One team submitted the results
past the 120 hours. This team, and the teams that in-
6Due to lack of space we can?t detail the full names of au-
thors and institutions that participated.The interested reader can
use the name of the runs in Tables 2 and 3 to find the relevant
paper in these proceedings.
cluded one of the organizers, are explicitly marked.
We want to stress that in these teams the organizers
did not allow the developers of the system to access
any data or information which was not available for
the rest of participants. After the submission dead-
line expired, the organizers published the gold stan-
dard in the task website, in order to ensure a trans-
parent evaluation process.
4.4 CORE Task Results
Table 2 shows the results of the CORE task, with
runs listed in alphabetical order. The correlation in
39
Team and run General Author People involved Time Location Event Subject Description Mean #
baseline .6691 .4278 .4460 .5002 .4835 .3062 .5015 .5810 .4894 8
BUAP-RUN1 .6798 .6166 .0670 .2761 .0163 .1612 .5167 .5283 .3577 14
BUAP-RUN2 .6745 .6093 .1285 .3721 .0163 .1660 .5094 .5546 .3788 13
BUAP-RUN3 .6992 .6345 .1055 .1461 .0000 -.0668 .3729 .5120 .3004 15
BUT-1 .3686 .7468 .3920 .5725 .3604 .2906 .2270 .5882 .4433 9
ECNUCS-Run1 .6040 .7362 .3663 .4685 .3844 .4057 .5229 .6027 .5113 5
ECNUCS-Run2 .6064 .5684 .3663 .4685 .3844 .4057 .5563 .6027 .4948 7
PolyUCOMP-RUN1 .4888 .6940 .3223 .3820 .3621 .1625 .3962 .4816 .4112 12
PolyUCOMP-RUN2 .4893 .6940 .3253 .3777 .3628 .1968 .3962 .4816 .4155 11
PolyUCOMP-RUN3 .4915 .6940 .3254 .3737 .3667 .2207 .3962 .4816 .4187 10
UBC UOS-RUN1? .7256 .4568 .4467 .5762 .4858 .3090 .5015 .5810 .5103 6
UBC UOS-RUN2? .7457 .6618 .6518 .7466 .7244 .6533 .7404 .7751 .7124 4
UBC UOS-RUN3? .7461 .6656 .6544 .7411 .7257 .6545 .7417 .7763 .7132 3
Unitor-SVRegressor lin .7564 .8076 .6758 .7090 .7351 .6623 .7520 .7745 .7341 2
Unitor-SVRegressor rbf .7981 .8158 .6922 .7471 .7723 .6835 .7875 .7996 .7620 1
Table 3: Results on TYPED task. The first row corresponds to the baseline. Note: ? signals team involving one of the
organizers.
each dataset is given, followed by the mean cor-
relation (the official measure), and the rank of the
run. The baseline ranks 73. The highest correla-
tions are for OnWN (84%, by deft) and HDL (78%,
by UMBC), followed by FNWN (58%, by UMBC)
and SMT (40%, by NTNU). This fits nicely with the
inter-tagger correlations (respectively 87, 85, 70 and
65, cf. Section 3). It also shows that the systems get
close to the human correlations in the OnWN and
HDL dataset, with bigger differences for FNWN and
SMT.
The result of the best run (by UMBC) is signif-
icantly different (p-value < 0.05) than all runs ex-
cept the second best. The second best run is only
significantly different to the runs ranking 7th and
below, and the third best to the 14th run and be-
low. The difference between consecutive runs was
not significant. This indicates that many system runs
performed very close to each other.
Only 13 runs included non-uniform confidence
scores. In 10 cases the confidence value allowed
to improve performance, sometimes as much as .11
absolute points. For instance, SXUCFN-run3 im-
proves from .4773 to .5458. The most notable ex-
ception is MayoClinicNLP-r2CDT, which achieves
a mean correlation of .5879 instead of .5572 if they
provide uniform confidence values.
The Table also shows the results of TakeLab
and DKPro. We train the DKPro and TakeLab-
sts12 models on all the training and test STS 2012
data. We additionally train another variant sys-
tem of TakeLab, TakeLab-best, where we use tar-
geted training where the model yields the best per-
formance for each test subset as follows: (1) HDL
is trained on MSRpar 2012 data; (2) OnWN is
trained on all 2012 data; (3) FnWN is trained on
2012 OnWN data; (4) SMT is trained on 2012 SM-
Teuroparl data. Note that Takelab-best is an upper
bound, as the best combination is selected on the
test dataset. TakeLab-sts12, TakeLab-best, DKPro
rank as 58th, 27th and 6th in this year?s system sub-
missions, respectively. The different results yielded
from TakeLab depending on the training data sug-
gests that some STS systems are quite sensitive to
the source of the sentence pairs, indicating that do-
main adaptation techniques could have a role in this
task. On the other hand, DKPro performed ex-
tremely well when trained on all available training,
with no special tweaking for each dataset.
4.5 TYPED Task Results
Table 3 shows the results of TYPED task. The
columns show the correlation for each type of sim-
ilarity, followed by the mean correlation (the offi-
cial measure), and the rank of the run. The best sys-
tem (from Unitor) is best in all types. The baseline
ranked 8th, but the performance difference with the
best system is quite significant. The best result is
significantly different (p-value < 0.02) to all runs.
The second and third best runs are only significantly
different from the run ranking 5th and below. Note
that in this dataset the correlations of the best system
are higher than the inter-tagger correlations. This
might indicate that the task has been solved, in the
sense that the features used by the top systems are
enough to characterize the problem and reach hu-
man performance, although the correlations of some
40
A
cr
on
ym
s
D
is
tr
ib
ut
io
na
lm
em
or
y
D
is
tr
ib
ut
io
na
lt
he
sa
ur
us
M
on
ol
in
gu
al
co
rp
or
a
M
ul
ti
li
ng
ua
lc
or
po
ra
O
pi
ni
on
an
d
S
en
ti
m
en
t
Ta
bl
es
of
pa
ra
ph
ra
se
s
W
ik
ip
ed
ia
W
ik
ti
on
ar
y
W
or
d
em
be
dd
in
gs
W
or
dN
et
C
or
re
fe
re
nc
e
D
ep
en
de
nc
y
pa
rs
e
D
is
tr
ib
ut
io
na
ls
im
il
ar
it
y
K
B
S
im
il
ar
it
y
L
D
A
L
em
m
at
iz
er
L
ex
ic
al
S
ub
st
it
ut
io
n
L
og
ic
al
in
fe
re
nc
e
M
et
ap
ho
r
or
M
et
on
ym
y
M
ul
ti
w
or
d
re
co
gn
it
io
n
N
am
ed
E
nt
it
y
re
co
gn
it
io
n
P
O
S
ta
gg
er
R
O
U
G
E
pa
ck
ag
e
S
co
pi
ng
S
ea
rc
h
en
gi
ne
S
em
an
ti
c
R
ol
e
L
ab
el
in
g
S
tr
in
g
si
m
il
ar
it
y
S
yn
ta
x
Te
xt
ua
le
nt
ai
lm
en
t
T
im
e
an
d
da
te
re
so
lu
ti
on
T
re
e
ke
rn
el
s
W
or
d
S
en
se
D
is
am
bi
gu
at
io
n
aolney-w3c3 x x x
BGU-1 x x x x x x x
BGU-2 x x x x x x x
BGU-3 x x x x x x x
CFILT-APPROACH x x x x x
CLaC-Run1 x x x x x x x x
CLaC-Run2 x x x x x x x x
CLaC-Run3 x x x x x x x x
CNGL-LPSSVR x x x x x
CNGL-LPSSVRTL x x x x x
CNGL-LSSVR x x x x x
CPN-combined.RandSubSpace x x x x x x x x
CPN-combined.SVM x x x x x x x x
CPN-individual.RandSubSpace x x x x x x x x
DeepPurple-length x x x x x x x
DeepPurple-linear x x x x x x x
DeepPurple-lineara x x x x x x x
deft-baseline x x x x
deft-baseline x x x x x x
DLS@CU-charSemantic x x x x
DLS@CU-charWordSemantic x x x x x x
DLS@CU-charWordSemantic x x x
ECNUCS-Run1 x x x x x x x
ECNUCS-Run2 x x x x x x x
ECNUCS-Run3 x x x x x x x
HENRY-run1 x x x x x x x x x
HENRY-run2 x x x x x x x x
IBM EG-run2 x x x x x x
IBM EG-run5 x x x x x x
IBM EG-run6 x x x x x
ikernels-sys1 x x x x x x x x x x x
ikernels-sys2 x x x x x x x x x x x
ikernels-sys3 x x x x x x x x x x x
INAOE-UPV-run1 x x x x x x x
INAOE-UPV-run2 x x x x x x x
INAOE-UPV-run3 x x x x x x x
KLUE-approach 1 x x x x x x x
KLUE-approach 2 x x x x x x
KnCe2013-all x x x x x x x x
KnCe2013-div x x x x x x x x
KnCe2013-div x x x x x x x x
LCL Sapienza-ADW1 x x x
LCL Sapienza-ADW2 x x x
LCL Sapienza-ADW3 x x x
LIPN-tAll x x x x x x x x x x
LIPN-tSp x x x x x x x x x x
MayoClinicNLP-r1wtCDT x x x x x x x x x x x x
MayoClinicNLP-r2CDT x x x x x x x x x x x x
MayoClinicNLP-r3wtCD x x x x x x x x x x x x
NTNU-RUN1 x x x x x x x x x x x x x x x x x x x x x x x
NTNU-RUN2 x x x x x x x x x x x x x x x x x x x x x x x
NTNU-RUN3 x x x x x x x x x x x x x x x x x x x x x x x
PolyUCOMP-RUN1 x x x x
SOFTCARDINALITY-run1 x
SOFTCARDINALITY-run2 x x x
SOFTCARDINALITY-run3 x x x
SXUCFN-run1 x x x
SXUCFN-run2 x x x
SXUCFN-run3 x x x
SXULLL-1 x x
UCam-A x x x x
UCam-B x x x x
UCam-C x x x x
UCSP-NC x x x x x
UMBC EBIQUITY-galactus x x x x x x x
UMBC EBIQUITY-ParingWords x x x x x x
UMBC EBIQUITY-saiyan x x x x x x x
UMCC DLSI-1 x x x x x x x x x x
UMCC DLSI-2 x x x x x x x x x x
UMCC DLSI-3 x x x x x x x x x
UNIBA-2STEPSML x x x x x x x x x x x
UNIBA-DSM PERM x x x x x x
UNIBA-STACKING x x x x x x x x x x x
Unimelb NLP-bahar x x
Unimelb NLP-concat x x x x x x x x x x
Unimelb NLP-stacking x x x x x x x x x x
Unitor-SVRegressor run1 x x x x x x
Unitor-SVRegressor run2 x x x x x x
Unitor-SVRegressor run3 x x x x x x
Total 11 2 12 54 12 5 11 36 7 3 54 3 3 48 40 2 67 14 3 3 10 24 55 3 3 4 9 6 34 9 13 6 6
Table 4: CORE task: Resources and tools used by the systems that submitted a description file. Leftmost columns
correspond to the resources, and rightmost to tools, in alphabetic order.
41
types could be too low for practical use.
5 Tools and resources used
The organizers asked participants to submit a de-
scription file, making special emphasis on the tools
and resources that were used. Tables 4 and 5 show
schematically the tools and resources as reported by
some of the participants for the CORE and TYPED
tasks (respectively). In the last row, the totals show
that WordNet and monolingual corpora were the
most used resources for both tasks, followed by
Wikipedia and the use of acronyms (for CORE and
TYPED tasks respectively). Dictionaries, multilin-
gual corpora, opinion and sentiment analysis, and
lists and tables of paraphrases are also used.
For CORE, generic NLP tools such as lemmati-
zation and PoS tagging are widely used, and to a
lesser extent, distributional similarity, knowledge-
based similarity, syntactic analysis, named entity
recognition, lexical substitution and time and date
resolution (in this order). Other popular tools are
Semantic Role Labeling, Textual Entailment, String
Similarity, Tree Kernels and Word Sense Disam-
biguation. Machine learning is widely used to com-
bine and tune components (and so, it is not men-
tioned in the tables). Several less used tools are
also listed but are used by three or less systems.
The top scoring systems use most of the resources
and tools listed (UMBC EBIQUITY-ParingWords,
MayoClinicNLP-r3wtCD). Other well ranked sys-
tems like deft-baseline are only based on distribu-
tional similarity. Although not mentioned in the
descriptions files, some systems used the publicly
available DKPro and Takelab systems.
For the TYPED task, the most used tools are lem-
matizers, Named Entity Recognizers, and PoS tag-
gers. Distributional and Knowledge-base similarity
is also used, and at least four systems used syntactic
analysis and time and date resolution.7
6 Conclusions and Future Work
We presented the 2013 *SEM shared task on Seman-
tic Textual Similarity.8 Two tasks were defined: a
7For a more detailed analysis, the reader is directed to the
papers in this volume.
8All annotations, evaluation scripts and system outputs are
available in the website for the task9. In addition, a collabora-
tively maintained site10, open to the STS community, contains
A
cr
on
ym
s
M
on
ol
in
gu
al
co
rp
or
a
W
ik
ip
ed
ia
W
or
dN
et
D
is
tr
ib
ut
io
na
ls
im
il
ar
it
y
K
B
S
im
il
ar
it
y
L
em
m
at
iz
er
M
ul
ti
w
or
d
re
co
gn
it
io
n
N
am
ed
E
nt
it
y
re
co
gn
it
io
n
P
O
S
ta
gg
er
S
yn
ta
x
T
im
e
an
d
da
te
re
so
lu
ti
on
T
re
e
ke
rn
el
s
BUT-1 x x x x x x x
PolyUCOMP-RUN2 x x x x
ECNUCS-Run1 x x x
ECNUCS-Run2 x x x x x x x
PolyUCOMP-RUN1 x x x x
PolyUCOMP-RUN3 x x x x
UBC UOS-RUN1 x x x x x x x x x x x
UBC UOS-RUN2 x x x x x x x x x x x x
UBC UOS-RUN3 x x x x x x x x x x x x
Unitor-SVRegressor lin x x x x x x x
Unitor-SVRegressor rbf x x x x x x x
Total 4 7 3 7 7 4 11 3 11 11 4 4 2
Table 5: TYPED task: Resources and tools used by
the systems that submitted a description file. Leftmost
columns correspond to the resources, and rightmost to
tools, in alphabetic order.
core task CORE similar to the STS 2012 task, and
a new pilot on typed-similarity TYPED. We had 34
teams participate in both tasks submitting 89 system
runs for CORE and 14 system runs for TYPED, in
total amounting to a 103 system evaluations. CORE
uses datasets which are related to but different from
those used in 2012: news headlines, MT evalua-
tion data, gloss pairs. The best systems attained
correlations close to the human inter tagger corre-
lations. The TYPED task characterizes, for the first
time, the reasons why two items are deemed simi-
lar. The results on TYPED show that the training
data provided allowed systems to yield high corre-
lation scores, demonstrating the practical viability
of this new task. In the future, we are planning on
adding more nuanced evaluation data sets that in-
clude modality (belief, negation, permission, etc.)
and sentiment. Also given the success rate of the
TYPED task, however, the data in this pilot is rel-
atively structured, hence in the future we are inter-
ested in investigating identifying reasons why two
pairs of unstructured texts as those present in CORE
are deemed similar.
Acknowledgements
We are grateful to the OntoNotes team for sharing OntoNotes
to WordNet mappings (Hovy et al 2006). We thank Lan-
guage Weaver, INC, DARPA and LDC for providing the SMT
data. This work is also partially funded by the Spanish Ministry
of Education, Culture and Sport (grant FPU12/06243). This
a comprehensive list of evaluation tasks, datasets, software and
papers related to STS.
42
work was partially funded by the DARPA BOLT and DEFT pro-
grams.
We want to thank Nikolaos Aletras, German Rigau and
Mark Stevenson for their help designing, annotating and col-
lecting the typed-similarity data. The development of the
typed-similarity dataset was supported by the PATHS project
(http://paths-project.eu) funded by the European Community?s
Seventh Framework Program (FP7/2007-2013) under grant
agreement no. 270082. The tasks were partially financed by
the READERS project under the CHIST-ERA framework (FP7
ERA-Net). We thank Europeana and all contributors to Euro-
peana for sharing their content through the API.
References
Eneko Agirre and Enrique Amigo?. In prep. Exploring
evaluation measures for semantic textual similarity. In
Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In COLING ?98
Proceedings of the 17th international conference on
Computational linguistics - Volume 1.
Daniel Bar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, in conjunction with the
1st Joint Conference on Lexical and Computational
Semantics.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo Gar-
cia, and David Horby. 2005. Europe media monitor -
system description. In EUR Report 22173-En, Ispra,
Italy.
Markus Dreyer and Daniel Marcu. 2012. Hyter:
Meaning-equivalent semantics for translation evalua-
tion. In Human Language Technologies: Conference
of the North American Chapter of the Association of
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P.
Flannery. 2002. Numerical Recipes: The Art of Sci-
entific Computing V 2.10 With Linux Or Single-Screen
License. Cambridge University Press.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
43
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 132?137, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UBC UOS-TYPED: Regression for Typed-similarity
Eneko Agirre
University of the Basque Country
Donostia, 20018, Basque Country
e.agirre@ehu.es
Nikolaos Aletras
University of Sheffield
Sheffield, S1 4DP, UK
n.aletras@dcs.shef.ac.uk
Aitor Gonzalez-Agirre
University of the Basque Country
Donostia, 20018, Basque Country
agonzalez278@ikasle.ehu.es
German Rigau
University of the Basque Country
Donostia, 20018, Basque Country
german.rigau@ehu.es
Mark Stevenson
University of Sheffield
Sheffield, S1 4DP, UK
m.stevenson@dcs.shef.ac.uk
Abstract
We approach the typed-similarity task using
a range of heuristics that rely on information
from the appropriate metadata fields for each
type of similarity. In addition we train a linear
regressor for each type of similarity. The re-
sults indicate that the linear regression is key
for good performance. Our best system was
ranked third in the task.
1 Introduction
The typed-similarity dataset comprises pairs of Cul-
tural Heritage items from Europeana1, a single ac-
cess point to digitised versions of books, paintings,
films, museum objects and archival records from in-
stitutions throughout Europe. Typically, the items
comprise meta-data describing a cultural heritage
item and, sometimes, a thumbnail of the item itself.
Participating systems need to compute the similarity
between items using the textual meta-data. In addi-
tion to general similarity, the dataset includes spe-
cific kinds of similarity, like similar author, similar
time period, etc.
We approach the problem using a range of sim-
ilarity techniques for each similarity types, these
make use of information contained in the relevant
meta-data fields.In addition, we train a linear regres-
sor for each type of similarity, using the training data
provided by the organisers with the previously de-
fined similarity measures as features.
We begin by describing our basic system in Sec-
tion 2, followed by the machine learning system in
1http://www.europeana.eu/
Section 3. The submissions are explained in Section
4. Section 5 presents our results. Finally, we draw
our conclusions in Section 6.
2 Basic system
The items in this task are taken from Europeana.
They cannot be redistributed, so we used the urls
and scripts provided by the organizers to extract the
corresponding metadata. We analysed the text in the
metadata, performing lemmatization, PoS tagging,
named entity recognition and classification (NERC)
and date detection using Stanford CoreNLP (Finkel
et al, 2005; Toutanova et al, 2003). A preliminary
score for each similarity type was then calculated as
follows:
? General: cosine similarity of TF.IDF vectors of
tokens, taken from all fields.
? Author: cosine similarity of TF.IDF vectors of
dc:Creator field.
? People involved, time period and location:
cosine similarity of TF.IDF vectors of loca-
tion/date/people entities recognized by NERC
in all fields.
? Events: cosine similarity of TF.IDF vectors of
event verbs and nouns. A list of verbs and
nouns possibly denoting events was derived us-
ing the WordNet Morphosemantic Database2.
? Subject and description: cosine similarity of
TF.IDF vectors of respective fields.
IDF values were calculated using a subset of Eu-
ropeana items (the Culture Grid collection), avail-
able internally. These preliminary scores were im-
2urlhttp://wordnetcode.princeton.edu/standoff-
files/morphosemantic-links.xls
132
proved using TF.IDF based on Wikipedia, UKB
(Agirre and Soroa, 2009) and a more informed time
similarity measure. We describe each of these pro-
cesses in turn.
2.1 TF.IDF
A common approach to computing document sim-
ilarity is to represent documents as Bag-Of-Words
(BOW). Each BOW is a vector consisting of the
words contained in the document, where each di-
mension corresponds to a word, and the weight is
the frequency in the corresponding document. The
similarity between two documents can be computed
as the cosine of the angle between their vectors. This
is the approached use above.
This approach can be improved giving more
weight to words which occur in only a few docu-
ments, and less weight to words occurring in many
documents (Baeza-Yates and Ribeiro-Neto, 1999).
In our system, we count document frequencies of
words using Wikipedia as a reference corpus since
the training data consists of only 750 items associ-
ated with short textual information and might not be
sufficient for reliable estimations. The TF.IDF sim-
ilarity between items a and b is defined as:
simtf.idf(a, b) =
?
w?a,b tfw,a ? tfw,b ? idf
2
w
??
w?a(tfw,a ? idfw)
2 ?
??
w?b(tfw,b ? idfw)
2
where tfw,x is the frequency of the term w in x ?
{a, b} and idfw is the inverted document frequency
of the word w measured in Wikipedia. We substi-
tuted the preliminary general similarity score by the
obtained using the TF.IDF presented in this section.
2.2 UKB
The semantic disambiguation UKB3 algorithm
(Agirre and Soroa, 2009) applies personalized
PageRank on a graph generated from the English
WordNet (Fellbaum, 1998), or alternatively, from
Wikipedia. This algorithm has proven to be very
competitive in word similarity tasks (Agirre et al,
2010).
To compute similarity using UKB we represent
WordNet as a graph G = (V,E) as follows: graph
nodes represent WordNet concepts (synsets) and
3http://ixa2.si.ehu.es/ukb/
dictionary words; relations among synsets are rep-
resented by undirected edges; and dictionary words
are linked to the synsets associated to them by di-
rected edges.
Our method is provided with a pair of vectors of
words and a graph-based representation of WordNet.
We first compute the personalized PageRank over
WordNet separately for each of the vector of words,
producing a probability distribution over WordNet
synsets. We then compute the similarity between
these two probability distributions by encoding them
as vectors and computing the cosine between the
vectors. We present each step in turn.
Once personalized PageRank is computed, it
returns a probability distribution over WordNet
synsets. The similarity between two vectors of
words can thus be implemented as the similarity be-
tween the probability distributions, as given by the
cosine between the vectors.
We used random walks to compute improved sim-
ilarity values for author, people involved, location
and event similarity:
? Author: UKB over Wikipedia using person en-
tities recognized by NERC in the dc:Creator
field.
? People involved and location: UKB over
Wikipedia using people/location entities recog-
nized by NERC in all fields.
? Events: UKB over WordNet using event nouns
and verbs recognized in all fields.
Results on the training data showed that perfor-
mance using this approach was quite low (with the
exception of events). This was caused by the large
number of cases where the Stanford parser did not
find entities which were in Wikipedia. With those
cases on mind, we combined the scores returned by
UKB with the similarity scores presented in Section
2 as follows: if UKB similarity returns a score, we
multiply both, otherwise we return the square of the
other similarity score. Using the multiplication of
the two scores, the results on the training data im-
proved.
2.3 Time similarity measure
In order to measure the time similarity between a
pair of items, we need to recognize time expres-
sions in both items. We assume that the year of
133
creation or the year denoting when the event took
place in an artefact are good indicators for time sim-
ilarity. Therefore, information about years is ex-
tracted from each item using the following pattern:
[1|2][0 ? 9]{3}. Using this approach, each item is
represented as a set of numbers denoting the years
mentioned in the meta-data.
Time similarity between two items is computed
based on the similarity between their associated
years. Similarity between two years is defined as:
simyear(y1, y2) = max{0, 1? |y1? y2| ? k}
k is a parameter to weight the difference between
two years, e.g. for k = 0.1 all items that have differ-
ence of 10 years or more assigned a score of 0. We
obtained best results for k = 0.1.
Finally, time similarity between items a and b is
computed as the maximum of the pairwise similarity
between their associated years:
simtime(a, b) = max?i?a
?j?b
{0, simyear(ai, bj)}
We substituted the preliminary time similarity
score by the measure obtained using the method pre-
sented in this section.
3 Applying Machine Learning
The above heuristics can be good indicators for the
respective kind of similarity, and can be thus applied
directly to the task. In this section, we take those
indicators as features, and use linear regression (as
made available by Weka (Hall et al, 2009)) to learn
models that fit the features to the training data.
We generated further similarity scores for gen-
eral similarity, including Latent Dirichlet Allocation
(LDA) (Blei et al, 2003), UKB and Wikipedia Link
Vector Model (WLVM)(Milne, 2007) using infor-
mation taken from all fields, as explained below.
3.1 LDA
LDA (Blei et al, 2003) is a statistical method that
learns a set of latent variables called topics from a
training corpus. Given a topic model, documents
can be inferred as probability distributions over top-
ics, ?. The distribution for a document i is denoted
as ?i. An LDA model is trained using the train-
ing set consisting of 100 topics using the gensim
package4. The hyperparameters (?, ?) were set to
1
num of topics . Therefore, each item in the test set is
represented as a topic distribution.
The similarity between a pair of items is estimated
by comparing their topic distributions following the
method proposed in Aletras et al (2012; Aletras and
Stevenson (2012). This is achieved by considering
each distribution as a vector (consisting of the topics
corresponding to an item and its probability) then
computing the cosine of the angle between them, i.e.
simLDA(a, b) =
~?a ? ~?b
|~?a| ? | ~?b|
where ~?a is the vector created from the probability
distribution generated by LDA for item a.
3.2 Pairwise UKB
We run UKB (Section 2.2) to generate a probabil-
ity distribution over WordNet synsets for all of the
words of all items. Similarity between two words
is computed by creating vectors from these distri-
butions and comparing them using the cosine of the
angle between the two vectors. If a words does not
appear in WordNet its similarity value to every other
word is set to 0. We refer to that similarity metric as
UKB here.
Similarity between two items is computed by per-
forming pairwise comparison between their words,
for each, selecting the highest similarity score:
sim(a, b) =
1
2
(?
w1?a
argmaxw2?b UKB(w1, w2)
|a|
+
?
w2?b
argmaxw1?a UKB(w2, w1)
|b|
)
where a and b are two items, |a| the number of
tokens in a and UKB(w1, w2) is the similarity be-
tween words w1 and w2.
3.3 WLVM
An algorithm described by Milne and Witten (2008)
associates Wikipedia articles which are likely to be
relevant to a given text snippet using machine learn-
ing techniques. We make use of that method to rep-
resent each item as a set of likely relevant Wikipedia
4http://pypi.python.org/pypi/gensim
134
articles. Then, similarity between Wikipedia arti-
cles is measured using the Wikipedia Link Vector
Model (WLVM) (Milne, 2007). WLVM uses both
the link structure and the article titles of Wikipedia
to measure similarity between two Wikipedia arti-
cles. Each link is weighted by the probability of it
occurring. Thus, the value of the weight w for a link
x? y between articles x and y is:
w(x? y) = |x? y| ? log
(
t?
z=1
t
z ? y
)
where t is the total number of articles in Wikipedia.
The similarity of articles is compared by forming
vectors of the articles which are linked from them
and computing the cosine of their angle. For exam-
ple the vectors of two articles x and y are:
x = (w(x? l1), w(x? l2), ..., w(x? ln))
y = (w(y ? l1), w(y ? l2), ..., w(y ? ln))
where x and y are two Wikipedia articles and x? li
is a link from article x to article li.
Since the items have been mapped to Wikipedia
articles, similarity between two items is computed
by performing pairwise comparison between articles
using WLVM, for each, selecting the highest simi-
larity score:
sim(a, b) =
1
2
(?
w1?a
argmaxw2?bWLVM(w1, w2)
|a|
+
?
w2?b
argmaxw1?aWLVM(w2, w1)
|b|
)
where a and b are two items, |a| the number of
Wikipedia articles in a and WLVM(w1, w2) is the
similarity between concepts w1 and w2.
4 Submissions
We selected three systems for submission. The first
run uses the similarity scores of the basic system
(Section 2) for each similarity types as follows:
? General: cosine similarity of TF.IDF vectors,
IDF based on Wikipedia (as shown in Section
2.1).
? Author: product of the scores obtained ob-
tained using TF.IDF vectors and UKB (as
shown in Section 2.2) using only the data ex-
tracted from dc:Creator field.
? People involved and location: product of co-
sine similarity of TF.IDF vectors and UKB (as
shown in Section 2.2) using the data extracted
from all fields.
? Time period: time similarity measure (as
shown in Section 2.3).
? Events: product of cosine similarity of TF.IDF
vectors and UKB (as shown in Section 2.2) of
event nouns and verbs recognized in all fields.
? Subject and description: cosine similarity of
TF.IDF vectors of respective fields (as shown
in Section 2).
For the second run we trained a ML model for
each of the similarity types, using the following fea-
tures:
? Cosine similarity of TF.IDF vectors as shown
in Section 2 for the eight similarity types.
? Four new values for general similarity: TF.IDF
(Section 2.1), LDA (Section 3.1), UKB and
WLVM (Section 3.3).
? Time similarity as shown in Section 2.3.
? Events similarity computed using UKB initial-
ized with the event nouns and verbs in all fields.
We decided not to use the product of TF.IDF
and UKB presented in Section 2.2 in this system
because our intention was to measure the power of
the linear regression ML algorithm to learn on the
given raw data.
The third run is similar, but includes all available
features (21). In addition to the above, we included:
? Author, people involved and location similar-
ity computed using UKB initialized with peo-
ple/location recognized by NERC in dc:Creator
field for author, and in all fields for people in-
volved and location.
? Author, people involved, location and event
similarity scores computed by the product of
TF.IDF vectors and UKB values as shown in
Section 2.2.
5 Results
Evaluation was carried out using the official scorer
provided by the organizers, which computes the
Pearson Correlation score for each of the eight sim-
ilarity types plus an additional mean correlation.
135
Team and run General Author People involved Time Location Event Subject Description Mean
UBC UOS-RUN1 0.7269 0.4474 0.4648 0.5884 0.4801 0.2522 0.4976 0.5389 0.5033
UBC UOS-RUN2 0.7777 0.6680 0.6767 0.7609 0.7329 0.6412 0.7516 0.8024 0.7264
UBC UOS-RUN3 0.7866 0.6941 0.6965 0.7654 0.7492 0.6551 0.7586 0.8067 0.7390
Table 1: Results of our systems on the training data, using cross-validation when necessary.
Team and run General Author People involved Time Location Event Subject Description Mean Rank
UBC UOS-RUN1 0.7256 0.4568 0.4467 0.5762 0.4858 0.3090 0.5015 0.5810 0.5103 6
UBC UOS-RUN2 0.7457 0.6618 0.6518 0.7466 0.7244 0.6533 0.7404 0.7751 0.7124 4
UBC UOS-RUN3 0.7461 0.6656 0.6544 0.7411 0.7257 0.6545 0.7417 0.7763 0.7132 3
Table 2: Results of our submitted systems.
5.1 Development
The three runs mentioned above were developed us-
ing the training data made available by the organiz-
ers. In order to avoid overfitting we did not change
the default parameters of the linear regressor, and
10-fold cross-validation was used for evaluating the
models on the training data. The results of our sys-
tems on the training data are shown on Table 1. The
table shows that the heuristics (RUN1) obtain low
results, and that linear regression improves results
considerably in all types. Using the full set of fea-
tures, RUN3 improves slightly over RUN2, but the
improvement is consistent across all types.
5.2 Test
The test dataset was composed of 750 pairs of items.
Table 2 illustrates the results of our systems in the
test dataset. The results of the runs are very similar
to those obtained on the training data, but the dif-
ference between RUN2 and RUN3 is even smaller.
Our systems were ranked #3 (RUN 3), #4 (RUN
2) and #6 (RUN 1) among 14 systems submitted
by 6 teams. Our systems achieved good correlation
scores for almost all similarity types, with the excep-
tion of author similarity, which is the worst ranked
in comparison with the rest of the systems.
6 Conclusions and Future Work
In this paper, we presented the systems submitted
to the *SEM 2013 shared task on Semantic Tex-
tual Similarity. We combined some simple heuris-
tics for each type of similarity, based on the appro-
priate metadata fields. The use of lineal regression
improved the results considerably across all types.
Our system fared well in the competition. We sub-
mitted three systems and the highest-ranked of these
achieved the third best results overall.
Acknowledgements
This work is partially funded by the PATHS
project (http://paths-project.eu) funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 270082. Aitor Gonzalez-Agirre is supported by
a PhD grant from the Spanish Ministry of Education,
Culture and Sport (grant FPU12/06243).
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th conference of the European chapter of
the Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Eneko Agirre, Montse Cuadros, German Rigau, and Aitor
Soroa. 2010. Exploring knowledge bases for sim-
ilarity. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC10). European Language Resources Associa-
tion (ELRA). ISBN: 2-9517408-6-7. Pages 373?377.?.
Nikolaos Aletras and Mark Stevenson. 2012. Computing
similarity between cultural heritage items using multi-
modal features. In Proceedings of the 6th Workshop
on Language Technology for Cultural Heritage, So-
cial Sciences, and Humanities, pages 85?93, Avignon,
France.
Nikolaos Aletras, Mark Stevenson, and Paul Clough.
2012. Computing similarity between items in a digi-
tal library of cultural heritage. J. Comput. Cult. Herit.,
5(4):16:1?16:19, December.
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern In-
formation Retrieval. Addison Wesley Longman Lim-
ited, Essex.
136
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022, March.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
D. Milne and I. Witten. 2008. Learning to Link
with Wikipedia. In Proceedings of the ACM Con-
ference on Information and Knowledge Management
(CIKM?2008), Napa Valley, California.
D. Milne. 2007. Computing semantic relatedness using
Wikipedia?s link structure. In Proceedings of the New
Zealand Computer Science Research Student Confer-
ence.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
137
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81?91,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 10: Multilingual Semantic Textual Similarity
Eneko Agirre
a
, Carmen Banea
b?
, Claire Cardie
c
, Daniel Cer
d
, Mona Diab
e?
Aitor Gonzalez-Agirre
a
, Weiwei Guo
f
, Rada Mihalcea
b
, German Rigau
a
, Janyce Wiebe
g
a
University of the Basque Country
Basque Country, Spain
b
University of Michigan
Ann Arbor, MI
c
Cornell University
Ithaca, NY
d
Google Inc.
Mountain View, CA
e
George Washington University
Washington, DC
f
Columbia University
New York, NY
g
University of Pittsburgh
Pittsburgh, PA
Abstract
In Semantic Textual Similarity, systems
rate the degree of semantic equivalence
between two text snippets. This year,
the participants were challenged with new
data sets for English, as well as the in-
troduction of Spanish, as a new language
in which to assess semantic similarity.
For the English subtask, we exposed the
systems to a diversity of testing scenar-
ios, by preparing additional OntoNotes-
WordNet sense mappings and news head-
lines, as well as introducing new gen-
res, including image descriptions, DEFT
discussion forums, DEFT newswire, and
tweet-newswire headline mappings. For
Spanish, since, to our knowledge, this is
the first time that official evaluations are
conducted, we used well-formed text, by
featuring sentences extracted from ency-
clopedic content and newswire. The an-
notations for both tasks leveraged crowd-
sourcing. The Spanish subtask engaged 9
teams participating with 22 system runs,
and the English subtask attracted 15 teams
with 38 system runs.
1 Introduction and motivation
Given two snippets of text, Semantic Textual Sim-
ilarity (STS) captures the notion that some texts
are more similar than others, measuring their de-
gree of semantic equivalence. Textual similar-
ity can range from complete unrelatedness to ex-
act semantic equivalence, and a graded similar-
ity intuitively captures the notion of intermediate
?
carmennb@umich.edu, mtdiab@gwu.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shades of similarity, as pairs of text may differ
from some minor nuanced aspects of meaning, to
relatively important semantic differences, to shar-
ing only some details, or to simply being related
to the same topic (cf. Section 2).
One of the goals of the STS task is to create a
unified framework for combining several seman-
tic components that otherwise have historically
tended to be evaluated independently and with-
out characterization of impact on NLP applica-
tions. By providing such a framework, STS al-
lows for an extrinsic evaluation of these modules.
Moreover, such an STS framework itself could in
turn be evaluated intrinsically and extrinsically as
a grey/black box within various NLP applications
such as Machine Translation (MT), Summariza-
tion, Generation, Question Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of
NLP tasks. STS is different from TE inasmuch
as it assumes bidirectional graded equivalence be-
tween the pair of textual snippets. In the case of
TE the equivalence is directional, e.g. a car is a
vehicle, but a vehicle is not necessarily a car. STS
also differs from both TE and Paraphrasing (in as
far as both tasks have been defined to date in the
literature) in that, rather than being a binary yes/no
decision (e.g. a vehicle is not a car), we define
STS to be a graded similarity notion (e.g. a ve-
hicle and a car are more similar than a wave and
a car). A quantifiable graded bidirectional notion
of textual similarity is useful for a myriad of NLP
tasks such as MT evaluation, information extrac-
tion, question answering, summarization, etc.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al., 2012). In addition, we held
81
year dataset pairs source
2012 MSRpar 1500 newswire
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 newswire
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2014 HDL 750 newswire headlines
2014 OnWN 750 glosses
2014 Deft-forum 450 forum posts
2014 Deft-news 300 news summary
2014 Images 750 image descriptions
2014 Tweet-news 750 tweet-news pairs
Table 2: English subtask: Summary of train (2012
and 2013) and test (2014) datasets.
a DARPA sponsored workshop at Columbia Uni-
versity.
1
In 2013, STS was selected as the offi-
cial Shared Task of the *SEM 2013 conference,
with two subtasks: The Core task, which is sim-
ilar to the 2012 task; and a Pilot task on Typed-
similarity between semi-structured records. The
Core task attracted 34 participants with 89 runs,
and the Typed-similarity task attracted 6 teams
with 14 runs.
For STS 2014 we defined two subtasks: En-
glish and Spanish. For the English subtask we pro-
vided five test datasets: two datasets that extend
already released genres (the OntoNotes-WordNet
sense mappings and news headlines) and three
new genres: image descriptions, DEFT discus-
sion forum data and newswire, as well as tweet-
newswire headline mappings. Participants could
use all datasets released in 2012 and 2013 as train-
ing data. The Spanish subtask introduced two di-
verse datasets on different genres, namely ency-
clopedic descriptions extracted from the Spanish
Wikipedia and contemporary Spanish newswire.
For the Spanish subtask, the participants had ac-
cess to a limited amount of labeled data, consist-
ing of 65 sentence pairs, which they could use for
training.
2 Task Description
2.1 English Subtask
The English dataset comprises pairs of news head-
lines (HDL), pairs of glosses (OnWN), image de-
scriptions (Images), DEFT-related discussion fo-
rums (Deft-forum) and news (Deft-news), and
1
http://www.cs.columbia.edu/
?
weiwei/
workshop/
tweet comments and newswire headline mappings
(Tweets).
For HDL, we used naturally occurring news
headlines gathered by the Europe Media Moni-
tor (EMM) engine (Best et al., 2005) from sev-
eral different news sources. EMM clusters to-
gether related news. Our goal was to generate
a balanced data set across the different similar-
ity ranges, hence we built two sets of headline
pairs: (i) a set where the pairs come from the same
EMM cluster, (ii) and another set where the head-
lines come from a different EMM cluster, then
we computed the string similarity between those
pairs. Accordingly, we sampled 375 headline pairs
of headlines that occur in the same EMM cluster,
aiming for pairs equally distributed between min-
imal and maximal similarity using simple string
similarity. We sampled other 375 pairs from the
different EMM cluster in the same manner.
For OnWN, we used the sense definition pairs
of OntoNotes (Hovy et al., 2006) and WordNet
(Fellbaum, 1998). Different from previous tasks,
the two definition sentences in a pair belong to dif-
ferent senses. We sampled 750 pairs based on a
string similarity ranging from 0.5 to 1.
The Images data set is a subset of the PAS-
CAL VOC-2008 data set (Rashtchian et al., 2010),
which consists of 1,000 images and has been used
by a number of image description systems. It was
also sampled from string similarity values between
0.6 and 1.
Deft-forum and Deft-news are from DEFT
data.
2
Deft-forum contains the forum post sen-
tences, and Deft-news are news summaries. We
selected 450 pairs for Deft-forum and 300 pairs for
Deft-news. They are sampled evenly from string
similarities falling in the interval 0.6 to 1.
The Tweets data set contains tweet-news pairs
selected from the corpus released in (Guo et al.,
2013), where each pair contains a sentence that
pertains to the news title, while the other one rep-
resents a Twitter comment on that particular news.
They are evenly sampled from string similarity
values between 0.5 and 1.
Table 1 shows the explanations and values as-
sociated with each score between 5 and 0. As
in prior years, we used Amazon Mechanical Turk
(AMT)
3
to crowdsource the annotation of the En-
glish pairs.
4
Annotators are presented with the
2
LDC2013E19, LDC2012E54
3
www.mturk.com
4
For STS 2013, we used CrowdFlower as a front-end to
82
Score English Spanish
5/4 The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
El p?ajaro se esta ba?nando en el lavabo.
El p?ajaro se est?a lavando en el aguamanil.
4 The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade
Kabul.
The US army invaded Kabul on May 7th last
year, 2010.
3 The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a
suspect.
?He is not a suspect anymore.? John said.
John dijo que ?el es considerado como testigo, y
no como sospechoso.
?
?
El ya no es un sospechoso,? John dijo.
2 The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
Ellos volaron del nido en grupos.
Volaron hacia el nido juntos.
1 The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
La mujer est?a tocando el viol??n.
La joven disfruta escuchar la guitarra.
0 The two sentences are completely dissimilar.
John went horse back riding at dawn with a
whole group of friends.
Sunrise at dawn is a magnificent view to take
in if you wake up early enough for it.
Al amanecer, Juan se fue a montar a caballo
con un grupo de amigos.
La salida del sol al amanecer es una magn??fica
vista que puede presenciar si usted se despierta
lo suficientemente temprano para verla.
Table 1: Similarity scores with explanations and examples for the English and Spanish subtasks, where
the sentences in Spanish are translations of the English ones.
A similarity score of 5 in English is mirrored by a maximum score of 4 in Spanish; the definitions pertaining to scores 3 and 4
in English were collapsed under a score of 3 in Spanish, with the definition ?The two sentences are mostly equivalent, but some
details differ.?
detailed instructions provided in Figure 1, and
are asked to label each STS sentence pair on our
six point scale, selecting from a dropdown box.
Five sentence pairs are presented to each annota-
tor at once, per human intelligence task (HIT), at
a payrate of $0.20; we collect five separate anno-
tations per sentence pair. Annotators were only el-
igible to work on the task if they had the Mechan-
ical Turk Master Qualification. This is a special
Amazon Mechanical Turk, since it provides numerous useful
tools to assist in running a successful annotation project using
crowdsourcing, such as support for hidden ?golden? questions
that can be used both to train annotators and to automatically
stop people who repeatedly make mistakes from contribut-
ing to the task. However, in 2013, CrowdFlower dropped
Amazon Mechanical Turk as an annotation source. When we
tried running pairs for STS 2014 on CrowdFlower using the
same templates that were successfully used for the 2013 task,
we found that we obtained significantly degraded annotation
quality, with an average Pearson (AMT provider vs. rest of
AMT providers) of only 22.8%. In contrast, when we ran the
task for 2014 on AMT, we obtained a one-vs-rest annotation
of 73.6%.
qualification conferred by AMT (using a priority
statistical model) to annotators who consistently
maintain a very high level of quality across a vari-
ety of tasks from numerous requesters). Access to
these skilled workers entails a 20% surcharge.
To monitor the quality of the annotations, we
use the gold dataset of 105 pairs that were manu-
ally annotated by the task organizers during STS
2013. We include one of these gold pairs in each
set of five sentence pairs, where the gold pairs are
indistinguishable from the rest. Unlike when we
ran on CrowdFlower for STS 2013, the gold pairs
are not used for training purposes, nor are workers
automatically banned from the task if they make
too many mistakes on annotating them. Rather, the
gold pairs are only used to help in identifying and
removing the data associated with poorly perform-
ing annotators. With few exceptions, 90% of the
answers from each individual annotator fall within
+/-1 of the answers selected by the organizers for
83
Figure 1: Annotation instructions for English subtask.
the gold dataset.
The distribution of scores obtained from the
AMT providers in the Deft-forum, Deft-news,
OnWN and tweet-news datasets is roughly uni-
form across the different grades of similarity, al-
though the scores are slightly higher for tweet-
news. Compared to the other data sets, the scores
for OnWN, were more bimodal, ranging between
4.6 to 5 and 0 to 0.4, when compared to middle
values (2.6-3.4).
In order to assess the annotation quality, we
measure the correlation of each annotator with the
average of the rest of the annotators, and then aver-
age the results. This approach to estimate the qual-
ity is identical to the method used for evaluations
(see Section 3), and it can thus be considered as
the upper bound of the systems. The inter-tagger
correlation for each English dataset is as follows:
? HDL: 79.4%
? OnWN: 67.2%
? Deft-forum: 58.6%
? Deft-news: 70.7%
? Images: 83.6%
? Tweets-news: 74.4%
The correlation figures are generally high (over
70%), with the exception of the OnWN and Deft
datasets, which score 67.2% and 58.6%, respec-
tively. The reason for the low inter-tagger correla-
tion on OnWN compared to the higher correlations
in previous years is that we only used unmapped
sense definitions, i.e., the two sentences in a pair
belong to two different senses. For the Deft-forum
dataset, we found that similarity values tend to be
lower than in the other datasets, and more annota-
tion disagreements happen in these low similarity
values.
2.2 Spanish Subtask
The Spanish subtask follows a setup similar to the
English subtask, except that the similarity scores
were adapted to fit a range from 0 to 4 (see Table
1). We thought that the distinction between a score
of 3 and 4 for the English task will pose more dif-
ficulty for us in conveying into Spanish, as the sole
difference between the two lies in how the annota-
tors perceive the importance of additional details
or missing information with respect to the core se-
mantic interpretation of the pair. As this aspect en-
tails a subjective judgement, and since it is the first
time that a Spanish STS evaluation is organized,
we casted the annotation guidelines into straight-
forward and unambiguous instructions, and thus
opted to use a similarity range from 0 to 4.
Prior to the evaluation window, we released 65
Spanish sentence pairs for trial / training. In or-
der to evaluate system performance under differ-
84
ent scenarios, we developed two test datasets, one
extracted from the Spanish Wikipedia
5
(December
2013 dump) and one from contemporary news ar-
ticles collected from media in Spanish (February
2014).
2.2.1 Spanish Wikipedia
The Wikipedia dump was processed using the
Parse::MediaWikiDump Perl library. We removed
all titles, html tags, wiki tags and hyperlinks
(keeping only the surface forms). Each article was
split into paragraphs, where the first paragraph
was considered to be the article?s abstract, while
the remaining ones were deemed to be its content.
Each of these were split into sentences using the
Perl library Uplug::PreProcess::SentDetect, and
only the sentences longer than eight words were
used. We iteratively computed the lexical simi-
larity
6
between every sentence in the abstract and
every sentence in the content, and retained those
pairs whose sentence length ratio was higher than
0.5, and their similarity scored over 0.35.
The final set of sentence pairs was split into five
bins, and their scores normalized to range from
0 to 1. The more interesting and difficult pairs
were found, perhaps not surprisingly, in bins 0 and
1, where synonyms/short paraphrases where more
frequent. An example extracted from those bins,
where the text in italics highlights the differences
between the two sentences:
? ?America? es el segundo continente m?as
grande del planeta, despu?es de Asia.
?America? is the second largest continent in the world,
following Asia.
? America corresponde a la segunda masa de
tierra m?as grande del planeta, luego de Asia.
America is the second largest land mass on the planet,
after Asia.
The Spanish verb ?Es? maps to (En:
7
is), ?cor-
responde a? (En: corresponds to), the phrase ?el
segundo continente? (En: the second continent) is
equivalent to ?la segunda masa de tierra? (the sec-
ond land mass), and ?despues? (En: following) to
?luego? (En: after). Despite the difference in vo-
cabulary choice, the two sentences are paraphrases
of each other.
From the candidate pairs, we manually selected
324 sentence pairs, in order to ensure a diverse
5
es.wikipedia.org
6
Algorithm based on the Linux diff command (Algo-
rithm::Diff Perl module).
7
?En? stands for English.
and challenging set. This set was annotated in two
ways, first by two graduate students in Computer
Science who are native speakers of Spanish, and
second by using AMT.
The AMT framework was set up to contain
seven sentence pairs per HIT, where six of them
were part of the test dataset, while one was used
for control. AMT providers were eligible to com-
plete a task if they had more than 500 accepted
HITs, with 90%+ acceptance rate.
8
We paid $0.30
per HIT, and each HIT was annotated by five AMT
providers. We sought to ensure that only Spanish
speaking annotators would complete the HITs by
providing all the information related to the task (its
title, abstract, description, guidelines and exam-
ples), as well as the control pair in Spanish only.
The participants were instructed to label the pairs
on a scale from 0 to 4 (see Table 1). Each sentence
pair was followed by a comment text box, which
the AMT providers used to provide the topic of the
sentences, corrections, etc.
The two students achieved a Pearson correla-
tion of 0.6974 on the Wikipedia dataset. To see
how their judgement compares to the crowd wis-
dom, we averaged the AMT scores for each pair,
and computed their correlation with our annota-
tors, obtaining 0.824 and 0.742, respectively. Sur-
prisingly enough, both these correlation values are
higher than the correlation among the annotators
themselves. When averaging the annotator scores
and comparing them with the AMT providers?
average score per pair, the correlation becomes
0.8546, indicating that the task is well defined,
and that the annotations contributed by the AMT
providers are of satisfactory quality. Given these
scores, the gold standard was annotated using the
average AMT provider judgement per pair.
2.2.2 Spanish News
The second Spanish dataset was extracted from
news articles published in Spanish language me-
dia from around the world in February 2014. The
hyperlinks to the articles were obtained by pars-
ing the ?International? page of Spanish Google
News,
9
which aggregates or clusters in real time
articles describing a particular event from a di-
verse pool of news sites, where each grouping
8
Initially, Amazon had automatically upgraded our anno-
tation task to require Master level providers (as those partici-
pating in the English annotations), yet after approximately 4
days, no HIT had been completed.
9
news.google.es
85
is labeled with the title of one of the predomi-
nant articles. By leveraging these clusters of links
pointing to the sites where the articles were orig-
inally published, we are able to gather raw text
that has a high probability of containing seman-
tically similar sentences. We encountered several
difficulties while mining the articles, ranging from
each article having its own formatting depend-
ing on the source site, to advertisements, cookie
requirements, to encoding for Spanish diacritics.
We used the lynx text-based browser,
10
which was
able to standardize the raw articles to a degree.
The output of the browser was processed using a
rule based approach taking into account continu-
ous text span length, ratio of symbols and num-
bers to the text, etc., in order to determine when
a paragraph is part of the article content. After
that, a second pass over the predictions corrected
mislabeled paragraphs if they were preceded and
followed by paragraphs identified as content. All
the content pertaining to articles on the same event
was joined, sentence split, and diff pairwise simi-
larities were computed. The set of candidate sen-
tences followed the same requirements as for the
Wikipedia dataset, namely length ratio higher than
0.5 and similarity score over 0.35. From these, we
manually extracted 480 sentence pairs which were
deemed to pose a challenge to an automated sys-
tem.
Due to the high correlations obtained between
the AMT providers? scores and the annotators?
scores on Wikipedia, the news dataset was only
annotated using AMT, following exactly the same
task setup as for Wikipedia.
3 Evaluation
Evaluation of STS is still an open issue.
STS experiments have traditionally used Pearson
product-moment correlation between the system
scores and the GS scores, or, alternatively, Spear-
man rank order correlation. In addition, we also
need a method to aggregate the results from each
dataset into an overall score. The analysis per-
formed in (Agirre and Amig?o, In prep) shows that
Pearson and averaging across datasets are the best
suited combination in general. In particular, Pear-
son is more informative than Spearman, in that
Spearman only takes the rank differences into ac-
count, while Pearson does account for value dif-
ferences as well. The study also showed that other
10
lynx.browser.org
alternatives need to be considered, depending on
the requirements of the target application.
We leave application-dependent evaluations for
future work, and focus on average Pearson correla-
tion. When averaging, we weight each individual
correlation by the size of the dataset. In order to
compute statistical significance among system re-
sults, we use a one-tailed parametric test based on
Fisher?s z-transformation (Press et al., 2002, equa-
tion 14.5.10). In addition, English subtask partic-
ipants could provide an optional confidence mea-
sure between 0 and 100 for each of their predic-
tions. Team RTM-DCU is the only one who has
provided these, and the evaluation of their runs us-
ing weighted Pearson (Pozzi et al., 2012) is listed
at the end of Table 3.
Participants
11
could take part in the shared task
with a maximum of 3 system runs per subtask.
3.1 English Subtask
In order to provide a simple word overlap baseline
(Baseline-tokencos), we tokenize the input sen-
tences splitting on white spaces, and then repre-
sent each sentence as a vector in the multidimen-
sional token space. Each dimension has 1 if the to-
ken is present in the sentence, 0 otherwise. Vector
similarity is computed using the cosine similarity
metric.
We also run the freely available system, Take-
Lab (
?
Sari?c et al., 2012), which yielded state of the
art performance in STS 2012 and strong results
out-of-the-box in 2013.
12
15 teams participated in the English subtask,
submitting 38 system runs. One team submitted
the results past the deadline, as explicitly marked
in Table 3. After the submission deadline expired,
the organizers published the gold standard and par-
ticipant submissions on the task website, in order
to ensure a transparent evaluation process.
Table 3 shows the results of the English sub-
task, with runs listed in alphabetical order. The
correlation in each dataset is given, followed
11
Participating teams: Bielefeld SC (McCrae et al.,
2013), BUAP (Vilari?no et al., 2014), DLS@CU (Sultan et
al., 2014b), FBK-TR (Vo et al., 2014), IBM EG (no in-
formation), LIPN (Buscaldi et al., 2014), Meerkat Mafia
(Kashyap et al., 2014), NTNU (Lynum et al., 2014), RTM-
DCU (Bic?ici and Way, 2014), SemantiKLUE (Proisi et al.,
2014), StanfordNLP (Socher et al., 2014), TeamZ (Gupta,
2014), UMCC DLSI SemSim (Chavez et al., 2014), UNAL-
NLP (Jimenez et al., 2014), UNED (Martinez-Romo et al.,
2011), UoW (Rios, 2014).
12
Code is available at http://ixa2.si.ehu.es/
stswiki
86
Run Name deft deft Headl images OnWN tweet Weighted mean Rank
forum news news
Baseline-tokencos 0.353 0.596 0.510 0.513 0.406 0.654 0.507 -
TakeLab 0.333 0.716 0.720 0.742 0.793 0.650 0.678 -
Bielefeld SC-run1 0.211 0.432 0.321 0.368 0.367 0.415 0.354 32
Bielefeld SC-run2 0.211 0.431 0.311 0.356 0.361 0.409 0.347 33
BUAP-EN-run1 0.456 0.686 0.689 0.697 0.654 0.771 0.671 19
DLS@CU-run1 0.483 0.766 0.765 0.821 0.723 0.764 0.734 7
DLS@CU-run2 0.483 0.766 0.765 0.821 0.859 0.764 0.761 1
FBK-TR-run1 0.322 0.523 0.547 0.601 0.661 0.462 0.535 25
FBK-TR-run2 0.167 0.421 0.485 0.521 0.572 0.359 0.441 28
FBK-TR-run3 0.305 0.405 0.471 0.489 0.551 0.438 0.459 27
IBM EG-run1 0.474 0.743 0.737 0.801 0.760 0.730 0.722 8
IBM EG-run2 0.464 0.641 0.710 0.747 0.732 0.696 0.684 15
LIPN-run1 0.454 0.640 0.653 0.809 - 0.551 0.508 26
LIPN-run2 0.084 - - - - - 0.010 35
Meerkat Mafia-Hulk 0.449 0.785 0.757 0.790 0.787 0.757 0.735 6
Meerkat Mafia-pairingWords 0.471 0.763 0.760 0.801 0.875 0.779 0.761 2
Meerkat Mafia-SuperSaiyan 0.492 0.771 0.767 0.768 0.802 0.765 0.741 5
NTNU-run1 0.437 0.714 0.722 0.800 0.835 0.411 0.663 20
NTNU-run2 0.508 0.766 0.753 0.813 0.777 0.792 0.749 4
NTNU-run3 0.531 0.781 0.784 0.834 0.850 0.675 0.755 3
SemantiKLUE-run1 0.337 0.608 0.728 0.783 0.848 0.632 0.687 14
SemantiKLUE-run2 0.349 0.643 0.733 0.773 0.855 0.640 0.694 13
StanfordNLP-run1 0.319 0.635 0.636 0.758 0.627 0.669 0.627 22
StanfordNLP-run2 0.304 0.679 0.621 0.715 0.625 0.636 0.610 24
StanfordNLP-run3 0.342 0.650 0.602 0.754 0.609 0.638 0.614 23
UMCC DLSI SemSim-run1 0.475 0.662 0.632 0.742 0.813 0.675 0.682 16
UMCC DLSI SemSim-run2 0.469 0.662 0.625 0.739 0.814 0.654 0.676 18
UMCC DLSI SemSim-run3 0.283 0.385 0.267 0.436 0.603 0.278 0.381 30
UNAL-NLP-run1 0.504 0.721 0.762 0.807 0.782 0.614 0.711 12
UNAL-NLP-run2 0.383 0.730 0.765 0.771 0.827 0.403 0.657 21
UNAL-NLP-run3 0.461 0.722 0.761 0.778 0.843 0.658 0.721 9
UNED-run22 p np 0.104 0.315 0.037 0.324 0.509 0.490 0.310 34
UNED-runS5K 10 np 0.118 0.506 0.057 0.498 0.488 0.579 0.379 31
UNED-runS5K 3 np 0.094 0.564 0.018 0.607 0.577 0.670 0.431 29
UoW-run1 0.342 0.751 0.754 0.776 0.799 0.737 0.714 11
UoW-run2 0.342 0.587 0.754 0.788 0.799 0.628 0.682 17
UoW-run3 0.342 0.763 0.754 0.788 0.799 0.753 0.721 10
?RTM-DCU-run1 0.434 0.697 0.620 0.699 0.806 0.688 0.671
?RTM-DCU-run2 0.397 0.681 0.613 0.666 0.799 0.669 0.651
?RTM-DCU-run3 0.308 0.556 0.630 0.647 0.800 0.553 0.608
?RTM-DCU-run1 0.418 0.685 0.622 0.698 0.833 0.687 0.673
?RTM-DCU-run2 0.383 0.674 0.609 0.663 0.826 0.669 0.653
?RTM-DCU-run3 0.273 0.553 0.633 0.644 0.825 0.568 0.611
Table 3: English evaluation results. Results at the top correspond to out-of-the-box systems. Results at
the bottom correspond to results using the confidence score.
Notes: ?-? for not submitted, ??? for post-deadline submission.
87
by the mean correlation (the official measure),
and the rank of the run. The highest correla-
tions are for OnWN (87.5%, by Meerkat Mafia)
and images (83.4%, by NTNU), followed by
Tweets (79.2%, by NTNU), HEADL (78.4%, by
NTNU) and deft news and forums (78.1% and
53.1%, respectively, by NTNU). Compared to the
inter-annotator agreement correlation, the ranking
among datasets is very similar, with the exception
of OnWN, as it gets the best score but has very low
agreement. One possible reason is that the partic-
ipants used previously available data. The results
of the best 4 top system runs are significantly dif-
ferent (p-value < 0.05) from the 5th top scoring
system run and below. The top 4 systems did not
show statistical significant variation among them.
Only three runs (cf. lower rows in Table 3) in-
cluded non-uniform confidence scores, barely af-
fecting their ranking.
Interestingly, the two top performing systems
on the English STS sub-task are both unsuper-
vised. DLS@CU (Sultan et al., 2014b) presents
an unsupervised algorithm which predicts the STS
score based on the proportion of word alignments
in the two sentences. Two related words are
aligned depending on how similar the two words
are, and also on how similar the contexts of the
words are in the respective sentences (Sultan et al.,
2014a). Meerkat Mafia pairingWords (Kashyap
et al., 2014) also follows a fully unsupervised ap-
proach. The authors train LSA on an English cor-
pus of three billion words using a sliding window
approach, resulting in a vocabulary size of 29,000
words associated with 300 dimensions. They ac-
count for named entities and out-of-vocabulary
words by leveraging external resources such as
DBpedia
13
and Wordnik.
14
In Spanish, the sys-
tem equivalent to this run ranked second following
a cross-lingual approach, by applying the English
system to the translated version of the dataset (see
3.2).
The Table also shows the results of TakeLab,
which was trained with all datasets from previ-
ous years. TakeLab would rank 18th, ten absolute
points below the best system, a smaller difference
than in 2013.
13
dbpedia.org
14
wordnick.com
3.2 Spanish Subtask
The Spanish subtask attracted 9 teams with 22
participating systems, out of which 16 were su-
pervised and 6 unsupervised. The participants
were from both Spanish (Colombia, Cuba, Mex-
ico, Spain), and non-Spanish speaking countries
(two teams from France, Germany, Ireland, UK,
US). The evaluation results appear in Table 4.
The top ranking system is the 2nd run of
UMCC DLSI SemSim (Chavez et al., 2014),
which achieves a weighted correlation of 0.807. It
entails a cross-lingual approach, as it leverages a
SVM-based English framework, by mapping the
Spanish words to their English equivalent using
the most common sense in WordNet 3.0. The clas-
sifier uses a combination of features, such as those
derived from traditional knowledge-based ((Lea-
cock and Chodorow, 1998; Wu and Palmer, 1994;
Lin, 1998), and others) and corpus-based metrics
(LSA (Landauer et al., 1997)), paired with lexi-
cal features (such as Dice-Similarity, Euclidean-
distance, etc.). It is trained on a cumulative En-
glish STS dataset comprising train and test data
released as part of tasks in SemEval2012 (Agirre
et al., 2012) and *Sem 2013 (Agirre et al., 2013),
as well as training data available from tasks 1 and
10 in SemEval 2014. Interestingly enough, run 2
of the system performs better than run 1, despite
the fact that it uses half the features, and focuses
on string based similarity measures only. This dif-
ference between runs is noticed on the Wikipedia
dataset only, and it amounts to 4% Pearson corre-
lation. While the system had a robust performance
on the Spanish subtask, for English, its overall
rank was 16, 18, and 33, respectively.
Coming in close at only 0.3% difference, is
Meerkat-Mafia PairingAvg (run 2) (Kashyap et
al., 2014), which also follows a cross-lingual ap-
proach, by applying the system the team devel-
oped for the English subtask to the translated ver-
sion of the datasets (see 3.1). The interesting as-
pect of their work is that in their first submission
(run 1), they only consider the similarity result-
ing from the sentence pair translation through the
Google Translate service.
15
In the second run,
they expand each sentence to 20 possible combi-
nations by accounting for the multiple translation
meanings of a given word, and considering the av-
erage similarity of all resulting pairs. While the
first run achieves a weighted correlation of 73.8%,
15
translate.google.com
88
Run Name System type Wikipedia News Weighted mean Rank
Bielefeld-SC-run1 unsupervised* 0.263 0.554 0.437 22
Bielefeld-SC-run2 unsupervised* 0.265 0.555 0.438 21
BUAP-run1 supervised 0.550 0.679 0.627 17
BUAP-run2 unsupervised 0.640 0.764 0.714 14
RTM-DCU-run1 supervised 0.422 0.700 0.588 18
RTM-DCU-run2 supervised 0.369 0.625 0.522 20
RTM-DCU-run3 supervised 0.424 0.641 0.554 19
LIPN-run1 supervised 0.652 0.826 0.756 11
LIPN-run2 supervised 0.716 0.832 0.785 6
LIPN-run3 supervised 0.716 0.809 0.771 10
Meerkat-Mafia-run1 unsupervised 0.668 0.785 0.738 13
Meerkat-Mafia-run2 unsupervised 0.743 0.845 0.804 2
Meerkat-Mafia-run3 supervised 0.738 0.822 0.788 5
TeamZ-run1 supervised 0.610 0.717 0.674 15
TeamZ-run2 supervised 0.604 0.710 0.667 16
UMCC-DLSI-run1 supervised 0.741 0.825 0.791 4
UMCC-DLSI-run2 supervised 0.7802 0.825 0.807 1
UNAL-NLP-run1 weakly supervised 0.7803 0.815 0.801 3
UNAL-NLP-run2 supervised 0.757 0.783 0.772 9
UNAL-NLP-run3 supervised 0.689 0.796 0.753 12
UoW-run1 supervised 0.748 0.800 0.779 7
UoW-run2 supervised 0.748 0.800 0.779 8
Table 4: Spanish evaluation results in terms of Pearson correlation.
the second one performs significantly better at
80.4%, indicating that the additional context may
also include multiple instances of accurate trans-
lations, hence significantly impacting the overall
similarity score. In English, the system equiva-
lent to run 2 in Spanish, namely Meerkat Mafia-
pairingWords, achieves a competitive ranked per-
formance across all six datasets, ranking second,
at an order of 10
?4
distance from the top sys-
tem. This supports the claim that, despite its unsu-
pervised nature, the system is quite versatile and
highly competitive with the top performing super-
vised frameworks, and that it may achieve an even
higher performance in Spanish if accurate sen-
tence translations were provided.
Overall, most systems were cross-lingual, rely-
ing on different translation approaches, such as 1)
translating the test data into English (as the two
systems above), and then exporting the score ob-
tained for the English sentences back to Spanish,
or 2) performing automatic translation of the En-
glish training data, and learning a classifier di-
rectly in Spanish. (Buscaldi et al., 2014) supple-
mented their training dataset with human annota-
tions conducted in Spanish, using definition pairs
extracted from a Spanish dictionary. A different
angle was explored by (Rios, 2014), who proposed
a multilingual framework using transfer learning
across English and Spanish by training on tradi-
tional lexical, knowledge-based and corpus-based
features. The semantic similarity task was ap-
proached from a monolingual perspective as well
(Gupta, 2014), by focusing on Spanish resources,
such as the trial data we released as part of the
subtask, and the Spanish WordNet;
16
these were
leveraged using meta-learning over variations of
overlap-based metrics. Following the same line,
(Bic?ici and Way, 2014) pursued language inde-
pendent methods, who avoided relying on task or
domain specific information through the usage of
referential translation machines. This approach
models textual semantic similarity as a decision in
terms of translation quality between two datasets
(in our case Spanish STS trial and test data) given
relevant examples from an in-language reference
corpus.
In comparison to the correlations obtained in the
English subtask, where the highest weighted mean
was 76.1%, for Spanish, we obtained 80.7%, prob-
ably due to the more formal nature of the datasets,
since Wikipedia and news articles employ mostly
well formed and grammatically correct sentences,
and we selected all snippets to be longer than 8
words. The overall correlation scores obtained for
English were hurt by the deft-forum data, which
scored significantly lower (at a maximum corre-
lation of 50.8%), when compared to all the other
datasets whose correlation was higher than 70%.
The OnWN data was most similar to our test sets,
and it attained a maximum of 85.9%.
16
grial.uab.es/descarregues.php
89
4 Conclusion
This year?s STS task comprised a multilingual
flair, by introducing Spanish datasets alongside the
English ones. In English, the datasets sought to ex-
pose the participating teams to more diverse sce-
narios compared to the previous years, by intro-
ducing image descriptions, forum and newswire
genre, and tweet-newswire headline mappings.
For Spanish, two datasets were developed consist-
ing of encyclopedic and newswire text acquired
from Spanish sources. Overall, the English sub-
task attracted 15 teams (with 38 system varia-
tions), while the Spanish subtask had 9 teams
(with 22 system runs). Most teams from the Span-
ish subtask have also submitted runs for the En-
glish evaluations.
Acknowledgments
The authors are grateful to Ver?onica P?erez-Rosas
and Vanessa Loza for their help with the anno-
tations for the Spanish subtask. This material is
based in part upon work supported by National
Science Foundation CAREER award #1361274
and IIS award #1018613, by DARPA-BAA-12-
47 DEFT grant #12475008, and by MINECO
CHIST-ERA READERS and SKATER projects
(PCIN-2013-002-C02-01, TIN2012-38584-C06-
02). Aitor Gonzalez Agirre is supported by a doc-
toral grant from MINECO. Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation or the Defense Advanced Re-
search Projects Agency.
References
Eneko Agirre and Enrique Amig?o. In prep. Exploring
evaluation measures for semantic textual similarity.
In Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 385?
393, Montr?eal, Canada, 7-8 June.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic textual similarity, including a pi-
lot on typed-similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013), pages 32?43.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo
Garcia, and David Horby. 2005. Europe me-
dia monitor - system description. In EUR Report
22173-En, Ispra, Italy.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Davide Buscaldi, Jorge J. Garcia Flores, Joseph Le
Roux, Nadi Tomeh, and Belem Priego Sanchez.
2014. LIPN: Introducing a new geographical con-
text similarity measure and a statistical similarity
measure based on the Bhattacharyya coefficient. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Alexander Chavez, Hector Davila, Yoan Gutierrez,
Antonio Fernandez-Orquin, Andr?es Montoyo, and
Rafael Munoz. 2014. UMCC DLSI SemSim: Mul-
tilingual system for measuring semantic textual sim-
ilarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Christiane Fellbaum. 1998. WordNet - An electronic
lexical database. MIT Press.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013.
Linking tweets to news: A framework to enrich on-
line short text data in social media. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics, pages 239?249.
Anubhav Gupta. 2014. TeamZ: Measuring semantic
textual similarity for Spanish using an overlap-based
approach. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, pages 57?60.
Sergio Jimenez, George Due?nas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi, and
Tim Finin. 2014. Meerkat Mafia: Multilingual and
cross-level semantic textual similarity systems. In
Proceedings of the 8th International Workshop on
90
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Thomas K. Landauer, Darrell Laham, Bob Rehder, and
M. E. Schreiner. 1997. How well can passage mean-
ing be derived without using word order? A compar-
ison of latent semantic analysis and humans. Cogni-
tive Science.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In WordNet: An Elec-
tronic Lexical Database, pages 305?332.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinality. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Juan Martinez-Romo, Lourdes Araujo, Javier Borge-
Holthoefer, Alex Arenas, Jos?e A. Capit?an, and
Jos?e A. Cuesta. 2011. Disentangling categori-
cal relationships through a graph of co-occurrences.
Phys. Rev. E, 84:046108, Oct.
John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1732?1740, Seattle,
Washington, USA.
Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste.
2012. Exponential smoothing weighted correla-
tions. The European Physical Journal B, 85(6).
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
recipes: The art of scientific computing V 2.10 with
Linux or single-screen license. Cambridge Univer-
sity Press.
Thomas Proisi, Stefan Evert, Paul Greiner, and Besim
Kabashi. 2014. SemantiKLUE: Robust semantic
similarity at multiple levels using maximum weight
matching. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon?s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, CSLDAMT ?10, pages 139?147, Strouds-
burg, PA, USA.
Miguel Rios. 2014. UoW: Multi-task learning Gaus-
sian process for semantic textual similarity. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
pages 207?218.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual ev-
idence. Transactions of the Association for Compu-
tational Linguistics, 2:219?230.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014b. DLS@CU: Sentence similarity from
word aligment. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Darnes Vilari?no, David Pinto, Sa?ul Le?on, Mireya To-
var, and Beatriz Beltr?an. 2014. BUAP: Evaluating
features for multilingual and cross-level semantic
textual similarity. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Ngoc Phuoc An Vo, Tommaso Caselli, and Octavian
Popescu. 2014. FBK-TR: Applying SVM with
multiple linguistic features for cross-level semantic
similarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138, Las Cruces, New Mex-
ico.
91
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 1?10,
Beijing, August 2010
KYOTO: an open platform for mining facts
Piek Vossen
VU University Amsterdam
p.vossen@let.vu.nl
German Rigau
Eneko Agirre
Aitor Soroa
University of the Basque 
Country
german.rigau/e.a-
girre/a.soroa@ehu.es
Monica Monachini
Roberto Bartolini
Istituto di Linguistica 
Computazionale, CNR
monica.monachini/r
oberto.bartolin-
i@ilc.cnr.it
Abstract
This  document  describes  an  open 
text-mining  system  that  was  developed 
for the Asian-European project KYOTO. 
The  KYOTO system uses  an  open text 
representation format and a central onto-
logy to  enable  extraction  of  knowledge 
and facts  from large volumes of text  in 
many different languages. We implemen-
ted a semantic tagging approach that per-
forms off-line reasoning. Mining of facts 
and  knowledge  is  achieved  through  a 
flexible pattern matching module that can 
work in much the same way for different 
languages,  can  handle  efficiently  large 
volumes of documents and is not restric-
ted to a specific domain. We applied the 
system to an English database on estuar-
ies.
1 Introduction
Traditionally, Information Extraction (IE) is the 
task of filling template information from previ-
ously unseen text which belongs to a predefined 
domain (Peshkin & Pfeffer 2003). Most systems 
in  the  Message  Understanding  Conferences 
(MUC,  1987-1998)  and the  Automatic  Content 
Extraction  program  (ACE)1 use  a  pipeline  of 
tools to achieve this, ranging from sophisticated 
NLP tools (like deep parsing) to shallower text-
processing (e.g. FASTUS (Appelt 1995)).
Standard  IE  systems  are  based  on  lan-
guage-specific  pattern  matching  (Kaiser  & 
1http://www.itl.nist.gov/iad/mig//tests/ace  
Miksch 2005), where each pattern consists of a 
regular  expression  and  an  associated  mapping 
from syntactic to logical form. In general, the ap-
proaches can be categorized into two groups: (1) 
the Knowledge Engineering approach (Appelt et 
al.1995), and (2) the learning approach, such as 
AutoSlog  (Appelt  et  al.  1993),  SRV  (Freitag 
1998), or RAPIER (Califf & R. Mooney 1999). 
Another  important  system  is  GATE (Cunning-
ham et al2002), which is a platform for creating 
IE systems. It uses regular expressions, but it can 
also  use  ontologies  to  perform semantic  infer-
ences  to  constrain  linguistic  patterns  semantic-
ally. The use of ontologies in IE is an emerging 
field (Bontcheva & Wilks 2004): linking text in-
stances with elements belonging to the ontology, 
instead of consulting flat gazetteers.
The major disadvantage of traditional IE sys-
tems is that they focus on satisfying precise, nar-
row, pre-specified requests from small homogen-
eous corpora (e.g., extract information about ter-
rorist events). Likewise, they are not flexible, are 
limited to specific types of knowledge and need 
to be built by knowledge engineers for each spe-
cific application and language. In fact most text 
mining  systems are  developed for  a  single  do-
main and a single language, and are not able to 
handle  knowledge  expressed  in  different  lan-
guages  or  expressed  and conceptualized  differ-
ently across cultures.
In this paper we describe an open platform for 
text-mining  or  IE that  can  be applied  to many 
different  languages  in  the  same  way  using  an 
open text representation system and a central on-
1
tology that  is  shared across  languages.  Ontolo-
gical implications are inserted in the text through 
off-line  reasoning and ontological  tagging.  The 
events and facts are extracted from large amounts 
of text using a flexible pattern-matching module, 
as specified by profiles  which comprise  ontolo-
gical and shallow linguistic patterns. The system 
is  developed  in  the  Asian-European  project 
KYOTO2.
In the next section,  we describe the general 
architecture of the KYOTO system. In section 3, 
we specify the knowledge structure that is used. 
Section  4,  describes  the  off-line  reasoning  and 
ontological tagging. In section 5, we describe the 
module for mining knowledge from the text that 
is enriched with ontological  statements.  Finally 
in section 6, we describe the first results of ap-
plying the system to databases on Estuaries.
2 KYOTO overview
The  KYOTO  project  allows  communities  to 
model terms and concepts in their domain and to 
use this knowledge to apply text mining on docu-
ments. The knowledge cycle in the KYOTO sys-
tem starts  with a set  of  source  documents pro-
duced by the community, such as PDFs and web-
sites.  Linguistic  processors  apply  tokenization, 
segmentation, morpho-syntactic analysis and  se-
mantic  processing  to  the  text  in  different  lan-
guages. The semantic processing involves the de-
tection of named-entities (persons, organizations, 
places,  time-expressions)  and  determining  the 
meaning of  words  in  the  text  according to  the 
given wordnet.  
The  output  of  the  linguistic  processors is 
stored in an XML annotation format that  is the 
same for  all  the languages,  called  the KYOTO 
Annotation  Format  (KAF,  Bosma  et  al  2009). 
This format incorporates standardized proposals 
for the linguistic annotation of text and represents 
them in an easy-to-use layered structure, which is 
compatible with the Linguistic Annotation Frame-
work  (LAF,  Ide  and  Romary  2003).  In  KAF, 
words, terms, constituents and syntactic depend-
encies  are  stored  in  separate  layers  with  refer-
ences across the structures. This makes it easier 
to harmonize the output of  linguistic processors 
2 Http://www.kyoto-project.eu
for different languages and to add new semantic 
layers to the basic output, when needed (Bosma 
et al 2009, Vossen et al 2010). All modules in 
KYOTO draw their input from these structures. 
In fact, the word-sense disambiguation process is 
carried out to the same KAF annotation in differ-
ent languages and is therefore the same for all the 
languages (Agirre et al 2009). In the current sys-
tem,  there  are  processors  for  English,  Dutch, 
Italian, Spanish, Basque, Chinese and Japanese.
The KYOTO system proceeds in 2 cycles (see 
Figure 1). In the 1st cycle, the Tybot (Term Yield-
ing Robot) extracts the most relevant terms from 
the documents. The Tybot is another generic pro-
gram that  can  do  this  for  all  the  different  lan-
guages in much the same way. The terms are or-
ganized as a structured hierarchy and, wherever 
possible,  related  to  generic  semantic  databases, 
i.e. wordnets for each language. In the left part of 
Figure 1, we show those terms in the input docu-
ment and their classification in wordnet. Terms in 
italics are present in the original wordnet, while 
underlined terms correspond to terms which were 
not in the original wordnet but were automatic-
ally discovered and linked to wordnet by Tybots. 
Straight  terms  correspond  to hyperonyms  in 
wordnet that do not necessarily occur in the text 
but are linked to ontological classes. The result of 
this  1st cycle  is a domain wordnet  for the target 
language.
The 2nd cycle of the system involves the actu-
al extraction of factual knowledge from the docu-
ments by the Kybots  (Knowledge Yielding Ro-
bots). Kybots use a collection of profiles that rep-
resent patterns of information of interest. In the 
profile, conceptual relations are expressed using 
ontological  and morpho-syntactic linguistic pat-
terns. Since the semantics is defined through the 
ontology,  it  is  possible  to  detect  similar  data 
across documents in different languages, even if 
expressed differently. In Figure 1, we give an ex-
ample of a conceptual pattern that relates organ-
isms that live in habitats. The Kybot can combine 
morpho-syntactic and semantic patterns. When a 
match is detected, the instantiation of the pattern 
is saved in a formal representation, either in KAF 
or in RDF. Since the wordnets in different lan-
guages are mapped to the same ontology and the 
text in these languages is represented in the same 
KAF,  similar  patterns  can  easily  be  applied  to 
multiple languages.
2
3 Ontological  and  lexical  background 
knowledge
As a semantic background model, we defined a 
3-layered  knowledge  architecture  following the 
principle  of  the  division  of  labour  (Putnam 
1975). In this model, the ontology does not need 
to be the central hub for all terms in a domain in 
all  languages.  Following the division  of labour 
principle, we can state that a computer does not 
need  to  distinguish  between  instances  of  a 
European Tree Frog and a Glass Tree frog. We 
assume  that  rigid  concepts  (as  defined  by 
Guarino and Welty 2002) are known to the do-
main experts and do not need to be defined form-
ally in the ontology but can remain in the avail-
able  background  resources,  such  as   databases 
with millions of species.  Terms in the documents 
are mostly non-rigid, e.g.  endangered frogs,  in-
vasive  frogs.  Such  non-rigid  terms  refer  to  in-
stances  of  species  in  contextual  circumstances. 
The processes and states are the important pieces 
of  information  that  matter  to the users  and are 
useful for mining text. The model therefore dis-
tinguishes between background vocabularies, do-
main terms,  wordnets and the central  ontology. 
The  background  vocabularies  are  automatically 
aligned  to  wordnet,  where  we  assume  that 
hyponymy relations to rigid synsets in wordnet 
declare those subconcepts as rigid subtypes too, 
without the necessity to include them in the onto-
logy.  For  non-rigid  terms,  we  defined  a  set  of 
mapping relations to the ontology through which 
we express their non-rigid involvement in these 
processes and states. Likewise, the ontology has 
been extended with processes and states for the 
domain  and  verbs  and  adjectives  have  been 
mapped to be able to detect expressions in text.
The  3-layered  knowledge  model  combines  the 
efforts from 3 different communities:
1.Domain  experts  in  social  communities  that 
continuously build background vocabularies;
2.Wordnet  specialists  that  define  the  basic  se-
mantic model for general concepts for a lan-
guage
3.Semantic Web specialists that define top-level 
and domain-specific ontologies that capture 
formal definitions of concepts;
We formalized the relations between these repos-
itories so that they can developed separately but 
combined within KYOTO to form a coherent and 
formal model.
3.1 Ontology
The KYOTO ontology currently consists of 1149 
classes divided over three layers. The top layer is 
based  on  DOLCE  (DOLCE-Lite-Plus  version 
3.9.7,  Masolo  et  al  2003)  and  OntoWordNet. 
This layer of the ontology has been modified for 
our purposes (Herold et. al. 2009).  The second 
layer consists of so-called Base Concepts (BCs) 
derived  from various  wordnets  (Vossen  1998, 
Izquierdo  et  al. 2007).  Examples  of  BCs  are: 
building,  vehicle,  animal,  plant,  change,  move,  
size, weight. The BCs are those synsets in Word-
Net 3.0 that have the most relations with other 
synsets in the wordnet hierarchies and are selec-
ted in a way that ensures complete coverage of 
the nominal and verbal part of WordNet. This has 
been  completed  for  the  nouns  (about  500 
synsets).  The ontology has also been adapted to 
include important concepts in the domain. Spe-
cial attention has been paid to represents the pro-
cesses  (perdurants)  in  which  objects  (endur-
ants)  of  the domain are  involved and qualities 
they may have. This is typically the information 
that is found in documents on the environment. 
We thus added 40 new event classes for repres-
enting  important  verbs  (e.g.  pollute, absorb, 
damage, drain) and 115 new qualities and qual-
ity-regions for representing important adjectives 
(e.g. airborne, acid, (un)healthy, clear). The full 
Figure 1: Two Cycles of processing in KYOTO
3
ontology can be downloaded from the KYOTO 
website, free for use. A considerable set of gener-
al verbs and adjectives (relevant for for the do-
main)  have  then  been  mapped  to  ontological 
classes: 189  verbal  synsets  and  222  adjectival 
synsets.
The  500  nominal  BCs  are  connected  to  the 
complete  WordNet  hierarchy,  whereas  the  189 
verbs represent 5,978 more specific verbal syn-
sets and the 222 adjectives represent  1,081 ad-
jectival synsets through the wordnet relations.
This basic ontology and the mapping to Word-
Net  are  used  to  model  the  shared  and  lan-
guage-neutral  concepts  and  relations  in the do-
main. Instances are excluded from the ontology. 
Instances will be detected in the documents and 
will be mapped to the ontology through instance 
to ontology relations (see below).  Likewise, we 
make a clear separation between the ontological 
model and the instantiation of the model as de-
scribed in the text.
3.2 Wordnet to ontology mappings
In addition to the ontology, we have wordnets for 
each language in the domain. In addition to the 
regular synset to synset relations in the wordnet, 
we will have a specific set of relations for map-
ping the synsets  to the ontology,  which are  all 
prefixed with sc_ standing for synset-to-concept. 
We differentiate between rigid and non-rigid con-
cepts in the wordnets through the mapping rela-
tions:
? sc_equivalenceOf: the synset is fully equi-
valent to the ontology Type & inherits all proper-
ties; the synset is Rigid
? sc_  subclassOf: the synset is a proper sub-
class of the ontology Type & inherits all proper-
ties; the synset is Rigid
? sc_domainOf: the synset is not a proper sub-
class  of  the  ontology  Type  &  is  not  disjoint 
(therefore orthogonal) with other synsets that are 
mapped to the same Type either through sc_sub-
classOf or sc_domainOf; the synset is non-Rigid 
but still inherits all properties of the target onto-
logy Type;  the synset  is  also related to a Role 
with a sc_playRole relation
? sc_playRole:  the  synset  denotes  instances 
for  which  the  context  of  the  Role  applies  for 
some period of time but this is not essential for 
the existence of the instances, i.e. if the context 
ceases to exist then the instances may still exist 
(Mizoguchi et al 2007).3
? sc_participantOf:  instances of the concept 
(denoted by the synset) participate in some en-
durant, where the specific role relation is indic-
ated by the playRole mapping. 
? sc_hasState: instances of the concept are in 
a particular state which is not essential and can 
be changed. There is no need to represent the role 
for a stative perdurant.
This model  extends  existing  WordNet  to  onto-
logy mappings.  For  instance,  in  the  SUMO to 
Wordnet mapping (Niles and Pease 2003), only 
the  sc_equivalenceOf and  sc_subclassOf rela-
tions  are  used,  represented  by  the symbols  ?=? 
and ?+? respectively. The SUMO-Wordnet map-
ping likewise does not systematically distinguish 
rigid from non-rigid  synsets.  In our  model,  we 
separate the linguistically and culturally specific 
vocabularies from the shared ontology while us-
ing the ontology  to interface  the concepts used 
by the various communities.
Using these mapping relations, we can express 
that the synset for  duck (which has a hypernym 
relation to the synset  bird, which, in its turn, has 
an  equivalence  relation  to  the  ontology  class 
bird) is  thus  a  proper  subclassOf  the  ontology 
class bird:
wn:duck hypernym wn:bird
wn:bird  sc_equivalenceOf ont:bird
For a concept such as migratory bird, which is 
also  a  hyponym of  bird in  wordnet  but  not  a 
proper subclass as a non-rigid concept, we thus 
create the following mapping:
wn:migratory bird 
? sc_domainOf ont:bird
? sc_playRole ont:done-by
? sc_participantOf ont:migration
This mapping indicates that the synset is used to 
refer to instances of endurants (not subclasses!), 
where the domain is restricted to birds. Further-
more, these instances participate in the process of 
3 Some terms involve more than one role,  e.g.  gas-
powered-vehicle.  Secondary  participants  are  related 
through  sc_hasCoParticipant and sc_playCoRole 
mappings.
4
migration in the role of  done-by. The properties 
of  the  process  migration are  further  defined  in 
the  ontology,  which  indicates  that  it  is  a  act-
ive-change-of-location  done-by  some  endurant, 
going from a source, via a path to some destina-
tion. The mapping relations from the wordnet to 
the ontology, need to satisfy the constraints of the 
ontology, i.e. only roles can be expressed that are 
compatible with the role-schema of the process 
in which they participate.
For  implied  non-essential  states,  we  use  the 
sc_hasState relation to express that a synset such 
as wild dog refers to instances of dogs that life in 
the wild but can stop being wild:
wn:wild dog ? sc_domainOf ont:dog
wn:wild dog ? sc_hasState ont:wild
Ideally, all processes and states that can be ap-
plied to endurants should be defined in the onto-
logy. This may hold for most verbs and adject-
ives in languages, which do not tend to extend in 
specific  domains  and  are  part  of  the  general 
vocabulary  (e.g.  to  pollute,  to  reduce,  wild). 
However, domain specific text contain many new 
nominal terms that refer to domain-specific pro-
cesses and states, e.g. air pollution, nitrogen pol-
lution,  nitrogen  reduction.  These  terms  are 
equally relevant as their counter-parts that refer 
to endurants involved in similar  processes, e.g. 
polluted air, polluting nitrogen or reduced nitro-
gen. We therefore use the reverse participant and 
role mappings to be able to define such terms for 
processes  as  subclasses  of  more  general  pro-
cesses  involving  specific  participants  in  a  spe-
cified role:
wn:air pollution
? sc_subcassOf ont:pollution (perdurant)
? sc_hasParticipant ont:air
? sc_hasRole ont:patient
wn:nitrogen pollution
? sc_subcassOf ont:pollution (perdurant)
? sc_hasParticipant ont:nitrogen
? sc_hasRole ont:done-by
 
Further  mapping  relations  are  described  in  the 
documentation on the KYOTO website. Through 
the mapping relations, we can keep the ontology 
relatively small and compact whereas we can still 
define  the  richness  of  the  vocabularies  of  lan-
guages in a precise way. The classes in the onto-
logy can be defined using rich axioms that model 
precise implications for inferencing. The wordnet 
to synset mappings can be used to define rather 
basic relations relative to the given ontology that 
still  captures  the  semantics  of  the  terms. The 
term definitions capture both relevance and per-
spective  (those  relations  that  matter  from  the 
point of the view of the term), on the one hand, 
and some semantics with respect to the concepts 
that are involved and their (role) relation on the 
other  hand.  Likewise,  the  KYOTO  system can 
model the linguistic and cultural diversity of lan-
guages in a domain but at the same time keep a 
firm anchoring to a basic and compact ontology.
3.3 Domain wordnet
We selected 3 representative documents on estu-
aries to extract relevant terms for the domain us-
ing the Tybot module. The terms have been re-
lated  through  structural  relations,  e.g.  nitrogen 
pollution is a hyponym of pollution, and through 
WordNet synsets that are assigned through WSD 
of the text.  We extracted 3950 candidate  terms 
form the KAF representations of the documents. 
Most of these are nouns (2818 terms). The nom-
inal  terms matched for 40% with wordnet syn-
sets, the verbs and adjectives for 98% and 85% 
respectively. For the domain wordnet, we restric-
ted ourselves to the nouns. From the new nomin-
al  terms,  environmentalists selected  390  terms 
that they deem to be important. These terms are 
connected to parent terms, which ultimately are 
connected to wordnet synsets.  The final domain 
wordnet contains 659 synsets: 197 synsets from 
the generic wordnet and 462 new synsets connec-
ted to the former.  The domain wordnet synsets 
got 990 mappings to the ontology, using the rela-
tions described in the previous section. There are 
86 synsets that have a sc_domainOf mapping, in-
dicating  that  they  are  non-rigid.  Note  that 
hyponyms of these synsets are also non-rigid by 
definition. These non-rigid synsets have complex 
mappings to processes and states in which  they 
are involved. The domain wordnet can be down-
loaded from the KYOTO website, free for use.
5
4 Off-line reasoning and ontological tag-
ging 
The ontological tagging represents the last phase 
in the KYOTO Linguistic  Processor  annotation 
pipeline.  It  consists  of  a three-step module  de-
vised to enrich the KAF documents with know-
ledge derived from the ontology. For each synset 
connected to a term, the first step   adds the Base 
Concepts to which the synset is related through 
the wordnet taxonomical relations. Then, through 
the synset to ontology mapping, it  adds the cor-
responding ontology type with appropriate rela-
tions. Once each synset is specified as to its onto-
logy type,  the  last  ontotagging  step  inserts  the 
full  set  of  ontological  implications  that  follow 
from the explicit ontology. The explicit ontology 
is a new data  structure consisting of a table with 
all  ontology nodes and all  ontological  implica-
tions expressed. The main purpose is to optimize 
<term lemma="pollution" pos="N" tid="t13444" type="open">
  <externalReferences>
   <externalRef reference="eng-30-00191142-n" reftype="baseConcept" resource="wn30g"/>
   <externalRef reference="Kyoto#change-eng-3.0-00191142-n" reftype="sc_subClassOf" resource="ontology">
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#contamination_pollution"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#accomplishment" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#event" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#perdurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#part" reference="DOLCE-Lite.owl#perdurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#specific-constant-constituent" reference="DOLCE-Lite.owl#perdurant" 
status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#has-quality" reference="DOLCE-Lite.owl#temporal-quality" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#spatio-temporal-particular" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#participant" reference="DOLCE-Lite.owl#endurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#has-quality" reference="DOLCE-Lite.owl#temporal-location_q" status="im-
plied"/>
    <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#particular" status="implied"/>
    </externalRef>
  </externalReferences>
</term>
Figure 2: An example of an OntoTagged output
<kprofile>
 <variables>
<var name="x" type="term" pos="N"/>
  <var name="y" type="term" 
       lemma="produce | generate | release | ! create"/>
  <var name="z" type="term"
       reference="DOLCE-Lite.owl#contamination_pollution"
       reftype="SubClassOf"/>
 </variables>
 <relations>
  <root span="y"/>
  <rel span="x" pivot="y" direction="preceding"/>
  <rel span="z" pivot="y" direction="following"/>
 </relations>
 <events>
  <event target="$y/@tid" lemma="$y/@lemma" pos="$y/@pos"/>
  <role target="$x/@tid" rtype="agent" lemma="$x/@lemma"/>
  <role target="$z/@tid" rtype="patient"lemma="$z/@lemma"/>$
 </events>
</kprofile>
Figure 3: An example of a Kybot profile
<kybotOut>
 <doc name="11767.mw.wsd.ne.onto.kaf">
  <event eid="e1" lemma="generate" pos="V" target="t3504"/>
  <role rid="r1" lemma="industry" rtype="agent" target="t3493" pos="N" event="e1"/>
  <role rid="r2" lemma="pollution" rtype="patient" target="t3495" pos="N" event="e1"/>
 </doc>
 <doc name="16266.mw.wsd.ne.onto.kaf">
  <event eid="e2" lemma="release" pos="V" target="t97"/>
  <role rid="r3" lemma="fuel" rtype="agent" target="t96" pos="N" event="e2"/>
  <role rid="r4" lemma="exhaust_gas" rtype="patient" target="t101" pos="V" event="e2"/>
 </doc>
</kybotOut>
Figure 4: An example of a Kybot output
6
the performance of the mining module over large 
quantities of documents. The advantage for Ky-
bots from ontotagging are many. First of all, they 
are  able  to  run  and  apply  pattern-matching  to 
Base  Concepts  and  ontological  classes  rather 
than just to words or synsets. Moreover, by mak-
ing explicit  the  implicit  ontological  statements, 
Kybots are able to find the same relations hidden 
in  different  expressions  with  different  surface 
realizations:  fish migration,  migratory  fish,  mi-
gration of fish, fishes that migrate, that directly 
or indirectly express the same relations. With on-
totagging,  they  share  the  same ontological  im-
plications which will allow Kybots to apply the 
same patterns and perform the extraction of facts. 
The implications will be represented in the same 
way across different languages, thus facilitating 
cross-lingual extraction of facts. Lastly, ontotag-
ging is a kind of off-line ontological reasoning: 
without  doing reasoning over concepts,  Kybots 
substantially  improve their  performance.  Figure 
2 shows the result of onto-tagging for the term 
pollution.
5 Event and fact extraction
Kybots (Knowledge Yielding Robots) are  com-
puter  programs  that  use  the  mined 
concepts and the generic  concepts  already con-
nected to the language wordnets and the KYOTO 
ontology to extract actual concept instances and 
relations in KAF documents. Kybots incorporate 
technology  for  the  extraction  of  relationships, 
either eventual or not, relative to the general or 
domain concepts already captured by the Tybots. 
That is, the extraction of factual knowledge is be-
ing carried out by the Kybot server by processing 
Kybot profiles on the linguistically enriched doc-
uments.
Kybots  are  defined  following  a  declarative 
format,  the  so  called  Kybot  
profiles, which describe general morpho-syntact-
ic  and  semantic  conditions  on  sequences  of 
terms. Profiles are compiled to generate the Ky-
bots, which scan over KAF documents searching 
for the patterns and extract the relevant informa-
tion from each matching.
Linguistic  patterns  include morphologic  con-
straints and also semantic conditions the matched 
terms must hold.  Kybot are thus able to search 
for term lemmas or part-of-speech tags but also 
for terms linked to ontological process and states 
using  the  mappings  described  in  Section  3.2. 
Thus, it is possible to detect similar eventual in-
formation  across  documents  in  different  lan-
guages, even if expressed differently.
5.1 Example of a Kybot Profile
Kybot Profiles are described using XML syn-
tax.  Figure 3 presents an example of a profile. 
Kybot profiles consist of three main parts: 
?Variable  declaration (<variables> element): 
In this section the search entities are defined. The 
example  defines  three  variables:  x (denoting 
terms  whose  part-of-speech is  noun),  y (which 
are  terms whose lemma is ?release?, ?produce? 
or  ?generate?  but   not  ?create?)  and  z (terms 
linked to  the  ontological  endurant  ?DOLCE-L-
ite.owl#contamination_pollution?, meaning ``be-
ing contaminated with harmful  substances''). 
?Declarations  of  the  relations  among  variables 
(<rel> element): specify the relations among the 
previously  defined variables.  The example pro-
file specifies y  as the main pivot, and states that 
variable  x must  be  preceding  variable  y in  the 
same sentence, and that variable  z must be fol-
lowing variable  y.  Thus,  the Kybot will  search 
for patterns like 'x ? y ? z' in a sentence.
?Output template (<events> element): describes 
the output to be produced on every matching. In 
the example, each match generates a new event 
targeting term  y,  which becomes the main term 
of the event. It also fills two roles of the event, 
the 'agent' role filled by term x and 'patient' role, 
filled by z. 
Figure  4  presents  the  output  of  the  Kybot 
when applied against the benchmark documents.
The Kybot output follows the stand-off architec-
ture when producing new information, and it thus 
forms  a  new KAF layer  on  the  original  docu-
ments.
6 Experimental results
We applied the KYOTO system and resources to 
English documents on estuaries. We collected 50 
URLs for two English estuaries: the Humber Es-
tuary in Hull (UK) and the Chesapeake Bay estu-
ary in the US and for background documents on 
bird  migration,  sedimentation,  habitat  destruc-
tion,  and  climate  change.  In  addition  to  the 
webpages, we extracted 815 PDF files from the 
sites. In total, 4625 files have been extracted. All 
7
the documents have been processed by the lin-
guistic  processor  for  English,  which  generated 
KAF representations for all the documents. From 
this  database,  3  documents  were  selected  for 
benchmarking.
The  documents  were  processed  by  applying 
multiword  tagging,  word-sense-disambiguation, 
named-entity-recognition  and  the  ontological 
tagging to the 3 documents and to the complete 
database; This was done twice: once without the 
domain model and once with the domain model. 
We thus created 4 datasets:  3 benchmark docu-
ments  processed  with  and  without  the  domain 
model; the complete database processed with and 
without the domain model.
Furthermore, we created Kybot profiles based 
on the type of information represented in the do-
main model. We applied the Kybots to all 4 data 
sets. We generate the following data files through 
an WN-LMF export of the domain wordnet:
1. a set of domain multiwords for the multi-
word tagger
2. an extension of the lexicon and the graph 
of  concepts  that  is  used  by  the  WSD 
module
3. an extension of the wordnet-to-ontology 
mappings for the ontotagger
In addition, we constructed mapping lists for all 
WordNet 3.0 synsets to Base Concepts and to ad-
jective and verbs that are matched to the onto-
logy.  These mappings provide the generic  con-
ceptual model based on wordnet and on the onto-
logy. 
Table 1 shows the effects of using the domain 
model for the first 3 modules. We can see that the 
domain  model  has  a  clear  effect  on  the multi-
word  detection  in  the  3  evaluation  documents. 
Using the domain model,  600 multiwords have 
been detected, against 145 with just the generic 
wordnet. This is obvious since the terms are ex-
tracted  from  the  same  documents.  However, 
when applying it  to the complete  database,  we 
see that  still  over 2,300 more multiwords have 
been  detected  using  the domain wordnet.  Note 
that the domain wordnet has only 97 multiwords 
and the generic wordnet has 19,126 multiwords. 
So 0.5% of the multiwords in the domain word-
net add 1.5 times more multiword tokens in the 
database. The third row specifies the number of 
synsets that have been assigned. We can see that 
for the domain model almost 400 more synsets 
have been detected. In the case of the full estuary 
database, we see that relatively few more have 
been detected, almost 1,500 while the database is 
80 times as big. If we look more closely at the 
numbers of actual  domain synsets detected,  we 
see the following results. In the benchmark docu-
ments  637 (or 5%) of  the synsets  is  a  domain 
wordnet  synset,  whereas  5,353 synsets  are  do-
main synsets in the full estuary database, which 
is only 0.52%. Note that in KAF multiwords are 
represented both as a single terms and in terms of 
their elements. The WSD module assigns synsets 
to  both.  The  domain  model  can  thus  only  add 
synsets compared to the processing without the 
domain. 
Finally, if we look at the named-entity-recogni-
tion module, we see a slight negative effect for 
the detection of named-entities due to the domain 
model.  The  named-entity-recognition  module 
does not consider the elements of multiwords but 
just  the multiword terms as a whole. Grouping 
terms  as  multiwords  thus  leads  to  less  named-
entities being detected. This is not necessarily a 
bad things, since the detection heavily over-gen-
erates and could have now more precision.
Table 1: Statistics on processing the estuary documents with and without domain model
bench mark documents (3) estuary documents (4742)
No Domain Domain No Domain Domain
terms 22,204 22,204 2,419,839 2,419,839
multiwords 145 600 4,389 6,671
12,526 12,910 1,021,598 1,023,017
158 126 41,681 40,714
67 66 10,288 10,233
synsets
ne location
ne date
8
Table 2 shows the effect of inserting ontologic-
al  implications  into  the  text  representation.For 
the benchmark documents, we see that more than 
half a million ontological implications have been 
inserted.  Of  these, 82% are implied references, 
that are extracted from the explicit ontology on 
the  basis  of  a  direct  mapping to  the  ontology. 
About  8% of  the  mappings  are  synset-to-onto-
logy mappings (sc) and 9.5% are mappings rep-
resenting the subclass hierarchy. The differences 
between using the domain model and not-using 
the domain model are minimal. For the complete 
database, the implications are 80 times as much 
but the proportions are similar.
Table 3 shows the type of sc-relations that oc-
cur.  Obviously,  sc_subClassOf  and  sc_equival-
entOf  are  the  most  frequent.  Nevertheless,  we 
still  find  about  500  mappings  that  present  the 
participation in a process or state. 
 
     30  reftype="sc_playCoRole"
     32  reftype="sc_hasCoParticipant"
     42  reftype="sc_partOf"
     59  reftype="sc_stateOf"
     92  reftype="sc_playRole"
     94  reftype="sc_hasRole"
     97  reftype="sc_participantOf"
   105  reftype="sc_hasParticipant"
   128  reftype="sc_domainOf"
   169  reftype="sc_hasState"
   312  reftype="sc_hasPart"
 3637  reftype="sc_equivalentOf"
42048  reftype="sc_subClassOf"
Table 3: Type of relations for the wordnet to ontology  
mappings using the domain model
The table clearly shows the impact of role rela-
tions  that  are  encoded  in  the  domain  wordnet. 
When  we  extract  the  mappings  for  the  files 
without the domain model (ony using the map-
pings to the generic wordnet), we get only equi-
valence and subclass mappings.
Finally to complete the knowledge cycle, we cre-
ated a few Kybot profiles for extracting events 
from the  onto-tagged  documents.  As  an  initial 
test, 3 profiles have been created:
1. events of destruction
2. destructions of locations
3. destruction of objects
Using  these  profiles,  we  extracted  211  events 
from the 3 benchmark documents with 396 roles. 
The profiles are created to run over the ontolo-
gical  types  inserted  by  the  ontotagger,  e.g.  re-
stricted to events and change_of_integrity.  Des-
pite the generality of the profiles, we still see a 
clear signature of the domain in the output. This 
is a good indication that we will be able to ex-
tract valuable events from the data, even though 
the  ontotagger  generates  a  massive  amount  of 
implications.  Especially  events  that  combine 
multiple  roles  appear  to  give  rich  information. 
For example, the following sentence:
"One of the greatest challenges to restoration is con-
tinued population growth and development, which 
destroys forests, wetlands and other natural areas"
yielded the following output:
   <event target="t1471" lemma="destroy" pos="V" 
eid="e74"/>
   <role target="t1477" rtype="patient" lemma="area" 
pos="N" event="e74" rid="r138"/>
   <role target="t1472" rtype="patient" 
lemma="forest" pos="N" event="e74" rid="r151"/>
   <role target="t1469" rtype="actor" lemma="devel-
opment" pos="N" event="e74" rid="r180"/>
Running the full set of profiles on the complete data-
base with almost 60 million ontological statements 
took about 2 hours. This shows that our approach is 
scalable and efficient.
Table 2: Ontological implications for the four data sets
bench mark documents (3) estuary documents (4272
No Domain Domain Domain
ontology references 555,677 576,432 48,708,300
implied ontology references 457,332 82.30% 474,916 82.39% 40,523,452 83.20%
direct ontology references 53,178 9.57% 54,769 9.50% 4,377,814 8.99%
45,167 8.13% 46,747 8.11% 3,807,034 7.82%domain synset to ontology mappings
9
7 Conclusions
In this paper, we described an open platform for 
text-mining  using wordnets  and a central  onto-
logy.  The  system  can  be  used  across  different 
languages and can be tailored to mine any type of 
conceptual relations. It can handle semantic im-
plications that are expressed in very different lin-
guistic expressions and yield systematic output. 
As future work, we will carry out benchmarking 
and testing of the mining of events, both for Eng-
lish and for the other languages in the KYOTO 
project.
Acknowledgements
The KYOTO project is co-funded by EU - FP7 
ICT Work Programme 2007 under Challenge 4 - 
Digital  libraries  and  Content,  Objective  ICT-
2007.4.2  (ICT-2007.4.4):  Intelligent  Contsent 
and Semantics  (challenge 4.2).  The Asian part-
ners from Tapei and Kyoto are funded from na-
tional funds. This work has been also supported 
by  Spanish  project  KNOW-2 (TIN2009-14715-
C04-01).
References
Agirre, E., & Soroa, A. (2009) Personalizing PageR-
ank for Word Sense Disambiguation. Proceedings 
of the 12th EACL, 2009. Athens, Greece. 
Agirre, E., Lopez de Lacalle, O., & Soroa, A. (2009) 
Knowledge-based WSD and specific domains: per-
forming over supervised WSD. Proceedings of IJ-
CAI. Pasadena, USA. http://ixa.si.ehu.es/ukb
?lvez J., Atserias J., Carrera J., Climent S., Laparra 
E., Oliver A. and Rigau G. (2008) Complete and 
Consistent  Annotation of  WordNet  using the Top 
Concept Ontology. Proceedings of LREC'08, Mar-
rakesh, Morroco. 2008.
Appelt Douglas E., Jerry R. Hobbs, John Bear, David 
Israel, Megumi Kameyama, Andrew Kehler, David 
Martin,  Karen Myers and Mabry Tyson. Descrip-
tion of the FASTUS System Used for MUC-6. In 
Proceedings  of  MUC-6,  pages  237?248.  San 
Mateo, Morgan Kaufmann, 1995.
Auer A., C. Bizer, G. Kobilarov, J. Lehmann, R. Cy-
ganiak and Z. Ives. DBpedia: A Nucleus for a Web 
of  Open  Data.  In  Proceedings  of  the
International  Semantic  Web  Conference  (ISWC), 
volume 4825 of  Lecture Notes  in Computer Sci-
ence, pages 722-735. 2007.
Bosma, W., Vossen, P., Soroa, A. , Rigau, G., Tesconi, 
M., Marchetti, A., Monachini, M., & Apiprandi, C. 
(2009) KAF: a generic semantic annotation format. 
In Proceedings of the 5th International Conference 
on Generative Approaches to the Lexicon Sept 17-
19, 2009, Pisa, Italy.
Fellbaum,  C.  (Ed.)  (1998)  WordNet:  An  Electronic 
Lexical Database. Cambridge, MA: MIT Press.
Freitag, D. (1998) Information extraction from html: 
Application  of  a  general  machine  learning  ap-
proach.  In  Proceedings  of  the  Fifteenth  National 
Conference on Artificial Intelligence, 1998.
Gangemi  A.,  Guarino  N.,  Masolo  C.,  Oltramari  A., 
Schneider  L.  (2002)  Sweetening  Ontologies  with 
DOLCE. Proceedings of EKAW. 2002
Ide, N. and L. Romary. 2003. Outline of the inter- na-
tional standard Linguistic Annotation Framework. 
In Proceedings of ACL 2003 Workshop on Lin-
guistic Annotation: Getting the Model Right, pages 
1?5.
Izquierdo R., Su?rez A. & Rigau G. Exploring the 
Automatic Selection of Basic Level Concepts. Pro-
ceedings of RANLP'07, Borovetz, Bulgaria. 
September, 2007.
Masolo, C., Borgo, S., Gangemi, A.,  Guarino, N. & 
Oltramari, A. (2003) WonderWeb Deliverable D18: 
Ontology Library, ISTC-CNR, Trento, Italy.
Mizoguchi R., Sunagawa E., Kozaki K. & Kitamura 
Y. (2007 A Model of Roles within an Ontology De-
velopment  Tool:  Hozo.  Journal  of  Applied  Onto-
logy, Vol.2, No.2, 159-179.
Niles, I. & Pease, A. (2001) Formal Ontology in In-
formation Systems. Proceedings of the internation-
al Conference on Formal Ontology in Information 
Systems ? Vol. 2001 Ogunquit, Maine,  USA
Niles, I. and A. Pease. Linking lexicons and ontolo-
gies:  Mapping  WordNet  to  the  Suggested  Upper 
Merged Ontology. In Proc. IEEE IKE, pages 412?
416, 2003.
Vossen, P. (Ed.) (1998) EuroWordNet: a multilingual 
database  with  lexical  semantic  networks  for 
European Languages. Kluwer, Dordrecht.
Vossen P., W. Bosma, E. Agirre, G. Rigau, A. Soroa 
(2010) A full Knowledge Cycle for Semantic Inter-
operability.  Proceedings  of  the  5th  Joint  ISO-
ACL/SIGSEM Workshop on Interoperable Semant-
ic Annotation, (ICGL 2010) Hong Kong, 2010.
10
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 94?100,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Enabling the Discovery of Digital Cultural Heritage Objects through
Wikipedia
Mark M Hall
Paul D Clough
Information School
Sheffield University
Sheffield, UK
m.mhall@shef.ac.uk
p.d.clough@shef.ac.uk
Oier Lopez de Lacalle1,2
1IKERBASQUE
Basque Foundation for Science
Bilbao, Spain
2School of Informatics
University of Edinburgh
Edinburgh, UK
oier.lopezdelacalle@gmail.es
Aitor Soroa
Eneko Agirre
IXA NLP Group
University of the Basque Country
Donostia, Spain
a.soroa@ehu.es
e.agirre@ehu.es
Abstract
Over the past years large digital cultural
heritage collections have become increas-
ingly available. While these provide ad-
equate search functionality for the expert
user, this may not offer the best support for
non-expert or novice users. In this paper
we propose a novel mechanism for intro-
ducing new users to the items in a collection
by allowing them to browse Wikipedia arti-
cles, which are augmented with items from
the cultural heritage collection. Using Eu-
ropeana as a case-study we demonstrate the
effectiveness of our approach for encourag-
ing users to spend longer exploring items
in Europeana compared with the existing
search provision.
1 Introduction
Large amounts of digital cultural heritage (CH)
information have become available over the past
years, especially with the rise of large-scale ag-
gregators such as Europeana1, the European ag-
gregator for museums, archives, libraries, and gal-
leries. These large collections present two chal-
lenges to the new user. The first is discovering
the collection in the first place. The second is
then discovering what items are present in the
collection. In current systems support for item
discovery is mainly through the standard search
paradigm (Sutcliffe and Ennis, 1998), which is
well suited for CH professionals who are highly
familiar with the collections, subject areas, and
have specific search goals. However, for new
users who do not have a good understanding of
what is in the collections, what search keywords
1http://www.europeana.eu
to use, and have vague search goals, this method
of access is unsatisfactory as this quote from
(Borgman, 2009) exemplifies:
?So what use are the digital libraries, if
all they do is put digitally unusable in-
formation on the web??
Alternative item discovery methodolo-
gies are required to introduce new users to
digital CH collections (Geser, 2004; Steem-
son, 2004). Exploratory search models
(Marchionini, 2006; Pirolli, 2009) that en-
able switching between collection overviews
(Hornb[Pleaseinsertintopreamble]k and Hertzum,
2011) and detailed exploration within the
collection are frequently suggested as more
appropriate.
We propose a novel mechanism that enables
users to discover an unknown, aggregated collec-
tion by browsing a second, known collection. Our
method lets the user browse through Wikipedia
and automatically augments the page(s) the user
is viewing with items drawn from the CH collec-
tion, in our case Europeana. The items are chosen
to match the page?s content and enable the user to
acquire an overview of what information is avail-
able for a given topic. The goal is to introduce
new users to the digital collection, so that they can
then successfully use the existing search systems.
2 Background
Controlled vocabularies are often seen as a
promising discovery methodology (Baca, 2003).
However, in the case of aggregated collections
such as Europeana, items from different providers
are frequently aligned to different vocabularies,
requiring an integration of the two vocabularies in
94
order to present a unified structure. (Isaac et al,
2007) describe the use of automated methods for
aligning vocabularies, however this is not always
successfully possible. A proposed alternative is
to synthesise a new vocabulary to cover all aggre-
gated data, however (Chaudhry and Jiun, 2005)
highlight the complexities involved in then link-
ing the individual items to the new vocabulary.
To overcome this automatic clustering and vi-
sualisations based directly on the meta-data have
been proposed, such as 2d semantic maps (An-
drews et al, 2001), automatically generated tree
structures (Chen et al, 2002), multi-dimensional
scaling (Fortuna et al, 2005; Newton et al, 2009),
self-organising maps (Lin, 1992), and dynamic
taxonomies (Papadakos et al, 2009). However
none of these have achieved sufficient success to
find widespread use as exploration interfaces.
Faceted search systems (van Ossenbruggen et
al., 2007; Schmitz and Black, 2008) have arisen
as a flexible alternative for surfacing what meta-
data is available in a collection. Unlike the meth-
ods listed above, faceted search does not require
complex pre-processing and the values to display
for a facet can be calculated on the fly. However,
aggregated collections frequently have large num-
bers of potential facets and values for these facets,
making it hard to surface a sufficiently large frac-
tion to support resource discovery.
Time-lines such as those proposed by (Luo et
al., 2012) do not suffer from these issues, but are
only of limited value if the user?s interest cannot
be focused through time. A user interested in ex-
amples of pottery across the ages or restricted to
a certain geographic area is not supported by a
time-line-based interface.
The alternative we propose is to use a second
collection that the user is familiar with and that
acts as a proxy to the unfamiliar collection. (Villa
et al, 2010) describe a similar approach where
Flickr is used as the proxy collection, enabling
users to search an image collection that has no
textual meta-data.
In our proposed approach items from the unfa-
miliar collection are surfaced via their thumbnail
images and similar approaches for automatically
retrieving images for text have been tried by (Zhu
et al, 2007; Borman et al, 2005). (Zhu et al,
2007) report success rates that approach the qual-
ity of manually selected images, however their
approach requires complex pre-processing, which
Figure 1: Architectural structure of the Wikiana sys-
tem
the dynamic nature of discovery prohibits.
Wikipedia was chosen as the discovery inter-
face as it is known to have good content cover-
age and frequently appears at the top of search
results (Schweitzer, 2008) for many topics, its
use has been studied (Lim, 2009; Lucassen and
Schraagen, 2010), and it is frequently used as
an information source for knowledge modelling
(Suchanek et al, 2008; Milne and Witten, 2008),
information extraction (Weld et al, 2009; Ni et
al., 2009), and similarity calculation (Gabrilovich
and Markovitch, 2007).
3 Discovering Europeana through
Wikipedia
As stated above our method lets users browse
Wikipedia and at the same time exposes them to
items taken from Europeana, enabling them to
discover items that exist in Europeana.
The Wikipedia article is augmented with Euro-
peana items at two levels. The article as a whole
is augmented with up to 20 items that in a pre-
processing step have been linked to the article and
at the same time each paragraph in the article is
augmented with one item relating to that para-
graph.
Our system (Wikiana, figure 1) sits between
the user and the data-providers (Wikipedia, Eu-
ropeana, and the pre-computed article augmenta-
tion links). When the user requests an article from
Wikiana, the system fetches the matching article
from Wikipedia and in a first step strips every-
thing except the article?s main content. It then
queries the augmentation database for Europeana
items that have been linked to the article and se-
lects the top 20 items from the results, as detailed
below. It then processes each paragraph and uses
95
Figure 2: Screenshot of the augmented article
?Mediterranean Sea? with the pre-processed article-
level augmentation at the top and the first two para-
graphs augmented with items as returned by the Euro-
peana API.
keywords drawn from the paragraphs (details be-
low) to query Europeana?s OpenSearch API for
items. A random item is selected from the result-
set and a link to its thumbnail image inserted into
the paragraph. The augmented article is then sent
to the user?s browser, which in turn requests the
thumbnail images from Europeana?s servers (fig.
2).
The system makes heavy use of caching to
speed up the process and also to reduce the
amount of load on the backend systems.
3.1 Article augmentation
To create the article-level augmentations we first
create a Wikipedia ?dictionary?, which maps
strings to Wikipedia articles. The mapping is cre-
ated by extracting all anchor texts from the inter-
article hyperlinks2 and mapping these to the ar-
ticles they link to. For instance, the string ?ro-
man coin? is used as an anchor in a link to the
Wikipedia article Roman currency3. Where
the same string points to multiple articles we se-
lect the most frequent article as the target. In the
case of ties an article is selected arbitrarily.
In a second step, we scan the subset of Eu-
ropeana selected for a European project, which
comprises SCRAN and Culture Grid collections
for English. The items in this sub-set are then
linked to Wikipedia articles. The sub-set of Euro-
2We used the 2008 Wikipedia dump to construct the dic-
tionary.
3http://en.wikipedia.org/wiki/Roman_
currency
<record>
<dc:identifier>http://www.kirkleesimage...</dc:identifier>
<dc:title>Roman Coins found in 1820..., Lindley</dc:title>
<dc:source>Kirklees Image Archive OAI Feed</dc:source>
<dc:language>EN-GB</dc:language>
<dc:subject>Kirklees</dc:subject>
<dc:type>Image</dc:type>
</record>
Figure 3: Example of an ESE record, some fields have
been omitted for clarity.
peana that was processed followed the Europeana
Semantic Elements (ESE) specifications4. Figure
3 shows an example of an ESE record describ-
ing a photograph of a Roman coin belonging to
the Kirklees Image Archive. We scan each ESE
record and try to match the ?dc:title? field with
the dictionary entries. In the example in figure
3, the item will be mapped to the Wikipedia ar-
ticle Roman currency because the string ?ro-
man coins? appears in the title.
As a result, we create a many-to-many mapping
between Wikipedia articles and Europeana items.
The Wikiana application displays at most 20 im-
ages per article, thus the Europeana items need to
be ranked. The goal is to rank interesting items
higher, with ?interestingness? defined as how un-
usual the items are in the collection. This metric
is an adaption of the standard inverse-document-
frequency formula used widely in Information
Retrieval and is adapted to identify items that have
meta-data field-values that are infrequent in the
collection. As in original IDF we diminish the
weight of values that occur very frequently in
the collection, the non-interesting items, and in-
creases the weight of values that occur rarely, the
interesting items. More formally the interesting-
ness ?i of an item i is calculated as follows:
?i =
#{titlei}
?title
log
Ntitle
c(titlei) + 1
+
#{desci}
?desc
log
Ndesc
c(desci) + 1
+
#{subji}
?subj
log
Nsubj
c(subji) + 1
where #{fieldi} is the length in words of the
field of the given item i, ?field is the average length
in words of the field in the collection, Nfield is the
total number of items containing that field in the
4http://version1.europeana.eu/web/
guest/technical-requirements
96
The Roman Empire (Latin: Imperium Romanum) was
the post-Republican period of the ancient Roman civ-
ilization, characterised by an autocratic form of gov-
ernment and large territorial holdings in Europe and
around the Mediterranean.
?Latin language? OR ?Ro-
man Republic? OR ?An-
cient Rome? or ?Autoc-
racy?
Figure 4: Example paragraph with the Wikipedia hy-
perlinks in bold. Below the search keywords extracted
from the hyperlinks and the resulting thumbnail image.
entire collection, and c(fieldi) is the frequency of
the value in that field.
Items are ranked by descending ?i and the for
the top 20 items, the thumbnails for the items are
added to the top of the augmented page.
3.2 Paragraph augmentation
The items found in the article augmentation tend
to be very focused on the article itself, thus to pro-
vide the user with a wider overview of available
items, each paragraph is also augmented. This
augmentation is done dynamically when an arti-
cle is requested. As stated above the augmen-
tation iterates over all paragraphs in the article
and for each article determines its core keywords.
As in the article augmentation the Wikipedia hy-
perlinks are used to define the core keywords, as
the inclusion of the link in the paragraph indi-
cates that this is a concept that the author felt was
relevant enough to link to. For each paragraph
the Wikipedia hyperlinks are extracted, the under-
scores replaced by spaces and these are then used
as the query keywords. The keywords are com-
bined using ?OR? and enclosed in speech-marks
to ensure only exact phrase matches are returned
and then submitted to Europeana?s OpenSearch
API (fig. 4). From the result set an item is ran-
domly selected and the paragraph is augmented
with the link to the item, the item?s thumbnail im-
age and its title. If there are no hyperlinks in a
paragraph or the search returns no results, then no
augmentation is performed for that paragraph.
4 Evaluation
The initial evaluation focuses on the paragraph
augmentation, as the quality of that heavily de-
pends on the results provided by Europeana?s API
and on a log-analysis looking at how users com-
Question Yes No
Familiar 18 18
Appropriate 9 27
Supports 4 32
Visually interesting 13 23
Find out more 3 33
Table 1: Evaluation experiment results reduced from
the 5-point Likert-like scale to a yes/no level.
ing to Europeana from Wikiana behave.
4.1 Paragraph augmentation evaluation
For the paragraph augmentation evaluation 18
wikipedia articles were selected from six topics
(Place, Person, Event, Time period, Concept, and
Work of Art). From each article the first para-
graph and a random paragraph were selected for
augmentation, resulting in a total set of 36 aug-
mented paragraphs. In the experiment interface
the participants were shown the text paragraph,
the augmented thumbnail image, and five ques-
tions (?How familiar are you with the topic??,
?How appropriate is the image??, ?How well does
the image support the core ideas of the para-
graph??, ?How visually interesting is the image??,
and ?How likely are you to click on the image
to find out more??). Each question used a five-
point Likert-like scale for the answers, with 1 as
the lowest score and 5 the highest. Neither the
topic nor the paragraph selection have a statisti-
cally significant influence on the results. To sim-
plify the analysis the results have been reduced
to a yes/no level, where an image is classified as
?yes? for that question if more than half the partic-
ipants rated the image 3 or higher on that question
(table 1).
Considering the simplicity of the augmentation
approach and the fact that the search API is not
under our control, the results are promising. 9
out of 36 (25%) of the items were classified as
appropriate. The non-appropriate images are cur-
rently being analysed to determine whether there
are shared characteristics in the query structure or
item meta-data that could be used to improve the
query or filter out non-appropriate result items.
The difficulty with automatically adding items
taken from Europeana is also highlighted by the
fact that only 13 of the 36 (36%) items were clas-
sified as interesting. While no correlation could
be found between the two interest and appro-
97
Category Sessions 1st q. Med 3rd q.
Wikiana 88 6 11 15.25
All users 577642 3 8 17
Table 2: Summary statistics for the number of items
viewed in per session for users coming from our sys-
tem (Wikiana) and for all Europeana users.
priate results, only one of the 23 uninteresting
items was judged appropriate, while 8 out of 9
of the appropriate items were also judged to be
interesting. We are now looking at whether the
item meta-data might allow filtering uninteresting
items, as they seem unlikely to be appropriate.
Additionally the approach taken by (Zhu et al,
2007), where multiple images are shown per para-
graph, is also being investigated, as this might re-
duce the impact of non-appropriate items.
4.2 Log analysis
Although the paragraph augmentation results are
not as good as we had hoped, a log analysis shows
that the system can achieve its goal of introduc-
ing new users to an unknown CH collection (Eu-
ropeana). The system has been available online
for three months, although not widely advertised,
and we have collected Europeana?s web-logs for
the same period. Using the referer information in
the logs we can distinguish users that came to Eu-
ropeana through our system from all other Euro-
peana users. Based on this classification the num-
ber of items viewed per session were calculated
(table 2). To prevent the evaluation experiment
influencing the log analysis only logs acquired be-
fore the experiment date were used.
Table 2 clearly shows that users coming
through our system exhibit different browsing pat-
terns. The first quartile is higher, indicating that
Wikiana users do not leave Europeana as quickly,
which is further supported by the fact that 30% of
the general users leave Europeana after viewing
three items or less, while for Wikiana users it is
only 19%. At the same time the third quartile is
lower, showing that Wikiana users are less likely
to have long sessions on Europeana. The differ-
ence in the session length distributions has also
been validated using a Kolmogorov-Smirnov test
(p = 0.00287, D = 0.1929).
From this data we draw the hypothesis that
Wikiana is at least in part successfully attracting
users to Europeana that would normally not visit
or not stay and that it successfully helps users
overcome that first hurdle that causes almost one
third of all Europeana users to leave after viewing
three or less items.
5 Conclusion and Future Work
Recent digitisation efforts have led to large dig-
ital cultural heritage (CH) collections and while
search facilities provide access to users familiar
with the collections there is a lack of methods for
introducing new users to these collections. In this
paper we propose a novel method for discover-
ing items in an unfamiliar collection by brows-
ing Wikipedia. As the user browses Wikipedia
articles, these are augmented with a number of
thumbnail images of items taken from the un-
known collection that are appropriate to the ar-
ticle?s content. This enables the new user to be-
come familiar with what is available in the collec-
tion without having to immediately interact with
the collection?s search interface.
An early evaluation of the very straightforward
augmentation process revealed that further work
is required to improve the appropriateness of the
items used to augment the Wikipedia articles. At
the same time a log analysis of Europeana brows-
ing sessions showed that users introduced to Eu-
ropeana through our system were less likely to
leave after viewing less than three items, pro-
viding clear indication that the methodology pro-
posed in this paper is successful in introducing
new users to a large, aggregated CH collection.
Future work will focus on improving the qual-
ity of the augmentation results by including more
collections into the article-level augmentation and
by introducing an ?interestingness? ranking into
the paragraph augmentation. We will also look at
evaluating the system in a task-based setting and
with existing, comparable systems.
Acknowledgements
The research leading to these results has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013)
under grant agreement n? 270082. We ac-
knowledge the contribution of all project part-
ners involved in PATHS (see: http://www.
paths-project.eu).
98
References
Keith Andrews, Christian Gutl, Josef Moser, Vedran
Sabol, and Wilfried Lackner. 2001. Search result
visualisation with xfind. In uidis, page 0050. Pub-
lished by the IEEE Computer Society.
Murtha Baca. 2003. Practical issues in applying meta-
data schemas and controlled vocabularies to cultural
heritage information. Cataloging & Classification
Quarterly, 36(3?4):47?55.
Christine L. Borgman. 2009. The digital future is
now: A call to action for the humanities. Digital
humanities quarterly, 3(4).
Andy Borman, Rada Mihalcea, and Paul Tarau. 2005.
Picnet: Augmenting semantic resources with pic-
torial representations. In Knowledge Collection
from Volunteer Contributors: Papers from the 2005
Spring Symposium, volume Technical Report SS-
05-03. American Association for Artificial Intelli-
gence.
Abdus Sattar Chaudhry and Tan Pei Jiun. 2005. En-
hancing access to digital information resources on
heritage: A case of development of a taxonomy at
the integrated museum and archives system in sin-
gapore. Journal of Documentation, 61(6):751?776.
Chaomei Chen, Timothy Cribbin, Jasna Kuljis, and
Robert Macredie. 2002. Footprints of information
foragers: behaviour semantics of visual exploration.
International Journal of Human-Computer Studies,
57(2):139 ? 163.
Blaz Fortuna, Marko Grobelnik, and Dunja Mladenic.
2005. Visualization of text document corpus. In-
formatica, 29:497?502.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings
of the 20th international joint conference on Artif-
ical intelligence, IJCAI?07, pages 1606?1611, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Guntram Geser. 2004. Resource discovery - position
paper: Putting the users first. Resource Discovery
Technologies for the Heritage Sector, 6:7?12.
Kasper Hornb?k and Morten Hertzum. 2011. The no-
tion of overview in information visualization. In-
ternational Journal of Human-Computer Studies,
69(7-8):509 ? 525.
Antoine Isaac, Stefan Schlobach, Henk Matthezing,
and Claus Zinn. 2007. Integrated access to cul-
tural heritage resources through representation and
alignment of controlled vocabularies. Library Re-
view, 67(3):187?199.
Sook Lim. 2009. How and why do college students
use wikipedia? Journal of the American Society for
Information Science and Technology, 60(11):2189?
2202.
Xia Lin. 1992. Visualization for the document space.
In Proceedings of the 3rd conference on Visualiza-
tion ?92, VIS ?92, pages 274?281, Los Alamitos,
CA, USA. IEEE Computer Society Press.
Teun Lucassen and Jan Maarten Schraagen. 2010.
Trust in wikipedia: how users trust information
from an unknown source. In Proceedings of the 4th
workshop on Information credibility, WICOW ?10,
pages 19?26, New York, NY, USA. ACM.
Dongning Luo, Jing Yang, Milos Krstajic, William
Ribarsky, and Daniel A. Keim. 2012. Eventriver:
Visually exploring text collections with temporal
references. Visualization and Computer Graphics,
IEEE Transactions on, 18(1):93 ?105, jan.
Gary Marchionini. 2006. Exploratory search: From
finding to understanding. Communications of the
ACM, 49(4):41?46.
David Milne and Ian H. Witten. 2008. Learning
to link with wikipedia. In Proceedings of the
17th ACM conference on Information and knowl-
edge management, CIKM ?08, pages 509?518, New
York, NY, USA. ACM.
Glen Newton, Alison Callahan, and Michel Dumon-
tier. 2009. Semantic journal mappiong for search
visualization in a large scale article digital library.
In Second Workshop on Very Large Digital Li-
braries at ECDL 2009.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng
Chen. 2009. Mining multilingual topics from
wikipedia. In Proceedings of the 18th international
conference on World wide web, WWW ?09, pages
1155?1156, New York, NY, USA. ACM.
Panagiotis Papadakos, Stella Kopidaki, Nikos Arme-
natzoglou, and Yannis Tzitzikas. 2009. Ex-
ploratory web searching with dynamic taxonomies
and results clustering. In Maristella Agosti,
Jose? Borbinha, Sarantos Kapidakis, Christos Pa-
patheodorou, and Giannis Tsakonas, editors, Re-
search and Advanced Technology for Digital Li-
braries, volume 5714 of Lecture Notes in Computer
Science, pages 106?118. Springer Berlin / Heidel-
berg.
Peter Pirolli. 2009. Powers of 10: Modeling
complex information-seeking systems at multiple
scales. Computer, 42(3):33?40.
Patrick L Schmitz and Michael T Black. 2008. The
delphi toolkit: Enabling semantic search for mu-
seum collections. In Museums and the Web 2008:
the international conference for culture and her-
itage on-line.
Nick J. Schweitzer. 2008. Wikipedia and psychology:
Coverage of concepts and its use by undergraduate
students. Teaching of Psychology, 35(2):81?85.
Michael Steemson. 2004. Digicult experts seek out
discovery technologies for cultural heritage. Re-
source Discovery Technologies for the Heritage
Sector, 6:14?20.
99
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203 ? 217. World Wide Web Conference
2007Semantic Web Track.
Alistair Sutcliffe and Mark Ennis. 1998. Towards a
cognitive theory of information retrieval. Interact-
ing with Computers, 10:321?351.
Jacco van Ossenbruggen, Alia Amin, Lynda Hard-
man, Michiel Hildebrand, Mark van Assem, Borys
Omelayenko, Guus Schreiber, Anna Tordai, Vic-
tor de Boer, Bob Wielinga, Jan Wielemaker, Marco
de Niet, Jos Taekema, Marie-France van Orsouw,
and Annemiek Teesing. 2007. Searching and an-
notating virtual heritage collections with semantic-
web technologies. In Museums and the Web 2007.
Robert Villa, Martin Halvey, Hideo Joho, David Han-
nah, and Joemon M. Jose. 2010. Can an interme-
diary collection help users search image databases
without annotations? In Proceedings of the 10th
annual joint conference on Digital libraries, JCDL
?10, pages 303?312, New York, NY, USA. ACM.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu.
2009. Using wikipedia to bootstrap open informa-
tion extraction. SIGMOD Rec., 37:62?68, March.
Xiaojin Zhu, Andrew B. Goldberg, Mohamed Eldawy,
Charles A. Dyer, and Bradley Strock. 2007. A text-
to-picture synthesis system for augmenting commu-
nication. In The integrated intelligence track of
the 22nd AAAI Conference on Artificial Intelligence
(AAAI-07).
100
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 1?10,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Generating Paths through Cultural Heritage Collections
Samuel Fernando1, Paula Goodale2, Paul Clough2,
Mark Stevenson1, Mark Hall2, Eneko Agirre3
1Department of Computer Science, University of Sheffield
2Information School, University of Sheffield
3Computer Science Department, University of the Basque Country
{s.fernando, p.goodale, p.d.clough,
r.m.stevenson, m.mhall}@sheffield.ac.uk
e.agirre@ehu.es
Abstract
Cultural heritage collections usually or-
ganise sets of items into exhibitions or
guided tours. These items are often
accompanied by text that describes the
theme and topic of the exhibition and pro-
vides background context and details of
connections with other items. The PATHS
project brings the idea of guided tours
to digital library collections where a tool
to create virtual paths are used to assist
with navigation and provide guides on par-
ticular subjects and topics. In this pa-
per we characterise and analyse paths of
items created by users of our online sys-
tem. The analysis highlights that most
users spend time selecting items relevant
to their chosen topic, but few users took
time to add background information to the
paths. In order to address this, we con-
ducted preliminary investigations to test
whether Wikipedia can be used to au-
tomatically add background text for se-
quences of items. In the future we would
like to explore the automatic creation of
full paths.
1 Introduction
Paths (or trails) have been studied as a means of
assisting users with the navigation of digital col-
lections as an alternative to standard keyword-
based search (Furuta et al, 1997; Reich et al,
1999; Shipman et al, 2000; White and Huang,
2010). Paths can be particularly useful to users
who are unfamilar with the content of digital col-
lections (e.g. historical documents) and may find
it difficult to formulate appropriate queries (Wil-
son et al, 2010). Paths can be used to assist users
with the navigation of collections through the pro-
vision of narratives and subject guides. From an
educational perspective paths can provide tangible
learning objects, created by teachers and followed
by students. Alternatively from a cultural her-
itage perspective paths can be used to create activ-
ity trails and guided tours support exploration by
visitors through collections of cultural artefacts.
This echoes the organised galleries and guided
tours found in physical museums. The existance
of tools, such as Walden?s paths1, Trailmeme2 and
Storify3, provide functionalities for users to record
and share paths through web resources and digital
libraries. From this perspective everyone can take
on role of curator and provide access to their own
personal collections.
We have developed an online system called
PATHS that allows curators and end-users to cre-
ate and view paths to navigate through the Eu-
ropeana4 cultural heriage collection. As part of
evaluations of the prototype PATHS system par-
ticipants have created paths on various topics. In
this paper we describe a number of these paths and
their characteristics. Analysing paths that are cre-
ated manually and characterising them can be seen
as a first step towards developing methods to sup-
port the creation of paths automatically and semi-
automatically. Within the context of the PATHS
project this is being considered to deal with the
following limitations of manual creation of paths.
Firstly, the effort required in generating them often
means that a sufficient number of paths on a vari-
ety of topics are not available. Secondly, the man-
ual creation of paths is a very time-consuming pro-
cess that would benefit from computational sup-
port in whatever form this might take. This pa-
per presents initial work in automatically creat-
ing paths and provides the following novel con-
1http://www.csdl.tamu.edu/walden/
2http://open.xerox.com/Services/
xerox-trails
3http://storify.com/
4http://www.europeana.eu/
1
tributions: (1) we present results of user stud-
ies describing what people want from paths and
how they use them to navigate digital collections;
(2) we analyse a set of manually-created paths
to identify their properties and be able to charac-
terise them; and (3) we present work on automati-
cally generating background text for sequences of
items, thus providing an efficient way to enrich
paths with additional information with little man-
ual input required.
The paper is structured as follows: Section 2 de-
scribes related work on the use of narratives in cul-
tural heritage and previous approaches to automat-
ically generate paths; Section 3 defines the prob-
lem of generating paths and describes the datasets
used in the experiments; Section 4 presents analy-
sis of manually-created paths; Section 5 shows re-
sults of using automatic methods to generate back-
ground text; and finally Section 6 concludes the
paper and provides avenues for further work.
2 Related Work
2.1 Narratives and Cultural Heritage
The potential of narrative in digital CH to sup-
port learning, creativity and exploration is clear,
providing opportunities for supporting a more ac-
tive user interaction, including deeper engagement
with context, representation of the collecting pro-
cess, and facilitation of a more entertaining expe-
rience of learning (Mulholland and Collins, 2002).
Walker et al (2013) also propose narrative as a
major element of interaction and informal learn-
ing, suggesting that meaning is made when the
links between people and artefacts, and interpreta-
tion and ideas are surfaced, especially within so-
cial groups. Their experiments involve the use
of mobile and handheld technologies in a physi-
cal museum environment, capturing audio annota-
tions, but have much in common with experimen-
tal systems designed for path creation online. In a
similar vein the StoryBank project utilises collec-
tions of photographs and audio narratives to create
and share stories as information in the developing
world (Frohlich and Rachovides, 2008).
Whilst technologies have aided the creation and
sharing of narratives in physical cultural encoun-
ters, Manovich (1999) critiques the lack of narra-
tive in digital cultural environments, offering that
online collections and many CH web sites are
databases with constantly changing content that
inevitably lack a cohesive and persistent story.
However, since ?narrative is constructed by link-
ing elements of this database in a particular or-
der? (Manovich, 1999), it is possible to offer users
any number of explicit ?trajectories? (narratives)
through a digital information space, and by merg-
ing database and narrative in this way, creating
a more dynamic, discovery-led experience. This
view might be interpreted at its simplest level as
a virtual representation of the guided tours rou-
tinely offered in physical CH spaces, and indeed
there is a small strand of research into the creation
of systems for generating and exploring online ex-
hibitions and tours from items held within digital
collections. A scenario of users creating and edit-
ing trails in a CH context is described by Walker
(2006), including functionality for collecting, or-
dering and annotating museum objects.
2.2 Automatically Creating Paths
Generation of implicit trails through physical and
virtual museum spaces has been related to the
learning process (Peterson and Levene, 2003). In
this example, trails are automatically created by
users as they navigate their way through an infor-
mation space, and may be used for individual or
collaborative purposes. Research on the applica-
tion of curated pathways in web environments has
often focused on providing trails pre-prepared by
experts (e.g. curators, educationalists) as a means
of assisting novice users to navigate information
online (Shipman et al, 2000). Indeed, it has been
found that domain knowledge or expertise can
considerably enhance the quality of trails created
(Yuan and White, 2012). Automatic extraction
and generation of trails in information spaces has
been explored as a means of harnessing the wis-
dom of crowds, using the mass actions of earlier
user behaviour to establish relevance, and recom-
mend content or navigation routes to later users.
Such trails can be readily mined from search en-
gine transaction logs and have been shown to pro-
vide added value (White and Huang, 2010; Has-
san and White, 2012; Liao et al, 2012). West and
Leskovec (2012) take this notion a stage further
and attempt to identify wayfinding strategies em-
ployed by browsers in Wikipedia, with the goal of
assisting future users in their navigation by surfac-
ing potentially useful hyperlinks.
Guided tours or pathways are essentially more
structured, purposeful forms of trails, taking the
user through a specific sequence of information
2
nodes and may also be automatically generated,
rather than manually curated as in the examples
above. Wheeldon and Levene (2003) offer an al-
gorithm for generating trails from site-search, en-
abling elements of structure and context to be in-
corporated into the trails created in this way, but
noting potential scalability issues for web scale
search tasks. In the CH domain, a small num-
ber of projects have attempted to automatically
generate digital content in the form of exhibi-
tions, tours and trails. Ma?kela? et al (2007) de-
scribe a system which utilises semantically an-
notated content to generate personalised ?exhi-
bitions? from a structured narrative-based search
query. Similarly, Zdrahal et al (2008) demonstrate
how pathways can be generated through a collec-
tion of semantically related documents to provide
a means of exploration, using non-NLP cluster-
ing and path creation techniques. Sophisticated
approaches such as linear programming and evo-
lutionary algorithms have also been proposed for
generating summaries and stories (McIntyre and
Lapata, 2010; Woodsend and Lapata, 2010). In
contrast, Wang et al (2007) use a recommender
system approach to generate museum tours on
the basis of ratings stored within a dynamic user
model, and Pechenizkiy and Calders (2007) pro-
pose the additional use of data mining techniques
on log data to improve this type of tour personali-
sation.
In summary, online tours and trails are made
possible either through manually curated content
generated through the efforts of experts or other
end users, or have been automatically generated
from the mining of large scale search logs, or from
collections benefitting from semantically-linked
content and/or detailed user models.
3 Methodology
This study brings together work from several ar-
eas of the PATHS project. An analysis of what
paths might be used for and what form they are ex-
pected to take, has had implications for the system
design and functionality and evaluation measures.
A user study focused upon evaluation of the first
prototype has provided manually-created paths as
a basis for analysing path content and attributes,
which in turn informs the desired characteristics
of automated paths and the algorithm designed for
generating paths automatically.
3.1 Utilisation of Paths
Initial user requirements interviews with 22 ex-
pert users in the heritage, education and profes-
sional domains found a strong affinity with the
path metaphor, revealing a range of different in-
terpretations of what it means in the CH context
and how they could be employed in an online en-
vironment to engage with key audiences. Eight
interpretations of the path metaphor emerged:
1. Path as search history
2. Path as information seeking journey
3. Path as linked metadata
4. Path as a starting point or way in
5. Path as a route through
6. Path as augmented reality
7. Path as information literacy journey / learn-
ing process
8. Path as transaction process
The first three of these are closest to the idea
of hypertext trails, with trails defined by user in-
teraction in 1 and 2, and trails defined automati-
cally, by the system in 3. Variations 4-6 are more
creative interpretations, all suggesting opportuni-
ties for guiding the user into and through collec-
tions, encouraging exploration and/or offering an
immersive experience, conducive with our initial
vision for the PATHS system.
In addition to expert-defined routes, 5 also in-
corporates the idea of users being able to see and
follow ?well-trodden path? defined by the cumula-
tive interactions of other users, thus extending the
opportunities for utilizing search histories. Con-
versely, 7 and 8 are both process oriented, al-
though 7 is experiential, user-defined, learning-
oriented, typified by trial and error and unique to
the individual, whilst 8 is a rigid process designed
to escort all users consistently through a standard
process of pre-defined steps.
A strong emphasis was placed on path content
being carefully selected or ?curated? by the path-
creator, with the addition of context and interpre-
tation so that the objects within the path convey
a narrative or meaning. Content may be derived
from one collection, but there were seen to be sig-
nificant benefits from including objects from di-
verse collections, along with other materials from
external web sites.
Paths facilitate topic-based information re-
trieval typified by the berry-picking mode of in-
teraction (Bates, 1989), rather than known item
searching. Furthermore, paths may be a useful tool
3
for personal information management in both for-
mal and informal research scenarios, enabling the
user to record, reuse and share their research activ-
ity, or helping them to organize their ideas. Cre-
ativity is also encouraged, as user-generated paths
provide the means to repurpose CH objects into
users? own narratives for private or public con-
sumption.
A summary of specific user scenarios high-
lighted by participants is given below:
? Teachers/lecturers presentations and class-
room activities
? Museum personnel curating collections, giv-
ing an overview, or covering a topic in depth
? Leisure users browsing, collecting interest-
ing and/or visually appealing content
? Researchers to aid image-based research,
sharing and discussing findings with fellow
researchers and supervisors
? Non-academic specialists (e.g. local histori-
ans) collecting and sharing items of interest
with other enthusiasts
3.2 Defining the Problem
To create a path or narrative that guides a user
through a set of items from a collection, whether
as a manual process or automatically, there are
three main activities: (1) the selection of items to
include in the path; (2) the arrangement of items
to form a path or narrative and (3) the annota-
tion of the path to with descriptive text and back-
ground information. We envision techniques to
automate the entire process; however, a first step is
to analyse existing manually-created paths to iden-
tify their characteristics and inform the automatic
creation of similar structures.
3.3 User Study
The manually generated paths used for this study
were created as part of a more detailed user study
to evaluate the first prototype, conducted using
a protocol informed by the Interactive IR eval-
uation framework (Borlund, 2003). Twenty-two
users, including subject experts, students and gen-
eral users (subject novices), each completed a 2-
hour session, during which they participated in the
following activities:
? Profile questionnaire and cognitive style test
? Familiarisation with the system
? 4x short information seeking tasks (5 minutes
each)
? 1x long simulated work task - path creation
(30 minutes)
? Task feedback questionnaire
? Session/system feedback questionnaire
? Think-after interview based upon the com-
plex task
Of most interest here is the simulated work task,
with associated observations, feedback and reflec-
tions. This task focused on the creation of a path,
using a scenario adapted to the type of user. Free-
dom was given in choosing a subject for the path,
and limited instructions were provided in what
might be needed to complete the task, for exam-
ple:
?Imagine you are a student who has been asked
to create a path as part of a university assignment.
You have been asked to use primary source ma-
terials to create a mini online exhibition suitable
for a target group within the general public and/or
school visitor categories. Your goal is to introduce
a historical or art-focussed topic in a popular, ac-
cessible way, and to encourage further use and ex-
ploration of cultural heritage resources.?
Data on the tasks was captured via log files, as
well as screen recording and observations using
the Morae usability software. Detailed analysis
was undertaken of user behaviour in the process of
completing the task, and of the paths created, from
both quantitative and qualitative perspectives.
4 Analysing Manually-created Paths
In this section we describe the results of analysing
the 22 paths created manually in the PATHS pro-
totype system.
4.1 User behaviour
On average users spend 25.3 mins on creating a
path (min=11.7; max=33.6) with an average of
201 mouse clicks (min=53; max=380). From the
observations, it was noted that some participants
spent quite a lot of time thinking about the task
and pondering their next move, whilst others en-
gaged in more rapid fire activity in the face of
uncertainty. Analysis of the screen recordings
showed a variety of primary interaction styles for
this task, with a fairly even split between serial
searching (33%) and serial browsing (39%), as the
two most popular strategies. Serial searching in-
volves repetitive search and reformulation, with
only a page or two of search results viewed before
searching again, and serial browsing involves very
4
few searches, with large numbers of search re-
sults pages viewed (over 50 pages in some cases).
These are then in effect, polar opposites of interac-
tion. Only 6% engaged primarily in exploring be-
haviour (using the explore and similar items con-
tent), and 22% of participants occupied the middle
ground, utilising a mix of search, browse and ex-
plore, with no strong preference for any one style.
4.2 Properties of paths
The mean number of items in a path was 10.7 (std
dev=6.7 items) with a minimum of 5 items and
maximum of 29 items. Most popular bin is 6-
10 items in a path (59%). We found 85% of the
items included in the paths included an image with
the metadata. The paths created were manually
categorised by theme to ascertain whether there
are any distinct preferences for the subject mat-
ter of content included. The most popular cate-
gories were paths about places (23%), art subjects
(23%) and history subjects (32%). These themes
are likely to have been influenced at least partly
by what content is currently available in our col-
lection, although the amount of art-related content
is much less than for history, and also appear to
have been influenced by the topics covered in ex-
isting paths in the system (e.g. places, topics re-
lated to the world wars). There were, however a
significant number of expert users who attempted
to build paths related to their own research inter-
ests, with varying degrees of success.
4.3 Descriptions and ordering
Once items have been selected and they have been
transferred in the path creation workspace, users
have the opportunity to modify and enhance their
path with a number of tools for adding content and
metadata, and for re-ordering the content. On cre-
ating the path, most users immediately went to the
metadata fields and added information for the path
description and duration fields, as well as a num-
ber of tags (or keywords). A short 1-2 line de-
scription of the path appears to be the norm and
was added in 91% of cases. Tags were added by
82% of users and a duration by only 46% of users.
It is clear from further investigation that the tags
were added incorrectly (without commas between
them) by a significant number of users and a tip
for successful use is required.
The items within a path can be annotated with
the user?s own contextual information, and can be
re-ordered into a more meaningful sequence, such
as a chronological or narrative sequence. These
more advanced features were used by significantly
fewer users, which could indicate a learning issue,
a lack of need, or a time constraint. On reviewing
the paths created by our evaluation participants it
is found that in 41% of cases, contextual informa-
tion was not added to any items in the path. There
are however 32% in which annotations were added
to all items (generally these were shorter paths
with fewer items), and a further 27% where anno-
tations were added to some or most of the items.
In 72% of cases the items in the paths created
were re-ordered to some degree, with 17% spend-
ing a considerable amount of time on this activity.
This finding is encouraging, as the default is for
items to be included in the path in the order they
were saved to the workspace, and re-ordering in-
dicates that users are thinking about their path as a
whole and trying to make sense of the information
it is intended to convey. Typical types of ordering
included chronology (32%), narrative (23%), ge-
ography (for example, a walking tour - 9%), theme
(9%) and ?interestingness? (5%).
5 Enriching paths with background
information
This section describes preliminary work on the
task of semi-automated path creation. In par-
ticular we describe efforts to enrich paths with
background contextual information using relevant
Wikipedia articles. The related work described
in Section 2.2 shows that there have been previ-
ous efforts to automatically select cultural heritage
items to form paths, trails and exhibitions. How-
ever to our knowledge no significant effort has
been made to automatically annotate such paths
with descriptive or contextual information. The
interviews described in Section 3.1 highlighted
the importance CH experts placed on having ad-
ditional information to give context for the items
in the path. It was also noted during the manual
path-creation exercise (Section 4.3) that a signif-
icant number of the users did not add any such
information to the path. The reasons for this are
unclear, but nevertheless there seems to be suffi-
cient motivation to devise automatic methods for
this task. Although the methods have previously
been well established in other tasks5 , we believe
5INEX Tweet Contextualization Track (https:
//inex.mmci.uni-saarland.de/tracks/qa/)
and Link-the-wiki Track (http://www.inex.otago.
ac.nz/tracks/wiki-link/wiki-link.asp)
5
this is the first time they have been applied for the
task of annotating sequences of items in this way.
5.1 Method
Manually generated paths contain sequences of
items selected from Europeana on some topic or
theme. Creators provide their own title, subject
keywords and description for the path. To aid
creation of paths we explore whether background
information could be generated automatically for
such paths. An approach is presented here which
shows promise as a potential way to achieve this
task. The input for this approach is a sequence of
items and a key Wikipedia article which describes
the overall topic of the path. The output comprises
sentences taken from a relevant Wikipedia article.
The aim is for this output to provide useful and
interesting additional background information re-
lated to the items and theme of the path. In this
paper experiments are focussed on how to select
good quality text to present as additional informa-
tion for the path. For this reason the key Wikipedia
article is manually chosen, and the task is to find a
good approach for selecting the most relevant sen-
tences from this key article for the text.
Two methods are tested in this paper. The first
method simply takes the first n sentences of the
article and outputs this. Since Wikipedia articles
are always structured to have a summary of the
article in the first paragraph we can expect this text
to perform well as a summary of the path topic.
The second method is more advanced and at-
tempts to find text in the article that is relevant to
the actual items that have been chosen for the path.
This approach uses the Wikipedia Miner software
(Milne and Witten, 2008) to add inline links to
the text in the items for this approach. This soft-
ware disambiguates terms in the text and then de-
tects links using various features such as the com-
monness of the term, the overall relatedness of the
terms in the text and so on. The result is text en-
riched with inline links to relevant Wikipedia arti-
cles. Each link also has an associated confidence
value which indicates how sure the software is that
the link is correctly disambiguated and relevant to
the text.
The approach works as follows for a sequence
of items S and a key article K. First Wikipedia
Miner is run over the items in S. The text input to
Wikipedia Miner comprises the title, subject and
description fields of each item. The output is a set
of article titles W comprising the titles of all the
linked articles which were found in the text fields
of S. For each title in W we also have the associ-
ated confidence value for the link as calculated by
Wikipedia Miner. The next step is to select from
K the most relevant sentences to output as the gen-
erated text. For each sentence in K a score is as-
signed if any of the words in the sentence match
one of the titles in W . The score is then simply the
sum of the confidence values associated with these
titles. The top scoring sentences are then output as
the background text. This method can be consid-
ered to be a kind of query based summarisation
(Jurafsky and Martin, 2008).
5.2 Results
The automatic approaches for generating back-
ground text were run over the items in the 22 man-
ually created paths described in the previous sec-
tion. To ensure a fair test the user-added text was
was discarded and only the original Europeana
metadata for the items was used as source for the
methods.
For each path a single key Wikipedia article was
manually found which best captured the overall
theme or topic of the path. For all paths at least
one reasonably good article was found, although
for some paths the articles were better matches for
the path topic than for others. Some examples are
shown in Table 1.
With the input ready, the approaches described
were run over the items and key articles to gener-
ate the additional text. This was limited to 3 sen-
tences to provide a short but useful piece of text
for viewers of the paths. For example one path
was entitled ?Bronze Age metallurgy?. The user
provided description was as follows:
Bronze Age metalsmiths produce a variety of
objects, however, we learn as much from the tools,
leftover debris and the way in which objects were
deposited as we do from the finished objects them-
selves.
The key article chosen for the path was ?Bronze
Age?. The items in the path included various ar-
tifacts and working debris from the Bronze Age.
For the first approach the output is the first 3 sen-
tences from this article:
The Bronze Age is a period characterized by
the use of copper and its alloy bronze as the chief
hard materials in the manufacture of some imple-
ments and weapons. Chronologically, it stands be-
6
Path title Key Wikipedia article
Canals in England Canals of the United Kingdom
A brief history of Sheffield History of Sheffield
Art and Criticism of John Ruskin John Ruskin
Early medieval features in English
Parish Churches
Architecture of the medieval cathedrals
of England
Brooches through the ages Brooch
Table 1: Key articles for paths
tween the Stone Age and Iron Age. The term Stone
Age implies the inability to smelt any ore, the term
Bronze Age implies the inability to smelt iron ore
and the term Iron Age implies the ability to man-
ufacture artifacts in any of the three types of hard
material.
For the second approach the top 3 highest scor-
ing sentences were output:
A region could be in the Bronze Age either by
smelting its own copper and alloying with tin or
by trading for bronze from production areas else-
where. The Bronze Age in Ireland commenced
around 2000 BC, when copper was alloyed with
tin and used to manufacture Ballybeg type flat axes
and associated metalwork. The Bronze Age is a
period characterized by the use of copper and its
alloy bronze as the chief hard materials in the
manufacture of some implements and weapons.
These sentences scored most highly since they
contained the most high-confidence terms from
the items, for example terms such as ?copper?, ?al-
loy? and ?Bronze Age?.
5.3 Evaluation
To evaluate the two approaches, 5 annotators were
presented with the paths and the text and asked to
rate each path on 3 dimensions:
? The relevance of the text to the theme and
items of the path. Text which relates strongly
to the path is scored highly while off-topic or
irrelevant text is given a low score.
? The coherence or quality of the text itself.
Text which appears well-written and well-
structured is scored highly, while poorly writ-
ten or incoherent text is given a low score.
? The contextualisation of the text in relation
to the path. To achieve a high score the
text should offer useful or interesting addi-
tional information which is not found else-
where within the content, i.e. the text helps
to provide a context for items in the path.
Annotators were asked to grade from A (very
good) to E (very poor) on each dimension. The
results are shown in Figure 1. The results for
the first 3 sentences are shown as First3 and for
the weighted approach as Weighted. For each di-
mension, the distribution of judgements across the
paths is shown. The First3 approach was found
to be superior in every dimension. For relevance
scores 90% of the scores were either A or B com-
pared to 63% for the Weighted approach. Sim-
ilarly for the coherence judgements 97% were A
or B compared to 62% for the weighted approach.
The reason for this superior performance seems to
be that the first few sentences of Wikipedia arti-
cles are deliberately created to give a short sum-
mary introduction of the topic of the article. This
explains the high scores for relevance and coher-
ence.
Both approaches scored lower on the contex-
tualisation dimension, with First3 getting 67%
A or B grades and the Weighted approach get-
ting 43%. There may be several reasons for this.
Firstly one problem is that the auto-generated text
sometimes repeats information that is already in
the path and item descriptions; thus the text fails
to meet the requirement of ?useful additional in-
formation?. Secondly the text is sometimes quite
general and vague, rather than focussing on spe-
cific details which might be most relevant to the
items chosen for the path.
To measure the agreement among the annotators
the following approach was used. First the scores
were converted to numeric values; A to 1, B to 2
and so on. Then the scores for each annotator were
compared to the average of the scores of all the
other annotators. The correlation was computing
using Spearman?s correlation coefficient. These
scores were then averaged amongst all annotators
to give a final agreement value. The results are
shown in Table 2.
7
Figure 1: Comparing the results of the two methods.
First3 Weighted
Relevance 0.57 0.57
Coherence 0.28 0.56
Contextualisation 0.56 0.78
Table 2: Agreement amongst annotators.
For both approaches there was good agreement
on the Relevance dimension. For the Coherence
dimension the First3 approach got quite a low
score. This may be because one annotator gave
lower scores for all paths, while the others all gave
consistently high scores, which seems to have
skewed the correlation co-efficient. For the con-
textualisation dimension the correlation scores for
high for both approaches, and the Weighted ap-
proach in particular achieved a very high agree-
ment value.
6 Conclusions
This paper presented results of interviews about
creating paths through cultural heritage collec-
tions. These results inform us on how people
want to navigate through cultural heritage collec-
tions using the path metaphor, how they wish to
make use of paths for their work and education,
and what information and qualities they consider
it important for a path to contain. The paper also
presents results from studies using the PATHS pro-
totype software where users were able to search
and explore a large digital library collection and
create their own paths of items from the collection
on topics of their interest.
From the interviews it was clear that the experts
considered it important that the paths contain ad-
ditional information to convey contextual informa-
tion to understand the meaning of the items in the
path. The results from the user studies showed that
this need was not being met in a significant num-
ber of cases; users were putting items together on
a topic but adding little or no descriptive text about
the topic and the items in the path. Therefore we
identified this as a key task which might benefit
from automatic methods. The simpler approach
which output the first n sentences from the key
Wikipedia article was found to generate the best
results. The resulting generated text was found to
be relevant and coherent. In most cases the text
was also found to add useful context about the
topic.
Future work will further refine the text genera-
tion approach. The approach depends on success-
fully identifying a good key article for each path.
In these experiments the key article was manually
chosen, however we are devising methods to se-
lect this article automatically. To correct the prob-
lem with repeated information a filtering approach
could eliminate information that is already con-
tained within the paths.
Acknowledgments
The research leading to these results was car-
ried out as part of the PATHS project (http:
//paths-project.eu) funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 270082.
References
Marcia J Bates. 1989. The design of browsing and
berrypicking techniques for the online search inter-
8
face. Online Information Review.
Pia Borlund. 2003. The IIR evaluation model: a
framework for evaluation of interactive information
retrieval systems. Information research, 8(3).
David M Frohlich and Dorothy Rachovides. 2008. Us-
ing digital stories for local and global information
sharing. In Community and International Develop-
ment, CHI 2008 Workshop.
R. Furuta, F.. Shipman, C. Marshall, D. Brenner, and
H. Hsieh. 1997. Hypertext paths and the World-
Wide Web: experiences with Walden?s Paths. In
Proceedings of the eighth ACM conference on Hy-
pertext, pages 167?176, New York, NY.
Ahmed Hassan and Ryen W White. 2012. Task tours:
helping users tackle complex search tasks. In Pro-
ceedings of the 21st ACM international conference
on Information and knowledge management, pages
1885?1889. ACM.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing (2nd Edition) (Prentice
Hall Series in Artificial Intelligence). Prentice Hall.
Zhen Liao, Yang Song, Li-wei He, and Yalou Huang.
2012. Evaluating the effectiveness of search task
trails. In Proceedings of the 21st international con-
ference on World Wide Web, pages 489?498. ACM.
Eetu Ma?kela?, Osma Suominen, and Eero Hyvo?nen.
2007. Automatic exhibition generation based on
semantic cultural content. In Proc. of the Cultural
Heritage on the Semantic Web Workshop at ISWC+
ASWC, volume 2007.
Lev Manovich. 1999. Database as symbolic form.
Convergence: The International Journal of Re-
search into New Media Technologies, 5(2):80?99.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1562?
1572. Association for Computational Linguistics.
D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceeding of the 17th ACM confer-
ence on Information and knowledge management,
pages 509?518. ACM.
Paul Mulholland and Trevor Collins. 2002. Using dig-
ital narratives to support the collaborative learning
and exploration of cultural heritage. In Database
and Expert Systems Applications, 2002. Proceed-
ings. 13th International Workshop on, pages 527?
531. IEEE.
Mykola Pechenizkiy and Toon Calders. 2007. A
framework for guiding the museum tours person-
alization. In Proceedings of the Workshop on Per-
sonalised Access to Cultural Heritage (PATCH07),
pages 11?28.
Don Peterson and Mark Levene. 2003. Trail records
and navigational learning. London review of Educa-
tion, 1(3):207?216.
S. Reich, L. Carr, D. De Roure, and W. Hall. 1999.
Where have you been from here? Trails in hypertext
systems. ACM Computing Surveys, 31.
Frank M Shipman, Richard Furuta, Donald Brenner,
Chung-Chi Chung, and Hao-wei Hsieh. 2000.
Guided paths through web-based collections: De-
sign, experiences, and adaptations. Journal of
the American Society for Information Science,
51(3):260?272.
K. Walker, A. Main, and Fass. J. 2013. User-
Generated Trails in Third Places. In HCI-3P Work-
shop on Human Computer Interaction for Third
Places at Computer Human Interaction 2013.
Kevin Walker. 2006. Story structures. building nar-
rative trails in museums. In Technology-Mediated
Narrative Environments for Learning, pages 103?
114. Sense Publishers.
Yiwen Wang, Lora M Aroyo, Natalia Stash, and Lloyd
Rutledge. 2007. Interactive user modeling for per-
sonalized access to museum collections: The ri-
jksmuseum case study. In User Modeling 2007,
pages 385?389. Springer.
Robert West and Jure Leskovec. 2012. Human
wayfinding in information networks. In Proceed-
ings of the 21st international conference on World
Wide Web, pages 619?628. ACM.
Richard Wheeldon and Mark Levene. 2003. The best
trail algorithm for assisted navigation of web sites.
In Web Congress, 2003. Proceedings. First Latin
American, pages 166?178. IEEE.
Ryen W White and Jeff Huang. 2010. Assessing the
scenic route: measuring the value of search trails in
web logs. In Proceedings of the 33rd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 587?594. ACM.
M. Wilson, Kulesm B., M. Schraefel, and B. Schnei-
derman. 2010. From keyword search to explo-
ration: Designing future search interfaces for the
web. Foundations and Trends in Web Science,
2(1):1?97.
Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic generation of story highlights. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 565?574. Associ-
ation for Computational Linguistics.
Xiaojun Yuan and Ryen White. 2012. Building the
trail best traveled: effects of domain knowledge on
web search trailblazing. In Proceedings of the 2012
ACM annual conference on Human Factors in Com-
puting Systems, pages 1795?1804. ACM.
9
Zdenek Zdrahal, Paul Mulholland, and Trevor Collins.
2008. Exploring pathways across stories. In Proc.
of International Conference on Distributed Human-
Machine Systems.
10
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 31?34,
Dublin, Ireland, August 23, 2014.
Exploring the Use of Word Embeddings and Random Walks on Wikipedia
for the CogAlex Shared Task
Josu Goikoetxea, Eneko Agirre, Aitor Soroa
IXA NLP Group, University of the Basque Country, Basque Country
jgoicoechea009@ikasle.ehu.es, e.agirre@ehu.es, a.soroa@ehu.es
Abstract
In our participation on the task we wanted to test three different kinds of relatedness algorithms:
one based on embeddings induced from corpora, another based on random walks on WordNet
and a last one based on random walks based on Wikipedia. All three of them perform similarly
in noun relatedness datasets like WordSim353, close to the highest reported values. Although
the task definition gave examples of nouns, the train and test data were based on the Edinburgh
Association Thesaurus, and around 50% of the target words were not nouns. The corpus-based
algorithm performed much better than the other methods in the training dataset, and was thus
submitted for the test.
1 Introduction
Measuring semantic similarity and relatedness between terms is an important problem in lexical seman-
tics (Budanitsky and Hirst, 2006). It has applications in many natural language processing tasks, such as
Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like
Information Retrieval. Most of the proposed techniques are evaluated over manually curated word simi-
larity datasets like WordSim353 (Finkelstein et al., 2002), in which the weights returned by the systems
for word pairs are compared with human ratings.
The techniques used to solve this problem can be roughly classified into two main categories: those
relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias)
(Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre
et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman,
2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013).
Our main objective when participating in the CogAlex shared task was to check how a sample of each
kind of technique would cope with the task. We thus selected one of the best corpus-based models to
date and another approach based on random walks over WordNet and Wikipedia.
2 Word Embeddings
Neural Networks have become quite a useful tool in NLP on the last years, specially in semantics. A
lot of models have been developed, but all of them share two characteristics: they learn meaning from
non-labeled corpora and represent meaning in a distributional way. These models learn the meaning of
words from corpora, and they represent it distributionally by the so-called embeddings. This embeddings
are low-dimensional and dense vectors composed by integers, where the dimensions are latent semantic
features of words.
We have used the Mikolov model (Mikolov et al., 2013) for this task, due to its effectiveness in
similarity experiments (Baroni et al., 2014). This neural network reduces the computational complexity
of previous architectures by deleting the hidden layer, and also, it?s able to train with larger corpora (more
than 10
9
words) and extract embeddings with larger dimensionality.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
31
The Mikolov model has two variants: Continuous Bag of Words (CBOW) and Skip-gram. The first
one is quite similar to the feedforward Neural Net Language Model, but instead of a hidden layer it has
a projection layer; so, all the words are projected in the same position. Word order has thus no influence
in the projection. Training criterion is as follows: knowing past and future words, it will predict the one
in the middle.
The Skip-gram model is related to the previous one. The main difference is that it uses each current
word as an input to a log-linear classifier with a continuous projection layer, and predicts words within a
certain range before and after the current word.
In order to participate in this shared task, we have used the word2vec tool
1
. On the one hand, we
have used embeddings trained with the Skip-gram model on part of Google News corpus (about 100
billion words). The vectors have 300 dimensions and are publicly available
2
. On the other hand, we have
adapted the distance program in word2vec, so that its input is the test-set file of the shared task. The
way distance works is as follows:
? Reads all the vectors from the embeddings file, and stores them in memory.
? Reads the test-set file, and line by line
? Saves the five entry words if they exist in vocabulary.
? Dimension by dimension, sums five entries? embeddings into one vector, and normalizes it.
? Calculates the semantic distance from the normalized vector to all words in vocabulary, and
selects the closest ones.
? Writes in output file the closest words along with their distances, writing the closest word first.
3 Random Walks on Wikipedia
In the last year there have been many attempts to apply graph based techniques to many NLP problems,
including word sense disambiguation (Agirre et al., 2014) or measuring semantic similarity and related-
ness between terms (Agirre et al., 2009). Those techniques consider a given Knowledge Base (KB) as a
graph, where vertices represent KB concepts and relations among concepts are represented by edges.
For this particular task we represented WikiPedia as a graph, where articles are the vertices and
links between articles are the edges. Contrary to other work using Wikipedia links (Gabrilovich and
Markovitch, 2007; Milne and Witten, 2008), the use of the whole graph allows to apply algorithms that
take into account the whole structure of Wikipedia. We applied PageRank and Personalized PageRank
on the Wikipedia graph using freely available software (Agirre and Soroa, 2009; Agirre et al., 2014)
3
.
The PageRank algorithm (Brin and Page, 1998) ranks the vertices in a graph according to their relative
structural importance. The main idea of PageRank is that whenever a link from v
i
to v
j
exists in a graph,
a vote from node i to node j is produced, and hence the rank of node j increases. Besides, the strength of
the vote from i to j also depends on the rank of node i: the more important node i is, the more strength
its votes will have. Alternatively, PageRank can also be viewed as the result of a random walk process,
where the final rank of node i represents the probability of a random walk over the graph ending on node
i, at a sufficiently large time. Personalized PageRank (Haveliwala, 2002) is a variant of the PageRank
algorithm which biases the computation to prefer certain nodes of the graph.
Our method also needs a dictionary, an association between strings and Wikipedia articles. We con-
struct the dictionary using article titles, redirections, disambiguation pages, and anchor text extracted
from a Wikipedia dump
4
. Mentions are lowercased and all text between parenthesis is removed. If the
mention links to a disambiguation page, it is associated with all possible articles the disambiguation page
points to. Each association between a string and article is scored with the prior probability, estimated as
the number of times that the mention occurs in the anchor text of an article divided by the total number
of occurrences of the mention.
1
http://word2vec.googlecode.com/svn/trunk/
2
https:\/\/docs.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\&export=download
3
http://ixa2.si.ehu.es/ukb
4
we used a 2013 Wikipedia dump to build the dictionary
32
The method to compute the answer for a given set of words is very simple. We just compute the
Personalized PageRank algorithm over Wikipedia, initializing the walk using the set of given words,
obtaining a probability distribution over all Wikipedia articles. We then choose the article with maximum
probability, and return the title of the article as the expected answer.
Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calcu-
lation after 30 iterations. Some preliminary experiments on a related Word Sense Disambiguation task
indicated that the algorithm was quite robust to these values, and we did not optimize them.
4 Development results
After running the random walks algorithms on the development data, it was clear that WordNet and
Wikipedia were not sufficient resources for the task, and they were performing poorly. The embeddings,
on the other hand, were doing a good job (accuracy of 14.1%, having returned a word on 1907 of the
2000 train instances). This is in contradiction with the results obtained in word relatedness datasets: for
instance, in the WordSim353 dataset (Gabrilovich and Markovitch, 2007) we obtain Spearman correla-
tions of 68.5 using random walks on WordNet, 72.8 using random walks on Wikipedia, and 71.0 using
the embeddings.
One important difference between datasets like WordSim353 and the CogCalex data, is that in Word-
Sim353, all words are nouns in singular. From a small sample of the CogaLex training data, on the
contrary, we saw that only around 50% of the target words
5
are nouns, with many occurrences of gram-
matical words, and words in plural. Wikipedia only contains nouns, and even if WordNet contains verbs
and adjectives, the semantic relations that we use are not able to check whether a meaning should be
lexicalized as an adjective (absent in the dataset) or noun (absence). Note also that the random walk
algorithm does not use co-occurrence data, and as such it is not able to capture that absent and minded
are closely related as in ?absent minded?.
These differences between the WordSim 353 and the CogaLex data would explain the different be-
haviour of the algorithms. We would also like to mention that the definition of the task mentioned
examples which are closer to the capabilities of WordNet and Wikipedia (e.g. given a set of words like
?gin, drink, scotch, bottle and soda? the expected answer would be whisky). From the definition of the
task, it looked as if the task was about recovering a word given a definition (as in dictionaries), but the
actual data was based on the Edinburgh Association Thesaurus, which is a different kind of resource.
5 Test results
Given the development much better results of the embeddings, we submitted a run based on those. We
obtained 16.35% accuracy, ranking fourth in the evaluation of all twelve submissions.
6 Conclusions
We tested three different kinds of relatedness algorithms: one based on embeddings induced from cor-
pora, another based on random walks on WordNet and a last one based on random walks based on
Wikipedia. All three of them perform similarly in noun relatedness datasets like WordSim353. Although
the task definition gave examples of content nouns alone, the train and test data were based on the Ed-
inburgh Association Thesaurus, and only around 50% of the target words were nouns. The embedding
performed much better than the other methods in this dataset.
Acknowledgements
This material is based in part upon work supported by MINECO, in the scope of the CHIST-ERA READ-
ERS (PCIN-2013-002-C02-01) and SKATER (TIN2012-38584-C06-02) projects.
5
The words that need to be predicted.
33
References
E. Agirre and A. Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of 14th
Conference of the European Chapter of the Association for Computational Linguistics, Athens, Greece.
E. Agirre, A. Soroa, E. Alfonseca, K. Hall, J. Kravalova, and M. Pasca. 2009. A Study on Similarity and Relat-
edness Using Distributional and WordNet-based Approaches. In Proceedings of annual meeting of the North
American Chapter of the Association of Computational Linguistics (NAAC), Boulder, USA, June.
E. Agirre, M. Cuadros, G. Rigau, and A. Soroa. 2010. Exploring Knowledge Bases for Similarity. In Proceedings
of the Seventh conference on International Language Resources and Evaluation (LREC?10), Valletta, Malta,
May. European Language Resources Association (ELRA).
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense
disambiguation. Computational Linguistics, 40(1):57?88.
M.A. Alvarez and S.J. Lim. 2007. A Graph Modeling of Semantic Similarity between Words. Proceedings of the
Conference on Semantic Computing, pages 355?362.
Marco Baroni, Georgiana Dinu, and Germn Kruszewski. 2014. Don?t count, predict! A systematic comparison of
context-counting vs. context-predicting semantic vectors. In Proceedings of ACL.
D. Bollegala, Matsuo Y., and M. Ishizuka. 2007. Measuring Semantic Similarity between Words using Web
Search Engines. In Proceedings of WWW?2007.
S. Brin and L. Page. 1998. The Anatomy of a Large-scale Hypertextual Web Search Engine. In Proceedings of the
seventh international conference on World Wide Web 7, WWW7, pages 107?117, Amsterdam, The Netherlands,
The Netherlands. Elsevier Science Publishers B. V.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-based Measures of Lexical Semantic Relatedness. Com-
putational Linguistics, 32(1):13?47.
H. Chen, M. Lin, and Y. Wei. 2006. Novel Association Measures using Web Search with Double Checking. In
Proceedings of COCLING/ACL 2006.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing Search
in Context: The Concept Revisited. ACM Transactions on Information Systems, 20(1):116?131.
E Gabrilovich and S Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic
analysis. In Proceedings of IJCAI 2007, pages 1606?1611, Hyderabad, India.
T.H. Haveliwala. 2002. Topic-sensitive PageRank. In Proceedings of the 11th international conference on World
Wide Web (WWW?02), pages 517?526, New York, NY, USA.
T. Hughes and D. Ramage. 2007. Lexical Semantic Relatedness with Random Graph Walks. In Proceedings of
EMNLP-CoNLL-2007, pages 581?589.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word repre-
sentations. In Proceedings of NAACL-HLT, pages 746?751.
D. Milne and I.H. Witten. 2008. An effective, low-cost measure of semantic relatedness obtained from wikipedia
links. In In Proceedings of the first AAAI Workshop on Wikipedia and Artifical Intellegence (WIKIAI?08),
Chicago, I.L.
M. Sahami and T.D. Heilman. 2006. A Web-based Kernel Function for Measuring the Similarity of Short Text
Snippets. Proc. of WWW, pages 377?386.
D. Yang and D.M.W. Powers. 2005. Measuring Semantic Similarity in the Taxonomy of WordNet. Proceedings
of the Australasian conference on Computer Science.
34
