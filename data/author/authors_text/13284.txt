2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 103?111,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Exploring Content Features for Automated Speech Scoring
Shasha Xie, Keelan Evanini, Klaus Zechner
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
{sxie,kevanini,kzechner}@ets.org
Abstract
Most previous research on automated speech
scoring has focused on restricted, predictable
speech. For automated scoring of unrestricted
spontaneous speech, speech proficiency has
been evaluated primarily on aspects of pro-
nunciation, fluency, vocabulary and language
usage but not on aspects of content and topi-
cality. In this paper, we explore features repre-
senting the accuracy of the content of a spoken
response. Content features are generated us-
ing three similarity measures, including a lex-
ical matching method (Vector Space Model)
and two semantic similarity measures (Latent
Semantic Analysis and Pointwise Mutual In-
formation). All of the features exhibit moder-
ately high correlations with human proficiency
scores on human speech transcriptions. The
correlations decrease somewhat due to recog-
nition errors when evaluated on the output of
an automatic speech recognition system; how-
ever, the additional use of word confidence
scores can achieve correlations at a similar
level as for human transcriptions.
1 Introduction
Automated assessment of a non-native speaker?s
proficiency in a given language is an attractive ap-
plication of automatic speech recognition (ASR) and
natural language processing (NLP) technology; the
technology can be used by language learners for
individual practice and by assessment providers to
reduce the cost of human scoring. While much
research has been done about the scoring of re-
stricted speech, such as reading aloud or repeating
sentences verbatim (Cucchiarini et al, 1997; Bern-
stein et al, 2000; Cucchiarini et al, 2000; Witt and
Young, 2000; Franco et al, 2000; Bernstein et al,
2010b), much less has been done about the scor-
ing of spontaneous speech. For automated scor-
ing of unrestricted, spontaneous speech, most auto-
mated systems have estimated the non-native speak-
ers? speaking proficiency primarily based on low-
level speaking-related features, such as pronuncia-
tion, intonation, rhythm, rate of speech, and fluency
(Cucchiarini et al, 2002; Zechner et al, 2007; Chen
et al, 2009; Chen and Zechner, 2011a), although a
few recent studies have explored features based on
vocabulary and grammatical complexity (Zechner et
al., 2007; Bernstein et al, 2010a; Bernstein et al,
2010b; Chen and Zechner, 2011b).
To date, little work has been conducted on au-
tomatically assessing the relatively higher-level as-
pects of spontaneous speech, such as the content
and topicality, the structure, and the discourse in-
formation. Automated assessment of these aspects
of a non-native speaker?s speech is very challeng-
ing for a number of reasons, such as the short length
of typical responses (approximately 100 words for
a typical 1 minute response, compared to over 300
words in a typical essay/written response), the spon-
taneous nature of the speech, and the presence of
disfluencies and possible grammatical errors. More-
over, the assessment system needs text transcripts
of the speech to evaluate the high level aspects, and
these are normally obtained from ASR systems. The
recognition accuracy of state-of-the-art ASR sys-
tems on non-native spontaneous speech is still rel-
atively low, which will sequentially impact the re-
103
liability and accuracy of automatic scoring systems
using these noisy transcripts. However, despite these
difficulties, it is necessary for an automated assess-
ment system to address the high level information
of a spoken response in order to fully cover all as-
pects that are considered by human raters. Thus, in
this paper we focus on exploring features to repre-
sent the high-level aspect of speech mainly on the
accuracy of the content.
As a starting point, we consider approaches that
have been used for the automated assessment of con-
tent in essays. However, due to the qualitative dif-
ferences between written essays and spontaneous
speech, the techniques developed for written texts
may not perform as well on spoken responses. Still,
as a baseline, we will evaluate the content features
used for essay scoring on spontaneous speech. In
addition to a straightforward lexical Vector Space
Model (VSM), we investigate approaches using two
other similarity measures, Latent Semantic Analysis
(LSA) and Pointwise Mutual Information (PMI), in
order to represent the semantic-level proficiency of
a speaker. All of the content features are analyzed
using both human transcripts and speech recognizer
output, so we can have a better understanding of the
impact of ASR errors on the performance of the fea-
tures. As expected, the results show that the per-
formance on ASR output is lower than when hu-
man transcripts are used. Therefore, we propose im-
proved content features that take into account ASR
confidence scores to emphasize responses whose es-
timated word accuracy is comparatively higher than
others. These improved features can obtain similar
performance when compared to the results using hu-
man transcripts.
This paper is organized as follows. In the next
section we introduce previous research on auto-
mated assessment of content in essays and spoken
responses. The content features we generated and
the model we used to build the final speaking scores
are described in Sections 3 and Section 4, respec-
tively. In Section 5 we show the performance of
all our proposed features. Finally, we conclude our
work and discuss potential future work in Section 6.
2 Related Work
Most previous research on assessment of non-native
speech has focused on restricted, predictable speech;
see, for example, the collection of articles in (Es-
kenazi et al, 2009). When assessing spontaneous
speech, due to relatively high word error rates of
current state-of-the-art ASR systems, predominantly
features related to low-level information have been
used, such as features related to fluency, pronuncia-
tion or prosody (Zechner et al, 2009).
For scoring of written language (automated essay
scoring), on the other hand, several features related
to the high level aspects have been used previously,
such as the content and the discourse information.
In one approach, the lexical content of an essay was
evaluated by using a VSM to compare the words
contained in each essay to the words found in a sam-
ple of essays from each score category (Attali and
Burstein, 2006). In addition, this system also used
an organization feature measuring the difference be-
tween the ideal structure of an essay and the actual
discourse elements found in the essay. The features
designed for measuring the overall organization of
an essay assumed a writing strategy that included
an introductory paragraph, at least a three-paragraph
body with each paragraph in the body consisting of
a pair of main point, supporting idea elements, and
a concluding paragraph. In another approach, the
content of written essays were evaluated using LSA
by comparing the test essays with essays of known
quality in regard of their degree of conceptual rele-
vance and the amount of relevant content (Foltz et
al., 1999).
There has been less work measuring spoken re-
sponses in terms of the higher level aspects. In
(Zechner and Xi, 2008), the authors used a content
feature together with other features related to vocab-
ulary, pronunciation and fluency to build an auto-
mated scoring system for spontaneous high-entropy
responses. This content feature was the cosine word
vector product between a test response and the train-
ing responses which have the highest human score.
The experimental results showed that this feature
did not provide any further contribution above a
baseline of only using non-content features, and
for some tasks the system performance was even
slightly worse after including this feature. However,
104
we think the observations about the content features
used in this paper were not reliable for the following
two reasons: the number of training responses was
limited (1000 responses), and the ASR system had a
relatively high Word Error Rate (39%).
In this paper, we provide further analysis on the
performance of several types of content features.
Additionally, we used a larger amount of training
data and a better ASR system in an attempt to extract
more meaningful and accurate content features.
3 Automatic Content Scoring
In automatic essay scoring systems, the content of an
essay is typically evaluated by comparing the words
it contains to the words found in a sample of es-
says from each score category (1-4 in our experi-
ments), where the scores are assigned by trained hu-
man raters. The basic idea is that good essays will
resemble each other in their word choice, as will
poor essays. We follow this basic idea when extract-
ing content features for spoken responses.
3.1 Scoring Features
For each test spoken response, we calculate its simi-
larity scores to the sample responses from each score
category. These scores indicate the degree of simi-
larity between the words used in the test response
and the words used in responses from different score
points. Using these similarity scores, 3 content fea-
tures are generated in this paper:
? Simmax: the score point which has the high-
est similarity score between test response and
score vector
? Sim4: the similarity score to the responses
with the highest score category (4 in our ex-
periments).
? Simcmb: the linear combination of the similar-
ity scores to each score category.
4?
i=1
wi ? Simi (1)
where wi is scaled to [-1, 1] to imply its positive
or negative impact.
3.2 Similarity Measures
There are many ways to calculate the similarity be-
tween responses. A simple and commonly used
method is the Vector Space Model, which is also
used in automated essay scoring systems. Under this
approach, all the responses are converted to vectors,
whose elements are weighted using TF*IDF (term
frequency, inverse document frequency). Then, the
cosine similarity score between vectors can be used
to estimate the similarity between the responses the
vectors originally represent.
Other than this lexical matching method, we also
try two additional similarity measures to better cap-
ture the semantic level information: Latent Semantic
Analysis (Landauer et al, 1998) and a corpus-based
semantic similarity measure based on pointwise mu-
tual information (Mihalcea et al, 2006). LSA has
been widely used for computing document similar-
ity and other information retrieval tasks. Under this
approach, Singular Value Decomposition (SVD) is
used to analyze the statistical relationship between a
set of documents and the words they contain. A m?n
word-document matrix X is first built, in which each
element Xij represents the weighted term frequency
of word i in document j. The matrix is decomposed
into a product of three matrices as follows:
X = U?V T (2)
where U is an m?m matrix of left-singular vectors,
? is an m?n diagonal matrix of singular values, and
V is the n? n matrix of right-singular vectors.
The top ranked k singular values in ? are kept,
and the left is set to be 0. So ? is reformulated as ?k.
The original matrix X is recalculated accordingly,
Xk = U?kV
T (3)
This new matrix Xk can be considered as a
smoothed or compressed version of the original ma-
trix. LSA measures the similarity of two documents
by calculating the cosine between the corresponding
compressed column vectors.
PMI was introduced to calculate the semantic
similarity between words in (Turney, 2001). It is
based on the word co-occurrence on a large corpus.
Given two words, their PMI is computed using:
PMI(w1, w2) = log2
p(w1&w2)
p(w1) ? p(w2)
(4)
105
This indicates the statistical dependency between w1
and w2, and can be used as a measure of the semantic
similarity of two words.
Given the word-to-word similarity, we can calcu-
late the similarity between two documents using the
following function,
sim(D1, D2) =
1
2
(
?
w?{D1} (maxSim(w,D2) ? idf(w))?
w?{D1} idf(w)
+
?
w?{D2}(maxSim(w,D1) ? idf(w)?
w?{D2} idf(w))
)
(5)
maxSim(w,Di) = maxwj?{Di}PMI(w,wj)
(6)
For each word w in document D1, we find the word
in document D2 which has the highest similarity
to w. Similarly, for each word in D2, we iden-
tify the most similar words in D1. The similarity
score between two documents is then calculated by
combining the similarity of the words they contain,
weighted by their word specificity (i.e., IDF values).
In this paper, we use these three similarity mea-
sures to calculate the similarity between the test re-
sponse and the training responses for each score cat-
egory. Using the VSM method, we convert all the
training responses in one score category into one big
vector, and for a given test response we calculate its
cosine similarity to this vector as its similarity to that
corresponding score point vector. For the other simi-
larity measures, we calculate the test response?s sim-
ilarity to each of the training responses in one score
category, and report the average score as its similar-
ity to this score point. We also tried using this av-
erage similarity score for the VSM method, but our
experimental results showed that this average score
obtained lower performance than using one big vec-
tor generated from all the training samples due to
data sparsity. After the similarity scores to each of
the four score categories are computed, the content
features introduced in Section 3.1 are then extracted
and are used to evaluate the speaking proficiency of
the speaker.
4 System Architecture
This section describes the architecture of our auto-
mated content scoring system, which is shown in
Figure 1. First, the test taker?s voice is recorded,
and sent to the automatic speech recognition system.
Second, the feature computation module takes the
output hypotheses from the speech recognizer and
generates the content features. The last component
considers all the scoring features, and produces the
final score for each spoken response.
Featu
re?
Comp
utatio
n
Reco
gnize
d?Wo
rds?
Scori
ng?
Comp
utatio
n?
Mod
ule
and?U
ttera
nces
Featu
res
Spee
ch
Scori
ngM
odel
Spee
ch?
Reco
gnize
r
Scori
ng?M
odel
Audio
Files
Spea
king?S
cores
Figure 1: Architecture of the automated content scoring
system.
While we are using human transcripts of spoken
responses as a baseline in this paper, we want to note
that in an operational system as depicted in this fig-
ure, the scoring features are computed and extracted
using the hypotheses from the ASR system, which
exhibits a relatively high word error rate. These
recognition errors will sequentially impact the pro-
cess of calculating the similarity and computing the
content scores, and decrease the performance of the
final speaking scores. In order to improve the system
performance in this ASR condition, we explore the
use of word confidence scores from the ASR system
during feature generation. In particular, the similar-
ity scores between the test response and each score
category are weighted using the recognition confi-
dence score of the response, so that the scores can
also contain information related to its acoustic accu-
racy. The confidence score for one response is the
average value of all the confidence scores for each
word contained in the response. In Section 5, we
will evaluate the performance of our proposed con-
tent features using both human transcripts and ASR
outputs, as well as the enhanced content features us-
106
ing ASR confidence scores.
5 Experimental Results
5.1 Data
The data we use for our experiments are from the
Test of English as a Foreign Language R? internet-
based test (TOEFL iBT) in which test takers respond
to several stimuli using spontaneous speech. This
data set contains 24 topics, of which 8 are opinion-
based tasks, and 16 are contextual-based tasks. The
opinion-based tasks ask the test takers to provide
information or opinions on familiar topics based
on their personal experience or background knowl-
edge. The purpose of these tasks is to measure the
speaking ability of examinees independent of their
ability to read or listen to English language. The
contextual-based tasks engage reading, listening and
speaking skills in combination to mimic the kinds
of communication expected of students in campus-
based situations and in academic courses. Test tak-
ers read and/or listen to some stimulus materials and
then respond to a question based on them. For each
of the tasks, after task stimulus materials and/or test
questions are delivered, the examinees are allowed a
short time to consider their response and then pro-
vide their responses in a spontaneous manner within
either 45 seconds (for the opinion-based tasks) or 60
seconds (for the contextual-based tasks).
For each topic, we randomly select 1800 re-
sponses for training, and 200 responses as develop-
ment set for parameter tuning. Our evaluation data
contains 1500 responses from the same English pro-
ficiency test, which contain the same 24 topics. All
of these data are scored on a 0-4 scale by expert hu-
man raters. In our automated scoring system, we use
a filtering model to identify responses which should
have a score of 0, such as responses with a technical
difficulty (e.g., equipment problems, high ambient
noise), responses containing uncooperative behavior
from the speakers (e.g., non-English speech, whis-
pered speech). So in this paper we only focused on
the responses with scores of 1-4. Statistics for this
data set are shown in Table 1. As the table shows,
the score distributions are similar across the train-
ing, development, and evaluation data sets.
5.2 Speech recognizer
We use an ASR system containing a cross-word
triphone acoustic model trained on approximately
800 hours of spoken responses from the same En-
glish proficiency test mentioned above and a lan-
guage model trained on the corresponding tran-
scripts, which contain a total of over 5 million
words. The Word Error Rate (WER) of this system
on the evaluation data set is 33%.
5.3 Evaluation metric
To measure the quality of the developed features, we
employ a widely used metric, the Pearson correla-
tion coefficient (r). In our experiments, we use the
value of the Pearson correlation between the feature
values and the human proficiency scores for each
spoken response.
5.4 Feature performance on transcripts
In Section 3.1, we introduced three features derived
from the similarity between the test responses and
the training responses for each score point. We first
build the training samples for each topic, and then
compare the test responses with their corresponding
models. Three similarity measures are used for cal-
culating the similarity scores, VSM, LSA, and the
PMI-based method. In order to avoid the impact of
recognition errors, we first evaluate these similarity
methods and content features using the human tran-
scripts. The Pearson correlation coefficients on the
evaluation data set for this experiment are shown in
Table 2. The parameters used during model build-
ing, such as the weights for each score category in
the feature Simcmb and the number of topics k in
LSA, are all tuned on the development set, and ap-
plied directly on the evaluation set.
The correlations show that even the simple vec-
tor space model can obtain a good correlation of
0.48 with the human rater scores. The feature
Simcmb performs the best across almost all the test
setups, since it combines the information from all
score categories. The PMI-based features outper-
form the other two similarity methods when evalu-
ated both on all responses or only on the contextual-
based topics. We also observe that the correlations
on contextual-based tasks are much higher than on
opinion-based tasks. The reason for this is that
107
Table 1: Summary statistics of training, development and evaluation data set.
Data sets Responses Speakers score avg score sd
Score distribution (percentage %)
1 2 3 4
Train 43200 8000 2.63 0.79 1750 (4.1) 15128 (35.0) 20828 (48.2) 4837 (11.2)
Dev 4800 3760 2.61 0.79 215 (4.5) 1719 (35.8) 2295 (47.8) 499 (10.4)
Eval 1500 250 2.57 0.81 95 (6.3) 549 (36.6) 685 (45.7) 152 (10.1)
Table 2: Pearson correlations of the content features using human transcripts.
VSM LSA PMI
Simmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 Simcmb
ALL 0.46 0.32 0.48 0.32 0.38 0.45 0.18 0.51 0.53
Contextual 0.50 0.51 0.58 0.36 0.55 0.57 0.21 0.57 0.62
Opinion 0.37 0.03 0.25 0.29 0.14 0.22 0.06 0.42 0.51
the contextual-based tasks are more constrained to
the materials provided with the test item, whereas
the opinion-based tasks are relatively open-ended.
Therefore, it is easier for the similarity measures to
track the content, the topics, or the vocabulary usage
of the contextual-based topics. Overall, the best cor-
relations are obtained using the feature combining
the similarity scores to each score category and the
PMI-based methods to calculate the similarity. Here,
the Pearson correlations are 0.53 for all responses,
and 0.62 for the contextual-based tasks only.
We also investigated whether additional perfor-
mance gains could be achieved by combining infor-
mation from the three different content features to
build a single overall content score, since the three
features may measure disparate aspects of the re-
sponse. The combination model we use is mul-
tiple regression, in which the score assigned to a
test response is estimated as a weighted linear com-
bination of a selected set of features. The fea-
tures are the similarity values to each score category
(Simi, i ? {1, 2, 3, 4}), calcuated using the three
similairty measures. In total we have 12 content
features. The regression model is also built on the
development set, and tested on the evaluation set.
The correlation for the final model is 0.60 on all
responses, which is significantly better than the in-
dividual models (0.48 for VSM, 0.45 for LSA, and
0.53 for PMI). Compared to results reported in pre-
vious work on similar speech scoring tasks but mea-
suring other aspects of speech, our correlation re-
sults are very competitive (Zechner and Xi, 2008;
Zechner et al, 2009).
5.5 Feature Performance on ASR output
The results shown in the previous section were ob-
tained using human transcripts of test responses, and
were reported in order to demonstrate the meaning-
fulness of the proposed features. However, in prac-
tical automated speech scoring systems, the only
available text is the output of the ASR system, which
may contain a large number of recognition errors.
Therefore, in this section we show the performance
of the content features extracted using ASR hy-
potheses. Note that we still use the human tran-
scripts of the training samples to train the models,
the parameter values and the regression weights;
however, we only use ASR output of the evaluation
data for testing the feature performance. These cor-
relations are shown in Table 3.
Compared to the results in Table 2, we find that
the VSM and LSA methods are very robust to recog-
nition errors, and we only observe slight correlation
decreases on these features. However, the decrease
for the PMI-based method is quite large. A possi-
ble reason for this is that this method is based on
word-to-word similarity computed on the training
data, so the mismatch between training and evalu-
ation set likely has a great impact on the computa-
tion of the similarity scores, since we train on human
transcripts, but test using ASR hypotheses. Likely
for the same reason, the regression model combining
all the features does not provide any further contri-
bution to the correlation result (0.44 when evaluated
108
Table 3: Pearson correlations of the content features using ASR output.
VSM LSA PMI
Simmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 Simcmb
ALL 0.43 0.34 0.48 0.30 0.37 0.43 0.11 0.24 0.42
Contextual 0.49 0.53 0.58 0.34 0.54 0.57 0.16 0.31 0.53
Opinion 0.30 0.05 0.07 0.25 0.12 0.15 0.05 0.17 0.27
Table 4: Pearson correlations of the content features using ASR output with confidence scores.
VSM LSA PMI
Simmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 Simcmb
ALL 0.43 0.36 0.48 0.30 0.40 0.45 0.11 0.39 0.51
Contextual 0.49 0.55 0.58 0.34 0.57 0.59 0.16 0.46 0.59
Opinion 0.30 0.24 0.25 0.25 0.18 0.20 0.05 0.32 0.40
on all responses).
In Section 4, we proposed using ASR confidence
scores during feature extraction to introduce acous-
tic level information and, thus, penalize responses
for which the ASR output is less likely to be correct.
Under this approach, all similarity scores are mul-
tiplied by the average word confidence score con-
tained in the test response. The performance of these
enhanced features is provided in Table 4. Compared
to the scores in Table 3, the enhanced features per-
form better than the basic features that do not take
the confidence scores into consideration. Using this
approach, we can improve the correlation scores for
most of the features, especially for the PMI-based
features. These features had lower correlations be-
cause of the recognition errors, but with the con-
fidence scores, they outperform the other features
when evaluated both on all responses or only on
contextual-based responses. Note that the correla-
tions for feature Simmax remains the same because
the same average confidence scores for each test re-
sponse is multiplied by the similarity scores to each
of the score points, so the score point obtaining the
highest similarity score is the same whether the con-
fidence scores are considered or not. The correlation
of the regression model also improves from 0.44 to
0.51 when the confidence scores are included. Over-
all, the best correlations for the individual similarity
features with the confidence scores are very close to
those obtained using human transcripts, as shown in
Tables 2 and 4: the difference is 0.53 vs. 0.51 for
all responses, and 0.62 vs. 0.59 for contextual-based
tasks only.
Because all models and parameter values are
trained on human transcripts, this experimental
setup might not be optimal for using ASR outputs.
For instance, the regression model does not outper-
form the results of individual features using ASR
outputs, although the confidence scores help im-
prove the overall correlation scores. We expect that
we can obtain better performance by using a regres-
sion model trained on ASR transcripts, which can
better model the impact of noisy data on the features.
In our future work, we will build sample responses
for each score category, tune the parameter values,
and train the regression model all on ASR hypothe-
ses. We hope this can solve the mismatch problem
during training and evaluation, and can provide us
even better correlation results.
6 Conclusion and Future Work
Most previous work on automated scoring of spon-
taneous speech used features mainly related to low-
level information, such as fluency, pronunciation,
prosody, as well as a few features measuring aspects
such as vocabulary diversity and grammatical accu-
racy. In this paper, we focused on extracting con-
tent features to measure the speech proficiency in
relatively higher-level aspect of spontaneous speech.
Three features were computed to measure the sim-
ilarity between a test response and a set of sam-
ple responses representing different levels of speak-
ing proficiency. The similarity was calculated using
different methods, including the lexical matching
109
method VSM, and two corpus-based semantic simi-
larity measures, LSA and PMI. Our experimental re-
sults showed that all the features obtained good cor-
relations with human proficiency scores if there are
no recognition errors in the text transcripts, with the
PMI-based method performing the best over three
similarity measures. However, if we used ASR tran-
scripts, we observed a marked performance drop for
the PMI-based method. Although we found that
VSM and LSA were very robust to ASR errors, the
overall correlations for the ASR condition were not
as good as using human transcripts. To solve this
problem, we proposed to use ASR confidence scores
to improve the feature performance, and achieved
similar results as when using human transcripts.
As we discussed in Section 5, all models were
trained using human transcripts, which might de-
crease the performance when these models are ap-
plied directly to the ASR outputs. In our future
work, we will compare models trained on human
transcripts and on ASR outputs, and investigate
whether we should use matching data for training
and evaluation, or whether we should not introduce
noise during training in order to maintain the validity
of the models. We will also investigate whether the
content features can provide additional information
for automated speech scoring, and help build better
scoring systems when they are combined with other
non-content features, such as the features represent-
ing fluency, pronunciation, prosody, vocabulary di-
versity information. We will also explore generating
other features measuring the higher-level aspects of
the spoken responses. For example, we can extract
features assessing the responses? relatedness to the
stimulus of an opinion-based task. For contextual-
based tasks, the test takers are asked to read or lis-
ten to some stimulus material, and answer a ques-
tion based on this information. We can build models
using these materials to check the correctness and
relatedness of the spoken responses.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater R? v.2. The Journal of Technology,
Learning, and Assessment, 4(3):3?30.
Jared Bernstein, John De Jong, David Pisoni, and Brent
Townshend. 2000. Two experiments on automatic
scoring of spoken language proficiency. In Proceed-
ings of Integrating Speech Tech. in Learning (InSTIL).
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010a. Fluency and structural complexity as predic-
tors of L2 oral proficiency. In Proceedings of Inter-
speech.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010b. Validating automated speaking tests. Lan-
guage Testing, 27(3):355?377.
Lei Chen and Klaus Zechner. 2011a. Applying rhythm
features to automatically assess non-native speech. In
Proceedings of Interspeech.
Miao Chen and Klaus Zechner. 2011b. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
ceedings of ACL-HLT.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven as-
sessment of non-native spontaneous speech. In Pro-
ceedings of NAACL-HLT.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of Dutch pronunciation by using
speech recognition technology. In IEEE Workshop on
Auotmatic Speech Recognition and Understanding.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learners?
fluency by means of automatic speech recognition
technology. Journal of the Acoustical Society of Amer-
ica, 107:989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learners?
fluency: comparisons between read and spontaneous
speech. Journal of the Acoustical Society of America,
111(6):2862?2873.
Maxine Eskenazi, Abeer Alwan, and Helmer Strik. 2009.
Spoken language technology for education. Speech
Communication, 51(10):831?1038.
Peter W. Foltz, Darrell Laham, and Thomas K. Landauer.
1999. The Intelligent Essay Assessor: Applications to
educational technology. Interactive multimedia Elec-
tronic Journal of Computer-Enhanced Learning, 1(2).
Horacio Franco, Leonardo Neumeyer, Vassilios Di-
galakis, and Orith Ronen. 2000. Combination of ma-
chine scores for automatic grading of pronunciation
quality. Speech Communication, 30(1-2):121?130.
Thomas K Landauer, Peter W. Foltz, and Darrell Laham.
1998. Introduction to Latent Semantic Analysis. Dis-
course Processes, 25:259?284.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of
the American Association for Artificial Intelligence,
September.
110
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
ECML.
Silke M. Witt and Steve J. Young. 2000. Phone-
level pronunciation scoring and assessment for interac-
tive language learning. Speech Communication, 30(1-
2):95?108.
Klaus Zechner and Xiaoming Xi. 2008. Towards auto-
matic scoring of a test of spoken language with het-
erogeneous task types. In Proceedings of the Third
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications.
Klaus Zechner, Derrick Higgins, and Xiaoming Xi.
2007. SpeechraterTM: A construct-driven approach
to score spontaneous non-native speech. In Proceed-
ings of the 2007 Workshop of the International Speech
Communication Association (ISCA) Special Interest
Group on Speech and Language Technology in Edu-
cation (SLaTE).
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken En-
glish. Speech Communication, 51(10):883?895.
111
Proceedings of NAACL-HLT 2013, pages 814?819,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Coherence Modeling for the Automated Assessment of  
Spontaneous Spoken Responses 
 
Xinhao Wang, Keelan Evanini, Klaus Zechner 
Educational Testing Service 
660 Rosedale Road 
Princeton, NJ 08541, USA 
xwang002,kevanini,kzechner@ets.org 
 
 
 
 
Abstract 
This study focuses on modeling discourse co-
herence in the context of automated assess-
ment of spontaneous speech from non-native 
speakers. Discourse coherence has always 
been used as a key metric in human scoring 
rubrics for various assessments of spoken lan-
guage. However, very little research has been 
done to assess a speaker's coherence in auto-
mated speech scoring systems. To address 
this, we present a corpus of spoken responses 
that has been annotated for discourse coher-
ence quality. Then, we investigate the use of 
several features originally developed for es-
says to model coherence in spoken responses. 
An analysis on the annotated corpus shows 
that the prediction accuracy for human holistic 
scores of an automated speech scoring system 
can be improved by around 10% relative after 
the addition of the coherence features.  Fur-
ther experiments indicate that a weighted F-
Measure of 73% can be achieved for the au-
tomated prediction of the coherence scores. 
1 Introduction 
In recent years, much research has been conducted 
into developing automated assessment systems to 
automatically score spontaneous speech from non-
native speakers with the goals of reducing the bur-
den on human raters, improving reliability, and 
generating feedback that can be used by language 
learners. Various features related to different as-
pects of speaking proficiency have been exploited, 
such as delivery features for pronunciation, proso-
dy, and fluency (Strik and Cucchiarini, 1999; Chen 
et al, 2009; Cheng, 2011; Higgins et al, 2011), as 
well as language use features for vocabulary and 
grammar, and content features (Chen and Zechner, 
2011; Xie et al, 2012). However, discourse-level 
features related to topic development have rarely 
been investigated in the context of automated 
speech scoring. This is despite the fact that an im-
portant criterion in the human scoring rubrics for 
speaking assessments is the evaluation of coher-
ence, which refers to the conceptual relations be-
tween different units within a response. 
Methods for automatically assessing discourse 
coherence in text documents have been widely 
studied in the context of applications such as natu-
ral language generation, document summarization, 
and assessment of text readability. For example, 
Foltz et al (1998) measured the overall coherence 
of a text by utilizing Latent Semantic Analysis 
(LSA) to calculate the semantic relatedness be-
tween adjacent sentences. Barzilay and Lee (2004) 
introduced an HMM-based model for the docu-
ment-level analysis of topics and topic transitions. 
Barzilay and Lapata (2005; 2008) presented an 
approach to coherence modeling which focused on 
the entities in the text and their grammatical transi-
tions between adjacent sentences, and calculated 
the entity transition probabilities on the document 
level. Pitler et al (2010) provided a summary of 
the performance of several different types of 
features for automated coherence evaluation, such 
as cohesive devices, adjacent sentence similarity, 
Coh-Metrix (Graesser et al, 2004), word co-
occurrence patterns, and entity-grid. 
In addition to studies on well-formed text, re-
searchers have also addressed coherence modeling 
on text produced by language learners, which may 
contain many spelling and grammar errors.  
Utilizing LSA and Random Indexing methods, 
Higgins et al (2004) measured the global 
814
coherence of students? essays by calculating the 
semantic relatedness between sentences and the 
corresponding prompts. In addition, Burstein et. al 
(2010) combined entity-grid features with writing 
quality features produced by an automated assess-
ment system of essays to predict the coherence 
scores of student essays. Recently, Yannakoudakis 
and Briscoe (2012) systematically analyzed a vari-
ety of coherence modeling methods within the 
framework of an automated assessment system for 
non-native free text responses and indicated that 
features based on Incremental Semantic Analysis 
(ISA), local histograms of words, the part-of-
speech IBM model, and word length were the most 
effective.   
In contrast to these previous studies involving 
well-formed text or learner text containing errors, 
this paper focuses on modeling coherence in spon-
taneous spoken responses as well as investigating 
discourse features in an attempt to extend the con-
struct coverage of an automated speech scoring 
system. In a related study, Hassanali et al (2012) 
investigated coherence modeling for spoken lan-
guage in the context of a story retelling task for the 
automated diagnosis of children with language im-
pairment. They annotated transcriptions of chil-
dren's narratives with coherence scores as well as 
markers of narrative structure and narrative quali-
ty; furthermore they built models to predict the 
coherence scores based on Coh-Metrix features 
and the manually annotated narrative features. The 
current study differs from this one in that it deals 
with free spontaneous spoken responses provided 
by students at a university level; these responses 
therefore contain more varied and more complicat-
ed information than the child narratives. 
The main contributions of this paper can be 
summarized as follows: First, we obtained coher-
ence annotations on a corpus of spontaneous spo-
ken responses drawn from a university-level 
English language proficiency assessment, and 
demonstrated an improvement of around 10% rela-
tive in the accuracy of the automated prediction of 
human holistic scores with the addition of the co-
herence annotations. Second, we applied the entity-
grid features and writing quality features from an 
automated essay scoring system to predict the co-
herence scores; the experimental results have 
shown promising correlations between some of 
these features and the coherence scores.  
2 Data and Annotation 
2.1 Data 
For this study, we collected 600 spoken responses 
from the international TOEFL? iBT assessment of 
English proficiency for non-native speakers. 100 
responses were drawn from each of 6 different test 
questions comprising two different speaking tasks: 
1) providing an opinion based on personal experi-
ence (N = 200) and 2) summarizing or discussing 
material provided in a reading and/or listening pas-
sage (N = 400). The spoken responses were all 
transcribed by humans with punctuation and capi-
talization. The average number of words contained 
in the responses was 104.4 (st. dev. = 34.4) and the 
average number of sentences was 5.5 (st. dev. = 
2.1).  
The spoken responses were all provided with 
holistic English proficiency scores on a scale of 1 - 
4 by expert human raters in the context of opera-
tional, high-stakes scoring for the spoken language 
assessment. The scoring rubrics address the fol-
lowing three main aspects of speaking proficiency: 
delivery (pronunciation, fluency, prosody), lan-
guage use (grammar and lexical choice), and topic 
development (content and coherence). In order to 
ensure a sufficient quantity of responses from each 
proficiency level for training and evaluating the 
coherence prediction features, the spoken respons-
es selected for this study were balanced based on 
the human scores as follows: 25 responses were 
selected randomly from each of the 4 score points 
(1 - 4) for each of the 6 test questions. In some 
cases, more than one response was selected from a 
given test-taker; in total, 471 distinct test-takers are 
represented in the data set. 
2.2 Annotation and Analysis 
The coherence annotation guidelines used for the 
spoken responses in this study were modified 
based on the annotation guidelines developed for 
written essays described in Burstein et al (2010). 
According to these guidelines, expert annotators 
provided each response with a score on a scale of 1 
- 3. The three score points were defined as follows: 
3 = highly coherent (contains no instances of con-
fusing arguments or examples), 2 = somewhat co-
herent (contains some awkward points in which the 
speaker's line of argument is unclear), 1 = barely 
815
coherent (the entire response was confusing and 
hard to follow; it was intuitively incoherent as a 
whole and the annotators had difficulties in identi-
fying specific weak points). For responses receiv-
ing a coherence score of 2, the annotators were 
required to highlight the specific awkward points 
in the response. In addition, the annotators were 
specifically required to ignore disfluencies and 
grammatical errors as much as possible; thus, they 
were instructed to not label sentences or clauses as 
awkward points solely because of the presence of 
disfluent or ungrammatical speech.  
Two annotators (not drawn from the pool of ex-
pert human raters who provided the holistic scores) 
made independent coherence annotations for all 
600 spoken responses. The distribution of annota-
tions across the three score points is presented in 
Table 1. The two annotators achieved a moderate 
inter-annotator agreement (Landis and Koch, 1977) 
of ? = 0.68 on the 3-point scale. The average of the 
two coherence scores provided by the two annota-
tors correlates with the holistic speaking proficien-
cy scores at r = 0.66, indicating that the overall 
proficiency scores of spoken responses can benefit 
from the discourse coherence annotations. 
 
 1 2 3 
# 1 160 (27%) 278 (46%) 162 (27%) 
# 2 125 (21%) 251 (42%) 224 (37%) 
Table 1. Distribution of coherence annotations from two 
annotators 
 
Furthermore, coherence features based on the 
human annotations were examined within the con-
text of an automated spoken language assessment 
system, SpeechRaterSM (Zechner et al, 2007; 
2009). We extracted 96 features related to pronun-
ciation, prosody, fluency, language use, and con-
tent development using SpeechRater. These 
features were either extracted directly from the 
speech signal or were based on the output of an 
automatic speech recognition system (with a word 
error rate of around 28%1
                                                          
1 Both the training and evaluation sets used to develop the 
speech recognizer consist of similar spoken responses drawn 
from the same assessment. However, there is no response 
overlap between these sets and the corpus used for discourse 
coherence annotation in this study. 
). By utilizing a decision 
tree classifier (the J48 implementation from Weka 
(Hall et al, 2009)), 4-fold cross validation was 
conducted on the 600 responses to train and evalu-
ate a scoring model for predicting the holistic pro-
ficiency scores. The resulting correlation between 
the predicted scores (based on the 96 baseline 
SpeechRater features) and the human holistic pro-
ficiency scores was r = 0.667.  
In order to model a spoken response's coher-
ence, three different features were extracted from 
the human annotations. Firstly, the average of the 
two annotators? coherence scores was directly used 
as a feature with a 5-point scale (henceforth 
Coh_5). Secondly, following the work in Burstein 
et al (2010), we collapsed the average coherence 
scores into a 2-point scale to deal with the 
difficulty in distinguishing somewhat and highly 
coherent responses. For this second feature 
(henceforth Coh_2), scores 1 and 1.5 were mapped 
to score 1, and scores 2, 2.5, and 3 were mapped to 
score 2. Finally, the number of awkward points 
was also counted as a feature (henceforth Awk). 
As shown in Table 2, when these three coherence 
features were combined separately with the 
SpeechRater features, the correlations could be 
improved from r = 0.667 to r > 0.7. Meanwhile, 
the accuracy (i.e., the percentage of correctly pre-
dicted holistic scores) could be improved from 
0.487 to a range between 0.535 and 0.543.  
 
Features r Accuracy 
SpeechRater 0.667 0.487 
SpeechRater+Coh_5 0.714 0.540 
SpeechRater+Coh_2 0.705 0.543 
SpeechRater+Awk 0.702 0.535 
SpeechRater+Coh_5+Awk 0.703 0.537 
SpeechRater+Coh_2+Awk 0.701 0.542 
Table 2. Improvement to an automated speech scoring 
system after the addition of human-assigned coherence 
scores and measures, showing both Pearson r correla-
tions and the ratio of correctly matched holistic scores 
between the system and human experts 
 
These experimental results demonstrate that the 
automatic scoring system can benefit from coher-
ence modeling either by directly using a human-
assigned coherence score or the identified awk-
ward points. However, the use of both kinds of 
annotations does not provide further improvement. 
When collapsing the average scores into a 2-point 
scale, there was a 0.009 correlation drop (not sta-
tistically significant), but the accuracy was slightly 
improved. In addition, due to the relatively small 
816
size of the set of available coherence annotations, 
we adopted the collapsed 2-point scale instead of 
the 5-point scale for the coherence prediction ex-
periments in the next section.  
2.3 Experimental Design 
As demonstrated in Section 2.2, the collapsed av-
erage coherence score can be used to improve the 
performance of an automated speech scoring sys-
tem. Therefore, this study treats coherence predic-
tion as a binary classification task: low-coherent 
vs. high-coherent, where the low-coherent re-
sponses are those with average scores 1 and 1.5, 
and the high-coherent responses are those with av-
erage scores 2, 2.5, and 3.  
For coherence modeling, we again use the J48 
decision tree from the Weka machine learning 
toolkit (Hall et al, 2009) and run 4-fold cross-
validation on the 600 annotated responses. The 
correlation coefficient (r) and the weighted aver-
age F-Measure2
In this experiment, we examine the performance 
of the entity-grid features and a set of features pro-
duced by the e-rater? system (an automated writ-
ing assessment system for learner essays) (Attali 
and Burstein, 2006) to predict the coherence scores 
of the spontaneous spoken responses, where all the 
features are extracted from human transcriptions of 
the responses.  
 are used as evaluation metrics.  
2.4 Entity Grid and e-rater Features 
First, we applied the algorithm from Barzilay and 
Lapata (2008) to extract entity-grid features, which 
calculated the vector of entity transition probabili-
ties across adjacent sentences.  Several different 
methods of representing the entities can be used 
before generating the entity-grid. First, all the enti-
ties can be described by their syntactic roles in-
cluding S (Subject), O (Object), and X (Other). 
Alternatively, these roles can also be reduced to P 
(Present) or N (Absent). Furthermore, entities can 
be defined as salient, when they appear two or 
more times, otherwise as non-salient. In this study, 
                                                          
2 The data distribution in the experimental corpus is unbal-
anced:  71% of the responses are high-coherent and 29% are 
low-coherent. Therefore, we adopt the weighted average F-
Measure to evaluate the performance of coherence prediction: 
first, the F1-Measure of each category is calculated, and then 
the percentages of responses in each category are used as 
weights to obtain the final weighted average F-Measure. 
we generated there basic entity grids: EG_SOX 
(entity grid with the syntactic roles S, O, and X), 
EG_REDUCED (entity grid with the reduced rep-
resentations P and N), and EG_SALIENT (entity 
grid with salient and non-salient entities). In addi-
tion to these entity-grid features, we also used 130 
writing quality features related to grammar, usage, 
mechanics, and style from e-rater to model the co-
herence. 
A baseline system for this task would simply as-
sign the majority class (high-coherent) to all of the 
responses; this baseline achieves an F-Measure of 
0.587. Table 3 shows that the EG_REDUCED and 
e-rater features can obtain F-Measures of 0.677 
and 0.726 as well as correlations with human 
scores of 0.20 and 0.33, respectively. However, the 
combination of the two sets of features only brings 
a very small improvement (from 0.33 to 0.34). In 
addition, our experiments show that by introducing 
the component of co-reference resolution for entity 
grid building, we can only get a very slight im-
provement on EG_SALIENT, but no improvement 
on EG_SOX and EG_REDUCED. That may be 
because it is generally more difficult to parse the 
transcriptions of spoken language than well-
formed text, and more errors are introduced during 
the process of co-reference resolution. 
 
 r F-Measure 
Baseline 0.0 0.587 
EG_SOX 0.16 0.664 
EG_REDUCED 0.2 0.677 
EG_SALIENT 0.2 0.678 
e-rater 0.33 0.726 
EG_SOX +e-rater 0.30 0.714 
EG_REDUCED +e-rater 0.34 0.73 
EG_SALIENT + e-rater 0.26 0.695 
Table 3. Performance of entity grid and e-rater features 
on the coherence modeling task  
2.5 Discussion and Future Work  
In order to further analyze these features, the  cor-
relation coefficients between various features and 
the average coherence scores (on a five-point 
scale) were calculated; Figure 1 shows the histo-
gram of these correlation values. As the figure 
shows, there are a total of approximately 50 fea-
tures with correlations larger than 0.1. Four of the 
entity-grid features have correlations between 0.15 
and 0.29. As for the writing quality features, some 
817
of them show high correlations with the average 
coherence scores, despite the fact that they are not 
explicitly related to discourse coherence, such as 
the number of good lexical collocations.  
Based on the above analysis, we plan to investi-
gate additional superficial features explicitly relat-
ed to discourse coherence, such as the distribution 
of conjunctions, pronouns, and discourse connec-
tives. Moreover, based on the research on well-
formed texts and learner essays, we will attempt to 
examine more effective features and models to bet-
ter cover the discourse aspects of spontaneous 
speech. For example, local semantic features relat-
ed to inter-sentential coherence and the ISA feature 
will be investigated on spoken responses. In addi-
tion, we will apply the features and build coher-
ence models using the output of automatic speech 
recognition in addition to human transcriptions. 
Finally, various coherence features or models will 
be integrated into a practical automated scoring 
system, and further experiments will be performed 
to measure their effect on the performance of au-
tomated assessment of spontaneous spoken re-
sponses.  
 
 
Figure1. Histogram of entity-grid and writing quality 
features based on their correlations with coherence 
scores 
 
3 Conclusion  
In this paper, we present a corpus of coherence 
annotations for spontaneous spoken responses pro-
vided in the context of an English speaking profi-
ciency assessment. Entity-grid features and fea-
tures from an automated essay scoring system were 
examined for coherence modeling of spoken re-
sponses. The analysis on the annotated corpus 
showed promising results for improving the per-
formance of an automated scoring system by 
means of modeling the coherence of spoken re-
sponses.  
Acknowledgments 
The authors wish to express our thanks to the dis-
course annotators Melissa Lopez and Matt Mulhol-
land for their dedicated work and our colleagues 
Jill Burstein and Slava Andreyev for their support 
in generating entity-grid features. 
References   
Yigal Attali and Jill Burstein. 2006. Automated essay 
scoring with e-rater? V.2.0. Journal of Technology, 
Learning, and Assessment. 4(3): 159-174. 
Regina Barzilay and Lillian Lee. 2004. Catching the 
drift: Probabilistic content models, with applications 
to generation and summarization. Proceedings of 
NAACL-HLT, 113-120. 
Regina Barzilay and Mirella Lapata. 2005. Modeling 
local coherence: An entity-based approach. 
Proceedings of ACL, 141-148. 
Regina Barzilay and Mirella Lapata. 2008. Modeling 
local coherence: An entity-based approach. 
Computational Linguistics, 34(1):1-34. 
Jill Burstein, Joel Tetreault and Slava Andreyev. 2010. 
Using entity-based features to model coherence in 
student essays. Proceedings of NAACL-HLT, 681-
684. Los Angeles, California. 
Lei Chen, Klaus Zechner and Xiaoming Xi. 2009. 
Improved pronunciation features for construct-
driven assessment of non-native spontaneous 
speech. Proceedings of NAACL-HLT, 442-449. 
Miao Chen and Klaus Zechner. 2011. Computing and 
evaluating syntactic complexity features for 
automated scoring of spontaneous non-native 
speech. Proceedings of ACL, 722-731. 
Jian Cheng. 2011. Automatic assessment of prosody in 
high-stakes English tests. Proceedings of 
Interspeech , 27-31. 
Peter W. Foltz, Walter Kintsch and Thomas K. 
Landauer. 1998. The measurement of textual 
coherence with Latent Semantic Analysis. Discourse 
Processes, 25(2&3):285-307. 
Arthur C. Graesser,  Danielle S. McNamara,  Max M. 
Louwerse and Zhiqiang Cai. 2004. Coh-Metrix: 
Analysis of text on cohesion and language. Behavior 
818
Research Methods, Instruments, & Computers, 
36(2):193-202. 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann and Ian H. Witten. 
2009. The WEKA data mining software: An update. 
SIGKDD Explorations, 11(1):10-18. 
Khairun-nisa Hassanali, Yang Liu and Thamar Solorio. 
2012. Coherence in child language narratives: A 
case study of annotation and automatic prediction of 
coherence. Proceedings of the Interspeech 
Workshop on Child, Computer and Interaction.  
Derrick Higgins, Jill Burstein, Daniel Marcu and 
Claudia Gentile. 2004. Evaluating multiple aspects 
of coherence in student essays. Proceedings of 
NAACL-HLT, 185-192. 
Derrick Higgins, Xiaoming Xi, Klaus Zechner and 
David Williamson.  2011. A three-stage approach to 
the automated scoring of spontaneous. Computer 
Speech and Language, 25:282-306. 
J. Richard Landis and Gary G. Koch. 1977. The 
measurement of observer agreement for categorical 
data. Biometrics, 33(1):159-174. 
Emily Pitler, Annie Louis and Ani Nenkova. 2010. 
Automatic evaluation of linguistic quality in multi-
document summarization. Proceedings of ACL. 
544?554. Uppsala. 
Helmer Strik and Catia Cucchiarini. 1999. Automatic 
assessment of second language learners' fluency. 
Proceedings of the 14th International Congress of 
Phonetic Sciences, 759-762. Berkeley, CA. 
Shasha Xie, Keelan Evanini and Klaus Zechner. 2012. 
Exploring content features for automated speech 
scoring. Proceedings of NAACL-HLT, 103-111. 
Helen Yannakoudakis and Ted Briscoe. 2012. Modeling 
coherence in ESOL learner texts. Proceedings of the 
7th Workshop on the Innovative Use of NLP for 
Building Educational Applications, 33-43. Montreal. 
Klaus Zechner, Derrick Higgins and Xiaoming Xi. 
2007.   SpeechRaterSM
Klaus Zechner, Derrick Higgins, Xiaoming Xi and 
David M. Williamson. 2009. Automatic scoring of 
non-native spontaneous speech in tests of spoken 
English. Speech Communication, 51(10):883-895. 
: A construct-driven approach 
to scoring spontaneous non-native speech. 
Proceedings of the International Speech 
Communication Association Special Interest Group 
on Speech and Language Technology in Education, 
128-131. 
 
819
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 53?56,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Amazon Mechanical Turk for Transcription of Non-Native Speech
Keelan Evanini, Derrick Higgins, and Klaus Zechner
Educational Testing Service
{KEvanini, DHiggins, KZechner}@ets.org
Abstract
This study investigates the use of Amazon
Mechanical Turk for the transcription of non-
native speech. Multiple transcriptions were
obtained from several distinct MTurk workers
and were combined to produce merged tran-
scriptions that had higher levels of agreement
with a gold standard transcription than the in-
dividual transcriptions. Three different meth-
ods for merging transcriptions were compared
across two types of responses (spontaneous
and read-aloud). The results show that the
merged MTurk transcriptions are as accurate
as an individual expert transcriber for the read-
aloud responses, and are only slightly less ac-
curate for the spontaneous responses.
1 Introduction
Orthographic transcription of large amounts of
speech is necessary for improving speech recogni-
tion results. Transcription, however, is a time con-
suming and costly procedure. Typical transcription
speeds for spontaneous, conversational speech are
around 7 to 10 times real-time (Glenn and Strassel,
2008). The transcription of non-native speech is an
even more difficult task?one study reports an aver-
age transcription time of 12 times real-time for spon-
taneous non-native speech (Zechner, 2009).
In addition to being more costly and time consum-
ing, transcription of non-native speech results in a
higher level of disagreement among transcribers in
comparison to native speech. This is especially true
when the speaker?s proficiency is low and the speech
contains large numbers of grammatical errors, in-
correct collocations, and disfluencies. For exam-
ple, one study involving highly predictable speech
shows a decline in transcriber agreement (measured
using Word Error Rate, WER) from 3.6% for na-
tive speech to 6.4% for non-native speech (Marge et
al., to appear). Another study involving spontaneous
non-native speech showed a range of WER between
15% and 20% (Zechner, 2009).
This study uses the Amazon Mechanical Turk
(MTurk) resource to obtain multiple transcriptions
for non-native speech. We then investigate several
methods for combining these multiple sources of in-
formation from individual MTurk workers (turkers)
in an attempt to obtain a final merged transcription
that is more accurate than the individual transcrip-
tions. This methodology results in transcriptions
that approach the level of expert transcribers on this
difficult task. Furthermore, a substantial savings in
cost can be achieved.
2 Previous Work
Due to its ability to provide multiple sources of
information for a given task in a cost-effective
way, several recent studies have combined multi-
ple MTurk outputs for NLP annotation tasks. For
example, one study involving annotation of emo-
tions in text used average scores from up to 10 turk-
ers to show the minimum number of MTurk anno-
tations required to achieve performance compara-
ble to experts (Snow et al, 2008). Another study
used preference voting to combine up to 5 MTurk
rankings of machine translation quality and showed
that the resulting judgments approached expert inter-
annotator agreement (Callison-Burch, 2009). These
53
tasks, however, are much simpler than transcription.
MTurk has been used extensively as a transcrip-
tion provider, as is apparent from the success of a
middleman site that act as an interface to MTurk
for transcription tasks.1 However, to our knowledge,
only one previous study has systematically evaluated
the quality of MTurk transcriptions (Marge et al,
to appear). This recent study also combined multi-
ple MTurk transcriptions using the ROVER method
(Fiscus, 1997) to produce merged transcriptions that
approached the accuracy of expert transcribers. Our
study is similar to that study, except that the speech
data used in our study is much more difficult to
transcribe?the utterances used in that study were rel-
atively predictable (providing route instructions for
robots), and contained speech from native speak-
ers and high-proficiency non-native speakers. Fur-
thermore, we investigate two additional merging al-
gorithms in an attempt to improve over the perfor-
mance of ROVER.
3 Experimental Design
3.1 Audio
The audio files used in this experiment consist of
responses to an assessment of English proficiency
for non-native speakers. Two different types of re-
sponses are examined: spontaneous and read-aloud.
In the spontaneous task, the speakers were asked to
respond with their opinion about a topic described
in the prompt. The speech in these responses is thus
highly unpredictable. In the read-aloud task, on the
other hand, the speakers were asked to read a para-
graph out loud. For these responses, the speech is
highly predictable; any deviations from the target
script are due to reading errors or disfluencies.
For this experiment, one set of 10 spontaneous
(SP) responses (30 seconds in duration) and two sets
of 10 read-aloud (RA) responses (60 seconds in du-
ration) were used. Table 1 displays the characteris-
tics of the responses in the three batches.
3.2 Transcription Procedure
The tasks were submitted to the MTurk interface in
batches of 10, and a turker was required to complete
the entire batch in order to receive payment. Turkers
1http://castingwords.com/
Batch Duration # of Words
(Mean)
# of Words
(Std. Dev.)
SP 30 sec. 33 14
RA1 60 sec. 97 4
RA2 60 sec. 93 10
Table 1: Characteristics of the responses used in the study
received $3 for a complete batch of transcriptions
($0.30 per transcription).
Different interfaces were used for transcribing the
two types of responses. For the spontaneous re-
sponses, the task was a standard transcription task:
the turkers were instructed to enter the words that
they heard in the audio file into a text box. For the
read-aloud responses, on the other hand, they were
provided with the target text of the prompt, one word
per line. They were instructed to make annotations
next to words in cases where the speaker deviated
from the target text (indicating substitutions, dele-
tions, and insertions). For both types of transcription
task, the turkers were required to successfully com-
plete a short training task before proceeding onto the
batch of 10 responses.
4 Methods for Merging Transcriptions
4.1 ROVER
The ROVER method was originally developed for
combining the results from multiple ASR systems to
produce a more accurate hypothesis (Fiscus, 1997).
This method iteratively aligns pairs of transcriptions
to produce a word transition network. A voting pro-
cedure is then used to produce the merged transcrip-
tion by selecting the most frequent word (including
NULL) in each correspondence set; ties are broken
by a random choice.
4.2 Longest Common Subsequence
In this method, the Longest Common Subsequence
(LCS) among the set of transcriptions is found by
first finding the LCS between two transcriptions,
comparing this output with the next transcription to
find their LCS, and iterating over all transcriptions in
this manner. Then, each transcription is compared to
the LCS, and any portions of the transcription that
are missing between words of the LCS are tallied.
Finally, words are interpolated into the LCS by se-
54
lecting the most frequent missing sequence from the
set of transcriptions (including the empty sequence);
as with the ROVER method, ties are broken by a ran-
dom choice among the most frequent candidates.
4.3 Lattice
In this method, a word lattice is formed from the
individual transcriptions by iteratively adding tran-
scriptions into the lattice to optimize the match be-
tween the transcription and the lattice. New nodes
are only added to the graph when necessary. Then,
to produce the merged transcription, the optimal
path through the lattice is determined. Three dif-
ferent configurations for computing the optimal path
through the lattice method were compared. In the
first configuration, ?Lattice (TW),? the weight of
a path through the lattice is determined simply by
adding up the total of the weights of each edge
in the path. Note that this method tends to fa-
vor longer paths over shorter ones, assuming equal
edge weights. In the next configuration, ?Lattice
(AEW),? a cost for each node based on the aver-
age edge weight is subtracted as each edge of the
lattice is traversed, in order to ameliorate the prefer-
ence for longer paths. Finally, in the third configura-
tion, ?Lattice (TWPN),? the weight of a path through
the lattice is defined as the total path weight in the
?Lattice (TW)? method, normalized by the number
of nodes in the path (again, to offset the preference
for longer paths).
4.4 WER calculation
All three of the methods for merging transcriptions
are sensitive to the order in which the individual
transcriptions are considered. Thus, in order to accu-
rately evaluate the methods, for each number of tran-
scriptions used to create the merged transcription,
N ? {3, 4, 5}, all possible permutations of all pos-
sible combinations were considered. This resulted
in a total of 5!(5?N)! merged transcriptions to be eval-
uated. For each N, the overall WER was computed
from this set of merged transcriptions.
5 Results
Tables 2 - 4 present the WER results for differ-
ent merging algorithms for the two batches of read-
aloud responses and the batch of spontaneous re-
sponses. In each table, the merging methods are or-
Method N=3 N=4 N=5
Individual Turkers 7.0%
Lattice (TWPN) 6.4% 6.4% 6.4%
Lattice (TW) 6.4% 6.4% 6.4%
LCS 6.0% 5.6% 5.6%
Lattice (AEW) 6.1% 6.0% 5.5%
ROVER 5.5% 5.2% 5.1%
Expert 4.7%
Table 2: WER results 10 read-aloud responses (RA1)
Method N=3 N=4 N=5
Individual Turkers 9.7%
Lattice (TW) 9.5% 9.5% 9.4%
Lattice (TWPN) 8.3% 8.0% 8.0%
Lattice (AEW) 8.2% 7.4% 7.8%
ROVER 7.9% 7.9% 7.6%
LCS 8.3% 8.0% 7.5%
Expert 8.1%
Table 3: WER results for 10 read-aloud responses (RA2)
dered according to their performance when all tran-
scriptions were used (N=5). In addition, the overall
WER results for the individual turkers and an expert
transcriber are provided for each set of responses.
In each case, the WER is computed by comparison
with a gold standard transcription that was created
by having an expert transcriber edit the transcription
of a different expert transcriber.
In all cases, the merged transcriptions have a
lower WER than the overall WER for the individual
turkers. Furthermore, for all methods, the merged
output using all 5 transcriptions has a lower (or
equal) WER to the output using 3 transcriptions. For
the first batch of read-aloud responses, the ROVER
method performed best, and reduced the WER in
the set of individual transcriptions by 27.1% (rela-
tive) to 5.1%. For the second batch of read-aloud
responses, the LCS method performed best, and re-
duced the WER by 22.6% to 7.5%. Finally, for the
batch of spontaneous responses, the Lattice (TW)
method performed best, and reduced the WER by
25.6% to 22.1%.
55
Method N=3 N=4 N=5
Individual Turkers 29.7%
Lattice (TWPN) 29.1% 28.9% 28.3%
LCS 29.2% 28.4% 27.0%
Lattice (AEW) 28.1% 25.8% 25.1%
ROVER 25.4% 24.5% 24.9%
Lattice (TW) 25.5% 23.5% 22.1%
Expert 18.3%
Table 4: WER results for 10 spontaneous responses
6 Conclusions
As is clear from the levels of disagreement be-
tween the expert transcriber and the gold standard
transcription for all three tasks, these responses are
much more difficult to transcribe accurately than
native spontaneous speech. For native speech, ex-
pert transcribers can usually reach agreement lev-
els over 95% (Deshmukh et al, 1996). For these
responses, however, the WER for the expert tran-
scriber was worse than this even for the read-aloud
speech. These low levels of agreement can be at-
tributed to the fact that the speech is drawn from a
wide range of English proficiency levels among test-
takers. Most of the responses contain disfluencies,
grammatical errors, and mispronunciations, leading
to increased transcriber uncertainty.
The results of merging multiple MTurk transcrip-
tions of this non-native speech showed an improve-
ment over the performance of the individual tran-
scribers for all methods considered. For the read-
aloud speech, the agreement level of the merged
transcriptions approached that of the expert tran-
scription when only three MTurk transcriptions were
used. For the spontaneous responses, the perfor-
mance of the best methods still lagged behind the ex-
pert transcription, even when five MTurk transcrip-
tions were used. Due to the consistent increase in
performance, and the low cost of adding additional
transcribers (in this study the cost was $0.30 per au-
dio minute for read-aloud speech and $0.60 per au-
dio minute for spontaneous speech), the approach of
combining multiple transcriptions should always be
considered when MTurk is used for transcription. It
is also possible that lower payments per task could
be provided without a decrease in transcription qual-
ity, as demonstrated by Marge et al (to appear). Ad-
ditional experiments will address the practicality of
producing more accurate merged transcriptions for
an ASR system?simply collecting larger amounts
of non-expert transcriptions may be a better invest-
ment than producing higher quality data (Novotney
and Callison-Burch, 2010).
It is interesting that the Lattice (TW) method
of merging transcriptions clearly outperformed all
other methods for the spontaneous responses, but
was less beneficial than the LCS and ROVER meth-
ods for read-aloud speech. It is likely that this is
caused by the preference of the Lattice (TW) method
for longer paths through the word lattice, since indi-
vidual transcribers of spontaneous speech may mark
different words as unitelligible, even though these
words exist in the gold standard transcription. Fur-
ther studies with a larger number of responses will
be needed to test this hypothesis.
References
Chris Callison-Burch. 2009. Fast, cheap and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. EMNLP.
Neeraj Deshmukh, Richard Jennings Duncan, Aravind
Ganapathiraju, and Joseph Picone. 1996. Benchmark-
ing human performance for continuous speech recog-
nition. In Proc. ICSLP.
Jonathan G. Fiscus. 1997. A post-processing system to
yield word error rates: Recognizer Ooutput Voting Er-
ror Reduction (ROVER). In Proc. ASRU.
Meghan Lammie Glenn and Stephanie Strassel. 2008.
Shared linguistic resources for the meeting domain.
In Lecture Notes in Computer Science, volume 4625,
pages 401?413. Springer.
Matthew Marge, Satanjeev Banerjee, and Alexander I.
Rudnicky. to appear. Using the Amazon Mechanical
Turk for transcription of spoken language. In Proc.
ICASSP.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
fast, and good enough: Automatic speech recognition
with non-expert transcription. In Proc. NAACL.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? But is it
good? Evaluating non-expert annotations for natural
language tasks. In Proc. EMNLP.
Klaus Zechner. 2009. What did they actually say?
Agreement and disagreement among transcribers of
non-native spontaneous speech responses in an En-
glish proficiency test. In Proc. ISCA-SLaTE.
56
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 152?160,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Non-scorable Response Detection for Automated Speaking Proficiency
Assessment
Su-Youn Yoon, Keelan Evanini, Klaus Zechner
Educational Testing Service
660 Rosedale Road, Princeton, NJ, USA
{syoon,kevanini,kzechner}@ets.org
Abstract
We present a method that filters out non-
scorable (NS) responses, such as responses
with a technical difficulty, in an automated
speaking proficiency assessment system. The
assessment system described in this study first
filters out the non-scorable responses and then
predicts a proficiency score using a scoring
model for the remaining responses.
The data were collected from non-native
speakers in two different countries, using two
different item types in the proficiency assess-
ment: items that elicit spontaneous speech and
items that elicit recited speech. Since the pro-
portion of NS responses and the features avail-
able to the model differ according to the item
type, an item type specific model was trained
for each item type. The accuracy of the mod-
els ranged between 75% and 79% in spon-
taneous speech items and between 95% and
97% in recited speech items.
Two different groups of features, signal pro-
cessing based features and automatic speech
recognition (ASR) based features, were im-
plemented. The ASR based models achieved
higher accuracy than the non-ASR based mod-
els.
1 Introduction
We developed a method that filters out non-scorable
(NS) responses as a supplementary module to an
automated speech proficiency assessment system.
In this study, the method was developed for a
telephony-based assessment of English proficiency
for non-native speakers. The examinees? responses
were collected from several different environmen-
tal conditions, and many of the utterances contain
background noise from diverse sources. In ad-
dition to the presence of noise, many responses
have other sub-optimal characteristics. For exam-
ple, some responses contain uncooperative behav-
ior from the speakers, such as non-English speech,
whispered speech, and non-responses. These types
of responses make it difficult to provide a valid as-
sessment of a speaker?s English proficiency. There-
fore, in order to address the diverse types of causes
for these problematic responses, we used a two step
approach: first, these problematic responses were
filtered out by a ?filtering model,? and only the re-
maining responses were scored using the automated
scoring model.
The overall architecture of our method, includ-
ing the automated speech proficiency scoring sys-
tem, is as follows: for a given spoken response,
the system performs speech recognition, yielding a
word hypothesis and time stamps. In addition to
word recognition, the system computes pitch and
power to generate prosodic features; the system cal-
culates descriptive statistics such as the mean and
standard deviation of pitch and power at both the
word level and response level. Given the word hy-
potheses and pitch/power features, it derives features
for automated proficiency scoring. Next, the non-
ASR based features are calculated separately using
signal processing techniques. Finally, given both
sets of features, the filtering model identifies NS re-
sponses.
This paper will proceed as follows: we will re-
view previous studies (Section 2), present the data
152
(Section 3), and then describe the structure of the
filtering model (Section 4). Next, the results will
be presented (Section 5), followed by a discussion
(Section 6), and we will conclude with a summary
of the importance of the findings (Section 7).
2 Previous Work
Higgins et al (2011) developed a ?filtering model?
that is conceptually similar to the one in this pa-
per. The model was trained and tested on a corpus
containing responses from non-native speakers to an
English proficiency assessment. This system used
a regression model based on four features which
were originally designed for automated speech pro-
ficiency scoring: the number of distinct words in the
speech recognition output, the average speech rec-
ognizer confidence score, the average power of the
speech signal, and the mean absolute deviation of
the speech signal power. This model was able to
identify responses which were also identified as NS
responses by human raters with an approximately
98% accuracy when a false positive rate (the propor-
tion of responses without technical difficulties that
were incorrectly flagged as problematic) was lower
than 1%.
Although there are few other studies which are di-
rectly related to the task of filtering out non-scorable
responses in the domain of automated speech profi-
ciency assessment, several signal processing studies
are related to this work. Traditionally, the Signal to
Noise Ratio (SNR) has been used to detect speech
with a large amount of background noise. This
method measures the ratio between the total energy
of the speech signal and the total energy of the noise;
if the SNR is low, then the speech contains loud
background noise. A low SNR results in lower in-
telligibility and increases the difficulty for both hu-
man and automated scoring. Furthermore, spectral
characteristics can be also applied to detect speech
with loud background noise, since noise has differ-
ent spectral characteristics than speech (noise tends
to have no or few peaks in the spectral domain).
If a response contains loud background noise, then
the spectral characteristics of the speech may be ob-
scured by noise and it may have similar character-
istics with the noise. These differences in spectral
characteristics have been used in audio information
retrieval Lu and Hankinson (1998).
Secondly, responses without valid speech can be
identified using Voice Activity Detection (VAD).
VAD is a technique which distinguishes human
speech from non-speech. When speech is clean,
VAD can be calculated by simply computing the
zero-crossing rate which signals the existence of
cyclic waves such as vowels. However, if the re-
sponse also contains loud background noises, more
sophisticated methods are required. In order to re-
move the influence of noise, Chang and Kim (2003),
Chang et al (2006), Shin et al (2005) and Sohn et
al. (1999) estimated the characteristics of the noise
spectrum and the distribution of noise, and compen-
sated for them when speech is identified. The perfor-
mance of these systems is heavily-influenced by the
accuracy of estimating characteristics of the back-
ground noise.
In this study, we used a set of ASR based fea-
tures and non-ASR based features. ASR based fea-
tures were similar to the ones used by Zechner et al
(2009). In addition to the features based on ASR
hypotheses, the ASR based feature set contained ba-
sic pitch and power related features since the ASR
system in this study also produced pitch and power
measurements in order to generate prosodic features.
The non-ASR based features were comprised of four
groups of features based on signal processing tech-
niques such as SNR, VAD, and pitch and power.
Features related to pitch and power were included in
both the ASR based features and the non-ASR based
features. Since the non-ASR based features were
originally implemented as an independent module
from the ASR-based system (it was implemented for
the case where the appropriate recognizer is unavail-
able), there is some degree of overlap between the
two feature sets.
3 Data
The data for this experiment were drawn from a pro-
totype of a telephony-based English language as-
sessment. Non-native speakers of English each re-
sponded to 40 test items designed to evaluate their
level of English proficiency. The test was composed
of items that elicited both spontaneous speech (here-
after SS) and recited speech (hereafter RS). In this
study, 8 items (four SS and four RS) were used for
153
each speaker.
Participants used either a cell phone or a land
line to complete the assessment, and the participants
were compensated for their time. The motivation
level of the participants was thus lower than in the
case of an actual high stakes assessment, where a
participant?s performance could have a substantial
impact on their future. In addition, the data collec-
tion procedure was less controlled than in an op-
erational testing environment; for example, some
recordings exhibited higher levels of ambient noise
than others. These two facts led to the quality of
some of the responses being lower than would be
expected in an operational assessment.
The data for this study were collected from partic-
ipants in two countries: India and China. For India,
4900 responses from 638 speakers were collected.
For China, 5565 responses from 702 speakers were
collected (some of the participants did not provide
responses to all 8 test items). Each response is ap-
proximately 45 sec in duration.
After the data was collected, all of the responses
were given scores on a three-point scale by trained
raters. The raters also labeled responses as ?non-
scorable? (NS), when appropriate. NS responses are
ones that could not be given a score according to the
rubrics of the three-point scale. These were due to
either a technical difficulty obscuring the content of
the response or an inappropriate response from the
participant.
The proportion of NS responses differs markedly
between the two countries. 852 of the responses in
the India data set (17% of the total) were labeled as
NS, compared to 1548 responses (28%) in the China
data set.
Table 1 provides the different types of NS re-
sponses that were annotated by the raters, along with
the relative frequency of each NS category com-
pared to the others.
Excluding the category ?Other?, background
noise, non-responses, and unrelated topic were the
most frequent types of NS response for both data
sets. However, the relative proportions of each type
differed somewhat between the two countries. For
example, the most frequent NS type in India was
background noise; 33% of NS responses were of this
type, 1.7 times higher than in China.
The proportion of unrelated topic responses was
NS Type India (%) China (%)
Background noise 33.2 19.6
Other 25.0 15.4
Unrelated response 18.9 40.1
Non-response 10.6 8.8
Non-English speech 4.9 6.4
Too soft 2.8 1.0
Background speech 2.0 1.9
Missing samples 1.5 4.0
Too loud 0.8 0.1
Cheating 0.3 2.7
Table 1: Different types of NS responses and their relative
frequency, in % of all NS for each country (ranked by
frequency of occurrence in India)
Data Partition
India China
# of re-
sponses
NS
(%)
# of re-
sponses
NS
(%)
SS-train 1114 31.6 1382 32.2
SS-eval 1271 27.5 1391 33.8
RS-train 1253 8.0 1392 22.4
RS-eval 1275 4.8 1400 22.9
Table 2: Item-type specific training and evaluation data
also high in both countries, but it was much higher
in the China data set: it was 19% in the responses
from India and 40% for China (more than twice as
high as in India). All responses which were not di-
rectly related to the prompt fell into this category.
For SS items, the majority included responses about
a different topic. For RS items, responses in which
the speakers read different prompts were classified
into this category.
The responses were divided into training and test-
ing for NS response detection. Due to the significant
difference in the proportion of NS responses and rel-
ative frequencies of NS types in the two data sets, fil-
tering models were trained separately for each coun-
try. In addition, since the proportions of NS re-
sponses and the available features varied according
to the item type, training and testing data were fur-
ther classified by item types. The proportions of NS
responses and the sizes of the partitions, along with
the percent of NS responses in each item type, are
shown in Table 2.
154
The partitions for testing the filtering model were
selected to maximize the number of speakers with
complete sets of responses; however, this constraint
was not able to be met for the training partitions in
the India data set (due to insufficient data). This ex-
plains the lower proportion of NS responses in the
India test partitions, since speakers with complete
sets of responses were less likely to provide bad re-
sponses. As Table 2 shows, NS responses were more
frequent among SS items than RS items: the pro-
portion of NS responses in SS items was four times
higher than in RS items in India and 1.5 times in
China.
4 Method
4.1 Overview
In this study, two different sets of features were used
in the model training process; ASR-based features
and non-ASR based features. For each item-type,
an item-type-specific filtering model was developed
using these two sets of features.
4.2 Feature generation
4.2.1 ASR based features
For this feature set, we used the features from
an automated speech proficiency scoring system.
This scoring system used an ASR engine containing
word-internal triphone acoustic models and item-
type-specific language models. Separate acoustic
models were trained for the data sets from the two
countries. The acoustic training data for the two
models consisted of 45.5 hours of speech from In-
dia and 123.1 hours of speech from China. In addi-
tion, separate language models were trained for the
SS and RS items for each country; for the RS items,
the language models also incorporated the texts of
the prompts.
A total of 61 features were available. Among
these features, many features were conceptually
similar but based on different normalization meth-
ods. These features showed a strong intercorrela-
tion. For this study, 30 features were selected and
classified into four groups according to their char-
acteristics: basic features, fluency features, ASR-
confidence features, and Word Error Rate (WER)
features.
The basic features are related to power and pitch,
and they capture the overall distribution of pitch and
power values in a speaker?s response using mean and
variance calculations. These features are relevant
since NS responses may have an abnormal distribu-
tion in energy. For instance, non-responses contain
very low energy. In order to detect these abnormal-
ities in speech signal, pitch and power related fea-
tures were calculated.
The fluency features measure the length of a re-
sponse in terms of duration and number of words.
In addition, this group contains features related to
speaking rate and silences, such as mean duration
and number of silence. In particular, these features
are effective in identifying Non-responses which
contain zero or only a few words.
The ASR-confidence group contains features pre-
dicting the performance of the speech recognizer.
Low speech recognition accuracy may be indicated
by low confidence scores.
Finally, the WER group provides features esti-
mating the similarity between the prompts and the
recognition output. In addition to the conventional
word error rate (WER), term error rate (TER) was
also implemented for the filtering model. TER is
a metric commonly used in spoken information re-
trieval, and it only accounts for errors in content
words. This measure may be more effective in iden-
tifying NS responses than conventional WER; for in-
stance, the overlap in function words between off-
topic responses and prompts can be correctly ig-
nored. TER was calculated according to the follow-
ing formula:
dif(Wc) =
{
0 ifCref (Wc) < Chyp(Wc)
Cref (Wc)? Chyp(Wc) otherwise
TER =
?
c?WC
dif(Wc)
?
c?WC
Cref (Wc)
(1)
where Cref (Wc) is the number of occurrences of
the word Wc in reference, Chyp(Wc) is the number
of occurrences of the word Wc in hypothesis, and
WC is the set of content words in reference.
Formula 1 differs from the conventional method
155
Group List of features
Basic mean/standard deviation/minimum/maximum of power, difference between maxi-
mum and minimum in power, mean/standard deviation/minimum/maximum of pitch,
difference between maximum and minimum in pitch
Fluency duration of whole speech part, number of words, speaking rate (word per sec),
mean/standard deviation of silence duration, number of silences, silences per sec and
silences per word
ASR score mean of confidence score, normalized Acoustic Model score by word length, normal-
ized Language Model score by number of words
Word Error Rate the word accuracy between prompt and ASR word hypothesis, correct words per
minute, term error rate
Table 3: List of ASR based features
of calculating TER in two ways. Firstly, content
words which occurred only in the word hypothesis
are ignored in the formula. Secondly, if a word oc-
curred in the word hypothesis more frequently than
in the reference, the difference is ignored. These
modifications were made to address characteristics
of the responses in the data. On the one hand, speak-
ers occasionally inserted a few words such as ?too
difficult? at the end of a response. In addition, a few
speakers repeated words contained in the prompt
multiple times. The two modifications to TER ad-
dress both of these issues.
All features from the four groups are summarized
in Table 3.
4.2.2 Non-ASR based features
A total of 12 features from four different groups
were implemented using non-ASR based methods
such as VAD and SNR. These features are listed in
Table 4.
Feature Category Feature
VAD proportion of voiced
frames in response, num-
ber and total duration of
voiced regions
Syllable number of syllables
Amplitude maximum, minimum,
mean, standard deviation
SNR SNR, speech peak
Table 4: List of non-ASR based features
VAD related features were implemented using the
ESPS speech analysis program. For every 10 mil-
lisecond interval, the voice frame detector deter-
mined whether the interval was voiced or not. Three
features were implemented using this voiced interval
information: the number of voiced intervals, ratio of
voiced intervals in the entire response, and the total
duration of voiced intervals.
In addition, the number of syllables was estimated
based on the flow of energy. The energy of the syl-
lable tends to reach its peak in the nucleus and the
dip in the boundaries. By counting the number of
such fluctuations in energy measurements, the num-
ber of syllables can be estimated. The Praat script
from De Jong and Wempe (2009) was used for this
purpose.
In order to detect the abnormalities in energy, am-
plitude based features were calculated. These fea-
tures were similar to the basic features in ASR based
features.
Finally, if a response contains loud background
noise, the ratio of speech to noise is low. SNR, the
mean noise level, and the peak speech level were
computed using the NIST audio quality assurance
package (NIST, 2009).
The VAD and syllable feature groups were de-
signed to estimate the number of syllables, the pro-
portion of speech to non-speech, and the total dura-
tion of speech intervals. These features were similar
to the number of words and duration of speech fea-
tures in the ASR-based feature set. Despite the con-
ceptual similarity, these features were implemented
since the two types of features were calculated us-
ing different characteristics of the spoken response.
156
The VAD and syllable features are based on the flow
of energy and the zero crossing rate and the ASR-
based features are based on the speech recognition.
In particular, the speech recognizer tends to gener-
ate word hypotheses even for responses that contain
no speech input, but VAD does not have such a ten-
dency. Due to this difference, VAD based features
may be more robust in the responses with no valid
speech.
4.3 Model building
For each response, both ASR features and non-ASR
features were calculated. In contrast to non-ASR
features, which were available for all responses,
ASR features (except the Basic group) were un-
available for some responses, namely, responses for
which the ASR system did not generate any word
hypotheses because no tokens received scores above
the rejection threshold. This causes a missing value
problem; about 7% of the responses did not have a
complete set of attributes.
Missing values are a common problem in machine
learning. One of the popular approaches is to replace
a missing value with a unique value such as the at-
tribute?s mean. Ding and Simonoff (2008) proposed
a method that replaces a missing value with an arbi-
trary unique value. This method is preferable when
missing of a value depends on the target value and
this relationship holds in both training and test data.
In this study, the missing values were replaced
with unique values due to the relationship between
the missing values and the target label; if the speech
recognizer did not produce any word hypotheses, the
response was highly likely to be a NS response. 63%
of the responses where the speech recognizer failed
to generate word hypotheses were NS responses.
Since all ASR-based features were continuous val-
ues, we used two real values: 0.0 for fluency features
and ASR features and 100.0 for word error rate fea-
tures. The fluency features and ASR features tend to
be 0.0 while the word error rate features tend to be
100.0 when the responses are NS responses.
A total of 42 features were used in the model
building. The only exception was WER; since WER
features were only available for the model based
on recited speech, they were calculated only for RS
items. Decision tree models were trained using the
J48 algorithm (WEKA implementation of C4.5) of
WEKA machine learning toolkit (Hall et al, 2009).
5 Results
For each item-type, three models were built to in-
vestigate the impact of each feature group: a model
using non-ASR features, a model using ASR fea-
tures, and a model using both features (the ?Com-
bined? model). Tables 5 and 6 present the accuracy
of the SS models and Tables 7 and 8 present the ac-
curacy of the RS models. In all tables, the base-
line was calculated using majority voting, and rep-
resented a system in which no responses were clas-
sified as NS; since the majority class was scorable,
the baseline using the majority voting did not predict
any response as non-scorable response. Therefore,
precision, recall, F-score are all 0 in this case.
Model Acc. Pre. Rec. F-score
Baseline 72.5 0 0 0
Non-ASR 77.0 0.645 0.364 0.465
ASR 79.0 0.683 0.444 0.538
Combined 78.6 0.657 0.461 0.542
Table 5: Performance of the SS model in India
Model Acc. Pre. Rec. F-score
Baseline 66.2 0 0 0
Non-ASR 68.9 0.601 0.240 0.343
ASR 72.9 0.718 0.326 0.448
Combined 72.9 0.720 0.323 0.446
Table 6: Performance of the SS model in China
Model Acc. Pre. Rec. F-score
Baseline 94.8 0 0 0
Non-ASR 95.7 0.684 0.210 0.321
ASR 97.2 0.882 0.484 0.625
Combined 96.8 0.769 0.484 0.594
Table 7: Performance of the RS model in India
In both item-types, the models using ASR-based
features achieved the best performance. The SS
model achieved 79% accuracy in India and 73% ac-
curacy in China, representing improvements of ap-
proximately 7% over the baseline. In both data sets,
the RS model achieved high accuracies: 97% accu-
racy in India and 96% accuracy in China. In India,
157
Model Acc. Pre. Rec. F-score
Baseline 77.1 0 0 0
Non-ASR 78.3 0.555 0.268 0.361
ASR 95.6 0.942 0.860 0.899
Combined 95.1 0.912 0.872 0.892
Table 8: Performance of the RS model in China
this represents a 2.4% improvement over the base-
line. Although the absolute value of this error re-
duction is not very large, the relative error reduc-
tion is 46%. In China, the improvement was more
salient; there was 18% improvement over baseline,
corresponding to a relative error reduction of 78%.
Additional experiments were conducted to deter-
mine the robustness of the filtering models to evalu-
ation data from a country not included in the train-
ing data. The evaluation sets from both item types
(SS and RS) in both countries (India and China)
were processed using three different models: 1) a
model trained using the ASR-based features for the
responses from the same country (the ?Same? con-
dition, whose results are identical to the ?ASR? re-
sults in Tables 5 - 8), 2) a model trained using the
ASR-based features for the responses from the other
country (the ?Different? condition), and 3) a model
trained using the ASR-based features for the re-
sponses from both countries (the ?Both? condition).
Table 9 presents the accuracy results for these four
sets of experiments.
Model
India China
SS RS SS RS
Same 79.0 97.2 72.9 95.6
Different 80.1 95.4 73.5 93.8
Both 80.0 96.5 74.0 95.9
Table 9: Accuracy results using training and evaluation
data from different countries
These results show that the models are quite ro-
bust to evaluation data from a different country. In
all cases, there is at most a small decline in perfor-
mance when training data from the other country is
used (in the case of the SS responses, there is even a
slight increase in performance). Table 9 also shows
that the RS models performed worse in the Different
Country condition (compared to the Same Country
condition) than the SS models. This difference is
likely due to the difference in the number of NS re-
sponses among the RS data in the two countries (as
shown in Table 2). However, the decline is still rel-
atively small, suggesting that it would be reasonable
to extend the filtering models to responses from ad-
ditional countries that were not seen in the training
data.
6 Discussion
Approximately 40 features were available for the
model building, but not all features had a signifi-
cant impact on the detection of NS responses. For
each item-type, the importance of features were fur-
ther investigated using a logistic regression analysis.
The training data of India and China were combined,
and a stepwise logistic regression analysis was per-
formed using the SPSS statistical analysis program.
For each item-type, the top 3 features are pre-
sented in Table 10; the features are presented in the
same order selected in the models.
Model RS SS
ASR TER,
speaking
rate, s.d. of
pitch
mean of confi-
dence scores,
speaking rate,
s.d. of power
Non-ASR number of
syllables,
number
and du-
ration of
voiced
regions
number of sylla-
bles, s.d. and
mean of ampli-
tude
Combined TER,
speaking
rate, s.s.dd.
pitch
mean of confi-
dence scores,
speaking rate,
number of
voiced regions
Table 10: Top 3 features in stepwise logistic regression
model
For the RS items, TER was the best feature and it
was the top feature for both the ASR feature based
model and the combined model. The top 3 features
in the combined model were the same as the ASR
feature based model, and non-ASR features were not
158
selected. In non-ASR based features, the number of
syllables was the best feature, followed by the VAD
based features.
For the SS items, the top 2 features were the same
in both the ASR feature based model and the com-
bined model. The combined model selected one
non-ASR based feature, namely, a VAD based fea-
ture. As with the RS items, the number of syllables
was the best feature, followed by the energy related
feature.
These results show the importance of WER fea-
tures. Most of the current features are designed for
signal level abnormalities such as responses with
large background noise or non-responses. For in-
stance, fluency features and VAD features are effec-
tive for non-response detection, since they can deter-
mine whether the responses contain valid speech or
not. SNR and pitch/power related features are use-
ful for identifying responses with large background
noise. However, no features except the WER group
can identify content-level abnormalities such as un-
related topic and non-English responses. The high
proportion of these two types of responses (24%
in India and 46% in China) may be the major ex-
planation for the lower accuracy of the model for
SS responses than for RS responses. In the future,
content-related features should also be developed for
spontaneous speech.
The features selected the first time in the logis-
tic model differed according to item-types. The re-
sults support the item-type-specific model approach
adopted in this paper; item-type-specific models can
assign strong weights to the item-type-specific fea-
tures that are most important.
As shown in Tables 5 - 8, the combination of non-
ASR and ASR features could not achieve any fur-
ther improvement over the model consisting only of
ASR based features. However, in all cases, the non-
ASR based model did lead to some improvement
over the baseline. The magnitude of this improve-
ment was greater in SS items than RS items; in par-
ticular, it was greatest among the SS items in the
India data set. This difference may be due to the dif-
ferent distributions of the NS types among the data
sets. The non-ASR based features can cover only
limited types of NS responses such as non-responses
and responses with background noise, and the pro-
portion of these types is much higher among the SS
responses from India.
In addition, in RS items, the poor performance of
the combined model may be related to the high per-
formance of TER. The stepwise regression analysis
showed that the combined model did not select any
of non-ASR based features.
7 Conclusion
In this study, filtering models were implemented as a
supplementary module to an automated proficiency
scoring system. Due to the difference in the avail-
able features and proportion of NS responses, item-
type specific models were trained.
The item-types heavily influenced the overall
characteristics of the filtering models. First, the pro-
portion of NS responses was significantly different
according to item-type; it was much higher in spon-
taneous speech items than recited speech items. Sec-
ondly, the word error rate feature group was only
available for recited speech. Although the word er-
ror rate feature group contained three features, they
improved the performance of the filtering model sig-
nificantly.
ASR feature based models outperformed non-
ASR feature based models, but non-ASR based fea-
tures may be useful for new tests. Finally, experi-
ments demonstrated that the country-specific mod-
els using the ASR-based features are relatively ro-
bust to responses from a different country. This re-
sult suggests that this approach can generalize well
to speakers from different countries.
In this study, large numbers of features (42 for RS
items and 39 for SS items) were used in the model
training, but some features were conceptually simi-
lar and not all of them were significantly important;
the logistic regression analsysis using traning data
showed that there was no significant improvement
after selecting 5 features for RS items and 13 fea-
tures for SS items. Use of non-significant features
in the model training may result in the overfitting
problems. In future research, the features will be
classified into subgroups based on their conceptual
similarities; groups of features with high intercorre-
lations will be reduced to include only the best per-
forming feature in each group. Thus, based on care-
ful pre-selection procedures, only high performing
features will be selected, and the model will be re-
159
trained.
In addition, many different types of NS responses
were lumped into one big category (NS); this may
increase the confusion between scorable and non-
scorable responses and decrease the model?s perfor-
mance. Some of NS types have very different char-
acteristics compared to other NS types and this fact
caused critical differences in the feature values. For
instance, non-responses contained zero or close to
zero words, whereas non-English responses and off-
topic responses typically had a word count similar to
scorable responses. This difference may reduce the
effectiveness of this feature. In order to avoid this
type of problem, we will classify NS types into small
numbers of subgroups and build a seperate model for
each subgroup.
References
Joon-Hyuk Chang and Nam Soo Kim. 2003. Voice ac-
tivity detection based on complex Laplacian model.
Electronics Letters, 39(7):632?634.
Joon-Hyuk Chang, Nam Soo Kim, and Sanjit K. Mitra.
2006. Voice activity detection based on multiple sta-
tistical models. IEEE Transactions on Signal Process-
ing, 54(6):1965?1976.
Nivja H. De Jong and Ton Wempe. 2009. Praat script
to detect syllable nuclei and measure speech rate au-
tomatically. Behavior research methods, 41(2):385?
390.
Yufeng Ding and Jeffrey S. Simonoff. 2008. An investi-
gation of missing data methods for classification trees.
Statistics Working Papers Series.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
In SIGKDD Explorations, volume 11.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David Williamson. 2011. A three-stage approach
to the automated scoring of spontaneous spoken re-
sponses. Computer Speech and Language, 25:282?
306, April.
Guojun Lu and Templar Hankinson. 1998. A technique
towards automatic audio classification and retrieval.
In Proceedings of the 4th International Conference on
Signal Processing, volume 2, pages 1142?1145.
NIST. 2009. The NIST SPeech Quality Assurance
(SPQA) Package Version 2.3. from http://www.
nist.gov/speech/tools/index.htm.
Jong Won Shin, Hyuk Jin Kwon, Suk Ho Jin, and
Nam Soo Kim. 2005. Voice activity detection based
on generalized gamma distribution. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 781?784.
Jongseo Sohn, Nam Soo Kim, and Wonyong Sung.
1999. A statistical model-based voice activity detec-
tion. IEEE Signal Processing Letter, 6(1):1?3.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken En-
glish. Speech Communication, 51(10):883?895.
160
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 157?162,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Prompt-based Content Scoring for Automated Spoken Language Assessment
Keelan Evanini
Educational Testing Service
Princeton, NJ 08541, USA
kevanini@ets.org
Shasha Xie
Microsoft
Sunnyvale, CA 94089
shxie@microsoft.com
Klaus Zechner
Educational Testing Service
Princeton, NJ 08541, USA
kzechner@ets.org
Abstract
This paper investigates the use of prompt-
based content features for the automated as-
sessment of spontaneous speech in a spoken
language proficiency assessment. The results
show that single highest performing prompt-
based content feature measures the number
of unique lexical types that overlap with the
listening materials and are not contained in
either the reading materials or a sample re-
sponse, with a correlation of r = 0.450 with
holistic proficiency scores provided by hu-
mans. Furthermore, linear regression scor-
ing models that combine the proposed prompt-
based content features with additional spoken
language proficiency features are shown to
achieve competitive performance with scoring
models using content features based on pre-
scored responses.
1 Introduction
A spoken language proficiency assessment should
provide information about how well the non-native
speaker will be able to perform a wide range of tasks
in the target language. Therefore, in order to provide
a full evaluation of the non-native speaker?s speak-
ing proficiency, the assessment should include some
tasks eliciting unscripted, spontaneous speech. This
goal, however, is hard to achieve in the context of
a spoken language assessment which employs auto-
mated scoring, due to the difficulties in developing
accurate automatic speech recognition (ASR) tech-
nology for non-native speech and in extracting valid
and reliable features. Because of this, most spo-
ken language proficiency assessments which use au-
tomated scoring have focused on restricted speech,
and have included tasks such as reading a word / sen-
tence / paragraph out loud, answering single-word
factual questions, etc. (Chandel et al, 2007; Bern-
stein et al, 2010).
In order to address this need, some automated
spoken language assessment systems have also in-
cluded tasks which elicit spontaneous speech. How-
ever, these systems have focused primarily on a non-
native speaker?s pronunciation, prosody, and fluency
in their scoring models (Zechner et al, 2009), since
these types of features are relatively robust to ASR
errors. Some recent studies have investigated the
use of features related to a spoken response?s con-
tent, such as (Xie et al, 2012). However, the ap-
proach to content scoring taken in that study requires
a large amount of responses for each prompt to be
provided with human scores in order to train the
content models. This approach is not practical for a
large-scale, high-stakes assessment which regularly
introduces many new prompts into the assessment?
obtaining the required number of scored training re-
sponses for each prompt would be quite expensive
and could lead to potential security concerns for the
assessment. Therefore, it would be desirable to de-
velop an approach to content scoring which does not
require a large amount of actual responses to train
the models. In this paper, we propose such a method
which uses the stimulus materials for each prompt
contained in the assessment to evaluate the content
in a spoken response.
157
2 Related Work
There has been little prior work concerning auto-
mated content scoring for spontaneous spoken re-
sponses (a few recent studies include (Xie et al,
2012) and (Chen and Zechner, 2012)); however, sev-
eral approaches have been investigated for written
responses. A standard approach for extended writ-
ten responses (e.g., essays) is to compare the con-
tent in a given essay to the content in essays that
have been provided with scores by human raters us-
ing similarity methods such as Content Vector Anal-
ysis (Attali and Burstein, 2006) and Latent Semantic
Analysis (Foltz et al, 1999). This method thus re-
quires a relatively large set of pre-scored responses
for each test question in order to train the content
models. For shorter written responses (e.g., short an-
swer questions targeting factual content) approaches
have been developed that compare the similarity be-
tween the content in a given response and a model
correct answer, and thus do not necessarily require
the collection of pre-scored responses. These ap-
proaches range from fully unsupervised text-to-text
similarity measures (Mohler and Mihalcea, 2009) to
systems that incorporate hand-crafted patterns iden-
tifying specific key concepts (Sukkarieh et al, 2004;
Mitchell et al, 2002).
For extended written responses, it is less practical
to make comparisons with model responses, due to
the greater length and variability of the responses.
However, another approach that does not require
pre-scored responses is possible for test questions
that have prompts with substantial amounts of in-
formation that should be included in the answer. In
these cases, the similarity between the response and
the prompt materials can be calculated, with the hy-
pothesis that higher scoring responses will incorpo-
rate certain prompt materials more than lower scor-
ing responses. This approach was taken by (Gure-
vich and Deane, 2007) which demonstrated that
lower proficiency non-native essay writers tend to
use more content from the reading passage, which is
visually accessible and thus easier to comprehend,
than the listening passage. The current study inves-
tigates a similar approach for spoken responses.
3 Data
The data used in this study was drawn from TOEFL
iBT, an international assessment of academic En-
glish proficiency for non-native speakers. For this
study, we focus on a task from the assessment which
elicits a 60 second spoken response from the test
takers. In their response, the test takers are asked
to use information provided in reading and listen-
ing stimulus materials to answer a question concern-
ing specific details in the materials. The responses
are then scored by expert human raters on a 4-point
scale using a scoring rubric that takes into account
the following three aspects of spoken English pro-
ficiency: delivery (e.g., pronunciation, prosody, flu-
ency), language use (e.g., grammar, lexical choice),
and topic development (e.g., content, discourse co-
herence). For this study, we used a total of 1189
responses provided by 299 unique speakers to four
different prompts1 (794 responses from 199 speak-
ers were used for training and 395 responses from
100 speakers were used for evaluation).
4 Methodology
We investigated several variations of simple features
that compare the lexical content of a spoken re-
sponse to following three types of prompt materials:
1) listening passage: a recorded lecture or dialogue
containing information relevant to the test question
(the number of words contained in each of the four
listening passages used in this study were 213, 223,
234, and 318), 2) reading passage: an article or es-
say containing additional information relevant to the
test question (the number of words contained in the
two reading passages were 94 and 111), and 3) sam-
ple response: a sample response provided by the test
designers containing the main ideas expected in a
model answer (the number of words contained in the
four sample responses were 41, 74, 102, and 133).
The following types of features were investi-
gated for each of the materials: 1) stimulus cosine:
the cosine similarity between the spoken response
and the various materials, 2) tokens/response,
types/response: the number of word tokens / types
that occur in both the spoken response and each of
1Two out of the four tasks in this study had only listening
materials; responses to these tasks are not included in the results
for the features which require reading materials.
158
the materials, divided by the number of word to-
kens / types in the response,2 and 3) unique tokens,
unique types: the number of word tokens / types that
occur in both the spoken response and one or two
of the materials, but do not occur in the remaining
material(s).
As a baseline, we also compare the proposed
content features based on the prompt materials to
content features based on collections of scored re-
sponses to the same prompts. This type of feature
has been shown to be effective for content scoring
both in non-native essays (Attali and Burstein, 2006)
and spoken responses (Xie et al, 2012), and is com-
puted by comparing the content in a test response to
content models trained using responses from each of
the score points. It is defined as follows:
? Simi: the similarity score between the words
in the spoken response and a content model
trained from responses receiving score i (i ?
1, 2, 3, 4 in this study)
The Simi features were trained on a corpus of
7820 scored responses (1955 for each of the four
prompts), and we investigated two different meth-
ods for computing the similarity between the test
responses and the content models: Content Vector
Analysis using the cosine similarity metric (CVA)
and Pointwise Mutual Information (PMI).
The spoken responses were processed using an
HMM-based triphone ASR system trained on 800
hours of non-native speech (approximately 15% of
the training data consisted of responses to the four
test questions in this study), and the ASR hypothe-
ses were used to compute the content features.3
5 Results
We first examine the performance of each of the
individual features by calculating their correlations
with the holistic English speaking proficiency scores
provided by expert human raters. These results for
2Dividing the number of matching word tokens / types by
the number of word tokens in the response factors out the over-
all length of the response from the calculation of the feature.
3Transcriptions were not available for the spoken responses
used in this study, so the exact WER of the ASR system is un-
known. However, the WER of the ASR system on a comparable
set of spoken responses is 28%.
the training partition are presented in Table 1.4
Feature Set Feature r
stimulus cosine
listening 0.384
reading 0.176
sample 0.384
tokens/response
listening 0.022
reading 0.096
sample 0.121
types/response
listening 0.426
reading 0.142
sample 0.128
unique tokens
L?RS 0.116
L?RS? 0.162
LR?S 0.219
LR?S? 0.337
unique types
L?RS 0.140
L?RS? 0.166
LR?S 0.259
LR?S? 0.450
CVA
Sim1 0.091
Sim2 0.186
Sim3 0.261
Sim4 0.311
PMI
Sim1 0.191
Sim2 0.261
Sim3 0.320
Sim4 0.361
Table 1: Correlations of individual content features with
holistic human scores on the training partition
As Table 1 shows, some of the individual content
features based on the prompt materials obtain higher
correlations with human scores than the baseline
CVA and PMI features based on scored responses.
Next, we investigated the overall contribution of the
content features to a scoring model that takes into
account features from various aspects of speaking
proficiency. To show this, we built a baseline lin-
ear regression model to predict the human scores us-
ing 9 features from 4 different aspects of speaking
4For the unique tokens and unique types features, each row
lists how the prompt materials were used in the similarity com-
parison as follows: R = reading, L = listening, S = sample,
and ? indicates no lexical overlap between the spoken response
and the material. For example, L?RS indicates content from the
test response that overlapped with both the reading passage and
sample response but was not contained in the listening material.
159
proficiency (fluency, pronunciation, prosody, and
grammar) produced by SpeechRater, an automated
speech scoring system (Zechner et al, 2009), as
shown in Table 2.
Category Features
Fluency normalized number of silences
> 0.15 sec, normalized number
of silences > 0.495 sec, average
chunk length, speaking rate, nor-
malized number of disfluencies
Pronunciation normalzied Acoustic Model
score from forced alignment
using a native speaker AM,
average normalized phone du-
ration differnce compared to a
reference corpus
Prosody mean deviation of distance be-
tween stressed syllables
Grammar Language Model score
Table 2: Baseline speaking proficiency features used in
the scoring model
In order to investigate the contribution of the vari-
ous types of content features to the scoring model,
linear regression models were built by adding the
features from each of the feature sets in Table 1 to
the baseline features. The models were trained using
the 794 responses in the training set and evaluated
on the 395 responses in the evaluation set. Table 3
presents the resulting correlations both for the indi-
vidual responses (N=395) as well as the sum of all
four responses from each speaker (N=97).5
As Table 3 shows, all of the scoring models us-
ing feature sets with the proposed content features
based on the prompt materials outperform the base-
line model. While none of the models incorporat-
ing features from a single feature set outperforms
the baseline CVA model using features based on
scored responses, a model incorporating all of the
proposed prompt-based content features, all prompt-
based, does outperform this baseline. Furthermore,
a model incorporating all of the content features
(both the proposed features and the baseline CVA /
PMI features), all content, outperforms a model us-
5Three speakers were removed from the evaluation set for
this analysis since they provided fewer than four responses.
Feature Set response r speaker r
Baseline 0.607 0.687
+ types/response 0.612 0.701
+ tokens/response 0.615 0.700
+ unique tokens 0.616 0.695
+ stimulus cosine 0.630 0.716
+ unique types 0.658 0.761
+ CVA 0.665 0.762
+ all prompt-based 0.677 0.779
+ PMI 0.723 0.818
+ CVA and PMI 0.723 0.818
+ all content 0.742 0.838
Table 3: Performance of scoring models with the addition
of content features
ing only the baseline CVA and PMI features.6
6 Discussion and Conclusion
This paper has demonstrated that the use of content
scoring features based solely on the prompt stimu-
lus materials and a sample response is a viable al-
ternative to using features based on content mod-
els trained on large sets of pre-scored responses for
the automated assessment of spoken language profi-
ciency. Under this approach, automated scoring sys-
tems for large-scale spoken language assessments
involving spontaneous speech can begin to address
an area of spoken language proficiency (content ap-
propriateness) which has mostly been neglected in
systems that have been developed to date. Com-
pared to an approach using pre-scored responses for
training the content models, the proposed approach
is much more cost effective and reduces the risk
that test materials will be seen by test takers prior
to the assessment; both of these attributes are cru-
cial benefits for large-scale, high-stakes language as-
sessments. Furthermore, the proposed prompt-based
content features, when combined in a linear regres-
sion model with other speaking proficiency features,
outperform a baseline set of CVA content features
which use models trained on pre-scored responses,
6While the prompt-based content features do result in im-
provements, neither of these two differences are statistically sig-
nificant at ? = 0.05 using the Hotelling-Williams Test, since
both the magnitude of the increase and the size of the data set
are relatively small.
160
and they add further improvement to a model incor-
porating the higher performing baseline with PMI
content features.
The results in Table 1 indicate that the indi-
vidual features based on overlapping lexical types
(types/response and unique types) perform slightly
better than the ones based on overlapping lexical to-
kens (tokens/response and unique tokens). This sug-
gests that it is important for test takers to use a range
of concepts that are contained in the stimulus mate-
rials in their responses. Similarly to the result from
(Gurevich and Deane, 2007), Table 1 also shows that
the features measuring overlap between the response
and the listening materials typically perform better
than the features measuring overlap between the re-
sponse and the reading materials; the best individ-
ual feature, LR?S? for unique types, measures the
amount of overlap with lexical types that are con-
tained in the listening stimulus, but absent from the
reading stimulus and sample response. This indi-
cates that the use of content from the listening ma-
terials is a better differentiator among students of
differing language proficiency levels than reading
materials, likely because test takers generally have
more difficulty understanding the content from lis-
tening materials.
Table 1 also shows the somewhat counterintu-
itive result that features based on no lexical over-
lap with the sample response produce higher corre-
lations than features based on lexical overlap with
the sample response, when there is lexical overlap
with the listening materials and no overlap with the
reading materials. That is, the LR?S? feature out-
performs the LR?S feature for both the unique types
and unique tokens features sets. However, as shown
in Section 4, the sample responses varied widely
in length (ranging from 41 to 133 words), and all
were substantially shorter than the listening materi-
als, which ranged from 213 to 318 words. Therefore,
it is likely that many of the important lexical items
from the sample response are also contained in the
listening materials. Thus, the LR?S feature provided
less information than the LR?S? feature.
The features used in this study are all based on
simple lexical overlap statistics, and are thus triv-
ial to implement. Future research will investigate
more sophisticated methods of text-to-text similar-
ity for prompt-based content scoring, such as those
used in (Mohler and Mihalcea, 2009). Furthermore,
future research will address the validity of the pro-
posed features by ensuring that there are ways to fil-
ter out responses that are too similar to the stimulus
materials, and thus indicate that the test taker simply
repeated the source verbatim.
7 Acknowledgments
The authors would like to thank Yigal Attali for shar-
ing his ideas about prompt-based content scoring.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater R? V.2. The Journal of Technol-
ogy, Learning, and Assessment, 4(3):3?30.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010. Validating automated speaking tests. Language
Testing, 27(3):355?377.
Abhishek Chandel, Abhinav Parate, Maymon Madathin-
gal, Himanshu Pant, Nitendra Rajput, Shajith Ikbal,
Om Deshmuck, and Ashish Verma. 2007. Sensei:
Spoken language assessment for call center agents. In
Proceedings of ASRU.
Miao Chen and Klaus Zechner. 2012. Using an ontol-
ogy for improved automated content scoring of spon-
taneous non-native speech. In Proceedings of the
7th Workshop on Innovative Use of NLP for Build-
ing Educational Applications, NAACL-HLT, Montre?al,
Canada. Association for Computational Linguistics.
Peter W. Foltz, Darrell Laham, and Thomas K. Landauer.
1999. The intelligent essay assessor: Applications to
educational technology. Interactive Multimedia Elec-
tronic Journal of Computer-Enhanced Learning, 1(2).
Olga Gurevich and Paul Deane. 2007. Document
similarity measures to distinguish native vs. non-
native essay writers. In Proceedings of NAACL HLT,
Rochester, NY.
Tom Mitchell, Terry Russell, Peter Broomhead, and
Nicola Aldridge. 2002. Towards robust computerised
marking of free-text responses. In Proceedings of
the 6th International Computer Assisted Assessment
(CAA) Conference, Loughborough.
Michael Mohler and Rada Mihalcea. 2009. Text-to-
text semantic similarity for automatic short answer
grading. In Proceedings of the European Chapter of
the Association for Computational Linguistics (EACL
2009), Athens, Greece.
Jana Sukkarieh, Stephen Pulman, and Nicholas Raikes.
2004. Auto-marking 2: An update on the UCLES-
Oxford University research into using computational
linguistics to score short, free text responses. In
161
International Association of Educational Assessment,
Philadelphia.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech scor-
ing. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 103?111, Montre?al, Canada. Association
for Computational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken En-
glish. Speech Communication, 51(10):883?895.
162
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 22?27,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automatic detection of plagiarized spoken responses
Keelan Evanini and Xinhao Wang
Educational Testing Service
660 Rosedale Road, Princeton, NJ, USA
{kevanini,xwang002}@ets.org
Abstract
This paper addresses the task of auto-
matically detecting plagiarized responses
in the context of a test of spoken En-
glish proficiency for non-native speakers.
A corpus of spoken responses containing
plagiarized content was collected from a
high-stakes assessment of English profi-
ciency for non-native speakers, and sev-
eral text-to-text similarity metrics were
implemented to compare these responses
to a set of materials that were identified
as likely sources for the plagiarized con-
tent. Finally, a classifier was trained using
these similarity metrics to predict whether
a given spoken response is plagiarized or
not. The classifier was evaluated on a
data set containing the responses with pla-
giarized content and non-plagiarized con-
trol responses and achieved accuracies of
92.0% using transcriptions and 87.1% us-
ing ASR output (with a baseline accuracy
of 50.0%).
1 Introduction
The automated detection of plagiarism has been
widely studied in the domain of written student
essays, and several online services exist for this
purpose.
1
In addition, there has been a series of
shared tasks using common data sets of written
language to compare the performance of a vari-
ety of approaches to plagiarism detection (Potthast
et al., 2013). In contrast, the automated detection
of plagiarized spoken responses has received little
attention from both the NLP and assessment com-
munities, mostly due to the limited application of
1
For example, http://turnitin.com/en_
us/features/originalitycheck, http:
//www.grammarly.com/plagiarism-checker/,
and http://www.paperrater.com/plagiarism_
checker.
automated speech scoring for the types of spo-
ken responses that could be affected by plagiarism.
Due to a variety of factors, though, this is likely to
change in the near future, and the automated detec-
tion of plagiarism in spoken language will become
an increasingly important application.
First of all, English continues its spread as the
global language of education and commerce, and
there is a need to assess the communicative com-
pentance of high volumes of highly proficient non-
native speakers. In order to provide a valid evalua-
tion of the complex linguistic skills that are nec-
essary for these speakers, the assessment must
contain test items that elicit spontaneous speech,
such as the Independent and Integrated Speaking
items in the TOEFL iBT test (ETS, 2012), the
Retell Lecture item in the Pearson Test of English
Academic (Longman, 2010), and the oral inter-
view in the IELTS Academic assessment (Cullen
et al., 2014). However, with the increased em-
phasis on complex linguistic skills in assessments
of non-native speech, there is an increased chance
that test takers will prepare canned answers using
test preparation materials prior to the examination.
Therefore, research should also be conducted on
detecting spoken plagiarized responses in order to
prevent this type of cheating strategy.
In addition, there will also likely be an increase
in spoken language assessments for native speak-
ers in the K-12 domain in the near future. Curricu-
lum developers and assessment designers are rec-
ognizing that the assessment of spoken commu-
nication skills is important for determining a stu-
dent?s college readiness. For example, the Com-
mon Core State Standards include Speaking &
Listening English Language Arts standards for
each grade that pertain to a student?s ability to
communicate information and ideas using spoken
language.
2
In order to assess these standards, it
2
http://www.corestandards.org/
ELA-Literacy/SL/
22
will be necessary to develop standardized assess-
ments for the K-12 domain that contain items elic-
iting spontaneous speech from the student, such as
presentations, group discussions, etc. Again, with
the introduction of these types of tasks, there is a
risk that a test taker?s spoken response will contain
prepared material drawn from an external source,
and there will be a need to automatically detect
this type of plagiarism on a large scale, in order to
provide fair and valid assessments.
In this paper, we present an initial study of au-
tomated plagiarism detection on spoken responses
containing spontaneous non-native speech. A data
set of actual plagiarized responses was collected,
and text-to-text similarity metrics were applied to
the task of classifying responses as plagiarized or
non-plagiarized.
2 Previous Work
A wide variety of techniques have been employed
in previous studies for the task of detecting plagia-
rized written documents, including n-gram over-
lap (Lyon et al., 2006), document fingerprinting
(Brin et al., 1995), word frequency statistics (Shiv-
akumar and Garcia-Molina, 1995), Information
Retrieval-based metrics (Hoad and Zobel, 2003),
text summarization evaluation metrics (Chen et
al., 2010), WordNet-based features (Nahnsen et
al., 2005), and features based on shared syntactic
patterns (Uzuner et al., 2005). This task is also
related to the widely studied task of paraphrase
recognition, which benefits from similar types of
features (Finch et al., 2005; Madnani et al., 2012).
The current study adopts several of these features
that are designed to be robust to the presence of
word-level modifications between the source and
the plagiarized text; since this study focuses on
spoken responses that are reproduced from mem-
ory and subsequently processed by a speech recog-
nizer, metrics that rely on exact matches are likely
to perform sub-optimally. To our knowledge, no
previous work has been reported on automatically
detecting similar spoken documents, although re-
search in the field of Spoken Document Retrieval
(Haputmann, 2006) is relevant.
Due to the difficulties involved in collecting cor-
pora of actual plagiarized material, nearly all pub-
lished results of approaches to the task of plagia-
rism detection have relied on either simulated pla-
giarism (i.e., plagiarized texts generated by experi-
mental human participants in a controlled environ-
ment) or artificial plagiarism (i.e., plagiarized texts
generated by algorithmically modifying a source
text) (Potthast et al., 2010). These results, how-
ever, may not reflect actual performance in a de-
ployed setting, since the characteristics of the pla-
giarized material may differ from actual plagia-
rized responses. To overcome this limitation, the
current study is based on a set of actual plagiarized
responses drawn from a large-scale assessment.
3 Data
The data used in this study was drawn from the
TOEFL
R
?
Internet-based test (TOEFL
R
?
iBT), a
large-scale, high-stakes assessment of English for
non-native speakers, which assesses English com-
munication skills for academic purposes. The
Speaking section of TOEFL iBT contains six
tasks, each of which requires the test taker to pro-
vide an extended response containing spontaneous
speech. Two of the tasks are referred to as In-
dependent tasks; these tasks cover topics that are
familiar to test takers and ask test takers to draw
upon their own ideas, opinions, and experiences in
a 45-second spoken response (ETS, 2012). Since
these two Independent tasks ask questions that are
not based on any stimulus materials that were pro-
vided to the test taker (such as a reading passage,
figure, etc.), the test takers can provide responses
that contain a wide variety of specific examples.
In some cases, test takers may attempt to game
the assessment by memorizing canned material
from an external source and adapting it to a ques-
tion that is asked in one of the Independent tasks.
This type of plagiarism can affect the validity of
a test taker?s speaking score; however, it is often
difficult even for trained human raters to recog-
nize plagiarized spoken responses, due to the large
number and variety of external sources that are
available from online test preparation sites.
In order to better understand the strategies used
by test takers who incorporated material from ex-
ternal sources into their spoken responses and to
develop a capability for automated plagiarism de-
tection for speaking items, a data set of opera-
tional spoken responses containing potentially pla-
giarized material was collected. This data set con-
tains responses that were flagged by human raters
as potentially containing plagiarized material and
then subsequently reviewed by rater supervisors.
In the review process, the responses were tran-
scribed and compared to external source materi-
23
als obtained through manual internet searches; if
it was determined that the presence of plagiarized
material made it impossible to provide a valid as-
sessment of the test taker?s performance on the
task, the response was assigned a score of 0. This
study investigates a set of 719 responses that were
flagged as potentially plagiarized between Octo-
ber 2010 and December 2011; in this set, 239 re-
sponses were assigned a score of 0 due to the pres-
ence of a significant amount of plagiarized con-
tent from an identified source. This set of 239 re-
sponses is used in the experiments described be-
low.
During the process of reviewing potentially pla-
giarized responses, the raters also collected a data
set of external sources that appeared to have been
used by test takers in their responses. In some
cases, the test taker?s spoken response was nearly
identical to an identified source; in other cases,
several sentences or phrases were clearly drawn
from a particular source, although some modifi-
cations were apparent. Table 1 presents a sample
source that was identified for several of the 239 re-
sponses in the data set.
3
Many of the plagiarized
responses contained extended sequences of words
that directly match idiosyncratic features of this
source, such as the phrases ?how romantic it can
ever be? and ?just relax yourself on the beach.?
In total, 49 different source materials were iden-
tified for all of the potentially plagiarized re-
sponses in the corpus.
4
In addition to the source
materials and the plagiarized responses, a set of
non-plagiarized control responses was also ob-
tained in order to conduct classification experi-
ments between plagiarized and non-plagiarized re-
sponses. Since the plagiarized responses were
collected over the course of more than one year,
they were drawn from many different TOEFL iBT
test forms; in total, the 239 plagiarized responses
comprise 103 distinct Independent test questions.
Therefore, it was not practical to obtain control
data from all of the test items that were represented
in the plagiarized set; rather, approximately 300
responses were extracted from each of the four test
3
This source is available from several online test prepara-
tion websites, for example http://www.mhdenglish.
com/eoenglish_article_view_1195.html.
4
A total of 39 sources were identified for the set of 239
responses in the Plagiarized set; however, all 49 identified
sources were used in the experiments in order to make the
experimental design more similar to an operational set-up in
which the exact set of source texts that will be represented in
a given set of plagiarized responses is not known.
Well, the place I enjoy the most is a small
town located in France. I like this small town
because it has very charming ocean view. I
mean the sky there is so blue and the beach
is always full of sunshine. You know how
romantic it can ever be, just relax yourself
on the beach, when the sun is setting down,
when the ocean breeze is blowing and the
seabirds are singing. Of course I like this
small French town also because there are
many great French restaurants. They offer
the best seafood in the world like lobsters and
tuna fishes. The most important, I have been
benefited a lot from this trip to France because
I made friends with some gorgeous French
girls. One of them even gave me a little watch
as a souvenir of our friendship.
Table 1: Sample source passage used in plagia-
rized responses
items that were most frequently represented in the
set of plagiarized responses. Table 2 provides a
summary of the three data sets used in the study,
along with summary statistics about the length of
the responses in each set.
Data Set N
Number of Words
Mean Std. Dev.
Sources 49 122.5 36.5
Plagiarized 239 109.1 18.9
Control 1196 84.9 24.1
Table 2: Summary of the data sets
As Table 2 shows, the plagiarized responses
are on average a little longer than the control re-
sponses. This is likely due to the fact that the pla-
giarized responses contain a large percentage of
memorized material, which the test takers are able
to produce using a fast rate of speech, since they
had likely rehearsed the content several times be-
fore taking the assessment.
4 Methodology
The general approach taken in this study for deter-
mining whether a spoken response is plagiarized
or not was to compare its content to the content of
each of the source materials that had been iden-
tified for the responses in this corpus. Given a
test response, a comparison was made with each
24
of the 49 reference sources using the following 9
text-to-text similarity metrics: 1) Word Error Rate
(WER), or edit distance between the response and
the source; 2) TER, similar to WER, but allowing
shifts of words within the text at a low edit cost
(Snover et al., 2006); 3) TER-Plus, an extension of
TER that includes matching based on paraphrases,
stemming, and synonym substitution (Snover et
al., 2008); 4) a WordNet similarity metric based on
presence in the same synset;
5
5) a WordNet sim-
ilarity metric based on the shortest path between
two words in the is-a taxonomy; 6) a WordNet
similarity metric similar to (5) that also takes into
account the maximum depth of the taxonomy in
which the words occur (Leacock and Chodorow,
1998); 7) a WordNet similarity metric based on the
depth of the Least Common Subsumer of the two
words (Wu and Palmer, 1994); 8) Latent Semantic
Analysis, using a model trained on the British Na-
tional Corpus (BNC, 2007); 9) BLEU (Papineni et
al., 2002). Most of these similarity metrics (with
the exception of WER and TER) are expected to
be robust to modifications between the source text
and the plagiarized response, since they do not rely
on exact string matches.
Each similarity metric was used to compute 4
different features comparing the test response to
each of the 49 source texts: 1) the document-level
similarity between the test response and the source
text; 2) the single maximum similarity value from
a sentence-by-sentence comparison between the
test response and the source text; 3) the average of
the similarity values for all sentence-by-sentence
comparisons between the test response and the
source text; 4) the average of the maximum simi-
larity values for each sentence in the test response,
where the maximum similarity of a sentence is ob-
tained by comparing it with each sentence in the
source text. The intuition behind using the fea-
tures that compare sentence-to-sentence similarity
as opposed to only the document-level similarity
feature is that test responses may contain a combi-
nation of both passages that were memorized from
a source text and novel content. Depending on the
amount of the response that was plagiarized, these
types of responses may also receive a score of 0;
so, in order to also detect these responses as pla-
5
For the WordNet-based similarity metrics, the similarity
scores for pairs of words were combined to obtain document-
and sentence-level similarity scores by taking the average
maximum pairwise similarity values, similar to the sentence-
level similarity feature defined in (4) below.
giarized, a sentence-by-sentence comparison ap-
proach may be more effective.
The experiments described below were con-
ducted using both human transcriptions of the spo-
ken responses as well as the output from an au-
tomated speech recognition (ASR) system. The
ASR system was trained on approximately 800
hours of TOEFL iBT responses; the system?s
WER on the data used in this study was 0.411 for
the Plagiarized set and 0.362 for the Control set.
Since the ASR output does not contain sentence
boundaries, these were obtained using a Maxi-
mum Entropy sentence boundary detection system
based on lexical features (Chen and Yoon, 2011).
Before calculating the similarity features, all of the
texts were preprocessed to normalize case, seg-
ment the text into sentences, and remove disfluen-
cies, including filled pauses (such as uh and um)
and repeated words. No stemming was performed
on the words in the texts for this study.
5 Results
As described in Section 4, 36 similarity features
were calculated between each spoken response
and each of the 49 source texts. In order to exam-
ine the performance of these features in discrim-
inating between plagiarized and non-plagiarized
responses, classification experiments were con-
ducted on balanced sets of Plagiarized and Con-
trol responses, and the results were averaged using
1000 random subsets of 239 responses from the
Control set.
6
In addition, the following different
feature sets were compared: All (all 36 features),
Doc (the 9 document-level features), and Sent (the
27 features based on sentence-level comparisons).
The J48 decision tree model from the Weka toolkit
(with the default parameter settings) was used
for classification, and 10-fold cross-validation was
performed using both transcriptions and ASR out-
put. Table 3 presents the results of these experi-
ments, including the means (and standard devia-
tions) of the accuracy and kappa (?) values (for all
experiments, the baseline accuracy is 50%).
6 Discussion and Future Work
As Table 3 shows, the classifier achieved a higher
accuracy when using the 9 document-level simi-
larity features compared to using the 27 sentence-
6
Experiments were also conducted using the full Control
set, and the results showed a similar relative performance of
the feature sets.
25
Text Features Accuracy ?
Trans.
All 0.903 (0.01) 0.807 (0.02)
Doc 0.920 (0.01) 0.839 (0.02)
Sent 0.847 (0.01) 0.693 (0.03)
ASR
All 0.852 (0.02) 0.703 (0.03)
Doc 0.871 (0.01) 0.742 (0.03)
Sent 0.735 (0.02) 0.470 (0.04)
Table 3: Mean Accuracy and ? values (and stan-
dard deviations) for classification results using the
239 responses in the Plagiarized set and 1000 ran-
dom subsets of 239 responses from the Control set
level similarity features. In addition, the combined
set of 36 features resulted in a slightly lower per-
formance than when only the 9 document-level
features were used. This suggests that the sentence
level features are not as robust as the document-
level features, probably due to the increased like-
lihood of chance similarities between sentences in
the response and a source text. Despite the fact
that the plagiarized spoken responses in this data
set may contain some original content (in particu-
lar, introductory material provided by the test taker
in an attempt to make the plagiarized content seem
more relevant to the specific test question), it ap-
pears that the document-level features are most ef-
fective. Table 3 also indicates that the performance
of the classifier decreases by approximately 5% -
10% when ASR output is used. This indicates that
the similarity metrics are reasonably robust to the
presence of speech recognition errors in the text,
and that the approach is viable in an operational
setting in which transcriptions of the spoken re-
sponses are not available.
A more detailed error analysis indicates that the
precision of the classifier, with respect to the Pla-
giarized class, is higher than the recall: on the
transcriptions, the average precision using the Doc
features was 0.948 (s.d.= 0.01), whereas the av-
erage recall was 0.888 (s.d.=0.01); for the ASR
set, the average precision was 0.904 (s.d.=0.02),
whereas the average recall was 0.831 (s.d.=0.02).
This means that the rate of false positives pro-
duced by this classifier is somewhat lower than the
rate of false negatives. In an operational scenario,
an automated plagiarized spoken response detec-
tion system such as this one would likely be de-
ployed in tandem with human raters to review the
results and provide a final decision about whether
a given spoken response was plagiarized or not. In
that case, it may be desirable to tune the classi-
fier parameters to increase the recall so that fewer
cases of plagiarism would go undetected, assum-
ing that there are suffient human reviewers avail-
able to process the increased number of false pos-
itives that would result from this approach. Im-
proving the classifier?s recall is also important for
practical applications of this approach, since the
distribution of actual responses is heavily imbal-
anced in favor of the non-plagiarized class. The
current set of experiments only used a relatively
small Control set of 1196 responses for which
transcriptions could be obtained in a cost effective
manner in order to be able to compare the system?s
performance using transcriptions and ASR output.
Since there was only a minor degradation in per-
formance when ASR output was used, future ex-
periments will be conducted using a much larger
Control set in order to approximate the distribution
of categories that would be observed in practice.
One drawback of the method described in this
study is that it requires matching source texts in
order to detect a plagiarized spoken response. This
means that plagiarized spoken responses based on
a given source text will not be detected by the
system until the appropriate source text has been
identified, thus limiting the system?s recall. Be-
sides attempting to obtain additional source texts
(either manually, as was done for this study, or by
automated means), this could also be addressed
by comparing a test response to all previously
collected spoken responses for a given popula-
tion of test takers in order to flag pairs of sim-
ilar responses. While this method would likely
produce a high number of false positives when
the ASR output was used, due to chance simi-
larities between two responses in a large pool of
test taker responses resulting from imperfect ASR,
performance could be improved by considering
additional information from the speech recognizer
when computing the similarity metrics, such as
the N-best list. Additional sources of informa-
tion that could be used for detecting plagiarized re-
sponses include stylistic patterns and prosodic fea-
tures; for example, spoken responses that are re-
produced from memory likely contain fewer filled
pauses and have a faster rate of speech than non-
plagiarized responses; these types of non-lexical
features should also be investigated in future re-
search into the detection of plagiarized spoken re-
sponses.
26
Acknowledgments
We would like to thank Beata Beigman Klebanov,
Dan Blanchard, Nitin Madnani, and three anony-
mous BEA-9 reviewers for their helpful com-
ments.
References
BNC. 2007. The British National Corpus, version 3.
Distributed by Oxford University Computing Ser-
vices on behalf of the BNC Consortium, http:
//www.natcorp.ox.ac.uk/.
Sergey Brin, James Davis, and Hector Garcia-Molina.
1995. Copy detection mechanisms for digital docu-
ments. In Proceedings of the ACM SIGMOD Annual
Conference, pages 398?409.
Lei Chen and Su-Youn Yoon. 2011. Detecting
structural events for assessing non-native speech.
In Proceedings of the 6th Workshop on Innovative
Use of NLP for Building Educational Applications,
NAACL-HLT, pages 38?45, Portland, OR. Associa-
tion for Computational Linguistics.
Chien-Ying Chen, Jen-Yuan Yeh, and Hao-Ren Ke.
2010. Plagiarism detection using ROUGE and
WordNet. Journal of Computing, 2(3):34?44.
Pauline Cullen, Amanda French, and Vanessa Jakeman.
2014. The Official Cambridge Guide to IELTS.
Cambridge University Press.
ETS. 2012. The Official Guide to the TOEFL
R
?
Test,
Fourth Edition. McGraw-Hill.
Andrew Finch, Young-Sook Hwang, and Eiichiro
Sumita. 2005. Using machine translation evalua-
tion techniques to determine sentence-level seman-
tic equivalence. In Proceedings of the Third Inter-
national Workshop on Paraphrasing, pages 17?24.
Alexander Haputmann. 2006. Automatic spoken doc-
ument retrieval. In Ketih Brown, editor, Encylclope-
dia of Language and Linguistics (Second Edition),
pages 95?103. Elsevier Science.
Timothy C. Hoad and Justin Zobel. 2003. Methods
for identifying versioned and plagiarised documents.
Journal of the American Society for Information Sci-
ence and Technology, 54:203?215.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database,
pages 305?332. MIT Press.
Pearson Longman. 2010. The Official Guide to Pear-
son Test of English Academic. Pearson Education
ESL.
Caroline Lyon, Ruth Barrett, and James Malcolm.
2006. Plagiarism is easy, but also easy to detect.
Plagiary, 1:57?65.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 182?
190, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Thade Nahnsen,
?
Ozlem Uzuner, and Boris Katz. 2005.
Lexical chains and sliding locality windows in
content-based text similarity detection. CSAIL
Technical Report, MIT-CSAIL-TR-2005-034.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Martin Potthast, Benno Stein, Alberto Barr?on-Cede?no,
and Paolo Rosso. 2010. An evaluation frame-
work for plagiarism detection. In Proceedings of the
23rd International Conference on Computational
Linguistics.
Martin Potthast, Matthias Hagen, Tim Gollub, Martin
Tippmann, Johannes Kiesel, Paolo Rosso, Efstathios
Stamatatos, and Benno Stein. 2013. Overview of
the 5th International Competition on Plagiarism De-
tection. In Pamela Forner, Roberto Navigli, and
Dan Tufis, editors, CLEF 2013 Evaluation Labs and
Workshop ? Working Notes Papers.
Narayanan Shivakumar and Hector Garcia-Molina.
1995. SCAM: A copy detection mechanism for digi-
tal documents. In Proceedings of the Second Annual
Conference on the Theory and Practice of Digital
Libraries.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Matt Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. 2008. TERp: A system descrip-
tion. In Proceedings of the First NIST Metrics
for Machine Translation Challenge (MetricsMATR),
Waikiki, Hawaii, October.
?
Ozlem Uzuner, Boris Katz, and Thade Nahnsen. 2005.
Using syntactic information to identify plagiarism.
In Proceedings of the 2nd Workshop on Building Ed-
ucational Applications using NLP. Ann Arbor.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proceedings of the 32nd
Annual Mmeeting of the Association for Computa-
tional Linguistics (ACL).
27
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 134?142,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automated Scoring of Speaking Items in an Assessment for Teachers of
English as a Foreign Language
Klaus Zechner, Keelan Evanini, Su-Youn Yoon, Lawrence Davis,
Xinhao Wang, Lei Chen, Chong Min Lee, Chee Wee Leong
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
{kzechner,kevanini,syoon,ldavis,xwang002,lchen,clee001,cleong}@ets.org
Abstract
This paper describes an end-to-end proto-
type system for automated scoring of spo-
ken responses in a novel assessment for
teachers of English as a Foreign Language
who are not native speakers of English.
The 21 speaking items contained in the as-
sessment elicit both restricted and moder-
ately restricted responses, and their aim is
to assess the essential speaking skills that
English teachers need in order to be effec-
tive communicators in their classrooms.
Our system consists of a state-of-the-art
automatic speech recognizer; multiple fea-
ture generation modules addressing di-
verse aspects of speaking proficiency, such
as fluency, pronunciation, prosody, gram-
matical accuracy, and content accuracy; a
filter that identifies and flags problematic
responses; and linear regression models
that predict response scores based on sub-
sets of the features. The automated speech
scoring system was trained and evaluated
on a data set involving about 1,400 test
takers, and achieved a speaker-level cor-
relation (when scores for all 21 responses
of a speaker are aggregated) with human
expert scores of 0.73.
1 Introduction
As English has become increasingly important as a
language of international business, trade, science,
and communication, efforts to promote teaching
English as a Foreign Language (EFL) have seen
substantially more emphasis in many non-English-
speaking countries worldwide in recent years. In
addition, the prevailing trend in English pedagogy
has been to promote the use of spoken English in
the classroom, as opposed to the respective native
languages of the EFL learners. However, due to
the high demand for EFL teachers in many coun-
tries, the training of these teachers has not always
caught up with these high expectations, so there is
a need for both governmental and private institu-
tions involved in the employment and training of
EFL teachers to assess their competence in the En-
glish language, as well as in English pedagogy.
Against this background, we developed a lan-
guage assessment for EFL teachers who are not
native speakers of English that addresses the four
basic English language skills of Reading, Listen-
ing, Writing and Speaking. This paper focuses
only on the speaking portion of the English assess-
ment, and, in particular, on the system that we de-
veloped to automatically compute scores for test
takers? spoken responses.
Several significant challenges needed to be ad-
dressed during the course of building this auto-
mated speech scoring system, including, but not
limited to:
? The 21 Speaking items belong to 8 differ-
ent task types with different characteristics;
therefore, we had to select features and build
scoring models for each task type separately.
? The test takers speak a variety of native lan-
guages, and thus have very different non-
native accents in their spoken English. Fur-
thermore, the test takers also exhibit a wide
range of speaking proficiency levels, which
contributes to the diversity of their spoken re-
sponses. Our speech recognizer therefore had
to be trained and adapted to a large database
of non-native speech.
? Since content accuracy is very important for
the types of tasks contained in the test, even
small error rates by the automatic speech
recognition (ASR) system can lead to a no-
ticeable impact on feature performance. This
fact motivated the development of a set of
134
features that are robust to speech recognition
errors.
? A significant amount of responses (more than
7%) exhibit issues that make them hard or
impossible to score automatically, e.g., high
noise levels, background speech, etc. We
therefore implemented a filter to identify
these non-scorable responses automatically.
The paper is organized as follows: Section 2
discusses related work; in Section 3, we present
the data used for system training and evaluation;
Section 4 describes the system architecture of the
automated speech scoring system. We detail the
methods we used to build our system in Section 5,
followed by an overview of the results in Section
6. Section 7 discusses our findings; finally, Sec-
tion 8 concludes the paper.
2 Related Work
Automated speech processing and scoring tech-
nology has been applied to a variety of domains
over the course of the past two decades, includ-
ing evaluation and tutoring of children?s literacy
skills (Mostow et al., 1994), preparation for high
stakes English proficiency tests for institutions of
higher education (Zechner et al., 2009), evalua-
tion of English skills of foreign-based call center
agents (Chandel et al., 2007), and evaluation of
aviation English (Pearson Education, Inc., 2011),
to name a few (for a comprehensive overview, see
(Eskenazi, 2009)).
Most of these applications elicit restricted
speech from the participants, and the most com-
mon item type by far is the Read Aloud, in which
the speaker reads a sentence or collection of sen-
tences out loud. Due to the constrained nature
of this task, it is possible to develop ASR sys-
tems that are relatively accurate, even with heav-
ily accented non-native speech. Several types of
features related to a non-native speaker?s ability
to produce English sounds and speech patterns
effectively have been extracted from these types
of responses. Some of the best performing of
these types of features include pronunciation fea-
tures, such as a phone?s spectral match to na-
tive speaker acoustic models (Witt, 1999) and a
phone?s duration compared to native speaker mod-
els (Neumeyer et al., 2000); fluency features, such
as the rate of speech, mean pause length, and num-
ber of disfluencies (Cucchiarini et al., 2000); and
prosody features, such as F0 and intensity slope
(Hoenig, 2002).
In addition to the large majority of applications
that elicit restricted speech, a small number of ap-
plications have also investigated automated scor-
ing of non-native spontaneous speech, in order
to more fully evaluate a speaker?s communicative
competence (e.g., (Cucchiarini et al., 2002) and
(Zechner et al., 2009)). In these systems, the same
types of pronunciation, fluency, and prosody fea-
tures can be extracted; furthermore, features re-
lated to additional aspects of a speaker?s profi-
ciency in the non-native language can be extracted,
such as vocabulary usage (Yoon et al., 2012), syn-
tactic complexity (Bernstein et al., 2010a; Chen
and Zechner, 2011), and topical content (Xie et al.,
2012).
As described in Section 1, the domain for the
automated speaking assessment investigated in
this study is teachers of EFL around the world.
Based on the fact that many of the item types are
designed to assess the test taker?s ability to pro-
ductively use English constructions and linguis-
tic units that commonly recur in English teach-
ing environments, several of the item types elicit
semi-restricted speech (see Table 1 below for a de-
scription of the different item types). These types
of responses fall somewhere between the heavily
restricted speech elicited by a Read Aloud task
and unconstrained spontaneous speech. In these
semi-restricted responses, the test taker may be
provided with a set of lexical items that should
be used to form a sentence; in addition, the test
taker is often asked to make the sentence conform
to a given grammatical template. Thus, the re-
sponses provided for a given prompt of this type
by multiple different speakers will often overlap
with each other; however, it is not possible to
specify a complete list of all possible responses.
These types of items have only infrequently been
examined in the context of automated speech scor-
ing. Some related item types that have been
explored previously include the Sentence Build
and Short item types described in (Bernstein et
al., 2010b); however, those item types typically
elicited a much narrower range of responses than
the semi-restricted ones in this study.
3 Data
The data used in this study was drawn from a pilot
administration of a language assessment for teach-
135
ers of English as a Foreign Language. This test
is designed to assess the ability of a non-native
teacher of English to use English in classroom set-
tings. The language forms and functions included
in this test are based on the materials included in a
curriculum that the test takers studied prior to tak-
ing the assessment. The assessment includes items
that cover the four language skills: Reading, Lis-
tening, Writing, and Speaking. There are a total of
8 different types of Speaking items included in the
assessment. These can be divided into the follow-
ing two categories, depending on how constrained
the test taker?s response is:
? Restricted Speech: In these item types, all
of the linguistic content expected in the
test taker?s response is presented in the test
prompt, and the test taker is asked to read or
repeat it aloud.
? Semi-restricted Speech: In these item types, a
portion of the linguistic content is presented
in the prompt, and the test taker is required to
provide the remaining content to formulate a
complete response.
Sets of 7 Speaking items are presented to the
test taker in thematic units, called ?lessons?, based
on their instructional goals; in total, each test taker
completed three lessons, and thus responded to 21
Speaking items. Table 1 presents descriptions of
the 8 different item types included in the assess-
ment.
The numbers of responses provided by the test
takers to each type (along with their respective re-
sponse durations) are as follows: four Multiple
Choice (10 seconds each), six Read Aloud (four 40
second responses and two 60 second responses),
two Repeat Aloud (15 seconds each), one Incom-
plete Sentence (20 seconds), one Key Words (15
seconds), five Chart (four 20 seconds and one 40
seconds), one Keyword Chart (15 seconds), and
one Visuals (15 seconds). Thus, each test taker
provided a total of approximately 9 minutes of au-
dio.
The responses were all double-scored by trained
human raters on a three-point scale (1 - 3). For
the Restricted Speech items, the raters assessed
the test taker?s pronunciation, pacing, and intona-
tion. For the Semi-restricted Speech items, the re-
sponses were also scored holistically on a 3-point
scale, but raters were also asked to take into ac-
count the appropriateness of the language used
Restricted Speech
Type Description
Multiple
Choice
(MC)
The test taker selects the correct
option and reads it aloud
Read Aloud
(RA)
The test taker reads aloud a set
of classroom instructions
Repeat
Aloud (RP)
The test taker listens to a student
utterance twice and then repeats
it
Semi-restricted Speech
Type Description
Incomplete
Sentence
(IS)
The test taker is given a sentence
fragment and completes the sen-
tence according to the instruc-
tions
Key Words
(KW)
The test taker uses the key words
provided to speak a sentence as
instructed
Chart (CH) The test taker uses an example
from a language chart and then
formulates a similar sentence us-
ing a given grammatical pattern
Keyword
Chart (KC)
The test taker constructs a sen-
tence using keywords provided
and information in a chart
Visuals (VI) The test taker is given two visu-
als and is asked to give instruc-
tions to students based on the
graphical information
Table 1: Types of speaking items included in the
assessment
(e.g., grammatical accuracy and content correct-
ness) in addition to aspects of fluency and pronun-
ciation. For some responses, the raters were not
able to provide a score on the 1 - 3 scale, e.g.,
because the audio response contained no speech
input, the test taker responded in their native lan-
guage, etc. These responses are labeled NS for
Non-Scoreable.
After receiving scores, all of the responses
were transcribed using standard English orthogra-
phy (disfluencies, such as filled pauses and par-
tial words are also included in the transcriptions).
Then, the responses were partitioned (with no
speaker overlap) into five sets for the training and
evaluation of the ASR system and the linear re-
gression scoring models. The amount of data and
136
human score distributions in each of these parti-
tions are displayed in Table 2.
4 System Architecture
The automated scoring system used for the teach-
ers? spoken language assessment consists of the
following four components, which are invoked
one after the other in a pipeline fashion (ETS
SpeechRater
SM
, (Zechner et al., 2009; Higgins et
al., 2011)):
? an automated speech recognizer, generating
word hypotheses from input audio recordings
of the test takers? responses
? a feature computation module that generates
features based on the ASR output, e.g., mea-
suring fluency, pronunciation, prosody, and
content accuracy
? a filtering model that flags responses that
should not be scored automatically due to is-
sues with audio quality, empty responses, etc.
? linear regression scoring models that predict
the score for each response based on a set of
selected features
Furthermore, we use Praat (Boersma and
Weenick, 2012) to extract power and pitch from
the speech signal; this information is used for
some of the feature computation modules, as well
as for the filtering model.
The ASR is an HMM-based triphone system
trained on approximately 800 hours of non-native
speech from a different data set; a background
Language Model (LM) was also trained on the
same data set. Subsequently, 8 adapted LMs were
trained (with an interpolation weight of 0.9 for the
in-domain data) using the responses in the ASR
Training partition for the 8 different item types
listed in Table 1. The ASR system obtained an
overall word error rate (WER) of 13.0% on the
ASR Evaluation partition and 15.6% on the Model
Evaluation partition. As would be expected, the
ASR system performed best on the responses that
were most restricted by the test item and per-
formed worse on the responses that were less re-
stricted. The WER ranged from 11.4% for the
RA responses to 41.4% for the IS responses in the
Model Evaluation partition.
5 Methodology
5.1 Speech features
The feature computation components of our
speech scoring system compute more than 100
features based on a speaker?s response. They be-
long to the following broad dimensions of speak-
ing proficiency: fluency, pronunciation, prosody,
vocabulary usage, grammatical complexity and
accuracy, and content accuracy (Zechner et al.,
2009; Chen and Yoon, 2012; Chen et al., 2009;
Zechner et al., 2011; Yoon et al., 2012; Yoon and
Bhat, 2012; Zechner and Wang, 2013).
After initial feature generation, we selected a set
of about 10 features for each of the 8 item types,
based on the following considerations
1
(Zechner
et al., 2009; Xi et al., 2008):
? empirical performance, i.e., feature correla-
tion with human scores
? construct
2
relevance, i.e., to what extent the
feature measures aspects of speaking profi-
ciency that are considered to be relevant and
important by content experts
? overall construct coverage, i.e., the feature set
should include features from all relevant con-
struct dimensions
? feature independence, i.e., the inter-
correlation between any two features of the
set should be low
Furthermore, some features were transformed
(e.g., by applying the inverse or log function), in
order to increase the normality of their distribu-
tions (an assumption of linear regression classi-
fiers). All feature values that exceeded a thresh-
old of 4 standard deviations from the mean were
replaced by the respective threshold (outlier trun-
cation).
The composition of feature sets is slightly dif-
ferent for the two item type categories: for the 3
restricted item types, features related to fluency,
pronunciation, prosody and read/repeat accuracy
were chosen, whereas for the 5 semi-restricted
item types, vocabulary and grammar features were
also added to the set. Further, while accuracy
1
While automated feature selection is conceivable in prin-
ciple, in our experience it typically does not result in a feature
set that meets all of these criteria well.
2
A construct is the set of knowledge, skills, and abilities
measured by a test.
137
Partition Spk. Resp. Dur. 1 2 3 NS
ASR Training 773 16,049 116.7 1,587 (9.9) 4,086 (25.5) 8,796 (54.8) 1,580 (9.8)
ASR Development 25 525 3.8 53 (10.1) 133 (25.3) 327 (62.3) 12 (2.3)
ASR Evaluation 25 525 3.8 31 (5.9) 114 (21.7) 326 (62.1) 54 (10.3)
Model Training 300 6,300 45.8 675 (10.7) 1,715 (27.2) 3,577 (56.8) 333 (5.3)
Model Evaluation 300 6,300 45.7 647 (10.3) 1,637 (26.0) 3,487 (55.3) 529 (8.4)
Total 1,423 29,699 215.8 2,993 (9.38) 7,685 (25.14) 16,513 (58.26) 2,508 (7.22)
Table 2: Amount of data contained in each partition (speakers, responses, hours of speech) and distribu-
tion of human scores (percentages of scores per partition in brackets).
features for the restricted items were based only
on string alignment measures, content accuracy
features for the semi-restricted items were more
diverse, e.g., based on regular expressions, key-
words, and language model scores (Zechner and
Wang, 2013). Table 3 lists the features that were
used in the scoring models for restricted and semi-
restricted item types, along with sub-constructs
they measure and their description.
5.2 Filtering model
In order to automatically identify responses that
have technical issues (e.g., loud background noise)
or are otherwise not scorable (e.g., empty re-
sponses), a decision tree-based filtering model was
developed using a combination of features derived
from ASR output and from pitch and energy in-
formation (Yoon et al., 2011; Jeon and Yoon,
2012). The filtering model was tested on the scor-
ing model evaluation data, and obtained an ac-
curacy rate (the exact agreement between the fil-
tering model and a human rater concerning the
distinction between scorable and non-scorable re-
sponses) of 97%; it correctly identified 90% of the
non-scorable responses in the data set with a false
positive rate of 21% (recall=0.90, precision=0.79,
F-score=0.84).
5.3 Scoring models
We used the Model Training set to train 8 linear
regression models for the 8 different item types,
using the previously determined feature sets. We
used the features as independent variables in these
models and the summed scores of two human
raters as the dependent variable. These trained
scoring models were then employed to score re-
sponses of the Model Evaluation data (exclud-
ing responses marked as non-scorable by human
raters) and rounded to the nearest integer to predict
the final scores for each response. These scores
were then evaluated against the first human rater
score (H1).
Item N S-H1 H1-H2 WER (%)
RA 1653 0.34 0.51 11.4
RP 543 0.41 0.73 21.8
MC 1036 0.67 0.83 17.1
CH 1372 0.44 0.67 26.3
KW 275 0.45 0.67 28.7
KC 274 0.57 0.74 28.8
IS 260 0.46 0.69 41.4
VI 272 0.43 0.80 30.4
Table 4: Correlations between system and first hu-
man rater (S-H1) and between two human raters
(H1-H2), for all responses of each item type in the
Model Evaluation partition (N). The last column
provides the average ASR word error rate (WER)
in percent.
Additionally, for responses flagged as non-
scorable by the automatic filtering model, the sec-
ond human rater score (H2) was used as final
item score in order to mimic the operational sce-
nario where human raters score responses that are
flagged by the filtering model.
We also compute the agreement between sys-
tem and human raters based on a set of all 21 re-
sponses of a speaker. Score imputation was used
for responses that were labeled as non-scorable by
both the system and H2; in this case, the response
was given the mean score of the total scorable
responses from the same speaker. Similarly, the
same score imputation rule was applied to the H1
scores.
6 Results
Table 4 presents the Pearson correlation coeffi-
cients between human and automated scores for
the responses from the 8 different item types along
with the human-human correlation for each item
type. Furthermore, we also provide the word error
rates of the ASR system for the same 8 item types
in the last column of the table.
138
Feature Sub-construct Description
Content Ed1 Read/repeat accu-
racy / Fluency
Correctly read words per minute
Content Ed2 Read/repeat accu-
racy
Read/repeat word error rate
Content RegEx Content accuracy Matching of regular expressions
Content WER Content accuracy Response discrepancy from high scoring responses
Content NGram Content accuracy N-grams in response matching high scoring response n-
grams
Fluency Rate Fluency Speaking rate
Fluency Chunk Fluency Average length of contiguous word chunks
Fluency Sil1 Fluency Frequency of long silences
Fluency Sil2 Fluency / Grammar Proportion of long within-clause-silences to all within-
clause-silences
Fluency Sil3 Fluency Mean length of silences within a clause
Fluency Disfl1 Fluency Frequency of interruption points (repair, repetition, false
start)
Fluency Disfl2 Fluency Number of disfluencies per second
Fluency Disfl3 Fluency Frequency of repetitions
Pron Vowels Pronunciation Average vowel duration differences relative to a native-
speaker model
Prosody1 Prosody Percentage of stressed syllables
Prosody2 Prosody Mean deviation of time intervals between stressed syllables
Prosody3 Prosody Mean distance between stressed syllables
Vocab1 Vocabulary / Flu-
ency
Number of word types divided by utterance duration
Grammar POS Grammar Part-of-speech based distributional similarity score be-
tween a response and responses with different score levels
Grammar LM Grammar Global language model score (normalized by response
length)
Table 3: List of features used for item type scoring models, with the sub-constructs they represent and
descriptions.
139
Comparison Pearson r
S-H1 0.725
S-H2 0.742
H1-H2 0.934
Table 5: Speaker-level performance (Pearson r
correlations) computed over the sum of all 21
scores from each speaker, N=272
Sub-construct Restricted Semi-restricted
Content 0.33?0.67 0.34?0.61
Fluency 0.19?0.33 0.20?0.33
Pronunciation 0.20?0.22 0.13?0.31
Prosody 0.18?0.24 0.12?0.27
Grammar ? 0.23?0.49
Vocabulary ? 0.21?0.32
Table 6: Range of Pearson r correlations for dif-
ferent features with human scores (H1) by sub-
construct for restricted and semi-restricted item
types.
Table 5 presents the Pearson correlation coeffi-
cients between the speaker-level scores produced
by the automated scoring system (S) and the two
sets of human scores (H1 and H2). These speaker-
level scores were computed based on the sum of
all 21 scores from each speaker in the Model Eval-
uation partition. Responses that received a non-
scorable rating from the human raters were im-
puted, as described above. Furthermore, 28 speak-
ers were excluded from this analysis because they
had more than 7 non-scorable responses each.
3
Finally, Table 6 provides an overview of Pear-
son correlation ranges with human rater scores
(H1) for the different features used in the scoring
models, summarized by the sub-constructs that the
features represent.
7 Discussion
When looking at Table 4, we see that the inter-rater
reliability for human raters ranges between 0.51
(for RA items) and 0.83 (for MC items). Inter-
rater reliability varies less for the 5 semi-restricted
item types (0.67?0.80), compared to the 3 re-
stricted item types (0.51?0.83). As for automated
score correlations with human raters, the Pearson
r coefficients range from 0.34 (RA) to 0.67 (MC).
3
In an operational setting, these test takers would not re-
ceive a test score; instead, they would have the opportunity to
take the test again.
Again, the variability of Pearson r coefficients is
larger for the 3 restricted item types (0.34?0.67)
than for the 5 semi-restricted item types (0.43?
0.57). The degradation in correlation between the
inter-human results and the machine-human re-
sults varies from 0.16 (MC) to 0.37 (VI).
Speech recognition word error rate does not
seem to have a strong influence on model perfor-
mance (RA items have the lowest WER with S-
H1 r=0.34, but r=0.46 for IS items that have the
highest WER). However, we found other factors
that affect model performance negatively; for ex-
ample, multiple repeats of responses by test tak-
ers contribute to the large performance difference
between S-H1 and H1-H2 for the RP items. In
general, we conjecture that using features for a
larger set of sub-construct areas?in the case of
semi-restricted item types?may contribute to the
lower variation of scoring model performance for
this subset of the data.
As for speaker-level results (Table 5), the over-
all degradation between the inter-human correla-
tion and the system-human correlations is of a
similar magnitude (around 0.2) as observed for
most of the individual item types. Still, the
speaker-level correlation of 0.73 is 0.26 higher
than the average item type correlation between the
system and H1.
When we look into more detail at the Pearson
r correlations between individual features used in
the item type scoring models and human scores
(Table 6), we can see that features related to con-
tent accuracy exhibit a substantially stronger per-
formance (r=0.33?0.67) than features related to
most other sub-constructs of speaking proficiency,
namely fluency, pronunciation, prosody, and vo-
cabulary (r ? 0.2). One exception is features
related to grammar, where correlations with hu-
man scores are as high as 0.49. Since related work
on scoring speech using features indicative of flu-
ency, pronunciation, etc. showed higher correla-
tions (e.g., (Cucchiarini et al., 1997; Franco et al.,
2000; Zechner et al., 2009)), we conjecture that
the reason behind this difference is likely to be
found in the fact that the responses in this assess-
ment for teachers of English are quite short (6?
14 words on average for all items except for Read
Aloud items that are about 46 words on average).
Since content features are less reliant on longer
stretches of speech, they still work fairly well for
most items in our corpus.
140
Finally, while the proportion of words contained
in responses in restricted items is much larger than
those contained in responses in semi-restricted
items, these two item type categories are more
evenly distributed over the whole test, i.e., each
test taker responds to 9 semi-restricted and 12 re-
stricted items, and the item scores are then aggre-
gated for a final score with equal weight given to
each item score.
8 Conclusion
This paper presented an overview of an automated
speech scoring system that was developed for a
language assessment for teachers of English as a
Foreign Language (EFL) whose native language
is not English. We described the main compo-
nents of this prototype system and their perfor-
mance: the ASR system, features generated from
ASR output, a filtering model to flag non-scorable
responses, and finally a set of linear regression
models, one for each of 8 different types of test
items.
We found that overall, the correlation between
our speech scoring system?s predicted scores and
human rater scores range between 0.34 and 0.67,
evaluated on responses from 8 item types. Further-
more, we found that correlations based on com-
plete sets of 21 spoken responses per test taker im-
prove to around r = 0.73.
Given the many significant challenges of this
work, including 8 different item types in the as-
sessment, responses from speakers from different
native languages and speaking proficiency levels,
sub-optimal audio conditions for a part of the data,
and a relatively small data set for both ASR system
adaptation and linear regression model training,
we find that the overall performance achieved by
our automated speech scoring system was a good
starting point for an eventual deployment in a low-
stakes assessment context.
Future work will aim at improving the perfor-
mance of the prediction models by the addition of
more features addressing different aspects of the
construct as well as an improved filtering model
for flagging the different types of problematic re-
sponses. Furthermore, agreement between human
raters, in particular for read-aloud items, could be
improved by refining rater rubrics and additional
rater training and monitoring.
Acknowledgments
The authors would like to thank Anastassia Louk-
ina and Jidong Tao for their comments on an ear-
lier version of this paper, and are also indebted
to the anonymous reviewers of BEA-9 and ASRU
2013 for their valuable comments and suggestions.
References
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010a. Fluency and structural complexity as pre-
dictors of L2 oral proficiency. In Proceedings of In-
terspeech.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010b. Validating automated speaking tests. Lan-
guage Testing, 27(3):355?377.
Paul Boersma and David Weenick. 2012. Praat: Doing
phonetics by computer, version 5.3.32. http://
www.praat.org.
Abhishek Chandel, Abhinav Parate, Maymon Ma-
dathingal, Himanshu Pant, Nitendra Rajput, Shajith
Ikbal, Om Deshmuck, and Ashish Verma. 2007.
Sensei: Spoken language assessment for call cen-
ter agents. In Proceedings of the IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU).
Lei Chen and Su-Youn Yoon. 2012. Application of
structural events detected on ASR outputs for auto-
mated speaking assessment. In Proceedings of In-
terspeech.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722?
731.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven
assessment of non-native spontaneous speech. In
Proceedings of NAACL-HLT.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of Dutch pronunciation by us-
ing speech recognition technology. In Proceedings
of the IEEE Workshop on Auotmatic Speech Recog-
nition and Understanding (ASRU).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers? fluency by means of automatic speech recogni-
tion technology. Journal of the Acoustical Society of
America, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers? fluency: Comparisons between read and spon-
taneous speech. Journal of the Acoustical Society of
America, 111(6):2862?2873.
141
Maxine Eskenazi. 2009. An overview of spoken lan-
guage technology for education. Speech Communi-
cation, 51(10):832?844.
Horacio Franco, Leonardo Neumeyer, Vassilios Di-
galakis, and Orith Ronen. 2000. Combination of
machine scores for automatic grading of pronuncia-
tion quality. Speech Communication, 30(1-2):121?
130.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David M. Williamson. 2011. A three-stage ap-
proach to the automated scoring of spontaneous spo-
ken responses. Computer Speech and Language,
25(2):282?306.
Florian Hoenig. 2002. Automatic assessment of non-
native prosody ? Annotation, modelling, and evalu-
ation. In Proceedings of the International Sympo-
sium on Automatic Detection of Errors in Pronun-
ciation Training (ISADEPT), pages 21?30, Stock-
holm, Sweden.
Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of Interspeech.
Jack Mostow, Steven F. Roth, Alexander G. Haupt-
mann, and Matthew Kane. 1994. A prototype read-
ing coach that listens. In Proceedings of the Twelfth
National Conference on Artificial Intelligence.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, 30:83?93.
Pearson Education, Inc. 2011. Versant
TM
Aviation English Test. http://www.
versanttest.com/technology/
VersantAviationEnglishTestValidation.
pdf.
Silke Witt. 1999. Use of speech recognition in
computer-assisted language learning. Ph.D. thesis,
Cambridge University.
Xiaoming Xi, Derrick Higgins, Klaus Zechner, and
David M. Williamson. 2008. Automated scoring of
spontaneous speech using SpeechRater v1.0. Edu-
cational Testing Service Research Report RR-08-62.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103?111, Montr?eal, Canada. Asso-
ciation for Computational Linguistics.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners? syntactic competence based on sim-
ilarity measures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 600?608, Jeju Island, Korea.
Association for Computational Linguistics.
Su-Youn Yoon, Keelan Evanini, and Klaus Zechner.
2011. Non-scorable response detection for auto-
mated speaking proficiency assessment. In Proceed-
ings of NAACL-HLT Workshop on Innovative Use of
NLP for Building Educational Applications.
Su-Youn Yoon, Suma Bhat, and Klaus Zechner. 2012.
Vocabulary profile as a measure of vocabulary so-
phistication. In Proceedings of the 7th Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, NAACL-HLT, Montr?eal, Canada. Associ-
ation for Computational Linguistics.
Klaus Zechner and Xinhao Wang. 2013. Automated
content scoring of spoken responses in an assess-
ment for teachers of english. In Proceedings of
the 8th Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL-HLT,
Atlanta. Association for Computational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883?895.
Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011.
Evaluating prosodic features for automated scoring
of non-native read speech. In Proceedings of the
IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU).
142
