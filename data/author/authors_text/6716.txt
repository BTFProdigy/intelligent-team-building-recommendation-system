Proceedings of NAACL HLT 2007, pages 1?8,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Exploiting acoustic and syntactic features for prosody labeling in
a maximum entropy framework
Vivek Rangarajan, Shrikanth Narayanan
Speech Analysis and Interpretation Laboratory
University of Southern California
Viterbi School of Electrical Engineering
vrangara@usc.edu,shri@sipi.usc.edu
Srinivas Bangalore
AT&T Research Labs
180 Park Avenue
Florham Park, NJ 07932, U.S.A.
srini@research.att.com
Abstract
In this paper we describe an automatic
prosody labeling framework that exploits
both language and speech information.
We model the syntactic-prosodic informa-
tion with a maximum entropy model that
achieves an accuracy of 85.2% and 91.5%
for pitch accent and boundary tone la-
beling on the Boston University Radio
News corpus. We model the acoustic-
prosodic stream with two different mod-
els, one a maximum entropy model and
the other a traditional HMM. We finally
couple the syntactic-prosodic and acoustic-
prosodic components to achieve signifi-
cantly improved pitch accent and bound-
ary tone classification accuracies of 86.0%
and 93.1% respectively. Similar experimen-
tal results are also reported on Boston Di-
rections corpus.
1 Introduction
Prosody refers to intonation, rhythm and lexical
stress patterns of spoken language that convey lin-
guistic and paralinguistic information such as em-
phasis, intent, attitude and emotion of a speaker.
Prosodic information associated with a unit of
speech, say, syllable, word, phrase or clause, influ-
ence all the segments of the unit in an utterance. In
this sense they are also referred to as suprasegmen-
tals (Lehiste, 1970). Prosody in general is highly
dependent on individual speaker style, gender, di-
alect and other phonological factors. The difficulty in
reliably characterizing suprasegmental information
present in speech has resulted in symbolic and para-
meteric prosody labeling standards like ToBI (Tones
and Break Indices) (Silverman et al, 1992) and Tilt
model (Taylor, 1998) respectively.
Prosody in spoken language can be characterized
through acoustic features or lexical features or both.
Acoustic correlates of duration, intensity and pitch,
like syllable nuclei duration, short time energy and
fundamental frequency (f0) are some acoustic fea-
tures that are perceived to confer prosodic promi-
nence or stress in English. Lexical features like parts-
of-speech, syllable nuclei identity, syllable stress of
neighboring words have also demonstrated high de-
gree of discriminatory evidence in prosody detection
tasks.
The interplay between acoustic and lexical fea-
tures in characterizing prosodic events has been suc-
cessfully exploited in text-to-speech synthesis (Bu-
lyko and Ostendorf, 2001; Ma et al, 2003), speech
recognition (Hasegawa-Johnson et al, 2005) and
speech understanding (Wightman and Ostendorf,
1994). Text-to-speech synthesis relies on lexical fea-
tures derived predominantly from the input text to
synthesize natural sounding speech with appropri-
ate prosody. In contrast, output of a typical auto-
matic speech recognition (ASR) system is noisy and
hence, the acoustic features are more useful in pre-
dicting prosody than the hypothesized lexical tran-
script which may be erroneous. Speech understand-
ing systems model both the lexical and acoustic fea-
tures at the output of an ASR to improve natural
language understanding. Another source of renewed
interest has come from spoken language translation
(No?th et al, 2000; Agu?ero et al, 2006). A pre-
requisite for all these applications is accurate prosody
detection, the topic of the present work.
In this paper, we describe our framework for build-
ing an automatic prosody labeler for English. We
report results on the Boston University (BU) Ra-
dio Speech Corpus (Ostendorf et al, 1995) and
Boston Directions Corpus (BDC) (Hirschberg and
Nakatani, 1996), two publicly available speech cor-
pora with manual ToBI annotations intended for ex-
periments in automatic prosody labeling. We con-
dition prosody not only on word strings and their
parts-of-speech but also on richer syntactic informa-
tion encapsulated in the form of Supertags (Banga-
lore and Joshi, 1999). We propose a maximum en-
tropy modeling framework for the syntactic features.
We model the acoustic-prosodic stream with two dif-
ferent models, a maximum entropy model and a more
traditional hidden markov model (HMM). In an au-
tomatic prosody labeling task, one is essentially try-
1
ing to predict the correct prosody label sequence for
a given utterance and a maximum entropy model of-
fers an elegant solution to this learning problem. The
framework is also robust in the selection of discrim-
inative features for the classification problem. So,
given a word sequence W = {w1, ? ? ? , wn} and a set
of acoustic-prosodic features A = {o1, ? ? ? , oT }, the
best prosodic label sequence L? = {l1, l2, ? ? ? , ln} is
obtained as follows,
L? = argmax
L
P (L|A,W ) (1)
= argmax
L
P (L|W ).P (A|L,W ) (2)
? argmax
L
P (L|?(W )).P (A|L,W ) (3)
where ?(W ) is the syntactic feature encoding of the
word sequence W . The first term in Equation (3)
corresponds to the probability obtained through our
maximum entropy syntactic model. The second term
in Equation (3), computed by an HMM corresponds
to the probability of the acoustic data stream which
is assumed to be dependent only on the prosodic la-
bel sequence.
The paper is organized as follows. In section 2
we describe related work in automatic prosody la-
beling followed by a description of the data used in
our experiments in section 3. We present prosody
prediction results from off-the-shelf synthesizers in
section 4. Section 5 details our proposed maximum
entropy syntactic-prosodic model for prosody label-
ing. In section 6, we describe our acoustic-prosodic
model and discuss our results in section 7. We finally
conclude in section 8 with directions for future work.
2 Related work
Automatic prosody labeling has been an active re-
search topic for over a decade. Wightman and Os-
tendorf (Wightman and Ostendorf, 1994) developed
a decision-tree algorithm for labeling prosodic pat-
terns. The algorithm detected phrasal prominence
and boundary tones at the syllable level. Bulyko
and Ostendorf (Bulyko and Ostendorf, 2001) used
a prosody prediction module to synthesize natural
speech with appropriate prosody. Verbmobil (No?th
et al, 2000) incorporated prosodic labeling into a
translation framework for improved linguistic analy-
sis and speech understanding.
Prosody has typically been represented either sym-
bolically, e.g., ToBI (Silverman et al, 1992) or
parametrically, e.g., Tilt Intonation Model (Tay-
lor, 1998). Parametric approaches either restrict
the variants of prosody by definition or automati-
cally learn prosodic patterns from data (Agu?ero et
al., 2006). The BU corpus is a widely used cor-
pus with symbolic representation of prosody. The
hand-labeled ToBI annotations make this an attrac-
tive corpus to perform prosody labeling experiments.
The main drawback of this corpus is that it com-
prises only read speech. Prosody labeling on sponta-
neous speech corpora like Boston Directions corpus
(BDC), Switchboard (SWBD) has garnered atten-
tion in (Hirschberg and Nakatani, 1996; Gregory and
Altun, 2004).
Automatic prosody labeling has been achieved
through various machine learning techniques, such
as decision trees (Hirschberg, 1993; Wightman and
Ostendorf, 1994; Ma et al, 2003), rule-based sys-
tems (Shimei and McKeown, 1999), bagging and
boosting on CART (Sun, 2002), hidden markov
models (Conkie et al, 1999), neural networks
(Hasegawa-Johnson et al, 2005),maximum-entropy
models (Brenier et al, 2005) and conditional ran-
dom fields (Gregory and Altun, 2004).
Prosody labeling of the BU corpus has been re-
ported in many studies (Hirschberg, 1993; Hasegawa-
Johnson et al, 2005; Ananthakrishnan and
Narayanan, 2005). Hirschberg (Hirschberg, 1993)
used a decision-tree based system that achieved
82.4% speaker dependent accent labeling accuracy
at the word level on the BU corpus using lexical fea-
tures. (Ross and Ostendorf, 1996) also used an ap-
proach similar to (Wightman and Ostendorf, 1994)
to predict prosody for a TTS system from lexical fea-
tures. Pitch accent accuracy at the word-level was
reported to be 82.5% and syllable-level accent accu-
racy was 80.2%. (Hasegawa-Johnson et al, 2005)
proposed a neural network based syntactic-prosodic
model and a gaussian mixture model based acoustic-
prosodic model to predict accent and boundary tones
on the BU corpus that achieved 84.2% accuracy in
accent prediction and 93.0% accuracy in intonational
boundary prediction. With syntactic information
alone they achieved 82.7% and 90.1% for accent and
boundary prediction, respectively. (Ananthakrish-
nan and Narayanan, 2005) modeled the acoustic-
prosodic information using a coupled hidden markov
model that modeled the asynchrony between the
acoustic streams. The pitch accent and boundary
tone detection accuracy at the syllable level were
75% and 88% respectively. Our proposed maximum
entropy syntactic model outperforms previous work.
On the BU corpus, with syntactic information alone
we achieve pitch accent and boundary tone accuracy
of 85.2% and 91.5% on the same training and test
sets used in (Chen et al, 2004; Hasegawa-Johnson
et al, 2005). Further, the coupled model with both
acoustic and syntactic information results in accura-
cies of 86.0% and 93.1% respectively. On the BDC
corpus, we achieve pitch accent and boundary tone
accuracies of 79.8% and 90.3%.
3 Data
The BU corpus consists of broadcast news stories in-
cluding original radio broadcasts and laboratory sim-
2
BU BDC
Corpus statistics f2b f1a m1b m2b h1 h2 h3 h4
# Utterances 165 69 72 51 10 9 9 9
# words (w/o punc) 12608 3681 5058 3608 2234 4127 1456 3008
# pitch accents 6874 2099 2706 2016 1006 1573 678 1333
# boundary tones (w IP) 3916 1059 1282 1023 498 727 361 333
# boundary tones (w/o IP) 2793 684 771 652 308 428 245 216
Table 1: BU and BDC dataset used in experiments
ulations recorded from seven FM radio announcers.
The corpus is annotated with orthographic transcrip-
tion, automatically generated and hand-corrected
part-of-speech tags and automatic phone alignments.
A subset of the corpus is also hand annotated with
ToBI labels. In particular, the experiments in this
paper are carried out on 4 speakers similar to (Chen
et al, 2004), 2 male and 2 female referred to here-
after asm1b, m2b, f1a and f2b. The BDC corpus is
made up of elicited monologues produced by subjects
who were instructed to perform a series of direction-
giving tasks. Both spontaneous and read versions of
the speech are available for four speakers h1, h2, h3
and h4 with hand-annotated ToBI labels and auto-
matic phone alignments, similar to the BU corpus.
Table 1 shows some of the statistics of the speakers
in the BU and BDC corpora.
In Table 1, the pitch accent and boundary tone
statistics are obtained by decomposing the ToBI la-
bels into binary classes using the mapping shown in
Table 2.
BU Labels Intermediate Mapping Coarse Mapping
H*,!H*
L* Single Accent
*,*?,X*? accent
H+!H*,L+H*,L+!H* Bitonal Accent
L*+!H,L*+H
L-L%,!H-L%,H-L%
H-H% Final Boundary tone
L-H%
%?,X%?,%H btone
L-,H-,!H- Intermediate Phrase (IP) boundary
-X?,-?
<,>,no label none none
Table 2: ToBI label mapping used in experiments
In all our prosody labeling experiments we adopt
a leave-one-out speaker validation similar to the
method in (Hasegawa-Johnson et al, 2005) for the
four speakers with data from one speaker for testing
and from the other three for training. For the BU
corpus, f2b speaker was always used in the training
set since it contains the most data. In addition to
performing experiments on all the utterances in BU
corpus, we also perform identical experiments on the
train and test sets reported in (Chen et al, 2004)
which is referred to as Hasegawa-Johnson et al set.
4 Baseline Experiments
We present three baseline experiments. One is sim-
ply based on chance where the majority class label is
predicted. The second is a baseline only for pitch ac-
cents derived from the lexical stress obtained through
look-up from a pronunciation lexicon labeled with
stress. Finally, the third and more concrete base-
line is obtained through prosody detection in current
speech synthesis systems.
4.1 Prosody labels derived from lexical
stress
Pitch accents are usually carried by the stressed syl-
lable in a particular word. Lexicons with phonetic
transcription and lexical stress are available in many
languages. Hence, one can use these lexical stress
markers within the syllables and evaluate the corre-
lation with pitch accents. Eventhough the lexicon
has a closed vocabulary, letter-to-sound rules can be
derived from it for unseen words. For each word car-
rying a pitch accent, we find the particular syllable
where the pitch accent occurs from the manual anno-
tation. For the same syllable, we predict pitch accent
based on the presence or absence of a lexical stress
marker in the phonetic transcription. The results are
presented in Table 3.
4.2 Prosody labeling with Festival and
AT&T Natural Voices R? Speech
Synthesizer
Festival (Black et al, 1998) and AT&T Natural
Voices R? (NV) speech synthesizer (att, ) are two
publicly available speech synthesizers that have a
prosody prediction module available. We performed
automatic prosody labeling using the two synthesiz-
ers to get a baseline.
4.2.1 AT&T Natural Voices R? Speech
Synthesizer
The AT&T NV R? speech synthesizer is a half
phone speech synthesizer. The toolkit accepts
an input text utterance and predicts appropriate
ToBI pitch accent and boundary tones for each of
3
Pitch accent Boundary tone
Corpus Speaker Set Prediction Module Chance Accuracy Chance Accuracy
Lexical stress 54.33 72.64 - -
Entire Set AT&T Natural Voices 54.33 81.51 81.14 89.10
Festival 54.33 69.55 81.14 89.54
Lexical stress 56.53 74.10 - -
BU Hasegawa-Johnson et al set AT&T Natural Voices 56.53 81.73 82.88 89.67
Festival 56.53 68.65 82.88 90.21
Lexical stress 57.60 67.42 - -
BDC Entire Set AT&T Natural Voices 57.60 68.49 88.90 84.90
Festival 57.60 64.94 88.90 85.17
Table 3: Classification results of pitch accents and boundary tones (in %) using Festival and AT&T NV R? synthesizer
the selected units (in this case, a pair of phones)
from the database. We reverse mapped the se-
lected half phone units to words, thus obtaining
the ToBI labels for each word in the input utter-
ance. The toolkit uses a rule-based procedure to
predict the ToBI labels from lexical information.
The pitch accent labels predicted by the toolkit are
Laccent  {H?,L?,none} and the boundary tones
are Lbtone  {L-L%,H-H%,L-H%,none}.
4.2.2 Festival Speech Synthesizer
Festival (Black et al, 1998) is an open-source unit
selection speech synthesizer. The toolkit includes
a CART-based prediction system that can predict
ToBI pitch accents and boundary tones for the input
text utterance. The pitch accent labels predicted by
the toolkit are Laccent  {H?,L+H?, !H?,none}
and the boundary tones are
Lbtone  {L-L%,H-H%,L-H%,none}. The
prosody labeling results obtained through both the
speech synthesis engines are presented in Table
3. The chance column in Table 3 is obtained by
predicting the most frequent label in the data set.
In the next sections, we describe our proposed
maximum entropy based syntactic model and HMM
based acoustic-prosodic model for automatic prosody
labeling.
5 Syntactic-prosodic Model
We propose a maximum entropy approach to model
the words, syntactic information and the prosodic
labels as a sequence. We model the prediction prob-
lem as a classification task as follows: given a se-
quence of words wi in a sentence W = {w1, ? ? ? , wn}
and a prosodic label vocabulary (li  L), we need
to predict the best prosodic label sequence L? =
{l1, l2, ? ? ? , ln}. We approximate the conditional
probability to be within a bounded n-gram context.
Thus,
L? = argmax
L
P (L|W,T, S) (4)
? argmax
L
n?
i
p(li|w
i+k
i?k, t
i+k
i?k, s
i+k
i?k) (5)
where W = {w1, ? ? ? , wn} is the word sequence and
T = {t1, ? ? ? , tn}, S = {s1, ? ? ? , sn} are the corre-
sponding part-of-speech and additional syntactic in-
formation sequences. The variable k controls the
context.
The BU corpus is automatically labeled (and
hand-corrected) with part-of-speech (POS) tags.
The POS inventory is the same as the Penn treebank
which includes 47 POS tags: 22 open class categories,
14 closed class categories and 11 punctuation labels.
We also automatically tagged the utterances using
the AT&T POS tagger. The POS tags were mapped
to function and content word categories 1 which was
added as a discrete feature. In addition to the POS
tags, we also annotate the utterance with Supertags
(Bangalore and Joshi, 1999). Supertags encapsulate
predicate-argument information in a local structure.
They are composed with each other using substi-
tution and adjunction operations of Tree-Adjoining
Grammars (TAGs) to derive a dependency analysis
of an utterance and its predicate-argument structure.
Even though there is a potential to exploit the de-
pendency structure between supertags and prosody
labels as demonstrated in (Hirschberg and Rambow,
2001), for this paper we use only the supertag labels.
Finally, we generate one feature vector (?) for
each word in the data set (with local contextual fea-
tures). The best prosodic label sequence is then,
L? = argmax
L
n?
i
P (li|?) (6)
To estimate the conditional distribution P (li|?) we
use the general technique of choosing the maximum
entropy (maxent) distribution that estimates the av-
erage of each feature over the training data (Berger
et al, 1996). This can be written in terms of Gibbs
distribution parameterized with weights ?, where V
is the size of the prosodic label set. Hence,
P (li|?) =
e?li .?
?V
l=1 e
?li .?
(7)
1function and content word features were obtained
through a look-up table based on POS
4
k=3
Corpus Speaker Set Syntactic features accent btone
correct POS tags 84.75 91.39
Entire Set AT&T POS + supertags 84.59 91.34
BU Joint Model (w AT&T POS + supertags) 84.60 91.36
correct POS tags 85.22 91.33
Hasegawa-Johnson et al set AT&T POS + supertags 84.95 91.21
Joint Model (w AT&T POS + supertags) 84.78 91.54
BDC Entire Set AT&T POS + supertags 79.81 90.28
Joint Model (w AT&T POS + supertags) 79.57 89.76
Table 4: Classification results (%) of pitch accents and boundary tones for different syntactic representation (k = 3)
We use the machine learning toolkit LLAMA
(Haffner, 2006) to estimate the conditional distribu-
tion using maxent. LLAMA encodes multiclass max-
ent as binary maxent to increase the training speed
and to scale the method to large data sets. Each of
the V classes in the label set L is encoded as a bit
vector such that, in the vector for class i, the ith bit
is one and all other bits are zero. Finally, V one-
versus-other binary classifiers are used as follows.
P (y|?) = 1? P (y?|?) =
e?y.?
e?y.? + e?y?.?
(8)
where ?y? is the parameter vector for the anti-label y?.
To compute P (li|?), we use the class independence
assumption and require that yi = 1 and for all j 6=
i, yj = 0.
P (li|?) = P (yi|?)
V?
j 6=i
P (yj |?) (9)
5.1 Joint Modeling of Accents and
Boundary Tones
Prosodic prominence and phrasing can also be
viewed as joint events occurring simultaneously. Pre-
vious work by (Wightman and Ostendorf, 1994) sug-
gests that a joint labeling approach may be more
beneficial in prosody labeling. In this scenario,
we treat each word to have one of the four labels
li  L = {accent-btone, accent-none, none-
btone, none-none}. We trained the classifier on
the joint labels and then computed the error rates for
individual classes. The results of prosody prediction
using the set of syntactic-prosodic features for k = 3
is shown in Table 4. The joint modeling approach
provides a marginal improvement in the boundary
tone prediction but is slightly worse for pitch accent
prediction.
5.2 Supertagger performance on
Intermediate Phrase boundaries
Perceptual experiments have indicated that inter-
annotator agreement for ToBI intermediate phrase
boundaries is very low compared to full-intonational
boundaries (Syrdal and McGory, 2000). Interme-
diate phrasing is important in TTS applications to
synthesize appropriate short pauses to make the ut-
terance sound natural. The significance of syntactic
features in the boundary tone prediction prompted
us to examine the effect of predicting intermediate
phrase boundaries in isolation. It is intuitive to ex-
pect supertags to perform well in this task as they
essentially form a local dependency analysis on an
utterance and provide an encoding of the syntactic
phrasal information. We performed this task as a
three way classification where li  L = {btone, ip,
none}. The results of the classifier on IPs is shown
in Table 5.
Model Syntactic features IP accuracy
correct POS tags 83.25
k=2 (bigram context) AT&T POS tags 83.32
supertags 83.37
correct POS tags 83.30
k=3 (trigram context) AT&T POS tags 83.46
supertags 83.74
Table 5: Accuracy (in %) obtained by leave-one out
speaker validation using IPs as a separate class on
entire speaker set
6 Acoustic-prosodic model
We propose two approaches to modeling the
acoustic-prosodic features for prosody prediction.
First, we propose a maximum entropy framework
similar to the syntactic model where we quantize
the acoustic features and model them as discrete
sequences. Second, we use a more traditional ap-
proach where we train continuous observation den-
sity HMMs to represent pitch accents and bound-
ary tones. We first describe the features used in the
acoustic modeling followed by a more detailed de-
scription of the acoustic-prosodic model.
6.1 Acoustic-prosodic features
The BU corpus contains the corresponding acoustic-
prosodic feature file for each utterance. The f0, RMS
energy (e) of the utterance along with features for
5
Pitch accent Boundary tone
Corpus Speaker Set Model Acoustics Acoustics+syntax Acoustics Acoustics+syntax
Entire Set Maxent acoustic model 80.09 84.53 84.10 91.56
HMM acoustic model 70.58 85.13 71.28 92.91
BU Hasegawa-Johnson et al set Maxent acoustic model 80.12 84.84 82.70 91.76
HMM acoustic model 71.42 86.01 73.43 93.09
BDC Entire Set Maxent acoustic model 74.51 78.64 83.53 90.49
Table 6: Classification results of pitch accents and boundary tones (in %) with acoustics only and acoustics+syntax
using both our models
distinction between voiced/unvoiced segment, cross-
correlation values at estimated f0 value and ratio of
first two cross correlation values are computed over
10 msec frame intervals. In our experiments, we use
these values rather than computing them explicitly
which is straightforward with most audio toolkits.
Both the energy and the f0 levels were normalized
with speaker specific means and variances. Delta
and acceleration coefficients were also computed for
each frame. The final feature vector is 6-dimensional
comprising of f0, ?f0, ?2f0, e, ?e, ?2e per frame.
6.2 Maximum Entropy acoustic-prosodic
model
We propose a maximum entropy modeling frame-
work to model the continuous acoustic-prosodic ob-
servation sequence as a discrete sequence through
the means of quantization. The quantized acoustic
stream is then used as a feature vector and the condi-
tional probabilities are approximated by an n-gram
model. This is equivalent to reducing the vocabu-
lary of the acoustic-prosodic features and hence of-
fers better estimates of the conditional probabilities.
Such an n-gram model of quantized continuous fea-
tures is similar to representing the set of features
with a linear fit as done in the tilt intonational model
(Taylor, 1998).
The quantized acoustic-prosodic feature stream is
modeled with a maxent acoustic-prosodic model sim-
ilar to the one described in section 5. Finally, we ap-
pend the syntactic and acoustic features to model the
combined stream with the maxent acoustic-syntactic
model, where the objective criterion for maximiza-
tion is Equation (1). The pitch accent and bound-
ary tone prediction accuracies for quantization per-
formed by considering only the first decimal place
is reported in Table 6. As expected, we found the
classification accuracy to drop with increasing num-
ber of bins used in the quantization due to the small
amount of training data.
6.3 HMM acoustic-prosodic model
We also investigated the traditional HMM approach
to model the high variability exhibited by the
acoustic-prosodic features. First, we trained sepa-
rate context independent single state Gaussian mix-
ture density HMMs for pitch accents and boundary
tones in a generative framework. The label sequence
was decoded using the viterbi algorithm. Next, we
trained HMMs with 3 state left-to-right topology
with uniform segmentation. The segmentations need
to be uniform due to lack of an acoustic-prosodic
model trained on the features pertinent to our task
to obtain forced segmentation.
The final label sequence using the maximum en-
tropy syntactic-prosodic model and the HMM based
acoustic-prosodic model was obtained by combin-
ing the syntactic and acoustic probabilities shown in
Equation (3). The syntactic-prosodic maxent model
outputs a posterior probability for each class per
word. We formed a lattice out of this structure and
composed it with the lattice generated by the HMM
acoustic-prosodic model. The best path was chosen
from the composed lattice through a Viterbi search.
The acoustic-prosodic probability P (A|L,W ) was
raised by a power of ? to adjust the weighting be-
tween the acoustic and syntactic model. The value of
? was chosen as 0.008 and 0.015 for pitch accent and
boundary tone respectively, by tuning on the train-
ing set. The results of the acoustic-prosodic model
and the coupled model are shown in Table 6.
7 Discussion
The baseline experiment with lexical stress obtained
from a pronunciation lexicon for prediction of pitch
accent yields substantially higher accuracy than
chance. This could be particularly useful in resource-
limited languages where prosody labels are usually
not available but one has access to a reasonable lex-
icon with lexical stress markers. Off-the-shelf speech
synthesizers like Festival and AT&T speech synthe-
sizer perform reasonably well in pitch accent and
boundary tone prediction. AT&T speech synthesizer
performs better than Festival in pitch accent predic-
tion and the latter performs better in boundary tone
prediction. This can be attributed to better rules
in the AT&T synthesizer for pitch accent prediction.
Boundary tones are usually highly correlated with
punctuation and Festival seems to capture this well.
However, both these synthesizers generate a high de-
6
gree of false alarms.
Our syntactic-prosodic maximum entropy model
proposed in section 5 outperforms previously re-
ported results on pitch accent and boundary tone
classification. Much of the gain comes from the ro-
bustness of the maximum entropy modeling in cap-
turing the uncertainty in the classification task. Con-
sidering the inter-annotator agreement for ToBI la-
bels is only about 81% for pitch accents and 93% for
boundary tones, the maximum entropy framework is
able to capture the uncertainty present in manual an-
notation. The supertag feature offers additional dis-
criminative information over the part-of-speech tags
(also as shown by (Hirschberg and Rambow, 2001).
The maximum entropy acoustic-prosodic model
discussed in section 6.2 performs reasonably well in
isolation. This is a simple method and the quantiza-
tion resolution can be adjusted based on the amount
of data available for training. However, the model
does not perform as well when combined with the
syntactic features. We conjecture that the gener-
alization provided by the acoustic HMM model is
complementary to that provided by the maximum
entropy model, resulting in better accuracy when
combined together as compared to that of a maxent-
based acoustic and syntactic model.
The weighted maximum entropy syntactic-
prosodic model and HMM acoustic-prosodic model
performs the best in pitch accent and boundary tone
classification. The classification accuracies are as
good as the inter-annotator agreement for the ToBI
labels. Our HMM acoustic-prosodic model is a gen-
erative model and does not assume the knowledge
of word boundaries in predicting the prosodic labels
as in most approaches (Hirschberg, 1993; Wightman
and Ostendorf, 1994; Hasegawa-Johnson et al,
2005). This makes it possible to have true parallel
prosody prediction during speech recognition. The
weighted approach also offers flexibility in prosody
labeling for either speech synthesis or speech recog-
nition. While the syntactic-prosodic model would
be more discriminative for speech synthesis, the
acoustic-prosodic model is more appropriate for
speech recognition.
8 Conclusions and Future Work
In this paper, we described a maximum entropy
modeling framework for automatic prosody label-
ing. We presented two schemes for prosody label-
ing that utilize the acoustic and syntactic informa-
tion from the input utterance, a maximum entropy
model that models the acoustic-syntactic informa-
tion as a sequence and the other that combines the
maximum entropy syntactic-prosodic model and a
HMM based acoustic-prosodic model. We also used
enriched syntactic information in the form of su-
pertags in addition to POS tags. The supertags
provide an improvement in both the pitch accent
and boundary tone classification. Especially, in the
case where the input utterance is automatically POS
tagged (and not hand-corrected), supertags provide
a marginal but definite improvement in prosody la-
beling. The maximum entropy syntactic-prosodic
model alone resulted in pitch accent and bound-
ary tone accuracies of 85.2% and 91.5% on training
and test sets identical to (Chen et al, 2004). As
far as we know, these are the best results on the
BU corpus using syntactic information alone and a
train-test split that does not contain the same speak-
ers. The acoustic-syntactic maximum entropy model
performs better than its syntactic-prosodic counter-
part for the boundary tone case but is slightly worse
for pitch accent scenario partly due to the approx-
imation involved in quantization. But these results
are still better than the baseline results from out-
of-the-box speech synthesizers. Finally, our com-
bined maximum entropy syntactic-prosodic model
and HMM acoustic-prosodic model performs the best
with pitch accent and boundary tone labeling accu-
racies of 86.0% and 93.1% respectively.
As a continuation of our work, we are incorpo-
rating our automatic prosody labeler in a speech-
to-speech translation framework. Typically, state-
of-the-art speech translation systems have a source
language recognizer followed by a machine transla-
tion system. The translated text is then synthesized
in the target language with prosody predicted from
text. In this process, some of the critical prosodic
information present in the source data is lost during
translation. With reliable prosody labeling in the
source language, one can transfer the prosody to the
target language (this is feasible for languages with
phrase level correspondence). The prosody labels by
themselves may or may not improve the translation
accuracy but they provide a framework where one
can obtain prosody labels in the target language from
the speech signal rather than depending on a lexical
prosody prediction module in the target language.
Acknowledgements
We would like to thank Vincent Goffin, Stephan
Kanthak, Patrick Haffner, Enrico Bocchieri for their
support with acoustic modeling tools. We are also
thankful to Alistair Conkie, Yeon-Jun Kim, Ann
Syrdal and Julia Hirschberg for their help and guid-
ance with the synthesis components and ToBI label-
ing standard.
References
P. D. Agu?ero, J. Adell, and A. Bonafonte. 2006.
Prosody generation for speech-to-speech transla-
7
tion. In Proceedings of ICASSP, Toulouse, France,
May.
S. Ananthakrishnan and S. Narayanan. 2005. An au-
tomatic prosody recognizer using a coupled multi-
stream acoustic model and a syntactic-prosodic
language model. In In Proceedings of ICASSP,
Philadelphia, PA, March.
AT&T Natural Voices speech synthesizer.
http://www.naturalvoices.att.com.
S. Bangalore and A. K. Joshi. 1999. Supertagging:
An approach to almost parsing. Computational
Linguistics, 25(2), June.
A. Berger, S. D. Pietra, and V. D. Pietra. 1996. A
maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?
71.
A. W. Black, P. Taylor, and R. Caley.
1998. The Festival speech synthesis system.
http://festvox.org/festival.
J. M. Brenier, D. Cer, and D. Jurafsky. 2005. The
detection of emphatic words using acoustic and
lexical features. In In Proceedings of Eurospeech.
I. Bulyko and M. Ostendorf. 2001. Joint prosody
prediction and unit selection for concatenative
speech synthesis. In Proc. of ICASSP.
K. Chen, M. Hasegawa-Johnson, and A. Cohen.
2004. An automatic prosody labeling system using
ANN-based syntactic-prosodic model and GMM-
based acoustic-prosodic model. In Proceedings of
ICASSP.
A. Conkie, G. Riccardi, and R. C. Rose. 1999.
Prosody recognition from speech utterances using
acoustic and linguistic based models of prosodic
events. In Proc. Eurospeech, pages 523?526, Bu-
dapest, Hungary.
M. Gregory and Y. Altun. 2004. Using conditional
random fields to predict pitch accent in conver-
sational speech. In 42nd Annual Meeting of the
Association for Computational Linguistics (ACL).
P. Haffner. 2006. Scaling large margin classifiers for
spoken language understanding. Speech Commu-
nication, 48(iv):239?261.
M. Hasegawa-Johnson, K. Chen, J. Cole, S. Borys,
S. Kim, A. Cohen, T. Zhang, J. Choi, H. Kim,
T. Yoon, and S. Chavara. 2005. Simultaneous
recognition of words and prosody in the boston
university radio speech corpus. Speech Communi-
cation, 46:418?439.
J. Hirschberg and C. Nakatani. 1996. A prosodic
analysis of discourse segments in direction-giving
monologues. In Proceedings of the 34th confer-
ence on Association for Computational Linguis-
tics, pages 286?293.
J. Hirschberg and O. Rambow. 2001. Learning
prosodic features using a tree representation. In
Proceedings of Eurospeech, pages 1175?1180, Aal-
borg.
J. Hirschberg. 1993. Pitch accent in context: Pre-
dicting intonational prominence from text. Artifi-
cial Intelligence, 63(1-2).
I. Lehiste. 1970. Suprasegmentals. MIT Press, Cam-
bridge, MA.
X. Ma, W. Zhang, Q. Shi, W. Zhu, and L. Shen.
2003. Automatic prosody labeling using both
text and acoustic information. In Proceedings of
ICASSP, volume 1, pages 516?519, April.
E. No?th, A. Batliner, A. Kie?ling, R. Kompe, and
H. Niemann. 2000. VERBMOBIL: The use of
prosody in the linguistic components of a speech
understanding system. IEEE Transactions on
Speech and Audio processing, 8(5):519?532.
M. Ostendorf, P. J. Price, and S. Shattuck-Hufnagel.
1995. The Boston University Radio News Corpus.
Technical Report ECS-95-001, Boston University,
March.
K. Ross and M. Ostendorf. 1996. Prediction of ab-
stract prosodic labels for speech synthesis. Com-
puter Speech and Language, 10:155?185, Oct.
P. Shimei and K. McKeown. 1999. Word infor-
mativeness and automatic pitch accent modeling.
In In Proceedings of EMNLP/VLC, College Park,
Maryland.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert,
and J. Hirschberg. 1992. ToBI: A standard for la-
beling English prosody. In Proceedings of ICSLP,
pages 867?870.
X. Sun. 2002. Pitch accent prediction using ensem-
ble machine learning. In Proc. of ICSLP.
A. K. Syrdal and J. McGory. 2000. Inter-transcriber
reliability of tobi prosodic labeling. In Proc. IC-
SLP, pages 235?238, Beijing, China.
P. Taylor. 1998. The tilt intonation model. In Proc.
ICSLP, volume 4, pages 1383?1386.
C. W. Wightman and M. Ostendorf. 1994. Auto-
matic labeling of prosodic patterns. IEEE Trans-
actions on Speech and Audio Processing, 2(3):469?
481.
8
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 225?228,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Enriching spoken language translation with dialog acts
Vivek Kumar Rangarajan Sridhar
Shrikanth Narayanan
Speech Analysis and Interpretation Laboratory
University of Southern California
vrangara@usc.edu,shri@sipi.usc.edu
Srinivas Bangalore
AT&T Labs - Research
180 Park Avenue
Florham Park, NJ 07932, U.S.A.
srini@research.att.com
Abstract
Current statistical speech translation ap-
proaches predominantly rely on just text tran-
scripts and do not adequately utilize the
rich contextual information such as conveyed
through prosody and discourse function. In
this paper, we explore the role of context char-
acterized through dialog acts (DAs) in statis-
tical translation. We demonstrate the integra-
tion of the dialog acts in a phrase-based statis-
tical translation framework, employing 3 lim-
ited domain parallel corpora (Farsi-English,
Japanese-English and Chinese-English). For
all three language pairs, in addition to produc-
ing interpretable DA enriched target language
translations, we also obtain improvements in
terms of objective evaluation metrics such as
lexical selection accuracy and BLEU score.
1 Introduction
Recent approaches to statistical speech translation
have relied on improving translation quality with
the use of phrase translation (Och and Ney, 2003;
Koehn, 2004). The quality of phrase translation
is typically measured using n-gram precision based
metrics such as BLEU (Papineni et al, 2002) and
NIST scores. However, in many dialog based speech
translation scenarios, vital information beyond what
is robustly captured by words and phrases is car-
ried by the communicative act (e.g., question, ac-
knowledgement, etc.) representing the function of
the utterance. Our approach for incorporating di-
alog act tags in speech translation is motivated by
the fact that it is important to capture and convey
not only what is being communicated (the words)
but how something is being communicated (the con-
text). Augmenting current statistical translation
frameworks with dialog acts can potentially improve
translation quality and facilitate successful cross-
lingual interactions in terms of improved informa-
tion transfer.
Dialog act tags have been previously used in the
VERBMOBIL statistical speech-to-speech transla-
tion system (Reithinger et al, 1996). In that work,
the predicted DA tags were mainly used to improve
speech recognition, semantic evaluation, and infor-
mation extraction modules. Discourse information
in the form of speech acts has also been used in in-
terlingua translation systems (Mayfield et al, 1995)
to map input text to semantic concepts, which are
then translated to target text.
In contrast with previous work, in this paper we
demonstrate how dialog act tags can be directly ex-
ploited in phrase based statistical speech translation
systems (Koehn, 2004). The framework presented
in this paper is particularly suited for human-human
and human-computer interactions in a dialog set-
ting, where information loss due to erroneous con-
tent may be compensated to some extent through the
correct transfer of the appropriate dialog act. The
dialog acts can also be potentially used for impart-
ing correct utterance level intonation during speech
synthesis in the target language. Figure 1 shows an
example where the detection and transfer of dialog
act information is beneficial in resolving ambiguous
intention associated with the translation output.
Figure 1: Example of speech translation output enriched with
dialog act
The remainder of this paper is organized as fol-
lows: Section 2 describes the dialog act tagger used
in this work, Section 3 formulates the problem, Sec-
tion 4 describes the parallel corpora used in our ex-
periments, Section 5 summarizes our experimental
results and Section 6 concludes the paper with a
brief discussion and outline for future work.
2 Dialog act tagger
In this work, we use a dialog act tagger trained on
the Switchboard DAMSL corpus (Jurafsky et al,
225
1998) using a maximum entropy (maxent) model.
The Switchboard-DAMSL (SWBD-DAMSL) cor-
pus consists of 1155 dialogs and 218,898 utterances
from the Switchboard corpus of telephone conver-
sations, tagged with discourse labels from a shal-
low discourse tagset. The original tagset of 375
unique tags was clustered to obtain 42 dialog tags
as in (Jurafsky et al, 1998). In addition, we also
grouped the 42 tags into 7 disjoint classes, based
on the frequency of the classes and grouped the re-
maining classes into an ?Other? category constitut-
ing less than 3% of the entire data. The simplified
tagset consisted of the following classes: statement,
acknowledgment, abandoned, agreement, question,
appreciation, other.
We use a maximum entropy sequence tagging
model for the automatic DA tagging. Given a se-
quence of utterances U = u1, u2, ? ? ? , un and a
dialog act vocabulary (di ? D, |D| = K), we
need to assign the best dialog act sequence D? =
d1, d2, ? ? ? , dn. The classifier is used to assign to
each utterance a dialog act label conditioned on a
vector of local contextual feature vectors comprising
the lexical, syntactic and acoustic information. We
used the machine learning toolkit LLAMA (Haffner,
2006) to estimate the conditional distribution using
maxent. The performance of the maxent dialog act
tagger on a test set comprising 29K utterances of
SWBD-DAMSL is shown in Table 1.
Accuracy (%)
Cues used (current utterance) 42 tags 7 tags
Lexical 69.7 81.9
Lexical+Syntactic 70.0 82.4
Lexical+Syntactic+Prosodic 70.4 82.9
Table 1: Dialog act tagging accuracies for various cues on the
SWBD-DAMSL corpus.
3 Enriched translation using DAs
If Ss, Ts and St, Tt are the speech signals and equiv-
alent textual transcription in the source and target
language, and Ls the enriched representation for the
source speech, we formalize our proposed enriched
S2S translation in the following manner:
S?t = argmaxSt
P (St|Ss) (1)
P (St|Ss) =
?
Tt,Ts,Ls
P (St, Tt, Ts, Ls|Ss) (2)
?
?
Tt,Ts,Ls
P (St|Tt, Ls).P (Tt, Ts, Ls|Ss) (3)
where Eq.(3) is obtained through conditional inde-
pendence assumptions. Even though the recogni-
tion and translation can be performed jointly (Ma-
tusov et al, 2005), typical S2S translation frame-
works compartmentalize the ASR, MT and TTS,
with each component maximized for performance
individually.
max
St
P (St|Ss) ? maxSt P (St|T
?
t , L?s)
?max
Tt
P (Tt|T ?s , L?s) (4)
?max
Ls
P (Ls|T ?s , Ss)?maxTs P (Ts|Ss)
where T ?s , T ?t and S?t are the arguments maximiz-
ing each of the individual components in the transla-
tion engine. L?s is the rich annotation detected from
the source speech signal and text, Ss and T ?s respec-
tively. In this work, we do not address the speech
synthesis part and assume that we have access to the
reference transcripts or 1-best recognition hypothe-
sis of the source utterances. The rich annotations
(Ls) can be syntactic or semantic concepts (Gu et
al., 2006), prosody (Agu?ero et al, 2006), or, as in
this work, dialog act tags.
3.1 Phrase-based translation with dialog acts
One of the currently popular and predominant
schemes for statistical translation is the phrase-
based approach (Koehn, 2004). Typical phrase-
based SMT approaches obtain word-level align-
ments from a bilingual corpus using tools such as
GIZA++ (Och and Ney, 2003) and extract phrase
translation pairs from the bilingual word alignment
using heuristics. Suppose, the SMT had access to
source language dialog acts (Ls), the translation
problem may be reformulated as,
T ?t = argmaxTt
P (Tt|Ts, Ls)
= argmax
Tt
P (Ts|Tt, Ls).P (Tt|Ls) (5)
The first term in Eq.(5) corresponds to a dialog act
specific MT model and the second term to a dia-
log act specific language model. Given sufficient
amount of training data such a system can possibly
generate hypotheses that are more accurate than the
scheme without the use of dialog acts. However, for
small scale and limited domain applications, Eq.(5)
leads to an implicit partitioning of the data corpus
226
Training Test
Farsi Eng Jap Eng Chinese Eng Farsi Eng Jap Eng Chinese Eng
Sentences 8066 12239 46311 925 604 506
Running words 76321 86756 64096 77959 351060 376615 5442 6073 4619 6028 3826 3897
Vocabulary 6140 3908 4271 2079 11178 11232 1487 1103 926 567 931 898
Singletons 2819 1508 2749 1156 4348 4866 903 573 638 316 600 931
Table 2: Statistics of the training and test data used in the experiments.
and might generate inferioir translations in terms of
lexical selection accuracy or BLEU score.
A natural step to overcome the sparsity issue is
to employ an appropriate back-off mechanism that
would exploit the phrase translation pairs derived
from the complete data. A typical phrase transla-
tion table consists of 5 phrase translation scores for
each pair of phrases, source-to-target phrase transla-
tion probability (?1), target-to-source phrase transla-
tion probability (?2), source-to-target lexical weight
(?3), target-to-word lexical weight (?4) and phrase
penalty (?5= 2.718). The lexical weights are the
product of word translation probabilities obtained
from the word alignments. To each phrase trans-
lation table belonging to a particular DA-specific
translation model, we append those entries from the
baseline model that are not present in phrase table
of the DA-specific translation model. The appended
entries are weighted by a factor ?.
(Ts ? Tt)L?s = (Ts ? Tt)Ls ? {?.(Ts ? Tt)
s.t. (Ts ? Tt) 6? (Ts ? Tt)Ls} (6)
where (Ts ? Tt) is a short-hand1 notation for a
phrase translation table. (Ts ? Tt)Ls is the DA-
specific phrase translation table, (Ts ? Tt) is the
phrase translation table constructed from entire data
and (Ts ? Tt)L?s is the newly interpolated phrase
translation table. The interpolation factor ? is used
to weight each of the four translation scores (phrase
translation and lexical probabilities for the bilan-
guage) with the phrase penalty remaining a con-
stant. Such a scheme ensures that phrase translation
pairs belonging to a specific DA model are weighted
higher and also ensures better coverage than a parti-
tioned data set.
4 Data
We report experiments on three different paral-
lel corpora: Farsi-English, Japanese-English and
1(Ts ? Tt) represents the mapping between source alpha-
bet sequences to target alhabet sequences, where every pair
(ts1, ? ? ? , tsn, tt1, ? ? ? , ttm) has a weight sequence ?1, ? ? ? , ?5
(five weights).
Chinese-English. The Farsi-English data used in
this paper was collected for human-mediated doctor-
patient mediated interactions in which an English
speaking doctor interacts with a Persian speaking
patient (Narayanan et al, 2006). We used a subset
of this corpus consisting of 9315 parallel sentences.
The Japanese-English parallel corpus is a part
of the ?How May I Help You? (HMIHY) (Gorin
et al, 1997) corpus of operator-customer conversa-
tions related to telephone services. The corpus con-
sists of 12239 parallel sentences. The conversations
are spontaneous even though the domain is lim-
ited. The Chinese-English corpus corresponds to the
IWSLT06 training and 2005 development set com-
prising 46K and 506 sentences respectively (Paul,
2006).
5 Experiments and Results
In all our experiments we assume that the same di-
alog act is shared by a parallel sentence pair. Thus,
even though the dialog act prediction is performed
for English, we use the predicted dialog act as the di-
alog act for the source language sentence. We used
the Moses2 toolkit for statistical phrase-based trans-
lation. The language models were trigram models
created only from the training portion of each cor-
pus. Due to the relatively small size of the corpora
used in the experiments, we could not devote a sep-
arate development set for tuning the parameters of
the phrase-based translation scheme. Hence, the ex-
periments are strictly performed on the training and
test sets reported in Table 23.
The lexical selection accuracy and BLEU scores
for the three parallel corpora is presented in Table 3.
Lexical selection accuracy is measured in terms of
the F-measure derived from recall ( |Res?Ref ||Ref | ? 100)
and precision ( |Res?Ref ||Res| ? 100), where Ref is the
set of words in the reference translation and Res is
2http://www.statmt.org/moses
3A very small subset of the data was reserved for optimizing
the interpolation factor (?) described in Section 3.1
227
F-score (%) BLEU (%)
w/o DA tags w/ DA tags w/o DA tags w/ DA tags
Language pair 7tags 42tags 7tags 42tags
Farsi-English 56.46 57.32 57.74 22.90 23.50 23.75
Japanese-English 79.05 79.40 79.51 54.15 54.21 54.32
Chinese-English 65.85 67.24 67.49 48.59 52.12 53.04
Table 3: F-measure and BLEU scores with and without use of dialog act tags.
the set of words in the translation output. Adding di-
alog act tags (either 7 or 42 tag vocabulary) consis-
tently improves both the lexical selection accuracy
and BLEU score for all the language pairs. The im-
provements for Farsi-English and Chinese-English
corpora are more pronounced than the improve-
ments in Japanese-English corpus. This is due to the
skewed distribution of dialog acts in the Japanese-
English corpus; 80% of the test data are statements
while other and questions category make up 16%
and 3.5% of the data respectively. The important
observation here is that, appending DA tags in the
form described in this work, can improve translation
performance even in terms of conventional objective
evaluation metrics. However, the performance gain
measured in terms of objective metrics that are de-
signed to reflect only the orthographic accuracy dur-
ing translation is not a complete evaluation of the
translation quality of the proposed framework. We
are currently planning of adding human evaluation
to bring to fore the usefulness of such rich anno-
tations in interpreting and supplementing typically
noisy translations.
6 Discussion and Future Work
It is important to note that the dialog act tags used
in our translation system are predictions from the
maxent based DA tagger described in Section 2. We
do not have access to the reference tags; thus, some
amount of error is to be expected in the DA tagging.
Despite the lack of reference DA tags, we are still
able to achieve modest improvements in the trans-
lation quality. Improving the current DA tagger and
developing suitable adaptation techniques are part of
future work.
While we have demonstrated here that using dia-
log act tags can improve translation quality in terms
of word based automatic evaluation metrics, the real
benefits of such a scheme would be attested through
further human evaluations. We are currently work-
ing on conducting subjective evaluations.
References
P. D. Agu?ero, J. Adell, and A. Bonafonte. 2006. Prosody
generation for speech-to-speech translation. In Proc.
of ICASSP, Toulouse, France, May.
A. Gorin, G. Riccardi, and J. Wright. 1997. How May I
Help You? Speech Communication, 23:113?127.
L. Gu, Y. Gao, F. H. Liu, and M. Picheny. 2006.
Concept-based speech-to-speech translation using
maximum entropy models for statistical natural con-
cept generation. IEEE Transactions on Audio, Speech
and Language Processing, 14(2):377?392, March.
P. Haffner. 2006. Scaling large margin classifiers for spo-
ken language understanding. Speech Communication,
48(iv):239?261.
D. Jurafsky, R. Bates, N. Coccaro, R. Martin, M. Meteer,
K. Ries, E. Shriberg, S. Stolcke, P. Taylor, and C. Van
Ess-Dykema. 1998. Switchboard discourse language
modeling project report. Technical report research
note 30, Johns Hopkins University, Baltimore, MD.
P. Koehn. 2004. Pharaoh: A beam search decoder for
phrasebased statistical machine translation models. In
Proc. of AMTA-04, pages 115?124.
E. Matusov, S. Kanthak, and H. Ney. 2005. On the in-
tegration of speech recognition and statistical machine
translation. In Proc. of Eurospeech.
L. Mayfield, M. Gavalda, W. Ward, and A. Waibel.
1995. Concept-based speech translation. In Proc. of
ICASSP, volume 1, pages 97?100, May.
S. Narayanan et al 2006. Speech recognition engineer-
ing issues in speech to speech translation system de-
sign for low resource languages and domains. In Proc.
of ICASSP, Toulose, France, May.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. Technical report, IBM T.J. Watson Re-
search Center.
M. Paul. 2006. Overview of the IWSLT 2006 Evaluation
Campaign. In Proc. of the IWSLT, pages 1?15, Kyoto,
Japan.
N. Reithinger, R. Engel, M. Kipp, and M. Klesen. 1996.
Predicting dialogue acts for a speech-to-speech trans-
lation system. In Proc. of ICSLP, volume 2, pages
654?657, Oct.
228
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 974?983, Dublin, Ireland, August 23-29 2014.
A Framework for Translating SMS Messages
Vivek Kumar Rangarajan Sridhar, John Chen, Srinivas Bangalore, Ron Shacham
AT&T Labs
1 AT&T Way, Bedminster, NJ 07921
vkumar,jchen,srini,rshacham@research.att.com
Abstract
Short Messaging Service (SMS) has become a popular form of communication. While it is
predominantly used for monolingual communication, it can be extremely useful for facilitating
cross-lingual communication through statistical machine translation. In this work we present an
application of statistical machine translation to SMS messages. We decouple the SMS transla-
tion task into normalization followed by translation so that one can exploit existing bitext re-
sources and present a novel unsupervised normalization approach using distributed representa-
tion of words learned through neural networks. We describe several surrogate data that are good
approximations to real SMS data feeds and use a hybrid translation approach using finite-state
transducers. Both objective and subjective evaluation indicate that our approach is highly suitable
for translating SMS messages.
1 Introduction
The preferred form of communication has been changing over time with advances in communication
technology. The majority of the world?s population now owns a mobile device and an ever increasing
fraction of users are resorting to Short Message Service (SMS) as the primary form of communication.
SMS offers an easy, convenient and condensed form of communication that is being embraced by
the younger demographic. Due to the inherent limit in the length of a message that can be transmitted,
SMS users have adopted several shorthand notations to compress the message; some that have become
standardized and many that are invented constantly. While SMS is predominantly used in a monolingual
mode, it has the potential to connect people speaking different languages. However, translating SMS
messages has several challenges ranging from the procurement of data in this domain to dealing with
noisy text (abbreviations, spelling errors, lack of punctuation, etc.) that is typically detrimental to trans-
lation quality. In this work we address all the elements involved in building a cross-lingual SMS service
that spans data acquisition, normalization, translation modeling, messaging infrastructure and user trial.
The rest of the paper is organized as follows. In Section 4, we present a variety of channels through
which we compiled SMS data followed by a description of our pipeline in Section 5 that includes nor-
malization, phrase segmentation and machine translation. Finally, we describe a SMS translation service
built using our pipeline in Section 6 along with results from a user trial. We provide some discussion in
Section 7 and conclude in Section 8.
2 Related Work
One of the main challenges of building a machine translation system for SMS messages is the lack of
training data in this domain. Typically, there are several legal restrictions in using consumer SMS data
that precludes one from either using it completely or forces one to use it in limited capacity. Only a
handful of such corpora are publicly available on the Web (Chen and Kan, 2013; Fairon and Paumier,
2006; Treurniet et al., 2012; Sanders, 2012; Tagg, 2009); they are limited in size and restricted to a few
language pairs.
The NUS SMS corpus (Chen and Kan, 2013) is probably the largest English SMS corpus consisting of
around 41000 messages. However, these messages are characteristic of Singaporean chat lingo and not
an accurate reflection of SMS style in other parts of the world. A corpus of 30000 French SMS messages
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
974
was collected in (Fairon and Paumier, 2006) to study the idiosyncrasies of SMS language in comparison
with standard French. More recently, (Pennell and Liu, 2011) have used twitter data as a surrogate
for SMS messages. Most of these previous efforts have focused on normalization, i.e., translation of
SMS text to canonical text while we are interested in translating SMS messages from one language into
another (Eidelman et al., 2011).
Several works have addressed the problem of normalizing SMS text. A majority of these works have
used statistical machine translation (character-level) to translate SMS text into standard text (Pennell and
Liu, 2011; Aw et al., 2009; Kobus et al., 2008). (Beaufort et al., 2010) used a finite-state framework
to learn the mapping between SMS and canonical form. A beam search decoder for normalizing social
media text was presented in (Wang and Tou Ng, 2013). All these approaches rely on supervised train-
ing data to train the normalization model. In contrast, we use an unsupervised approach to learn the
normalization lexicon of word forms in SMS to standard text.
While several works have addressed the problem of normalizing SMS using machine translation, there
has been little to no work on the translation of SMS messages across languages on a large scale. Machine
translation of instant messages from English-to-Spanish was proposed in (Bangalore et al., 2002) where
multiple translation hypotheses from several off-the-shelf translation engines were combined using con-
sensus decoding. However, the approach did not consider any specific strategies for normalization and
the fidelity of training bitext is questionable since it was obtained using automatic machine translation.
Several products that enable multilingual communication with the aid of machine translation in con-
ventional chat, email, etc., are available in the market. However, most of these models are trained on
relatively clean bitext.
3 Problem Formulation
The objective in SMS translation is to translate a foreign sentence f
sms
= f
sms
1
, ? ? ? , f
sms
J
into target
(English) sentence e = e
I
1
= e
1
, ? ? ? , e
I
. In general it is hard to procure such SMS bitext due to lack
of data and high cost of annotation. However, we typically have access to bitext in non-SMS domain.
Let f = f
1
, ? ? ? , f
J
be the normalized version of the SMS input sentence. Given f
sms
, we choose the
sentence with highest probability among all possible target sentences,
?
e(f
sms
) = argmax
e
{P(e|f
sms
)} (1)
P (e|f
sms
) ? P (e)
?
f
P (f
sms
, f |e) (2)
= P (e)
?
f
P (f
sms
|f , e)P (f |e) (3)
If one applies the max-sum approximation and assumes that P (f
sms
|f , e) is independent of e,
?
e(f
sms
) = argmax
e
P (f
?
|e)P (e) (4)
where f
?
= argmax
f
P (f
sms
|f). Hence, the SMS translation problem can be decoupled into normal-
ization followed by statistical machine translation
1
.
4 Data
Typically, one has access to a large corpus of general bitext {f , e} while data from the SMS domain
{f
sms
, e} is sparse. Compiling a large corpus of SMS messages is not straightforward as there are
several restrictions on the use of consumer SMS data. We are not aware of any large monolingual or
bilingual corpus of true SMS messages besides those mentioned in Section 2. To compile a corpus of
SMS messages, we used three sources of data: transcriptions of speech-based SMS collected through
1
One can also use a lattice output from the normalization to jointly optimize over e and f
975
smartphones, data collected through Amazon Mechanical Turk
2
and Twitter
3
as a surrogate for SMS-
like messages. We describe the composition of each of these data sources in the following subsections.
Corpus Message #count Corpus Message #count
i love you 988157 ily2
hello 881635 n a meeting
hi 607536 Amazon Mechanical Turk check facebook N/A
how are you 470999 kewl
Speech SMS what?s up 251044 call u n a few
what are you doing 218289 lol 472556
where are you 191912 Twitter haha 232428
call 191430 lmao 102018
lol 105618 omg 709504
how?s it going 102977 thanks for the rt 300254
Table 1: Examples of English messages collected from various sources in this work
4.1 Speech-based SMS
In the absence of access to a real feed of SMS messages, we used transcription of speech-based SMS
messages collected through a smartphone application. A majority of these messages were collected
while the users used the application in their cars. We had access to a total of 41.3 million English and
2.4 million Spanish automatic transcriptions. To avoid the use of erroneous transcripts, we sorted the
messages by frequency and manually translated the top 40,000 English and 10,000 Spanish messages,
respectively. Our final English-Spanish bitext corpus from this source of data consisted of 50,000 parallel
sentences. Table 1 shows the high frequency messages in this dataset.
4.2 Amazon Mechanical Turk
The SMS messages from speech-based interaction does not consist of any shorthands or orthographic
errors as the decoding vocabulary of the automatic speech recognizer is fixed. We posted a task on
Amazon Mechanical Turk, where we took the speech-based SMS messages and asked the turkers to enter
three responses to each message as they would on a smartphone. We iteratively posted the responses from
the turkers as messages to obtain more messages. We obtained a total of 1000 messages in English and
Spanish, respectively. Unlike the speech data, the responses contained several shorthands.
4.3 Twitter
Twitter is used by a large number of users for broadcasting messages, opinions, etc. The language used in
Twitter is similar to SMS and contains plenty of shorthands, spelling errors even though it is typically not
directed towards another individual. We compiled a data set of Twitter messages that we subsequently
translated to obtain a bilingual corpus. We used the Twitter4j API
4
to stream Twitter data for a set of
keywords (function words) over a week. The raw data consisted of roughly 106 million tweets. Subse-
quently, we performed some basic normalization (removal of @user, #tags, filtering advertisements, web
addresses) to obtain SMS-like tweets. Finally, we sorted the data by frequency and picked the top 10000
tweets. Eliminating the tweets present in either of the two previous sources resulted in 6790 messages
that we manually translated.
5 Framework
The user input is first stripped of any accents (Spanish), segmented into short chunks using an automatic
punctuation classifier. Subsequently, any shorthand in the message is expanded out using expansion dic-
tionaries (constructed manually and automatically) and finally translated using a phrase-based translation
2
https://www.mturk.com
3
https://twitter.com
4
http://twitter4j.org/en/
976
model. Our framework allows the use of confusion networks in case of ambiguous shorthand expansions.
We describe each component of the pipeline in detail in the following sections.
5.1 Tokenization
Our initial analysis of SMS messages from users, especially in Spanish indicated that while some users
use accented characters in orthography, several others omit it for the sake of faster responses and con-
venience. Hence, we decided to train all our models on unaccented characters. Given a message, we
convert all accented characters to their corresponding unaccented forms, e.g., ba?no? bano, followed by
lowercasing of all characters. We do not perform any other kind of tokenization.
5.2 Unsupervised SMS Normalization
In Section 5.2, we described a static lookup table for expanding abbreviations and shorthands typically
encountered in SMS messages, e.g., 4ever?forever. While a static lookup table provides a reasonable
way of handling common SMS abbreviations, it has limited coverage. In order to build a larger nor-
malization lexicon, we used distributed representation of words to induce the lexicon in an unsupervised
manner. Distributed word representations (Bengio et al., 2003; Collobert and Weston, 2008; Turian et al.,
2010) induced through deep neural networks have been shown to be useful in several natural language
processing applications. We use the notion of distributional similarity that is automatically induced
through the word representations for learning automatic normalization lexicons.
Canonical form Noisy form
love loveeee, loveeeee, looove, love, wuv, wove, love, laffff, love, wuvvv, luhhhh, love, luvvv, luv
starbucks starbs, sbucks
once oncee, 1ce
tomorrow tmrw, tomorrow, 2moro, tmrrw, tomarrow, tomoro, tomoz, 2mrw, tmr, tm, tmwr, 2mm, tmw, 2morro
forever foreva, 5ever, foreverrrr, forver, foreeverrr, 4ever, 5eva, 4eva, foreevaa, forevs, foreve
because cause, cos, coz, ?cos, ?cause, bc, because, becuz, bcuz, cuz, bcus, bcoz, because
homework hwk, hw, hmwk, hmwrk, hmw, homeworkk, homwork, hmk, honework, homeowork
igualmente igualmentee, igualment, iwalmente
siempre simpre, siempre, 100pre, siempre, ciempre, siempre, siiempre, siemore, siempr, siemre, siempe
adios adi, a10, adio
contigo contigoo, cntigo, conmigo, contigoooo, kontigo, conmigoo, conmiqo
demasiado demaciado, demasido, demasiademente, demasiao
Table 2: Examples from the unsupervised normalization lexicon induced through deep learning
We started with the 106 million tweets described in Section 4.3 and used a deep neural network iden-
tical to that used in (Collobert and Weston, 2008), i.e., the network consisted of a lookup table, hidden
layer with 100 nodes and a linear layer with one output. However, we used a context of 5 words and
corrupted the centre word instead of the last word to learn the distributed representations. We performed
stochastic gradient minimization over 1000 epochs on the twitter data. Subsequently, we took the En-
glish and Spanish vocabularies in our translation model and found the 50 nearest neighbors using cosine
distance for each word. We trained the above representations using the Torch toolkit (Collobert et al.,
2011).
Feature English Spanish
dimension Precision Recall Precision Recall
100 70.4 97.4 69.8 97.3
200 72.2 97.5 79.2 100
300 70.4 97.4 71.6 100
Table 3: Performance of the unsupervised normalization procedure. Only 1-best for each word was
considered.
Once we obtained the 50 nearest neighbors for each word in the clean vocabulary, we used a com-
bination of cosine metric threshold and Levenshtein distance (weighted equally) between the consonant
977
skeleton of the strings to construct the mapping lexicon. Finally, we inverted the table to obtain a nor-
malization lexicon. Our procedure currently finds only one-to-one mappings. We took 60 singleton
entries from the static normalization tables reported in Section 5.2 and evaluated the performance of our
approach. The results are shown in Table 3 and some examples of learned normalizations are shown in
Table 2.
5.3 Phrase Segmentation
In many SMS messages, multiple clauses may be concatenated without explicit punctuation. For exam-
ple, the message hi babe hope you?re well sorry i missed your call needs to be interpreted as hi babe.
hope you?re well. sorry, i missed your call. We perform phrase segmentation using an automatic punc-
tuation classifier trained on SMS messages with punctuation. The classifier learns how to detect end of
sentence markers, i.e. periods, as well as commas in the input stream of unpunctuated words.
An English punctuation classifier and a Spanish punctuation classifier was trained. The former was
trained on two million words of smartphone data described in Section 4.1 while the latter was trained
on 223,000 words of Spanish subtitles from the OpenSubtitles
5
corpus. From each of these data sets, a
maximum entropy classifier was trained. Both classifiers utilized both unigram word and part of speech
(POS) features of a window size of two words around the target word to be classified. A POS tagger
trained on the English Penn Treebank provided English POS tags. Likewise, a Spanish POS tagger
provided Spanish POS tags. The training data for the Spanish tagger, 1.6 million words in size, was
obtained by running the Spanish Freeling parser over the Spanish version of TED talk transcripts. Results
are shown in Table 4. Both phrase segmenters detect end of sentence well. The Spanish phrase segmenter
detects commas better than the English one. This might be due to differences in the training sets; commas
appear about 20 times more often in the Spanish data than in the English data.
Class Precision Recall F-measure
English period 89.7 90.9 90.3
comma 61.1 10.9 18.5
Spanish period 94.3 87.4 90.7
comma 74.2 37.4 49.7
Table 4: Performance of automatic phrase segmentation (numbers are in %)
5.4 Machine Translation
We used a phrase-based translation framework with the phrase table represented as a finite-state trans-
ducer (Rangarajan Sridhar et al., 2013). Our framework proceeds by using the standard procedure of
performing word alignment using GIZA++ (Och and Ney, 2003) and obtaining phrases from the word
alignment using heuristics (Zens and Ney, 2004) and subsequently scoring them. The phrase table is
then represented as a finite-state transducer (FST). The FST decoder was used with minimum error rate
training (MERT) to compute a set of weights for the log-linear model. It is important to note that the
cost of arcs of the FST is a composite score (dot product of scores and weights) and hence requires an
additional lookup during the N-best generation phase in MERT to obtain the component scores. The
model is equivalent to Moses (?) phrase translation without reordering.
We noticed from the data collected in Section 4 that in typical SMS scenarios, a lot of phrases are stock
phrases and hence caching these phrases may result in high accuracies instead of deriving the translation
using a statistical model. We took the data created in Section 4 and created a FST to represent the
sentences. The motivation is to increase the precision of common entries as well as reduce the latency
involved in retrieving a translation from a statistical model. An example of the FST translation paradigm
is shown in Figure 1
We experimented with the notion of using a consensus-based word alignment by combining the align-
ment obtained through different alignment tools. We used GIZA++ (Och and Ney, 2003), Berkeley
5
http://www.opensubtitles.org
978
step1.fsm0
1how:how 2how^are:how^are 3how^are^you:how^are^you
are:are are^you:are^you you:you

WIP

LM)
hello how are you
hello 
how are you


ex.fst
0
hello:holathanks:gracias
how^do^you^do:como^estas
Cached Table
Statistical Model
bestpath(
hola como estas
hello.fsm
0
hello:hello
ptable.fst0/0
how:que/1.822how:como/0.458how^are^you:como^estas/1.106how^are^you:como^esta^usted/2.358are^you:estan/1.998
are^you:estas/0.757you:que/1.460you:tu/0.757
Figure 1: Illustration of the hybrid translation approach using FSTs. WIP and LM refer to the finite state
automata for word insertion penalty and language model, respectively.
Alignment strategy en2es es2en
GIZA++ 28.45 31.83
Pialign 28.08 33.48
Berkeley aligner 27.82 32.01
Union 28.01 33.14
Majority voting 27.32 32.96
Table 5: BLEU scores obtained using different alignment strategies. Only the statistical translation model
was used in the evaluation.
aligner (Liang et al., 2006) and the Phrasal ITG aligner (Pialign) (Neubig et al., 2011). We combined the
alignments in two different ways, taking the union of alignments or majority vote for each target word.
For training the translation model, we used a total of 28.5 million parallel sentences obtained from the
following sources: Opensubtitles (Tiedemann and Lars Nygaard, 2004), Europarl (Koehn, 2005), TED
talks (Cettolo et al., 2012) and Web. The bitext was processed to eliminate spurious pairs by restricting
the English and Spanish vocabularies to the top 150k frequent words as evidenced in a large collection of
monolingual corpora. We also eliminated bitext with ratio of English to Spanish words less than 0.5. The
initial model was optimized using MERT over 1000 parallel sentences from the SMS domain. Results of
the machine translation experiments are shown in Table 5. The test set used was 456 messages collected
in a real SMS interaction (see Section 6.1). The results indicate that consensus alignment procedure is not
superior to the individual alignment outputs. Furthermore, the BLEU scores obtained through both the
consensus procedures are not statistically significant with respect to the BLEU score obtained from the
individual alignment tools. Hence, we used with the phrase translation table obtained using the Phrasal
ITG aligner in all our experiments.
6 SMS Translation Service
In order to test the SMS translation models described in the previous sections, we created the infrastruc-
ture to intercept SMS messages, translate and deliver them in the preferred language of the recipient. The
users were simply asked to register their numbers with a particular language through a Web portal and
subsequently, all messages received by a user would be in the registered language. Some screenshots of
interaction between users is shown in Figure 2. For the messages that are translated, we show both the
original and translated messages. In cases where the translated message is longer than the character limit
per message, we split the message over two message boxes.
979
6.1 User Evaluation
Figure 2: Screenshots of the SMS interface with translation
In order to test the SMS translation models described in the previous sections, we created the infras-
tructure to intercept SMS messages, translate and deliver them in the preferred language of the recipient.
For the messages that are translated, we show both the original and translated messages. In cases where
the translated message is longer than the character limit per message, we split the message over two
message boxes. As part of the study we enrolled 20 English and 5 Spanish participants. The Spanish
participants were bilingual while the English users had little to no knowledge of Spanish. Some of these
interactions turned out to be short while others were had a large number of turns. We collected the
messages exchanged over 2 days that amounted to 241 English and 215 Spanish messages.
0!5!
10!15!
20!25!
30!35!
40!45!
0!1!
2!3!
4!5!
6!7!
8!9!
All! Most! Much! Little! None!
Perce
ntage
 of pa
rticipa
nts!
Numb
er of p
articip
ants!
Adequacy of Translation!
Figure 3: Subjective ratings regarding the adequacy of using SMS translation
We manually translated the 456 messages to create a test data set for evaluation purposes. In the
absence of real SMS feeds in training, this test set is the closest we have to real SMS field data. The BLEU
scores using the entire pipeline (normalization, punctuation, cached and statistical machine translation)
for English-Spanish and Spanish-English was 31.25 and 37.19, respectively. We also created a survey
for the participants to evaluate fluency and adequacy (LDC, 2005) Figures 3 and 4 show the survey
results for adequacy and fluency, respectively. The results indicate that a majority of the people found
the translation quality to be sufficiently adequate while the fluency was between good and non-native.
7 Discussion
The SMS bitext described in Section 4 consists of a total 58790 unique parallel sentences in the SMS
domain. While the bulk of the data (speech-based) does not contain abbreviations and spelling errors, it
980
0!10!
20!30!
40!50!
60!
0!2!
4!6!
8!10!
12!
Flawless! Good! Non-nativ
e! Disfluent! Incompreh
ensible!
Perce
ntage
 of pa
rticipa
nts!
Numb
er of p
articip
ants!
Fluency of Translation!
Figure 4: Subjective ratings regarding the fluency of using SMS translation
is highly representative of SMS messages and in fact is perfectly suited for statistical machine translation
that typically uses normalized and tokenized data. The iterative procedure using Amazon Mechanical
Turk is a good approach to procuring surrogate SMS data. We plan to continue harvesting data using this
approach.
The unsupervised normalization lexicon learning using deep learning performs a good job of learning
SMS shorthands. However, the induced lexicon contains only one-to-one word mappings. If one were
to form compound words for a given dataset, the procedure can be potentially used for learning many-
to-one and many-to-many mappings. Our framework also learns spelling errors rather well. It may also
be possible to use distributed representations learned through log-linear models (Mikolov et al., 2013)
for our task. However, this is beyond the scope of the work presented in this paper. Finally, we used
only 1-best match for the unsupervised lexicon used in this work. One can potentially use a confusion
network and compose it with the FST model to achieve higher accuracies. Our scheme results in fairly
high precision with almost no false negatives (recall is extremely high) and can be reliably applied for
normalization. The unsupervised normalization scheme did not yield significant improvements in BLEU
score since our test set contained only 4 instances where shorthands were used.
Conventionally, sentence segmentation has been useful in improving the quality of statistical machine
translation (Matusov et al., 2006; Matusov et al., 2005). Such segmentation, albeit into shorter phrases,
is also useful for SMS translation. In the absence of phrase segmentation, the BLEU scores for English-
Spanish and Spanish-English drop to 29.65 and 23.95, respectively. The degradation for Spanish-English
messages is quite severe (drop from 37.19 to 23.95) as the lack of segmentation greatly reduces the use of
the cached table. In the absence of segmentation, the cached table was used for 12.8% and 14.4% of the
total phrases for English-Spanish and Spanish-English, respectively. However, with phrase segmentation
the cached table was used for 29.2% and 39.2% of total phrases.
The subjective results obtained from the user trial augur well for the real use of translation technology
as a feature in SMS. One of the issues in the study was balancing the English and Spanish participants.
Since we had access to more English participants (20) in comparison with Spanish participants (5), the
rate of exchange was slow. However, since SMS messages are not required to be real-time, participants
still engaged in a meaningful conversation. Subjective evaluation results using LDC criteria indicate
that most users were happy with the adequacy of translation while the fluency was rated as average. In
general, SMS messages are not very fluent due to character limit imposed on the exchanges and hence
machine translation has to use potentially disfluent source text.
8 Conclusion
We presented an application of statistical machine translation for translating SMS messages. We decou-
pled SMS translation into normalization followed by translation. Our unsupervised SMS normalization
approach exploits the distributional similarity of words and learns SMS shorthands with good accuracy.
We used a hybrid translation approach to exploit the repetitive nature of high frequency SMS messages.
Both objective and subjective evaluation experiments indicate that our system generates translation with
high quality while addressing the idiosyncrasies of SMS messages.
981
References
A. Aw, M. Zhang, J. Xiao, and J. Su. 2009. A phrase-based statistical model for SMS text normalization. In
Proceedings of COLING, pages 33?40.
S. Bangalore, V. Murdock, and G. Riccardi. 2002. Bootstrapping bilingual data using consensus translation for a
multilingual instant messaging system. In Proceedings of COLING.
R. Beaufort, S. Roekhaut, L. A. Cougnon, and C. Fairon. 2010. A hybrid rule/model-based finite-state framework
for normalizing sms messages. In Proceedings of ACL, pages 770?779.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic language model. Journal of
Machine Learning Research, 3:1137?1155.
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In
Proceedings of EAMT.
T. Chen and M. Y. Kan. 2013. Creating a live, public short message service corpus: the NUS SMS corpus.
Language Resources and Evaluation, 47(2):299?335.
R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: deep neural networks
with multitask learning. In Proceedings of ICML.
R. Collobert, K. Kavukcuoglu, and C. Farabet. 2011. Torch7: A matlab-like environment for machine learning.
In BigLearn, NIPS Workshop.
V. Eidelman, K. Hollingshead, and P. Resnik. 2011. Noisy SMS Machine Translation in Low-Density Languages.
In Proceedings of 6th Workshop on Statistical Machine Translation.
C. Fairon and S. Paumier. 2006. A translated corpus of 30,000 french SMS. In Proceedings of LREC.
C. Kobus, F. Yvon, and G. Damnati. 2008. Normalizing sms: Are two metaphors better than one? In Proceedings
of COLING, pages 441?448.
P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.
LDC. 2005. Linguistic data annotation specification: Assessment of fluency and adequacy in translations. Tech-
nical report, Revision 1.5.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of NAACL-HLT, pages
104?111.
E. Matusov, G. Leusch, O. Bender, and H. Ney. 2005. Evaluating machine translation output with automatic
sentence segmentation. In Proceedings of IWSLT, pages 148?154.
E. Matusov, A. Mauser, and H. Ney. 2006. Automatic sentence segmentation and punctuation prediction for
spoken language translation. In Proceedings of IWSLT, pages 158?165.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013. Efficient estimation of word representations in vector space.
In Proceedings of Workshop at ICLR.
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised
model for joint phrase alignment and extraction. In Proceedings of the ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational
Linguistics, 29(1):19?51.
D. Pennell and Y. Liu. 2011. A character-level machine translation approach for normalization of SMS abbrevia-
tions. In Proceedings of IJCNLP.
V. K. Rangarajan Sridhar, J. Chen, S. Bangalore, A. Ljolje, and R. Chengalvarayan. 2013. Segmentation strategies
for streaming speech translation. In Proceedings of NAACL-HLT.
E. Sanders. 2012. Collecting and analysing chats and tweets in SoNaR. In Proceedings of LREC.
C. Tagg. 2009. Across-frequency in convolutive blind source separation. dissertation, University of Birmingham.
J. Tiedemann and L. Lars Nygaard. 2004. The OPUS corpus - parallel & free. In Proceedings of LREC.
982
M. Treurniet, O. De Clercq, H. van den Heuvel, and N. Oostdijk. 2012. Collecting a corpus of Dutch SMS. In
Proceedings of LREC, pages 2268?2273.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semi-
supervised learning. In Proceedings of ACL.
P. Wang and H. Tou Ng. 2013. A beam-search decoder for normalization of social media text with application to
machine translation. In Proceedings of NAACL-HLT.
Richard Zens and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. In In
Proceedings of HLT-NAACL, pages 257?264.
983
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 437?445,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Real-time Incremental Speech-to-Speech Translation of Dialogs
Srinivas Bangalore, Vivek Kumar Rangarajan Sridhar, Prakash Kolan
Ladan Golipour, Aura Jimenez
AT&T Labs - Research
180 Park Avenue
Florham Park, NJ 07932, USA
vkumar,srini,pkolan,ladan,aura@research.att.com
Abstract
In a conventional telephone conversation be-
tween two speakers of the same language, the
interaction is real-time and the speakers pro-
cess the information stream incrementally. In
this work, we address the problem of incre-
mental speech-to-speech translation (S2S) that
enables cross-lingual communication between
two remote participants over a telephone. We
investigate the problem in a novel real-time
Session Initiation Protocol (SIP) based S2S
framework. The speech translation is per-
formed incrementally based on generation of
partial hypotheses from speech recognition.
We describe the statistical models comprising
the S2S system and the SIP architecture for
enabling real-time two-way cross-lingual dia-
log. We present dialog experiments performed
in this framework and study the tradeoff in ac-
curacy versus latency in incremental speech
translation. Experimental results demonstrate
that high quality translations can be generated
with the incremental approach with approxi-
mately half the latency associated with non-
incremental approach.
1 Introduction
In recent years, speech-to-speech translation (S2S)
technology has played an increasingly important
role in narrowing the language barrier in cross-
lingual interpersonal communication. The improve-
ments in automatic speech recognition (ASR), statis-
tical machine translation (MT), and, text-to-speech
synthesis (TTS) technology has facilitated the serial
binding of these individual components to achieve
S2S translation of acceptable quality.
Prior work on S2S translation has primarily fo-
cused on providing either one-way or two-way trans-
lation on a single device (Waibel et al, 2003; Zhou
et al, 2003). Typically, the user interface requires
the participant(s) to choose the source and target lan-
guage apriori. The nature of communication, either
single user talking or turn taking between two users
can result in a one-way or cross-lingual dialog inter-
action. In most systems, the necessity to choose the
directionality of translation for each turn does take
away from a natural dialog flow. Furthermore, single
interface based S2S translation (embedded or cloud-
based) is not suitable for cross-lingual communica-
tion when participants are geographically distant, a
scenario more likely in a global setting. In such a
scenario, it is imperative to provide real-time and
low latency communication.
In a conventional telephone conversation between
two speakers of the same language, the interaction
is real-time and the speakers process the informa-
tion stream incrementally. Similarly, cross-lingual
dialog between two remote participants will greatly
benefit through incremental translation. While in-
cremental decoding for text translation has been
addressed previously in (Furuse and Iida, 1996;
Sankaran et al, 2010), we address the problem in
a speech-to-speech translation setting for enabling
real-time cross-lingual dialog. We address the prob-
lem of incrementality in a novel session initiation
protocol (SIP) based S2S translation system that en-
ables two people to interact and engage in cross-
lingual dialog over a telephone (mobile phone or
landline). Our system performs incremental speech
recognition and translation, allowing for low latency
interaction that provides an ideal setting for remote
dialog aimed at accomplishing a task.
We present previous work in this area in Section 2
and introduce the problem of incremental translation
in Section 3. We describe the statistical models used
in the S2S translation framework in Section 4 fol-
lowed by a description of the SIP communication
437
framework for real-time translation in Section 5. In
Section 6, we describe the basic call flow of our sys-
tem following which we present dialog experiments
performed using our framework in Section 8. Fi-
nally, we conclude in Section 9 along with directions
for future work.
2 Previous Work
Most previous work on speech-to-speech transla-
tion systems has focused on a single device model,
i.e., the user interface for translation is on one de-
vice (Waibel et al, 1991; Metze et al, 2002; Zhou
et al, 2003; Waibel et al, 2003). The device typi-
cally supports multiple source-target language pairs.
A user typically chooses the directionality of transla-
tion and a toggle feature is used to switch the direc-
tionality. However, this requires physical presence
of the two conversants in one location.
On the other hand, text chat between users over
cell phones has become increasingly popular in the
last decade. While the language used in the inter-
action is typically monolingual, there have been at-
tempts to use statistical machine translation to en-
able cross-lingual text communication (Chen and
Raman, 2008). But this introduces a significant
overhead as the users need to type in the responses
for each turn. Moreover, statistical translation sys-
tems are typically unable to cope with telegraphic
text present in chat messages. A more user friendly
approach would be to use speech as the modality for
communication.
One of the first attempts for two-way S2S trans-
lation over a telephone between two potentially re-
mote participants was made as part of the Verbmobil
project (Wahlster, 2000). The system was restricted
to certain topics and speech was the only modality.
Furthermore, the spontaneous translation of dialogs
was not incremental. One of the first attempts at in-
cremental text translation was demonstrated in (Fu-
ruse and Iida, 1996) using a transfer-driven machine
translation approach. More recently, an incremen-
tal decoding framework for text translation was pre-
sented in (Sankaran et al, 2010). To the best of
our knowledge, incremental speech-to-speech trans-
lation in a dialog setting has not been addressed in
prior work. In this work, we address this problem
using first of a kind SIP-based large vocabulary S2S
translation system that can work with both smart-
phones and landlines. The speech translation is per-
formed incrementally based on generation of partial
hypotheses from speech recognition. Our system
displays the recognized and translated text in an in-
cremental fashion. The use of SIP-based technology
also supports an open form of cross-lingual dialog
without the need for attention phrases.
3 Incremental Speech-to-Speech
Translation
In most statistical machine translation systems, the
input source text is translated in entirety, i.e., the
search for the optimal target string is constrained
on the knowledge of the entire source string. How-
ever, in applications such as language learning and
real-time speech-to-speech translation, incremen-
tally translating the source text or speech can pro-
vide seamless communication and understanding
with low latency. Let us assume that the input string
(either text or speech recognition hypothesis) is f =
f1, ? ? ? , fJ and the target string is e = e1, ? ? ? , eI .
Among all possible target sentences, we will choose
the one with highest probability:
e?(f) = argmax
e
Pr(e|f) (1)
In an incremental translation framework, we do not
observe the entire string f . Instead, we observe Qs
sequences, S = s1 ? ? ? sk ? ? ? sQs , i.e., each sequence
sk = [fjkfjk+1 ? ? ? fj(k+1)?1], j1 = 1, jQs+1 =
J + 11. Let the translation of each foreign sequence
sk be denoted by tk = [eikeik+1 ? ? ? ei(k+1)?1], i1 =
1, iQs+1 = I+1. Given this setting, we can perform
decoding using three different approaches. Assum-
ing that each partial source input is translated inde-
pendently, i.e., chunk-wise translation, we get,
e?(f) = argmax
t1
Pr(t1|s1) ? ? ? argmax
tk
Pr(tk|sk)
(2)
We call the decoding in Eq. 2 as partial decoding.
The other option is to translate the partial source in-
1For simplicity, we assume that the incremental and non-
incremental hypotheses are equal in length
438
put conditioned on the history, i.e.,
e?(f) = argmax
t1
Pr(t1|s1) ? ? ?
argmax
tk
Pr(tk|s1, ? ? ? , sk, t
?
1, ? ? ? , t
?
k?1) (3)
where t?i denotes the best translation for source se-
quence si. We term the result obtained through Eq. 3
as continue-partial. The third option is to wait for
all the partials to be generated and then decode the
source string which we call complete decoding, i.e.,
e?(f) = argmax
e
Pr(e|s1, ? ? ? , sk) (4)
Typically, the hypothesis e? will be more accurate
than e? as the translation process is non-incremental.
In the best case, one can obtain e? = e?. While the de-
coding described in Eq. 2 has the lowest latency, it
is likely to result in inferior performance in compari-
son to Eq. 1 that will have higher latency. One of the
main issues in incremental speech-to-speech trans-
lation is that the translated sequences need to be im-
mediately synthesized. Hence, there is tradeoff be-
tween the amount of latency versus accuracy as the
synthesized audio cannot be revoked in case of long
distance reordering. In this work, we focus on incre-
mental speech translation and defer the problem of
incremental synthesis to future work. We investigate
the problem of incrementality using a novel SIP-
based S2S translation system, the details of which
we discuss in the subsequent sections.
4 Speech-to-Speech Translation
Components
In this section, we describe the training data, pre-
processing steps and statistical models used in the
S2S system.
4.1 Automatic Speech Recognition
We use the AT&T WATSONSM real-time speech
recognizer (Goffin et al, 2004) as the speech recog-
nition module. WATSONSM uses context-dependent
continuous density hidden Markov models (HMM)
for acoustic modeling and finite-state networks for
network optimization and search. The acoustic mod-
els are Gaussian mixture tied-state three-state left-
to-right HMMs. All the acoustic models in this work
were initially trained using the Maximum Likeli-
hood Estimation (MLE) criterion, and followed by
discriminative training through Minimum Phone Er-
ror (MPE) criterion. We also employed Gaussian
Selection (Bocchieri, 1993) to decrease the real-time
factor during the recognition procedure.
The acoustic models for English and Span-
ish were mainly trained on short utterances in
the respective language, acquired from SMS and
search applications on smartphones. The amount
of training data for the English acoustic model
is around 900 hours of speech, while the data
for training the Spanish is approximately half that
of the English model. We used a total of 107
phonemes for the English acoustic model, com-
posed of digit-specific, alpha-specific, and general
English phonemes. Digit-specific and alpha-specific
phonemes were applied to improve the recognition
accuracy of digits and alphas in the speech. The
number of phonemes for Spanish was 34, and, no
digit- or alpha-specific phonemes were included.
The pronunciation dictionary for English is a hand-
labeled dictionary, with pronunciation for unseen
words being predicted using custom rules. A rule-
based dictionary was used for Spanish.
We use AT&T FSM toolkit (Mohri et al, 1997)
to train a trigram language model (LM). The lan-
guage model was linearly interpolated from 18 and
17 components for English and Spanish, respec-
tively. The data for the the LM components was
obtained from several sources that included LDC,
Web, and monolingual portion of the parallel data
described in section 4.2. An elaborate set of lan-
guage specific tokenization and normalization rules
was used to clean the corpora. The normalization
included spelling corrections, conversion of numer-
als into words while accounting for telephone num-
bers, ordinal, and, cardinal categories, punctuation,
etc. The interpolation was performed by tuning the
language model weights on a development set us-
ing perplexity metric. The development set was 500
sentences selected randomly from the IWSLT cor-
pus (Paul, 2006). The training vocabulary size for
English acoustic model is 140k and for the language
model is 300k. For the Spanish model, the train-
ing vocabulary size is 92k, while for testing, the
language model includes 370k distinct words. In
our experiments, the decoding and LM vocabularies
439
were the same.
4.2 Machine Translation
The phrase-based translation experiments reported
in this work was performed using the Moses2
toolkit (Koehn et al, 2007) for statistical machine
translation. Training the translation model starts
from the parallel sentences from which we learn
word alignments by using GIZA++ toolkit (Och
and Ney, 2003). The bidirectional word alignments
obtained using GIZA++ were consolidated by us-
ing the grow-diag-final option in Moses. Subse-
quently, we learn phrases (maximum length of 7)
from the consolidated word alignments. A lexical-
ized reordering model (msd-bidirectional-fe option
in Moses) was used for reordering the phrases in
addition to the standard distance based reordering
(distortion-limit of 6). The language models were
interpolated Kneser-Ney discounted trigram models,
all constructed using the SRILM toolkit (Stolcke,
2002). Minimum error rate training (MERT) was
performed on a development set to optimize the fea-
ture weights of the log-linear model used in trans-
lation. During decoding, the unknown words were
preserved in the hypotheses.
The parallel corpus for phrase-based transla-
tion was obtained from a variety of sources: eu-
roparl (Koehn, 2005), jrc-acquis corpus (Steinberger
et al, 2006), opensubtitle corpus (Tiedemann and
Lars Nygaard, 2004), web crawling as well as hu-
man translation. The statistics of the data used for
English-Spanish is shown in Table 1. About 30% of
the training data was obtained from the Web (Ran-
garajan Sridhar et al, 2011). The development set
(identical to the one used in ASR) was used in
MERT training as well as perplexity based optimiza-
tion of the interpolated language model. The lan-
guage model for MT and ASR was constructed from
identical data.
4.3 Text-to-speech synthesis
The translated sentence from the machine trans-
lation component is synthesized using the AT&T
Natural VoicesTM text-to-speech synthesis en-
gine (Beutnagel et al, 1999). The system uses unit
selection synthesis with half phones as the basic
2http://www.statmt.org/moses
en-es
Data statistics en es
# Sentences 7792118 7792118
# Words 98347681 111006109
Vocabulary 501450 516906
Table 1: Parallel data used for training translation
models
units. The database was recorded by professional
speakers of the language. We are currently using fe-
male voices for English as well as Spanish.
5 SIP Communication Framework for
Real-time S2S Translation
The SIP communication framework for real-time
language translation comprises of three main com-
ponents. Session Initiation Protocol (SIP) is becom-
ing the de-facto standard for signaling control for
streaming applications such as Voice over IP. We
present a SIP communication framework that uses
Real-time Transport Protocol (RTP) for packetiz-
ing multimedia content and User Datagram Proto-
col (UDP) for delivering the content. In this work,
the content we focus on is speech and text infor-
mation exchanged between two speakers in a cross-
lingual dialog. For two users conversing in two dif-
ferent languages (e.g., English and Spanish), the me-
dia channels between them will be established as
shown in Figure 1. In Figure 1, each client (UA) is
responsible for recognition, translation, and synthe-
sis of one language input. E.g., the English-Spanish
UA recognizes English text, converts it into Spanish,
and produces output Spanish audio. Similarly, the
Spanish-English UA is responsible for recognition
of Spanish speech input, converting it into English,
and producing output English audio. We describe
the underlying architecture of the system below.
5.1 Architecture
1. End point SIP user agents: These are the SIP
end points that exchange SIP signaling mes-
sages with the SIP Application server (AS) for
call control.
2. SIP User Agents: Provide a SIP interface to the
core AT&T WATSONSM engine that incorpo-
rates acoustic and language models for speech
440
SIP UA
(en->es)
SIP UA
(es->en)
     APP    
 SERVER
   Caller 
(English)
  Callee 
(Spanish)
Caller Eng Audio
Callee English Audio (Translated)
Callee Spanish Audio
Caller Spanish Audio (Translated)
C
a
l
l
e
r
 
E
n
g
l
i
s
h
 
T
e
x
t
 
 
 
 
(
R
e
c
o
g
n
i
z
e
d
)
C
a
l
l
e
e
 
E
n
g
l
i
s
h
 
T
e
x
t
 
 
 
 
(
T
r
a
n
s
l
a
t
e
d
)
C
a
l
l
e
e
 
S
p
a
n
i
s
h
 
T
e
x
t
 
 
 
 
(
R
e
c
o
g
n
i
z
e
d
)
C
a
l
l
e
e
 
E
n
g
l
i
s
h
 
T
e
x
t
 
 
 
 
(
T
r
a
n
s
l
a
t
e
d
)
C
a
l
l
e
r
 
S
p
a
n
i
s
h
 
T
e
x
t
 
 
 
 
 
(
T
r
a
n
s
l
a
t
e
d
)
C
a
l
l
e
e
 
S
p
a
n
i
s
h
 
T
e
x
t
 
 
 
 
 
(
R
e
c
o
g
n
i
z
e
d
)
C
a
l
l
e
r
 
E
n
g
l
i
s
h
 
T
e
x
t
 
 
 
 
 
(
R
e
c
o
g
n
i
z
e
d
)
C
a
l
l
e
r
 
S
p
a
n
i
s
h
 
T
e
x
t
 
 
 
 
 
(
T
r
a
n
s
l
a
t
e
d
)
SIP Channel for Signaling Setup and Text (recognized + translated)
Media Channel for RTP Audio
Figure 1: SIP communication framework used for real-time speech-to-speech translation. The example
shows the setup between two participants in English(en) and Spanish (es)
recognition.
3. SIP Application Server (AS): A standard SIP
B2BUA (back to back user agent) that receives
SIP signaling messages and forwards them to
the intended destination. The machine transla-
tion component (server running Moses (Koehn
et al, 2007)) is invoked from the AS.
In our communication framework, the SIP AS re-
ceives a call request from the calling party. The AS
infers the language preference of the calling party
from the user profile database and forwards the call
to the called party. Based on the response, AS in-
fers the language preference of the called party from
the user profile database. If the languages of the
calling and called parties are different, the AS in-
vites two SIP UAs into the call context. The AS ex-
changes media parameters derived from the calling
and called party SIP messages with that of the SIP
UAs. The AS then forwards the media parameters
of the UAs to the end user SIP agents.
The AS, the end user SIP UAs, and the SIP UAs
are all RFC 3261 SIP standard compliant. The end
user SIP UAs are developed using PJSIP stack that
uses PJMedia for RTP packetization of audio and
network transmission. For our testing, we have
implemented the end user SIP UAs to run on Ap-
ple IOS devices. The AS is developed using E4SS
(Echarts for SIP Servlets) software and deployed on
Sailfin Java container. It is deployed on a Linux box
installed with Cent OS version 5. The SIP UAs are
written in python for interfacing with external SIP
devices, and use proprietary protocol for interfacing
with the core AT&T WATSONSM engine.
6 Typical Call Flow
Figure 2 shows the typical call flow involved in set-
ting up the cross-lingual dialog. The caller chooses
the number of the callee from the address book or
enters it using the keypad. Subsequently, the call is
initiated and the underlying SIP channels are estab-
lished to facilitate the call. The users can then con-
verse in their native language with the hypotheses
displayed in an IM-like fashion. The messages of
the caller appear on the left side of the screen while
those of the callee appear on the right. Both the
recognition and translation hypotheses are displayed
incrementally for each side of the conversation. In
our experiments, the caller and the callee naturally
followed a protocol of listening to the other party?s
synthesized output before speaking once they were
accustomed to the interface. One of the issues dur-
ing speech recognition is that, the user can poten-
tially start speaking as the TTS output from the other
441
Figure 2: Illustration of call flow. The call is established using SIP and the real-time conversation appears
in the bubbles in a manner similar to Instant Messaging. For illustration purposes, the caller (Spanish) and
callee (English) are assumed to have set their language preferences in the setup menu.
participant is being played. We address the feedback
problem from the TTS output by muting the micro-
phone when TTS output is played.
7 Dialog Data
The system described above provides a natural way
to collect cross-lingual dialog data. We used our
system to collect a corpus of 40 scripted dialogs in
English and Spanish. A bilingual (English-Spanish)
speaker created dialog scenarios in the travel and
hospitality domain and the scripted dialog was used
as reference material in the call. Two subjects partic-
ipated in the data collection, a male English speaker
and female Spanish speaker. The subjects were in-
structed to read the lines verbatim. However, due to
ASR errors, the subjects had to repeat or improvise
few turns (about 10%) to sustain the dialog. The av-
erage number of turns per scenario in the collected
corpus is 13; 6 and 7 turns per scenario for English
and Spanish, respectively. An example dialog be-
tween two speakers is shown in Table 2.
8 Experiments
In this section, we describe speech translation ex-
periments performed on the dialog corpus collected
through our system. We present baseline results fol-
lowed by results of incremental translation.
8.1 Baseline Experiments
The models described in Section 4 were used to es-
tablish baseline results on the dialog corpus. No
A: Hello, I am calling from room four twenty one
the T.V. is not working. Do you think you can send
someone to fix it please?
B: Si, Sen?or enseguida enviamos a alguien para que
la arregle. Si no le cambiaremos de habitacio?n.
A: Thank you very much.
B: Estamos aqu para servirle. Lla?menos si necesita
algo ma?s.
Table 2: Example of a sample dialog scenario.
contextual information was used in these experi-
ments, i.e., the audio utterances were decoded in-
dependently. The ASR WER for English and Span-
ish sides of the dialogs is shown in Figure 3. The
average WER for English and Spanish side of the
conversations is 27.73% and 22.83%, respectively.
The recognized utterances were subsequently trans-
lated using the MT system described above. The
MT performance in terms of Translation Edit Rate
(TER) (Snover et al, 2006) and BLEU (Papineni
et al, 2002) is shown in Figure 4. The MT per-
formance is shown across all the turns for both ref-
erence transcriptions and ASR output. The results
show that the performance of the Spanish-English
MT model is better in comparison to the English-
Spanish model on the dialog corpus. The perfor-
mance on ASR input drops by about 18% compared
to translation on reference text.
442
08.5
17.0
25.5
34.0
Reference ASR
23.87
28.21
26.96
33.58
B
L
E
U
Spanish-English
English-Spanish
0
17.5
35.0
52.5
70.0
Reference ASR
63.42
59.19
55.34
47.26
T
E
R
 
Spanish-English
English-Spanish
Figure 4: TER (%) and BLEU of English-Spanish and Spanish-English MT models on reference transcripts
and ASR output
Figure 3: WER (%) of English and Spanish acoustic
models on the dialog corpus
8.2 Segmentation of ASR output for MT
Turn taking in a dialog typically involves the sub-
jects speaking one or more utterances in a turn.
Since, machine translation systems are trained on
chunked parallel texts (40 words or less), it is ben-
eficial to segment the ASR hypotheses before trans-
lation. Previous studies have shown significant im-
provements in translation performance through the
segmentation of ASR hypotheses (Matusov et al,
2007). We experimented with the notion of seg-
mentation defined by silence frames in the ASR out-
put. A threshold of 8-10 frames (100 ms) was found
to be suitable for segmenting the ASR output into
sentence chunks. We did not use any lexical fea-
tures for segmenting the turns. The BLEU scores for
different silence thresholds used in segmentation is
shown in Figure 5. The BLEU scores improvement
for Spanish-English is 1.6 BLEU points higher than
the baseline model using no segmentation. The im-
provement for English-Spanish is smaller but statis-
tically significant. Analysis of the dialogs revealed
that the English speaker tended to speak his turns
without pausing across utterance chunks while the
Spanish speaker paused a lot more. The results in-
dicate that in a typical dialog interaction, if the par-
ticipants observe inter-utterance pause (80-100 ms)
within a turn, it serves as a good marker for segmen-
tation. Further, exploiting such information can po-
tentially result in improvements in MT performance
as the model is typically trained on sentence level
parallel text.
12.0
13.8
15.6
17.4
19.2
21.0
22.8
24.6
26.4
28.2
30.0
50 80 110 140 170 200 500
25.21 25.20
24.86
24.80
24.34
24.27
23.87
28.76 28.76
28.21
28.18
27.76
27.62
26.96
B
L
E
U
Silence threshold for segmentation (ms)
Figure 5: BLEU score of English-Spanish and
Spanish-English MT models on the ASR output us-
ing silence segmentation
8.3 Incremental Speech Translation Results
Figure 6 shows the BLEU score for incremental
speech translation described in Section 3. In the fig-
ure, partial refers to Eq. 2, continue-partial refers to
Eq. 3 and complete refers to Eq. 4. The continue-
partials option was exercised by using the continue-
443
02.78
5.56
8.33
11.11
13.89
16.67
19.44
22.22
25.00
10 20 30 40 50 60 70 80 90 100 200 300 400 500 600 700 800 900 1000
B
L
E
U
Speech Recognizer timeouts (msec)
Partial (Eq. 1)
Moses ?continue-partial? (Eq. 2)
Complete (Eq. 3)
Figure 6: BLEU score (Spanish-English) for incremental speech translation across varying timeout periods
in the speech recognizer
partial-translation parameter in Moses (Koehn et al,
2007). The partial hypotheses are generated as a
function of speech recognizer timeouts. Timeout is
defined as the time interval with which the speech
recognizer generates partial hypotheses. For each
timeout interval, the speech recognizer may or may
not generate a partial result based on the search path
at that instant in time. As the timeout interval in-
creases, the performance of incremental translation
approaches that of non-incremental translation. The
key is to choose an operating point such that the
user perception of latency is minimal with accept-
able BLEU score. It is interesting that very good
performance can be attained at a timeout of 500 ms
in comparison with non-incremental speech trans-
lation, i.e., the latency can be reduced in half with
acceptable translation quality. The continue-partial
option in Moses performs slightly better than the
partial case as it conditions the decision on prior
source input as well as translation.
In Table 3, we present the latency measurements
of the various components in our framework. We do
not have a row for ASR since it is not possible to get
the start time for each recognition run as the RTP
packets are continuously flowing in the SIP frame-
work. The latency between various system compo-
nents is very low (5-30 ms). While the average time
taken for translation (incremental) is ? 100 ms, the
TTS takes the longest time as it is non-incremental
in the current work. It can also been seen that the
average time taken for generating incremental MT
output is half that of TTS that is non-incremental.
The overall results show that the communication in
our SIP-based framework has low latency.
Components Caller Callee Average
ASR output to MT input 6.8 0.1 3.4
MT 100.4 108.8 104.6
MT output to TTS 22.1 33.1 27.6
TTS 246 160.3 203.1
Table 3: Latency measurements (in ms) for the S2S
components in the real-time SIP framework.
9 Conclusion
In this paper, we introduced the problem of incre-
mental speech-to-speech translation and presented
first of a kind two-way real-time speech-to-speech
translation system based on SIP that incorporates
the notion of incrementality. We presented details
about the SIP framework and demonstrated the typ-
ical call flow in our application. We also presented
a dialog corpus collected using our framework and
benchmarked the performance of the system. Our
framework allows for incremental speech transla-
tion and can provide low latency translation. We
are currently working on improving the accuracy of
incremental translation. We are also exploring new
algorithms for performing reordering aware incre-
mental speech-to-speech translation, i.e., translating
source phrases such that text-to-speech synthesis can
be rendered incrementally.
444
References
M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou, and
A. Syrdal. 1999. The AT&T Next-Gen TTS sys-
tem. In Proceedings of Joint Meeting of ASA, EAA
and DEGA.
E. Bocchieri. 1993. Vector quantization for the efficient
computation of continuous density likelihoods. Pro-
ceedings of ICASSP.
Charles L. Chen and T. V. Raman. 2008. Axsjax: a talk-
ing translation bot using google im: bringing web-2.0
applications to life. In Proceedings of the 2008 inter-
national cross-disciplinary conference on Web acces-
sibility (W4A).
O. Furuse and H. Iida. 1996. Incremental translation uti-
lizing constituent boundary patterns. In Proc. of Col-
ing ?96.
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,
Dilek Hakkani Tur, Andrej Ljolje, and Sarangarajan
Parthasarathy. 2004. The AT&T Watson Speech Rec-
ognizer. Technical report, September.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, Shen W.,
C. Moran, R. Zens, C. J. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
E. Matusov, D. Hillard, M. Magimai-Doss, D. Hakkani-
Tu?r, M. Ostendorf, and H. Ney. 2007. Improving
speech translation with automatic boundary predic-
tion. In Proceedings of Interspeech.
F. Metze, J. McDonough, H. Soltau, A. Waibel, A. Lavie,
S. Burger, C. Langley, L. Levin, T. Schultz, F. Pianesi,
R. Cattoni, G. Lazzari, N. Mana, and E. Pianta. 2002.
The NESPOLE! speech-to-speech translation system.
M. Mohri, F. Pereira, and M. Riley. 1997. Att
general-purpose finite-state machine software tools,
http://www.research.att.com/sw/tools/fsm/.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
M. Paul. 2006. Overview of the iwslt 2006 evaluation
campaign. In Proceedings of the International Work-
shop of Spoken Language Translation, Kyoto, Japan.
V. K. Rangarajan Sridhar, L. Barbosa, and S. Bangalore.
2011. A scalable approach to building a parallel cor-
pus from the Web. In Proceedings of Interspeech.
B. Sankaran, A. Grewal, and A. Sarkar. 2010. Incre-
mental decoding for phrase-based statistical machine
translation. In Proceedings of the fifth Workshop on
Statistical Machine Translation and Metrics.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of AMTA.
R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-
javec, and D. Tufis. 2006. The JRC-Acquis: A multi-
lingual aligned parallel corpus with 20+ languages. In
Proceedings of LREC.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of ICSLP.
J. Tiedemann and L. Lars Nygaard. 2004. The OPUS
corpus - parallel & free. In Proceedings of LREC.
Wolfgang Wahlster, editor. 2000. Verbmobil: Founda-
tions of Speech-to-Speech Translation. Springer.
A. Waibel, A. N. Jain, A. E. McNair, H. Saito, A. G.
Hauptmann, and J. Tebelskis. 1991. JANUS: a
speech-to-speech translation system using connection-
ist and symbolic processing strategies. In Proceedings
of ICASSP, pages 793?796, Los Alamitos, CA, USA.
A. Waibel, A. Badran, A. W. Black, R. Frederk-
ing, G. Gates, A. Lavie, L. Levin, K. Lenzo,
L. M. Tomokiyo, J. Reichert, T. Schultz, W. Dorcas,
M. Woszczyna, and J. Zhang. 2003. Speechalator:
two-way speech-to-speech translation on a consumer
PDA. In Proceedings of the European Conference on
Speech Communication and Technology, pages 369?
372.
B. Zhou, Y. Gao, J. Sorenson, D. Dechelotte, and
M. Picheny. 2003. A hand-held speech-to-speech
translation system. In Proceedings of ASRU.
445
Proceedings of NAACL-HLT 2013, pages 230?238,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Segmentation Strategies for Streaming Speech Translation
Vivek Kumar Rangarajan Sridhar, John Chen, Srinivas Bangalore
Andrej Ljolje, Rathinavelu Chengalvarayan
AT&T Labs - Research
180 Park Avenue, Florham Park, NJ 07932
vkumar,jchen,srini,alj,rathi@research.att.com
Abstract
The study presented in this work is a first ef-
fort at real-time speech translation of TED
talks, a compendium of public talks with dif-
ferent speakers addressing a variety of top-
ics. We address the goal of achieving a sys-
tem that balances translation accuracy and la-
tency. In order to improve ASR performance
for our diverse data set, adaptation techniques
such as constrained model adaptation and vo-
cal tract length normalization are found to be
useful. In order to improve machine transla-
tion (MT) performance, techniques that could
be employed in real-time such as monotonic
and partial translation retention are found to
be of use. We also experiment with inserting
text segmenters of various types between ASR
and MT in a series of real-time translation ex-
periments. Among other results, our experi-
ments demonstrate that a good segmentation
is useful, and a novel conjunction-based seg-
mentation strategy improves translation qual-
ity nearly as much as other strategies such
as comma-based segmentation. It was also
found to be important to synchronize various
pipeline components in order to minimize la-
tency.
1 Introduction
The quality of automatic speech-to-text and speech-
to-speech (S2S) translation has improved so signifi-
cantly over the last several decades that such systems
are now widely deployed and used by an increasing
number of consumers. Under the hood, the individ-
ual components such as automatic speech recogni-
tion (ASR), machine translation (MT) and text-to-
speech synthesis (TTS) that constitute a S2S sys-
tem are still loosely coupled and typically trained
on disparate data and domains. Nevertheless, the
models as well as the pipeline have been optimized
in several ways to achieve tasks such as high qual-
ity offline speech translation (Cohen, 2007; Kings-
bury et al, 2011; Federico et al, 2011), on-demand
web based speech and text translation, low-latency
real-time translation (Wahlster, 2000; Hamon et al,
2009; Bangalore et al, 2012), etc. The design of a
S2S translation system is highly dependent on the
nature of the audio stimuli. For example, talks, lec-
tures and audio broadcasts are typically long and re-
quire appropriate segmentation strategies to chunk
the input signal to ensure high quality translation.
In contrast, single utterance translation in several
consumer applications (apps) are typically short and
can be processed without the need for additional
chunking. Another key parameter in designing a
S2S translation system for any task is latency. In
offline scenarios where high latencies are permit-
ted, several adaptation strategies (speaker, language
model, translation model), denser data structures (N-
best lists, word sausages, lattices) and rescoring pro-
cedures can be utilized to improve the quality of
end-to-end translation. On the other hand, real-
time speech-to-text or speech-to-speech translation
demand the best possible accuracy at low latencies
such that communication is not hindered due to po-
tential delay in processing.
In this work, we focus on the speech translation
of talks. We investigate the tradeoff between accu-
racy and latency for both offline and real-time trans-
lation of talks. In both these scenarios, appropriate
segmentation of the audio signal as well as the ASR
hypothesis that is fed into machine translation is crit-
ical for maximizing the overall translation quality of
the talk. Ideally, one would like to train the models
on entire talks. However, such corpora are not avail-
able in large amounts. Hence, it is necessary to con-
230
form to appropriately sized segments that are similar
to the sentence units used in training the language
and translation models. We propose several non-
linguistic and linguistic segmentation strategies for
the segmentation of text (reference or ASR hypothe-
ses) for machine translation. We address the prob-
lem of latency in real-time translation as a function
of the segmentation strategy; i.e., we ask the ques-
tion ?what is the segmentation strategy that maxi-
mizes the number of segments while still maximiz-
ing translation accuracy??.
2 Related Work
Speech translation of European Parliamentary
speeches has been addressed as part of the TC-
STAR project (Vilar et al, 2005; Fu?gen et al, 2006).
The project focused primarily on offline translation
of speeches. Simultaneous translation of lectures
and speeches has been addressed in (Hamon et al,
2009; Fu?gen et al, 2007). However, the work fo-
cused on a single speaker in a limited domain. Of-
fline speech translation of TED1 talks has been ad-
dressed through the IWSLT 2011 and 2012 evalua-
tion tracks. The talks are from a variety of speakers
with varying dialects and cover a range of topics.
The study presented in this work is the first effort on
real-time speech translation of TED talks. In com-
parison with previous work, we also present a sys-
tematic study of the accuracy versus latency tradeoff
for both offline and real-time translation on the same
dataset.
Various utterance segmentation strategies for of-
fline machine translation of text and ASR output
have been presented in (Cettolo and Federico, 2006;
Rao et al, 2007; Matusov et al, 2007). The work
in (Fu?gen et al, 2007; Fu?gen and Kolss, 2007)
also examines the impact of segmentation on of-
fline speech translation of talks. However, the real-
time analysis in that work is presented only for
speech recognition. In contrast with previous work,
we tackle the latency issue in simultaneous transla-
tion of talks as a function of segmentation strategy
and present some new linguistic and non-linguistic
methodologies. We investigate the accuracy versus
latency tradeoff across translation of reference text,
utterance segmented speech recognition output and
1http://www.ted.com
partial speech recognition hypotheses.
3 Problem Formulation
The basic problem of text translation can be formu-
lated as follows. Given a source (French) sentence
f = fJ1 = f1, ? ? ? , fJ , we aim to translate it into
target (English) sentence e? = e?I1 = e?1, ? ? ? , e?I .
e?(f) = arg max
e
Pr(e|f) (1)
If, as in talks, the source text (reference or ASR hy-
pothesis) is very long, i.e., J is large, we attempt
to break down the source string into shorter se-
quences, S = s1 ? ? ? sk ? ? ? sQs , where each sequence
sk = [fjkfjk+1 ? ? ? fj(k+1)?1], j1 = 1, jQs+1 =
J + 1. Let the translation of each foreign sequence
sk be denoted by tk = [eikeik+1 ? ? ? ei(k+1)?1], i1 =
1, iQs+1 = I
?
+ 12. The segmented sequences can
be translated using a variety of techniques such as
independent chunk-wise translation or chunk-wise
translation conditioned on history as shown in Eqs. 2
and 3, respectively. In Eq. 3, t?i denotes the best
translation for source sequence si.
e?(f) = arg max
t1
Pr(t1|s1) ? ? ? arg max
tk
Pr(tk|sk)
(2)
e?(f) = arg max
t1
Pr(t1|s1) arg max
t2
Pr(t2|s2, s1, t
?
1)
? ? ? arg max
tk
Pr(tk|s1, ? ? ? , sk, t
?
1, ? ? ? , t
?
k?1)
(3)
Typically, the hypothesis e? will be more accurate
than e? for long texts as the models approximating
Pr(e|f) are conventionally trained on short text seg-
ments. In Eqs. 2 and 3, the number of sequences Qs
is inversely proportional to the time it takes to gen-
erate partial target hypotheses. Our main focus in
this work is to obtain a segmentation S such that the
quality of translation is maximized with minimal la-
tency. The above formulation for automatic speech
recognition is very similar except that the foreign
string f? = f?J1 = f?1, ? ? ? , f?J? is obtained by decoding
the input speech signal.
2The segmented and unsegmented talk may not be equal in
length, i.e., I 6= I
?
231
Model Language Vocabulary #words #sents Corpora
Acoustic Model en 46899 2611144 148460 1119 TED talks
ASR Language Model en 378915 3398460155 151923101 Europarl, WMT11 Gigaword, WMT11 News crawl
WMT11 News-commentary, WMT11 UN, IWSLT11 TED training
Parallel text en 503765 76886659 7464857 IWSLT11 TED training talks, Europarl, JRC-ACQUIS
Opensubtitles, Web data
MT es 519354 83717810 7464857
Language Model es 519354 83717810 7464857 Spanish side of parallel text
Table 1: Statistics of the data used for training the speech translation models.
4 Data
In this work, we focus on the speech translation
of TED talks, a compendium of public talks from
several speakers covering a variety of topics. Over
the past couple of years, the International Work-
shop on Spoken Language Translation (IWSLT) has
been conducting the evaluation of speech translation
on TED talks for English-French. We leverage the
IWSLT TED campaign by using identical develop-
ment (dev2010) and test data (tst2010). However,
English-Spanish is our target language pair as our
internal projects are cater mostly to this pair. As a
result, we created parallel text for English-Spanish
based on the reference English segments released as
part of the evaluation (Cettolo et al, 2012).
We also harvested the audio data from the TED
website for building an acoustic model. A total
of 1308 talks in English were downloaded, out of
which we used 1119 talks recorded prior to Decem-
ber 2011. We split the stereo audio file and dupli-
cated the data to account for any variations in the
channels. The data for the language models was also
restricted to that permitted in the IWSLT 2011 eval-
uation. The parallel text for building the English-
Spanish translation model was obtained from sev-
eral corpora: Europarl (Koehn, 2005), JRC-Acquis
corpus (Steinberger et al, 2006), Opensubtitle cor-
pus (Tiedemann and Lars Nygaard, 2004), Web
crawling (Rangarajan Sridhar et al, 2011) as well as
human translation of proprietary data. Table 1 sum-
marizes the data used in building the models. It is
important to note that the IWSLT evaluation on TED
talks is completely offline. In this work, we perform
the first investigation into the real-time translation of
these talks.
5 Speech Translation Models
In this section, we describe the acoustic, language
and translation models used in our experiments.
5.1 Acoustic and Language Model
We use the AT&T WATSONSM speech recog-
nizer (Goffin et al, 2004). The speech recogni-
tion component consisted of a three-pass decoding
approach utilizing two acoustic models. The mod-
els used three-state left-to-right HMMs representing
just over 100 phonemes. The phonemes represented
general English, spelled letters and head-body-tail
representation for the eleven digits (with ?zero? and
?oh?). The pronunciation dictionary used the appro-
priate phoneme subset, depending on the type of the
word. The models had 10.5k states and 27k HMMs,
trained on just over 300k utterances, using both of
the stereo channels. The baseline model training was
initialized with several iterations of ML training, in-
cluding two builds of context dependency trees, fol-
lowed by three iterations of Minimum Phone Error
(MPE) training.
The Vocal Tract Length Normalization (VTLN)
was applied in two different ways. One was esti-
mated on an utterance level, and the other at the talk
level. No speaker clustering was attempted in train-
ing. The performance at test time was comparable
for both approaches on the development set. Once
the warps were estimated, after five iterations, the
ML trained model was updated using MPE training.
Constrained model adaptation (CMA) was applied
to the warped features and the adapted features were
recognized in the final pass with the VTLN model.
All the passes used the same LM. For offline recog-
nition the warps, and the CMA adaptation, are per-
formed at the talk level. For the real-time speech
translation experiments, we used the VTLN model.
232
The English language model was built using the
permissible data in the IWSLT 2011 evaluation. The
texts were normalized using a variety of cleanup,
number and spelling normalization techniques and
filtered by restricting the vocabulary to the top
375000 types; i.e., any sentence containing a to-
ken outside the vocabulary was discarded. First, we
removed extraneous characters beyond the ASCII
range followed by removal of punctuations. Sub-
sequently, we normalized hyphenated words and re-
moved words with more than 25 characters. The re-
sultant text was normalized using a variety of num-
ber conversion routines and each corpus was fil-
tered by restricting the vocabulary to the top 150000
types; i.e., any sentence containing a token outside
the vocabulary was discarded. The vocabulary from
all the corpora was then consolidated and another
round of filtering to the top 375000 most frequent
types was performed. The OOV rate on the TED
dev2010 set is 1.1%. We used the AT&T FSM
toolkit (Mohri et al, 1997) to train a trigram lan-
guage model (LM) for each component (corpus). Fi-
nally, the component language models were interpo-
lated by minimizing the perplexity on the dev2010
set. The results are shown in Table 2.
Accuracy (%)
Model dev2010 test2010
Baseline MPE 75.5 73.8
VTLN 78.8 77.4
CMA 80.5 80.0
Table 2: ASR word accuracies on the IWSLT data
sets.3
5.2 Translation Model
We used the Moses toolkit (Koehn et al, 2007) for
performing statistical machine translation. Mini-
mum error rate training (MERT) was performed on
the development set (dev2010) to optimize the fea-
ture weights of the log-linear model used in trans-
lation. During decoding, the unknown words were
preserved in the hypotheses. The data used to train
the model is summarized in Table 1.
3We used the standard NIST scoring package as we did not
have access to the IWSLT evaluation server that may normalize
and score differently
We also used a finite-state implementation of
translation without reordering. Reordering can pose
a challenge in real-time S2S translation as the text-
to-speech synthesis is monotonic and cannot retract
already synthesized speech. While we do not ad-
dress the text-to-speech synthesis of target text in
this work, we perform this analysis as a precursor
to future work. We represent the phrase transla-
tion table as a weighted finite state transducer (FST)
and the language model as a finite state acceptor
(FSA). The weight on the arcs of the FST is the
dot product of the MERT weights with the transla-
tion scores. In addition, a word insertion penalty
was also applied to each word to penalize short hy-
potheses. The decoding process consists of compos-
ing all possible segmentations of an input sentence
with the phrase table FST and language model, fol-
lowed by searching for the best path. Our FST-based
translation is the equivalent of phrase-based transla-
tion in Moses without reordering. We present re-
sults using the independent chunk-wise strategy and
chunk-wise translation conditioned on history in Ta-
ble 3. The chunk-wise translation conditioned on
history was performed using the continue-partial-
translation option in Moses.
6 Segmentation Strategies
The output of ASR for talks is a long string of
words with no punctuation, capitalization or seg-
mentation markers. In most offline ASR systems,
the talk is first segmented into short utterance-like
audio segments before passing them to the decoder.
Prior work has shown that additional segmentation
of ASR hypotheses of these segments may be nec-
essary to improve translation quality (Rao et al,
2007; Matusov et al, 2007). In a simultaneous
speech translation system, one can neither find the
optimal segmentation of the entire talk nor tolerate
high latencies associated with long segments. Con-
sequently, it is necessary to decode the incoming au-
dio incrementally as well as segment the ASR hy-
potheses appropriately to maximize MT quality. We
present a variety of linguistic and non-linguistic seg-
mentation strategies for segmenting the source text
input into MT. In our experiments, they are applied
to different inputs including reference text, ASR 1-
best hypothesis for manually segmented audio and
233
incremental ASR hypotheses from entire talks.
6.1 Non-linguistic segmentation
The simplest method is to segment the incoming text
according to length in number of words. Such a pro-
cedure can destroy semantic context but has little to
no overhead in additional processing. We experi-
ment with segmenting the text according to word
window sizes of length 4, 8, 11, and 15 (denoted
as data sets win4, win8, win11, win15, respectively
in Table 3). We also experiment with concatenating
all of the text from one TED talk into a single chunk
(complete talk).
A novel hold-output model was also developed in
order to segment the input text. Given a pair of par-
allel sentences, the model segments the source sen-
tence into minimally sized chunks such that crossing
links and links of one target word to many source
words in an optimal GIZA++ alignment (Och and
Ney, 2003) occur only within individual chunks.
The motivation behind this model is that if a segment
s0 is input at time t0 to an incremental MT system,
it can be translated right away without waiting for a
segment si that is input at a later time ti, ti > 0. The
hold-output model detects these kinds of segments
given a sequence of English words that are input
from left to right. A kernel-based SVM was used to
develop this model. It tags a token t in the input with
either the label HOLD, meaning to chunk it with the
next token, or the label OUTPUT, meaning to output
the chunk constructed from the maximal consecutive
sequence of tokens preceding t that were all tagged
as HOLD. The model considers a five word and POS
window around the target token t. Unigram, bigram,
and trigram word and POS features based upon this
window are used for classification. Training and de-
velopment data for the model was derived from the
English-Spanish TED data (see Table 1) after run-
ning it through GIZA++. Accuracy of the model on
the development set was 66.62% F-measure for the
HOLD label and 82.75% for the OUTPUT label.
6.2 Linguistic segmentation
Since MT models are trained on parallel text sen-
tences, we investigate segmenting the source text
into sentences. We also investigate segmenting the
text further by predicting comma separated chunks
within sentences. These tasks are performed by
training a kernel-based SVM (Haffner et al, 2003)
on a subset of English TED data. This dataset con-
tained 1029 human-transcribed talks consisting of
about 103,000 sentences containing about 1.6 mil-
lion words. Punctuation in this dataset was normal-
ized as follows. Different kinds of sentence ending
punctuations were transformed into a uniform end of
sentence marker. Double-hyphens were transformed
into commas. Commas already existing in the input
were kept while all other kinds of punctuation sym-
bols were deleted. A part of speech (POS) tagger
was applied to this input. For speed, a unigram POS
tagger was implemented which was trained on the
Penn Treebank (Marcus et al, 1993) and used or-
thographic features to predict the POS of unknown
words. The SVM-based punctuation classifier relies
on a five word and POS window in order to classify
the target word. Specifically, token t0 is classified
given as input the window t?2t?1tot1t2. Unigram,
bigram, and trigram word and POS features based on
this window were used for classification. Accuracy
of the classifier on the development set was 60.51%
F-measure for sentence end detection and 43.43%
F-measure for comma detection. Subsequently, data
sets pred-sent (sentences) and pred-punct (comma-
separated chunks) were obtained. Corresponding to
these, two other data sets ref-sent and ref-punct were
obtained based upon gold-standard punctuations in
the reference.
Besides investigating the use of comma-separated
segments, we investigated other linguistically moti-
vated segments. These included conjunction-word
based segments. These segments are separated at
either conjunction (e.g. ?and,? ?or?) or sentence-
ending word boundaries. Conjunctions were iden-
tified using the unigram POS tagger. F-measure
performance for detecting conjunctions by the tag-
ger on the development set was quite high, 99.35%.
As an alternative, text chunking was performed
within each sentence, with each chunk correspond-
ing to one segment. Text chunks are non-recursive
syntactic phrases in the input text. We investi-
gated segmenting the source into text chunks us-
ing TreeTagger, a decision-tree based text chun-
ker (Schmid, 1994). Initial sets of text chunks
were created by using either gold-standard sentence
boundaries or boundaries detected using the punc-
tuation classifier, yielding the data sets chunk-ref-
234
Reference text ASR 1-best
BLEU Mean BLEU Mean
Segmentation Segmentation Independent chunk-wise chunk-wise #words Independent chunk-wise chunk-wise #words
type strategy FST Moses with history per segment FST Moses with history per segment
win4 22.6 21.0 25.5 3.9?0.1 17.7 17.1 20.0 3.9?0.1
win8 26.6 26.2 28.2 7.9?0.3 20.6 20.9 22.3 7.9?0.2
Non-linguistic win11 27.2 27.4 29.2 10.9? 0.3 21.5 21.8 23.1 10.9?0.4
win15 28.5 28.5 29.4 14.9?0.6 22.3 22.8 23.3 14.9?0.7
ref-hold 13.3 14.0 17.1 1.6?1.9 12.7 13.1 17.5 1.5?1.0
pred-hold 15.9 15.7 16.3 2.2?1.9 12.6 12.9 17.4 1.5?1.0
complete talk 23.8 23.9 ? 2504 18.8 19.2 ? 2515
ref-sent 30.6 31.5 30.5 16.7?11.8 24.3 25.1 24.4 17.0?11.6
ref-punct 30.4 31.5 30.3 7.1?5.3 24.2 25.1 24.1 8.7?6.1
pred-punct 30.6 31.5 30.4 8.7?8.8 24.1 25.0 24.0 8.8?6.8
conj-ref-eos 30.5 31.5 30.2 11.2?7.5 24.1 24.9 24.0 11.5?7.7
conj-pred-eos 30.3 31.2 30.3 10.9?7.9 24.0 24.8 24.0 11.4?8.5
chunk-ref-punct 17.9 18.9 21.4 1.3?0.7 14.5 15.2 16.9 1.4?0.7
Linguistic lgchunk1-ref-punct 21.0 21.8 25.1 1.7?1.0 16.9 17.4 19.6 1.8?1.0
lgchunk2-ref-punct 22.4 23.1 26.0 2.1?1.1 17.9 18.4 20.4 2.1?1.1
lgchunk3-ref-punct 24.3 25.1 27.4 2.5?1.7 19.2 19.9 21.3 2.5?1.7
chunk-pred-punct 17.9 18.9 21.4 1.3?0.7 14.5 15.1 16.9 1.4?0.7
lgchunk1-pred-punct 21.2 21.9 25.2 1.8?1.0 16.7 17.2 19.7 1.8?1.0
lgchunk2-pred-punct 22.6 23.1 26.0 2.1?1.2 17.7 18.3 20.5 2.1?1.2
lgchunk3-pred-punct 24.5 25.3 27.4 2.6?1.8 19.1 20.0 21.3 2.5?1.7
Table 3: BLEU scores at the talk level for reference text and ASR 1-best for various segmentation strategies.
The ASR 1-best was performed on manually segmented audio chunks provided in tst2010 set.
punct and chunk-pred-punct. Chunk types included
NC (noun chunk), VC (verb chunk), PRT (particle),
and ADVC (adverbial chunk).
Because these chunks may not provide sufficient
context for translation, we also experimented with
concatenating neighboring chunks of certain types
to form larger chunks. Data sets lgchunk1 concate-
nate together neighboring chunk sequences of the
form NC, VC or NC, ADVC, VC, intended to cap-
ture as single chunks instances of subject and verb.
In addition to this, data sets lgchunk2 capture chunks
such as PC (prepositional phrase) and VC followed
by VC (control and raising verbs). Finally, data sets
lgchunk3 capture as single chunks VC followed by
NC and optionally followed by PRT (verb and its di-
rect object).
Applying the conjunction segmenter after the
aforementioned punctuation classifier in order to de-
tect the ends of sentences yields the data set conj-
pred-eos. Applying it on sentences derived from the
gold-standard punctuations yields the data set conj-
ref-eos. Finally, applying the hold-output model to
sentences derived using the punctuation classifier
produces the data set pred-hold. Obtaining English
sentences tagged with HOLD and OUTPUT directly
from the output of GIZA++ on English-Spanish sen-
tences in the reference produces the data set ref-hold.
The strategies containing the keyword ref for ASR
simply means that the ASR hypotheses are used in
place of the gold reference text.
 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
 0
 100
0
 200
0
 300
0
 400
0
 500
0
 600
0
processing time per token (sec)
ASR
 tim
eou
t (m
s)ASR
+MT
 (BL
EU)
10.0
11.7
12.6
13.3
13.7
14.0
14.1
14.3
14.6
14.7
14.7
14.8
ASR
+Pu
nct 
Seg
+MT
 (BL
EU)
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
Figure 1: Latencies and BLEU scores for tst2010 set
using incremental ASR decoding and translation
We also performed real-time speech translation by
using incremental speech recognition, i.e., the de-
coder returns partial hypotheses that, independent of
235
the pruning during search, will not change in the
future. Figure 1 shows the plot for two scenarios:
one in which the partial hypotheses are sent directly
to machine translation and another where the best
segmentation strategy pred-punct is used to segment
the partial output before sending it to MT. The plot
shows the BLEU scores as a function of ASR time-
outs used to generate the partial hypotheses. Fig-
ure 1 also shows the average latency involved in in-
cremental speech translation.
7 Discussion
The BLEU scores for the segmentation strategies
over ASR hypotheses was computed at the talk level.
Since the ASR hypotheses do not align with the
reference source text, it is not feasible to evalu-
ate the translation performance using the gold refer-
ence. While other studies have used an approximate
edit distance algorithm for resegmentation of the hy-
potheses (Matusov et al, 2005), we simply concate-
nate all the segments and perform the evaluation at
the talk level.
The hold segmentation strategy yields the poor-
est translation performance. The significant drop in
BLEU score can be attributed to relatively short seg-
ments (2-4 words) that was generated by the model.
The scheme oversegments the text and since the
translation and language models are trained on sen-
tence like chunks, the performance is poor. For ex-
ample, the input text the sea should be translated
as el mar, but instead the hold segmenter chunks it
as the?sea which MT?s chunk translation renders as
el?el mar. It will be interesting to increase the span
of the hold strategy to subsume more contiguous se-
quences and we plan to investigate this as part of
future work.
The chunk segmentation strategy yields quite poor
translation performance. In general, it does not
make the same kinds of errors that the hold strat-
egy makes; for example, the input text the sea will
be treated as one NC chunk by the chunk seg-
mentation strategy, leading MT to translate it cor-
rectly as el mar. The short chunk sizes of chunk
lead to other kinds of errors. For example, the in-
put text we use will be chunked into the NC we
and the VC use, which will be translated incor-
rectly as nosotros?usar; the infinitive usar is se-
lected rather than the properly conjugated form us-
amos. However, there is a marked improvement in
translation accuracy with increasingly larger chunk
sizes (lgchunk1, lgchunk2, and lgchunk3). Notably,
lgchunk3 yields performance that approaches that of
win8 with a chunk size that is one third of win8?s.
The conj-pred-eos and pred-punct strategies work
the best, and it can be seen that the average seg-
ment length (8-12 words) generated in both these
schemes is very similar to that used for training the
models. It is also about the average latency (4-5
seconds) that can be tolerated in cross-lingual com-
munication, also known as ear-voice span (Lederer,
1978). The non-linguistic segmentation using fixed
word length windows also performs well, especially
for the longer length windows. However, longer
windows (win15) increase the latency and any fixed
length window typically destroys the semantic con-
text. It can also be seen from Table 3 that translat-
ing the complete talk is suboptimal in comparison
with segmenting the text. This is primarily due to
bias on sentence length distributions in the training
data. Training models on complete talks is likely to
resolve this issue. Contrasting the use of reference
segments as input to MT (ref-sent, ref-punct, conj-
ref-eos) versus the use of predicted segments (pred-
sent, pred-punct, conj-pred-eos, respectively), it is
interesting to note that the MT accuracies never dif-
fered greatly between the two, despite the noise in
the set of predicted segments.
The performance of the real-time speech transla-
tion of TED talks is much lower than the offline sce-
nario. First, we use only a VTLN model as perform-
ing CMA adaptation in a real-time scenario typically
increases latency. Second, the ASR language model
is trained on sentence-like units and decoding the en-
tire talk with this LM is not optimal. A language
model trained on complete talks will be more appro-
priate for such a framework and we are investigating
this as part of current work.
Comparing the accuracies of different speech
translation strategies, Table 3 shows that pred-punct
performs the best. When embedded in an incremen-
tal MT speech recognition system, Figure 1 shows
that it is more accurate than the system that sends
partial ASR hypotheses directly to MT. This advan-
tage decreases, however, when the ASR timeout pa-
rameter is increased to more than five or six sec-
236
onds. In terms of latency, Figure 1 shows that the
addition of the pred-punct segmenter into the incre-
mental system introduces a significant delay. About
one third of the increase in delay can be attributed
to merely maintaining the two word lookahead win-
dow that the segmenter?s classifier needs to make
decisions. This is significant because this kind of
window has been used quite frequently in previous
work on simultaneous translation (cf. (Fu?gen et al,
2007)), and yet to our knowledge this penalty asso-
ciated with this configuration was never mentioned.
The remaining delay can be attributed to the long
chunk sizes that the segmenter produces. An inter-
esting aspect of the latency curve associated with the
segmenter in Figure 1 is that there are two peaks at
ASR timeouts of 2,500 and 4,500 ms, and that the
lowest latency is achieved at 3,000 ms rather than at
a smaller value. This may be attributed to the fact
that the system is a pipeline consisting of ASR, seg-
menter, and MT, and that 3,000 ms is roughly the
length of time to recite comma-separated chunks.
Consequently, the two latency peaks appear to cor-
respond with ASR producing segments that are most
divergent with segments that the segmenter pro-
duces, leading to the most pipeline ?stalls.? Con-
versely, the lowest latency occurs when the timeout
is set so that ASR?s segments most resemble the seg-
menter?s output to MT.
8 Conclusion
We investigated various approaches for incremen-
tal speech translation of TED talks, with the aim
of producing a system with high MT accuracy and
low latency. For acoustic modeling, we found that
VTLN and CMA adaptation were useful for increas-
ing the accuracy of ASR, leading to a word accuracy
of 80% on TED talks used in the IWSLT evalua-
tion track. In our offline MT experiments retention
of partial translations was found useful for increas-
ing MT accuracy, with the latter being slightly more
helpful. We experimented with several linguistic
and non-linguistic strategies for text segmentation
before translation. Our experiments indicate that a
novel segmentation into conjunction-separated sen-
tence chunks resulted in accuracies almost as high
and latencies almost as short as comma-separated
sentence chunks. They also indicated that signifi-
cant noise in the detection of sentences and punc-
tuation did not seriously impact the resulting MT
accuracy. Experiments on real-time simultaneous
speech translation using partial recognition hypothe-
ses demonstrate that introduction of a segmenter in-
creases MT accuracy. They also showed that in or-
der to reduce latency it is important for buffers in dif-
ferent pipeline components to be synchronized so as
to minimize pipeline stalls. As part of future work,
we plan to extend the framework presented in this
work for performing speech-to-speech translation.
We also plan to address the challenges involved in
S2S translation across languages with very different
word order.
Acknowledgments
We would like to thank Simon Byers for his help
with organizing the TED talks data.
References
S. Bangalore, V. K. Rangarajan Sridhar, P. Kolan,
L. Golipour, and A. Jimenez. 2012. Real-time in-
cremental speech-to-speech translation of dialogs. In
Proceedings of NAACL:HLT, June.
M. Cettolo and M. Federico. 2006. Text segmentation
criteria for statistical machine translation. In Proceed-
ings of the 5th international conference on Advances
in Natural Language Processing.
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3:
Web Inventory of Transcribed and Translated Talks. In
Proceedings of EAMT.
J. Cohen. 2007. The GALE project: A description and
an update. In Proceedings of ASRU Workshop.
M. Federico, L. Bentivogli, M. Paul, and S. Stu?ker. 2011.
Overview of the IWSLT 2011 evaluation campaign. In
Proceedings of IWSLT.
C. Fu?gen and M. Kolss. 2007. The influence of utterance
chunking on machine translation performance. In Pro-
ceedings of Interspeech.
C. Fu?gen, M. Kolss, D. Bernreuther, M. Paulik, S. Stuker,
S. Vogel, and A. Waibel. 2006. Open domain speech
recognition & translation: Lectures and speeches. In
Proceedings of ICASSP.
C. Fu?gen, A. Waibel, and M. Kolss. 2007. Simultaneous
translation of lectures and speeches. Machine Trans-
lation, 21:209?252.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tu?r,
A. Ljolje, and S. Parthasarathy. 2004. The AT&T
Watson Speech Recognizer. Technical report, Septem-
ber.
237
P. Haffner, G. Tu?r, and J. Wright. 2003. Optimizing
svms for complex call classification. In Proceedings
of ICASSP?03.
O. Hamon, C. Fu?gen, D. Mostefa, V. Arranz, M. Kolss,
A. Waibel, and K. Choukri. 2009. End-to-end evalua-
tion in simultaneous translation. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), March.
B. Kingsbury, H. Soltau, G. Saon, S. Chu, Hong-Kwang
Kuo, L. Mangu, S. Ravuri, N. Morgan, and A. Janin.
2011. The IBM 2009 GALE Arabic speech translation
system. In Proceedings of ICASSP.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, Shen W.,
C. Moran, R. Zens, C. J. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit.
M. Lederer. 1978. Simultaneous interpretation: units of
meaning and other features. In D. Gerver and H. W.
Sinaiko, editors, Language interpretation and commu-
nication, pages 323?332. Plenum Press, New York.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn treebank. Computational Linguistics,
19(2):313?330.
E. Matusov, G. Leusch, O. Bender, and H. Ney. 2005.
Evaluating machine translation output with automatic
sentence segmentation. In Proceedings of IWSLT.
E. Matusov, D. Hillard, M. Magimai-Doss, D. Hakkani-
Tu?r, M. Ostendorf, and H. Ney. 2007. Improving
speech translation with automatic boundary predic-
tion. In Proceedings of Interspeech.
M. Mohri, F. Pereira, and M. Riley. 1997. At&t
general-purpose finite-state machine software tools,
http://www.research.att.com/sw/tools/fsm/.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
V. K. Rangarajan Sridhar, L. Barbosa, and S. Bangalore.
2011. A scalable approach to building a parallel cor-
pus from the Web. In Proceedings of Interspeech.
S. Rao, I. Lane, and T. Schultz. 2007. Optimizing sen-
tence segmentation for spoken language translation. In
Proceedings of Interspeech.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Language Pro-
cessing.
R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-
javec, and D. Tufis. 2006. The JRC-Acquis: A multi-
lingual aligned parallel corpus with 20+ languages. In
Proceedings of LREC.
J. Tiedemann and L. Lars Nygaard. 2004. The OPUS
corpus - parallel & free. In Proceedings of LREC.
D. Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney.
2005. Statistical machine translation of European par-
liamentary speeches. In Proceedings of MT Summit.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
238
