Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 466?474,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Multimodal Subjectivity Analysis of Multiparty Conversation
Stephan Raaijmakers
TNO Information and
Communication Technology
Delft, The Netherlands
stephan.raaijmakers@tno.nl
Khiet Truong
TNO Defense, Security and Safety
Soesterberg, The Netherlands
khiet.truong@tno.nl
Theresa Wilson
School of Informatics
University of Edinburgh
Edingburgh, UK
twilson@inf.ed.ac.uk
Abstract
We investigate the combination of several
sources of information for the purpose of sub-
jectivity recognition and polarity classification
in meetings. We focus on features from two
modalities, transcribed words and acoustics,
and we compare the performance of three dif-
ferent textual representations: words, charac-
ters, and phonemes. Our experiments show
that character-level features outperform word-
level features for these tasks, and that a care-
ful fusion of all features yields the best perfor-
mance. 1
1 Introduction
Opinions, sentiments and other types of subjective
content are an important part of any meeting. Meet-
ing participants express pros and cons about ideas,
they support or oppose decisions, and they make
suggestions that may or may not be adopted. When
recorded and archived, meetings become a part of
the organizational knowledge, but their value is lim-
ited by the ability of tools to search and summa-
rize meeting content, including subjective content.
While progress has been made on recognizing pri-
marily objective meeting content, for example, in-
formation about the topics that are discussed (Hsueh
and Moore, 2006) and who is assigned to work on
given tasks (Purver et al, 2006), there has been
1This work was supported by the Dutch BSIK-project Mul-
timediaN, and the European IST Programme Project FP6-
0033812. This paper only reflects the authors? views and fund-
ing agencies are not liable for any use that may be made of the
information contained herein.
fairly little work specifically directed toward recog-
nizing subjective content.
In contrast, there has been a wealth of research
over the past several years on automatic subjectiv-
ity and sentiment analysis in text, including on-line
media. Partly inspired by the rapid growth of so-
cial media, such as blogs, as well as on-line news
and reviews, researchers are now actively address-
ing a wide variety of new tasks, ranging from blog
mining (e.g., finding opinion leaders in an on-line
community), to reputation management (e.g. find-
ing negative opinions about a company on the web),
to opinion-oriented summarization and question an-
swering. Yet many challenges remain, including
how best to represent and combine linguistic infor-
mation for subjectivity analysis. With the additional
modalities that are present when working with face-
to-face spoken communication, these challenges are
even more pronounced.
The work in this paper focuses on two tasks: (1)
recognizing subjective utterances and (2) discrimi-
nating between positive and negative subjective ut-
terances. An utterance may be subjective because
the speaker is expressing an opinion, because the
speaker is discussing someone else?s opinion, or be-
cause the speaker is eliciting the opinion of someone
else with a question.
We approach the above tasks as supervised ma-
chine learning problems, with the specific goal of
finding answers to the following research questions:
? Given a variety of information sources, such
as text arising from (transcribed) speech,
phoneme representations of the words in an ut-
terance, and acoustic features extracted from
466
the audio layer, which of these sources are par-
ticularly valuable for subjectivity analysis in
multiparty conversation?
? Does the combination of these sources lead to
further improvement?
? What are the optimal representations of these
information sources in terms of feature design
for a machine learning component?
A central tenet of our approach is that subword
representations, such as character and phoneme n-
grams, are beneficial for the tasks at hand.
2 Subword Features
Previous work has demonstrated that textual units
below the word level, such as character n-grams,
are valuable sources of information for various
text classification tasks. An example of character
n-grams is the set of 3-grams {#se, sen, ent,
nti, tim, ime, men, ent, nt#, t#a,
#an, ana, nal, aly, lys, ysi, sis,
is#} for the two-word phrase sentiment analysis.
The special symbol # represents a word boundary.
While it is not directly obvious that there is much
information in these truncated substrings, character
n-grams have successfully been used for fine-
grained classification tasks, such as named-entity
recognition (Klein et al, 2003) and subjective
sentence recognition (Raaijmakers and Kraaij,
2008), as well as a variety of document-level tasks
(Stamatatos, 2006; Zhang and Lee, 2006; Kanaris
and Stamatatos, 2007).
The informativeness of these low-level features
comes in part from a form of attenuation (Eisner,
1996): a slight abstraction of the underlying data
that leads to the formation of string equivalence
classes. For instance, words in a sentence will in-
variably share many character n-grams. Since ev-
ery unique character n-gram in an utterance consti-
tutes a separate feature, this leads to the formation
of string classes, which is a form of abstraction. For
example, Zhang and Lee (2006) investigate similar
subword representations, called key substring group
features. By compressing substrings in a corpus in a
trie (a prefix tree), and labeling entire sets of distri-
butionally equivalent substrings with one group la-
bel, an attenuation effect is obtained that proves very
beneficial for a number of text classification tasks.
Aside from attenuation effects, character n-
grams, especially those that represent word bound-
aries, have additional benefits. Treating word
boundaries as characters captures micro-phrasal in-
formation: short strings that express the transition
of one word to another. Stemming occurs naturally
within the set of initial character n-grams of a word,
where the suffix is left out. Also, some part-of-
speech information is captured. For example, the
modals could, would, should can be represented by
the 4-gram, ould, and the set of adverbs ending in
-ly can be represented by the 3-gram ly#.
A challenging thought is to extend the use of n-
grams to the level of phonemes, which comprise
the first symbolic level in the process of sound to
grapheme conversion. If n-grams of phonemes com-
pare favorably to word n-grams for the purpose of
sentiment classification, then significant speedups
can be obtained for online sentiment classification,
since tokenization of the raw speech signal can make
a halt at the phoneme level.
3 Data
For this work we use 13 meetings from the AMI
Meeting Corpus (Carletta et al, 2005). Each meet-
ing has four participants and is approximately 30
minutes long. The participants play specific roles
(e.g., Project Manager, Marketing Expert) and to-
gether function as a design team. Within the set of
13 meetings, there are a total of 20 participants, with
each participant taking part in two or three meet-
ings as part of the same design team. Meetings with
the same set of participants represent different stages
in the design process (e.g., Conceptual Design, De-
tailed Design).
The meetings used in the experiments have been
annotated for subjective content using the AMIDA
annotation scheme (Wilson, 2008). Table 1 lists
the types of annotations that are marked in the data.
There are three main categories of annotations, sub-
jective utterances, subjective questions, and objec-
tive polar utterances. A subjective utterance is a
span of words (or possibly sounds) where a pri-
vate state is being expressed either through choice of
words or prosody. A private state (Quirk et al, 1985)
467
is an internal mental or emotional state, including
opinions, beliefs, sentiments, emotions, evaluations,
uncertainties, and speculations, among others. Al-
though typically when a private state is expressed
it is the private state of the speaker, as in example
(1) below, an utterance may also be subjective be-
cause the speaker is talking about the private state
of someone else. For example, in (2) the negative
opinion attributed to the company is what makes the
utterance subjective.
(1) Finding them is really a pain, you know
(2) The company?s decided that teletext is out-
dated
Subjective questions are questions in which the
speaker is eliciting the private state of someone else.
In other words, the speaker is asking about what
someone else thinks, feels, wants, likes, etc., and the
speaker is expecting a response in which the other
person expresses what he or she thinks, feels, wants,
or likes. For example, both (3) and (4) below are
subjective questions.
(3) Do you like the large buttons?
(4) What do you think about the large buttons?
Objective polar utterances are statements or phrases
that describe positive or negative factual information
about something without conveying a private state.
The sentence The camera broke the first time I used
it gives an example of negative factual information;
generally, something breaking the first time it is used
is not good.
For the work in this paper, we focus on recog-
nizing subjectivity in general and distinguishing be-
tween positive and negative subjective utterances.
Positive subjective utterances are those in which any
of the following types of private states are expressed:
agreements, positive sentiments, positive sugges-
tions, arguing for something, beliefs from which
positive sentiments can be inferred, and positive re-
sponses to subjective questions. Negative subjective
utterances express private states that are the oppo-
site of those represented by the positive subjective
category: disagreements, negative sentiments, nega-
tive suggestions, arguing against something, beliefs
from which negative sentiments can be inferred, and
negative responses to subjective questions. Example
(5) below contains two positive subjective utterances
Table 1: AMIDA Subjectivity Annotation Types
Subjective Utterances
positive subjective
negative subjective
positive and negative subjective
uncertainty
other subjective
subjective fragment
Subjective Questions
positive subjective question
negative subjective question
general subjective question
Objective Polar Utterances
positive objective
negative objective
and one negative subjective utterance. Each annota-
tion is indicated by a pair of angle brackets.
(5) Um ?POS-SUBJ it?s very easy to use?.
Um ?NEG-SUBJ but unfortunately it does
lack the advanced functions? ?POS-SUBJ
which I I quite like having on the controls?.
The positive and negative subjective category is for
marking cases of positive and negative subjectivity
that are so closely interconnected that it is difficult
or impossible to separate the two. For example, (6)
below is marked as both positive and negative sub-
jective.
(6) Um ?POS-AND-NEG-SUBJ they?ve also
suggested that we um we only use the remote
control to control the television, not the VCR,
DVD or anything else?.
In (Wilson, 2008), agreement is measured for each
class separately at the level of dialogue act segments.
If a dialogue act overlaps with an annotation of a
particular type, then the segment is considered to
be labelled with that type. Table 2 gives the Kappa
(Cohen, 1960) and % agreement for subjective seg-
ments, positive and negative subjective segments,2
and subjective questions.
2A positive subjective segment is any dialogue act segment
that overlaps with a positive subjective utterance or a positive-
and-negative subjective utterance. The negative subjective seg-
ments are defined similarly.
468
Table 2: Interannotator agreement for the AMIDA sub-
jectivity annotations
Kappa % Agree
Subjective 0.56 79
Pos Subjective 0.58 84
Neg Subjective 0.62 92
Subjective Question 0.56 95
4 Experiments
We conduct two sets of classification experiments.
For the first set of experiments (Task 1), we auto-
matically distinguish between subjective and non-
subjective utterances. For the second set of ex-
periments (Task 2), we focus on distinguishing be-
tween positive and negative subjective utterances.
For both tasks, we use the manual dialogue act seg-
ments available as part of the AMI Corpus as the unit
of classification. For Task 1, a segment is considered
subjective if it overlaps with either a subjective utter-
ance or subjective question annotation. For Task 2,
the segments being classified are those that overlap
with positive or negative subjective utterances. For
this task, we exclude segments that are both positive
and negative. Although limiting the set of segments
to be classified to just those that are positive or nega-
tive makes the task somewhat artificial, it also allows
us to focus in on the performance of features specifi-
cally for this task.3 We use 6226 subjective and 8707
non-subjective dialog acts for Task 1 (with an aver-
age duration of 1.9s, standard deviation of 2.0s), and
3157 positive subjective and 1052 negative subjec-
tive dialog acts for Task 2 (average duration of 2.6s,
standard deviation of 2.3s).
The experiments are performed using 13-fold
cross validation. Each meeting constitutes a separate
fold for testing, e.g., all the segments from meeting 1
make up the test set for fold 1. Then, for a given fold,
the segments from the remaining 12 meetings are
used for training and parameter tuning, with roughly
a 85%, 7%, and 8% split between training, tuning,
and testing sets for each fold. The assignment to
training versus tuning set was random, with the only
constraint being that a segment could only be in the
tuning set for one fold of the data.
3In practice, this excludes about 7% of the positive/negative
segments.
The experiments we perform involve two steps.
First, we train and optimize a classifier for each type
of feature using BoosTexter (Schapire and Singer,
2000) AdaBoost.MH. Then, we investigate the per-
formance of all possible combinations of features
using linear combinations of the individual feature
classifiers.
4.1 Features
The two modalities that are investigated, prosodic,
and textual, are represented by four different
sets of features: prosody (PROS), word n-
grams (WORDS), character n-grams (CHARS), and
phoneme n-grams (PHONES).
Based on previous research on prosody modelling
in a meeting context (Wrede and Shriberg, 2003)
and on the literature in emotion research (Banse and
Scherer, 1996) we extract PROS features that are
mainly based on pitch, energy and the distribution of
energy in the long-term averaged spectrum (LTAS)
(see Table 3). These features are extracted at the
word level and aggregated to the dialogue-act level
by taking the average over the words per dialogue
act. We then normalize the features per speaker per
meeting by converting the raw feature values to z-
scores (z = (x ? ?)/?).
Table 3: Prosodic features used in experiments.
pitch mean, standard deviation, min-
imum, maximum, range, mean
absolute slope
intensity (en-
ergy)
mean, standard deviation, min-
imum, maximum, range, RMS
energy
distribution en-
ergy in LTAS
slope, Hammerberg index, cen-
tre of gravity, skewness
The textual features, WORDS and CHARS, and
the PHONES features are based on a manual tran-
scription of the speech. The PHONES were pro-
duced through dictionary lookup on the words in the
reference transcription. Both CHARS and PHONES
representations include word boundaries as informa-
tive tokens. The textual features for a given seg-
ment are simply all the WORDS/CHARS/PHONES
in that segment. Selection of n-grams is performed
by the learning algorithm.
469
4.2 Single Source Classifiers
We train four single source classifiers using BoosT-
exter, one for each type of feature. For the WORDS,
CHARS, and PHONES, we optimize the classi-
fier by performing a grid search over the parame-
ter space, varying the number of rounds of boosting
(100, 500, 1000, 2000, 5000), the length of the n-
gram (1, 2, 3, 4, 5), and the type of n-gram. Boos-
Texter can be run with three different n-gram con-
figurations: n-gram, s-gram, and f -gram. For the
default configuration (n-gram), BoosTexter searches
for n-grams up to length n. For example, if n = 3,
BoosTexter will consider 1-grams, 2-grams, and 3-
grams. For the s-gram configuration, BoosTexter
will in addition consider sparse n-grams (i.e., n-
grams containing wildcards), such as the * idea. For
the f -gram configuration, BoosTexter will only con-
sider n-grams of a maximum fixed length, e.g., if
n = 3 BoosTexter will only consider 3-grams. For
the PROS classifier, only the number of rounds of
boosting was varied. The parameters are selected
for each fold separately; the parameter set that pro-
duces the highest subjective F1 score on the tuning
set for Task 1, and the highest positive subjective F1
score for Task 2, is used to train the final classifier
for that fold.
4.3 Classifier combination
After the single source classifiers have been trained,
they have to be combined into an aggregate classi-
fier. To this end, we decided to apply a simple linear
interpolation strategy. Linear interpolation of mod-
els is the weighted combination of simple models to
form complex models, and has its roots in generative
language models (Jelinek and Mercer, 1980). (Raai-
jmakers, 2007) has demonstrated its use for discrim-
inative machine learning.
In the present binary class setting, BoosTexter
produces two decision values, one for every class.
For every individual single-source classifier (i.e.,
PROS, WORDS, CHARS and PHONES), separate
weights are estimated that are applied to the decision
values for the two classes produced by these classi-
fiers. These weights express the relative importance
of the single-source classifiers.
The prediction of an aggregate classifier for a
class c is then simply the sum of all weights for
all participating single-source classifiers applied to
the decision values these classifiers produce for this
class. The class with the maximum score wins, just
as in the simple non-aggregate case.
Formally, then, this linear interpolation strategy
finds for n single-source classifiers n interpolation
weights ?1, . . . ?n that minimize the empirical loss
(measured by a loss function L), with ?j the weight
of classifier j (? ? [0, 1]), and C jc (xi) the decision
value of class c produced by classifier j for datum xi
(a feature vector). The two classes are denoted with
0, 1. The true class for datum xi is denoted with x?i.
The loss function is in our case based on subjective
F-measure (Task 1) or positive subjective F-measure
(Task 2) measured on heldout development training
and test data.
The aggregate prediction x?i for datum xi on the
basis of n single-source classifiers then becomes
x?i = arg maxc (
n
?
j=1
?j ? Cjc=0(xi),
n
?
j=1
?j ? Cjc=1(xi))
(1)
and the lambdas are defined as
?nj = arg min?nj ?[0,1]
k
?
i
L(x?i, x?i;?j , . . . , ?n) (2)
The search process for these weights can easily be
implemented with a simple grid search over admis-
sible ranges.
In the experiments described below, we investi-
gate all possible combinations of the four differ-
ent sets of features (PROS, WORDS, CHARS, and
PHONES) to determine which combination yields
the best performance for subjectivity and subjective
polarity recognition.
5 Results and Discussion
Results for the two tasks are given in Tables 4 and 5
and in Figures 1 and 2. We use two baselines, listed
at the top of each table. The bullets in a given row
indicate the features that are being evaluated for a
given experiment. In Table 4, subjective F1, recall,
and precision are reported as well as overall accu-
racy. In Table 4, the F1, recall, and precision scores
are for the positive subjective class. All values in the
tables are averages over the 13 folds.
470
Table 4: Results Task 1: Subjective vs. Non-Subjective.
PROS WORDS CHARS PHONES F1 PREC REC ACC
BASE-SUBJ always chooses subjective class 60.3 43.4 100 43.4
BASE-RAND randomly chooses a class based on priors 41.8 42.9 41.3 50.6
single
? 54.6 55.3 54.5 63.1
? 60.5 68.5 54.5 71.0
? 61.7 67.5 57.2 71.1
? 60.3 66.4 55.5 70.2
double
? ? 63.9 72.1 57.6 73.4
? ? 65.6 71.9 60.3 74.0
? ? 64.6 72.3 58.4 73.7
? ? 66.2 73.8 60.1 74.9
? ? 65.2 73.2 58.8 74.3
? ? 66.1 72.8 60.7 74.5
triple
? ? ? 66.5 74.3 60.3 75.1
? ? ? 65.5 73.5 59.0 74.5
? ? ? 66.5 73.3 60.8 74.8
? ? ? 66.9 74.3 60.9 75.3
quartet ? ? ? ? 67.1 74.5 61.2 75.4
Table 5: Results Task 2: Positive Subjective vs. Negative Subjective.
PROS WORDS CHARS PHONES F1 PREC REC ACC
BASE-POS-SUBJ always chooses positive subjective class 85.6 75.0 100 75.0
BASE-RAND randomly chooses a class based on priors 75.1 74.4 76.1 62.4
single
? 84.8 74.8 98.1 73.9
? 85.6 79.6 93.1 76.8
? 85.9 81.9 90.5 78.0
? 85.5 80.5 91.3 77.0
double
? ? 88.7 83.0 95.4 81.9
? ? 88.7 83.1 95.1 81.8
? ? 88.5 83.3 94.4 81.6
? ? 89.5 84.2 95.7 83.3
? ? 89.2 83.7 95.5 82.8
? ? 89.0 84.2 94.6 82.6
triple
? ? ? 89.6 84.0 96.1 83.4
? ? ? 89.3 83.6 95.8 82.8
? ? ? 89.2 83.7 95.5 82.7
? ? ? 89.8 84.4 96.0 83.8
quartet ? ? ? ? 89.9 84.4 96.2 83.8
It is quite obvious that the combination of differ-
ent sources of information is beneficial, and in gen-
eral, the more information the better the results. The
best performing classifier for Task 1 uses all the fea-
tures, achieving a subjective F1 of 67.1. For Task 2,
the best performing classifier also uses all the fea-
tures, although it does not perform significantly bet-
ter than the classifier using only WORDS, CHARS,
and PHONES.4 This classifier achieves a positive-
subjective F1 of 89.9.
We measured the effects of adding more infor-
mation to the single source classifiers. These re-
sults are listed in Table 6. Of the various feature
types, prosody seems to be the least informative for
both subjectivity and polarity classification. In ad-
dition to producing the single-source classifier with
the lowest performance for both tasks, Table 6 shows
that when prosody is added, of all the features it is
least likely to yield significant improvements.
4We measured significance with the non-parametric
Wilcoxon signed rank test, p < 0.05.
Throughout the experiments, adding an additional
type of textual feature always yields higher results.
In all cases but two, these improvements are sig-
nificant. The best performing of the features are
the character n-grams. Of the single-source exper-
iments, the character n-grams achieve the best per-
formance, with significant improvements in F1 over
the other single-source classifiers for both Task 1
and Task 2. Also, adding character n-grams to other
feature combinations always gives significant im-
provements in performance.
An obvious question that remains is what the ef-
fect is of classifier interpolation on the results. To
answer this question, we conducted two additional
experiments for both tasks. First, we investigated
the performance of an uninterpolated combination
of the four single-source classifiers. In essence, this
combines the separate feature spaces without explic-
itly weighting them. Second, we investigated the re-
sults of training a single BoosTexter model using all
the features, essentially merging all feature spaces
471
Table 6: Addition of features separately (for Task 1 and 2): ?+? for a row-column pair (r, c) means that the addition
of column feature c to the row features r significantly improved r?s F1; ?-? indicates no significant improvement; ?X?
means ?not applicable?
+PROS + WORDS +CHARS +PHONES
Task 1 2 1 2 1 2 1 2
PROS X X + + + + + +
WORDS - + X X + + + +
CHARS - + - + X X - +
PHONES - + + + + + X X
PROS+WORDS X X X X + + + +
PROS+CHARS X X + + X X + +
PROS+PHONES X X + + + + X X
WORDS+CHARS + - X X X X + +
WORDS+PHONES + - X X + + X X
CHARS+PHONES + + + + X X X X
PROS+WORDS+CHARS X X X X X X + +
PROS+WORDS+PHONES X X X X + + X X
PROS+CHARS+PHONES X X + + X X X X
WORDS+CHARS+PHONES + - X X X X X X
F1
 m
ea
su
re
m
ajo
rity
ra
ndpro
s
wo
rds
ch
ars
ph
on
es
pro
s+
wo
rds
pro
s+
ch
ars
pro
s+
ph
on
es
wo
rds
+c
ha
rs
wo
rds
+p
ho
ne
s
ch
ars
+p
ho
ne
s
pro
s+
wo
rds
+c
ha
rs
pro
s+
wo
rds
+p
ho
ne
s
pro
s+
ph
on
es
+c
ha
rs
wo
rds
+c
ha
rs+
ph
on
es
pro
s+
wo
rds
+c
ha
rs+
ph
on
es
40
45
50
55
60
65
70
Figure 1: Results (F1) experiment 1: subjective vs. non-
subjective.
into one agglomerate feature space. The results for
these experiments are given in Table 7, along with
the results from the all-feature interpolated classifi-
cation for comparison.
The results in Table 7 show that interpolation
outperforms both the unweighted and single-model
combinations for both tasks. For Task 1, the ef-
fect of interpolation compared to a single model is
marginal (a .03 point difference in F1). However,
compared to the uninterpolated combination, inter-
polation gives a clear 3.1 points improvement of F1.
For Task 2, interpolation outperforms both the unin-
terpolated and single-model classifiers, with 2 and 3
points improvements in F1, respectively.
F1
 m
ea
su
re
m
ajo
rity
ra
ndpro
s
wo
rds
ch
ars
ph
on
es
pro
s+
wo
rds
pro
s+
ch
ars
pro
s+
ph
on
es
wo
rds
+c
ha
rs
wo
rds
+p
ho
ne
s
ch
ars
+p
ho
ne
s
pro
s+
wo
rds
+c
ha
rs
pro
s+
wo
rds
+p
ho
ne
s
pro
s+
ph
on
es
+c
ha
rs
wo
rds
+c
ha
rs+
ph
on
es
pro
s+
wo
rds
+c
ha
rs+
ph
on
es
70
75
80
85
90
95
10
0
Figure 2: Results (F1) experiment 2: positive subjective
vs. negative subjective.
6 Related Work
Previous work has demonstrated that textual units
below the word level, such as character n-grams,
are valuable sources of information. Character-
level models have successfully been used for named-
entity recognition (Klein et al, 2003), predicting
authorship (Keselj et al, 2003; Stamatatos, 2006),
text categorization (Zhang and Lee, 2006), web page
genre identification (Kanaris and Stamatatos, 2007),
and sentence-level subjectivity recognition (Raaij-
makers and Kraaij, 2008) In spoken-language data,
Hsueh (2008) achieves good results using chains
of phonemes to automatically segment meetings ac-
cording to topic. However, to the best of our knowl-
edge there has been no investigation to date on the
472
Table 7: Results of interpolated classifiers compared to
uninterpolated and single-model classifiers for all fea-
tures.
Task Combination ACC REC PREC F1
1
interpolated 75.4 61.2 74.5 67.1
uninterpolated 73.0 58.7 70.6 64.0
single model 74.7 62.1 72.7 66.8
2
interpolated 83.8 96.2 84.4 89.9
uninterpolated 79.8 98.0 79.7 87.9
single model 79.5 91.0 83.3 86.9
combination of character-level, phoneme-level, and
word-level models for any natural language classifi-
cation tasks.
In text, there has been a significant amount of
research on subjectivity and sentiment recognition,
ranging from work at the phrase level to work on
classifying sentences and documents. Sentence-
level subjectivity classification (e.g., (Riloff and
Wiebe, 2003; Yu and Hatzivassiloglou, 2003)) and
sentiment classification (e.g., (Yu and Hatzivas-
siloglou, 2003; Kim and Hovy, 2004; Hu and Liu,
2004; Popescu and Etzioni, 2005)) is the research
in text most closely related to our work. Of the
sentence-level research, the most similar is work
by Raaijmakers and Kraaij (2008) comparing word-
spanning character n-grams to word-internal char-
acter n-grams for subjectivity classification in news
data. They found that character n-grams spanning
words perform the best.
Research on recognizing subjective content in
multiparty conversation includes work by Somasun-
daran et al (2007) on recognizing sentiments and
arguing in meetings, work by Neiberg el al. (2006)
on recognizing positive, negative, and neutral emo-
tions in meetings, work on recognizing agreements
and disagreements in meetings (Hillard et al, 2003;
Galley et al, 2004; Hahn et al, 2006), and work
by Wrede and Shriberg (2003) on recognizing meet-
ing hotspots. Somasundaran et al use lexical and
discourse features to recognize sentences and turns
where meeting participants express sentiments or ar-
guing. They also use the AMI corpus in their work;
however, the use of different annotations and task
definitions makes it impossible to directly compare
their results and ours. Neiberg et al use acoustic?
prosodic features (Mel-frequency Cepstral Coeffi-
cients (MFCCs) and pitch features) and lexical n-
grams for recognizing emotions in the ISL Meeting
Corpus (Laskowski and Burger, 2006).
Agreements and disagreements are a subset of the
private states represented by the positive and neg-
ative subjective categories used in this work. To
recognise agreements and disagreements automati-
cally, Hillard et al train 3-way decision tree clas-
sifiers (agreement, disagreement, other) using both
word-based and prosodic features. Galley et al
model this task as a sequence tagging problem, and
investigate whether features capturing speaker inter-
actions are useful for recognizing agreements and
disagreements. Hahn et al investigate the use of
contrast classifiers (Peng et al, 2003) for the task,
using only lexical features.
Hotspots are places in a meeting in which the par-
ticipants are highly involved in the discussion. Al-
though high involvement does not necessarily equate
subjective content, in practice, we expect more sen-
timents, opinions, and arguments to be expressed
when participants are highly involved in the discus-
sion. In their work on recognizing meeting hotspots,
Wrede and Shriberg focus on evaluating the contri-
bution of various prosodic features, ignoring lexi-
cal features completely. The results of their study
helped to inform our choice of prosodic features for
the experiments in this paper.
7 Conclusions
In this paper, we investigated the use of prosodic
features, word n-grams, character n-grams, and
phoneme n-grams for subjectivity recognition and
polarity classification of dialog acts in multiparty
conversation. We show that character n-grams
outperform prosodic features, word n-grams and
phoneme n-grams in subjectiviy recognition and po-
larity classification. Combining these features sig-
nificantly improves performance. Comparing the
additive value of the four information sources avail-
able, prosodic information seem to be least in-
formative while character-level information indeed
proves to be a very valuable source. For subjectiv-
ity recognition, a combination of prosodic, word-
level, character-level, and phoneme-level informa-
tion yields the best performance. For polarity clas-
sification, the best performance is achieved with a
473
combination of words, characters and phonemes.
References
R. Banse and K. R. Scherer. 1996. Acoustic profiles in
vocal emotion expression. Journal of Personality and
Social Psychology, pages 614?636.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Wellner.
2005. The AMI meeting corpus. In Proceedings of
the Measuring Behavior Symposium on ?Annotating
and Measuring Meeting Behavior?.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46.
J. Eisner. 1996. An empirical comparison of probability
models for dependency grammar. In Technical Report
IRCS-96-11, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: Use of bayesian networks to model
pragmatic dependencies. In Proceedings of ACL.
S. Hahn, R. Ladner, and M. Ostendorf. 2006. Agree-
ment/disagreement classification: Exploiting unla-
beled data using contrast classifiers. In Proceedings
of HLT/NAACL.
D. Hillard, M. Ostendorf, and E. Shriberg. 2003. De-
tection of agreement vs. disagreement in meetings:
Training with unlabeled data. In Proceedings of
HLT/NAACL.
P. Hsueh and J. Moore. 2006. Automatic topic seg-
mentation and lablelling in multiparty dialogue. In
Proceedings of IEEE/ACM Workshop on Spoken Lan-
guage Technology.
P. Hsueh. 2008. Audio-based unsupervised segmentation
of meeting dialogue. In Proceedings of ICASSP.
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proceedings of KDD.
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of Markov source parameters from sparse data.
In Proceedings, Workshop on Pattern Recognition in
Practice, pages 381?397.
I. Kanaris and E. Stamatatos. 2007. Webpage genre iden-
tification using variable-length character n-grams. In
Proceedings of ICTAI.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. N-
gram-based author profiles for authorship attribution.
In Proceedings of PACLING.
S. Kim and Eduard Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of Coling.
D. Klein, J. Smarr, H. Nguyen, and C.D. Manning. 2003.
Named entity recognition with character-level models.
In Proceedings of CoNLL.
K. Laskowski and S. Burger. 2006. Annotation and anal-
ysis of emotionally relevant behavior in the ISL meet-
ing corpus. In Proceedings of LREC 2006.
D. Neiberg, K. Elenius, and K. Laskowski. 2006. Emo-
tion recognition in spontaneous speech using GMMs.
In Proceedings of INTERSPEECH.
K. Peng, S. Vucetic, B. Han, H. Xie, and Z Obradovic.
2003. Exploiting unlabeled data for improving ac-
curacy of predictive data mining. In Proceedings of
ICDM.
A. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings of
HLT/EMNLP.
M. Purver, P. Ehlen, and J. Niekrasz. 2006. Detecting
action items in multi-party meetings: Annotation and
initial experiments. In Proceedings of MLMI.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman, New York.
S. Raaijmakers and W. Kraaij. 2008. A shallow ap-
proach to subjectivity classification. In Proceedings
of ICWSM.
S. Raaijmakers. 2007. Sentiment classification with in-
terpolated information diffusion kernels. In Proceed-
ings of the First International Workshop on Data Min-
ing and Audience Intelligence for Advertising (AD-
KDD?07).
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
EMNLP.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135?168.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In Pro-
ceedings of SIGdial.
E. Stamatatos. 2006. Ensemble-based author identifica-
tion using character n-grams. In Proceedings of TIR.
T. Wilson. 2008. Annotating subjective content in meet-
ings. In Proceedings of LREC.
B. Wrede and E. Shriberg. 2003. Spotting ?hot spots?
in meetings: Human judgments and prosodic cues. In
Proceedings of EUROSPEECH.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
Proceedings of EMNLP.
D. Zhang and W. S. Lee. 2006. Extracting key-substring-
group features for text classification. In Proceedings
of KDD.
474
Recognizing Contextual Polarity:
An Exploration of Features for Phrase-Level
Sentiment Analysis
Theresa Wilson?
University of Edinburgh
Janyce Wiebe??
University of Pittsburgh
Paul Hoffmann??
University of Pittsburgh
Many approaches to automatic sentiment analysis begin with a large lexicon of words marked
with their prior polarity (also called semantic orientation). However, the contextual polarity of
the phrase in which a particular instance of a word appears may be quite different from the
word?s prior polarity. Positive words are used in phrases expressing negative sentiments, or
vice versa. Also, quite often words that are positive or negative out of context are neutral in
context, meaning they are not even being used to express a sentiment. The goal of this work is to
automatically distinguish between prior and contextual polarity, with a focus on understanding
which features are important for this task. Because an important aspect of the problem is
identifying when polar terms are being used in neutral contexts, features for distinguishing
between neutral and polar instances are evaluated, as well as features for distinguishing between
positive and negative contextual polarity. The evaluation includes assessing the performance
of features across multiple machine learning algorithms. For all learning algorithms except
one, the combination of all features together gives the best performance. Another facet of the
evaluation considers how the presence of neutral instances affects the performance of features for
distinguishing between positive and negative polarity. These experiments show that the presence
of neutral instances greatly degrades the performance of these features, and that perhaps the
best way to improve performance across all polarity classes is to improve the system?s ability to
identify when an instance is neutral.
1. Introduction
Sentiment analysis is a type of subjectivity analysis (Wiebe 1994) that focuses on iden-
tifying positive and negative opinions, emotions, and evaluations expressed in natural
language. It has been a central component in applications ranging from recognizing
? School of Informatics, Edinburgh EH8 9LW, U.K. E-mail: twilson@inf.ed.ac.uk.
?? Department of Computer Science, Pittsburgh, PA 15260, USA. E-mail: {wiebe,hoffmanp}@cs.pitt.edu.
Submission received: 14 November 2006; revised submission received: 8March 2008; accepted for publication:
16 April 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 3
inflammatory messages (Spertus 1997), to tracking sentiments over time in online
discussions (Tong 2001), to classifying positive and negative reviews (Pang, Lee, and
Vaithyanathan 2002; Turney 2002). Although a great deal of work in sentiment analy-
sis has targeted documents, applications such as opinion question answering (Yu and
Hatzivassiloglou 2003; Maybury 2004; Stoyanov, Cardie, and Wiebe 2005) and re-
view mining to extract opinions about companies and products (Morinaga et al 2002;
Nasukawa and Yi 2003) require sentence-level or even phrase-level analysis. For exam-
ple, if a question answering system is to successfully answer questions about people?s
opinions, it must be able not only to pinpoint expressions of positive and negative
sentiments, such as we find in sentence (1), but also to determine when an opinion is not
being expressed by a word or phrase that typically does evoke one, such as condemned
in sentence (2).
(1) African observers generally approved (positive) of his victory while
Western governments denounced (negative) it.
(2) Gavin Elementary School was condemned in April 2004.
A common approach to sentiment analysis is to use a lexicon with information
about which words and phrases are positive and which are negative. This lexicon may
be manually compiled, as is the case with the General Inquirer (Stone et al 1966), a
resource often used in sentiment analysis. Alternatively, the information in the lexicon
may be acquired automatically. Acquiring the polarity of words and phrases is itself
an active line of research in the sentiment analysis community, pioneered by the
work of Hatzivassiloglou and McKeown (1997) on predicting the polarity or semantic
orientation of adjectives. Various techniques have been proposed for learning the
polarity of words. They include corpus-based techniques, such as using constraints
on the co-occurrence in conjunctions of words with similar or opposite polarity
(Hatzivassiloglou and McKeown 1997) and statistical measures of word association
(Turney and Littman 2003), as well as techniques that exploit information about lexical
relationships (Kamps and Marx 2002; Kim and Hovy 2004) and glosses (Esuli and
Sebastiani 2005; Andreevskaia and Bergler 2006) in resources such as WordNet.
Acquiring the polarity of words and phrases is undeniably important, and there
are still open research challenges, such as addressing the sentiments of different senses
of words (Esuli and Sebastiani 2006b; Wiebe and Mihalcea 2006), and so on. However,
what the polarity of a given word or phrase is when it is used in a particular context is
another problem entirely. Consider, for example, the underlined positive and negative
words in the following sentence.
(3) Philip Clapp, president of the National Environment Trust, sums up well
the general thrust of the reaction of environmental movements: ?There is
no reason at all to believe that the polluters are suddenly going to become
reasonable.?
The first underlined word is Trust. Although many senses of the word trust express a
positive sentiment, in this case, the word is not being used to express a sentiment at
all. It is simply part of an expression referring to an organization that has taken on
the charge of caring for the environment. The adjective well is considered positive, and
indeed it is positive in this context. However, the same is not true for the words reason
400
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
and reasonable. Out of context, we would consider both of these words to be positive.1
In context, the word reason is being negated, changing its polarity from positive to
negative. The phrase no reason at all to believe changes the polarity of the proposition that
follows; because reasonable falls within this proposition, its polarity becomes negative.
The word polluters has a negative connotation, but here in the context of the discussion
of the article and its position in the sentence, polluters is being used less to express a
sentiment and more to objectively refer to companies that pollute. To clarify how the
polarity of polluters is affected by its subject role, consider the purely negative sentiment
that emerges when it is used as an object: They are polluters.
We call the polarity that would be listed for a word in a lexicon the word?s prior
polarity, and we call the polarity of the expression in which a word appears, con-
sidering the context of the sentence and document, the word?s contextual polarity.
Although words often do have the same prior and contextual polarity, many times
a word?s prior and contextual polarities differ. Words with a positive prior polarity
may have a negative contextual polarity, or vice versa. Quite often words that are
positive or negative out of context are neutral in context, meaning that they are not
even being used to express a sentiment. Similarly, words that are neutral out of context,
neither positive or negative, may combine to create a positive or negative expression in
context.
The focus of this work is on the recognition of contextual polarity?in particular,
disambiguating the contextual polarity of words with positive or negative prior polar-
ity. We begin by presenting an annotation scheme for marking sentiment expressions
and their contextual polarity in the Multi-perspective Question Answering (MPQA)
opinion corpus. We show that, given a set of subjective expressions (identified from
the existing annotations in the MPQA corpus), contextual polarity can be annotated
reliably.
Using the contextual polarity annotations, we conduct experiments in automatically
distinguishing between prior and contextual polarity. Beginning with a large lexicon of
clues tagged with prior polarity, we identify the contextual polarity of the instances
of those clues in the corpus. The process that we use has two steps, first classifying
each clue as being in a neutral or polar phrase, and then disambiguating the contextual
polarity of the clues marked as polar. For each step in the process, we experiment with a
variety of features and evaluate the performance of the features using several different
machine learning algorithms.
Our experiments reveal a number of interesting findings. First, being able to accu-
rately identify neutral contextual polarity?when a positive or negative clue is not being
used to express a sentiment?is an important aspect of the problem. The importance of
neutral examples has previously been noted for classifying the sentiment of documents
(Koppel and Schler 2006), but ours is the first work to explore how neutral instances
affect classifying the contextual polarity of words and phrases. In particular, we found
that the performance of features for distinguishing between positive and negative po-
larity greatly degrades when neutral instances are included in the experiments.
We also found that achieving the best performance for recognizing contextual po-
larity requires a wide variety of features. This is particularly true for distinguishing
1 It is open to question whether reason should be listed as positive in a sentiment lexicon, because the more
frequent senses of reason involve intention, not sentiment. However, any existing sentiment lexicon one
would start with will have some noise and errors. The task in this article is to disambiguate instances of
the entries in a given sentiment lexicon.
401
Computational Linguistics Volume 35, Number 3
between neutral and polar instances. Although some features help to increase polar or
neutral recall or precision, it is only the combination of features together that achieves
significant improvements in accuracy over the baselines. Our experiments show that for
distinguishing between positive and negative instances, features capturing negation are
clearly the most important. However, there is more to the story than simple negation.
Features that capture relationships between instances of clues also perform well, indi-
cating that identifying features that represent more complex interdependencies between
sentiment clues may be an important avenue for future research.
The remainder of this article is organized as follows. Section 2 gives an overview
of some of the things that can influence contextual polarity. In Section 3, we describe
our corpus and present our annotation scheme and inter-annotator agreement study
for marking contextual polarity. Sections 4 and 5 describe the lexicon used in our
experiments and how the contextual polarity annotations are used to determine the
gold-standard tags for instances from the lexicon. In Section 6, we consider what kind of
performance can be expected from a simple, prior-polarity classifier. Section 7 describes
the features that we use for recognizing contextual polarity, and our experiments
and results are presented in Section 8. In Section 9 we discuss related work, and we
conclude in Section 10.
2. Polarity Influencers
Phrase-level sentiment analysis is not a simple problem. Many things besides negation
can influence contextual polarity, and even negation is not always straightforward.
Negation may be local (e.g., not good), or involve longer-distance dependencies such as
the negation of the proposition (e.g., does not look very good) or the negation of the subject
(e.g., no one thinks that it?s good). In addition, certain phrases that contain negation words
intensify rather than change polarity (e.g., not only good but amazing). Contextual polarity
may also be influenced bymodality: whether the proposition is asserted to be real (realis)
or not real (irrealis) (no reason at all to believe is irrealis, for example); word sense (e.g.,
Environmental Trust vs. He has won the people?s trust); the syntactic role of a word in the
sentence: whether the word is the subject or object of a copular verb (consider polluters
are versus they are polluters); and diminishers such as little (e.g., little truth, little threat).
Polanyi and Zaenen (2004) give a detailed discussion of many of these types of polarity
influencers. Many of these contextual polarity influencers are represented as features in
our experiments.
Contextual polarity may also be influenced by the domain or topic. For example,
the word cool is positive if used to describe a car, but it is negative if it is used to
describe someone?s demeanor. Similarly, a word such as fever is unlikely to be expressing
a sentiment when used in a medical context. We use one feature in our experiments to
represent the topic of the document.
Another important aspect of contextual polarity is the perspective of the person
who is expressing the sentiment. For example, consider the phrase failed to defeat
in the sentence Israel failed to defeat Hezbollah. From the perspective of Israel, failed
to defeat is negative. From the perspective of Hezbollah, failed to defeat is positive.
Therefore, the contextual polarity of this phrase ultimately depends on the perspec-
tive of who is expressing the sentiment. Although automatically detecting this kind
of pragmatic influence on polarity is beyond the scope of this work, this as well as
the other types of polarity influencers all are considered when annotating contextual
polarity.
402
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
3. Data and Annotations
For the experiments in this work, we need a corpus that is annotated comprehensively
for sentiment expressions and their contextual polarity. Rather than building a corpus
from scratch, we chose to add contextual polarity annotations to the existing annota-
tions in the Multi-perspective Question Answering (MPQA) opinion corpus2 (Wiebe,
Wilson, and Cardie 2005).
The MPQA corpus is a collection of English-language versions of news documents
from the world press. The documents contain detailed, expression-level annotations
of attributions and private states (Quirk et al 1985). Private states are mental and
emotional states; they include beliefs, speculations, intentions, and sentiments, among
others. Although sentiments are not distinguished from other types of private states
in the existing annotations, they are a subset of what already is annotated. This makes
the annotations in the MPQA corpus a good starting point for annotating sentiment
expressions and their contextual polarity.
3.1 Annotation Scheme
When developing our annotation scheme for sentiment expressions and contextual
polarity, there were three main questions to address. First, which of the existing annota-
tions in the MPQA corpus have the possibility of being sentiment expressions? Second,
which of the possible sentiment expressions actually are expressing sentiments? Third,
what coding scheme should be used for marking contextual polarity?
TheMPQA annotation scheme has four types of annotations: objective speech event
frames, two types of private state frames, and agent frames that are used for marking
speakers of speech events and experiencers of private states. A full description of
the MPQA annotation scheme and an agreement study evaluating key aspects of the
scheme are found in Wiebe, Wilson, and Cardie (2005).
The two types of private state frames, direct subjective frames and expressive sub-
jective element frames, are where we will find sentiment expressions. Direct subjective
frames are used to mark direct references to private states as well as speech events in
which private states are being expressed. For example, in the following sentences, fears,
praised, and said are all marked as direct subjective annotations.
(4) The U.S. fears a spill-over of the anti-terrorist campaign.
(5) Italian senator Renzo Gubert praised the Chinese government?s efforts.
(6) ?The report is full of absurdities,? he said.
The word fears directly refers to a private state; praised refers to a speech event
in which a private state is being expressed; and said is marked as direct subjective
because a private state is being expressed within the speech event referred to by
said. Expressive subjective elements indirectly express private states through the way
something is described or through a particular wording. In example (6), the phrase
full of absurdities is an expressive subjective element. Subjectivity (Banfield 1982; Wiebe
2 Available at http://nrrc.mitre.org/NRRC/publications.htm.
403
Computational Linguistics Volume 35, Number 3
1994) refers to the linguistic expression of private states, hence the names for the two
types of private state annotations.
All expressive subjective elements are included in the set of annotations that have
the possibility of being sentiment expressions, but the direct subjective frames to include
in this set can be pared down further. Direct subjective frames have an attribute, expres-
sion intensity, that captures the contribution of the annotated word or phrase to the
overall intensity of the private state being expressed. Expression intensity ranges from
neutral to high. In the given sentences, fears and praised have an expression intensity of
medium, and said has an expression intensity of neutral. A neutral expression intensity
indicates that the direct subjective phrase itself is not contributing to the expression
of the private state. If this is the case, then the direct subjective phrase cannot be
a sentiment expression. Thus, only direct subjective annotations with a non-neutral
expression intensity are included in the set of annotations that have the possibility of
being sentiment expressions. We call this set of annotations, the union of the expres-
sive subjective elements and the direct subjective frames with a non-neutral intensity,
the subjective expressions in the corpus; these are the annotations we will mark for
contextual polarity.
Table 1 gives a sample of subjective expressions marked in the MPQA corpus.
Although many of the words and phrases express what we typically think of as
sentiments, others do not, for example, believes, very definitely, and unconditionally and
without delay.
Now that we have identified which annotations have the possibility of being sen-
timent expressions, the next question is which of these annotated words and phrases
are actually expressing sentiments. We define a sentiment as a positive or negative
emotion, evaluation, or stance. On the left of Table 2 are examples of positive sentiments;
examples of negative sentiments are on the right.
Table 1
Sample of subjective expressions from the MPQA corpus.
victory of justice and freedom such a disadvantageous situation
grown tremendously must
such animosity not true at all
throttling the voice imperative for harmonious society
disdain and wrath glorious
so exciting disastrous consequences
could not have wished for a better situation believes
freak show the embodiment of two-sided justice
if you?re not with us, you?re against us appalling
vehemently denied very definitely
everything good and nice once and for all
under no circumstances shameful mum
most fraudulent, terrorist and extremist enthusiastically asked
number one democracy hate
seems to think gross misstatement
indulging in blood-shed and their lunaticism surprised, to put it mildly
take justice to pre-historic times unconditionally and without delay
so conservative that it makes Pat Buchanan look vegetarian
those digging graves for others, get engraved themselves
lost the reputation of commitment to principles of human justice
ultimately the demon they have reared will eat up their own vitals
404
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 2
Examples of positive and negative sentiments.
Positive sentiments Negative sentiments
Emotion I?m happy I?m sad
Evaluation Great idea! Bad idea!
Stance She supports the bill She?s against the bill
The final issue to address is the actual annotation scheme for marking contextual
polarity. The scheme we developed has four tags: positive, negative, both, and neutral.
The positive tag is used to mark positive sentiments. The negative tag is used to mark
negative sentiments. The both tag is applied to expressions in which both a positive and
negative sentiment are being expressed. Subjective expressions with positive, negative, or
both tags are our sentiment expressions. The neutral tag is used for all other subjective
expressions, including emotions, evaluations, and stances that are neither positive or
negative. Instructions for the contextual-polarity annotation scheme are available at
http://www.cs.pitt.edu/mpqa/databaserelease/polarityCodingInstructions.txt.
Following are examples from the corpus of each of the different contextual-polarity
annotations. Each underlinedword or phrase is a subjective expression that wasmarked
in the original MPQA annotations.3 In bold following each subjective expression is the
contextual polarity with which it was annotated.
(7) Thousands of coup supporters celebrated (positive) overnight, waving
flags, blowing whistles . . .
(8) The criteria set by Rice are the following: the three countries in question are
repressive (negative) and grave human rights violators (negative) . . .
(9) Besides, politicians refer to good and evil (both) only for purposes of
intimidation and exaggeration.
(10) Jerome says the hospital feels (neutral) no different than a hospital in the
states.
As a final note on the annotation scheme, annotators are asked to judge the con-
textual polarity of the sentiment that is ultimately being conveyed by the subjective
expression, that is, once the sentence has been fully interpreted. Thus, the subjective
expression, they have not succeeded, and will never succeed, is marked as positive in the
following sentence:
(11) They have not succeeded, and will never succeed (positive), in breaking
the will of this valiant people.
The reasoning is that breaking the will of a valiant people is negative, so to not succeed
in breaking their will is positive.
3 Some sentences contain additional subjective expressions that are not underlined as examples.
405
Computational Linguistics Volume 35, Number 3
Table 3
Contingency table for contextual polarity agreement.
Neutral Positive Negative Both Total
Neutral 123 14 24 0 161
Positive 16 73 5 2 96
Negative 14 2 167 1 184
Both 0 3 0 3 6
Total 153 92 196 6 447
Table 4
Contingency table for contextual polarity agreement, borderline cases removed.
Neutral Positive Negative Both Total
Neutral 113 7 8 0 128
Positive 9 59 3 0 71
Negative 5 2 156 1 164
Both 0 2 0 2 4
Total 127 70 167 3 367
3.2 Agreement Study
To measure the reliability of the polarity annotation scheme, we conducted an agree-
ment study with two annotators4 using 10 documents from the MPQA corpus. The 10
documents contain 447 subjective expressions. Table 3 shows the contingency table for
the two annotators? judgments. Overall agreement is 82%, with a kappa value of 0.72.
As part of the annotation scheme, annotators are asked to judge how certain they
are in their polarity tags. For 18% of the subjective expressions, at least one annotator
used the uncertain tag whenmarking polarity. If we consider these cases to be borderline
and exclude them from the study, percent agreement increases to 90% and kappa rises to
0.84. Table 4 shows the revised contingency table with the uncertain cases removed. This
shows that annotator agreement is especially highwhen both annotators are certain, and
that annotators are certain for over 80% of their tags.
Note that all annotations are included in the experiments.
3.3 Contextual Polarity Annotations
In total, all 19,962 subjective expressions in the 535 documents (11,112 sentences) of the
MPQA corpus were annotated with their contextual polarity as just described.5 Three
annotators carried out the task: the two who participated in the annotation study and
a third who was trained later.6 Table 5 gives the distribution of the contextual polarity
tags. Looking at this table, we see that a small majority of subjective expressions (54.6%)
4 Both annotators are authors of this article.
5 The revised version of the MPQA corpus with the contextual polarity annotations is available at
http://www.cs.pitt.edu/mpqa.
6 The third annotator received training until her reliability of performance on the task was comparable to
that of the first two annotators who participated in the study.
406
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 5
Distribution of contextual polarity tags.
Neutral Positive Negative Both Total
9,057 3,311 7,294 299 19,961
45.4% 16.6% 36.5% 1.5% 100%
are expressing a positive, negative, or both (positive and negative) sentiment. We refer to
these expressions as polar in context. Many of the subjective expressions are neutral
and do not express a sentiment. This suggests that, although sentiment is a major type
of subjectivity, distinguishing other prominent types of subjectivity will be important
for future work in subjectivity analysis.
As many NLP applications operate at the sentence level, one important issue to
consider is the distribution of sentences with respect to the subjective expressions
they contain. In the 11,112 sentences in the MPQA corpus, 28% contain no subjective
expressions, 24% contain only one, and 48% contain two or more. Of the 5,304 sentences
containing two or more subjective expressions, 17% contain mixtures of positive and
negative expressions, and 61% contain mixtures of polar (positive/negative/both) and
neutral subjective expressions.
4. Prior-Polarity Subjectivity Lexicon
For the experiments in this article, we use a lexicon of over 8,000 subjectivity clues.
Subjectivity clues are words and phrases that may be used to express private states. In
other words, subjectivity clues have subjective usages, though they may have objective
usages as well. For this work, only single-word clues are used.
To compile the lexicon, we began with the list of subjectivity clues from Riloff and
Wiebe (2003), which includes the positive and negative adjectives fromHatzivassiloglou
and McKeown (1997). The words in this list were grouped in previous work according
to their reliability as subjectivity clues. Words that are subjective in most contexts are
considered strong subjective clues, indicated by the strongsubj tag. Words that may
only have certain subjective usages are considered weak subjective clues, indicated by
the weaksubj tag.
We expanded the list using a dictionary and a thesaurus, and added words from the
General Inquirer positive and negative word lists (Stone et al 1966) that we judged to be
potentially subjective.7 We also gave the new words strongsubj and weaksubj reliability
tags. The final lexicon has a coverage of 67% of subjective expressions in the MPQA
corpus, where coverage is the percentage of subjective expressions containing one or
more instances of clues from the lexicon. The coverage of just sentiment expressions is
even higher: 75%.
The next step was to tag the clues in the lexicon with their prior polarity: positive,
negative, both, or neutral. A word in the lexicon is tagged as positive if out of context
it seems to evoke something positive, and negative if it seems to evoke something
negative. If a word has both positive and negative meanings, it is tagged with the
polarity that seems the most common. A word is tagged as both if it is at the same time
7 In the end, about 70% of the words from the General Inquirer positive word list and 80% of the words
from the negative word list were included in the subjectivity lexicon.
407
Computational Linguistics Volume 35, Number 3
both positive and negative. For example, the word bittersweet evokes something both
positive and negative. Words like brag are also tagged as both, because the one who is
bragging is expressing something positive, yet at the same time describing someone as
bragging is expressing a negative evaluation of that person. A word is tagged as neutral
if it does not evoke anything positive or negative.
For words that came from positive and negative word lists (Stone et al 1966;
Hatzivassiloglou and McKeown 1997), we largely retained their original polarity.
However, we did change the polarity of a word if we strongly disagreed with its
original class.8 For example, the word apocalypse is listed as positive in the General
Inquirer; we changed its prior polarity to negative for our lexicon.
By far, the majority of clues in the lexicon (92.8%) are marked as having either
positive (33.1%) or negative (59.7%) prior polarity. Only a small number of clues (0.3%)
are marked as having both positive and negative polarity. We refer to the set of clues
marked as positive, negative, or both as sentiment clues. A total of 6.9% of the clues in
the lexicon are marked as neutral. Examples of neutral clues are verbs such as feel, look,
and think, and intensifiers such as deeply, entirely, and practically. Although the neutral
clues make up a small proportion of the total words in the lexicon, we retain them for
our later experiments in recognizing contextual polarity because many of them are good
clues that a sentiment is being expressed (e.g., feels slighted, feels satisfied, look kindly on,
look forward to). Including them increases the coverage of the system.
At the end of the previous section, we considered the distribution of sentences in the
MPQA corpus with respect to the subjective expressions they contain. It is interesting
to compare that distribution with the distribution of sentences with respect to the
instances they contain of clues from the lexicon. We find that there are more sentences
with two or more clue instances (62%) than sentences with two or more subjective
expressions (48%). More importantly, many more sentences have mixtures of positive
and negative clue instances than actually have mixtures of positive and negative sub-
jective expressions. Only 880 sentences have a mixture of both positive and negative
subjective expressions, whereas 3,234 sentences have a mixture of positive and negative
clue instances. Thus, a large number of positive and negative instances are either neutral
in context, or they are combining to form more complex polarity expressions. Either
way, this provides strong evidence of the need to be able to disambiguate the contextual
polarity of subjectivity and sentiment clues.
5. Definition of the Gold Standard
In the experiments described in the following sections, the goal is to classify the con-
textual polarity of the expressions that contain instances of the subjectivity clues in our
lexicon. However, determining which clue instances are part of the same expression and
identifying expression boundaries are not the focus of this work. Thus, instead of trying
to identify and label each expression, in the following experiments, each clue instance
is labeled individually as to its contextual polarity.
We define the gold-standard contextual polarity of a clue instance in terms of the
manual annotations (Section 3) as follows. If a clue instance is not in a subjective
expression (and therefore not in a sentiment expression), its gold class is neutral. If
a clue instance appears in just one subjective expression or in multiple subjective
8 We decided on a different polarity for about 80 of the words in our lexicon that appeared on other
positive and negative word lists.
408
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
expressions with the same contextual polarity, its gold class is the contextual polarity
of the subjective expression(s). If a clue instance appears in a mixture of negative and
neutral subjective expressions, its gold class is negative; if it is in amixture of positive and
neutral subjective expressions, its gold class is positive. Finally, if a clue instance appears
in at least one positive and one negative subjective expression (or in a subjective ex-
pression marked as both), then its gold class is both. A clue instance can appear in more
than one subjective expression because in the MPQA annotation scheme, it is possible
for direct subjective frames and expressive subjective elements frames to overlap.
6. A Prior-Polarity Classifier
Before delving into the task of recognizing contextual polarity, an important question
to address is how useful prior polarity alone is for identifying contextual polarity. To
answer this question, we create a classifier that simply assumes the contextual polarity
of a clue instance is the same as the clue?s prior polarity. We explore this classifier?s
performance on a small amount of development data, which is not part of the data used
in the subsequent experiments.
This simple classifier has an accuracy of 48%. From the confusion matrix given in
Table 6, we see that 76% of the errors result from words with non-neutral prior polarity
appearing in phrases with neutral contextual polarity. Only 12% of the errors result from
words with neutral prior polarity appearing in expressions with non-neutral contextual
polarity, and only 11% of the errors come from words with a positive or negative prior
polarity appearing in expressions with the opposite contextual polarity. Table 6 also
shows that positive clues tend to be used in negative expressions far more often than
negative clues tend to be used in positive expressions.
Given that by far the largest number of errors come from clues with positive,
negative, or both prior polarity appearing in neutral contexts, we were motivated to try
a two-step approach to the problem of sentiment classification. The first step, Neutral?
Polar Classification, tries to determine if an instance is neutral or polar in context. The
second step, Polarity Classification, takes all instances that step one classified as polar,
and tries to disambiguate their contextual polarity. This two-step approach is illustrated
in Figure 1.
7. Features
The features used in our experiments were motivated both by the literature and by
exploration of the contextual-polarity annotations in our development data. A number
Table 6
Confusion matrix for the prior-polarity classifier on the development set.
Prior-Polarity Classifier
Neutral Positive Negative Both Total
Neutral 798 784 698 4 2284
Gold Positive 81 371 40 0 492
Class Negative 149 181 622 0 952
Both 4 11 13 5 33
Total 1032 1347 1373 9 3761
409
Computational Linguistics Volume 35, Number 3
Figure 1
Two-step approach to recognizing contextual polarity.
of features were inspired by the paper on contextual-polarity influencers by Polanyi
and Zaenan (2004). Other features are those that have been found useful in the past
for recognizing subjective sentences (Wiebe, Bruce, and O?Hara 1999; Wiebe and Riloff
2005).
7.1 Features for Neutral?Polar Classification
For distinguishing between neutral and polar instances, we use the features listed in
Table 7. For ease of description, we group the features into six sets: word features, gen-
eral modification features, polarity modification features, structure features, sentence
features, and one document feature.
Word Features In addition to the word token (the token of the clue instance being
classified), the word features include the parts of speech of the previous word, the word
itself, and the next word. The prior polarity and reliability class features represent those
pieces of information about the clue which are taken from the lexicon.
General Modification Features These are binary features that capture different
types of relationships involving the clue instance.
The first four features involve relationships with the word immediately before or af-
ter the clue instance. The preceded by adjective feature is true if the clue instance is a noun
preceded by an adjective. The preceded by adverb feature is true if the preceding word
is an adverb other than not. The preceded by intensifier feature is true if the preceding
word is an intensifier, and the self intensifier feature is true if the clue instance itself is an
intensifier. A word is considered to be an intensifier if it appears in a list of intensifiers
and if it precedes a word of the appropriate part of speech (e.g., an intensifier adjective
must come before a noun). The list of intensifiers is a compilation of those listed in Quirk
et al (1985), intensifiers identified from existing entries in the subjectivity lexicon, and
intensifiers identified during explorations of the development data.
The modifies/modifed by features involve the dependency parse tree of the sentence,
obtained by first parsing the sentence (Collins 1997) and then converting the tree into
its dependency representation (Xia and Palmer 2001). In a dependency representation,
every node in the tree structure is a surface word (i.e., there are no abstract nodes such
as NP or VP). The parent word is called the head, and its children are its modifiers. The
410
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 7
Features for neutral?polar classification.
Word Features
word token
word part of speech
previous word part of speech
next word part of speech
prior polarity: positive, negative, both, neutral
reliability class: strongsubj or weaksubj
General Modification Features
preceded by adjective: binary
preceded by adverb (other than not): binary
preceded by intensifier: binary
self intensifier: binary
modifies strongsubj: binary
modifies weaksubj: binary
modified by strongsubj: binary
modified by weaksubj: binary
Polarity Modification Features
modifies polarity: positive, negative, neutral, both, notmod
modified by polarity: positive, negative, neutral, both, notmod
conjunction polarity: positive, negative, neutral, both, notmod
Structure Features
in subject: binary
in copular: binary
in passive: binary
Sentence Features
strongsubj clues in current sentence: 0, 1, 2, 3 (or more)
strongsubj clues in previous sentence: 0, 1, 2, 3 (or more)
strongsubj clues in next sentence: 0, 1, 2, 3 (or more)
weaksubj clues in current sentence: 0, 1, 2, 3 (or more)
weaksubj clues in previous sentence: 0, 1, 2, 3 (or more)
weaksubj clues in next sentence: 0, 1, 2, 3 (or more)
adjectives in sentence: 0, 1, 2, 3 (or more)
adverbs in sentence (other than not): 0, 1, 2, 3 (or more)
cardinal number in sentence: binary
pronoun in sentence: binary
modal in sentence (other than will): binary
Document Feature
document topic/domain
edge between a parent and a child specifies the grammatical relationship between the
two words. Figure 2 shows an example of a dependency parse tree. Instances of clues in
the tree are marked with the clue?s prior polarity and reliability class from the lexicon.
For each clue instance, the modifies/modifed by features capture whether there are
adj, mod, or vmod relationships between the clue instance and any other instances from
the lexicon. Specifically, the modifies strongsubj feature is true if the clue instance and
its parent share an adj, mod, or vmod relationship, and if its parent is an instance of
a strongsubj clue from the lexicon. The modifies weaksubj feature is the same, except
that it looks in the parent for an instance of a weaksubj clue. The modified by strongsubj
411
Computational Linguistics Volume 35, Number 3
Figure 2
The dependency tree for the sentence The human rights report poses a substantial challenge to the
U.S. interpretation of good and evil. Prior polarity and reliability class are marked in parentheses
for words that match clues from the lexicon.
feature is true for a clue instance if one of its children is an instance of a strongsubj
clue, and if the clue instance and its child share an adj, mod, or vmod relationship. The
modified by weaksubj feature is the same, except that it looks for instances of weaksubj
clues in the children. Although the adj and vmod relationships are typically local, the
mod relationship involves longer-distance as well as local dependencies. Figure 2 helps
to illustrate these features. The modifies weaksubj feature is true for substantial, because
substantial modifies challenge, which is an instance of a weaksubj clue. For rights, the
modifies weaksubj feature is false, because rightsmodifies report, which is not an instance
of a weaksubj clue. The modified by weaksubj feature is false for substantial, because it has
no modifiers that are instances of weaksubj clues. For challenge, the modified by weaksubj
feature is true because it is being modified by substantial, which is an instance of a
weaksubj clue.
Polarity Modification Features The modifies polarity, modified by polarity, and conj
polarity features capture specific relationships between the clue instance and other senti-
ment clues it may be related to. If the clue instance and its parent in the dependency tree
share an obj, adj, mod, or vmod relationship, the modifies polarity feature is set to the prior
polarity of the parent. If the parent is not in the prior-polarity lexicon, its prior polarity
is considered neutral. If the clue instance is at the root of the tree and has no parent,
the value of the feature is notmod. The modified by polarity feature is similar, looking
for adj,mod, and vmod relationships and other sentiment clues in the children of the clue
instance. The conj polarity feature determines if the clue instance is in a conjunction. If so,
the value of this feature is its sibling?s prior polarity. As before, if the sibling is not in the
412
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
lexicon, its prior polarity is neutral. If the clue instance is not in a conjunction, the value
for this feature is notmod. Figure 2 also helps to illustrate thesemodification features. The
word substantial with positive prior polarity modifies the word challenge with negative
prior polarity. Therefore the modifies polarity feature is negative for substantial, and the
modified by polarity feature is positive for challenge. The words good and evil are in a con-
junction together; thus the conj polarity feature is negative for good and positive for evil.
Structure Features These are binary features that are determined by starting with
the clue instance and climbing up the dependency parse tree toward the root, looking
for particular relationships, words, or patterns. The in subject feature is true if we find
a subj relationship on the path to the root. The in copular feature is true if in subject is
false and if a node along the path is both a main verb and a copular verb. The in passive
feature is true if a passive verb pattern is found on the climb.
The in subject and in copular features were motivated by the intuition that the syn-
tactic role of a word may influence whether a word is being used to express a sentiment.
For example, consider the word polluters in each of the following two sentences.
(12) Under the application shield, polluters are allowed to operate if they have
a permit.
(13) ?The big-city folks are pointing at the farmers and saying you are
polluters . . . ?
In the first sentence, polluters is simply being used as a referring expression. In the
second sentence, polluters is clearly being used to express a negative evaluation of the
farmers. Themotivation for the in passive feature was previous work by Riloff andWiebe
(2003), who found that different words aremore or less likely to be subjective depending
on whether they are in the active or passive.
Sentence Features These are features that previously were found useful for
sentence-level subjectivity classification (Wiebe, Bruce, and O?Hara 1999; Wiebe and
Riloff 2005). They include counts of strongsubj and weaksubj clue instances in the cur-
rent, previous and next sentences, counts of adjectives and adverbs other than not in
the current sentence, and binary features to indicate whether the sentence contains a
pronoun, a cardinal number, and a modal other than will.
Document Feature There is one document feature representing the topic or domain
of the document. The motivation for this feature is that whether or not a word is
expressing a sentiment or even a private state in general may depend on the subject
of the discourse. For example, the words fever and sufferer may express a negative
sentiment in certain contexts, but probably not in a health or medical context, as is the
case in the following sentence.
(14) The disease can be contracted if a person is bitten by a certain tick or if a
person comes into contact with the blood of a congo fever sufferer.
In the creation of the MPQA corpus, about two-thirds of the documents were
selected to be on one of the 10 topics listed in Table 8. The documents for each topic were
identified by human searches and by an information retrieval system. The remaining
documents were semi-randomly selected from a very large pool of documents from
the world press. In the corpus, these documents are listed with the topic miscellaneous.
Rather than leaving these documents unlabeled, we chose to label them using the
413
Computational Linguistics Volume 35, Number 3
Table 8
Topics in the MPQA corpus.
Topic Description
argentina Economic collapse in Argentina
axisofevil U.S. President?s State of the Union Address
guantanamo Detention of prisoners in Guantanamo Bay
humanrights U.S. State Department Human Rights Report
kyoto Kyoto Protocol ratification
settlements Israeli settlements in Gaza and the West Bank
space Space missions of various countries
taiwan Relationship between Taiwan and China
venezuela Presidential coup in Venezuela
zimbabwe Presidential election in Zimbabwe
following general domain categories: economics, general politics, health, report events,
and war and terrorism.
7.2 Features for Polarity Classification
Table 9 lists the features that we use for step two, polarity classification. Word token,
word prior polarity, and the polarity-modification features are the same as described for
neutral?polar classification.
We use two features to capture two different types of negation. The negated feature
is a binary feature that is used to capture more local negations: Its value is true if a
negation word or phrase is found within the four words preceding the clue instance,
and if the negation word is not also in a phrase that acts as an intensifier rather than a
negator. Examples of phrases that intensify rather than negate are not only and nothing if
not. The negated subject feature captures a longer-distance type of negation. This feature
Table 9
Features for polarity classification.
Word Features
word token
word prior polarity: positive, negative, both, neutral
Negation Features
negated: binary
negated subject: binary
Polarity Modification Features
modifies polarity: positive, negative, neutral, both, notmod
modified by polarity: positive, negative, neutral, both, notmod
conj polarity: positive, negative, neutral, both, notmod
Polarity Shifters
general polarity shifter: binary
negative polarity shifter: binary
positive polarity shifter: binary
414
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
is true if the subject of the clause containing the clue instance is negated. For example,
the negated subject feature is true for support in the following sentence.
(15) No politically prudent Israeli could support either of them.
The last three polarity features look in a window of four words before the clue
instance, searching for the presence of particular types of polarity influencers. Gen-
eral polarity shifters reverse polarity (e.g., little truth, little threat). Negative polarity
shifters typically make the polarity of an expression negative (e.g., lack of understand-
ing). Positive polarity shifters typically make the polarity of an expression positive
(e.g., abate the damage). The polarity influencers that we used were identified through
explorations of the development data.
8. Experiments in Recognizing Contextual Polarity
We have two primary goals with our experiments in recognizing contextual polarity.
The first is to evaluate the features described in Section 7 as to their usefulness for
this task. The second is to investigate the importance of recognizing neutral instances?
recognizing when a sentiment clue is not being used to express a sentiment?for classi-
fying contextual polarity.
To evaluate features, we investigate their performance, both together and sep-
arately, across several different learning algorithms. Varying the learning algorithm
allows us to verify that the features are robust and that their performance is not the
artifact of a particular algorithm. We experiment with four different types of machine
learning: boosting, memory-based learning, rule learning, and support vector learning.
For boosting, we use BoosTexter (Schapire and Singer 2000) AdaBoost.MH. For rule
learning, we use Ripper (Cohen 1996). For memory-based learning, we use TiMBL
(Daelemans et al 2003b) IB1 (k-nearest neighbor). For support vector learning, we
use SVM-light and SVM-multiclass (Joachims 1999). SVM-light is used for the experi-
ments involving binary classification (neutral?polar classification), and SVM-multiclass
is used for experiments with more than two classes. These machine learning algorithms
were chosen because they have been used successfully for a number of natural language
processing tasks, and they represent several different types of learning.
For all of the classification algorithms except for SVM, the features for a clue in-
stance are represented as they are presented in Section 7. For SVM, the representations
for numeric and discrete-valued features are changed. Numeric features, such as the
count of strongsubj clue instances in a sentence, are scaled to range between 0 and 1.
Discrete-valued features, such as the reliability class feature, are converted into multiple
binary features. For example, the reliability class feature is represented by two binary
features: one for whether the clue instance is a strongsubj clue and one for whether the
clue instance is a weaksubj clue.
To investigate the importance of recognizing neutral instances, we perform two sets
of polarity classification (step two) experiments. First, we experiment with classifying
the polarity of all gold-standard polar instances?the clue instances identified as polar
in context by the manual polarity annotations. Second, we experiment with using the
polar instances identified automatically by the neutral?polar classifiers. Because the
second set of experiments includes the neutral instances misclassified in step one, we
can compare results for the two sets of experiments to see how the noise of neutral
instances affects the performance of the polarity features.
415
Computational Linguistics Volume 35, Number 3
All experiments are performed using 10-fold cross validation over a test set of
10,287 sentences from 494MPQA corpus documents. Wemeasure performance in terms
of accuracy, recall, precision, and F-measure. Accuracy is simply the total number of
instances correctly classified. Recall, precision, and F-measure for a given class C are
defined as follows. Recall is the percentage of all instances of class C correctly identified.
Rec(C) =
| instances of C correctly identified |
| all instances of C |
Precision is the percentage of instances classified as class C that are class C in truth.
Prec(C) =
| instances of C correctly identified |
| all instances identified as C |
F-measure is the harmonic mean of recall and precision.
F(C) =
2 ?Rec(C)? Prec(C)
Rec(C)+ Prec(C)
All results reported are averages over the 10 folds.
8.1 Neutral?Polar Classification
In our two-step process for recognizing contextual polarity, the first step is neutral?polar
classification, determining whether each instance of a clue from the lexicon is neutral or
polar in context. In our test set, there are 26,729 instances of clues from the lexicon. The
features we use for this step were listed above in Table 7 and described in Section 7.1.
In this section, we perform two sets of experiments. In the first, we compare
the results of neutral?polar classification using all the neutral?polar features against
two baselines. The first baseline uses just the word token feature. The second baseline
(word+priorpol) uses the word token and prior polarity features. In the second set of
experiments, we explore the performance of different sets of features for neutral?polar
classification.
Research has shown that the performance of learning algorithms for NLP tasks can
vary widely depending on their parameter settings, and that the optimal parameter
settings can also vary depending on the set of features being evaluated (Daelemans
et al 2003a; Hoste 2005). Although the goal of this work is not to identify the optimal
configuration for each algorithm and each set of features, we still want to make a rea-
sonable attempt to find a good configuration for each algorithm. To do this, we perform
10-fold cross validation of the more challenging baseline classifier (word+priorpol)
on the development data, varying select parameter settings. The results from those
experiments are then used to select the parameter settings for each algorithm. For
BoosTexter, we vary the number of rounds of boosting. For TiMBL, we vary the value
for k (the number of neighbors) and the distance metric (overlap or modified value
difference metric [MVDM]). For Ripper, we vary whether negative tests are disallowed
for nominal (-!n) and set (-!s) valued attributes and howmuch to simplify the hypothesis
(-S). For SVM, we experiment with linear, polynomial, and radial basis function kernels.
Table 10 gives the settings selected for the neutral?polar classification experiments for
the different learning algorithms.
416
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 10
Algorithm settings for neutral?polar classification.
Algorithm Settings
BoosTexter 2,000 rounds of boosting
TiMBL k=25, MVDM distance metric
Ripper -!n, -S 0.5
SVM linear kernel
8.1.1 Classification Results. The results for the first set of experiments are given in
Table 11. For each algorithm, we give the results for the two baseline classifiers, followed
by the results for the classifier trained using all the neutral?polar features. The results
shown in bold are significantly better than both baselines (two-sided t-test, p? 0.05) for
the given algorithm.
Working together, how well do the neutral?polar features perform? For BoosTexter,
TiMBL, and Ripper, the classifiers trained using all the features improve significantly
over the two baselines in terms of accuracy, polar recall, polar F-measure, and neutral
precision. Neutral F-measure is also higher, but not significantly so. These consistent
results across three of the four algorithms show that the neutral?polar features are
helpful for determining when a sentiment clue is actually being used to express a
sentiment.
Interestingly, Ripper is the only algorithm for which the word-token baseline per-
formed better than the word+priorpol baseline. Nevertheless, the prior polarity feature
is an important component in the performance of the Ripper classifier using all the
features. Excluding prior polarity from this classifier results in a significant decrease in
Table 11
Results for neutral?polar classification (step one).
Polar Neutral
Acc Rec Prec F Rec Prec F
BoosTexter
word token baseline 74.0 41.9 77.0 54.3 92.7 73.3 81.8
word+priorpol baseline 75.0 55.6 70.2 62.1 86.2 76.9 81.3
neutral?polar features 76.5 58.3 72.4 64.6 87.1 78.2 82.4
TiMBL
word token baseline 74.6 47.9 73.9 58.1 90.1 74.8 81.8
word+priorpol baseline 74.6 48.2 73.7 58.3 90.0 74.9 81.7
neutral?polar features 76.5 59.5 71.7 65.0 86.3 78.5 82.3
Ripper
word token baseline 66.3 11.2 80.6 19.6 98.4 65.6 78.7
word+priorpol baseline 65.5 07.7 84.5 14.1 99.1 64.8 78.4
neutral?polar features 71.4 49.4 64.6 56.0 84.2 74.1 78.8
SVM
word token baseline 74.6 47.9 73.9 58.1 90.1 74.8 81.8
word+priorpol baseline 75.6 54.5 72.5 62.2 88.0 76.8 82.0
neutral?polar features 75.3 52.6 72.7 61.0 88.5 76.2 81.9
417
Computational Linguistics Volume 35, Number 3
performance for every metric. Decreases range from 2.5% for neutral recall to 9.5% for
polar recall.
The best SVM classifier is the word+priorpol baseline. In terms of accuracy, this
classifier does not perform much worse than the BoosTexter and TiMBL classifiers that
use all the neutral?polar features: The SVM word+priorpol baseline classifier has an
accuracy of 75.6%, and both the BoosTexter and TiMBL classifiers have an accuracy of
76.5%. However, the BoosTexter and TiMBL classifiers using all the features perform
notably better in terms of polar recall and F-measure. The BoosTexter and TiMBL
classifiers have polar recalls that are 7% and 9.2% higher than the SVM baseline. Polar
F-measures for BoosTexter and TiMBL are 3.9% and 4.5% higher. These increases are
significant for p ? 0.01.
8.1.2 Feature Set Evaluation. To evaluate the contribution of the various features for
neutral?polar classification, we perform a series of experiments in which different
sets of neutral?polar features are added to the word+priorpol baseline and new clas-
sifiers are trained. We then compare the performance of these new classifiers to the
word+priorpol baseline, with the exception of the Ripper classifiers, which we compare
to the higher word baseline. Table 12 lists the sets of features tested in these experiments.
The feature sets generally correspond to how the neutral?polar features are presented
in Table 7, although some of the groups are broken down into more fine-grained sets
that we believe capture meaningful distinctions.
Table 13 gives the results for these experiments. Increases and decreases for a
given metric as compared to the word+priorpol baseline (word baseline for Ripper)
are indicated by + or ?, respectively. Where changes are significant at the p ? 0.1 level,
++ or ? ? are used, and where changes are significant at the p ? 0.05 level, +++ or ? ? ?
are used. An ?nc? indicates no change (a change of less than ? 0.05) compared to the
baseline.
What does Table 13 reveal about the performance of various feature sets for neutral?
polar classification?Most noticeable is that no individual feature sets stand out as strong
performers. The only significant improvements in accuracy come from the PARTS-
OF-SPEECH and RELIABILITY-CLASS feature sets for Ripper. These improvements are
perhaps not surprising given that the Ripper baseline was much lower to begin with.
Very few feature sets show any improvement for SVM. Again, this is not unexpected
given that all the features together performed worse than the word+priorpol baseline
Table 12
Neutral?polar feature sets for evaluation.
Experiment Features
PARTS-OF-SPEECH parts of speech for clue instance, previous word, and next word
RELIABILITY-CLASS reliability class of clue instance
PRECEDED-POS preceded by adjective, preceded by adverb
INTENSIFY preceded by intensifier, self intensifier
RELCLASS-MOD modifies strongsubj/weaksubj, modified by strongsubj/weaksubj
POLARITY-MOD polarity-modification features
STRUCTURE structure features
CURSENT-COUNTS strongsubj/weaksubj clue instances in sentence
PNSENT-COUNTS strongsubj/weaksubj clue instances in previous/next sentence
CURSENT-OTHER adjectives/adverbs/cardinal number/pronoun/modal in sentence
TOPIC document topic
418
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 13
Results for neutral?polar feature ablation experiments.
Polar Neut Polar Neut
BoosTexter Acc F F Ripper Acc F F
PARTS-OF-SPEECH + ? + PARTS-OF-SPEECH +++ +++ ? ? ?
RELIABILITY-CLASS + ? + RELIABILITY-CLASS +++ +++ +
PRECEDED-POS nc ? nc PRECEDED-POS ? ? ?
INTENSIFY - nc - INTENSIFY ? ? ? ? ?
RELCLASS-MOD + ++ + RELCLASS-MOD + +++ +
POLARITY-MOD nc ? + POLARITY-MOD ? +++ ?
STRUCTURE ? ? ? ? + STRUCTURE ? + ?
CURSENT-COUNTS + ? ? ? + CURSENT-COUNTS ? ? +++ ? ? ?
PNSENT-COUNTS + ? ? ? + PNSENT-COUNTS ? ? ? +++ ? ? ?
CURSENT-OTHER nc ? + CURSENT-OTHER ? ? ? +++ ? ? ?
TOPIC + + + TOPIC ? +++ ? ? ?
Polar Neut Polar Neut
TiMBL Acc F F SVM Acc F F
PARTS-OF-SPEECH + +++ + PARTS-OF-SPEECH ? ? ? ? ? ?
RELIABILITY-CLASS + + nc RELIABILITY-CLASS + ? +
PRECEDED-POS nc + nc PRECEDED-POS nc nc nc
INTENSIFY nc nc nc INTENSIFY nc nc nc
RELCLASS-MOD + + + RELCLASS-MOD nc + nc
POLARITY-MOD + + + POLARITY-MOD ? ? ? ? ? ? ?
STRUCTURE nc + ? STRUCTURE ? + ?
CURSENT-COUNTS ? + ? CURSENT-COUNTS ? ? ?
PNSENT-COUNTS + +++ ? PNSENT-COUNTS ? ? ?
CURSENT-OTHER + +++ ? CURSENT-OTHER ? ? ?
TOPIC ? + ? TOPIC ? ? ?
Increases and decreases for a given metric as compared to the word+priorpol baseline
(word baseline for Ripper) are indicated by + or ?, respectively; ++ or ? ? indicates the
change is significant at the p < 0.1 level; +++ or ? ? ? indicates significance at the
p < 0.05 level; nc indicates no change.
for SVM. The performance of the feature sets for BoosTexter and TiMBL are perhaps
the most revealing. In the previous experiments using all the features together, these
algorithms produced classifiers with the same high performance. In these experiments,
six different feature sets for each algorithm show improvements in accuracy over the
baseline, yet none of those improvements are significant. This suggests that achieving
the highest performance for neutral?polar classification requires a wide variety of fea-
tures working together in combination.
We further test this result by evaluating the effect of removing the features that
produced either no change or a drop in accuracy from the respective all-feature classi-
fiers. For example, we train a TiMBL neutral?polar classifier using all the features except
for those in the PRECEDED-POS, INTENSIFY, STRUCTURE, CURSENT-COUNTS, and TOPIC
feature sets, and then compare the performance of this new classifier to the TiMBL, all-
feature classifier. Although removing the non-performing features has little effect for
BoosTexter, performance does drop for both TiMBL and Ripper. The primary source of
this performance drop is a decrease in polar recall: 2% for TiMBL and 3.2% for Ripper.
419
Computational Linguistics Volume 35, Number 3
Although no feature sets stand out in Table 13 as far as giving an overall high
performance, there are some features that consistently improve performance across
the different algorithms. The reliability class of the clue instance (RELIABILITY-CLASS)
improves accuracy over the baseline for all four algorithms. It is the only feature that
does so. The RELCLASS-MOD features give improvements for all metrics for BoosTexter,
Ripper, and TiMBL, as well as improving polar F-measure for SVM. The PARTS-OF-
SPEECH features are also fairly consistent, improving performance for all the algorithms
except for SVM. There are also a couple of feature sets that consistently do not improve
performance for any of the algorithms: the INTENSIFY and PRECEDED-POS features.
8.2 Polarity Classification
For the second step of recognizing contextual polarity, we classify the polarity of all clue
instances identified as polar in step one. The features for polarity classification were
listed in Table 9 and described in Section 7.2.
We investigate the performance of the polarity features under two conditions:
(1) perfect neutral?polar recognition and (2) automatic neutral?polar recognition. For
condition 1, we identify the polar instances according to the gold-standard, manual
contextual-polarity annotations. In the test data, 9,835 instances of the clues from the
lexicon are polar in context according to the manual annotations. Experiments under
condition 1 classify these instances as having positive, negative, or both (positive and
negative) polarity. For condition 2, we take the best performing neutral?polar classifier
for each algorithm and use the output from those algorithms to identify the polar
instances. Because polar instances now are being identified automatically, there will be
noise in the form of misclassified neutral instances. Therefore, for experiments under
condition 2 we include the neutral class and perform four-way classification instead of
three-way. Condition 1 allows us to investigate the performance of the different polarity
features without the noise of misclassified neutral instances. Also, because the set of
polar instances being classified is the same for all the algorithms, condition 1 allows
us to compare the performance of the polarity features across the different algorithms.
However, condition 2 is themore natural one. It allows us to see how the noise of neutral
instances affects the performance of the polarity features.
The following sections describe three sets of experiments. First, we investigate the
performance of the polarity features used together for polarity classification under
condition 1. As before, the word and word+priorpol classifiers provide our baselines. In
the second set of experiments, we explore the performance of different sets of features
for polarity classification, again assuming perfect recognition of the polar instances.
Finally, we experiment with polarity classification using all the polarity features under
condition 2, automatic recognition of the polar instances.
As before, we use the development data to select the parameter settings for each al-
gorithm. The settings for polarity classification are given in Table 14. They were selected
based on the performance of the word+priorpol baseline classifier under condition 2.
8.2.1 Classification Results: Condition 1. The results for polarity classification using all the
polarity features, assuming perfect neutral?polar recognition for step one, are given in
Table 15. For each algorithm, we give the results for the two baseline classifiers, followed
by the results for the classifier trained using all the polarity features. For the metrics
where the polarity features perform statistically better than both baselines (two-sided
t-test, p ? 0.05), the results are given in bold.
420
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 14
Algorithm settings for polarity classification.
Algorithm Settings
BoosTexter 2,000 rounds of boosting
TiMBL k=1, MVDM distance metric
Ripper -!s, -S 0.5
SVM linear kernel
Table 15
Results for polarity classification (step two) using gold-standard polar instances.
Positive Negative Both
Acc Rec Prec F Rec Prec F Rec Prec F
BoosTexter
word token baseline 78.7 57.7 72.8 64.4 91.5 80.8 85.8 12.9 53.6 20.8
word+priorpol baseline 79.7 70.5 68.8 69.6 87.2 85.1 86.1 13.7 53.7 21.8
polarity features 83.2 76.7 74.3 75.5 89.7 87.7 88.7 11.8 54.2 19.4
TiMBL
word token baseline 78.5 63.3 69.2 66.1 88.6 82.5 85.4 14.1 51.0 22.1
word+priorpol baseline 79.4 69.7 68.4 69.1 87.0 84.8 85.9 14.6 53.5 22.9
polarity features 82.2 75.4 73.3 74.3 88.5 87.6 88.0 18.3 34.6 23.9
Ripper
word token baseline 70.0 14.5 74.5 24.3 98.3 69.7 81.6 09.1 74.4 16.2
word+priorpol baseline 78.9 75.5 65.2 70.0 83.8 86.4 85.1 09.8 75.4 17.4
polarity features 83.2 77.8 73.5 75.6 89.2 87.8 88.5 09.8 74.9 17.4
SVM
word token baseline 69.9 62.4 69.6 65.8 76.0 84.1 79.9 14.1 31.2 19.4
word+priorpol baseline 78.2 76.7 63.7 69.6 82.2 86.7 84.4 09.8 75.4 17.4
polarity features 81.6 74.9 71.1 72.9 88.1 86.6 87.3 09.5 77.6 16.9
Howwell do the polarity features perform working all together? For all algorithms,
the polarity classifier using all the features significantly outperforms both baselines
in terms of accuracy, positive F-measure, and negative F-measure. These consistent
improvements in performance across all four algorithms show that these features are
quite useful for polarity classification.
One interesting thing that Table 15 reveals is that negative polarity words are much
more straightforward to recognize than positive polarity words, at least in this corpus.
For the negative class, precisions and recalls for the word+priorpol baseline range from
82.2 to 87.2. For the positive class, precisions and recalls for the word+priorpol baseline
range from 63.7 to 76.7. However, it is with the positive class that polarity features seem
to help themost. With the addition of the polarity features, positive F-measure improves
by 5 points on average; improvements in negative F-measures average only 2.75 points.
8.2.2 Feature Set Evaluation. To evaluate the performance of the various features for
polarity classification, we again perform a series of ablation experiments. As before, we
start with the word+priorpol baseline classifier, add different sets of polarity features,
train new classifiers, and compare the results of the new classifiers to the baseline.
421
Computational Linguistics Volume 35, Number 3
Table 16
Polarity feature sets for evaluation.
Experiment Features
NEGATION negated, negated subject
POLARITY-MOD modifies polarity, modified by polarity, conjunction polarity
SHIFTERS general, negative, positive polarity shifters
Table 17
Results for polarity feature ablation experiments.
Positive Negative
Acc Rec Prec F Rec Prec F
BoosTexter
NEGATION +++ ++ +++ +++ +++ + +++
POLARITY-MOD ++ +++ + +++ + ++ +
SHIFTERS + + + + + + +
TiMBL
NEGATION +++ +++ +++ +++ +++ +++ +++
POLARITY-MOD + + + + ? + +
SHIFTERS + + + + ? + +
Ripper
NEGATION +++ ? ? +++ +++ +++ ? +++
POLARITY-MOD + +++ ++ +++ + + +
SHIFTERS + ? + + + ? +
SVM
NEGATION +++ ? +++ +++ +++ + +++
POLARITY-MOD + ? +++ + + ? +
SHIFTERS + ? + + + + +
Increases and decreases for a given metric as compared to the
word+priorpol baseline are indicated by + or ?, respectively;
++ or ? ? indicates the change is significant at the p < 0.1 level;
+++ or ? ? ? indicates significance at the p < 0.05 level.
Table 16 lists the sets of features tested in each experiment, and Table 17 shows the
results of the experiments. Results are reported as they were previously in Section 8.1.2,
with increases and decreases compared to the baseline for a given metric indicated by +
or ?, respectively.
Looking at Table 17, we see that all three sets of polarity features help to increase
performance as measured by accuracy and positive and negative F-measures. This is
true for all the classification algorithms. As we might expect, including the negation
features has the most marked effect on the performance of polarity classification, with
statistically significant improvements for most metrics across all the algorithms.9 The
9 Although the negation features give the best performance improvements of the three feature sets, these
classifiers still do not perform as well as the respective all-feature polarity classifiers for each algorithm.
422
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
polarity-modification features also seem to be important for polarity classification,
in particular for disambiguating the positive instances. For all the algorithms except
TiMBL, including the polarity-modification features results in significant improvements
for at least one of the positive metrics. The polarity shifters also help classification, but
they seem to be the weakest of the features: Including them does not result in significant
improvements for any algorithm.
Another question that is interesting to consider is how much the word token feature
contributes to polarity classification, given all the other polarity features. Is it enough
to know the prior polarity of a word, whether it is being negated, and how it is related
to other polarity influencers? To answer this question, we train classifiers using all the
polarity features except for word token. Table 18 gives the results for these classifiers;
for comparison, the results for the all-feature polarity classifiers are also given. Inter-
estingly, excluding the word token feature produces only small changes in the overall
results. The results for BoosTexter and Ripper are slightly lower, and the results for
SVM are practically unchanged. TiMBL actually shows a slight improvement, with the
exception of the both class. This provides further evidence of the strength of the polarity
features. Also, a classifier not tied to actual word tokens may potentially be a more
domain-independent classifier.
8.2.3 Classification Results: Condition 2. The experiments in Section 8.2.1 show that the
polarity features perform well under the ideal condition of perfect recognition of polar
instances. The next question to consider is how well the polarity features perform
under the more natural but less-than-perfect condition of automatic recognition of
polar instances. To investigate this, the polarity classifiers (including the baselines) for
each algorithm in these experiments start with the polar instances identified by the
best performing neutral?polar classifier for that algorithm (from Section 8.1.1). The
results for these experiments are given in Table 19. As before, statistically significant
improvements over both baselines are given in bold.
How well do the polarity features perform in the presence of noise from misclas-
sified neutral instances? Our first observation comes from comparing Table 15 with
Table 19: Polarity classification results are much lower for all classifiers with the noise
of neutral instances. Yet in spite of this, the polarity features still produce classifiers that
Table 18
Results for polarity classification without and with the word token feature.
Acc Pos F Neg F Both F
BoosTexter
excluding word token 82.5 74.9 88.0 17.4
all polarity features 83.2 75.5 88.7 19.4
TiMBL
excluding word token 83.2 75.9 88.4 17.3
all polarity features 82.2 74.3 88.0 23.9
Ripper
excluding word token 82.9 75.4 88.3 17.4
all polarity features 83.2 75.6 88.5 17.4
SVM
excluding word token 81.5 72.9 87.3 16.8
all polarity features 81.6 72.9 87.3 16.9
423
C
o
m
p
u
ta
tio
n
a
l
L
in
g
u
istics
V
o
lu
m
e
3
5
,
N
u
m
b
e
r
3
Table 19
Results for polarity classification (step two) using automatically identified polar instances.
Positive Negative Both Neutral
Acc R P F R P F R P F R P F
BoosTexter
word token 61.5 62.3 62.7 62.5 86.4 64.6 74.0 11.4 49.3 18.5 20.8 44.5 28.3
word+priorpol 63.3 70.0 57.9 63.4 81.3 71.5 76.1 12.5 47.3 19.8 30.9 47.5 37.4
polarity feats 65.9 73.6 62.2 67.4 84.9 72.3 78.1 13.4 40.7 20.2 31.0 50.6 38.4
TiMBL
word token 60.1 68.3 58.9 63.2 81.8 65.0 72.5 11.2 39.6 17.4 21.6 43.1 28.8
word+priorpol 61.0 73.2 53.4 61.8 80.6 69.8 74.8 12.7 41.7 19.5 23.0 44.2 30.3
polarity feats 64.4 75.3 58.6 65.9 81.1 73.0 76.9 16.9 32.7 22.3 32.1 50.0 39.1
Ripper
word token 54.4 22.2 69.4 33.6 95.1 50.7 66.1 00.0 00.0 00.0 21.7 76.5 33.8
word+priorpol 51.4 24.0 71.7 35.9 97.7 48.9 65.1 00.0 00.0 00.0 09.2 75.8 16.3
polarity feats 54.8 38.0 67.2 48.5 95.5 52.7 67.9 00.0 00.0 00.0 14.5 66.8 23.8
SVM
word token 64.5 70.0 60.9 65.1 70.9 74.9 72.9 16.6 41.5 23.7 53.3 51.0 52.1
word+priorpol 62.8 89.0 51.2 65.0 88.4 69.2 77.6 11.1 48.5 18.0 02.4 58.3 04.5
polarity feats 64.1 90.8 53.0 66.9 90.4 70.1 79.0 12.7 52.3 20.4 02.2 61.4 04.3
4
2
4
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
outperform the baselines. For three of the four algorithms, the classifier using all the
polarity features has the highest accuracy. For BoosTexter and TiMBL, the improvements
in accuracy over both baselines are significant. Also for all algorithms, using the polarity
features gives the highest positive and negative F-measures.
Because the set of polarity instances being classified by each algorithm is different,
we cannot directly compare the results from one algorithm to the next.
8.3 Two-step versus One-step Recognition of Contextual Polarity
Although the two-step approach to recognizing contextual polarity allows us to focus
our investigation on the performance of features for both neutral?polar classification
and polarity classification, the question remains: How does the two-step approach
compare to recognizing contextual polarity in a single classification step? The results
shown in Table 20 help to answer this question. The first row in Table 20 for each
algorithm shows the combined result for the two stages of classification. For BoosTexter,
TiMBL, and Ripper, this is the combination of results from using all the neutral?polar
features for step one, together with the results from using all of the polarity features for
step two.10 For SVM, this is the combination of results from the word+priorpol baseline
from step one, together with results for using all the polarity features for step two.
Recall that the word+priorpol classifier was the best neutral?polar classifier for SVM
(see Table 11). The second rows for BoosTexter, TiMBL, and Ripper show the results of
a single classifier trained to recognize contextual polarity using all the neutral?polar
and polarity features together. For SVM, the second row shows the results of classifying
the contextual polarity using just the word token feature. This classifier outperformed
all others for SVM. In the table, the best result for each metric for each algorithm is
highlighted in bold.
When comparing the two-step and one-step approaches, contrary to our expecta-
tions, we see that the one-step approach performs about as well or better than the
two-step approach for recognizing contextual polarity. For SVM, the improvement in
accuracy achieved by the two-step approach is significant, but this is not true for
the other algorithms. One fairly consistent difference between the two approaches is
that the two-step approach scores slightly higher for neutral F-measure, and the one-
step approach achieves higher F-measures for the polarity classes. The difference in
negative F-measure is significant for BoosTexter, TiMBL, and Ripper. The exception to
this is SVM. For SVM, the two-step approach achieves significantly higher positive and
negative F-measures.
One last question we consider is how much the neutral?polar features contribute
to the performance of the one-step classifiers. The third line in Table 20 for BoosTexter,
TiMBL, and Ripper gives the results for a one-step classifier trainedwithout the neutral?
polar features. Although the differences are not always large, excluding the neutral?
polar features consistently degrades performance in terms of accuracy and positive,
negative, and neutral F-measures. The drop in negative F-measure is significant for all
three algorithms, the drop in neutral F-measure is significant for BoosTexter and TiMBL,
and the drop in accuracy is significant for TiMBL and Ripper (and for BoosTexter at the
p ? 0.1 level).
10 To clarify, Section 8.2.3 only reported results for instances identified as polar in step one. Here, we report
results for all clue instances, including the instances classified as neutral in step one.
425
Computational Linguistics Volume 35, Number 3
Table 20
Results for contextual polarity classification for both two-step and one-step approaches.
Acc Pos F Neg F Both F Neutral F
BoosTexter
two-step 74.5 47.1 57.5 12.9 83.4
one-step all feats 74.3 49.1 59.8 14.1 82.9
one-step ? neut-pol feats 73.3 48.4 58.7 16.3 81.9
TiMBL
two-step 74.1 47.6 56.4 13.8 83.2
one-step all feats 73.9 49.6 59.3 15.2 82.6
one-step ? neut-pol feats 72.5 49.5 56.9 21.6 81.4
Ripper
two-step 68.9 26.6 49.0 00.0 80.1
one-step all feats 69.5 30.2 52.8 14.0 79.4
one-step ? neut-pol feats 67.0 28.9 33.0 11.4 78.6
SVM
two-step 73.1 46.6 58.0 13.0 82.1
one-step 71.6 43.4 51.7 17.0 81.6
The modest drop in performance that we see when excluding the neutral?polar
features in the one-step approach seems to suggest that discriminating between neutral
and polar instances is helpful but not necessarily crucial. However, consider Figure 3.
In this figure, we show the F-measures for the positive, negative, and both classes for
the BoosTexter polarity classifier that uses the gold-standard neutral/polar instances
(from Table 15) and for the BoosTexter one-step polarity classifier that uses all features
(from Table 20). Plotting the same sets of results for the other three algorithms produces
very similar figures. The difference when the classifiers have to contend with the noise
from neutral instances is dramatic. Although Table 20 shows that there is room for
improvement across all the contextual polarity classes, Figure 3 shows us that perhaps
the best way to achieve these improvements is to improve the ability to discriminate the
neutral class from the others.
Figure 3
Chart showing the positive, negative, and both class F-measures for the BoosTexter classifier that
uses the gold-standard neutral/polar classes and the BoosTexter one-step classifier that uses all
the features.
426
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
9. Related Work
9.1 Phrase-Level Sentiment Analysis
Other researchers who have worked on classifying the contextual polarity of sentiment
expressions are Yi et al (2003), Popescu and Etzioni (2005), and Suzuki, Takamura, and
Okumura (2006). Yi et al use a lexicon and manually developed patterns to classify
contextual polarity. Their patterns are high-quality, yielding quite high precision over
the set of expressions that they evaluate. Popescu and Etzioni use an unsupervised clas-
sification technique called relaxation labeling (Hummel and Zucker 1983) to recognize
the contextual polarity of words that are at the heads of select opinion phrases. They
take an iterative approach, using relaxation labeling first to determine the contextual
polarities of the words, then again to label the polarities of the words with respect to
their targets. A third stage of relaxation labeling then is used to assign final polarities to
the words, taking into consideration the presence of other polarity terms and negation.
Aswe do, Popescu and Etzioni use features that represent conjunctions and dependency
relations between polarity words. Suzuki et al use a bootstrapping approach to classify
the polarity of tuples of adjectives and their target nouns in Japanese blogs. Included
in the features that they use are the words that modify the adjectives and the word that
the adjective modifies. They consider the effect of a single negation term, the Japanese
equivalent of not.
Our work in recognizing contextual polarity differs from this research on
expression-level sentiment analysis in several ways. First, the set of expressions they
evaluate is limited either to those that target specific items of interest, such as products
and product features, or to tuples of adjectives and nouns. In contrast, we seek to classify
the contextual polarity of all instances of words from a large lexicon of subjectivity clues
that appear in the corpus. Included in the lexicon are not only adjectives, but nouns,
verbs, adverbs, and even modals.
Our work also differs from other research in the variety of features that we use. As
other researchers do, we consider negation and the words that directly modify or are
modified by the expression being classified. However, with negation, we have features
for both local and longer-distance types of negation, and we take care to count negation
terms only when they are actually being used to negate, excluding, for example, nega-
tion terms when they are used in phrases that intensify (e.g., not only). We also include
contextual features to capture the presence of other clue instances in the surrounding
sentences, and features that represent the reliability of clues from the lexicon.
Finally, a unique aspect of the work presented in this article is the evaluation of
different features for recognizing contextual polarity. We first presented the features
explored in this research in Wilson, Wiebe, and Hoffman (2005), but this work signif-
icantly extends that initial evaluation. We explore the performance of features across
different learning algorithms, and we evaluate not only features for discriminating
between positive and negative polarity, but features for determining when a word is
or is not expressing a sentiment in the first place (neutral in context). This is also the
first work to evaluate the effect of neutral instances on the performance of features for
discriminating between positive and negative contextual polarity.
9.2 Other Research in Sentiment Analysis
Recognizing contextual polarity is just one facet of the research in automatic senti-
ment analysis. Research ranges from work on learning the prior polarity (semantic
427
Computational Linguistics Volume 35, Number 3
orientation) of words and phrases (e.g., Hatzivassiloglou and McKeown 1997; Kamps
and Marx 2002; Turney and Littman 2003; Hu and Liu 2004; Kim and Hovy 2004; Esuli
and Sebastiani 2005; Takamura, Inui, and Okumura 2005; Popescu and Etzioni 2005;
Andreevskaia and Bergler 2006; Esuli and Sebastiani 2006a; Kanayama and Nasukawa
2006) to characterizing the sentiment of documents, such as recognizing inflammatory
messages (Spertus 1997), tracking sentiment over time in online discussions (Tong
2001), and classifying the sentiment of online messages (e.g., Das and Chen 2001;
Koppel and Schler 2006), customer feedback data (Gamon 2004), or product and movie
reviews (e.g., Turney 2002; Pang, Lee, and Vaithyanathan 2002; Dave, Lawrence, and
Pennock 2003; Beineke, Hastie, and Vaithyanathan 2004; Mullen and Collier 2004; Bai,
Padman, and Airoldi 2005; Whitelaw, Garg, and Argamon 2005; Kennedy and Inkpen
2006; Koppel and Schler 2006).
Identifying prior polarity is a different task than recognizing contextual polarity,
although the two tasks are complementary. The goal of identifying prior polarity is
to automatically acquire the polarity of words or phrases for listing in a lexicon. Our
work on recognizing contextual polarity begins with a lexicon of words with established
prior polarities and then disambiguates in the corpus the polarity being expressed
by the phrases in which instances of those words appear. To make the relationship
between that task and ours clearer, someword lists that are used to evaluatemethods for
recognizing prior polarity (positive and negative word lists from the General Inquirer
[Stone et al 1966] and lists of positive and negative adjectives created for evaluation by
Hatzivassiloglou and McKeown [1997]) are included in the prior-polarity lexicon used
in our experiments.
For the most part, the features explored in this work differ from the ones used to
identify prior polarity with just a few exceptions. Using a feature to capture conjunc-
tions between clue instances was motivated in part by the work of Hatzivassiloglou and
McKeown (1997). They use constraints on the co-occurrence in conjunctions of words
with similar or opposite polarity to predict the prior polarity of adjectives. Esuli and
Sebastiani (2005) consider negation in some of their experiments involving WordNet
glosses. Takamura et al (2005) use negation words and phrases, including phrases such
as lack of that are members in our lists of polarity shifters, and conjunctive expressions
that they collect from corpora.
Esuli and Sebastiani (2006a) is the only work in prior-polarity identification to
include a neutral (objective) category and to consider a three-way classification between
positive, negative, and neutral words. Although identifying prior polarity is a different
task, they report a finding similar to ours, namely, that accuracy is lower when neutral
words are included.
Some research in sentiment analysis classifies the sentiments of sentences. Morinaga
et al (2002), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Hu and Liu (2004),
and Grefenstette et al (2004)11 all begin by first creating prior-polarity lexicons. Yu and
Hatzivassiloglou then assign a sentiment to a sentence by averaging the prior semantic
orientations of instances of lexicon words in the sentence. Thus, they do not identify the
contextual polarity of individual phrases containing clue instances, which is the focus
of this work. Morinaga et al only consider the positive or negative clue instance in
each sentence that is closest to some target reference; Kim and Hovy, Hu and Liu, and
Grefenstette et al multiply or count the prior polarities of clue instances in the sentence.
11 In Grefenstette et al (2004), the units that are classified are fixed windows around named entities rather
than sentences.
428
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
These researchers also consider local negation to reverse polarity, with Morinaga et al
also taking into account the negating effect of words like insufficient. However, they
do not use the other types of features that we consider in our experiments. Kaji and
Kitsuregawa (2006) take a different approach to recognizing positive and negative
sentences. They bootstrap from information easily obtained in ?Pro? and ?Con?
HTML tables and lists, and from one high-precision linguistic pattern, to automatically
construct a large corpus of positive and negative sentences. They then use this corpus to
train a naive Bayes sentence classifier. In contrast to our work, sentiment classification
in all of this research is restricted to identifying only positive and negative sentences
(excluding our both and neutral categories). In addition, only one sentiment is assigned
per sentence; our system assigns contextual polarity to individual expressions, which
would allow for a sentence to be assigned to multiple sentiment categories. As we saw
when exploring the contextual polarity annotations, it is not uncommon for sentences
to contain more than one sentiment expression.
Classifying the sentiment of documents is a very different task than recognizing
the contextual polarity of words and phrases. However, some researchers have re-
ported findings about document-level classification that are similar to our findings
about phrase-level classification. Bai et al (2005) argue that dependencies among key
sentiment terms are important for classifying document sentiment. Similarly, we show
that features for capturing when clue instances modify each other are important for
phrase-level classification, in particular, for identifying positive expressions. Gamon
(2004) achieves his best results for document classification using a wide variety of
features, including rich linguistic features, such as features that capture constituent
structure, features that combine part-of-speech and semantic relations (e.g., sentence
subject or negated context), and features that capture tense information. We also achieve
our best results for phrase-level classification using a wide variety of features, many
of which are linguistically rich. Kennedy and Inkpen (2006) report consistently higher
results for document sentiment classification when select polarity influencers, including
negators and intensifiers, are included.12 Koppel and Schler (2006) demonstrate the
importance of neutral examples for document-level classification. In this work, we show
that being able to correctly identify neutral instances is also very important for phrase-
level sentiment analysis.
10. Conclusions and Future Work
Being able to determine automatically the contextual polarity of words and phrases is
an important problem in sentiment analysis. In the research presented in this article, we
tackle this problem and show that it is much more complex than simply determining
whether a word or phrase is positive or negative. In our analysis of a corpus with
annotations of subjective expressions and their contextual polarity, we find that positive
and negative words from a lexicon are used in neutral contexts much more often than
they are used in expressions of the opposite polarity. The importance of identifying
12 Das and Chen (2001), Pang, Lee, and Vaithyanathan (2002), and Dave, Lawrence, and Pennock (2003) also
represent negation. In their experiments, words which follow a negation term are tagged with a negation
marker and then treated as new words. Pang, Lee and Vaithyanathan report that representing negation in
this way slightly helps their results, whereas Dave, Lawrence, and Pennock report a slightly detrimental
effect. Whitelaw, Garg, and Argamon (2005) also represent negation terms and intensifiers. However, in
their experiments, the effect of negation is not separately evaluated, and intensifiers are not found to be
beneficial.
429
Computational Linguistics Volume 35, Number 3
when contextual polarity is neutral is further revealed in our classification experiments:
When neutral instances are excluded, the performance of features for distinguishing
between positive and negative polarity greatly improves.
A focus of this research is on understanding which features are important for
recognizing contextual polarity. We experiment with a wide variety of linguistically
motivated features, and we evaluate the performance of these features using several
different machine learning algorithms. Features for distinguishing between neutral and
polar instances are evaluated, as well as features for distinguishing between positive
and negative contextual polarity. For classifying neutral and polar instances, we find
that, although some features produce significant improvements over the baseline in
terms of polar or neutral recall or precision, it is the combination of features together
that is needed to achieve significant improvements in accuracy. For classifying positive
and negative contextual polarity, features for capturing negation prove to be the most
important. However, we find that features that also perform well are those that cap-
ture when a word is (or is not) modifying or being modified by other polarity terms.
This suggests that identifying features that represent more complex interdependencies
between polarity clues will be an important avenue for future research.
Another direction for future work will be to expand our lexicon using existing
techniques for acquiring the prior polarity of words and phrases. It follows that a larger
lexicon will have a greater coverage of sentiment expressions. However, expanding the
lexicon with automatically acquired prior-polarity tags may result in an even greater
proportion of neutral instances to contend with. Given the degradation in performance
created by the neutral instances, whether expanding the lexicon automatically will
result in improved performance for recognizing contextual polarity is an empirical
question.
Finally, the overall goal of our research is to use phrase-level sentiment analysis in
higher-level NLP tasks, such as opinion question answering and summarization.
Acknowledgments
We would like to thank the anonymous
reviewers for their valuable comments and
suggestions. This work was supported in
part by an Andrew Mellow Predoctoral
Fellowship, by the NSF under grant
IIS-0208798, by the Advanced Research and
Development Activity (ARDA), and by the
European IST Programme through the
AMIDA Integrated Project FP6-0033812.
References
Andreevskaia, Alina and Sabine Bergler.
2006. Mining WordNet for fuzzy
sentiment: Sentiment tag extraction from
WordNet glosses. In Proceedings of the 11th
Meeting of the European Chapter of the
Association for Computational Linguistics
(EACL-2006), pages 209?216, Trento.
Bai, Xue, Rema Padman, and Edoardo
Airoldi. 2005. On learning parsimonious
models for extracting consumer opinions.
In Proceedings of the 38th Annual Hawaii
International Conference on System
Sciences (HICSS?05) - Track 3, page 75.2,
Waikoloa, HI.
Banfield, Ann. 1982. Unspeakable Sentences.
Routledge and Kegan Paul, Boston.
Beineke, Philip, Trevor Hastie, and
Shivakumar Vaithyanathan. 2004. The
sentimental factor: Improving review
classification via human-provided
information. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics (ACL-04),
pages 263?270, Barcelona.
Cohen, William W. 1996. Learning trees
and rules with set-valued features. In
Proceedings of the 13th National Conference
on Artificial Intelligence, pages 709?717,
Portland, OR.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics
(ACL-97), pages 16?23, Madrid.
Daelemans, Walter, Ve?ronique Hoste,
Fien De Meulder, and Bart Naudts.
2003a. Combined optimization of feature
selection and algorithm parameter
430
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
interaction in machine learning of
language. In Proceedings of the 14th
European Conference on Machine Learning
(ECML-2003), pages 84?95,
Cavtat-Dubrovnik.
Daelemans, Walter, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 2003b.
TiMBL: Tilburg Memory Based Learner,
version 5.0 Reference Guide. ILK Technical
Report 03-10, Induction of Linguistic
Knowledge Research Group, Tilburg
University. Available at http://ilk.uvt.
nl/downloads/pub/papers/ilk0310.pdf.
Das, Sanjiv Ranjan and Mike Y. Chen. 2001.
Yahoo! for Amazon: Sentiment parsing
from small talk on the Web. In Proceedings
of the August 2001 Meeting of the European
Finance Association (EFA), Barcelona,
Spain. Available at http://ssrn.com/
abstract=276189.
Dave, Kushal, Steve Lawrence, and David M.
Pennock. 2003. Mining the peanut
gallery: Opinion extraction and
semantic classification of product
reviews. In Proceedings of the 12th
International World Wide Web Conference
(WWW2003), Budapest. Available at
http://www2003.org.
Esuli, Andrea and Fabrizio Sebastiani. 2005.
Determining the semantic orientation of
terms through gloss analysis. In
Proceedings of ACM SIGIR Conference on
Information and Knowledge Management
(CIKM-05), pages 617?624, Bremen.
Esuli, Andrea and Fabrizio Sebastiani. 2006a.
Determining term subjectivity and term
orientation for opinion mining. In
Proceedings the 11th Meeting of the European
Chapter of the Association for Computational
Linguistics (EACL-2006), pages 193?200,
Trento.
Esuli, Andrea and Fabrizio Sebastiani. 2006b.
SentiWordNet: A publicly available lexical
resource for opinion mining. In Proceedings
of LREC-06, the 5th Conference on Language
Resources and Evaluation, pages 417?422,
Genoa.
Gamon, Michael. 2004. Sentiment
classification on customer feedback data:
Noisy data, large feature vectors, and the
role of linguistic analysis. In Proceedings
of the 20th International Conference on
Computational Linguistics (COLING-2004),
pages 611?617, Geneva.
Grefenstette, Gregory, Yan Qu, James G.
Shanahan, and David A. Evans. 2004.
Coupling niche browsers and affect
analysis for an opinion mining application.
In Proceedings of the Conference Recherche
d?Information Assistee par Ordinateur
(RIAO-2004), pages 186?194, Avignon.
Hatzivassiloglou, Vasileios and Kathy
McKeown. 1997. Predicting the semantic
orientation of adjectives. In Proceedings of
the 35th Annual Meeting of the Association
for Computational Linguistics (ACL-97),
pages 174?181, Madrid.
Hoste, Ve?ronique. 2005. Optimization Issues in
Machine Learning of Coreference Resolution.
Ph.D. thesis, Language Technology Group,
University of Antwerp.
Hu, Minqing and Bing Liu. 2004. Mining
and summarizing customer reviews. In
Proceedings of ACM SIGKDD Conference
on Knowledge Discovery and Data Mining
2004 (KDD-2004), pages 168?177,
Seattle, WA.
Hummel, Robert A. and Steven W. Zucker.
1983. On the foundations of relaxation
labeling processes. IEEE Transactions on
Pattern Analysis and Machine Intelligence
(PAMI), 5(3):167?187.
Joachims, Thorsten. 1999. Making large-scale
SVM learning practical. In B. Scholkopf,
C. Burgess, and A. Smola, editors,
Advances in Kernel Methods ? Support Vector
Learning, pages 169?184. MIT Press,
Cambridge, MA.
Kaji, Nobuhiro and Masaru Kitsuregawa.
2006. Automatic construction of
polarity-tagged corpus from HTML
documents. In Proceedings of the
COLING/ACL 2006 Main Conference
Poster Sessions, pages 452?459, Sydney.
Kamps, Jaap and Maarten Marx. 2002.
Words with attitude. In Proceedings of the
1st International Conference on Global
WordNet, pages 332?341, Mysore.
Kanayama, Hiroshi and Tetsuya Nasukawa.
2006. Fully automatic lexicon expansion
for domain-oriented sentiment analysis.
In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP-2006), pages 355?363,
Sydney.
Kennedy, Alistair and Diana Inkpen. 2006.
Sentiment classification of movie reviews
using contextual valence shifters.
Computational Intelligence, 22(2):110?125.
Kim, Soo-Min and Eduard Hovy. 2004.
Determining the sentiment of opinions.
In Proceedings of the 20th International
Conference on Computational Linguistics
(COLING-2004), pages 1267?1373, Geneva.
Koppel, Moshe and Jonathan Schler. 2006.
The importance of neutral examples for
learning sentiment. Computational
Intelligence, 22(2):100?109.
431
Computational Linguistics Volume 35, Number 3
Maybury, Mark T., editor. 2004. New
Directions in Question Answering. American
Association for Artificial Intelligence,
Menlo Park, CA.
Morinaga, Satoshi, Kenji Yamanishi, Kenji
Tateishi, and Toshikazu Fukushima. 2002.
Mining product reputations on the Web.
In Proceedings of the 8th ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining (KDD-2002),
pages 341?349, Edmonton.
Mullen, Tony and Nigel Collier. 2004.
Sentiment analysis using support
vector machines with diverse
information sources. In Proceedings
of the Conference on Empirical Methods
in Natural Language Processing
(EMNLP-2004), pages 412?418,
Barcelona.
Nasukawa, Tetsuya and Jeonghee Yi.
2003. Sentiment analysis: Capturing
favorability using natural language
processing. In Proceedings of the 2nd
International Conference on Knowledge
Capture (K-CAP 2003), pages 70?77,
Sanibel Island, FL.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine
learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2002),
pages 79?86, Philadelphia, PA.
Polanyi, Livia and Annie Zaenen. 2004.
Contextual valence shifters. InWorking
Notes of the AAAI Spring Symposium on
Exploring Attitude and Affect in Text:
Theories and Applications, pages 106?111,
The AAAI Press, Menlo Park, CA.
Popescu, Ana-Maria and Oren Etzioni.
2005. Extracting product features and
opinions from reviews. In Proceedings
of the Human Language Technologies
Conference/Conference on Empirical
Methods in Natural Language Processing
(HLT/EMNLP-2005), pages 339?346,
Vancouver.
Quirk, Randolph, Sidney Greenbaum,
Geoffry Leech, and Jan Svartvik. 1985.
A Comprehensive Grammar of the English
Language. Longman, New York.
Riloff, Ellen and Janyce Wiebe. 2003.
Learning extraction patterns for subjective
expressions. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 105?112,
Sapporo.
Schapire, Robert E. and Yoram Singer. 2000.
BoosTexter: A boosting-based system for
text categorization.Machine Learning,
39(2/3):135?168.
Spertus, Ellen. 1997. Smokey: Automatic
recognition of hostile messages. In
Proceedings of the 8th Annual Conference
on Innovative Applications of Artificial
Intelligence (IAAI-97), pages 1058?1065,
Providence, RI.
Stone, Philip J., Dexter C. Dunphy,
Marshall S. Smith, and Daniel M. Ogilvie.
1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press,
Cambridge, MA.
Stoyanov, Veselin, Claire Cardie, and
Janyce Wiebe. 2005. Multi-perspective
question answering using the OpQA
corpus. In Proceedings of the Human
Language Technologies Conference/
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP-2005),
pages 923?930, Vancouver.
Suzuki, Yasuhiro, Hiroya Takamura, and
Manabu Okumura. 2006. Application of
semi-supervised learning to evaluative
expression classification. In Proceedings of
the 7th International Conference on Intelligent
Text Processing and Computational
Linguistics (CICLing-2006), pages 502?513,
Mexico City.
Takamura, Hiroya, Takashi Inui, and
Manabu Okumura. 2005. Extracting
emotional polarity of words using spin
model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL-05), pages 133?140,
Ann Arbor, MI.
Tong, Richard. 2001. An operational
system for detecting and tracking
opinions in online discussions. In
Working Notes of the SIGIR Workshop on
Operational Text Classification, pages 1?6,
New Orleans, LA.
Turney, Peter. 2002. Thumbs up or thumbs
down? Semantic orientation applied to
unsupervised classification of reviews.
In Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL-02), pages 417?424,
Philadelphia, PA.
Turney, Peter and Michael L. Littman. 2003.
Measuring praise and criticism: Inference
of semantic orientation from association.
ACM Transactions on Information Systems
(TOIS), 21(4):315?346.
Whitelaw, Casey, Navendu Garg, and
Shlomo Argamon. 2005. Using appraisal
groups for sentiment analysis. In
Proceedings of the 14th ACM International
Conference on Information and Knowledge
432
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Management (CIKM-2005), pages 625?631,
Bremen.
Wiebe, Janyce. 1994. Tracking point of view
in narrative. Computational Linguistics,
20(2):233?287.
Wiebe, Janyce, Rebecca Bruce, and
Thomas O?Hara. 1999. Development
and use of a gold standard data set
for subjectivity classifications. In
Proceedings of the 37th Annual Meeting
of the Association for Computational
Linguistics (ACL-99), pages 246?253,
College Park, MD.
Wiebe, Janyce and Rada Mihalcea.
2006. Word sense and subjectivity.
In Proceedings of the 21st International
Conference on Computational Linguistics
and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 1065?1072, Sydney.
Wiebe, Janyce and Ellen Riloff. 2005.
Creating subjective and objective sentence
classifiers from unannotated texts. In
Proceedings of the 6th International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-2005),
pages 486?497, Mexico City.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions
of opinions and emotions in language.
Language Resources and Evaluation
(formerly Computers and the Humanities),
39(2/3):164?210.
Wilson, Theresa, Janyce Wiebe, and Paul
Hoffmann. 2005. Recognizing contextual
polarity in phrase-level sentiment
analysis. In Proceedings of the Human
Language Technologies Conference/
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP-2005),
pages 347?354, Vancouver.
Xia, Fei and Martha Palmer. 2001.
Converting dependency structures to
phrase structures. In Proceedings of the
Human Language Technology Conference
(HLT-2001), pages 1?5, San Diego, CA.
Yi, Jeonghee, Tetsuya Nasukawa, Razvan
Bunescu, and Wayne Niblack. 2003.
Sentiment analyzer: Extracting sentiments
about a given topic using natural language
processing techniques. In Proceedings of the
3rd IEEE International Conference on Data
Mining (ICDM?03), pages 427?434,
Melbourne, FL.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 129?136,
Sapporo.
433

Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 347?354, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
twilson@cs.pitt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Paul Hoffmann
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
hoffmanp@cs.pitt.edu
Abstract
This paper presents a new approach to
phrase-level sentiment analysis that first
determines whether an expression is neu-
tral or polar and then disambiguates the
polarity of the polar expressions. With this
approach, the system is able to automat-
ically identify the contextual polarity for
a large subset of sentiment expressions,
achieving results that are significantly bet-
ter than baseline.
1 Introduction
Sentiment analysis is the task of identifying positive
and negative opinions, emotions, and evaluations.
Most work on sentiment analysis has been done at
the document level, for example distinguishing pos-
itive from negative reviews. However, tasks such
as multi-perspective question answering and sum-
marization, opinion-oriented information extraction,
and mining product reviews require sentence-level
or even phrase-level sentiment analysis. For exam-
ple, if a question answering system is to successfully
answer questions about people?s opinions, it must be
able to pinpoint expressions of positive and negative
sentiments, such as we find in the sentences below:
(1) African observers generally approved+ of his
victory while Western governments denounced?
it.
(2) A succession of officers filled the TV
screen to say they supported+ the people and that
the killings were ?not tolerable?.?
(3) ?We don?t hate+ the sinner,? he says,
?but we hate? the sin.?
A typical approach to sentiment analysis is to start
with a lexicon of positive and negative words and
phrases. In these lexicons, entries are tagged with
their a priori prior polarity: out of context, does
the word seem to evoke something positive or some-
thing negative. For example, beautiful has a positive
prior polarity, and horrid has a negative prior polar-
ity. However, the contextual polarity of the phrase
in which a word appears may be different from the
word?s prior polarity. Consider the underlined polar-
ity words in the sentence below:
(4) Philip Clapp, president of the National Environ-
ment Trust, sums up well the general thrust of the
reaction of environmental movements: ?There is no
reason at all to believe that the polluters are sud-
denly going to become reasonable.?
Of these words, ?Trust,? ?well,? ?reason,? and ?rea-
sonable? have positive prior polarity, but they are
not all being used to express positive sentiments.
The word ?reason? is negated, making the contex-
tual polarity negative. The phrase ?no reason at all
to believe? changes the polarity of the proposition
that follows; because ?reasonable? falls within this
proposition, its contextual polarity becomes nega-
tive. The word ?Trust? is simply part of a referring
expression and is not being used to express a senti-
ment; thus, its contextual polarity is neutral. Simi-
larly for ?polluters?: in the context of the article, it
simply refers to companies that pollute. Only ?well?
has the same prior and contextual polarity.
Many things must be considered in phrase-level
sentiment analysis. Negation may be local (e.g., not
good), or involve longer-distance dependencies such
as the negation of the proposition (e.g., does not
look very good) or the negation of the subject (e.g.,
347
no one thinks that it?s good). In addition, certain
phrases that contain negation words intensify rather
than change polarity (e.g., not only good but amaz-
ing). Contextual polarity may also be influenced by
modality (e.g., whether the proposition is asserted to
be real (realis) or not real (irrealis) ? no reason at all
to believe is irrealis, for example); word sense (e.g.,
Environmental Trust versus He has won the peo-
ple?s trust); the syntactic role of a word in the sen-
tence (e.g., polluters are versus they are polluters);
and diminishers such as little (e.g., little truth, lit-
tle threat). (See (Polanya and Zaenen, 2004) for a
more detailed discussion of contextual polarity in-
fluencers.)
This paper presents new experiments in automat-
ically distinguishing prior and contextual polarity.
Beginning with a large stable of clues marked with
prior polarity, we identify the contextual polarity of
the phrases that contain instances of those clues in
the corpus. We use a two-step process that employs
machine learning and a variety of features. The
first step classifies each phrase containing a clue as
neutral or polar. The second step takes all phrases
marked in step one as polar and disambiguates their
contextual polarity (positive, negative, both, or neu-
tral). With this approach, the system is able to auto-
matically identify the contextual polarity for a large
subset of sentiment expressions, achieving results
that are significantly better than baseline. In addi-
tion, we describe new manual annotations of contex-
tual polarity and a successful inter-annotator agree-
ment study.
2 Manual Annotation Scheme
To create a corpus for the experiments below, we
added contextual polarity judgments to existing an-
notations in the Multi-perspective Question Answer-
ing (MPQA) Opinion Corpus1, namely to the an-
notations of subjective expressions2. A subjective
expression is any word or phrase used to express
an opinion, emotion, evaluation, stance, speculation,
1The MPQA Corpus is described in (Wiebe et al, 2005) and
available at nrrc.mitre.org/NRRC/publications.htm.
2In the MPQA Corpus, subjective expressions are direct
subjective expressions with non-neutral expression intensity,
plus all the expressive subjective elements. Please see (Wiebe
et al, 2005) for more details on the existing annotations in the
MPQA Corpus.
etc. A general covering term for such states is pri-
vate state (Quirk et al, 1985). In the MPQA Cor-
pus, subjective expressions of varying lengths are
marked, from single words to long phrases.
For this work, our focus is on sentiment expres-
sions ? positive and negative expressions of emo-
tions, evaluations, and stances. As these are types of
subjective expressions, to create the corpus, we just
needed to manually annotate the existing subjective
expressions with their contextual polarity.
In particular, we developed an annotation
scheme3 for marking the contextual polarity of sub-
jective expressions. Annotators were instructed to
tag the polarity of subjective expressions as positive,
negative, both, or neutral. The positive tag is for
positive emotions (I?m happy), evaluations (Great
idea!), and stances (She supports the bill). The neg-
ative tag is for negative emotions (I?m sad), eval-
uations (Bad idea!), and stances (She?s against the
bill). The both tag is applied to sentiment expres-
sions that have both positive and negative polarity.
The neutral tag is used for all other subjective ex-
pressions: those that express a different type of sub-
jectivity such as speculation, and those that do not
have positive or negative polarity.
Below are examples of contextual polarity anno-
tations. The tags are in boldface, and the subjective
expressions with the given tags are underlined.
(5) Thousands of coup supporters celebrated (posi-
tive) overnight, waving flags, blowing whistles . . .
(6) The criteria set by Rice are the following: the
three countries in question are repressive (nega-
tive) and grave human rights violators (negative)
. . .
(7) Besides, politicians refer to good and evil
(both) only for purposes of intimidation and
exaggeration.
(8) Jerome says the hospital feels (neutral) no dif-
ferent than a hospital in the states.
The annotators were asked to judge the contex-
tual polarity of the sentiment that is ultimately be-
ing conveyed by the subjective expression, i.e., once
the sentence has been fully interpreted. Thus, the
subjective expression, they have not succeeded, and
3The annotation instructions are available at
http://www.cs.pitt.edu/?twilson.
348
will never succeed, was marked as positive in the
sentence, They have not succeeded, and will never
succeed, in breaking the will of this valiant people.
The reasoning is that breaking the will of a valiant
people is negative; hence, not succeeding in break-
ing their will is positive.
3 Agreement Study
To measure the reliability of the polarity annotation
scheme, we conducted an agreement study with two
annotators, using 10 documents from the MPQA
Corpus. The 10 documents contain 447 subjective
expressions. Table 1 shows the contingency table for
the two annotators? judgments. Overall agreement is
82%, with a Kappa (?) value of 0.72.
Neutral Positive Negative Both Total
Neutral 123 14 24 0 161
Positive 16 73 5 2 96
Negative 14 2 167 1 184
Both 0 3 0 3 6
Total 153 92 196 6 447
Table 1: Agreement for Subjective Expressions
(Agreement: 82%, ?: 0.72)
For 18% of the subjective expressions, at least one
annotator used an uncertain tag when marking po-
larity. If we consider these cases to be borderline
and exclude them from the study, percent agreement
increases to 90% and Kappa rises to 0.84. Thus, the
annotator agreement is especially high when both
are certain. (Note that all annotations are included
in the experiments described below.)
4 Corpus
In total, 15,991 subjective expressions from 425
documents (8,984 sentences) were annotated with
contextual polarity as described above. Of these sen-
tences, 28% contain no subjective expressions, 25%
contain only one, and 47% contain two or more. Of
the 4,247 sentences containing two or more subjec-
tive expressions, 17% contain mixtures of positive
and negative expressions, and 62% contain mixtures
of polar (positive/negative/both) and neutral subjec-
tive expressions.
The annotated documents are divided into two
sets. The first (66 documents/1,373 sentences/2,808
subjective expressions) is a development set, used
for data exploration and feature development. We
use the second set (359 documents/7,611 sen-
tences/13,183 subjective expressions) in 10-fold
cross-validation experiments, described below.
5 Prior-Polarity Subjectivity Lexicon
For the experiments in this paper, we use a lexicon of
over 8,000 subjectivity clues. Subjectivity clues are
words and phrases that may be used to express pri-
vate states, i.e., they have subjective usages (though
they may have objective usages as well). For this
work, only single-word clues are used.
To compile the lexicon, we began with a list of
subjectivity clues from (Riloff and Wiebe, 2003).
The words in this list were grouped in previous work
according to their reliability as subjectivity clues.
Words that are subjective in most contexts were
marked strongly subjective (strongsubj), and those
that may only have certain subjective usages were
marked weakly subjective (weaksubj).
We expanded the list using a dictionary and a
thesaurus, and also added words from the General
Inquirer positive and negative word lists (General-
Inquirer, 2000) which we judged to be potentially
subjective. We also gave the new words reliability
tags, either strongsubj or weaksubj.
The next step was to tag the clues in the lexicon
with their prior polarity. For words that came from
positive and negative word lists (General-Inquirer,
2000; Hatzivassiloglou and McKeown, 1997), we
largely retained their original polarity, either posi-
tive or negative. We assigned the remaining words
one of the tags positive, negative, both or neutral.
By far, the majority of clues, 92.8%, are
marked as having either positive (33.1%) or nega-
tive (59.7%) prior polarity. Only a small number of
clues (0.3%) are marked as having both positive and
negative polarity. 6.9% of the clues in the lexicon
are marked as neutral. Examples of these are verbs
such as feel, look, and think, and intensifiers such as
deeply, entirely, and practically. These words are in-
cluded because, although their prior polarity is neu-
tral, they are good clues that a sentiment is being
expressed (e.g., feels slighted, look forward to). In-
cluding them increases the coverage of the system.
349
6 Experiments
The goal of the experiments described below is to
classify the contextual polarity of the expressions
that contain instances of the subjectivity clues in
our lexicon. What the system specifically does is
give each clue instance its own label. Note that the
system does not try to identify expression bound-
aries. Doing so might improve performance and is a
promising avenue for future research.
6.1 Definition of the Gold Standard
We define the gold standard used to train and test the
system in terms of the manual annotations described
in Section 2.
The gold standard class of a clue instance that is
not in a subjective expression is neutral: since the
clue is not even in a subjective expression, it is not
contained in a sentiment expression.
Otherwise, if a clue instance appears in just one
subjective expression (or in multiple subjective ex-
pressions with the same contextual polarity), then
the class assigned to the clue instance is the class
of the subjective expression(s). If a clue appears
in at least one positive and one negative subjective
expression (or in a subjective expression marked as
both), then its class is both. If it is in a mixture of
negative and neutral subjective expressions, its class
is negative; if it is in a mixture of positive and neu-
tral subjective expressions, its class is positive.
6.2 Performance of a Prior-Polarity Classifier
An important question is how useful prior polarity
alone is for identifying contextual polarity. To an-
swer this question, we create a classifier that sim-
ply assumes that the contextual polarity of a clue in-
stance is the same as the clue?s prior polarity, and we
explore the classifier?s performance on the develop-
ment set.
This simple classifier has an accuracy of 48%.
From the confusion matrix given in Table 2, we see
that 76% of the errors result from words with non-
neutral prior polarity appearing in phrases with neu-
tral contextual polarity.
6.3 Contextual Polarity Disambiguation
The fact that words with non-neutral prior polarity
so frequently appear in neutral contexts led us to
Prior-Polarity Classifier
Neut Pos Neg Both Total
Neut 798 784 698 4 2284
Pos 81 371 40 0 492
Gold Neg 149 181 622 0 952
Both 4 11 13 5 33
Total 1032 1347 1373 9 3761
Table 2: Confusion matrix for the prior-polarity
classifier on the development set.
adopt a two-step approach to contextual polarity dis-
ambiguation. For the first step, we concentrate on
whether clue instances are neutral or polar in context
(where polar in context refers to having a contextual
polarity that is positive, negative or both). For the
second step, we take all clue instances marked as
polar in step one, and focus on identifying their con-
textual polarity. For both steps, we develop classi-
fiers using the BoosTexter AdaBoost.HM (Schapire
and Singer, 2000) machine learning algorithm with
5000 rounds of boosting. The classifiers are evalu-
ated in 10-fold cross-validation experiments.
6.3.1 Neutral-Polar Classification
The neutral-polar classifier uses 28 features, listed
in Table 3.
Word Features: Word context is a bag of three
word tokens: the previous word, the word itself, and
the next word. The prior polarity and reliability
class are indicated in the lexicon.
Modification Features: These are binary rela-
tionship features. The first four involve relationships
with the word immediately before or after: if the
word is a noun preceded by an adjective, if the pre-
ceding word is an adverb other than not, if the pre-
ceding word is an intensifier, and if the word itself
is an intensifier. A word is considered an intensifier
if it appears in a list of intensifiers and if it precedes
a word of the appropriate part-of-speech (e.g., an in-
tensifier adjective must come before a noun).
The modify features involve the dependency parse
tree for the sentence, obtained by first parsing the
sentence (Collins, 1997) and then converting the tree
into its dependency representation (Xia and Palmer,
2001). In a dependency representation, every node
in the tree structure is a surface word (i.e., there are
no abstract nodes such as NP or VP). The edge be-
tween a parent and a child specifies the grammatical
relationship between the two words. Figure 1 shows
350
Word Features Sentence Features Structure Features
word token strongsubj clues in current sentence: count in subject: binary
word part-of-speech strongsubj clues in previous sentence: count in copular: binary
word context strongsubj clues in next sentence: count in passive: binary
prior polarity: positive, negative, both, neutral weaksubj clues in current sentence: count
reliability class: strongsubj or weaksubj weaksubj clues in previous sentence: count
Modification Features weaksubj clues in next sentence: count Document Feature
preceeded by adjective: binary adjectives in sentence: count document topic
preceeded by adverb (other than not): binary adverbs in sentence (other than not): count
preceeded by intensifier: binary cardinal number in sentence: binary
is intensifier: binary pronoun in sentence: binary
modifies strongsubj: binary modal in sentence (other than will): binary
modifies weaksubj: binary
modified by strongsubj: binary
modified by weaksubj: binary
Table 3: Features for neutral-polar classification
The human rights
report
a
poses
substantial
challenge
to
USthe
interpretation
of
good and evil
det det
det
adj adj
objsubj
mod
mod
conj conjpobj
pobj
p
p
(pos) (neg)
(pos)
(neg)
(pos)
Figure 1: The dependency tree for the sentence The human
rights report poses a substantial challenge to the US interpre-
tation of good and evil. Prior polarity is marked in parentheses
for words that match clues from the lexicon.
an example. The modifies strongsubj/weaksubj fea-
tures are true if the word and its parent share an
adj, mod or vmod relationship, and if its parent is
an instance of a clue from the lexicon with strong-
subj/weaksubj reliability. The modified by strong-
subj/weaksubj features are similar, but look for rela-
tionships and clues in the word?s children.
Structure Features: These are binary features
that are determined by starting with the word in-
stance and climbing up the dependency parse tree
toward the root, looking for particular relationships,
words, or patterns. The in subject feature is true if
we find a subj relationship. The in copular feature is
true if in subject is false and if a node along the path
is both a main verb and a copular verb. The in pas-
sive features is true if a passive verb pattern is found
on the climb.
Sentence Features: These are features that were
found useful for sentence-level subjectivity classifi-
cation by Wiebe and Riloff (2005). They include
counts of strongsubj and weaksubj clues in the cur-
rent, previous and next sentences, counts of adjec-
tives and adverbs other than not in the current sen-
tence, and binary features to indicate whether the
sentence contains a pronoun, a cardinal number, and
a modal other than will.
Document Feature: There is one document fea-
ture representing the topic of the document. A doc-
ument may belong to one of 15 topics ranging from
specific (e.g., the 2002 presidential election in Zim-
babwe) to more general (e.g., economics) topics.
Table 4 gives neutral-polar classification results
for the 28-feature classifier and two simpler classi-
fiers that provide our baselines. The first row in the
table lists the results for a classifier that uses just
one feature, the word token. The second row shows
the results for a classifier that uses both the word to-
ken and the word?s prior polarity as features. The
results for the 28-feature classifier are listed in the
last row. The 28-feature classifier performs signifi-
cantly better (1-tailed t-test, p ? .05) than the two
simpler classifiers, as measured by accuracy, polar
F-measure, and neutral F-measure (? = 1). It has an
accuracy of 75.9%, with a polar F-measure of 63.4
and a neutral F-measure of 82.1.
Focusing on the metrics for polar expressions, it?s
interesting to note that using just the word token as a
feature produces a classifier with a precision slightly
better than the 28-feature classifier, but with a recall
that is 20% lower. Adding a feature for the prior
351
Word Features
word token
word prior polarity: positive, negative, both, neutral
Polarity Features
negated: binary
negated subject: binary
modifies polarity: positive, negative, neutral, both, notmod
modified by polarity: positive, negative, neutral, both, notmod
conj polarity: positive, negative, neutral, both, notmod
general polarity shifter: binary
negative polarity shifter: binary
positive polarity shifter: binary
Table 6: Features for polarity classification
polarity improves recall so that it is only 4.4% lower,
but this hurts precision, which drops to 4.2% lower
than the 28-feature classifier?s precision. It is only
with all the features that we get the best result, good
precision with the highest recall.
The clues in the prior-polarity lexicon have
19,506 instances in the test set. According to the
28-feature neutral-polar classifier, 5,671 of these in-
stances are polar in context. It is these clue instances
that are passed on to the second step in the contex-
tual disambiguation process, polarity classification.
6.3.2 Polarity Classification
Ideally, this second step in the disambiguation
process would be a three-way classification task, de-
termining whether the contextual polarity is posi-
tive, negative or both. However, although the major-
ity of neutral expressions have been filtered out by
the neutral-polar classification in step one, a number
still remain. So, for this step, the polarity classifica-
tion task remains four-way: positive, negative, both,
and neutral.
Table 6 lists the features used by the polarity clas-
sifier. Word token and word prior polarity are un-
changed from the neutral-polar classifier. Negated
is a binary feature that captures whether the word is
being locally negated: its value is true if a negation
word or phrase is found within the four preceeding
words or in any of the word?s children in the de-
pendency tree, and if the negation word is not in a
phrase that intensifies rather than negates (e.g., not
only). The negated subject feature is true if the sub-
ject of the clause containing the word is negated.
The modifies polarity, modified by polarity, and
conj polarity features capture specific relationships
between the word instance and other polarity words
it may be related to. If the word and its parent in
the dependency tree share an obj, adj, mod, or vmod
relationship, the modifies polarity feature is set to
the prior polarity of the word?s parent (if the parent
is not in our prior-polarity lexicon, its prior polarity
is set to neutral). The modified by polarity feature
is similar, looking for adj, mod, and vmod relation-
ships and polarity clues within the word?s children.
The conj polarity feature determines if the word is
in a conjunction. If so, the value of this feature is its
sibling?s prior polarity (as above, if the sibling is not
in the lexicon, its prior polarity is neutral). Figure 1
helps to illustrate these features: modifies polarity is
negative for the word ?substantial,? modified by po-
larity is positive for the word ?challenge,? and conj
polarity is negative for the word ?good.?
The last three polarity features look in a window
of four words before, searching for the presence of
particular types of polarity influencers. General po-
larity shifters reverse polarity (e.g., little truth, lit-
tle threat). Negative polarity shifters typically make
the polarity of an expression negative (e.g., lack of
understanding). Positive polarity shifters typically
make the polarity of an expression positive (e.g.,
abate the damage).
The polarity classification results for this second
step in the contextual disambiguation process are
given in Table 5. Also listed in the table are results
for the two simple classifiers that provide our base-
lines. The first line in Table 5 lists the results for
the classifier that uses just one feature, the word to-
ken. The second line shows the results for the clas-
sifier that uses both the word token and the word?s
prior polarity as features. The last line shows the
results for the polarity classifier that uses all 10 fea-
tures from Table 6.
Mirroring the results from step one, the more
complex classifier performs significantly better than
the simpler classifiers, as measured by accuracy
and all of the F-measures. The 10-feature classi-
fier achieves an accuracy of 65.7%, which is 4.3%
higher than the more challenging baseline provided
by the word + prior polarity classifier. Positive F-
measure is 65.1 (5.7% higher); negative F-measure
is 77.2 (2.3% higher); and neutral F-measure is 46.2
(13.5% higher).
Focusing on the metrics for positive and negative
expressions, we again see that the simpler classifiers
352
Acc Polar Rec Polar Prec Polar F Neut Rec Neut Prec Neut F
word token 73.6 45.3 72.2 55.7 89.9 74.0 81.2
word+priorpol 74.2 54.3 68.6 60.6 85.7 76.4 80.7
28 features 75.9 56.8 71.6 63.4 87.0 77.7 82.1
Table 4: Results for Step 1 Neutral-Polar Classification
Positive Negative Both Neutral
Acc Rec Prec F Rec Prec F Rec Prec F Rec Prec F
word token 61.7 59.3 63.4 61.2 83.9 64.7 73.1 9.2 35.2 14.6 30.2 50.1 37.7
word+priorpol 63.0 69.4 55.3 61.6 80.4 71.2 75.5 9.2 35.2 14.6 33.5 51.8 40.7
10 features 65.7 67.1 63.3 65.1 82.1 72.9 77.2 11.2 28.4 16.1 41.4 52.4 46.2
Table 5: Results for Step 2 Polarity Classification.
Experiment Features Removed
AB1 negated, negated subject
AB2 modifies polarity, modified by polarity
AB3 conj polarity
AB4 general, negative, and positive polarity shifters
Table 7: Features for polarity classification
take turns doing better or worse for precision and
recall. Using just the word token, positive preci-
sion is slightly higher than for the 10-feature clas-
sifier, but positive recall is 11.6% lower. Add the
prior polarity, and positive recall improves, but at
the expense of precision, which is 12.6% lower than
for the 10-feature classifier. The results for negative
expressions are similar. The word-token classifier
does well on negative recall but poorly on negative
precision. When prior polarity is added, negative
recall improves but negative precision drops. It is
only with the addition of the polarity features that we
achieve both higher precisions and higher recalls.
To explore how much the various polarity features
contribute to the performance of the polarity classi-
fier, we perform four experiments. In each experi-
ment, a different set of polarity features is excluded,
and the polarity classifier is retrained and evaluated.
Table 7 lists the features that are removed for each
experiment.
The only significant difference in performance in
these experiments is neutral F-measure when the
modification features (AB2) are removed. These
ablation experiments show that the combination of
features is needed to achieve significant results over
baseline for polarity classification.
7 Related Work
Much work on sentiment analysis classifies docu-
ments by their overall sentiment, for example deter-
mining whether a review is positive or negative (e.g.,
(Turney, 2002; Dave et al, 2003; Pang and Lee,
2004; Beineke et al, 2004)). In contrast, our ex-
periments classify individual words and phrases. A
number of researchers have explored learning words
and phrases with prior positive or negative polarity
(another term is semantic orientation) (e.g., (Hatzi-
vassiloglou and McKeown, 1997; Kamps and Marx,
2002; Turney, 2002)). In contrast, we begin with
a lexicon of words with established prior polarities,
and identify the contextual polarity of phrases in
which instances of those words appear in the cor-
pus. To make the relationship between that task
and ours clearer, note that some word lists used to
evaluate methods for recognizing prior polarity are
included in our prior-polarity lexicon (General In-
quirer lists (General-Inquirer, 2000) used for evalu-
ation by Turney, and lists of manually identified pos-
itive and negative adjectives, used for evaluation by
Hatzivassiloglou and McKeown).
Some research classifies the sentiments of sen-
tences. Yu and Hatzivassiloglou (2003), Kim and
Hovy (2004), Hu and Liu (2004), and Grefenstette et
al. (2001)4 all begin by first creating prior-polarity
lexicons. Yu and Hatzivassiloglou then assign a sen-
timent to a sentence by averaging the prior semantic
orientations of instances of lexicon words in the sen-
tence. Thus, they do not identify the contextual po-
larity of individual phrases containing clues, as we
4In (Grefenstette et al, 2001), the units that are classified are
fixed windows around named entities rather than sentences.
353
do in this paper. Kim and Hovy, Hu and Liu, and
Grefenstette et al multiply or count the prior po-
larities of clue instances in the sentence. They also
consider local negation to reverse polarity. However,
they do not use the other types of features in our
experiments, and they restrict their tags to positive
and negative (excluding our both and neutral cate-
gories). In addition, their systems assign one sen-
timent per sentence; our system assigns contextual
polarity to individual expressions. As seen above,
sentences often contain more than one sentiment ex-
pression.
Nasukawa, Yi, and colleagues (Nasukawa and Yi,
2003; Yi et al, 2003) classify the contextual polarity
of sentiment expressions, as we do. Thus, their work
is probably most closely related to ours. They clas-
sify expressions that are about specific items, and
use manually developed patterns to classify polarity.
These patterns are high-quality, yielding quite high
precision, but very low recall. Their system classi-
fies a much smaller proportion of the sentiment ex-
pressions in a corpus than ours does.
8 Conclusions
In this paper, we present a new approach to
phrase-level sentiment analysis that first determines
whether an expression is neutral or polar and then
disambiguates the polarity of the polar expressions.
With this approach, we are able to automatically
identify the contextual polarity for a large subset of
sentiment expressions, achieving results that are sig-
nificantly better than baseline.
9 Acknowledgments
This work was supported in part by the NSF under
grant IIS-0208798 and by the Advanced Research
and Development Activity (ARDA).
References
P. Beineke, T. Hastie, and S. Vaithyanathan. 2004. The sen-
timental factor: Improving review classification via human-
provided information. In ACL-2004.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL-1997.
K. Dave, S. Lawrence, and D. M. Pennock. 2003. Mining the
peanut gallery: Opinion extraction and semantic classifica-
tion of product reviews. In WWW-2003.
The General-Inquirer. 2000.
http://www.wjh.harvard.edu/?inquirer/spreadsheet guide.htm.
G. Grefenstette, Y. Qu, J.G. Shanahan, and D.A. Evans. 2001.
Coupling niche browsers and affect analysis for an opinion
mining application. In RIAO-2004.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting the
semantic orientation of adjectives. In ACL-1997.
M. Hu and B. Liu. 2004. Mining and summarizing customer
reviews. In KDD-2004.
J. Kamps and M. Marx. 2002. Words with attitude. In 1st
International WordNet Conference.
S-M. Kim and E. Hovy. 2004. Determining the sentiment of
opinions. In Coling 2004.
T. Nasukawa and J. Yi. 2003. Sentiment analysis: Capturing
favorability using natural language processing. In K-CAP
2003.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In ACL-2004.
L. Polanya and A. Zaenen. 2004. Contextual valence shifters.
In Working Notes ? Exploring Attitude and Affect in Text
(AAAI Spring Symposium Series).
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A
Comprehensive Grammar of the English Language. Long-
man, New York.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In EMNLP-2003.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
P. Turney. 2002. Thumbs up or thumbs down? Semantic orien-
tation applied to unsupervised classification of reviews. In
ACL-2002.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In CICLing-
2005.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expres-
sions of opinions and emotions in language. Language Re-
sources and Evalution (formerly Computers and the Human-
ities), 1(2).
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT-2001.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003. Senti-
ment analyzer: Extracting sentiments about a given topic us-
ing natural language processing techniques. In IEEE ICDM-
2003.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identify-
ing the polarity of opinion sentences. In EMNLP-2003.
354
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34?35,
Vancouver, October 2005.
OpinionFinder: A system for subjectivity analysis
Theresa Wilson?, Paul Hoffmann?, Swapna Somasundaran?, Jason Kessler?,
Janyce Wiebe??, Yejin Choi?, Claire Cardie?, Ellen Riloff?, Siddharth Patwardhan?
?Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, Cornell University, Ithaca, NY 14853
?School of Computing, University of Utah, Salt Lake City, UT 84112
{twilson,hoffmanp,swapna,jsk44,wiebe}@cs.pitt.edu,
{ychoi,cardie}@cs.cornell.edu, {riloff,sidd}@cs.utah.edu
1 Introduction
OpinionFinder is a system that performs subjectivity
analysis, automatically identifying when opinions,
sentiments, speculations, and other private states are
present in text. Specifically, OpinionFinder aims to
identify subjective sentences and to mark various as-
pects of the subjectivity in these sentences, includ-
ing the source (holder) of the subjectivity and words
that are included in phrases expressing positive or
negative sentiments.
Our goal with OpinionFinder is to develop a sys-
tem capable of supporting other Natural Language
Processing (NLP) applications by providing them
with information about the subjectivity in docu-
ments. Of particular interest are question answering
systems that focus on being able to answer opinion-
oriented questions, such as the following:
How is Bush?s decision not to ratify the
Kyoto Protocol looked upon by Japan and
other US allies?
How do the Chinese regard the human
rights record of the United States?
To answer these types of questions, a system needs
to be able to identify when opinions are expressed in
text and who is expressing them. Other applications
that would benefit from knowledge of subjective lan-
guage include systems that summarize the various
viewpoints in a document or that mine product re-
views. Even typical fact-oriented applications, such
as information extraction, can benefit from subjec-
tivity analysis by filtering out opinionated sentences
(Riloff et al, 2005).
2 OpinionFinder
OpinionFinder runs in two modes, batch and inter-
active. Document processing is largely the same for
both modes. In batch mode, OpinionFinder takes a
list of documents to process. Interactive mode pro-
vides a front-end that allows a user to query on-line
news sources for documents to process.
2.1 System Architecture Overview
OpinionFinder operates as one large pipeline. Con-
ceptually, the pipeline can be divided into two parts.
The first part performs mostly general purpose doc-
ument processing (e.g., tokenization and part-of-
speech tagging). The second part performs the sub-
jectivity analysis. The results of the subjectivity
analysis are returned to the user in the form of
SGML/XML markup of the original documents.
2.2 Document Processing
For general document processing, OpinionFinder
first runs the Sundance partial parser (Riloff and
Phillips, 2004) to provide semantic class tags, iden-
tify Named Entities, and match extraction patterns
that correspond to subjective language (Riloff and
Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tok-
enize, sentence split, and part-of-speech tag the data,
and the Abney stemmer2 is used to stem. In batch
mode, OpinionFinder parses the data again, this time
to obtain constituency parse trees (Collins, 1997),
which are then converted to dependency parse trees
(Xia and Palmer, 2001). Currently, this stage is only
1http://opennlp.sourceforge.net/
2SCOL version 1g available at http://www.vinartus.net/spa/
34
available for batch mode processing due to the time
required for parsing. Finally, a clue-finder is run to
identify words and phrases from a large subjective
language lexicon.
2.3 Subjectivity Analysis
The subjectivity analysis has four components.
2.3.1 Subjective Sentence Classification
The first component is a Naive Bayes classifier
that distinguishes between subjective and objective
sentences using a variety of lexical and contextual
features (Wiebe and Riloff, 2005; Riloff and Wiebe,
2003). The classifier is trained using subjective and
objective sentences, which are automatically gener-
ated from a large corpus of unannotated data by two
high-precision, rule-based classifiers.
2.3.2 Speech Events and Direct Subjective
Expression Classification
The second component identifies speech events
(e.g., ?said,? ?according to?) and direct subjective
expressions (e.g., ?fears,? ?is happy?). Speech
events include both speaking and writing events.
Direct subjective expressions are words or phrases
where an opinion, emotion, sentiment, etc. is di-
rectly described. A high-precision, rule-based clas-
sifier is used to identify these expressions.
2.3.3 Opinion Source Identification
The third component is a source identifier that
combines a Conditional Random Field sequence
tagging model (Lafferty et al, 2001) and extraction
pattern learning (Riloff, 1996) to identify the sources
of speech events and subjective expressions (Choi
et al, 2005). The source of a speech event is the
speaker; the source of a subjective expression is the
experiencer of the private state. The source identifier
is trained on the MPQA Opinion Corpus3 using a
variety of features. Because the source identifier re-
lies on dependency parse information, it is currently
only available in batch mode.
2.3.4 Sentiment Expression Classification
The final component uses two classifiers to iden-
tify words contained in phrases that express pos-
itive or negative sentiments (Wilson et al, 2005).
3The MPQA Opinion Corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
The first classifier focuses on identifying sentiment
expressions. The second classifier takes the senti-
ment expressions and identifies those that are pos-
itive and negative. Both classifiers were developed
using BoosTexter (Schapire and Singer, 2000) and
trained on the MPQA Corpus.
3 Related Work
Please see (Wiebe and Riloff, 2005; Choi et al,
2005; Wilson et al, 2005) for discussions of related
work in automatic opinion and sentiment analysis.
4 Acknowledgments
This work was supported by the Advanced Research
and Development Activity (ARDA), by the NSF
under grants IIS-0208028, IIS-0208798 and IIS-
0208985, and by the Xerox Foundation.
References
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identi-
fying sources of opinions with conditional random fields and
extraction patterns. In HLT/EMNLP 2005.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL-1997.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML-2001.
E. Riloff and W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems. Technical Report UUCS-04-
015, School of Computing, University of Utah.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In EMNLP-2003.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction. In
AAAI-2005.
E. Riloff. 1996. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
Artificial Intelligence, 85:101?134.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In CICLing-
2005.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis. In
HLT/EMNLP 2005.
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT-2001.
35
c? 2004 Association for Computational Linguistics
Learning Subjective Language
Janyce Wiebe? Theresa Wilson?
University of Pittsburgh University of Pittsburgh
Rebecca Bruce? Matthew Bell?
University of North Carolina University of Pittsburgh
at Asheville
Melanie Martin?
New Mexico State University
Subjectivity in natural language refers to aspects of language used to express opinions, evalua-
tions, and speculations. There are numerous natural language processing applications for which
subjectivity analysis is relevant, including information extraction and text categorization. The
goal of this work is learning subjective language from corpora. Clues of subjectivity are gener-
ated and tested, including low-frequency words, collocations, and adjectives and verbs identified
using distributional similarity. The features are also examined working together in concert. The
features, generated from different data sets using different procedures, exhibit consistency in
performance in that they all do better and worse on the same data sets. In addition, this article
shows that the density of subjectivity clues in the surrounding context strongly affects how likely
it is that a word is subjective, and it provides the results of an annotation study assessing the
subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion
piece recognition (a type of text categorization and genre detection) to demonstrate the utility of
the knowledge acquired in this article.
1. Introduction
Subjectivity in natural language refers to aspects of language used to express opin-
ions, evaluations, and speculations (Banfield 1982; Wiebe 1994). Many natural lan-
guage processing (NLP) applications could benefit from being able to distinguish
subjective language from language used to objectively present factual information.
Current extraction and retrieval technology focuses almost exclusively on the sub-
ject matter of documents. However, additional aspects of a document influence its
relevance, including evidential status and attitude (Kessler, Nunberg, Schu?tze 1997).
Information extraction systems should be able to distinguish between factual infor-
mation (which should be extracted) and nonfactual information (which should be
? Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260.
E-mail{wiebe,mbell}@cs.pitt.edu.
? Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260. Email: twilson@cs.pitt.edu.
? Department of Computer Science, University of North Carolina at Asheville, Asheville, NC 28804.
E-mail: bruce@cs.unca.edu
? Department of Computer Science, New Mexico State University, Las Cruces, NM 88003. E-mail:
mmartin@cs.nmsu.edu.
Submission received: 20 March 2002; Revised submission received: 30 September 2003; Accepted for
publication: 23 January 2004
278
Computational Linguistics Volume 30, Number 3
discarded or labeled as uncertain). Question-answering systems should distinguish
between factual and speculative answers. Multi-perspective question answering aims
to present multiple answers to the user based upon speculation or opinions derived
from different sources (Carbonell 1979; Wiebe et al 2003). Multidocument summa-
rization systems should summarize different opinions and perspectives. Automatic
subjectivity analysis would also be useful to perform flame recognition (Spertus 1997;
Kaufer 2000), e-mail classification (Aone, Ramos-Santacruze, and Niehaus 2000), intel-
lectual attribution in text (Teufel and Moens 2000), recognition of speaker role in radio
broadcasts (Barzialy et al 2000), review mining (Terveen et al 1997), review classifi-
cation (Turney 2002; Pang, Lee, and Vaithyanathan 2002), style in generation (Hovy
1987), and clustering documents by ideological point of view (Sack 1995). In general,
nearly any information-seeking system could benefit from knowledge of how opin-
ionated a text is and whether or not the writer purports to objectively present factual
material.
To perform automatic subjectivity analysis, good clues must be found. A huge
variety of words and phrases have subjective usages, and while some manually de-
veloped resources exist, such as dictionaries of affective language (General-Inquirer
2000; Heise 2000) and subjective features in general-purpose lexicons (e.g., the atti-
tude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]), there is no
comprehensive dictionary of subjective language. In addition, many expressions with
subjective usages have objective usages as well, so a dictionary alone would not suffice.
An NLP system must disambiguate these expressions in context.
The goal of our work is learning subjective language from corpora. In this article,
we generate and test subjectivity clues and contextual features and use the knowledge
we gain to recognize subjective sentences and opinionated documents.
Two kinds of data are available to us: a relatively small amount of data manually
annotated at the expression level (i.e., labels on individual words and phrases) of Wall
Street Journal and newsgroup data and a large amount of data with existing document-
level annotations from the Wall Street Journal (opinion pieces, such as editorials and
reviews, versus nonopinion pieces). Both are used as training data to identify clues
of subjectivity. In addition, we cross-validate the results between the two types of
annotation: The clues learned from the expression-level data are evaluated against the
document-level annotations, and those learned using the document-level annotations
are evaluated against the expression-level annotations.
There were a number of motivations behind our decision to use document-level
annotations, in addition to our manual annotations, to identify and evaluate clues
of subjectivity. The document-level annotations were not produced according to our
annotation scheme and were not produced for the purpose of training and evaluating
an NLP system. Thus, they are an external influence from outside the laboratory. In
addition, there are a great number of these data, enabling us to evaluate the results
on a larger scale, using multiple large test sets. This and cross-training between the
two types of annotations allows us to assess consistency in performance of the various
identification procedures. Good performance in cross-validation experiments between
different types of annotations is evidence that the results are not brittle.
We focus on three types of subjectivity clues. The first are hapax legomena, the set
of words that appear just once in the corpus. We refer to them here as unique words.
The set of all unique words is a feature with high frequency and significantly higher
precision than baseline (Section 3.2).
The second are collocations (Section 3.3). We demonstrate a straightforward method
for automatically identifying collocational clues of subjectivity in texts. The method is
first used to identify fixed n-grams, such as of the century and get out of here. Interest-
279
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
ingly, many include noncontent words that are typically on stop lists of NLP systems
(e.g., of, the, get, out, here in the above examples). The method is then used to identify
an unusual form of collocation: One or more positions in the collocation may be filled
by any word (of an appropriate part of speech) that is unique in the test data.
The third type of subjectivity clue we examine here are adjective and verb fea-
tures identified using the results of a method for clustering words according to dis-
tributional similarity (Lin 1998) (Section 3.4). We hypothesized that two words may
be distributionally similar because they are both potentially subjective (e.g., tragic, sad,
and poignant are identified from bizarre). In addition, we use distributional similarity
to improve estimates of unseen events: A word is selected or discarded based on the
precision of it together with its n most similar neighbors.
We show that the various subjectivity clues perform better and worse on the same
data sets, exhibiting an important consistency in performance (Section 4.2).
In addition to learning and evaluating clues associated with subjectivity, we ad-
dress disambiguating them in context, that is, identifying instances of clues that are
subjective in context (Sections 4.3 and 4.4). We find that the density of clues in the
surrounding context is an important influence. Using two types of annotations serves
us well here, too. It enables us to use manual judgments to identify parameters for
disambiguating instances of automatically identified clues. High-density clues are high
precision in both the expression-level and document-level data. In addition, we give
the results of a new annotation study showing that most high-density clues are in sub-
jective text spans (Section 4.5). Finally, we use the clues together to perform document-
level classification, to further demonstrate the utility of the acquired knowledge (Sec-
tion 4.6).
At the end of the article, we discuss related work (Section 5) and conclusions
(Section 6).
2. Subjectivity
Subjective language is language used to express private states in the context of a
text or conversation. Private state is a general covering term for opinions, evaluations,
emotions, and speculations (Quirk et al 1985). The following are examples of subjective
sentences from a variety of document types.
The first two examples are from Usenet newsgroup messages:
(1) I had in mind your facts, buddy, not hers.
(2) Nice touch. ?Alleges? whenever facts posted are not in your persona of
what is ?real.?
The next one is from an editorial:
(3) We stand in awe of the Woodstock generation?s ability to be unceasingly
fascinated by the subject of itself. (?Bad Acid,? Wall Street Journal,
August 17, 1989)
The next example is from a book review:
(4) At several different layers, it?s a fascinating tale. (George Melloan,
?Whose Spying on Our Computers?? Wall Street Journal, November 1,
1989)
280
Computational Linguistics Volume 30, Number 3
The last one is from a news story:
(5) ?The cost of health care is eroding our standard of living and sapping
industrial strength,? complains Walter Maher, a Chrysler
health-and-benefits specialist. (Kenneth H. Bacon, ?Business and Labor
Reach a Consensus on Need to Overhaul Health-Care System,? Wall
Street Journal, November 1, 1989)
In contrast, the following are examples of objective sentences, sentences without sig-
nificant expressions of subjectivity:
(6) Bell Industries Inc. increased its quarterly to 10 cents from 7 cents a
share.
(7) Northwest Airlines settled the remaining lawsuits filed on behalf of 156
people killed in a 1987 crash, but claims against the jetliner?s maker are
being pursued, a federal judge said. (?Northwest Airlines Settles Rest of
Suits,? Wall Street Journal, November 1, 1989)
A particular model of linguistic subjectivity underlies the current and past re-
search in this area by Wiebe and colleagues. It is most fully presented in Wiebe and
Rapaport (1986, 1988, 1991) and Wiebe (1990, 1994). It was developed to support NLP
research and combines ideas from several sources in fields outside NLP, especially
linguistics and literary theory. The most direct influences on the model were Dolezel
(1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda
(1973, 1976) (pragmatics of point of view), Chatman (1978) (story versus discourse),
Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic
description of opaque contexts), and especially Banfield (1982) (theory of subjectivity
versus communication).1
The remainder of this section sketches our conceptualization of subjectivity and
describes the annotation projects it underlies.
Subjective elements are linguistic expressions of private states in context. Subjec-
tive elements are often lexical (examples are stand in awe, unceasingly, fascinated in (3)
and eroding, sapping, and complains in (5)). They may be single words (e.g., complains)
or more complex expressions (e.g., stand in awe, what a NP). Purely syntactic or mor-
phological devices may also be subjective elements (e.g., fronting, parallelism, changes
in aspect).
A subjective element expresses the subjectivity of a source, who may be the writer
or someone mentioned in the text. For example, the source of fascinating in (4) is
the writer, while the source of the subjective elements in (5) is Maher (according to
the writer). In addition, a subjective element usually has a target, that is, what the
subjectivity is about or directed toward. In (4), the target is a tale; in (5), the target of
Maher?s subjectivity is the cost of health care.
Note our parenthetical above??according to the writer??concerning Maher?s
subjectivity. Maher is not directly speaking to us but is being quoted by the writer.
Thus, the source is a nested source, which we notate (writer, Maher); this represents
the fact that the subjectivity is being attributed to Maher by the writer. Since sources
1 For additional citations to relevant work from outside NLP, please see Banfield (1982), Fludernik (1993),
Wiebe (1994), and Stein and Wright (1995).
281
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
are not directly addressed by the experiments presented in this article, we merely
illustrate the idea here with an example, to give the reader an idea:
The Foreign Ministry said Thursday that it was ?surprised, to put it
mildly? by the U.S. State Department?s criticism of Russia?s human
rights record and objected in particular to the ?odious? section on
Chechnya. (Moscow Times, March 8, 2002]
Let us consider some of the subjective elements in this sentence, along with their
sources:
surprised, to put it mildly: (writer, Foreign Ministry, Foreign Ministry)
to put it mildly: (writer, Foreign Ministry)
criticism: (writer, Foreign Ministry, Foreign Ministry, U.S. State Department)
objected: (writer, Foreign Ministry)
odious: (writer, Foreign Ministry)
Consider surprised, to put it mildly. This refers to a private state of the Foreign Ministry
(i.e., it is very surprised). This is in the context of The Foreign Ministry said, which is in
a sentence written by the writer. This gives us the three-level source (writer, Foreign
Ministry, Foreign Ministry). The phrase to put it mildly, which expresses sarcasm, is
attributed to the Foreign Ministry by the writer (i.e., according to the writer, the Foreign
Ministry said this). So its source is (writer, Foreign Ministry). The subjective element
criticism has a deeply nested source: According to the writer, the Foreign Ministry said
it is surprised by the U.S. State Department?s criticism.
The nested-source representation allows us to pinpoint the subjectivity in a sen-
tence. For example, there is no subjectivity attributed directly to the writer in the
above sentence: At the level of the writer, the sentence merely says that someone
said something and objected to something (without evaluating or questioning this).
If the sentence started The magnificent Foreign Ministry said. . . , then we would have an
additional subjective element, magnificent, with source (writer).
Note that subjective does not mean not true. Consider the sentence John criticized
Mary for smoking. The verb criticized is a subjective element, expressing negative eval-
uation, with nested source (writer, John). But this does not mean that John does not
believe that Mary smokes. (In addition, the fact that John criticized Mary is being
presented as true by the writer.)
Similarly, objective does not mean true. A sentence is objective if the language used
to convey the information suggests that facts are being presented; in the context of
the discourse, material is objectively presented as if it were true. Whether or not the
source truly believes the information, and whether or not the information is in fact
true, are considerations outside the purview of a theory of linguistic subjectivity.
An aspect of subjectivity highlighted when we are working with NLP applications
is ambiguity. Many words with subjective usages may be used objectively. Examples
are sapping and eroding. In (5), they are used subjectively, but one can easily imagine
objective usages, in a scientific domain, for example. Thus, an NLP system may not
merely consult a list of lexical items to accurately identify subjective language but
must disambiguate words, phrases, and sentences in context. In our terminology, a
potential subjective element (PSE) is a linguistic element that may be used to express
282
Computational Linguistics Volume 30, Number 3
Table 1
Data Sets and Annotations used in Experiments. Annotators M, MM, and T are
co-authors of this paper. D and R are not.
Name Source Number of Words Annotators Type of
annotation
WSJ-SE Wall Street Journal 18,341 D,M Subjective elements
NG-SE Newsgroup 15,413 M Subjective elements
NG-FE Newsgroup 88,210 MM,R Flame elements
OP1 Wall Street Journal 640,975 M,T Documents
Composed of 4 data sets: W9-4,W9-10,W9-22,W-33
OP2 Wall Street Journal 629,690 M,T Documents
Composed of 4 data sets: W9-2,W9-20,W9-21,W-23
subjectivity. A subjective element is an instance of a potential subjective element, in a
particular context, that is indeed subjective in that context (Wiebe 1994).
In this article, we focus on learning lexical items that are associated with subjec-
tivity (i.e., PSEs) and then using them in concert to disambiguate instances of them
(i.e., to determine whether the instances are subjective elements).
2.1 Manual Annotations
In our subjectivity annotation projects, we do not give the annotators lists of particular
words and phrases to look for. Rather, we ask them to label sentences according to
their interpretations in context. As a result, the annotators consider a large variety of
expressions when performing annotations.
We use data that have been manually annotated at the expression level, the sen-
tence level, and the document level. For diversity, we use data from the Wall Street
Journal Treebank as well as data from a corpus of Usenet newsgroup messages. Table
1 summarizes the data sets and annotations used in this article. None of the datasets
overlap. The annotation types listed in the table are those used in the experiments
presented in this article.
In our first subjectivity annotation project (Wiebe, Bruce, and O?Hara 1999; Bruce
and Wiebe 1999), a corpus of sentences from the Wall Street Journal Treebank Corpus
(Marcus, Santorini, and Marcinkiewicz 1993) (corpus WSJ-SE in Table 1) was annotated
at the sentence level by multiple judges. The judges were instructed to classify a sen-
tence as subjective if it contained any significant expressions of subjectivity, attributed
to either the writer or someone mentioned in the text, and to classify the sentence as
objective, otherwise. After multiple rounds of training, the annotators independently
annotated a fresh test set of 500 sentences from WSJ-SE. They achieved an average
pairwise kappa score of 0.70 over the entire test set, an average pairwise kappa score
of 0.80 for the 85% of the test set for which the annotators were somewhat sure of
their judgments, and an average pairwise kappa score of 0.88 for the 70% of the test
set for which the annotators were very sure of their judgments.
We later asked the same annotators to identify the subjective elements in WSJ-
SE. Specifically, each annotator was given the subjective sentences he identified in
283
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
the previous study and asked to put brackets around the words he believed caused
the sentence to be classified as subjective.2 For example (subjective elements are in
parentheses):
They paid (yet) more for (really good stuff).
(Perhaps you?ll forgive me) for reposting his response.
No other instructions were given to the annotators and no training was performed for
the expression-level task. A single round of tagging was performed, with no commu-
nication between annotators. There are techniques for analyzing agreement when an-
notations involve segment boundaries (Litman and Passonneau 1995; Marcu, Romera,
and Amorortu 1999), but our focus in this article is on words. Thus, our analyses are
at the word level: Each word is classified as either appearing in a subjective element
or not. Punctuation and numbers are excluded from the analyses. The kappa value for
word agreement in this study is 0.42.
Another two-level annotation project was performed in Wiebe et al (2001), this
time involving document-level and expression-level annotations of newsgroup data
(NG-FE in Table 1). In that project, we were interested in annotating flames, inflam-
matory messages in newsgroups or listservs. Note that inflammatory language is a
kind of subjective language. The annotators were instructed to mark a message as
a flame if the main intention of the message is a personal attack and the message
contains insulting or abusive language.
After multiple rounds of training, three annotators independently annotated a
fresh test set of 88 messages from NG-FE. The average pairwise percentage agreement
is 92% and the average pairwise kappa value is 0.78. These results are comparable to
those of Spertus (1997), who reports 98% agreement on noninflammatory messages
and 64% agreement on inflammatory messages.
Two of the annotators were then asked to identify the flame elements in the entire
corpus NG-FE. Flame elements are the subset of subjective elements that are perceived
to be inflammatory. The two annotators were asked to do this in the entire corpus, even
those messages not identified as flames, because messages that were not judged to be
flames at the document level may contain some individual inflammatory phrases. As
above, no training was performed for the expression-level task, and a single round of
tagging was performed, without communication between annotators. Agreement was
measured in the same way as in the subjective-element study above. The kappa value
for flame element annotations in corpus NG-FE is 0.46.
An additional annotation project involved a single annotator, who performed
subjective-element annotations on the newsgroup corpus NG-SE.
The agreement results above suggest that good levels of agreement can be achieved
at higher levels of classification (sentence and document), but agreement at the expres-
sion level is more challenging. The agreement values are lower for the expression-level
annotations but are still much higher than that expected by chance.
Note that our word-based analysis of agreement is a tough measure, because it
requires that exactly the same words be identified by both annotators. Consider the
following example from WSJ-SE:
D: (played the role well) (obligatory ragged jeans a thicket of long hair
and rejection of all things conventional)
2 We are grateful to Aravind Joshi for suggesting this level of annotation.
284
Computational Linguistics Volume 30, Number 3
M: played the role (well) (obligatory) (ragged) jeans a (thicket) of long
hair and (rejection) of (all things conventional)
Judge D in the example consistently identifies entire phrases as subjective, while judge
M prefers to select discrete lexical items.
Despite such differences between annotators, the expression-level annotations
proved very useful for exploring hypotheses and generating features, as described
below.
Since this article was written, a new annotation project has been completed. A
10,000-sentence corpus of English-language versions of world news articles has been
annotated with detailed subjectivity information as part of a project investigating
multiple-perspective question answering (Wiebe et al 2003). These annotations are
much more detailed than the annotations used in this article (including, for example,
the source of each private state). The interannotator agreement scores for the new
corpus are high and are improvements over the results of the studies described above
(Wilson and Wiebe 2003).
The current article uses existing document-level subjective classes, namely edito-
rials, letters to the editor, Arts & Leisure reviews, and Viewpoints in the Wall Street
Journal. These are subjective classes in the sense that they are text categories for which
subjectivity is a key aspect. We refer to them collectively as opinion pieces. All other
types of documents in the Wall Street Journal are collectively referred to as nonopinion
pieces.
Note that opinion pieces are not 100% subjective. For example, editorials contain
objective sentences presenting facts supporting the writer?s argument, and reviews
contain sentences objectively presenting facts about the product beign reviewed. Sim-
ilarly, nonopinion pieces are not 100% objective. News reports present opinions and
reactions to reported events (van Dijk 1988); they often contain segments starting with
expressions such as critics claim and supporters argue. In addition, quoted-speech sen-
tences in which individuals express their subjectivity are often included (Barzilay et
al. 2000). For concreteness, let us consider WSJ-SE, which, recall, has been manually
annotated at the sentence level. In WSJ-SE, 70% of the sentences in opinion pieces
are subjective and 30% are objective. In nonopinion pieces, 44% of the sentences are
subjective and only 56% are objective. Thus, while there is a higher concentration of
subjective sentences in opinion versus nonopinion pieces, there are many subjective
sentences in nonopinion pieces and objective sentences in opinion pieces.
An inspection of some data reveals that some editorial and review articles are not
marked as such by the Wall Street Journal. For example, there are articles whose purpose
is to present an argument rather than cover a news story, but they are not explicitly
labeled as editorials by the Wall Street Journal. Thus, the opinion piece annotations of
data sets OP1 and OP2 in Table 1 have been manually refined. The annotation instruc-
tions were simply to identify any additional opinion pieces that were not marked as
such. To test the reliability of this annotation, two judges independently annotated
two Wall Street Journal files, W9-22 and W9-33, each containing approximately 160,000
words. This is an ?annotation lite? task: With no training, the annotators achieved
kappa values of 0.94 and 0.95, and each spent an average of three hours per Wall
Street Journal file.
3. Generating and Testing Subjective Features
3.1 Introduction
The goal in this section is to learn lexical subjectivity clues of various types, single
words as well as collocations. Some require no training data, some are learned us-
285
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
ing the expression-level subjective-element annotations as training data, and some
are learned using the document-level opinion piece annotations as training data (i.e.,
opinion piece versus nonopinion piece). All of the clues are evaluated with respect to
the document-level opinion piece annotations. While these evaluations are our focus,
because many more opinion piece than subjective-element data exist, we do evaluate
the clues learned from the opinion piece data on the subjective-element data as well.
Thus, we cross-validate the results both ways between the two types of annotations.
Throughout this section, we evaluate sets of clues directly, by measuring the pro-
portion of clues that appear in subjective documents or expressions, seeking those that
appear more often than expected. In later sections, the clues are used together to find
subjective sentences and to perform text categorization.
The following paragraphs give details of the evaluation and experimental design
used in this section.
The proportion of clues in subjective documents or expressions is their precision.
Specifically, the precision of a set S with respect to opinion pieces is
prec(S) =
number of instances of members of S in opinion pieces
total number of instances of members of S in the data
The precision of a set S with respect to subjective elements is
prec(S) =
number of instances of members of S in subjective elements
total number of instances of members of S in the data
In the above, S is a set of types (not tokens). The counts are of tokens (i.e., instances
or occurrences) of members of S.
Why use a set rather than individual items? Many good clues of subjectivity occur
with low frequency (Wiebe, McKeever, and Bruce 1998). In fact, as we shall see below,
uniqueness in the corpus is an informative feature for subjectivity classification. Thus,
we do not want to discard low-frequency clues, because they are a valuable source of
information, and we do not want to evaluate individual low-frequency lexical items,
because the results would be unreliable. Our strategy is thus to identify and evaluate
sets of words and phrases, rather than individual items.
What kinds of results may we expect? We cannot expect absolutely high precision
with respect to the opinion piece classifications, even for strong clues, for three reasons.
First, for our purposes, the data are noisy. As mentioned above, while the proportion
of subjective sentences is higher in opinion than in nonopinion pieces, the proportions
are not 100 and 0: Opinion pieces contain objective sentences, and nonopinion pieces
contain subjective sentences.
Second, we are trying to learn lexical items associated with subjectivity, that is,
PSEs. As discussed above, many words and phrases with subjective usages have ob-
jective usages as well. Thus, even in perfect data with no noise, we would not expect
100% precision. (This is the motivation for the work on density presented in section
4.4.)
Third, the distribution of opinions and nonopinions is highly skewed in favor of
nonopinions: Only 9% of the articles in the combination of OP1 and OP2 are opinion
pieces.
In this work, increases in precision over a baseline precision are used as evidence
that promising sets of PSEs have been found. Our main baseline for comparison is
the number of word instances in opinion pieces, divided by the total number of word
instances:
Baseline Precision =
number of word instances in opinion pieces
total number of word instances
286
Computational Linguistics Volume 30, Number 3
Table 2
Frequencies and increases in precision of unique
words in subjective-element data. Baseline
frequency is the total number of words, and
baseline precision is the proportion of words in
subjective elements.
WSJ-SE
D M
freq +prec +prec
Unique words 2,615 +.07 +.12
Baseline 18,341 .07 .08
Words and phrases with higher proportions than this appear more than expected in
opinion pieces.
To further evaluate the quality of a set of PSEs, we also perform the following
significance test. For a set of PSEs in a given data set, we test the significance of the
difference between (1) the proportion of words in opinion pieces that are PSEs and (2)
the proportion of words in nonopinion pieces that are PSEs, using the z-significance
test for two proportions.
Before we continue, there are a few more technical items to mention concerning
the data preparation and experimental design:
? All of the data sets are stemmed using Karp?s morphological analyzer
(Karp et al 1994) and part-of-speech tagged using Brill?s (1992) tagger.
? When the opinion piece classifications are used for training, the existing
classifications, assigned by the Wall Street Journal, are used. Thus, the
processes using them as training data may be applied to more data to
learn more clues, without requiring additional manual annotation.
? When the opinion piece data are used for testing, the manually refined
classifications (described at the end of Section 2.1) are used.
? OP1 and OP2 together comprise eight treebank files. Below, we often
give results separately for the component files, allowing us to assess the
consistency of results for the various types of clues.
3.2 Unique Words
In this section, we show that low-frequency words are associated with subjectivity in
both the subjective-element and opinion piece data. Apparently, people are creative
when they are being opinionated.
Table 2 gives results for unique words in subjective-element data. Recall that
unique words are those that appear just once in the corpus, that is, hapax legomena.
The first row of Table 2 gives the frequency of unique words in WSJ-SE, followed
by the percentage-point improvements in precision over baseline for unique words in
subjective elements marked by two annotators (denoted as D and M in the table). The
second row gives baseline frequency and precisions. Baseline frequency is the total
number of words in WSJ-SE. Baseline precision for an annotator is the proportion of
words included in subjective elements by that annotator. Specifically, consider anno-
tator M. The baseline precision of words in subjective elements marked by M is 0.08,
287
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 3
Frequencies and increases in precision for words that appear exactly once in the data sets
composing OP1. For each data set, baseline frequency is the total number of words, and
baseline precision is the proportion of words in opinion pieces.
W9-04 W9-10 W9-22 W9-33
freq +prec freq +prec freq +prec freq +prec
Unique words 4,794 +.15 4,763 +.16 4,274 +.11 4,567 +.11
Baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14
but the precision of unique words in these same annotations is 0.20, 0.12 points higher
than the baseline. This is a 150% improvement over the baseline.
The number of unique words in opinion pieces is also higher than expected. Table
3 compares the precision of the set of unique words to the baseline precision (i.e.,
the precision of the set of all words that appear in the corpus) in the four WSJ files
composing OP1. Before this analysis was performed, numbers were removed from the
data (we are not interested in the fact that, say, the number 163,213.01 appears just once
in the corpus). The number of words in each data set and baseline precisions are listed
at the bottom of the table. The freq columns give total frequencies. The +prec columns
show the percentage-point improvements in precision over baseline. For example, in
W9-10, unique words have precision 0.34: 0.18 baseline plus an improvement over
baseline of 0.16. The difference in the proportion of words that are unique in opinion
pieces and the proportion of words that are unique in nonopinion pieces is highly
significant, with p < 0.001 (z ? 22) for all of the data sets. Note that not only does the
set of unique words have higher than baseline precision, the set is a frequent feature.
The question arises, how does corpus size affect the precision of the set of unique
words? Presumably, uniqueness in a larger corpus is more meaningful than uniqueness
in a smaller one. The results in Figure 1 provide evidence that it is. The y-axis in Figure
1 represents increase in precision over baseline and the x-axis represents corpus size.
Five graphs are plotted, one for the set of words that appear exactly once (uniques),
one for the set of words that appear exactly twice ( freq2), one for the set of words that
appear exactly three times ( freq3), etc.
In Figure 1, increases in precision are given for corpora of size n, where n =
20, 40, . . . , 2420, 2440 documents. Each data point is an average over 25 sample corpora
of size n. The sample corpora were chosen from the concatenation of OP1 and OP2, in
which 9% of the documents are opinion pieces. The sample corpora were created by
randomly selecting documents from the large corpus, preserving the 9% distribution
of opinion pieces. At the smallest corpus size (containing 20 documents), the average
number of words is 9,617. At the largest corpus size (containing 2440 documents), the
average is 1,225,186 words.
As can be seen in the figure, the precision of unique and other low-frequency
words increases with corpus size, with increases tapering off at the largest corpus size
tested. Words with frequency 2 also realize a nice increase, although one that is not as
dramatic, in precision over baseline. Even words of frequency 3, 4, and 5 show modest
increases.
To help us understand the importance of low-frequency words in large as opposed
to small data sets, we can consider the following analogy. With collectible trading
cards, rare cards are the most valuable. However, if we have some cards and are
trying to determine thier value, looking in only a few packs of cards will not tell us if
288
Computational Linguistics Volume 30, Number 3
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
20 620 1220 1820 2420
Corpus Size (documents)
Increase in 
Precision
uniques freq2 freq3 freq4 freq5
Figure 1
Precision of low-frequency words as corpus size increases.
any of our cards are valuable. Only by looking at many packs of cards can we make
a determination as to which are the rare ones. Only in samples of sufficient size is
uniqueness informative.
The results in this section suggest that an NLP system using uniqueness features
to recognize subjectivity should determine uniqueness with respect to the test data
augmented with an additional store of (unannotated) data.
3.3 Identifying Potentially Subjective Collocations from Subjective-Element and
Flame-Element Annotations
In this section, we describe experiments in identifying potentially subjective colloca-
tions.
Collocations are selected from the subjective-element data (i.e., NG-SE, NG-FE, and
WSJ-SE), using the union of the annotators? tags for the data sets tagged by multiple
taggers. The results are then evaluated on opinion piece data.
The selection procedure is as follows. First, all 1-grams, 2-grams, 3-grams, and
4-grams are extracted from the data. In this work, each constituent of an n-gram is
a word-stem, part-of-speech pair. For example, (in-prep the-det can-noun) is a 3-gram
that matches trigrams consisting of preposition in, followed by determiner the, and
ending with noun can.
A subset of the n-grams are then selected based on precision. The precision of an
n-gram is the number of subjective instances of that n-gram in the data divided by
the total number of instances of that n-gram in the data. An instance of an n-gram is
subjective if each word occurs in a subjective element in the data.
n-grams are selected based on two criteria. First, the precision of the n-gram must
be greater than the baseline precision (i.e., the proportion of all word instances that
289
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
are in subjective elements). Second, the precision of the n-gram must be greater than
the maximum precision of its constituents. This criterion is used to avoid selecting
unnecessarily long collocations. For example, scumbag is a strongly subjective clue. If
be a scumbag does not have higher precision than scumbag alone, we do not want to
select it.
Specifically, let (W1, W2) be a bigram consisting of consecutive words W1 and W2.
(W1,W2) is identified as a potential subjective element if prec(W1, W2) ? 0.1 and:
prec(W1, W2) > max(prec(W1), prec(W2))
For trigrams, we extend the second condition as follows. Let (W1, W2, W3) be a trigram
consisting of consecutive words W1, W2, and W3. The condition is then
prec(W1, W2, W3) > max(prec(W1, W2), prec(W3))
or
prec(W1, W2, W3) > max(prec(W1), prec(W2, W3))
The selection of 4-grams is similar to the selection of 3-grams, comparing the 4-gram
first with the maximum of the precisions of word W1 and trigram (W2, W3, W4) and
then with the maximum of the precisions of trigram (W1,W2,W3) and word W4. We
call the n-gram collocations identified as above fixed-n-grams.
We also define a type of collocation called a unique generalized n-gram (ugen-n-
gram). Such collocations have placeholders for unique words. As will be seen below,
these are our highest-precision features.
To find and select such generalized collocations, we first find every word that
appears just once in the corpus and replace it with a new word, UNIQUE (but re-
membering the part of speech of the original word). In essence, we treat the set of
single-instance words as a single, frequently occurring word (which occurs with var-
ious parts of speech). Precisely the same method used for extracting and selecting
n-grams above is used to obtain the potentially subjective collocations with one or
more positions filled by a UNIQUE, part-of-speech pair.
To test the ugen-n-grams extracted from the subjective-element training data using
the method outlined above, we assess their precision with respect to opinion piece
data. As with the training data, all unique words in the test data are replaced by
UNIQUE. When a ugen-n-gram is matched against the test data, the UNIQUE fillers
match words (of the appropriate parts of speech) that are unique in the test data.
Table 4 shows the results of testing the fixed-n-gram and the ugen-n-gram patterns
identified as described above on the four data sets composing OP1. The freq columns
give total frequencies, and the +prec columns show the improvements in precision
from the baseline. The number of words in each data set and baseline precisions are
given at the bottom of the table. For all n-gram features besides the fixed-4-grams and
ugen-4-grams, the proportion of features in opinion pieces is significantly greater than
the proportion of features in nonopinion pieces.3
The question arises, how much overlap is there between instances of fixed-n-grams
and instances of ugen-n-grams? In the test data of Table 4, there are a total of 8,577
fixed-n-grams instances. Only 59 of these, fewer than 1% are contained (wholly or in
part) in ugen-n-gram instances. This small intersection set shows that two different
types of potentially subjective collocations are being recognized.
3 Specifically, the difference between (1) the number of feature instances in opinion pieces divided by the
number of words in opinion pieces and (2) the number of feature instances in nonopinion pieces
divided by the number of words in nonopinion pieces is significant (p < 0.05) for all data sets.
290
Computational Linguistics Volume 30, Number 3
Table 4
Frequencies and increases in precision of fixed-n-gram and ugen-n-gram collocations
learned from the subjective-element data. For each data set, baseline frequency is the total
number of words, and baseline precision is the proportion of words in opinion pieces.
W9-04 W9-10 W9-22 W9-33
freq +prec freq +prec freq +prec freq +prec
fixed-2-grams 1,840 +.07 1,972 +.07 1,933 +.04 1,839 +.05
ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17
fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05
ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15
fixed-4-grams 18 +.15 17 +.06 12 +.29 14 ?.07
ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25
baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14
Randomly selected examples of our learned collocations that appear in the test
data are given in Tables 5 and 6. It is interesting to note that the unique generalized
collocations were learned from the training data by their matching different unique
words from the ones they match in the test data.
3.4 Generating Features from Document-Level Annotations Using Distributional
Similarity
In this section, we identify adjective and verb PSEs using distributional similarity.
Opinion-piece data are used for training, and (a different set of) opinion-piece data
and the subjective-element data are used for testing.
With distributional similarity, words are judged to be more or less similar based
on their distributional patterning in text (Lee 1999; Lee and Pereira 1999). Our
Table 5
Random sample of fixed-3-gram collocations in OP1.
one-noun of-prep his-det worst-adj of-prep all-det
quality-noun of-prep the-det to-prep do-verb so-adverb
in-prep the-det company-noun you-pronoun and-conj your-pronoun
have-verb taken-verb the-det rest-noun of-prep us-pronoun
are-verb at-prep least-adj but-conj if-prep you-pronoun
as-prep a-det weapon-noun continue-verb to-to do-verb
purpose-noun of-prep the-det could-modal have-verb be-verb
it-pronoun seem-verb to-prep to-pronoun continue-verb to-prep
have-verb be-verb the-det do-verb something-noun about-prep
cause-verb you-pronoun to-to evidence-noun to-to back-adverb
that-prep you-pronoun are-verb i-pronoun be-verb not-adverb
of-prep the-det century-noun of-prep money-noun be-prep
291
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 6
Random sample of unique generalized collocations in OP1. U: UNIQUE.
Pattern Instances
U-adj as-prep: drastic as; perverse as; predatory as
U-adj in-prep: perk in; unsatisfying in; unwise in
U-adverb U-verb: adroitly dodge; crossly butter; unceasingly fascinate
U-noun back-adverb: cutting back; hearken back
U-verb U-adverb: coexist harmoniously; flouncing tiresomely
ad-noun U-noun: ad hoc; ad valorem
any-det U-noun: any over-payment; any tapings; any write-off
are-verb U-noun: are escapist; are lowbrow; are resonance
but-conj U-noun: but belch; but cirrus; but ssa
different-adj U-noun: different ambience; different subconferences
like-prep U-noun: like hoffmann; like manute; like woodchuck
national-adj U-noun: national commonplace; national yonhap
particularly-adverb U-adj: particularly galling; particularly noteworthy
so-adverb U-adj: so monochromatic; so overbroad; so permissive
this-det U-adj: this biennial; this inexcusable; this scurrilous
your-pronoun U-noun: your forehead; your manuscript; your popcorn
U-adj and-conj U-adj: arduous and raucous; obstreperous and abstemious
U-noun be-verb a-det: acyclovir be a; siberia be a
U-noun of-prep its-pronoun: outgrowth of its; repulsion of its
U-verb and-conj U-verb: wax and brushed; womanize and booze
U-verb to-to a-det: cling to a; trek to a
are-verb U-adj to-to: are opaque to; are subject to
a-det U-noun and-conj: a blindfold and; a rhododendron and
a-det U-verb U-noun: a jaundice ipo; a smoulder sofa
it-pronoun be-verb U-adverb: it be humanly; it be sooo
than-prep a-det U-noun: than a boob; than a menace
the-det U-adj and-conj: the convoluted and; the secretive and
the-det U-noun that-prep: the baloney that; the cachet that
to-to a-det U-adj: to a gory; to a trappist
to-to their-pronoun U-noun: to their arsenal; to their subsistence
with-prep an-det U-noun: with an alias; with an avalanche
292
Computational Linguistics Volume 30, Number 3
trainingPrec(s) is the precision of s in the training data
validationPrec(s) is the precision of s in the validation data
testPrec(s) is the precision of s in the test data
(similarly for trainingFreq, validationFreq, and testFreq)
S = the set of all adjectives (verbs) in the training data
for T in [0.01,0.04,. . .,0.70]:
for n in [2,3,. . .,40]:
retained = {}
For si in S:
if trainingPrec({si} ? Ci,n) > T:
retained = retained ? {si} ? Ci,n
RT,n = retained
ADJpses = {} (VERBpses = {})
for T in [0.01,0.04,. . .,0.70]:
for n in [2,3,. . .,40]:
if validationPrec(RT,n) ? 0.28 (0.23 for verbs)
and validationFreq(RT,n) ? 100:
ADJpses = ADJpses ? RT,n (VERBpses = VERBpses ? RT,n)
Results in Table 7 show testPrec(ADJpses) and testFreq(ADJpses).
Figure 2
Algorithm for selecting adjective and verb features using distributional similarity.
motivation for experimenting with it to identify PSEs was twofold. First, we hypoth-
esized that words might be distributionally similar because they share pragmatic us-
ages, such as expressing subjectivity, even if they are not close synonyms. Second,
as shown above, low-frequency words appear more often in subjective texts than ex-
pected. We did not want to discard all low-frequency words from consideration but
cannot effectively judge the suitability of individual words. Thus, to decide whether
to retain a word as a PSE, we consider the precision not of the individual word, but
of the word together with a cluster of words similar to it.
Many variants of distributional similarity have been used in NLP (Lee 1999; Lee
and Pereira 1999). Dekang Lin?s (1998) method is used here. In contrast to many
implementations, which focus exclusively on verb-noun relationships, Lin?s method
incorporates a variety of syntactic relations. This is important for subjectivity recogni-
tion, because PSEs are not limited to verb-noun relationships. In addition, Lin?s results
are freely available.
A set of seed words begins the process. For each seed si, the precision of the set
{si}?Ci,n in the training data is calculated, where Ci,n is the set of n words most similar
to si, according to Lin?s (1998) method. If the precision of {si} ? Ci,n is greater than a
threshold T, then the words in this set are retained as PSEs. If it is not, neither si nor
the words in Ci,n are retained. The union of the retained sets will be denoted RT,n, that
is, the union of all sets {si} ? Ci,n with precision on the training set > T.
In Wiebe (2000), the seeds (the sis) were extracted from the subjective-element
annotations in corpus WSJ-SE. Specifically, the seeds were the adjectives that appear
at least once in a subjective element in WSJ-SE. In this article, the opinion piece corpus
is used to move beyond the manual annotations and small corpus of the earlier work,
and a much looser criterion is used to choose the initial seeds: All of the adjectives
(verbs) in the training data are used.
The algorithm for the process is given in Figure 2. There is one small difference
for adjectives and verbs noted in the figure, that is, the precision threshold of 0.28 for
293
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 7
Frequencies and increases in precision for adjective and verb features identified
using distributional similarity with filtering. For each test data set, baseline
frequency is the total number of words, and baseline precision is the proportion of
words in opinion pieces.
Baseline ADJpses VERBpses
Training Validation Test freq prec freq +prec freq +prec
W9-10 W9-22
W9-22 W9-10 W9-33 153,634 .14 1,576 +.12 1,490 +.11
W9-10 W9-33
W9-33 W9-10 W9-22 155,135 .13 859 +.15 535 +.11
W9-22 W9-33
W9-33 W9-22 W9-10 156,334 .18 249 +.22 224 +.10
All pairings of W9-10,
W9-22,W9-33 W9-4 156,421 .19 1,872 +.17 1,777 +.15
adjectives versus 0.23 for verbs. These thresholds were determined using validation
data.
Seeds and their clusters are assessed on a training set for many parameter settings
(cluster size n from 2 through 40, and precision threshold T from 0.01 through 0.70
by .03). As mentioned above, each (n, T) parameter pair yields a set of adjectives RT,n,
that is, the union of all sets {si}?Ci,n with precision on the training set > T. A subset,
ADJpses, of those sets is chosen based on precision and frequency in a validation set.
Finally, the ADJpses are tested on the test set.
Table 7 shows the results for four opinion piece test sets. Multiple training-
validation data set pairs are used for each test set, as given in Table 7. The results
are for the union of the adjectives (verbs) chosen for each pair. The freq columns give
total frequencies, and the +prec columns show the improvements in precision from
the baseline. For each data set, the difference between the proportion of instances
of ADJpses in opinion pieces and the proportion in nonopinion pieces is significant
(p < 0.001, z ? 9.2). The same is true for VERBpses (p < 0.001, z ? 4.1).
In the interests of testing consistency, Table 8 shows the results of assessing the
adjective and verb features generated from opinion piece data (ADJpses and VERBpses
Table 8
Average frequencies and increases in precision in subjective-element data
of the sets tested in Table 7. The baselines are the precisions of
adjectives/verbs that appear in subjective elements in the
subjective-element data.
Adj baseline Verb baseline ADJpses VERBpses
freq prec freq prec freq +prec freq +prec
WSJ-SE-D 1,632 .13 2,980 .15 136 +.16 151 +.10
WSJ-SE-M 1,632 .19 2,980 .12 136 +.24 151 +.13
NG-SE 1,104 .37 2,629 .15 185 +.25 275 +.08
294
Computational Linguistics Volume 30, Number 3
Table 9
Frequencies and increases in precision for all features. For each data set, baseline
frequency is the total number of words, and baseline precision is the proportion of
words in opinion pieces. freq: total frequency; +prec: increase in precision over baseline.
W9-04 W9-10 W9-22 W9-33
freq +prec freq +prec freq +prec freq +prec
Unique words 4794 +.15 4763 +.16 4274 +.11 4567 +.11
Fixed-2-grams 1840 +.07 1972 +.07 1933 +.04 1839 +.05
ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17
Fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05
ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15
Fixed-4-grams 18 +.15 17 +.06 12 +.29 14 ?.07
ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25
Adjectives 1872 +.17 249 +.22 859 +.15 1576 +.12
Verbs 1777 +.15 224 +.10 535 +.11 1490 +.11
Baseline 156421 .19 156334 .18 155135 .13 153634 .14
in Table 7) on the subjective-element data. The left side of the table gives baseline
figures for each set of subjective-element annotations. The right side of the table gives
the average frequencies and increases in precision over baseline for the ADJpses and
VERBpses sets on the subjective-element data. The baseline figures in the table are the
frequencies and precisions of the sets of adjectives and verbs that appear at least once
in a subjective element. Since these sets include words that appear just once in the
corpus (and thus have 100% precision), the baseline precision is a challenging one.
Testing the VERBpses and ADJpses on the subjective-element data reveals some inter-
esting consistencies for these subjectivity clues. The precision increases of the VERBpses
on the subjective-element data are comparable to their increases on the opinion piece
data. Similarly, the precision increases of the ADJpses on the subjective-element data
are as good as or better than the performance of this set of PSEs on the opinion piece
data. Finally, the precisions increases for the ADJpses are higher than for the VERBpses
on all data sets. This is again consistent with the higher performance of the ADJpses
sets in the opinion piece data sets.
4. Features Used in Concert
4.1 Introduction
In this section, we examine the various types of clues used together. In preparation for
this work, all instances in OP1 and OP2 of all of the PSEs identified as described in
Section 3 have been automatically identified. All training to define the PSE instances
in OP1 was performed on data separate from OP1, and all training to define the PSE
instances in OP2 was performed on data separate from OP2.
4.2 Consistency in Precision among Data Sets
Table 9 summarizes the results from previous sections in which the opinion piece data
are used for testing. The performance of the various features is consistently good or
bad on the same data sets: the performance is better for all features on W9-10 and
W9-04 than on W9-22 and W9-33 (except for the ugen-4-grams, which occur with very
low frequency, and the verbs, which have low frequency in W9-10). This is so despite
the fact that the features were generated using different procedures and data: The
295
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
0. PSEs = all adjs, verbs, modals, nouns, and adverbs that appear at least
once in an SE (except not, will, be, have).
1. PSEinsts = the set of all instances of PSEs
2. HiDensity = {}
3. For P in PSEinsts:
4. leftWin(P) = the W words before P
5. rightWin(P) = the W words after P
6. density(P) = number of SEs whose first or last
word is in leftWin(P) or rightWin(P)
7. if density(P) ? T:
HiDensity = HiDensity ? {P}
8. prec(PSEinsts) =
number of PSEinsts in subject elements
|PSEinsts|
9. prec(HiDensity) =
number of HiDensity in subject elements
|HiDensity|
Figure 3
Algorithm for calculating density in subjective-element data.
adjectives and verbs were generated from WSJ document-level opinion piece classifi-
cations; the n-gram features were generated from newsgroup and WSJ expression-level
subjective-element classifications; and the unique unigram feature requires no training.
This consistency in performance suggests that the results are not brittle.
4.3 Choosing Density Parameters from Subjective-Element Data
In Wiebe (1994), whether a PSE is interpreted to be subjective depends, in part, on
how subjective the surrounding context is. We explore this idea in the current work,
assessing whether PSEs are more likely to be subjective if they are surrounded by sub-
jective elements. In particular, we experiment with a density feature to decide whether
or not a PSE instance is subjective: If a sufficient number of subjective elements are
nearby, then the PSE instance is considered to be subjective; otherwise, it is discarded.
The density parameters are a window size W and a frequency threshold T.
In this section, we explore the density of manually annotated PSEs in subjective-
element data and choose density parameters to use in Section 4.4, in which we apply
them to automatically identified PSEs in opinion piece data.
The process for calculating density in the subjective-element data is given in Fig-
ure 3. The PSEs are defined to be all adjectives, verbs, modals, nouns, and adverbs that
appear at least once in a subjective element, with the exception of some stop words
(line 0 of Figure 3). Note that these PSEs depend only on the subjective-element man-
ual annotations, not on the automatically identified features used elsewhere in the
article or on the document-level opinion piece classes. PSEinsts is the set of PSE
instances to be disambiguated (line 1). HiDensity (initialized on line 2) will be the
subset of PSEinsts that are retained. In the loop, the density of each PSE instance
P is calculated. This is the number of subjective elements that begin or end in the
W words preceding or following P (line 6). P is retained if its density is at least T
(line 7).
Lines 8?9 of the algorithm assess the precision of the original (PSEinsts) and new
(HiDensity) sets of PSE instances. If prec(HiDensity) is greater than prec(PSEinsts), then
296
Computational Linguistics Volume 30, Number 3
Table 10
Most frequent entry in the top three precision intervals for each
subjective-element data set.
WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SE
Baseline freq 1,566 1,245 1,167 1,108 3,303
Baseline prec .49 .47 .41 .36 .51
Range .87?.92 .95?1.0 .95?1.0 .95?1.0 .95?1.0
T, W 10, 20 12, 50 20, 50 14, 100 10, 10
freq 76 12 1 1 3
prec .89 1.0 1.0 1.0 1.0
Range .82?.87 .90?.95 .73?.78 .51?.56 .67?.72
T, W 6, 10 12, 60 46, 190 22, 370 26, 90
freq 63 22 53 221 664
prec .84 .91 .78 .51 .67
Range .77?.82 .84?.89 .66?.71 .46?.51 .63?.67
T, W 12, 40 12, 80 18, 60 16, 310 8, 30
freq 292 42 53 358 1504
prec .78 .88 .68 .47 .63
there is evidence that the number of subjective elements near a PSE instance is related
to its subjectivity in context.
To create more data points for this analysis, WSJ-SE was split into two (WSJ-SE1
and WSJ-SE2) and annotations of the two judges are considered separately. WSJ-SE2-D,
for example, refers to D?s annotations of WSJ-SE2. The process in Figure 3 was repeated
for different parameter settings (T in [1, 2, 4, . . . , 48] and W in [1, 10, 20, . . . , 490]) on each
of the SE data sets. To find good parameter settings, the results for each data set were
sorted into five-point precision intervals and then sorted by frequency within each
interval. Information for the top three precision intervals for each data set are shown
in Table 10, specifically, the parameter values (i.e., T and W) and the frequency and
precision of the most frequent result in each interval. The intervals are in the rows
labeled Range. For example, the top three precision intervals for WSJ-SE1-M, 0.87-0.92,
0.82-0.87, and 0.77-0.82 (no parameter values yield higher precision than 0.92). The
top of Table 10 gives baseline frequencies and precisions, which are |PSEinsts| and
prec(PSEinsts), respectively, in line 8 of Figure 3.
The parameter values exhibit a range of frequencies and precisions, with the ex-
pected trade-off between precision and frequency. We choose the following parameters
to test in Section 4.4: For each data set, for each precision interval whose lower bound
is at least 10 percentage points higher than the baseline for that data set, the top
two (T, W) pairs yielding the highest frequencies in that interval are chosen. Among
the five data sets, a total of 45 parameter pairs were so selected. This exercise was
completed once, without experimenting with different parameter settings.
4.4 Density for Disambiguation
In this section, density is exploited to find subjective instances of automatically iden-
tified PSEs. The process is shown in Figure 4. There are only two differences between
the algorithms in Figures 3 and 4. First, in Figure 3, density is defined in terms of
the number of subjective elements nearby. However, subjective-element annotations
are not available in test data. Thus in Figure 4, density is defined in terms of the
297
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
0. PSEinsts = the set of instances in the test
data of all PSEs described in Section 3
1. HiDensity = {}
2. For P in PSEinsts:
3. leftWin(P) = the W words before P
4. rightWin(P) = the W words after P
5. density(P) = number of PSEinsts whose first or last
word is in leftWin(P) or rightWin(P)
6. if density(P) ? T:
HiDensity = HiDensity ? {P}
7. prec(PSEinsts) = # of PSEinsts in OPs|PSEinsts|
8. prec(HiDensity) = # of HiDensity in OPs|HiDensity|
Figure 4
Algorithm for calculating density in opinion piece (OP) data
number of other PSE instances nearby, where PSEinsts consists of all instances of the
automatically identified PSEs described in Section 3, for which results are given in
Table 9.
Second, in Figure 4, we assess precision with respect to the document-level classes
(lines 7?8). The test data are OP1.
An interesting question arose when we were defining the PSE instances: What
should be done with words that are identified to be PSEs (or parts of PSEs) according
to multiple criteria? For example, sunny, radiant, and exhilarating are all unique in
corpus OP1, and are all members of the adjective PSE feature defined for testing
on OP1. Collocations add additional complexity. For example, consider the sequence
and splendidly, which appears in the test data. The sequence and splendidly matches
the ugen-2-gram (and-conj U-adj), and the word splendidly is unique. In addition, a
sequence may match more than one n-gram feature. For example, is it that matches
three fixed-n-gram features: is it, is it that, and it that.
In the current experiments, the more PSEs a word matches, the more weight it
is given. The hypothesis behind this treatment is that additional matches represent
additional evidence that a PSE instance is subjective. This hypothesis is realized as
follows: Each match of each member of each type of PSE is considered to be a PSE
instance. Thus, among them, there are 11 members in PSEinsts for the five phrases
sunny, radiant, exhilarating, and splendidly, and is it that, one for each of the matches
mentioned above.
The process in Figure 4 was conducted with the 45 parameter pair values (T and
W) chosen from the subjective-element data as described in Section 4.3. Table 11 shows
results for a subset of the 45 parameters, namely, the most frequent parameter pair
chosen from the top three precision intervals for each training set. The bottom of the
table gives a baseline frequency and a baseline precision in OP1, defined as |PSEinsts|
and prec(PSEinsts), respectively, in line 7 of Figure 4.
The density features result in substantial increases in precision. Of the 45 parameter
pairs, the minimum percentage increase over baseline is 22%. Fully 24% of the 45
parameter pairs yield increases of 200% or more; 38% yield increases between 100%
298
Computational Linguistics Volume 30, Number 3
Table 11
Results for high-density PSEs in test data OP1 using parameters chosen
from subjective-element data.
WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SE
T, W 10, 20 12, 50 20, 50 14, 100 10, 10
freq 237 3,176 170 10,510 8
prec .87 .72 .97 .57 1.0
T, W 6, 10 12, 60 46, 190 22, 370 26, 90
freq 459 5,289 1,323 21,916 787
prec .68 .68 .95 .37 .92
T, W 12, 40 12, 80 18, 60 16, 310 8, 30
freq 1,398 9,662 906 24,454 3,239
prec .79 .58 .87 .34 .67
PSE baseline: freq = 30,938, prec = .28
and 199%, and 38% yield increases between 22% and 99%. In addition, the increases
are significant. Using the set of high-density PSEs defined by the parameter pair with
the least increase over baseline, we tested the difference in the proportion of PSEs
in opinion pieces that are high-density and the proportion of PSEs in nonopinion
pieces that are high-density. The difference between these two proportions is highly
significant (z = 46.2, p < 0.0001).
Notice that, except for one blip (T, W = 6, 10 under WSJ-SE-M), the precisions
decrease and the frequencies increase as we go down each column in Table 11. The
same pattern can be observed with all 45 parameter pairs (results not included here
because of space considerations). But the parameter pairs are ordered in Table 11
based on performance in the manually annotated subjective-element data, not based
on performance in the test data. For example, the entry in the first row, first column
(T, W = 10, 20) is the parameter pair giving the highest frequency in the top precision
interval of WSJ-SE-M (frequency and precision in WSJ-SE-M, using the process of
Figure 3). Thus, the relative precisions and frequencies of the parameter pairs are
carried over from the training to the test data. This is quite a strong result, given that
the PSEs in the training data are from manual annotations, while the PSEs in the test
data are our automatically identified features.
4.5 High-Density Sentence Annotations
To assess the subjectivity of sentences with high-density PSEs, we extracted the 133
sentences in corpus OP2 that contain at least one high-density PSE and manually
annotated them. We refer to these sentences as the system-identified sentences.
We chose the density-parameter pair (T, W = 12, 30), based on its precision and
frequency in OP1. This parameter setting yields results that have relatively high pre-
cision and low frequency. We chose a low-frequency setting to make the annotation
study feasible.
The extracted sentences were independently annotated by two judges. One is a
coauthor of this article (judge 1), and the other has performed subjectivity annota-
tion before, but is not otherwise involved in this research (judge 2). Sentences were
annotated according to the coding instructions of Wiebe, Bruce, and O?Hara (1999)
which, recall, are to classify a sentence as subjective if there is a significant expression
of subjectivity of either the writer or someone mentioned in the text, in the sentence.
299
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 12
Examples of system-identified sentences.
(1) The outburst of shooting came nearly two weeks after clashes between Moslem worshippers and oo
Somali soldiers.
(2.a) But now the refugees are streaming across the border and alarming the world. ss
(2.b) In the middle of the crisis, Erich Honecker was hospitalized with a gall stone operation. oo
(2.c) It is becoming more and more obvious that his gallstone-age communism is dying with him: . . . ss
(3.a) Not brilliantly, because, after all, this was a performer who was collecting paychecks from lounges ss
at Hiltons and Holiday Inns, but creditably and with the air of someone for whom
?Ten Cents a Dance? was more than a bit autobiographical.
(3.b) ?It was an exercise of blending Michelle?s singing with Susie?s singing,? explained Ms. Stevens. oo
(4) Enlisted men and lower-grade officers were meat thrown into a grinder. ss
(5) ?If you believe in God and you believe in miracles, there?s nothing particularly crazy about that.? ss
(6) He was much too eager to create ?something very weird and dynamic,? ss
?catastrophic and jolly? like ?this great and coily thing? ?Lolita.?
(7) The Bush approach of mixing confrontation with conciliation strikes some people as sensible, perhaps ss
even inevitable, because Mr. Bush faces a Congress firmly in the hands of the opposition.
(8) Still, despite their efforts to convince the world that we are indeed alone, the visitors do seem to keep ss
coming and, like the recent sightings, there?s often a detail or two that suggests they may
actually be a little on the dumb side.
(9) As for the women, they?re pathetic. ss
(10) At this point, the truce between feminism and sensationalism gets might uneasy. ss
(11) MMPI?s publishers say the test shouldn?t be used alone to diagnose ss
psychological problems or in hiring; it should be given in conjunction with other tests.
(12) While recognizing that professional environmentalists may feel threatened, ss
I intend to urge that UV-B be monitored whenever I can.
Table 13
Sentence annotation contingency table; judge 1 counts are in rows and
judge 2 counts are in columns.
Subjective Objective Unsure
Subjective 98 2 3
Objective 2 14 0
Unsure 2 11 1
In addition to the subjective and objective classes, a judge can tag a sentence as unsure
if he or she is unsure of his or her rating or considers the sentence to be borderline.
An equal number (133) of other sentences were randomly selected from the corpus
to serve as controls. The 133 system-identified sentences and the 133 control sentences
were randomly mixed together. The judges were asked to annotate all 266 sentences,
not knowing which were system-identified and which were control. Each sentence
was presented with the sentence that precedes it and the sentence that follows it in
the corpus, to provide some context for interpretation.
Table 12 shows examples of the system-identified sentences. Sentences classified
by both judges as objective are marked oo and those classified by both judges as
subjective are marked ss.
300
Computational Linguistics Volume 30, Number 3
Table 14
Examples of subjective sentences adjacent to system-identified sentences.
Bathed in cold sweat, I watched these Dantesque scenes, holding tightly the
damp hand of Edek or Waldeck who, like me, were convinced that there was no God.
?The Japanese are amazed that a company like this exists in Japan,? says Kimindo
Kusaka, head of the Softnomics Center, a Japanese management-research organization.
And even if drugs were legal, what evidence do you have that the habitual drug user
wouldn?t continue to rob and steal to get money for clothes, food or shelter?
The moral cost of legalizing drugs is great, but it is a cost that apparently lies
outside the narrow scope of libertarian policy prescriptions.
I doubt that one exists.
They were upset at his committee?s attempt to pacify the program critics by
cutting the surtax paid by the more affluent elderly and making up the loss by
shifting more of the burden to the elderly poor and by delaying some benefits by a year.
Judge 1 classified 103 of the system-identified sentences as subjective, 16 as ob-
jective, and 14 as unsure. Judge 2 classified 102 of the system-identified sentences as
subjective, 27 as objective; and 4 as unsure. The contingency table is given in Table 13.4
The kappa value using all three classes is 0.60, reflecting the highly skewed distri-
bution in favor of subjective sentences, and the disagreement on the lower-frequency
classes (unsure and objective). Consistent with the findings in Wiebe, Bruce, and
O?Hara (1999), the kappa value for agreement on the sentences for which neither
judge is unsure is very high: 0.86.
A different breakdown of the sentences is illuminating. For 98 of the sentences (call
them SS), judges 1 and 2 tag the sentence as subjective. Among the other sentences, 20
appear in a block of contiguous system-identified sentences that includes a member of
SS. For example, in Table 12, (2.a) and (2.c) are in SS and (2.b) is in the same block of
subjective sentences as they are. Similarly, (3.a) is in SS and (3.b) is in the same block.
Among the remaining 15 sentences, 6 are adjacent to subjective sentences that
were not identified by our system (so were not annotated by the judges). All of those
sentences contain significant expressions of subjectivity of the writer or someone men-
tioned in the text, the criterion used in this work for classifying a sentence as subjective.
Samples are shown in Table 14.
Thus, 93% of the sentences identified by the system are subjective or are near
subjective sentences. All the sentences, together with their tags and the sentences
adjacent to them, are available on the Web at www.cs.pitt.edu/? wiebe.
4.6 Using Features for Opinion Piece Recognition
In this section, we assess the usefulness of the PSEs identified in Section 3 and listed
in Table 9 by using them to perform document-level classification of opinion pieces.
Opinion-piece classification is a difficult task for two reasons. First, as discussed in Sec-
tion 2.1, both opinionated and factual documents tend to be composed of a mixture of
subjective and objective language. Second, the natural distribution of documents in our
data is heavily skewed toward nonopinion pieces. Despite these hurdles, using only
4 In contrast, Judge 1 classified only 53 (45%) of the control sentences as subjective, and Judge 2
classified only 47 (36%) of them as subjective.
301
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
our PSEs, we achieve positive results in opinion-piece classification using the basic k-
nearest-neighbor (KNN) algorithm with leave-one-out cross-validation (Mitchell 1997).
Given a document, the basic KNN algorithm classifies the document according to
the majority classification of the document?s k closest neighbors. For our purposes, each
document is characterized by one feature, the count of all PSE instances (regardless
of type) in the document, normalized by document length in words. The distance
between two documents is simply the absolute value of the difference between the
normalized PSE counts for the two documents.
With leave-one-out cross-validation, the set of n documents to be classified is
divided into a training set of size n?1 and a validation set of size 1. The one document
in the validation set is then classified according to the majority classification of its k
closest-neighbor documents in the training set. This process is repeated until every
document is classified.
Which value to use for k is chosen during a preprocessing phase. During the pre-
processing phase, we run the KNN algorithm with leave-one-out cross-validation on
a separate training set, for odd values of k from 1 to 15. The value of k that results in
the best classification during the preprocessing phase is the one used for later KNN
classification.
For the classification experiment, the data set OP1 was used in the preprocess-
ing phase to select the value of k, and then classification was performed on the 1,222
documents in OP2. During training on OP1, k equal to 15 resulted in the best classifi-
cation. On the test set, OP2, we achieved a classification accuracy of 0.939; the baseline
accuracy for choosing the most frequent class (nonopinion pieces) was 0.915. Our clas-
sification accuracy represents a 28% reduction in error and is significantly better than
baseline according to McNemar?s test (Everitt 1997).
The positive results from the opinion piece classification show the usefulness of
the various PSE features when used together.
5. Relation to Other Work
There has been much work in other fields, including linguistics, literary theory, psy-
chology, philosophy, and content analysis, involving subjective language. As men-
tioned in Section 2, the conceptualization underlying our manual annotations is based
on work in literary theory and linguistics, most directly Dolez?el (1973), Uspensky
(1973), Kuroda (1973, 1976), Chatman (1978), Cohn (1978), Fodor (1979), and Banfield
(1982). We also mentioned existing knowledge resources such as affective lexicons
(General-Inquirer 2000; Heise 2000) and annotations in more general-purpose lexicons
(e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]).
Such knowledge may be used in future work to complement the work presented in
this article, for example, to seed the distributional-similarity process described in Sec-
tion 3.4.
There is also work in fields such as content analysis and psychology on statisti-
cally characterizing texts in terms of word lists manually developed for distinctions
related to subjectivity. For example, Hart (1984) performs counts on a manually de-
veloped list of words and rhetorical devices (e.g., ?sacred? terms such as freedom)
in political speeches to explore potential reasons for public reactions. Anderson and
McMaster (1998) use fixed sets of high-frequency words to assign connotative scores
to documents and sections of documents along dimensions such as how pleasant,
acrimonious, pious, or confident, the text is.
What distinguishes our work from work on subjectivity in other fields is that
we focus on (1) automatically learning knowledge from corpora, (2) automatically
302
Computational Linguistics Volume 30, Number 3
performing contextual disambiguation, and (3) using knowledge of subjectivity in
NLP applications. This article expands and integrates the work reported in Wiebe and
Wilson (2002), Wiebe, Wilson, and Bell (2001), Wiebe et al (2001) and Wiebe (2000).
Previous work in NLP on the same or related tasks includes sentence-level and
document-level subjectivity classifications. At the sentence level, Wiebe, Bruce, and
O?Hara (1999) developed a machine learning system to classify sentences as subjec-
tive or objective. The accuracy of the system was more than 20 percentage points
higher than a baseline accuracy. Five part-of-speech features, two lexical features, and
a paragraph feature were used. These results suggested to us that there are clues to
subjectivity that might be learned automatically from text and motivated the work
reported in the current article. The system was tested in 10-fold cross validation ex-
periments using corpus WSJ-SE, a small corpus of only 1,001 sentences. As discussed
in Section 1, a main goal of our current work is to exploit existing document-level
annotations, because they enable us to use much larger data sets, they were created
outside our research group, and they allow us to assess consistency of performance
by cross-validating between our manual annotations and the existing document-level
annotations. Because the document-level data are not annotated at the sentence level,
sentence-level classification is not highlighted in this article. The new sentence annota-
tion study to evaluate sentences with high-density features (Section 4.5) uses different
data from WSJ-SE, because some of the features (n-grams and density parameters)
were identified using WSJ-SE as training data.
Other previous work in NLP has addressed related document-level classifications.
Spertus (1997) developed a system for recognizing inflammatory messages. As men-
tioned earlier in the article, inflammatory language is a type of subjective language,
so the task she addresses is closely related to ours. She uses machine learning to
select among manually developed features. In contrast, the focus in our work is on
automatically identifying features from the data.
A number of projects investigating genre detection include editorials as one of the
targeted genres. For example, in Karlgren and Cutting (1994), editorials are one of fif-
teen categories, and in Kessler, Nunberg, and Schu?tze (1997), editorials are one of six.
Given the goal of these works to perform genre detection in general, they use low-level
features that are not specific to editorials. Neither shows significant improvements for
editorial recognition. Argamon, Koppel, and Avneri (1998) address a slightly different
task, though it does involve editorials. Their goal is to distinguish not only, for ex-
ample, news from editorials, but also these categories in different publications. Their
best results are distinguishing among the news categories of different publications;
their lowest results involve editorials. Because we focus specifically on distinguishing
opinion pieces from nonopinion pieces, our results are better than theirs for those
categories. In addition, in contrast to the above studies, the focus of our work is on
learning features of subjectivity. We perform opinion piece recognition in order to
assess the usefulness of the various features when used together.
Other previous NLP research has used features similar to ours for other NLP tasks.
Low-frequency words have been used as features in information extraction (Weeber,
Vos, and Baayen 2000) and text categorization (Copeck et al 2000). A number of
researchers have worked on mining collocations from text to extend lexicographic
resources for machine translation and word sense disambiguation (e.g., Smajda 1993;
Lin 1999; Biber 1993).
In Samuel, Carberry, and Vijay-Shanker?s (1998) work on identifying collocations
for dialog-act recognition, a filter similar to ours was used to eliminate redundant
n-gram features: n-grams were eliminated if they contained substrings with the same
entropy score as or a better entropy score than the n-gram.
303
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
While it is common in studies of collocations to omit low-frequency words and
expressions from analysis, because they give rise to invalid or unrealistic statistical
measures (Church and Hanks, 1990), we are able to identify higher-precision colloca-
tions by including placeholders for unique words (i.e., the ugen-n-grams). We are not
aware of other work that uses such collocations as we do.
Features identified using distributional similarity have previously been used for
syntactic and semantic disambiguation (Hindle 1990; Dagan, Pereira, and Lee 1994)
and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999).
We are not aware of other work identifying and using density parameters as
described in this article.
Since our experiments, other related work in NLP has been performed. Some of
this work addresses related but different classification tasks. Three studies classify
reviews as positive or negative (Turney 2002; Pang, Lee, and Vaithyanathan 2002;
Dave, Lawrence, Pennock 2003). The input is assumed to be a review, so this task
does not include finding subjective documents in the first place. The first study listed
above (Turney 2002) uses a variation of the semantic similarity procedure presented
in Wiebe (2000) (Section 3.4). The third (Dave, Lawrence, and Pennock 2003) uses n-
gram features identified with a variation of the procedure presented in Wiebe, Wilson,
and Bell (2001) (Section 3.3). Tong (2001) addresses finding sentiment timelines, that
is, tracking sentiments over time in multiple documents. For clues of subjectivity, he
uses manually developed lexical rules, rather than automatically learning them from
corpora. Similarly, Gordon et al (2003) use manually developed grammars to detect
some types of subjective language. Agrawal et al (2003) partition newsgroup authors
into camps based on quotation links. They do not attempt to recognize subjective
language.
The most closely related new work is Riloff, Wiebe, and Wilson (2003), Riloff
and Wiebe (2003) and Yu and Hatzivassiloglou (2003). The first two focus on finding
additional types of subjective clues (nouns and extraction patterns identified using
extraction pattern bootstrapping). Yu and Hatzivassiloglou (2003) perform opinion text
classification. They also use existing WSJ document classes for training and testing,
but they do not include the entire corpus in their experiments, as we do. Their opinion
piece class consists only of editorials and letters to the editor, and their nonopinion
class consists only of business and news. They report an average F-measure of 96.5%.
Our result of 94% accuracy on document level classification is almost comparable.
They also perform sentence-level classification.
We anticipate that knowledge of subjective language may be usefully exploited in
a number of NLP application areas and hope that the work presented in this article will
encourage others to experiment with subjective language in their applications. More
generally, there are many types of artificial intelligence systems for which state-of-
affairs types such as beliefs and desires are central, including systems that perform plan
recognition for understanding narratives (Dyer 1982; Lehnert et al 1983), for argument
understanding (Alvarado, Dyer, and Flowers 1986), for understanding stories from
different perspectives (Carbonell 1979), and for generating language under different
pragmatic constraints (Hovy 1987). Knowledge of linguistic subjectivity could enhance
the abilities of such systems to recognize and generate expressions referring to such
states of affairs in natural text.
6. Conclusions
Knowledge of subjective language promises to be beneficial for many NLP applica-
tions including information extraction, question answering, text categorization, and
304
Computational Linguistics Volume 30, Number 3
summarization. This article has presented the results of an empirical study in ac-
quiring knowledge of subjective language from corpora in which a number of fea-
ture types were learned and evaluated on different types of data with positive re-
sults.
We showed that unique words are subjective more often than expected and that
unique words are valuable clues to subjectivity. We also presented a procedure for au-
tomatically identifying potentially subjective collocations, including fixed collocations
and collocations with placeholders for unique words. In addition, we used the results
of a method for clustering words according to distributional similarity (Lin 1998) to
identify adjectival and verbal clues of subjectivity.
Table 9 summarizes the results of testing all of the above types of PSEs. All show
increased precision in the evaluations. Together, they show consistency in performance.
In almost all cases they perform better or worse on the same data sets, despite the
fact that different kinds of data and procedures are used to learn them. In addition,
PSEs learned using expression-level subjective-element data have precisions higher
than baseline on document-level opinion piece data, and vice versa.
Having a large stable of PSEs, it was important to disambiguate whether or not
PSE instances are subjective in the contexts in which they appear. We discovered that
the density of other potentially subjective expressions in the surrounding context is
important. If a clue is surrounded by a sufficient number of other clues, then it is
more likely to be subjective than if there were not. Parameter values were selected
using training data manually annotated at the expression level for subjective elements
and then tested on data annotated at the document level for opinion pieces. All of
the selected parameters led to increases in precision on the test data, and most lead to
increases over 100%. Once again we found consistency between expression-level and
document-level annotations. PSE sets defined by density have high precision in both
the subjective-element data and the opinion piece data. The large differences between
training and testing suggest that our results are not brittle.
Using a density feature selected from a training set, sentences containing high-
density PSEs were extracted from a separate test set, and manually annotated by two
judges. Fully 93% of the sentences extracted were found to be subjective or to be near
subjective sentences. Admittedly, the chosen density feature is a high-precision, low-
frequency one. But since the process is fully automatic, the feature could be applied to
more unannotated text to identify regions containing subjective sentences. In addition,
because the precision and frequency of the density features are stable across data sets,
lower-precision but higher-frequency options are available.
Finally, the value of the various types of PSEs was demonstrated with the task
of opinion piece classification. Using the k-nearest-neighbor classification algorithm
with leave-one-out cross-validation, a classification accuracy of 94% was achieved on
a large test set, with a reduction in error of 28% from the baseline.
Future work is required to determine how to exploit density features to improve
the performance of text categorization algorithms. Another area of future work is
searching for clues to objectivity, such as the politeness features used by Spertus (1997).
Still another is identifying the type of a subjective expression (e.g., positive or neg-
ative evaluative), extending work such as Hatzivassiloglou and McKeown (1997) on
classifying lexemes to the classification of instances in context (compare, e.g., ?great!?
and ?oh great.?)
In addition, it would be illuminating to apply our system to data annotated with
discourse trees (Carlson, Marcu, and Okurowski 2001). We hypothesize that most ob-
jective sentences identified by our system are dominated in the discourse by subjective
sentences and that we are moving toward identifying subjective discourse segments.
305
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Acknowledgments
We thank the anonymous reviewers for
their helpful and constructive comments.
This research was supported in part by the
Office of Naval Research under grants
N00014-95-1-0776 and N00014-01-1-0381.
References
Agrawal, Rakesh, Sridhar Rajagopalan,
Ramakrishnan Srikant, and Yirong Xu.
2003. Mining newsgroups using networks
arising from social behavior. In
Proceedings of the 12th International World
Wide Web Conference (WWW2003),
Budapest, May 20-24.
Alvarado, Sergio J., Michael G. Dyer, and
Margot Flowers. 1986. Editorial
comprehension in oped through
argument units. In Proceedings of the Fifth
National Conference on Artificial Intelligence
(AAAI-86), Philadelphia, August 11?15,
pages 250?256.
Anderson, Clifford W. and George C.
McMaster. 1989. Quantification of
rewriting by the Brothers Grimm: A
comparison of successive versions of
three tales. Computers and the Humanities,
23(4?5):341?346.
Aone, Chinatsu, Mila Ramos-Santacruz, and
William J. Niehaus. 2000. Assentor: An
NLP-based solution to e-mail monitoring.
In Proceedings of the 12th Innovative
Applications of Artificial Intelligence
Conference (IAAI-2000), Austin, TX,
August 1?3, pages 945?950.
Argamon, Shlomo, Moshe Koppel, and
Galit Avneri. 1998. Routing documents
according to style. In Proceedings of the
First International Workshop on Innovative
Internet Information Systems (IIIS-98), Pisa,
Italy, June 8?9.
Banfield, Ann. 1982. Unspeakable Sentences.
Routledge and Kegan Paul, Boston.
Barzilay, Regina, Michael Collins, Julia
Hirschberg, and Steve Whittaker. 2000.
The rules behind roles: Identifying
speaker role in radio broadcasts. In
Proceedings of the 17th National Conference on
Artificial Intelligence (AAAI-2000), Austin,
TX, July 30?August 3, pages 679?684.
Biber, Douglas. 1993. Co-occurrrence
patterns among collocations: A tool for
corpus-based lexical knowledge
acquisition. Computational Linguistics,
19(3):531?538.
Brill, Eric. 1992. A simple rule-based part of
speech tagger. In Proceedings of the 3rd
Conference on Applied Natural Language
Processing (ANLP-92), Trenton, Italy, April
1?3 pages 152?155.
Bruce, Rebecca and Janyce Wiebe. 1999.
Recognizing subjectivity: A case study of
manual tagging. Natural Language
Engineering, 5(2):187?205.
Carbonell, Jaime G. 1979. Subjective
Understanding: Computer Models of Belief
Systems. Ph.D. thesis, and Technical
Report no. 150, Department of Computer
Science, Yale University, New Haven, CT.
Carlson, Lynn, Daniel Marcu, and
Mary Ellen Okurowski. 2001. Building a
discourse-tagged corpus in the
framework of rhetorical structure theory.
In Proceedings of the Second SIG dial
Workshop on Discourse and Dialogue
(SIGdial-2001), Aalborg, Denmark,
September 1?2, pages 30?39.
Chatman, Seymour. 1978. Story and
Discourse: Narrative Structure in Fiction and
Film. Cornell University Press, Ithaca, NY.
Church, Kenneth W. and Patrick Hanks.
1990. Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16:22?29.
Cohn, Dorrit. 1978. Transparent Minds:
Narrative Modes for Representing
Consciousness in Fiction. Princeton
University Press, Princeton, NJ.
Copeck, Terry, Kim Barker, Sylvain Delisle,
and Stan Szpakowicz. 2000. Automating
the measurement of linguistic features to
help classify texts as technical. In
Proceedings of the Seventh Conference on
Automatic NLP (TALN-2000), Lausanne,
Switzerland, October 16?18, pages
101?110.
Dagan, Ido, Fernando Pereira, and Lillian
Lee. 1994. Similarity-based estimation of
word cooccurrence probabilities. In
Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics
(ACL-94), Las Cruces, NM, June 27?30,
pages 272?278.
Dave, Kushal, Steve Lawrence, and
David M. Pennock. 2003. Mining the
peanut gallery: Opinion extraction and
semantic classification of produce
reviews. In Proceedings of the 12th
International World Wide Web Conference
(WWW2003), Budapest, May 20?24.
Dolez?el, Lubomir. 1973. Narrative Modes in
Czech Literature. University of Toronto
Press, Toronto, Ontario, Canada.
Dyer, Michael G. 1982. Affect processing for
narratives. In Proceedings of the Second
National Conference on Artificial Intelligence
(AAAI-82), Pittsburgh, August 18?20,
pages 265?268.
Everitt, Brian S. 1977. The Analysis of
Contingency Tables. Chapman and Hall,
London.
306
Computational Linguistics Volume 30, Number 3
Fludernik, Monika. 1993. The Fictions of
Language and the Languages of Fiction.
Routledge, London.
Fodor, Janet Dean. 1979. The Linguistic
Description of Opaque Contexts, volume 13
of Outstanding Dissertations in Linguistics.
Garland, New York and London.
General-Inquirer, The. 2000. Available at
http://www.wjh.harvard.edu/?
inquirer/spreadsheet guide.htm.
Gordon, Andrew, Abe Kazemzadeh, Anish
Nair, and Milena Petrova. 2003.
Recognizing expressions of commonsense
psychology in English text. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL-03),
Sapporo, Japan, July 7?12, pages 208?215.
Hart, Roderick P. 1984. Systematic analysis
of political discourse: The development of
diction. In K. Sanders et al, editors,
Political Communication Yearbook: 1984.
Southern Illinois University Press,
Carbondale, pages 97?134.
Hatzivassiloglou, Vasileios and Kathy
McKeown. 1997. Predicting the semantic
orientation of adjectives. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics (ACL-97),
Madrid, July 12, pages 174?181.
Heise, David. 2000. Affect control theory.
Available at
http://www.indiana.edu/socpsy/ACT/
index.htm.
Hindle, Don. 1990. Noun classification from
predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics
(ACL-90), Pittsburgh, June 6?9, pages
268?275.
Hovy, Eduard. 1987. Generating Natural
Language under Pragmatic Constraints. Ph.D.
thesis, Yale University, New Haven, CT.
Karlgren, Jussi and Douglass Cutting. 1994.
Recognizing text genres with simple
metrics using discriminant analysis. In
Proceedings of the Fifteenth International
Conference on Computational Linguistics
(COLING-94), pages 1071?1075.
Karp, Daniel, Yves Schabes, Martin Zaidel,
and Dania Egedi. 1994. A freely available
wide coverage morphological analyzer for
English. In Proceedings of the 15th
International Conference on Computational
Linguistics (COLING-94), Nantes, France
pages 922?928.
Kaufer, David. 2000. Flaming: A white paper.
Available at www.eudora.com.
Kessler, Brett, Geoffrey Nunberg, and
Hinrich Schu?tze. 1997. Automatic
detection of text genre. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics (ACL-97),
Madrid, July 7?12, pages 32?38.
Kuroda, S.-Y. 1973. Where epistemology,
style and grammar meet: A case study
from the Japanese. In P. Kiparsky and
S. Anderson, editors, A Festschrift for
Morris Halle. Holt, Rinehart & Winston,
New York, pages 377?391.
Kuroda, S.-Y. 1976. Reflections on the
foundations of narrative theory?from a
linguistic point of view. In T. A. van Dijk,
editor, Pragmatics of Language and
Literature. North-Holland, Amsterdam,
pages 107?140.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-99), College Park, MD,
pages 25?32.
Lee, Lillian and Fernando Pereira. 1999.
Distributional similarity models:
Clustering vs. nearest neighbors. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL-99), College Park, MD,
pages 33?40.
Lehnert, Wendy G., Michael Dyer, Peter
Johnson, C. J. Yang, and Steve Harley.
1983. BORIS: An Experiment in In-Depth
Understanding of Narratives. Artificial
Intelligence, 20:15?62.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics (ACL-98),
Montreal, August 10?14, pages 768?773.
Lin, Dekang. 1999. Automatic identification
of non-compositional phrases. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL-99), College Park, MD, pages
317?324.
Litman, Diane J. and Rebecca J. Passonneau.
1995. Combining multiple knowledge
sources for discourse segmentation. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics
(ACL-95), Cambridge, MA, June 26?30,
pages 108?115.
Macleod, Catherine, Ralph Grishman, and
Adam Meyers. 1998. Complex syntax
reference manual. Technical report, New
York University.
Marcu, Daniel, Magdalena Romera, and
Estibaliz Amorrortu. 1999. Experiments in
constructing a corpus of discourse trees:
Problems, annotation choices, issues. In
Proceedings of the International Workshop on
Levels of Representation in Discourse
(LORID-99), Edinburgh, July 6?9 pages
71?78.
307
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Marcus, Mitch, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Mitchell, Tom. 1997. Machine Learning.
McGraw-Hill, Boston.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine
learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2002),
Philadelphia, July 6?7, pages 79?86.
Quirk, Randolph, Sidney Greenbaum,
Geoffry Leech, and Jan Svartvik. 1985. A
Comprehensive Grammar of the English
Language. Longman, New York.
Riloff, Ellen and Rosie Jones. 1999. Learning
dictionaries for information extraction by
multi-level Bootstrapping. In Proceedings
of the 16th National Conference on Artificial
Intelligence (AAAI-1999), Orlando, FL, July
18?22, pages 474?479.
Riloff, Ellen and Janyce Wiebe. 2003.
Learning extraction patterns for subjective
expressions. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), Sapporo, Japan,
July 11?12, pages 105?112.
Riloff, Ellen, Janyce Wiebe, and Theresa
Wilson. 2003. Learning subjective nouns
using extraction pattern bootstrapping. In
Proceedings of the Seventh Conference on
Natural Language Learning (CoNLL-2003),
Edmonton, Alberta, Canada, May 31?June
1, pages 25?32.
Sack, Warren. 1995. Representing and
recognizing point of view. In Proceedings
of the AAAI Fall Symposium on AI
Applications in Knowledge Navigation and
Retrieval, Cambridge, MA, page 152.
Samuel, Ken, Sandra Carberry, and
K. Vijay-Shanker. 1998. Dialogue act
tagging with transformation-based
learning. In Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics (ACL-98), Montreal, August
10?14, pages 1150?1156.
Smajda, Frank. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,
19:143?177.
Spertus, Ellen. 1997. Smokey: Automatic
recognition of hostile messages. In
Proceedings of the Ninth Annual Conference
on Innovative Applications of Artificial
Intelligence (IAAI-97), Providence, RI, July
27?31, pages 1058?1065.
Stein, Dieter and Susan Wright, editors.
1995. Subjectivity and Subjectivisation.
Cambridge University Press, Cambridge.
Terveen, Loren, Will Hill, Brian Amento,
David McDonald, and Josh Creter. 1997.
Building task-specific interfaces to high
volume conversational data. In
Proceedings of the Conference on Human
Factors in Computing Systems (CHI-97), Los
Angeles, April 18?23, pages 226?233.
Teufel, Simone and Marc Moens. 2000.
What?s yours and what?s mine:
Determining intellectual attribution in
scientific texts. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing and the Workshop on
Very Large Corpora (EMNLP/VLC-2000),
Hong Kong, October 7?8, pages 9?17.
Tong, Richard. 2001. An operational system
for detecting and tracking opinions in
on-line discussions. In Working Notes of the
SIGIR Workshop on Operational Text
Classification, New Orleans, September
9?13, pages 1?6.
Turney, Peter. 2002. Thumbs up or thumbs
down? Semantic orientation applied to
unsupervised classification of reviews. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics
(ACL-2000), Philadelphia, July 7?12, pages
417?424.
Uspensky, Boris. 1973. A Poetics of
Composition. University of California
Press, Berkeley, and Los Angeles.
van Dijk, Teun A. 1988. News as Discourse.
Erlbaum, Hillsdale, NJ.
Weeber, Marc, Rein Vos, and R. Harald
Baayen. 2000. Extracting the
lowest-frequency words: Pitfalls and
possibilities. Computational Linguistics,
26(3):301?317.
Wiebe, Janyce and Theresa Wilson. 2002.
Learning to disambiguate potentially
subjective expressions. In Proceedings of the
Sixth Conference on Natural Language
Learning (CoNLL-2002), Taipei, Taiwan,
pages 112?118.
Wiebe, Janyce. 1990. Recognizing Subjective
Sentences: A Computational Investigation of
Narrative Text. Ph.D. thesis, State
University of New York at Buffalo.
Wiebe, Janyce. 1994. Tracking point of view
in narrative. Computational Linguistics,
20(2):233?287.
Wiebe, Janyce. 2000. Learning subjective
adjectives from corpora. In Proceedings of
the 17th National Conference on Artificial
Intelligence (AAAI-2000), Austin, TX, July
30?August 3, pages 735?740.
Wiebe, Janyce, Eric Breck, Chris Buckley,
Claire Cardie, Paul Davis, Bruce Fraser,
Diane Litman, David Pierce, Ellen Riloff,
Theresa Wilson, David Day, and Mark
Maybury. 2003. Recognizing and
308
Computational Linguistics Volume 30, Number 3
organizing opinions expressed in the
world press. In Working Notes of the AAAI
Spring Symposium in New Directions in
Question Answering, Palo Alto, CA, pages
12?19.
Wiebe, Janyce, Rebecca Bruce, Matthew Bell,
Melanie Martin, and Theresa Wilson.
2001. A corpus study of evaluative and
speculative language. In Proceedings of the
Second ACL SIGdial Workshop on Discourse
and Dialogue (SIGdial-2001), Aalborg,
Denmark, September 1?2, pages 186?195.
Wiebe, Janyce, Rebecca Bruce, and Thomas
O?Hara. 1999. Development and use of a
gold standard data set for subjectivity
classifications. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL-99), College
Park, MD, pages 246?253.
Wiebe, Janyce, Kenneth McKeever, and
Rebecca Bruce. 1998. Mapping
collocational properties into machine
learning features. In Proceedings of the Sixth
Workshop on Very Large Corpora (WVLC-98),
Montreal, August 15?16, pages 225?233.
Wiebe, Janyce and William J. Rapaport.
1986. Representing de re and de dicto belief
reports in discourse and narrative.
Proceedings of the IEEE, 74:1405?1413.
Wiebe, Janyce and William J. Rapaport.
1988. A computational theory of
perspective and reference in narrative. In
Proceedings of the 26th Annual Meeting of the
Association for Computational Linguistics
(ACL-88), Buffalo, NY, pages 131?138.
Wiebe, Janyce M. and William J. Rapaport.
1991. References in narrative text. Nou?s,
25(4):457?486.
Wiebe, Janyce, Theresa Wilson, and
Matthew Bell. 2001. Identifying
collocations for recognizing opinions. In
Proceedings of the ACL-01 Workshop on
Collocation: Computational Extraction,
Analysis, and Exploitation, Toulouse,
France, July 7, pages 24?31.
Wilson, Theresa and Janyce Wiebe. 2003.
Annotating opinions in the world press.
In Proceedings of the Fourth SIGdial Workshop
on Discourse and Dialogue (SIGdial-2003),
Sapporo, Japan, July 5?6, pages 13?22.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), Sapporo, Japan,
July 11?12, pages 129?136.
Identifying Opinionated Sentences
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
twilson@cs.pitt.edu
David R. Pierce
Department of Computer Science
and Engineering
University of Buffalo
The State University of New York
drpierce@cse.buffalo.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
1 Introduction
Natural language processing applications that summa-
rize or answer questions about news and other discourse
need to process information about opinions, emotions,
and evaluations. For example, a question answering sys-
tem that could identify opinions in the news could answer
questions such as the following:
Was the 2002 presidential election in Zim-
babwe regarded as fair?
What was the world-wide reaction to the
2001 annual U.S. report on human rights?
In the news, editorials, reviews, and letters to the editor
are sources for finding opinions, but even in news reports,
segments presenting objective facts are often mixed with
segments presenting opinions and verbal reactions. This
is especially true for articles that report on controversial
or ?lightning rod? topics. Thus, there is a need to be able
to identify which sentences in a text actually contain ex-
pressions of opinions and emotions.
We demonstrate a system that identifies opinionated
sentences. In general, an opinionated sentence is a sen-
tence that contains a significant expression of an opin-
ion, belief, emotion, evaluation, speculation, or senti-
ment. The system was built using data and other re-
sources from a summer workshop on multi-perspective
question answering (Wiebe et al, 2003) funded under
ARDA NRRC.1
1This work was performed in support of the Northeast Re-
gional Research Center (NRRC) which is sponsored by the
Advanced Research and Development Activity in Information
Technology (ARDA), a U.S. Government entity which sponsors
and promotes research of import to the Intelligence Community
which includes but is not limited to the CIA, DIA, NSA, NIMA,
and NRO.
2 Opinion Recognition System
2.1 System Architecture
The opinion recognition system takes as input a URL
or raw text document and produces as output an HTML
version of the document with the opinionated sentences
found by the system highlighted in bold. Figure 2.1
shows a news article that was processed by the system.
When the opinion recognition system receives a docu-
ment, it first uses GATE (Cunningham et al, 2002) (mod-
ified to run in batch mode) to tokenize, sentence split,
and part-of-speech tag the document. Then the document
is stemmed and searched for features of opinionated lan-
guage. Finally, opinionated sentences are identified using
the features found, and they are highlighted in the output.
2.2 Features
The system uses a combination of manually and auto-
matically identified features. The manually identified
features were culled from a variety of sources, includ-
ing (Levin, 1993) and (Framenet, 2002). In addition to
features learned in previous work (Wiebe et al, 1999;
Wiebe et al, 2001), the automatically identified features
include new features that were learned using information
extraction techniques (Riloff and Jones, 1999; Thelen and
Riloff, 2002) applied to an unannotated corpus of world
news documents.
2.3 Evaluation
We evaluated the system component that identifies opin-
ionated sentences on a corpus of 109 documents (2200
sentences) from the world news. These articles were an-
notated for expressions of opinions as part of the summer
workshop on multi-perspective question answering. In
this test corpus, 59% of sentences are opinionated sen-
tences. By varying system settings, the opinionated sen-
tence recognizer may be tuned to be very precise (91%
precision), identifying only those sentences it is very sure
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 33-34
                                                         Proceedings of HLT-NAACL 2003
Figure 1: Example of an article processed by the opinionated sentence recognition system. Sentences identified by the
system are highlighted in bold.
are opinionated (33% recall), or less precise (82% preci-
sion), identifing many more opinionated sentences (77%
recall), but also making more errors.
References
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust nlp tools and applications. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Framenet. 2002. http://www.icsi.berkeley.edu/   framenet/.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
M. Thelen and E. Riloff. 2002. A bootstrapping method
for learning semantic lexicons using extraction pattern
contexts. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing.
J. Wiebe, R. Bruce, and T. O?Hara. 1999. Development
and use of a gold standard data set for subjectivity clas-
sifications. In Proc. 37th Annual Meeting of the Assoc.
for Computational Linguistics (ACL-99), pages 246?
253, University of Maryland, June. ACL.
J. Wiebe, T. Wilson, and M. Bell. 2001. Identifying col-
locations for recognizing opinions. In Proc. ACL-01
Workshop on Collocation: Computational Extraction,
Analysis, and Exploitation, July.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff, T. Wilson,
D. Day, and M. Maybury. 2003. Recognizing and
organizing opinions expressed in the world press. In
Working Notes - New Directions in Question Answer-
ing (AAAI Spring Symposium Series).
A Corpus Study of Evaluative and Speculative Language
Janyce Wiebe

, Rebecca Bruce
y
, Matthew Bell

, Melanie Martin
z
, Theresa Wilson

University of Pittsburgh

, University of North Carolina at Asheville
y
, New Mexico State University
z
wiebe,mbell,twilson@cs.pitt.edu, bruce@cs.unca.edu, mmartin@cs.nmsu.edu
Abstract
This paper presents a corpus study
of evaluative and speculative language.
Knowledge of such language would be
useful in many applications, such as
text categorization and summarization.
Analyses of annotator agreement and of
characteristics of subjective language are
performed. This study yields knowl-
edge needed to design eective machine
learning systems for identifying subjec-
tive language.
1 Introduction
Subjectivity in natural language refers to aspects
of language used to express opinions and evalua-
tions (Baneld, 1982; Wiebe, 1994). Subjectivity
tagging is distinguishing sentences used to present
opinions and other forms of subjectivity (subjec-
tive sentences) from sentences used to objectively
present factual information (objective sentences).
This task is especially relevant for news report-
ing and Internet forums, in which opinions of var-
ious agents are expressed. There are numerous
applications for which subjectivity tagging is rele-
vant. Two are information retrieval and informa-
tion extraction. Current extraction and retrieval
technology focuses almost exclusively on the sub-
ject matter of documents. However, additional
aspects of a document inuence its relevance, in-
cluding, e.g., the evidential status of the material
presented, and the attitudes expressed about the
topic (Kessler et al, 1997). Knowledge of subjec-
tive language would also be useful in ame recog-
nition (Spertus, 1997; Kaufer, 2000), email clas-
sication (Aone et al, 2000), intellectual attribu-
tion in text (Teufel and Moens, 2000), recogniz-
ing speaker role in radio broadcasts (Barzilay et
al., 2000), review mining (Terveen et al, 1997),
generation and style (Hovy, 1987), clustering doc-
uments by ideological point of view (Sack, 1995),
and any other application that would benet from
knowledge of how opinionated the language is, and
whether or not the writer purports to objectively
present factual material.
To use subjectivity tagging in applications,
good linguistic clues must be found. As with many
pragmatic and discourse distinctions, existing lex-
ical resources are not comprehensively coded for
subjectivity. The goal of our current work is learn-
ing subjectivity clues from corpora. This paper
contributes to this goal by empirically examin-
ing subjectivity. We explore annotating subjectiv-
ity at dierent levels (expression, sentence, docu-
ment) and produce corpora annotated at dierent
levels. Annotator agreement is analyzed to un-
derstand and assess the viability of such annota-
tions. In addition, because expression-level anno-
tations are ne-grained and thus very informative,
these annotations are examined to gain knowledge
about subjectivity.
We also use our annotations and existing ed-
itorial annotations to generate and test features
of subjectivity. Altogether, the observations and
results of these studies provide valuable informa-
tion that will facilitate designing eective machine
learning systems for recognizing subjectivity.
The remainder of this paper rst provides back-
ground about subjectivity, then presents results
for document-level annotations, followed by an
analysis of expression-level annotations. Results
for features generated using document-level anno-
tations are next, ending with conclusions.
2 Subjectivity
Sentence (1) is an example of a simple subjective
sentence, and (2) is an example of a simple objec-
tive sentence:
1
(1) At several dierent layers, it's a fascinating
tale.
1
The term subjectivity is due to Ann Baneld
(1982). For references to work on subjectivity, please
see (Baneld, 1982; Fludernik, 1993; Wiebe, 1994;
Stein and Wright, 1995).
(2) Bell Industries Inc. increased its quarterly to
10 cents from 7 cents a share.
The main types of subjectivity are:
1. Evaluation. This category includes emotions
such as hope and hatred as well as evalua-
tions, judgements, and opinions. Examples
of expressions involving positive evaluation
are enthused, wonderful, and great product!.
Examples involving negative evaluation are
complained, you idiot!, and terrible product.
2. Speculation. This category includes anything
that removes the presupposition of events oc-
curring or states holding, such as speculation
and uncertainty. Examples of speculative ex-
pressions are speculated, and maybe.
Following are examples of strong negative
evaluative language from a corpus of Usenet
newsgroup messages:
(3a) I had in mind your facts, buddy, not hers.
(3b) Nice touch. \Alleges" whenever facts posted
are not in your persona of what is \real".
Following is an example of opinionated, edito-
rial language, taken from an editorial in the Wall
Street Journal:
(4) We stand in awe of the Woodstock genera-
tion's ability to be unceasingly fascinated by the
subject of itself.
Sentences (5) and (6) illustrate the fact that
sentences about speech events may be subjective
or objective:
(5) Northwest Airlines settled the remaining
lawsuits led on behalf of 156 people killed in
a 1987 crash, but claims against the jetliner's
maker are being pursued, a federal judge said.
(6) \The cost of health care is eroding our stan-
dard of living and sapping industrial strength,"
complains Walter Maher, a Chrysler health-and-
benets specialist.
In (5), the material about lawsuits and claims is
presented as factual information, and a federal
judge is given as the source of information. In
(6), in contrast, a complaint is presented. An NLP
system performing information extraction on (6)
should not treat the material in the quoted string
as factual information, with the complainer as a
source of information, whereas a corresponding
treatment of sentence (5) would be appropriate.
Subjective sentences often contain individual
expressions of subjectivity. Examples are fasci-
nating in (1), and eroding, sapping, and complains
in (6). The following paragraphs mention aspects
of subjectivity expressions that are relevant for
NLP applications.
First, although some expressions, such as !, are
subjective in all contexts, many, such as sapping
and eroding, may or may not be subjective, de-
pending on the context in which they appear. A
potential subjective element (PSE) is a linguistic
element that may be used to express subjectivity.
A subjective element is an instance of a potential
subjective element, in a particular context, that is
indeed subjective in that context (Wiebe, 1994).
Second, a subjective element expresses the sub-
jectivity of a source, who may be the writer or
someone mentioned in the text. For example, the
source of fascinating in (1) is the writer, while
the source of the subjective elements in (6) is Ma-
her. In addition, a subjective element has a tar-
get, i.e., what the subjectivity is about or directed
toward. In (1), the target is a tale; in (6), the tar-
get of Maher's subjectivity is the cost of health
care. These are examples of object-centric sub-
jectivity, which is about an object mentioned in
the text (other examples: \I love this project";
\The software is horrible"). Subjectivity may also
be addressee-oriented, i.e., directed toward the lis-
tener or reader (e.g., \You are an idiot").
Third, there may be multiple subjective ele-
ments in a sentence, possibly of dierent types
and attributed to dierent sources and targets.
For example, in (4), subjectivity of the Woodstock
generation is described (specically, its fascina-
tion with itself). In addition, subjectivity of the
writer is expressed (e.g., `we stand in awe'). As de-
scribed below, individual subjective elements were
annotated as part of this work, rening previous
work on sentence-level annotations. Finally, PSEs
may be complex expressions such as `village id-
iot', `powers that be', `You' NP, and `What a'
NP. There is a great variety of such expressions,
including many studied under the rubric of idioms
(see, for example, (Nunberg et al, 1994)). We ad-
dress learning such expressions in another project.
3 Previous Work on Subjectivity
Tagging
In previous work (Wiebe et al, 1999; Bruce and
Wiebe, 1999), a corpus of sentences from the Wall
Street Journal Treebank Corpus (Marcus et al,
1993) was manually annotated with subjectivity
classications by multiple judges. The judges were
instructed to consider a sentence to be subjective
if they perceived any signicant expression of sub-
jectivity (of any source) in the sentence, and to
consider the sentence to be objective, otherwise.
Agreement was summarized in terms of Cohen's
 (Cohen, 1960), which compares the total proba-
bility of agreement to that expected if the taggers'
classications were statistically independent (i.e.,
\chance agreement"). After two rounds of tag-
ging by three judges, an average pairwise  value
of .69 was achieved on a test set. The EM learn-
ing algorithm was used to produce corrected tags
representing the consensus opinions of the taggers
(Goodman, 1974; Dawid and Skene, 1979). An
automatic system to perform subjectivity tagging
was developed using the new tags as training and
testing data. In 10-fold cross validation experi-
ments, a probabilistic classier obtained an aver-
age accuracy on subjectivity tagging of 72.17%,
more than 20 percentage points higher than a
baseline accuracy obtained by always choosing the
more frequent class. Five part-of-speech features,
two lexical features, and a paragraph feature were
used.
To identify richer features, (Wiebe, 2000) used
Lin's (1998) method for clustering words accord-
ing to distributional similarity, seeded by a small
amount of detailed manual annotation, to auto-
matically identify adjective PSEs. There are two
parameters of this process, neither of which was
varied in (Wiebe, 2000): C, the cluster size con-
sidered, and FT , a ltering threshold, such that, if
the seed word and the words in its cluster have, as
a set, lower precision than the ltering threshold
on the training data, the entire cluster, includ-
ing the seed word, is ltered out. This process is
adapted for use in the current paper, as described
in section 7.
4 Choices in Annotation
In expression-level annotation, the judges rst
identify the sentences they believe are subjective.
They next identify the subjective elements in
the sentence, i.e., the expressions they feel are
responsible for the subjective classication. For
example (subjective elements are in parentheses):
They promised (yet) more for (really good stu).
(Perhaps you'll forgive me) for reposting his
response.
Subjective-element (expression-level) annota-
tions are probably the most natural. Ultimately,
we would like to recognize the subjective elements
in a text, and their types, targets, and sources.
However, both manual and automatic tagging at
this level are dicult because the tags are very
ne-grained, and there is no predetermined clas-
sication unit; a subjective element may be a sin-
gle word or a large expression. Thus, in the short
term, it is probably best to use subjective-element
annotations for knowledge acquisition (analysis,
training, feature generation) alone, and not target
automatic classication of subjective elements.
In this work, document-level subjectivity anno-
tations are text categories of which subjectivity
is a key aspect. We use three text categories:
editorials (Kessler et al, 1997), reviews, and
\ames", i.e., hostile messages (Spertus, 1997;
Kaufer, 2000). For ease of discussion, we group
editorials and reviews together under the term
opinion pieces.
There are benets to using such document-level
annotations. First, they are more directly re-
lated to applications (e.g., ltering hostile mes-
sages and mining reviews from Internet forums).
Second, there are existing annotations to be ex-
ploited, such as editorials and arts reviews marked
as such by newspapers, as well as on-line product
reviews accompanied by formal numerical ratings
(e.g., 4 on a scale from 1 to 5).
However, a challenging aspect of such data is
that opinion pieces and ames contain objective
sentences, while documents in other text cate-
gories contain subjective sentences. News reports
present reactions to and attitudes toward reported
events (van Dijk 1988); they often contain seg-
ments starting with expressions such as critics
claim and supporters argue. In addition, quoted-
speech sentences in which individuals express their
subjectivity are often included (Barzilay et al,
2000). On the other hand, editorials contain ob-
jective sentences presenting facts supporting the
writer's argument, and reviews contain sentences
objectively presenting facts about the product.
This \impure" aspect of opinionated text cate-
gories must be considered when such data is used
for training and testing. Some specic results are
given below in section 7.
We believe that sentence-level classications
will continue to provide an important level of
analysis. The sentence provides a prespeci-
ed classication unit
2
and, while sentence-level
judgements are not as ne-grained as subjective-
2
While sentence boundaries are not always unam-
biguous in unedited text or spoken language, the data
can always be segmented into sentence-like units be-
fore subjectivity tagging is performed.
element judgements, they do not involve the large
amount of noise we face with document-level an-
notations.
5 Document-Level Annotation
Results
5.1 Flame Annotations
In this study, newsgroup messages were assigned
the tags ame or not-ame. The corpus con-
sists of 1140 Usenet newsgroup messages, bal-
anced among the categories alt, sci, comp, and rec
in the Usenet hierarchy. The corpus was divided,
preserving the category balance, into a training set
of 778 messages and a test set of 362 messages.
The annotators were instructed to mark a mes-
sage as a ame if the \main intention of the mes-
sage is a personal attack, containing insulting or
abusive language." A number of policy decisions
were made in the instructions, dealing, primarily,
with included messages (part or all of a previous
message, included in the current message as part
of a reply). Some additional issues addressed in
the instructions were who the attack was directed
at, nonsense, sarcasm, humor, rants, and raves.
During the training phase, two annotators, MM
and R, participated in multiple rounds of tagging,
revising the annotation instructions as they pro-
ceeded. During the testing phase, MM and R in-
dependently annotated the test set, achieving a 
value on these messages of 0.69. A third annota-
tor, L, trained on 492 messages from the training
set, and then annotated 88 of the messages in the
test set. The pairwise  values on this set of 88
are: MM & R: 0.80; MM & L: 0.75; R & MM:
0.80; for an average pairwise  of .78.
This study provides evidence for the viability
of document-level ame annotation. We plan to
build a ame-recognition system in the future. As
will be seen below, MM and R also tagged this
data at the subjective-element level.
5.2 Opinion-Piece Classications
Our opinion-piece classications are built on exist-
ing annotations in the Wall Street Journal. Specif-
ically, there are articles explicitly identied to be
Editorials, Letters to the Editor, Arts & Leisure,
and Viewpoints; together, we call these opinion
pieces. This data is a good resource for subjectiv-
ity recognition. However, an inspection of some
data revealed that some editorials and reviews are
not marked as such. For example, there are arti-
cles written in the rst person, and the purpose of
the article is to present an argument rather than
cover a news story, but there is no explicit indi-
cation that they are editorials. To create high
quality test data, two judges manually annotated
WSJ data for opinion pieces. The instructions
were to nd any additional opinion pieces that
are not marked as such. The annotators also had
the option of disagreeing with the existing anno-
tations, but did not opt to do so in any instances.
One judge annotated all articles in four datasets
of the Wall Street Journal Treebank corpus (Mar-
cus et al, 1993) (W9-4, W9-10, W9-22, and W9-
33, each approximately 160K words) as well as
the corpus of Wall Street Journal articles used in
(Wiebe et al, 1999) (called WSJ-SE below). An-
other judge annotated all articles in two of the
datasets (W9-22 and W9-33).
This annotation task appears to be relatively
easy. With no training at all, the  values are very
high: .94 for dataset W9-33 and .95 for dataset
W9-22.
The agreement data for W9-22 is given in Table
1 in the form of a contingency table. In section
7, this data is used to generate and test candidate
potential subjective elements (PSEs).
6 Subjective-Element Annotation
Results and Analyses
6.1 Annotations and Data
These subsections analyze subjective element an-
notations performed on three datasets, WSJ-SE,
NG-FE, and NG-SE.
WSJ-SE is the corpus of 1001 sentences of the
Wall Street Journal Treebank Corpus referred to
above in section 3. Recall that the sentences of
this corpus were manually annotated with subjec-
tivity classications as described in (Wiebe et al,
1999; Bruce and Wiebe, 1999).
For this paper, two annotators (D and M ) were
asked to identify the subjective elements in WSJ-
SE. Specically, the taggers were given the sub-
jective sentences identied in the previous study,
and asked to put brackets around the words they
believe cause the sentence to be classied as sub-
jective.
Note that inammatory language is a kind of
subjective language. NG-FE is a subset of the
Usenet newsgroup corpus used in the document-
level ame-annotation study described in section
5.1. Specically, NG-FE consists of the 362-
message test set for taggers R and MM. For this
study, R and MM were asked to identify the ame
elements in NG-FE. Flame elements are the sub-
set of subjective elements that are perceived to
be inammatory. R and MM were asked to do
this in all 362 messages, because some messages
that were not judged to be ames at the message
level do contain individual inammatory phrases
Tagger 2
Op Not Op
Tagger 1 Op n
11
= 23 n
12
= 0 n
1+
= 23
Not Op n
21
= 2 n
22
= 268 n
2+
= 270
n
+1
= 25 n
+2
= 268 n
++
= 293
Table 1: Contingency Table for Opinion Piece Agreement in W9-22
(in these cases, the tagger does not believe that
these phrases express the main intent of the mes-
sage).
In addition to the above annotations, tagger M
performed subjective-element tagging on a dier-
ent set of Usenet newsgroup messages, corpus NG-
SE. The size of this corpus is 15413 words.
In datasets WSJ-SE and NG-SE, the taggers
were also asked to specify one of ve subjective
element types: e+ (positive evaluative), e  (neg-
ative evaluative), e? (some other type of evalua-
tion), u (uncertainty), and o (none of the above),
with the option to assign multiple types to an in-
stance. All corpora were stemmed (Karp et al,
1992) and part-of-speech tagged (Brill, 1992).
6.2 Agreement Among Taggers
There are techniques for analyzing agreement
when annotations involve segment boundaries
(Litman and Passonneau, 1995; Marcu et al,
1999), but our focus in this paper is on words.
Thus, our analyses are at the word level: each
word is classied as either appearing in a subjec-
tive element or not. Punctuation is excluded from
our analyses. The WSJ data is divided into two
subsets in this section, Exp1 and Exp2.
As mentioned above, in WSJ-SE Exp1 and
Exp2, the taggers also classied subjective ele-
ments with respect to the type of subjectivity
being expressed. Subjectivity type agreement is
again analyzed at the word level, but, in this anal-
ysis, only the words classied as belonging to sub-
jective elements by both taggers are considered.
Table 2 provides  values for word agreement
in NG-FE (the ame data) as well as for WSJ-SE
Exp1 and Exp2. The task of identifying subjec-
tive elements in a body of text is dicult, and the
agreement results reect this fact; agreement is
much stronger than that expected by chance, but
less than what we would like to see when verify-
ing a new classication. Further renement of the
coding manual is required. Additionally, it may be
possible to rene the classications automatically
using methods such as those described in (Wiebe
et al, 1999). In this analysis, we explore the pat-
terns of agreement exhibited by the taggers in an
eort to better understand the classication.
We begin by looking at word agreement. Word
agreement is higher in the ame experiment
(NG-FE) than it is in either WSJ experiment
(WSJ-SE Exp1 and Exp2). Looking at the WSJ
data provides one plausible explanation for the
lower word agreement in the WSJ experiments.
As exhibited in the subjective elements identied
for the single clause below,
D: (e+ played the role well) (e? obligatory
ragged jeans a thicket of long hair and rejection
of all things conventional)
M : (e+ well) (e? obligatory) (e- ragged) (e?
thicket) (e- rejection) (e- all things conventional)
tagger D consistently identies entire phrases
as subjective, while Tagger M prefers to select
discrete lexical items. This dierence in inter-
pretation of the tagging instructions does not
occur in the ame experiment. Nonetheless, even
within the ame data, there are many instances
where both taggers identify the same segment of
a sentence as forming a subjective element but
disagree on the boundaries of that segment, as in
the example below.
R: (classic case of you deliberately misinterpret-
ing my comments)
MM : (you deliberately misinterpreting my
comments)
These patterns of partial agreement are also evi-
dent in the  values for words from specic syn-
tactic categories (see Table 2 again). In the WSJ
data, agreement on determiners is particularly low
because they are often included as part of a phrase
by tagger D but typically not included in the spe-
cic lexical items chosen by tagger M. Interest-
ingly, in the WSJ experiments, the taggers most
frequently agreed on the selection of modals and
adjectives, while in the ame experiment, agree-
ment was highest on nouns and adjectives. The
high agreement on adjectives in both genres is con-
All Words Nouns Verbs Modals Adj's Adverbs Det's
NG-FE 0:4657 0:5213 0.4571 0:4008 0:5011 0:3576 0:4286
WSJ-SE, Exp1 0:4228 0:3999 0.4235 0:6992 0:6000 0:4328 0:2661
WSJ-SE, Exp2 0:3703 0:3705 0.4261 0:4298 0:4294 0:2256 0:1234
Table 2:  Values for Word Agreement
sistent with results from other work (Bruce and
Wiebe, 1999; Wiebe et al, 1999), but high agree-
ment on nouns in the ame data verses high agree-
ment on modals in the WSJ data suggests a genre
specic usage of these categories. This would be
the case if, for example, modals were most fre-
quently used to express uncertainty, a type of sub-
jectivity that would be relatively rare in ames.
Turning to subjective-element type, in both
WSJ experiments, the  values for type agreement
are comparable to those for word agreement. Re-
call that multiple types may be assigned to a single
subjective instance. All such instances in the WSJ
data are u in combination with an evaluative tag
(i.e., e+, e- and e?), and they are not common:
each tagger assigned multiple tags to fewer than
7% of the subjective instances. However, if partial
matches between type tags are recognized, i.e., if
they share a common tag, then the  values im-
prove signicantly. Table 3 shows both types of
results.
It is interesting to note the variation in type agree-
ment for words of dierent syntactic categories.
Agreement on adjectives is consistently high while
the agreement on the type of subjectivity ex-
pressed by modals and adverbs is consistently low.
This contrasts with the fact that word agreement
for modals, in particular, and, to a lesser extent,
adverbs was high. This lack of agreement sug-
gests that the type of subjectivity expressed by
adjectives is more easily distinguished than that
of modals or adverbs. This is particularly impor-
tant because the number of adjectives included in
subjective elements is high. In contrast, the num-
bers of modals and adverbs are relatively low.
Additional insight can be gained by combining
the 3 evaluative classications (i.e., e+, e- and
e?) to form a single tag, e, representing any
form of evaluative expression. Table 4 presents
type agreement results for the tag set e, u, o.
In contrasting Tables 3 and 4, it is surprising
to note that most of the  values decrease when
the distinction among the evaluative types is re-
moved. This suggests that the three evaluative
types are natural classications. Only for adverbs
does type agreement improve with the smaller
tag set; this indicates that it is dicult to dis-
tinguish the evaluative nature of adverbs. Note
also that agreement for modals is not impacted
by the change in tag sets. This fact supports the
hypothesis that modals are used primary to ex-
press uncertainty. As a nal point, we look at
patterns of agreement in type classication using
the models of symmetry, marginal homogeneity,
quasi-independence, and quasi-symmetry. Each
model tests for a specic pattern of agreement:
symmetry tests the interchangeability of taggers,
marginal homogeneity veries the absence of bias
among taggers, quasi-independence veries that
the taggers act independently when they disagree,
and quasi-symmetry tests for the presence of any
pattern in their disagreements. For a more com-
plete description of these models and their use
in analyzing intercoder reliability see (Bruce and
Wiebe, 1999). In short, the results presented in
Table 5 indicate that the taggers are not inter-
changeable: they exhibit biases in their type clas-
sications, and there is a pattern of correlated dis-
agreement in the assignment of the original type
tags. Surprisingly, the taggers appear to act in-
dependently when they disagree in assigning the
compressed type tags (i.e., tags e, u and o). This
shift in the pattern of disagreement between tag-
gers again suggests that the compression of the
evaluative tags was inappropriate. Additionally,
these ndings suggest that it may be possible to
automatically correct the type biases expressed
by the taggers using the technique described in
(Bruce and Wiebe, 1999), a topic that will be in-
vestigated in future work.
6.3 Uniqueness
Based on previous work (Wiebe et al, 1998), we
hypothesized that low-frequency words are associ-
ated with subjectivity. Table 6 provides evidence
that the number of unique words (words that ap-
pear just once) in subjective elements is higher
than expected. The rst row gives information
for all words and the second gives information for
words that appear just once. The gures in the
Num columns are total counts, and the gures in
the P columns give the proportion that appear in
subjective elements. The Agree columns give in-
All Words Nouns Verbs Modals Adj's Adverbs Det's
Exp1 Full Match 0:4216 0:4228 0.2933 0:1422 0:5919 0:1207 0:5000
Partial Match 0:5156 0:4570 0.4447 0:3011 0:6607 0:3305 0:5000
Exp2 Full Match 0:3041 0:2353 0.2765 0:1429 0:5794 0:1207 0:0000
Partial Match 0:4209 0:2353 0.3994 0:3494 0:6719 0:4439 0:1429
Table 3:  Values for Type Agreement Using All Types in the WSJ Data
All Words Nouns Verbs Modals Adj's Adverbs Det's
Exp1 Full Match 0:3377 0:0440 0.1648 0:1968 0:5443 0:3810 0:0000
Partial Match 0:5287 0:1637 0.3765 0:4903 0:8125 0:3810 0:0000
Exp2 Full Match 0:2569 0:0000 0.1923 0:1509 0:4783 0:1707 0:1429
Partial Match 0:4789 0:0000 0.4167 0:4000 0:8056 0:7671 0:4000
Table 4:  Values for Type Agreement Using E,O,U in the WSJ Data
Sym. M.H. Q.S. Q.I.
Exp1 All Types G
2
112:351 92:447 19:904 66:771
Sig. 0:000 0:000 0:527 0:007
e,o,u G
2
85:478 84:142 1:336 12:576
Sig. 0:000 0:000 0:248 0:027
Exp2 All Types G
2
94:669 76:247 18:422 58:892
Sig. 0:000 0:000 0:241 0:001
e,o,u G
2
66:822 66:819 0:003 0:0003
Sig. 0:000 0:000 0:986 0:987
Table 5: Tests for Patterns of Agreement in WSJ Type-Tagged Data
WSJ-SE NG-FE
D M Agree Agree R MM
Num P Num P Num P Num P Num P Num P
All words 18341 .07 18341 .08 16857 .04 15413 .15 86279 .01 88210 .02
unique 2615 .14 2615 .20 2522 .15 2348 .17 5060 .07 4836 .03
Table 6: Proportions of Unique Words in Subjective Elements
formation for the subset of the corresponding data
set upon which the two annotators agree.
Comparison of rows 1 and 2 across columns
shows that the proportion of unique words that
are subjective is higher than the proportion of all
words that are subjective. In all cases, this dier-
ence in proportions is highly statistically signi-
cant.
6.4 Types and Context
An interesting question is, when a word appears
in multiple subjective elements, are those subjec-
tive elements all the same type? Table 7 shows
that a signicant portion are used in more than
one type. Each item considered in the table is a
word-POS pair that appears more than once in the
corpus. The gures shown are the total number of
word-POS items that appear more than once (the
columns labeled MultInst) and the proportion of
those items that appear in more than one type
of subjective element (the columns labeled Mult-
Type). These results highlight the need for contex-
tual disambiguation. For example, one thinks of
great as a positive evaluative term, but its polarity
depends on the context; it can be used negatively
evaluatively in a context such as \Just great." A
goal of performing subjective-element annotations
is to support learning such local contextual inu-
ences.
7 Generating and Testing PSEs
using Document-Level
Annotations
This section uses the opinion-piece annotations to
expand our set of PSEs beyond those that can be
derived from the subjective-element annotations.
Precision is used to assess feature quality. The
precision of feature F for class C is the number
of Fs that occur in units of class C over the total
number of Fs that occur anywhere in the data.
An important motivation for using the opinion-
piece data is that there is a large amount of it,
and manually rening existing annotations as de-
scribed in section 5.2 is much easier and more re-
liable than other types of subjectivity annotation.
However, we cannot expect absolutely high pre-
cisions for two reasons. First, the distribution of
opinions and non-opinions is highly skewed in fa-
vor of non-opinions. For example, in Table 1, tag-
ger 1 classies only 23 of 293 articles as opinion
pieces. Second, as discussed in section 4, opin-
ion pieces contain objective sentences and non
opinion-pieces contain subjective sentences. For
example, in WSJ-SE, which has been annotated
at the sentence and document levels, 70% of the
sentences in opinion pieces are subjective and 30%
are objective. In non-opinion pieces, 44% of the
sentences are subjective and only 56% are objec-
tive.
To give an idea of expected precisions, let us
consider the precision of subjective sentences with
respect to opinion pieces. Suppose that 15% of
the sentences in the dataset are in opinions, 85%
in non-opinions. Let us assume the proportions of
subjective and objective sentences in opinion and
non-opinion pieces given just above. Let N be the
total number of sentences. The desired precision
is the number of subjective sentences in opinions
over the total number of subjective sentences. It
is .22:
p=.15 * N * .70 / (.15 * N * .70 + .85 * N * .44).
In addition, we are assessing PSEs, which are
only potentially subjective; many have objective
as well as subjective uses.
Thus, even if precisions are much lower than 1,
we use increases in precision over a baseline as ev-
idence of promising PSEs. The baseline for com-
parison is the number of word instances in opin-
ion pieces, divided by the total number of word
instances. Table 8 shows the precisions for three
types of PSEs. The freq columns give total fre-
quencies, and the +prec columns show the im-
provements in precision from the baseline. The
baseline precisions are given at the bottom of the
table.
As mentioned above, (Wiebe, 2000) showed suc-
cess automatically identifying adjective PSEs us-
ing Lin's method, seeded by a small amount of de-
tailed manual annotations. Desiring to move away
from manually annotated data, for this paper the
same process is used, but the seed words are all
the adjectives (verbs) in the training data. In ad-
dition, in the current setting, there are no a priori
values to use for parameters C (cluster size) and
FT (ltering threshold), as there were in (Wiebe,
2000), and results vary with dierent parameter
settings. Thus, a train-validate-test process is ap-
propriate. In Table 8, the numbers given under,
e.g., W9-10, are the results obtained when W9-10
is used as the test set. One of the other datasets,
say W9-22, was used as the training set, meaning
that all the adjectives (verbs) in that dataset are
the seed words, and all ltering was performed us-
ing only that data. The seed-ltering process was
repeated with dierent settings of C and FT , pro-
ducing a dierent set of adjectives (verbs) for each
setting. A third dataset, say W9-33, was used as a
validation set, i.e., among all the sets of adjectives
generated from the training set, those with good
performance on the validation set were selected as
WSJ-SE-M WSJ-SE-D NG-SE-M
MultInst MultType MultInst MultType MultInst MultType
413 .17 378 .16 571 .29
Table 7: Word-POS-Types Used in Multiple Types of Subjective Elements
W9-10 W9-22 W9-33 W9-04
freq +prec freq +prec freq +prec freq +prec
adjectives 373 .21 1340 .11 2137 .09 2537 .14
verbs 721 .16 1436 .08 3139 .07 3720 .11
unique words 6065 .10 5441 .07 6045 .06 6171 .09
baseline precision .17 .13 .14 .18
freq: Total frequency +prec: Increase in precision over baseline
Table 8: Frequencies and Increases in Precision
the PSEs to test on the test set. A set was consid-
ered to have good performance on the validation
set if its precision is at least .25 and its frequency
is at least 100. Since this process is meant to
be a method for mining existing document-level
annotations for PSEs, the existing opinion-piece
annotations were used for training and validation.
Our manual opinion-piece annotations were used
for testing.
The row labeled unique words shows the preci-
sion on the test set of the individual words that
are unique in the test set. The increase over base-
line precision shows that low-frequency words can
be informative for recognizing subjectivity.
Note that the features all do better and worse
on the same data sets. This shows that the subjec-
tivity is somehow harder to identify in, say, W9-33
than in W9-10; it also shows an important consis-
tency among the features, even though they are
identied in dierent ways.
8 Conclusions
This paper presents the results of an empirical ex-
amination of subjectivity at the dierent levels of
a text: the expression level, the sentence level,
and the document level. While analysis of subjec-
tivity is perhaps most natural and precise at the
expression level, document-level annotations are
freely available from a number of sources and are
appropriate for many applications. The sentence-
level annotation is a workable intermediate level:
sentence-level judgments are not as ne-grained as
expression-level judgments, and they don't involve
the large amount of noise found at the document
level.
As part of this examination, we present a study
of annotator agreement characterizing the di-
culty of identifying subjectivity at the dierent
levels of a text. The results demonstrate that not
only can subjectivity be identied at the docu-
ment level with high reliability, but that it is also
possible to identify expression-level subjectivity,
albeit with lower reliability.
Using manual annotations, we are able to char-
acterize subjective language. At the expression
level, we found that it is natural to distinguish
among positively evaluative, negatively evalua-
tive, and speculative uses of a word. We also
found that subjective text contains a high pro-
portion of unique word occurrences, much more so
than ordinary text. Rather than ignoring or dis-
carding unique words, we demonstrate that the
occurrence of a unique word is a PSE. We also
found that agreement is higher for some syntac-
tic word classes, e.g., for adjectives in comparison
with determiners.
Finally, we are able to mine PSEs from text
tagged at the document level. Given the diculty
of evaluating PSEs in document-level subjectiv-
ity classication due to the mix of subjective and
objective sentences, the PSEs identied in this
study exhibit relatively high precision. In future
work, we will investigate document-level classi-
cation using these PSEs, as well as other methods
for extracting PSEs from text tagged at the doc-
ument level; methods to be investigated include
mutual-bootstrapping and/or co-training.
References
C. Aone, M. Ramos-Santacruz, and W. Niehaus.
2000. Assentor: An nlp-based solution to e-mail
monitoring. In Proc. IAAI-2000, pages 945{
950.
A. Baneld. 1982. Unspeakable Sentences. Rout-
ledge and Kegan Paul, Boston.
R. Barzilay, M. Collins, J. Hirschberg, and
S. Whittaker. 2000. The rules behind roles:
Identifying speaker role in radio broadcasts. In
Proc. AAAI.
E. Brill. 1992. A simple rule-based part of speech
tagger. In Proc. of the 3rd Conference on Ap-
plied Natural Language Processing (ANLP-92),
pages 152{155.
R. Bruce and J. Wiebe. 1999. Recognizing subjec-
tivity: A case study of manual tagging. Natural
Language Engineering, 5(2).
J. Cohen. 1960. A coecient of agreement for
nominal scales. Educational and Psychological
Meas., 20:37{46.
A. P. Dawid and A. M. Skene. 1979. Max-
imum likelihood estimation of observer error-
rates using the EM algorithm. Applied Statis-
tics, 28:20{28.
M. Fludernik. 1993. The Fictions of Language
and the Languages of Fiction. Routledge, Lon-
don.
L. Goodman. 1974. Exploratory latent structure
analysis using both identiable and unidenti-
able models. Biometrika, 61:2:215{231.
E. Hovy. 1987. Generating Natural Language un-
der Pragmatic Constraints. Ph.D. thesis, Yale
University.
D. Karp, Y. Schabes, M. Zaidel, and D. Egedi.
1992. A freely available wide coverage mor-
phological analyzer for English. In Proc. of
the 14th International Conference on Compu-
tational Linguistics (COLING-92).
D. Kaufer. 2000. Flaming: A White Paper.
www.eudora.com.
B. Kessler, G. Nunberg, and H. Schutze. 1997.
Automatic detection of text genre. In Proc.
ACL-EACL-97.
D. Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. COLING-ACL '98,
pages 768{773.
Diane J. Litman and R. J. Passonneau. 1995.
Combining multiple knowledge sources for dis-
course segmentation. In Proc. 33rd Annual
Meeting of the Association for Computational
Linguistics (ACL-95), pages 108{115. Associa-
tion for Computational Linguistics, june.
D. Marcu, M. Romera, and E. Amorrortu. 1999.
Experiments in constructing a corpus of dis-
course trees: Problems, annotation choices, is-
sues. In The Workshop on Levels of Represen-
tation in Discourse, pages 71{78.
M. Marcus, Santorini, B., and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The penn treebank. Computational Lin-
guistics, 19(2):313{330.
G. Nunberg, I. Sag, and T. Wasow. 1994. Idioms.
Language, 70:491{538.
W. Sack. 1995. Representing and recognizing
point of view. In Proc. AAAI Fall Symposium
on AI Applications in Knowledge Navigation
and Retrieval.
E. Spertus. 1997. Smokey: Automatic recogni-
tion of hostile messages. In Proc. IAAI.
D. Stein and S. Wright, editors. 1995. Subjectiv-
ity and Subjectivisation. Cambridge University
Press, Cambridge.
L. Terveen, W. Hill, B. Amento, D. McDonald,
and J. Creter. 1997. Building task-specic in-
terfaces to high volume conversational data. In
Proc. CHI 97, pages 226{233.
S. Teufel and M. Moens. 2000. What's yours and
what's mine: Determining intellectual attribu-
tion in scientic texts. In Proc. Joint SIGDAT
Converence on EMNLP and VLC.
J. Wiebe, K. McKeever, and R. Bruce. 1998.
Mapping collocational properties into machine
learning features. In Proc. 6th Workshop on
Very Large Corpora (WVLC-98), pages 225{
233, Montreal, Canada, August. ACL SIGDAT.
J. Wiebe, R. Bruce, and T. O'Hara. 1999. Devel-
opment and use of a gold standard data set for
subjectivity classications. In Proc. 37th An-
nual Meeting of the Assoc. for Computational
Linguistics (ACL-99), pages 246{253, Univer-
sity of Maryland, June. ACL.
J. Wiebe. 1994. Tracking point of view in narra-
tive. Computational Linguistics, 20(2):233{287.
J. Wiebe. 2000. Learning subjective adjectives
from corpora. In 17th National Conference on
Articial Intelligence (AAAI-2000).
           	  
                                    
 
    

  Learning Subjective Nouns using Extraction Pattern Bootstrapping?
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
twilson@cs.pitt.edu
Abstract
We explore the idea of creating a subjectiv-
ity classifier that uses lists of subjective nouns
learned by bootstrapping algorithms. The goal
of our research is to develop a system that
can distinguish subjective sentences from ob-
jective sentences. First, we use two bootstrap-
ping algorithms that exploit extraction patterns
to learn sets of subjective nouns. Then we
train a Naive Bayes classifier using the subjec-
tive nouns, discourse features, and subjectivity
clues identified in prior research. The boot-
strapping algorithms learned over 1000 subjec-
tive nouns, and the subjectivity classifier per-
formed well, achieving 77% recall with 81%
precision.
1 Introduction
Many natural language processing applications could
benefit from being able to distinguish between factual
and subjective information. Subjective remarks come
in a variety of forms, including opinions, rants, allega-
tions, accusations, suspicions, and speculation. Ideally,
information extraction systems should be able to distin-
guish between factual information (which should be ex-
tracted) and non-factual information (which should be
discarded or labeled as uncertain). Question answering
systems should distinguish between factual and specula-
tive answers. Multi-perspective question answering aims
to present multiple answers to the user based upon specu-
lation or opinions derived from different sources. Multi-
? This work was supported in part by the National Sci-
ence Foundation under grants IIS-0208798 and IRI-9704240.
The data preparation was performed in support of the North-
east Regional Reseach Center (NRRC) which is sponsored by
the Advanced Research and Development Activity (ARDA), a
U.S. Government entity which sponsors and promotes research
of import to the Intelligence Community which includes but is
not limited to the CIA, DIA, NSA, NIMA, and NRO.
document summarization systems need to summarize dif-
ferent opinions and perspectives. Spam filtering systems
must recognize rants and emotional tirades, among other
things. In general, nearly any system that seeks to iden-
tify information could benefit from being able to separate
factual and subjective information.
Subjective language has been previously studied in
fields such as linguistics, literary theory, psychology, and
content analysis. Some manually-developed knowledge
resources exist, but there is no comprehensive dictionary
of subjective language.
Meta-Bootstrapping (Riloff and Jones, 1999) and
Basilisk (Thelen and Riloff, 2002) are bootstrapping al-
gorithms that use automatically generated extraction pat-
terns to identify words belonging to a semantic cate-
gory. We hypothesized that extraction patterns could
also identify subjective words. For example, the pat-
tern ?expressed <direct object>? often extracts subjec-
tive nouns, such as ?concern?, ?hope?, and ?support?.
Furthermore, these bootstrapping algorithms require only
a handful of seed words and unannotated texts for train-
ing; no annotated data is needed at all.
In this paper, we use the Meta-Bootstrapping and
Basilisk algorithms to learn lists of subjective nouns from
a large collection of unannotated texts. Then we train
a subjectivity classifier on a small set of annotated data,
using the subjective nouns as features along with some
other previously identified subjectivity features. Our ex-
perimental results show that the subjectivity classifier
performs well (77% recall with 81% precision) and that
the learned nouns improve upon previous state-of-the-art
subjectivity results (Wiebe et al, 1999).
2 Subjectivity Data
2.1 The Annotation Scheme
In 2002, an annotation scheme was developed
for a U.S. government-sponsored project with a
team of 10 researchers (the annotation instruc-
tions and project reports are available on the Web
at http://www.cs.pitt.edu/?wiebe/pubs/ardasummer02/).
                                                               Edmonton, May-June 2003
                                                    held at HLT-NAACL 2003 , pp. 25-32
                                            Proceeings of the Seventh CoNLL conference
The scheme was inspired by work in linguistics and
literary theory on subjectivity, which focuses on how
opinions, emotions, etc. are expressed linguistically in
context (Banfield, 1982). The scheme is more detailed
and comprehensive than previous ones. We mention only
those aspects of the annotation scheme relevant to this
paper.
The goal of the annotation scheme is to identify and
characterize expressions of private states in a sentence.
Private state is a general covering term for opinions, eval-
uations, emotions, and speculations (Quirk et al, 1985).
For example, in sentence (1) the writer is expressing a
negative evaluation.
(1) ?The time has come, gentlemen, for Sharon, the as-
sassin, to realize that injustice cannot last long.?
Sentence (2) reflects the private state of Western coun-
tries. Mugabe?s use of ?overwhelmingly? also reflects a
private state, his positive reaction to and characterization
of his victory.
(2) ?Western countries were left frustrated and impotent
after Robert Mugabe formally declared that he had over-
whelmingly won Zimbabwe?s presidential election.?
Annotators are also asked to judge the strength of each
private state. A private state can have low, medium, high
or extreme strength.
2.2 Corpus and Agreement Results
Our data consists of English-language versions of foreign
news documents from FBIS, the U.S. Foreign Broadcast
Information Service. The data is from a variety of publi-
cations and countries. The annotated corpus used to train
and test our subjectivity classifiers (the experiment cor-
pus) consists of 109 documents with a total of 2197 sen-
tences. We used a separate, annotated tuning corpus of
33 documents with a total of 698 sentences to establish
some experimental parameters.1
Each document was annotated by one or both of two
annotators, A and T. To allow us to measure interanno-
tator agreement, the annotators independently annotated
the same 12 documents with a total of 178 sentences. We
began with a strict measure of agreement at the sentence
level by first considering whether the annotator marked
any private-state expression, of any strength, anywhere
in the sentence. If so, the sentence should be subjective.
Otherwise, it is objective. Table 1 shows the contingency
table. The percentage agreement is 88%, and the ? value
is 0.71.
1The annotated data will be available to U.S. government
contractors this summer. We are working to resolve copyright
issues to make it available to the wider research community.
Tagger T
Subj Obj
Tagger A Subj nyy = 112 nyn = 16
Obj nny = 6 nnn = 44
Table 1: Agreement for sentence-level annotations
Tagger T
Subj Obj
Tagger A Subj nyy = 106 nyn = 9
Obj nny = 0 nnn = 44
Table 2: Agreement for sentence-level annotations, low-
strength cases removed
One would expect that there are clear cases of objec-
tive sentences, clear cases of subjective sentences, and
borderline sentences in between. The agreement study
supports this. In terms of our annotations, we define
a sentence as borderline if it has at least one private-
state expression identified by at least one annotator, and
all strength ratings of private-state expressions are low.
Table 2 shows the agreement results when such border-
line sentences are removed (19 sentences, or 11% of the
agreement test corpus). The percentage agreement in-
creases to 94% and the ? value increases to 0.87.
As expected, the majority of disagreement cases in-
volve low-strength subjectivity. The annotators consis-
tently agree about which are the clear cases of subjective
sentences. This leads us to define the gold-standard that
we use in our experiments. A sentence is subjective if it
contains at least one private-state expression of medium
or higher strength. The second class, which we call ob-
jective, consists of everything else. Thus, sentences with
only mild traces of subjectivity are tossed into the objec-
tive category, making the system?s goal to find the clearly
subjective sentences.
3 Using Extraction Patterns to Learn
Subjective Nouns
In the last few years, two bootstrapping algorithms have
been developed to create semantic dictionaries by ex-
ploiting extraction patterns: Meta-Bootstrapping (Riloff
and Jones, 1999) and Basilisk (Thelen and Riloff, 2002).
Extraction patterns were originally developed for infor-
mation extraction tasks (Cardie, 1997). They represent
lexico-syntactic expressions that typically rely on shal-
low parsing and syntactic role assignment. For example,
the pattern ?<subject> was hired? would apply to sen-
tences that contain the verb ?hired? in the passive voice.
The subject would be extracted as the hiree.
Meta-Bootstrapping and Basilisk were designed to
learn words that belong to a semantic category (e.g.,
?truck? is a VEHICLE and ?seashore? is a LOCATION).
Both algorithms begin with unannotated texts and seed
words that represent a semantic category. A bootstrap-
ping process looks for words that appear in the same ex-
traction patterns as the seeds and hypothesizes that those
words belong to the same semantic class. The principle
behind this approach is that words of the same semantic
class appear in similar pattern contexts. For example, the
phrases ?lived in? and ?traveled to? will co-occur with
many noun phrases that represent LOCATIONS.
In our research, we want to automatically identify
words that are subjective. Subjective terms have many
different semantic meanings, but we believe that the same
contextual principle applies to subjectivity. In this sec-
tion, we briefly overview these bootstrapping algorithms
and explain how we used them to generate lists of subjec-
tive nouns.
3.1 Meta-Bootstrapping
The Meta-Bootstrapping (?MetaBoot?) process (Riloff
and Jones, 1999) begins with a small set of seed words
that represent a targeted semantic category (e.g., 10
words that represent LOCATIONS) and an unannotated
corpus. First, MetaBoot automatically creates a set of ex-
traction patterns for the corpus by applying and instanti-
ating syntactic templates. This process literally produces
thousands of extraction patterns that, collectively, will ex-
tract every noun phrase in the corpus. Next, MetaBoot
computes a score for each pattern based upon the num-
ber of seed words among its extractions. The best pat-
tern is saved and all of its extracted noun phrases are
automatically labeled as the targeted semantic category.2
MetaBoot then re-scores the extraction patterns, using the
original seed words as well as the newly labeled words,
and the process repeats. This procedure is called mutual
bootstrapping.
A second level of bootstrapping (the ?meta-? boot-
strapping part) makes the algorithm more robust. When
the mutual bootstrapping process is finished, all nouns
that were put into the semantic dictionary are re-
evaluated. Each noun is assigned a score based on how
many different patterns extracted it. Only the five best
nouns are allowed to remain in the dictionary. The other
entries are discarded, and the mutual bootstrapping pro-
cess starts over again using the revised semantic dictio-
nary.
3.2 Basilisk
Basilisk (Thelen and Riloff, 2002) is a more recent boot-
strapping algorithm that also utilizes extraction patterns
to create a semantic dictionary. Similarly, Basilisk be-
gins with an unannotated text corpus and a small set of
2Our implementation of Meta-Bootstrapping learns individ-
ual nouns (vs. noun phrases) and discards capitalized words.
seed words for a semantic category. The bootstrapping
process involves three steps. (1) Basilisk automatically
generates a set of extraction patterns for the corpus and
scores each pattern based upon the number of seed words
among its extractions. This step is identical to the first
step of Meta-Bootstrapping. Basilisk then puts the best
patterns into a Pattern Pool. (2) All nouns3 extracted by a
pattern in the Pattern Pool are put into a Candidate Word
Pool. Basilisk scores each noun based upon the set of
patterns that extracted it and their collective association
with the seed words. (3) The top 10 nouns are labeled as
the targeted semantic class and are added to the dictio-
nary. The bootstrapping process then repeats, using the
original seeds and the newly labeled words.
The main difference between Basilisk and Meta-
Bootstrapping is that Basilisk scores each noun based
on collective information gathered from all patterns that
extracted it. In contrast, Meta-Bootstrapping identifies
a single best pattern and assumes that everything it ex-
tracted belongs to the same semantic class. The second
level of bootstrapping smoothes over some of the prob-
lems caused by this assumption. In comparative experi-
ments (Thelen and Riloff, 2002), Basilisk outperformed
Meta-Bootstrapping. But since our goal of learning sub-
jective nouns is different from the original intent of the
algorithms, we tried them both. We also suspected they
might learn different words, in which case using both al-
gorithms could be worthwhile.
3.3 Experimental Results
The Meta-Bootstrapping and Basilisk algorithms need
seed words and an unannotated text corpus as input.
Since we did not need annotated texts, we created a much
larger training corpus, the bootstrapping corpus, by gath-
ering 950 new texts from the FBIS source mentioned
in Section 2.2. To find candidate seed words, we auto-
matically identified 850 nouns that were positively corre-
lated with subjective sentences in another data set. How-
ever, it is crucial that the seed words occur frequently
in our FBIS texts or the bootstrapping process will not
get off the ground. So we searched for each of the 850
nouns in the bootstrapping corpus, sorted them by fre-
quency, and manually selected 20 high-frequency words
that we judged to be strongly subjective. Table 3 shows
the 20 seed words used for both Meta-Bootstrapping and
Basilisk.
We ran each bootstrapping algorithm for 400 itera-
tions, generating 5 words per iteration. Basilisk gener-
ated 2000 nouns and Meta-Bootstrapping generated 1996
nouns.4 Table 4 shows some examples of extraction pat-
3Technically, each head noun of an extracted noun phrase.
4Meta-Bootstrapping will sometimes produce fewer than 5
words per iteration if it has low confidence in its judgements.
cowardice embarrassment hatred outrage
crap fool hell slander
delight gloom hypocrisy sigh
disdain grievance love twit
dismay happiness nonsense virtue
Table 3: Subjective Seed Words
Extraction Patterns Examples of Extracted Nouns
expressed <dobj> condolences, hope, grief,
views, worries, recognition
indicative of <np> compromise, desire, thinking
inject <dobj> vitality, hatred
reaffirmed <dobj> resolve, position, commitment
voiced <dobj> outrage, support, skepticism,
disagreement, opposition,
concerns, gratitude, indignation
show of <np> support, strength, goodwill,
solidarity, feeling
<subject> was shared anxiety, view, niceties, feeling
Table 4: Extraction Pattern Examples
terns that were discovered to be associated with subjec-
tive nouns.
Meta-Bootstrapping and Basilisk are semi-automatic
lexicon generation tools because, although the bootstrap-
ping process is 100% automatic, the resulting lexicons
need to be reviewed by a human.5 So we manually re-
viewed the 3996 words proposed by the algorithms. This
process is very fast; it takes only a few seconds to classify
each word. The entire review process took approximately
3-4 hours. One author did this labeling; this person did
not look at or run tests on the experiment corpus.
Strong Subjective Weak Subjective
tyranny scum aberration plague
smokescreen bully allusion risk
apologist devil apprehensions drama
barbarian liar beneficiary trick
belligerence pariah resistant promise
condemnation venom credence intrigue
sanctimonious diatribe distortion unity
exaggeration mockery eyebrows failures
repudiation anguish inclination tolerance
insinuation fallacies liability persistent
antagonism evil assault trust
atrocities genius benefit success
denunciation goodwill blood spirit
exploitation injustice controversy slump
humiliation innuendo likelihood sincerity
ill-treatment revenge peaceful eternity
sympathy rogue pressure rejection
Table 5: Examples of Learned Subjective Nouns
5This is because NLP systems expect dictionaries to have
high integrity. Even if the algorithms could achieve 90% ac-
curacy, a dictionary in which 1 of every 10 words is defined
incorrectly would probably not be desirable.
B M B ? M B ? M
StrongSubj 372 192 110 454
WeakSubj 453 330 185 598
Total 825 522 295 1052
Table 6: Subjective Word Lexicons after Manual Review
(B=Basilisk, M=MetaBootstrapping)
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 200 400 600 800 1000 1200 1400 1600 1800 2000
%
 o
f W
or
ds
 S
ub
jec
tiv
e
Number of Words Generated
?Basilisk?
?MetaBoot?
Figure 1: Accuracy during Bootstrapping
We classified the words as StrongSubjective, WeakSub-
jective, or Objective. Objective terms are not subjective at
all (e.g., ?chair? or ?city?). StrongSubjective terms have
strong, unambiguously subjective connotations, such as
?bully? or ?barbarian?. WeakSubjective was used for
three situations: (1) words that have weak subjective con-
notations, such as ?aberration? which implies something
out of the ordinary but does not evoke a strong sense of
judgement, (2) words that have multiple senses or uses,
where one is subjective but the other is not. For example,
the word ?plague? can refer to a disease (objective) or an
onslaught of something negative (subjective), (3) words
that are objective by themselves but appear in idiomatic
expressions that are subjective. For example, the word
?eyebrows? was labeled WeakSubjective because the ex-
pression ?raised eyebrows? probably occurs more often
in our corpus than literal references to ?eyebrows?. Ta-
ble 5 shows examples of learned words that were classi-
fied as StrongSubjective or WeakSubjective.
Once the words had been manually classified, we could
go back and measure the effectiveness of the algorithms.
The graph in Figure 1 tracks their accuracy as the boot-
strapping progressed. The X-axis shows the number of
words generated so far. The Y-axis shows the percent-
age of those words that were manually classified as sub-
jective. As is typical of bootstrapping algorithms, ac-
curacy was high during the initial iterations but tapered
off as the bootstrapping continued. After 20 words,
both algorithms were 95% accurate. After 100 words
Basilisk was 75% accurate and MetaBoot was 81% accu-
rate. After 1000 words, accuracy dropped to about 28%
for MetaBoot, but Basilisk was still performing reason-
ably well at 53%. Although 53% accuracy is not high for
a fully automatic process, Basilisk depends on a human
to review the words so 53% accuracy means that the hu-
man is accepting every other word, on average. Thus, the
reviewer?s time was still being spent productively even
after 1000 words had been hypothesized.
Table 6 shows the size of the final lexicons created
by the bootstrapping algorithms. The first two columns
show the number of subjective terms learned by Basilisk
and Meta-Bootstrapping. Basilisk was more prolific, gen-
erating 825 subjective terms compared to 522 for Meta-
Bootstrapping. The third column shows the intersection
between their word lists. There was substantial overlap,
but both algorithms produced many words that the other
did not. The last column shows the results of merging
their lists. In total, the bootstrapping algorithms produced
1052 subjective nouns.
4 Creating Subjectivity Classifiers
To evaluate the subjective nouns, we trained a Naive
Bayes classifier using the nouns as features. We also in-
corporated previously established subjectivity clues, and
added some new discourse features. In this section, we
describe all the feature sets and present performance re-
sults for subjectivity classifiers trained on different com-
binations of these features. The threshold values and fea-
ture representations used in this section are the ones that
produced the best results on our separate tuning corpus.
4.1 Subjective Noun Features
We defined four features to represent the sets of subjec-
tive nouns produced by the bootstrapping algorithms.
BA-Strong: the set of StrongSubjective nouns generated
by Basilisk
BA-Weak: the set of WeakSubjective nouns generated
by Basilisk
MB-Strong: the set of StrongSubjective nouns generated
by Meta-Bootstrapping
MB-Weak: the set of WeakSubjective nouns generated
by Meta-Bootstrapping
For each set, we created a three-valued feature based on
the presence of 0, 1, or ? 2 words from that set. We used
the nouns as feature sets, rather than define a separate
feature for each word, so the classifier could generalize
over the set to minimize sparse data problems. We will
refer to these as the SubjNoun features.
4.2 Previously Established Features
Wiebe, Bruce, & O?Hara (1999) developed a machine
learning system to classify subjective sentences. We ex-
perimented with the features that they used, both to com-
pare their results to ours and to see if we could benefit
from their features. We will refer to these as the WBO
features.
WBO includes a set of stems positively correlated with
the subjective training examples (subjStems) and a set
of stems positively correlated with the objective training
examples (objStems). We defined a three-valued feature
for the presence of 0, 1, or ? 2 members of subjStems
in a sentence, and likewise for objStems. For our exper-
iments, subjStems includes stems that appear ? 7 times
in the training set, and for which the precision is 1.25
times the baseline word precision for that training set.
objStems contains the stems that appear ? 7 times and
for which at least 50% of their occurrences in the training
set are in objective sentences. WBO also includes a bi-
nary feature for each of the following: the presence in the
sentence of a pronoun, an adjective, a cardinal number, a
modal other than will, and an adverb other than not.
We also added manually-developed features found by
other researchers. We created 14 feature sets represent-
ing some classes from (Levin, 1993; Ballmer and Bren-
nenstuhl, 1981), some Framenet lemmas with frame ele-
ment experiencer (Baker et al, 1998), adjectives manu-
ally annotated for polarity (Hatzivassiloglou and McKe-
own, 1997), and some subjectivity clues listed in (Wiebe,
1990). We represented each set as a three-valued feature
based on the presence of 0, 1, or ? 2 members of the set.
We will refer to these as the manual features.
4.3 Discourse Features
We created discourse features to capture the density of
clues in the text surrounding a sentence. First, we com-
puted the average number of subjective clues and objec-
tive clues per sentence, normalized by sentence length.
The subjective clues, subjClues, are all sets for which
3-valued features were defined above (except objStems).
The objective clues consist only of objStems. For sen-
tence S, let ClueRatesubj(S) = |subjClues in S||S| and
ClueRateobj(S) = |objStems in S||S| . Then we define
AvgClueRatesubj to be the average of ClueRate(S)
over all sentences S and similarly for AvgClueRateobj.
Next, we characterize the number of subjective and
objective clues in the previous and next sentences as:
higher-than-expected (high), lower-than-expected (low),
or expected (medium). The value for ClueRatesubj(S)
is high if ClueRatesubj(S) ? AvgClueRatesubj ? 1.3;
low if ClueRatesubj(S) ? AvgClueRatesubj/1.3; oth-
erwise it is medium. The values for ClueRateobj(S) are
defined similarly.
Using these definitions we created four features:
ClueRatesubj for the previous and following sen-
tences, and ClueRateobj for the previous and follow-
ing sentences. We also defined a feature for sentence
length. Let AvgSentLen be the average sentence length.
SentLen(S) is high if length(S) ? AvgSentLen?1.3;
low if length(S) ? AvgSentLen/1.3; and medium oth-
erwise.
4.4 Classification Results
We conducted experiments to evaluate the performance
of the feature sets, both individually and in various com-
binations. Unless otherwise noted, all experiments in-
volved training a Naive Bayes classifier using a particu-
lar set of features. We evaluated each classifier using 25-
fold cross validation on the experiment corpus and used
paired t-tests to measure significance at the 95% confi-
dence level. As our evaluation metrics, we computed ac-
curacy (Acc) as the percentage of the system?s classifica-
tions that match the gold-standard, and precision (Prec)
and recall (Rec) with respect to subjective sentences.
Acc Prec Rec
(1) Bag-Of-Words 73.3 81.7 70.9
(2) WBO 72.1 76.0 77.4
(3) Most-Frequent 59.0 59.0 100.0
Table 7: Baselines for Comparison
Table 7 shows three baseline experiments. Row (3)
represents the common baseline of assigning every sen-
tence to the most frequent class. The Most-Frequent
baseline achieves 59% accuracy because 59% of the sen-
tences in the gold-standard are subjective. Row (2) is
a Naive Bayes classifier that uses the WBO features,
which performed well in prior research on sentence-level
subjectivity classification (Wiebe et al, 1999). Row (1)
shows a Naive Bayes classifier that uses unigram bag-of-
words features, with one binary feature for the absence
or presence in the sentence of each word that appeared
during training. Pang et al (2002) reported that a similar
experiment produced their best results on a related clas-
sification task. The difference in accuracy between Rows
(1) and (2) is not statistically significant (Bag-of-Word?s
higher precision is balanced by WBO?s higher recall).
Next, we trained a Naive Bayes classifier using only
the SubjNoun features. This classifier achieved good
precision (77%) but only moderate recall (64%). Upon
further inspection, we discovered that the subjective
nouns are good subjectivity indicators when they appear,
but not every subjective sentence contains one of them.
And, relatively few sentences contain more than one,
making it difficult to recognize contextual effects (i.e.,
multiple clues in a region). We concluded that the ap-
propriate way to benefit from the subjective nouns is to
use them in tandem with other subjectivity clues.
Acc Prec Rec
(1) 76.1 81.3 77.4 WBO+SubjNoun+
manual+discourse
(2) 74.3 78.6 77.8 WBO+SubjNoun
(3) 72.1 76.0 77.4 WBO
Table 8: Results with New Features
Table 8 shows the results of Naive Bayes classifiers
trained with different combinations of features. The ac-
curacy differences between all pairs of experiments in
Table 8 are statistically significant. Row (3) uses only
the WBO features (also shown in Table 7 as a baseline).
Row (2) uses the WBO features as well as the SubjNoun
features. There is a synergy between these feature sets:
using both types of features achieves better performance
than either one alone. The difference is mainly precision,
presumably because the classifier found more and better
combinations of features. In Row (1), we also added the
manual and discourse features. The discourse features
explicitly identify contexts in which multiple clues are
found. This classifier produced even better performance,
achieving 81.3% precision with 77.4% recall. The 76.1%
accuracy result is significantly higher than the accuracy
results for all of the other classifiers (in both Table 8 and
Table 7).
Finally, higher precision classification can be obtained
by simply classifying a sentence as subjective if it con-
tains any of the StrongSubjective nouns. On our data, this
method produces 87% precision with 26% recall. This
approach could support applications for which precision
is paramount.
5 Related Work
Several types of research have involved document-level
subjectivity classification. Some work identifies inflam-
matory texts (e.g., (Spertus, 1997)) or classifies reviews
as positive or negative ((Turney, 2002; Pang et al, 2002)).
Tong?s system (Tong, 2001) generates sentiment time-
lines, tracking online discussions and creating graphs of
positive and negative opinion messages over time. Re-
search in genre classification may include recognition of
subjective genres such as editorials (e.g., (Karlgren and
Cutting, 1994; Kessler et al, 1997; Wiebe et al, 2001)).
In contrast, our work classifies individual sentences, as
does the research in (Wiebe et al, 1999). Sentence-level
subjectivity classification is useful because most docu-
ments contain a mix of subjective and objective sen-
tences. For example, newspaper articles are typically
thought to be relatively objective, but (Wiebe et al, 2001)
reported that, in their corpus, 44% of sentences (in arti-
cles that are not editorials or reviews) were subjective.
Some previous work has focused explicitly on learn-
ing subjective words and phrases. (Hatzivassiloglou and
McKeown, 1997) describes a method for identifying the
semantic orientation of words, for example that beauti-
ful expresses positive sentiments. Researchers have fo-
cused on learning adjectives or adjectival phrases (Tur-
ney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe,
2000) and verbs (Wiebe et al, 2001), but no previous
work has focused on learning nouns. A unique aspect
of our work is the use of bootstrapping methods that ex-
ploit extraction patterns. (Turney, 2002) used patterns
representing part-of-speech sequences, (Hatzivassiloglou
and McKeown, 1997) recognized adjectival phrases, and
(Wiebe et al, 2001) learned N-grams. The extraction
patterns used in our research are linguistically richer pat-
terns, requiring shallow parsing and syntactic role assign-
ment.
In recent years several techniques have been developed
for semantic lexicon creation (e.g., (Hearst, 1992; Riloff
and Shepherd, 1997; Roark and Charniak, 1998; Cara-
ballo, 1999)). Semantic word learning is different from
subjective word learning, but we have shown that Meta-
Bootstrapping and Basilisk could be successfully applied
to subjectivity learning. Perhaps some of these other
methods could also be used to learn subjective words.
6 Conclusions
This research produced interesting insights as well as per-
formance results. First, we demonstrated that weakly
supervised bootstrapping techniques can learn subjec-
tive terms from unannotated texts. Subjective features
learned from unannotated documents can augment or en-
hance features learned from annotated training data us-
ing more traditional supervised learning techniques. Sec-
ond, Basilisk and Meta-Bootstrapping proved to be use-
ful for a different task than they were originally intended.
By seeding the algorithms with subjective words, the ex-
traction patterns identified expressions that are associated
with subjective nouns. This suggests that the bootstrap-
ping algorithms should be able to learn not only general
semantic categories, but any category for which words
appear in similar linguistic phrases. Third, our best sub-
jectivity classifier used a wide variety of features. Sub-
jectivity is a complex linguistic phenomenon and our evi-
dence suggests that reliable subjectivity classification re-
quires a broad array of features.
References
C. Baker, C. Fillmore, and J. Lowe. 1998. The berkeley
framenet project. In Proceedings of the COLING-ACL.
T. Ballmer and W. Brennenstuhl. 1981. Speech Act Clas-
sification: A Study in the Lexical Analysis of English
Speech Activity Verbs. Springer-Verlag.
A. Banfield. 1982. Unspeakable Sentences. Routledge
and Kegan Paul, Boston.
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 120?126.
C. Cardie. 1997. Empirical Methods in Information Ex-
traction. AI Magazine, 18(4):65?79.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In ACL-EACL
1997.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th Interna-
tional Conference on Computational Linguistics.
J. Karlgren and D. Cutting. 1994. Recognizing text gen-
res with simple metrics using discriminant analysis. In
COLING-94.
B. Kessler, G. Nunberg, and H. Schutze. 1997. Auto-
matic detection of text genre. In Proc. ACL-EACL-97.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? sentiment classification using machine learning
techniques. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman, New York.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the 16th National Conference on Ar-
tificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124.
B. Roark and E. Charniak. 1998. Noun-phrase
Co-occurrence Statistics for Semi-automatic Seman-
tic Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110?1116.
E. Spertus. 1997. Smokey: Automatic recognition of
hostile messages. In Proc. IAAI.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing.
R. Tong. 2001. An operational system for detecting and
tracking opinions in on-line discussion. In SIGIR 2001
Workshop on Operational Text Classification.
P. Turney. 2002. Thumbs Up or Thumbs Down? Seman-
tic Orientation Applied to Unsupervised Classification
of Reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics.
J. Wiebe, R. Bruce, and T. O?Hara. 1999. Development
and use of a gold standard data set for subjectivity clas-
sifications. In Proc. 37th Annual Meeting of the Assoc.
for Computational Linguistics (ACL-99).
J. Wiebe, T. Wilson, and M. Bell. 2001. Identifying col-
locations for recognizing opinions. In Proc. ACL-01
Workshop on Collocation: Computational Extraction,
Analysis, and Exploitation, July.
J. Wiebe. 1990. Recognizing Subjective Sentences: A
Computational Investigation of Narrative Text. Ph.D.
thesis, State University of New York at Buffalo.
J. Wiebe. 2000. Learning subjective adjectives from cor-
pora. In 17th National Conference on Artificial Intelli-
gence.
Annotating Opinions in the World Press
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260, USA
twilson@cs.pitt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260, USA
wiebe@cs.pitt.edu
Abstract
In this paper we present a detailed
scheme for annotating expressions of
opinions, beliefs, emotions, sentiment
and speculation (private states) in the
news and other discourse. We explore
inter-annotator agreement for individ-
ual private state expressions, and show
that these low-level annotations are use-
ful for producing higher-level subjec-
tive sentence annotations.
1 Introduction
In this paper we present a detailed scheme for
annotating expressions of opinions, beliefs, emo-
tions, sentiment, speculation and other private
states in newspaper articles. Private state is a
general term that covers mental and emotional
states, which cannot be directly observed or ver-
ified (Quirk et al, 1985). For example, we can
observe evidence of someone else being happy,
but we cannot directly observe their happiness.
In natural language, opinions, emotions and other
private states are expressed using subjective lan-
guage (Banfield, 1982; Wiebe, 1994).
Articles in the news are composed of a mix-
ture of factual and subjective material. Writers
of editorials frequently include facts to support
their arguments, and news reports often mix seg-
ments presenting objective facts with segments
presenting opinions and verbal reactions (van
Dijk, 1988). However, natural language pro-
cessing applications that retrieve or extract infor-
mation from or that summarize or answer ques-
tions about news and other discourse have fo-
cused primarily on factual information and thus
could benefit from knowledge of subjective lan-
guage. Traditional information extraction and in-
formation retrieval systems could learn to concen-
trate on objectively presented factual information.
Question answering systems could identify when
an answer is speculative rather than certain. In
addition, knowledge of how opinions and other
private states are realized in text would directly
support new tasks, such as opinion-oriented in-
formation extraction (Cardie et al, 2003). The
ability to extract opinions when they appear in
documents would benefit multi-document sum-
marization systems seeking to summarize differ-
ent opinions and perspectives, as well as multi-
perspective question-answering systems trying to
answer opinion-based questions.
The annotation scheme we present in this paper
was developed as part of a U.S. government-
sponsored project (ARDA AQUAINT NRRC)1
to investigate multiple perspectives in question
answering (Wiebe et al, 2003). We implemented
the scheme in GATE2, a General Architecture
for Text Engineering (Cunningham et al, 2002).
General instructions for annotating opinions and
specific instructions for downloading and using
GATE to perform the annotations are available at
1This work was performed in support of the Northeast
Regional Research Center (NRRC) which is sponsored by
the Advanced Research and Development Activity in In-
formation Technology (ARDA), a U.S. Government entity
which sponsors and promotes research of import to the In-
telligence Community which includes but is not limited to
the CIA, DIA, NSA, NIMA, and NRO.
2GATE is freely available from the University of
Sheffield at http://gate.ac.uk.
http://www.cs.pitt.edu/ ?wiebe/pubs/ardasummer02.
The annotated data will be available to U.S. gov-
ernment contractors this summer. We are working
to resolve copyright issues to make it available to
the wider research community.
In developing this annotation scheme, we had
two goals. The first was to develop a represen-
tation for opinions and other private states that
was built on work in linguistics and literary the-
ory on subjectivity (please see (Banfield, 1982;
Fludernik, 1993; Wiebe, 1994; Stein and Wright,
1995) for references). The study of subjectiv-
ity in language focuses on how private states are
expressed linguistically in context. Our second
goal was to develop an annotation scheme that
would be useful for corpus-based research on
subjective language and for the development of
applications such as multi-perspective question-
answering systems. The annotation scheme that
resulted is more detailed and comprehensive than
previous ones for subjective language.
Our study of the annotations produced by the
annotation scheme gives two important results.
First, we find that trained annotators can consis-
tently perform detailed opinion annotations with
good agreement (0.81 Kappa). Second, the agree-
ment results are better than in previous sentence-
level annotation studies, suggesting that adding
detail can help the annotators perform more re-
liably.
In the sections that follow, we first review how
opinions and other private states are expressed in
language (section 2) and give a brief overview
of previous work in subjectivity tagging (section
3). We then describe our annotation scheme for
private state expressions (section 4) and give the
results of an annotation study (section 5). We
conclude with a discussion of our findings from
the annotation study and and future work (section
??). In the appendix, we give sample annotations
as well as a snapshot of the annotations in GATE.
2 Expressing Private States in Text
2.1 Private States, Speech Events, and
Expressive Subjective Elements
There are two main ways that private states are
expressed in language. Private states may be ex-
plicitly mentioned, or they may be expressed in-
directly by the types of words and the style of lan-
guage that a speaker or writer uses. An example
of an explicitly-mentioned private state is ?frus-
trated? in sentence (1).
(1) Western countries were left frus-
trated and impotent after Robert
Mugabe formally declared that he
had overwhelmingly won Zimbabwe?s
presidential election.
Although most often verbs, it is interesting to note
that explicit mentions of private states may also
be nouns, such as ?concern? in ?international con-
cern? and ?will? in ?will of the people.? They may
even be adjectives, such as ?fearful? in ?fearful
populace.?
The second way that private states are generally
expressed is indirectly using expressive subjective
elements (Banfield, 1982). For example, the pri-
vate states in sentences (2) and (3) are expressed
entirely by the words and the style of language
that is used.
(2) The time has come, gentlemen, for
Sharon, the assassin, to realize that in-
justice cannot last long.
(3) ?We foresaw electoral fraud but not
daylight robbery,? Tsvangirai said.
In (2), although the writer does not explicitly
say that he hates Sharon, his choice of words
clearly demonstrates a negative attitude. In sen-
tence (3), describing the election as ?daylight rob-
bery? clearly reflects the anger being experienced
by the speaker, Tsvangirai. As used in these sen-
tences, the phrases ?The time has come,? ?gentle-
men,? ?the assassin,? ?injustice cannot last long,?
?fraud,? and ?daylight robbery? are all expressive
subjective elements. Expressive subjective ele-
ments are used by people to express their frus-
tration, anger, wonder, positive sentiment, mirth,
etc., without explicitly stating that they are frus-
trated, angry, etc. Sarcasm and irony often in-
volve expressive subjective elements.
When looking for opinions and other private
states in text, an annotator must consider speech
events as well as explicitly-mentioned private
states. In this work, we use speech event to refer
to any event of speaking or writing. However, the
mere presence of a speech event does not indicate
a private state. Both sentences (3) above and (4)
below contain speech events indicated by ?said.?
As mentioned previously, sentence (3) is opinion-
ated, while in (4) the information is presented as
factual.
(4) Medical Department head Dr
Hamid Saeed said the patient?s blood
had been sent to the Institute for
Virology in Johannesburg for analysis.
For speech terms such as ?said,? ?added,? ?told,?
?announce,? and ?report,? an annotator deter-
mines if there is a private state mainly by looking
inside the scope of the speech term for expressive
subjective elements.
Occasionally, we also find private states that
are expressed by direct physical actions. We call
such actions private state actions. Examples are
booing someone, sighing heavily, shaking ones
fist angrily, waving ones hand dismissively, and
frowning. ?Applauding? in sentence (5) is an ex-
ample of a positive-evaluative private state action.
(5) As the long line of would-be voters
marched in, those near the front of the
queue began to spontaneously applaud
those who were far behind them.
2.2 Nested Sources
An important aspect of a private state or speech
event is its source. The source of a speech event
is the speaker or writer. The source of a private
state is the experiencer of the private state, i.e.,
the person whose opinion or emotion is being ex-
pressed. Obviously, the writer of an article is a
source, because he wrote the sentences compos-
ing the article, but the writer may also write about
other people?s private states and speech events,
leading to multiple sources in a single sentence.
For example, each of the following sentences has
two sources: the writer (because he wrote the sen-
tences), and Sue (because she is the source of a
speech event in (6) and of private states in (7) and
(8), namely thinking and being afraid).
(6) Sue said, ?The election was fair.?
(7) Sue thinks that the election was fair.
(8) Sue is afraid to go outside.
Note, however, that we don?t really know what
Sue says, thinks or feels. All we know is what the
writer tells us. Sentence (6), for example, does
not directly present Sue?s speech event but rather
Sue?s speech event according to the writer. Thus,
we have a natural nesting of sources in a sentence.
The nesting of sources may be quite deep and
complex. For example, consider sentence (9).
(9) The Foreign Ministry said Thursday
that it was ?surprised, to put it mildly?
by the U.S. State Department?s criti-
cism of Russia?s human rights record
and objected in particular to the ?odi-
ous? section on Chechnya.
There are three sources in this sentence: the
writer, the Foreign Ministry, and the U.S. State
Department. The writer is the source of the over-
all sentence. The remaining explicitly mentioned
private states and speech events in (9) have the
following nested sources:
said: (writer, Foreign Ministry)
surprised, to put it mildly:
(writer, Foreign Ministry, Foreign Ministry)
criticism:
(writer, Foreign Ministry, U.S. State Dept.)
objected: (writer, Foreign Ministry)
Expressive subjective elements may also have
nested sources. In sentence (9), ?to put it mildly?
and ?odious? are expressive subjective elements,
both with nested source (writer, Foreign Min-
istry). We might expect that an expressive subjec-
tive element always has the same nested source
as the immediately dominating private state or
speech term. Although this is the case for ?odi-
ous? in (9) (the nested source of ?odious? and
?objected? is the same), it is not the same for ?big-
ger than Jesus? in (10):
(10) ?It is heresy,? said Cao. ?The
?Shouters? claim they are bigger than
Jesus.?
The nested source of the subjectivity expressed
by ?bigger than Jesus? is Cao, while the nested
source of ?claim? is (writer, Cao, Shouters).3
3(10) is an example of a de re rather than de dicto propo-
sitional attitude report (Rapaport, 1986).
3 Previous Work on Subjectivity
Tagging
In previous work (Wiebe et al, 1999), a corpus of
sentences from the Wall Street Journal Treebank
Corpus (Marcus et al, 1993) was manually anno-
tated with subjectivity classifications by multiple
judges. The judges were instructed to classify a
sentence as subjective if it contained any signif-
icant expressions of subjectivity, attributed to ei-
ther the writer or someone mentioned in the text,
and to classify the sentence as objective, other-
wise. The judges rated the certainty of their an-
swers on a scale from 0 to 3.
Agreement in the study was summarized in
terms of Cohen?s Kappa (   ) (Cohen, 1960),
which compares the total probability of agree-
ment to that expected if the taggers? classifica-
tions were statistically independent (i.e., ?chance
agreement?). After two rounds of tagging by
three judges, an average pairwise   value of 0.69
was achieved on a test set. On average, the judges
rated 15% of the sentences as very uncertain (rat-
ing 0). When these sentences are removed, the
average pairwise   value is 0.79. When sentences
with uncertainty judgment 0 or 1 are removed (on
average 30% of the sentences), the average pair-
wise   is 0.88.
4 An Annotation Scheme for Private
States
The annotation scheme described in this section
is more detailed and comprehensive the previ-
ous ones for subjective language. In (Wiebe et
al., 1999), summary subjective/objective judg-
ments were performed at the sentence level. For
this work, annotators are asked to mark within
each sentence the word spans that indicate speech
events or that are expressions of private states.
For every span that an annotator marks, there are
a number of attributes the annotator may set to
characterize the annotation.
The annotation scheme has two main com-
ponents. The first is an annotation type for
explicitly-mentioned private states and speech
events. The second is an annotation type for ex-
pressive subjective elements. Table 1 lists the at-
tributes that may be assigned to these two types
of annotations. In addition, there is an annotation
Explicit private states/speech events
nested-source
onlyfactive: yes, no
overall-strength: low, medium, high, extreme
on-strength: neutral, low, medium, high, extreme
attitude-type: positive, negative, both (exploratory)
attitude-toward (exploratory)
is-implicit
minor
Expressive subjective elements
nested-source
strength: low, medium, high, extreme
attitude-type: positive, negative, other (exploratory)
Table 1: Attributes for the two main annotation
types. For attributes that take on one of a fixed set
of values, the set of possible values are given.
type, agent, that annotators may use to mark the
noun phrase (if one exists) of the source of a pri-
vate state or speech event.
4.1 Explicitly-mentioned Private State and
Speech Event Annotations
An important part of the annotation scheme is
represented by the onlyfactive attribute. This at-
tribute is marked on every private state and speech
event annotation. The onlyfactive attribute is used
to indicate whether the source of the private state
or speech event is indeed expressing an emo-
tion, opinion or other private state. By defini-
tion, any expression that is an explicit private state
(e.g., ?think?, ?believe,? ?hope,? ?want?) or a pri-
vate state mixed with speech (e.g., ?berate,? ?ob-
ject,? ?praise?) is onlyfactive=no. On the other
hand, neutral speech events (e.g., ?said,? ?added,?
?told?) may be either onlyfactive=yes or onlyfac-
tive=no, depending on their contents. For ex-
ample, the annotation for ?said? in sentence (3)
would be marked onlyfactive=no, but the annota-
tion for ?said? in sentence (4) would be marked
onlyfactive=yes (sentences in section 2).
Note that even if onlyfactive=no, the sentence
may express something the nested source believes
is factual. Consider the sentence ?John criti-
cized Mary for smoking.? John expresses a private
state (his negative evaluation of Mary?s smoking).
However, this does not mean that John does not
believe that Mary smokes.
Like the onlyfactive attribute, the nested-source
attribute is included on every private state and
speech event annotation. The nested source (i.e.,
(writer, Foreign Ministry, U.S. State Dept.)) is
typed in by the annotator.
When an annotation is marked onlyfactive=no,
additional attributes are used to characterize the
private state. The overall-strength attribute is
used to indicate the overall strength of the pri-
vate state (considering the explicit private state
or speech event phrase as well as everything in-
side its scope). It?s value may range from low
to extreme. The on-strength attribute is used to
measure the contribution made specifically by the
explicit private state or speech event phrase. For
example, the on-strength of ?said? is typically
neutral, the on-strength of ?criticize? is typically
medium, and the on-strength of ?vehemently de-
nied? is typically high or extreme. (As for all as-
pects of this annotation scheme, the annotators
are asked to make these judgments in context.)
A speech event that is onlyfactive=yes has on-
strength=neutral and no overall-strength. Thus,
there is no need to include the overall-strength
and on-strength attributes for onlyfactive=yes an-
notations.
4.1.1 Implicit Speech Event Annotations
Implicit speech events posed a problem when
we developed the annotation scheme. Implicit
speech events are speech events in the discourse
for which there is no explicit speech event phrase,
and thus no obvious place to attach the anno-
tation. For example, most of the writer?s sen-
tences do not include a phrase such as ?I say.?
Also, direct quotes are not always accompanied
by discourse parentheticals (such as ?, she said?).
Our solution was to add the is-implicit attribute to
the annotation type for private states and speech
events, which may then be used to mark implicit
speech event annotations.
4.1.2 Minor Private States and Speech
Events
Depending on its goals, an application may
need to identify all private state and speech event
expressions in a document, or it may want to find
only those opinions and other private states that
are significant and real in the discourse. By ?sig-
nificant?, we mean that a significant portion of the
contents of the private state or speech event are
given within the sentence where the annotation
is marked. By ?real?, we mean that the private
state or speech event is presented as an existing
event within the domain of discourse, e.g., it is
not hypothetical. We use the term minor for pri-
vate states and speech events that are not signif-
icant or not real. Annotators mark minor private
state and speech event annotations by including
the minor attribute.
The following sentences all contain one or
more minor private states or speech events (high-
lighted in bold).
(11) Such wishful thinking risks mak-
ing the US an accomplice in the de-
struction of human rights. (not signif-
icant)
(12) If the Europeans wish to influence
Israel in the political arena... (in a con-
ditional, so not real)
(13) ?And we are seeking a declara-
tion that the British government de-
mands that Abbasi should not face trial
in a military tribunal with the death
penalty.? (not real, i.e., the declaration
of the demand is just being sought)
(14) The official did not say how many
prisoners were on the flight. (not real
because the saying event did not occur)
(15) No one who has ever studied realist
political science will find this surpris-
ing. (not real since a specific ?surprise?
state is not referred to; note that the
subject noun phrase is attributive rather
than referential (Donnellan, 1966))
4.2 Expressive Subjective Element
Annotations
As with private state/speech event annotations,
the nested-source attribute is included on every
expressive subjective element annotation. In ad-
dition to marking the source of an expression, the
nested-source is also functioning as a link. Within
a sentence, the nested-source chains together all
the pieces that together indicate the overall pri-
vate state of a particular source.
In addition to nested-source, the strength at-
tribute is used to characterize expressive subjec-
tive element annotations. The strength of an ex-
pressive subjective element may range from low
to extreme (see Table 1).
4.3 Exploratory Attributes
We are exploring additional attributes that allow
an annotator to further characterize the type of
attitude being expressed by a private state. An
annotator may use the attitude-type attribute to
mark an onlyfactive=no private state/speech event
annotation or an expressive subjective element
annotation as positive or negative. An attitude-
toward attribute may also be included on private
state/speech event annotations to indicate the par-
ticular target of an evaluation, emotion, etc.
5 Annotation Study
The data in our study consists of English-
language versions of foreign news documents
from FBIS, the U.S. Foreign Broadcast Informa-
tion Service. The data is from a variety of publi-
cations and countries. To date, 252 articles have
been annotated with the scheme described in sec-
tion 4.
To measure agreement on various aspects of the
annotation scheme, three annotators (A, M, and
S) independently annotated 13 documents with a
total of 210 sentences. None of the annotators are
authors of this paper. The articles are from a vari-
ety of topics and were selected so that 1/3 of the
sentences are from news articles reporting on ob-
jective topics (objective articles), 1/3 of the sen-
tences are from news articles reporting on opin-
ionated topics (?hot-topic? articles), and 1/3 of
the sentences are from editorials.
In the instructions to the annotators, we asked
them to rate the annotation difficulty of each arti-
cle on a scale from 1 to 3, with 1 being the eas-
iest and 3 being the most difficult. The annota-
tors were not told which articles were objective
or which articles were editorials, only that they
were being given a variety of different articles to
annotate.
We hypothesized that the editorials would be
the hardest to annotate and that the objective ar-
ticles would be the easiest. The ratings that the
annotators assigned to the articles support this hy-
pothesis. The annotators rated an average of 44%
of the articles in the study as easy (rating 1) and
26% as difficult (rating 3). But, they rated an av-
erage of 73% of the objective articles as easy, and
89% of the editorials as difficult.
It makes intuitive sense that ?hot-topic? articles
would be more difficult to annotate than objective
articles and that editorials would be more difficult
still. Editorials and ?hot-topic? articles contain
many more expressions of private states, requir-
ing an annotator to make more judgments than
they would for objective articles.
5.1 Agreement for Expressive Subjective
Element Annotations
For annotations that involve marking spans of
text, such as expressive subjective element an-
notations, it is not unusual for two annotators to
identify the same expression in the text, but to
differ in how they mark the boundaries.4 For
example, both annotators A and M saw expres-
sive subjectivity in the phrase, ?such a disadvan-
tageous situation.? But, while A marked the entire
phrase as a single expressive subjective element,
M marked the individual words, ?such? and ?dis-
advantageous.? Because the annotators will iden-
tify a different number of annotations, as well as
different (but hopefully strongly overlapping) sets
of expressions, we need an agreement metric that
can measure agreement between sets of objects.
We use the   metric to measure agreement
for expressive subjective elements (and later for
private state/speech event annotations).
  is a directional measure of agreement. Let

and  be the sets of spans annotated by anno-
tators   and 	 . We compute the agreement of 	 to
  as:
 
 	







This measure of agreement corresponds to the no-
tion of precision and recall as used to evaluate, for
example, named entity recognition. The  
 	
metric corresponds to the recall if   is the gold-
standard and 	 the system, and to precision, if they
are reversed.
In the 210 sentences in the annotation study, the
annotators A, M, and S respectively marked 311,
352 and 249 expressive subjective elements. Ta-
ble 2 shows the pairwise agreement for these sets
of annotations. For example, M agrees with 76%
of the expressive subjective elements marked by
4In the coding instructions, we did not attempt to define
rules to try to enforce boundary agreement.
mother of terrorism
if the world has to rid itself from this menace, the perpetrators across the border had to be dealt with firmly
indulging in blood-shed and their lunaticism
ultimately the demon they have reared will eat up their own vitals
Table 3: Extreme strength expressive subjective elements
     	
  
  average
A M 0.76 0.72
A S 0.68 0.81
M S 0.59 0.74
0.72
Table 2: Inter-annotator Agreement: Expressive
subjective elements
A, and A agrees with 72% of the expressive
subjective elements marked by M. The average
agreement in Table 2 is the arithmetic mean of all
six   .
We hypothesized that the stronger the expres-
sion of subjectivity, the more likely the annota-
tors are to agree. To test this hypothesis, we mea-
sure agreement for the expressive subjective ele-
ments rated with a strength of medium or higher
by at least one annotator. This excludes on av-
erage 29% of the expressive subjective elements.
The average pairwise agreement rises to 0.80.
When measuring agreement for the expressive
subjective elements rated high or extreme, this ex-
cludes an average 65% of expressive subjective
elements, and the average pairwise agreement in-
creases to 0.88. Thus, annotators are more likely
to agree when the expression of subjectivity is
strong. Table 3 gives examples of expressive sub-
jective elements that at least one annotator rated
as extreme.
5.2 Agreement for Private State/Speech
Event Annotations
For private state and speech event annotations, we
again use   to measure agreement between the
sets of expressions identified by each annotator.
The three annotators, A, M, and S, respectively
marked 338, 285, and 315 explicit expressions of
private states and speech events. Implicit speech
events for the writer of course are excluded. Table
4 shows the pairwise agreement for these sets of
annotations.
The average pairwise agreement for explicit
private state and speech event expressions is 0.82,
     	    average
A M 0.75 0.91
A S 0.80 0.85
M S 0.86 0.75
0.82
Table 4: Inter-annotator Agreement: Explicitly-
mentioned private states and speech events
which indicates that they are easier to annotate
than expressive subjective elements.
5.3 Agreement for Attributes
In this section, we focus on the annotators? agree-
ment for judgments that reflect whether or not
an opinion, emotion, sentiment, speculation, or
other private state is being expressed. We con-
sider these judgments to be at the core of the an-
notation scheme. Two attributes, onlyfactive and
on-strength, carry information about whether a
private state is being expressed.
For onlyfactive judgments, we measure pair-
wise agreement between annotators for the set
of private state and speech event annotations that
both annotators identified. Because we are now
measuring agreement over the same set of objects
for each annotator, we use Kappa (   ) to capture
how well the annotators agree.
Table 5 shows the contingency table for the on-
lyfactive judgments made by annotators A and M.
The Kappa scores for all annotator pairs are given
in Table 7. For their onlyfactive judgments, i.e.,
whether or not an opinion or other private state
is being expressed, the annotators have an aver-
age pairwise Kappa of 0.81. Under Krippendorf?s
scale (Krippendorf, 1980), this allows for definite
conclusions.
With many judgments that characterize natural
language, one would expect that there are clear
cases as well as borderline cases, which would be
more difficult to judge. The agreement study in-
dicates that this is certainly true for private states.
In terms of our annotations, we define an explicit
private state or speech event to be borderline-
    

 
	
 
  

  Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 53?60,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Annotating Attributions and Private States
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
twilson@cs.pitt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This paper describes extensions to a corpus
annotation scheme for the manual annotation
of attributions, as well as opinions, emotions,
sentiments, speculations, evaluations and other
private states in language. It discusses the
scheme with respect to the ?Pie in the Sky?
Check List of Desirable Semantic Information
for Annotation. We believe that the scheme is a
good foundation for adding private state anno-
tations to other layers of semantic meaning.
1 Introduction
This paper describes a fine-grained annotation scheme
for key components and properties of opinions, emo-
tions, sentiments, speculations, evaluations, and other
private states in text. We first give an overview of the
core scheme. We then describe recent extensions to the
scheme, namely refined annotations of attitudes and tar-
gets, or objects, of private states. Finally, we discuss re-
lated items from the ?Pie in the Sky? Check List of De-
sirable Semantic Information for Annotation, and related
work. We believe our scheme would provide a founda-
tion for adding private state annotations to other layers of
semantic and pragmatic meaning.
2 The Core Scheme
This section overviews the core of the annotation scheme.
Further details may be found in (Wilson and Wiebe,
2003; Wiebe et al, 2005).
2.1 Means of Expressing Private States
The goals of the annotation scheme are to represent inter-
nal mental and emotional states, and to distinguish sub-
jective information from material presented as fact. As
a result, the annotation scheme is centered on the no-
tion of private state, a general term that covers opinions,
beliefs, thoughts, feelings, emotions, goals, evaluations,
and judgments. As Quirk et al (1985) define it, a private
state is a state that is not open to objective observation
or verification: ?a person may be observed to assert that
God exists, but not to believe that God exists. Belief is in
this sense ?private?.? (p. 1181) Following literary theo-
rists such as Banfield (1982), we use the term subjectivity
for linguistic expressions of private states in the contexts
of texts and conversations.
We can further view private states in terms of their
functional components ? as states of experiencers hold-
ing attitudes, optionally toward targets. For example, for
the private state in the sentence John hates Mary, the ex-
periencer is ?John,? the attitude is ?hate,? and the target
is ?Mary.?
We create private state frames for three main types of
private state expressions in text:
? explicit mentions of private states
? speech events expressing private states
? expressive subjective elements
An example of an explicit mention of a private state is
?fears? in (1):
(1) ?The U.S. fears a spill-over,? said Xirao-
Nima.
An example of a speech event expressing a private state
is ?said? in (2):
(2) ?The report is full of absurdities,? Xirao-
Nima said.
Note that we use the term speech event to refer to both
speaking and writing events.
The phrase ?full of absurdities? in (2) above is an ex-
pressive subjective element (Banfield, 1982). Other ex-
amples can be found in (3):
(3) The time has come, gentlemen, for
Sharon, the assassin, to realize that injustice
cannot last long.
53
The private states in this sentence are expressed entirely
by the words and the style of language that is used. In
(3), although the writer does not explicitly say that he
hates Sharon, his choice of words clearly demonstrates
a negative attitude toward him. As used in these sen-
tences, the phrases ?The time has come,? ?gentlemen,?
?the assassin,? and ?injustice cannot last long? are all ex-
pressive subjective elements. Expressive subjective el-
ements are used by people to express their frustration,
anger, wonder, positive sentiment, etc., without explic-
itly stating that they are frustrated, angry, etc. Sarcasm
and irony often involve expressive subjective elements.
2.2 Private State Frames
We propose two types of private state frames: expressive
subjective element frames will be used to represent
expressive subjective elements; and direct subjective
frames will be used to represent both subjective speech
events (i.e., speech events expressing private states) and
explicitly mentioned private states. The frames have the
following attributes:
Direct subjective (subjective speech event or explicit
private state) frame:
? text anchor: a pointer to the span of text that rep-
resents the speech event or explicit mention of a pri-
vate state.
? source: the person or entity that expresses or expe-
riences the private state, possibly the writer.
? target: the target or topic of the private state, i.e.,
what the speech event or private state is about.
? properties:
? intensity: the intensity of the private state (low,
medium, high, or extreme).
? expression intensity: the contribution of the
speech event or private state expression itself
to the overall intensity of the private state. For
example, ?say? is often neutral, even if what is
uttered is not neutral, while ?excoriate? itself
implies a very strong private state.
? insubstantial: true, if the private state is not
substantial in the discourse. For example, a pri-
vate state in the context of a conditional often
has the value true for attribute insubstantial.
? attitude type: the type of attitude(s) compos-
ing the private state.
Expressive subjective element frame:
? text anchor: a pointer to the span of text that de-
notes the subjective or expressive phrase.
? source: the person or entity that is expressing the
private state, possibly the writer.
? properties:
? intensity: the intensity of the private state.
? attitude type
2.3 Objective Speech Event Frames
To distinguish opinion-oriented material from material
presented as factual, we also define objective speech
event frames. These are used to represent material that is
attributed to some source, but is presented as objective
fact. They include a subset of the slots in private state
frames:
Objective speech event frame:
? text anchor: a pointer to the span of text that de-
notes the speech event.
? source: the speaker or writer.
? target: the target or topic of the speech event, i.e.,
the content of what is said.
For example, an objective speech event frame is cre-
ated for ?said? in the following sentence (assuming no
undue influence from the context):
(4) Sargeant O?Leary said the incident took
place at 2:00pm.
That the incident took place at 2:00pm is presented as a
fact with Sargeant O?Leary as the source of information.
2.4 Agent Frames
The annotation scheme includes an agent frame for noun
phrases that refer to sources of private states and speech
events, i.e., for all noun phrases that act as the experi-
encer of a private state, or the speaker/writer of a speech
event. Each agent frame generally has two slots. The text
anchor slot includes a pointer to the span of text that de-
notes the noun phrase source. The source slot contains
a unique alpha-numeric ID that is used to denote this
source throughout the document. The agent frame as-
sociated with the first informative (e.g., non-pronominal)
reference to this source in the document includes an id
slot to set up the document-specific source-id mapping.
2.5 Nested Sources
The source of a speech event is the speaker or writer. The
source of a private state is the experiencer of the private
state, i.e., the person whose opinion or emotion is being
expressed. The writer of an article is always a source, be-
cause he or she wrote the sentences of the article, but the
writer may also write about other people?s private states
54
and speech events, leading to multiple sources in a single
sentence. For example, each of the following sentences
has two sources: the writer (because he or she wrote the
sentences), and Sue (because she is the source of a speech
event in (5) and of private states in (6) and (7)).
(5) Sue said, ?The election was fair.?
(6) Sue thinks that the election was fair.
(7) Sue is afraid to go outside.
Note, however, that we don?t really know what Sue says,
thinks or feels. All we know is what the writer tells us.
For example, Sentence (5) does not directly present Sue?s
speech event but rather Sue?s speech event according to
the writer. Thus, we have a natural nesting of sources in
a sentence.
In particular, private states are often filtered through
the ?eyes? of another source, and private states are of-
ten directed toward the private states of others. Consider
sentence (1) above and (8) following:
(8) China criticized the U.S. report?s criticism
of China?s human rights record.
In sentence (1), the U.S. does not directly state its fear.
Rather, according to the writer, according to Xirao-Nima,
the U.S. fears a spill-over. The source of the private state
expressed by ?fears? is thus the nested source ?writer,
Xirao-Nima, U.S.?. In sentence (8), the U.S. report?s crit-
icism is the target of China?s criticism. Thus, the nested
source for ?criticism? is ?writer, China, U.S. report?.
Note that the shallowest (left-most) agent of all nested
sources is the writer, since he or she wrote the sentence.
In addition, nested source annotations are composed of
the IDs associated with each source, as described in
the previous subsection. Thus, for example, the nested
source ?writer, China, U.S. report? would be represented
using the IDs associated with the writer, China, and the
report being referred to, respectively.
2.6 Examples
We end this section with examples of direct subjective,
expressive subjective element, and objective speech event
frames (sans target and attitude type attributes, which are
discussed in the next section).
First, we show the frames that would be associated
with sentence (9), assuming that the relevant source ID?s
have already been defined:
(9) ?The US fears a spill-over,? said Xirao-
Nima.
Objective speech event:
Text anchor: the entire sentence
Source: <writer>
Implicit: true
Objective speech event:
Text anchor: said
Source: <writer,Xirao-Nima>
Direct subjective:
Text anchor: fears
Source: <writer,Xirao-Nima,U.S.>
Intensity: medium
Expression intensity: medium
The first objective speech event frame represents that, ac-
cording to the writer, it is true that Xirao-Nima uttered
the quote and is a professor at the university referred
to. The implicit attribute is included because the writer?s
speech event is not explicitly mentioned in the sentence
(i.e., there is no explicit phrase such as ?I write?).
The second objective speech event frame represents
that, according to the writer, according to Xirao-Nima, it
is true that the US fears a spillover. Finally, when we drill
down to the subordinate clause we find a private state: the
US fear of a spillover. Such detailed analyses, encoded
as annotations on the input text, would enable a person
or an automated system to pinpoint the subjectivity in a
sentence, and attribute it appropriately.
Now, consider sentence (10):
(10) ?The report is full of absurdities,? Xirao-
Nima said.
Objective speech event:
Text anchor: the entire sentence
Source: <writer>
Implicit: true
Direct subjective:
Text anchor: said
Source: <writer,Xirao-Nima>
Intensity: high
Expression intensity: neutral
Expressive subjective element:
Text anchor: full of absurdities
Source: <writer,Xirao-Nima>
Intensity: high
The objective frame represents that, according to the
writer, it is true that Xirao-Nima uttered the quoted string.
The second frame is created for ?said? because it is a sub-
jective speech event: private states are conveyed in what
is uttered. Note that intensity is high but expression inten-
sity is neutral: the private state being expressed is strong,
but the specific speech event phrase ?said? does not it-
self contribute to the intensity of the private state. The
third frame is for the expressive subjective element ?full
of absurdities.?
3 Annotation Process
To date, over 11,000 sentences in 550 documents have
been annotated according to the annotation scheme de-
scribed above. The documents are English-language ver-
sions of news documents from the world press. The doc-
uments are from 187 different news sources in a variety
55
of countries. The original documents and their annota-
tions are available at
http://nrrc.mitre.org/NRRC/publications.htm.
The annotation process and inter-annotator agreement
studies are described in (Wiebe et al, 2005). Here, we
want to highlight two themes of the annotation instruc-
tions:
1. There are no fixed rules about how particular words
should be annotated. The instructions describe the
annotations of specific examples, but do not state
that specific words should always be annotated a cer-
tain way.
2. Sentences should be interpreted with respect to the
contexts in which they appear. The annotators
should not take sentences out of context and think
what they could mean, but rather should judge them
as they are being used in that particular sentence and
document.
We believe that these general strategies for annotation
support the creation of corpora that will be useful for
studying expressions of subjectivity in context.
4 Extensions: Attitude and Target
Annotations
Before we describe the new attitude and target annota-
tions, consider the following sentence.
(11) ?I think people are happy because Chavez
has fallen.?
This sentence contains two private states, represented by
direct subjective annotations anchored on ?think? and
?happy,? respectively.
The word ?think? is used to express an opinion about
what is true according to its source (a positive arguing
attitude type; see Section 4.1). The target of ?think? is
?people are happy because Chavez has fallen.?
The word ?happy? clearly expresses a positive attitude,
with target ?Chavez has fallen.? However, looking more
closely at the private state for ?happy,? we see that we
can also infer a negative attitude toward Chavez, from
the phrase ?happy because Chavez has fallen.?
Sentence (11) illustrates some of the things we need to
consider when representing attitudes and targets. First,
we see that more than one type of attitude may be in-
volved when a private state is expressed. In (11), there
are three (a positive attitude, a negative attitude, and a
positive arguing attitude). Second, more than one target
may be associated with a private state. Consider ?happy?
in (11). The target of the positive attitude is ?Chavez has
fallen,? while the target of the inferred negative attitude
is ?Chavez.?
Positive Attitudes Positive Arguing
Negative Attitudes Negative Arguing
Positive Intentions Speculation
Negative Intentions Other Attitudes
Table 1: Attitude Types
The representation also must support multiple targets
for a single attitude, as illustrated by Sentence (12):
(12) Tsvangirai said the election result was a
clear case of highway robbery by Mugabe, his
government and his party, Zanu-PF.
In (12), the phrase ?a clear case of highway robbery? ex-
presses a negative attitude of Tsvangirai. This negative
attitude has two targets: ?the election results? and ?Mu-
gabe, his government and his party, Zanu-PF.?
To capture the kind of detailed attitude and target in-
formation that we described above, we propose two new
types of annotations: attitude frames and target frames.
We describe these new annotations in Sections 4.2 and
4.3, but first we introduce the set of attitude types that we
developed for the annotation scheme.
4.1 Types of Attitudes
One of our goals in extending the annotation scheme for
private states was to develop a set of attitude types that
would be useful for NLP applications. It it also important
that the set of attitude types provide good coverage for the
range of possible private states. Working with our anno-
tators and looking at the private states already annotated,
we developed the set of attitude types listed in Table 1.
Below we give a brief description of each attitude
type, followed by an example. In each example, the span
of text that expresses the attitude type is in bold, and the
span of text that refers to the target of the attitude type (if
a target is given) is in angle brackets.
Positive Attitudes: positive emotions, evaluations, judg-
ments and stances.
(13) The Namibians went as far as to say
?Zimbabwe?s election system? was ?water
tight, without room for rigging?.
Negative Attitudes: negative emotions, evaluations,
judgments and stances.
(14) His disenfranchised supporters were
seething.
Positive Arguing: arguing for something, arguing that
something is true or so, arguing that something did hap-
pen or will happen, etc.
56
(15) Iran insists ?its nuclear program is purely
for peaceful purposes?.
Negative Arguing: arguing against something, arguing
that something is not true or not so, arguing that some-
thing did not happen or will not happen, etc.
(16) Officials in Panama denied that ?Mr.
Chavez or any of his family members had asked
for asylum?.
Positive Intentions: aims, goals, plans, and other overtly
expressed intentions.
(17) The Republic of China government be-
lieves in the US committment ?to separating
its anti-terrorism campaign from the Taiwan
Strait issue?, an official said Thursday.
Negative Intentions: expressing that something is not an
aim, not a goal, not an intention, etc.
(18) The Bush administration has no plans ?to
ease sanctions against mainland China?.
Speculation: speculation or uncertainty about what may
or may not be true, what may or may not happen, etc.
(19) ?The president is likely to endorse the
bill?.
Other Attitudes: other types of attitudes that do not fall
into one of the above categories.
(20) To the surprise of many, ?the dollar hit
only 2.4 pesos and closed at 2.1?.
4.2 Attitude Frames
With the introduction of the attitude frames, two issues
arise. First, which spans of text should the new atti-
tudes be anchored to? Second, how do we tie the attitude
frames back to the private states that they are part of?
The following sentence illustrates the first issue.
(21) The MDC leader said systematic cheating,
spoiling tactics, rigid new laws, and shear ob-
struction - as well as political violence and in-
timidation - were just some of the irregularities
practised by the authorities in the run-up to, and
during the poll.
In (21), there are 5 private state frames attributed
to the MDC leader: a direct subjective frame an-
chored to ?said,? and four expressive subjective ele-
ment frames anchored respectively to ?systematic cheat-
ing . . . obstruction,? ?as well as,? ?violence and intimida-
tion,? and ?just some of the irregularities.? We could cre-
ate an attitude frame for each of these private state frames,
but we believe the following is a better solution. For each
direct subjective frame, the annotator is asked to consider
the direct subjective annotation and everything within the
scope of the annotation when deciding what attitude types
are being expressed by the source of the direct subjective
frame. Then, for each attitude type identified, the an-
notator creates an attitude frame and anchors the frame
to whatever span of text completely captures the attitude
type. In to sentence (21), this results in just one attitude
frame being created to represent the negative attitude of
the MDC leader. The anchor for this attitude frame begins
with ?systematic cheating? and ends with ?irregularities.?
Turning to the second issue, tying attitude frames to
their private states, we do two things. First, we create a
unique ID for the attitude frame. Then, we change the
attitude type attribute on the direct subjective annotation
into a new attribute called an attitude link. We place the
attitude frame ID into the attitude link slot. The attitude
link slot can hold more then one attitude frame ID, allow-
ing us to represent a private state composed of more than
one type of attitude.
Because we expect the attitude annotations to overlap
with most of the expressive subjective element annota-
tions, we chose not to link attitude frames to expressive
subjective element frames. However, this would be pos-
sible to do should it become necessary.
The attitude frame has the following attributes:
Attitude frame:
? id: a unique alphanumeric ID for identifying the at-
titude annotation. The ID is used to link the attitude
annotation to the private state it is part of.
? text anchor: a pointer to the span of text that cap-
tures the attitude being expressed.
? attitude type: one of the attitude types listed in Ta-
ble 1.
? target link: one or more target annotation IDs (see
Section 4.3).
? intensity: the intensity of the attitude.
? properties:
? inferred: true, if the attitude is inferred.
? sarcastic: true, if the attitude is realized
through sarcasm.
? repetition: true, if the attitude is realized
through the repetition of words, phrases, or
syntax.
? contrast: true, if the attitude is realized only
through contrast with another attitude.
57
Of the four attitude-frame properties, inferred was al-
ready discussed. The property sarcastic marks attitudes
expressed using sarcasm. In general, we think this prop-
erty will be of interest for NLP applications working with
opinions. Detecting sarcasm may also help a system learn
to distinguish between positive and negative attitudes.
The sarcasm in Sentence (22), below, makes the word
?Great? an expression of a negative rather than a positive
attitude.
(22) ?Great, keep on buying dollars so there?ll
be more and more poor people in the country,?
shouted one.
The repetition and contrast properties are also for mark-
ing different ways in which an attitude might be realized.
We feel these properties will be useful for developing an
automatic system for recognizing different types of atti-
tudes.
4.3 Target Frames
The target frame is used to mark the target of each atti-
tude. A target frame has two slots, the id slot and the text
anchor slot. The id slot contains a unique alpha-numeric
ID for identifying the target annotation. We use the target
frame ID to link the target back to the attitude frame. The
attitude frame has a target-link slot that can hold one or
more target frame IDs. This allows us to represent when
a single attitude is directed at more than one target.
The text anchor slot has a pointer to the span of text that
denotes the target. If there is more than one reference to
the target in the sentence, the most syntactically relevant
reference is chosen.
To illustrate what we mean by syntactically relevant,
consider the following sentence.
(23) African observers generally approved of
?his victory? while Western governments de-
nounced ?it?.
The target of the two attitudes (in bold) in the above sen-
tence is the same entity in the discourse. However, al-
though we anchor the target for the first attitude to ?his
victory,? the anchor for the target of the second attitude is
the pronoun ?it.? As the direct object of the span that de-
notes the attitude ?denounced,? ?it? is more syntactically
relevant than ?his victory.?
4.4 Illustrative Examples
Figures 4.4 and 4.4 give graphical representations for the
annotations in sentences (11) and (12). With attitude
frame and target frame extensions, we are able to capture
more detail about the private states being expressed in the
text than the original core scheme presented in (Wiebe et
al., 2005).
5 Pie in the Sky Annotation
Among the items on the ?Pie in the Sky? Check List
of Desirable Semantic Information for Annotation, 1 the
most closely related are epistemic values (?attitude??),
epistemic, deontic, and personal attitudes. These all
fundamentally involve a self (Banfield, 1982), a subject
of consciousness who is the source of knowledge as-
sessments, judgments of certainty, judgments of obliga-
tion/permission, personal attitudes, and so on. Any ex-
plicit epistemic, deontic, or personal attitude expressions
are represented by us as private state frames, either direct
subjective frames (e.g., for verbs such as ?know? refer-
ring to an epistemic state) or expressive subjective ele-
ment frames (e.g., for modals such as ?must? or ?ought
to?). Importantly, many deontic, epistemic, and personal
attitude expressions do not directly express the speaker
or writer?s subjectivity, but are attributed by the speaker
or writer to agents mentioned in the text (consider, e.g.,
?John believes that Mary should quit her job?). Our frame
and nested-source representations were designed to sup-
port attributing subjectivity to appropriate sources. In fu-
ture work, additional attributes could be added to private
state frames to distinguish between, for example, deontic
and epistemic usages of ?must? and to represent different
epistemic values.
Other phenomena on the list overlap with subjectivity,
such as modality and social style/register. As mentioned
above, some modal expressions are subjective, such as
those expressing deontic or epistemic judgments. How-
ever, hypotheticals and future expressions need not be
subjective. For example, ?The company announced that
if its profits decrease in the next quarter, it will lay off
some employees? may easily be interpreted as presenting
objective fact. As for style, some are subjective by their
nature. One is the literary style represented thought, used
to present consciousness in fiction (Cohn, 1978; Banfield,
1982). Others are sarcastic or dismissive styles of speak-
ing or writing. In our annotation scheme, sentences per-
ceived to represent a character?s consciousness are repre-
sented with private-state frames, as are expressions per-
ceived to be sarcastic or dismissive. On the other hand,
some style distinctions, such as degree of formality, are
often realized in other ways than with explicit subjective
expressions (e.g., ?can?t? versus ?cannot?).
Polarity, another item on the checklist, also overlaps
with subjective positive and negative attitude types. Al-
though many negative and positive polarity words are sel-
dom used outside subjective expressions (such as ?hate?
and ?love?), others often are. For example, words such
as ?addicted? and ?abandoned? are included as negative
polarity terms in the General Inquirer lexicon (General-
Inquirer, 2000), but they can easily appear in objective
1Available at: http://nlp.cs.nyu.edu/meyers/frontiers/2005.html
58
 direct subjective frame
   text anchor: think
   source: <writer, I>
   intensity: medium
   expression intensity: medium
   attitude link: a10
 attitude frame
   id: a10
   text anchor: think 
   attitude type: positive arguing
   intensity: medium
   target link: t10
 direct subjective frame
   text anchor: are happy
   source: <writer, I, people>
   intensity: medium
   expression intensity: medium
   attitude link: a20    , a30
 target frame
   id: t30 
   text anchor: Chavez
 attitude frame
   id: a20
   text anchor: are happy
   attitude type: positive attitude
   intensity: medium
   target link: t20  target frame
   id: t20
   text anchor: Chavez has fallen
 target frame
   id: t10
   text anchor: people are happy 
     because Chavez has fallen 
  
 attitude frame
   id: a30
   text anchor: are happy because 
      Chavez has fallen
   attitude type: negative attitude
   intensity: medium
   inferred: true
   target link: t30
 objective speech event
   text anchor: the entire sentence
   source: <writer>
   implicit: true
Figure 1: Graphical representation of annotations for Sentence (11)
 direct subjective frame
   text anchor: said
   source: <writer, Tsvangirai>
   intensity: high
   expression intensity: neutral
   attitude link: a40
 attitude frame
   id: a40
   text anchor: clear case of highway robbery 
   attitude type: negative attitude
   intensity: high
   target link: t40    , t45
 target frame
   id: t40
   text anchor: election result
 target frame
   id: t45
   text anchor: Mugabe, his government 
       and his party, Zanu-PF
 objective speech event
   text anchor: the entire sentence
   source: <writer>
   implicit: true
 expressive subjective element frame
   source: <writer, Tsvangirai>
   text anchor: clear case of highway robbery 
   intensity: high
   
Figure 2: Graphical representation of annotations for Sentence (12)
59
sentences (e.g., ?Thomas De Quincy was addicted to
opium and lived in an abandoned shack?).
Integrating subjectivity with other layers of annotation
proposed in the ?Pie in the Sky? project would afford the
opportunity to investigate how they interact. It would
also enrich our subjectivity representations. While our
scheme promises to be a good base, much remains to be
added. For example, annotations of thematic roles and
co-reference would add needed structure to the target an-
notations, which are now only spans of text. In addi-
tion, temporal and modal annotations would flesh out the
insubstantial attribute, which is currently only a binary
marker. Furthermore, individual private state expressions
must be integrated with respect to the discourse context.
For example, which expressions of opinions oppose ver-
sus support one another? Which sentences presented as
objective fact are included to support a subjective opin-
ion? A challenging dimension to add to the ?Pie in the
Sky? project would be the deictic center as conceived of
in (Duchan et al, 1995), which consists of here, now, and
I reference points updated as the text or conversation un-
folds. Our annotation scheme was developed with this
framework in mind.
6 Related Work
The work most similar to ours is Appraisal Theory (Mar-
tin, 2000; White, 2002) from systemic functional linguis-
tics (see Halliday (19851994)). Both Appraisal Theory
and our annotation scheme are concerned with identify-
ing and characterizing expressions of opinions and emo-
tions in context. The two schemes, however, make differ-
ent distinctions. Appraisal Theory distinguishes different
types of positive and negative attitudes and also various
types of ?intersubjective positioning? such as attribution
and expectation. Appraisal Theory does not distinguish,
as we do, the different ways that private states may be ex-
pressed (i.e., directly, or indirectly using expressive sub-
jective elements). It also does not include a representa-
tion for nested levels of attribution.
In addition to Appraisal Theory, subjectivity annota-
tion of text in context has also been performed in Yu and
Hatzivassiloglou (2003), Bruce and Wiebe (1999), and
Wiebe et al (2004). The annotations in Yu and Hatzi-
vassiloglou (2003) are sentence-level subjective vs. ob-
jective and polarity judgments. The annotation schemes
used in Bruce and Wiebe (1999) and Wiebe et al (2004)
are earlier, much less detailed versions of the annotation
scheme presented in this paper.
7 Conclusion
We have described extensions to an annotation scheme
for private states and objective speech events in lan-
guage. We look forward to integrating and elaborating
this scheme with other layers of semantic meaning in the
future.
8 Acknowledgments
This work was supported in part by the National Sci-
ence Foundation under grant IIS-0208798 and by the Ad-
vanced Research and Development Activity (ARDA).
References
A. Banfield. 1982. Unspeakable Sentences. Routledge and
Kegan Paul, Boston.
R. Bruce and J. Wiebe. 1999. Recognizing subjectivity: A case
study of manual tagging. Natural Language Engineering,
5(2):187?205.
D. Cohn. 1978. Transparent Minds: Narrative Modes for
Representing Consciousness in Fiction. Princeton Univer-
sity Press, Princeton, NJ.
J. Duchan, G. Bruder, and L. Hewitt, editors. 1995. Deixis
in Narrative: A Cognitive Science Perspective. Lawrence
Erlbaum Associates.
The General-Inquirer. 2000.
http://www.wjh.harvard.edu/?inquirer/spreadsheet guide.htm.
M.A.K. Halliday. 1985/1994. An Introduction to Functional
Grammar. London: Edward Arnold.
J.R. Martin. 2000. Beyond exchange: APPRAISAL systems
in English. In Susan Hunston and Geoff Thompson, editors,
Evaluation in Text: Authorial stance and the construction of
discourse, pages 142?175. Oxford: Oxford University Press.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A
Comprehensive Grammar of the English Language. Long-
man, New York.
P.R.R. White. 2002. Appraisal: The language of attitudi-
nal evaluation and intersubjective stance. In Verschueren,
Ostman, blommaert, and Bulcaen, editors, The Handbook
of Pragmatics, pages 1?27. Amsterdam/Philadelphia: John
Benjamins Publishing Company.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin. 2004.
Learning subjective language. Computational Linguistics,
30(3):277?308.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expres-
sions of opinions and emotions in language. Language Re-
sources and Evalution (formerly Computers and the Human-
ities), 1(2).
T. Wilson and J. Wiebe. 2003. Annotating opinions in the
world press. In SIGdial-03.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identifying
the polarity of opinion sentences. In EMNLP-2003.
60
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 109?116, New York City, June 2006. c?2006 Association for Computational Linguistics
Which Side are You on? Identifying Perspectives at the Document and
Sentence Levels
Wei-Hao Lin
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
whlin@cs.cmu.edu
Theresa Wilson, Janyce Wiebe
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
{twilson,wiebe}@cs.pitt.edu
Alexander Hauptmann
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
alex@cs.cmu.edu
Abstract
In this paper we investigate a new problem
of identifying the perspective from which
a document is written. By perspective we
mean a point of view, for example, from
the perspective of Democrats or Repub-
licans. Can computers learn to identify
the perspective of a document? Not every
sentence is written strongly from a per-
spective. Can computers learn to identify
which sentences strongly convey a partic-
ular perspective? We develop statistical
models to capture how perspectives are
expressed at the document and sentence
levels, and evaluate the proposed mod-
els on articles about the Israeli-Palestinian
conflict. The results show that the pro-
posed models successfully learn how per-
spectives are reflected in word usage and
can identify the perspective of a document
with high accuracy.
1 Introduction
In this paper we investigate a new problem of au-
tomatically identifying the perspective from which
a document is written. By perspective we mean
a ?subjective evaluation of relative significance, a
point-of-view.?1 For example, documents about the
Palestinian-Israeli conflict may appear to be about
the same topic but reveal different perspectives:
1The American Heritage Dictionary of the English Lan-
guage, 4th ed.
(1) The inadvertent killing by Israeli forces of
Palestinian civilians ? usually in the course of
shooting at Palestinian terrorists ? is
considered no different at the moral and ethical
level than the deliberate targeting of Israeli
civilians by Palestinian suicide bombers.
(2) In the first weeks of the Intifada, for example,
Palestinian public protests and civilian
demonstrations were answered brutally by
Israel, which killed tens of unarmed protesters.
Example 1 is written from an Israeli perspective;
Example 2 is written from a Palestinian perspec-
tive. Anyone knowledgeable about the issues of
the Israeli-Palestinian conflict can easily identify the
perspectives from which the above examples were
written. However, can computers learn to identify
the perspective of a document given a training cor-
pus?
When an issue is discussed from different per-
spectives, not every sentence strongly reflects the
perspective of the author. For example, the follow-
ing sentences were written by a Palestinian and an
Israeli.
(3) The Rhodes agreements of 1949 set them as
the ceasefire lines between Israel and the Arab
states.
(4) The green line was drawn up at the Rhodes
Armistice talks in 1948-49.
Examples 3 and 4 both factually introduce the back-
ground of the issue of the ?green line? without ex-
pressing explicit perspectives. Can we develop a
109
system to automatically discriminate between sen-
tences that strongly indicate a perspective and sen-
tences that only reflect shared background informa-
tion?
A system that can automatically identify the per-
spective from which a document is written will be
a valuable tool for people analyzing huge collec-
tions of documents from different perspectives. Po-
litical analysts regularly monitor the positions that
countries take on international and domestic issues.
Media analysts frequently survey broadcast news,
newspapers, and weblogs for differing viewpoints.
Without the assistance of computers, analysts have
no choice but to read each document in order to iden-
tify those from a perspective of interest, which is ex-
tremely time-consuming. What these analysts need
is to find strong statements from different perspec-
tives and to ignore statements that reflect little or no
perspective.
In this paper we approach the problem of learning
individual perspectives in a statistical framework.
We develop statistical models to learn how perspec-
tives are reflected in word usage, and we treat the
problem of identifying perspectives as a classifica-
tion task. Although our corpus contains document-
level perspective annotations, it lacks sentence-level
annotations, creating a challenge for learning the
perspective of sentences. We propose a novel sta-
tistical model to overcome this problem. The ex-
perimental results show that the proposed statisti-
cal models can successfully identify the perspective
from which a document is written with high accu-
racy.
2 Related Work
Identifying the perspective from which a document
is written is a subtask in the growing area of au-
tomatic opinion recognition and extraction. Sub-
jective language is used to express opinions, emo-
tions, and sentiments. So far, research in automatic
opinion recognition has primarily addressed learn-
ing subjective language (Wiebe et al, 2004; Riloff
et al, 2003), identifying opinionated documents (Yu
and Hatzivassiloglou, 2003) and sentences (Yu and
Hatzivassiloglou, 2003; Riloff et al, 2003), and dis-
criminating between positive and negative language
(Pang et al, 2002; Morinaga et al, 2002; Yu and
Hatzivassiloglou, 2003; Turney and Littman, 2003;
Dave et al, 2003; Nasukawa and Yi, 2003; Popescu
and Etzioni, 2005; Wilson et al, 2005). While by its
very nature we expect much of the language that is
used when presenting a perspective or point-of-view
to be subjective, labeling a document or a sentence
as subjective is not enough to identify the perspec-
tive from which it is written. Moreover, the ideol-
ogy and beliefs authors possess are often expressed
in ways other than positive or negative language to-
ward specific targets.
Research on the automatic classification of movie
or product reviews as positive or negative (e.g.,
(Pang et al, 2002; Morinaga et al, 2002; Turney
and Littman, 2003; Nasukawa and Yi, 2003; Mullen
and Collier, 2004; Beineke et al, 2004; Hu and Liu,
2004)) is perhaps the most similar to our work. As
with review classification, we treat perspective iden-
tification as a document-level classification task, dis-
criminating, in a sense, between different types of
opinions. However, there is a key difference. A pos-
itive or negative opinion toward a particular movie
or product is fundamentally different from an overall
perspective. One?s opinion will change from movie
to movie, whereas one?s perspective can be seen as
more static, often underpinned by one?s ideology or
beliefs about the world.
There has been research in discourse analysis that
examines how different perspectives are expressed
in political discourse (van Dijk, 1988; Pan et al,
1999; Geis, 1987). Although their research may
have some similar goals, they do not take a compu-
tational approach to analyzing large collections of
documents. To the best of our knowledge, our ap-
proach to automatically identifying perspectives in
discourse is unique.
3 Corpus
Our corpus consists of articles published on the
bitterlemonswebsite2. The website is set up to
?contribute to mutual understanding [between Pales-
tinians and Israelis] through the open exchange of
ideas.?3 Every week an issue about the Israeli-
Palestinian conflict is selected for discussion (e.g.,
2http://www.bitterlemons.org
3http://www.bitterlemons.org/about/
about.html
110
?Disengagement: unilateral or coordinated??), and
a Palestinian editor and an Israeli editor each con-
tribute one article addressing the issue. In addition,
the Israeli and Palestinian editors invite one Israeli
and one Palestinian to express their views on the
issue (sometimes in the form of an interview), re-
sulting in a total of four articles in a weekly edi-
tion. We choose the bitterlemons website for
two reasons. First, each article is already labeled
as either Palestinian or Israeli by the editors, allow-
ing us to exploit existing annotations. Second, the
bitterlemons corpus enables us to test the gen-
eralizability of the proposed models in a very real-
istic setting: training on articles written by a small
number of writers (two editors) and testing on arti-
cles from a much larger group of writers (more than
200 different guests).
We collected a total of 594 articles published on
the website from late 2001 to early 2005. The dis-
tribution of documents and sentences are listed in
Table 1. We removed metadata from all articles, in-
Palestinian Israeli
Written by editors 148 149
Written by guests 149 148
Total number of documents 297 297
Average document length 740.4 816.1
Number of sentences 8963 9640
Table 1: The basic statistics of the corpus
cluding edition numbers, publication dates, topics,
titles, author names and biographic information. We
used OpenNLP Tools4 to automatically extract sen-
tence boundaries, and reduced word variants using
the Porter stemming algorithm.
We evaluated the subjectivity of each sentence us-
ing the automatic subjective sentence classifier from
(Riloff and Wiebe, 2003), and find that 65.6% of
Palestinian sentences and 66.2% of Israeli sentences
are classified as subjective. The high but almost
equivalent percentages of subjective sentences in the
two perspectives support our observation in Sec-
tion 2 that a perspective is largely expressed using
subjective language, but that the amount of subjec-
tivity in a document is not necessarily indicative of
4http://sourceforge.net/projects/
opennlp/
its perspective.
4 Statistical Modeling of Perspectives
We develop algorithms for learning perspectives us-
ing a statistical framework. Denote a training corpus
as a set of documents Wn and their perspectives la-
bels Dn, n = 1, . . . ,N , where N is the total number
of documents in the corpus. Given a new document
W? with a unknown document perspective, the per-
spective D? is calculated based on the following con-
ditional probability.
P (D?|W? , {Dn,Wn}Nn=1) (5)
We are also interested in how strongly each sen-
tence in a document conveys perspective informa-
tion. Denote the intensity of the m-th sentence of
the n-th document as a binary random variable Sm,n.
To evaluate Sm,n, how strongly a sentence reflects
a particular perspective, we calculate the following
conditional probability.
P (Sm,n|{Dn,Wn}Nn=1) (6)
4.1 Na??ve Bayes Model
We model the process of generating documents from
a particular perspective as follows:
pi ? Beta(?pi, ?pi)
? ? Dirichlet(??)
Dn ? Binomial(1, pi)
Wn ? Multinomial(Ln, ?d)
First, the parameters pi and ? are sampled once from
prior distributions for the whole corpus. Beta and
Dirichlet are chosen because they are conjugate pri-
ors for binomial and multinomial distributions, re-
spectively. We set the hyperparameters ?pi, ?pi, and
?? to one, resulting in non-informative priors. A
document perspective Dn is then sampled from a bi-
nomial distribution with the parameter pi. The value
of Dn is either d0 (Israeli) or d1 (Palestinian). Words
in the document are then sampled from a multino-
mial distribution, where Ln is the length of the doc-
ument. A graphical representation of the model is
shown in Figure 1.
111
pi ?
Dn Wn
N
Figure 1: Na??ve Bayes Model
The model described above is commonly known
as a na??ve Bayes (NB) model. NB models have
been widely used for various classification tasks,
including text categorization (Lewis, 1998). The
NB model is also a building block for the model
described later that incorporates sentence-level per-
spective information.
To predict the perspective of an unseen document
using na??ve Bayes , we calculate the posterior distri-
bution of D? in (5) by integrating out the parameters,
? ?
P (D?, pi, ?|{(Dn,Wn)}Nn=1, W? )dpid? (7)
However, the above integral is difficult to compute.
As an alternative, we use Markov Chain Monte
Carlo (MCMC) methods to obtain samples from the
posterior distribution. Details about MCMC meth-
ods can be found in Appendix A.
4.2 Latent Sentence Perspective Model
We introduce a new binary random variable, S, to
model how strongly a perspective is reflected at the
sentence level. The value of S is either s1 or s0,
where s1 indicates a sentence is written strongly
from a perspective while s0 indicates it is not. The
whole generative process is modeled as follows:
pi ? Beta(?pi, ?pi)
? ? Beta(?? , ?? )
? ? Dirichlet(??)
Dn ? Binomial(1, pi)
Sm,n ? Binomial(1, ?)
Wm,n ? Multinomial(Lm,n, ?)
The parameters pi and ? have the same semantics as
in the na??ve Bayes model. S is naturally modeled as
a binomial variable, where ? is the parameter of S.
S represents how likely it is that a sentence strongly
conveys a perspective. We call this model the La-
tent Sentence Perspective Model (LSPM) because S
is not directly observed. The graphical model repre-
sentation of LSPM is shown in Figure 2.
pi ? ?
Dn
Sm,n Wm,n
N
Mn
Figure 2: Latent Sentence Perspective Model
To use LSPM to identify the perspective of a new
document D? with unknown sentence perspectives S?,
we calculate posterior probabilities by summing out
possible combinations of sentence perspective in the
document and parameters.
? ? ?
?
Sm,n
?
S?
P (D?, Sm,n, S?, pi, ?, ?| (8)
{(Dn,Wn)}Nn=1, W? )dpid?d?
As before, we resort to MCMC methods to sample
from the posterior distributions, given in Equations
(5) and (6).
As is often encountered in mixture models, there
is an identifiability issue in LSPM. Because the val-
ues of S can be permuted without changing the like-
lihood function, the meanings of s0 and s1 are am-
biguous. In Figure 3a, four ? values are used to rep-
resent the four possible combinations of document
perspective d and sentence perspective intensity s. If
we do not impose any constraints, s1 and s0 are ex-
changeable, and we can no longer strictly interpret
s1 as indicating a strong sentence-level perspective
and s0 as indicating that a sentence carries little or
no perspective information. The other problem of
this parameterization is that any improvement from
LSPM over the na??ve Bayes model is not necessarily
112
d0
?d0,s0
s0
?d0,s1
s1
d1
?d1,s0
s0
?d0,s0
s1
(a) s0 and s1 are not identifiable
s1
?d0,s1
d0
?d1,s1
d1 ?s0
s0
(b) sharing ?d1,s0 and
?d0,s0
Figure 3: Two different parameterization of ?
due to the explicit modeling of sentence-level per-
spective. S may capture aspects of the document
collection that we never intended to model. For ex-
ample, s0 may capture the editors? writing styles and
s1 the guests? writing styles in the bitterlemons
corpus.
We solve the identifiability problem by forcing
?d1,s0 and ?d0,s0 to be identical and reducing the
number of ? parameters to three. As shown in Fig-
ure 3b, there are separate ? parameters conditioned
on the document perspective (left branch of the tree,
d0 is Israeli and d1 is Palestinian), but there is single
? parameter when S = s0 shared by both document-
level perspectives (right branch of the tree). We as-
sume that the sentences with little or no perspective
information, i.e., S = s0, are generated indepen-
dently of the perspective of a document. In other
words, sentences that are presenting common back-
ground information or introducing an issue and that
do not strongly convey any perspective should look
similar whether they are in Palestinian or Israeli doc-
uments. By forcing this constraint, we become more
confident that s0 represents sentences of little per-
spectives and s1 represents sentences of strong per-
spectives from d1 and d0 documents.
5 Experiments
5.1 Identifying Perspective at the Document
Level
We evaluate three different models for the task
of identifying perspective at the document level:
two na??ve Bayes models (NB) with different infer-
ence methods and Support Vector Machines (SVM)
(Cristianini and Shawe-Taylor, 2000). NB-B uses
full Bayesian inference and NB-M uses Maximum
a posteriori (MAP). We compare NB with SVM not
only because SVM has been very effective for clas-
sifying topical documents (Joachims, 1998), but also
to contrast generative models like NB with discrimi-
native models like SVM. For training SVM, we rep-
resent each document as a V -dimensional feature
vector, where V is the vocabulary size and each co-
ordinate is the normalized term frequency within the
document. We use a linear kernel for SVM and
search for the best parameters using grid methods.
To evaluate the statistical models, we train them
on the documents in the bitterlemons corpus
and calculate how accurately each model predicts
document perspective in ten-fold cross-validation
experiments. Table 2 reports the average classi-
fication accuracy across the the 10 folds for each
model. The accuracy of a baseline classifier, which
randomly assigns the perspective of a document as
Palestinian or Israeli, is 0.5, because there are equiv-
alent numbers of documents from the two perspec-
tives.
Model Data Set Accuracy Reduction
Baseline 0.5
SVM Editors 0.9724
NB-M Editors 0.9895 61%
NB-B Editors 0.9909 67%
SVM Guests 0.8621
NB-M Guests 0.8789 12%
NB-B Guests 0.8859 17%
Table 2: Results for Identifying Perspectives at the
Document Level
The last column of Table 2 is error reduction
relative to SVM. The results show that the na??ve
Bayes models and SVM perform surprisingly well
on both the Editors and Guests subsets of the
bitterlemons corpus. The na??ve Bayes mod-
els perform slightly better than SVM, possibly be-
cause generative models (i.e., na??ve Bayes models)
achieve optimal performance with a smaller num-
ber of training examples than discriminative models
(i.e., SVM) (Ng and Jordan, 2002), and the size of
the bitterlemons corpus is indeed small. NB-B,
which performs full Bayesian inference, improves
113
on NB-M, which only performs point estimation.
The results suggest that the choice of words made
by the authors, either consciously or subconsciously,
reflects much of their political perspectives. Statis-
tical models can capture word usage well and can
identify the perspective of documents with high ac-
curacy.
Given the performance gap between Editors and
Guests, one may argue that there exist distinct edit-
ing artifacts or writing styles of the editors and
guests, and that the statistical models are capturing
these things rather than ?perspectives.? To test if the
statistical models truly are learning perspectives, we
conduct experiments in which the training and test-
ing data are mismatched, i.e., from different subsets
of the corpus. If what the SVM and na??ve Bayes
models learn are writing styles or editing artifacts,
the classification performance under the mismatched
conditions will be considerably degraded.
Model Training Testing Accuracy
Baseline 0.5
SVM Guests Editors 0.8822
NB-M Guests Editors 0.9327 43%
NB-B Guests Editors 0.9346 44%
SVM Editors Guests 0.8148
NB-M Editors Guests 0.8485 18%
NB-B Editors Guests 0.8585 24%
Table 3: Identifying Document-Level Perspectives
with Different Training and Testing Sets
The results on the mismatched training and test-
ing experiments are shown in Table 3. Both SVM
and the two variants of na??ve Bayes perform well
on the different combinations of training and testing
data. As in Table 2, the na??ve Bayes models per-
form better than SVM with larger error reductions,
and NB-B slightly outperforms NB-M. The high ac-
curacy on the mismatched experiments suggests that
statistical models are not learning writing styles or
editing artifacts. This reaffirms that document per-
spective is reflected in the words that are chosen by
the writers.
We list the most frequent words (excluding stop-
words) learned by the the NB-M model in Ta-
ble 4. The frequent words overlap greatly be-
tween the Palestinian and Israeli perspectives, in-
cluding ?state,? ?peace,? ?process,? ?secure? (?se-
curity?), and ?govern? (?government?). This is in
contrast to what we expect from topical text classi-
fication (e.g., ?Sports? vs. ?Politics?), in which fre-
quent words seldom overlap. Authors from differ-
ent perspectives often choose words from a simi-
lar vocabulary but emphasize them differently. For
example, in documents that are written from the
Palestinian perspective, the word ?palestinian? is
mentioned more frequently than the word ?israel.?
It is, however, the reverse for documents that are
written from the Israeli perspective. Perspectives
are also expressed in how frequently certain people
(?sharon? v.s. ?arafat?), countries (?international?
v.s. ?america?), and actions (?occupation? v.s. ?set-
tle?) are mentioned. While one might solicit these
contrasting word pairs from domain experts, our re-
sults show that statistical models such as SVM and
na??ve Bayes can automatically acquire them.
5.2 Identifying Perspectives at the Sentence
Level
In addition to identifying the perspective of a docu-
ment, we are interested in knowing which sentences
of the document strongly conveys perspective in-
formation. Sentence-level perspective annotations
do not exist in the bitterlemons corpus, which
makes estimating parameters for the proposed La-
tent Sentence Perspective Model (LSPM) difficult.
The posterior probability that a sentence strongly
covey a perspective (Example (6)) is of the most in-
terest, but we can not directly evaluate this model
without gold standard annotations. As an alterna-
tive, we evaluate how accurately LSPM predicts the
perspective of a document, again using 10-fold cross
validation. Although LSPM predicts the perspec-
tive of both documents and sentences, we will doubt
the quality of the sentence-level predictions if the
document-level predictions are incorrect.
The experimental results are shown in Table 5.
We include the results for the na??ve Bayes models
from Table 3 for easy comparison. The accuracy of
LSPM is comparable or even slightly better than that
of the na??ve Bayes models. This is very encouraging
and suggests that the proposed LSPM closely cap-
tures how perspectives are reflected at both the doc-
ument and sentence levels. Examples 1 and 2 from
the introduction were predicted by LSPM as likely to
114
Palestinian palestinian, israel, state, politics, peace, international, people, settle, occupation, sharon,
right, govern, two, secure, end, conflict, process, side, negotiate
Israeli israel, palestinian, state, settle, sharon, peace, arafat, arab, politics, two, process, secure,
conflict, lead, america, agree, right, gaza, govern
Table 4: The top twenty most frequent stems learned by the NB-M model, sorted by P (w|d)
Model Training Testing Accuracy
Baseline 0.5
NB-M Guests Editors 0.9327
NB-B Guests Editors 0.9346
LSPM Guests Editors 0.9493
NB-M Editors Guests 0.8485
NB-B Editors Guests 0.8585
LSPM Editors Guests 0.8699
Table 5: Results for Perspective Identification at the
Document and Sentence Levels
contain strong perspectives, i.e., large Pr(S? = s1).
Examples 3 and 4 from the introduction were pre-
dicted by LSPM as likely to contain little or no per-
spective information, i.e., high Pr(S? = s0).
The comparable performance between the na??ve
Bayes models and LSPM is in fact surprising. We
can train a na??ve Bayes model directly on the sen-
tences and attempt to classify a sentence as reflect-
ing either a Palestinian or Israeli perspective. A sen-
tence is correctly classified if the predicted perspec-
tive for the sentence is the same as the perspective
of the document from which it was extracted. Us-
ing this model, we obtain a classification accuracy of
only 0.7529, which is much lower than the accuracy
previously achieved at the document level. Identify-
ing perspectives at the sentence level is thus more
difficult than identifying perspectives at the docu-
ment level. The high accuracy at the document level
shows that LSPM is very effective in pooling evi-
dence from sentences that individually contain little
perspective information.
6 Conclusions
In this paper we study a new problem of learning to
identify the perspective from which a text is written
at the document and sentence levels. We show that
much of a document?s perspective is expressed in
word usage, and statistical learning algorithms such
as SVM and na??ve Bayes models can successfully
uncover the word patterns that reflect author per-
spective with high accuracy. In addition, we develop
a novel statistical model to estimate how strongly
a sentence conveys perspective, in the absence of
sentence-level annotations. By introducing latent
variables and sharing parameters, the Latent Sen-
tence Perspective Model is shown to capture well
how perspectives are reflected at the document and
sentence levels. The small but positive improvement
due to sentence-level modeling in LSPM is encour-
aging. In the future, we plan to investigate how con-
sistently LSPM sentence-level predictions are with
human annotations.
Acknowledgment
This material is based on work supported by
the Advanced Research and Development Activity
(ARDA) under contract number NBCHC040037.
A Gibbs Samplers
Based the model specification described in Sec-
tion 4.2 we derive the Gibbs samplers (Chen et al,
2000) for the Latent Sentence Perspective Model as
follows,
pi(t+1) ? Beta(?pi +
N
?
n=1
dn + d?(t+1),
?pi + N ?
N
?
n=1
dn + 1 ? d?(t+1))
? (t+1) ? Beta(?? +
N
?
n=1
Mn
?
m=1
sm,n +
M?
?
m=1
s?m,
?? +
N
?
n=1
Mn ?
N
?
n=1
Mn
?
m=1
sm,n + M? ?
M?
?
m=1
s?m)
115
?(t+1) ? Dirichlet(?? +
N
?
n=1
Mn
?
m=1
wm,n)
Pr(S(t+1)n,m = s1) ? P (Wm,n|Sm,n = 1, ?(t))
Pr(S(t+1)m,n = 1|?,Dn)
Pr(D?(t+1) = d1) ?
M?
?
m=1
dbinom(? (t+1)d )
M?
?
m=1
dmultinom(?d,m?(t))dbinom(pi(t))
where dbinom and dmultinom are the density func-
tions of binomial and multinomial distributions, re-
spectively. The superscript t indicates that a sample
is from the t-th iteration. We run three chains and
collect 5000 samples. The first half of burn-in sam-
ples are discarded.
References
Philip Beineke, Trevor Hastie, and Shivakumar
Vaithyanathan. 2004. The sentimental factor:
Improving review classification via human-provided
information. In Proceedings of ACL-2004.
Ming-Hui Chen, Qi-Man Shao, and Joseph G. Ibrahim.
2000. Monte Carlo Methods in Bayesian Computa-
tion. Springer-Verlag.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW-2003.
Michael L. Geis. 1987. The Language of Politics.
Springer.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of KDD-2004.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of ECML-1998.
David D. Lewis. 1998. Naive (Bayes) at forty: The inde-
pendence assumption in information retrieval. In Pro-
ceedings of ECML-1998.
S. Morinaga, K. Yamanishi, K. Tateishi, and
T. Fukushima. 2002. Mining product reputations on
the web. In Proceedings of KDD-2002.
Tony Mullen and Nigel Collier. 2004. Sentiment analy-
sis using support vector machines with diverse infor-
mation sources. In Proceedings of EMNLP-2004.
T. Nasukawa and J. Yi. 2003. Sentiment analysis: Cap-
turing favorability using natural language processing.
In Proceedings of K-CAP 2003.
Andrew Y. Ng and Michael Jordan. 2002. On discrim-
inative vs. generative classifiers: A comparison of lo-
gistic regression and naive bayes. In NIPS-2002, vol-
ume 15.
Zhongdang Pan, Chin-Chuan Lee, Joseph Man Chen, and
Clement Y.K. So. 1999. One event, three stories: Me-
dia narratives of the handover of hong kong in cultural
china. Gazette, 61(2):99?112.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP-
2002.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT/EMNLP-2005, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP-2003.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of CoNLL-2003.
Peter Turney and Michael L. Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM TOIS, 21(4):315?346.
T.A. van Dijk. 1988. News as Discourse. Lawrence
Erlbaum, Hillsdale, NJ.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational Linguistics, 30(3).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP-
2005.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP-2003.
116
Proceedings of the Linguistic Annotation Workshop, pages 191?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Panel Session: Discourse Annotation
Manfred Stede
Dept. of Linguistics
University of Potsdam
stede@ling.uni-potsdam.de
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Eva Hajic?ova?
Faculty of Math. and Physics
Charles University
hajicova@ufal.ms.mff.cuni.cz
Brian Reese
Dept. of Linguistics
Univ. of Texas at Austin
bjreese@mail.utexas.edu
Simone Teufel
Computer Laboratory
Univ. of Cambridge
sht25@cl.cam.uk
Bonnie Webber
School of Informatics
Univ. of Edinburgh
bonnie@inf.ed.ac.uk
Theresa Wilson
Dept. of Comp. Science
Univ. of Pittsburgh
twilson@cs.pitt.edu
1 Introduction
The classical ?success story? of corpus annotation
are the various syntax treebanks that provide struc-
tural analyses of sentences and have enabled re-
searchers to develop a range of new and highly suc-
cessful data-oriented approaches to sentence pars-
ing. In recent years, however, a number of corpora
have been constructed that provide annotations on
the discourse level, i.e. information that reaches be-
yond the sentence boundaries. Phenomena that have
been annotated include coreference links, the scope
of connectives, and coherence relations. Many of
these are phenomena on whose handling there is
not a general agreement in the research community,
and therefore the question of ?recycling? corpora by
other people and for other purposes is often diffi-
cult. (To some extent, this is due to the fact that dis-
course annotation deals ?only? with surface reflec-
tions of underlying, abstract objects.) At the same
time, the efforts needed for building high-quality
discourse corpora are considerable, and thus one
should be careful in deciding how to invest those ef-
forts. One aspect of providing added-value with an-
notation projects is that of shared corpora: If a vari-
ety of annotation efforts is executed on the same pri-
mary data, the series of annotation levels can yield
insights that the creators of the individual levels had
not explicitly planned for. A clear case is the rela-
tionship between coherence relations and connective
use: When both levels are marked individually and
with independent annotation guidelines, then after-
wards the correlations between coherence relations,
cue usage (and possibly other factors, if annotated)
can be studied systematically. This conception of
multi-level annotation presupposes, of course, that
the technical problems of setting annotation levels
in correspondence to one another be resolved.
The panel on discourse annotation is organized
by Manfred Stede and Janyce Wiebe. It aims at
surveying the scene of discourse corpora, exploring
chances for synergy, and identifying desiderata for
future corpus creation projects. In preparation for
the panel, the participants have provided the follow-
ing short descriptions of the various copora in whose
construction they have been involved.
2 Prague Dependency Treebank
(Eva Hajic?ova?, Prague)
One of the maxims of the work on the Prague De-
pendency Treebank is that one should not overlook,
disregard and thus lose what the sentence structure
offers when one attempts to analyze the structure of
discourse, thus moving from ?the trees? to ?the for-
est?. Therefore, we emphasize that discourse anno-
tation should make use of every possible detail the
annotation of the component parts of the discourse,
namely the sentences, puts at our disposal. This
is, of course, not only true for the surface shape of
the sentence (i.e., the surface means of expression),
but (and most importantly) for the underlying repre-
sentation of sentences. The panel contribution will
introduce the (multilayered) annotation scenario of
the Prague Dependency Treebank and illustrate the
point using some of the particular features of the un-
derlying structure of sentences that can be made use
of in planning the scenario of discourse ?treebanks?.
191
3 SDRT in Newspaper Text
(Brian Reese, Austin)
We are currently working under the auspices of
an NSF grant to build and train a discourse parser
and codependent anaphora resolution program to
test discourse theories empirically. The training re-
quires the construction of a corpus annotated with
discourse structure and coreference information. So
far, we have annotated the MUC61 corpus for dis-
course structure and are in the process of annotating
the ACE22 corpus; both corpora are already anno-
tated for coreference. One of the goals of the project
is to investigate whether using the right frontier con-
straint improves the system?s performance in resolv-
ing anaphors. Here we detail some experiences we
have had with the discourse annotation process.
An implementation of the extant SDRT (Asher and
Lascarides, 2003) glue logic for building discourse
structures is insufficient to deal with open domain
text, and we cannot envision an extended version
at the present time able to deal with the problem.
Thus, we have opted for a machine learning based
approach to discourse parsing based on superficial
features, like BNL. To build an implementation to
test these ideas, we have had to devise a corpus of
texts annotated for discourse structure in SDRT.
Each of the 60 texts in the MUC6 corpus, and now
18 of the news stories in ACE2, were annotated by
two people familiar with SDRT. The annotators then
conferred and agreed upon a gold standard. Our
annotation effort took the hierarchical structure of
SDRT seriously and built graphs in which the nodes
are discourse units and the arcs represent discourse
relations between the units. The units could either be
simple (elementary discourse units: EDUs) or they
could be complex. We assumed that in principle the
units were recursively generated and could have an
arbitrary though finite degree of complexity.
4 Potsdam Commentary Corpus
(Manfred Stede, Potsdam)
Construction of the Potsdam Commentary Corpus
(PCC) began in 2003 and is still ongoing. It is a
1The Message Understanding Conference, www-nlpir.
nist.gov/related projects/muc/.
2The Automated Content Extraction program,
www.nist.gov/speech/tests/ace/.
genre-specific corpus of German newspaper com-
mentaries, taken from the daily papers Ma?rkische
Allgemeine Zeitung and Tagesspiegel. One central
aim is to provide a tool for studying mechanisms
of argumentation and how they are reflected on the
linguistic surface. The corpus on the one hand is a
collection of ?raw? data, which is used for genre-
oriented statistical explorations. On the other hand,
we have identified two sub-corpora that are subject
to a rich multi-level annotation (MLA).
The PCC176 (Stede, 2004) is a sub-corpus that
is available upon request for research purposes. It
consists of 176 relatively short commentaries (12-
15 sentences), with 33.000 tokens in total. The
sentences have been PoS-tagged automatically (and
manually checked); sentence syntax was anno-
tated semi-automatically using the TIGER scheme
(Brants et al, 2002) and Annotate3 tool. In addition,
we annotated coreference (PoCos (Krasavina and
Chiarcos, 2007)) and rhetorical structure according
to RST (Mann and Thompson, 1988). Our anno-
tation software architecture consists of a variety of
standard, external tools that can be used effectively
for the different annotation types. Their XML output
is then automatically converted to a generic format
(PAULA, (Dipper, 2005)), which is read into the lin-
guistic database ANNIS (Dipper et al, 2004), where
the annotations are aligned, so that the data can be
viewed and queried across annotation levels.
The PCC10 is a sub-corpus of 10 commentaries
that serves as ?testbed? for further developing the
annotation levels. On the one hand, we are apply-
ing recent guidelines on annotation of information
structure (Go?tze et al, 2007). On the other hand,
based on experiences with the RST annotation, we
are replacing the rhetorical trees with a set of dis-
tinct, simpler annotation layers: thematic structure,
conjunctive relations (Martin, 1992), and argumen-
tation structure (Freeman, 1991); these are comple-
mented by the other levels mentioned above for the
PCC176. The primary motivation for this step is the
high degree of arbitrariness that annotators reported
when producing the RST trees (see (Stede, 2007)).
By separating the thematic from the intentional in-
formation, and accounting for the surface-oriented
3www.coli.uni-saarland.de/projects/
sfb378/negra-corpus/annotate.html
192
conjunctive relations (which are similar to what is
annotated in the PDTB, see Section 6), we hope to
? make annotation easier: handling several ?sim-
ple? levels individually should be more effec-
tive than a single, very complex annotation
step;
? end up with less ambiguity in the annotations,
since the reasons for specific decisions can be
made explicit (by annotations on ?simpler? lev-
els);
? be more explicit than a single tree can be: if a
discourse fulfills, for example, a function both
for thematic development and for the writer?s
intention, they can both be accounted for;
? provide the central information that a ?tradi-
tional? rhetorical tree conveys, without loosing
essential information.
5 AZ Corpus
(Simone Teufel, Cambridge)
The Argumentative Zoning (AZ) annotation scheme
(Teufel, 2000; Teufel and Moens, 2002) is con-
cerned with marking argumentation steps in scien-
tific articles. One example for an argumentation step
is the description of the research goal, another an
overt comparison of the authors? work with rival ap-
proaches. In our scheme, these argumentation steps
have to be associated with text spans (sentences or
sequences of sentences). AZ?Annotation is the la-
belling of each sentence in the text with one of these
labels (7 in the original scheme in (Teufel, 2000)).
The AZ labels are seen as relations holding between
the meanings of these spans, and the rhetorical act
of the entire paper. (Teufel et al, 1999) reports on
interannotator agreement studies with this scheme.
There is a strong interrelationship between the ar-
gumentation in a paper, and the citations writers use
to support their argument. Therefore, a part of the
computational linguistics corpus has a second layer
of annotation, called CFC (Teufel et al, 2006) or
Citation Function Classification. CFC? annotation
records for each citation which rhetorical function it
plays in the argument. This is following the spirit of
research in citation content analysis (e.g., (Moravc-
sik and Murugesan, 1975)). An example for a ci-
tation function would be ?motivate that the method
used is sound?. The annotation scheme contains
12 functions, clustered into ?superiority?, ?neutral
comparison/contrast?, ?praise or usage? and ?neu-
tral?.
One type of research we hope to do in the future
is to study the relationship between these rhetori-
cal phonemena with more traditional discourse phe-
nomena, e.g. anaphoric expressions.
The CmpLg/ACL Anthology corpora consist of
320/9000 papers in computational linguistics. They
are partially annotated with AZ and CFC markup. A
subcorpus of 80 parallelly annotated papers (AZ and
CFF) can be obtained from us for research (12000
sentences, 1756 citations). We are currently port-
ing both schemes to chemistry in the framework
of the EPSRC-sponsored project SciBorg. In the
course of this work a larger, more general AZ an-
notation scheme was developed. The SciBorg effort
will result in an AZ/CFC?annotated chemistry cor-
pus available to the community in 2009.
In terms of challenges, the most time-consuming
aspects of creating this annotated corpus were for-
mat conversions on the corpora, and cyclic adapta-
tions of scheme and guidelines. Another problem is
the simplification of annotating only full sentences;
sometimes, annotators would rather mark a clause
or sometimes even just an NP. However, we found
these cases to be relatively rare.
6 Penn Discourse Treebank
(Bonnie Webber, Edinburgh)
The Penn Discourse TreeBank (Miltsakaki et al,
2004; Prasad et al, 2004; Webber, 2005) anno-
tates discourse relations over the Wall Street Jour-
nal corpus (Marcus et al, 1993), in terms of dis-
course connectives and their arguments. Following
the approach towards discourse structure in (Webber
et al, 2003), the PDTB takes a lexicalized approach,
treating discourse connectives as the anchors of the
relations and thus as discourse-level predicates tak-
ing two Abstract Objects as their arguments. An-
notated are the text spans that give rise to these ar-
guments. There are primarily two types of connec-
tives in the PDTB: explicit and implicit, the latter
being inserted between adjacent paragraph-internal
sentence pairs not related by an explicit connective.
193
Also annotated in the PDTB is the attribution of
each discourse relation and of its arguments (Dinesh
et al, 2005; Prasad et al, 2007). (Attribution itself
is not considered a discourse relation.) A prelimi-
nary version of the PDTB was released in April 2006
(PDTB-Group, 2006), and is available for download
at http://www.seas.upenn.edu/?pdtb. This release only has
implicit connectives annotated in three sections of
the corpus. The annotation of all implicit connec-
tives, along with a hierarchical semantic classifica-
tion of all connectives (Miltsakaki et al, 2005), will
appear in the final release of the PDTB in August
2007.
Here I want to mention three of the challenges we
have faced in developing the PDTB:
(I) Words and phrases that can function as con-
nectives can also serve other roles. (Eg, when can be
a relative pronoun, as well as a subordinating con-
junction.) It has been difficult to identify all and
only those cases where a token functions as a dis-
course connective, and in many cases, the syntactic
analysis in the Penn TreeBank (Marcus et al, 1993)
provides no help. For example, is as though always a
subordinating conjunction (and hence a connective)
or do some tokens simply head a manner adverbial
(eg, seems as though . . . versus seems more rushed
as though . . . )? Is also sometimes a discourse con-
nective relating two abstract objects and other times,
an adverb that presupposes that a particular property
holds of some other entity? If so, when one and
when the other? In the PDTB, annotation has erred
on the side of false positives.
(II) In annotating implicit connectives, we discov-
ered systematic non-lexical indicators of discourse
relations. In English, these include cases of marked
syntax (eg, Had I known the Queen would be here,
I would have dressed better.) and cases of sentence-
initial PPs and adjuncts with anaphoric or deictic
NPs such as at the other end of the spectrum, adding
to that speculation. These cases labelled ALTLEX,
for ?alternative lexicalisation? have not been anno-
tated as connectives in the PDTB because they are
fully productive (ie, not members of a more eas-
ily annotated closed set of tokens). They comprise
about 1% of the cases the annotators have consid-
ered. Future discourse annotation will benefit from
further specifying the types of these cases.
(III) The way in which spans are annotated as ar-
guments to connectives also raises a challenge. First,
because the PDTB annotates both structural and
anaphoric connectives (Webber et al, 2003), a span
can serve as argument to >1 connective. Secondly,
unlike in the RST corpus (Carlson et al, 2003) or the
Discourse GraphBank (Wolf and Gibson, 2005), dis-
course segments are not separately annotated, with
annotators then identifying what discourse relations
hold between them. Instead, in annotating argu-
ments, PDTB annotators have selected the minimal
clausal text span needed to interpret the relation.
This could comprise an embedded, subordinate or
coordinate clause, an entire sentence, or a (possi-
bly disjoint) sequence of sentences. As a result,
there are fairly complex patterns of spans within and
across sentences that serve as arguments to differ-
ent connectives, and there are parts of sentences that
don?t appear within the span of any connective, ex-
plicit or implicit. The result is that the PDTB pro-
vides only a partial but complexly-patterned cover
of the corpus. Understanding what?s going on and
what it implies for discourse structure (and possibly
syntactic structure as well) is a challenge we?re cur-
rently trying to address (Lee et al, 2006).
7 MPQA Opinion Corpus
(Theresa Wilson, Pittsburgh)
Our opinion annotation scheme (Wiebe et al, 2005)
is centered on the notion of private state, a gen-
eral term that covers opinions, beliefs, thoughts, sen-
timents, emotions, intentions and evaluations. As
Quirk et al (1985) define it, a private state is a state
that is not open to objective observation or verifica-
tion. We can further view private states in terms of
their functional components ? as states of experi-
encers holding attitudes, optionally toward targets.
For example, for the private state expressed in the
sentence John hates Mary, the experiencer is John,
the attitude is hate, and the target is Mary.
We create private state frames for three main types
of private state expressions (subjective expressions)
in text:
? explicit mentions of private states, such as
?fears? in ?The U.S. fears a spill-over?
? speech events expressing private states, such as
?said? in ?The report is full of absurdities,?
194
Xirao-Nima said.
? expressive subjective elements, such as ?full of
absurdities? in the sentence just above.
Frames include the source (experiencer) of the
private state, the target, and various properties such
as polarity (positive, negative, or neutral) and inten-
sity (high, medium, or low). Sources are nested. For
example, for the sentence ?China criticized the U.S.
report?s criticism of China?s human rights record?,
the source is ?writer, China, U.S. report?, reflecting
the facts that the writer wrote the sentence and the
U.S. report?s criticism is the target of China?s criti-
cism. It is common for multiple frames to be created
for a single clause, reflecting various levels of nest-
ing and the type of subjective expression.
The annotation scheme has been applied to a
corpus, called the ?Multi-Perspective Question An-
swering (MPQA) Corpus,? reflecting its origins in
the 2002 NRRC Workshop on Multi-Perspective
Question Answering (MPQA) (Wiebe et al, 2003)
sponsored by ARDA AQUAINT (it is also called
?OpinionBank?). It contains 535 documents and a
total of 11,114 sentences. The articles in the cor-
pus are from 187 different foreign and U.S. news
sources, dating from June 2001 to May 2002. Please
see (Wiebe et al, 2005) and Theresa Wilson?s forth-
coming PhD dissertation for further information, in-
cluding the results of inter-coder agreement studies.
References
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation. Cambridge University Press, Cam-
bridge.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In J. van
Kuppevelt & R. Smith, editor, Current Directions in
Discourse and Dialogue. Kluwer, New York.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005. At-
tribution and the (non-)alignment of syntactic and dis-
course arguments of connectives. In ACL Workshop
on Frontiers in Corpus Annotation, Ann Arbor MI.
Stefanie Dipper, Michael Go?tze, Manfred Stede, and Till-
mann Wegst. 2004. Annis: A linguistic database for
exploring information structure. In Interdisciplinary
Studies on Information Structure, ISIS Working papers
of the SFB 632 (1), pages 245?279.
Stefanie Dipper. 2005. XML-based stand-off represen-
tation and exploitation of multi-level linguistic annota-
tion. In Rainer Eckstein and Robert Tolksdorf, editors,
Proceedings of Berliner XML Tage, pages 39?50.
James B. Freeman. 1991. Dialectics and the
Macrostructure of Argument. Foris, Berlin.
Michael Go?tze, Cornelia Endriss, Stefan Hinterwimmer,
Ines Fiedler, Svetlana Petrova, Anne Schwarz, Stavros
Skopeteas, Ruben Stoel, and Thomas Weskott. 2007.
Information structure. In Information structure in
cross-linguistic corpora: annotation guidelines for
morphology, syntax, semantics, and information struc-
ture, volume 7 of ISIS Working papers of the SFB 632,
pages 145?187.
Olga Krasavina and Christian Chiarcos. 2007. Potsdam
Coreference Scheme. In this volume.
Alan Lee, Rashmi Prasad, Aravind Joshi, Nikhil Dinesh,
and Bonnie Webber. 2006. Complexity of dependen-
cies in discourse. In Proc. 5th Workshop on Treebanks
and Linguistic Theory (TLT?06), Prague.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. TEXT, 8:243?281.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large scale anno-
tated corpus of English: The Penn TreeBank. Compu-
tational Linguistics, 19:313?330.
James R. Martin. 1992. English text: system and struc-
ture. John Benjamins, Philadelphia/Amsterdam.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating discourse connec-
tives and their arguments. In NAACL/HLT Workshop
on Frontiers in Corpus Annotation, Boston.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Experiments
on sense annotation and sense disambiguation of dis-
course connectives. In 4t Workshop on Treebanks and
Linguistic Theory (TLT?05), Barcelona, Spain.
Michael J. Moravcsik and Poovanalingan Murugesan.
1975. Some results on the function and quality of ci-
tations. Soc. Stud. Sci., 5:88?91.
The PDTB-Group. 2006. The Penn Discourse TreeBank
1.0 annotation manual. Technical Report IRCS 06-01,
University of Pennsylvania.
195
Rashmi Prasad, Eleni Miltsakaki, Aravind Joshi, and
Bonnie Webber. 2004. Annotation and data mining
of the Penn Discourse TreeBank. In ACL Workshop
on Discourse Annotation, Barcelona, Spain, July.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind Joshi,
and Bonnie Webber. 2007. Attribution and its annota-
tion in the Penn Discourse TreeBank. TAL (Traitement
Automatique des Langues.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech, and
Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, New York.
Manfred Stede. 2004. The Potsdam commentary corpus.
In Proceedings of the ACL Workshop on Discourse An-
notation, pages 96?102, Barcelona.
Manfred Stede. 2007. RST revisited: disentangling nu-
clearity. In Cathrine Fabricius-Hansen and Wiebke
Ramm, editors, ?Subordination? versus ?coordination?
in sentence and text ? from a cross-linguistic perspec-
tive. John Benjamins, Amsterdam. (to appear).
Simone Teufel and Marc Moens. 2002. Summaris-
ing scientific articles ? experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?446.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumenta-
tion in research articles. In Proceedings of the 9th Eu-
ropean Conference of the ACL (EACL-99), pages 110?
117, Bergen, Norway.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. An annotation scheme for citation function. In
Proceedings of SIGDIAL-06, Sydney, Australia.
Simone Teufel. 2000. Argumentative Zoning: Infor-
mation Extraction from Scientific Text. Ph.D. thesis,
School of Cognitive Science, University of Edinburgh,
Edinburgh, UK.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29:545?587.
Bonnie Webber. 2005. A short introduction to the Penn
Discourse TreeBank. In Copenhagen Working Papers
in Language and Speech Processing.
Janyce Wiebe, Eric Breck, Chris Buckley, Claire Cardie,
Paul Davis, Bruce Fraser, Diane Litman, David Pierce,
Ellen Riloff, Theresa Wilson, David Day, and Mark
Maybury. 2003. Recognizing and organizing opinions
expressed in the world press. In Working Notes of the
AAAI Spring Symposium in New Directions in Ques-
tion Answering, pages 12?19, Palo Alto, California.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31:249?287.
196
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643?1654,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Open Domain Targeted Sentiment
Margaret Mitchell Jacqueline Aguilar Theresa Wilson Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
{m.mitchell,jacqui.aguilar}@jhu.edu, Theresa.Wilson@oberlin.edu, vandurme@cs.jhu.edu
Abstract
We propose a novel approach to sentiment
analysis for a low resource setting. The in-
tuition behind this work is that sentiment
expressed towards an entity, targeted senti-
ment, may be viewed as a span of sentiment
expressed across the entity. This represen-
tation allows us to model sentiment detec-
tion as a sequence tagging problem, jointly
discovering people and organizations along
with whether there is sentiment directed to-
wards them. We compare performance in
both Spanish and English on microblog data,
using only a sentiment lexicon as an exter-
nal resource. By leveraging linguistically-
informed features within conditional random
fields (CRFs) trained to minimize empiri-
cal risk, our best models in Spanish signifi-
cantly outperform a strong baseline, and reach
around 90% accuracy on the combined task of
named entity recognition and sentiment pre-
diction. Our models in English, trained on a
much smaller dataset, are not yet statistically
significant against their baselines.
1 Introduction
Sentiment analysis is a multi-faceted problem. De-
termining when a positive or negative sentiment is
being expressed is a large part of the challenge, but
identifying other attributes, such as the target of the
sentiment, is also crucial if the ultimate goal is to
pinpoint and extract opinions. Consider the exam-
ples below, all of which contain a positive sentiment:
(1) So happy that Kentucky lost to Tennessee!
(2) Kentucky versus Kansas I can hardly wait...
(3) Kentucky is the best alley-oop throwing team
since Sherman Douglas? Syracuse squads!!
The entities in these examples are college basket-
ball teams, and the events referred to are games. In
(1), although there is a positive sentiment, the tar-
get of the sentiment is an event (Kentucky losing to
Tennessee). However, from the positive sentiment
toward this event, we can infer that the speaker has
a negative sentiment toward Kentucky and a positive
sentiment toward Tennessee. In (2), the positive sen-
timent is toward a future event, but we are not given
enough information to infer a sentiment toward the
mentioned entities. In (3), Kentucky is the direct
target of the positive sentiment. We can also in-
fer a positive sentiment toward Douglas?s Syracuse
teams, and even toward Douglas himself.
These examples illustrate the importance of the
target when interpreting sentiment in context. If we
are looking for sentiments toward Kentucky, for ex-
ample, we would want to identify (1) as negative, (2)
as neutral (no sentiment) and (3) as positive. How-
ever, if we are looking for sentiment toward Ten-
nessee, we would want to identify (1) as positive,
and (2) and (3) as neutral.
The expression of these and other kinds of sen-
timent can be understood as involving three items:
(1) An experiencer
(2) An attitude
(3) A target (optionally)
Research in sentiment analysis often focuses on (2),
predicting overall sentiment polarity (Agarwal et al,
2011; Bora, 2012). Recent work has begun to com-
bine (2) with (3), examining how to automatically
predict the sentiment polarity expressed towards a
target entity (Jiang et al, 2011; Chen et al, 2012)
for a fixed set of targets. This topic-dependent sen-
timent classification requires that the target entity be
1643
Figure 1: Sentiment expressed across an entity.
given, and returns statements expressing sentiment
towards the given entity.
In this paper, we take a step towards open-domain,
targeted sentiment analysis by investigating how to
detect both the named entity and the sentiment ex-
pressed toward it. We observe that sentiment ex-
pressed towards a target entity may be possible to
learn in a graphical model along the span of the en-
tity itself: Similar to how named entity recognition
(NER) learns labels along the span of each word in
an entity name, sentiment may be expressed along
the entity as well. A small example is shown in Fig-
ure 1. We focus on people and organizations (voli-
tional named entities), which are the primary targets
of sentiment in our microblog data (see Table 1).
Both NER and opinion expression extraction have
achieved impressive results using conditional ran-
dom fields (CRFs) (Lafferty et al, 2001) to define
the conditional probability of entity categories (Mc-
Callum and Li, 2003; Choi et al, 2006; Yang and
Cardie, 2013). We develop such models to jointly
predict the NE and the sentiment expressed towards
it using minimum risk training (Stoyanov and Eis-
ner, 2012). We learn our models on informal Span-
ish and English language taken from the social net-
work Twitter,1 where the language variety makes
NLP particularly challenging (see Figure 2).
Our ultimate goal is to develop models that will
be useful for low resource languages, where a sen-
timent lexicon may be known or bootstrapped, but
more sophisticated linguistic tools may not be read-
ily available. We therefore do not rely on an external
part-of-speech tagger or parser, which are often used
for features in fine-grained sentiment analysis; such
tools are not available in many languages, and if they
are, are not usually adapted for noisy social media.
Instead, we use information from sentiment lex-
icons and some simple hand-written features, and
otherwise use only features of the word that can be
1www.twitter.com
@[user] le dijo erralo muy por lo bajo jaja un grande
juancito grandes amigos mios
@[user] he told him it was very on the dl haha a great
juancito great friends of mine
@[user] buenos d??as Profe!! Nos quedamos acciden-
tados otra vez en la carretera vieja guarenas echando
gasoil, estamos a la interperie
@[user] good morning, Prof!! We were wrecked again
on the old guarenas highway while getting diesel, we?re
out in the open
Sin a?nimo de ofender a los Militares, que realmente
se merecen ese aumento y ma?s. Pero, do?nde queda la
misma recompensa para Me?dicos.
I do not intend to offend the military in the slightest,
they truly deserve the raise and more. However, I?m
wondering whether doctors will ever receive a similar
compensation.
Figure 2: Messages on Twitter use a wide range of
formality, style, and errors, which makes extracting in-
formation particularly difficult. Examples from Spanish
(screen names anonymized), with approximate transla-
tions in English.
extracted without supervision. These include fea-
tures based on unsupervised word tags (Brown clus-
ters) and a method that automatically syllabifies a
word based on the orthography of the language. All
tools and code used for this research are released
with this paper.2
2 Related Work
As the scale of social media has grown, using
sources such as Twitter to mine public sentiment
has become increasingly promising. Commer-
cial systems include Sentiment1403 (products and
brands) and tweetfeel4 (suggests searching for pop-
ular movies, celebrities and companies).
The majority of academic research has focused on
supervised classification of message sentiment irre-
spective of target (Barbosa and Feng, 2010; Pak and
Paroubek, 2010; Bifet and Frank, 2010; Davidov et
al., 2010; Kouloumpis et al, 2011; Agarwal et al,
2011). Large datasets are collected for this work by
leveraging the sentiment inherent in emoticons (e.g.,
smilies and frownies) and/or select Twitter hashtags
(e.g., #bestdayever, #fail), resulting in noisy collec-
2www.m-mitchell.com/code
3www.sentiment140.com
4www.tweetfeel.com
1644
tions appropriate for initial exploration. Prior work
includes: the use of a social network (Speriosu et
al., 2011; Tan et al, 2011; Calais Guerra et al,
2011; Jiang et al, 2011; Li et al, 2012; Hu et
al., 2013); user-adapted models based on collabo-
rative online-learning (Li et al, 2010b); unsuper-
vised, joint sentiment-topic modeling (Saif et al,
2012); tracking changing sentiment during debates
(Diakopoulos and Shamma, 2010); and how ortho-
graphic conventions such as word-lengthening can
be used to adapt a Twitter-specific sentiment lexicon
(Brody and Diakopoulos, 2011).
Efforts in targeted sentiment (Bermingham and
Smeaton, 2010; Jin and Ho, 2009; Li et al, 2010a;
Jiang et al, 2011; Tan et al, 2011; Wang et al,
2011; Li et al, 2012; Chen et al, 2012), have mostly
focused on topic-dependent analysis. In these ap-
proaches, messages are collected on a fixed set of
topics/targets, such as products or sports teams, and
sentiment is learned for the given set. In contrast,
we aim to predict sentiment in tweets for any named
person or organization. We refer to this task as open
domain targeted sentiment analysis.
Within topic-dependent sentiment analysis, sev-
eral approaches have explored applying CRFs or
HMMs to extract sentiment and target words from
text (Jin and Ho, 2009; Li et al, 2010a). In these
approaches, opinion expressions are extracted, and
polarity is annotated across the opinion expression.
However, as noted by many researchers in senti-
ment, opinion orientation towards a specific target
is often not equal to the orientation of a neighbor-
ing opinion expression; and opinion expressions in
one context may not be opinion expressions in an-
other (Kim and Hovy, 2006), making open domain
approaches particularly challenging.
The above work by Jiang et al (2011) is most
similar to our own. They do not use joint learning,
but they do incorporate a number of parse-based fea-
tures designed to capture relationships between sen-
timent terms and topic references. In our work these
relationships are captured by the CRF model, and
we compare against their approach in Section 6.
Recent work by Yang and Cardie (2013) is sim-
ilar in spirit to our own, where the identification
of opinion holders, opinion targets, and opinion ex-
pressions is modeled as a sequence tagging problem
using a CRF. However, similar to previous work ap-
plying CRFs to extract sentiment, Yang and Cardie
use syntactic relations to connect an opinion target
to an opinion expression. In contrast, we model
the expression of sentiment polarity across the senti-
ment target itself, extracting both the sentiment tar-
get and the sentiment expressed towards it within the
same span of words. This allows us to use surround-
ing context to determine sentiment polarity without
identifying explicit opinion expressions or relying
on a parser to help link expression to target.
Most work in targeted sentiment outside the mi-
croblogging domain has been in relation to prod-
uct review mining (e.g., Yi et al (2003), Hu and
Liu (2004), Popescu and Etzioni (2005), Qiu et al
(2011)). Rather than identify named entities (NEs),
this work seeks to identify products and their fea-
tures mentioned in reviews, and classify these for
sentiment. Recent work by Qui et al jointly learns
targets and opinion words, and Jakob and Gurevych
(2010) use CRFs to extract the targets of opinions,
but do not attempt to classify the sentiment toward
these targets. To the best of our knowledge, this is
the first work to approach targeted sentiment in a low
resource setting and to jointly predict NEs and tar-
geted sentiment.
3 Data
Twitter Collection We use the Spanish/English
Twitter dataset of Etter et al (2013) to train and test
our models. Approximately 30,000 Spanish tweets
and 10,000 English were labeled for named entities
in BIO encoding: The start of an NE is labeled B-
{NE} and the rest of the NE is labeled I-{NE}. The
NE COUNT NEUTRAL POS NEG
PERSON 5462 80% 20% 0%
ORGANIZATION 4408 80% 20% 0%
LOCATION 1405 100% 0% 0%
URL 1030 100% 0% 0%
TIME 535 70% 10% 20%
DATE 222 100% 0% 0%
MONEY 95 90% 0% 10%
PERCENT 81 80% 20% 0%
TELEPHONE 23 100% 0% 0%
EMAIL 8 100% 0% 0%
Table 1: Distribution of named entities in our Spanish
Twitter corpus. Targeted sentiment percentages are based
on expert annotations from a random sample of 10 (or
all) of of each entity. Most entities are not sentiment tar-
gets (NEUTRAL). PERSON and ORGANIZATION are most
frequent, and among the top recipients of sentiment.
1645
full set of NE categories are shown in Table 1. For
example, the sequence ?Mark Twain? would be la-
beled B-PERSON, I-PERSON. We are interested in both
PERSON and ORGANIZATION entities, which make
up the majority of named entities in this data, and we
evaluate these using the more general entity category
VOLITIONAL. Removing retweets, 7,105 Spanish
tweets contained a total of 9,870 volitional entities
and 2,350 English tweets contained a total of 3,577
volitional entities.
Sentiment Lexicons We use two sentiment lex-
icon sources in each language. For English, we
use the MPQA lexicon (Wilson et al, 2005), which
identifies 12,296 manually and semi-automatically
produced subjective terms along with their polarity.
For the second lexicon, we use SentiWordNet 3.0
(Baccianella et al, 2010), which assigns positive and
negative polarity scores to WordNet synsets. We use
the majority polarity of all words with a subjectivity
score above 0.5.
For Spanish, the first lexicon is obtained from
Volkova et al (2013), who automatically trans-
lated strongly subjective terms from the MPQA lex-
icon (Wilson et al, 2005) into Spanish. The re-
sulting Spanish lexicon contains about 65K words.
The second lexicon is available from Perez-Rosas
et al (2012). This contains approximately 1000
sentiment-bearing words collected leveraging man-
ual resources and 2000 collected leveraging auto-
matic resources.
Annotation To collect sentiment labels, we
use crowdsourcing through Amazon?s Mechanical
Turk.5 Annotators (?Turkers?) were shown six
tweets at a time, each with a single highlighted
named entity. Turkers were instructed to (1) se-
lect the sentiment being expressed towards the en-
tity (positive, negative, or no sentiment); and (2)
rate their level of confidence in their selection. Fol-
lowing best practices on collecting language data
with Mechanical Turk (Callison-Burch and Dredze,
2010), two controls were placed among each set of
six tweets to screen out unreliable judgments. An
example prompt is shown in Figure 3.
Each ?tweet, NE? pair was shown to three Turk-
ers, and those with majority consensus on sentiment
polarity were extracted. Tweets without sentiment
5www.mturk.com/mturk
ORGANIZATION PERSON
Named Entity
Freq
uenc
y in T
weet
s
0
500
1000
1500
2000
2500
Positive
Negative
Neutral
Figure 4: Targeted sentiment annotated for Spanish.
Majority
POS NEUTRAL NEG
M
in
or
it
y POS 757 1249 130
NEUTRAL 707 2151 473
NEG 129 726 452
Table 2: Number of targeted sentiment instances where
at least two of the three annotators (Majority) agreed.
Common disagreements with a third annotator (Minority)
were over whether no sentiment or positive sentiment was
expressed, and whether no sentiment or negative sent-
ment was expressed.
consensus on all NEs were removed. In Spanish, this
yielded 6,658 unique ?tweet, NE? pairs. In English,
which is a smaller data set, this yielded 3,288 unique
pairs. We split the data into folds for 10-fold cross-
validation, developing on the data from one fold and
reporting results for the remaining nine.
The distribution of sentiment for the named en-
tities annotated by Turkers is shown in Figure 4.
Neutral (no targeted sentiment) dominates, followed
by positive sentiment for both organizations and
people. As shown in Table 2, common disagree-
ments were over whether or not there was targeted
positive sentiment, and whether or not there was
targeted negative sentiment. This is in line with
previous research showing that distinguishing pos-
itive sentiment from no sentiment (and distinguish-
ing negative sentiment from no sentiment) is often
more challenging than distinguishing between pos-
itive and negative sentiment (Wilson et al, 2009).
Indeed, we see that it was more common for annota-
tors to disagree than to agree on targeted sentiment,
particularly for negative targeted sentiment, where
more instances had NEUTRAL/NEGATIVE disagree-
ment than NEGATIVE three-way agreement.
1646
Figure 3: Example Tweet shown to Turkers.
Variable Possible values
Sentiment (s) NOT-TARG, SENT-TARG
(PIPE & JOINT models)
Named Entity (l) O, B-VOLITIONAL, I-VOLITIONAL
(PIPE & JOINT models)
Combined Sent/NE (y) O, B+NOT-TARG, I+NOT-TARG
(COLL models) B+SENT-TARG, I+SENT-TARG
Table 3: Possible values for random variables, targeted
subjectivity (is/is not sentiment target). COLL models
collapse targeted subjectivity and NE label into one node.
Variable Possible values
Sentiment (s) NOT-TARG, POS, NEG
(PIPE & JOINT models)
Named Entity (l) O, B-VOLITIONAL, I-VOLITIONAL
(PIPE & JOINT models)
Combined Sent/NE (y) O, B+NOT-TARG, I+NOT-TARG
(COLL models) B+POS, I+POS
B+NEG, I+NEG
Table 4: Possible values for random variables, targeted
sentiment. The COLL models collapse both targeted sen-
timent and NE label into one node.
4 Targeted Subjectivity and Sentiment
Formally, we define the problem as follows: Given
an observed message w = (w1 . . . wn), where n is
the number of words in the message and wj(1 ?
j ? n) is a word, we learn the probability of a
label sequence l = (l1 . . . ln), where li ? the set
of named entity values; and a sentiment sequence
s = (s1 . . . sn), where si ? the set of sentiment val-
ues. We additionally explore simpler linear-chain
models that learn the probability of a single label
sequence y = (y1 . . . yn), where yi ? the set of con-
joined entity+sentiment values (Tables 3 and 4).
Our basic model is a linear conditional random
field, an undirected graph that represents the con-
ditional distribution p(l, s|w).6 Sentiment towards
a named entity may be modeled in a CRF as a se-
6For the COLL models, this is instead the conditional distri-
bution p(y|w), where entity and sentiment labels are conjoined
in one sequence assignment y.
quence of random variables for sentiment s con-
nected to named entities l. In all models, entity vari-
ables are connected by a factor to their neighbors
in sequence, and we include skip-chains (Finkel and
Manning, 2010) connecting identical words where
at least one is capitalized. Our model strategies in-
clude: a pipeline that first learns volitional entities
then sentiment directed towards them (PIPE); one
that jointly learns volitional entities along with sen-
timent directed towards them (JOINT); and one that
learns volitional entities and targeted sentiment with
combined labels (COLL) (Figure 5).
Using these models, we explore two primary
tasks: (1) the task of detecting whether sentiment
is targeted at an entity, which we refer to as targeted
subjectivity; and (2) the task of detecting whether
positive, negative, or neutral sentiment (no senti-
ment) is targeted at an entity, which we refer to as
targeted sentiment. Moving from targeted subjectiv-
ity prediction to targeted sentiment prediction is pos-
sible by changing the sentiment target (SENT-TARG)
variable into two variables, one for positive targeted
sentiment (POS) and one for negative (NEG). Possi-
ble values for targeted subjectivity are shown in Ta-
ble 3, and possible values for targeted sentiment are
shown in Table 4.
In the pipeline models (PIPE), we first build a
CRF where each word is connected by a factor to
an entity label li ? l. In a second model, every ob-
served volitional entity node is connected by a factor
to a sentiment label si ? s. An example is shown in
Figure 5 (1).
In the joint models (JOINT), each si ? s is con-
nected by a factor to the corresponding entity label
in the sequence, li ? l. Sentiment in this model
is partially observed: All sentiment variables are
treated as latent except for the sentiment connected
to the volitional entity. An example is shown in Fig-
ure 5 (2).
1647
In the collapsed models (COLL), we combine sen-
timent and named entity into one label sequence
(e.g., O, B+SENT-TARG, I+SENT-TARG). An example
is shown in Figure 5 (3). The JOINT and PIPE mod-
els therefore predict named entity sequences, their
category labels, and the sentiment expressed towards
volitional named entities.7 The collapsed models
predict volitional labels and targeted sentiment as
combined categories. The COLL and PIPE models
are considerably faster than JOINT models, where
exact inference is intractable.
1. PIPELINE MODEL (PIPE)
Step 1: Volitional Named Step 2: Sentiment
Entity Recognition
2. JOINT MODEL 3. COLLAPSED MODEL
(JOINT) (COLL)
Figure 5: Example CRFs for targeted subjectivity with
observed variables (dark nodes), predicted variables
(white nodes) and hidden variables (light grey nodes).
5 Training
Minimum-Risk CRF Training We use the
ERMA system (Stoyanov et al, 2011) to learn our
models.8 ERMA (Empirical Risk Minimization un-
der Approximations) learns parameters to minimize
loss on the training data. Predicting NE labels using
a linear-chain CRF trained with empirical risk mini-
mization has been shown to result in a statistically
significant improvement over the common approach
of maximum likelihood estimation (Stoyanov and
Eisner, 2012). All models are trained to optimize
7We found that learning the VOLITIONAL categories dur-
ing training rather than maintaining beliefs about separate
named entities during inference (ORGANIZATION, PERSON)
and then post-processing to VOLITIONAL leads to slightly bet-
ter accuracy.
8sites.google.com/site/ermasoftware
log likelihood using 20 iterations of stochastic
gradient descent, and a maximum of 100 iterations
of belief propagation to compute the marginals for
each example.
Features Features of the models are shown in Ta-
ble 5. For an observed word, features are extracted
for the word itself as well as within a context win-
dow of three words in either direction. Words seen
only once are treated as out-of-vocabulary. Surface
features and linguistic features are concatenated in
groups of two and three to create further features.
All algorithms and code that we have developed for
feature extraction are available online.9
Because we aim to develop models that do not
heavily rely on language-specific resources, we are
interested in exploring unsupervised and lightly
supervised methods for learning relevant features.
Rather than use part-of-speech tags, we therefore
use Brown cluster labels as unsupervised word tags
(Brown et al, 1992; Koo et al, 2008). Brown
clustering is a distributional similarity method that
merges pairs of word clusters in the training data10
to create the smallest decrease in corpus likelihood,
using a bigram language model on the clusters. For
our task, we cut clusters at length 3 and length 5,
and these serve as rough part-of-speech tags without
the need to train additional models. For example,
the word hello is tagged as belonging to cluster 011
(length 3) and 01111 (length 5).
During development, we found that being able
to syllabify the word (break the word into sylla-
bles) was a positive indicator of people names, but
a negative indicator of organization names. This
observation can be approximated automatically us-
ing constraints from the sonority sequencing princi-
ple (Hooper, 1976; Clements, 1990; Blevins, 1996;
Morelli, 2003) on a language?s orthography. This
is a phonotactic principle that states that syllables
will tend to have a sonority peak, usually a vowel,
in the center of the syllable, followed on either side
by consonants with decreasing sonority. Although
languages may violate this principle, the core idea
that a vowel forms the nucleus of a syllable with op-
9www.m-mitchell.com/code
10For Spanish, we train on a sample of ?7 million Spanish
tweets. For English, we train on the essays (Pennebaker et al,
2007) and Facebook data (Kosinskia et al, 2013) available from
ICWSM 2013.
1648
tional consonants before (the onset) and after (the
coda) can be used to begin to automatically learn
syllable structure.11 We learn this in an unsuper-
vised way, using the most frequent (seen more than
1,000 times) word-initial non-vowel sequences from
the Brown cluster data as allowable syllable onset
consonants. Similarly, the most frequent word-final
non-vowel sequences are learned as possible sylla-
ble codas. For each word, we then attempt to seg-
ment syllables using the learned onsets and codas
around each vowel. If a word cannot be syllabified,
it is often an initialism (e.g., CND, lsat).
We follow the approach from the out-of-
vocabulary assignment in the Berkeley parser
(Petrov et al, 2006) to encode common surface
patterns such as capitalization and lexical patterns
such as verb endings as a single feature for words
we have seen once or less. We also use the Jer-
boa toolkit (Van Durme, 2012) to extract further
language-independent features from the data, such
as features for emoticons and binning for repeated
characters (like !!!). In addition, we include features
for whether the word is three or four letters, which
is often used for acronyms and initialisms in several
languages (including Spanish and English); whether
the word is neighbored by a punctuation mark; word
identity; word length; message length; and position
in the sentence.
We utilize a speaker of each language to simply
list word forms for sentiment features that may be
indicative of sentiment, totaling less than two hours
of annotation time. This set includes intensifiers
(e.g., hella, freakin? in English; e.g., muy, suma-
mente in Spanish), positive/negative abbreviations
(WTF, pso), positive/negative slang words, and pos-
itive/negative prefix and suffixes (e.g., anti- in En-
glish and Spanish, -ito in Spanish).
6 Experiments
We are interested in both PERSON and ORGANIZA-
TION entities, and evaluate these in the collapsed
category VOLITIONAL. This suggests that the data
may be pre-processed to label all volitional entities
as VOLITIONAL NEs, or the models may be learned
with the traditional named entities in place, and post-
11Further development is necessary to extend a similar idea
to languages that do not ordinarily mark all vowels in their or-
thography, such as Hebrew and Arabic.
SURFACE FEATURES
binned word length, message length, and sen-
tence position; Jerboa features; word identity; word
lengthening; punctuation characters, has digit; has
dash; is lower case; is 3 or 4 letters; first letter capi-
talized; more than one letter capitalized, etc.
LINGUISTIC FEATURES
function words; can syllabify; curse words; laugh
words; words for good, bad, no, my; slang words; ab-
breviations; intensifiers; subjective suffixes and pre-
fixes (such as diminutive forms); common verb end-
ings; common noun endings
BROWN CLUSTERING FEATURES
cluster at length 3; cluster at length 5
SENTIMENT FEATURES
is sentiment-bearing word; prior sentiment polarity
Table 5: Features used in model.
processed to identify those that are VOLITIONAL.
We explored results using both methods, and found
that training models on VOLITIONAL tags yielded
the best performance overall; we report numbers for
this approach below.
We compare against a baseline (BASE-NS) where
we use our volitional entity labels and assign no
sentiment directed towards the entity (the majority
case). This is a strong baseline to isolate how our
methods perform specifically for the task of identi-
fying sentiment targeted at an entity.
We report on precision, recall, and sensitivity for
the tasks of NER and targeted subjectivity/sentiment
prediction in isolation; and we report on accuracy
for the targeted subjectivity and targeted sentiment
models. For sentiment, a true positive is an instance
where the label has sentiment, and a true negative is
an instance where the label has no sentiment (neu-
tral). For NER, a true positive is an instance where
the label is a B- or I- label; a true negative is an
instance where the label is O. The three systems
are evaluated against one another for NER, subjec-
tivity (entity has/does not have sentiment expressed
towards it), and sentiment (positive/negative/no sen-
timent) using paired t-tests across folds, with a Bon-
ferroni correction to set ? to 0.02.
NER We include results for the isolated task of vo-
litional named entity recognition in Table 6. In both
Spanish and English, all three models are roughly
comparable for precision, recall, and specificity. The
task of finding O tags ? spans that are not named en-
tities ? works especially well (NE spec). Common
1649
Spanish English
Model Joint Pipe Coll Joint Pipe Coll
NE prec 65.2 64.3 65.1 59.8 62.3 60.5
NE rec 65.8 64.7 61.2 60.2 57.2 56.5
NE spec 95.4 95.2 95.6 94.3 95.1 94.7
Table 6: Average precision, recall, and specificity for vo-
litional entity NER (in %).
mistakes include confusing B- labels with I- labels.
Subjectivity and Sentiment Table 7 shows results
for the isolated task of predicting the presence of
sentiment about a volitional entity. In Spanish, the
pipeline models (PIPE) perform optimally for sub-
jectivity recall (Subj rec), and significantly above
the COLL models (p<.001). Precision and speci-
ficity are comparable across models. In English as
in Spanish, the collapsed model is particularly poor
at subjectivity recall.
As discussed in Section 2, the subtask of predict-
ing whether subjectivity is expressed towards an en-
tity is comparable to the main task of Jiang et al
(2011), and so we compare our approach here. The
Jiang et al study is similar to the current study in that
they aim to detect targeted sentiment, but it differs
from the current study in that they focus exclusively
on subjectivity towards five manually selected enti-
ties: {Obama, Google, iPad, Lakers, Lady Gaga}.
They also evaluate on artificially balanced evalu-
ation data, and evaluate sentiment polarity (posi-
tive/negative) separately from subjectivity (has/does
not have sentiment).
Our dataset includes any entity labeled as PERSON
or ORGANIZATION, and is not balanced (most tar-
gets have no sentiment expressed towards them; see
Table 1), thus we can only roughly compare against
their approach. Lakers and Lady Gaga are rare in
our collection (appearing less than 3 times), and so
we updated the comparison set prior to evaluation to:
{Obama, Google, iPad, BBC, Tebow}. On this set, a
baseline that always guesses no sentiment reaches an
accuracy of 66.9%, compared to Jiang et al?s 65.5%
accuracy on a balanced set (not strictly compara-
ble, but provided for reference). The JOINT mod-
els reach an accuracy of 71.04% on this set, demon-
strating this approach as potentially useful for topic-
dependent targeted sentiment.
Table 8 shows results for the task of predicting
the polarity of the sentiment expressed about an en-
tity. In Spanish, the PIPE models significantly out-
Spanish English
Model Joint Pipe Coll Joint Pipe Coll
Subj prec 58.3 58.8 58.9 46.6 52.2 45.9
Subj rec 40.1 50.9 19.1 44.5 48.5 16.4
Subj spec 79.6 77.5 77.8 77.6 80.8 74.0
Table 7: Average precision, recall, and specificity (in %)
for subjectivity prediction (has/does not have sentiment)
along the target entity.
Spanish English
Model Joint Pipe Coll Joint Pipe Coll
Sent prec 36.6 45.8 42.5 31.6 42.9 38.5
Sent rec 38.0 40.6 15.5 36.6 34.8 9.7
Sent spec 67.1 75.2 73.3 72.3 82.0 78.1
Table 8: Average precision, recall, and specificity (in %)
for sentiment prediction (positive/negative/no sentiment)
along the target entity.
perform the COLL models on sentiment recall, and
the JOINT models on sentiment precision (p<.01).
In English, PIPE significantly outperforms JOINT on
precision (p<.001).
Targeted Subjectivity and Targeted Sentiment
The JOINT and PIPE models work reasonably
well for the isolated tasks of NER and subjectiv-
ity/sentiment prediction. We now examine results
for targeted subjectivity ? labeling an entity and pre-
dicting whether there is sentiment directed towards
it ? in Table 9; and targeted sentiment ? labeling an
entity and predicting what the sentiment directed to-
wards it is ? in Table 10.
We evaluate using two accuracy metrics: Acc-all,
which measures the accuracy of the entire named en-
tity span along with the sentiment span; and Acc-
Bsent, which measures the accuracy of identifying
the start of a named entity (B- labels) along with
the sentiment expressed towards it. Acc-all primar-
ily measures the correctness of O labels, while Acc-
Bsent focuses on the beginning of named entities.
For the targeted subjectivity task, our JOINT mod-
els perform optimally in Spanish, and significantly
above their baselines. For the Acc-Bsent task, JOINT
models perform best, significantly outperforming
their baseline for subjectivity prediction. In English,
where our data is half the size, we do not see a statis-
tically significant difference between the predictive
models and the no sentiment baselines.
For the targeted sentiment task, the JOINT mod-
els again perform relatively well in Spanish (Table
10), labeling volitional entities, predicting whether
or not there is sentiment targeted towards them, and
1650
Model Joint Joint
Base
Pipe Pipe
Base
Coll Coll
Base
Sp
a Acc-all 89.5* 89.3 89.3** 89.1 89.5* 89.3
Acc-Bsent 32.1*** 29.5 30.9*** 28.3 30.1** 28.1
E
ng Acc-all 88.0 88.1 88.6 88.6 87.9 88.1
Acc-Bsent 30.4 30.8 30.7 30.3 28.1 29.2
***p<.001 **p<.01 *p<.05
Table 9: Average accuracy on Targeted Subjectivity Pre-
diction: Identifying volitional entities and whether they
are a sentiment target. In the core task, Acc-Bsent, the
best model in Spanish is JOINT, significantly outperform-
ing the baseline. In English, the best model (PIPE) does
not significantly improve over its baseline.
Model Joint Joint
Base
Pipe Pipe
Base
Coll Coll
Base
Sp
a Acc-all 89.4 89.4 89.0 89.0 89.2 89.3
Acc-Bsent 29.7* 29.0 30.0 29.2 28.9 29.0
E
ng Acc-all 88.0 88.1 88.2 88.4 87.7 88.1
Acc-Bsent 30.4 30.6 30.5 30.8 27.9 29.8
*p<.05
Table 10: Average accuracy on Targeted Sentiment Pre-
diction: Identifying volitional entities and the polarity
of the sentiment expressed towards them. The Spanish
JOINT models significantly improve over their baseline
for the core task. In English, no models outperform their
baseline.
the sentiment polarity above their no sentiment base-
lines. We find this to be the most difficult task: It
may be clear that sentiment is being expressed to-
wards an entity, but it is not always clear what the
polarity of that sentiment is. Error analysis is given
below in this section. In the smaller English set, the
models do not outperform the no sentiment baseline.
7 Discussion
Feature Analysis Examples of some of the top-
weighted features in the Spanish models are shown
in Table 11. In addition to lexical identity and Brown
cluster, we find that positive indicators include pos-
itive suffixes such as diminutive forms, whether the
word can be syllabized (Section 5), and whether it is
three or four letters.
Error Analysis Because it is relatively common
for there not to be sentiment targeted at a named en-
tity, it is difficult to tease out the polarity in instances
where there is targeted sentiment. Similarly, our pre-
dictions are most reliable for detecting the absence
of a named entity (O labels).
Label confusions are shown in Table 12. Mistakes
are often made by confusing B- labels (the start of
B-VOLITIONAL FEATURES
Negative is a function word; jerboa tags; followed by a word
with 3 or 4 letters that cannot be syllabified
Positive ends in -a, -o, or -s; is capitalized; has one non-
initial capital letter; is 3 or 4 letters
B-VOLITIONAL, POS FEATURES
Negative preceded by a curse word; followed by a word
with a positive suffix; immediately preceded by a
word with a negative prefix
Positive not in a sentiment lexicon; preceded by a happy
emoticon; followed by an exclamation or a ?my?
word; immediately preceded by a laugh; has two
or more sentiment-bearing words in the sentence
B-VOLITIONAL, NEG FEATURES
Negative is immediately followed by a question mark or
positive abbreviation word
Positive preceded by a ?bad? word or curse word; has four
or more sentiment lexicon items
B-VOLITIONAL, NOT-TARG FEATURES
Negative immediately followed by a ?no? word or word with
a negative prefix; is preceded by a question mark;
is immediately preceded by a curse word or laugh;
is followed by an exclamation mark
Positive not followed by sentiment lexicon word
Table 11: Example strongly weighted features for a
Spanish joint sentiment model. In addition to lexical
identity, we find that curse words and positive and neg-
ative prefixes are used to detect volitional entities and the
sentiment directed towards them.
an entity) with I- labels (inside an entity); and by
predicting sentiment polarity when the gold annota-
tions say there is not sentiment targeted at the entity.
Some example errors are shown in Figure 13. In
(1), ?CANSADO? (?TIRED?) was predicted to be
volitional, while ?Matthew? was not. In (2), ?Ma-
tias del r??o? was not predicted to be an entity, likely
due to the fact that the capitalization patterns we see
in this sentence are indicative of the start of a sen-
tence rather than a proper name (similar to 1). In (3),
a.
Observed
B I O
P
re
di
ct
ed B 423 21 186
I 36 236 135
O 197 90 7168
b.
Observed
POS NEG NEUT
POS 68 24 42
NEG 58 65 102
NEUT 115 61 468
Table 12: Predicted vs. observed values for a joint model.
(a) For named entities, most common confusions were
between B-VOLITIONAL and O labels. (b) For sentiment,
most common mistakes were to predict that a positive
sentiment was neutral (no sentiment), and that a neutral
sentiment was negative.
1651
NE prediction errors
1.
Spanish: Cuando estoy CANSADO , e?l es mi DESCANSO . Mateo . 11 : 29 .
Predicted: O O B-VOLITIONAL O O O O O O O O O O O O
Gold: O O O O O O O O O B-VOLITIONAL O O O O O
English: When I?m TIRED , he is my REST . Matthew . 11 : 29 .
2.
Spanish: Matias del r??o fue una lata . . .
Predicted: O O O O O O . . .
Gold: B-VOLITIONAL I-VOLITIONAL I-VOLITIONAL O O O . . .
English: Matias del r??o was a drag . . .
Sentiment prediction errors
3.
Spanish: Mario que dio este contigo
Predicted: NOT-TARG - - - -
Gold: POSITIVE - - - -
English: Mario may God be with you
4.
Spanish: . . . si de verdad estas en cielo , ayudame Superman !!!
Predicted: - - - - - - - - POSITIVE -
Gold: - - - - - - - - NOT-TARG -
English: . . . if you really are in the skies , help me Superman !!!
Sentiment and NE prediction errors
5.
Spanish: Salen del gobierno de Humala dos connotados izquierdistas, Giesecke y Eiguiguren
Predicted:
O O O O B-VOLITIONAL I-VOLITIONAL O O O B-VOLITIONAL O B-VOLITIONAL
- - - - NOT-TARG NOT-TARG - - - NOT-TARG - NOT-TARG
Gold:
O O O O B-VOLITIONAL O O O O B-VOLITIONAL O B-VOLITIONAL
- - - - NOT-TARG - - - - NEGATIVE - NOT-TARG
English: Leaving the Humala government are two notorious leftists , Giesecke and Eiguiguren
Table 13: Example errors made by joint models.
sentiment may not be clear without spelling correc-
tion: ?dio? should be ?dios?, meaning ?God?; other-
wise, ?dio? is the word for ?gave?. Humans can eas-
ily fix the spelling error, which changes the overall
reading of the expression. In (4), the positive polar-
ity item ?verdad? (?believe?) and the exclamation
marks (!!!) were likely used as indicators of posi-
tive sentiment; however, in this case the annotators
marked the targeted sentiment as neutral. In (5), the
?Humala? entity was predicted to be longer than it is
(?Hamala dos? or ?Hamala two?). It was also pre-
dicted that both ?Giesecke? and ?Eiguiguren? had
no sentiment expressed towards them; annotators
disagreed, with the majority of those who annotated
?Giesecke? marking negative sentiment, and the ma-
jority of those who annotated ?Eiguiguren? mark-
ing no sentiment. This highlights some of the diffi-
culty in predicting sentiment discussed in Section 3,
where annotators will often disagree as to whether
there is no sentiment or positive/negative sentiment.
During development, we found that the collapsed
model (COLL) performed best on small amounts of
data. However, as we scaled up the amount of data
we trained on, the PIPE and JOINT models signif-
icantly improved, while the COLL models did not
have significant performance gains.
8 Conclusion
We have introduced the task of open domain targeted
sentiment: predicting sentiment directed towards an
entity along with discovering the entity itself. Our
approach is developed to find targeted sentiment to-
wards both person and organization named entities
by modeling sentiment as a span along the entity.
We find that by modeling targeted sentiment in
this way, we can reliably detect entities and whether
or not they are sentiment targets above a no senti-
ment baseline. How best to determine the polarity
of the sentiment expressed towards the entity, how-
ever, is still an open issue. Our data suggests that
it is usually not clear-cut whether sentiment is being
expressed or not; the strong disagreement between
annotators suggests that detecting sentiment polar-
ity in microblogs is difficult even for humans.
In future work, we hope to explore further meth-
ods for teasing apart sentiment polarity expressed to-
wards a target. This research has achieved promis-
ing results for detecting sentiment targets without re-
lying on external supervised models, and we hope
that the features and approaches developed here can
aid in sentiment analysis in noisy text and languages
without rich linguistic resources.
1652
References
A. Agarwal, B. Xie, I. Vovsha, O. Rambow, and R. Pas-
sonneau. 2011. Sentiment analysis of twitter data. In
Proceedings of the Workshop on Language in Social
Media.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10).
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proceedings of Coling: Posters.
Adam Bermingham and Alan F Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proceedings of CIKM-2010.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in Twitter streaming data. In Proceed-
ings of the International Conference on Discovery Sci-
ence (DS-2010).
Juliette Blevins. 1996. The syllable in phonological the-
ory. In John A. Goldswmith, editor, The Handbook
of Phonological Theory. Blackwell Publishing, Black-
well Reference Online.
N. N. Bora. 2012. Summarizing public opinions in
tweets. In Proceedings of CICLing-2012.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP-2011.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J.C. Lai,
and R.L. Mercer. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
Pedro Henrique Calais Guerra, Adriano Veloso, Wagner
Meira Jr, and Virg??lio Almeida. 2011. From bias to
opinion: a transfer-learning approach to real-time sen-
timent analysis. In Proceedings of the KDD-2011.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL:HLT Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun
Wang, and Amit P. Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity
from twitter. In Proceedings of ICWSM-2012.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. Proceedings of EMNLP 2006.
G. N. Clements. 1990. The role of the sonority cycle in
core syllabification. In J. Kingston and M. Beckman,
editors, Papers in Laboratory Phonology, pages 283?
333. CUP, Cambridge.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of Coling: Posters.
Nicholas A Diakopoulos and David A Shamma. 2010.
Characterizing debate performance via aggregated
twitter sentiment. In Proceedings of CHI-2010.
David Etter, Francis Ferraro, Ryan Cotterell, Olivia
Buzek, and Benjamin Van Durme. 2013. Nerit:
Named entity recognition for informal text. Techni-
cal Report 11, Human Language Technology Center
of Excellence, Johns Hopkins University, July.
Jenny Rose Finkel and Christopher D. Manning. 2010.
Hierarchical joint learning: Improving joint parsing
and named entity recognition with non-jointly labeled
data. In Proceedings of ACL-2010.
Joan B. Hooper. 1976. The syllable in phonological the-
ory. Language, 48(3):525?540.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of KDD.
Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. 2013. Ex-
ploiting social relations for sentiment analysis in mi-
croblogging. In Proceedings of the 6th ACM Inter-
national Conference on Web Search and Data Mining
(WSDM-2013).
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single-and cross-domain setting
with conditional random fields. In Proceedings of
EMNLP.
Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao. 2011.
Target-dependent twitter sentiment classification. In
Proceedings of ACL-2011.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized
hmm-based learning framework for web opinion min-
ing. Proceedings of ICML 2009.
Soo-Min Kim and Eduard Hovy. 2006. Identifying and
analyzing judgment opinions. Proceedings of NAACL
2006.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL/HLT.
Michal Kosinskia, David Stillwell, and Thore Graepel.
2013. Private trains and attributes are predictable from
digital records of human behavior. Proc. of the Na-
tional Academy of Sciences of the USA, 110(5).
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of ICWSM-
2011.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML-2001.
1653
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010a.
Structure-aware review mining and summarization.
Proceedings of Coling 2010.
Guangxia Li, Steven CH Hoi, Kuiyu Chang, and Ramesh
Jain. 2010b. Micro-blogging sentiment detection
by collaborative online learning. In Proceedings of
ICDM-2010.
Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and De-
quan Zheng. 2012. Combining social cognitive theo-
ries with linguistic features for multi-genre sentiment
analysis. In Proceedings of the Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC-2012).
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction, and web-enhanced lexicons.
In Proceedings of CoNLL-2003.
Frida Morelli. 2003. The relative harmony of /s+stop/
onsets: Obstruent clusters and the sonority sequenc-
ing principle. In C. Fery and R. van de Vijver, edi-
tors, The syllable in optimality theory, pages 356?371.
CUP, New York.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of LREC-2010.
James W. Pennebaker, Roger J. Booth, and Martha E.
Francis. 2007. Linguistic inquiry and word count:
Liwc2007, operator?s manual.
Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-
cea. 2012. Learning sentiment lexicons in span-
ish. Proceedings of the Conference on Language Re-
sources and Evaluations (LREC 2012).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
Coling:ACL-2006.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT:EMNLP-2005.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Linguis-
tics, 37(1).
Hassan Saif, Yulan He, and Harith Alani. 2012. Allevi-
ating data sparsity for twitter sentiment analysis. Pro-
ceedings of the WWW Workshop on Making Sense of
Microposts (# MSM2012).
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter polarity classification
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the EMNLP-2011
Workshop on Unsupervised Learning in NLP.
Veselin Stoyanov and Jason Eisner. 2012. Minimum-
risk training of approximate crf-based nlp systems. In
Proceedings of NAACL:HLT-2012.
Veselin Stoyanov, Alexander Ropson, and Jason Eis-
ner. 2011. Empirical risk minimization of graphi-
cal model parameters given approximate inference, de-
coding, and model structure. In AIStats.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings of
the KDD-2011.
Benjamin Van Durme. 2012. Jerboa: A toolkit for ran-
domized and streaming algorithms. Technical report,
Human Language Technology Center of Excellence,
Johns Hopkins University.
Svitlana Volkova, Theresa Wilson, and David Yarowsky.
2013. Exploring sentiment in social media: Boot-
strapping subjectivity clues from multilingual twitter
streams. In Association for Computational Linguistics
(ACL).
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and
Ming Zhang. 2011. Topic sentiment analysis in Twit-
ter: A graph-based hashtag sentiment classification ap-
proach. In Proceedings of CIKM-2011.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of HLT-EMNLP.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman. 2009.
Recognizing contextual polarity: An exploration of
features for phrase-level sentiment analysis. Compu-
tational Linguistics, 35(3).
Bishan Yang and Claire Cardie. 2013. Joint inference for
fine-grained opinion extraction. Proceedings of ACL
2013.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Ex-
tracting sentiments about a given topic using natural
language processing techniques. In Proceedings of
ICDM-2003.
1654
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1815?1827,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploring Demographic Language Variations to Improve Multilingual
Sentiment Analysis in Social Media
Svitlana Volkova
Center for Language and
Speech Processing
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Theresa Wilson
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD
taw@jhu.edu
David Yarowsky
Center for Language and
Speech Processing
Johns Hopkins University
Baltimore, MD
yarowsky@cs.jhu.edu
Abstract
Different demographics, e.g., gender or age,
can demonstrate substantial variation in their
language use, particularly in informal contexts
such as social media. In this paper we focus on
learning gender differences in the use of sub-
jective language in English, Spanish, and Rus-
sian Twitter data, and explore cross-cultural
differences in emoticon and hashtag use for
male and female users. We show that gen-
der differences in subjective language can ef-
fectively be used to improve sentiment anal-
ysis, and in particular, polarity classification
for Spanish and Russian. Our results show
statistically significant relative F-measure im-
provement over the gender-independent base-
line 1.5% and 1% for Russian, 2% and 0.5%
for Spanish, and 2.5% and 5% for English for
polarity and subjectivity classification.
1 Introduction
Sociolinguistics and dialectology have been study-
ing the relationships between language and speech at
the phonological, lexical and morphosyntactic lev-
els and social identity for decades (Picard, 1997;
Gefen and Ridings, 2005; Holmes and Meyerhoff,
2004; Macaulay, 2006; Tagliamonte, 2006). Re-
cent studies have focused on exploring demographic
language variations in personal email communica-
tion, blog posts, and public discussions (Boneva et
al., 2001; Mohammad and Yang, 2011; Eisenstein
et al, 2010; O?Connor et al, 2010; Bamman et al,
2012). However, one area that remains largely unex-
plored is the effect of demographic language varia-
tion on subjective language use, and whether these
differences may be exploited for automatic senti-
ment analysis. With the growing commercial im-
portance of applications such as personalized rec-
ommender systems and targeted advertising (Fan
and Chang, 2009), detecting helpful product review
(Ott et al, 2011), tracking sentiment in real time
(Resnik, 2013), and large-scale, low-cost, passive
polling (O?Connor et al, 2010), we believe that sen-
timent analysis guided by user demographics is a
very important direction for research.
In this paper, we focus on gender demographics
and language in social media to investigate differ-
ences in the language used to express opinions in
Twitter for three languages: English, Spanish, and
Russian. We focus on Twitter data because of its vol-
ume, dynamic nature, and diverse population world-
wide.1 We find that some words are more or less
likely to be positive or negative in context depend-
ing on the the gender of the author. For example, the
word weakness is more likely to be used in a pos-
itive way by women (Chocolate is my weakness!)
but in a negative way by men (Clearly they know
our weakness. Argggg). The Russian word ???????
(achieve) is used in a positive way by male users and
in a negative way by female users.
Our goals of this work are to (1) explore the gen-
der bias in the use of subjective language in so-
cial media, and (2) incorporate this bias into models
to improve sentiment analysis for English, Spanish,
and Russian. Specifically, in this paper we:
? investigate multilingual lexical variations in the
use of subjective language, and cross-cultural
1As of May 2013, Twitter has 500m users (140m of them
in the US) from more than 100 countries.
1815
emoticon and hashtag usage on a large scale in
Twitter data;2
? show that gender bias in the use of subjec-
tive language can be used to improve sentiment
analysis for multiple languages in Twitter.
? demonstrate that simple, binary features repre-
senting author gender are insufficient; rather, it
is the combination of lexical features, together
with set-count features representing gender-
dependent sentiment terms that is needed for
statistically significant improvements.
To the best of our knowledge, this work is the first
to show that incorporating gender leads to signifi-
cant improvements for sentiment analysis, particu-
larly subjectivity and polarity classification, for mul-
tiple languages in social media.
2 Related Work
Numerous studies since the early 1970?s have inves-
tigated gender-language differences in interaction,
theme, and grammar among other topics (Schiffman,
2002; Sunderland et al, 2002). More recent research
has studied gender differences in telephone speech
(Cieri et al, 2004; Godfrey et al, 1992) and emails
(Styler, 2011). Mohammad and Yang (2011) ana-
lyzed gender differences in the expression of senti-
ment in love letters, hate mail, and suicide notes, and
emotional word usage across genders in email.
There has also been a considerable amount of
work in subjectivity and sentiment analysis over
the past decade, including, more recently, in mi-
croblogs (Barbosa and Feng, 2010; Bermingham
and Smeaton, 2010; Pak and Paroubek, 2010; Bifet
and Frank, 2010; Davidov et al, 2010; Li et
al., 2010; Kouloumpis et al, 2011; Jiang et al,
2011; Agarwal et al, 2011; Wang et al, 2011;
Calais Guerra et al, 2011; Tan et al, 2011; Chen
et al, 2012; Li et al, 2012). In spite of the surge of
research in both sentiment and social media, only a
limited amount of work focusing on gender identi-
fication has looked at differences in subjective lan-
guage across genders within social media. Thel-
wall (2010) found that men and women use emoti-
cons to differing degrees on MySpace, e.g., female
2Gender-dependent and independent lexical resources of
subjective terms in Twitter for Russian, Spanish and English can
be found here: http://www.cs.jhu.edu/~svitlana/
users express positive emoticons more than male
users. Other researchers included subjective patterns
as features for gender classification of Twitter users
(Rao et al, 2010). They found that the majority of
emotion-bearing features, e.g., emoticons, repeated
letters, exasperation, are used more by female than
male users, which is consistent with results reported
in other recent work (Garera and Yarowsky, 2009;
Burger et al, 2011; Goswami et al, 2009; Argamon
et al, 2007). Other related work is that of Otter-
bacher (2010), who studied stylistic differences be-
tween male and female reviewers writing product
reviews, and Mukherjee and Liu (2010), who ap-
plied positive, negative and emotional connotation
features for gender classification in microblogs.
Although previous work has investigated gen-
der differences in the use of subjective language,
and features of sentiment have been used in gender
identification, to the best of our knowledge no one
has yet investigated whether gender differences in
the use of subjective language can be exploited to
improve sentiment classification in English or any
other language. In this paper we seek to answer this
question for the domain of social media.
3 Data
For the experiments in this paper, we use three sets
of data for each language: a large pool of data (800K
tweets) labeled for gender but unlabeled for senti-
ment, plus 2K development data and 2K test data
labeled for both sentiment and gender. We use the
unlabeled data to bootstrap Twitter-specific lexicons
and investigate gender differences in the use of sub-
jective language. We use the development data for
parameter tuning while bootstrapping, and the test
data for sentiment classification.
For English, we download tweets from the corpus
created by Burger et al (2011). This dataset con-
tains 2,958,103 tweets from 184K users, excluding
retweets. Retweets are omitted because our focus is
on the sentiment of the person tweeting; in retweets,
the words originate from a different user. All users
in this corpus have gender labels, which Burger et
al. automatically extracted from self-reported gen-
der on Facebook or MySpace profiles linked to by
the Twitter users. English tweets are identified using
a compression-based language identification (LID)
1816
tool (Bergsma et al, 2012). According to LID,
there are 1,881,620 (63.6%) English tweets from
which we select a random, gender-balanced sample
of 0.8M tweets. Burger?s corpus does not include
Russian and Spanish data on the same scale as En-
glish. Therefore, for Russian and Spanish we con-
struct a new Twitter corpus by downloading tweets
from followers of region-specific news and media
Twitter feeds. We use LID to identify Russian and
Spanish tweets, and remove retweets as before. In
this data, gender is labeled automatically based on
user first and last name morphology with a precision
above 0.98 for all languages.
Sentiment labels for tweets in the development
and test sets are obtained using Amazon Mechanical
Turk. For each tweet we collect annotations from
five workers and use majority vote to determine the
final label for the tweet. Snow et al (2008) show
that for a similar task, labeling emotion and valence,
on average four non-expert labelers are needed to
achieve an expert level of annotation. Below are the
example Russian tweets labeled for sentiment:
? Pos: ??? ?? ??????? ?????? ???? ? ??-
????? ????? ???????? ???... (It is a great
pleasure to go to bed after a long day at work...)
? Neg: ????????? ???????? ???????? ??-
???? ??? ??????! (Dear Mr. Prokhorov just
buy the elections!)
? Both: ????????? ???? ?? ??????? ?????!
?? ???? ?????????? ????????? ??? ????
????? :) (It was crowded at the local market!
But I got presents for my family:-))
? Neutral: ???? ????? ?????? ????? (Kiev is
a very old city).
Table 1 gives the distribution of tweets over senti-
ment and gender labels for the development and test
sets for English (EDEV, ETEST), Spanish (SDEV,
STEST), and Russian (RDEV, RTEST).
Data Pos Neg Both Neut ? ?
EDEV 617 357 202 824 1,176 824
ETEST 596 347 195 862 1,194 806
SDEV 358 354 86 1,202 768 1,232
STEST 317 387 93 1203 700 1,300
RDEV 452 463 156 929 1,016 984
RTEST 488 380 149 983 910 1,090
Table 1: Gender and sentiment label distribution in the
development and test sets for all languages.
4 Subjective Language and Gender
To study the intersection of subjective language and
gender in social media, ideally we would have a
large corpus labeled for both. Although our large
corpus is labeled for gender, it is not labeled for sen-
timent. Only the 4K tweets for each language that
compose the development and test sets are labeled
for both gender and sentiment. Obtaining sentiment
labels for all tweets would be both impractical and
expensive. Instead we use large multilingual senti-
ment lexicons developed specifically for Twitter as
described below. Using these lexicons we can begin
to explore the relationship between subjective lan-
guage and gender in the large pool of data labeled
for gender but unlabeled for sentiment. We also
look at the relationship between gender and the use
of different hashtags and emoticons. These can be
strong indicators of sentiment in social media, and in
fact are sometimes used to create noisy training data
for sentiment analysis in Twitter (Pak and Paroubek,
2010; Kouloumpis et al, 2011).
4.1 Bootstrapping Subjectivity Lexicons
Recent work by Banea et.al (2012) classifies meth-
ods for bootstrapping subjectivity lexicons into two
types: corpus-based and dictionary-based. Corpus-
based methods extract subjectivity lexicons from
unlabeled data using different similarity metrics
to measure the relatedness between words, e.g.,
Pointwise Mutual Information (PMI). Corpus-based
methods have been used to bootstrap lexicons
for ENGLISH (Turney, 2002) and other languages,
including ROMANIAN (Banea et al, 2008) and
JAPANESE (Kaji and Kitsuregawa, 2007).
Dictionary-based methods rely on relations be-
tween words in existing lexical resources. For exam-
ple, Rao and Ravichandran (2009) construct HINDI
and FRENCH sentiment lexicons using relations in
WordNet (Miller, 1995), Rosas et. al. (2012) boot-
strap a SPANISH lexicon using SentiWordNet (Bac-
cianella et al, 2010) and OpinionFinder,3 Clematide
and Klenner (2010), Chetviorkin et al (2012) and
Abdul-Mageed et. al. (2011) automatically expand
and evaluate GERMAN, RUSSIAN and ARABIC sub-
jective lexicons.
3www.cs.pitt.edu/mpqa/opinionfinder
1817
We use the corpus-based, language-independent
approach proposed by Volkova et al (2013) to boot-
strap Twitter-specific subjectivity lexicons. To start,
the new lexicon is seeded with terms from the initial
lexicon LI . On each iteration, tweets in the unla-
beled data are labeled using the current lexicon. If a
tweet contains one or more terms from the lexicon it
is marked subjective, otherwise neutral. Tweet po-
larity is determined in a similar way, but takes into
account negation. For every term not in the lexi-
con with a frequency threshold, the probability of
that word appearing in a subjective sentence is cal-
culated. The top k terms with a subjective probabil-
ity are then added to the lexicon. Bootstrapping con-
tinues until there are no more new terms meeting the
criteria to add to the lexicon. The parameters are op-
timized using a grid search on the development data
using F-measure for subjectivity classification. In
Table 2 we report size and term polarity from the ini-
tial LI and the bootstrapped LB lexicons. Although
more sophisticated bootstrapping methods exist, this
approach has been shown to be effective for atomi-
cally learning subjectivity lexicons in multiple lan-
guages on a large scale without any external, rich,
lexical resources, e.g., WordNet, or advanced NLP
tools, e.g., syntactic parsers (Wiebe, 2000) or infor-
mation extraction tools (Riloff and Wiebe, 2003).
For English, seed terms for bootstrapping are
the strongly subjective terms in the MPQA lexicon
(Wilson et al, 2005). For Spanish and Russian, the
seed terms are obtained by translating the English
seed terms using a bi-lingual dictionary, collecting
subjectivity judgments from MTurk on the transla-
tions, filtering out translations that are not strongly
subjective, and expanding the resulting word lists
with plurals and inflectional forms.
To verify that bootstrapping does provide a bet-
ter resource than existing dictionary-expanded lexi-
cons, we compare our Twitter-specific lexicons LB
English Spanish Russian
LEI L
E
B L
S
I L
S
B L
R
I L
R
B
Pos 2.3 16.8 2.9 7.7 1.4 5.3
Neg 2.8 4.7 5.2 14.6 2.3 5.5
Total 5.1 21.5 8.1 22.3 3.7 10.8
Table 2: The initial LI and the bootstrapped LB (high-
lighted) lexicon term count (LI ? LB) with polarity
across languages (thousands).
to the corresponding initial lexicons LI and the ex-
isting state-of-the-art subjective lexicons including:
? 8K strongly subjective English terms from Sen-
tiWordNet ?E (Baccianella et al, 2010);
? 1.5K full strength terms from the Spanish sen-
timent lexicon ?S (Perez-Rosas et al, 2012);
? 5K terms from the Russian sentiment lexicon
?R (Chetviorkin and Loukachevitch, 2012).
For that we apply rule-based subjectivity classi-
fication on the test data.4 This subjectivity classi-
fier predicts that a tweet is subjective if it contains
at least one, or at least two subjective terms from
the lexicon. To make a fair comparison, we auto-
matically expand ?E with plurals and inflectional
forms, ?S with the inflectional forms for verbs, and
?R with the inflectional forms for adverbs, adjec-
tives and verbs. We report precision, recall and F-
measure results in Table 3 and show that our boot-
strapped lexicons outperform the corresponding ini-
tial lexicons and the external resources.
Subj ? 1 Subj ? 2
P R F P R F
?E 0.67 0.49 0.57 0.76 0.16 0.27
LEI 0.69 0.73 0.71 0.79 0.34 0.48
LEB 0.64 0.91 0.75 0.7 0.74 0.72
?S 0.52 0.39 0.45 0.62 0.07 0.13
LSI 0.50 0.73 0.59 0.59 0.36 0.45
LSB 0.44 0.91 0.59 0.51 0.71 0.59
?R 0.61 0.49 0.55 0.74 0.17 0.29
LRI 0.72 0.34 0.46 0.83 0.07 0.13
LRB 0.64 0.58 0.61 0.74 0.23 0.35
Table 3: Precision, recall and F-measure results for sub-
jectivity classification using the external ?, initial LI and
bootstrapped LB lexicons for all languages.
4.2 Lexical Evaluation
With our Twitter-specific sentiment lexicons, we
can now investigate how the subjective use of these
terms differs depending on gender for our three lan-
guages. Figure 1 illustrates what we expect to find.
{F} and {M} are the sets of subjective terms used
by females and males, respectively. We expect that
some terms will be used by males, but never by fe-
males, and vice-versa. The vast majority, however,
will be used by both genders. Within this set of
shared terms, many words will show little difference
4A similar rule-based approach using terms from the
MPQA lexicon is suggested by (Riloff and Wiebe, 2003).
1818
Figure 1: Gender-dependent vs. independent subjectivity
terms (+ and - indicates term polarity).
Figure 2: The distribution of gender-dependent GDep
and gender-independent GInd sentiment terms.
in their subjective use when considering gender, but
there will be some words for which gender will have
an influence. Of particular interest for our work are
words in which the polarity of a term as it is used in
context is gender-influenced, the extreme case being
terms that flip their polarity depending on the gender
of the user. Polarity may be different because the
concept represented by the term tends to be viewed
in a different light depending on gender. There are
also words like weakness in which a more positive or
more negative word sense tends to be used by men
or women. In Figure 2 we show the distribution of
gender-specific and gender-independent terms from
the LB lexicons for all languages.
To identify gender-influenced terms in our lexi-
cons, we start by randomly sampling 400K male and
400K female tweets for each language from the data.
Next, for both genders we calculate the probability
of term ti appearing in a tweet with another subjec-
tive term (Eq.1), and the probability of it appearing
with a positive or negative term (Eq.2-3) from LB .
pti(subj?g) =
c(ti, P, g) + c(ti,N, g)
c(ti, g)
, (1)
where g ? F,M and P and N are positive and nega-
tive sets of terms from the initial lexicon LI .
pti(+?g) =
c(ti, P, g)
c(ti, P, g) + c(ti,N, g)
(2)
pti(??g) =
c(ti,N, g)
c(ti, P, g) + c(ti,N, g)
(3)
We introduce a novel metric ?p+ti to measure po-
larity change across genders. For every subjective
term ti we want to maximize the difference5:
?p+ti = ?pti(+?F ) ? pti(+?M)?
s.t.
RRRRRRRRRRRR
1 ?
tfsubjti (F )
tfsubjti (M)
RRRRRRRRRRRR
? ?, tfsubjti (M) ? 0, (4)
where p(+?F ) and p(+?M) are probabilities that
term ti is positive for females and males respec-
tively; tfsubjti (F ) and tf
subj
ti (M) are correspond-
ing term frequencies (if tfsubjti (F ) > tf
subj
ti (M) the
fraction is flipped); ? is a threshold that controls
the level of term frequency similarity6. The terms
in which polarity is most strongly gender-influenced
are those with ?? 0 and ?p+ti ? 1.
Table 4 shows a sample of the most strongly
gender-influenced terms from the initial LI and the
bootstrapped LB lexicons for all languages. A plus
(+) means that the term tends to be used positively
by women and minus (?) means that the term tends
to be used positively by men. For instance, in En-
glish we found that perfecting is used with negative
polarity by male users but with positive polarity by
female users; the term dogfighting has negative po-
larity for women but positive polarity for men.
4.3 Hashtags
People may also express positive or negative senti-
ment in their tweets using hashtags. From our bal-
anced samples of 800K tweets for each language,
we extracted 611, 879, and 71 unique hashtags for
English, Spanish, and Russian, respectively. As we
did for terms in the previous section, we evaluated
the subjective use of the hashtags. Some of these are
clearly expressing sentiment (#horror), while others
seem to be topics that people are frequently opinion-
ated about (#baseball, #latingrammy, #spartak).
5One can also maximize ?p?ti = ?pti(??F ) ? pti(??M)?.
6? = 0 means term frequencies are identical for both gen-
ders; ?? 1 indicates increasing gender divergence.
1819
English Initial Terms LEI ?p
+ ? English Bootstrapped Terms LEB ?p
+ ?
perfecting + 0.7 0.2 pleaseeeeee + 0.7 0.0
weakened + 0.1 0.0 adorably + 0.6 0.4
saddened ? 0.1 0.0 creatively ? 0.6 0.5
misbehaving ? 0.4 0.0 dogfighting ? 0.7 0.5
glorifying ? 0.7 0.5 overdressed ? 1.0 0.3
Spanish Initial Terms LSI Spanish Bootstrapped Terms L
S
B
fiasco (fiasco) + 0.7 0.3 cafe?na (caffeine) + 0.7 0.5
triunfar (succeed) + 0.7 0.0 claro (clear) + 0.7 0.3
inconsciente (unconscious) ? 0.6 0.2 cancio (dog) ? 0.3 0.3
horroriza (horrifies) ? 0.7 0.3 llevara (take) ? 0.8 0.3
groseramente (rudely) ? 0.7 0.3 recomendarlo (recommend) ? 1.0 0.0
Russian Initial Terms LRI Russian Bootstrapped Terms L
R
B
?????????? (magical) + 0.7 0.3 ???????? (dream!) + 0.7 0.3
???????????? (sensational) + 0.7 0.3 ???????? (dancing) + 0.7 0.3
????????? (adorable) ? 0.7 0.0 ?????? (complicated) ? 1.0 0.0
????????? (temptation) ? 0.7 0.3 ??????????? (young) ? 1.0 0.0
??????????? (deserve) ? 1.0 0.0 ??????? (achieve) ? 1.0 0.0
Table 4: Sample of subjective terms sorted by ?p+ to show lexical differences and polarity change across genders
(module is not applied as defined in Eq.1 to demonstrate the polarity change direction).
English ?p+ ? Spanish ?p+ ? Russian ?p+ ?
#parenting + 0.7 0.0 #rafaelnarro (politician) + 1.0 0.0 #????? (advise) + 1.0 0.0
#vegas ? 0.2 0.8 #amores (loves) + 0.2 1.0 #ukrlaw + 1.0 1.0
#horror ? 0.6 0.7 #britneyspears + 0.1 0.3 #spartak (soccer team) ? 0.7 0.9
#baseball ? 0.6 0.9 #latingrammy ? 0.5 0.1 #??? (dreams) ? 1.0 0.0
#wolframalpha ? 0.7 1.0 #metallica (music band) ? 0.5 0.8 #iphones ? 1.0 1.0
Table 5: Hashtag examples with opposite polarity across genders for English, Spanish, and Russian.
Table 5 gives the hashtags, correlated with sub-
jective language, that are most strongly gender-
influenced. Analogously to ?p+ values in Table 4, a
plus (+) means the hashtag is more likely to be used
positively by women, and a minus (?) means the
hashtag is more likely to be used positively by men.
For example, in English we found that male users
tend to express positive sentiment in tweets men-
tioning #baseball, while women tend to be nega-
tive about this hashtag. The opposite is true for the
hashtag #parenting.
4.4 Emoticons
We investigate how emoticons are used differently
by men and women in social media following the
work by (Bamman et al, 2012). For that we rely on
the lists of emoticons from Wikipedia7 and present
the cross-cultural and gender emoticon differences
in Figure 3. The frequency of each emoticon is given
7List of emoticons from Wikipedia http://en.
wikipedia.org/wiki/List_of_emoticons
on the right of each language chart, with probability
of use by a male user in that language given on the
x-axis. The top 8 emoticons are the same across lan-
guages and sorted by English frequency.
We found that emoticons in English data are used
more overall by female users, which is consistent
with previous findings in Schnoebelen?s work.8 In
addition, we found that some emoticons like :-)
(smile face) and :-o (surprised) are used equally by
both genders, at least in Twitter. When comparing
English emoticon usage to other languages, there are
some similarities, but also some clear differences. In
Spanish data, several emoticons are more likely to be
used by male than by female users, e.g., :-o (sur-
prised) and :-& (tongue-tied), and the difference in
probability of use by males and females is greater
for the emoticons, as compared to the same emoti-
cons for English. Interestingly, in Russian Twitter
8Language and emotion (talks, essays and reading notes)
www.stanford.edu/~tylers/emotions.shtml
1820
p(Male|Emoticon)
0.0 0.2 0.4 0.6 0.8 1.0
 34.4K
 8.7K 
 4.1K 
 2.7K 
 0.9K 
 0.7K 
 0.4K 
 0.1K 
 0.1K 
 0.1K 
:)  
:(  
:-)  
:-&  
:-(  
:[  
:-/  
8)  
()  
:-o  
p(Male|Emoticon)
0.0 0.2 0.4 0.6 0.8 1.0
 19.1K
 9.5K 
 1.5K 
 0.1K 
 0.3K 
 0.3K 
 0.1K 
 1.5K 
 0.1K 
 0.1K 
:)  
:(  
:-)  
:-&  
:-(  
:[  
:-/  
8)  
%)  
:-o  
p(Male|Emoticon)
0.0 0.2 0.4 0.6 0.8 1.0
 41.5K
 4.5K 
 4.6K 
 0.4K 
 0.4K 
 0.1K 
 0.1K 
 0.4K 
 0.4K 
 0.1K 
:)  
:(  
:-)  
:-&  
:-(  
:[  
:-/  
8)  
%)  
()  
Figure 3: Probability of gender and emoticons for English, Spanish and Russian (from left to right).
data emoticons tend to be used more or equally by
male users rather than female users.
5 Experiments
The previous section showed that there are gender
differences in the use of subjective language, hash-
tags, and emoticons in Twitter. We aim leverage
these differences to improve subjectivity and po-
larity classification for the informal, creative and
dynamically changing multilingual Twitter data.9
For that we conduct experiments using gender-
independent GInd and gender-dependent GDep
features and compare the results to evaluate the in-
fluence of gender on sentiment classification.
We experiment with two classification ap-
proaches: (I) rule-based classifier which uses only
subjective terms from the lexicons designed to verify
if the gender differences in subjective language cre-
ate enough of a signal to influence sentiment classifi-
cation; (II) state-of-the-art supervised models which
rely on lexical features as well as lexicon set-count
features.10,11 Moreover, to show that the gender-
9For polarity classification we distinguish between positive
and negative instances, which is the approach typically reported
in the literature for recognizing polarity (Velikovich et al, 2010;
Yessenalina and Cardie, 2011; Taboada et al, 2011)
10A set-count feature is a count of the number of instances
from a set of terms that appears in a tweet.
11We also experimented with repeated punctuation (!!, ??)
and letters (nooo, reealy), which are often used in sentiment
classification in social media. However, we found these features
sentiment signal can be learned by more than one
classifier we apply a variety of classifiers imple-
mented in Weka (Hall et al, 2009). For that we do
10-fold cross validation over English, Spanish, and
Russian test data (ETEST, STEST and RTEST) la-
beled with subjectivity (pos, neg, both vs. neut) and
polarity (pos vs. neg) as described in Section 3.
5.1 Models
For the rule-basedGIndRBsubj classifier, tweets are la-
beled as subjective or neutral as follows:
GIndRBsubj = {
1 if w? ? f? ? 0.5,
0 otherwise
(5)
where w? ? f? stands for weighted set features, e.g.,
terms from LI only, emoticons E, or different part-
of-speech tags (POS) from LB weighted using w =
p(subj) = p(subj?M) + p(subj?F ) subjectivity
score as shown in Eq.1. We experiment with the
POS tags to show the contribution of each POS to
sentiment classification.
Similarly, for the rule-based GIndRBpol classifier,
tweets are labeled as positive or negative:
GIndRBpol = {
1 if w?+ ? f?+ ? w?? ? f??,
0 otherwise
(6)
where f?+, f?? are feature sets that include only posi-
tive and negative features fromLI orLB;w+ andw?
to be noisy and adding them decreased performance.
1821
0.6 0.7 0.8 0.9
0.62
0.64
0.66
0.68
0.70
Recall
Preci
sion
+E
+A
+R
+V
+N
L_I
+E
+A
+R
+V
+N
L_I
(a) Rule-based subjectivity
0.65 0.70 0.75 0.80 0.85 0.90
0.65
0.70
0.75
0.80
0.85
Recall
Preci
sion
L_I
+E
+A
+R
+V
+N
L_I
+E
+A
+R
+V
+N
(b) Rule-based polarity
BL R N B AB RF J48 SVM
C lassifiers
F-me
asure
0.55
0.60
0.65
0.70
0.75
0.80
0.85
GIn d Subj
AN D
GDepSubj
AN D
GIndPol
AN D
GDepPol
AN D
(c) SL subjectivity and polarity
Figure 4: Rule-based (RB) and Supervised Learning (SL) sentiment classification results for English. LI - the initial
lexicon, E - emoticons, A,R,V,N are adjectives, adverbs, verbs, nouns from LB .
are positive and negative polarity scores estimated
using Eq.2 - 3 such as: w+ = p(+?M) + p(+?F ) and
w? = p(??M) + p(??F ).
The gender-dependent rule-based classifiers are
defined in a similar way. Specifically, f? is replaced
by f?M and f?F in Eq.5 and f??, f?+ are replaced
by f?M?, f?F? and f?M+, f?F+ respectively in Eq.6.
We learn subjectivity s? and polarity p? score vectors
using Eq.1-3. The difference between GInd and
GDep models is that GInd scores w?, w?+ and w??
are not conditioned on gender.
For gender-independent classification using su-
pervised models, we build feature vectors using lex-
ical features V represented as term frequencies, to-
gether with set-count features from the lexicons:
f?GIndsubj = [LI , LB,E, V ];
f?GIndpol = [L
+
I , L
+
B,E
+, L?I , L
?
B,E
?, V ].
Finally, for gender-dependent supervised models,
we try different feature combinations. (A) We ex-
tract set-count features for gender-dependent subjec-
tive terms from LI , LB, and E jointly:
f?GDep?Jsubj = [L
M
I , L
M
B ,E
M , LFI , L
F
B,E
F , V ];
f?Dep?Jpol = [L
M+
I , L
M+
B ,E
M+, LF+I , L
F+
B ,E
F+
LM?I , L
M?
B ,E
M?, LF?I , L
F?
B ,E
F?, V ].
(B) We extract disjoint (prefixed) gender-specific
features (in addition to lexical features V ) by rely-
ing only on female set-count features when classify-
ing female tweets; and only male set-count features
for male tweets. We refer to the joint features as
GInd?J andGDep?J , and to the disjoint features
GInd ?D and GDep ?D.
5.2 Results
Figures 4a and 4b show performance improvements
for subjectivity and polarity classification under the
rule-based approach when taking into account gen-
der. The left figure shows precision-recall curves
for subjective vs. neutral classification, and the mid-
dle figure shows precision-recall curves for positive
vs. negative classification. We measure performance
starting with features from LI , and then incremen-
tally add emoticon features E and features from LB
one part of speech at a time to show the contribution
of each part of speech for sentiment classification.12
This experiment shows that there is a clear improve-
ment for the models parameterized with gender, at
least for the simple, rule-based model.
For the supervised models we experiment with
a variety of learners for English to show that gen-
der differences in subjective language improve sen-
timent classification for many learning algorithms.
We present the results in Figure 4c. For subjectiv-
ity classification, Support Vector Machines (SVM),
Naive Bayes (NB) and Bayesian Logistic Regres-
sion (BLR) achieve the best results, with improve-
ments in F-measure ranging from 0.5 - 5%. The po-
larity classifiers overall achieve much higher scores,
with improvements for GDep features ranging from
1-2%. BLR with Gaussian prior is the top scorer
12POS from the Twitter POSTagger (Gimpel et al, 2011).
1822
P R F A Arand P R F A Arand
English subj vs. neutral p(subj)=0.57 English pos vs. neg p(pos)=0.63
GIndLR 0.62 0.58 0.60 0.66 ? 0.78 0.83 0.80 0.71 ?
GDep ? J 0.64 0.62 0.63 0.68 0.66 0.80 0.83 0.82 0.73 0.70
?R,% +3.23 +6.90 +5.00 +3.03 3.03? +2.56 0.00 +2.50 +2.82 4.29?
GIndSVM 0.66 0.70 0.68 0.72 ? 0.79 0.86 0.82 0.77 ?
GDep ?D 0.66 0.71 0.68 0.72 0.70 0.80 0.87 0.83 0.78 0.76
?R,% ?0.45 +0.71 0.00 ?0.14 2.85? +0.38 +0.23 +0.24 +0.41 2.63?
Spanish subj vs. neutral p(subj)=0.40 Spanish pos vs. neg p(pos)=0.45
GIndLL 0.67 0.71 0.68 0.61 ? 0.71 0.63 0.67 0.71 ?
GDep ? J 0.67 0.72 0.69 0.62 0.61 0.72 0.65 0.68 0.71 0.68
?R,% 0.00 +1.40 +0.58 +0.73 1.64? +2.53 +3.17 +1.49 0.00 4.41?
GIndSVM 0.68 0.79 0.73 0.65 ? 0.66 0.65 0.65 0.69 ?
GDep ?D 0.68 0.79 0.73 0.66 0.65 0.68 0.67 0.67 0.71 0.68
?R,% +0.35 +0.21 +0.26 +0.54 1.54? +2.43 +2.44 +2.51 +2.08 4.41?
Russian subj vs. neutral p(subj)=0.51 Russian pos vs. neg p(pos)=0.58
GIndLR 0.66 0.68 0.67 0.67 ? 0.66 0.72 0.69 0.62 ?
GDep ? J 0.66 0.69 0.68 0.67 0.66 0.68 0.73 0.70 0.64 0.63
?R,% 0.00 +1.47 +0.75 0.00 1.51? +3.03 +1.39 +1.45 +3.23 1.58?
GIndSVM 0.67 0.75 0.71 0.70 ? 0.64 0.73 0.68 0.62 ?
GDep ?D 0.67 0.76 0.71 0.70 0.69 0.65 0.74 0.69 0.63 0.62
?R,% ?0.30 +1.46 +0.56 +0.14 1.44? +0.93 +1.92 +1.46 +1.49 1.61?
Table 6: Sentiment classification results obtained using gender-dependent and gender-independent joint and disjoint
features for Logistic Regression (LR) and SVM models.
for polarity classification with an F-measure of 82%.
We test our results for statistical significance us-
ing McNemar?s Chi-squared test (p-value < 0.01) as
suggested by Dietterich (1998). Only three classi-
fiers, J48, AdaBoostM1 (AB) and Random Forest
(RF) do not always show significant improvements
for GDep features over GInd features. However,
for the majority of classifiers, GDep models outper-
formGIndmodels for both tasks, demonstrating the
robustness of GDep features for sentiment analysis.
In Table 6 we report results for subjectivity and
polarity classification using the best performing
classifiers (as shown in Figure 4c) :
- Logistic Regression (LR) (Genkin et al, 2007)
for GInd ? J and GDep ? J models.
- SVM model with radial-based kernel for
GInd ? D and GDep ? D models. We use
LibSVM implementation (EL-Manzalawy and
Honavar, 2005).
Each ?R(%) row shows the relative percent im-
provements in terms of precision P , recall R, F-
measure F and accuracy A for GDep compared to
GInd models. Our results show that differences in
subjective language across genders can be exploited
to improve sentiment analysis, not only for English
but for multiple languages. For Spanish and Russian
results are lower for subjectivity classification, we
suspect, because lexical features V are already in-
flected for gender and set-count features are down-
weighted by the classifier. For polarity classifica-
tion, on the other hand, gender-dependent features
provide consistent, significant improvements (1.5-
2.5%) across all languages.
As a reality check, Table 6 also reports accuracies
(in Arand columns) for experiments that use random
permutations of male and female subjective terms,
which are then encoded as gender-dependent set-
count features as before. We found that all gender-
dependent models, GDep ? J and GDep ?D, out-
performed their random equivalents for both subjec-
tivity and polarity classification (as reflected by rel-
ative accuracy decrease ? forArand compared toA).
These results further confirm the existence of gen-
der bias in subjective language for any of our three
languages and its importance for sentiment analysis.
Finally, we check whether encoding gender as
a binary feature would be sufficient to improve
sentiment classification. For that we encode fea-
1823
English Spanish Russian
P R P R P R
(a) 0.73 0.93 0.68 0.63 0.66 0.74
(b) 0.72 0.94 0.69 0.64 0.66 0.74
(c) 0.78 0.83 0.71 0.63 0.66 0.72
(d) 0.69 0.93 0.71 0.62 0.65 0.76
(e) 0.80 0.83 0.72 0.65 0.68 0.73
Table 7: Precision and recall results for polarity classifi-
cation: encoding gender as a binary feature vs. gender-
dependent features GDep ? J .
tures such as: (a) unigram term frequencies V , (b)
term frequencies and gender binary V +GBin, (c)
gender-independent GInd, (d) gender-independent
and gender binary GBin + GInd, and (e) gender-
dependent GDep ? J . We train logistic-regression
model for polarity classification and report precision
and recall results in Table 7. We observe that includ-
ing gender as a binary feature does not yield signif-
icant improvements compared to GDep ? J for all
three languages.
6 Conclusions
We presented a qualitative and empirical study that
analyses substantial and interesting differences in
subjective language between male and female users
in Twitter, including hashtag and emoticon usage
across cultures. We showed that incorporating au-
thor gender as a model component can significantly
improve subjectivity and polarity classification for
English (2.5% and 5%), Spanish (1.5% and 1%) and
Russian (1.5% and 1%). In future work we plan to
develop new models for joint modeling of personal-
ized sentiment, user demographics e.g., age and user
preferences e.g., political favorites in social media.
Acknowledgments
The authors thank the anonymous reviewers for
helpful comments and suggestions.
References
Muhammad Abdul-Mageed, Mona T. Diab, and Mo-
hammed Korayem. 2011. Subjectivity and sentiment
analysis of modern standard Arabic. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: short papers - Volume 2, pages 587?591.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media (LSM?11), pages 30?38.
Shlomo Argamon, Moshe Koppel, James W. Pen-
nebaker, and Jonathan Schler. 2007. Min-
ing the blogosphere: Age, gender and the va-
rieties of self-expression. First Monday, 12(9).
http://www.firstmonday.org/ojs/index.php/fm/article/
view/2003/1878.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
pages 2200?2204.
David Bamman, Jacob Eisenstein, and Tyler Schnoebe-
len. 2012. Gender in Twitter: styles, stances, and so-
cial networks. Computing Research Repository.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2008.
A bootstrapping method for building subjectivity lex-
icons for languages with scarce resources. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC?08), pages
2764?2767.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on Twitter from biased and noisy
data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (COLING?10),
pages 36?44.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media (LSM?12), pages 65?74.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proceedings of the 19th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM?10), pages 1833?1836.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in Twitter streaming data. In Proceed-
ings of the 13th International Conference on Discovery
Science (DS?10), pages 1?15.
Bonka Boneva, Robert Kraut, and David Frohlich. 2001.
Using email for personal relationships: The differ-
ence gender makes. American Behavioral Scientist,
45(3):530?549.
John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on Twit-
ter. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1301?1309.
1824
Pedro Henrique Calais Guerra, Adriano Veloso, Wagner
Meira Jr, and Virg?lio Almeida. 2011. From bias to
opinion: a transfer-learning approach to real-time sen-
timent analysis. In Proceedings of the17th Interna-
tional Conference on Knowledge Discovery and Data
Mining (KDD?11), pages 150?158.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun
Wang, and Amit P. Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity
from Twitter. In Proceedings of the Sixth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM?12), pages 50?57.
Ilia Chetviorkin and Natalia V. Loukachevitch. 2012.
Extraction of Russian sentiment lexicon for prod-
uct meta-domain. In Proceedings of the 25rd In-
ternational Conference on Computational Linguistics
(COLING?12), pages 593?610.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher corpus: a resource for the next gen-
erations of speech-to-text. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation (LREC?04), pages 69?71.
Simon Clematide and Manfred Klenner. 2010. Eval-
uation and extension of a polarity lexicon for Ger-
man. In Proceedings of the Workshop on Computa-
tional Approaches to Subjectivity and Sentiment Anal-
ysis (WASSA?10), pages 7?13.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING?10), pages 241?249.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Computation, 10(7):1895?1923.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for ge-
ographic lexical variation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP?10), pages 1277?1287.
Yasser EL-Manzalawy and Vasant Honavar, 2005.
WLSVM: Integrating LibSVM into Weka Environment.
http://www.cs.iastate.edu/ yasser/wlsvm.
Teng-Kai Fan and Chia-Hui Chang. 2009. Sentiment-
oriented contextual advertising. Advances in Informa-
tion Retrieval, 5478:202?215.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 710?718.
David Gefen and Catherine M. Ridings. 2005. If you
spoke as she does, sir, instead of the way you do: a so-
ciolinguistics perspective of gender differences in vir-
tual communities. SIGMIS Database, 36(2):78?92.
Alexander Genkin, David D. Lewis, and David Madigan.
2007. Large-scale Bayesian logistic regression for text
categorization. Technometrics, 49:291?304.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2, pages 42?47.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: telephone speech corpus
for research and development. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP?92), pages 517?520.
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009. Stylometric analysis of bloggers age and gen-
der. In Proceedings of AAAI Conference on Weblogs
and Social Media, pages 214?217.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Exploratory Newsletter, 11(1):10?18.
Janet Holmes and Miriam Meyerhoff. 2004. The Hand-
book of Language and Gender. Blackwell Publishing.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 151?160.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building
lexicon for sentiment analysis from massive collection
of HTML documents. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP?07), pages 1075?1083.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM?11), pages 538?541.
Guangxia Li, Steven Hoi, Kuiyu Chang, and Ramesh
Jain. 2010. Micro-blogging sentiment detection
by collaborative online learning. In Proceedings of
IEEE 10th International Conference on Data Mining
(ICDM?10), pages 893?898.
Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and
Dequan Zheng. 2012. Combining social cognitive
theories with linguistic features for multi-genre senti-
ment analysis. In Proceedings of the 26th Pacific Asia
1825
Conference on Language,Information and Computa-
tion (PACLIC?12), pages 27?136.
Ronald Macaulay. 2006. Pure grammaticalization: The
development of a teenage intensifier. Language Varia-
tion and Change, 18(03):267?283.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2012.
Multilingual subjectivity and sentiment analysis. In
Proceedings of the Association for Computational Lin-
guistics (ACL?12).
George A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?41.
Saif Mohammad and Tony Yang. 2011. Tracking senti-
ment in mail: How genders differ on emotional axes.
In Proceedings of the 2nd Workshop on Computa-
tional Approaches to Subjectivity and Sentiment Anal-
ysis (WASSA?11), pages 70?79.
Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?10), pages 207?217.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. A mixture model of de-
mographic lexical variation. In Proceedings of NIPS
Workshop on Machine Learning in Computational So-
cial Science, pages 1?7.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
309?319.
Jahna Otterbacher. 2010. Inferring gender of movie re-
viewers: exploiting writing style, content and meta-
data. In Proceedings of the 19th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM?10), pages 369?378.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
pages 1320?1326.
Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-
cea. 2012. Learning sentiment lexicons in Spanish.
In Proceedings of the 8th International Conference
on Language Resources and Evaluation (LREC?12),
pages 3077?3081.
Rosalind W. Picard. 1997. Affective computing. MIT
Press.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL?09),
pages 675?682.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Contents
(SMUC?10), pages 37?44.
Philip Resnik. 2013. Getting real(-time) with live
polling. http://vimeo.com/68210812.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP?03), pages 105?
112.
Harold Schiffman. 2002. Bibliography of gender and
language. http://ccat.sas.upenn.edu/ haroldfs/popcult/
bibliogs/gender/genbib.htm.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?08), pages 254?263.
Will Styler. 2011. The EnronSent Corpus.
Technical report, University of Colorado
at Boulder Institute of Cognitive Science.
http://verbs.colorado.edu/enronsent/.
Jane Sunderland, Ren-Feng Duann, and Paul
Bake. 2002. Gender and genre bibliography.
www.ling.lancs.ac.uk/pubs/clsl/clsl122.pdf.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Sali A. Tagliamonte. 2006. Analysing Sociolinguistic
Variation. Cambridge University Press, 1st. Edition.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings of
the 17th International Conference on Knowledge Dis-
covery and Data Mining (KDD?11), pages 1397?1405.
Mike Thelwall, David Wilkinson, and Sukhvinder Uppal.
2010. Data mining emotion in social network com-
munication: Gender differences in MySpace. Journal
of the American Society for Information Science and
Technology, 61(1):190?199.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics
(ACL?02), pages 417?424.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability
of web-derived polarity lexicons. In Proceedings of
1826
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL?10), pages 777?785.
Svitlana Volkova, Theresa Wilson, and David Yarowsky.
2013. Exploring sentiment in social media: Boot-
strapping subjectivity clues from multilingual Twitter
streams. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL?13), pages 505?510.
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and
Ming Zhang. 2011. Topic sentiment analysis in Twit-
ter: A graph-based hashtag sentiment classification ap-
proach. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM?11), pages 1031?1040.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence (AAAI?00, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP?05), pages 347?354.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP?11), pages
172?182.
1827
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 306?314,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
CLex: A Lexicon for Exploring Color, Concept and Emotion
Associations in Language
Svitlana Volkova
Johns Hopkins University
3400 North Charles
Baltimore, MD 21218, USA
svitlana@jhu.edu
William B. Dolan
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
billdol@microsoft.com
Theresa Wilson
HLTCOE
810 Wyman Park Drive
Baltimore, MD 21211, USA
taw@jhu.edu
Abstract
Existing concept-color-emotion lexicons
limit themselves to small sets of basic emo-
tions and colors, which cannot capture the
rich pallet of color terms that humans use
in communication. In this paper we begin
to address this problem by building a novel,
color-emotion-concept association lexicon
via crowdsourcing. This lexicon, which we
call CLEX, has over 2,300 color terms, over
3,000 affect terms and almost 2,000 con-
cepts. We investigate the relation between
color and concept, and color and emotion,
reinforcing results from previous studies, as
well as discovering new associations. We
also investigate cross-cultural differences in
color-emotion associations between US and
India-based annotators.
1 Introduction
People typically use color terms to describe the
visual characteristics of objects, and certain col-
ors often have strong associations with particu-
lar objects, e.g., blue - sky, white - snow. How-
ever, people also take advantage of color terms to
strengthen their messages and convey emotions in
natural interactions (Jacobson and Bender, 1996;
Hardin and Maffi, 1997). Colors are both indica-
tive of and have an effect on our feelings and emo-
tions. Some colors are associated with positive
emotions, e.g., joy, trust and admiration and some
with negative emotions, e.g., aggressiveness, fear,
boredom and sadness (Ortony et al 1988).
Given the importance of color and visual de-
scriptions in conveying emotion, obtaining a
deeper understanding of the associations between
colors, concepts and emotions may be helpful for
many tasks in language understanding and gener-
ation. A detailed set of color-concept-emotion as-
sociations (e.g., brown - darkness - boredom; red -
blood - anger) could be quite useful for sentiment
analysis, for example, in helping to understand
what emotion a newspaper article, a fairy tale, or
a tweet is trying to evoke (Alm et al 2005; Mo-
hammad, 2011b; Kouloumpis et al 2011). Color-
concept-emotion associations may also be useful
for textual entailment, and for machine translation
as a source of paraphrasing.
Color-concept-emotion associations also have
the potential to enhance human-computer inter-
actions in many real- and virtual-world domains,
e.g., online shopping, and avatar construction in
gaming environments. Such knowledge may al-
low for clearer and hopefully more natural de-
scriptions by users, for example searching for
a sky-blue shirt rather than blue or light blue
shirt. Our long term goal is to use color-emotion-
concept associations to enrich dialog systems
with information that will help them generate
more appropriate responses to users? different
emotional states.
This work introduces a new lexicon of color-
concept-emotion associations, created through
crowdsourcing. We call this lexicon CLEX1. It
is comparable in size to only two known lexi-
cons: WORDNET-AFFECT (Strapparava and Val-
itutti, 2004) and EMOLEX (Mohammad and Tur-
ney, 2010). In contrast to the development of
these lexicons, we do not restrict our annotators
to a particular set of emotions. This allows us to
1Available for download at:
http://research.microsoft.com/en-us/
downloads/
Questions about the data and the access process may be
sent to svitlana@jhu.edu
306
collect more linguistically rich color-concept an-
notations associated with mood, cognitive state,
behavior and attitude. We also do not have any
restrictions on color naming, which helps us to
discover a rich lexicon of color terms and collo-
cations that represent various hues, darkness, sat-
uration and other natural language collocations.
We also perform a comprehensive analysis of
the data by investigating several questions includ-
ing: What affect terms are evoked by a certain
color, e.g., positive vs. negative? What con-
cepts are frequently associated with a particular
color? What is the distribution of part-of-speech
tags over concepts and affect terms in the data col-
lected without any preselected set of affect terms
and concepts? What affect terms are strongly as-
sociated with a certain concept or a category of
concepts and is there any correlation with a se-
mantic orientation of a concept?
Finally, we share our experience collecting the
data using crowdsourcing, describe advantages
and disadvantages as well as the strategies we
used to ensure high quality annotations.
2 Related Work
Interestingly, some color-concept associations
vary by culture and are influenced by the tra-
ditions and beliefs of a society. As shown in
(Sable and Akcay, 2010) green represents danger
in Malaysia, envy in Belgium, love and happiness
in Japan; red is associated with luck in China and
Denmark, but with bad luck in Nigeria and Ger-
many and reflects ambition and desire in India.
Some expressions involving colors share the
same meaning across many languages. For in-
stance, white heat or red heat (the state of high
physical and mental tension), blue-blood (an aris-
tocrat, royalty), white-collar or blue collar (of-
fice clerks). However, there are some expres-
sions where color associations differ across lan-
guages, e.g., British or Italian black eye becomes
blue in Germany, purple in Spain and black-butter
in France; your French, Italian and English neigh-
bors are green with envy while Germans are yel-
low with envy (Bortoli and Maroto, 2001).
There has been little academic work on con-
structing color-concept and color-emotion lexi-
cons. The work most closely related to ours
collects concept-color (Mohammad, 2011c) and
concept-emotion (EMOLEX) associations, both
relying on crowdsourcing. His project involved
collecting color and emotion annotations for
10,170 word-sense pairs from Macquarie The-
saurus2. They analyzed their annotations, looking
for associations with the 11 basic color terms from
Berlin and Key (1988). The set of emotion labels
used in their annotations was restricted to the set
of 8 basic emotions proposed by Plutchik (1980).
Their annotators were restricted to the US, and
produced 4.45 annotations per word-sense pair on
average.
There is also a commercial project from Cym-
bolism3 to collect concept-color associations. It
has 561,261 annotations for a restricted set of 256
concepts, mainly nouns, adjectives and adverbs.
Other work on collecting emotional aspect
of concepts includes WordNet-Affect (WNA)
(Strapparava and Valitutti, 2004), the General En-
quirer (GI) (Stone et al 1966), Affective Forms
of English Words (Bradley and Lang, 1999) and
Elliott?s Affective Reasoner (Elliott, 1992).
The WNA lexicon is a set of affect terms from
WordNet (Miller, 1995). It contains emotions,
cognitive states, personality traits, behavior, at-
titude and feelings, e.g., joy, doubt, competitive,
cry, indifference, pain. Total of 289 affect terms
were manually extracted, but later the lexicon was
extended using WordNet semantic relationships.
WNA covers 1903 affect terms - 539 nouns, 517
adjectives, 238 verbs and 15 adverbs.
The General Enquirer covers 11,788 concepts
labeled with 182 category labels including cer-
tain affect categories (e.g., pleasure, arousal, feel-
ing, pain) in addition to positive/negative seman-
tic orientation for concepts4.
Affective Forms of English Words is a work
which describes a manually collected set of nor-
mative emotional ratings for 1K English words
that are rated in terms of emotional arousal (rang-
ing from calm to excited), affective valence (rang-
ing from pleasant to unpleasant) and dominance
(ranging from in control to dominated).
Elliott?s Affective Reasoner is a collection of
programs that is able to reason about human emo-
tions. The system covers a set of 26 emotion cat-
egories from Ortony et al1988).
Kaya (2004) and Strapparava and Ozbal (2010)
both have worked on inferring emotions associ-
ated with colors using semantic similarity. Their
2http://www.macquarieonline.com.au
3http://www.cymbolism.com/
4http://www.wjh.harvard.edu/?inquirer/
307
research found that Americans perceive red as ex-
citement, yellow as cheer, purple as dignity and
associate blue with comfort and security. Other
research includes that geared toward discovering
culture-specific color-concept associations (Gage,
1993) and color preference, for example, in chil-
dren vs. adults (Ou et al 2011).
3 Data Collection
In order to collect color-concept and color-
emotion associations, we use Amazon Mechani-
cal Turk5. It is a fast and relatively inexpensive
way to get a large amount of data from many cul-
tures all over the world.
3.1 MTurk and Data Quality
Amazon Mechanical Turk is a crowdsourcing
platform that has been extensively used for ob-
taining low-cost human annotations for various
linguistic tasks over the last few years (Callison-
Burch, 2009). The quality of the data obtained
from non-expert annotators, also referred to as
workers or turkers, was investigated by Snow et
al (2008). Their empirical results show that the
quality of non-expert annotations is comparable
to the quality of expert annotations on a variety of
natural language tasks, but the cost of the annota-
tion is much lower.
There are various quality control strategies that
can be used to ensure annotation quality. For in-
stance, one can restrict a ?crowd? by creating a
pilot task that allows only workers who passed
the task to proceed with annotations (Chen and
Dolan, 2011). In addition, new quality control
mechanisms have been recently introduced e.g.,
Masters. They are groups of workers who are
trusted for their consistent high quality annota-
tions, but to employ them costs more.
Our task required direct natural language in-
put from workers and did not include any mul-
tiple choice questions (which tend to attract more
cheating). Thus, we limited our quality control ef-
forts to (1) checking for empty input fields and (2)
blocking copy/paste functionality on a form. We
did not ask workers to complete any qualification
tasks because it is impossible to have gold stan-
dard answers for color-emotion and color-concept
associations. In addition, we limited our crowd to
5http://www.mturk.com
a set of trusted workers who had been consistently
working on similar tasks for us.
3.2 Task Design
Our task was designed to collect a linguistically
rich set of color terms, emotions, and concepts
that were associated with a large set of colors,
specifically the 152 RGB values corresponding to
facial features of cartoon human avatars. In to-
tal we had 36 colors for hair/eyebrows, 18 for
eyes, 27 for lips, 26 for eye shadows, 27 for fa-
cial mask and 18 for skin. These data is necessary
to achieve our long-term goal which is to model
natural human-computer interactions in a virtual
world domain such as the avatar editor.
We designed two MTurk tasks. For Task 1, we
showed a swatch for one RGB value and asked
50 workers to name the color, describe emotions
this color evokes and define a set of concepts as-
sociated with that color. For Task 2, we showed a
particular facial feature and a swatch in a particu-
lar color, and asked 50 workers to name the color
and describe the concepts and emotions associ-
ated with that color. Figure 1 shows what would
be presented to worker for Task 2.
Q1. How would you name this color?
Q2. What emotion does this color evoke?
Q3. What concepts do you associate with it?
Figure 1: Example of MTurk Task 2. Task 1 is the
same except that only a swatch is given.
The design that we suggested has a minor lim-
itation in that a color swatch may display differ-
ently on different monitors. However, we hope to
overcome this issue by collecting 50 annotations
per RGB value. The example color
e
? emotion
c
?
concept associations produced by different anno-
tators ai are shown below:
? [R=222, G=207, B=186] (a1) light golden
yellow
e
? purity, happiness
c
? butter cookie,
vanilla; (a2) gold
e
? cheerful, happy
c
? sun,
corn; (a3) golden
e
? sexy
c
? beach, jewelery.
? [R=218, G=97, B=212] (a4) pinkish pur-
ple
e
? peace, tranquility, stressless
c
? justin
308
bieber?s headphones, someday perfume; (a5)
pink
e
? happiness
c
? rose, bougainvillea.
In addition, we collected data about workers?
gender, age, native language, number of years of
experience with English, and color preferences.
This data is useful for investigating variance in an-
notations for color-emotion-concept associations
among workers from different cultural and lin-
guistic backgrounds.
4 Data Analysis
We collected 15,200 annotations evenly divided
between the two tasks over 12 days. In total, 915
workers (41% male, 51% female and 8% who did
not specify), mainly from India and United States,
completed our tasks as shown in Table 1. 18%
workers produced 20 or more annotations. They
spent 78 seconds on average per annotation with
an average salary rate $2.3 per hour ($0.05 per
completed task).
Country Annotations
India 7844
United States 5824
Canada 187
United Kingdom 172
Colombia 100
Table 1: Demographic information about annota-
tors: top 5 countries represented in our dataset.
In total, we collected 2,315 unique color terms,
3,397 unique affect terms, and 1,957 unique con-
cepts for the given 152 RGB values. In the
sections below we discuss our findings on color
naming, color-emotion and color-concept associ-
ations. We also give a comparison of annotated
affect terms and concepts from CLEX and other
existing lexicons.
4.1 Color Terms
Berlin and Kay (1988) state that as languages
evolve they acquire new color terms in a strict
chronological order. When a language has only
two colors they are white (light, warm) and black
(dark, cold). English is considered to have 11 ba-
sic colors: white, black, red, green, yellow, blue,
brown, pink, purple, orange and gray, which is
known as the B&K order.
In addition, colors can be distinguished along at
most three independent dimensions of hue (olive,
orange), darkness (dark, light, medium), satura-
tion (grayish, vivid), and brightness (deep, pale)
(Mojsilovic, 2002). Interestingly, we observe
these dimensions in CLEX by looking for B&K
color terms and their frequent collocations. We
present the top 10 color collocations for the B&K
colors in Table 2. As can be seen, color terms
truly are distinguished by darkness, saturation and
brightness terms e.g., light, dark, greenish, deep.
In addition, we find that color terms are also as-
sociated with color-specific collocations, e.g., sky
blue, chocolate brown, pea green, salmon pink,
carrot orange. These collocations were produced
by annotators to describe the color of particular
RGB values. We investigate these color-concept
associations in more details in Section 4.3.
In total, the CLEX has 2,315 unique color
Color Co-occurrences
?
white off, antique, half, dark, black, bone,
milky, pale, pure, silver
0.62
black light, blackish brown, brownish,
brown, jet, dark, green, off, ash,
blackish grey
0.43
red dark, light, dish brown, brick, or-
ange, brown, indian, dish, crimson,
bright
0.59
green dark, light, olive, yellow, lime, for-
est, sea, dark olive, pea, dirty
0.54
yellow light, dark, green, pale, golden,
brown, mustard, orange, deep,
bright
0.63
blue light, sky, dark, royal, navy, baby,
grey, purple, cornflower, violet
0.55
brown dark, light, chocolate, saddle, red-
dish, coffee, pale, deep, red,
medium
0.67
pink dark, light, hot, pale, salmon, baby,
deep, rose, coral, bright
0.55
purple light, dark, deep, blue, bright,
medium, pink, pinkish, bluish,
pretty
0.69
orange light, burnt, red, dark, yellow,
brown, brownish, pale, bright, car-
rot
0.68
gray dark, light, blue, brown, charcoal,
leaden, greenish, grayish blue, pale,
grayish brown
0.62
Table 2: Top 10 color term collocations for the
11 B&K colors; co-occurrences are sorted by fre-
quency from left to right in a decreasing order;
?10
1 p(? | color) is a total estimated probability
of the top 10 co-occurrences.
309
Agreement Color Term
% of overall Exact match 0.492
agreement Substring match 0.461
Free-marginal Exact match 0.458
Kappa Substring match 0.424
Table 3: Inter-annotator agreement on assigning
names to RGB values: 100 annotators, 152 RGB
values and 16 color categories including 11 B&K
colors, 4 additional colors and none of the above.
names for the set of 152 RGB values. The
inter-annotator agreement rate on color naming is
shown in Table 3. We report free-marginal Kappa
(Randolph, 2005) because we did not force an-
notators to assign certain number of RGB values
to a certain number of color terms. Additionally,
we report inter-annotator agreement for an exact
string match e.g., purple, green and a substring
match e.g., pale yellow = yellow = golden yellow.
4.2 Color-Emotion Associations
In total, the CLEX lexicon has 3,397 unique af-
fect terms representing feelings (calm, pleasure),
emotions (joy, love, anxiety), attitudes (indiffer-
ence, caution), and mood (anger, amusement).
The affect terms in CLEX include the 8 basic emo-
tions from (Plutchik, 1980): joy, sadness, anger,
fear, disgust, surprise, trust and anticipation6
CLEX is a very rich lexicon because we did not
restrict our annotators to any specific set of affect
terms. A wide range of parts-of-speech are rep-
resented, as shown in the first column in Table 4.
For instance, the term love is represented by other
semantically related terms such as: lovely, loved,
loveliness, loveless, love-able and the term joy is
represented as enjoy, enjoyable, enjoyment, joy-
ful, joyfulness, overjoyed.
POS Affect Terms, % Concepts, %
Nouns 79 52
Adjectives 12 29
Adverbs 3 5
Verbs 6 12
Table 4: Main syntactic categories for affect terms
and concepts in CLEX.
The manually constructed portion of
WORDNET-AFFECT includes 101 positive
and 188 negative affect terms (Strapparava and
6The set of 8 Plutchik?s emotions is a superset of emotions
from (Ekman, 1992).
Valitutti, 2004). Of this set, 41% appeared at
least once in CLEX. We also looked specifically
at the set of terms labeled as emotions in the
WORDNET-AFFECT hierarchy. Of these, 12 are
positive emotions and 10 are negative emotions.
We found that 9 out of 12 positive emotion
terms (except self-pride, levity and fearlessness)
and 9 out of 10 negative emotion terms (except in-
gratitude) also appear in CLEX as shown in Table
5. Thus, we can conclude that annotators do not
associate any colors with self-pride, levity, fear-
lessness and ingratitude. In addition, some emo-
tions were associated more frequently with colors
than others. For instance, positive emotions like
calmness, joy, love are more frequent in CLEX
than expectation and ingratitude; negative emo-
tions like sadness, fear are more frequent than
shame, humility and daze.
Positive Freq. Negative Freq.
calmness 1045 sadness 356
joy 527 fear 250
love 482 anxiety 55
hope 147 despair 19
affection 86 compassion 10
enthusiasm 33 dislike 8
liking 5 shame 5
expectation 3 humility 3
gratitude 3 daze 1
Table 5: WORDNET-AFFECT positive and neg-
ative emotion terms from CLEX. Emotions are
sorted by frequency in decreasing order from the
total 27,802 annotations.
Next, we analyze the color-emotion associ-
ations in CLEX in more detail and compare
them with the only other publicly-available color-
emotion lexicon, EMOLEX. Recall that EMOLEX
(Mohammad, 2011a) has 11 B&K colors associ-
ated with 8 basic positive and negative emotions
from (Plutchik, 1980). Affect terms in CLEX are
not labeled as conveying positive or negative emo-
tions. Instead, we use the overlapping 289 affect
terms between WORDNET-AFFECT and CLEX
and propagate labels from WORDNET-AFFECT to
the corresponding affect terms in CLEX. As a re-
sult we discover positive and negative affect term
associations with the 11 B&K colors. Table 6
shows the percentage of positive and negative af-
fect term associations with colors for both CLEX
and EMOLEX.
310
Positive Negative
CLEX EL CLEX EL
white 2.5 20.1 0.3 2.9
black 0.6 3.9 9.3 28.3
red 1.7 8.0 8.2 21.6
green 3.3 15.5 2.7 4.7
yellow 3.0 10.8 0.7 6.9
blue 5.9 12.0 1.6 4.1
brown 6.5 4.8 7.6 9.4
pink 5.6 7.8 1.1 1.2
purple 3.1 5.7 1.8 2.5
orange 1.6 5.4 1.7 3.8
gray 1.0 5.7 3.6 14.1
Table 6: The percentage of affect terms associated
with B&K colors in CLEX and EMOLEX (similar
color-emotion associations are shown in bold).
The percentage of color-emotion associations
in CLEX and EMOLEX differs because the set of
affect terms in CLEX consists of 289 positive and
negative affect terms compared to 8 affect terms
in EMOLEX. Nevertheless, we observe the same
pattern as (Mohammad, 2011a) for negative emo-
tions. They are associated with black, red and
gray colors, except yellow becomes a color of
positive emotions in CLEX. Moreover, we found
the associations with the color brown to be am-
biguous as it was associated with both positive
and negative emotions. In addition, we did not ob-
serve strong associations between white and pos-
itive emotions. This may be because white is the
color of grief in India. The rest of the positive
emotions follow the EMOLEX pattern and are as-
sociated with green, pink, blue and purple colors.
Next, we perform a detailed comparison be-
tween CLEX and EMOLEX color-emotion asso-
ciations for the 11 B&K colors and the 8 basic
emotions from (Plutchik, 1980) in Table 7. Recall
that annotations in EMOLEX are done by workers
from the USA only. Thus, we report two num-
bers for CLEX - annotations from workers from
the USA (CA) and all annotations (C). We take
EMOLEX results from (Mohammad, 2011c). We
observe a strong correlation between CLEX and
EMOLEX affect lexicons for some color-emotion
associations. For instance, anger has a strong as-
sociation with red and brown, anticipation with
green, fear with black, joy with pink, sadness
with black, brown and gray, surprise with yel-
low and orange, and finally, trust is associated
with blue and brown. Nonetheless, we also found
a disagreement in color-emotion associations be-
tween CLEX and EMOLEX. For instance antic-
ipation is associated with orange in CLEX com-
pared to white, red or yellow in EMOLEX. We also
found quite a few inconsistent associations with
the disgust emotion. This inconsistency may be
explained by several reasons: (a) EMOLEX asso-
ciates emotions with colors through concepts, but
CLEX has color-emotion associations obtained
directly from annotators; (b) CLEX has 3,397
affect terms compared to 8 basic emotions in
EMOLEX. Therefore, it may be introducing some
ambiguous color-emotion associations.
Finally, we investigate cross-cultural differ-
ences in color-emotion associations between the
two most representative groups of our annotators:
US-based and India-based. We consider the 8
Plutchik?s emotions and allow associations with
all possible color terms (rather than only 11 B&K
colors). We show top 5 colors associated with
emotions for two groups of annotators in Figure 2.
For example, we found that US-based annotators
associate pink with joy, dark brown with trust vs.
India-based annotators who associate yellow with
joy and blue with trust.
4.3 Color-Concept Associations
In total, workers annotated the 152 RGB values
with 37,693 concepts which is on average 2.47
concepts compared to 1.82 affect term per anno-
tation. CLEX contains 1,957 unique concepts in-
cluding 1,667 nouns, 23 verbs, 28 adjectives, and
12 adverbs. We investigate an overlap of con-
cepts by part-of-speech tag between CLEX and
other lexicons including EMOLEX (EL), Affec-
tive Norms of English Words (AN), General In-
quirer (GI). The results are shown in Table 8.
Finally, we generate concept clusters associ-
ated with yellow, white and brown colors in Fig-
ure 3. From the clusters, we observe the most
frequent k concepts associated with these colors
have a correlation with either positive or negative
emotion. For example, white is frequently associ-
ated with snow, milk, cloud and all of these con-
cepts evolve positive emotions. This observation
helps resolve the ambiguity in color-emotion as-
sociations we found in Table 7.
5 Conclusions
We have described a large-scale crowdsourcing
effort aimed at constructing a rich color-emotion-
311
white black red green yellow blue brown pink purple orange grey
anger
C - 3.6 43.4 0.3 0.3 0.3 3.3 0.6 0.3 1.5 2.1
CA - 3.8 40.6 0.8 - - 4.5 - 0.8 2.3 0.8
EA 2.1 30.7 32.4 5.0 5.0 2.4 6.6 0.5 2.3 2.5 9.9
sadness
C 0.3 24.0 0.3 0.6 0.3 4.2 11.4 0.3 2.2 0.3 10.3
CA - 22.2 - 0.6 - 5.3 9.4 - 4.1 - 12.3
EA 3.0 36.0 18.6 3.4 5.4 5.8 7.1 0.5 1.4 2.1 16.1
fear
C 0.8 43.0 8.9 2.0 1.2 0.4 6.1 0.4 0.8 0.4 2.0
CA - 29.5 10.5 3.2 1.1 - 3.2 - 1.1 1.1 4.2
EA 4.5 31.8 25.0 3.5 6.9 3.0 6.1 1.3 2.3 3.3 11.8
disgust
C - 2.3 1.1 11.2 1.1 1.1 24.7 1.1 3.4 1.1 -
CA - - - 14.8 1.8 - 33.3 - 1.8 - -
EA 2.0 33.7 24.9 4.8 5.5 1.9 9.7 1.1 1.8 3.5 10.5
joy
C 1.0 0.2 0.2 3.4 5.7 4.2 4.2 9.1 4.4 4.0 0.6
CA 0.9 - 0.3 3.3 4.5 4.8 2.7 10.6 4.2 3.9 0.6
EA 21.8 2.2 7.4 14.1 13.4 11.3 3.1 11.1 6.3 5.8 2.8
trust
C - - 1.2 3.5 1.2 17.4 8.1 1.2 1.2 5.8 1.2
CA - - 3.0 6.1 3.0 3.0 9.1 - - 3.0 3.0
EA 22.0 6.3 8.4 14.2 8.3 14.4 5.9 5.5 4.9 3.8 5.8
surprise
C - - - 3.3 6.7 6.7 3.3 3.3 6.7 13.3 3.3
CA - - - - 5.6 5.6 - 5.6 11.1 11.1 -
EA 11.0 13.4 21.0 8.3 13.5 5.2 3.4 5.2 4.1 5.6 8.8
anticipation
C - - - 5.3 5.3 - 5.3 5.3 - 15.8 5.3
CA - - - - - - - 10.0 - 10.0 10.0
EA 16.2 7.5 11.5 16.2 10.7 9.5 5.7 5.9 3.1 4.9 8.4
Table 7: The percentage of the 8 basic emotions associated with 11 B&K colors in CLEX vs. EMOLEX,
e.g., sadness is associated with black by 36% of annotators in EMOLEX(EA), 22.1% in CLEX(CA) by
US-based annotators only and 24% in CLEX(C) by all annotators; we report zero associations by ?-?.
(a) Joy - US: 331, I: 154 (b) Trust - US: 33, I: 47 (c) Surprise - US: 18, I: 12 (d) Anticipation - US: 10, I: 9
(e) Anger - US: 133, I: 160 (f) Sadness - US: 171, I: 142 (g) Fear - US: 95, I: 105 (h) Disgust - US: 54, I: 16
Figure 2: Apparent cross-cultural differences in color-emotion associations between US- and India-
based annotators. 10.6% of US workers associated joy with pink, while 7.1% India-based workers
associated joy with yellow (based on 331 joy associations from the US and from 154 India).
312
(a) Yellow (b) Brown (c) White
Figure 3: Concept clusters of color-concept associations for ambiguous colors: yellow, white, brown.
concept association lexicon, CLEX. This lexicon
links concepts, color terms and emotions to spe-
cific RGB values. This lexicon may help to dis-
ambiguate objects when modeling conversational
interactions in many domains. We have examined
the association between color terms and positive
or negative emotions.
Our work also investigated cross-cultural dif-
ferences in color-emotion associations between
India- and US-based annotators. We identified
frequent color-concept associations, which sug-
gests that concepts associated with a particular
color may express the same sentiment as the color.
Our future work includes applying statistical
inference for discovering a hidden structure of
concept-emotion associations. Moreover, auto-
matically identifying the strength of association
between a particular concept and emotions is an-
other task which is more difficult than just iden-
tifying the polarity of the word. We are also in-
terested in using a similar approach to investigate
CLEX?AN CLEX?EL CLEX?GI
Noun 287 Noun 574 Noun 708
Verb 4 Verb 13 Verb 17
Adj 28 Adj 53 Adj 66
Adv 1 Adv 2 Adv 3
320 642 794
AN\CLEX EL\CLEX GI\CLEX
712 7,445 11,101
CLEX\AN CLEX\EL CLEX\GI
1,637 1,315 1,163
Table 8: An overlap of concepts by part-of-
speech tag between CLEX and existing lexicons.
CLEX?GI stands for the intersection of sets,
CLEX\GI denotes the difference of sets.
the way that colors are associated with concepts
and emotions in languages other than English.
Acknowledgments
We are grateful to everyone in the NLP group
at Microsoft Research for helpful discussion and
feedback especially Chris Brocket, Piali Choud-
hury, and Hassan Sajjad. We thank Natalia Rud
from Tyumen State University, Center of Linguis-
tic Education for helpful comments and sugges-
tions.
References
Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: machine
learning for text-based emotion prediction. In
Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ?05, pages 579?586,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Brent Berlin and Paul Kay. 1988. Basic Color Terms:
their Universality and Evolution. Berkley: Univer-
sity of California Press.
M. Bortoli and J. Maroto. 2001. Translating colors in
web site localisation. In In The Proceedings of Eu-
ropean Languages and the Implementation of Com-
munication and Information Technologies (Elicit).
M. Bradley and P. Lang. 1999. Affective forms for
english words (anew): Instruction manual and af-
fective ranking.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s me-
chanical turk. In EMNLP ?09: Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 286?295, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
313
David L. Chen and William B. Dolan. 2011. Building
a persistent workforce on mechanical turk for mul-
tilingual data collection. In Proceedings of The 3rd
Human Computation Workshop (HCOMP 2011),
August.
Paul Ekman. 1992. An argument for basic emotions.
Cognition & Emotion, 6(3):169?200.
Clark Davidson Elliott. 1992. The affective reasoner:
a process model of emotions in a multi-agent sys-
tem. Ph.D. thesis, Evanston, IL, USA. UMI Order
No. GAX92-29901.
J. Gage. 1993. Color and culture: Practice and mean-
ing from antiquity to abstraction, univ. of calif.
C. Hardin and L. Maffi. 1997. Color Categories in
Thought and Language.
N. Jacobson and W. Bender. 1996. Color as a deter-
mined communication. IBM Syst. J., 35:526?538,
September.
N. Kaya. 2004. Relationship between color and emo-
tion: a study of college students. College Student
Journal.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proc. ICWSM.
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, 38:39?
41.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: using
mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ?10, pages 26?
34, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Saif Mohammad. 2011a. Colourful language: Mea-
suring word-colour associations. In Proceedings
of the 2nd Workshop on Cognitive Modeling and
Computational Linguistics, pages 97?106, Port-
land, Oregon, USA, June. Association for Compu-
tational Linguistics.
Saif Mohammad. 2011b. From once upon a time
to happily ever after: Tracking emotions in novels
and fairy tales. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 105?114, Portland, OR, USA, June. Associa-
tion for Computational Linguistics.
Saif M. Mohammad. 2011c. Even the abstract have
colour: consensus in word-colour associations. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies: short papers - Volume 2,
HLT ?11, pages 368?373, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Aleksandra Mojsilovic. 2002. A method for color
naming and description of color composition in im-
ages. In Proc. IEEE Int. Conf. Image Processing,
pages 789?792.
Andrew Ortony, Gerald L. Clore, and Allan Collins.
1988. The Cognitive Structure of Emotions. Cam-
bridge University Press, July.
Li-Chen Ou, M. Ronnier Luo, Pei-Li Sun, Neng-
Chung Hu, and Hung-Shing Chen. 2011. Age ef-
fects on colour emotion, preference, and harmony.
Color Research and Application.
R. Plutchik, 1980. A general psychoevolutionary the-
ory of emotion, pages 3?33. Academic press, New
York.
Justus J. Randolph. 2005. Author note: Free-marginal
multirater kappa: An alternative to fleiss fixed-
marginal multirater kappa.
P. Sable and O. Akcay. 2010. Color: Cross cultural
marketing perspectives as to what governs our re-
sponse to it. In In The Proceedings of ASSBS, vol-
ume 17.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 254?263, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
Carlo Strapparava and Gozde Ozbal. 2010. The color
of emotions in text. COLING, pages 28?32.
C. Strapparava and A. Valitutti. 2004. Wordnet-affect:
an affective extension of wordnet. In In: Proceed-
ings of the 4th International Conference on Lan-
guage Resources and Evaluation (LREC 2004), Lis-
bon, pages 1083?1086.
314
Proceedings of NAACL-HLT 2013, pages 1010?1019,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Broadly Improving User Classification via
Communication-Based Name and Location Clustering on Twitter
Shane Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, David Yarowsky
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
shane.a.bergsma@gmail.com, mdredze@cs.jhu.edu, vandurme@cs.jhu.edu, taw@jhu.edu, yarowsky@cs.jhu.edu
Abstract
Hidden properties of social media users, such
as their ethnicity, gender, and location, are of-
ten reflected in their observed attributes, such
as their first and last names. Furthermore,
users who communicate with each other of-
ten have similar hidden properties. We pro-
pose an algorithm that exploits these insights
to cluster the observed attributes of hundreds
of millions of Twitter users. Attributes such
as user names are grouped together if users
with those names communicate with other
similar users. We separately cluster millions
of unique first names, last names, and user-
provided locations. The efficacy of these clus-
ters is then evaluated on a diverse set of clas-
sification tasks that predict hidden users prop-
erties such as ethnicity, geographic location,
gender, language, and race, using only pro-
file names and locations when appropriate.
Our readily-replicable approach and publicly-
released clusters are shown to be remarkably
effective and versatile, substantially outper-
forming state-of-the-art approaches and hu-
man accuracy on each of the tasks studied.
1 Introduction
There is growing interest in automatically classify-
ing users in social media by various hidden prop-
erties, such as their gender, location, and language
(e.g. Rao et al (2010), Cheng et al (2010), Bergsma
et al (2012)). Predicting these and other proper-
ties for users can enable better advertising and per-
sonalization, as well as a finer-grained analysis of
user opinions (O?Connor et al, 2010), health (Paul
and Dredze, 2011), and sociolinguistic phenomena
(Eisenstein et al, 2011). Classifiers for user prop-
erties often rely on information from a user?s social
network (Jernigan and Mistree, 2009; Sadilek et al,
2012) or the textual content they generate (Pennac-
chiotti and Popescu, 2011; Burger et al, 2011).
Here, we propose and evaluate classifiers that bet-
ter exploit the attributes that users explicitly provide
in their user profiles, such as names (e.g., first names
like Mary, last names like Smith) and locations (e.g.,
Brasil). Such attributes have previously been used as
?profile features? in supervised user classifiers (Pen-
nacchiotti and Popescu, 2011; Burger et al, 2011;
Bergsma et al, 2012). There are several motivations
for exploiting these data. Often the only informa-
tion available for a user is a name or location (e.g.
for a new user account). Profiles also provide an
orthogonal or complementary source of information
to a user?s social network and textual content; gains
based on profiles alone should therefore add to gains
based on other data. The decisions of profile-based
classifiers could also be used to bootstrap training
data for other classifiers that use complementary fea-
tures.
Prior work has encoded profile attributes via lex-
ical or character-based features (e.g. Pennacchiotti
and Popescu (2011), Burger et al (2011), Bergsma
et al (2012)). Unfortunately, due to the long-tailed
distribution of user attributes, a profile-based classi-
fier will encounter many examples at test time that
were not observed during training. For example,
suppose a user wassim hassan gives their location as
tanger. If the attribute tokens wassim, hassan, and
tanger do not occur in training (nor indicative sub-
1010
strings), then a classifier can only guess at the user?s
ethnicity and location. In social media, the preva-
lence of fake names and large variations in spelling,
slang, and language make matters worse.
Our innovation is to enhance attribute-based clas-
sifiers with new data, derived from the communica-
tions of Twitter users with those attributes. Users
with the name tokens wassim and hassan often talk
to users with Arab names like abdul and hussein.
Users listing their location as tanger often talk to
users from morocco. Since users who communicate
often share properties such as ethnicity and location
(?8), the user wassim hassan might be an Arab who
uses the French spelling of the city Tangier.
Our challenge is to encode these data in a form
readily usable by a classifier. Our approach is to
represent each unique profile attribute (e.g. tanger
or hassan) as a vector that encodes the communi-
cation pattern of users with that attribute (e.g. how
often they talk to users from morocco, etc.); we then
cluster the vectors to discover latent groupings of
similar attributes. Based on transitive (third party)
connections, tanger and tangier can appear in the
same cluster, even if no two users from these loca-
tions talk directly. To use the clusters in an attribute-
based classifier, we add new features that indicate
the cluster memberships of the attributes. Clustering
thus lets us convert a high-dimensional space of all
attribute pairs to a low-dimensional space of cluster
memberships. This makes it easier to share our data,
yields fewer parameters for learning, and creates at-
tribute groups that are interpretable to humans.
We cluster names and locations in a very large
corpus of 168 million Twitter users (?2) and use a
distributed clustering algorithm to separately clus-
ter millions of first names, last names, and user-
provided locations (?3). We evaluate the use of our
cluster data as a novel feature in supervised classi-
fiers, and compare our result to standard classifiers
using character and token-level features (?4). The
cluster data enables significantly improved perfor-
mance in predicting the gender, location, and lan-
guage of social media users, exceeding both ex-
isting state-of-the-art machine and human perfor-
mance (?6). Our cluster data can likewise im-
prove performance in other domains, on both es-
tablished and new NLP tasks as further evaluated
in this paper (?6). We also propose a way to
First names: maria, david, ana, daniel, michael, john,
alex, jessica, carlos, jose, chris, sarah, laura, juan
Last names: silva, santos, smith, garcia, oliveira, ro-
driguez, jones, williams, johnson, brown, gonzalez
Locations: brasil, indonesia, philippines, london,
jakarta, s?o paulo, rio de janeiro, venezuela, brazil
Table 1: Most frequent profile attributes for our collection
of 168 million Twitter users, in descending order
enhance a geolocation system by using commu-
nication patterns, and show strong improvements
over a hand-engineered baseline (?7). We share
our clusters with the community to use with other
tasks. The clusters, and other experimental data, are
available for download from www.clsp.jhu.edu/
~sbergsma/TwitterClusters/.
2 Attribute Associations on Twitter
Data and Processing Our raw Twitter data com-
prises the union of 2.2 billion tweets from 05/2009
to 10/2010 (O?Connor et al, 2010), 1.8 billion
tweets collected from 07/2011 to 08/2012, and 80
million tweets collected from followers of 10 thou-
sand location and language-specific Twitter feeds.
We implemented each stage of processing using
MapReduce (Dean and Ghemawat, 2008). The total
computation (from extracting profiles to clustering
attributes) was 1300 days of wall-clock CPU time.
Attribute Extraction Tweets provide the name
and self-reported location of the tweeter. We find
126M unique users with these attributes in our data.
When tweets mention other users via an @user con-
struction, Twitter also includes the profile name of
the mentioned user; we obtain a further 42M users
from these cases. We then normalize the extracted
attributes by converting to lower-case, deleting sym-
bols, numbers, and punctuation, and removing com-
mon honorifics and suffixes like mr/mrs and jr/sr.
Common prefixes like van and de la are joined to
the last-name token.1 This processing yields 8.3M
1www.clsp.jhu.edu/~sbergsma/TwitterClusters/
also provides our scripts for normalizing attributes. The scripts
can be used to ensure consistency/compatibility between
arbitrary datasets and our shared cluster data. Note we use no
special processing for the companies, organizations, and spam-
mers among our users, nor for names arising from different
conventions (e.g. 1-word names, reversed first/last names).
1011
henrik: fredrik 5.87, henrik 5.82, anders 5.73, johan
5.69, andreas 5.59, martin 5.54, magnus 5.41
courtney: taylor 8.03, ashley 7.92, courtney 7.92,
emily 7.91, lauren 7.82, katie 7.72, brittany 7.69
ilya: sergey 5.85, alexey 5.62, alexander 5.59, dmitry
5.51, ????????? 5.46, anton 5.44, andrey 5.40
Table 2: Top associates and PMIs for three first names.
unique locations, 7.4M unique last names, and 5.5M
unique first names. These three sets provide the tar-
get attributes that we cluster in ?3. Table 1 shows
the most frequent names in each of these three sets.
User-User Links We extract each user mention as
an undirected communication link between the user
tweeting and the mentioned user (including self-
mentions but not retweets). We consider each user-
user link as a single event; we count it once no mat-
ter how often two specific users interact. We extract
436M user-user links in total.
Attribute-Attribute Pairs We use our profile data
to map each user-user link to an attribute-attribute
pair; we separately count each pair of first names,
last names, and locations. For example, the first-
name pair (henrik, fredrik) occurs 181 times. Rather
than using the raw count, we calculate the associa-
tion between attributes a1 and a2 via their pointwise
mutual information (PMI), following prior work in
distributional clustering (Lin and Wu, 2009):
PMI(a1, a2) = log
P(a1, a2)
P(a1)P(a2)
PMI essentially normalizes the co-occurrence by
what we would expect if the attributes were indepen-
dently distributed. We smooth the PMI by adding a
count of 0.5 to all co-occurrence events.
The most highly-associated name attributes re-
flect similarities in ethnicity and gender (Table 2).
The most highly-ranked associates for locations are
often nicknames and alternate/misspellings of those
locations. For example, the locations charm city,
bmore, balto, westbaltimore, b a l t i m o r e, bal-
timoreee, and balitmore each have the U.S. city of
baltimore as their highest-PMI associate. We show
how this can be used to help geolocate users (?7).
3 Attribute Clustering
Representation We first represent each target at-
tribute as a feature vector, where each feature corre-
sponds to another attribute of the same type as the
target and each value gives the PMI between this at-
tribute and the target (as in Table 2).2 To help cluster
the long-tail of infrequent attributes, we also include
orthographic features. For first and last names, we
have binary features for the last 2 characters in the
string. For locations, we have binary features for
(a) any ideographic characters in the string and (b)
each token (with diacritics removed) in the string.
We normalize the feature vectors to unit length.
Distributed K-Means Clustering Our approach
to clustering follows Lin and Wu (2009) who used k-
means to cluster tens of millions of phrases. We also
use cosine similarity to compute the closest centroid
(i.e., we use the spherical k-means clustering algo-
rithm (Dhillon and Modha, 2001)). We keep track
of the average cosine similarity between each vector
and its nearest centroid; this average is guaranteed
to increase at each iteration.
Like Lin and Wu (2009), we parallelize the al-
gorithm using MapReduce. Each mapper finds the
nearest centroids for a portion of the vectors, while
also computing the partial sums of the vectors as-
signed to each centroid. The mappers emit the cen-
troid IDs as keys and the partial sums as values.
The Reducer aggregates the partial sums from each
partition and re-normalizes each sum vector to unit
length to obtain the new centroids. We also use an
inverted index at each iteration that, for each input
feature, lists which centroids each feature belongs
to. Using this index greatly speeds up the centroid
similarity computations.
Clustering Details We cluster with nine separate
configurations: over first names, last names, and lo-
cations, and each with 50, 200, and 1000 cluster
centroids (denoted C50, C200, and C1000). Since k-
2We decided to restrict the features for a target to be at-
tributes of the same type (e.g., we did not use last name as-
sociations for a first name target) because each attribute type
conveys distinct information. For example, first names convey
gender and age more than last names. By separately cluster-
ing representations using first names, last names, and locations,
each clustering can capture its own distinct latent-class associa-
tions.
1012
Cluster 463 (Serbian): pavlovic?, jovanovic, jo-
vanovic?, stankovic?, srbija, markovic?, petrovic?,
radovic, nenad, milenkovic, nikolic, sekulic, todor-
ovic, stojanovic, petrovic, aleksic, ilic, markovic
Cluster 544 (Black South African): ngcobo, nkosi,
dlamini, ndlovu, mkhize, mtshali, sithole, mathebula,
mthembu, khumalo, ngwenya, shabangu, nxumalo,
buthelezi, radebe, mabena, zwane, mbatha, sibiya
Cluster 449 (Turkish): s?ahin, ?elik, ?zt?rk, ko?, ?ak?r,
karatas?, aktas?, g?ng?r, ?zkan, balc?, g?m?s?, akkaya,
gen?, sar?, y?ksel, g?nes?, yig?it, yal??n, orhan, sag?lam,
g?ler, demirci, k???k, yavuz, bayrak, ?zcan, altun
Cluster 656 (Indonesian): utari, oktaviana, apriani,
mustika, septiana, febrianti, kurniawati, indriani, nur-
janah, septian, cahya, anggara, yuliani, purnamasari,
sukma, wijayanti, pramesti, ningrum, yanti, wulansari
Table 3: Example C1000 last-name clusters
Cluster 56 [sim=0.497]: gregg, bryn, bret, stewart,
lyndsay, howie, elyse, jacqui, becki, rhett, meaghan,
kirstie, russ, jaclyn, zak, katey, seamus, brennan,
fraser, kristie, stu, jaimie, kerri, heath, carley, griffin
Cluster 104 [sim=0.442]: stephon, devonte, deion,
demarcus, janae, tyree, jarvis, donte, dewayne, javon,
destinee, tray, janay, tyrell, jamar, iesha, chyna,
jaylen, darion, lamont, marquise, domonique, alexus
Cluster 132 [sim=0.292]: moustafa, omnya, menna-
tallah, ?C?@, shorouk, ragab, ?


??, radwa, moemen,
mohab, hazem, yehia, ? K
Q k, Z @Q?? @, mennah, ?
 Q?? ?,
abdelrahman, ?


	
????, H. 	Qk, Q?A

K, nermeen, hebatallah
...
Table 4: C200 soft clustering for first name yasmeen
means is not guaranteed to reach a global optimum,
we use ten different random initializations for each
configuration, and select the one with the highest av-
erage similarity after 20 iterations. We run this one
for an additional 30 iterations and take the output as
our final set of centroids for that configuration.
The resulting clusters provide data that could help
classify hidden properties of social media users. For
example, Table 3 shows that last names often clus-
ter by ethnicity, even at the sub-national level (e.g.
Zulu tribe surnames nkosi, dlamini, mathebula, etc.).
Note the Serbian names include two entries that are
not last names: srbija, the Serbian word for Serbia,
and nenad, a common Serbian first name.
Soft Clustering Rather than assigning each at-
tribute to its single highest-similarity cluster, we can
assign each vector to its N most similar clusters.
These soft-cluster assignments often reflect different
social groups where a name or location is used. For
example, the name yasmeen is similar to both com-
mon American names (Cluster 56), African Ameri-
can names (Cluster 104), and Arabic names (Clus-
ter 132) (Table 4). As another example, the C1000
assignments for the location trujillo comprise sep-
arate clusters containing towns and cities in Peru,
Venezuela, Colombia, etc., reflecting the various
places in the Latin world with this name. In general,
the soft cluster assignment is a low-dimensional rep-
resentation of each of our attributes. Although it can
be interpretable to humans, it need not be in order to
be useful to a classifier.
4 Classification with Cluster Features
Our motivating problem is to classify users for hid-
den properties such as their gender, location, race,
ethnicity, and language. We adopt a discriminative
solution. We encode the relevant data for each in-
stance in a feature vector and train a (linear) support
vector machine classifier (Cortes and Vapnik, 1995).
SVMs represent the state-of-the-art on many NLP
classification tasks, but other classifiers could also
be used. For multi-class classification, we use a one-
versus-all strategy, a competitive approach on most
multi-class problems (Rifkin and Klautau, 2004).
The input to our system is one or more observed
user attributes (e.g. name and location fields from
a user profile). We now describe how features are
created from these attributes in both state-of-the-art
systems and via our new cluster data.
Token Features (Tok) are binary features that in-
dicate the presence of a specific attribute (e.g., first-
name=bob). Burger et al (2011) and Bergsma et al
(2012) used Tok features to encode user profile fea-
tures. For multi-token fields (e.g. location), our Tok
features also indicate the specific position of each
token (e.g., loc1=s?o, loc2=paulo, locN=brasil).
Character N-gram Features (Ngm) give the
count of all character n-grams of length 1-to-4 in the
input. Ngm features have been used in user classifi-
cation (Burger et al, 2011) and represent the state-
1013
of-the-art in detecting name ethnicity (Bhargava and
Kondrak, 2010). We add special begin/end charac-
ters to the attributes to mark the prefix and suffix po-
sitions. We also use a smoothed log-count; we found
this to be most effective in preliminary work.
Cluster Features (Clus) indicate the soft-cluster
memberships of the attributes. We have features for
the top-2, 5, and 20 most similar clusters in the C50,
C200, and C1000 clusterings, respectively. Like Lin
and Wu (2009), we ?side-step the matter of choos-
ing the optimal value k in k-means? by using fea-
tures from clusterings at different granularities. Our
feature dimensions correspond to cluster IDs; fea-
ture values give the similarity to the cluster centroid.
Other strategies (e.g. hard clustering, binary fea-
tures) were less effective in preliminary work.
5 Classification Experiments
5.1 Methodology
Our main objective is to assess the value of us-
ing cluster features (Clus). We add these features
to classifiers using Tok+Ngm features, which repre-
sents the current state-of-the-art. We compare these
feature settings on both Twitter tasks (?5.2) and
tasks not related to social-media (?5.3). For each
task, we randomly divide the gold standard data into
50% train, 25% development and 25% test, unless
otherwise noted. As noted above, the gold-standard
datasets for all of our experiments are available for
download. We train our SVM classifiers using the
LIBLINEAR package (Fan et al, 2008). We optimize
the classifier?s regularization parameter on develop-
ment data, and report our final results on the held-
out test examples. We report accuracy: the propor-
tion of test examples classified correctly. For com-
parison, we report the accuracy of a majority-class
baseline on each task (Base).
Classifying hidden properties of social media
users is challenging (Table 5). Pennacchiotti and
Popescu (2011) even conclude that ?profile fields do
not contain enough good-quality information to be
directly used for user classification.? To provide in-
sight into the difficulty of the tasks, we had two hu-
mans annotate 120 examples from each of the test
sets, and we average their results to give a ?Human?
performance number. The two humans are experts in
Country: 53 possible countries
United States courtland dante cali baby
United States tinas twin on the court
Brazil thamires gomez macap? ap
Denmark marte clason NONE
Lang. ID: 9 confusable languages
Bulgarian valentina getova NONE
Russian borisenko yana edinburgh
Bulgarian NONE blagoevgrad
Ukrainian andriy kupyna ternopil
Farsi kambiz barahouei NONE
Urdu musadiq sanwal jammu
Ethnicity: 13 European ethnicities
German dennis hustadt
Dutch bernhard hofstede
French david coste
Swedish mattias bjarsmyr
Portuguese helder costa
Race: black or white
black kerry swain
black darrell foskey
white ty j larocca
black james n jones
white sean p farrell
Table 5: Examples of class (left) and input (names, loca-
tions) for some of our evaluation tasks.
this domain and have very wide knowledge of global
names and locations.
5.2 Twitter Applications
Country A number of recent papers have consid-
ered the task of predicting the geolocation of users,
using both user content (Cheng et al, 2010; Eisen-
stein et al, 2010; Hecht et al, 2011; Wing and
Baldridge, 2011; Roller et al, 2012) and social net-
work (Backstrom et al, 2010; Sadilek et al, 2012).
Here, we first predict user location at the level of
the user?s location country. To our knowledge, we
are the first to exploit user locations and names for
this prediction. For this task, we obtain gold data
from the portion of Twitter users who have GPS en-
abled (geocoded tweets). We were able to obtain a
very large number of gold instances for this task, so
selected only 10K for testing, 10K for development,
and retained the remaining 782K for training.
Language ID Identifying the language of users
is an important prerequisite for building language-
specific social media resources (Tromp and Pech-
1014
enizkiy, 2011; Carter et al, 2013). Bergsma et al
(2012) recently released a corpus of tweets marked
for one of nine languages grouped into three confus-
able character sets: Arabic, Farsi, and Urdu tweets
written in Arabic characters; Hindi, Nepali, and
Marathi written in Devanagari, and Russian, Bulgar-
ian, and Ukrainian written in Cyrillic. The tweets
were marked for language by native speakers via
Amazon Mechanical Turk. We again discard the
tweet content and extract each user?s first name, last
name, and user location as our input data, while tak-
ing the annotated language as the class label.
Gender We predict whether a Twitter user is male
or female using data from Burger et al (2011). This
data was created by linking Twitter users to struc-
tured profile pages on other websites where users
must select their gender. Unlike prior systems using
this data (Burger et al, 2011; Van Durme, 2012), we
make the predictions using only user names.
5.3 Other Applications
Origin Knowing the origin of a name can improve
its automatic pronunciation (Llitjos and Black,
2001) and transliteration (Bhargava and Kondrak,
2010). We evaluate our cluster data on name-origin
prediction using a corpus of names marked as ei-
ther Indian or non-Indian by Bhargava and Kondrak
(2010). Since names in this corpus are not marked
for entity type, we include separate cluster features
from both our first and last name clusters.
Ethnicity We also evaluate on name-origin data
from Konstantopoulos (2007). This data derives
from lists of football players on European national
teams; it marks each name (with diacritics removed)
as arising from one of 13 European languages. Fol-
lowing prior work, we test in two settings: (1) using
last names only, and (2) using first and last names.
Race We also evaluate our ability to identify eth-
nic groups at a sub-national level. To obtain data
for this task, we mined the publicly-available arrest
records on mugshots.com for the U.S. state of New
Jersey (a small but diverse and densely-populated
area). Over 99% of users were listed as either black
or white, and we structure the task as a binary clas-
sification problem between these two classes. We
predict the race of each person based purely on their
name; this contrasts with prior work in social media
which looked at identifying African Americans on
the basis of their Twitter content (Eisenstein et al,
2011; Pennacchiotti and Popescu, 2011).
6 Classification Results
Table 6 gives the results on each task. The system in-
corporating our novel Clus features consistently im-
proves over the Ngm+Tok system; all differences be-
tween All and Ngm+Tok are significant (McNemar?s,
p<0.01). The relative reduction in error from adding
Clus features ranges between 7% and 51%. The All
system including Clus features also exceeds human
performance on all studied tasks.
On Country, the U.S. is the majority class, oc-
curring in 42.5% of cases.3 It is impressive that
All so significantly exceeds Tok+Ngm (86.7% vs.
84.8%); with 782K training examples, we did not
expect such room for improvement. Both names and
locations play an important role: All achieves 66%
using names alone and 70% with only location. On
the subset of data where all three attributes are non-
empty, the full system achieves 93% accuracy.
Both feature classes are likewise important for
Lang. ID; All achieves 67% with only first+last
names, 72% with just locations, but 83% with both.
Our smallest improvement is on Gender. This
task is easier (with higher human/system accuracy)
and has plenty of training data (more data per class
than any other task); there is thus less room to im-
prove. Looking at the feature weights, the strongest-
weighted female cluster apparently captures a sub-
community of Justin Bieber fans (showing loyalty
with ?first names? jbieber, belieb, biebz, beliebing,
jbiebs, etc.). Just because a first name like madison
has a high similarity to this cluster does not imply
girls named Madison are Justin Bieber fans; it sim-
ply means that Madisons have similar names to the
friends of Justin Bieber fans (who tend to be girls).
Also, note that while the majority of the 34K users in
our training data are assigned this cluster somewhere
in their soft clustering, only 6 would be assigned this
3We tried other baselines: e.g., we predict countries if they
are substrings of the location (otherwise predicting U.S.); and
we predict countries if they often occur as a string following
the given location in our profile data (e.g., we predict Spain for
Madrid since Madrid, Spain is common). Variations on these
approaches consistently performed between 48% and 56%.
1015
Task Input
Num. Num.
Base Human Tok Ngm Clus
Tok+
All ?
Train Class Ngm
Country first+last+loc 781920 53 42.5 71.7 83.0 84.5 80.2 84.8 86.7 12.5
Lang. ID first+last+loc 2492 9 27.0 74.2 74.6 80.6 71.1 80.4 82.7 11.7
Gender first+last 33805 2 52.4 88.3 85.3 88.6 79.5 89.5 90.2 6.7
Origin entity name 500 2 52.4 80.4 - 75.6 81.2 75.6 88.0 50.8
Ethnicity last 6026 13 20.8 47.9 - 54.6 48.5 54.6 62.4 17.2
Ethnicity first+last 7457 13 21.2 53.3 67.6 77.5 73.6 78.4 81.3 13.4
Race first+last 7977 2 54.7 71.4 80.4 81.6 84.6 82.4 84.6 12.5
Table 6: Task details and accuracy (%) for attribute-based classification tasks. ? = relative error reduction (%) of All
(Tok+Ngm+Clus) over Ngm+Tok. All always exceeds both Tok+Ngm and the human performance.
cluster in a hard clustering. This clearly illustrates
the value of the soft clustering representation.
Note the All system performed between 83% and
90% on each Twitter task. This level of performance
strongly refutes the prevailing notion that Twitter
profile information is useless in general (Pennac-
chiotti and Popescu, 2011) and especially for geolo-
cation (Cheng et al, 2010; Hecht et al, 2011).
We now move to applications beyond social me-
dia. Bhargava and Kondrak (2010) have the current
state-of-the-art on Origin and Ethnicity based on an
SVM using character-n-gram features; we reimple-
mented this as Ngm. We obtain a huge improvement
over their work using Clus, especially on Origin
where we reduce error by >50%.4 This improve-
ment can partly be attributed to the small amount of
training data; with fewer parameters to learn, Clus
learns more from limited data than Ngm. We like-
wise see large improvements over the state-of-the-
art on Ethnicity, on both last name and full name
settings.
Finally, Clus features also significantly improve
accuracy on the new Race task. Our cluster data can
therefore help to classify names into sub-national
groups, and could potentially be used to infer other
interesting communities such as castes in India and
religious divisions in many countries.
In general, the relative value of our cluster models
varies with the amount of training data; we see huge
gains on the smaller Origin data but smaller gains
on the large Gender set. Figure 1 shows how per-
formance of Clus and Ngm varies with training data
on Race. Again, Clus is especially helpful with less
4Note Tok is not used here because the input is a single token
and training and test splits have distinct instances.
 60
 65
 70
 75
 80
 85
 10  100  1000  10000
A
cc
ur
ac
y
Number of training examples
Clus
Ngm
Figure 1: Learning curve on Race: Clus perform as well
with 30 training examples as Ngm features do with 1000.
data; thousands of training examples are needed for
Ngm to rival the performance of Clus using only a
handful. Since labeled data is generally expensive
to obtain or in short supply, our method for exploit-
ing unlabeled Twitter data can both save money and
improve top-end performance.
7 Geolocation by Association
There is a tradition in computational linguistics of
grouping words both by the similarity of their con-
text vectors (Hindle, 1990; Pereira et al, 1993; Lin,
1998) and directly by their statistical association in
text (Church and Hanks, 1990; Brown et al, 1992).
While the previous sections explored clusters built
by vector similarity, we now explore a direct appli-
cation of our attribute association data (?2).
We wish to use this data to improve an existing
Twitter geolocation system based on user profile lo-
cations. The system operates as follows: 1) normal-
1016
ize user-provided locations using a set of regular ex-
pressions (e.g. remove extra spacing, punctuation);
2) look up the normalized location in an alias list;
3) if found, map the alias to a unique string (target
location), corresponding to a structured location ob-
ject that includes geo-coordinates.
The alias list we are currently using is based on
extensive work in hand-writing aliases for the most
popular Twitter locations. For example, the current
aliases for Nashville, Tennessee include nashville,
nashville tn, music city, etc. Our objective is to im-
prove on this human-designed list by automatically
generating aliases using our association data.
Aliases by Association For each target, we pro-
pose new aliases from the target?s top-PMI asso-
ciates (?2). To become an alias, the PMI between
the alias and target must be above a threshold,
the alias must occur more than a fixed number of
times in our profile data, the alias must be within
the top-N1 associates of the target, and the target
must be within the top-N2 associates of the alias.
We merge our automatic aliases with the manually-
written aliases. The new aliases for Nashville, Ten-
nessee include east nashville, nashville tenn, music
city usa, nashvegas, cashville tn, etc.
Experiments To evaluate the geolocation system,
we use tweets from users with GPS enabled (?5.2).
For each tweet, we resolve the location using the
system and compare to the gold coordinates. The
system can skip a location if it does not match the
alias list; more than half of the locations are skipped,
which is consistent with prior work (Hecht et al,
2011). We evaluate the alias lists using two mea-
sures: (1) its coverage: the percentage of locations it
resolves, and (2) its precision: of the ones resolved,
the percentage that are correct. We define a correct
resolution to be one where the resolved coordinates
are within 50 miles of the gold coordinates.
We use 56K gold tweets to tune the parameters of
our automatic alias-generator, trading off coverage
and precision. We tune such that the system using
these aliases obtains the highest possible coverage,
while being at least as precise as the baseline system.
We then evaluate both the baseline set of aliases and
our new set on 56K held-out examples.
Results On held-out test data, the geolocation sys-
tem using baseline aliases has a coverage of 38.7%
and a precision of 59.5%. Meanwhile, the system
using the new aliases has a coverage of 44.6% and
a precision of 59.4%. With virtually the same pre-
cision, the new aliases are thus able to resolve 15%
more users. This provides an immediate benefit to
our existing Twitter research efforts.
Note that our alias lists can be viewed as clus-
ters of locations. In ongoing work, we are exploring
techniques based on discriminative learning to infer
alias lists using not only Clus information but also
Ngm and Tok features as in the previous sections.
8 Related Work
In both real-world and online social networks, ?peo-
ple socialize with people who are like them in terms
of gender, sexual orientation, age, race, education,
and religion? (Jernigan and Mistree, 2009). So-
cial media research has exploited this for two main
purposes: (1) to predict friendships based on user
properties, and (2) to predict user properties based
on friendships. Friendship prediction systems (e.g.
Facebook?s friend suggestion tool) use features such
as whether both people are computer science ma-
jors (Taskar et al, 2003) or whether both are at the
same location (Crandall et al, 2010; Sadilek et al,
2012). The inverse problem has been explored in the
prediction of a user?s location given the location of
their peers (Backstrom et al, 2010; Cho et al, 2011;
Sadilek et al, 2012). Jernigan and Mistree (2009)
predict a user?s sexuality based on the sexuality of
their Facebook friends, while Garera and Yarowsky
(2009) predict a user?s gender partly based on the
gender of their conversational partner. Jha and El-
hadad (2010) predict the cancer stage of users of
an online cancer discussion board; they derive com-
plementary information for prediction from both the
text a user generates and the cancer stage of the peo-
ple that a user interacts with.
The idea of clustering data in order to provide fea-
tures for supervised systems has been successfully
explored in a range of NLP tasks, including named-
entity-recognition (Miller et al, 2004; Lin and Wu,
2009; Ratinov and Roth, 2009), syntactic chunking
(Turian et al, 2010), and dependency parsing (Koo
et al, 2008; T?ckstr?m et al, 2012). In each case,
1017
the clusters are derived from the distribution of the
words or phrases in text, not from their communica-
tion pattern. It would be interesting to see whether
prior distributional clusters can be combined with
our communication-based clusters to achieve even
better performance. Indeed, there is evidence that
features derived from text can improve the predic-
tion of name ethnicity (Pervouchine et al, 2010).
There has been an explosion of work in recent
years in predicting user properties in social net-
works. Aside from the work mentioned above that
analyzes a user?s social network, a large amount
of work has focused on inferring user properties
based on the content they generate (e.g. Burger
and Henderson (2006), Schler et al (2006), Rao
et al (2010), Mukherjee and Liu (2010), Pennac-
chiotti and Popescu (2011), Burger et al (2011), Van
Durme (2012)).
9 Conclusion and Future Work
We presented a highly effective and readily repli-
cable algorithm for generating language resources
from Twitter communication patterns. We clustered
user attributes based on both the communication of
users with those attributes as well as substring sim-
ilarity. Systems using our clusters significantly out-
perform state-of-the-art algorithms on each of the
tasks investigated, and exceed human performance
on each task as well. The power and versatility of
our clusters is exemplified by the fact we reduce er-
ror by a larger margin on each of the non-Twitter
tasks than on any Twitter task itself.
Twitter provides a remarkably large sample and
effectively a partial census of much of the world?s
population, with associated metadata, descriptive
content and sentiment information. Our ability to
accurately assign numerous often unspecified prop-
erties such as race, gender, language and ethnicity to
such a large user sample substantially increases the
sociological insights and correlations one can derive
from such data.
References
Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical predic-
tion with social and spatial proximity. In Proc. WWW,
pages 61?70.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
HLT-NAACL, pages 693?696.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
John D. Burger and John C. Henderson. 2006. An ex-
ploration of observable features related to blogger age.
In Proc. AAAI Spring Symposium: Computational Ap-
proaches to Analyzing Weblogs, pages 15?20.
John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on Twitter. In
Proc. EMNLP, pages 1301?1309.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013. Microblog Language Identification: Overcom-
ing the Limitations of Short, Unedited and Idiomatic
Text. Language Resources and Evaluation Journal.
(forthcoming).
Zhiyuan Cheng, James Caverlee, and Kyumin Lee. 2010.
You are where you tweet: a content-based approach
to geo-locating Twitter users. In Proc. CIKM, pages
759?768.
Eunjoon Cho, Seth A. Myers, and Jure Leskovec. 2011.
Friendship and mobility: user movement in location-
based social networks. In Proc. KDD, pages 1082?
1090.
Kenneth W. Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Mach. Learn., 20(3):273?297.
David J. Crandall, Lars Backstrom, Dan Cosley, Sid-
dharth Suri, Daniel Huttenlocher, and Jon Kleinberg.
2010. Inferring social ties from geographic coinci-
dences. Proceedings of the National Academy of Sci-
ences, 107(52):22436?22441.
Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107?113.
Inderjit S. Dhillon and Dharmendra S. Modha. 2001.
Concept decompositions for large sparse text data us-
ing clustering. Mach. Learn., 42(1-2):143?175.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. EMNLP, pages
1277?1287.
1018
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. 2011.
Discovering sociolinguistic associations with struc-
tured sparsity. In Proc. ACL, pages 1365?1374.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proc. ACL-IJCNLP, pages 710?718.
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H. Chi.
2011. Tweets from Justin Bieber?s heart: the dynamics
of the location field in user profiles. In Proc. CHI,
pages 237?246.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proc. ACL, pages
268?275.
Carter Jernigan and Behram F. T. Mistree. 2009. Gaydar:
Facebook friendships expose sexual orientation. First
Monday, 14(10). [Online].
Mukund Jha and Noemie Elhadad. 2010. Cancer stage
prediction based on patient online discourse. In Proc.
2010 Workshop on Biomedical Natural Language Pro-
cessing, pages 64?71.
Stasinos Konstantopoulos. 2007. What?s in a name? In
Proc. Computational Phonology Workshop, RANLP.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
ACL-08: HLT, pages 595?603.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proc. ACL-IJCNLP,
pages 1030??1038.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. Coling-ACL, pages 768?774.
Ariadna Font Llitjos and Alan W. Black. 2001. Knowl-
edge of language origin improves pronunciation accu-
racy of proper names. In Proceedings of EuroSpeech-
01, pages 1919?1922.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proc. HLT-NAACL, pages 337?342.
Arjun Mukherjee and Bing Liu. 2010. Improving gender
classification of blog authors. In Proc. EMNLP, pages
207?217.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. ICWSM, pages 122?129.
Michael Paul and Mark Dredze. 2011. You are what you
tweet: Analyzing Twitter for public health. In Proc.
ICWSM, pages 265?272.
Marco Pennacchiotti and Ana-Maria Popescu. 2011. A
machine learning approach to Twitter user classifica-
tion. In Proc. ICWSM, pages 281?288.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Proc.
ACL, pages 183?190.
Vladimir Pervouchine, Min Zhang, Ming Liu, and
Haizhou Li. 2010. Improving name origin recogni-
tion with context features and unlabelled data. In Col-
ing 2010: Posters, pages 972?978.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. International Workshop on
Search and Mining User-Generated Contents, pages
37?44.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
CoNLL, pages 147?155.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. J. Mach. Learn. Res., 5:101?
141.
Stephen Roller, Michael Speriosu, Sarat Rallapalli, Ben-
jamin Wing, and Jason Baldridge. 2012. Supervised
text-based geolocation using language models on an
adaptive grid. In Proc. EMNLP-CoNLL, pages 1500?
1510.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proc. WSDM, pages 723?732.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James W. Pennebaker. 2006. Effects of age and
gender on blogging. In Proc. AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 199?205.
Oscar T?ckstr?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer
of linguistic structure. In Proc. NAACL-HLT, pages
477?487.
Ben Taskar, Ming-Fai Wong, Pieter Abbeel, and Daphne
Koller. 2003. Link prediction in relational data. In
Proc. NIPS, volume 15.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
based n-gram language identication on short texts. In
Proc. 20th Machine Learning conference of Belgium
and The Netherlands, pages 27?34.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. ACL, pages
384?394.
Benjamin Van Durme. 2012. Streaming analysis of dis-
course participants. In Proc. EMNLP-CoNLL, pages
48?58.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proc. ACL, pages 955?964.
1019
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 505?510,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues
from Multilingual Twitter Streams
Svitlana Volkova
CLSP
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Theresa Wilson
HLTCOE
Johns Hopkins University
Baltimore, MD
taw@jhu.edu
David Yarowsky
CLSP
Johns Hopkins University
Baltimore, MD
yarowsky@cs.jhu.edu
Abstract
We study subjective language in social
media and create Twitter-specific lexi-
cons via bootstrapping sentiment-bearing
terms from multilingual Twitter streams.
Starting with a domain-independent, high-
precision sentiment lexicon and a large
pool of unlabeled data, we bootstrap
Twitter-specific sentiment lexicons, us-
ing a small amount of labeled data to
guide the process. Our experiments on
English, Spanish and Russian show that
the resulting lexicons are effective for
sentiment classification for many under-
explored languages in social media.
1 Introduction
The language that people use to express opinions
and sentiment is extremely diverse. This is true for
well-formed data, such as news and reviews, and
it is particularly true for data from social media.
Communication in social media is informal, ab-
breviations and misspellings abound, and the per-
son communicating is often trying to be funny,
creative, and entertaining. Topics change rapidly,
and people invent new words and phrases.
The dynamic nature of social media together
with the extreme diversity of subjective language
has implications for any system with the goal
of analyzing sentiment in this domain. General,
domain-independent sentiment lexicons have low
coverage. Even models trained specifically on so-
cial media data may degrade somewhat over time
as topics change and new sentiment-bearing terms
crop up. For example, the word ?occupy? would
not have been indicative of sentiment before 2011.
Most of the previous work on sentiment lexicon
construction relies on existing natural language
processing tools, e.g., syntactic parsers (Wiebe,
2000), information extraction (IE) tools (Riloff
and Wiebe, 2003) or rich lexical resources such
as WordNet (Esuli and Sebastiani, 2006). How-
ever, such tools and lexical resources are not avail-
able for many languages spoken in social media.
While English is still the top language in Twitter,
it is no longer the majority. Thus, the applicabil-
ity of these approaches is limited. Any method for
analyzing sentiment in microblogs or other social
media streams must be easily adapted to (1) many
low-resource languages, (2) the dynamic nature of
social media, and (3) working in a streaming mode
with limited or no supervision.
Although bootstrapping has been used for learn-
ing sentiment lexicons in other domains (Turney
and Littman, 2002; Banea et al, 2008), it has not
yet been applied to learning sentiment lexicons for
microblogs. In this paper, we present an approach
for bootstrapping subjectivity clues from Twitter
data, and evaluate our approach on English, Span-
ish and Russian Twitter streams. Our approach:
? handles the informality, creativity and the dy-
namic nature of social media;
? does not rely on language-dependent tools;
? scales to the hundreds of new under-explored
languages and dialects in social media;
? classifies sentiment in a streaming mode.
To bootstrap subjectivity clues from Twitter
streams we rely on three main assumptions:
i. sentiment-bearing terms of similar orienta-
tion tend to co-occur at the tweet level (Tur-
ney and Littman, 2002);
ii. sentiment-bearing terms of opposite orienta-
tion do not co-occur at the tweet level (Ga-
mon and Aue, 2005);
iii. the co-occurrence of domain-specific and
domain-independent subjective terms serves
as a signal of subjectivity.
505
2 Related Work
Mihalcea et.al (2012) classifies methods for boot-
strapping subjectivity lexicons into two types:
corpus-based and dictionary-based.
Dictionary-based methods rely on existing lex-
ical resources to bootstrap sentiment lexicons.
Many researchers have explored using relations in
WordNet (Miller, 1995), e.g., Esuli and Sabastiani
(2006), Andreevskaia and Bergler (2006) for En-
glish, Rao and Ravichandran (2009) for Hindi and
French, and Perez-Rosas et al (2012) for Spanish.
Mohammad et al (2009) use a thesaurus to aid
in the construction of a sentiment lexicon for En-
glish. Other works (Clematide and Klenner, 2010;
Abdul-Mageed et al, 2011) automatically expands
and evaluates German and Arabic lexicons. How-
ever, the lexical resources that dictionary-based
methods need, do not yet exist for the majority of
languages in social media. There is also a mis-
match between the formality of many language re-
sources, such as WordNet, and the extremely in-
formal language of social media.
Corpus-based methods extract subjectivity and
sentiment lexicons from large amounts of unla-
beled data using different similarity metrics to
measure the relatedness between words. Hatzivas-
siloglou and McKeown (1997) were the first to ex-
plore automatically learning the polarity of words
from corpora. Early work by Wiebe (2000) iden-
tifies clusters of subjectivity clues based on their
distributional similarity, using a small amount of
data to bootstrap the process. Turney (2002) and
Velikovich et al (2010) bootstrap sentiment lexi-
cons for English from the web by using Pointwise
Mutual Information (PMI) and graph propaga-
tion approach, respectively. Kaji and Kitsuregawa
(2007) propose a method for building sentiment
lexicon for Japanese from HTML pages. Banea
et al (2008) experiment with Lexical Semantic
Analysis (LSA) (Dumais et al, 1988) to bootstrap
a subjectivity lexicon for Romanian. Kanayama
and Nasukawa (2006) bootstrap subjectivity lexi-
cons for Japanese by generating subjectivity can-
didates based on word co-occurrence patterns.
In contrast to other corpus-based bootstrapping
methods, we evaluate our approach on multiple
languages, specifically English, Spanish, and Rus-
sian. Also, as our approach relies only on the
availability of a bilingual dictionary for translating
an English subjectivity lexicon and crowdsourcing
for help in selecting seeds, it is more scalable and
better able to handle the informality and the dy-
namic nature of social media. It also can be effec-
tively used to bootstrap sentiment lexicons for any
language for which a bilingual dictionary is avail-
able or can be automatically induced from parallel
corpora.
3 Data
For the experiments in this paper, we use three
sets of data for each language: 1M unlabeled
tweets (BOOT) for bootstrapping Twitter-specific
lexicons, 2K labeled tweets for development data
(DEV), and 2K labeled tweets for evaluation
(TEST). DEV is used for parameter tuning while
bootstrapping, and TEST is used to evaluating the
quality of the bootstrapped lexicons.
We take English tweets from the corpus con-
structed by Burger et al (2011) which con-
tains 2.9M tweets (excluding retweets) from 184K
users.1 English tweets are identified automati-
cally using a compression-based language identifi-
cation (LID) tool (Bergsma et al, 2012). Accord-
ing to LID, there are 1.8M (63.6%) English tweets,
which we randomly sample to create BOOT, DEV
and TEST sets for English. Unfortunately, Burger?s
corpus does not include Russian and Spanish data
on the same scale as English. Therefore, for
other languages we construct a new Twitter corpus
by downloading tweets from followers of region-
specific news and media feeds.
Sentiment labels for tweets in DEV and TEST
sets for all languages are obtained using Amazon
Mechanical Turk. For each tweet we collect an-
notations from five workers and use majority vote
to determine the final label for the tweet. Snow
et al (2008) show that for a similar task, labeling
emotion and valence, on average four non-expert
labelers are needed to achieve an expert level of
annotation. Table 1 gives the distribution of tweets
over sentiment labels for the development and test
sets for English (E-DEV, E-TEST), Spanish (S-
DEV, S-TEST), and Russian (R-DEV, R-TEST).
Below are examples of tweets in Russian with En-
glish translations labeled with sentiment:
? Positive: ? ?????? ??????? ???????
? ???? ??????? (Planning for delicious
breakfast and lots of movies);
? Negative: ???? ????????, ? ? ??? ??????
(I want to die and I will do that);
1They provided the tweet IDs, and we used the Twitter
Corpus Tools to download the tweets.
506
Data Positive Neg Both Neutral
E-DEV 617 357 202 824
E-TEST 596 347 195 862
S-DEV 358 354 86 1,202
S-TEST 317 387 93 1203
R-DEV 452 463 156 929
R-TEST 488 380 149 983
Table 1: Sentiment label distribution in develop-
ment DEV and test TEST datasets across languages.
? Both: ??????? ???????? ?????? ???
????? ?? ?? ????. ???? ?????? ????-
?? (I want to write about the movie rougher
but I will not. Although the actors are good);
? Neutral: ?????? ????? ????? ????????
?????? ?????? (Why clever thoughts come
only at night?).
4 Lexicon Bootstrapping
To create a Twitter-specific sentiment lexicon for
a given language, we start with a general-purpose,
high-precision sentiment lexicon2 and bootstrap
from the unlabeled data (BOOT) using the labeled
development data (DEV) to guide the process.
4.1 High-Precision Subjectivity Lexicons
For English we seed the bootstrapping pro-
cess with the strongly subjective terms from the
MPQA lexicon3 (Wilson et al, 2005). These
terms have been previously shown to be high-
precision for recognizing subjective sentences
(Riloff and Wiebe, 2003).
For the other languages, the subjective seed
terms are obtained by translating English seed
terms using a bilingual dictionary, and then col-
lecting judgments about term subjectivity from
Mechanical Turk. Terms that truly are strongly
subjective in translation are used for seed terms
in the new language, with term polarity projected
from the English. Finally, we expand the lexicons
with plurals and inflectional forms for adverbs, ad-
jectives and verbs.
4.2 Bootstrapping Approach
To bootstrap, first the new lexicon LB(0) is seeded
with the strongly subjective terms from the orig-
inal lexicon LI . On each iteration i ? 1, tweets
in the unlabeled data are labeled using the lexicon
2Other works on generating domain-specific sentiment
lexicons e.g., from blog data (Jijkoun et al, 2010) also start
with a general, domain-specific lexicon.
3http://www.cs.pitt.edu/mpqa/
from the previous iteration, LB(i?1). If a tweet
contains one or more terms from LB(i?1) it is con-
sidered subjective, otherwise objective. The polar-
ity of subjective tweets is determined in a similar
way: if the tweet contains ? 1 positive terms, tak-
ing into account the negation, it is considered neg-
ative; if it contains ? 1 negative terms, taking into
account the negation, it is considered positive.4 If
it contains both positive and negative terms, it is
considered to be both. Then, for every term not in
LB(i?1) that has a frequency ? ?freq, the proba-
bility of that term being subjective is calculated as
shown in Algorithm 1 line 10. The top ?k terms
with a subjective probability ? ?pr are then added
to LB(i). The polarity of new terms is determined
based on the probability of the term appearing in
positive or negative tweets as shown in line 18.5
The bootstrapping process terminates when there
are no more new terms meeting the criteria to add.
Algorithm 1 BOOTSTRAP (?, ?pr, ?freq, ?topK )
1: iter = 0, ? = 0.5, LB(~?)? LI(?)
2: while (stop 6= true) do
3: LiterB (~?)? ?,?LiterB (~?)? ?
4: for each new term w ? {V \ LB(~?)} do
5: for each tweet t ? T do
6: if w ? t then
7: UPDATE c(w,LB(~?)), c(w,LposB (~?)), c(w)8: end if
9: end for
10: psubj(w)? c(w,LB(~?))c(w)
11: ppos(w)? c(w,L
pos
B (~?))
c(w,LB(~?))
12: LiterB (~?)? w, psubj(w), ppol(w)13: end for
14: SORT LiterB (~?) by psubj(w)15: while (K ? ?topK) do
16: for each new term w ? LiterB (~?) do
17: if [psubj(w) ? ?pr and cw ? ?freq then
18: if [ppos(w) ? 0.5] then
19: wpol ? positive
20: else
21: wpol ? negative
22: end if
23: ?LiterB (~?)? ?LiterB (~?) + wpol24: end if
25: end for
26: K = K + 1
27: end while
28: if [?LiterB (~?) == 0] then29: stop? true
30: end if
31: LB(~?)? LB(~?) + ?LiterB (~?)32: iter = iter + 1
33: end while
4If there is a negation in the two words before a sentiment
term, we flip its polarity.
5Polarity association probabilities should sum up to 1
ppos(w|LB(~?)) + pneg(w|LB(~?)) = 1.
507
English Spanish Russian
LEI LEB LSI LSB LRI LRB
Pos 2.3 16.8 2.9 7.7 1.4 5.3
Neg 2.8 4.7 5.2 14.6 2.3 5.5
Total 5.1 21.5 8.1 22.3 3.7 10.8
Table 2: The original and the bootstrapped (high-
lighted) lexicon term count (LI ? LB) with polar-
ity across languages (thousands).
The set of parameters ~? is optimized using a grid
search on the development data using F-measure
for subjectivity classification. As a result, for En-
glish ~? = [0.7, 5, 50] meaning that on each itera-
tion the top 50 new terms with a frequency ? 5
and probability ? 0.7 are added to the lexicon.
For Spanish, the set of optimal parameters ~? =
[0.65, 3, 50] and for Russian - ~? = [0.65, 3, 50]. In
Table 2 we report size and term polarity from the
original LI and the bootstrapped LB lexicons.
5 Lexicon Evaluations
We evaluate our bootstrapped sentiment lexicons
English LEB , Spanish LSB and Russian LRB by com-
paring them with existing dictionary-expanded
lexicons that have been previously shown to be ef-
fective for subjectivity and polarity classification
(Esuli and Sebastiani, 2006; Perez-Rosas et al,
2012; Chetviorkin and Loukachevitch, 2012). For
that we perform subjectivity and polarity classifi-
cation using rule-based classifiers6 on the test data
E-TEST, S-TEST and R-TEST.
We consider how the various lexicons perform
for rule-based classifiers for both subjectivity and
polarity. The subjectivity classifier predicts that
a tweet is subjective if it contains a) at least one,
or b) at least two subjective terms from the lexi-
con. For the polarity classifier, we predict a tweet
to be positive (negative) if it contains at least one
positive (negative) term taking into account nega-
tion. If the tweet contains both positive and nega-
tive terms, we take the majority label.
For English we compare our bootstrapped lex-
icon LEB against the original lexicon LEI and
strongly subjective terms from SentiWordNet 3.0
(Esuli and Sebastiani, 2006). To make a fair
comparison, we automatically expand SentiWord-
Net with noun plural forms and verb inflectional
forms. In Figure 1 we report precision, recall
6Similar approach to a rule-based classification using
terms from he MPQA lexicon (Riloff and Wiebe, 2003).
and F-measure results. They show that our boot-
strapped lexicon significantly outperforms Senti-
WordNet for subjectivity classification. For polar-
ity classification we get comparable F-measure but
much higher recall for LEB compared to SWN .
(a) Subj ? 1 (b) Subj ? 2 (c) Polarity
Lexicon Fsubj?1 Fsubj?2 Fpolarity
SWN 0.57 0.27 0.78
LEI 0.71 0.48 0.82
LEB 0.75 0.72 0.78
Figure 1: Precision (x-axis), recall (y-axis) and
F-measure (in the table) for English: LEI = ini-
tial lexicon, LEB = bootstrapped lexicon, SWN =
strongly subjective terms from SentiWordNet.
For Spanish we compare our bootstrapped lex-
icon LSB against the original LSI lexicon, and the
full and medium strength terms from the Span-
ish sentiment lexicon constructed by Perez-Rosas
et el. (2012). We report precision, recall and F-
measure in Figure 2. We observe that our boot-
strapped lexicon yields significantly better perfor-
mance for subjectivity classification compared to
both full and medium strength terms. However,
our bootstrapped lexicon yields lower recall and
similar precision for polarity classification.
(a) Subj ? 1 (b) Subj ? 2 (c) Polarity
Lexicon Fsubj?1 Fsubj?2 Fpolarity
SM 0.44 0.17 0.64
SF 0.47 0.13 0.66
LSI 0.59 0.45 0.58
LSB 0.59 0.59 0.55
Figure 2: Precision (x-axis), recall (y-axis) and F-
measure (in the table) for Spanish: LSI = initial
lexicon, LSB = bootstrapped lexicon, SF = full
strength terms; SM = medium strength terms.
508
For Russian we compare our bootstrapped lex-
icon LRB against the original LRI lexicon, and the
Russian sentiment lexicon constructed by Chetv-
iorkin and Loukachevitchet (2012). The external
lexicon in Russian P was built for the domain
of product reviews and does not include polarity
judgments for subjective terms. As before, we
expand the external lexicon with the inflectional
forms for adverbs, adjectives and verbs. We report
results for Russian in Figure 3. We find that for
subjectivity our bootstrapped lexicon shows better
performance compared to the external lexicon (5k
terms). However, the expanded external lexicon
(17k terms) yields higher recall with a significant
drop in precision. Note that for Russian, we report
polarity classification results for LRB and LRI lexi-
cons only because P does not have polarity labels.
(a) Subj ? 1 (b) Subj ? 2 (c) Polarity
Lexicon Fsubj?1 Fsubj?2 Fpolarity
P 0.55 0.29 ?
PX 0.62 0.47 ?
LRI 0.46 0.13 0.73
LRB 0.61 0.35 0.73
Figure 3: Precision (x-axis), recall (y-axis) and F-
measure for Russian: LRI = initial lexicon, LRB =
bootstrapped lexicon, P = external sentiment lex-
icon, PX = expanded external lexicon.
We next perform error analysis for subjectiv-
ity and polarity classification for all languages and
identify common errors to address them in future.
For subjectivity classification we observe that
applying part-of-speech tagging during the boot-
strapping could improve results for all languages.
We could further improve the quality of the lex-
icon and reduce false negative errors (subjec-
tive tweets classified as neutral) by focusing on
sentiment-bearing terms such as adjective, adverbs
and verbs. However, POS taggers for Twitter are
only available for a limited number of languages
such as English (Gimpel et al, 2011). Other false
negative errors are often caused by misspellings.7
7For morphologically-rich languages, our approach cov-
ers different linguistic forms of terms but not their mis-
spellings. However, it can be fixed by an edit-distance check.
We also find subjective tweets with philosophi-
cal thoughts and opinions misclassified, especially
in Russian, e.g., ?????? ?? ?????? ?? ??????
? ?????????? ???????? ????? ?? ??? ???-
?? ??? ?? ??????? ?? ???????? (Sometimes we
are not ready to fulfill our dreams yet but, at the
same time, we do not want to scare them). Such
tweets are difficult to classify using lexicon-based
approaches and require deeper linguistic analysis.
False positive errors for subjectivity classifica-
tion happen because some terms are weakly sub-
jective and can be used in both subjective and
neutral tweets e.g., the Russian term ??????????
(brag) is often used as subjective, but in a tweet
??????? ?? ????? ?????????? ??????? (never
brag about your future) it is used as neutral. Simi-
larly, the Spanish term buenas (good) is often used
subjectively but it is used as neutral in the follow-
ing tweet ?@Diveke me falto el buenas! jaja que
onda que ha pasado? (I miss the good times we
had, haha that wave has passed!).
For polarity classification, most errors happen
because our approach relies on either positive or
negative polarity scores for a term but not both.8
However, in the real world terms may sometimes
have both usages. Thus, some tweets are misclas-
sified (e.g., ?It is too warm outside?). We can
fix this by summing over weighted probabilities
rather than over term counts. Additional errors
happen because tweets are very short and convey
multiple messages (e.g., ?What do you mean by
unconventional? Sounds exciting!?) Thus, our ap-
proach can be further improved by adding word
sense disambiguation and anaphora resolution.
6 Conclusions
We propose a scalable and language independent
bootstrapping approach for learning subjectivity
clues from Twitter streams. We demonstrate the
effectiveness of the bootstrapping procedure by
comparing the resulting subjectivity lexicons with
state-of the-art sentiment lexicons. We perform
error analysis to address the most common error
types in the future. The results confirm that the
approach can be effectively exploited and further
improved for subjectivity classification for many
under-explored languages in social media.
8During the bootstrapping we calculate probability for a
term to be positive and negative, e.g., p(warm|+) = 0.74
and p(warm|?) = 0.26. But during polarity classification
we rely on the highest probability score and consider it to be
?the polarity? for the term e.g., positive for warm.
509
References
Muhammad Abdul-Mageed, Mona T. Diab, and Mo-
hammed Korayem. 2011. Subjectivity and senti-
ment analysis of modern standard arabic. In Pro-
ceedings of ACL/HLT.
Alina Andreevskaia and Sabine Bergler. 2006. Min-
ing wordnet for fuzzy sentiment: Sentiment tag ex-
traction from WordNet glosses. In Proceedings of
EACL.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In Proceedings of LREC.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of 2nd Workshop on
Language in Social Media.
John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twittier. In Proceedings of EMNLP.
Ilia Chetviorkin and Natalia V. Loukachevitch. 2012.
Extraction of Russian sentiment lexicon for product
meta-domain. In Proceedings of COLING.
Simon Clematide and Manfred Klenner. 2010. Eval-
uation and extension of a polarity lexicon for Ger-
man. In Proceedings of the 1st Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis.
Susan T. Dumais, George W. Furnas, Thomas K. Lan-
dauer, Scott Deerwester, and Richard Harshman.
1988. Using latent semantic analysis to improve
access to textual information. In Proceedings of
SIGCHI.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of LREC.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting
low association with known sentiment terms. In
Proceedings of the ACL Workshop on Feature Engi-
neering for Machine Learning in Natural Language
Processing.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twittier: annotation, features, and experiments.
In Proceedings of ACL.
Vasileios Hatzivassiloglou and Kathy McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating focused topic-
specific sentiment lexicons. In Proceedings of ACL.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive
collection of html documents. In Proceedings of
EMNLP.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In Proceedings of
EMNLP.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2012. Multilingual subjectivity and sentiment anal-
ysis. In Proceedings of ACL.
George A. Miller. 1995. Wordnet: a lexical database
for English. Communications of the ACM, 38(11).
Saif Mohammad, Cody Dunne, and Bonnie Dorr.
2009. Generating high-coverage semantic orienta-
tion lexicons from overtly marked words and a the-
saurus. In Proceedings of EMNLP.
Veronica Perez-Rosas, Carmen Banea, and Rada Mi-
halcea. 2012. Learning sentiment lexicons in Span-
ish. In Proceedings of LREC.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceed-
ings of EACL.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? but is it
good?: Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP.
Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from a
hundred-billion-word corpus. Computing Research
Repository.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of ACL.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The viabil-
ity of web-derived polarity lexicons. In Proceedings
of NAACL.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of AAAI.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of EMNLP.
510
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 2: Sentiment Analysis in Twitter
Preslav Nakov
QCRI, Qatar Foundation
pnakov@qf.org.qa
Zornitsa Kozareva
USC Information Sciences Institute
kozareva@isi.edu
Alan Ritter
University of Washington
aritter@cs.washington.edu
Sara Rosenthal
Columbia University
sara@cs.columbia.edu
Veselin Stoyanov
JHU HLTCOE
ves@cs.jhu.edu
Theresa Wilson
JHU HLTCOE
taw@jhu.edu
Abstract
In recent years, sentiment analysis in social
media has attracted a lot of research interest
and has been used for a number of applica-
tions. Unfortunately, research has been hin-
dered by the lack of suitable datasets, com-
plicating the comparison between approaches.
To address this issue, we have proposed
SemEval-2013 Task 2: Sentiment Analysis in
Twitter, which included two subtasks: A, an
expression-level subtask, and B, a message-
level subtask. We used crowdsourcing on
Amazon Mechanical Turk to label a large
Twitter training dataset alng with additional
test sets of Twitter and SMS messages for both
subtasks. All datasets used in the evaluation
are released to the research community. The
task attracted significant interest and a total
of 149 submissions from 44 teams. The best-
performing team achieved an F1 of 88.9% and
69% for subtasks A and B, respectively.
1 Introduction
In the past decade, new forms of communication,
such as microblogging and text messaging have
emerged and become ubiquitous. Twitter messages
(tweets) and cell phone messages (SMS) are often
used to share opinions and sentiments about the sur-
rounding world, and the availability of social con-
tent generated on sites such as Twitter creates new
opportunities to automatically study public opinion.
Working with these informal text genres presents
new challenges for natural language processing be-
yond those encountered when working with more
traditional text genres such as newswire.
Tweets and SMS messages are short in length: a
sentence or a headline rather than a document. The
language they use is very informal, with creative
spelling and punctuation, misspellings, slang, new
words, URLs, and genre-specific terminology and
abbreviations, e.g., RT for re-tweet and #hashtags.1
How to handle such challenges so as to automati-
cally mine and understand the opinions and senti-
ments that people are communicating has only very
recently been the subject of research (Jansen et al,
2009; Barbosa and Feng, 2010; Bifet et al, 2011;
Davidov et al, 2010; O?Connor et al, 2010; Pak and
Paroubek, 2010; Tumasjan et al, 2010; Kouloumpis
et al, 2011).
Another aspect of social media data, such as Twit-
ter messages, is that they include rich structured in-
formation about the individuals involved in the com-
munication. For example, Twitter maintains infor-
mation about who follows whom. Re-tweets (re-
shares of a tweet) and tags inside of tweets provide
discourse information. Modeling such structured in-
formation is important because it provides means for
empirically studying social interactions where opin-
ion is conveyed, e.g., we can study the properties of
persuasive language or those associated with influ-
ential users.
Several corpora with detailed opinion and senti-
ment annotation have been made freely available,
e.g., the MPQA corpus (Wiebe et al, 2005) of
newswire text. These corpora have proved very
valuable as resources for learning about the lan-
guage of sentiment in general, but they did not focus
on social media.
1Hashtags are a type of tagging for Twitter messages.
312
Twitter RT @tash jade: That?s really sad, Charlie RT ?Until tonight I never realised how fucked up I was? -
Charlie Sheen #sheenroast
SMS Glad to hear you are coping fine in uni... So, wat interview did you go to? How did it go?
Table 1: Examples of sentences from each corpus that contain subjective phrases.
While some Twitter sentiment datasets have al-
ready been created, they were either small and pro-
prietary, such as the i-sieve corpus (Kouloumpis
et al, 2011), or they were created only for Span-
ish like the TASS corpus2 (Villena-Roma?n et al,
2013), or they relied on noisy labels obtained from
emoticons and hashtags. They further focused on
message-level sentiment, and no Twitter or SMS
corpus with expression-level sentiment annotations
has been made available so far.
Thus, the primary goal of our SemEval-2013 task
2 has been to promote research that will lead to a
better understanding of how sentiment is conveyed
in Tweets and SMS messages. Toward that goal,
we created the SemEval Tweet corpus, which con-
tains Tweets (for both training and testing) and SMS
messages (for testing only) with sentiment expres-
sions annotated with contextual phrase-level polar-
ity as well as an overall message-level polarity. We
used this corpus as a testbed for the system evalua-
tion at SemEval-2013 Task 2.
In the remainder of this paper, we first describe
the task, the dataset creation process, and the evalu-
ation methodology. We then summarize the charac-
teristics of the approaches taken by the participating
systems and we discuss their scores.
2 Task Description
We had two subtasks: an expression-level subtask
and a message-level subtask. Participants could
choose to participate in either or both subtasks. Be-
low we provide short descriptions of the objectives
of these two subtasks.
Subtask A: Contextual Polarity Disambiguation
Given a message containing a marked instance
of a word or a phrase, determine whether that
instance is positive, negative or neutral in that
context. The boundaries for the marked in-
stance were provided: this was a classification
task, not an entity recognition task.
2http://www.daedalus.es/TASS/corpus.php
Subtask B: Message Polarity Classification
Given a message, decide whether it is of
positive, negative, or neutral sentiment. For
messages conveying both a positive and a
negative sentiment, whichever is the stronger
one was to be chosen.
Each participating team was allowed to submit re-
sults for two different systems per subtask: one con-
strained, and one unconstrained. A constrained sys-
tem could only use the provided data for training,
but it could also use other resources such as lexi-
cons obtained elsewhere. An unconstrained system
could use any additional data as part of the training
process; this could be done in a supervised, semi-
supervised, or unsupervised fashion.
Note that constrained/unconstrained refers to the
data used to train a classifier. For example, if other
data (excluding the test data) was used to develop
a sentiment lexicon, and the lexicon was used to
generate features, the system would still be con-
strained. However, if other data (excluding the test
data) was used to develop a sentiment lexicon, and
this lexicon was used to automatically label addi-
tional Tweet/SMS messages and then used with the
original data to train the classifier, then such a sys-
tem would be unconstrained.
3 Dataset Creation
In the following sections we describe the collection
and annotation of the Twitter and SMS datasets.
3.1 Data Collection
Twitter is the most common micro-blogging site on
the Web, and we used it to gather tweets that express
sentiment about popular topics. We first extracted
named entities using a Twitter-tuned NER system
(Ritter et al, 2011) from millions of tweets, which
we collected over a one-year period spanning from
January 2012 to January 2013; we used the public
streaming Twitter API to download tweets.
313
Instructions: Subjective words are ones which convey an opinion. Given a sentence, identify whether it is objective,
positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the sentence and mark
the position of its start and end in the text boxes below. The number above each word indicates its position. The
word/phrase will be generated in the adjacent textbox so that you can confirm that you chose the correct range.
Choose the polarity of the word or phrase by selecting one of the radio buttons: positive, negative, or neutral. If a
sentence is not subjective please select the checkbox indicating that ?There are no subjective words/phrases?. Please
read the examples and invalid responses before beginning if this is your first time answering this hit.
Figure 1: Instructions provided to workers on Mechanical Turk followed by a screenshot.
Average # of Total Phrase Count Vocabulary
Corpus Words Characters Positive Negative Neutral Size
Twitter - Training 25.4 120.0 5,895 3,131 471 20,012
Twitter - Dev 25.5 120.0 648 430 57 4,426
Twitter - Test 25.4 121.2 2,734 1,541 160 11,736
SMS - Test 24.5 95.6 1,071 1,104 159 3,562
Table 2: Statistics for Subtask A.
We then identified popular topics as those named
entities that are frequently mentioned in association
with a specific date (Ritter et al, 2012). Given this
set of automatically identified topics, we gathered
tweets from the same time period which mentioned
the named entities. The testing messages had differ-
ent topics from training and spanned later periods.
To identify messages that express sentiment to-
wards these topics, we filtered the tweets us-
ing SentiWordNet (Baccianella et al, 2010). We
removed messages that contained no sentiment-
bearing words, keeping only those with at least one
word with positive or negative sentiment score that
is greater than 0.3 in SentiWordNet for at least one
sense of the words. Without filtering, we found class
imbalance to be too high.3
Twitter messages are rich in social media features,
including out-of-vocabulary (OOV) words, emoti-
cons, and acronyms; see Table 1. A large portion of
the OOV words are hashtags (e.g., #sheenroast)
and mentions (e.g., @tash jade).
3Filtering based on an existing lexicon does bias the dataset
to some degree; however, note that the text still contains senti-
ment expressions outside those in the lexicon.
Corpus Positive Negative Objective
/ Neutral
Twitter - Training 3,662 1,466 4,600
Twitter - Dev 575 340 739
Twitter - Test 1,573 601 1,640
SMS - Test 492 394 1,208
Table 3: Statistics for Subtask B.
We annotated the same Twitter messages with an-
notations for subtask A and subtask B. However,
the final training and testing datasets overlap only
partially between the two subtasks since we had
to throw away messages with low inter-annotator
agreement, and this differed between the subtasks.
For testing, we also annotated SMS messages, taken
from the NUS SMS corpus4 (Chen and Kan, 2012).
Tables 2 and 3 show statistics about the corpora we
created for subtasks A and B.
4http://wing.comp.nus.edu.sg/SMSCorpus/
314
A B
Lower Avg. Upper Avg.
Twitter - Train 64.7 82.4 90.8 82.7
Twitter - Dev 51.2 74.7 87.8 78.4
Twitter - Test 68.8 83.6 90.9 76.9
SMS - Test 66.5 88.5 81.2 77.6
Table 4: Bounds for datasets in subtasks A and B.
3.2 Annotation Guidelines
The instructions provided to the annotators, along
with an example, are shown in Figure 1. We pro-
vided several additional examples to the annotators,
shown in Table 5.
In addition, we filtered spammers by considering
the following kinds of annotations invalid:
? containing overlapping subjective phrases;
? subjective but without a subjective phrase;
? marking every single word as subjective;
? not having the overall sentiment marked.
3.3 Annotation Process
Our datasets were annotated for sentiment on Me-
chanical Turk. Each sentence was annotated by five
Mechanical Turk workers (Turkers). In order to
qualify for the hits, the Turker had to have an ap-
proval rate greater than 95% and have completed 50
approved hits. Each Turker was paid three cents
per hit. The Turker had to mark all the subjec-
tive words/phrases in the sentence by indicating their
start and end positions and say whether each subjec-
tive word/phrase was positive, negative, or neutral
(subtask A). They also had to indicate the overall
polarity of the sentence (subtask B).
Figure 1 shows the instructions and an exam-
ple provided to the Turkers. The first five rows
of Table 6 show an example of the subjective
words/phrases marked by each of the workers.
For subtask A, we combined the annotations of
each of the workers using intersection as indicated
in the last row of Table 6. A word had to appear
in 2/3 of the annotations in order to be considered
subjective. Similarly, a word had to be labeled with
a particular polarity (positive, negative, or neutral)
2/3 of the time in order to receive that label.
We also experimented with combining annota-
tions by computing the union of the sentences, and
taking the sentence of the worker who annotated the
most hits, but we found that these methods were
not as accurate. Table 4 shows the lower, average,
and upper bounds for all the hits by computing the
bounds for each hit and averaging them together.
This gives a good indication about how well we can
expect the systems to perform. For example, even if
we used the best annotator each time, it would still
not be possible to get perfect accuracy.
For subtask B, the polarity of the entire sentence
was determined based on the majority of the labels.
If there was a tie, the sentence was discarded. In
order to reduce the number of sentences lost, we
combined the objective and the neutral labels, which
Turkers tended to mix up. Table 4 shows the aver-
age bound for subtask B by computing the bounds
for each hit and averaging them together. Since the
polarity is chosen based on the majority, the upper
bound is 100%.
4 Scoring
For both subtasks, the participating systems were
required to perform a three-way classification ? a
particular marked phrase (for subtask A) or an en-
tire message (for subtask B) was to be classified as
positive, negative, or objective. For each system,
we computed a score for predicting positive/negative
phrases/messages vs. the other two classes.
For instance, to compute positive precision, Ppos,
we find the number of phrases/messages that a sys-
tem correctly predicted to be positive, and we divide
that number by the total number of messages it pre-
dicted to be positive. To compute recall, for the pos-
itive class, Rpos, we find the number of messages
correctly predicted to be positive and we divide that
number by the total number of positive messages in
the gold standard.
We then calculate F-score for the positive labels,
the harmonic average of precision and recall as fol-
lows Fpos = 2
PposRpos
Ppos+Rpos
. We carry out a similar
computation to calculate Fneg, which is F1 for neg-
ative messages.
The overall score for each system run is then
given by the average of the F1-scores for the posi-
tive and negative classes: F = (Fpos + Fneg)/2.
315
Authorities are only too aware that Kashgar is 4,000 kilometres (2,500 miles) from Beijing but only a tenth of
the distance from the Pakistani border, and are desperate to ensure instability or militancy does not leak over the
frontiers.
Taiwan-made products stood a good chance of becoming even more competitive thanks to wider access to overseas
markets and lower costs for material imports, he said.
?March appears to be a more reasonable estimate while earlier admission cannot be entirely ruled out,? according
to Chen, also Taiwan?s chief WTO negotiator.
friday evening plans were great, but saturday?s plans didnt go as expected ? i went dancing & it was an ok club,
but terribly crowded :-(
WHY THE HELL DO YOU GUYS ALL HAVE MRS. KENNEDY! SHES A FUCKING DOUCHE
AT&T was okay but whenever they do something nice in the name of customer service it seems like a favor, while
T-Mobile makes that a normal everyday thin
obama should be impeached on TREASON charges. Our Nuclear arsenal was TOP Secret. Till HE told our enemies
what we had. #Coward #Traitor
My graduation speech: ?I?d like to thanks Google, Wikipedia and my computer! :D #iThingteens
Table 5: List of example sentences with annotations that were provided to the annotators. All subjective phrases are
italicized. Positive phrases are in green, negative phrases are in red, and neutral phrases are in blue.
Worker 1 I would love to watch Vampire Diaries :) and some Heroes! Great combination 9/13
Worker 2 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Worker 3 I would love to watch Vampire Diaries :) and some Heroes! Great combination 10/13
Worker 4 I would love to watch Vampire Diaries :) and some Heroes! Great combination 13/13
Worker 5 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Intersection I would love to watch Vampire Diaries :) and some Heroes! Great combination
Table 6: Example of a sentence annotated for subjectivity on Mechanical Turk. Words and phrases that were marked as
subjective are italicized and highlighted in bold. The first five rows are annotations provided by Turkers, and the final
row shows their intersection. The final column shows the accuracy for each annotation compared to the intersection.
Note that ignoring Fneutral does not reduce the
task to predicting positive vs. negative labels only
(even though some participants have chosen to do
so) since the gold standard still contains neutral
labels which are to be predicted: Fpos and Fneg
would suffer if these examples are labeled as posi-
tive and/or negative instead of neutral.
We provided participants with a scorer. In addi-
tion to outputting the overall F-score, it produced
a confusion matrix for the three prediction classes
(positive, negative, and objective), and it also vali-
dated the data submission format.
5 Participants and Results
The results for subtask A are shown in Tables 7 and
8 for Twitter and for SMS messages, respectively;
those for subtask B are shown in Table 9 for Twit-
ter and in Table 10 for SMS messages. Systems are
ranked by their scores for the constrained runs; the
ranking based on scores for unconstrained runs is
shown as a subindex.
For both subtasks, there were teams that only sub-
mitted results for the Twitter test set. Some teams
submitted both a constrained and an unconstrained
version (e.g., AVAYA and teragram). As one would
expect, the results on the Twitter test set tended to be
better than those on the SMS test set since the SMS
data was out-of-domain with respect to the training
(Twitter) data.
Moreover, the results for subtask A were signifi-
cantly better than those for subtask B, which shows
that it is a much easier task, probably because there
is less ambiguity at the phrase-level.
5.1 Subtask A: Contextual Polarity
Table 7 shows that subtask A, Twitter, attracted 23
teams, who submitted 21 constrained and 7 uncon-
strained systems. Five teams submitted both a con-
strained and an unconstrained system, and two other
teams submitted constrained systems that are on
the boundary between being constrained and uncon-
strained.
316
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 88.93 yes yes
AVAYA 86.98 87.38(1) yes yes
BOUNCE 86.79 yes yes
LVIC-LIMSI 85.70 yes yes
FBM 85.50 yes semi
GU-MLT-LT 85.19 yes yes
UNITOR 84.60 yes yes
USNA 81.31 yes yes
Serendio 80.04 yes yes
ECNUCS 79.48 80.15(2) yes yes
TJP 78.16 yes yes
?columbia-nlp 74.94 yes yes
teragram 74.89(3) yes yes
sielers 74.41 yes yes
KLUE 73.74 yes yes
OPTWIMA 69.17 36.91(6) yes yes
swatcs 67.19 63.86(5) no yes
Kea 63.94 yes yes
senti.ue-en 62.79 71.38(4) yes yes
uottawa 60.20 yes yes
IITB 54.80 yes yes
SenselyticTeam 53.88 yes yes
SU-sentilab 34.73(7) no yes
Majority Baseline 38.10 N/A N/A
Table 7: Results for subtask A on the Twitter dataset. The
? marks a team that includes a task coorganizer, and the
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
One system was semi-supervised, and the rest
were supervised. The supervised systems used clas-
sifiers such as SVM (8 systems), Naive Bayes (7 sys-
tems), and Maximum Entropy (3 systems). Other
approaches used include an ensemble of classifiers,
manual rules, and a linear classifier. Two of the sys-
tems chose not to predict neutral as a possible clas-
sification label.
The average F1-measure on the Twitter test set
was 74.1% for constrained systems and 60.5% for
unconstrained ones; this does not mean that using
additional data does not help, it just shows that the
best teams only participated with a constrained sys-
tem. NRC-Canada had the best constrained system
with an F1-measure of 88.9%, and AVAYA had the
best unconstrained one with F1=87.4%.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
GU-MLT-LT 88.37 yes yes
NRC-Canada 88.00 yes yes
?AVAYA 83.94 85.79(1) yes yes
UNITOR 82.49 yes yes
TJP 81.23 yes yes
LVIC-LIMSI 80.16 yes yes
USNA 79.82 yes yes
ECNUCS 76.69 77.34(2) yes yes
sielers 73.48 yes yes
FBM 72.95 no semi
teragram 72.83 72.83(4) yes yes
KLUE 70.54 yes yes
?columbia-nlp 70.30 yes yes
senti.ue-en 66.09 74.13(3) yes yes
swatcs 66.00 67.68(5) no yes
Kea 63.27 yes yes
uottawa 55.89 yes yes
SU-sentilab 55.38(6) no yes
SenselyticTeam 51.13 yes yes
OPTWIMA 37.32 36.38(7) yes yes
Majority Baseline 31.50 N/A N/A
Table 8: Results for subtask A on the SMS dataset. The
? indicates a late submission, the ? marks a team that
includes a task co-organizer, and the  indicates a sys-
tem submitted as constrained but which used additional
Tweets or additional sentiment-annotated text to collect
statistics that were then used as a feature.
Table 8 shows the results for the SMS test set,
where 20 teams submitted 19 constrained and 7 un-
constrained systems (again, this included two teams
that submitted boundary systems, marked accord-
ingly). The average F-measure on this test set
was 70.8% for constrained systems and 65.7% for
unconstrained systems. The best constrained sys-
tem was that of GU-MLT-LT with an F-measure of
88.4%, and AVAYA had the best unconstrained sys-
tem with an F1 of 85.8%.
5.2 Subtask B: Message Polarity
Table 9 shows that subtask B, Twitter, attracted 38
teams, who submitted 36 constrained and 15 uncon-
strained systems (and two boundary ones).
The average F1-measure was 53.7% for the con-
strained and 54.6% for the unconstrained systems.
317
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 69.02 yes yes
GU-MLT-LT 65.27 yes yes
teragram 64.86 64.86(1) yes yes
BOUNCE 63.53 yes yes
KLUE 63.06 yes yes
AMI&ERIC 62.55 61.17(3) yes yes/semi
FBM 61.17 yes yes
AVAYA 60.84 64.06(2) yes yes/semi
SAIL 60.14 61.03(4) yes yes
UT-DB 59.87 yes yes
FBK-irst 59.76 yes yes
nlp.cs.aueb.gr 58.91 yes yes
UNITOR 58.27 59.50(5) yes semi
LVIC-LIMSI 57.14 yes yes
Umigon 56.96 yes yes
NILC USP 56.31 yes yes
DataMining 55.52 yes semi
ECNUCS 55.05 58.42(6) yes yes
nlp.cs.aueb.gr 54.73 yes yes
ASVUniOfLeipzig 54.56 yes yes
SZTE-NLP 54.33 53.10(9) yes yes
CodeX 53.89 yes yes
Oasis 53.84 yes yes
NTNU 53.23 50.71(10) yes yes
UoM 51.81 45.07(15) yes yes
SSA-UO 50.17 yes no
SenselyticTeam 50.10 yes yes
UMCC DLSI (SA) 49.27 48.99(12) yes yes
bwbaugh 48.83 54.37(8) yes yes/semi
senti.ue-en 47.24 47.85(13) yes yes
SU-sentilab 45.75(14) yes yes
OPTWIMA 45.40 54.51(7) yes yes
REACTION 45.01 yes yes
uottawa 42.51 yes yes
IITB 39.80 yes yes
IIRG 34.44 yes yes
sinai 16.28 49.26(11) yes yes
Majority Baseline 29.19 N/A N/A
Table 9: Results for subtask B on the Twitter dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
These averages are much lower than those for sub-
task A, which indicates that subtask B is harder,
probably because a message can contain parts ex-
pressing both positive and negative sentiment.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 68.46 yes yes
GU-MLT-LT 62.15 yes yes
KLUE 62.03 yes yes
AVAYA 60.00 59.47(1) yes yes/semi
teragram 59.10(2) yes yes
NTNU 57.97 54.55(6) yes yes
CodeX 56.70 yes yes
FBK-irst 54.87 yes yes
AMI&ERIC 53.63 52.62(7) yes yes/semi
ECNUCS 53.21 54.77(5) yes yes
UT-DB 52.46 yes yes
SAIL 51.84 51.98(8) yes yes
UNITOR 51.22 48.88(10) yes semi
SZTE-NLP 51.08 55.46(3) yes yes
SenselyticTeam 51.07 yes yes
NILC USP 50.12 yes yes
REACTION 50.11 yes yes
SU-sentilab 49.57(9) no yes
nlp.cs.aueb.gr 49.41 55.28(4) yes yes
LVIC-LIMSI 49.17 yes yes
FBM 47.40 yes yes
ASVUniOfLeipzig 46.50 yes yes
senti.ue-en 44.65 46.72(12) yes yes
SSA UO 44.39 yes no
UMCC DLSI (SA) 43.39 40.67(14) yes yes
UoM 42.22 35.22(15) yes yes
OPTWIMA 40.98 47.15(11) yes yes
uottawa 40.51 yes yes
bwbaugh 39.73 43.43(13) yes yes/semi
IIRG 22.16 yes yes
Majority Baseline 19.03 N/A N/A
Table 10: Results for subtask B on the SMS dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
Once again, NRC-Canada had the best con-
strained system with an F1-measure of 69%, fol-
lowed by teragram, which had the best uncon-
strained system with an F1-measure of 64.9%.
As Table 10 shows, the average F1-measure on
the SMS test set was 50.2% for constrained and
50.3% for unconstrained systems. NRC-Canada had
the best constrained system with an F1=68.5%, and
AVAYA had the best unconstrained one with F1-
measure of 59.5%.
318
5.3 Overall
Overall, the results achieved by the best teams were
very strong, especially for the simpler subtask A:
? F1=88.93, NRC-Canada on subtask A, Twitter;
? F1=88.37, GU-MLT-LT on subtask A, SMS;
? F1=69.02, NRC-Canada on subtask B, Twitter;
? F1=68.46, NRC-Canada on subtask B, SMS.
We can see that the strongest team overall was that
of NRC-Canada, which was ranked first on three of
the four conditions; and it was second on subtask A,
SMS. There were two other teams that were strong
across both tasks and on both test sets: GU-MLT-LT
and AVAYA. Three other teams, namely teragram,
BOUNCE and KLUE, were ranked in the top-3 in at
least one subtask and test set.
6 Discussion
We have seen that most participants restricted them-
selves to the provided data and submitted con-
strained systems. Indeed, the best systems for each
of the two subtasks and for each of the two testing
datasets were constrained systems; of course, this
does not mean that additional data would not be use-
ful. Curiously, in some cases where a team submit-
ted a constrained and unconstrained run, the uncon-
strained run actually performed worse.
Not surprisingly, most systems were supervised;
there were only five semi-supervised systems, and
there was only one unsupervised system. One ad-
ditional team declared their system as unsupervised
since it was not making use of the training data; we
still classified it as supervised though since it did use
supervision ? in the form of manual rules.
Most participants predicted all three labels (posi-
tive, negative and neutral), even though some partic-
ipants opted for not predicting neutral, which made
some sense since the final F1-score was averaged
over the positive and the negative predictions only.
The most popular classifiers included SVM, Max-
Ent, linear classifier, Naive Bayes; in some cases,
manual rules or ensembles of classifiers were used.
A variety of features were used, including word-
related (e.g., words, stems, n-grams, word clus-
ters), word-shape (e.g., punctuation, capitalization),
syntactic (e.g., POS tags, dependency relations),
Twitter-specific (e.g., repeated characters, emoti-
cons, URLs, hashtags, slang, abbreviations), and
sentiment-related (e.g., negation); one team also
used discourse relations. Almost all participants re-
lied heavily of various sentiment lexicons, the most
popular ones being MPQA and SentiWordNet, as
well as AFINN and Bing Liu?s Opinion Lexicon;
some participants used their own lexicons ? preex-
isting or built from the provided data.
Given that Twitter messages are noisy, most par-
ticipants did some preprocessing, including tok-
enization, stemming, lemmatization, stopword re-
moval, normalization/removal of URLs, hashtags,
users, slang, emoticons, repeated vowels, punctua-
tion; some even did pronoun resolution.
7 Conclusion
We have described a new task that entered SemEval-
2013: task 2 on Sentiment Analysis on Twitter. The
task has attracted a very high number of participants:
149 submissions from 44 teams.
We believe that the datasets that we have created
as part of the task and which we have released to the
community5 under a Creative Commons Attribution
3.0 Unported License,6 will be found useful by re-
searchers beyond SemEval.
Acknowledgments
The authors would like to thank Kathleen McKeown
for her insight in creating the Amazon Mechanical
Turk annotation task.
Funding for the Amazon Mechanical Turk anno-
tations was provided by the JHU Human Language
Technology Center of Excellence and by the Of-
fice of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab. All
statements of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
5http://www.cs.york.ac.uk/semeval-2013/task2/
6http://creativecommons.org/licenses/by/3.0/
319
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Nicoletta Calzolari (chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh International Conference
on Language Resources and Evaluation, LREC ?10,
pages 2200?2204, Valletta, Malta.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer, and
Ricard Gavalda`. 2011. Detecting sentiment change in
Twitter streaming data. Journal of Machine Learning
Research - Proceedings Track, 17:5?11.
Tao Chen and Min-Yen Kan. 2012. Creating a live, pub-
lic short message service corpus: the NUS SMS cor-
pus. Language Resources and Evaluation, pages 1?
37.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
Twitter and Amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 107?116, Upp-
sala, Sweden.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The Good
the Bad and the OMG! In Lada A. Adamic, Ricardo A.
Baeza-Yates, and Scott Counts, editors, Proceedings of
the Fifth International Conference on Weblogs and So-
cial Media, ICWSM? 11, pages 538?541, Barcelona,
Spain.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In William W. Cohen and
Samuel Gosling, editors, Proceedings of the Fourth
International Conference on Weblogs and Social
Media, ICWSM ?10, Washington, DC, USA.
Alexander Pak and Patrick Paroubek. 2010. Twitter
based system: Using Twitter for disambiguating senti-
ment ambiguous adjectives. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 436?439, Los Angeles, CA, USA.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an exper-
imental study. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?11, pages 1524?1534, Edinburgh, United
Kingdom.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?12, pages 1104?1112, Beijing, China.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about po-
litical sentiment. In William W. Cohen and Samuel
Gosling, editors, Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, pages 178?185, Washington, DC, USA.
The AAAI Press.
Julio Villena-Roma?n, Sara Lana-Serrano, Euge-
nio Mart??nez-Ca?mara, and Jose? Carlos Gonza?lez
Cristo?bal. 2013. TASS - Workshop on Sentiment
Analysis at SEPLN. Procesamiento del Lenguaje
Natural, 50:37?44.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
320
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 65?74,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Language Identification for Creating Language-Specific Twitter Collections
Shane Bergsma? Paul McNamee?,? Mossaab Bagdouri? Clayton Fink? Theresa Wilson?
?Human Language Technology Center of Excellence, Johns Hopkins University
?Johns Hopkins University Applied Physics Laboratory, Laurel, MD
?Department of Computer Science, University of Maryland, College Park, MD
sbergsma@jhu.edu, mcnamee@jhu.edu, mossaab@umd.edu, clayton.fink@jhuapl.edu, taw@jhu.edu
Abstract
Social media services such as Twitter offer an
immense volume of real-world linguistic data.
We explore the use of Twitter to obtain authen-
tic user-generated text in low-resource lan-
guages such as Nepali, Urdu, and Ukrainian.
Automatic language identification (LID) can
be used to extract language-specific data from
Twitter, but it is unclear how well LID per-
forms on short, informal texts in low-resource
languages. We address this question by an-
notating and releasing a large collection of
tweets in nine languages, focusing on confus-
able languages using the Cyrillic, Arabic, and
Devanagari scripts. This is the first publicly-
available collection of LID-annotated tweets
in non-Latin scripts, and should become a
standard evaluation set for LID systems. We
also advance the state-of-the-art by evaluat-
ing new, highly-accurate LID systems, trained
both on our new corpus and on standard ma-
terials only. Both types of systems achieve
a huge performance improvement over the
existing state-of-the-art, correctly classifying
around 98% of our gold standard tweets. We
provide a detailed analysis showing how the
accuracy of our systems vary along certain di-
mensions, such as the tweet-length and the
amount of in- and out-of-domain training data.
1 Introduction
Twitter is an online social-networking service that
lets users send and receive short texts called tweets.
Twitter is enormously popular; more than 50 mil-
lion users log in daily and billions of tweets are sent
each month.1 Tweets are publicly-available by de-
1http://mashable.com/2011/09/08/
Twitter-has-100-million-active-users/
fault and thus provide an enormous and growing free
resource of authentic, unedited text by ordinary peo-
ple. Researchers have used Twitter to study how hu-
man language varies by time zone (Kiciman, 2010),
census area (Eisenstein et al, 2011), gender (Burger
et al, 2011), and ethnicity (Fink et al, 2012). Twit-
ter also provides a wealth of user dialog, and a vari-
ety of dialog acts have been observed (Ritter et al,
2010) and predicted (Ritter et al, 2011).
Of course, working with Twitter is not all roses
and rainbows. Twitter is a difficult domain because
unlike, for example, news articles, tweets are short
(limited to 140 characters), vary widely in style,
and contain many spelling and grammatical errors.
Moreover, unlike articles written by a particular
news organization, a corpus constructed from Twit-
ter will contain tweets in many different languages.
This latter point is particularly troubling because
the majority of language-processing technology is
predicated on knowing which language is being pro-
cessed. We are pursuing a long-term effort to build
social media collections in a variety of low-resource
languages, and we need robust language identifica-
tion (LID) technology. While LID is often viewed
as a solved problem (McNamee, 2005), recent re-
search has shown that LID can be made arbitrarily
difficult by choosing domains with (a) informal writ-
ing, (b) lots of languages to choose from, (c) very
short texts, and (d) unbalanced data (Hughes et al,
2006; Baldwin and Lui, 2010). Twitter exhibits all
of these properties. While the problem of LID on
Twitter has been considered previously (Tromp and
Pechenizkiy, 2011; Carter et al, 2013), these studies
have only targeted five or six western European lan-
guages, and not the diversity of languages and writ-
ing systems that we would like to process.
65
Our main contribution is the release of a large col-
lection of tweets in nine languages using the Cyril-
lic, Arabic, and Devanagari alphabets. We test dif-
ferent methods for obtaining tweets in a given tar-
get language (?2). We then use an online crowd-
sourcing platform to have these tweets annotated by
fluent speakers of that language (?3). We generate
over 18,000 triple-consensus tweets, providing the
first publicly-available collection of LID-annotated
tweets in non-Latin scripts. The annotated cor-
pus is available online at: http://apl.jhu.edu/
?
paulmac/lid.html. We anticipate our multilin-
gual Twitter collection becoming a standard evalua-
tion set for LID systems.
We also implement two LID approaches and eval-
uate these approaches against state-of-the-art com-
petitors. ?4.1 describes a discriminative classifier
that leverages both the tweet text and the tweet meta-
data (such as the user name, location, and landing
pages for shortened URLs). ?4.2 describes an effi-
cient tool based on compression language models.
Both types of systems achieve a huge improvement
over existing state-of-the-art approaches, including
the Google Compact Language Detector (part of the
Chrome browser), and a recent LID system from
Lui and Baldwin (2011). Finally, we provide further
analysis of our systems in this unique domain, show-
ing how accuracy varies with the tweet-length and
the amount of in-domain and out-of-domain train-
ing data. In addition to the datasets, we are releasing
our compression language model tool for public use.
2 Acquiring Language-Specific Tweets
We use two strategies to collect tweets in specific
languages: (?2.1) we collect tweets by users who
follow language-specific Twitter sources, and (?2.2)
we use the Twitter API to collect tweets from users
who are likely to speak the target language.
2.1 Followers of Language-Specific Sources
Our first method is called the Sources method and
involves a three-stage process. First, Twitter sources
for the target language are manually identified.
Sources are Twitter users or feeds who: (a) tweet
in the target language, (b) have a large number of
followers, and (c) act as hubs (i.e., have a high
followers-to-following ratio). Twitter sources are
typically news or media outlets (e.g. BBC News),
celebrities, politicians, governmental organizations,
but they may just be prominent bloggers or tweeters.
Once sources are identified, we use the Twitter
API (dev.twitter.com) to query each source for
its list of followers. We then query the user data for
the followers in batches of 100 tweets. For users
whose data is public, a wealth of information is
returned, including the total number of tweets and
their most recent tweet. For users who had tweeted
above a minimum number of times. and whose
most-recent-tweet tweet was in the character set for
the target language, we obtained their most recent
100-200 tweets and added them to our collection.2
While we have used the above approach to ac-
quire data in a number of different languages, for the
purposes of our annotated corpus (?3), we select the
subsets of users who exclusively follow sources in
one of our nine target languages (Table 1). We also
filter tweets that do not contain at least one charac-
ter in the target?s corresponding writing system (we
plan to address romanized tweets in future work).
2.2 Direct Twitter-API Collection
While we are most interested in users who follow
news articles, we also tested other methods for ob-
taining language-specific tweets. First, we used the
Twitter API to collect tweets from locations where
we expected to get some number of tweets in the tar-
get language. We call this method the Twit-API col-
lection method. To geolocate our tweets, the Twit-
ter API?s geotag method allowed us to collect tweets
within a specified radius of a given set of coordi-
nates in latitude and longitude. To gather a sam-
ple of tweets in our target languages, we queried
for tweets from cities with populations of at least
200,000 where speakers of the target language are
prominent (e.g., Karachi, Pakistan for Urdu; Tehran,
Iran for Farsi; etc.). We collected tweets within a ra-
dius of 25 miles of the geocoordinates. We also used
the Search API to persistently poll for tweets from
users identified by Twitter as being in the queried
location. For Urdu, we also relied on the language-
2Tromp and Pechenizkiy (2011) also manually identified
language-specific Twitter feeds, but they use tweets from these
sources directly as gold standard data, while we target the users
who simply follow such sources. We expect our approach to
obtain more-authentic and less-edited user language.
66
identification code returned by the API for each
tweet; we filter all our geolocated Urdu tweets that
are not marked as Urdu.
We also obtained tweets through an information-
retrieval approach that has been used elsewhere for
creating minority language corpora (Ghani et al,
2001). We computed the 25 most frequent unique
words in a number of different languages (that is,
words that do not occur in the vocabularies of other
languages). Unfortunately, we found no way to en-
force that the Twitter API return only tweets con-
taining one or more of our search terms (e.g., re-
turned tweets for Urdu were often in Arabic and did
not contain our Urdu search terms). There is a lack
of documentation on what characters are supported
by the search API; it could be that the API cannot
handle certain of our terms. We thus leave further
investigation of this method for future work.
3 Annotating Tweets by Language
The general LID task is to take as input some piece
of text, and to produce as output a prediction of what
language the text is written in. Our annotation and
prediction systems operate at the level of individual
tweets. An alternative would have been to assume
that each user only tweets in a single language, and
to make predictions on an aggregation of multiple
tweets. We operate on individual tweets mainly be-
cause (A) we would like to quantify how often users
switch between languages and (B) we are also inter-
ested in domains and cases where only tweet-sized
amounts of text are available. When we do have
multiple tweets per user, we can always aggregate
the scores on individual predictions (?6 has some ex-
perimental results using prediction aggregation).
Our human annotation therefore also focuses on
validating the language of individual tweets. Tweets
verified by three independent annotators are ac-
cepted into our final gold-standard data.
3.1 Amazon Mechanical Turk
To access annotators with fluency in each language,
we crowdsourced the annotation using Amazon Me-
chanical Turk (mturk.com). AMT is an online la-
bor marketplace that allows requesters to post tasks
for completion by paid human workers. Crowd-
sourcing via AMT has been shown to provide high-
quality data for a variety of NLP tasks (Snow et al,
2008; Callison-Burch and Dredze, 2010), including
multilingual annotation efforts in translation (Zaidan
and Callison-Burch, 2011b), dialect identification
(Zaidan and Callison-Burch, 2011a), and building
bilingual lexicons (Irvine and Klementiev, 2010).
3.2 Annotation Task
From the tweets obtained in ?2, we took a random
sample in each target language, and posted these
tweets for annotation on AMT. Each tweet in the
sample was assigned to a particular AMT job; each
job comprised the annotation of 20 tweets. The job
description requested workers that are fluent in the
target language and gave an example of valid and
invalid tweets in that language. The job instructions
asked workers to mark whether each tweet was writ-
ten for speakers of the target language. If the tweet
combines multiple languages, workers were asked
to mark as the target language if ?most of the text is
in [that language] excluding URLs, hash-tags, etc.?
Jobs were presented to workers as HTML pages with
three buttons alongside each tweet for validating the
language. For example, for Nepali, a Worker can
mark that a tweet is ?Nepali?, ?Not Nepali?, or ?Not
sure.? We paid $0.05 per job and requested that each
job be completed by three workers.
3.3 Quality Control
To ensure high annotation quality, we follow our
established practices in only allowing our tasks to
be completed by workers who have previously com-
pleted at least 50 jobs on AMT, and who have had at
least 85% of their jobs approved. Our jobs also dis-
play each tweet as an image; this prevents workers
from pasting the tweet into existing online language
processing services (like Google Translate).
We also have control tweets in each job to allow
us to evaluate worker performance. A positive con-
trol is a tweet known to be in the target language;
a negative control is a tweet known to be in a dif-
ferent language. Between three to six of the twenty
tweets in each job were controls. The controls are
taken from the sources used in our Sources method
(?2.1); e.g., our Urdu controls come from sources
like BBC Urdu?s Twitter feed. To further validate
the controls, we also applied our open-domain LID
system (?4.2) and filtered any Source tweets whose
67
Language Method Purity Gold Tweets
Arabic Sources 100% 1174
Farsi Sources 100% 2512
Urdu Sources 55.4% 1076
Arabic Twit-API 99.9% 1254
Farsi Twit-API 99.7% 2366
Urdu Twit-API 61.0% 1313
Hindi Sources 97.5% 1214
Nepali Sources 97.3% 1681
Marathi Sources 91.4% 1157
Russian Sources 99.8% 2005
Bulgarian Sources 92.2% 1886
Ukrainian Sources 14.3% 631
Table 1: Statistics of the Annotated Multilingual Twitter
Corpus: 18,269 total tweets in nine languages.
predicted language was not the expected language.
Our negative controls are validated tweets in a lan-
guage that uses the same alphabet as the target (e.g.,
our negative controls for Ukrainian were taken from
our LID-validated Russian and Bulgarian sources).
We collect aggregate statistics for each Worker
over the control tweets of all their completed jobs.
We conservatively discard any annotations by work-
ers who get below 80% accuracy on either the posi-
tive or negative control tweets.
3.4 Dataset Statistics
Table 1 gives the number of triple-validated ?Gold?
tweets in each language, grouped into those using
the Arabic, Devanagari and Cyrillic writing sys-
tems. The Arabic data is further divided into tweets
acquired using the Sources and Twit-API methods.
Table 1 also gives the Purity of the acquired re-
sults; that is, the percentage of acquired tweets that
were indeed in the target language. The Purity
is calculated as the number of triple-verified gold
tweets divided by the total number of tweets where
the three annotators agreed in the annotation (thus
triply-marked either Yes, No, or Not sure).
For major languages (e.g. Arabic and Russian),
we can accurately obtain tweets in the target lan-
guage, perhaps obviating the need for LID. For the
Urdu sets, however, a large percentage of tweets are
not in Urdu, and thus neither collection method is
reliable. An LID tool is needed to validate the data.
A native Arabic speaker verified that most of our
invalid Urdu tweets were Arabic. Ukrainian is the
most glaringly impure language that we collected,
with less than 15% of our intended tweets actually
in Ukrainian. Russian is widely spoken in Ukraine
and seems to be the dominant language on Twitter,
but more analysis is required. Finally, Marathi and
Bulgarian also have significant impurities.
The complete annotation of all nine languages
cost only around $350 USD. While not insignificant,
this was a small expense relative to the total human
effort we are expending on this project. Scaling our
approach to hundreds of languages would only cost
on the order of a few thousand dollars, and we are
investigating whether such an effort could be sup-
ported by enough fluent AMT workers.
4 Language Identification Systems
We now describe the systems we implemented
and/or tested on our annotated data. All the ap-
proaches are supervised learners, trained from a col-
lection of language-annotated texts. At test time, the
systems choose an output language based on the in-
formation they have derived from the annotated data.
4.1 LogR: Discriminative LID
We first adopt a discriminative approach to LID.
Each tweet to be classified has its relevant informa-
tion encoded in a feature vector, x?. The annotated
training data can be represented as N pairs of la-
bels and feature vectors: {(y1, x?1), ..., (yN , x?N )}.
To train our model, we use (regularized) logistic re-
gression (a.k.a. maximum entropy) since it has been
shown to perform well on a range of NLP tasks
and its probabilistic outputs are useful for down-
stream processing (such as aggregating predictions
over multiple tweets). In multi-class logistic regres-
sion, the probability of each class takes the form of
exponential functions over features:
p(y = k|x?) = exp(w?k ? x?)?
j exp(w?j ? x?)
For LID, the classifier predicts the language k that
has the highest probability (this is also the class with
highest weighted combination of features, w?k ? x?).
The training procedure tunes the weights to optimize
for correct predictions on training data, subject to a
tunable L2-regularization penalty on the weight vec-
tor norm. For our experiments, we train and test our
logistic regression classifier (LogR) using the effi-
cient LIBLINEAR package (Fan et al, 2008).
68
We use two types of features in our classifier:
Character Features encode the character
N-grams in the input text; characters are the
standard information source for most LID systems
(Cavnar and Trenkle, 1994; Baldwin and Lui, 2010).
We have a unique feature for each unique N-gram in
our training data. N-grams of up-to-four characters
were optimal on development data. Each feature
value is the (smoothed) log-count of how often
the corresponding N-gram occurs in that instance.
Prior to extracting the N-grams, we preprocess each
tweet to remove URLs, hash-tags, user mentions,
punctuation and we normalize all digits to 0.
Meta features encode user-provided information
beyond the tweet text. Similar information has pre-
viously been used to improve the accuracy of LID
classifiers on European-language tweets (Carter et
al., 2013). We have features for the tokens in
the Twitter user name, the screen name, and self-
reported user location. We also have features for
prefixes of these tokens, and flags for whether the
name and location are in the Latin script. Our meta
features also include features for the hash-tags, user-
mentions, and URLs in the tweet. We provide fea-
tures for the protocol (e.g. http), hostname, and top-
level domain (e.g. .com) of each link in a tweet. For
shortened URLs (e.g. via bit.ly), we query the
URL server to obtain the final link destination, and
provide the URL features for this destination link.
4.2 PPM: Compression-Based LID
Our next tool uses compression language models,
which have been proposed for a variety of NLP
tasks including authorship attribution (Pavelec et al,
2009), text classification (Teahan, 2000; Frank et al,
2000), spam filtering (Bratko et al, 2006), and LID
(Benedetto et al, 2002). Our method is based on the
prediction by partial matching (PPM) family of al-
gorithms and we use the PPM-A variant (Cleary et
al., 1984). The algorithm processes a string and de-
termines the number of bits required to encode each
character using a variable-length context. It requires
only a single parameter, the maximal order, n; we
use n = 5 for the experiments in this paper. Given
training data for a number of languages, the method
seeks to minimize cross-entropy and thus selects the
Language Wikip. All
Arabic 372 MB 1058 MB
Farsi 229 MB 798 MB
Urdu 30 MB 50 MB
Hindi 235 MB 518 MB
Nepali 31 MB 31 MB
Marathi 32 MB 66 MB
Russian 563 MB 564 MB
Bulgarian 301 MB 518 MB
Ukrainian 461 MB 463 MB
Table 2: Size of other PPM training materials.
language which would most compactly encode the
text we are attempting to classify.
We train this method both on our Twitter data and
on large collections of other material. These ma-
terials include corpora obtained from news sources,
Wikipedia, and government bodies. For our ex-
periments we divide these materials into two sets:
(1) just Wikipedia and (2) all sources, including
Wikipedia. Table 2 gives the sizes of these sets.
4.3 Comparison Systems
We compare our two new systems with the best-
available commercial and academic software.
TextCat: TextCat3 is a widely-used stand-alone
LID program. Is is an implementation of the
N-gram-based algorithm of Cavnar and Trenkle
(1994), and supports identification in ?about 69 lan-
guages? in its downloadable form. Unfortunately,
the available models do not support all of our target
languages, nor are they compatible with the standard
UTF-8 Unicode character encoding. We therefore
modified the code to process UTF-8 characters and
re-trained the system on our Twitter data (?5).
Google CLD: Google?s Chrome browser includes
a tool for language-detection (the Google Compact
Language Detector), and this tool is included as a li-
brary within Chrome?s open-source code. Mike Mc-
Candless ported this library to its own open source
project.4 The CLD tool makes predictions using text
4-grams. It is designed for detecting the language
of web pages, and can take meta-data hints from the
domain of the webpage and/or the declared webpage
3http://odur.let.rug.nl/vannoord/TextCat/
4http://code.google.com/p/
chromium-compact-language-detector/
69
Dataset Train Development Test
Arabic 2254 1171 1191
Devanagari 2099 991 962
Cyrillic 2243 1133 1146
Table 3: Number of tweets used in experiments, by writ-
ing system/classification task
encoding, but it also works on stand-alone text.5 We
use it in its original, unmodified form. While there
are few details in the source code itself, the train-
ing data for this approach was apparently obtained
through Google?s internal data collections.
Lui and Baldwin ?11: Lui and Baldwin (2011) re-
cently released a stand-alone LID tool, which they
call langid.py.6 They compared this system to
state-of-the-art LID methods and found it ?to be
faster whilst maintaining competitive accuracy.? We
use this system with its provided models only, as
the software readme notes ?training a model for
langid.py is a non-trivial process, due to the large
amount of computations required.? The sources of
the provided models are described in Lui and Bald-
win (2011). Although many languages are sup-
ported, we restrict the system to only choose be-
tween our data?s target languages (?5).
5 Experiments
The nine languages in our annotated data use one of
three different writing systems: Arabic, Devanagari,
or Cyrillic. We therefore define three classification
tasks, each choosing between three languages that
have the same writing system. We divide our an-
notated corpus into training, development and test
data for these experiments (Table 3). For the Ara-
bic data, we merge the tweets obtained via our two
collection methods (?2); for Devanagari/Cyrillic, all
tweets are obtained using the Sources method. We
ensure that tweets by a unique Twitter user occur
in at most only one of the sets. The proportion of
each language in each set is roughly the same as the
proportions of gold tweets in Table 1. All of our
Twitter-trained systems learn their models from this
training data, while all hyperparameter tuning (such
5Google once offered an online language-detection API, but
this service is now deprecated; moreover, it was rate-limited and
not licensed for research use (Lui and Baldwin, 2011).
6https://github.com/saffsd/langid.py
System Arab. Devan. Cyrill.
Trained on Twitter Corpus:
LogR: meta 79.8 74.7 82.0
LogR: chars 97.1 96.2 96.1
LogR: chars+meta 97.4 96.9 98.3
PPM 97.1 95.3 95.8
TextCat 96.3 89.1 90.3
Open-Domain: Trained on Other Materials:
Google CLD 90.5 N/A 91.4
Lui and Baldwin ?11 91.4 78.4 88.8
PPM (Wikip.) 97.6 95.8 95.7
PPM (All) 97.6 97.1 95.8
Trained on both Twitter and Other Materials:
PPM (Wikip.+Twit) 97.9 97.0 95.9
PPM (All+Twit) 97.6 97.9 96.0
Table 4: LID accuracy (%) of different systems on held-
out tweets. High LID accuracy on tweets is obtainable,
whether training in or out-of-domain.
as tuning the regularization parameter of the LogR
classifier) is done on the development set. Our eval-
uation metric is Accuracy: what proportion of tweets
in each held-out test set are predicted correctly.
6 Results
For systems trained on the Twitter data, both our
LogR and PPM system strongly outperform TextCat,
showing the effectiveness of our implemented ap-
proaches (Table 4). Meta features improve LogR
on each task. For systems trained on external data,
PPM strongly outperforms other systems, making
fewer than half the errors on each task. We also
trained PPM on both the relatively small number of
Twitter training samples and the much larger number
of other materials. The combined system is as good
or better than the separate models on each task.
We get more insight into our systems by seeing
how they perform as we vary the amount of train-
ing data. Figure 1 shows that with only a few hun-
dred annotated tweets, the LogR system gets over
90% accuracy, while performance seems to plateau
shortly afterwards. A similar story holds as we
vary the amount of out-of-domain training data for
the PPM system; performance improves fairly lin-
early as exponentially more training data is used, but
eventually begins to level off. Not only is PPM an
effective system, it can leverage a lot of training ma-
70
 50
 60
 70
 80
 90
 100
 10  100  1000
Ac
cu
ra
cy
 (%
)
Number of training tweets
Arabic
Devanagari
Cyrillic
Figure 1: The more training data the better, but accuracy
levels off: learning curve for LogR-chars (note log-scale).
 50
 60
 70
 80
 90
 100
 100  1000  10000 100000 1e+06  1e+07
Ac
cu
ra
cy
 (%
)
Number of characters of training data
Arabic
Devanagari
Cyrillic
Figure 2: Accuracy of PPM classifier using varying
amounts of Wikipedia training text (also on log-scale).
terials in order to obtain its high accuracy.
In Figure 3, we show how the accuracy of our sys-
tems varies over tweets grouped into bins by their
length. Performance on short tweets is much worse
than those closer to 140 characters in length.
We also examined aggregating predictions over
multiple tweets by the same user. We extracted all
users with ?4 tweets in the Devanagari test set (87
users in total). We then averaged the predictions of
the LogR system on random subsets of a user?s test
tweets, making a single decision for all tweets in a
subset. We report the mean accuracy of running this
approach 100 times with random subsets of 1, 2, 3,
and all 4 tweets used in the prediction. Even with
only 2 tweets per user, aggregating predictions can
reduce relative error by almost 60% (Table 5).
Encouraged by the accuracy of our systems on an-
notated data, we used our PPM system to analyze
a large number of un-annotated tweets. We trained
PPM models for 128 languages using data that in-
cludes Wikipedia (February 2012), news (e.g., BBC
News, Voice of America), and standard corpora such
 88
 90
 92
 94
 96
 98
 100
 40  60  80  100  120  140
Ac
cu
ra
cy
 (%
)
Avgerage length of tweet (binned)
Arabic
Devanagari
Cyrillic
Figure 3: The longer the tweet, the better: mean accuracy
of LogR by average length of tweet, with tweets grouped
into five bins by length in characters.
Number of Tweets 1 2 3 4
Accuracy 97.0 98.7 98.8 98.9
Table 5: The benefits of aggregating predictions by user:
Mean accuracy of LogR-chars as you make predictions
on multiple Devanagari tweets at a time
as Europarl, JRC-Acquis, and various LDC releases.
We then made predictions in the TREC Tweets2011
Corpus.7
We observed 65 languages in roughly 10 million
tweets. We calculated two other proportions using
auxiliary data:8 (1) the proportion of Wikipedia arti-
cles written in each language, and (2) the proportion
of speakers that speak each language. We use these
proportions to measure a language?s relative repre-
sentation on Twitter: we divide the tweet-proportion
by the Wikipedia and speaker proportions. Table 6
shows some of the most over-represented Twitter
languages compared to Wikipedia. E.g., Indonesian
is predicted to be 9.9 times more relatively com-
mon on Twitter than Wikipedia. Note these are pre-
dictions only; some English tweets may be falsely
marked as other languages due to English impurities
in our training sources. Nevertheless, the good rep-
resentation of languages with otherwise scarce elec-
tronic resources shows the potential of using Twitter
to build language-specific social media collections.
7http://trec.nist.gov/data/tweets/ This corpus, de-
veloped for the TREC Microblog track (Soboroff et al, 2012), contains
a two-week Twitter sample from early 2011. We processed all tweets
that were obtained with a ?200? response code using the twitter-corpus-
tools package.
8From http://meta.wikimedia.org/wiki/List_of_
Wikipedias_by_speakers_per_article
71
Language Num. % of Tweets/ Tweets/
Tweets Tot. Wikip. Speakers
Indonesian 1055 9.0 9.9 3.1
Thai 238 2.0 5.7 1.9
Japanese 2295 19.6 5.0 8.8
Korean 446 3.8 4.0 3.2
Swahili 46 0.4 3.4 0.4
Portuguese 1331 11.4 3.2 2.8
Marathi 58 0.5 2.9 0.4
Malayalam 30 0.3 2.2 0.4
Nepali 23 0.2 2.1 0.8
Macedonian 61 0.5 1.9 13.9
Bengali 25 0.2 1.9 0.1
Turkish 174 1.5 1.7 1.1
Arabic 162 1.4 1.6 0.3
Chinese 346 3.0 1.4 0.2
Spanish 696 5.9 1.4 0.7
Telugu 39 0.3 1.4 0.3
Croatian 79 0.7 1.3 6.1
English 2616 22.3 1.2 2.1
Table 6: Number of tweets (1000s) and % of total for lan-
guages that appear to be over-represented on Twitter (vs.
proportion of Wikipedia and proportion of all speakers).
7 Related Work
Researchers have tackled language identification us-
ing statistical approaches since the early 1990s.
Cavnar and Trenkle (1994) framed LID as a text
categorization problem and made their influential
TextCat tool publicly-available. The related problem
of identifying the language used in speech signals
has also been well-studied; for speaker LID, both
phonetic and sequential information may be help-
ful (Berkling et al, 1994; Zissman, 1996). Insights
from LID have also been applied to related problems
such as dialect determination (Zaidan and Callison-
Burch, 2011a) and identifying the native language
of non-native speakers (Koppel et al, 2005).
Recently, LID has received renewed interest as a
mechanism to help extract language-specific corpora
from the growing body of linguistic materials on the
web (Xia et al, 2009; Baldwin and Lui, 2010). Work
along these lines has found LID to be far from a
solved problem (Hughes et al, 2006; Baldwin and
Lui, 2010; Lui and Baldwin, 2011); the web in gen-
eral has exactly the uneven mix of style, languages,
and lengths-of-text that make the real problem quite
difficult. New application areas have also arisen,
each with their own unique challenges, such as LID
for search engine queries (Gottron and Lipka, 2010),
or person names (Bhargava and Kondrak, 2010).
The multilinguality of Twitter has led to the de-
velopment of ways to ensure language purity. Rit-
ter et al (2010) use ?a simple function-word-driven
filter. . . to remove non-English [Twitter] conversa-
tions,? but it?s unclear how much non-English sur-
vives the filtering and how much English is lost.
Tromp and Pechenizkiy (2011) and Carter et al
(2013) perform Twitter LID, but only targeting six
common European languages. We focus on low-
resource languages, where training data is scarce.
Our data and systems could enable better LID
for services like indigenoustweets.com, which
aims to ?strengthen minority languages through so-
cial media.?
8 Conclusions
Language identification is a key technology for ex-
tracting authentic, language-specific user-generated
text from social media. We addressed a previously
unexplored issue: LID performance on Twitter text
in low-resource languages. We have created and
made available a large corpus of human-annotated
tweets in nine languages and three non-Latin writ-
ing systems, and presented two systems that can pre-
dict tweet language with very high accuracy.9 While
challenging, LID on Twitter is perhaps not as diffi-
cult as first thought (Carter et al, 2013), although
performance depends on the amount of training data,
the length of the tweet, and whether we aggregate
information across multiple tweets by the same user.
Our next step will be to develop a similar approach
to handle romanized text. We also plan to develop
tools for identifying code-switching (switching lan-
guages) within a tweet.
Acknowledgments
We thank Chris Callison-Burch for his help with the
crowdsourcing. The first author was supported by the
Natural Sciences and Engineering Research Council of
Canada. The third author was supported by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015
9The annotated corpus and PPM system are available online
at: http://apl.jhu.edu/
?
paulmac/lid.html
72
References
Timothy Baldwin and Marco Lui. 2010. Language iden-
tification: The long and the short of the matter. In
Proc. HLT-NAACL, pages 229?237.
Dario Benedetto, Emanuele Caglioti, and Vittorio Loreto.
2002. Language trees and zipping. Physical Review
Letters, 88(4):2?5.
Kay Berkling, Takayuki Arai, and Etienne Barnard.
1994. Analysis of phoneme-based features for lan-
guage identification. In Proc. ICASSP, pages 289?
292.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
HLT-NAACL, pages 693?696.
Andrej Bratko, Gordon V. Cormack, Bogdan Filipic,
Thomas R. Lynam, and Blaz Zupan. 2006. Spam
filtering using statistical data compression models.
JMLR, 6:2673?2698.
John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on Twitter. In
Proc. EMNLP, pages 1301?1309.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proc. NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazon?s Me-
chanical Turk, pages 1?12.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013. Microblog Language Identification: Overcom-
ing the Limitations of Short, Unedited and Idiomatic
Text. Language Resources and Evaluation Journal.
(forthcoming).
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proc. Symposium on
Document Analysis and Information Retrieval, pages
161?175.
John G. Cleary, Ian, and Ian H. Witten. 1984. Data
compression using adaptive coding and partial string
matching. IEEE Transactions on Communications,
32:396?402.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. ACL, pages 1365?1374.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. JMLR, 9:1871?
1874.
Clayton Fink, Jonathon Kopecky, Nathan Bos, and Max
Thomas. 2012. Mapping the Twitterverse in the devel-
oping world: An analysis of social media use in Nige-
ria. In Proc. International Conference on Social Com-
puting, Behavioral Modeling, and Prediction, pages
164?171.
Eibe Frank, Chang Chui, and Ian H. Witten. 2000. Text
categorization using compression models. In Proc.
DCC-00, IEEE Data Compression Conference, Snow-
bird, US, pages 200?209. IEEE Computer Society
Press.
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2001.
Automatic web search query generation to create mi-
nority language corpora. In Proceedings of the 24th
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?01, pages 432?433, New York, NY, USA. ACM.
Thomas Gottron and Nedim Lipka. 2010. A comparison
of language identification approaches on short, query-
style texts. In Proc. ECIR, pages 611?614.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew Mackinlay. 2006. Reconsid-
ering language identification for written language re-
sources. In Proc. LREC, pages 485?488.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Proc. NAACL HLT 2010 Workshop
on Creating Speech and Language Data with Ama-
zon?s Mechanical Turk, pages 108?113.
Emre Kiciman. 2010. Language differences and meta-
data features on Twitter. In Proc. SIGIR 2010 Web
N-gram Workshop, pages 47?51.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proc. KDD, pages 624?628.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Proc.
IJCNLP, pages 553?561.
Paul McNamee. 2005. Language identification: a solved
problem suitable for undergraduate instruction. J.
Comput. Sci. Coll., 20(3):94?101.
D. Pavelec, L. S. Oliveira, E. Justino, F. D. Nobre Neto,
and L. V. Batista. 2009. Compression and stylometry
for author identification. In Proc. IJCNN, pages 669?
674, Piscataway, NJ, USA. IEEE Press.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Proc.
HLT-NAACL, pages 172?180.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proc. EMNLP, pages 583?593.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proc. EMNLP, pages 254?263.
Ian Soboroff, Dean McCullough, Jimmy Lin, Craig Mac-
donald, Iadh Ounis, and Richard McCreadie. 2012.
Evaluating real-time search over tweets. In Proc.
ICWSM.
73
William John Teahan. 2000. Text classification and
segmentation using minimum cross-entropy. In Proc.
RIAO, pages 943?961.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
based n-gram language identication on short texts. In
Proc. 20th Machine Learning conference of Belgium
and The Netherlands, pages 27?34.
Fei Xia, William Lewis, and Hoifung Poon. 2009. Lan-
guage ID in the context of harvesting language data off
the web. In Proc. EACL, pages 870?878.
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The arabic online commentary dataset: an annotated
dataset of informal arabic with high dialectal content.
In Proc. ACL, pages 37?41.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proc. ACL, pages 1220?1229.
Marc A. Zissman. 1996. Comparison of four ap-
proaches to automatic language identification of tele-
phone speech. IEEE Transactions on Speech and Au-
dio Processing, 4(1):31?44.
74
