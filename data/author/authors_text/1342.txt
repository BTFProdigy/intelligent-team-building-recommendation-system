Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793?801,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Learning with Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Abstract
Determining the polarity of a sentiment-
bearing expression requires more than a sim-
ple bag-of-words approach. In particular,
words or constituents within the expression
can interact with each other to yield a particu-
lar overall polarity. In this paper, we view such
subsentential interactions in light of composi-
tional semantics, and present a novel learning-
based approach that incorporates structural in-
ference motivated by compositional seman-
tics into the learning procedure. Our exper-
iments show that (1) simple heuristics based
on compositional semantics can perform bet-
ter than learning-based methods that do not in-
corporate compositional semantics (accuracy
of 89.7% vs. 89.1%), but (2) a method that
integrates compositional semantics into learn-
ing performs better than all other alterna-
tives (90.7%). We also find that ?content-
word negators?, not widely employed in pre-
vious work, play an important role in de-
termining expression-level polarity. Finally,
in contrast to conventional wisdom, we find
that expression-level classification accuracy
uniformly decreases as additional, potentially
disambiguating, context is considered.
1 Introduction
Determining the polarity of sentiment-bearing ex-
pressions at or below the sentence level requires
more than a simple bag-of-words approach. One of
the difficulties is that words or constituents within
the expression can interact with each other to yield
a particular overall polarity. To facilitate our discus-
sion, consider the following examples:
1: [I did [not]? have any [doubt]? about it.]+
2: [The report [eliminated]? my [doubt]?.]+
3: [They could [not]? [eliminate]? my [doubt]?.]?
In the first example, ?doubt? in isolation carries
a negative sentiment, but the overall polarity of the
sentence is positive because there is a negator ?not?,
which flips the polarity. In the second example, both
?eliminated? and ?doubt? carry negative sentiment
in isolation, but the overall polarity of the sentence
is positive because ?eliminated? acts as a negator for
its argument ?doubt?. In the last example, there are
effectively two negators ? ?not? and ?eliminated? ?
which reverse the polarity of ?doubt? twice, result-
ing in the negative polarity for the overall sentence.
These examples demonstrate that words or con-
stituents interact with each other to yield the
expression-level polarity. And a system that sim-
ply takes the majority vote of the polarity of indi-
vidual words will not work well on the above exam-
ples. Indeed, much of the previous learning-based
research on this topic tries to incorporate salient in-
teractions by encoding them as features. One ap-
proach includes features based on contextual va-
lence shifters1 (Polanyi and Zaenen, 2004), which
are words that affect the polarity or intensity of sen-
timent over neighboring text spans (e.g., Kennedy
and Inkpen (2005), Wilson et al (2005), Shaikh et
al. (2007)). Another approach encodes frequent sub-
sentential patterns (e.g., McDonald et al (2007)) as
features; these might indirectly capture some of the
subsentential interactions that affect polarity. How-
1For instance, ?never?, ?nowhere?, ?little?, ?most?, ?lack?,
?scarcely?, ?deeply?.
793
ever, both types of approach are based on learning
models with a flat bag-of-features: some structural
information can be encoded as higher order features,
but the final representation of the input is still a flat
feature vector that is inherently too limited to ade-
quately reflect the complex structural nature of the
underlying subsentential interactions. (Liang et al,
2008)
Moilanen and Pulman (2007), on the other hand,
handle the structural nature of the interactions more
directly using the ideas from compositional seman-
tics (e.g., Montague (1974), Dowty et al (1981)). In
short, the Principle of Compositionality states that
the meaning of a compound expression is a func-
tion of the meaning of its parts and of the syntac-
tic rules by which they are combined (e.g., Mon-
tague (1974), Dowty et al (1981)). And Moilanen
and Pulman (2007) develop a collection of compo-
sition rules to assign a sentiment value to individual
expressions, clauses, or sentences. Their approach
can be viewed as a type of structural inference, but
their hand-written rules have not been empirically
compared to learning-based alternatives, which one
might expect to be more effective in handling some
aspects of the polarity classification task.
In this paper, we begin to close the gap between
learning-based approaches to expression-level po-
larity classification and those founded on composi-
tional semantics: we present a novel learning-based
approach that incorporates structural inference mo-
tivated by compositional semantics into the learning
procedure.
Adopting the view point of compositional seman-
tics, our working assumption is that the polarity of a
sentiment-bearing expression can be determined in a
two-step process: (1) assess the polarities of the con-
stituents of the expression, and then (2) apply a rela-
tively simple set of inference rules to combine them
recursively. Rather than a rigid application of hand-
written compositional inference rules, however, we
hypothesize that an ideal solution to the expression-
level polarity classification task will be a method
that can exploit ideas from compositional seman-
tics while providing the flexibility needed to handle
the complexities of real-world natural language ?
exceptions, unknown words, missing semantic fea-
tures, and inaccurate or missing rules. The learning-
based approach proposed in this paper takes a first
step in this direction.
In addition to the novel learning approach, this
paper presents new insights for content-word nega-
tors, which we define as content words that can
negate the polarity of neighboring words or con-
stituents. (e.g., words such as ?eliminated? in the
example sentences). Unlike function-word nega-
tors, such as ?not? or ?never?, content-word nega-
tors have been recognized and utilized less actively
in previous work. (Notable exceptions include e.g.,
Niu et al (2005), Wilson et al (2005), and Moilanen
and Pulman (2007).2)
In our experiments, we compare learning- and
non-learning-based approaches to expression-level
polarity classification ? with and without com-
positional semantics ? and find that (1) simple
heuristics based on compositional semantics outper-
form (89.7% in accuracy) other reasonable heuris-
tics that do not incorporate compositional seman-
tics (87.7%); they can also perform better than sim-
ple learning-based methods that do not incorporate
compositional semantics (89.1%), (2) combining
learning with the heuristic rules based on compo-
sitional semantics further improves the performance
(90.7%), (3) content-word negators play an impor-
tant role in determining the expression-level polar-
ity, and, somewhat surprisingly, we find that (4)
expression-level classification accuracy uniformly
decreases as additional, potentially disambiguating,
context is considered.
In what follows, we first explore heuristic-based
approaches in ?2, then we present learning-based ap-
proaches in ?3. Next we present experimental results
in ?4, followed by related work in ?5.
2 Heuristic-Based Methods
This section describes a set of heuristic-based meth-
ods for determining the polarity of a sentiment-
bearing expression. Each assesses the polarity of the
words or constituents using a polarity lexicon that
indicates whether a word has positive or negative
polarity, and finds negators in the given expression
using a negator lexicon. The methods then infer the
expression-level polarity using voting-based heuris-
tics (? 2.1) or heuristics that incorporate composi-
tional semantics (?2.2). The lexicons are described
2See ?5. Related Work for detailed discussion.
794
VOTE NEG(1) NEG(N) NEGEX(1) NEGEX(N) COMPO
type of negators none function-word function-word & content-word
maximum # of negations applied 0 1 n 1 n n
scope of negators N/A over the entire expression compositional
Table 1: Heuristic methods. (n refers to the number of negators found in a given expression.)
Rules Examples
1 Polarity( not [arg1] ) = ? Polarity( arg1 ) not [bad]arg1.
2 Polarity( [VP] [NP] ) = Compose( [VP], [NP] ) [destroyed]VP [the terrorism]NP .
3 Polarity( [VP1] to [VP2] ) = Compose( [VP1], [VP2] ) [refused]V P1 to [deceive]V P2 the man.
4 Polarity( [adj] to [VP] ) = Compose( [adj], [VP] ) [unlikely]adj to [destroy]V P the planet.
5 Polarity( [NP1] [IN] [NP2] ) = Compose( [NP1], [NP2] ) [lack]NP1 [of]IN [crime]NP2 in rural areas.
6 Polarity( [NP] [VP] ) = Compose( [VP], [NP] ) [pollution]NP [has decreased]V P .
7 Polarity( [NP] be [adj] ) = Compose( [adj], [NP] ) [harm]NP is [minimal]adj .
Definition of Compose( arg1, arg2 )
Compose( arg1, arg2 ) =
For COMPOMC: if (arg1 is a negator) then ? Polarity( arg2 )
(COMPOsition with Majority Class) else if (Polarity( arg1 ) == Polarity( arg2 )) then Polarity( arg1 )
else the majority polarity of data
Compose( arg1, arg2 ) =
For COMPOPR: if (arg1 is a negator) then ? Polarity( arg2 )
(COMPOsition with PRiority) else Polarity( arg1 )
Table 2: Compositional inference rules motivated by compositional semantics.
in ?2.3.
2.1 Voting
We first explore five simple heuristics based on vot-
ing. VOTE is defined as the majority polarity vote
by words in a given expression. That is, we count
the number of positive polarity words and negative
polarity words in a given expression, and assign the
majority polarity to the expression. In the case of a
tie, we default to the prevailing polarity of the data.
For NEG(1), we first determine the majority polar-
ity vote as above, and then if the expression contains
any function-word negator, flip the polarity of the
majority vote once. NEG(N) is similar to NEG(1), ex-
cept we flip the polarity of the majority vote n times
after the majority vote, where n is the number of
function-word negators in a given expression.
NEGEX(1) and NEGEX(N) are defined similarly as
NEG(1) and NEG(N) above, except both function-
word negators and content-word negators are con-
sidered as negators when flipping the polarity of the
majority vote. See Table 1 for summary. Note that a
word can be both a negator and have a negative prior
polarity. For the purpose of voting, if a word is de-
fined as a negator per the voting scheme, then that
word does not participate in the majority vote.
For brevity, we refer to NEG(1) and NEG(N) col-
lectively as NEG, and NEGEX(1) and NEGEX(N) col-
lectively as NEGEX.
2.2 Compositional semantics
Whereas the heuristics above use voting-based in-
ference, those below employ a set of hand-written
rules motivated by compositional semantics. Table 2
shows the definition of the rules along with moti-
vating examples. In order to apply a rule, we first
detect a syntactic pattern (e.g., [destroyed]V P [the
terrorism]NP ), then apply the Compose function as
defined in Table 2 (e.g., Compose([destroyed], [the
terrorism]) by rule #2).3
3Our implementation uses part-of-speech tags and function-
words to coarsely determine the patterns. An implementation
795
Compose first checks whether the first argument is
a negator, and if so, flips the polarity of the second
argument. Otherwise, Compose resolves the polar-
ities of its two arguments. Note that if the second
argument is a negator, we do not flip the polarity of
the first argument, because the first argument in gen-
eral is not in the semantic scope of the negation.4 In-
stead, we treat the second argument as a constituent
with negative polarity.
We experiment with two variations of the Com-
pose function depending on how conflicting polari-
ties are resolved: COMPOMC uses a Compose func-
tion that defaults to the Majority Class of the po-
larity of the data,5 while COMPOPR uses a Compose
function that selects the polarity of the argument that
has higher semantic PRiority. For brevity, we refer
to COMPOPR and COMPOMC collectively as COMPO.
2.3 Lexicons
The polarity lexicon is initialized with the lexicon
of Wilson et al (2005) and then expanded using the
General Inquirer dictionary.6 In particular, a word
contained in at least two of the following categories
is considered as positive: POSITIV, PSTV, POSAFF,
PLEASUR, VIRTUE, INCREAS, and a word contained
in at least one of the following categories is consid-
ered as negative: NEGATIV, NGTV, NEGAFF, PAIN,
VICE, HOSTILE, FAIL, ENLLOSS, WLBLOSS, TRAN-
LOSS.
For the (function- and content-word) negator lex-
icon, we collect a handful of seed words as well as
General Inquirer words that appear in either NOTLW
or DECREAS category. Then we expand the list of
content-negators using the synonym information of
WordNet (Miller, 1995) to take a simple vote among
senses.
based on parse trees might further improve the performance.
4Moilanen and Pulman (2007) provide more detailed dis-
cussion on the semantic scope of negations and the semantic
priorities in resolving polarities.
5The majority polarity of the data we use for our experi-
ments is negative.
6Available at http://www.wjh.harvard.edu/?inquirer/.
When consulting the General Inquirer dictionary, senses with
less than 5% frequency and senses specific to an idiom are
dropped.
3 Learning-Based Methods
While we expect that a set of hand-written heuristic
rules motivated by compositional semantics can be
effective for determining the polarity of a sentiment-
bearing expression, we do not expect them to be per-
fect. Interpreting natural language is such a com-
plex task that writing a perfect set of rules would
be extremely challenging. Therefore, a more ideal
solution would be a learning-based method that can
exploit ideas from compositional semantics while
providing the flexibility to the rigid application of
the heuristic rules. To this end, we present a novel
learning-based approach that incorporates inference
rules inspired by compositional semantics into the
learning procedure (?3.2). To assess the effect of
compositional semantics in the learning-basedmeth-
ods, we also experiment with a simple classifica-
tion approach that does not incorporate composi-
tional semantics (?3.1). The details of these two
approaches are elaborated in the following subsec-
tions.
3.1 Simple Classification (SC)
Given an expression x consisting of n words x1,
..., xn, the task is to determine the polarity y ?
{positive, negative} of x. In our simple binary
classification approach, x is represented as a vec-
tor of features f(x), and the prediction y is given by
argmaxyw?f(x, y), wherew is a vector of parameters
learned from training data. In our experiment, we
use an online SVM algorithm called MIRA (Margin
Infused Relaxed Algorithm) (Crammer and Singer,
2003)7 for training.
For each x, we encode the following features:
? Lexical: We add every word xi in x, and also
add the lemma of xi produced by the CASS
partial parser toolkit (Abney, 1996).
? Dictionary: In order to mitigate the problem of
unseen words in the test data, we add features
that describeword categories based on theGen-
eral Inquirer dictionary. We add this feature for
each xi that is not a stop word.
? Vote: We experiment with two variations of
voting-related features: for SC-VOTE, we add
7We use the Java implementation of this algorithm
available at http://www.seas.upenn.edu/?strctlrn/StructLearn
/StructLearn.html.
796
Simple Classification Classification with Compositional Inference
y ? argmaxy score(y) Find K best z and denote them as Z = {z(1), ..., z(K)}
l? loss flat(y?, y) s.t. ? i < j, score(z(i)) > score(z(j))
w? update(w, l, y?, y) zbad ? mink z(k) s.t. loss compo(y?, z(k), x) > 0
(if such zbad not found in Z, skip parameter update for this.)
If loss compo(y?, z?, x) > 0
zgood ? mink z(k) s.t. loss compo(y?, z(k), x) = 0
z? ? zgood
(if such zgood not found in Z, stick to the original z?.)
l ? loss compo(y?, zbad, x)? loss compo(y?, z?, x)
w? update(w, l, z?, zbad)
Definitions of score functions and loss functions
score(y) := w ? f(x, y) score(z) :=
?
i score(zi) :=
?
i w ? f(x, zi, i)
loss flat(y?, y) := if (y? = y) 0 else 1 loss compo(y?, z, x) := if (y? = C(x, z)) 0 else 1
Figure 1: Training procedures. y? ? {positive, negative} denotes the true label for a given expression x = x1, ..., xn.
z? denotes the pseudo gold standard for hidden variables z.
a feature that indicates the dominant polarity of
words in the given expression, without consid-
ering the effect of negators. For SC-NEGEX,
we count the number of content-word nega-
tors as well as function-word negators to de-
termine whether the final polarity should be
flipped. Then we add a conjunctive feature that
indicates the dominant polarity together with
whether the final polarity should be flipped. For
brevity, we refer to SC-VOTE and SC-NEGEX
collectively as SC.
Notice that in this simple binary classification set-
ting, it is inherently difficult to capture the compo-
sitional structure among words in x, because f(x, y)
is merely a flat bag of features, and the prediction
is governed simply by the dot product of f(x, y) and
the parameter vector w.
3.2 Classification with Compositional
Inference (CCI)
Next, instead of determining y directly from x,
we introduce hidden variables z = (z1, ..., zn)
as intermediate decision variables, where zi ?
{positive, negative, negator, none}, so that zi
represents whether xi is a word with posi-
tive/negative polarity, or a negator, or none of the
above. For simplicity, we let each intermediate de-
cision variable zi (a) be determined independently
from other intermediate decision variables, and (b)
For each token xi,
if xi is a word in the negator lexicon
then z?i ? negator
else if xi is in the polarity lexicon as negative
then z?i ? negative
else if xi is in the polarity lexicon as positive
then z?i ? positive
else
then z?i ? none
Figure 2: Constructing Soft Gold Standard z?
depend only on the input x, so that zi = argmaxziw ?
f(x, zi, i), where f(x, zi, i) is the feature vector en-
coding around the ith word (described on the next
page). Once we determine the intermediate decision
variables, we apply the heuristic rules motivated by
compositional semantics (from Table 2) in order to
obtain the final polarity y of x. That is, y = C(x, z),
where C is the function that applies the composi-
tional inference, either COMPOPR or COMPOMC.
For training, there are two issues we need to
handle: the first issue is dealing with the hidden
variables z. Because the structure of composi-
tional inference C does not allow dynamic program-
ming, it is intractable to perform exact expectation-
maximization style training that requires enumerat-
ing all possible values of the hidden variables z. In-
stead, we propose a simple and tractable training
797
rule based on the creation of a soft gold standard for
z. In particular, we exploit the fact that in our task,
we can automatically construct a reasonably accu-
rate gold standard for z, denoted as z?: as shown in
Figure 2, we simply rely on the negator and polar-
ity lexicons. Because z? is not always correct, we
allow the training procedure to replace z? with po-
tentially better assignments as learning proceeds: in
the event that the soft gold standard z? leads to an in-
correct prediction, we search for an assignment that
leads to a correct prediction to replace z?. The exact
procedure is given in Figure 1, and will be discussed
again shortly.
Figure 1 shows how we modify the parameter up-
date rule of MIRA (Crammer and Singer, 2003) to
reflect the aspect of compositional inference. In the
event that the soft gold standard z? leads to an incor-
rect prediction, we search for zgood, the assignment
with highest score that leads to a correct prediction,
and replace z? with zgood. In the event of no such
zgood being found among the K-best assignments of
z, we stick with z?.
The second issue is finding the assignment of z
with the highest score(z) = ?i w ? f(x, zi, i) that
leads to an incorrect prediction y = C(x, z). Be-
cause the structure of compositional inference C
does not allow dynamic programming, finding such
an assignment is again intractable. We resort to enu-
merating only over K-best assignments instead. If
none of the K-best assignments of z leads to an in-
correct prediction y, then we skip the training in-
stance for parameter update.
Features. For each xi in x, we encode the follow-
ing features:
? Lexical: We include the current word xi as well
as the lemma of xi produced by CASS partial
parser toolkit (Abney, 1996). We also add a
boolean feature to indicate whether the current
word is a stop word.
? Dictionary: In order to mitigate the problem
with unseen words in the test data, we add fea-
tures that describe word categories based on the
General Inquirer dictionary. We add this fea-
ture for each xi that is not a stop word. We
also add a number of boolean features that pro-
vide following properties of xi using the polar-
ity lexicon and the negator lexicon:
? whether xi is a function-word negator
? whether xi is a content-word negator
? whether xi is a negator of any kind
? the polarity of xi according to Wilson et
al. (2005)?s polarity lexicon
? the polarity of xi according to the lexicon
derived from the General Inquirer dictio-
nary
? conjunction of the above two features
? Vote: We encode the same vote feature that we
use for SC-NEGEX described in ? 3.1.
As in the heuristic-based compositional semantics
approach (? 2.2), we experiment with two variations
of this learning-based approach: CCI-COMPOPR
and CCI-COMPOMC, whose compositional infer-
ence rules are COMPOPR and COMPOMC respec-
tively. For brevity, we refer to both variations col-
lectively as CCI-COMPO.
4 Experiments
The experiments below evaluate our heuristic- and
learning-based methods for subsentential sentiment
analysis (? 4.1). In addition, we explore the role
of context by expanding the boundaries of the
sentiment-bearing expressions (? 4.2).
4.1 Evaluation with given boundaries
For evaluation, we use the Multi-Perspective Ques-
tion Answering (MPQA) corpus (Wiebe et al,
2005), which consists of 535 newswire documents
manually annotated with phrase-level subjectivity
information. We evaluate on all strong (i.e., inten-
sity of expression is ?medium? or higher), sentiment-
bearing (i.e., polarity is ?positive? or ?negative?) ex-
pressions.8 As a result, we can assume the bound-
aries of the expressions are given. Performance is
reported using 10-fold cross-validation on 400 doc-
uments; a separate 135 documents were used as a
development set. Based on pilot experiments on the
development data, we set parameters for MIRA as
follows: slack variable to 0.5, and the number of
incorrect labels (constraints) for each parameter up-
date to 1. The number of iterations (epochs) for
training is set to 1 for simple classification, and to 4
8We discard expressions with confidencemarked as ?uncer-
tain?.
798
Heuristic-Based Learning-Based
VOTE NEG NEG NEG NEG COMPO COMPO SC SC CCI CCI
(1) (N) EX EX MC PR VOTE NEG COMPO COMPO
(1) (N) EX MC PR
86.5 82.0 82.2 87.7 87.7 89.7 89.4 88.5 89.1 90.6 90.7
Table 3: Performance (in accuracy) on MPQA dataset.
Heuristic-Based Learning-Based
VOTE NEG NEG NEG NEG COMPO COMPO SC SC CCI CCI
Data (1) (N) EX EX MC PR VOTE NEG COMPO COMPO
(1) (N) EX MC PR
[-0,+0] 86.5 82.0 82.2 87.7 87.7 89.7 89.4 88.5 89.1 90.6 90.7
[-1,+1] 86.4 81.0 81.2 87.2 87.2 89.3 89.0 88.3 88.4 89.5 89.4
[-5,+5] 85.9 79.0 79.4 85.7 85.6 88.2 88.0 86.4 87.1 88.7 88.7
[-?,+?] 85.3 75.8 76.9 83.9 83.9 87.0 86.9 85.8 85.8 87.3 87.5
Table 4: Performance (in accuracy) on MPQA data set with varying boundaries of expressions.
for classification with compositional inference. We
use K = 20 for classification with compositional
inference.
Results. Performance is reported in Table 3. In-
terestingly, the heuristic-based methods NEG (?
82.2%) that only consider function-word negators
perform even worse than VOTE (86.5%), which does
not consider negators. On the other hand, theNEGEX
methods (87.7%) that do consider content-word
negators as well as function-word negators perform
better than VOTE. This confirms the importance of
content-word negators for determining the polari-
ties of expressions. The heuristic-based methods
motivated by compositional semantics COMPO fur-
ther improve the performance over NEGEX, achiev-
ing up to 89.7% accuracy. In fact, these heuris-
tics perform even better than the SC learning-based
methods (? 89.1%). This shows that heuristics that
take into account the compositional structure of the
expression can perform better than learning-based
methods that do not exploit such structure.
Finally, the learning-based methods that in-
corporate compositional inference CCI-COMPO (?
90.7%) perform better than all of the previous
methods. The difference between CCI-COMPOPR
(90.7%) and SC-NEGEX (89.1%) is statistically sig-
nificant at the .05 level by paired t-test. The dif-
ference between COMPO and any other heuristic that
is not based on computational semantics is also
statistically significant. In addition, the difference
between CCICOMPOPR (learning-based) and COM-
POMC (non-learning-based) is statistically signifi-
cant, as is the difference between NEGEX and VOTE.
4.2 Evaluation with noisy boundaries
One might wonder whether employing additional
context outside the annotated expression boundaries
could further improve the performance. Indeed, con-
ventional wisdom would say that it is necessary to
employ such contextual information (e.g., Wilson et
al. (2005)). In any case, it is important to determine
whether our results will apply to more real-world
settings where human-annotated expression bound-
aries are not available.
To address these questions, we gradually relax
our previous assumption that the exact boundaries of
expressions are given: for each annotation bound-
ary, we expand the boundary by x words for each
direction, up to sentence boundaries, where x ?
{1, 5,?}. We stop expanding the boundary if it
will collide with the boundary of an expression with
a different polarity, so that we can consistently re-
cover the expression-level gold standard for evalua-
tion. This expansion is applied to both the training
and test data, and the performance is reported in Ta-
ble 4. From this experiment, we make the following
observations:
? Expanding the boundaries hurts the perfor-
799
mance for any method. This shows that most of
relevant context for judging the polarity is con-
tained within the expression boundaries, and
motivates the task of finding the boundaries of
opinion expressions.
? The NEGEX methods perform better than VOTE
only when the expression boundaries are rea-
sonably accurate. When the expression bound-
aries are expanded up to sentence boundaries,
they perform worse than VOTE. We conjecture
this is because the scope of negators tends to be
limited to inside of expression boundaries.
? The COMPO methods always perform better
than any other heuristic-based methods. And
their performance does not decrease as steeply
as the NEGEX methods as the expression
boundaries expand. We conjecture this is be-
cause methods based on compositional seman-
tics can handle the scope of negators more ade-
quately.
? Among the learning-based methods, those that
involve compositional inference (CCI-COMPO)
always perform better than those that do not
(SC) for any boundaries. And learning with
compositional inference tend to perform bet-
ter than the rigid application of heuristic rules
(COMPO), although the relative performance
gain decreases once the boundaries are relaxed.
5 Related Work
The task focused on in this paper is similar to that
of Wilson et al (2005) in that the general goal of the
task is to determine the polarity in context at a sub-
sentence level. However, Wilson et al (2005) for-
mulated the task differently by limiting their evalua-
tion to individual words that appear in their polarity
lexicon. Also, their approach was based on a flat bag
of features, and only a few examples of what we call
content-word negators were employed.
Our use of compositional semantics for the task
of polarity classification is preceded by Moilanen
and Pulman (2007), but our work differs in that
we integrate the key idea of compositional seman-
tics into learning-based methods, and that we per-
form empirical comparisons among reasonable al-
ternative approaches. For comparison, we evalu-
ated our approaches on the polarity classification
task from SemEval-07 (Strapparava and Mihalcea,
2007). We achieve 88.6% accuracy with COMPOPR,
90.1% with SCNEGEX, and 87.6% with CCICOM-
POMC.9 There are a number of possible reasons for
our lower performance vs. Moilanen and Pulman
(2007) on this data set. First, SemEval-07 does not
include a training data set for this task, so we use
400 documents from the MPQA corpus instead. In
addition, the SemEval-07 data is very different from
the MPQA data in that (1) the polarity annotation
is given only at the sentence level, (2) the sentences
are shorter, with simpler structure, and not as many
negators as the MPQA sentences, and (3) there are
many more instances with positive polarity than in
the MPQA corpus.
Nairn et al (2006) also employ a ?polarity? prop-
agation algorithm in their approach to the semantic
interpretation of implicatives. However, their notion
of polarity is quite different from that assumed here
and in the literature on sentiment analysis. In partic-
ular, it refers to the degree of ?commitment? of the
author to the truth or falsity of a complement clause
for a textual entailment task.
McDonald et al (2007) use a structured model
to determine the sentence-level polarity and the
document-level polarity simultaneously. But deci-
sions at each sentence level does not consider struc-
tural inference within the sentence.
Among the studies that examined content-word
negators, Niu et al (2005) manually collected a
small set of such words (referred as ?words that
change phases?), but their lexicon was designed
mainly for the medical domain and the type of nega-
tors was rather limited. Wilson et al (2005) also
manually collected a handful of content-word nega-
tors (referred as ?general polarity shifters?), but not
extensively. Moilanen and Pulman (2007) collected
a more extensive set of negators semi-automatically
using WordNet 2.1, but the empirical effect of such
words was not explicitly investigated.
9For lack of space, we only report our performance on in-
stances with strong intensities as defined in Moilanen and Pul-
man (2007), which amounts to only 208 test instances. The
cross-validation set of MPQA contains 4.9k instances.
800
6 Conclusion
In this paper, we consider the task of determining
the polarity of a sentiment-bearing expression, con-
sidering the effect of interactions among words or
constituents in light of compositional semantics. We
presented a novel learning-based approach that in-
corporates structural inference motivated by compo-
sitional semantics into the learning procedure. Our
approach can be considered as a small step toward
bridging the gap between computational semantics
and machine learning methods. Our experimen-
tal results suggest that this direction of research is
promising. Future research includes an approach
that learns the compositional inference rules from
data.
Acknowledgments
This work was supported in part by National Science
Foundation Grants BCS-0624277 and IIS-0535099
and by Department of Homeland Security Grant
N0014-07-1-0152. We also thank Eric Breck, Lil-
lian Lee, Mats Rooth, the members of the Cornell
NLP reading seminar, and the EMNLP reviewers for
insightful comments on the submitted version of the
paper.
References
Steven Abney. 1996. Partial parsing via finite-state
cascades. Journal of Natural Language Engineering,
2(4):337344.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. JMLR
3:951.
David R. Dowty, Robert E. Wall and Stanley Peters.
1981. Introduction to Montague Semantics.
Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWord-
Net: A Publicly Available Lexical Resource for Opin-
ion Mining. In Proceedings of 5th Conference on Lan-
guage Resources and Evaluation (LREC),.
Percy Liang, Hal Daume? III and Dan Klein. 2008. Struc-
ture Compilation: Trading Structure for Features. In
International Conference on Machine Learning.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining (KDD-2004).
Alistair Kennedy and Diana Inkpen. 2005. Senti-
ment Classification of Movie and Product Reviews Us-
ing Contextual Valence Shifters. In Proceedings of
FINEXIN 2005, Workshop on the Analysis of Infor-
mal and Formal Information Exchange during Nego-
tiations.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of COLING.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells and Jeff Reynar. 2007. Structured Models for
Fine-to-Coarse Sentiment Analysis. In Proceedings of
Association for Computational Linguistics (ACL) .
George A. Miller. 1995. WordNet: a lexical database for
English. In Communications of the ACM, 38(11):3941
Richard Montague. 1974. Formal Philosophy; Selected
papers of Richard Montague. Yale University Press.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007).
Rowan Nairn, Cleo Condoravdi and Lauri Karttunen
2006. Computing relative polarity for textual infer-
ence. In Inference in Computational Semantics (ICoS-
5).
Yun Niu, Xiaodan Zhu, Jianhua Li and Graeme Hirst.
2005. Analysis of polarity information inmedical text.
In Proceedings of the American Medical Informatics
Association 2005 Annual Symposium (AMIA).
Livia Polanyi and Annie Zaenen. 2004. Contextual lex-
ical valence shifters. In Exploring Attitude and Affect
in Text: Theories and Applications: Papers from the
2004 Spring Symposium, AAAI.
Mostafa Shaikh, Helmut Prendinger and Mitsuru
Ishizuka. 2007. Assessing sentiment of text by se-
mantic dependency and contextual valence analysis.
In Proc 2nd Int?l Conf on Affective Computing and In-
telligent Interaction (ACII?07).
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of Se-
mEval.
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. In Language Resources and Evalua-
tion (formerly Computers and the Humanities), 39(2-
3):165210.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP.
801
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 590?598,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Adapting a Polarity Lexicon using Integer Linear Programming
for Domain-Specific Sentiment Classification
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Abstract
Polarity lexicons have been a valuable re-
source for sentiment analysis and opinion
mining. There are a number of such lexi-
cal resources available, but it is often sub-
optimal to use them as is, because general
purpose lexical resources do not reflect
domain-specific lexical usage. In this pa-
per, we propose a novel method based on
integer linear programming that can adapt
an existing lexicon into a new one to re-
flect the characteristics of the data more
directly. In particular, our method collec-
tively considers the relations among words
and opinion expressions to derive the most
likely polarity of each lexical item (posi-
tive, neutral, negative, or negator) for the
given domain. Experimental results show
that our lexicon adaptation technique im-
proves the performance of fine-grained po-
larity classification.
1 Introduction
Polarity lexicons have been a valuable resource for
sentiment analysis and opinionmining. In particu-
lar, they have been an essential ingredient for fine-
grained sentiment analysis (e.g., Kim and Hovy
(2004), Kennedy and Inkpen (2005), Wilson et al
(2005)). Even though the polarity lexicon plays an
important role (Section 3.1), it has received rela-
tively less attention in previous research. In most
cases, polarity lexicon construction is discussed
only briefly as a preprocessing step for a sentiment
analysis task (e.g., Hu and Liu (2004), Moilanen
and Pulman (2007)), but the effect of different al-
ternative polarity lexicons is not explicitly inves-
tigated. Conversely, research efforts that focus
on constructing a general purpose polarity lexicon
(e.g., Takamura et al (2005), Andreevskaia and
Bergler (2006), Esuli and Sebastiani (2006), Rao
and Ravichandran (2009)) generally evaluate the
lexicon in isolation from any potentially relevant
NLP task, and it is unclear how the new lexicon
might affect end-to-end performance of a concrete
NLP application.
It might even be unrealistic to expect that there
can be a general-purpose lexical resource that
can be effective across all relevant NLP applica-
tions, as general-purpose lexicons will not reflect
domain-specific lexical usage. Indeed, Blitzer
et al (2007) note that the polarity of a particu-
lar word can carry opposite sentiment depending
on the domain (e.g., Andreevskaia and Bergler
(2008)).
In this paper, we propose a novel method based
on integer linear programming to adapt an existing
polarity lexicon into a new one to reflect the char-
acteristics of the data more directly. In particular,
our method considers the relations among words
and opinion expressions collectively to derive the
most likely polarity of each word for the given do-
main.
Figure 1 depicts the key insight of our approach
using a bipartite graph. On the left hand side, each
node represents a word, and on the right hand side,
each node represents an opinion expression. There
is an edge between a word wi and an opinion ex-
pression ej , if the word wi appears in the expres-
sion ej . We assume the possible polarity of each
expression is one of the following three values:
{positive, neutral, negative}, while the possible
polarity of each word is one of: {positive, neutral,
negative or negator}. Strictly speaking, negator is
not a value for polarity, but we include them in our
lexicon, because valence shifters or negators have
been shown to play an important role for sentiment
analysis (e.g., Polanyi and Zaenen (2004), Moila-
nen and Pulman (2007), Choi and Cardie (2008)).
Typically, the ultimate goal of the sentiment
analysis task is to determine the expression-level
(or sentiment/ document-level) polarities, rather
590
than the correct word-level polarities with respect
to the domain. Therefore, word-level polarities
can be considered as latent information. In this pa-
per, we show how we can improve the word-level
polarities of a general-purpose polarity lexicon by
utilizing the expression-level polarities, and in re-
turn, how the adapted word-level polarities can
improve the expression-level polarities.
In Figure 1, there are two types of relations
we could exploit when adapting a general-purpose
polarity lexicon into a domain-specific one. The
first are word-to-word relations within each ex-
pression. That is, if we are not sure about the
polarity of a certain word, we can still make a
guess based on the polarities of other words within
the same expression and knowledge of the polar-
ity of the expression. The second type of relations
are word-to-expression relations: e.g., some words
appear in expressions that take on a variety of po-
larities, while other words are associated with ex-
pressions of one polarity class or another.
In relation to previous research, analyz-
ing word-to-word (intra-expression) relations
is most related to techniques that determine
expression-level polarity in context (e.g., Wilson
et al (2005)), while exploring word-to-expression
(inter-expression) relations has connections to
techniques that employ more of a global-view of
corpus statistics (e.g., Kanayama and Nasukawa
(2006)).1
While most previous research exploits only one
or the other type of relation, we propose a unified
method that can exploit both types of semantic re-
lation, while adapting a general purpose polarity
lexicon into a domain specific one. We formulate
our lexicon adaptation task using integer linear
programming (ILP), which has been shown to be
very effective when solving problems with com-
plex constraints (e.g., Roth and Yih (2004), Denis
and Baldridge (2007)). And the word-to-word and
word-to-expression relations discussed above can
be encoded as soft and hard constraints in ILP. Un-
fortunately, one class of constraint that we would
like to encode (see Section 2) will require an
exponentially many number of constraints when
grounded into an actual ILP problem. We there-
fore propose an approximation scheme to make
the problem more practically solvable.
We evaluate the effect of the adapted lex-
1In case of document-level polarity classification, word-
to-expression relations correspond to word-to-document re-
lations.
exp 
exp
exp
expw w w 
w

w

w  w
w w
w
+
?
w
w
w
w 
w
=
+
?
?
=
?
Figure 1: The relations among words and expres-
sions. + indicates positive, - indicates negative, =
indicates neutral, and ? indicates a negator.
icon in the context of a concrete NLP task:
expression-level polarity classification. Experi-
mental results show that our lexicon adaptation
technique improves the accuracy of two com-
petitive expression-level polarity classifiers from
64.2% - 70.4% to 67.0% - 71.2%..
2 An Integer Linear Programming
Approach
In this section, we describe how we formulate the
lexicon adaptation task using integer linear pro-
gramming. Before we begin, we assume that we
have a general-purpose polarity lexicon L, and a
polarity classification algorithm f(el,L), that can
determine the polarity of the opinion expression el
based on the words in el and the initial lexicon L.
The polarity classification algorithm f(?) can be
either a heuristic-based one, or a machine-learning
based one ? we consider it as a black box for now.
Constraints for word-level polarities: For
each word xi, we define four binary variables:
x+i , x=i , x
?
i , x?i to represent positive, neutral, neg-
ative polarity, and negators respectively. If x?i = 1
for some ? ? {+,=,?,?}, then the word xi has
the polarity ?. The following inequality constraint
states that at least one polarity value must be cho-
sen for each word.
x+i + x=i + x?i + x?i >= 1 (1)
If we allow only one polarity per word, then the
above inequality constraint should be modified as
an equality constraint. Although most words tend
to associate with a single polarity, some can take
on more than one polarity. In order to capture this
observation, we introduce an auxiliary binary vari-
able ?i for each word xi. Then the next inequality
591
constraint states that at most two polarities can be
chosen for each word.
x+i + x=i + x?i + x?i <= 1 + ?i (2)
Next we introduce the initial part of our objec-
tive function.
maximize
?
i
(
w+i x
+
i + w=i x=i
+ w?i x?i + w?i x?i
? w??i
)
+ ? ? ? (3)
For the auxiliary variable ?i, we apply a con-
stant weight w? to discourage ILP from choosing
more than one polarity for each word. We can al-
low more than two polarities for each word, by
adding extra auxiliary variables and weights. For
each variable x?i , we define its weight w?i , which
indicates how likely it is that word xi carries the
polarity ?. We define the value of w?i using two
different types of information as follows:
w?i := Lw?i + Cw?i
where Lw?i is the degree of polarity ? for word xi
determined by the general-purpose polarity lexi-
con L, and Cw?i is the degree of polarity ? deter-
mined by the corpus statistics as follows:2
Cw?i :=
# of xi in expressions with polarity ?
# of xi in the corpus C
Note that the occurrence of word xi in an ex-
pression ej with a polarity ? does not necessar-
ily mean that the polarity of xi should also be
?, as the interpretation of the polarity of an ex-
pression is more than just a linear sum of the
word-level polarities (e.g., Moilanen and Pulman
(2007)). Nonetheless, not all expressions require
a complicated inference procedure to determine
their polarity. Therefore, Cw?i still provides useful
information about the likely polarity of each word
based on the corpus statistics.
From the perspective of Chomskyan linguistics,
the weights Lw?i based on the prior polarity from
the lexicon can be considered as having a ?com-
petence? component , while Cw?i derived from
the corpus counts can be considered as a ?perfor-
mance? component (Noam Chomsky (1965)).
2If a word xi is in an expression that is not an opinion,
then we count it as an occurrence with neutral polarity.
Constraints for content-word negators: Next
we describe a constraint that exploits knowledge
of the typical distribution of content-word nega-
tors in natural language. Content-word negators
are words that are not function words, but act se-
mantically as negators (Choi and Cardie, 2008).3
Although it is possible to artificially construct a
very convoluted sentence with lots of negations, it
is unlikely for multiple layers of negations to ap-
pear very often in natural language (Pickett et al
(1996)). Therefore, we allow at most one content-
word negator for each expression el. Because we
do not restrict the number of function-word nega-
tors, our constraint still gives room for multiple
layers of negations.
?
i??(el)
x?i <= 1 (4)
In the above constraint, ?(el) indicates the set
of indices of content words appearing in el . For
instance, if i ? ?(el), then xi appears in el. This
constraint can be polished further to accommodate
longer expressions where multiple content-word
negators are more likely to appear, by adding a
separate constraint with a sliding window.
Constraints for expression-level polarities:
Before we begin, we introduce pi(el) that will be
used often in the remaining section. For each ex-
pression el, we define pi(el) to be the set of con-
tent words appearing in el, together with the most
likely polarity proposed by a general-purpose po-
larity lexicon L. For instance, if x+i ? pi(el), then
the polarity of word xi is + according to L.
Next we encode constraints that consider
expression-level polarities. If the polarity classifi-
cation algorithm f(el,L) makes an incorrect pre-
diction for el using the original lexicon L, then we
need to encourage ILP to fix the error by suggest-
ing different word-level polarities. We capture this
idea by the following constraint:
?
x?i?pi(el)
x?i <= |pi(el)| ? 1 + ?l (5)
The auxiliary binary variable ?l is introduced
for each el so that the assignment pi(el) does not
have to be changed if paying for the cost w? in the
objective function. (See equation (10).) That is,
suppose the ILP solver assigns ?1? to all variables
3Examples of content-word negators are destroy, elimi-
nate, prevent etc.
592
in ?(el), (which corresponds to keeping the orig-
inal lexicon as it is for all words in the given ex-
pression el), then the auxiliary variable ?l must be
also set as ?1? in order to satisfy the constraint (5).
Because ?l is associated with a negative weight
in the objective function, doing so will act against
maximizing the objective function. This way, we
discourage the ILP solver to preserve the original
lexicon as it is.
To verify the constraint (5) further, suppose that
the ILP solver assigns ?1? for all variables in ?(el)
except for one variable. (Notice that doing so cor-
responds to proposing a new polarity for one of
the words in the given expression el.) Then the
constraint (5) will hold regardless of whether the
ILP solver assigns ?0? or ?1? to ?l. Because ?l is
associated with a negative weight in the objective
function, the ILP solver will then assign ?0? to ?l to
maximize the objective function. In other words,
we encourage the ILP solver to modify the original
lexicon for the given expression el .
We use this type of soft constraint in order to
cope with the following two noise factors: first, it
is possible that some annotations are noisy. Sec-
ond, f(el,L) is not perfect, and might not be able
to make a correct prediction even with the correct
word-level polarities.
Next we encode a constraint that is the oppo-
site of the previous one. That is, if the polarity
classification algorithm f(el,L) makes a correct
prediction on el using the original lexicon L, then
we encourage ILP to keep the original word-level
polarities for words in el.
?
x?i?pi(el)
x?i >= |pi(el)| ? |pi(el)|?l (6)
Interpretation of constraint (6) with the auxil-
iary binary variable ?l is similar to that of con-
straint (5) elaborated above.
Notice that in equation (5), we encouraged ILP
to fix the current lexicon L for words in el, but
we have not specified the consequence of a mod-
ified lexicon (L?) in terms of expression-level po-
larity classification f(el,L?). Certain changes to
L might not fix the prediction error for el, and
those might even cause extra incorrect predictions
for other expressions. Then it would seem that we
need to replicate constraints (5) & (6) for all per-
mutations of word-level polarities. However, do-
ing so would incur exponentially many number of
constraints (4|el|) for each expression.4
To make the problem more practically solv-
able, we only consider changes to the lexicon that
are within edit-one distance with respect to pi(el).
More formally, let us define pi?(el) to be the set of
content words appearing in el, together with the
most likely polarity proposed by a modified polar-
ity lexicon L?. Then we need to consider all pi?(el)
such that |pi?(el)? pi(el)| = |pi(el)| ? 1. There are
(4?1)|el| number of different pi?(el), and we index
them as pi?k(el). We then add following constraints
similarly as equation (5) & (6):
?
x?i?pi?k(el)
x?i <= |pi?k(el)| ? 1 + ?(l,k) (7)
if the polarity classification algorithm f(?) makes
an incorrect prediction based on pi?k(el). And,
?
x?i?pi?k(el)
x?i >= |pi?k(el)| ? |pi?k(el)|?(l,k) (8)
if the polarity classification algorithm f(?) makes
a correct prediction based on pi?k(el). Remember
that none of the constraints (5) - (8) enforces as-
signment pi(el) or pi?k(el) as a hard constraint. In
order to enforce at least one of them to be chosen,
we add the following constraint:
?
x?i?pi(el)
x?i >= |pi(el)| ? 1 (9)
This constraint ensures that the modified lexi-
con L? is not drastically different from L. Assum-
ing that the initial lexicon L is a reasonably good
one, constraining the search space for L? will reg-
ulate that L? does not turn into a degenerative one
that overfits to the current corpus C.
Objective function: Finally, we introduce our
full objective function.
4For certain simple polarity classification algorithm
f(el,L), it is possible to write polynomially many number of
constraints. However our approach intends to be more gen-
eral by treating f(el,L) as a black box, so that algorithms
that do not factor nicely can also be considered as an option.
593
maximize
?
i
(
w+i x
+
i + w=i x=i
+ w?i x?i + w?i x?i
? w??i
)
?
?
l
w??l?l
?
?
l,k
w??(l,k)?(l,k) (10)
We have already described the first part of the
objective function (equation (3)), thus we only de-
scribe the last two terms here. w? is defined simi-
larly as w?; it is a constant weight that applies for
any auxiliary binary variable ?l and ?(l,k).
We further define ?l and ?(l,k) as secondary
weights, or amplifiers to adjust the constant weight
w?. To enlighten the motivation behind the am-
plifiers ?l and ?(l,k), we bring out the following
observations:
1. Among the incorrect predictions for
expression-level polarity classification,
some are more incorrect than the other.
For instance, classifying positive class to
negative class is more wrong than classifying
positive class to neutral class. Therefore, the
cost of not fixing very incorrect predictions
should be higher than the cost of not fixing
less incorrect predictions. (See [R2] and
[R3] in Table 1.)
2. If the current assignment pi(el) for expression
el yields a correct prediction using the classi-
fier y(el,L), then there is not much point in
changingL toL?, even if y(el,L?) also yields
a correct prediction. In this case, we would
like to assign slightly higher confidence in the
original lexicon L then the new one L?. (See
[R1] in Table 1.)
3. Likewise, if the current assignment pi(el) for
expression el yields an incorrect prediction
using the classifier y(el,L), then there is not
much point in changing L to L?, if y(el,L?)
also yields an equally incorrect prediction.
Again we assign slightly higher confidence in
the original lexicon L than the new one L? in
such cases. (Compare each row in [R2] with
a corresponding row in [R3] in Table 1.)
[R1] If pi(el) correct ?l ? 1.5
If pi?k(el) correct ?(l,k) ? 1.0
[R2] If pi(el) very incorrect ?l ? 1.0
If pi(el) less incorrect ?l ? 0.5
[R3] If pi?k(el) very incorrect ?(l,k) ? 1.5
If pi?k(el) less incorrect ?(l,k) ? 1.0
Table 1: The value of amplifiers ?l and ?(l,k).
To summarize, for correct predictions, the de-
gree of ? determines the degree of cost of (unde-
sirably) altering the current lexicon for el. For in-
correct predictions, the degree of ? determines the
degree of cost of not fixing the current lexicon for
el.
3 Experiments
In the experiment section, we seek for answers for
the following questions:
Q1 What is the effect of a polarity lexicon on the
expression-level polarity classification task?
In particular, is it useful when using a ma-
chine learning technique that might be able to
learn the necessary polarity information just
based on the words in the training data, with-
out consulting a dictionary? (Section 3.1)
Q2 What is the effect of an adapted polarity lex-
icon on the expression-level polarity classifi-
cation task? (Section 3.2)
Notice that we include the neutral polarity in the
polarity classification. It makes our task much
harder (e.g., Wilson et al (2009)) than those that
assume inputs are guaranteed to be either strongly
positive or negative (e.g., Pang et al (2002), Choi
and Cardie (2008)). But in practice, one can-
not expect that a given input is strongly polar, as
automatically extracted opinions are bound to be
noisy. Furthermore, Wiebe et al (2005) discuss
that some opinion expressions do carry a neutral
polarity.
We experiment with the Multi-Perspective
Question Answering (MPQA) corpus (Wiebe et
al., 2005) for evaluation. It contains 535 newswire
documents annotated with phrase-level subjectiv-
ity information. We evaluate on all opinion ex-
pressions that are known to have high level of
inter-annotator agreement. That is, we include
opinions with intensity marked as ?medium? or
594
higher, and exclude those with annotation confi-
dence marked as ?uncertain?. To focus our study
on the direct influence of the polarity lexicon upon
the sentiment classification task, we assume the
boundaries of the expressions are given. How-
ever, our approach can be readily used in tan-
dem with a system that extracts opinion expres-
sions (e.g., Kim and Hovy (2005), Breck et al
(2007)). Performance is reported using 10-fold
cross-validation on 400 documents, and a separate
135 documents were used as a development set.
For the general-purpose polarity lexicon, we ex-
pand the polarity lexicon of Wilson et al (2005)
with General Inquirer dictionary as suggested by
Choi and Cardie (2008).
We report the performance in twomeasures: ac-
curacy for 3-way classification, and average error
distance. The reason why we consider average er-
ror distance is because classifying a positive class
into a negative class is worse than classifying a
positive class into a neutral one. We define the er-
ror distance between ?neutral? class and any other
class as 1, while the error distance between ?posi-
tive? class and ?negative? class as 2. If a predicted
polarity is correct, then the error distance is 0. We
compute the error distance of each prediction and
take the average over all predictions in the test
data.
3.1 Experiment-I: Effect of a Polarity
Lexicon
To verify the effect of a polarity lexicon on the
expression-level polarity classification task, we
experiment with simple classification-based ma-
chine learning technique. We use the Mallet
(McCallum, 2002) implementation of Conditional
Random Fields (CRFs) (Lafferty et al, 2001).5 To
highlight the influence of a polarity lexicon, we
compare the performance of CRFs with and with-
out features derived from polarity lexicons.
Features: We encode basic features as words
and lemmas for all content words in the given ex-
pression. The performance of CRFs using only the
basic features are given in the first row of the Ta-
ble 2. Next we encode features derived from po-
larity lexicons as follows.
? The output of Vote & Flip algorithm. (Sec-
tion 3.2 & Figure 2.)
5We use the CRF implementation of Mallet (McCallum,
2002) with Markov-order 0, which is equivalent to Maximum
Entropy models (Berger et al (1996)).
Accuracy Avg. Error Distance
Without Lexicon 63.9 0.440
With Lexicon 70.4 0.334
Table 2: Effect of a polarity lexicon on expression-
level classification using CRFs
? Number of positive, neutral, negative, and
negators in the given expression.
? Number of positive (or negative) words in
conjunction with number of negators.
? (boolean) Whether the number of positive
words dominates negative ones.
? (boolean) Whether the number of negative
words dominates positive ones.
? (boolean) None of the above two cases
? Each of the above three boolean values in
conjunction with the number of negators.
Results: Table 2 shows the performance of
CRFs with and without features that consult the
general-purpose lexicon. As expected, CRFs can
perform reasonably well (accuracy = 63.9%) even
without consulting the dictionary, by learning di-
rectly from the data. However, having the polarity
lexicon boosts the performance significantly (ac-
curacy = 70.4%), demonstrating that lexical re-
sources are very helpful for fine-grained sentiment
analysis. The difference in performance is statisti-
cally significant by paired t-test for both accuracy
(p < 0.01) and average error distance (p < 0.01).
3.2 Experiment-II: Adapting a Polarity
Lexicon
In this section, we assess the quality of the adapted
lexicon in the context of an expression-level polar-
ity classification task. In order to perform the lex-
icon adaptation via ILP, we need an expression-
level polarity classification algorithm f(el,L) as
described in Section 2. According to Choi and
Cardie (2008), voting algorithms that recognize
content-word negators achieve a competitive per-
formance, so we will use a variant of it for sim-
plicity. Because none of the algorithms proposed
by Choi and Cardie (2008) is designed to handle
the neutral polarity, we invent our own version as
shown in Figure 2.
595
For each expression ei,
nPositive? # of positive words in ei
nNeutral ? # of neutral words in ei
nNegative? # of negative words in ei
nNegator ? # of negating words in ei
if (nNegator % 2 = 0)
then fF lipPolarity ? false
else
then fF lipPolarity ? true
if (nPositive > nNegative) & ? fF lipPolarity
then Polarity(ei)? positive
else if (nPositive > nNegative) & fF lipPolarity
then Polarity(ei)? negative
else if (nPositive < nNegative) & ? fF lipPolarity
then Polarity(ei)? negative
else if (nPositive < nNegative) & fF lipPolarity
then Polarity(ei)? neutral
else if nNeutral > 0
then Polarity(ei)? neutral
else
then Polarity(ei)? default polarity (the most
prominent polarity in the corpus)
Figure 2: Vote & Flip Algorithm
It might look a bit complex at first glance,
but the intuition is simple. The variable
fFlipPolarity determines whether we need to
flip the overall majority polarity based on the num-
ber of negators in the given expression. If the
positive (or negative) polarity words dominate the
given expression, and if there is no need to flip
the majority polarity, then we take the positive (or
negative) polarity as the overall polarity. If the
positive (or negative) polarity words dominate the
given expression, and if we need to flip the major-
ity polarity, then we take the negative (or neutral)
polarity as the overall polarity.
Notice that the result of flipping the negative po-
larity is neutral, not positive. In our pilot study, we
found that this strategy works better than flipping
the negative polarity to positive.6 Finally, if the
number of positive words and the negative words
tie, and there is any neutral word, then we assign
the neutral polarity. In this case, we don?t worry if
6This finding is not surprising. For instance, if we con-
sider the polarity of ?She did not get hurt much from the ac-
cident.?, it can be viewed as neutral; although it is good that
one did not hurt much, it is still bad that there was an acci-
dent. Hence it gives a mixed feeling, which corresponds to
the neutral polarity.
there is a negator, because flipping a neutral polar-
ity would still result in a neutral polarity. If none of
above condition is met, than we default to the most
prominent polarity of the data, which is the nega-
tive polarity in the MPQA corpus. We name this
simple algorithm as Vote & Flip algorithm. The
performance is shown in the first row in Table 2.
Next we describe the implementation part of the
ILP. For 10 fold-cross validation, we formulate the
ILP problem using the training data (360 docu-
ments), and then test the effect of the adapted lex-
icon on the remaining 40 documents. We include
only those content words that appeared more than
3 times in the training data. From the pilot test us-
ing the development set, we picked the value of
w? as 0.1. We found that having the auxiliary
variables ?l which allow more than one polarity
per word does not necessarily help with the per-
formance, so we omitted them. We suspect it is
because the polarity classifiers we experimented
with is not highly capable of disambiguating dif-
ferent lexical usages and select the right polarity
for a given context. We use CPLEX integer pro-
gramming solver to solve our ILP problems. On a
machine with 4GHz CPU, it took several minutes
to solve each ILP problem.
In order to assess the effect of the adapted lex-
icon using CRFs, we need to first train the CRFs
model. Using the same training set used for the
lexicon adaptation would be suboptimal, because
the features generated from the adapted lexicon
will be unrealistically good in that particular data.
Therefore, we prepared a separate training data for
CRFs using 135 documents from the development
set.
Results: Table 3 shows the comparison of the
original lexicon and the adapted lexicon in terms
of polarity classification performance using the
Vote & Flip algorithm. The adapted lexicon im-
proves the accuracy as well as reducing the aver-
age error distance. The difference in performance
is statistically significant by paired t-test for both
accuracy (p < 0.01) and average error distance
(p < 0.01).
Table 4 shows the comparison of the original
lexicon and the adapted lexicon using CRFs. The
improvement is not as substantial as that of Vote &
Flip algorithm but the difference in performance is
also statistically significant for both accuracy (p =
0.03) and average error distance (p = 0.04).
596
Accuracy Avg. Error Distance
Original Lexicon 64.2 0.395
Adapted Lexicon 67.0 0.365
Table 3: Effect of an adapted polarity lexicon on
expression-level classification using the Vote &
Flip Algorithm
Accuracy Avg. Error Distance
Original Lexicon 70.4 0.334
Adapted Lexicon 71.2 0.327
Table 4: Effect of an adapted polarity lexicon on
expression-level classification using CRFs
4 Related Work
There are a number of previous work that focus
on building polarity lexicons (e.g., Takamura et
al. (2005), Kaji and Kitsuregawa (2007), Rao and
Ravichandran (2009)). But most of them evalu-
ated their lexicon in isolation from any potentially
relevant NLP task, and it is unclear how the new
lexicon might affect end-to-end performance of a
concrete NLP application. Our work differs in that
we try to draw a bridge between general purpose
lexical resources and a domain-specific NLP ap-
plication.
Kim and Hovy (2005) and Banea et al (2008)
present bootstrapping methods to construct a sub-
jectivity lexicon and measure the effect of the new
lexicon for sentence-level subjectivity classifica-
tion. However, their lexicons only tell whether a
word is a subjective one, but not the polarity of the
sentiment. Furthermore, the construction of lexi-
con is still an isolated step from the classification
task. Our work on the other hand allows the classi-
fication task to directly influence the construction
of lexicon, enabling the lexicon to be adapted for
a concrete NLP application and for a specific do-
main.
Wilson et al (2005) pioneered the expression-
level polarity classification task using the MPQA
corpus. The experimental results are not directly
comparable to ours, because Wilson et al (2005)
limit the evaluation only for the words that ap-
peared in their polarity lexicon. Choi and Cardie
(2008) also focus on the expression-level polarity
classification, but their evaluation setting is not as
practical as ours in that they assume the inputs are
guaranteed to be either strongly positive or nega-
tive.
5 Conclusion
In this paper, we present a novel lexicon adapta-
tion technique based on integer linear program-
ming to reflect the characteristics of the domain
more directly. In particular, our method collec-
tively considers the relations among words and
opinion expressions to derive the most likely po-
larity of each lexical item for the given domain.
We evaluate the effect of our lexicon adaptation
technique in the context of a concrete NLP ap-
plication: expression-level polarity classification.
The positive results from our experiments encour-
age further research for lexical resource adaptation
techniques.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant BCS-0624277 and by the
Department of Homeland Security under ONR
Grant N0014-07-1-0152. We also thank the
EMNLP reviewers for insightful comments.
References
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Over-
coming Domain Dependence in Sentiment Tagging.
ACL
Alina Andreevskaia and Sabine Bergler. 2006. Min-
ing WordNet For a Fuzzy Sentiment: Sentiment Tag
Extraction From WordNet Glosses. EACL
Carmen Banea, Rada Mihalcea, and JanyceWiebe.
2008. A Bootstrapping Method for Building Sub-
jectivity Lexicons for Languages with Scarce Re-
sources. LREC
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. In Computational Lin-
guistics, 22(1)
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes, and
Blenders: Domain Adaptation for Sentiment Classi-
fication. Association for Computational Linguistics
- ACL 2007
Eric Breck, Yejin Choi and Claire Cardie. 2007. Iden-
tifyingExpressions of Opinion in Context. In IJCAI.
Yejin Choi and Claire Cardie. 2008. Learning with
Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis. EMNLP
Noam Chomsky. 1965. Aspects of the theory of syn-
tax. Cambridge, MA: MIT Press.
597
Pascal Denis and Jason Baldridge. 2007. Joint deter-
mination of anaphoricity and coreference resolution
using integer programming. NAACL
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A Publicly Available Lexical Resource
for Opinion Mining. In Proceedings of 5th Con-
ference on Language Resources and Evaluation
(LREC),.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining (KDD-2004).
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing Lexicon for Sentiment Analysis from Massive
Collection of HTML Documents. In EMNLP-
CoNLL.
Hiroshi Kanayama Tetsuya Nasukawa. 2006. Fully
Automatic Lexicon Expansion for Domain-oriented
Sentiment Analysis. In ACL.
Alistair Kennedy and Diana Inkpen. 2005. Sentiment
Classification of Movie and Product Reviews Us-
ing Contextual Valence Shifters. In Proceedings of
FINEXIN 2005, Workshop on the Analysis of Infor-
mal and Formal Information Exchange during Ne-
gotiations.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of COL-
ING.
Soo-Min Kim and Eduard Hovy. 2005. Automatic De-
tection of Opinion Bearing Words and Sentences. In
Companion Volume to the Proceedings of the Sec-
ond International Joint Conference on Natural Lan-
guage Processing (IJCNLP-05)
John Lafferty, Andrew Kachites McCallum and Fer-
nando Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007).
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In EMNLP.
Joseph Pickett et al 1996. The American heritage
book of English usage: A practical and authoritative
guide to contemporary English. Houghton Mifflin
Company.
Livia Polanyi and Annie Zaenen. 2004. Contextual
lexical valence shifters. In Exploring Attitude and
Affect in Text: Theories and Applications: Papers
from the 2004 Spring Symposium, AAAI.
Delip Rao and Deepak Ravichandran. 2009. Semi-
Supervised Polarity Lexicon Induction. In EACL.
Dan Roth and Wen-tau Yih. 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. In CoNLL.
Hiroya Takamura, Takashi Inui, andManabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In ACL.
Janyce Wiebe, Theresa Wilson and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. In LanguageResources and Eval-
uation (formerly Computers and the Humanities),
39(2-3):165210.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of
HLT/EMNLP.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: an explo-
ration of features for phrase-level sentiment analy-
sis. In Computational Linguistics 35(3).
598
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 355?362, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Identifying Sources of Opinions with Conditional Random Fields and
Extraction Patterns
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Ellen Riloff and Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,sidd}@cs.utah.edu
Abstract
Recent systems have been developed for
sentiment classification, opinion recogni-
tion, and opinion analysis (e.g., detect-
ing polarity and strength). We pursue an-
other aspect of opinion analysis: identi-
fying the sources of opinions, emotions,
and sentiments. We view this problem as
an information extraction task and adopt
a hybrid approach that combines Con-
ditional Random Fields (Lafferty et al,
2001) and a variation of AutoSlog (Riloff,
1996a). While CRFs model source iden-
tification as a sequence tagging task, Au-
toSlog learns extraction patterns. Our re-
sults show that the combination of these
two methods performs better than either
one alone. The resulting system identifies
opinion sources with 79.3% precision and
59.5% recall using a head noun matching
measure, and 81.2% precision and 60.6%
recall using an overlap measure.
1 Introduction
In recent years, there has been a great deal of in-
terest in methods for automatically identifying opin-
ions, emotions, and sentiments in text. Much of
this research explores sentiment classification, a text
categorization task in which the goal is to classify
a document as having positive or negative polar-
ity (e.g., Das and Chen (2001), Pang et al (2002),
Turney (2002), Dave et al (2003), Pang and Lee
(2004)). Other research efforts analyze opinion ex-
pressions at the sentence level or below to recog-
nize opinions, their polarity, and their strength (e.g.,
Dave et al (2003), Pang and Lee (2004), Wilson et
al. (2004), Yu and Hatzivassiloglou (2003), Wiebe
and Riloff (2005)). Many applications could ben-
efit from these opinion analyzers, including prod-
uct reputation tracking (e.g., Morinaga et al (2002),
Yi et al (2003)), opinion-oriented summarization
(e.g., Cardie et al (2004)), and question answering
(e.g., Bethard et al (2004), Yu and Hatzivassiloglou
(2003)).
We focus here on another aspect of opinion
analysis: automatically identifying the sources of
the opinions. Identifying opinion sources will
be especially critical for opinion-oriented question-
answering systems (e.g., systems that answer ques-
tions of the form ?How does [X] feel about [Y]??)
and opinion-oriented summarization systems, both
of which need to distinguish the opinions of one
source from those of another.1
The goal of our research is to identify direct and
indirect sources of opinions, emotions, sentiments,
and other private states that are expressed in text.
To illustrate the nature of this problem, consider the
examples below:
S1: Taiwan-born voters favoring independence...
1In related work, we investigate methods to identify the
opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and
Riloff (2005), Wilson et al (2005)) and the nesting structure
of sources (e.g., Breck and Cardie (2004)). The target of each
opinion, i.e., what the opinion is directed towards, is currently
being annotated manually for our corpus.
355
S2: According to the report, the human rights
record in China is horrendous.
S3: International officers believe that the EU will
prevail.
S4: International officers said US officials want the
EU to prevail.
In S1, the phrase ?Taiwan-born voters? is the di-
rect (i.e., first-hand) source of the ?favoring? sen-
timent. In S2, ?the report? is the direct source of
the opinion about China?s human rights record. In
S3, ?International officers? are the direct source of
an opinion regarding the EU. The same phrase in
S4, however, denotes an indirect (i.e., second-hand,
third-hand, etc.) source of an opinion whose direct
source is ?US officials?.
In this paper, we view source identification as an
information extraction task and tackle the problem
using sequence tagging and pattern matching tech-
niques simultaneously. Using syntactic, semantic,
and orthographic lexical features, dependency parse
features, and opinion recognition features, we train a
linear-chain Conditional Random Field (CRF) (Laf-
ferty et al, 2001) to identify opinion sources. In ad-
dition, we employ features based on automatically
learned extraction patterns and perform feature in-
duction on the CRF model.
We evaluate our hybrid approach using the NRRC
corpus (Wiebe et al, 2005), which is manually
annotated with direct and indirect opinion source
information. Experimental results show that the
CRF model performs well, and that both the extrac-
tion patterns and feature induction produce perfor-
mance gains. The resulting system identifies opinion
sources with 79.3% precision and 59.5% recall us-
ing a head noun matching measure, and 81.2% pre-
cision and 60.6% recall using an overlap measure.
2 The Big Picture
The goal of information extraction (IE) systems is
to extract information about events, including the
participants of the events. This task goes beyond
Named Entity recognition (e.g., Bikel et al (1997))
because it requires the recognition of role relation-
ships. For example, an IE system that extracts in-
formation about corporate acquisitions must distin-
guish between the company that is doing the acquir-
ing and the company that is being acquired. Sim-
ilarly, an IE system that extracts information about
terrorism must distinguish between the person who
is the perpetrator and the person who is the victim.
We hypothesized that IE techniques would be well-
suited for source identification because an opinion
statement can be viewed as a kind of speech event
with the source as the agent.
We investigate two very different learning-based
methods from information extraction for the prob-
lem of opinion source identification: graphical mod-
els and extraction pattern learning. In particular, we
consider Conditional Random Fields (Lafferty et al,
2001) and a variation of AutoSlog (Riloff, 1996a).
CRFs have been used successfully for Named En-
tity recognition (e.g., McCallum and Li (2003),
Sarawagi and Cohen (2004)), and AutoSlog has per-
formed well on information extraction tasks in sev-
eral domains (Riloff, 1996a). While CRFs treat
source identification as a sequence tagging task, Au-
toSlog views the problem as a pattern-matching task,
acquiring symbolic patterns that rely on both the
syntax and lexical semantics of a sentence. We hy-
pothesized that a combination of the two techniques
would perform better than either one alone.
Section 3 describes the CRF approach to identify-
ing opinion sources and the features that the system
uses. Section 4 then presents a new variation of Au-
toSlog, AutoSlog-SE, which generates IE patterns to
extract sources. Section 5 describes the hybrid sys-
tem: we encode the IE patterns as additional features
in the CRF model. Finally, Section 6 presents our
experimental results and error analysis.
3 Semantic Tagging via Conditional
Random Fields
We defined the problem of opinion source identifi-
cation as a sequence tagging task via CRFs as fol-
lows. Given a sequence of tokens, x = x1x2...xn,
we need to generate a sequence of tags, or labels,
y = y1y2...yn. We define the set of possible label
values as ?S?, ?T?, ?-?, where ?S? is the first to-
ken (or Start) of a source, ?T? is a non-initial token
(i.e., a conTinuation) of a source, and ?-? is a token
that is not part of any source.2
A detailed description of CRFs can be found in
2This is equivalent to the IOB tagging scheme used in syn-
tactic chunkers (Ramshaw and Marcus, 1995).
356
Lafferty et al (2001). For our sequence tagging
problem, we create a linear-chain CRF based on
an undirected graph G = (V,E), where V is the
set of random variables Y = {Yi|1 ? i ? n},
one for each of n tokens in an input sentence;
and E = {(Yi?1, Yi)|1 < i ? n} is the set
of n ? 1 edges forming a linear chain. For each
sentence x, we define a non-negative clique poten-
tial exp(
?K
k=1 ?kfk(yi?1, yi, x)) for each edge, and
exp(?K?k=1 ??kf ?k(yi, x)) for each node, where fk(...)
is a binary feature indicator function, ?k is a weight
assigned for each feature function, and K and K ?
are the number of features defined for edges and
nodes respectively. Following Lafferty et al (2001),
the conditional probability of a sequence of labels y
given a sequence of tokens x is:
P (y|x) = 1Zx
exp
?
X
i,k
?k fk(yi?1, yi, x)+
X
i,k
??k f ?k(yi, x)
?
(1)
Zx =
X
y
exp
?
X
i,k
?k fk(yi?1, yi, x) +
X
i,k
??k f ?k(yi, x)
?
(2)
where Zx is a normalization constant for each
x. Given the training data D, a set of sen-
tences paired with their correct ?ST-? source la-
bel sequences, the parameters of the model are
trained to maximize the conditional log-likelihood
?
(x,y)?D P (y|x). For inference, given a sentence x
in the test data, the tagging sequence y is given by
argmaxy?P (y?|x).
3.1 Features
To develop features, we considered three properties
of opinion sources. First, the sources of opinions are
mostly noun phrases. Second, the source phrases
should be semantic entities that can bear or express
opinions. Third, the source phrases should be di-
rectly related to an opinion expression. When con-
sidering only the first and second criteria, this task
reduces to named entity recognition. Because of the
third condition, however, the task requires the recog-
nition of opinion expressions and a more sophisti-
cated encoding of sentence structure to capture re-
lationships between source phrases and opinion ex-
pressions.
With these properties in mind, we define the fol-
lowing features for each token/word xi in an input
sentence. For pedagogical reasons, we will describe
some of the features as being multi-valued or cate-
gorical features. In practice, however, all features
are binarized for the CRF model.
Capitalization features We use two boolean fea-
tures to represent the capitalization of a word:
all-capital, initial-capital.
Part-of-speech features Based on the lexical cat-
egories produced by GATE (Cunningham et al,
2002), each token xi is classified into one of a set
of coarse part-of-speech tags: noun, verb, adverb,
wh-word, determiner, punctuation, etc. We do the
same for neighboring words in a [?2,+2] window
in order to assist noun phrase segmentation.
Opinion lexicon features For each token xi, we in-
clude a binary feature that indicates whether or not
the word is in our opinion lexicon ? a set of words
that indicate the presence of an opinion. We do the
same for neighboring words in a [?1,+1] window.
Additionally, we include for xi a feature that in-
dicates the opinion subclass associated with xi, if
available from the lexicon. (e.g., ?bless? is clas-
sified as ?moderately subjective? according to the
lexicon, while ?accuse? and ?berate? are classified
more specifically as ?judgments?.) The lexicon is
initially populated with approximately 500 opinion
words 3 from (Wiebe et al, 2002), and then aug-
mented with opinion words identified in the training
data. The training data contains manually produced
phrase-level annotations for all expressions of opin-
ions, emotions, etc. (Wiebe et al, 2005). We col-
lected all content words that occurred in the training
set such that at least 50% of their occurrences were
in opinion annotations.
Dependency tree features For each token xi, we
create features based on the parse tree produced by
the Collins (1999) dependency parser. The purpose
of the features is to (1) encode structural informa-
tion, and (2) indicate whether xi is involved in any
grammatical relations with an opinion word. Two
pre-processing steps are required before features can
be constructed:
3Some words are drawn from Levin (1993); others are from
Framenet lemmas (Baker et al 1998) associated with commu-
nication verbs.
357
1. Syntactic chunking. We traverse the depen-
dency tree using breadth-first search to identify
and group syntactically related nodes, produc-
ing a flatter, more concise tree. Each syntac-
tic ?chunk? is also assigned a grammatical role
(e.g., subject, object, verb modifier, time,
location, of-pp, by-pp) based on its con-
stituents. Possessives (e.g., ?Clinton?s idea?)
and the phrase ?according to X? are handled as
special cases in the chunking process.
2. Opinion word propagation. Although the
opinion lexicon contains only content words
and no multi-word phrases, actual opinions of-
ten comprise an entire phrase, e.g., ?is really
willing? or ?in my opinion?. As a result, we
mark as an opinion the entire chunk that con-
tains an opinion word. This allows each token
in the chunk to act as an opinion word for fea-
ture encoding.
After syntactic chunking and opinion word propa-
gation, we create the following dependency tree fea-
tures for each token xi:
? the grammatical role of its chunk
? the grammatical role of xi?1?s chunk
? whether the parent chunk includes an opinion
word
? whether xi?s chunk is in an argument position
with respect to the parent chunk
? whether xi represents a constituent boundary
Semantic class features We use 7 binary fea-
tures to encode the semantic class of each word
xi: authority, government, human, media,
organization or company, proper name,
and other. The other class captures 13 seman-
tic classes that cannot be sources, such as vehicle
and time.
Semantic class information is derived from named
entity and semantic class labels assigned to xi by the
Sundance shallow parser (Riloff, 2004). Sundance
uses named entity recognition rules to label noun
phrases as belonging to named entity classes, and
assigns semantic tags to individual words based on
a semantic dictionary. Table 1 shows the hierarchy
that Sundance uses for semantic classes associated
with opinion sources. Sundance is also used to rec-
ognize and instantiate the source extraction patterns
PROPER NAMEAUTHORITY LOCATION
CITY
COUNTRY
PLANET
PROVINCE
PERSON NAME
PERSON DESC
NATIONALITY
TITLE
COMPANY
GOVERNMENT
MEDIA
ORGANIZATION
HUMAN
SOURCE
Figure 1: The semantic hierarchy for opinion
sources
that are learned by AutoSlog-SE, which is described
in the next section.
4 Semantic Tagging via Extraction
Patterns
We also learn patterns to extract opinion sources us-
ing a statistical adaptation of the AutoSlog IE learn-
ing algorithm. AutoSlog (Riloff, 1996a) is a super-
vised extraction pattern learner that takes a train-
ing corpus of texts and their associated answer keys
as input. A set of heuristics looks at the context
surrounding each answer and proposes a lexico-
syntactic pattern to extract that answer from the text.
The heuristics are not perfect, however, so the result-
ing set of patterns needs to be manually reviewed by
a person.
In order to build a fully automatic system that
does not depend on manual review, we combined
AutoSlog?s heuristics with statistics from the an-
notated training data to create a fully automatic
supervised learner. We will refer to this learner
as AutoSlog-SE (Statistically Enhanced variation
of AutoSlog). AutoSlog-SE?s learning process has
three steps:
Step 1: AutoSlog?s heuristics are applied to every
noun phrase (NP) in the training corpus. This
generates a set of extraction patterns that, col-
lectively, can extract every NP in the training
corpus.
Step 2: The learned patterns are augmented with
selectional restrictions that semantically con-
strain the types of noun phrases that are legiti-
mate extractions for opinion sources. We used
358
the semantic classes shown in Figure 1 as se-
lectional restrictions.
Step 3: The patterns are applied to the training cor-
pus and statistics are gathered about their ex-
tractions. We count the number of extrac-
tions that match annotations in the corpus (cor-
rect extractions) and the number of extractions
that do not match annotations (incorrect extrac-
tions). These counts are then used to estimate
the probability that the pattern will extract an
opinion source in new texts:
P (source | patterni) =
correct sources
correct sources + incorrect sources
This learning process generates a set of extraction
patterns coupled with probabilities. In the next sec-
tion, we explain how these extraction patterns are
represented as features in the CRF model.
5 Extraction Pattern Features for the CRF
The extraction patterns provide two kinds of infor-
mation. SourcePatt indicates whether a word
activates any source extraction pattern. For exam-
ple, the word ?complained? activates the pattern
?<subj> complained? because it anchors the ex-
pression. SourceExtr indicates whether a word is
extracted by any source pattern. For example, in the
sentence ?President Jacques Chirac frequently com-
plained about France?s economy?, the words ?Pres-
ident?, ?Jacques?, and ?Chirac? would all be ex-
tracted by the ?<subj> complained? pattern.
Each extraction pattern has frequency and prob-
ability values produced by AutoSlog-SE, hence we
create four IE pattern-based features for each token
xi: SourcePatt-Freq, SourceExtr-Freq,
SourcePatt-Prob, and SourceExtr-Prob,
where the frequency values are divided into three
ranges: {0, 1, 2+} and the probability values are di-
vided into five ranges of equal size.
6 Experiments
We used the Multi-Perspective Question Answering
(MPQA) corpus4 for our experiments. This corpus
4The MPQA corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
consists of 535 documents that have been manu-
ally annotated with opinion-related information in-
cluding direct and indirect sources. We used 135
documents as a tuning set for model development
and feature engineering, and used the remaining 400
documents for evaluation, performing 10-fold cross
validation. These texts are English language ver-
sions of articles that come from many countries and
cover many topics.5
We evaluate performance using 3 measures: over-
lap match (OL), head match (HM), and exact match
(EM). OL is a lenient measure that considers an ex-
traction to be correct if it overlaps with any of the an-
notated words. HM is a more conservative measure
that considers an extraction to be correct if its head
matches the head of the annotated source. We report
these somewhat loose measures because the annota-
tors vary in where they place the exact boundaries
of a source. EM is the strictest measure that requires
an exact match between the extracted words and the
annotated words. We use three evaluation metrics:
recall, precision, and F-measure with recall and pre-
cision equally weighted.
6.1 Baselines
We developed three baseline systems to assess the
difficulty of our task. Baseline-1 labels as sources
all phrases that belong to the semantic categories
authority, government, human, media,
organization or company, proper name.
Table 1 shows that the precision is poor, suggest-
ing that the third condition described in Section 3.1
(opinion recognition) does play an important role in
source identification. The recall is much higher but
still limited due to sources that fall outside of the se-
mantic categories or are not recognized as belong-
ing to these categories. Baseline-2 labels a noun
phrase as a source if any of the following are true:
(1) the NP is the subject of a verb phrase containing
an opinion word, (2) the NP follows ?according to?,
(3) the NP contains a possessive and is preceded by
an opinion word, or (4) the NP follows ?by? and at-
taches to an opinion word. Baseline-2?s heuristics
are designed to address the first and the third condi-
tions in Section 3.1. Table 1 shows that Baseline-2
is substantially better than Baseline-1. Baseline-3
5This data was obtained from the Foreign Broadcast Infor-
mation Service (FBIS), a U.S. government agency.
359
Recall Prec F1
OL 77.3 28.8 42.0
Baseline-1 HM 71.4 28.6 40.8
EM 65.4 20.9 31.7
OL 62.4 60.5 61.4
Baseline-2 HM 59.7 58.2 58.9
EM 50.8 48.9 49.8
OL 49.9 72.6 59.2
Baseline-3 HM 47.4 72.5 57.3
EM 44.3 58.2 50.3
OL 48.5 81.3 60.8
Extraction Patterns HM 46.9 78.5 58.7
EM 41.9 70.2 52.5
CRF: OL 56.1 81.0 66.3
basic features HM 55.1 79.2 65.0
EM 50.0 72.4 59.2
CRF: OL 59.1 82.4 68.9
basic + IE pattern HM 58.1 80.5 67.5
features EM 52.5 73.3 61.2
CRF-FI: OL 57.7 80.7 67.3
basic features HM 56.8 78.8 66.0
EM 51.7 72.4 60.3
CRF-FI: OL 60.6 81.2 69.4
basic + IE pattern HM 59.5 79.3 68.0
features EM 54.1 72.7 62.0
Table 1: Source identification performance table
labels a noun phrase as a source if it satisfies both
Baseline-1 and Baseline-2?s conditions (this should
satisfy all three conditions described in Section 3.1).
As shown in Table 1, the precision of this approach
is the best of the three baselines, but the recall is the
lowest.
6.2 Extraction Pattern Experiment
We evaluated the performance of the learned extrac-
tion patterns on the source identification task. The
learned patterns were applied to the test data and
the extracted sources were scored against the manual
annotations.6 Table 1 shows that the extraction pat-
terns produced lower recall than the baselines, but
with considerably higher precision. These results
show that the extraction patterns alone can identify
6These results were obtained using the patterns that had a
probability > .50 and frequency > 1.
nearly half of the opinion sources with good accu-
racy.
6.3 CRF Experiments
We developed our CRF model using the MALLET
code from McCallum (2002). For training, we used
a Gaussian prior of 0.25, selected based on the tun-
ing data. We evaluate the CRF using the basic fea-
tures from Section 3, both with and without the IE
pattern features from Section 5. Table 1 shows that
the CRF with basic features outperforms all of the
baselines as well as the extraction patterns, achiev-
ing an F-measure of 66.3 using the OL measure,
65.0 using the HM measure, and 59.2 using the
EM measure. Adding the IE pattern features fur-
ther increases performance, boosting recall by about
3 points for all of the measures and slightly increas-
ing precision as well.
CRF with feature induction. One limitation of
log-linear function models like CRFs is that they
cannot form a decision boundary from conjunctions
of existing features, unless conjunctions are explic-
itly given as part of the feature vector. For the
task of identifying opinion sources, we observed
that the model could benefit from conjunctive fea-
tures. For instance, instead of using two separate
features, HUMAN and PARENT-CHUNK-INCLUDES-
OPINION-EXPRESSION, the conjunction of the two
is more informative.
For this reason, we applied the CRF feature in-
duction approach introduced by McCallum (2003).
As shown in Table 1, where CRF-FI stands for the
CRF model with feature induction, we see consis-
tent improvements by automatically generating con-
junctive features. The final system, which com-
bines the basic features, the IE pattern features,
and feature induction achieves an F-measure of 69.4
(recall=60.6%, precision=81.2%) for the OL mea-
sure, an F-measure of 68.0 (recall=59.5%, preci-
sion=79.3%) for the HM measure, and an F-measure
of 62.0 (recall=54.1%, precision=72.7%) for the EM
measure.
6.4 Error Analysis
An analysis of the errors indicated some common
mistakes:
? Some errors resulted from error propagation in
360
our subsystems. Errors from the sentence bound-
ary detector in GATE (Cunningham et al, 2002)
were especially problematic because they caused
the Collins parser to fail, resulting in no depen-
dency tree information.
? Some errors were due to complex and unusual
sentence structure, which our rather simple fea-
ture encoding for CRF could not capture well.
? Some errors were due to the limited coverage of
the opinion lexicon. We failed to recognize some
cases when idiomatic or vague expressions were
used to express opinions.
Below are some examples of errors that we found
interesting. Doubly underlined phrases indicate in-
correctly extracted sources (either false positives
or false negatives). Opinion words are singly
underlined.
False positives:
(1) Actually, these three countries do have one common
denominator, i.e., that their values and policies do not
agree with those of the United States and none of them
are on good terms with the United States.
(2) Perhaps this is why Fidel Castro has not spoken out
against what might go on in Guantanamo.
In (1), ?their values and policies? seems like a rea-
sonable phrase to extract, but the annotation does not
mark this as a source, perhaps because it is some-
what abstract. In (2), ?spoken out? is negated, which
means that the verb phrase does not bear an opinion,
but our system failed to recognize the negation.
False negatives:
(3) And for this reason, too, they have a moral duty to
speak out, as Swedish Foreign Minister Anna Lindh,
among others, did yesterday.
(4) In particular, Iran and Iraq are at loggerheads with
each other to this day.
Example (3) involves a complex sentence structure
that our system could not deal with. (4) involves an
uncommon opinion expression that our system did
not recognize.
7 Related Work
To our knowledge, our research is the first to auto-
matically identify opinion sources using the MPQA
opinion annotation scheme. The most closely re-
lated work on opinion analysis is Bethard et al
(2004), who use machine learning techniques to
identify propositional opinions and their holders
(sources). However, their work is more limited
in scope than ours in several ways. Their work
only addresses propositional opinions, which are
?localized in the propositional argument? of cer-
tain verbs such as ?believe? or ?realize?. In con-
trast, our work aims to find sources for all opinions,
emotions, and sentiments, including those that are
not related to a verb at all. Furthermore, Berthard
et al?s task definition only requires the identifica-
tion of direct sources, while our task requires the
identification of both direct and indirect sources.
Bethard et al evaluate their system on manually
annotated FrameNet (Baker et al, 1998) and Prop-
Bank (Palmer et al, 2005) sentences and achieve
48% recall with 57% precision.
Our IE pattern learner can be viewed as a cross
between AutoSlog (Riloff, 1996a) and AutoSlog-
TS (Riloff, 1996b). AutoSlog is a supervised learner
that requires annotated training data but does not
compute statistics. AutoSlog-TS is a weakly super-
vised learner that does not require annotated data
but generates coarse statistics that measure each pat-
tern?s correlation with relevant and irrelevant docu-
ments. Consequently, the patterns learned by both
AutoSlog and AutoSlog-TS need to be manually re-
viewed by a person to achieve good accuracy. In
contrast, our IE learner, AutoSlog-SE, computes
statistics directly from the annotated training data,
creating a fully automatic variation of AutoSlog.
8 Conclusion
We have described a hybrid approach to the problem
of extracting sources of opinions in text. We cast
this problem as an information extraction task, using
both CRFs and extraction patterns. Our research is
the first to identify both direct and indirect sources
for all types of opinions, emotions, and sentiments.
Directions for future work include trying to in-
crease recall by identifying relationships between
opinions and sources that cross sentence boundaries,
and relationships between multiple opinion expres-
sions by the same source. For example, the fact that
a coreferring noun phrase was marked as a source
in one sentence could be a useful clue for extracting
the source from another sentence. The probability or
the strength of an opinion expression may also play
a useful role in encouraging or suppressing source
extraction.
361
9 Acknowledgments
We thank the reviewers for their many helpful com-
ments, and the Cornell NLP group for their advice
and suggestions for improvement. This work was
supported by the Advanced Research and Develop-
ment Activity (ARDA), by NSF Grants IIS-0208028
and IIS-0208985, and by the Xerox Foundation.
References
C. Baker, C. Fillmore & J. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of the COLING-ACL.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou & D. Juraf-
sky. 2004. Automatic extraction of opinion propositions and
their holders. In Proceedings of AAAI Spring Symposium on
Exploring Attitude and Affect in Text.
D. Bikel, S. Miller, R. Schwartz & R. Weischedel. 1997.
Nymble: A High-Performance Learning Name-Finder. In
Proceedings of the Fifth Conference on Applied Natural Lan-
guage Processing.
E. Breck & C. Cardie. 2004. Playing the Telephone Game:
Determining the Hierarchical Structure of Perspective and
Speech Expressions. In Proceedings of 20th International
Conference on Computational Linguistics.
C. Cardie, J. Wiebe, T. Wilson & D. Litman. 2004. Low-
level annotations and summary representations of opinions
for multiperspective QA. In New Directions in Question An-
swering. AAAI Press/MIT Press.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
H. Cunningham, D. Maynard, K. Bontcheva & V. Tablan. 2002.
GATE: A Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. In Proceed-
ings of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
S. Das & M. Chen. 2001. Yahoo for amazon: Extracting market
sentiment from stock message boards. In Proceedings of the
8th Asia Pacific Finance Association Annual Conference.
K. Dave, S. Lawrence & D. Pennock. 2003. Mining the peanut
gallery: Opinion extraction and semantic classification of
product reviews. In International World Wide Web Confer-
ence.
J. Lafferty, A. K. McCallum & F. Pereira. 2001. Conditional
Random Fields: Probabilistic Models for Segmenting and
Labeling Sequence Data. In Proceedings of 18th Interna-
tional Conference on Machine Learning.
B. Levin. 1993. English Verb Classes and Alternations: A
Preliminary Investigation. University of Chicago Press.
A. K. McCallum. 2002. MALLET: A Machine Learning for
Language Toolkit. http://mallet.cs.umass.edu.
A. K. McCallum. 2003. Efficiently Inducing Features of Con-
ditional Random Fields. In Conference on Uncertainty in
Artificial Intelligence.
A. K. McCallum & W. Li. 2003. Early Results for Named
Entity Recognition with Conditional Random Fields, Feature
Induction and Web-Enhanced Lexicons. In Conference on
Natural Language Learning.
S. Morinaga, K. Yamanishi, K. Tateishi & T. Fukushima 2002.
Mining Product Reputations on the Web. In Proceedings of
the 8th Internatinal Conference on Knowledge Discover and
Data Mining.
M. Palmer, D. Gildea & P. Kingsbury. 2005. The Proposition
Bank: An Annotated Corpus of Semantic Roles. In Compu-
tational Linguistics 31.
B. Pang, L. Lee & S. Vaithyanathan. 2002. Thumbs up? sen-
timent classification using machine learning techniques. In
Proceedings of the 2002 Conference on Empirical Methods
in Natural Language Processing.
B. Pang & L. Lee. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on mini-
mum cuts. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics.
L. A. Ramshaw & M. P. Marcus. 1995. Nymble: A High-
Performance Learning Name-Finder. In Proceedings of the
3rd Workshop on Very Large Corpora.
E. Riloff. 1996a. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
In Artificial Intelligence, Vol. 85.
E. Riloff. 1996b. Automatically Generating Extraction Patterns
from Untagged Text. In Proceedings of the 13th National
Conference on Artificial Intelligence.
E. Riloff & J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In Proceesings of 2003 Conference
on Empirical Methods in Natural Language Processing.
E. Riloff & W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems Technical Report UUCS-04-
015, School of Computing, University of Utah.
S. Sarawagi & W. W. Cohen. 2004. Semi-Markov Condi-
tional Random Fields for Information Extraction 18th An-
nual Conference on Neural Information Processing Systems.
P. Turney. 2002. Thumbs up or thumbs down? semantic orien-
tation applied to unsupervised classification of reviews. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics.
T. Wilson, J. Wiebe & R. Hwa. 2004. Just how mad are you?
finding strong and weak opinion clauses. In Proceedings of
the 9th National Conference on Artificial Intelligence.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J. Wiebe,
Y. Choi, C. Cardie, E. Riloff & S. Patwardhan. 2005. Opin-
ionFinder: A system for subjectivity analysis. Demonstra-
tion Description in Conference on Empirical Methods in
Natural Language Processing.
J. Yi, T. Nasukawa, R. Bunescu & W. Niblack. 2003. Sentiment
Analyzer: Extracting Sentiments about a Given Topic using
Natural Language Processing Techniques. In Proceedings of
the 3rd IEEE International Conference on Data Mining.
H. Yu & V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identify-
ing the polarity of opinion sentences. In Proceedings of the
Conference on Empirical Methods in Natural Language Pro-
cessing.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis, B. Fraser,
D. Litman, D. Pierce, E. Riloff & T. Wilson. 2002. NRRC
Summer Workshop on Multiple-Perspective Question An-
swering: Final Report.
J. Wiebe & E. Riloff. 2005. Creating subjective and objective
sentence classifiers from unannotated texts. Sixth Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics.
J. Wiebe, T. Wilson & C. Cardie. 2005. Annotating expressions
of opinions and emotions in language. Language Resources
and Evaluation, 1(2).
362
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34?35,
Vancouver, October 2005.
OpinionFinder: A system for subjectivity analysis
Theresa Wilson?, Paul Hoffmann?, Swapna Somasundaran?, Jason Kessler?,
Janyce Wiebe??, Yejin Choi?, Claire Cardie?, Ellen Riloff?, Siddharth Patwardhan?
?Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, Cornell University, Ithaca, NY 14853
?School of Computing, University of Utah, Salt Lake City, UT 84112
{twilson,hoffmanp,swapna,jsk44,wiebe}@cs.pitt.edu,
{ychoi,cardie}@cs.cornell.edu, {riloff,sidd}@cs.utah.edu
1 Introduction
OpinionFinder is a system that performs subjectivity
analysis, automatically identifying when opinions,
sentiments, speculations, and other private states are
present in text. Specifically, OpinionFinder aims to
identify subjective sentences and to mark various as-
pects of the subjectivity in these sentences, includ-
ing the source (holder) of the subjectivity and words
that are included in phrases expressing positive or
negative sentiments.
Our goal with OpinionFinder is to develop a sys-
tem capable of supporting other Natural Language
Processing (NLP) applications by providing them
with information about the subjectivity in docu-
ments. Of particular interest are question answering
systems that focus on being able to answer opinion-
oriented questions, such as the following:
How is Bush?s decision not to ratify the
Kyoto Protocol looked upon by Japan and
other US allies?
How do the Chinese regard the human
rights record of the United States?
To answer these types of questions, a system needs
to be able to identify when opinions are expressed in
text and who is expressing them. Other applications
that would benefit from knowledge of subjective lan-
guage include systems that summarize the various
viewpoints in a document or that mine product re-
views. Even typical fact-oriented applications, such
as information extraction, can benefit from subjec-
tivity analysis by filtering out opinionated sentences
(Riloff et al, 2005).
2 OpinionFinder
OpinionFinder runs in two modes, batch and inter-
active. Document processing is largely the same for
both modes. In batch mode, OpinionFinder takes a
list of documents to process. Interactive mode pro-
vides a front-end that allows a user to query on-line
news sources for documents to process.
2.1 System Architecture Overview
OpinionFinder operates as one large pipeline. Con-
ceptually, the pipeline can be divided into two parts.
The first part performs mostly general purpose doc-
ument processing (e.g., tokenization and part-of-
speech tagging). The second part performs the sub-
jectivity analysis. The results of the subjectivity
analysis are returned to the user in the form of
SGML/XML markup of the original documents.
2.2 Document Processing
For general document processing, OpinionFinder
first runs the Sundance partial parser (Riloff and
Phillips, 2004) to provide semantic class tags, iden-
tify Named Entities, and match extraction patterns
that correspond to subjective language (Riloff and
Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tok-
enize, sentence split, and part-of-speech tag the data,
and the Abney stemmer2 is used to stem. In batch
mode, OpinionFinder parses the data again, this time
to obtain constituency parse trees (Collins, 1997),
which are then converted to dependency parse trees
(Xia and Palmer, 2001). Currently, this stage is only
1http://opennlp.sourceforge.net/
2SCOL version 1g available at http://www.vinartus.net/spa/
34
available for batch mode processing due to the time
required for parsing. Finally, a clue-finder is run to
identify words and phrases from a large subjective
language lexicon.
2.3 Subjectivity Analysis
The subjectivity analysis has four components.
2.3.1 Subjective Sentence Classification
The first component is a Naive Bayes classifier
that distinguishes between subjective and objective
sentences using a variety of lexical and contextual
features (Wiebe and Riloff, 2005; Riloff and Wiebe,
2003). The classifier is trained using subjective and
objective sentences, which are automatically gener-
ated from a large corpus of unannotated data by two
high-precision, rule-based classifiers.
2.3.2 Speech Events and Direct Subjective
Expression Classification
The second component identifies speech events
(e.g., ?said,? ?according to?) and direct subjective
expressions (e.g., ?fears,? ?is happy?). Speech
events include both speaking and writing events.
Direct subjective expressions are words or phrases
where an opinion, emotion, sentiment, etc. is di-
rectly described. A high-precision, rule-based clas-
sifier is used to identify these expressions.
2.3.3 Opinion Source Identification
The third component is a source identifier that
combines a Conditional Random Field sequence
tagging model (Lafferty et al, 2001) and extraction
pattern learning (Riloff, 1996) to identify the sources
of speech events and subjective expressions (Choi
et al, 2005). The source of a speech event is the
speaker; the source of a subjective expression is the
experiencer of the private state. The source identifier
is trained on the MPQA Opinion Corpus3 using a
variety of features. Because the source identifier re-
lies on dependency parse information, it is currently
only available in batch mode.
2.3.4 Sentiment Expression Classification
The final component uses two classifiers to iden-
tify words contained in phrases that express pos-
itive or negative sentiments (Wilson et al, 2005).
3The MPQA Opinion Corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
The first classifier focuses on identifying sentiment
expressions. The second classifier takes the senti-
ment expressions and identifies those that are pos-
itive and negative. Both classifiers were developed
using BoosTexter (Schapire and Singer, 2000) and
trained on the MPQA Corpus.
3 Related Work
Please see (Wiebe and Riloff, 2005; Choi et al,
2005; Wilson et al, 2005) for discussions of related
work in automatic opinion and sentiment analysis.
4 Acknowledgments
This work was supported by the Advanced Research
and Development Activity (ARDA), by the NSF
under grants IIS-0208028, IIS-0208798 and IIS-
0208985, and by the Xerox Foundation.
References
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identi-
fying sources of opinions with conditional random fields and
extraction patterns. In HLT/EMNLP 2005.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL-1997.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML-2001.
E. Riloff and W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems. Technical Report UUCS-04-
015, School of Computing, University of Utah.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In EMNLP-2003.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction. In
AAAI-2005.
E. Riloff. 1996. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
Artificial Intelligence, 85:101?134.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In CICLing-
2005.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis. In
HLT/EMNLP 2005.
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT-2001.
35
Proceedings of NAACL HLT 2007, pages 65?72,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Structured Local Training and Biased Potential Functions for Conditional
Random Fields with Application to Coreference Resolution
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Abstract
Conditional Random Fields (CRFs) have shown
great success for problems involving structured out-
put variables. However, for many real-world NLP
applications, exact maximum-likelihood training is
intractable because computing the global normal-
ization factor even approximately can be extremely
hard. In addition, optimizing likelihood often does
not correlate with maximizing task-specific evalu-
ation measures. In this paper, we present a novel
training procedure, structured local training, that
maximizes likelihood while exploiting the benefits
of global inference during training: hidden vari-
ables are used to capture interactions between lo-
cal inference and global inference. Furthermore,
we introduce biased potential functions that empir-
ically drive CRFs towards performance improve-
ments w.r.t. the preferred evaluation measure for
the learning task. We report promising experimen-
tal results on two coreference data sets using two
task-specific evaluation measures.
1 Introduction
Undirected graphical models such as Conditional
Random Fields (CRFs) (Lafferty et al, 2001) have
shown great success for problems involving struc-
tured output variables (e.g. Wellner et al (2004),
Finkel et al (2005)). For many real-world NLP ap-
plications, however, the required graph structure can
be very complex, and computing the global normal-
ization factor even approximately can be extremely
hard. Previous approaches for training CRFs have
either (1) opted for a training method that no longer
maximizes the likelihood, (e.g. McCallum and Well-
ner (2004), Roth and Yih (2005)) 1, or (2) opted for a
1Both McCallum and Wellner (2004) and Roth and Yih
(2005) used the voted perceptron algorithm (Collins, 2002) to
train intractable CRFs.
simplified graph structure to avoid intractable global
normalization (e.g. Roth and Yih (2005), Wellner et
al. (2004)).
Solutions of the first type replace the computation
of the global normalization factor
?
y p(y|x) with
argmaxy p(y|x) during training, since finding an
argmax of a probability distribution is often an eas-
ier problem than finding the entire probability distri-
bution. Training via the voted perceptron algorithm
(Collins, 2002) or using a max-margin criterion also
correspond to the first option (e.g. McCallum and
Wellner (2004), Finley and Joachims (2005)). But
without the global normalization, the maximum-
likelihood criterion motivated by the maximum en-
tropy principle (Berger et al, 1996) is no longer a
feasible option as an optimization criterion.
The second solution simplifies the graph struc-
ture for training, and applies complex global infer-
ence only for testing. In spite of the discrepancy
between the training model and the testing model,
it has been empirically shown that (1) performing
global inference only during testing can improve
performance (e.g. Finkel et al (2005), Roth and Yih
(2005)), and (2) full-blown global training can of-
ten perform worse due to insufficient training data
(e.g. Punyakanok et al (2005)). Importantly, how-
ever, attempts to reduce the discrepancy between the
training and test models ? by judiciously adding the
effect of global inference to the training ? have pro-
duced substantial performance improvements over
locally trained models (e.g. Cohen and Carvalho
(2005), Sutton and McCallum (2005a)).
In this paper, we present structured local training,
a novel training procedure for maximum-likelihood
65
training of undirected graphical models, such as
CRFs. The procedure maximizes likelihood while
exploiting the benefits of global inference during
training by capturing the interactions between local
inference and global inference via hidden variables.
Furthermore, we introduce biased potential func-
tions that redefine the likelihood for CRFs so that
the performance of CRFs trained under the max-
imum likelihood criterion correlates better empiri-
cally with the preferred evaluation measures such as
F-score and MUC-score.
We focus on the problem of coreference resolu-
tion; however, our approaches are general and can
be extended to other NLP applications with struc-
tured output. Our approaches also extend to non-
conditional graphical models such as Markov Ran-
dom Fields. In experiments on two coreference data
sets, structured local training reduces the error rate
significantly (3.5%) for one coreference data set and
minimally (? 1%) for the other. Experiments using
biased potential functions increase recall uniformly
and significantly for both data sets and both task-
specific evaluation measures. Results for the com-
bination of the two techniques are promising, but
mixed: pairwise F1 increases by 0.8-5.5% for both
data sets; MUC F1 increases by 3.5% for one data
set, but slightly hurts performance for the second
data set.
In ?2, we describe structured local training, and
follow with experimental results in ?3. In ?4, we
describe biased potential functions and follow with
experimental results in ?5. We discuss related work
in ?6.
2 Structured Local Training
2.1 Definitions
For clarity, we define the following terms that we
will use throughout the paper.
? local inference: 2 Inference factored into smaller
independent pieces, without considering the
structure of the output space.
? global inference: Inference applied on the entire
set of output variables, considering the structure
of the output space.
2In this paper, inference refers to the operation of finding the
argmax in particular.
? local training: Training that does not invoke
global inference at each iteration.
? global training: Training that does invoke global
inference at each iteration.
2.2 A Motivating Example for Coreference
Resolution
In this section, we present an example of the coref-
erence resolution problem to motivate our approach.
It has been shown that global inference-based train-
ing for coreference resolution outperforms training
with local inference only (e.g. Finley and Joachims
(2005), McCallum and Wellner (2004)). In particu-
lar, the output of coreference resolution must obey
equivalence relations, and exploiting such structural
constraints on the output space during training can
improve performance. Consider the coreference res-
olution task for the following text.
It was after the passage of this act, that Mary(1)?s attitude
towards Elizabeth(1) became overtly hostile. The deliber-
ations surrounding the act seem to have revived all Mary?s
memories of the humiliations she had suffered at the
hands of Anne Boleyn. At the same time, Elizabeth(2)?s
continuing prevarications over religion confirmed that she
was indeed her mother?s daughter.
In the above text, the ?she? in the last sen-
tence is coreferent with both mentions of
?Elizabeth?. However, when we consider
?she? and ?Elizabeth(1)? in isolation from the
remaining coreference chain, it can be difficult for
a machine learning method to determine whether
the pair is coreferent or not. Indeed, such a
pair may not look very different from the pair
?she? and ?Mary(1)? in terms of feature vectors.
It is much easier, however, to determine that
?she? and ?Elizabeth(2)? are coreferent, or that
?Elizabeth(1)? and ?Elizabeth(2)? are coreferent.
Only by taking the transitive closure of these pair-
wise coreference relations does it become clear that
?she? and ?Elizabeth(1)? are coreferent. In other
words, global training might handle potentially
confusing coreference cases better because it allows
parameter learning (for each pairwise coreference
decision) to be informed by global inference.
We argue that, with appropriate modification to
the learning instances, local training is adequate for
the coreference resolution task. Specifically, we pro-
pose that confusing pairs in the training data ? such
66
as ?she? and ?Elizabeth(1)? ? be learned as not-
coreferent, so long as the global inference step can
fix this error by exploiting the structure of the out-
put space, i.e. by exploiting the equivalence rela-
tions. This is the key idea of structured local train-
ing, which we elaborate formally in the following
section.
2.3 A Hidden-Variable Model
In this section, we present a general description of
structured local training. Let y be a vector of out-
put variables for structured output, and let x be a
vector of input variables. In order to capture the in-
teractions between global inference and local infer-
ence, we introduce hidden variables h, |h| = |y|,
so that the global inference for p(y, h|x) can be fac-
tored into two components using the product rule, as
follows:
p(y, h|x) = p(y|h, x) p(h|x)
= p(y|h) p(h|x)
The second component p(h|x) on the right hand side
corresponds to the local model, for which the infer-
ence factorizes into smaller independent pieces, e.g.
argmaxhp(h|x) = {argmaxh
i
?(h
i
, x)}. And the
first component p(y|h, x) on the right hand side cor-
responds to the global model, whose inference may
not factorize nicely. Further, we assume that y is in-
dependent of x given h, so that p(y|h, x) = p(y|h).
That is to say, h captures sufficient information from
x, so that given h, global inference of y only de-
pends on h. The quantity of p(y|x) then is given by
marginalizing out h as follows:
p(y|x) =
?
h
p(y, h|x)
Intuitively, the hidden variables h represent the lo-
cal decisions that can lead to a good y after global
inference is applied. In the case of coreference reso-
lution, one natural factorization would be that global
inference is a clustering algorithm, and local infer-
ence is a classification decision on each pair of noun
phrases (or mentions).3 In this paper, we assume
3Formally, we define each y
i
? y to be the coreference de-
cision for the ith pair of mentions, and x
i
? x be the input
regarding the ith pair of mentions. Then h
i
corresponds to the
local coreference decision that can lead to a good coreference
decision y
i
after the clustering algorithm has been applied.
that we only parameterize the local model p(h|x),
although it would be possible to extend the parame-
terization to the global model as well, depending on
the particular application under consideration. The
similarity between a pair of mentions is parameter-
ized via log-linear models. However, once we have
the similarity scores extracted via local inference,
the clustering algorithm does not require further pa-
rameterization.
For training, we apply the standard Expectation-
Maximization (EM) algorithm (Dempster et al,
1977) as follows:
? E Step: Compute a distribution
?P (t) = P (h|y, x, ?(t?1))
? M Step: Set ?(t) to ? that maximizes
E
?
P
(t)
[logP (y, h|x, ?)]
By repeatedly applying the above two steps for
t = 1, 2, ..., the value of ? converges to the local
maxima of the conditional log likelihood L(?) =
logP (y|x, ?).
2.4 Application to Coreference Resolution
For y
i
? y (and h
i
? h) in the coreference resolution
task, y
i
= 1 (and h
i
= 1) corresponds to ith pair of
mentions being coreferent, and y
i
= 0 (and h
i
= 0)
corresponds to ith pair being not coreferent.
[Local Model P (h|x)] For the local model, we de-
fine cliques as individual nodes,4 and parameterize
each clique potential as
?(h
i
, x) = ?(h
i
, x
i
) = exp
?
k
?
k
f
k
(h
i
, x
i
)
Let ?(h|x) ?
?
i
?(h
i
, x
i
). Then,
P (h|x) =
?(h, x)
?
h ?(h, x)
Notice that in this model, finding argmaxhP (h|x)
corresponds to simply finding argmax
h
i
?(h
i
, x
i
) in-
dependently for each h
i
? h.
4Each node in the graphical representation of CRFs corre-
sponds to the coreferent decision for each pair of mentions. This
corresponds to the ?Model 3? of McCallum and Wellner (2004).
67
ALGORITHM-1
INPUT: x, true labeling y?, current local model P (h|x)
GOAL: Find the highest confidence labeling y?
such that y? = single-link-clustering(y?)
h? ? argmaxhP (h|x)
h? ? single-link-clustering(h?)
construct a graph G = (V, E), where
E = {h?
i
: h?
i
? h? s.t. y?
i
= 1}
V = {v : v is a NP referred by a h?
i
? E}
with edge cost cost
h
?
i
= ?(h?
i
, x
i
) if h?
i
6= y?
i
with edge cost cost
h
?
i
= 0 if h?
i
= y?
i
find a minimum spanning tree(or forest) M of G
for each h?
i
? h?
if h?
i
= y?
i
y?
i
? h?
i
else if h?
i
? M
y?
i
? 1
else
y?
i
? 0
end for
return y?
Figure 1: Algorithm to find the highest confidence labeling y?
that can be clustered to the true labeling y?
[Global Model P (y|h)] For the global model, we
assume a deterministic clustering algorithm is given.
In particular, we focus on single-link clustering, as it
has been shown to be effective for coreference reso-
lution (e.g. Ng and Cardie (2002)). With single-link
clustering, P (y|h) = 1 if h can be clustered to y,
and P (y|h) = 0 if h cannot be clustered to y.5
[Computation of the E-step] The E-step requires
computation of the distribution of P (h|y, x, ?(t?1)),
which we will simply denote as P (h|y, x), since all
our distributions are implicitly conditioned on the
model parameters ?.
P (h|y, x) =
P (h, y|x)
P (y|x)
? P (y|h) P (h|x)
Notice that when computing P (h|y, x), the denomi-
nator P (y|x) stays as a constant for different values
of h. The E-step requires enumeration of all possible
values of h, but it is intractable with our formulation,
because inference for the global model P (y|h) does
not factor out nicely. Therefore, we must resort to an
5Single-link clustering simply takes the transitive closure,
and does not consider the distance metric. In a pilot study, we
also tried a variant of a stochastic clustering algorithm that takes
into account the distance metric (set as the probabilities from
the local model) for the global model, but the performance was
worse.
ALGORITHM-2
INPUT: x, true labeling y?, current local model P (h|x)
GOAL: Find a high confidence labeling y? that is
close to the true labeling y?
h? ? argmaxhP (h|x)
h? ? single-link-clustering(h?)
for each h?
i
? h?
if h?
i
= y?
i
y?
i
? h?
i
else
y?
i
? y?
i
end for
return y?
Figure 2: Algorithm to find a high confidence labeling y? that
is close to the true labeling y?
approximation method. Neal and Hinton (1998) an-
alyze and motivate various approximate EM training
methods. One popular choice in practice is called
?Viterbi training?, a variant of the EM algorithm,
which has been shown effective in many NLP ap-
plications. Viterbi training approximates the distri-
bution by assigning all probability mass to a single
best assignment. The algorithm for this is shown in
Figure 1.
We propose another approximation option for the
E-step that is given by Figure 2. Intuitively, when
the current local model misses positive coreference
decisions, the first algorithm constructs a y? that is
closest to h? for single-link clustering to recover the
true labeling y?, while the second algorithm con-
structs a y? that is closer to y? by preserving all of
the missing positive coreference decisions. 6
[Computation of M-step] Because P (y|h) is not
parameterized, finding argmax
?
P (y, h|x) reduces
to finding argmax
?
P (h|x), which is standard CRF
training. In order to speed up the training, we start
convex optimization for CRFs using the parame-
ter values ?(t?1) from the previous M-step. For
the very first iteration of EM, we start by setting
P (y?|x) = 1 for E-step, so that the first M-step will
finds argmax
?
P (y?|x).
6In a pilot study, we found that ALGORITHM-2 per-
forms slightly better than ALGORITHM-1. We also tried two
other approximation options, but none performed as well as
ALGORITHM-2. One of them removes the confusing sub-
instances and has the effect of setting a uniform distribution on
those sub-instances. The other computes the actual distribution
on a subset of sub-instances. For brevity, we only present ex-
perimental results using ALGORITHM-2 in this paper.
68
[Inference on the test data] It is intractable to
marginalize out h from P (y, h|x). Therefore, sim-
ilar to the Viterbi-training in the E-step, we approx-
imate the distribution of h by argmaxhP (h|X).
3 Experiments?I
Data set: We evaluate our approach with two
coreference data sets: MUC6 (MUC-6, 1995) and
MPQA7(Wiebe et al, 2005). For the MUC6 data set,
we extract noun phrases (mentions) automatically,
but for MPQA, we assume mentions for corefer-
ence resolution are given as in Stoyanov and Cardie
(2006). For MUC6, we use the standard training/test
data split. For MPQA, we use 150 documents for
training, and 50 documents for testing.
Configuration: We follow Ng and Cardie (2002)
for feature vector construction for each pair of men-
tions,8 and Finley and Joachims (2005) for con-
structing a training/testing instance for each docu-
ment: a training/testing instance consists of all pairs
of mentions in a document. Then, a single pair of
mentions is a sub-instance. We use the Mallet9 im-
plementation of CRFs, and set a Gaussian prior of
1.0 for all experiments. At each M-step, we train
CRFs starting from the parameters from the previous
M-step. We train CRFs up to 200 iterations, but be-
cause we start training CRFs from the previous pa-
rameters, the convergence from the second M-step
becomes much faster. We apply up to 5 EM itera-
tions, and choose best performing ?(t), 2 ? t ? 5
based on the performance on the training data.10
Hypothesis: For the baseline (BASE) we employ
the locally trained model for pairwise decisions
without global inference. Clustering is applied only
at test time, in order to make the assignment on the
output variables coherent. We hypothesize that for
the baseline, maximizing the likelihood for training
will correlate more with the pairwise accuracy of the
7Available at http://nrrc.mitre.org/NRRC/publications.htm.
8In particular, our feature set corresponds to ?All Features?
in Ng and Cardie (2002), and we discretized numeric values.
9Available at http://mallet.cs.umass.edu.
10Selecting ?(t) on a separate tuning data would be better, but
the data for MUC6 in particular is very limited. Notice that we
don?t pick ?1 when reporting the performance of SLT, because
it is identical to the baseline.
MUC6
after clustering before clustering
e % R % P % F % e % R % P % F %
BASE 1.50 59.2 56.2 57.7 1.18 38.0 85.6 52.6
SLT 1.28 49.8 67.3 57.2 1.35 26.4 84.3 40.2
MPQA
after clustering before clustering
e % R % P % F % e % R % P % F %
BASE 9.83 75.8 57.0 65.1 7.05 52.1 83.4 64.1
SLT 6.39 62.1 80.6 70.2 7.39 43.7 90.1 58.9
Table 1: Performance of Structured Local Training: SLT re-
duces error rate (e %) after applying single-link clustering.
incoherent decisions before clustering than the pair-
wise accuracy of the coherent decisions after cluster-
ing. We also hypothesize that by performing struc-
tured local training (SLT), maximizing the likeli-
hood will correlate more with the pairwise accuracy
after clustering.
Results: Experimental results are shown in Ta-
ble 1. We report error rate (error rate = 100 ?
accuracy) on the pairwise decisions (e %), and F1-
score (F %) on the coreferent pairs.11 For compar-
ison, we show numbers from both after and before
single-link clustering is applied. As hypothesized,
the error rate of BASE increases after clustering,
while the error rate of SLT decreases after cluster-
ing. Moreover, the error rate of SLT is considerably
lower than that of BASE after clustering. However,
the F1-score does not correlate with the error rate.
That is, a lower error rate does not always lead to a
higher F1-score, which motivates the Biased Poten-
tial Functions that we introduce in the next section.
Notice that when we compare the precision/recall
breakdown after clustering, SLT has higher precision
and lower recall than BASE.
4 Biased Potential Functions
We introduce biased potential functions for train-
ing CRFs to empirically favor preferred evaluation
measures for the learning task, such as F-score and
MUC-score that have been considered hard for tradi-
11Error rate and F1-score on the coreferent pairs are not ideal
measures for the quality of clustering, however, we show them
here in order to contrast the effect of SLT. We present MUC-
scores for the same experimental settings in Table 3.
69
tional likelihood-based methods to optimize for. In-
tuitively, biased potential functions emphasize those
sub-components of an instance that can be of greater
importance than the rest of an instance.
4.1 Definitions
The conditional probability of P (y|x)12 for CRFs is
given by (Lafferty et al, 2001)
P (y|x) =
?
i
?(C
i
, x)
?
y
?
i
?(C
i
, x)
where ?(C
i
, x) is a potential function defined over
each clique C
i
. Potential functions are typically pa-
rameterized in an exponential form as follows.
?(C
i
, x) = exp
?
k
?
k
f
k
(C
i
, x)
where ?
k
are the parameters and f
k
(?) are fea-
ture indicator functions. Because the Hammersley-
Clifford theorem (1971) for undirected graphical
models holds for any non-negative potential func-
tions, we propose alternative potential functions as
follows.
?(C
i
, x) =
{
??(C
i
, x) if ?(C
i
, x) = true
?(C
i
, x) otherwise
where ? is a non-negative bias factor, and ?(C
i
, x)
is a predicate (or an indicator function) to check cer-
tain properties on (C
i
, x).13 Examples of possible
?(?) would be whether the true assignment for C
i
in the training data contains certain class values, or
whether the current observation indexed by C
i
has
particular characteristics. More specific details will
be given in ?4.2.
Training and testing with biased potential func-
tions is mostly identical to the traditional log-linear
formulations by ?(?) as defined above, except for
small and straightforward modifications to the com-
putation of the likelihood and the derivative of the
likelihood.
12For the local model described in Section 2, y should be
replaced with h. We use y in this section however, as it is a
more conventional notation in general.
13In our problem formulation, cliques are individual nodes,
and potential functions are defined over the observations in-
dexed by the current i only: i.e. ?(C
i
, x) = ?(y
i
, x
i
),
?(C
i
, x) = ?(y
i
, x
i
) and ?(C
i
, x) = ?(y
i
, x
i
).
The key idea for biased potential functions is
nothing new, as it is conceptually similar to in-
stance weighting for problems with non-structured
output (e.g. Aha and Goldstone (1992), Cardie et al
(1997)). However, biased potential functions differ
technically in that they emphasize desired subcom-
ponents without altering the i.i.d. assumption, and
still weight each instance alike. Despite the con-
ceptual simplicity, we are not aware of any previ-
ous work that explored biased potential functions for
problems with structured output.
4.2 Applications to Coreference Resolution
[Bias on Coreferent Pairs] For coreference res-
olution, pairs that are coreferent are in a minority
class14, and biased potential functions can mitigate
this skewed data problem, by amplifying the clique
potentials that correspond to coreferent pairs. We
define ?(y
i
, x
i
) to be true if and only if the true as-
signment for y
i
in the training data is ?coreferent?.
Notice that ?(?) does not depend on what particu-
lar value y
i
might take, but only depends on the true
value of y
i
in the training data. For testing, ?(y
i
, x
i
)
will be always false.15
[Bias on Closer Coreferent Pairs] For corefer-
ence resolution, we hypothesize that coreferent pairs
for closer mentions have more significance, because
they tend to have clearer linguistic clues to deter-
mine coreference. We further hypothesize that by
emphasizing only close coreferent pairs, we can
have our model favor the MUC score. For this, we
define ?(y
i
, x
i
) to be true if and only if x
i
is for a
pair of mentions that are the closest coreferent pair.
5 Experiments?II
Data sets and configurations for experiments are
identical to those used in ?3.
Hypothesis: We hypothesize that using biased po-
tential functions, maximizing the likelihood for
training can correlate better with F1-score or MUC-
score than the pairwise accuracy. In particular,
14Only 1.72% of the pairs are coreferent in the MUC6 data,
and about 12% are coreferent in the MPQA data.
15Notice that ?(y
i
, x
i
) changes the surface of the likelihood
for training, but does not affect the inference of finding the
argmax in our local model. That is, argmax
y
i
?(y
i
, x
i
) =
argmax
y
i
?(y
i
, x
i
) (with y
i
replaced with h
i
).
70
MUC6
pairwise MUC
e % R % P % F % R % P % F %
BASE 1.18 38.0 85.6 52.6 59.0 75.8 66.4
BASIC-P11.5 1.20 38.9 82.1 52.8 64.2 71.8 67.8
BASIC-P13.0 1.32 46.9 71.3 56.6 68.9 64.3 66.5
BASIC-Pa1.5 1.15 44.2 79.9 56.9 62.1 68.7 65.2
BASIC-Pa3.0 1.44 52.5 62.9 57.2 70.9 60.5 65.3
MPQA
pairwise MUC
e % R % P % F % R % P % F %
BASE 7.05 52.1 83.4 64.1 75.6 81.5 78.4
BASIC-P11.5 7.18 54.6 79.6 64.8 77.7 76.5 77.1
BASIC-P13.0 7.22 59.9 75.4 66.8 83.3 71.7 77.1
BASIC-Pa1.5 7.65 59.7 72.2 65.4 79.8 73.2 76.4
BASIC-Pa3.0 8.22 69.2 65.1 67.1 85.8 67.8 75.7
Table 2: Performance of Biased Potential Functions: pairwise
scores are taken before single-link-clustering is applied.
we hypothesize that biasing on every coreferent
pair will correlate more with F1-score, and bias-
ing on close coreferent pairs will correlate more
with MUC-score. In general, we expect that bias-
ing on coreferent pairs will boost recall, potentially
decreasing precision.
Results [BPF]: Experimental results for biased
potential functions, without structured local train-
ing, are shown in Table 2. BASIC-P1? denotes local
training with biased potential on the closest corefer-
ent pairs with bias factor ?, and BASIC-Pa? denotes
local training with biased potential on the all coref-
erent pairs with bias factor ?, where ? = 1.5 or 3.0.
For brevity, we only show pairwise numbers before
applying single-link-clustering.16 As hypothesized,
biased potential functions in general boost recall at
the cost of precision. Also, for a fixed value of
?, BASIC-P1? gives better MUC-F1 than BASIC-
Pa? , and BASIC-Pa? gives better pairwise-F1 than
BASIC-P1? for both data sets.
Results [SLT+BPF]: Experimental results that
combine SLT and BPF are shown in Table 3. Sim-
ilarly as before, SLT-Px? denotes SLT with biased
potential scheme Px, with bias factor ?. For brevity,
16This is because we showed in ?3 that basic local training
does not correlate well with pairwise scores after clustering, and
in order to see the direct effect of biased potential functions, we
examine pairwise numbers before clustering.
MUC6
pairwise MUC
e % R % P % F % R % P % F %
BASE 1.50 59.2 56.2 57.7 59.0 75.8 66.4
SLT 1.28 49.8 67.3 57.2 56.3 77.8 65.3
SLT-P11.5 1.19 52.8 70.6 60.4 59.3 74.6 66.1
SLT-P13.0 1.42 63.5 57.9 60.6 67.5 70.7 69.1
SLT-Pa1.5 1.43 58.6 58.5 58.5* 64.0 73.6 68.5
SLT-Pa3.0 1.71 65.2 50.3 56.8 70.5 69.3 69.9*
MPQA
pairwise MUC
e % R % P % F % R % P % F %
BASE 9.83 75.8 57.0 65.1 75.6 81.5 78.4
SLT 6.39 62.1 80.6 70.2 69.1 88.2 77.5
SLT-P11.5 6.54 64.9 77.4 70.6* 72.2 84.5 77.9*
SLT-P13.0 9.09 77.2 59.6 67.3 78.4 79.5 78.9
SLT-Pa1.5 6.74 65.2 75.7 70.1 72.4 87.2 79.1
SLT-Pa3.0 14.71 78.2 43.9 56.2 80.5 73.8 77.0
Table 3: Performance of Biased Potential Functions with
Structured Local Training: All numbers are taken after single-
link clustering.
we only show numbers after applying single-link-
clustering. Unlike the results shown in Table 2,
for a fixed value of ?, SLT-P1? correlates better
with pairwise-F1, and SLT-Pa? correlates better with
MUC-F1. This indicates that when biased poten-
tial functions are used in conjunction with SLT, the
effect of biased potential functions can be different
from the case without SLT. Comparing F1-scores in
Table 2 and Table 3, we see that the combination of
biased potential functions with SLT improves per-
formance in general. In particular, SLT-P13.0 and
SLT-Pa1.5 consistently improve performance over
BASE on both data sets, for both pairwise-F1 and
MUC-F1. We present performance scores for all
variations of configurations for reference, but we
also mark the particular configuration SLT-Px? (by
?*? on F1-scores) that is chosen when selecting the
configuration based on the performance on the train-
ing data for each performance measure. To con-
clude, structured local training with biased poten-
tial functions bring a substantial improvement for
MUC-F1 score, from 66.4% to 69.9% for MUC6
data set. For pairwise-F1, the performance increase
from 57.7% to 58.5% for MUC6, and from 65.1% to
70.6% for MPQA.17
17Performance on the MPQA data for MUC-F1 is slightly
decreased from 78.4% to 77.9%. Note the MUC scores for the
71
6 Related Work
Structured local training is motivated by recent re-
search that has shown that reducing the discrep-
ancy between the training model and testing model
can improve the performance without incurring the
heavy computational overhead of full-blown global
inference-based training. 18 (e.g. Cohen and Car-
valho (2005), Sutton and McCallum (2005a), Sutton
and McCallum (2005b)). Our work differs in that
(1) we use hidden variables to capture the interac-
tions between local inference and global inference,
(2) we present an application to coreference resolu-
tion, while previous work has shown applications for
variants of sequence tagging. McCallum and Well-
ner (2004) showed a global training approach with
CRFs for coreference resolution, but they used the
voted perceptron algorithm for training, which no
longer maximizes the likelihood. In addition, they
assume that all and only those noun phrases involved
in coreference resolution are given.
The performance of our system on MUC6 data
set is comparable to previously reported systems.
Using the same feature set, Ng and Cardie (2002)
reports 64.5% of MUC-score, while our system
achieved 69.9%. Ng and Cardie (2002) reports
70.4% of MUC-score using hand-selected features.
With an additional feature selection or feature induc-
tion step, the performance of our system might fur-
ther improve. McCallum and Wellner (2004) reports
73.42% of MUC-score on MUC6 data set, but their
experiments assumed perfect identification of all and
only those noun phrases involved in a coreference
relation, thus substantially simplifying the task.
7 Conclusion
We present a novel training procedure, structured
local training, that maximizes likelihood while
exploiting the benefits of global inference during
training. This is achieved by incorporating hidden
variables to capture the interactions between local
MPQA baseline are already quite high to begin with.
18The computational cost for SLT in our experiments were
about twice of the cost for the local training of the baseline. This
is the case because M-step converges very fast from the second
EM iteration, by initializing CRFs using parameters from the
previous M-step. Biased potential functions hardly adds extra
computational cost. In practice, BPFs reduce training time sub-
stantially: we observed that the higher the bias is, the quicker
CRFs converge.
inference and global inference. In addition, we
introduce biased potential functions that allow
CRFs to empirically favor performance measures
such as F1-score or MUC-score. We focused on the
application of coreference resolution in this paper,
but the key ideas of our approaches can be extended
to other applications, and other machine learning
techniques motivated by Markov networks.
Acknowledgments We thank the reviewers as well
as Eric Breck and Ves Stoyanov for their many helpful com-
ments. This work was supported by the Advanced Research and
Development Activity (ARDA), by NSF Grants BCS-0624277,
IIS-0535099, and IIS-0208028, and by gifts from Google and
the Xerox Foundation.
References
D.W. Aha and R.L. Goldstone. 1992. Concept learning and flexible weighting. In
Proc. of the Fourteenth Annual Conference of the Cognitive Science Society.
A. Berger, S.D. Pietra, V.D. Pietra 1996. A Maximum Entropy Approach to
Natural Language Processing. In Computational Linguistics,22.
C. Cardie and N. Howe. 1997. Improving Minority Class Prediction Using Case-
Specific Feature Weights. In ICML.
W.W. Cohen and V. Carvalho. 2005. Stacked Sequential Learning. In IJCAI.
M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models:
Theory and Experiments with Perceptron Algorithms. In EMNLP.
A.P. Dempster, N. M. Laird and D. B. Rubin. 1977. Maximum Likelihood from
Incomplete Data via the EM Algorithm. In Journal of the Loyal Statistical
Society, B.39.
J. Finkel, T. Grenager and C. D. Manning. 2005. Incorporating Non-local Infor-
mation Into Information Extraction Systems By Gibbs Sampling. In ACL.
T. Finley and T. Joachims. 2005. Supervised Clustering with Support Vector
Machines. In ICML.
J. Hammersley and P. Clifford. 1971. Markov fields on finite graphs and lattices.
Unpublished manuscript.
J. Lafferty, A. McCallum and F. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML.
A. McCallum and B. Wellner. 2004. Conditional Models of Identity Uncertainty
with Application to Noun Coreference. In NIPS.
MUC-6 1995. In Proc. of the Sixth Message Understanding Conference (MUC-6)
Morgan Kaufmann.
R. M. Neal and G. E. Hinton. 1998. A view of the EM algorithm that justies
incremental, sparse, and other variants. In Learning in Graphical Models,
Kluwer.
V. Ng and C. Cardie. 2002. Improving Machine Learning Approaches to Coref-
erence Resolution. In ACL.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak 2005. Learning and Inference
over Constrained Output. In IJCAI.
D. Roth and W. Yih. 2005. Integer Linear Programming Inference for Conditional
Random Fields. In ICML.
V. Stoyanov and C. Cardie. 2006. Partially Supervised Coreference Resolution
for Opinion Summarization through Structured Rule Learning. In EMNLP.
C. Sutton and A. McCallum. 2005. Fast, Piecewise Training for Discriminative
Finite-state and Parsing Models. In Technical Report IR-403, University of
Massachusetts.
C. Sutton and A. McCallum. 2005. Piecewise Training for Undirected Models.
In UAI.
B. Wellner, A. McCallum, F. Peng and M. Hay. 2004. An Integrated, Conditional
Model of Information Extraction and Coreference with Application to Citation
Matching. In UAI.
J. Wiebe and T. Wilson and C. Cardie 2005. Annotating Expressions of Opinions
and Emotions in Language. In Language Resources and Evaluation, volume
39, issue 2-3.
72
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 431?439,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Joint Extraction of Entities and Relations for Opinion Recognition
Yejin Choi and Eric Breck and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,ebreck,cardie}@cs.cornell.edu
Abstract
We present an approach for the joint ex-
traction of entities and relations in the con-
text of opinion recognition and analysis.
We identify two types of opinion-related
entities ? expressions of opinions and
sources of opinions ? along with the link-
ing relation that exists between them. In-
spired by Roth and Yih (2004), we employ
an integer linear programming approach
to solve the joint opinion recognition task,
and show that global, constraint-based in-
ference can significantly boost the perfor-
mance of both relation extraction and the
extraction of opinion-related entities. Per-
formance further improves when a seman-
tic role labeling system is incorporated.
The resulting system achieves F-measures
of 79 and 69 for entity and relation extrac-
tion, respectively, improving substantially
over prior results in the area.
1 Introduction
Information extraction tasks such as recognizing
entities and relations have long been considered
critical to many domain-specific NLP tasks (e.g.
Mooney and Bunescu (2005), Prager et al (2000),
White et al (2001)). Researchers have further
shown that opinion-oriented information extrac-
tion can provide analogous benefits to a variety of
practical applications including product reputation
tracking (Morinaga et al, 2002), opinion-oriented
question answering (Stoyanov et al, 2005), and
opinion-oriented summarization (e.g. Cardie et
al. (2004), Liu et al (2005)). Moreover, much
progress has been made in the area of opinion ex-
traction: it is possible to identify sources of opin-
ions (i.e. the opinion holders) (e.g. Choi et al
(2005) and Kim and Hovy (2005b)), to determine
the polarity and strength of opinion expressions
(e.g. Wilson et al (2005)), and to recognize propo-
sitional opinions and their sources (e.g. Bethard
et al (2004)) with reasonable accuracy. To date,
however, there has been no effort to simultane-
ously identify arbitrary opinion expressions, their
sources, and the relations between them. Without
progress on the joint extraction of opinion enti-
ties and their relations, the capabilities of opinion-
based applications will remain limited.
Fortunately, research in machine learning has
produced methods for global inference and joint
classification that can help to address this defi-
ciency (e.g. Bunescu and Mooney (2004), Roth
and Yih (2004)). Moreover, it has been shown that
exploiting dependencies among entities and/or re-
lations via global inference not only solves the
joint extraction task, but often boosts performance
on the individual tasks when compared to clas-
sifiers that handle the tasks independently ? for
semantic role labeling (e.g. Punyakanok et al
(2004)), information extraction (e.g. Roth and Yih
(2004)), and sequence tagging (e.g. Sutton et al
(2004)).
In this paper, we present a global inference ap-
proach (Roth and Yih, 2004) to the extraction
of opinion-related entities and relations. In par-
ticular, we aim to identify two types of entities
(i.e. spans of text): entities that express opin-
ions and entities that denote sources of opinions.
More specifically, we use the term opinion expres-
sion to denote all direct expressions of subjectiv-
ity including opinions, emotions, beliefs, senti-
ment, etc., as well as all speech expressions that
introduce subjective propositions; and use the term
source to denote the person or entity (e.g. a re-
431
port) that holds the opinion.1 In addition, we
aim to identify the relations between opinion ex-
pression entities and source entities. That is, for
a given opinion expression Oi and source entity
Sj , we determine whether the relation Li,j def=
(Sj expresses Oi) obtains, i.e. whether Sj is the
source of opinion expression Oi. We refer to this
particular relation as the link relation in the rest
of the paper. Consider, for example, the following
sentences:
S1. [Bush](1) intends(1) to curb the increase in
harmful gas emissions and is counting on(1)
the good will(2) of [US industrialists](2) .
S2. By questioning(3) [the Imam](4)?s edict(4) [the
Islamic Republic of Iran](3) made [the people
of the world](5) understand(5)...
The underlined phrases above are opinion expres-
sions and phrases marked with square brackets are
source entities. The numeric superscripts on en-
tities indicate link relations: a source entity and
an opinion expression with the same number sat-
isfy the link relation. For instance, the source en-
tity ?Bush? and the opinion expression ?intends?
satisfy the link relation, and so do ?Bush? and
?counting on.? Notice that a sentence may con-
tain more than one link relation, and link relations
are not one-to-one mappings between sources and
opinions. Also, the pair of entities in a link rela-
tion may not be the closest entities to each other, as
is the case in the second sentence, between ?ques-
tioning? and ?the Islamic Republic of Iran.?
We expect the extraction of opinion relations to
be critical for many opinion-oriented NLP appli-
cations. For instance, consider the following ques-
tion that might be given to a question-answering
system:
? What is the Imam?s opinion toward the Islamic
Republic of Iran?
Without in-depth opinion analysis, the question-
answering system might mistake example S2 as
relevant to the query, even though S2 exhibits the
opinion of the Islamic Republic of Iran toward
Imam, not the other way around.
Inspired by Roth and Yih (2004), we model
our task as global, constraint-based inference over
separately trained entity and relation classifiers.
In particular, we develop three base classifiers:
two sequence-tagging classifiers for the extraction
1See Wiebe et al (2005) for additional details.
of opinion expressions and sources, and a binary
classifier to identify the link relation. The global
inference procedure is implemented via integer
linear programming (ILP) to produce an optimal
and coherent extraction of entities and relations.
Because many (60%) opinion-source relations
appear as predicate-argument relations, where the
predicate is a verb, we also hypothesize that se-
mantic role labeling (SRL) will be very useful for
our task. We present two baseline methods for
the joint opinion-source recognition task that use
a state-of-the-art SRL system (Punyakanok et al,
2005), and describe two additional methods for in-
corporating SRL into our ILP-based system.
Our experiments show that the global inference
approach not only improves relation extraction
over the base classifier, but does the same for in-
dividual entity extractions. For source extraction
in particular, our system achieves an F-measure of
78.1, significantly outperforming previous results
in this area (Choi et al, 2005), which obtained an
F-measure of 69.4 on the same corpus. In addition,
we achieve an F-measure of 68.9 for link relation
identification and 82.0 for opinion expression ex-
traction; for the latter task, our system achieves
human-level performance.2
2 High-Level Approach and Related
Work
Our system operates in three phases.
Opinion and Source Entity Extraction We
begin by developing two separate token-level
sequence-tagging classifiers for opinion expres-
sion extraction and source extraction, using linear-
chain Conditional Random Fields (CRFs) (Laf-
ferty et al, 2001). The sequence-tagging classi-
fiers are trained using only local syntactic and lex-
ical information to extract each type of entity with-
out knowledge of any nearby or neighboring enti-
ties or relations. We collect n-best sequences from
each sequence tagger in order to boost the recall of
the final system.
Link Relation Classification We also develop
a relation classifier that is trained and tested on
all pairs of opinion and source entities extracted
from the aforementioned n-best opinion expres-
sion and source sequences. The relation classifier
is modeled using Markov order-0 CRFs(Lafferty
2Wiebe et al (2005) reports human annotation agreement
for opinion expression as 82.0 by F1 measure.
432
et al, 2001), which are equivalent to maximum en-
tropy models. It is trained using only local syntac-
tic information potentially useful for connecting a
pair of entities, but has no knowledge of nearby or
neighboring extracted entities and link relations.
Integer Linear Programming Finally, we for-
mulate an integer linear programming problem for
each sentence using the results from the previous
two phases. In particular, we specify a number
of soft and hard constraints among relations and
entities that take into account the confidence val-
ues provided by the supporting entity and relation
classifiers, and that encode a number of heuristics
to ensure coherent output. Given these constraints,
global inference via ILP finds the optimal, coher-
ent set of opinion-source pairs by exploiting mu-
tual dependencies among the entities and relations.
While good performance in entity or relation
extraction can contribute to better performance of
the final system, this is not always the case. Pun-
yakanok et al (2004) notes that, in general, it is
better to have high recall from the classifiers in-
cluded in the ILP formulation. For this reason, it is
not our goal to directly optimize the performance
of our opinion and source entity extraction models
or our relation classifier.
The rest of the paper is organized as follows.
Related work is outlined below. Section 3 de-
scribes the components of the first phase of our
system, the opinion and source extraction classi-
fiers. Section 4 describes the construction of the
link relation classifier for phase two. Section 5
describes the ILP formulation to perform global
inference over the results from the previous two
phases. Experimental results that compare our ILP
approach to a number of baselines are presented in
Section 6. Section 7 describes how SRL can be in-
corporated into our global inference system to fur-
ther improve the performance. Final experimental
results and discussion comprise Section 8.
Related Work The definition of our source-
expresses-opinion task is similar to that of Bethard
et al (2004); however, our definition of opin-
ion and source entities are much more extensive,
going beyond single sentences and propositional
opinion expressions. In particular, we evaluate
our approach with respect to (1) a wide variety
of opinion expressions, (2) explicit and implicit3
sources, (3) multiple opinion-source link relations
3Implicit sources are those that are not explicitly men-
tioned. See Section 8 for more details.
per sentence, and (4) link relations that span more
than one sentence. In addition, the link rela-
tion model explicitly exploits mutual dependen-
cies among entities and relations, while Bethard
et al (2004) does not directly capture the potential
influence among entities.
Kim and Hovy (2005b) and Choi et al (2005)
focus only on the extraction of sources of
opinions, without extracting opinion expressions.
Specifically, Kim and Hovy (2005b) assume a pri-
ori existence of the opinion expressions and ex-
tract a single source for each, while Choi et al
(2005) do not explicitly extract opinion expres-
sions nor link an opinion expression to a source
even though their model implicitly learns approxi-
mations of opinion expressions in order to identify
opinion sources. Other previous research focuses
only on the extraction of opinion expressions (e.g.
Kim and Hovy (2005a), Munson et al (2005) and
Wilson et al (2005)), omitting source identifica-
tion altogether.
There have also been previous efforts to si-
multaneously extract entities and relations by ex-
ploiting their mutual dependencies. Roth and
Yih (2002) formulated global inference using a
Bayesian network, where they captured the influ-
ence between a relation and a pair of entities via
the conditional probability of a relation, given a
pair of entities. This approach however, could not
exploit dependencies between relations. Roth and
Yih (2004) later formulated global inference using
integer linear programming, which is the approach
that we apply here. In contrast to our work, Roth
and Yih (2004) operated in the domain of factual
information extraction rather than opinion extrac-
tion, and assumed that the exact boundaries of en-
tities from the gold standard are known a priori,
which may not be available in practice.
3 Extraction of Opinion and Source
Entities
We develop two separate sequence tagging classi-
fiers for opinion extraction and source extraction,
using linear-chain Conditional Random Fields
(CRFs) (Lafferty et al, 2001). The sequence tag-
ging is encoded as the typical ?BIO? scheme.4
Each training or test instance represents a sen-
tence, encoded as a linear chain of tokens and their
4
?B? is for the token that begins an entity, ?I? is for to-
kens that are inside an entity, and ?O? is for tokens outside an
entity.
433
associated features. Our feature set is based on
that of Choi et al (2005) for source extraction5,
but we include additional lexical and WordNet-
based features. For simplicity, we use the same
features for opinion entity extraction and source
extraction, and let the CRFs learn appropriate fea-
ture weights for each task.
3.1 Entity extraction features
For each token xi, we include the following fea-
tures. For details, see Choi et al (2005).
word: words in a [-4, +4] window centered on xi.
part-of-speech: POS tags in a [-2, +2] window.6
grammatical role: grammatical role (subject, ob-
ject, prepositional phrase types) of xi derived from
a dependency parse.7
dictionary: whether xi is in the opinion expres-
sion dictionary culled from the training data and
augmented by approximately 500 opinion words
from the MPQA Final Report8. Also computed
for tokens in a [-1, +1] window and for xi?s parent
?chunk? in the dependency parse.
semantic class: xi?s semantic class.9
WordNet: the WordNet hypernym of xi.10
4 Relation Classification
We also develop a maximum entropy binary clas-
sifier for opinion-source link relation classifica-
tion. Given an opinion-source pair, Oi-Sj , the re-
lation classifier decides whether the pair exhibits
a valid link relation, Li,j . The relation classifier
focuses only on the syntactic structure and lexical
properties between the two entities of a given pair,
without knowing whether the proposed entities are
correct. Opinion and source entities are taken from
the n-best sequences of the entity extraction mod-
els; therefore, some are invariably incorrect.
From each sentence, we create training and test
instances for all possible opinion-source pairings
that do not overlap: we create an instance for Li,j
only if the span of Oi and Sj do not overlap.
For training, we also filter out instances for
which neither the proposed opinion nor source en-
5We omit only the extraction pattern features.
6Using GATE: http://gate.ac.uk/
7Provided by Rebecca Hwa, based on the Collins parser:
ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz
8https://rrc.mitre.org/pubs/mpqaFinalReport.pdf
9Using SUNDANCE: (http://www.cs.utah.edu/r?iloff/
publications.html#sundance)
10http://wordnet.princeton.edu/
tity overlaps with a correct opinion or source en-
tity per the gold standard. This training instance
filtering helps to avoid confusion between exam-
ples like the following (where entities marked in
bold are the gold standard entities, and entities
in square brackets represent the n-best output se-
quences from the entity extraction classifiers):
(1) [The president] s1 walked away from [the
meeting] o1, [ [revealing] o2 his disap-
pointment] o3 with the deal.
(2) [The monster] s2 walked away, [revealing] o4
a little box hidden underneath.
For these sentences, we construct training in-
stances for L1,1, L1,2, and L1,3, but not L2,4,
which in fact has very similar sentential structure
as L1,2, and hence could confuse the learning al-
gorithm.
4.1 Relation extraction features
The training and test instances for each (potential)
link Li,j (with opinion candidate entity Oi and
source candidate entity Sj) include the following
features.
opinion entity word: the words contained in Oi.
phrase type: the syntactic category of the con-
stituent in which the entity is embedded, e.g. NP
or VP. We encode separate features for Oi and Sj .
grammatical role: the grammatical role of the
constituent in which the entity is embedded.
Grammatical roles are derived from dependency
parse trees, as done for the entity extraction classi-
fiers. We encode separate features for Oi and Sj .
position: a boolean value indicating whether Sj
precedes Oi.
distance: the distance between Oi and Sj in num-
bers of tokens. We use four coarse categories: ad-
jacent, very near, near, far.
dependency path: the path through the depen-
dency tree from the head of Sj to the head of Oi.
For instance, ?subj?verb? or ?subj?verb?obj?.
voice: whether the voice of Oi is passive or active.
syntactic frame: key intra-sentential relations be-
tween Oi and Sj . The syntactic frames that we use
are:
? [E1:role] [distance] [E2:role], where distance
? {adjacent, very near, near, far}, and Ei:role
is the grammatical role of Ei. Either E1 is an
opinion entity and E2 is a source, or vice versa.
? [E1:phrase] [distance] [E2:phrase], where
Ei:phrase is the phrasal type of entity Ei.
434
? [E1:phrase] [E2:headword], where E2 must be
the opinion entity, and E1 must be the source en-
tity (i.e. no lexicalized frames for sources). E1
and E2 can be contiguous.
? [E1:role] [E2:headword], where E2 must be the
opinion entity, and E1 must be the source entity.
? [E1:phrase] NP [E2:phrase] indicates the
presence of specific syntactic patterns, e.g.
?VP NP VP? depending on the possible phrase
types of opinion and source entities. The three
phrases do not need to be contiguous.
? [E1:phrase] VP [E2:phrase] (See above.)
? [E1:phrase] [wh-word] [E2:phrase] (See
above.)
? Src [distance] [x] [distance] Op, where x ?
{by, of, from, for, between, among, and, have,
be, will, not, ], ?, . . . }.
When a syntactic frame is matched to a sen-
tence, the bracketed items should be instantiated
with particular values corresponding to the sen-
tence. Pattern elements without square brackets
are constants. For instance, the syntactic frame
?[E1:phrase] NP [E2:phrase]? may be instantiated
as ?VP NP VP?. Some frames are lexicalized with
respect to the head of an opinion entity to reflect
the fact that different verbs expect source enti-
ties in different argument positions (e.g. SOURCE
blamed TARGET vs. TARGET angered SOURCE).
5 Integer Linear Programming
Approach
As noted in the introduction, we model our task
as global, constraint-based inference over the sep-
arately trained entity and relation classifiers, and
implement the inference procedure as binary in-
teger linear programming (ILP) ((Roth and Yih,
2004), (Punyakanok et al, 2004)). ILP consists
of an objective function which is a dot product
between a vector of variables and a vector of
weights, and a set of equality and inequality con-
straints among variables. Given an objective func-
tion and a set of constraints, LP finds the opti-
mal assignment of values to variables, i.e. one that
minimizes the objective function. In binary ILP,
the assignments to variables must be either 0 or 1.
The variables and constraints defined for the opin-
ion recognition task are summarized in Table 1 and
explained below.
Entity variables and weights For each opinion
entity, we add two variables, Oi and O?i, where
Oi = 1 means to extract the opinion entity, and
Objective function f
=
?
i(woiOi) +
?
i(w?oiO?i)
+
?
j(wsjSj) +
?
j(w?sj S?j)
+
?
i,j(wli,jLi,j) +
?
i(w?li,j L?i,j)
?i, Oi + O?i = 1
?j, Sj + S?j = 1
?i, j, Li,j + L?i,j = 1
?i, Oi =
?
j Li,j
?j, Sj + Aj =
?
i Li,j
?j, Aj ? Sj ? 0
?i, j, i < j, Xi + Xj = 1,X ? {S,O}
Table 1: Binary ILP formulation
O?i = 1 means to discard the opinion entity. To
ensure coherent assignments, we add equality con-
straints ?i, Oi + O?i = 1. The weights woi and
w?oi for Oi and O?i respectively, are computed as
a negative conditional probability of the span of
an entity to be extracted (or suppressed) given the
labelings of the adjacent variables of the CRFs:
woi
def= ?P (xk, xk+1, ..., xl|xk?1, xl+1)
where xk = ?B?
& xm = ?I? for m ? [k + 1, l]
w?oi
def= ?P (xk, xk+1, ..., xl|xk?1, xl+1)
where xm = ?O? for m ? [k, l]
where xi is the value assigned to the random vari-
able of the CRF corresponding to an entity Oi.
Likewise, for each source entity, we add two vari-
ables Sj and S?j and a constraint Sj + S?j = 1. The
weights for source variables are computed in the
same way as opinion entities.
Relation variables and weights For each link
relation, we add two variables Li,j and L?i,j , and
a constraint Li,j + L?i,j = 1. By the definition of
a link, if Li,j = 1, then it is implied that Oi = 1
and Sj = 1. That is, if a link is extracted, then the
pair of entities for the link must be also extracted.
Constraints to ensure this coherency are explained
in the following subsection. The weights for link
variables are based on probabilities from the bi-
nary link classifier.
Constraints for link coherency In our corpus, a
source entity can be linked to more than one opin-
ion entity, but an opinion entity is linked to only
435
one source. Nonetheless, the majority of opinion-
source pairs involve one-to-one mappings, which
we encode as hard and soft constraints as follows:
For each opinion entity, we add an equality con-
straint Oi =
?
j Li,j to enforce that only one
link can emanate from an opinion entity. For each
source entity, we add an equality constraint and an
inequality constraint that together allow a source
to link to at most two opinions: Sj +Aj =
?
i Li,j
and Aj ? Sj ? 0, where Aj is an auxiliary vari-
able, such that its weight is some positive constant
value that suppresses Aj from being assigned to 1.
And Aj can be assigned to 1 only if Sj is already
assigned to 1. It is possible to add more auxiliary
variables to allow more than two opinions to link
to a source, but for our experiments two seemed to
be a reasonable limit.
Constraints for entity coherency When we use
n-best sequences where n > 1, proposed entities
can overlap. Because this should not be the case
in the final result, we add an equality constraint
Xi + Xj = 1, X ? {S,O} for all pairs of entities
with overlapping spans.
Adjustments to weights To balance the preci-
sion and recall, and to take into account the per-
formance of different base classifiers, we apply ad-
justments to weights as follows.
1) We define six coefficients cx and c?x, where
x ? {O,S,L} to modify a group of weights
as follows.
?i, x, wxi := wxi ? cx;
?i, x, w?xi := w?xi ? c?x;
In general, increasing cx will promote recall,
while increasing c?x will promote precision.
Also, setting co > cs will put higher confi-
dence on the opinion extraction classifier than
the source extraction classifier.
2) We also define one constant cA to set the
weights for auxiliary variable Ai. That is,
?i, wAi := cA.
3) Finally, we adjust the confidence of the link
variable based on n-th-best sequences of the en-
tity extraction classifiers as follows.
?i, wLi,j := wLi,j ? d
where d def= 4/(3 + min(m,n)), when Oi is
from an m-th sequence and Sj is from a n-th
sequence.11
11This will smoothly degrade the confidence of a link
based on the entities from higher n-th sequences. Values of d
decrease as 4/4, 4/5, 4/6, 4/7....
6 Experiments?I
We evaluate our system using the NRRC Multi-
Perspective Question Answering (MPQA) corpus
that contains 535 newswire articles that are man-
ually annotated for opinion-related information.
In particular, our gold standard opinion entities
correspond to direct subjective expression anno-
tations and subjective speech event annotations
(i.e. speech events that introduce opinions) in the
MPQA corpus (Wiebe et al, 2005). Gold stan-
dard source entities and link relations can be ex-
tracted from the agent attribute associated with
each opinion entity. We use 135 documents as a
development set and report 10-fold cross valida-
tion results on the remaining 400 documents in all
experiments below.
We evaluate entity and link extraction using
both an overlap and exact matching scheme.12 Be-
cause the exact start and endpoints of the man-
ual annotations are somewhat arbitrary, the over-
lap scheme is more reasonable for our task (Wiebe
et al, 2005). We report results according to both
matching schemes, but focus our discussion on re-
sults obtained using overlap matching.13
We use the Mallet14 implementation of CRFs.
For brevity, we will refer to the opinion extraction
classifier as CRF-OP, the source extraction classi-
fier as CRF-SRC, and the link relation classifier as
CRF-LINK. For ILP, we use Matlab, which pro-
duced the optimal assignment in a matter of few
seconds for each sentence. The weight adjustment
constants defined for ILP are based on the devel-
opment data.15
The link-nearest baselines For baselines, we
first consider a link-nearest heuristic: for each
opinion entity extracted by CRF-OP, the link-
nearest heuristic creates a link relation with the
closest source entity extracted by CRF-SRC. Re-
call that CRF-SRC and CRF-OP extract entities
from n-best sequences. We test the link-nearest
heuristic with n = {1, 2, 10} where larger n will
boost recall at the cost of precision. Results for the
12Given two links L1,1 = (O1, S1) and L2,2 = (O2, S2),
exact matching requires the spans of O1 and O2, and the
spans of S1 and S2, to match exactly, while overlap matching
requires the spans to overlap.
13Wiebe et al (2005) also reports the human annotation
agreement study via the overlap scheme.
14Available at http://mallet.cs.umass.edu
15co = 2.5, c?o = 1.0, cs = 1.5, c?s = 1.0, cL = 2.5, c?L =
2.5, cA = 0.2. Values are picked so as to boost recall while
reasonably suppressing incorrect links.
436
Overlap Match Exact Match
r(%) p(%) f(%) r(%) p(%) f(%)
NEAREST-1 51.6 71.4 59.9 26.2 36.9 30.7
NEAREST-2 60.7 45.8 52.2 29.7 19.0 23.1
NEAREST-10 66.3 20.9 31.7 28.2 00.0 00.0
SRL 59.7 36.3 45.2 32.6 19.3 24.2
SRL+CRF-OP 45.6 83.2 58.9 27.6 49.7 35.5
ILP-1 51.6 80.8 63.0 26.4 42.0 32.4
ILP-10 64.0 72.4 68.0 31.0 34.8 32.8
Table 2: Relation extraction performance
NEAREST-n : link-nearest heuristic w/ n-best
SRL : all V-A0 frames from SRL
SRL+CRF-OP : all V-A0 filtered by CRF-OP
ILP-n : ILP applied to n-best sequences
link-nearest heuristic on the full source-expresses-
opinion relation extraction task are shown in the
first three rows of table 2. NEAREST-1 performs
the best in overlap-match F-measure, reaching
59.9. NEAREST-10 has higher recall (66.3%), but
the precision is really low (20.9%). Performance
of the opinion and source entity classifiers will be
discussed in Section 8.
SRL baselines Next, we consider two base-
lines that use a state-of-the-art SRL system (Pun-
yakanok et al, 2005). In many link relations,
the opinion expression entity is a verb phrase and
the source entity is in an agent argument posi-
tion. Hence our second baseline, SRL, extracts
all verb(V)-agent(A0) frames from the output of
the SRL system and provides an upper bound on
recall (59.7%) for systems that use SRL in isola-
tion for our task. A more sophisticated baseline,
SRL+CRF-OP, extracts only those V-A0 frames
whose verb overlaps with entities extracted by the
opinion expression extractor, CRF-OP. As shown
in table 2, filtering out V-A0 frames that are in-
compatible with the opinion extractor boosts pre-
cision to 83.2%, but the F-measure (58.9) is lower
than that of NEAREST-1.
ILP results The ILP-n system in table 2 de-
notes the results of the ILP approach applied to the
n-best sequences. ILP-10 reaches an F-measure
of 68.0, a significant improvement over the high-
est performing baseline16 , and also a substantial
improvement over ILP-1. Note that the perfor-
mance of NEAREST-10 was much worse than that
16Statistically significant by paired-t test, where p <
0.001.
Overlap Match Exact Match
r(%) p(%) f(%) r(%) p(%) f(%)
ILP-1 51.6 80.8 63.0 26.4 42.0 32.4
ILP-10 64.0 72.4 68.0 31.0 34.8 32.8
ILP+SRL-f -1 51.7 81.5 63.3 26.6 42.5 32.7
ILP+SRL-f -10 65.7 72.4 68.9 31.5 34.3 32.9
ILP+SRL-fc-10 64.0 73.5 68.4 28.4 31.3 29.8
Table 3: Relation extraction with ILP and SRL
ILP-n : ILP applied to n-best sequences
ILP+SRL-f -n : ILP w/ SRL features, n-best
ILP+SRL-fc-n : ILP w/ SRL features,
and SRL constraints, n-best
of NEAREST-1, because the 10-best sequences in-
clude many incorrect entities whereas the corre-
sponding ILP formulation can discard the bad en-
tities by considering dependencies among entities
and relations.17
7 Additional SRL Incorporation
We next explore two approaches for more directly
incorporating SRL into our system.
Extra SRL Features for the Link classifier We
incorporate SRL into the link classifier by adding
extra features based on SRL. We add boolean fea-
tures to check whether the span of an SRL argu-
ment and an entity matches exactly. In addition,
we include syntactic frame features as follows:
? [E1:srl-arg] [E2:srl-arg], where Ei:srl-arg indi-
cates the SRL argument type of entity Ei.
? [E1.srl-arg] [E1:headword] [E2:srl-arg], where
E1 must be an opinion entity, and E2 must be a
source entity.
Extra SRL Constraints for the ILP phase We
also incorporate SRL into the ILP phase of our
system by adding extra constraints based on SRL.
In particular, we assign very high weights for links
that match V-A0 frames generated by SRL, in or-
der to force the extraction of V-A0 frames.
17A potential issue with overlap precision and recall is that
the measures may drastically overestimate the system?s per-
formance as follows: a system predicting a single link rela-
tion whose source and opinion expression both overlap with
every token of a document would achieve 100% overlap pre-
cision and recall. We can ensure this does not happen by mea-
suring the average number of (source, opinion) pairs to which
each correct or predicted pair is aligned (excluding pairs not
aligned at all). In our data, this does not exceed 1.08, (except
for baselines), so we can conclude these evaluation measures
are behaving reasonably.
437
Opinion Source Link
r(%) p(%) f(%) r(%) p(%) f(%) r(%) p(%) f(%)
Before ILP CRF-OP/SRC/LINK with 1 best 76.4 88.4 81.9 67.3 81.9 73.9 60.5 50.5 55.0
merged 10 best 95.7 31.2 47.0 95.3 24.5 38.9 N/A
After ILP ILP-SRL-f -10 75.1 82.9 78.8 80.6 75.7 78.1 65.7 72.4 68.9
ILP-SRL-f -10 ? CRF-OP/SRC with 1 best 82.3 81.7 82.0 81.5 73.4 77.3 N/A
Table 4: Entity extraction performance (by overlap-matching)
8 Experiments?II
Results using SRL are shown in Table 3 (on the
previous page). In the table, ILP+SRL-f denotes
the ILP approach using the link classifier with
the extra SRL ?f ?eatures, and ILP+SRL-fc de-
notes the ILP approach using both the extra SRL
?f ?eatures and the SRL ?c?onstraints. For compar-
ison, the ILP-1 and ILP-10 results from Table 2
are shown in rows 1 and 2.
The F-measure score of ILP+SRL-f -10 is 68.9,
about a 1 point increase from that of ILP-10,
which shows that extra SRL features for the link
classifier further improve the performance over
our previous best results.18 ILP+SRL-fc-10 also
performs better than ILP-10 in F-measure, al-
though it is slightly worse than ILP+SRL-f -10.
This indicates that the link classifier with extra
SRL features already makes good use of the V-A0
frames from the SRL system, so that forcing the
extraction of such frames via extra ILP constraints
only hurts performance by not allowing the extrac-
tion of non-V-A0 pairs in the neighborhood that
could have been better choices.
Contribution of the ILP phase In order to
highlight the contribution of the ILP phase for our
task, we present ?before? and ?after? performance
in Table 4. The first row shows the performance
of the individual CRF-OP, CRF-SRC, and CRF-
LINK classifiers before the ILP phase. Without the
ILP phase, the 1-best sequence generates the best
scores. However, we also present the performance
with merged 10-best entity sequences19 in order
to demonstrate that using 10-best sequences with-
out ILP will only hurt performance. The precision
of the merged 10-best sequences system is very
low, however the recall level is above 95% for both
18Statistically significant by paired-t test, where p <
0.001.
19If an entity Ei extracted by the ith-best sequence over-
laps with an entity Ej extracted by the jth-best sequence,
where i < j, then we discard Ej . If Ei and Ej do not over-
lap, then we extract both entities.
CRF-OP and CRF-SRC, giving an upper bound for
recall for our approach. The third row presents
results after the ILP phase is applied for the 10-
best sequences, and we see that, in addition to the
improved link extraction described in Section 7,
the performance on source extraction is substan-
tially improved, from F-measure of 73.9 to 78.1.
Performance on opinion expression extraction de-
creases from F-measure of 81.9 to 78.8. This de-
crease is largely due to implicit links, which we
will explain below. The fourth row takes the union
of the entities from ILP-SRL-f -10 and the entities
from the best sequences from CRF-OP and CRF-
SRC. This process brings the F-measure of CRF-
OP up to 82.0, with a different precision-recall
break down from those of 1-best sequences with-
out ILP phase. In particular, the recall on opinion
expressions now reaches 82.3%, while maintain-
ing a high precision of 81.7%.
Overlap Match Exact Match
r(%) p(%) f(%) r(%) p(%) f(%)
DEV.CONF 65.7 72.4 68.9 31.5 34.3 32.9
NO.CONF 63.7 76.2 69.4 30.9 36.7 33.5
Table 5: Relation extraction with ILP weight ad-
justment. (All cases using ILP+SRL-f -10)
Effects of ILP weight adjustment Finally, we
show the effect of weight adjustment in the ILP
formulation in Table 5. The DEV.CONF row shows
relation extraction performance using a weight
configuration based from the development data.
In order to see the effect of weight adjustment,
we ran an experiment, NO.CONF, using fixed de-
fault weights.20 Not surprisingly, our weight ad-
justment tuned from the development set is not the
optimal choice for cross-validation set. Neverthe-
less, the weight adjustment helps to balance the
precision and recall, i.e. it improves recall at the
20To be precise, cx = 1.0, c?x = 1.0 for x ? {O, S, L},
but cA = 0.2 is the same as before.
438
cost of precision. The weight adjustment is more
effective when the gap between precision and re-
call is large, as was the case with the development
data.
Implicit links A good portion of errors stem
from the implicit link relation, which our system
did not model directly. An implicit link relation
holds for an opinion entity without an associated
source entity. In this case, the opinion entity is
linked to an implicit source. Consider the follow-
ing example.
? Anti-Soviet hysteria was firmly oppressed.
Notice that opinion expressions such as ?Anti-
Soviet hysteria? and ?firmly oppressed? do not
have associated source entities, because sources of
these opinion expressions are not explicitly men-
tioned in the text. Because our system forces
each opinion to be linked with an explicit source
entity, opinion expressions that do not have ex-
plicit source entities will be dropped during the
global inference phase of our system. Implicit
links amount to 7% of the link relations in our
corpus, so the upper bound for recall for our ILP
system is 93%. In the future we will extend our
system to handle implicit links as well. Note that
we report results against a gold standard that in-
cludes implicit links. Excluding them from the
gold standard, the performance of our final sys-
tem ILP+SRL-f -10 is 72.6% in recall, 72.4% in
precision, and 72.5 in F-measure.
9 Conclusion
This paper presented a global inference approach
to jointly extract entities and relations in the con-
text of opinion oriented information extraction.
The final system achieves performance levels that
are potentially good enough for many practical
NLP applications.
Acknowledgments We thank the reviewers for their
many helpful comments and Vasin Punyakanok for running
our data through his SRL system. This work was sup-
ported by the Advanced Research and Development Activity
(ARDA), by NSF Grants IIS-0535099 and IIS-0208028, and
by gifts from Google and the Xerox Foundation.
References
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou and
D. Jurafsky 2004. Automatic Extraction of Opin-
ion Propositions and their Holders. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text.
R. Bunescu and R. J. Mooney 2004. Collective In-
formation Extraction with Relational Markov Net-
works. In ACL.
C. Cardie, J. Wiebe, T. Wilson and D. Litman 2004.
Low-Level Annotations and Summary Representa-
tions of Opinions for Multi-Perspective Question
Answering. New Directions in Question Answering.
Y. Choi, C. Cardie, E. Riloff and S. Patwardhan 2005.
Identifying Sources of Opinions with Conditional
Random Fields and Extraction Patterns. In HLT-
EMNLP.
S. Kim and E. Hovy 2005. Automatic Detection of
Opinion Bearing Words and Sentences. In IJCNLP.
S. Kim and E. Hovy 2005. Identifying Opinion
Holders for Question Answering in Opinion Texts.
In AAAI Workshop on Question Answering in Re-
stricted Domains.
J. Lafferty, A. K. McCallum and F. Pereira 2001 Con-
ditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In ICML.
B. Liu, M. Hu and J. Cheng 2005 Opinion Observer:
Analyzing and Comparing Opinions on the Web. In
WWW.
R. J. Mooney and R. Bunescu 2005 Mining Knowl-
edge from Text Using Information Extraction. In
SIGKDD Explorations.
S. Morinaga, K. Yamanishi, K. Tateishi and T.
Fukushima 2002. Mining product reputations on
the Web. In KDD.
M. A. Munson, C. Cardie and R. Caruana. 2005. Opti-
mizing to arbitrary NLP metrics using ensemble se-
lection. In HLT-EMNLP.
J. Prager, E. Brown, A. Coden and D. Radev 2000.
Question-answering by predictive annotation. In SI-
GIR.
V. Punyakanok, D. Roth and W. Yih 2005. General-
ized Inference with Multiple Semantic Role Label-
ing Systems (Shared Task Paper). In CoNLL.
V. Punyakanok, D. Roth, W. Yih and D. Zimak 2004.
Semantic Role Labeling via Integer Linear Program-
ming Inference. In COLING.
D. Roth and W. Yih 2004. A Linear Programming For-
mulation for Global Inference in Natural Language
Tasks. In CoNLL.
D. Roth and W. Yih 2002. Probabilistic Reasoning for
Entity and Relation Recognition. In COLING.
V. Stoyanov, C. Cardie and J. Wiebe 2005. Multi-
Perspective Question Answering Using the OpQA
Corpus. In HLT-EMNLP.
C. Sutton, K. Rohanimanesh and A. K. McCallum
2004. Dynamic Conditional Random Fields: Fac-
torized Probabilistic Models for Labeling and Seg-
menting Sequence Data. In ICML.
M. White, T. Korelsky, C. Cardie, V. Ng, D. Pierce and
K. Wagstaff 2001. Multi-document Summarization
via Information Extraction In HLT.
J. Wiebe and T. Wilson and C. Cardie 2005. Annotat-
ing Expressions of Opinions and Emotions in Lan-
guage. In Language Resources and Evaluation, vol-
ume 39, issue 2-3.
T. Wilson, J. Wiebe and P. Hoffmann 2005. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment
Analysis. In HLT-EMNLP.
439
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1092?1103,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning General Connotation of Words using Graph-based Algorithms
Song Feng Ritwik Bose Yejin Choi
Department of Computer Science
Stony Brook University
NY 11794, USA
songfeng, rbose, ychoi@cs.stonybrook.edu
Abstract
In this paper, we introduce a connotation lex-
icon, a new type of lexicon that lists words
with connotative polarity, i.e., words with pos-
itive connotation (e.g., award, promotion) and
words with negative connotation (e.g., cancer,
war). Connotation lexicons differ from much
studied sentiment lexicons: the latter concerns
words that express sentiment, while the former
concerns words that evoke or associate with
a specific polarity of sentiment. Understand-
ing the connotation of words would seem to
require common sense and world knowledge.
However, we demonstrate that much of the
connotative polarity of words can be inferred
from natural language text in a nearly unsu-
pervised manner. The key linguistic insight
behind our approach is selectional preference
of connotative predicates. We present graph-
based algorithms using PageRank and HITS
that collectively learn connotation lexicon to-
gether with connotative predicates. Our em-
pirical study demonstrates that the resulting
connotation lexicon is of great value for sen-
timent analysis complementing existing senti-
ment lexicons.
1 Introduction
In this paper, we introduce a connotation lexicon,
a new type of lexicon that lists words with conno-
tative polarity, i.e., words with positive connotation
(e.g., award, promotion) and words with negative
connotation (e.g., cancer, war). Connotation lexi-
cons differ from sentiment lexicons that are studied
in much of previous research (e.g., Esuli and Sebas-
tiani (2006), Wilson et al (2005a)): the latter con-
cerns words that express sentiment either explicitly
or implicitly, while the former concerns words that
evoke or even simply associate with a specific polar-
ity of sentiment. To our knowledge, there has been
no previous research that investigates polarized con-
notation lexicons.
Understanding the connotation of words would
seem to require common sense and world knowl-
edge at first glance, which in turn might seem to re-
quire human encoding of knowledge base. However,
we demonstrate that much of the connotative polar-
ity of words can be inferred from natural language
text in a nearly unsupervised manner.
The key linguistic insight behind our approach is
selectional preference of connotative predicates. We
define a connotative predicate as a predicate that
has selectional preference on the connotative polar-
ity of some of its semantic arguments. For instance,
in the case of the connotative predicate ?prevent?,
there is strong selectional preference on negative
connotation with respect to the thematic role (se-
mantic role) ?THEME?. That is, statistically speak-
ing, people tend to associate negative connotation
with the THEME of ?prevent?, e.g., ?prevent can-
cer? or ?prevent war?, rather than positive conno-
tation, e.g., ?prevent promotion?. In other words,
even though it is perfectly valid to use words with
positive connotation in the THEME role of ?pre-
vent?, statistically more dominant connotative po-
larity is negative. Similarly, the THEME of ?con-
gratulate? or ?praise? has strong selectional prefer-
ence on positive connotation.
The theoretical concept supporting the selective
1092
accomplish, achieve, advance, advocate, admire,
applaud, appreciate, compliment, congratulate,
develop, desire, enhance, enjoy, improve, praise,
promote, respect, save, support, win
Table 1: Positively Connotative Predicates w.r.t. THEME
alleviate, accuse, avert, avoid, cause, complain,
condemn, criticize, detect, eliminate, eradicate,
mitigate, overcome, prevent, prohibit, protest, re-
frain, suffer, tolerate, withstand
Table 2: Negatively Connotative Predicates w.r.t. THEME
preference of connotative predicates is that of se-
mantic prosody in corpus linguistics. Semantic
prosody describes how some of the seemingly neu-
tral words (e.g., ?cause?) can be perceived with pos-
itive or negative polarity because they tend to col-
locate with words with corresponding polarity (e.g.,
Sinclair (1991), Louw et al (1993), Stubbs (1995),
Stefanowitsch and Gries (2003)). In this work, we
demonstrate that statistical approaches that exploit
this very concept of semantic prosody can success-
fully infer connotative polarity of words.
Having described the key linguistic insight, we
now illustrate our graph-based algorithms. Figure 1
depicts the mutually reinforcing relation between
connotative predicates (nodes on the left-hand side)
and words with connotative polarity (node on the
right-hand side). The thickness of edges represents
the strength of the association between predicates
and arguments. For brevity, we only consider conno-
tation of words that appear in the THEME thematic
role.
We expect that words that appear often in the
THEME role of various positively (or negatively)
connotative predicates are likely to be words with
positive (or negative) connotation. Likewise, pred-
icates whose THEME contains words with mostly
positive (or negative) connotation are likely to be
positively (or negatively) connotative predicates. In
short, we can induce the connotative polarity of
words using connotative predicates, and inversely,
we can learn new connotative predicates based on
words with connotative polarity.
We hypothesize that this mutually reinforcing re-
Prevent 
Avoid 
Alleviate Cancer 
Incident 
Promotion 
Overcome Tragedy 
Figure 1: Bipartite graph of connotative predicates and
arguments. Edge weights are proportionate to the associ-
ation strength.
lation between connotative predicates and their ar-
guments can be captured via graph centrality in
graph-based algorithms. Given a small set of seed
words for connotative predicates, our algorithms
collectively learn connotation lexicon together with
connotative predicates in a nearly unsupervised
manner. A number of different graph representa-
tions are explored using both PageRank (Page et al,
1999) and HITS (Kleinberg, 1999) algorithms. Em-
pirical study demonstrates that our graph based al-
gorithms are highly effective in learning both con-
notation lexicon and connotative predicates.
Finally, we quantify the practical value of our
connotation lexicon in concrete sentiment analysis
applications, and demonstrate that the connotation
lexicon is of great value for sentiment classification
tasks complementing conventional sentiment lexi-
cons.
2 Connotation Lexicon & Connotative
Predicate
In this section, we define connotation lexicon and
connotative predicates more formally, and contrast
them against words in conventional sentiment lexi-
cons.
2.1 Connotation Lexicon
This lexicon lists words with positive and negative
connotation, as defined below.
? Words with positive connotation: In this
work, we define words with positive connota-
tion as those that describe physical objects or
abstract concepts that people generally value,
cherish or care about. For instance, we regard
words such as ?freedom?, ?life?, or ?health? as
1093
words with positive connotation. Some of these
words may express subjectivity either explic-
itly or implicitly, e.g., ?joy? or ?satisfaction?.
However, a substantial number of words with
positive connotation are purely objective, such
as ?life?, ?health?, ?tenure?, or ?scientific?.
? Words with negative connotation: We define
words with negative connotation as those that
describe physical objects or abstract concepts
that people generally disvalue or avoid. Sim-
ilarly as before, some of these words may ex-
press subjectivity (e.g., ?disappointment?, ?hu-
miliation?), while many other are purely objec-
tive (e.g., ?bedbug?, ?arthritis, ?funeral?).
Note that this explicit and intentional inclusion of
objective terms makes connotation lexicons differ
from sentiment lexicons: most conventional senti-
ment lexicons have focused on subjective words by
definition (e.g., Wilson et al (2005b)), as many re-
searchers use the term sentiment and subjectivity in-
terchangeably (e.g., Wiebe et al (2005)).
2.2 Connotative Predicate
In this work, connotative predicates are those that
exhibit selectional preference on the connotative po-
larity of some of their arguments. We emphasize that
the polarity of connotative predicates does not coin-
cide with the polarity of sentiment in conventional
sentiment lexicons, as will be elaborated below.
? Positively connotative predicate: In this
work, we define positively connotative predi-
cates as those that expect positive connotation
in some arguments. For example, ?congratu-
late? or ?save? are positively connotative pred-
icates that expect words with positive conno-
tation in the THEME argument: people typi-
cally congratulate something positive, and save
something people care about. More examples
are shown in Table 1.
? Negatively connotative predicate: In this
work, we define negatively connotative predi-
cates as those that expect negative connotation
in some arguments. For instance, predicates
such as ?prevent? or ?suffer? tend to project
negative connotation in the THEME argument.
More examples are shown in Table 2.
Note that positively connotative predicates are not
necessarily positive sentiment words. For instance
?save? is not a positive sentiment word in the
lexicon published by Wilson et al (2005b). In-
versely, (strongly) positive sentiment words are not
necessarily (strongly) positively connotative predi-
cates, e.g., ?illuminate?, ?agree?. Likewise, neg-
atively connotative predicates are not necessarily
negative sentiment words. For instance, predicates
such as ?prevent?, ?detect?, or ?cause? are not
negative sentiment words, but they tend to corre-
late with negative connotation in the THEME argu-
ment. Inversely, (strongly) negative sentiment words
are not necessarily (strongly) negatively connotative
predicates, e.g., ?abandon? (?abandoned [something
valuable]?).
3 Graph Representation
In this section, we explore the graphical representa-
tion of our task. Figure 1 depicts the key intuition as
a bipartite graph, where the nodes on the left-hand
side correspond to connotative predicates, and the
nodes on the right-hand side correspond to words in
the THEME argument. There is an edge between a
predicate p and an argument a, if the argument a
appears in the THEME role of the predicate p. For
brevity, we explore only verbs as the predicates, and
words in the THEME role of the predicates as argu-
ments. Our work can be readily extended to exploit
other predicate-argument relations however.
Note that there are many sources of noise in the
construction of graph. For instance, some of the
predicates might be negated, changing the semantic
dynamics between the predicate and the argument.
In addition, there might be many unusual combina-
tions of predicates and arguments, either due to data
processing errors or due to idiosyncratic use of lan-
guage. Some of such combinations can be valid ones
(e.g., ?prevent promotion?), challenging the learning
algorithm with confusing evidence.
We hypothesize that by focusing on the important
part of the graph via centrality analysis , it is possible
to infer connotative polarity of words despite various
noise introduced in the graph structure. This implies
that it is important to construct the graph structure so
as to capture important linguistic relations between
predicates and arguments. With this goal in mind,
1094
we next explore the directionality of the edges and
different strategies to assign weights to them.
3.1 Undirected (Symmetric) Graph
First we explore undirected edges. In this case,
we assign weight for each undirected edge between
a predicate p and an argument a. Intuitively, the
weight should correspond to the strength of relat-
edness or association between the predicate p and
the argument a. We use Pointwise Mutual Infor-
mation (PMI), as it has been used by many pre-
vious research to quantify the association between
two words (e.g., Turney (2001), Church and Hanks
(1990)). The PMI score between p and a is defined
as follows:
w(p? a) := PMI(p, a) = log P (p, a)P (p)P (a)
The log of the ratio is positive when the pair of
words tends to co-occur and negative when the pres-
ence of one word correlates with the absence of the
other word.
3.2 Directed (Asymmetric) Graph
Next we explore directed edges. That is, for each
connected pair of a predicate p and an argument a,
there are two edges in opposite directions: e(p? a)
and e(a ? p). In this case, we explore the use
of asymmetric weights using conditional probabil-
ity. In particular, we define weights as follows:
w(p? a) := P (a|p) = P (p, a)P (p)
w(a? p) := P (p|a) = P (p, a)P (a)
Having defined the graph structure, next we explore
algorithms that analyze graph centrality via random
walks. In particular, we investigate the use of HITS
algorithm (Section 4), and PageRank (Section 5).
4 Lexicon Induction using HITS
The graph representation described thus far (Sec-
tion 3) captures general semantic relations between
predicates and arguments, rather than those specific
to connotative predicates and arguments. Therefore
in this section, we explore techniques to augment
the graph representation so as to bias the centrality
of the graph toward connotative predicates and argu-
ments.
In order to establish a learning bias, we start with
a small set of seed words for just connotative predi-
cates. We use 20 words for each polarity, as listed in
Table 1 and Table 2. These seed words act as prior
knowledge in our learning. We explore two different
techniques to incorporate prior knowledge into ran-
dom walk, as will be elaborated in Section 4.2 & 4.3,
followed by brief description of HITS in Section 4.1.
4.1 Hyperlink-Induced Topic Search (HITS)
HITS (Hyperlink-Induced Topic Search) algorithm
(Kleinberg, 1999), also known as Hubs and author-
ities, is a link analysis algorithm that is particularly
suitable to model mutual reinforcement between two
different types of nodes: hubs and authorities. The
definitions of hubs and authorities are given recur-
sively. A (good) hub is a node that points to many
(good) authorities, and a (good) authority is a node
pointed by many (good) hubs.
Notice that the mutually reinforcing relation-
ship is precisely what we intend to model between
connotative predicates and arguments. Let G =
(P,A,E) be the bipartite graph, where P is the set
of nodes corresponding to connotative predicates, A
is the set of nodes corresponding to arguments, and
E is the set of edges among nodes. (Pi, Aj) ? E
if and only if the predicate Pi and the argument Ai
occur together as a predicate ? argument pair in the
corpus. The co-occurrence matrix derived from our
corpus is denoted as L, where
Lij =
{
w(i, j) if(Pi, Aj) ? E
0 otherwise
The value of w(i, j) is set to w(i ? j) as defined
in Section 3.1 for undirected graphs, and w(i ? j)
defined in Section 3.2 for directed graphs.
Let a(Ai) and h(Ai) be the authority and hub
score respectively, for a given node Ai ? A. Then
we compute the authority and hub score recursively
as follows:
a(Ai) =
?
Pi,Aj?E
w(i, j)h(Aj) +
?
Pj ,Ai?E
h(Pj)w(j, i)
h(Ai) =
?
Pi,Aj?E
w(i, j)a(Aj) +
?
Pj ,Ai?E
a(Pj)w(j, i)
1095
The scores a(Pi) and h(Pi) for Pi ? P are defined
similarly as above.
In what follows, we describe two different tech-
niques to incorporate prior knowledge. Note that it
is possible to apply each of the following techniques
to both directed and undirected graph representa-
tions introduced in Section 3. Also note that for each
technique, we construct two separate graphsG+ and
G? corresponding to positive and negative polarity
respectively. That is, G+ learns positively connota-
tive predicates and arguments, while G? learns neg-
atively connotative predicates and arguments.
4.2 Prior Knowledge via Truncated Graph
First we introduce a method based on graph trunca-
tion. In this method, when constructing the bipartite
graph, we limit the set of predicates P to only those
words in the seed set, instead of including all words
that can be predicates. In a way, the truncated graph
representation can be viewed as the query induced
graph on which the original HITS algorithm was in-
vented (Kleinberg, 1999).
The truncated graph is very effective in reducing
the level of noise that can be introduced by predi-
cates of the opposite polarity. It may seem like we
cannot discover new connotative predicates in the
truncated graph however, as the graph structure is
limited only to the seed predicates. We address this
issue by alternating truncation to different side of the
graph, i.e., left (predicates) or right (arguments), as
illustrated in Figure 1, through multiple rounds of
HITS.
For instance, we start with the graph G =
(P o, A,E(P o)) that is truncated only on the left-
hand side, with the seed predicates P o. Here,E(P o)
denotes the reduced set of edges discarding those
edges that connect to predicates not in P o. Then, we
apply HITS algorithm until convergence to discover
new words with connotation, and this completes the
first round of HITS.
Next we begin the second round. Let Ao be the
new words with connotation that are found in the
first round. We now set Ao as seed words for the
second phase of HITS, where we construct a new
graph G = (P,Ao, E(Ao)) that is truncated only
on the right-hand side, with full candidate words for
predicates included on the left-hand side. This al-
ternation can be repeated multiple times to discover
many new connotative predicates and arguments.
4.3 Prior Knowledge via Focussed Graph
In the truncated graph described above, one poten-
tial concern is that the discovery of new words with
connotation is limited to those that happen to corre-
late well with the seed predicates. To mitigate this
problem, we explore an alternative technique based
on the full graph, which we will name as focussed
graph.
In this method, instead of truncating the graph, we
simply emphasize the important portion of the graph
via edge weights. That is, we assign high weights to
those edges that connect a seed predicate with an ar-
gument, while assigning low weights for those edges
that connect to a predicate outside the seed set. This
way, we allow predicates not in the seed set to par-
ticipate in hubs and authority scores, but in a much
suppressed way. This method can be interpreted as
a smoothed version of the truncated graph described
in Section 4.2.
More formally, if the node Ai is connected to
the seed predicate Pj , the value of co-occurrence
matrix Lij is defined by prior knowledge(e.g.
PMI(Ai, Pj) or P (Ai|Pj) ), otherwise a small con-
stant  is assigned to the edge.
Lij =
{
w(i, j) ifPj ? Eo
 otherwise
Similarly to the truncated graph, we proceed with
multiple rounds of HITS, focusing different part of
the bipartite graph alternately.
5 Lexicon Induction using PageRank
In this section, we explore the use of another popu-
lar approach for link analysis: PageRank (Page et
al., 1999). We first describe PageRank algorithm
briefly in Section 5.1, then introduce two different
techniques to incorporate prior knowledge in Sec-
tion 5.2 and 5.3.
5.1 PageRank
Let G = (V,E) be the graph, where vi ? V =
P ? A are nodes (words) for the disjunctive set of
predicates (P ) and arguments (A), and e(i,j) ? E
are edges. Let In(i) be the set of nodes with an
edge leading to ni and similarly, Out(i) be the set
1096
of nodes that ni has an edge leading to. At a given
iteration of the algorithm, we update the score of ni
as follows:
S(i) = ?
?
j?In(i)
S(j)? w(i, j)|Out(i)| + (1? ?) (1)
where the value ? is constant damping factor. The
value of ? is typically set to 0.85. The value of
w(i, j) is set to w(i?j) as defined in Section 3.1 for
undirected graphs, and w(i ? j) as defined in Sec-
tion 3.2 for directed graphs. As before, we will con-
sider two different techniques to incorporate prior
knowledge into the graph analysis as follows.
5.2 Prior Knowledge via Truncated Graph
Unlike HITS, which was originally invented for a
query-induced graph, PageRank is typically applied
to the full graph. However, we can still apply the
truncation technique introduced in Section 4.2 to
PageRank as well. To do so, when constructing the
bipartite graph, we limit the set of predicates P to
only those words in the seed set, instead of including
all words that can be predicates. Graph truncation
eliminates the noise that can be introduced by pred-
icates of the opposite polarity. However, in order to
learn new predicates, we need to perform multiple
rounds of PageRank, truncating different side of the
bipartite graph alternately. Refer to Section 4.2 for
futher details.
5.3 Prior Knowledge via Teleportation
We next explore what is known as teleportation
technique for topic sensitive PageRank (Haveliwala,
2002). For this, we use the following equation that
is slightly augmented from Equation 1.
S(i) = ?
?
j?In(i)
S(j)? w(i, j)|Out(i)| + (1? ?) i (2)
Here, the new term i is a smoothing factor that pre-
vents cliques in the graph from garnering reputation
through feedback (Bianchini et al (2005)). In or-
der to emphasize important portion of the graph, i.e.,
subgraphs connected to the seed set, we assign non-
zero  scores to only those important nodes, i.e., the
seed set. Intuitively, this will cause the random walk
to restart from the seed set with (1??) = 0.15 prob-
ability for each step.
6 The Use of Google Web 1T Data
In order to implement the network of connotative
predicates and arguments, we need a substantially
large amount of documents. The quality of the co-
occurrence statistics is expected to be proportionate
to the size of corpus, but collecting and process-
ing such a large amount of data is not trivial. We
therefore resort to the Google Web 1T data (Brants
and Franz., 2006), which consists of Google n-gram
counts (frequency of occurrence of each n-gram) for
1 ? n ? 5. The use of Web 1T data will lessen the
challenge with respect to data acquisition, while still
allowing us to enjoy the co-occurrence statistics of
web-scale data. Because Web 1T data is just n-gram
statistics, rather than a collection of normal docu-
ments, it does not provide co-occurrence statistics of
any random word pairs. However, it provides a nice
approximation to the particular co-occurrence statis-
tics we are interested in, which are, predicate ? ar-
gument pairs. This is because the THEME argument
of a verb predicate is typically on the right hand side
of the predicate, and the argument is within the close
range of the predicate.
We now describe how to derive co-occurrence
statistics of each predicate ? argument pair using the
Web 1T data. For a given predicate p and an argu-
ment a, we add up the count (frequency) of all n-
grams (2 ? n ? 5) that match the following pattern:
[p] [?]n?2 [a]
where p must be the first word (head), a must be the
last word (tail), and [?]n?2 matches any n? 2 num-
ber of words between p and a. Note that this rule
enforces the argument a to be on the right hand side
of the predicate p. To reduce the level of noise, we
do not allow the wildcard [?] to match any punctu-
ation mark, as such n-grams are likely to cross sen-
tence boundaries representing invalid predicate ? ar-
gument relations. We consider a word as a predicate
if it is tagged as a verb by a Part-of-Speech tagger
(Toutanova and Manning, 2000). For argument [a],
we only consider content-words.
The use of web n-gram statistics necessarily in-
vites certain kinds of noise. For instance, some of
the [p] [?]n?2 [a] patterns might not correspond to
a valid predicate ? argument relation. However, we
expect that our graph-based algorithms ? HITS and
1097
Lexicon FREQ HITS-sT HITS-aT HITS-sF HITS-aF Page-aT Page-aF
Top 100 73.6 67.8 77.7 67.8 48.4 76.3 77.0
Top 1000 67.8 60.6 68.8 60.6 38.0 68.4 68.5
Top MAX 65.8 57.6 66.5 57.6 39.1 65.5 65.7
Table 3: Comparison Result with General Inquirer Lexicon(%)
Lexicon FREQ HITS-sT HITS-aT HITS-sF HITS-aF Page-aT Page-aF
Top 100 83.0 79.3 86.3 79.3 55.8 86.3 87.2
Top 1000 80.3 67.3 81.3 67.3 46.5 80.7 80.3
Top MAX 71.5 62.7 72.2 62.7 45.4 71.1 72.3
Table 4: Comparison Result with OpinionFinder (%)
PageRank ? will be able to discern valid relations
from noise, by focusing on the important part of the
graph. In other words, we expect that good predi-
cates will be supported by good arguments, and vice
versa, thereby resulting in a reliable set of predicates
and arguments that are mutually supported by each
other.
7 Experiments
As a baseline, we use a simple method dubbed
FREQ, which uses co-occurrence frequency with
respect to the seed predicates. Using the pattern
[p] [?]n?2 [a] (see Section 6), we collect two sets
of n-gram records: one set using the positive con-
notative predicates, and the other using the negative
connotative predicates. With respect to each set, we
calculate the following for each word a,
? Given [a], the number of unique [p] as f1
? Given [a], the number of unique phrases [?]n?2
as f2
? The number of occurrences of [a] as f3
We then obtain the score ?a+ for positive connota-
tion and ?a? for negative connotation using the fol-
lowing equations that take a linear combination of
f1, f2, and f3 that we computed above with respect
to each polarity.
?a+ = ?? ?f1+ + ? ? ?f2+ + ? ? ?f3+ (3)
?a? = ?? ?f1? + ? ? ?f2? + ? ? ?f3? (4)
Note that the coefficients ?, ? and ? are determined
experimentally. We assign positive polarity to the
word a, if ?a+ >> ?a? and vice versa.
7.1 Comparison against Sentiment Lexicon
The polarity defined in the connotation lexicon dif-
fers from that of conventional sentiment lexicons in
which we aim to recognize more subtle sentiment
that correlates with words. Nevertheless, we provide
agreement statistics between our connotation lexi-
con and conventional sentiment lexicons for com-
parison purposes. We collect statistics with respect
to the following two resources: General Inquirer
(Stone and Hunt, 1963) and Opinion Finder (Wilson
et al, 2005b).
For polarity ? ? {+,?}, let countsentlex(?) denote
the total number of words labeled as ? in a given
sentiment lexicon, and let countagreement(?) denote
the total number of words labeled as ? by both the
given sentiment lexicon and our connotation lexi-
con. In addition, let countoverlap(?) denote the total
number of words that are labeled as ? by our conno-
tation lexicon that are also included in the reference
lexicon with or without the same polarity. Then we
compute prec? as follows:
prec? % =
countagreement(?)
countoverlap(?)
? 100
We compare prec? % for three different segments
of our lexicon: the top 100, top 1000, and the entire
lexicon. We compare the lexicons provided by the
seven variations of our algorithm. Results are shown
in Table 3 & 4.
The acronym of each different method is defined
as follows: HITS-sT & HITS-aT correspond to
the Symmetric (undirected) and Asymmetric (di-
rected) version of the Truncated method respec-
tively. HITS-sF & HITS-aF correspond to the
1098
Positive: include, offer, obtain, allow, build, in-
crease, ensure, contain, pursue, fulfill, maintain,
recommend, represent, require, respect
Negative: abate, die, condemn, deduce, investi-
gate, commit, correct, apologize, debilitate, dis-
pel, endure, exacerbate, indicate, induce, mini-
mize
Table 5: Examples of newly discovered connotative pred-
icates
Positive: boogie, housewarming, persuasiveness,
kickoff, playhouse, diploma, intuitively, monu-
ment, inaugurate, troubleshooter, accompanist
Negative: seasickness, overleap, gangrenous,
suppressing, fetishist, unspeakably, doubter,
bloodmobile, bureaucratized
Table 6: Examples of newly discovered words with con-
notations: these words are treated as neutral in some con-
ventional sentiment lexicons.
symmetric and asymmetric version of the Focused
method. Finally, Page-aT & Page-aF correspond to
the Truncation and teleportation (Focused) respec-
tively.
Asymmetric HITS on a directed truncated graph
(HITS-aT) and topic-sensitive PageRank (Page-aF)
achieve the best performance in most cases, espe-
cially for top ranked words which have a higher
average frequency. The difference between these
two top performers is not large, but statistically
significant using wilcoxon test with p < 0.03.
Standard PageRank (Page-aT) achieves the third
best performance overall. All these top performing
ones (HITS-aT, Page-aF, Page-aT) outperform the
baseline approach (FREQ) statistically significantly
with p < 0.001. For brevity, we omit the PageRank
results based on the undirected graphs, as the perfor-
mance of those was not as good as that of directed
ones.
7.2 Extrinsic Evaluation via Sentiment
Analysis
Next we perform extrinsic evaluation to quantify the
practical value of our connotation lexicon in con-
crete sentiment analysis applications. In particular,
we make use of our connotation lexicon for binary
sentiment classification tasks in two different ways:
? Unsupervised classification by voting. We de-
fine r as the ratio of positive polarity words to
negative polarity words in the lexicon. In our
experiment, penalty is 0 for positive and ?0.5
for negative.
score(x+) = 1 + penalty+(r,#positive)
score(x?) = ?1 + penalty?(r,#negative)
? Supervised classification using SVM. We use
bag-of-words features for baseline. In order
to quantify the effect of different lexicons, we
add additional features based on the following
scores as defined below:
scoreraw(x) =
?
wx
s(w)
scorepurity(x) =
scoreraw(x)?
wx abs(s(w))
The two corpora we use are SemEval2007 (Strap-
parava and Mihalcea, 2007) and Sentiment Twitter.1
The Twitter dataset consists of tweets containing ei-
ther a smiley emoticon (representing positive senti-
ment) or a frowny emoticon (representing negative
sentiment), we randomly select 50000 smiley tweets
and 50000 frowny tweets.2 We perform a 5-fold
cross validation.
In Table 8, we find very promising results, partic-
ularly for Twitter dataset, which is known to be very
noisy. Notice that the use of Top 6k words from
our connotation lexicon along with OpinionFinder
lexicon boost the performance up to 78.0%, which
is significantly better than than 71.4% using only
the conventional OpinionFinder lexicon. This result
shows that our connotation lexicon nicely comple-
ments existing sentiment lexicon, improving practi-
cal sentiment analysis tasks.
1http://www.stanford.edu/? alecmgo/cs224n/twitterdata.
2009.05.25.c.zip
2We filter out stop-words and words appearing less than 3
times. For Twitter, we also remove usernames of the format
@username occurring within tweet bodies.
1099
Algorithm 1st Round 2nd RoundAcc. F-val Acc. F-val
Voting 68.7 65.4 71.0 68.5
Bag of Words 69.9 65.1 69.9 65.1
(??) + OpFinder 74.7 75.0 74.7 75.0
BoW + Top 2k 73.3 74.5 73.7 75.4
(??) + OpFinder 72.8 73.5 75.0 77.6
BoW + Top 6k 76.6 77.1 74.5 75.3
(??) + OpFinder 74.1 73.5 75.2 76.0
BoW + Top 10k 74,1 73.5 74.2 73.8
(??) + OpFinder 73.5 74.3 74.7 75.1
Table 7: SemEval Classification Result(%) ? (??) denotes
that all features in the previous row are copied over.
Algorithm 1st Round 2nd RoundAcc. F-val Acc. F-val
Voting 60.4 59.1 62.6 61.3
Bag of Words 69.9 72.1 69.9 72.1
(??) + OpFinder 70.3 71.4 70.3 71.4
BoW + Top 2k 71.3 65.4 72.7 73.3
(??) + OpFinder 69.4 63.1 73.1 74.6
BoW + Top 6k 77.2 69.0 76.4 77.6
(??) + OpFinder 76.4 72.0 76.8 78.0
BoW + Top 10k 73.3 73.5 73.7 74.1
(??) + OpFinder 74.1 69.5 73.5 74.2
Table 8: Twitter Classification Result(%) ? (??) denotes
that all features in the previous row are copied over.
7.3 Intrinsic Evaluation via Human Judgment
In order to measure the quality of the connotation
lexicon, we also perform human judgment study on
a subset of the lexicon. Human judges are asked to
quantify the degree of connotative polarity of each
given word using an integer value between 1 and 5,
where 1 and 5 correspond to the most negative and
positive connotation respectively. When computing
the annotator agreement score or evaluating our con-
notation lexicon against human judgment, we con-
solidate 1 and 2 into a single negative class and 4
and 5 into a single positive class. The Kappa score
between two human annotators is 0.78.
As a control set, we also include 100 words taken
from the General Inquirer lexicon: 50 words with
positive sentiment, and 50 words with negative sen-
timent. These words are included so as to mea-
sure the quality of human judgment against a well-
established sentiment lexicon. The words were pre-
sented in a random order so that the human judges
will not know which words are from the General In-
quirer lexicon and which are from our connotative
lexicon. For the words in the control set, the anno-
tators achieved 94% (97% lenient) accuracy on the
positive set and 97% on the negative set.
Note that some words appear in both positive and
negative connotation graphs, while others appear in
only one of them. For instance, if a given word x
appears as an argument for only positive connotative
predicates, but never for negative ones, then xwould
appear only in the positive connotation graph. This
means that for such word, we can assume the conno-
tative polarity even without applying the algorithms
for graph centrality. Therefore, we first evaluate the
accuracy of the polarity of such words that appear
only in one of the connotation graphs. We discard
words with low frequency (300 in terms of Google
n-gram frequency), and randomly select 50 words
from each polarity. The accuracy of such words is
88% by strict evaluation and 94.5% by lenient eval-
uation, where lenient evaluation counts words in our
polarized connotation lexicon to be correct if the hu-
man judges assign non-conflicting polarities, i.e., ei-
ther neutral or identical polarity.
For words that appear in both positive and nega-
tive connotation graphs, we determine the final po-
larity of such words as one with higher scores given
by HITS or PageRank. We randomly select words
that rank at 5% of top 100, top 1000, top 2000, and
top 5000 by each algorithm for human judgment.
We only evaluate the top performing algorithms ?
HITS-aT and Page-aF ? and FREQ baseline. The
stratified performance for each of these methods is
given in Table 9.
8 Related Work
Graph based approaches have been used in many
previous research for lexicon induction. A tech-
nique named label propagation (Zhu and Ghahra-
mani, 2002) has been used by Rao and Ravichan-
dran (2009) and Velikovich et al (2010), while ran-
dom walk based approaches, PageRank in particular,
have been used by Esuli and Sebastiani (2007). In
our work, we explore the use of both HITS (Klein-
berg, 1999) and PageRank (Page et al, 1999) and
1100
Average Positive Negative
Top # Str. Len. Str. Len. Str. Len.
FREQ
@100 73.5 87.3 72.2 91.1 74.7 83.5
@1000 51.8 78.6 44.4 75.6 81.8 90.9
@2000 66.9 74.7 73.1 84.2 57.3 60.0
@5000 61.5 81.3 61.4 84.1 62.0 70.0
HITS-aT
@100 61.3 79.8 74.4 93.3 47.0 65.1
@1000 39.6 75.5 48.1 77.8 30.8 73.1
@2000 57.7 72.1 78.0 86.0 41.0 60.7
@5000 55.6 73.5 69.7 85.7 44.3 63.8
Page-aF
@100 63.0 78.6 74.7 91.2 50.0 64.6
@1000 53.7 72.2 54.5 72.7 53.1 71.9
@2000 56.5 79.6 67.2 91.8 42.6 63.8
@5000 57.1 76.2 75.7 91.0 43.3 65.3
Table 9: Human Annotation Accuracies(%) ? Str. de-
notes strict evaluation & Len. denotes lenient evaluation.
present systematic comparison of various options for
graph representation and encoding of prior knowl-
edge. We are not aware of any previous research
that made use of HITS algorithm for connotation or
sentiment lexicon induction.
Much of previous research investigated the use of
dictionary network (e.g., WordNet) for lexicon in-
duction (e.g., Kamps et al (2004), Takamura et al
(2005), Adreevskaia and Bergler (2006), Esuli and
Sebastiani (2006), Su and Markert (2009), Moham-
mad et al (2009)), while relatively less research in-
vestigated the use of web documents (e.g., Kaji and
Kitsuregawa (2007), Velikovich et al (2010))).
Wilson et al (2005b) first introduced the sen-
timent lexicon, spawning a great deal of research
thereafter. At the beginning, sentiment lexicons
were designed to include only those words that ex-
press sentiment, that is, subjective words. However
in recent years, sentiment lexicons started expand-
ing to include some of those words that simply asso-
ciate with sentiment, even if those words are purely
objective (e.g., Velikovich et al (2010), Baccianella
et al (2010)). This trend applies even to the most re-
cent version of the lexicon of Wilson et al (2005b).
We conjecture that this trend of broader coverage
suggests that such lexicons are practically more use-
ful than sentiment lexicons that include only those
words that are strictly subjective. In this work, we
make this transition more explicit and intentional,
by introducing a novel connotation lexicon.
Mohammad and Turney (2010) focussed on emo-
tion evoked by common words and phrases. The
spirit of their work shares some similarity with ours
in that it aims to find the emotion evoked by words,
as opposed to expressed. Two main differences are:
(1) our work aims to discover even more subtle asso-
ciation of words with sentiment, and (2) we present
a nearly unsupervised approach, while Mohammad
and Turney (2010) explored the use of Mechanical
Turk to build the lexicon based on human judgment.
In the work of Osgood et al (1957), it has been
discussed that connotative meaning of words can
be measured in multiple scales of semantic differ-
ential, for example, the degree of ?goodness? and
?badness?. Our work presents statistical approaches
that measure one such semantic differential auto-
matically. Our graph construction to capture word-
to-word relation is analogous to that of Collins-
Thompson and Callan (2007), where the graph rep-
resentation was used to model more general defini-
tions of words.
9 Conclusion
We introduced the connotation lexicon, a novel lex-
icon that list words with connotative polarity, which
will be made publically available. We also pre-
sented graph-based algorithms for learning conno-
tation lexicon together with connotative predicates
in a nearly unsupervised manner. Our approaches
are grounded on the linguistic insight with respect to
the selectional preference of connotative predicates.
Empirical study demonstrates the practical value of
the connotation lexicon for sentiment analysis en-
couraging further research in this direction.
Acknowledgments
We wholeheartedly thank the reviewers for very
helpful and insightful comments.
References
Alina Adreevskaia and Sabine Bergler. 2006. Mining
wordnet for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 209?216.
1101
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Monica Bianchini, Marco Gori, and Franco Scarselli.
2005. Inside pagerank. ACM Trans. Internet Technol.,
5:92?128, February.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Comput. Linguist., 16:22?29, March.
K. Collins-Thompson and J. Callan. 2007. Automatic
and human scoring of word definition responses. In
Proceedings of NAACL HLT, pages 476?483.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC06, pages
417?422.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pagerank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 424?
431. Association for Computational Linguistics.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
Proceedings of the Eleventh International World Wide
Web Conference, Honolulu, Hawaii.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive collec-
tion of HTML documents. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1075?1083.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to mea-
sure semantic orientation of adjectives. In Proceed-
ings of the 4th International Conference on Language
Resources and Evaluation (LREC), pages 1115?1118.
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. JOURNAL OF THE ACM,
46(5):604?632.
B. Louw, M. Baker, G. Francis, and E. Tognini-Bonelli.
1993. Irony in the text or insincerity in the writer?
the diagnostic potential of semantic prosodies. TEXT
AND TECHNOLOGY IN HONOUR OF JOHN SIN-
CLAIR, pages 157?176.
Saif Mohammad and Peter Turney. 2010. Emotions
evoked by common words and phrases: Using me-
chanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 26?34, Los Angeles, CA, June.
Association for Computational Linguistics.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
599?608, Singapore, August. Association for Compu-
tational Linguistics.
C. E. Osgood, G. Suci, and P. Tannenbaum. 1957. The
measurement of meaning. University of Illinois Press,
Urbana, IL.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-66,
Stanford InfoLab, November.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In EACL ?09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 675?682, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
John Sinclair. 1991. Corpus, concordance, colloca-
tion. Describing English language. Oxford University
Press.
A. Stefanowitsch and S.T. Gries. 2003. Collostructions:
Investigating the interaction of words and construc-
tions. International Journal of Corpus Linguistics,
8(2):209?243.
Philip J. Stone and Earl B. Hunt. 1963. A computer ap-
proach to content analysis: studies using the general
inquirer system. In Proceedings of the May 21-23,
1963, spring joint computer conference, AFIPS ?63
(Spring), pages 241?256, New York, NY, USA. ACM.
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: affective text. In SemEval ?07: Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations, pages 70?74, Morristown, NJ, USA.
Association for Computational Linguistics.
M. Stubbs. 1995. Collocations and semantic profiles:
on the cause of the trouble with quantitative studies.
Functions of language, 2(1):23?55.
Fangzhong Su and Katja Markert. 2009. Subjectivity
recognition on word senses via semi-supervised min-
cuts. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 1?9. Association for Computational
Linguistics.
1102
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL-05, 43rd Annual
Meeting of the Association for Computational Linguis-
tics, Ann Arbor, US. Association for Computational
Linguistics.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In In EMNLP/VLC
2000, pages 63?70.
Peter Turney. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164?
210.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: a system for subjectivity anal-
ysis. In Proceedings of HLT/EMNLP on Interactive
Demonstrations, pages 34?35, Morristown, NJ, USA.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 347?354, Morristown, NJ, USA. Association for
Computational Linguistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
1103
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1522?1533, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Characterizing Stylistic Elements in Syntactic Structure
Song Feng Ritwik Banerjee Yejin Choi
Department of Computer Science
Stony Brook University
NY 11794, USA
songfeng, rbanerjee, ychoi@cs.stonybrook.edu
Abstract
Much of the writing styles recognized in
rhetorical and composition theories involve
deep syntactic elements. However, most
previous research for computational sty-
lometric analysis has relied on shallow
lexico-syntactic patterns. Some very re-
cent work has shown that PCFG models
can detect distributional difference in syn-
tactic styles, but without offering much in-
sights into exactly what constitute salient
stylistic elements in sentence structure
characterizing each authorship. In this
paper, we present a comprehensive ex-
ploration of syntactic elements in writing
styles, with particular emphasis on inter-
pretable characterization of stylistic ele-
ments. We present analytic insights with
respect to the authorship attribution task
in two different domains.
1 Introduction
Much of the writing styles recognized in rhetor-
ical and composition theories involve deep syn-
tactic elements in style (e.g., Bain (1887), Kem-
per (1987) Strunk and White (2008)). However,
previous research for automatic authorship at-
tribution and computational stylometric analy-
sis have relied mostly on shallow lexico-syntactic
patterns (e.g., Mendenhall (1887), Mosteller
and Wallace (1984), Stamatatos et al(2001),
Baayen et al(2002), Koppel and Schler (2003),
Zhao and Zobel (2007), Luyckx and Daelemans
(2008)).
Some very recent works have shown that
PCFG models can detect distributional differ-
ence in sentence structure in gender attribution
(Sarawgi et al 2011), authorship attribution
(Raghavan et al 2010), and native language
identification (Wong and Dras, 2011). However,
still very little has been understood exactly what
constitutes salient stylistic elements in sentence
structures that characterize each author. Al-
though the work of Wong and Dras (2011) has
extracted production rules with highest informa-
tion gain, their analysis stops short of providing
insight any deeper than what simple n-gram-
level analysis could also provide.1 One might
even wonder whether PCFG models are hing-
ing mostly on leaf production rules, and whether
there are indeed deep syntactic differences at all.
This paper attempts to answer these questions.
As an example of syntactic stylistic elements
that have been much discussed in rhetorical the-
ories, but have not been analyzed computation-
ally, let us consider two contrasting sentence
styles: loose (cumulative) and periodic:2 a loose
sentence places the main clause at the begin-
ning, and then appends subordinate phrases and
clauses to develop the main message. In con-
trast, a periodic sentence starts with subordi-
nate phrases and clauses, suspending the most
1For instance, missing determiners in English text
written by Chinese speakers, or simple n-gram anomaly
such as frequent use of ?according to? by Chinese speak-
ers (Wong and Dras, 2011).
2Periodic sentences were favored in classical times,
while loose sentences became more popular in the modern
age.
1522
Hobbs Joshi Lin McDon
S?ROOT ? S , CC S PP?PRN ? IN NP NP?S ? NN CD NP?NP ? DT NN POS
NP?PP ? DT NP?PP ? NP PRN SBAR NP?NP ? DT NN NNS WHNP?SBAR ? IN
VP?VP ? TO VP S?ROOT ? PP NP VP . S?ROOT ? SBAR , NP VP . NP?PP ? NP SBAR
PP?PP ? IN S PRN?NP ? -LRB- PP -RRB- NP?PP ? NP : NP SBAR?PP ? WHADVP S
NP?PP ? NP , PP NP?NP ? NNP S?ROOT ? PP , NP VP . SBAR?S ? WHNP S
VP?S ? VBZ ADJP S S?SBAR ? PP NP VP NP?NP ? PDT DT NNS PP?NP ? IN SBAR
VP?SINV ? VBZ S?ROOT ? LST NP VP . NP?VP ? DT NN SBAR SBAR?NP ? WHNP S
VP?S ? VBD S CONJP?NP ? RB RB IN SBAR?S ? WHADVP S SBAR?PP ? SBAR CC SBAR
VP?S ? VBG PP NP?PP ? NP PRN PP PRN?NP ? -LRB- NP -RRB- PP?VP ? IN
ADVP?VP ? RB PP NP?NP ? NP , NP NP?PP ? NN NN S?SBAR ? VP
Table 1: Top 10 most discriminative production rules for each author in the scientific domain.
loose Christopher Columbus finally
reached the shores of San Salvador
after months of uncertainty at
sea, the threat of mutiny, and a
shortage of food and water.
periodic After months of uncertainty at sea,
the threat of mutiny, and a short-
age of food and water, Christopher
Columbus finally reached the shores
of San Salvador.
Table 2: Loose/Periodic sentence with identical set
of words and POS tags
important part to the end. The example in Ta-
ble 2 highlights the difference:
Notice that these two sentences comprise of an
identical set of words and part-of-speech. Hence,
shallow lexico-syntactic analysis will not be able
to catch the pronounced stylistic difference that
is clear to a human reader.
One might wonder whether we could gain in-
teresting insights simply by looking at the most
discriminative production rules in PCFG trees.
To address this question, Table 1 shows the
top ten most discriminative production rules
for authorship attribution for scientific articles,3
ranked by LIBLINEAR (Fan et al 2008).4 Note
that terminal production rules are excluded so
as to focus directly on syntax.
It does provide some insights, but not to a sat-
isfactory degree. For instance, Hobbs seems to
favor inverted declarative sentences (SINV) and
adverbs with prepositions (RB PP). While the
latter can be easily obtained by simple part-of-
3See Section 2 for the description of the dataset.
4We use Berkeley PCFG parser (Petrov and Klein,
2007) for all experiments.
speech analysis, the former requires using parse
trees. We can also observe that none of the
top 10 most discriminative production rules for
Hobbs includes SBAR tag, which represents sub-
ordinate clauses. But examining discriminative
rules alone is limited in providing more compre-
hensive characterization of idiolects.
Can we unveil something more in deep syntac-
tic structure that can characterize the collective
syntactic difference between any two authors?
For instance, what can we say about distribu-
tional difference between loose and periodic sen-
tences discussed earlier for each author? As can
be seen in Table 1, simply enumerating most dis-
criminative rules does not readily answer ques-
tions such as above.
In general, production rules in CFGs do not
directly map to a wide variety of stylistic el-
ements in rhetorical and composition theories.
This is only as expected however, partly because
CFGs are not designed for stylometric analysis
in the first place, and also because some syntac-
tic elements can go beyond the scope of context
free grammars.
As an attempt to reduce this gap between
modern statistical parsers and cognitively recog-
nizable stylistic elements, we explore two com-
plementary approaches:
1. Translating some of the well known stylistic
elements of rhetorical theories into PCFG
analysis (Section 3).
2. Investigating different strategies of analyz-
ing PCFG trees to extract author charac-
teristics that are interesting as well as in-
terpretable (Sections 4 & 5).
1523
Algorithm 1 Sentence Type-1 Identification
Input: Parse tree t(Nr) of sentence s
Output: Type of s.
if S ? Ltop then
if SBAR /? ?(Nr) then
return COMPOUND
else
return COMPLEX-COMPOUND
else
if VP ? Ltop then
if SBAR /? ?(Nr) then
return SIMPLE
else
return COMPLEX
return OTHER
We present analytic insights with respect to
the authorship attribution task in two distinct
domains.
2 Data
For the empirical analysis of authorship attri-
bution, we use two different datasets described
below. Sections 3, 4 & 5 provide the details of
our stylometric analysis.
Scientific Paper We use the ACL Anthol-
ogy Reference Corpus (Bird et al 2008). Since
it is nearly impossible to determine the gold-
standard authorship of a paper written by multi-
ple authors, we select 10 authors who have pub-
lished at least 8 single-authored papers. We in-
clude 8 documents per author, and remove cita-
tions, tables, formulas from the text using sim-
ple heuristics.5
Novels We collect 5 novels from 5 English au-
thors: Charles Dickens, Edward Bulwer-Lytton,
Jane Austen, Thomas Hardy and Walter Scott.
We select the first 3000 sentences from each
novel and group every 50 consecutive sentences
into 60 documents per novel per author.
5Some might question whether the size of the dataset
used here is relatively small in comparison to typical
dataset comprised of thousands of documents in conven-
tional text categorization. We point out that authorship
attribution is fundamentally different from text catego-
rization in that it is often practically impossible to collect
more than several documents for each author. Therefore,
it is desirable that the attribution algorithms to detect
the authors based on very small samples.
Algorithm 2 Sentence Type-II Identification
Input: Parse tree t(Nr) of sentence s
Output: Type of s.
k ? 1
while k ? ? do
if Ltopk 6= VP then
if S ? ?(Ltopk ) or SBAR ? ?(L
top
k ) then
return PERIODIC
else
if S ? ?(Ltopk ) or SBAR ? ?(L
top
k ) then
return LOOSE
return OTHER
3 Sentence Types
In this section, we examine well-known sentence
types that are recognized in the literature, but
have not been analyzed computationally.
Type-I Identification ? Simple/Complex/
Compound/Complex-Compound: PCFG
trees do not provide this information directly,
hence we must construct an algorithm to derive
it. The key to identifying these sentences is the
existence of dependent and independent clauses.
For the former, we rely on the SBAR tag, while
for the latter, we first define the sequence of
nodes right below the root (e.g., [NP VP .] shown
in the horizontal box in Figure 1). We call this
the top structural level. We then check whether
S (in addition to the root S) appears in this
sequence.
Formally, let Ltop = {Ni} be the set of nodes
in the top structural level, and ? = |Ltop|. Let
t(Nr) be the tree rooted at Nr, and ?(Nr) de-
note the set of nodes in t(Nr). Algorithm 1
shows the procedure to determine the type-I
class of a sentence based on its PCFG tree.6
Type-II Identification ? Loose/Periodic:
A sentence can also be classified as loose or
periodic, and we present Algorithm 2 for this
identification. We perform a mini-evaluation on
20 previously unseen sentences for each type7.
Our algorithm was able to perform type-I iden-
tification on all sentences correctly. In type-II
6Note that Algorithm 1 & 2 rely on the use of Berkeley
parser (Petrov and Klein, 2007).
7These were gathered from several online quizzes
for English learners. E.g., http://grammar.about.com,
http://a4esl.org
1524
Type Hobbs Joshi Lin McDon
simple 40.0 41.7 50.2 27.9
cplex 40.8 40.7 37.6 48.4
cpnd 7.9 5.6 3.9 5.5
cpxnd 8.5 9.2 7.7 15.5
other 2.8 2.8 0.6 2.7
loose 27.6 26.4 26.9 30.8
perio 11.1 11.7 15.2 16.4
other 61.3 61.9 57.9 52.8
Table 3: Sentence Types (%) in scientific data.
identification, it labeled all loose sentences cor-
rectly, and achieved 90% accuracy on periodic
sentences.
Discussion Tables 3 & 4 show the sentence
type distribution in scientific data and novels,
respectively.8 We see that different authors are
characterized by different distribution of sen-
tence types. For instance, in Table 3, Lin is
a prolific user of simple sentences while McDon
prefers employing complex sentences. McDon
also uses complex-compound sentences quite of-
ten (15.5%), more than twice as frequently as
Lin. Notice that all authors use loose sen-
tences much more often than periodic sentences,
a known trend in modern English.
In Table 4, we see the opposite trend among
19th-century novels: with the exception of Jane
Austen, all authors utilize periodic sentences
comparatively more often. We also notice
that complex and complex-compound sentences
abound, as expected from classic literary proses.
Can we determine authorship solely based on the
distribution of sentence types?
We experiment with a SVM classifier using just
6 features (one feature for each sentence type in
Table 3), and we achieve accuracy 36.0% with
the scientific data. Given that a random base-
line would achieve only about 10% accuracy, this
demonstrates that the distribution of sentence
types does characterize an idiolect to some de-
gree.
8Due to space limitation, we present analyses based
on 4 authors from the scientific data.
Type Dickens B-Lyt Austen Hardy Scott
simple 26.0 21.2 23.9 25.6 17.5
cplex 24.4 21.8 24.8 25.6 31.8
cpnd 15.3 15.2 12.6 16.3 11.7
cpxnd 20.8 23.3 31.1 18.9 28.7
other 13.5 18.5 7.6 13.6 10.3
loose 11.5 10.8 17.9 14.5 15.3
perio 19.5 13.6 14.0 16.2 18.0
other 69.0 75.6 68.1 69.3 66.7
Table 4: Sentence Types (%) in Novels
4 Syntactic Elements Based on
Production Rules
In this section, we examine three different as-
pects of syntactic elements based on production
rules.
4.1 Syntactic Variations
We conjecture that the variety of syntactic
structure, which most previous research in com-
putational stylometry has not paid much atten-
tion to, provides an interesting insight into au-
thorship. One way to quantify the degree of syn-
tactic variations is to count the unique produc-
tion rules. In Tables 5, we show the extent of
syntactic variations employed by authors using
the standard deviation ? and the coverage of an
author:
C(a) :=
|R(a)|
| ?a R(a)|
? 100
whereR(a) denotes the set of unique production
rules used by author a, and ?a iterates over all
authors. In order to compare among authors,
we also show these parameters normalized with
respect to the highest value. Our default setting
is to exclude all lexicalized rules in the produc-
tions to focus directly on the syntactic varia-
tions. In our experiments (Section 6), however,
we do augment the rules with (a) ancestor nodes
to capture deeper syntactic structure and (b)
lexical (leaf) nodes.
As hypothesized, these statistics provide us
new insights into the authorship. For instance,
we find that McDon employs a wider variety of
syntactic structure than others, while Lin?s writ-
ing exhibits relatively the least variation. More-
over, comparing Joshi and Hobbs, it is inter-
esting to see the standard deviation differ a lot
1525
Hobbs Joshi Lin McDon Dickens B-Lyt Austen Hardy Scott
C 36.0 37.6 32.8 42.6 30.9 28.8 36.2 30.0 24.1
Cnorm 0.84 0.88 0.77 1.0 0.85 0.79 1.0 0.83 0.67
? 51.5 39.2 63.3 44.4 88.3 81.6 98.0 125.3 114.7
?norm 0.81 0.62 1.0 0.7 0.7 0.65 0.78 1.0 0.92
Table 5: Syntactic variations of different authors in the scientific domain.
Hobbs Joshi Lin McDon
# 136 # 142 # 124 # 161
S ? S CC S . S ? ADVP PP NP VP . S ? SBAR NP VP . S ? S NP VP .
S ? CC NP VP . S ? PP NP ADVP VP . FRAG ? NP : S . S ? S : S .
S ? S VP . S ? NP VP S ? NP VP . S ? SBAR VP .
S ? NP NP VP . S ? S S CC S . S ? PP VP . S ? SBAR S CC S .
S ? PP NP VP . S ? ADVP NP VP . S ? NP ADVP VP . S ? NP PP VP .
Table 6: Most discriminative sentence outlines in the scientific data. #N shows the number of unique
sentence outlines of each author.
(51.5 and 39.2), in spite of their C scores being
similar: 36.0% and 37.6%, respectively. This
indicates that Hobbs tends to use a certain sub-
set production rules much more frequently than
Joshi. Lin exhibits the highest standard devia-
tion in spite of having least syntactic variation,
indicating that he uses a much smaller subset of
productions regularly, while ocassionally deviat-
ing to other rules.
Similarly, among novels, Jane Austen?s writ-
ing has the highest amount of variation, while
Walter Scott?s writing style is the least varied.
Even though authors from both datasets display
similar C scores (Table 5), the difference in ? is
noteworthy. The significantly higher linguistic
variation is to be expected in creative writing
of such stature. It is interesting to note that
the authors with highest coverage ? Austen and
Dickens ? have much lower deviation in their
syntactic structure when compared to Hardy
and Scott. This indicates that while Austen and
Dickens consistently employ a wider variety of
sentence structures in their writing, Hardy and
Scott follow a relatively more uniform style with
sporadic forays into diverse syntactic constructs.
4.2 Sentence Outlines
Although the approach of Section 4.1 give us a
better and more general insight into the char-
acteristics of each author, its ability to provide
insight on deep syntactic structure is still lim-
ited, as it covers production rules at all levels of
the tree. We thus shift our focus to the top level
of the trees, e.g., the second level (marked in a
horizontal box) in Tree (1) of Figure 1, which
gives us a better sense of sentence outlines.
Tables 6 and 7 present the most discrimina-
tive sentence outlines of each author in the scien-
tific data and novels, respectively. We find that
McDon is a prolific user of subordinate clauses,
indicating his bias towards using complex sen-
tences. The rule ?S ? SBAR S CC S? shows
his inclination towards complex-compound sen-
tences as well. These inferences are further sup-
ported by the observations in Table 3. Another
observation of possible interest is the tendency
of Joshi and Lin to begin sentences with prepo-
sitional phrases.
In comparing Table 6 and Table 7, notice
the significantly higher presence of complex and
compound-complex structures in the latter9.
The most discriminating sentence outlines for
Jane Austen, for instance, are all indicative of
complex-compound sentences. This is further
supported by Table 4.
5 Syntactic Elements Based on Tree
Topology
In this section, we investigate quantitative tech-
niques to capture stylistic elements in the tree
9The presence of ?FRAG? is not surprising. Inten-
tional use of verbless sentence fragments, known as sce-
sis onomaton, was often employed by authors such as
Dickens and Bulwer-Lytton (Quinn, 1995).
1526
Dickens Bulwer-Lytton Austen Hardy Scott
# 1820 # 1696 # 2137 # 1772 # 1423
SQ ? NNP . SBARQ ? WHNP S . S ? S : CC S . S ? S NP VP . S ? NP PRN VP .
FRAG ? NP . FRAG ? INTJ NP . S ? S CC S : CC S . S ? ADVP NP VP . S ? PP NP VP .
SINV ? NP VP NP . S ? S : S CC S . S ? S : CC S : CC S . S ? FRAG : S . S ? S S : S .
INTJ ? UH . FRAG ? CC NP . S ? S : S : CC S . S ? INTJ NP VP . S ? NP PP VP .
SBARQ ? WHNP SQ . FRAG ? NP ADJP . S ? SBAR S : CC S . S ? NP VP . S ? ADVP PRN NP VP .
Table 7: Most discriminative sentence outlines in the novel data. #N shows the number of unique sentence
outlines of each author.
Metrics Scientific Data Novels
Hobbs Joshi Lin McDon Dickens B-Lyt Austen Hardy Scott
sen-len avg 23.7 26.0 21.0 32.2 24.1 26.7 31.4 21.5 34.1
hT avg 5.8 5.3 5.9 4.8 4.7 5.0 5.4 4.9 5.9
hF avg 2.4 2.1 2.5 1.9 1.9 1.9 2.1 1.9 2.1
wL avg 5.0 4.8 5.5 4.2 4.1 4.4 4.7 3.8 4.9
?H avg 1.2 1.1 1.1 1.0 1.1 1.1 1.3 1.2 1.4
?S avg 1.9 1.8 1.8 1.7 1.0 1.1 1.2 1.0 1.4
Table 8: Tree topology metrics for scientific data and novels.
topology. Figure 1 shows three different parse
trees to accompany our discussion.10 Notice
that sentence (1) is a loose sentence, and sen-
tence (2) is periodic. In general, loose sentences
grow deep and unbalanced, while periodic sen-
tences are relatively more balanced and wider.
For a tree t rooted at NR with a height n, let
T be the set of leaf nodes, and let F be the set
of furcation nodes, and let ?(Ni, Nj) denote the
length of the shortest path from Ni to Nj . In-
spired by the work of Shao (1990), we analyze
tree topology with the following four measure-
ments:
? Leaf height (hT = {hTi , Ni ? T }), where
hTi = ?(Ni, NR) Ni ? T . For instance, the
leaf height of ?free? of Tree (2) in Fig. 1
is 6.
? Furcation height (hF = {hFi , Ni ? F}),
where hFi is the maximum leaf height within
the subtree rooted at Ni. In Figure 1, for
example, the furcation height of the VP in
Tree (2) (marked in triangle) is 3.
? Level width (wL = {wl, 1 ? l ? n}),
where wl = |{Ni : ?(Ni, NR) = l}|. E.g., w4
of Tree (1) in Figure 1 is 6.
10Example sentences are taken from Lin (1997), Joshi
(1992), and Lin (1995).
? Horizontal ?H = {?Hi , Ni ? F} , and
Vertical Imbalance ?S = {?Si , Ni ? F}.
Let C be the set of child nodes of Nk. If
|C| ? 2, then
?Hk =
?
?
?
? 1
n
|C|?
i=1
(hFi ?H)
2
where H = 1|C|
?|C|
i=1 h
F
i . Similarly,
?Sk =
?
?
?
? 1
n
|C|?
i=1
(s(Ni)? S)2
where S = 1|C|
?|C|
i=1 s(Ni) and s(Ni) is the
number of leaf nodes of tree rooted at Ni.
As shown in Figure 1, the imbalance of the
internal node VP in Tree (2) (marked in
triangle) is 0.5 horizontally, and 0.5 verti-
cally.
To give an intuition on the relation between
these measurements and different tree struc-
tures, Table 9 provides the measurements of the
three trees shown in Figure 1.
Note that all three sentences are of similar
length but show different tree structures. Tree
(1) and Tree (2) differ in that Tree (1) is
highly unbalanced and grows deep, while Tree
1527
Figure 1: Parsed trees
Metrics Tree (1) Tree (2) Tree (3)
# of tokens 15 13 13
maxi {hTi } 11 6 6
maxi {wLi } 6 9 9
maxi {?Hi } 4.97 1.6 1.7
maxi {?Si } 4 1.5 4.7
Table 9: Tree Topology Statistics for Figure 1.
(2) is much better balanced and grows shorter
but wider. Comparing Tree (2) and Tree (3),
they have the same max Leaf height, Level
width, and Horizontal Imbalance, but the
latter has bigger Vertical Imbalance, which
quantifies the imbalance in terms of the text
span covered by subtrees.
We provide these topological metrics for au-
thors from both datasets in Table 8.
6 Experiments & Evaluation
In our experiments, we utilize a set of features
motivated by PCFG trees. These consist of sim-
ple production rules and other syntactic features
based on tree-traversals. Table 10 describes
these features with examples from Tree (2), us-
ing the portion marked by the triangle.
These sets of production rules and syntax fea-
tures are used to build SVM classifiers using LI-
BLINEAR (Fan et al 2008), wherein all fea-
ture values are encoded as term-frequencies nor-
malized by document size. We run 5-fold cross-
validation with training and testing split first as
80%/20%, and then as 20%/80%.
We would like to point out that the latter con-
figuration is of high practical importance in au-
thorship attribution, since we may not always
have sufficient training data in realistic situa-
tions, e.g., forensics (Luyckx and Daelemans,
2008).
Lexical tokens provide strong clues by creat-
ing features that are specific to each author: re-
search topics in the scientific data, and proper
nouns such as character names in novels. To
lessen such topical bias, we lemmatize and rank
words according to their frequency (in the entire
dataset), and then consider the top 2,000 words
only. Leaf-node productions with words outside
this set are disregarded.
Our experimental results (Tables 11 & 12)
show that not only do deep syntactic features
perform well on their own, but they also signif-
icantly improve over lexical features. We also
show that adding the style11 features further
improves performance.
1528
Features
pr Rules excluding terminal productions.
E.g., VP ? VBG NP
synv Traversal from a non-leaf node to its grand-
parent (embedded rising).
E.g., VP?S ? PP
synh Left-to-right traversal in the set of all non-
leaf children of a node.
E.g., VBG ? NP (for node VP)
synv+h synv ? synh
syn0 No tree traversal. Feature comprises inte-
rior nodes only.
syn? Union of all edges to child nodes, except
when child is a leaf node.
E.g., {VP ? VBG, VP ? NP}
synl syn? ? { edge to parent node}
style11 The set of 11 extra stylistic features. 6 val-
ues from the distribution of sentence types
(Section 3), and 5 topological metrics (Sec-
tion 5) characterizing the height, width and
imbalance of a tree.
Variations
p?r Each production rule is augmented with the
grandparent node.
? Terminal (leaf) nodes are included.
Table 10: Features and their lexico-syntactic varia-
tions. Illustration: p?r? denotes the set of production
rules pr (including terminal productions) that are
augmented with their grandparent nodes.
To quantify the amount of authorship infor-
mation carried in the set style11, we experi-
ment with a SVM classifier using only 11 fea-
tures (one for each metric), and achieve accu-
racy of 42.0% and 52.0% with scientific data
and novels, respectively. Given that a random-
guess baseline would achieve only 10% and 20%
(resp.), and that the classification is based on
just 11 features, this experiment demonstrates
how effectively the tree topology statistics cap-
ture idiolects. In general, lexicalized features
yield higher performance even after removing
topical words. This is expected since tokens
such as function words play an important role
in determining authorship (e.g., Mosteller and
Wallace (1984), Garcia and Martin (2007), Arg-
amon et al(2007)).
A more important observation, however, is
that even after removing the leaf production
rules, accuracy as high as 93% (scientific) and
92.2% (novels) are obtained using syntactic fea-
Features Scientific Novels
+style11 +style11
style11 20.6 ? 43.1 ?
Unigram 56.9 ? 69.3 ?
synh 53.7 53.7 68.3 67.9
syn0 22.9 31.1 57.8 62.5
syn? 43.4 44.0 63.6 65.7
synl 51.1 51.7 71.3 72.8
synv+h 54.0 55.7 72.0 73.2
syn?h 63.1 64.0 72,1 73.2
syn?0 56.6 56.0 73.1 74.1
syn?? 56.3 57.2 74.0 74.9
syn?l 64.6 65.4 74.9 75.3
syn?v+h 64.0 67.7 74.0 74.7
pr 50.3 53.4 67.0 66.7
p?r 59.1 60.6 69.7 68.7
pr? 63.7 65.1 71.5 73.2
p?r? 66.3 69.4 73.6 74.9
Table 11: Authorship attribution with 20% train-
ing data. Improvement with addition of style11
shown in bold.
tures, which demonstrates that there are syn-
tactic patterns unique to each author. Also no-
tice that using only production rules, we achieve
higher accuracy in novels (90.1%), but the ad-
dition of style11 features yields better results
with scientific data (93.0%).
Using different amounts of training data pro-
vides insight about the influence of lexical clues.
In the scientific dataset, increasing the amount
of training data decreases the average perfor-
mance difference between lexicalized and unlex-
icalized features: 13.5% to 11.6%. In novels,
however, we see the opposite trend: 6.1% in-
creases to 8.1%.
We further observe that with scientific data,
increasing the amount of training data improves
the average performance across all unlexicalized
feature-sets from 50.0% to 82.9%, an improve-
ment of 32.8%. For novels, the corresponding
improvement is small in comparison: 17.0%.
This difference is expected. While authors
such as Dickens or Hardy have their unique writ-
ing styles that a classifier can learn based on few
documents, capturing idiolects in the more rigid
domain of scientific writing is far from obvious
with little training data.
1529
Features Scientific Novels
+style11 +style11
style11 42.0 ? 52.0 ?
Unigram 88.0 ? 92.7 ?
synh 85.0 85.0 87.6 88.9
syn0 40.0 53.0 66.4 72.3
syn? 78.0 82.0 80.3 82.3
synl 85.0 92.0 89.3 92.2
synv+h 89.0 93.0 90.1 91.2
syn?h 93.0 93.0 93.7 93.9
syn?0 92.0 94.0 92.1 93.2
syn?? 93.0 94.0 93.4 94.5
syn?l 93.0 95.0 94.9 95.2
syn?v+h 94.0 96.0 94.7 94.8
pr 85.0 86.0 86.7 86.7
p?r 87.0 89.0 88.2 89.3
pr? 93.0 94.0 92.1 93.2
p?r? 94.0 95.0 94.5 95.1
Table 12: Authorship attribution with 80% train-
ing data.
Turning to lexicalized features, we note that
with more training data, lexical cues perform
better in scientific domain than in novels. With
80% data used for training, the average per-
formance of lexicalized feature-sets with science
data is 94.4%, and slightly lower at 94.3% for
novels. With less training data, however, these
figures are 63.5% and 74.3% respectively.
Finally, we point out that adding the style
features derived from sentence types and tree
topologies almost always improves the perfor-
mance. In scientific data, syn?v+h with style11
features shows the best performance (96%),
while syn?l yields the best results for novels
(95.2%). For unlexicalized features, adding
style11 to synv+h and synl yields respective
improvements of 4.0% and 2.9% in the two
datasets.
7 Related Work
There are several hurdles in authorship attribu-
tion. First and foremost, writing style is ex-
tremely domain-dependent. Much of previous
research has focused on several domains of writ-
ing, such as informal modern writing in blogs
and online messages (Zheng et al 2006), rela-
tively formal contemporary texts such as news
articles (Raghavan et al 2010), or classical lit-
erature like novels and proses (e.g., (Burrows,
2002), (Hoover, 2004)).
The nature of these features have also var-
ied considerably. Character level n-grams have
been used by several researchers; most notably
by Peng et al(2003), by Houvardas and Sta-
matatos (2006) for feature selection, and by Sta-
matatos (2006) in ensemble learning. Keselj et
al. (2003) employed frequency measures on n-
grams for authorship attribution.
Others, such as Zhao and Zobel (2005), Arg-
amon and Levitan (2004), Garcia and Martin
(2007), have used word-level approaches instead,
incorporating the differential use of function
words by authors.
More sophisticated linguistic cues have been
explored as well: parts-of-speech n-grams
(Diederich et al 2003), word-level statistics to-
gether with POS-sequences (Luyckx and Daele-
mans, 2008), syntactic labels from partial pars-
ing (Hirst and Feiguina, 2007), etc. The use
of syntactic features from parse trees in au-
thorship attribution was initiated by Baayen et
al. (1996), and more recently, Raghavan et al
(2010) have directly employed PCFG language
models in this area.
Syntactic features from PCFG parse trees
have also been used for gender attribution
(Sarawgi et al 2011), genre identification (Sta-
matatos et al 2000), native language identifi-
cation (Wong and Dras, 2011) and readability
assessment (Pitler and Nenkova, 2008). The
primary focus of most previous research, how-
ever, was to attain better classification accuracy,
rather than providing linguistic interpretations
of individual authorship and their stylistic ele-
ments.
Our work is the first to attempt authorship
attribution of scientific papers, a contemporary
domain where language is very formal, and the
stylistic variations have limited scope. In ad-
dition to exploring this new domain, we also
present a comparative study expounding the
role of syntactic features for authorship attri-
bution in classical literature. Furthermore, our
work is also the first to utilize tree topological
1530
features (Chan et al 2010) in the context of
stylometric analysis.
8 Conclusion
In this paper, we have presented a comprehen-
sive exploration of syntactic elements in writing
styles, with particular emphasis on interpretable
characterization of stylistic elements, thus dis-
tinguishing our work from other recent work on
syntactic stylometric analysis. Our analytical
study provides novel statistically supported in-
sights into stylistic elements that have not been
computationally analyzed in previous literature.
In the future, we plan to investigate the use of
syntactic feature generators for text categoriza-
tion (e.g., Collins and Duffy (2002), Moschitti
(2008), Pighin and Moschitti (2009)) for stylom-
etry analysis.
Acknowledgments Yejin Choi is partially
supported by the Stony Brook University Office
of the Vice President for Research. We thank
reviewers for many insightful and helpful com-
ments.
References
Shlomo Argamon and Shlomo Levitan. 2004. Mea-
suring the usefulness of function words for author-
ship attribution. Literary and Linguistic Comput-
ing, pages 1?3.
Shlomo Argamon, Casey Whitelaw, Paul Chase,
Sobhan Raj Hota, Navendu Garg, and Shlomo
Levitan. 2007. Stylistic text classification using
functional lexical features: Research articles. J.
Am. Soc. Inf. Sci. Technol., 58(6):802?822.
H. Baayen, H. Van Halteren, and F. Tweedie. 1996.
Outside the cave of shadows: Using syntactic an-
notation to enhance authorship attribution. Lit-
erary and Linguistic Computing, 11(3):121.
H. Baayen, H. van Halteren, A. Neijt, and
F. Tweedie. 2002. An experiment in authorship
attribution. In 6th JADT. Citeseer.
A. Bain. 1887. English Composition and Rhetoric:
Intellectual elements of style. D. Appleton and
company.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph,
M.Y. Kan, D. Lee, B. Powley, D.R. Radev, and
Y.F. Tan. 2008. The acl anthology reference
corpus: A reference dataset for bibliographic re-
search in computational linguistics. In Proc.
of the 6th International Conference on Language
Resources and Evaluation Conference (LREC08),
pages 1755?1759.
J. Burrows. 2002. Delta: A measure of stylistic dif-
ference and a guide to likely authorship. Literary
and Linguistic Computing, 17(3):267?287.
Samuel W. K. Chan, Lawrence Y. L. Cheung, and
Mickey W. C. Chong. 2010. Tree topological fea-
tures for unlexicalized parsing. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics: Posters, COLING ?10, pages
117?125, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: kernels over
discrete structures, and the voted perceptron. In
Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, ACL ?02,
pages 263?270, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
J. Diederich, J. Kindermann, E. Leopold, and
G. Paass. 2003. Authorship attribution with
support vector machines. Applied Intelligence,
19(1):109?123.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?
1874.
Antonion Miranda Garcia and Javier Calle Mar-
tin. 2007. Function words in authorship attribu-
tion studies. Literary and Linguistic Computing,
22(1):49?66.
Graeme Hirst and Olga Feiguina. 2007. Bigrams of
syntactic labels for authorship discrimination of
short texts. Literary and Linguistic Computing,
22(4):405?417.
D. L. Hoover. 2004. Testing burrow?s delta. Literary
and Linguistic Computing, 19(4):453?475.
J. Houvardas and E. Stamatatos. 2006. N-gram fea-
ture selection for author identification. In Proc.
of the 12th International Conference on Artificial
Intelligence: Methodology, Systems and Applica-
tions, volume 4183 of LNCS, pages 77?86, Varna,
Bulgaria. Springer.
Aravind K. Joshi. 1992. Statistical language mod-
eling. In Proceedings of a Workshop Held at Har-
riman, New York, February 23-26, 1992. Associa-
tion for Computational Linguistics.
S. Kemper. 1987. Life-span changes in syntactic
complexity. Journal of gerontology, 42(3):323.
Vlado Keselj, Fuchun Peng, Nick Cercone, and
Calvin Thomas. 2003. N-gram-based author pro-
files for authorship attribution. In Proc. of the
1531
Pacific Association for Computational Linguistics,
pages 255?264.
M. Koppel and J. Schler. 2003. Exploiting stylistic
idiosyncrasies for authorship attribution. In Pro-
ceedings of IJCAI, volume 3, pages 69?72. Cite-
seer.
D. Lin. 1995. University of manitoba: descrip-
tion of the pie system used for muc-6. In Pro-
ceedings of the 6th conference on Message under-
standing, pages 113?126. Association for Compu-
tational Linguistics.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Pro-
ceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics and Eighth
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 64?71.
Association for Computational Linguistics.
Kim Luyckx and Walter Daelemans. 2008. Author-
ship attribution and verification with many au-
thors and limited data. In COLING ?08, pages
513?520.
T.C. Mendenhall. 1887. The characteristic curves of
composition. Science, ns-9(214S):237?246.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization.
In Proceedings of the 17th ACM conference on In-
formation and knowledge management, CIKM ?08,
pages 253?262, New York, NY, USA. ACM.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case
of the Federalist Papers. Springer-Verlag.
Fuchun Peng, Dale Schuurmans, Shaojun Wang, and
Vlado Keselj. 2003. Language independent au-
thorship attribution using character level language
models. In Proceedings of the tenth conference on
European chapter of the Association for Compu-
tational Linguistics - Volume 1, EACL ?03, pages
267?274, Stroudsburg, PA, USA. Association for
Computational Linguistics.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411.
Daniele Pighin and Alessandro Moschitti. 2009. Re-
verse engineering of tree kernel feature spaces. In
Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing: Vol-
ume 1 - Volume 1, EMNLP ?09, pages 111?120,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: a unified framework for predicting
text quality. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 186?195, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Arthus Quinn. 1995. Figures of Speech: 60 Ways To
Turn A Phrase. Routledge.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using
probabilistic context-free grammars. In Proceed-
ings of the ACL 2010 Conference Short Papers,
pages 38?42, Uppsala, Sweden. Association for
Computational Linguistics.
Ruchita Sarawgi, Kailash Gajulapalli, and Yejin
Choi. 2011. Gender attribution: tracing stylo-
metric evidence beyond topic and genre. In Pro-
ceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning, CoNLL ?11,
pages 78?86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
K.T. Shao. 1990. Tree balance. Systematic Biology,
39(3):266.
Efstathios Stamatatos, George Kokkinakis, and
Nikos Fakotakis. 2000. Automatic text catego-
rization in terms of genre and author. Comput.
Linguist., 26(4):471?495.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis.
2001. Computer-based authorship attribution
without lexical measures. Computers and the Hu-
manities, 35(2):193?214.
E. Stamatatos. 2006. Ensemble-based author iden-
tification using character n-grams. ReCALL, page
4146.
W. Strunk and E.B. White. 2008. The elements of
style. Penguin Group USA.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identifica-
tion. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1600?1610, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ying Zhao and Justin Zobel. 2005. Effective
and scalable authorship attribution using func-
tion words. In Proceedings of the Second Asia
conference on Asia Information Retrieval Technol-
ogy, AIRS?05, pages 174?189, Berlin, Heidelberg.
Springer-Verlag.
Y. Zhao and J. Zobel. 2007. Searching with style:
Authorship attribution in classic literature. In
Proceedings of the thirtieth Australasian confer-
ence on Computer science-Volume 62, pages 59?
68. Australian Computer Society, Inc.
Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan
Huang. 2006. A framework for authorship identi-
fication of online messages: Writing-style features
1532
and classification techniques. J. Am. Soc. Inf. Sci.
Technol., 57(3):378?393.
1533
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1246?1258,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Understanding and Quantifying Creativity in Lexical Composition
Polina Kuznetsova Jianfu Chen Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{pkuznetsova,jianchen,ychoi}@cs.stonybrook.edu
Abstract
Why do certain combinations of words such
as ?disadvantageous peace? or ?metal to the
petal? appeal to our minds as interesting ex-
pressions with a sense of creativity, while
other phrases such as ?quiet teenager?, or
?geometrical base? not as much? We present
statistical explorations to understand the char-
acteristics of lexical compositions that give
rise to the perception of being original, inter-
esting, and at times even artistic. We first ex-
amine various correlates of perceived creativ-
ity based on information theoretic measures
and the connotation of words, then present ex-
periments based on supervised learning that
give us further insights on how different as-
pects of lexical composition collectively con-
tribute to the perceived creativity.
1 Introduction
An essential property of natural language is the gen-
erative capacity that makes it possible for people to
express indefinitely many thoughts through indef-
initely many different ways of composing phrases
and sentences (Chomsky, 1965). The possibility of
novel, creative expressions never seems to exhaust.
Various types of writers, such as novelists, journal-
ists, movie script writers, and creatives in adver-
tising, continue creating novel phrases and expres-
sions that are original while befitting in expressing
the desired meaning in the given situation. Consider
unique phrases such as ?geological split personal-
ity?, or ?intoxicating Shangri-La of shoes?,1 that
1Examples from New York Times articles in 2013.
continue flowing into the online text drawing atten-
tion from readers.
Writers put significant effort in choosing the per-
fect words in completing their compositions, as a
well-chosen combination of words is impactful in
readers? minds for rendering the precise intended
meaning, as well as stimulating an increased level
of cognitive responses and attention. Metaphors in
particular, one of the quintessential forms of lin-
guistic creativity, have been discussed extensively
by studies across multiple disciplines, e.g., Cog-
nitive Science, Psychology, Linguistics, and Liter-
ature (e.g., Lakoff and Johnson (1980), McCurry
and Hayes (1992), Goatly (1997)). Moreover, re-
cent studies based on fMRI begin to discover bio-
logical evidences that support the impact of creative
phrases on people?s minds. These studies report that
unconventional metaphoric expressions elicit signif-
icantly increased involvement of brain processing
when compared against the effect of conventional
metaphors or literal expressions (e.g., Mashal et al
(2007), Mashal et al (2009)).
Several linguistic elements, e.g., syntax, seman-
tics, and pragmatics, are likely to be working to-
gether in order to lead to the perception of creativ-
ity. However, their underlying mechanisms by and
large are yet to be investigated. In this paper, as a
small step toward quantitative understanding of lin-
guistic creativity, we present a focused study on lex-
ical composition two content words.
Being creative, by definition, implies qualities
such as being unique, novel, unfamiliar or uncon-
ventional. But not every unfamiliar combination of
words would appeal as creative. For example, unfa-
1246
miliar biomedical terms, e.g., ?cardiac glycosides?,
are only informative without appreciable creativity.
Similarly, less frequent combinations of words, e.g.,
?rotten detergent? or ?quiet teenager?, though de-
scribing situations that are certainly uncommon, do
not bring about the sense of creativity. Finally, some
unique combinations of words can be just nonsensi-
cal , e.g., ?elegant glycosides?.
Different studies assumed different definitions of
linguistic creativity depending on their context and
end goals (e.g., Chomsky (1976), Zhu et al (2009),
Gerva?s (2010), Maybin and Swann (2007), Carter
and McCarthy (2004)). In this paper, as an opera-
tional definition, we consider a phrase creative if it
is (a) unconventional or uncommon, and (b) expres-
sive in an interesting, imaginative, or inspirational
way.
A system that can recognize creative expressions
could be of practical use for many aspiring writers
who are often in need of inspirational help in search-
ing for the optimal choice of words. Such a system
can also be integrated into automatic assessment of
writing styles and quality, and utilized to automat-
ically construct a collection of interesting expres-
sions from the web, which may be potentially useful
for enriching natural language generation systems.
With these practical goals in mind, we aim to un-
derstand phrases with linguistic creativity in a broad
scope. Similarly as the work of Zhu et al (2009),
our study encompasses phrases that evoke the sense
of interestingness and creativity in readers? minds,
rather than focusing exclusively on clearly but nar-
rowly defined figure of speeches such as metaphors
(e.g., Shutova (2010)), similes (e.g., Veale et al
(2008), Hao and Veale (2010)), and humors (e.g.,
Mihalcea and Strapparava (2005), Purandare and
Litman (2006)). Unlike the study of Zhu et al
(2009), however, we concentrate specifically on how
combinations of different words give rise to the
sense of creativity, as this is an angle that has not
been directly studied before. We leave the roles of
syntactic elements as future research.
We first examine various correlates of perceived
creativity based on information theoretic measures
and the connotation of words, then present experi-
ments based on supervised learning that give us fur-
ther insights on how different aspects of lexical com-
position collectively contribute to the perceived cre-
ativity.
2 Theories of Creativity and Hypotheses
Many researchers, from the ancient philosophers to
the modern time scientists, have proposed theories
that attempt to explain the mechanism of creative
process. In this section, we draw connections from
some of these theories developed for general human
creativity to the problem of quantitatively interpret-
ing linguistic creativity in lexical composition.
2.1 Divergent Thinking and Composition
Divergent thinking (e.g., McCrae (1987)), which
seeks to generate multiple unstereotypical solutions
to an open ended problem has been considered as
the key element in creative process, which contrasts
with convergent thinking that find a single, cor-
rect solution (e.g., Cropley (2006)). Applying the
same high-level idea to lexical composition, diver-
gent composition that explores an unusual, uncon-
ventional set of words is more likely to be creative.
Note that the key novelty then lies in the composi-
tional operation itself, i.e., the act of putting together
a set of words in an unexpected way, rather than the
rareness of individual words being used. In recent
years there has been a swell of work on composi-
tional distributional semantics that captures the com-
positional aspects of language understanding, such
as sentiment analysis (e.g., Yessenalina and Cardie
(2011), Socher et al (2011)) and language model-
ing (e.g., Mitchell and Lapata (2009), Baroni and
Zamparelli (2010), Guevara (2011), Clarke (2012),
Rudolph and Giesbrecht (2010)). However, none
has examined the compositional nature in quantify-
ing creativity in lexical composition.
We consider two computational approaches to
capture the notion of creative composition. The first
is via various information theoretic measures, e.g.,
relative entropy reduction, to measure the surprisal
of seeing the next word given the previous word.
The second is via supervised learning, where we ex-
plore different modeling techniques to capture the
statistical regularities in creative compositional op-
erations. In particular, we will explore (1) compo-
sitional operations of vector space models, (2) ker-
nels capturing the non-linear composition of differ-
ent dimensions in the meaning space, (3) the use of
1247
neural networks as an alternative to incorporate non-
linearity in vector composition. (See ?5).
2.2 Latent Memory and Creative Semantic
Subspace
Although we expect that unconventional composi-
tion has a connection to creativeness of resulting
phrases, that alone does not explain many counter
examples where the composition itself is uncommon
but the resulting expression is not creative due to
lack of interestingness or imagination, e.g., ?room
and water?.2 Therefore, we must consider addi-
tional conditions that give rise to creative phrases.
Let S represent the semantic space, i.e., the set of
all possible semantic representation that can be ex-
pressed by a phrase that is composed of two content
words.3 Then we hypothesize that some subsets of
semantic space {Si|Si ? S} are semantically futile
regions for appreciable linguistic creativity, regard-
less of how novel the composition in itself might be.
Such regions may include technical domains such as
law or pharmacology. Similarly, we expect seman-
tically fruitful subsets of semantic space where cre-
ative expressions are more frequently found. For in-
stance, phrases such as ?guns and roses? and ?metal
to the petal? are semantically close to each other and
yet both can be considered as interesting and cre-
ative (as opposed to one of them losing the sense of
creativity due to its semantic proximity to the other).
This notion of creative semantic subspace con-
nects to theories that suggest that latent memories
serve as motives for creative ideas and that one?s
creativity is largely depending on prior experience
and knowledge one has been exposed to (e.g., Freud
(1908), Necka (1999), Glaskin (2011), Cohen and
Levinthal (1990), Amabile (1997)), a point also
made by Einstein: ?The secret to creativity is know-
ing how to hide your sources.?
Figure 5 presents visualized supports for creative
semantic subspace,4 where we observe that phrases
in the neighborhood of legal terms are generally
not creative, while the semantic neighborhood of
2With additional context this example may turn into a cre-
ative one, but for simplicity we focus on phrases with two con-
tent words considered out of context.
3Investigation on recursive composition of more than two
content words and the influence of syntactic packaging is left as
future research.
4See ?6 for more detailed discussion.
Source # of # of Avg
uniq sent sent Entropy
words len
QUOTESraw 29498 49402 28 173.05
GLOSSESraw 20869 7745 53 96.79
Table 1: Entropy of word distribution in datasets
Dataset
# of word pairs percentage
total #(-) #(+) #(+)/total %
GLOSSES 1912 149 18 0.94
QUOTES 3298 204 35 1.06
Table 2: Distribution of creative(+)/common(-) word
pairs over GLOSSES and QUOTES dataset.
?kingdom? and ?power? is relatively more fruitful
for composing creative (i.e., unique and uncommon
while being imaginative and interesting, per our op-
erational definition of creativity given in ?1) word
pairs, e.g., invisible empire?. In our empirical in-
vestigation, this notion of semantically fruitful and
futile semantic subspaces are captured using dis-
tributional semantic space models under supervised
learning framework (?5).
2.3 Affective Language
Another angle we probe is the connection between
creative expressions and the use of affective lan-
guage. This idea is supported in part by previ-
ous research that explored the connection between
figurative languages such as metaphors and senti-
ment (e.g., Fussell and Moss (1998), Rumbell et
al. (2008), Rentoumi et al (2012)). The focus of
previous work was either on interpretation of the
sentiment in metaphors, or the use of metaphors
in the description of affect. In contrast, we aim
to quantify the correlation between creative expres-
sions (beyond metaphors) and the use of sentiment-
laden words in a more systematic way. This explo-
ration has a connection to the creative semantic sub-
space discussed earlier (?2.2), but pays a more direct
attention to the aspect of sentiment and connotation.
3 Creative Language Dataset
We start our investigation by considering two types
of naturally existing collection of sentences: (1)
quotes and (2) dictionary glosses. We expect that
quotes are likely to be rich in creative expressions,
while dictionary glosses stand in the opposite spec-
1248
less	 ?freq	 ? more	 ?freq	 ?
(a)	 ? (b)	 ? (c)	 ?
0	 ?
5	 ?
10	 ?
15	 ?
20	 ?
25	 ?
30	 ?
1	 ? 5	 ? 9	 ? 13	 ? 17	 ? 21	 ? 25	 ? 29	 ? 33	 ? 37	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
0	 ?
5	 ?
10	 ?
15	 ?
20	 ?
25	 ?
1	 ? 9	 ? 17	 ? 25	 ? 33	 ? 41	 ? 49	 ? 57	 ? 65	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
1	 ? 11	 ? 21	 ? 31	 ? 41	 ? 51	 ? 61	 ? 71	 ? 81	 ? 91	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
less	 ?freq	 ? more	 ?freq	 ? less	 ?freq	 ? more	 ?freq	 ?
Glosses	 ? Quotes	 ? All	 ?
Figure 1: Distribution of creative (double lines in blue) versus common (single lines in red) word pairs with varying
ranges of frequencies (x-axis) for GLOSSES, QUOTES and both datasets combined.
lower	 ?val	 ? higher	 ?val	 ?
(a)	 ? (b)	 ? (c)	 ?
lower	 ?val	 ? higher	 ?val	 ? lower	 ?val	 ? higher	 ?val	 ?
0	 ?
5	 ?
10	 ?
15	 ?
20	 ?
1	 ? 5	 ? 9	 ? 13	 ? 17	 ? 21	 ? 25	 ? 29	 ? 33	 ? 37	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
0	 ?
5	 ?
10	 ?
15	 ?
20	 ?
25	 ?
1	 ? 9	 ? 17	 ? 25	 ? 33	 ? 41	 ? 49	 ? 57	 ? 65	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 11	 ? 21	 ? 31	 ? 41	 ? 51	 ? 61	 ? 71	 ? 81	 ? 91	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
Glosses	 ? Quotes	 ? All	 ?
Figure 2: Distribution of creative (double lines in blue) versus common (single lines in red) word pairs with varying
ranges of PMI values (x-axis) for GLOSSES, QUOTES and both datasets combined.
trum of being creative.
QUOTESraw: We crawled inspirational quotes
from ?Brainy Quote?.5
GLOSSESraw: We collected glosses from Ox-
ford Dictionary and Merriam-Webster Dictionary.6
Overall we crawled about 8K definitions. Table 1
shows statistics of the dataset.7
Entropy of word distribution We conjecture that
QUOTES and GLOSSES are different in terms of
word variety, which can be quantified by the entropy
5http://www.brainyquote.com/
6http://oxforddictionaries.com/ and http://www.merriam-
webster.com/. We only consider words appearing in both dic-
tionaries to avoid unusual words such as compound words, e.g.,
?zero-base?.
7QUOTESraw contain 30K unique words and GLOSSESraw
has 20K unique words. QUOTESraw have much arger number
of sentences, while its average sentence is shorter.
of word distributions. To compute the entropy for
each dataset, we use ngram statistics from the corre-
sponding dataset to measure the probability of each
word. As expected, QUOTES dataset has higher
entropy than GLOSSES in Table 1.
3.1 Creative Word Pairs
We extract word pairs corresponding to the follow-
ing syntactic patterns: [NN NN], [JJ NN], [NN JJ]
and [JJ JJ]. Not all pairs from QUOTESraw are cre-
ative, and likewise, not all pairs from GLOSSESraw
are uncreative. Therefore, we perform manual an-
notations to a subset of the collected pairs as fol-
lows. We obtain a small subset of pairs by apply-
ing stratified sampling based on bigram frequency
buckets: first we sort word pairs by their bigram
frequencies obtained from Web 1T corpus (Brants
and Franz (2006)), group them into consecutive fre-
1249
quency buckets each of which containing 400 word
pairs, then sample 40 word pairs from each bucket.
We label word pairs using Amazon Mechnical
Turk (AMT) (e.g., Snow et al (2008)). We ask three
turkers to score each pair in 1-5 scale, where 1 is the
least creative and 5 is the most creative. We then
obtain the final creativity scale score by averaging
the scores over 3 users. In addition, we ask turkers
a series of yes/no questions to help turkers to deter-
mine whether the given pair is creative or not.8 We
determine the final label of a word pair based on two
scores, creativity scale score and yes/no question-
based score. If creativity scale score is 4 or 5 and
question-based score is positive, we label the pair as
creative. Similarly, if creativity scale score is 1 or
2 and question-based score is negative, we label the
pair as common. We discard the rest from the final
dataset. This filtering process is akin to the removal
of neural sentiment in the early work of sentiment
analysis (e.g., Pang et al (2002)).9 Table 2 shows
the statistics of the resulting dataset.
Creative Pairs and their Frequencies: To gain
insights on the stratified sample of word pairs, we
plot the label (? {creative, common}) distribution
of word pairs as a function of simple statistics, such
as a range (bucket) of bigram frequencies or PMI
values of the given pair of words. Both bigram fre-
quencies and PMI scores are computed based on
Google Web 1T corpus Brants and Franz (2006).
Figure 1 shows the results for word frequencies. As
expected, word pairs with high frequencies are much
more likely to be common, while word pairs with
low frequencies can be either of the two. Also as ex-
pected, pairs extracted from QUOTES are relatively
more likely to be creative than those from GLOSSES.
In any case, it is clear that not all rare pairs are cre-
ative.
Creative Pairs and their PMI Scores: Similarly
as above, Figure 2 plots the relation between the
distribution of labels of word pairs and their corre-
sponding PMI. As expected, pairs with high PMI are
more likely to be common, though the trend is not as
8E.g., ?is this word combination boring and not original??
or ?does it provoke unusual imagination??.
9Cohen?s Kappa and Pearson Correlation on the filtered data
are 0.69 and 0.72 respectively. Corresponding scores for the un-
filtered data drop to 0.26 and 0.29 respectively. All the experi-
ments are performed on the filtered data.
Common Creative
quiet teenager inglorious success
constant longitude thorny existence
watery juice relaxed symmetry
noble political sardonic destiny
diet cooking dispassionate history
verbal interpretation poetical enthusiasm
unwelcome situation verbal beauty
migratory tuna earth breathe
lousy businessman disadvantageous peace
terrific marriage alchemical marriage
solved issue deep nonsense
Table 3: Sample Creative / Common Word Pairs
skewed as before.
Final Dataset: From our initial annotation study,
it became apparent to us that creative pairs are very
rare, perhaps not surprisingly, even among infre-
quent pairs. In order to build the word pair corpus
with as many creative pairs as possible, we focus on
infrequent word pairs for further annotation, from
which we construct a larger and balanced set of cre-
ative and common word pairs, with 394 word pairs
for each class. The specific construction procedure
is as follows: first combine all of the word pairs
extracted from both QUOTESraw and GLOSSESraw
as a single dataset, sort them by bigram frequency,
group them into consecutive frequency buckets each
of which has 40 word pairs; finally balance each fre-
quency bucket, by discarding word pairs with higher
frequency value from the larger class in that bucket.
Examples of labeled word pairs are shown in Ta-
ble 3. Hereafter we use this balanced dataset of word
pairs for all experiments.10
4 Creativity Measures
4.1 Information Measures
In this section we explore information theoretic
measures to quantify the surprisal aspect of creative
word pairs, relating to the divergent, compositional
nature of creativity discussed in ?2.1.
Entropy of Context Seeing a word w changes our
expectation on what might follow next. Some words
have stronger selective preference (higher entropy)
than others.
10The resulting dataset is available at http://www.cs.
stonybrook.edu/?pkuznetsova/creativity/
1250
10	 ?
30	 ?
50	 ?
70	 ?
90	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
MI(w1,w2)	 ?	 ?
30	 ?
40	 ?
50	 ?
60	 ?
70	 ?
80	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?
%	 ?
of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
Lconn(w1,w2)	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
70	 ?
80	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
Lsubj(w1,w2)	 ?
10	 ?
30	 ?
50	 ?
70	 ?
90	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
H(w1w2)	 ?
10	 ?
30	 ?
50	 ?
70	 ?
90	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?
%	 ?
of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?	 ?
RH(w1,w2)	 ?
10	 ?
30	 ?
50	 ?
70	 ?
90	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?
%	 ?
of
	 ?w
or
dp
air
s	 ?
bucket	 ?
KL(w1w2,w2)	 ?	 ?
(a)	 ? (b)	 ? (c)	 ?
(d)	 ? (e)	 ? (f)	 ?
diff	 ? diff	 ?
Figure 3: Distribution of creative (double lines in blue) versus common (single lines in red) word pairs with varying
ranges of information or polarity measures (x-axis).
0	 ?
0.01	 ?
0.02	 ?
0.03	 ?
0.04	 ?
0.05	 ?
0.06	 ?
Mi
lto
n	 ? as	 ? bes
t	 ?
bu
sy	 ?
cle
ar	 ?
com
mo
n	 ?
coo
l	 ?
dee
p	 ?
ear
ly	 ? end
	 ?
exp
ens
ive
	 ?
fin
al	 ? gla
d	 ?
har
d	 ? ho
t	 ?
ing
ot	 ? las
t	 ?
like
ly	 ?
ma
ny	 ? nex
t	 ?
ow
n	 ?
ple
ase
d	 ?
pro
fes
sio
nal
	 ?
rar
ely
	 ?
rol
e	 ?
ser
iou
sly
	 ?
slo
w	 ?
spe
cia
l	 ?
suc
ces
s	 ? to	 ?
val
uab
le	 ?
we
lco
me
	 ?
Figure 4: Conditional probability of neighboring words
for ?inglorious? (filled / red) and ?very? (unfilled / blue).
For instance, the entropy after seeing ?very?
would be higher than that after seeing ?inglorious?,
as the former can be used in a wider variety of con-
text than the later. Figure 4 visualizes relatively
more skewed distribution of ?inglorious?. We com-
pute the entropy of future context conditioning on
w1, w2 and w1w2, which we denote as H(w1),
H(w2), H(w1w2) respectively, latter is shown in
Figure 3 ? a.11
11As before, language models are drawn from Google Web
Relative Entropy Transformation In order to fo-
cus more directly on the relative change of entropy
as a result of composition, we compute Relative En-
tropy Transformation:
RH(w1, w2) =
|H(w1)?H(w1w2)|
H(w1) +H(w1w2)
(1)
As expected (Figure 3 ? b and Table 4), this relative
quantity captures creativity better than the absolute
measure H(w1w2) computed above. The idea be-
hind this measure has a connection to uncertainty
reduction in psycholinguistic literature (e.g., Frank
(2010), Hale (2003), Hale (2006)).
KL divergence To capture unusual combinations
of words, we compare the difference between the
distributional contexts of w1 and w1w2 so that
KL(w1w2, w1) =
?
wi?V
P (wi|w1, w2) log
P (wi|w1, w2)
P (wi|w1)
(2)
Figure (3 ? c) shows thatKL(w1w2, w1)12 is among
1T corpus Brants and Franz (2006).
12We also compute KL(w1, w2) in a similar manner as
KL(w1w2, w1)
1251
the effective measures in capturing creative pairs.
Mutual Information Finally, we consider mutual
information (Figure 3 ? d):
MI(w1, w2) =
?
wi?V
P (wi|w1, w2)?
log
P (wi|w1, w2)
P (wi|w1) ? P (wi|w2)
(3)
Correlation coefficients Pearson coefficients for
all measures are shown in Table 4. Interestingly, in-
formation theoretic measures that compare the dis-
tribution of word?s context, such as RH(w1, w2),
KL(w1w2, w1) and MI(w1, w2), capture the sur-
prisal aspect of creativity better than simple frequen-
cies or PMI scores that do not consider contextual
changes. But even for those cases when the corre-
lation is statistically significant, the values are not
too high. We conjecture that there are two reasons
for this. First, Pearson assumes linear correlations,
hence not sensitive enough to capture non-linear cor-
relations that are evident in graphs shown in Fig-
ure 3. Second, these measures only capture the sur-
prisal aspect of creativity, missing the other impor-
tant qualities: interestingness or imaginativeness.
4.2 Sentiment and Connotation
Next we investigate the connection between creativ-
ity and sentiment, as illustrated in ?2.3. We con-
sider both sentiment (more explicit) and connotation
(more implicit) words,13 and consider them with
or without distinguishing the polarity (i.e., positive,
negative). To determine sentiment and connotation,
we use lexicons provided by OpinionFinder (Wilson
et al (2005)) and Feng et al (2013) respectively. We
denote polarity of a word wi as L(wi).14 When wi
has a negative polarity L(wi) is assigned a value of
-1, and when wi is positive L(wi) is equal to 1. We
assume that a word is neutral when it is not in the
lexicon, assigning 0 to L(wi). For a word pair w1w2
we compute absolute difference Ldiff (w1, w2) be-
tween polarities of tokens in a word pair in order to
catch examples such as ?inglorious success?.
13E.g., expressions such as ?blue sky? or ?white sand? are
not sentiment-laden, but do have positive connotation.
14We denote polarity from OpinionFinder as Lsubj and con-
notation as Lconn
Measure Corr Coeff p-value? adj p-value??
pointwise, noncontextual
Freq(w1w2) 0.014 0.67 0.86
PMI(w1, w2) 0.011 0.75 0.86
information theoretic, contextual
E(w1) -0.038 0.26 0.49
E(w2) -0.126 0.00019 0.00083
E(w1, w2) 0.013 0.71 0.86
RH(w1, w2) 0.113 0.00081 0.0024
KL(w1w2, w1) 0.134 7.152-05 0.00054
KL(w1, w2) -0.080 0.018 0.039
MI(w1, w2) 0.125 0.00022 0.00083
sentiment & connotation
Lsubj(w1) 0.006 0.87 0.87
Lsubj(w2) 0.031 0.36 0.60
Ldiffsubj (w1, w2) 0.168 6.67e-07 1.00e-05
Lconn(w1) 0.023 0.49 0.74
Lconn(w2) 0.008 0.80 0.86
Ldiffconn(w1, w2) 0.082 0.015 0.038
Table 4: Pearson correlation between various measures
and creativity of word pairs. Boldface denotes statistical
significance (p ? 0.05).
note *: Two-tailed p-value, 394 word pairs per class
note **: We used Benjamini-Hochberg method to adjust
p-values for multiple tests
Table 4 shows Pearson coefficient for sentiment
and connotation based measures. It turns out that
polarity of each word on its own does not have a
high impact on the creativity of a word pair. Rather,
it is the difference between the two words that gives
rise the sense of creativity.
4.3 Learning to Recognize Creativity
Now we put together all measures explored in ?4.1
and 4.2 in a supervised-learning framework. As ex-
pected, rather than either one alone, the combination
of various measures leads to the best performance:
~F12 = [RH(w1, w2);KL(w1, w2);H(w1w2);
Ldiffconn(w1, w2);PMI(w1, w2);
H(w2);KL(w1w2, w1);KL(w2, w1);
Ldiffsubj (w1, w2);MI(w1, w2);
Freq(w1w2);H(w1)]
Table 5 shows the performance of the above fea-
ture vector with 12 features using libsvm (Chang and
Lin, 2011). We use C-Support Vector Classification
(C-SVC). Performance is reported in accuracy using
5-fold cross validation.15
15Among these 12 features, the feature selection algorithm
1252
5 Learning Creative Pairs with
Distributional Semantic Vectors
The measures explored in ?4 were largely unin-
formed of distributional semantic dimensions of
each word. However, in order to pursue the concep-
tual aspect of creativity illustrated in ?2.2, that is, the
notion of semantic subspaces that are inherently fu-
tile or fruitful for creativity, we need to incorporate
semantic representations more directly. We there-
fore explore the use of distributional vector space
models. Another goal of this section will be addi-
tional learning-based investigation to the composi-
tional nature of creative word pairs, complementing
the investigation in ?4, which focused on the com-
positional aspect of creativity described in ?2.1.
With above goals in mind, in what follows, we ex-
plore three different ways to learn compositional as-
pect of creative word pairs: (1) learning with explicit
compositional vector operations (?5.1), (2) learning
nonlinear composition via kernels (?5.2), (3) learn-
ing nonlinear composition via deep learning (?5.3).
Note that in all these approaches, the notion of cre-
ative semantic subspace is integrated indirectly, as
the feature representation always incorporates the
resulting (composed) vector representations.
Baseline & Configuration We consider the con-
catenation of two word vectors [~w1; ~w2] as the base-
line, since it can be viewed as what simple bag-of-
word features would be. Since the size of creative
pair dataset is not at scale yet, we choose to work
with vector space models that are in reduced dimen-
sions. We experimented with both Non-Negative
Sparse Embedding (Murphy et al (2012)) and neu-
ral semantic vectors of Huang et al (2012), but re-
port experiments with the latter only as those gave
us slightly better results.
5.1 Compositional Vector Operations
We consider the following compositional vector op-
erations inspired by recent studies for composi-
tional distributional semantics (e.g., Guevara (2011),
Clarke (2012), Mitchell and Lapata (2008), Wid-
dows (2008)).
? ADD: ~w1 + ~w2
? DIFF: abs(~w1 ? ~w2)
of Chen and Lin (2005) determines that the most two important
ones are RH(w1, w2) and KL(w1, w2).
? MULT: ~w1 .? ~w2
? MIN: min{~w1, ~w2}
? MAX: max{~w1, ~w2}
All operations take two input vectors ? Rn, and
output a vector ? Rn. Each operation is applied
element-wise. We then perform binary classifica-
tion over the composed vectors using linear SVM.
Besides using features based on the composed vec-
tors, we also experiment with features based on con-
catenating multiple composed vectors, in the hope to
capture more diverse compositional operations. See
Table 5 for more details and experimental results.
5.2 Learning Nonlinear Composition via
Kernels
As an alternative to explicit vector compositions, we
also probe implicit operations based on non-linear
combinations of semantic dimensions using kernels
(e.g., Scho?lkopf and Smola (2002), Shawe-Taylor
and Cristianini (2004)), in particular:
? Polynomial: K(x, y) = (?xT y + r)d, ? > 0
? RBF: K(x, y) = exp(?? ?x? y?2), ? > 0
? Laplacian: K(x, y) = exp(?? ?x? y?), ? > 0
5.3 Learning Non-linear Composition via Deep
Learning
Yet another alternative to model non-linear com-
position is deep learning. To learn the non-linear
transformation of a pair of semantic vectors, we ex-
plore the use of autoencoders (e.g., Pollack (1990),
Voegtlin and Dominey (2005)). We follow the for-
mulation of vector composition proposed by Socher
et al (2011) except that we do not stack autoen-
coders for recursion. More specifically, given the
two input words ~w1, ~w2 ? Rn, we want to learn
a vector space representation of their combination
~p ? Rn. The recursive auto encoder (RAE) of
Socher et al (2011) models the composition of a
word pair as a non-linear transformation of their
concatenation [~w1; ~w2]:
~p = f(M1[~w1; ~w2] +~b1) (4)
where M1 ? Rn?2n. After adding a bias term
~b1 ? Rn, a nonlinear element-wise function f such
as tanh is applied to the resulting vector. The repre-
sentation ~p of the word pair is then fed into a recon-
struction layer to reconstruct the two input vectors,
1253
Methods Accuracy
Creativity measures (?4.3)
~F12 62.30
Baseline: vector concatenation (no composition)
[~w1; ~w2] 67.51
Explicit vector composition (?5.1)
~w1 + ~w2 66.62
abs(~w1 ? ~w2) 60.03
min{~w1, ~w2} 66.08
max{~w1, ~w2} 64.97
~w1 .? ~w2 56.34
[abs(~w1 ? ~w2); ~w1; ~w2] 69.54
[max{~w1, ~w2}; ~w1; ~w2] 68.02
Non-linear composition via kernels (?5.2)
Polynomial 65.86
RBF 69.16
Laplacian 68.15
Non-linear composition via deep learning (?5.3)
f(M1[~w1; ~w2] +~b1) 67.25
Table 5: Performance comparison of creativity classifiers.
Incorrectly predicted y? Semantically close y?
word pairs word pairs
CONFUSION DUE TO WORD SIMILARITY (20/42)
?entire carton? - ?whole angst? +
?outdated tax? - ?graconian tax? +
?dismissive way? - ?amorous way? +
?insidious part? + ?leather part? -
CONFUSION DUE TO SUBJECTIVE LABELING (8/42)
?independent + ?wonderful -
religion? religion?
WORD SENSE DISAMBIGUATION PROBLEMS (2/42)
?fiscal cliff? - ?winding lake? +
?opera window? + ?work-shop floor? -
Table 6: Error analysis: y? denotes the true label. For
each incorrectly predicted word pair (left column), we
show an example of semantically close word pairs (right
column) with the opposite true label that might have con-
fused learning.
and a softmax layer to predict the probability of the
word pair being creative and not creative. We ini-
tialize the word vectors using the pre-learned vector
space representations in Huang et al (2012).
5.4 Experimental Results
Table 5 shows the performance comparison of dif-
ferent features sets and algorithms. In all cases,
parameters are tuned from the training portion of
the data. We see that simple vector composition
alone does not perform better than vector concate-
nation [~w1; ~w2]. However, combining abs(~w1? ~w2)
or max{~w1; ~w2} with [~w1; ~w2] perform better than
concatenation. Kernels with non-linear transforma-
tion of feature space generally improve performance
over linear SVM, suggesting that kernels capture
some of the interesting compositional aspect of cre-
ativity that is not covered by some of the explicit
vector compositions considered in ?5.1. We also ex-
perimented with additional features driven from the
creativity measures explored in ?4, but we omit their
results as those did not help improving the perfor-
mance. Unfortunately learning nonlinear composi-
tion with deep learning did not yield better results.
We conjecture that it is due to the small dataset we
were able to obtain for this study, which may have
not been enough to learn the rich parameter space of
the nonlinear transformation matrix.
6 Analysis and Insight
Error analysis We manually inspected a ran-
domly chosen 42 error cases, and characterize the
potential causes of those errors. Examples of three
types of errors are shown in Table 6. For each incor-
rectly predicted word pair, we also show a seman-
tically close word pair with the opposite true label
that might have confused the learning algorithm.
Visualization To gain additional insight, we
project word pairs represented in their vec-
tor concatenations onto 2-dimensional space us-
ing t-Distributed Stochastic Neighbor Embedding
(van der Maaten and Hinton (2008)). Figure 5
shows some of the interesting regions of the pro-
jection: some regions are relatively futile in hav-
ing creative phrases (e.g., regions involving simple
adjectives such as ?good?, ?bad?, regions corre-
sponding to legal terms), while some regions are rel-
atively more fruitful (e.g., regions involving abstract
adjectives such as ?infinite?, ?universal?, ?funda-
mental?). There are also many other regions (e.g., in
the vicinity of ?true?, ?perfect? or ?intelligent? in
Figure 5) where the separation between creative and
noncreative phrases are not as prominent. In those
regions, compositional aspects would play a bigger
role in determining creativity than memorizing fruit-
ful semantic subspaces.
1254
?? infinite promise 
?? infinite leisure 
?? universal aspiration 
?? perfect disorder 
?? absolute barbarism 
?? fundamental soul 
?? theoretical wisdom 
?? fundamental key 
?? technological refinement 
?? good marathon 
?? good custodian 
?? universal anguish 
?? pure phenomenology 
?? logical market 
?? true perversion 
?? perfect fire 
?? perfect land 
?? true ambition 
?? true golfer 
?? normal professor 
?? normal adulthood 
?? bad profession 
?? bad motivation 
?? intelligent manipulation 
?? intelligent vocabulary 
?? honest coward 
?? human spark ?? human architecture 
?? human masterpiece 
?? human incompetence 
?? legal slavery ?? legal corporation 
?? legal trading 
?? legal progress 
?? judicial verdict 
?? -------- -------- 
?? -------- -------- 
?? -------- --------- 
?? -------- -------- 
?? -------- -------- 
?? -------- --------- 
?? -------- --------- 
?? -------- -------- 
?? -------- -------- 
?? -------- -------- 
?? -------- --------- ?? -------- -------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- -------- 
?? -------- -------- 
?? -------- -------- 
?? invisible empire 
?? omnipotent realm 
?? finite realm 
?? -------- -------- 
?? -------- -------- 
?? -------- -------- 
?? -------- -------- 
?? -------- -------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- -------- 
?? -------- -------- 
Figure 5: Creative (blue bold) and not creative (red italic) word pairs graph.
7 Related Work
Among computational approaches that touch on lin-
guistic creativity, many focused on metaphor (e.g.,
Dunn (2013), Krishnakumaran and Zhu (2007),
Mashal et al (2007), Rumbell et al (2008), Ren-
toumi et al (2012), Mashal et al (2009)). Other lin-
guistic devices and phenomena related to creativity
include irony (e.g., Davidov et al (2010), Gonza?lez-
Iba?n?ez et al (2011), Filatova (2012)), neologism
(e.g., Cartoni (2008)), humor (e.g., Mihalcea and
Strapparava (2005), Purandare and Litman (2006)),
and similes (e.g., Hao and Veale (2010)).
Veale (2011) proposed the new task of creative
text retrieval to harvest expressions that potentially
convey the same meaning as the query phrase in
a fresh or unusual way. Our work contributes to
the retrieval process of recognizing more creative
phrases. Ozbal and Strapparava (2012) explored
automatic creative naming of commercial products
and services, focusing on the generation of creative
phrases within a specific domain. Costello (2002)
investigated the cognitive process that guides peo-
ple?s choice of words when making up a novel noun-
noun compound. In contrast, we present a data-
driven investigation to quantifying creativity in lex-
ical composition. Memorability is loosely related to
linguistic creativity (Danescu-Niculescu-Mizil et al
(2012)) as some of the creative quotes may be more
memorable, but not all creative phrases are memo-
rable and vice versa.
8 Conclusion
We presented the first study that focuses on learn-
ing and quantifying creativity in lexical composi-
tions, exploring statistical techniques motivated by
three different theories and hypotheses of creativ-
ity, ranging from divergent thinking, compositional
structure, creative semantic subspace, and the con-
nection to sentiment and connotation. Our experi-
mental results suggest the viability of learning cre-
ative language, and point to promising directions for
future research.
Acknowledgments This research was supported
in part by the Stony Brook University Office of the
Vice President for Research, and in part by gift from
Google. We thank anonymous reviewers for insight-
ful comments and suggestions.
References
T. Amabile. 1997. Motivating creativity in organiza-
tions: On doing what you love and loving what you
do. California Management Review, 40(1):39?58.
1255
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183?1193, Stroudsburg, PA, USA.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
corpus version 1.1. Google Inc.
Ronald Carter and Michael McCarthy. 2004. Talking,
creating: interactional language, creativity, and con-
text. Applied Linguistics, 25(1):62?88.
Bruno Cartoni. 2008. Lexical resources for automatic
translation of constructed neologisms: the case study
of relational adjectives. In LREC.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Yi-Wei Chen and Chih-Jen Lin. 2005. Combining svms
with various feature selection strategies. In Taiwan
University. Springer-Verlag.
Noam Chomsky. 1965. Aspects of the Theory of Syntax,
volume 11. The MIT press.
Carol Chomsky. 1976. Creativity and innovation in child
language. Journal of Education, Boston.
Daoud Clarke. 2012. A context-theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1):41?71.
W.M. Cohen and D.A. Levinthal. 1990. Absorptive Ca-
pacity: A New Perspective on Learning and Innova-
tion. Administrative Science Quarterly, 35(1).
Fintan J. Costello. 2002. Investigating creative language:
People?s choice of words in the production of novel
noun-noun compounds. In Proceedings of the 24th
Annual Conference of the Cognitive Science Society.
Arthur Cropley. 2006. In praise of convergent thinking.
Creativity Research Journal, 18(3):391?404.
Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon
Kleinberg, and Lillian Lee. 2012. You had me at
hello: How phrasing affects memorability. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume
1, pages 892?901. Association for Computational Lin-
guistics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 107?116. Association for
Computational Linguistics.
Jonathan Dunn. 2013. What metaphor identifica-
tion systems can tell us about metaphor-in-language.
Meta4NLP 2013, page 1.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and Yejin
Choi. 2013. Connotation lexicon: A dash of senti-
ment beneath the surface meaning. In Proceedings of
the 51th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers), Sofia,
Bulgaria, Angust. Association for Computational Lin-
guistics.
Elena Filatova. 2012. Irony and sarcasm: Corpus gen-
eration and analysis using crowdsourcing. In LREC,
pages 392?398.
Stefan L Frank. 2010. Uncertainty reduction as a mea-
sure of cognitive processing effort. In Proceedings of
the 2010 workshop on cognitive modeling and com-
putational linguistics, pages 81?89. Association for
Computational Linguistics.
Sigmund Freud. 1908. Creative writers and day-
dreaming. Standard edition, 9:143?153.
Susan R. Fussell and Mallie M. Moss. 1998. Figurative
language in descriptions of emotional states. In Social
and cognitive approaches to interpersonal communi-
cation.
Pablo Gerva?s. 2010. Engineering linguistic creativ-
ity: Bird flight and jet planes. In Proceedings of
the NAACL HLT 2010 Second Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 23?
30. Association for Computational Linguistics.
Katie Glaskin. 2011. Dreams, memory, and the ances-
tors: creativity, culture, and the science of sleep. Jour-
nal of the royal anthropological institute, 17(1):44?62.
Andrew Goatly. 1997. The language of metaphors.
Routledge.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In ACL (Short Papers), pages 581?586.
Citeseer.
Emiliano Guevara. 2011. Computing semantic composi-
tionality in distributional semantics. In Proceedings of
the Ninth International Conference on Computational
Semantics (IWCS 2011), pages 135?144. Citeseer.
John Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101?123.
John Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):643?672.
Yanfen Hao and Tony Veale. 2010. An ironic fist
in a velvet glove: Creative mis-representation in the
construction of ironic similes. Minds and Machines,
20(4):635?650.
Eric H. Huang, Richard Socher, Christopher D. Manning,
and Andrew Y. Ng. 2012. Improving Word Represen-
tations via Global Context and Multiple Word Proto-
types. In Annual Meeting of the Association for Com-
putational Linguistics (ACL).
1256
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources. In
Proceedings of the Workshop on Computational ap-
proaches to Figurative Language, pages 13?20. Asso-
ciation for Computational Linguistics.
George Lakoff and Mark Johnson. 1980. Metaphors we
Live by. University of Chicago Press, Chicago.
N. Mashal, M. Faust, T Hendler, and M. Jung-Beeman.
2007. An fmri investigation of the neural correlates
underlying the processing of novel metaphoric expres-
sions. Brain and Language, pages 115 ? 126.
N Mashal, M Faust, T Hendler, and M Jung-Beeman.
2009. An fmri study of processing novel metaphoric
sentences. Laterality, (1):30?54.
Janet Maybin and Joan Swann. 2007. Everyday creativ-
ity in language: Textuality, contextuality, and critique.
Applied Linguistics, 28(4):497?517.
Robert R McCrae. 1987. Creativity, divergent thinking,
and openness to experience. Journal of personality
and social psychology, 52(6):1258.
Susan M. McCurry and Steven C. Hayes. 1992. Clinical
and experimental perspectives on metaphorical talk.
Clinical Psychology Review, 12(7):763 ? 785.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-
ing computers laugh: Investigations in automatic hu-
mor recognition. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 531?
538, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings of
ACL-08: HLT, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2009. Language mod-
els based on semantic composition. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 1-Volume 1, pages
430?439. Association for Computational Linguistics.
Brian Murphy, Partha Pratim Talukdar, and Tom
Mitchell. 2012. Learning effective and interpretable
semantic models using non-negative sparse embed-
ding. In COLING, pages 1933?1950.
Edward Necka. 1999. Memory and creativity. Ency-
clopedia of creativity, ed. by MA Runco, SR Pritzker,
2:193?99.
Gozde Ozbal and Carlo Strapparava. 2012. A compu-
tational approach to the automation of creative nam-
ing. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 703?711, Jeju Island, Korea, July.
Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, pages 79?86. Associa-
tion for Computational Linguistics.
J. B. Pollack. 1990. Recursive distributed representation.
Artificial Intelligence, 46:77?105.
Amruta Purandare and Diane Litman. 2006. Humor:
Prosody analysis and automatic recognition for f* r* i*
e* n* d* s*. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 208?215. Association for Computational Lin-
guistics.
Vassiliki Rentoumi, George A. Vouros, Vangelis
Karkaletsis, and Amalia Moser. 2012. Investigating
metaphorical language in sentiment analysis: A sense-
to-sentiment perspective. ACM Trans. Speech Lang.
Process., 9(3):6:1?6:31, November.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 907?916. Asso-
ciation for Computational Linguistics.
Tim Rumbell, John Barnden, Mark Lee, and Alan
Wallington. 2008. Affect in metaphor: Developments
with wordnet.
Bernhard Scho?lkopf and Alexander J Smola. 2002.
Learning with kernels. The MIT Press.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
methods for pattern analysis. Cambridge university
press.
Ekaterina Shutova. 2010. Models of metaphor in nlp.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ?10,
pages 688?697, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the conference on empirical
methods in natural language processing, pages 254?
263. Association for Computational Linguistics.
Richard Socher, Jeffrey Pennington, Eric H Huang, An-
drew Y Ng, and Christopher D Manning. 2011. Semi-
supervised recursive autoencoders for predicting sen-
timent distributions. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 151?161. Association for Computational
Linguistics.
L.J.P. van der Maaten and G.E. Hinton. 2008. Visualiz-
ing high-dimensional data using t-sne.
Tony Veale, Yanfen Hao, and Guofu Li. 2008. Multilin-
gual harvesting of cross-cultural stereotypes. In ACL,
pages 523?531.
1257
Tony Veale. 2011. Creative language retrieval: A ro-
bust hybrid of information retrieval and linguistic cre-
ativity. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 278?287, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Thomas Voegtlin and Peter F. Dominey. 2005. Linear
recursive distributed representations. Neural Netw.,
18(7):878?895, September.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the Sec-
ond AAAI Symposium on Quantum Interaction.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: A system for subjectivity analysis. In
Proceedings of HLT/EMNLP on Interactive Demon-
strations, pages 34?35. Association for Computational
Linguistics.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 172?182. As-
sociation for Computational Linguistics.
Xiaojin Zhu, Zhiting Xu, and Tushar Khot. 2009. How
creative is your writing? a linguistic creativity mea-
sure from computer science and cognitive psychology
perspectives. In Proceedings of the Workshop on Com-
putational Approaches to Linguistic Creativity, pages
87?93. Association for Computational Linguistics.
1258
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1443?1448,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Where Not to Eat? Improving Public Policy by Predicting Hygiene
Inspections Using Online Reviews
Jun Seok Kang? Polina Kuznetsova?
?Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{junkang,pkuznetsova,ychoi}
@cs.stonybrook.edu
Michael Luca? Yejin Choi?
?Harvard Business School
Soldiers Field Road
Boston, MA 02163
mluca@hbs.edu
Abstract
This paper offers an approach for governments
to harness the information contained in social
media in order to make public inspections and
disclosure more efficient. As a case study, we
turn to restaurant hygiene inspections ? which
are done for restaurants throughout the United
States and in most of the world and are a fre-
quently cited example of public inspections
and disclosure. We present the first empiri-
cal study that shows the viability of statistical
models that learn the mapping between tex-
tual signals in restaurant reviews and the hy-
giene inspection records from the Department
of Public Health. The learned model achieves
over 82% accuracy in discriminating severe
offenders from places with no violation, and
provides insights into salient cues in reviews
that are indicative of the restaurant?s sanitary
conditions. Our study suggests that public
disclosure policy can be improved by mining
public opinions from social media to target in-
spections and to provide alternative forms of
disclosure to customers.
1 Introduction
Public health inspection records help customers to
be wary of restaurants that have violated health
codes. In some counties and cities, e.g., LA, NYC,
it is required for restaurants to post their inspec-
tion grades at their premises, which have shown
to affect the revenue of the business substantially
(e.g., Jin and Leslie (2005), Henson et al (2006)),
thereby motivating restaurants to improve their sani-
tary practice. Other studies have reported correlation
between the frequency of unannounced inspections
per year, and the average violation scores, confirm-
ing the regulatory role of inspections in improving
the hygiene quality of the restaurants and decreasing
food-borne illness risks (e.g., Jin and Leslie (2003),
Jin and Leslie (2009), Filion and Powell (2009),
NYC-DoHMH (2012)).
However, one practical challenge in the current
inspection system is that the department of health
has only limited resources to dispatch inspectors,
leaving out a large number of restaurants with un-
known hygiene grades. We postulate that online re-
views written by the very citizens who have visited
those restaurants can serve as a proxy for predicting
the likely outcome of the health inspection of any
given restaurant. Such a prediction model can com-
plement the current inspection system by enlight-
ening the department of health to make a more in-
formed decision when allocating inspectors, and by
guiding customers when choosing restaurants.
Our work shares the spirit of recently emerging
studies that explores social media analysis for pub-
lic health surveillance, in particular, monitoring in-
fluenza or food-poisoning outbreaks from micro-
blogs (e.g., Aramaki et al (2011), Sadilek et al
(2012b), Sadilek et al (2012a), Sadilek et al (2013),
Lamb et al (2013), Dredze et al (2013), von Etter
et al (2010)). However, no prior work has examined
the utility of review analysis as a predictive tool for
accessing hygiene of restaurants, perhaps because
the connection is not entirely conspicuous: after all,
customers are neither familiar with inspection codes,
nor have the full access to the kitchen, nor have been
asked to report on the hygiene aspects of their expe-
1443
review count*review count (filtered)*
Coe
fficie
nt
0.05
0.10
(a)0 10 20 30 40 50
np review count*np review count (filtered)*
  
0.05
0.10
(b)0 10 20 30 40 50
avg review rating*avg review rating (filtered)*
Coe
fficie
nt
?0.05
?0.03
(c)0 10 20 30 40 50
avg review length*avg review length(filtered)*
  
0
0.05
(d)0 10 20 30 40 50Inspection Penalty Score Threshold
Figure 1: Spearman?s coefficients of factors & inspection
penalty scores. ?*?: statistically significant (p ? 0.05)
rience.
In this work, we report the first empirical study
demonstrating the utility of review analysis for pre-
dicting health inspections, achieving over 82% accu-
racy in discriminating severe offenders from places
with no violation, and find predictive cues in reviews
that correlate with the inspection results.
2 Data
We scraped entire reviews written for restaurants in
Seattle from Yelp over the period of 2006 to 2013.1
The inspection records of Seattle is publicly avail-
able at www.datakc.org. More than 50% of the
restaurants listed under Yelp did not have inspection
records, implying the limited coverage of inspec-
tions. We converted street addresses into canonical
forms when matching restaurants between Yelp and
inspection database. After integrating reviews with
inspection records, we obtained about 13k inspec-
1Available at http://www.cs.stonybrook.edu/
?junkang/hygiene/
bimodality*bimodality (filtered)*
Coe
fficie
nt
0
0.05
0.10
(a)0 10 20 30 40 50
fake review count*fake review count (filtered)
   
0
0.05
0.10
(b)0 10 20 30 40 50
Inspection Penalty Score Threshold
Figure 2: Spearman?s coefficients of factors & inspection
penalty scores. ?*?: statistically significant (p ? 0.05)
tions over 1,756 restaurants with 152k reviews. For
each restaurant, there are typically several inspec-
tion records. We defined an ?inspection period? of
each inspection record as the period of time start-
ing from the day after the previous inspection to the
day of the current inspection. If there is no previ-
ous inspection, then the period stretches to the past
6 months in time. Each inspection period corre-
sponds to an instance in the training or test set. We
merge all reviews within an inspection period into
one document when creating the feature vector.
Note that non-zero penalty scores may not nec-
essarily indicate alarming hygiene issues. For ex-
ample, violating codes such as ?proper labeling? or
?proper consumer advisory posted for raw or under-
cooked foods? seem relatively minor, and unlikely to
be noted and mentioned by reviewers. Therefore, we
focus on restaurants with severe violations, as they
are exactly the set of restaurants that inspectors and
customers need to pay the most attention to. To de-
fine restaurants with ?severe violations? we experi-
ment with a varying threshold t, such that restaurants
with score ? t are labeled as ?unhygienic?.2
3 Correlates of Inspection Penalty Scores
We examine correlation between penalty scores and
several statistics of reviews:
I. Volume of Reviews:
2For restaurants with ?hygienic? labels, we only consider
those without violation, as there are enough number of such
restaurants to keep balanced distribution between two classes.
1444
61.42 61.46 66.61
70.83 77.16
81.37
Acc
urac
y (%
)
60
80
Inspection Penalty Score Threshold
0 10 20 30 40 50
Figure 3: Trend of penalty score thresholds & accuracies.
? count of all reviews
? average length of all reviews
II. Sentiment of Reviews: We examine whether
the overall sentiment of the customers correlates
with the hygiene of the restaurants based on follow-
ing measures:
? average review rating
? count of negative (? 3) reviews
III. Deceptiveness of Reviews: Restaurants with
bad hygiene status are more likely to attract negative
reviews, which would then motivate the restaurants
to solicit fake reviews. But it is also possible that
some of the most assiduous restaurants that abide
by health codes strictly are also diligent in solicit-
ing fake positive reviews. We therefore examine the
correlation between hygiene violations and the de-
gree of deception as follows.
? bimodal distribution of review ratings
The work of Feng et al (2012) has shown
that the shape of the distribution of opinions,
overtly skewed bimodal distributions in partic-
ular, can be a telltale sign of deceptive review-
ing activities. We approximately measure this
by computing the variance of review ratings.
? volume of deceptive reviews based on linguistic
patterns
We also explore the use of deception classifiers
based on linguistic patterns (Ott et al, 2011)
to measure the degree of deception. Since no
deception corpus is available in the restaurant
domain, we collected a set of fake reviews and
truthful reviews (250 reviews for each class),
following Ott et al (2011).3
310 fold cross validation on this dataset yields 79.2% accu-
racy based on unigram and bigram features.
Features Acc. MSE SCC
- *50.00 0.500 -
review count *50.00 0.489 0.0005
np review count *52.94 0.522 0.0017
cuisine *66.18 0.227 0.1530
zip code *67.32 0.209 0.1669
avrg. rating *57.52 0.248 0.0091
inspection history *72.22 0.202 0.1961
unigram 78.43 0.461 0.1027
bigram *76.63 0.476 0.0523
unigram + bigram 82.68 0.442 0.0979
all 81.37 0.190 0.2642
Table 1: Feature Compositions & Respective Accuracies,
Respective Mean Squared Errors(MSE) & Squared Cor-
relation Coefficients (SCC), np=non-positive
Filtering Reviews: When computing above statis-
tics over the set of reviews corresponding to each
restaurant, we also consider removing a subset of re-
views that might be dubious or just noise. In partic-
ular, we remove reviews that are too far away (delta
? 2) from the average review rating. Another filter-
ing rule can be removing all reviews that are clas-
sified as deceptive by the deception classifier ex-
plained above. For brevity, we only show results
based on the first filtering rule, as we did not find
notable differences in different filtering strategies.
Results: Fig 1 and 2 show Spearman?s rank corre-
lation coefficient with respect to the statistics listed
above, with and without filtering, computed at dif-
ferent threshold cutoffs ? {0, 10, 20, 30, 40, 50} of
inspection scores. Although coefficients are not
strong,4 they are mostly statistically significant with
p ? 0.05 (marked with ?*?), and show interesting
contrastive trends as highlighted below.
In Fig 1, as expected, average review rating is neg-
atively correlated with the inspection penalty scores.
Interestingly, all three statistics corresponding to the
volume of customer reviews are positively corre-
lated with inspection penalty. What is more inter-
esting is that if potentially deceptive reviews are fil-
tered, then the correlation gets stronger, which sug-
gests the existence of deceptive reviews covering up
unhappy customers. Also notice that correlation is
4Spearman?s coefficient assumes monotonic correlation. We
suspect that the actual correlation of these factors and inspection
scores are not entirely monotonic.
1445
Hygienic gross, mess, sticky, smell, restroom, dirty
Basic Ingredients: beef, pork, noodle, egg, soy,
ramen, pho,
Cuisines Vietnamese, Dim Sum, Thai, Mexican,
Japanese, Chinese, American, Pizza, Sushi, Indian,
Italian, Asian
Sentiment: cheap, never,
Service & Atmosphere cash, worth, district, delivery,
think, really, thing, parking, always, usually, definitely
- door: ?The wait is always out the door when I
actually want to go there?,
- sticker: ?I had sticker shock when I saw the prices.?,
- student: ?heap, large portions and tasty = the perfect
student food!?,
- the size: ?i was pretty astonished at the size of all the
plates for the money.?,
- was dry: ?The beef was dry, the sweet soy and
anise-like sauce was TOO salty (almost inedible).?,
- pool: ?There are pool tables, TV airing soccer games
from around the globe and of course - great drinks!?
Table 2: Lexical Cues & Examples - Unhygienic (dirty)
generally stronger when higher cutoffs are used (x-
axis), as expected. Fig 2 looks at the relation be-
tween the deception level and the inspection scores
more directly. As suspected, restaurants with high
penalty scores show increased level of deceptive re-
views.
Although various correlates of hygiene scores ex-
amined so far are insightful, these alone are not in-
formative enough to be used as a predictive tool,
hence we explore content-based classification next.
4 Content-based Prediction
We examine the utility of the following features:
Features based on customers? opinion:
1. Aggregated opinion: average review rating
2. Content of the reviews: unigram, bigram
Features based on restaurant?s metadata:
3. Cuisine: e.g., Thai, Italian, as listed under Yelp
4. Location: first 5 digits of zip code
5. Inspection History: a boolean feature (?hy-
gienic? or ?unhygienic?), a numerical feature
(previous penalty score rescaled ? [0, 1]), a nu-
meric feature (average penalty score over all
previous inspections)
Hygienic:
Cooking Method & Garnish: brew, frosting, grill,
crush, crust, taco, burrito, toast
Healthy or Fancier Ingredients: celery, calamity,
wine, broccoli, salad, flatbread, olive, pesto
Cuisines : Breakfast, Fish & Chips, Fast Food,
German, Diner, Belgian, European, Sandwiches,
Vegetarian
Whom & When: date, weekend, our, husband,
evening, night
Sentiment: lovely, yummy, generous, friendly, great,
nice
Service & Atmosphere: selection, attitude,
atmosphere, ambiance, pretentious
Table 3: Lexical Cues & Examples - Hygienic (clean)
6. Review Count
7. Non-positive Review Count
Classification Results We use liblinear?s SVM
(Fan et al, 2008) with L1 regularization and 10 fold
cross validation. We filter reviews that are farther
than 2 from the average rating. We also run Sup-
port Vector Regression (SVR) using liblinear. Fig 3
shows the results. As we increase the threshold, the
accuracy also goes up in most cases. Table 1 shows
feature ablation at threshold t = 50, and ?*? denotes
statistically significant (p?0.05) difference over the
performance with all features based on student t-test.
We find that metadata information of restaurants
such as location and cuisine alone show good predic-
tive power, both above 66%, which are significantly
higher than the expected accuracy of random guess-
ing (50%).
Somewhat unexpected outcome is aggregated
opinion, which is the average review rating during
the corresponding inspection period, as it performs
not much better than chance (57.52%). This result
suggest that the task of hygiene prediction from re-
views differs from the task of sentiment classifica-
tion of reviews.
Interestingly, the inspection history feature alone
is highly informative, reaching accuracy upto 72%,
suggesting that the past performance is a good pre-
dictor of the future performance.
Textual content of the reviews (unigram+bigram)
turns out to be the most effective features, reaching
upto 82.68% accuracy. Lastly, when all the features
1446
are combined together, the performance decreases
slightly to 81.37%, perhaps because n-gram features
perform drastically better than all others.
4.1 Insightful Cues
Table 2 and 3 shows representative lexical cues for
each class with example sentences excerpted from
actual reviews when context can be helpful.
Hygiene: Interestingly, hygiene related words are
overwhelmingly negative, e.g., ?gross?, ?mess?,
?sticky?. What this suggests is that reviewers do
complain when the restaurants are noticeably dirty,
but do not seem to feel the need to complement on
cleanliness as often. Instead, they seem to focus on
other positive aspects of their experience, e.g., de-
tails of food, atmosphere, and their social occasions.
Service and Atmosphere: Discriminative fea-
tures reveal that it is not just the hygiene related
words that are predictive of the inspection results of
restaurants. It turns out that there are other quali-
ties of restaurants, such as service and atmosphere,
that also correlate with the likely outcome of inspec-
tions. For example, when reviewers feel the need
to talk about ?door?, ?student?, ?sticker?, or ?the
size? (see Table 2 and 3), one can extrapolate that
the overall experience probably was not glorious. In
contrast, words such as ?selection?, ?atmosphere?,
?ambiance? are predictive of hygienic restaurants,
even including those with slightly negative connota-
tion such as ?attitude? or ?pretentious?.
Whom and When: If reviewers talk about details
of their social occasions such as ?date?, ?husband?,
it seems to be a good sign.
The way food items are described: Another in-
teresting aspect of discriminative words are the way
food items are described by reviewers. In general,
mentions of basic ingredients of dishes, e.g., ?noo-
dle?, ?egg?, ?soy? do not seem like a good sign. In
contrast, words that help describing the way dish is
prepared or decorated, e.g., ?grill?, ?toast?, ?frost-
ing?, ?bento box? ?sugar? (as in ?sugar coated?)
are good signs of satisfied customers.
Cuisines: Finally, cuisines have clear correlations
with inspection outcome, as shown in Table 2 and 3.
5 Related Work
There have been several recent studies that probe the
viability of public health surveillance by measuring
relevant textual signals in social media, in particu-
lar, micro-blogs (e.g., Aramaki et al (2011), Sadilek
et al (2012b), Sadilek et al (2012a), Sadilek et al
(2013), Lamb et al (2013), Dredze et al (2013), von
Etter et al (2010)). Our work joins this line of re-
search but differs in two distinct ways. First, most
prior work aims to monitor a specific illness, e.g.,
influenza or food-poisoning by paying attention to
a relatively small set of keywords that are directly
relevant to the corresponding sickness. In contrast,
we examine all words people use in online reviews,
and draw insights on correlating terms and concepts
that may not seem immediately relevant to the hy-
giene status of restaurants, but nonetheless are pre-
dictive of the outcome of the inspections. Second,
our work is the first to examine online reviews in the
context of improving public policy, suggesting addi-
tional source of information for public policy mak-
ers to pay attention to.
Our work draws from the rich body of research
that studies online reviews for sentiment analysis
(e.g., Pang and Lee (2008)) and deception detec-
tion (e.g., Mihalcea and Strapparava (2009), Ott et
al. (2011), Feng et al (2012)), while introducing
the new task of public hygiene prediction. We ex-
pect that previous studies for aspect-based sentiment
analysis (e.g., Titov and McDonald (2008), Brody
and Elhadad (2010), Wang et al (2010)) would be a
fruitful venue for further investigation.
6 Conclusion
We have reported the first empirical study demon-
strating the promise of review analysis for predicting
health inspections, introducing a task that has poten-
tially significant societal benefits, while being rele-
vant to much research in NLP for opinion analysis
based on customer reviews.
Acknowledgments
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research, and in part by gift from Google. We thank
anonymous reviewers and Adam Sadilek for helpful
comments and suggestions.
1447
References
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza epi-
demics using twitter. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1568?1576, Edinburgh, Scotland,
UK., July. Association for Computational Linguistics.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 804?812, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mark Dredze, Michael J. Paul, Shane Bergsma, and Hieu
Tran. 2013. Carmen: A twitter geolocation system
with applications to public health. In AAAI Workshop
on Expanding the Boundaries of Health Informatics
Using AI (HIAI).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Song Feng, Longfei Xing, Anupam Gogar, and Yejin
Choi. 2012. Distributional footprints of deceptive
product reviews. In ICWSM.
Katie Filion and Douglas A Powell. 2009. The use of
restaurant inspection disclosure systems as a means of
communicating food safety information. Journal of
Foodservice, 20(6):287?297.
Spencer Henson, Shannon Majowicz, Oliver Masakure,
Paul Sockett, Anria Johnes, Robert Hart, Debora Carr,
and Lewinda Knowles. 2006. Consumer assessment
of the safety of restaurants: The role of inspection
notices and other information cues. Journal of Food
Safety, 26(4):275?301.
Ginger Zhe Jin and Phillip Leslie. 2003. The effect of
information on product quality: Evidence from restau-
rant hygiene grade cards. The Quarterly Journal of
Economics, 118(2):409?451.
Ginger Zhe Jin and Phillip Leslie. 2005. The case in
support of restaurant hygiene grade cards.
Ginger Zhe Jin and Phillip Leslie. 2009. Reputational
incentives for restaurant hygiene. American Economic
Journal: Microeconomics, pages 237?267.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
twitter. In the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL-HLT).
Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 309?
312, Suntec, Singapore, August. Association for Com-
putational Linguistics.
NYC-DoHMH. 2012. Restaurant grading in new york
city at 18 months. New York City Department of
Health and Mental Hygiene.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
309?319, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Adam Sadilek, Henry Kautz, and Vincent Silenzio.
2012a. Predicting disease transmission from geo-
tagged micro-blog data. In Twenty-Sixth AAAI Con-
ference on Artificial Intelligence.
Adam Sadilek, Henry A. Kautz, and Vincent Silenzio.
2012b. Modeling spread of disease from social in-
teractions. In John G. Breslin, Nicole B. Ellison,
James G. Shanahan, and Zeynep Tufekci, editors,
ICWSM. The AAAI Press.
Adam Sadilek, Sean Brennan, Henry Kautz, and Vincent
Silenzio. 2013. nemesis: Which restaurants should
you avoid today? First AAAI Conference on Human
Computation and Crowdsourcing.
Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL-08: HLT, pages 308?316,
Columbus, Ohio, June. Association for Computational
Linguistics.
Peter von Etter, Silja Huttunen, Arto Vihavainen, Matti
Vuorinen, and Roman Yangarber. 2010. Assess-
ment of utility in web mining for the domain of pub-
lic health. In Proceedings of the NAACL HLT 2010
Second Louhi Workshop on Text and Data Mining of
Health Documents, pages 29?37, Los Angeles, Cal-
ifornia, USA, June. Association for Computational
Linguistics.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data: a rat-
ing regression approach. In Proceedings of the 16th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 783?792.
ACM.
1448
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1753?1764,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Success with Style: Using Writing Style to Predict the Success of Novels
Vikas Ganjigunte Ashok Song Feng Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
vganjiguntea, songfeng, ychoi@cs.stonybrook.edu
Abstract
Predicting the success of literary works is a
curious question among publishers and aspir-
ing writers alike. We examine the quantitative
connection, if any, between writing style and
successful literature. Based on novels over
several different genres, we probe the predic-
tive power of statistical stylometry in discrim-
inating successful literary works, and identify
characteristic stylistic elements that are more
prominent in successful writings. Our study
reports for the first time that statistical stylom-
etry can be surprisingly effective in discrim-
inating highly successful literature from less
successful counterpart, achieving accuracy up
to 84%. Closer analyses lead to several new
insights into characteristics of the writing style
in successful literature, including findings that
are contrary to the conventional wisdom with
respect to good writing style and readability.
1 Introduction
Predicting the success of novels is a curious ques-
tion among publishers, professional book reviewers,
aspiring and even expert writers alike. There are po-
tentially many influencing factors, some of which
concern the intrinsic content and quality of the book,
such as interestingness, novelty, style of writing, and
engaging storyline, but external factors such as so-
cial context and even luck can play a role. As a re-
sult, recognizing successful literary work is a hard
task even for experts working in the publication in-
dustries. Indeed, even some of the best sellers and
award winners can go through several rejections be-
fore they are picked up by a publisher.1
Perhaps due to its obvious complexity of the prob-
lem, there has been little previous work that attempts
to build statistical models that predict the success of
literary works based on their intrinsic content and
quality. Some previous studies do touch on the no-
tion of stylistic aspects in successful literature, e.g.,
extensive studies in Literature discuss literary styles
of significant authors (e.g., Ellega?rd (1962), Mc-
Gann (1998)), while others consider content char-
acteristics such as plots, characteristics of charac-
ters, action, emotion, genre, cast, of the best-selling
novels and blockbuster movies (e.g., Harvey (1953),
Hall (2012), Yun (2011)).
All these studies however, are qualitative in na-
ture, as they rely on the knowledge and insights of
human experts on literature. To our knowledge, no
prior work has undertaken a systematic quantitative
investigation on the overarching characterization of
the writing style in successful literature. In consid-
eration of widely different styles of authorship (e.g.,
Escalante et al (2011), Peng et al (2003), Argamon
et al (2003)), it is not even readily clear whether
there might be common stylistic elements that help
discriminating highly successful ones from less suc-
cessful counterpart.
In this work, we present the first study that in-
vestigates this unstudied and unexpected connection
between stylistic elements and the literary success.
The key findings of our research reveal that there
exists distinct linguistic patterns shared among suc-
1E.g., Paul Harding?s ?Tinkers? that won 2010 Pulitzer Prize
for Fiction and J. K. Rowling?s ?Harry Potter and the Philoso-
pher?s Stone? that sold over 450 million copies.
1753
cessful literature, at least within the same genre,
making it possible to build a model with surprisingly
high accuracy (up to 84%) in predicting the success
of a novel. This result is surprising for two reasons.
First, we tackle the hard task of predicting the suc-
cess of novels written by previously unseen authors,
avoiding incidental learning of authorship signature,
since previous research demonstrated that one can
achieve very high accuracy in authorship attribution
(as high as 96% in some experimental setup) (e.g.,
Raghavan et al (2010), Feng et al (2012)). Sec-
ond, we aim to discriminate highly successful nov-
els from less successful, but nonetheless published
books written by professional writers, which are un-
doubtedly of higher quality than average writings.
It is important to note that the task we tackle here
is much harder than discriminating highly success-
ful works from those that have not even passed the
scrutinizing eyes of publishers.
In order to quantify the success of literary works,
and to obtain corresponding gold standard labels,
one needs to first define ?success?. For practi-
cal convenience, we largely rely on the download
counts available at Project Gutenberg as a surrogate
to quantify the success of novels. For a small num-
ber of novels however, we also consider award re-
cipients (e.g., Pulitzer, Nobel), and Amazon?s sales
records to define a novel?s success. We also ex-
tend our empirical study to movie scripts, where we
quantify the success of movies based on the aver-
age review scores at imdb.com. We leave analysis
based on other measures of literary success as future
research.
In this study, we do not attempt to separate out
success based on literary quality (award winners)
from success based on popularity (commercial hit,
often in spite of bad literary quality), mainly because
it is not practically easy to determine whether the
high download counts are due to only one reason or
the other. We expect that in many cases, the two
different aspects of success are likely to coincide,
however. In the case of the corpus obtained from
Project Gutenberg, where most of our experiments
are conducted, we expect that the download counts
are more indicative of success based on the literary
quality (which then may have resulted in popularity)
rather than popularity without quality.
We examine several genres in fiction and movie
GENRE #BOOKS ?? ?+
Adventure 409 17 100
Detective / Mystery 374 25 90
Fiction 1148 7 125
Historical Fiction 374 25 115
Love Stories 342 16 85
Poetry 580 9 70
Science Fiction 902 30 100
Short Stories 1117 9 224
Table 1: # of books available per genre at Gutenberg with
download thresholds used to define more successful (?
?+) and less successful (? ??) classes.
scripts, e.g., adventure stories, mystery, fiction, his-
torical fiction, sci-fi, short stories, as well as poetry,
and present systematic analyses based on lexical and
syntactic features which have been known to be ef-
fective in a variety of NLP tasks ranging from au-
thorship attribution (e.g., Raghavan et al (2010)),
genre detection (e.g., Rayson et al (2001), Douglas
and Broussard (2000)), gender identification (e.g.,
Sarawgi et al (2011)) and native language detection
(e.g., Wong and Dras (2011)).
Our empirical results demonstrate that (1) statis-
tical stylometry can be surprisingly effective in dis-
criminating successful literature, achieving accuracy
up to 84%, (2) some elements of successful styles
are genre-dependent while others are more univer-
sal. In addition, this research results in (3) find-
ings that are somewhat contrary to the conventional
wisdom with respect to the connection between suc-
cessful writing styles and readability, (4) interesting
correlations between sentiment / connotation and the
literary success, and finally, (5) comparative insights
between fiction and nonfiction with respect to the
successful writing style.
2 Dataset Construction
For our experiments, we procure novels from project
Gutenberg2. Project Gutenberg houses over 40, 000
books available for free download in electronic for-
mat and provides a catalog containing brief descrip-
tions (title, author, genre, language, download count,
etc.) of these books. We experiment with genres in
Table 1, which have sufficient number of books al-
lowing us to construct reasonably sized datasets.
We use the download counts in Gutenberg-catalog
2http://www.gutenberg.org/
1754
Figure 1: Differences in POS tag distribution between more successful and less successful books across different
genres. Negative (positive) value indicates higher percentage in less (more) successful class.
as a surrogate to measure the degree of success of
novels. For each genre, we determine a lower bound
(?+) and an upper bound (??) of download counts as
shown in Table 1 to categorize the available books
as more successful and less successful respectively.
These thresholds are set to obtain at least 50 books
for each class, and for each genre. To balance the
data, for each genre, we construct a dataset of 100
novels (50 per class).
We make sure that no single author has more than
2 books in the resulting dataset, and in the major-
ity of the cases, only one book has been taken from
each author.3 Furthermore, we make sure that the
books from the same author do not show up in both
training and test data. These constraints make sure
that we learn general linguistic patterns of success-
ful novels, rather than a particular writing style of a
few successful authors.
3 Methodology
In what follows, we describe five different aspects of
linguistic styles we measure quantitatively. The first
three correspond to the features that have been fre-
quently utilized in previous studies in related tasks,
3The complete list of novels used for each genre in our
dataset is available at http://www.cs.stonybrook.
edu/?ychoi/successwithstyle/
e.g., genre detection (e.g., Kessler et al (1997))
and authorship attribution (e.g., Stamatatos (2009)),
while the last two are newly explored in this work.
I. Lexical Choices: unigrams and bigrams.
II. Distribution of Word Categories: Many pre-
vious studies have shown that the distribution of
part-of-speech (POS) tags alone can reveal surpris-
ing insights on genre and authorship (e.g., Koppel
and Schler (2003)), hence we examine their distri-
butions with respect to the success of literary works.
III. Distribution of Grammar Rules: Recent
studies reported that features based on CFG rules are
helpful in authorship attribution (e.g., Raghavan et
al. (2010), Feng et al (2012)). We experiment with
four different encodings of production rules:
? ?: lexicalized production rules (all production
rules, including those with terminals)
? ?G: lexicalized production rules prepended
with the grandparent node.
? ?: unlexicalized production rules (all produc-
tion rules except those with terminals).
? ?G: unlexicalized production rules prepended
with the grandparent node.
1755
FEATURE
GENRE
Avg
Avg w/o
HistoryAdven Myster Fiction Histor Love Poetr Sci-fi Short
POS 74.0 63.9 72.0 47.0 65.9 63.0 63.0 67.0 64.5 66.9
Unigram 84.0 73.0 75.0 60.0 82.0 71.0 61.0 57.0 70.3 71.8
Bigram 81.0 73.0 75.0 51.0 72.0 70.0 59.0 57.0 67.2 69.5
? 73.0 71.0 75.0 54.0 78.0 74.0 71.0 77.0 71.6 74.1
?G 75.0 74.0 75.0 58.0 81.0 72.0 76.0 77.0 73.5 75.7
? 72.0 70.0 65.0 53.0 70.0 66.0 64.0 71 66.3 68.2
?G 72.0 69.0 74.0 55.0 75.0 69.0 67.0 73.0 69.2 71.2
?+Unigram 79.0 73.0 73.0 59.0 80.0 73.0 71.0 73.0 72.6 74.5
?G+Unigram 80.0 74.0 74.0 56.0 82.0 72.0 73.0 72.0 72.8 75.2
?+Unigram 82.0 72.0 73.0 56.0 81.0 69.0 62.0 59.0 69.2 71.1
?G+Unigram 80.0 73.0 74.0 58.0 82.0 70.0 65.0 58.0 70 71.7
PHR 74.0 65.0 65.0 56.0 64.0 62.0 69.0 71.0 65.7 67.1
PHR+CLS 75.0 69.0 64.0 61.0 59.0 62.0 69.0 67.0 65.7 66.4
PHR+Unigram 80.0 74.0 71.0 56.0 79.0 73.0 67.0 66.0 70.7 72.8
PHR+CLS+Unigram 80.0 75.0 71.0 56.0 79.0 73.0 66.0 66.0 70.7 72.8
Table 2: Classification results in accuracy (%).
IV. Distribution of Constituents: PCFG gram-
mar rules are overly specific to draw a big picture
on the distribution of large, recursive syntactic units.
We hypothesize that the distribution of constituents
can serve this purpose, and that it will reveal inter-
esting and more interpretable insights into writing
styles in highly successful literature. Despite its rel-
ative simplicity, we are not aware of previous work
that looks at the distribution of constituents directly.
In particular, we are interested in examining the dis-
tribution of phrasal and/or clausal tags as follows:
(i) Phrasal tag percent (PHR) - percentage distribu-
tion of phrasal tags.4 (ii) Clausal tag percent (CLS)
- percentage distribution of clausal tags.
V. Distribution of Sentiment and Connotation:
Finally, we examine whether the distribution of sen-
timent and connotation words, and their polarity, has
any correlation with respect to the success of literary
works. We are not aware of any previous work that
looks into this connection.
4 Prediction Performance
We use LibLinear SVM (Fan et al, 2008) with
L2 tuned over training data, and all performance is
based on 5-fold cross validation. We take 1000 sen-
tences from the beginning of each book. POS fea-
tures are encoded as unit normalized frequency and
all other features are encoded as tf-idf.5
4The percentage of any phrasal tag is the count of occurrence
of that tag over the sum of counts of all phrasal tags.
5POS tags are obtained using the Stanford POS tagger
(Toutanova and Manning, 2000), and parse trees are based on
the Stanford parser (Klein and Manning, 2003).
Prediction Results Table 2 shows the classifica-
tion results. The best performance reaches as high
as 84% in accuracy. In fact, in all genres except
for history, the best performance is at least 74%,
if not higher. Another notable observation is that
even in the poetry genre, which is not prose, the ac-
curacy gets as high as 74%. This level of perfor-
mance is not entirely anticipated, given that (1) the
test data consists of books written only by previously
unseen authors, and (2) each author has widely dif-
ferent writing style, and (3) we do not have training
data at scale, and (4) we aim to tackle the hard task
of discriminating highly successful ones from less
successful, but nonetheless successful ones, as all of
them were, after all, good enough to be published.6
Prediction with Varying Thresholds of Down-
load Counts Before we proceed to comprehensive
analysis of writing style that are prominent in more
successful literature (?5), in Table 3, we present how
the prediction accuracy varies as we adjust the defi-
nition of more-successful and less-successful litera-
ture, by gradually increasing (decreasing) the thresh-
old ?? (?+). As we reduce the gap between ?? and
?+, the performance decreases, which shows that in-
deed there are notable statistical differences in lin-
guistic patterns between novels with high and low
download counts, and the stylistic difference mono-
tonically increases (thereby higher prediction accu-
racy) as we increase the gap between two classes.
6In our pilot study, we also experimented with the binary
classification task of discriminating highly successful ones from
those that are not even published (unpublished online novels),
and it was a much easier task as expected.
1756
?? ?+ ACCURACY
17 100 84.0
25 90 78.4
35 80 77.6
45 70 76.4
55 60 73.5
Table 3: Accuracy (%) with varying thresholds of down-
load counts for ADVENTURE with unigram features.
This is particularly interesting as the size of training
data set is actually monotonically decreasing (mak-
ing it harder to achieve high accuracy) while we in-
crease the separation between ?? and ?+.
5 Analysis of Successful Writing Styles
5.1 Insights Based on Lexical Choices
It is apparent from Table 2 that unigram features
yield curiously high performance in many genres.
We therefore examine discriminative unigrams for
ADVENTURE, shown in Table 4. Interestingly, less
successful books rely on verbs that are explicitly de-
scriptive of actions and emotions (e.g., ?wanted?,
?took?, ?promised?, ?cried?, ?cheered?, etc.), while
more successful books favor verbs that describe
thought-processing (e.g., ?recognized?, ?remem-
bered?), and verbs that serve the purpose of quotes
and reports (e.g,. ?say?). Also, more successful
books use discourse connectives and prepositions
more frequently, while less successful books rely
more on topical words that could be almost cliche?,
e.g., ?love?, typical locations, and involve more ex-
treme (e.g., ?breathless?) and negative words (e.g.,
?risk?).
5.2 Distribution of Sentiment & Connotation
We also determine the distribution of sentiment and
connotation words separately for each class (Table
5) to check if there exists a connection with respect
to successful writing styles.7 We first compare dis-
tribution of sentiment and connotation for the entire
words. As can be seen in Table 5 ? Top, there are
not notable differences. However, when we compare
distribution only with respect to discriminative uni-
grams only (i.e., features with non-zero weights), as
7We use MPQA subjectivity lexicon (Wilson et al, 2005)
and connotation lexicon (Feng et al, 2013) for determining sen-
timent and connotation of words respectively.
Less Successful
CATEGORY UNIGRAMS
Negative
never, risk, worse, slaves, hard,
murdered, bruised, heavy, prison,
Body Parts face, arm, body, skins
Location
room, beach, bay, hills,
avenue, boat, door
Emotional / want, went, took, promise,
Action Verbs cry, shout, jump, glare, urge
Extreme Words
never, very, breathless, sacred
slightest, absolutely, perfectly
Love Related desires, affairs
More Successful
CATEGORY UNIGRAMS
Negation not
Report / Quote said, words, says
Self Reference I, me , my
Connectives
and, which, though, that,
as, after, but, where, what,
whom, since, whenever
Prepositions up, into, out, after, in, within
Thinking Verbs recognized, remembered
Table 4: Discriminative unigrams for ADVENTURE.
shown in Table 5 ? Bottom, we find substantial dif-
ferences in all genres. In particular, discriminative
unigrams that characterize less successful novels in-
volve significantly more sentiment-laden words.
5.3 Distribution of Word Categories
Summarized analysis of POS distribution across all
genres is reported in Table 6. It can be seen that
prepositions, nouns, pronouns, determiners and ad-
jectives are predictive of highly successful books
whereas less successful books are characterized by
higher percentage of verbs, adverbs, and foreign
words. Per genre distributions of POS tags are vi-
sualized in Figure 1. Interestingly, some POS tags
show almost universal patterns (e.g., prepositions
(IN), NNP, WP, VB), while others are more genre-
specific.
In Relation to Journalism Style The work of
Douglas and Broussard (2000) reveals that informa-
tive writing (journalism) involves increased use of
nouns, prepositions, determiners and coordinating
conjunctions whereas imaginative writing (novels)
involves more use of verbs and adverbs, as has been
also confirmed by Rayson et al (2001). Compar-
ing their findings with Table 6, we find that highly
1757
Adven Myster Fiction Histor Love Poetr Sci-fi Short
- + - + - + - + - + - + - + - +
+ve S 4.7 4.9 4.8 4.6 5.6 4.9 5.0 5.1 5.5 5.1 6.3 5.7 4.1 3.7 4.7 4.8
-ve S 4.0 4.0 4.0 4.0 4.3 4.2 4.2 4.2 4.1 4.2 4.3 4.3 2.9 2.9 3.8 4.0
Tot S 8.7 8.9 8.9 8.7 9.9 9.0 9.2 9.3 9.6 9.3 10.6 9.9 7.0 6.7 8.5 8.9
+ve C 22.3 22.5 22.3 22.5 23.7 23.0 23.0 23.2 23.34 23.3 23.8 22.9 21.2 20.6 22.6 22.7
-ve C 19.4 19.6 19.8 19.8 20.3 19.5 19.2 19.4 20.2 20.4 17.7 17.4 16.6 16.7 18.3 18.9
Total C 41.7 42.1 42.1 42.3 44.0 42.5 42.3 42.6 43.5 43.7 41.5 40.3 37.9 37.3 41.0 41.6
+ve S 3.5 1.8 4.1 2.0 3.7 1.4 3.0 1.0 3.4 1.3 3.9 2.0 7.3 5.9 5.1 2.7
-ve S 5.5 3.4 6.3 3.6 5.5 2.9 4.7 1.9 5.1 2.6 5.8 3.3 9.0 8.0 7.3 4.8
Total S 9.1 5.2 10.4 5.6 9.2 4.3 7.7 3.0 8.5 3.9 9.7 5.2 16.3 13.9 12.4 7.5
+ve C 12.9 8.9 14.3 9.8 12.9 8.5 11.5 6.2 12.0 7.7 14.0 9.6 19.6 19.2 16.5 11.9
-ve C 14.1 9.8 15.2 10.9 13.7 9.9 12.4 7.0 12.9 8.5 14.3 10.3 20.0 19.7 17.0 13.3
Total C 27.0 18.7 29.5 20.7 26.6 18.4 23.9 13.2 24.87 16.1 28.3 19.8 39.7 38.9 33.5 25.2
Table 5: Top: Distribution of sentiment (connotation) among entire unigrams. Bottom: distribution of sentiment
(connotation) among discriminative unigrams. ?S? and ?C? stand for sentiment and connotation respectively.
More Successful
CATEGORY SUB-CATEGORY DIFF
Prepositions General 0.00592
Determiners General 0.00226
Nouns
Plural 0.00189
Proper (Singular) 0.00016
Coord. conj. General 0.00118
Numbers General 0.00102
Pronouns
Posesseive 0.00081
General WH 0.00042
Possessive WH 5.4E-05
Adjectives
General 0.00102
Superlative 0.00011
Less Successful
CATEGORY SUB-CATEGORY DIFF
Adverbs
General -0.00272
General WH -0.00028
Verbs
Base -0.00239
Non-3rd sing. present -0.00084
Past tense -0.00041
Past participle -0.00039
3rd person sing. present -0.00036
Modal -0.00091
Foreign General -0.00067
Symbols General -0.00018
Interjections General -0.00016
Table 6: Top discriminative POS tags.
successful books tend to bear closer resemblance to
informative articles.
5.4 Distribution of Constituents
It can be seen in Table 2 that deep syntactic fea-
tures expressed in terms of different encodings of
production rules consistently yield good perfor-
mance across almost all genres. Production rules
are overly specific to gain more generalized, in-
terpretable, high-level insights however (Feng et
al., 2012). Therefore, similarly as word categories
(POS), we consider the categories of nonterminal
nodes of the parse trees, in particular, phrasal and
clausal tags, as they represent the gist of constituent
structure that goes beyond shallow syntactic infor-
mation represented by POS.
Table 8 shows how the distribution of phrasal and
clausal tags differ for successful books when com-
puted over all genres. Positive (negative) DIFF val-
ues indicate that the corresponding tags are favored
in more successful (less successful) books when
counted across all genres. We also report the num-
ber of genres (#Genres) in which the individual dif-
ference is positive / negative.
In terms of phrasal tags, we find that more suc-
cessful books are composed of higher percentage of
PP, NP and wh-noun phrases (WHNP), whereas less
successful books are composed of higher percentage
of VP, adverb phrases (ADVP), interjections (INTJ)
and fragments (FRAG). Notice that this observation
is inline with our earlier findings with respect to the
distribution of POS.
In regard to clausal tags, more successful books
involve more clausal tags that are necessary for com-
plex sentence structure and inverted sentence struc-
ture (SBAR, SBARQ and SQ) whereas less success-
ful books rely more on simple sentence structure (S).
Figure 2 shows the visualization of the distribution
of these phrasal and clausal tags.
It is also worth to mention that phrasal and clausal
1758
Figure 2: Difference between phrasal and clausal tag percentage distributions of more successful and less successful
books across different genres. Specifically, we plot D??D+, where D+ is the phrasal tag distribution (in %) of more
successful books and D? is the phrasal tag distribution (in %) of less successful books.
READABILITY INDICES More Succ. Less Succ.
FOG index 9.88 9.80
Flesch index 87.48 87.64
Table 7: Readability: Lower FOG and higher Flesch in-
dicate higher readability (numbers in Boldface).
tags alone can yield classification performance that
are generally better than that of POS tags, in spite of
the very small feature set (26 tags in total). In fact,
constituent tags deliver the best performance in case
of historical fiction genre (Table 2).
Connection to Readability Pitler and Nenkova
(2008) provide comprehensive insights into assess-
ment of readability. In their work, among the most
discriminating features characterizing text with bet-
ter readability is increased use of verb phrases (VP).
Interestingly, contrary to the conventional wisdom ?
that readability is of desirable quality of good writ-
ings ? our findings in Table 2 suggest that the in-
creased use of VP correlates strongly with the writ-
ing style of the opposite spectrum of highly success-
ful novels.
As a secondary way of probing the connection be-
tween readability and the writing style of successful
literature, we also compute two different readabil-
ity measures that have been used widely in prior
literature (e.g., Sierra et al (1992), Blumenstock
(2008), Ali et al (2010)): (i) Flesch reading ease
score (Flesch, 1948), (ii) Gunning FOG index (Gun-
ning, 1968). The overall weighted average readabil-
ity scores are reported in Table 7. Again, we find that
less successful novels have higher readability com-
pared to more successful ones.
The work of Sawyer et al (2008) provides yet
another interesting contrasting point, where the au-
thors found that award winning academic papers in
marketing journals correlate strongly with increased
readability, characterized by higher percentage of
simple sentences. We conjecture that this opposite
trend is likely to be due to difference between fic-
tion and nonfiction, leaving further investigation as
future research.
In sum, our analysis reveals an intriguing and
unexpected observation on the connection between
readability and the literary success ? that they cor-
relate into the opposite directions. Surely our find-
ings only demonstrate correlation, not to be con-
1759
Phrasal + ? DIFF #+Gen/#
?
Gen
ADJP 0.030 0.031 -6E-4 5/3
ADJP 0.030 0.031 -6E-4 5/3
ADVP 0.052 0.054 -0.002 2/6
CONJP 3E-4 3E-4 2E-5 5/3
FRAG 0.008 0.008 -1E-4 2/6
LST 2E-4 1E-4 5E-5 6/2
NAC 9E-6 6E-6 3E-6 5/3
NP 0.459 0.453 0.005 6/2
NX 1E-4 1E-4 -4E-7 3/5
PP 0.122 0.117 0.005 7/1
PRN 0.005 0.004 2E-4 4/4
PRT 0.010 0.010 -5E-4 3/5
QP 0.001 0.001 7E-5 6/2
RRC 8E-5 8E-5 6E-6 6/2
UCP 8E-4 7E-4 1E-4 8/0
VP 0.292 0.300 -0.008 1/7
WHADJP 2E-4 2E-4 -5E-5 1/7
WHAVP 0 0 0 -
WHNP 0.013 0.012 0.001 8/0
WHPP 0.001 9E-4 1E-4 6/2
X 0.001 0.001 -4E-5 4/4
Clausal + ? DIFF +Gen/#
?
Gen
SBAR 0.166 0.164 0.002 4/4
SQ 0.020 0.018 0.002 7/1
SBARQ 0.014 0.013 0.001 7/1
SINV 0.018 0.018 -6E-5 5/3
S 0.781 0.785 -0.004 3/5
Table 8: Overall Phrasal / Clausal Tag Distribution and
analysis. All values are rounded to [3-5] decimal places.
fused as causation, between readability and literary
success. We conjecture that the conceptual complex-
ity of highly successful literary work might require
syntactic complexity that goes against readability.
6 Literature beyond Project Gutenberg
One might wonder how the prediction algorithms
trained on the dataset based on Project Gutenberg
might perform on books not included at Guten-
berg. This section attempts to address such a ques-
tion. Due to the limited availability of electronically
available books that are free of charge however, we
could not procure more than a handful of books.8
6.1 Highly Successful Books
First, we apply the classifiers trained on the Project
Gutenberg dataset (all genres merged) on a few ex-
tremely successful novels (Pulitzer prize, National
Award recipients, etc). Table 9 shows the results of
8We report our prediction results on all books beyond
Project Gutenberg of which we managed to get electronic
copies, i.e., the results in Table 9 are not cherry-picked.
MORE SUCCESSFUL
BOOK (Q) PDKL UPDKL Su S??
?Don Quixote? 0.139 0.152 + +
? Miguel De Cervantes
?Other Voices, Other Rooms? 0.014 0.010 + +
? Truman Capote
?The Fixer? 0.013 0.015 + +
? Bernard Malamud
?Robinson Crusoe? 0.042 0.051 + +
? Daniel Defoe
?The old man and the sea? 0.065 0.060 + +
? Ernest Hemingway
?A Tale of Two Cities? 0.027 0.030 + +
? Charles Dickens
?Independence Day? 0.031 0.026 + +
? Richard Ford
?Rabbit At Rest? 0.047 0.048 + +
? John Updike
?American Pastoral? 0.039 0.043 + +
? Philip Roth
?Dr Jackel and Mr. Hyde? 0.036 0.037 + +
? Robert Stevenson
LESS SUCCESSFUL
?The lost symbol? 0.046 0.042 - -
? Dan Brown
?The magic barrel? 0.0288 0.0284 + -
? Bernard Malamud
?Two Soldiers? 0.130 0.117 - +
? William Faulkner
?My life as a man? 0.046 0.052 - +
? Philip Roth
Table 9: Prediction on books beyond Gutenberg. Shaded
entries indicate incorrect predictions.
two classification options: (1) KL-divergence based,
and (2) unigram-feature based.
Although KL-divergence based prediction was
not part of the classifiers that we explored in the pre-
vious sections, we include it here mainly to provide
better insights as to which well-known books share
closer structural similarity to either more or less suc-
cessful writing style. As a probability model, we use
the distributions of phrasal tags, as those can give us
insights on deep syntactic structure while suppress-
ing potential noises due to topical variances. Table 9
shows symmetrised KL-divergence between each of
the previously unseen novels and the collection of
books from Gutenberg corresponding to more suc-
cessful (less successful) labels. For prediction, the
label with smaller KL is chosen.
Based only on the distribution of 26 phrasal tags,
the KL divergence classifier is able to make correct
1760
predictions on 7 out of 10 books, a surprisingly high
performance based on mere 26 features. Of course,
considering only the distribution of phrasal tags is
significantly less informed than considering numer-
ous other features that have shown substantially bet-
ter performance, e.g., unigrams and CFG rewrite
rules. Therefore, we also present the SVM classi-
fier trained on unigram features. It turns out uni-
gram features are powerful enough to make correct
predictions for all ten books in Table 9.
Hemingway and Minimalism It is good to think
about where and why KL-divergence-based ap-
proach fails. In fact, when we included Heming-
way?s The Old Man and the Sea into the test set, we
were expecting some level of confusions when rely-
ing only on high-level syntactic structure, as Hem-
ingway?s signature style is minimalism, with 70%
of his sentences corresponding to simple sentences.
Not surprisingly, more adequately informed clas-
sifiers, e.g., SVM with unigram features, are still
able to recognize Hemingway?s writings as those of
highly successful ones.
6.2 Less Successful Books
In order to obtain less successful books, we consider
the Amazon seller?s rank included in the product de-
tails of a book. The less successful books considered
in Table 9 had an Amazon seller?s rank beyond 200k
(higher rank indicating less commercial success) ex-
cept Dan Brown?s The lost symbol, which we in-
cluded mainly because of negative critiques it had
attracted from media despite its commercial success.
As shown in Table 9, all three classifiers make (ar-
guably) correct predictions on Dan Brown?s book.9
This result also supports our earlier assumption on
the nature of novels available at Project Gutenberg
? that they would be more representative of liter-
ary success than general popularity (with or without
literary quality).
7 Predicting Success of Movie Scripts
We have seen successful results in the novel domain,
but can stylometry-based prediction work on very
different domains, such as screenplays? Unlike nov-
els, movie scripts are mostly in dialogues, which
9Most notable pattern based on phrasal tag analysis is a sig-
nificantly increased use of fragments (FRAG), which associates
strongly with less successful books in our dataset.
FEATURE Adven Fanta Roman Thril
POS 62.0 58.0 61.7 56.0
Unigram 62.0 81.3 70.0 80.0
Bigrams 73.3 84.7 80.8 76.0
? 66.0 81.3 70.0 76.0
?G 62.0 69.3 86.7 60.0
? 62.0 81.3 78.3 76.0
?G 69.3 77.3 77.5 68.0
?+Uni 62.0 85.3 70.0 76.0
?G+Uni 54.7 81.3 70.0 76.0
?+Uni 58.0 89.3 70.0 76.0
?G+Uni 58.0 84.7 70.0 76.0
PHR 46.0 42.7 65.8 80.0
PHR+CLR 76.7 31.3 65.8 80.0
PHR+Uni 62.0 81.3 70.0 80.0
PHR+CLR+Uni 62.0 81.3 70.0 80.0
Table 10: Classification results on movie dialogue data
(rating ? 8 vs rating ? 5.5).
are likely to be more informal. Also, what to keep
in mind is that much of the success of movies de-
pends on factors beyond the quality of writing of the
scripts, such as the quality of acting, the popularity
of actors, budgets, artistic taste of directors and pro-
ducers, editing and so forth.
We use the Movie Script Dataset introduced in
Danescu-Niculescu-Mizil and Lee (2011). It in-
cludes the dialogue scripts of 617 movies. The aver-
age rating of all movies is 6.87. We consider movies
with IMDb rating ? 8 as ?more successful?, the
ones with IMDb rating ? 5.5 as ?less successful?.
We combine all the dialogues of each movie and
filter out the movies with less than 200 sentences.
There are 11 genres (?ADVENTURE?, ?FANTASY?,
?ROMANCE?, ?THRILLER?, ?ACTION?, ?COMEDY?,
?CRIME?, ?DRAMA?, ?HORROR?, ?MYSTERY?, ?SCI-
FI?) with 15 movies or more per class, we take 15
movies per class and perform classification tasks
with the same experiment setting as Table 2.
Table 10, we show some of the example genres
with relatively successful outcome, reaching as high
as 89.3% accuracy in FANTASY genre. We would
like to note however that in many other genres, the
prediction did not work as well as it did for the novel
domain. We suspect that there are at least two rea-
sons for this: it must be partly due to very limited
data size ? only 15 instances per class with the rat-
ing threshold we selected for defining the success of
1761
movies. The second reason is due to many other ex-
ternal factors that can also influence the success of
movies, as discussed earlier.
8 Related Work
Predicting success of novels and movies: To the
best of our knowledge, our work is the first that pro-
vides quantitative insights into the unstudied con-
nection between the writing style and the success of
literary works. There have been some previous work
that aims to gain insights into the secret recipe of
successful books, but most were qualitative, based
only on a dozen of books, focusing mainly on the
high-level content of the books, such as the per-
sonalities of protagonists, antagonists, the nature of
plots (e.g., Harvey (1953), Yun (2011)). In con-
trast, our work examines a considerably larger col-
lection of books (800 in total) over eight different
sub-genres, providing insights into lexical, syntac-
tic, and discourse patterns that characterize the writ-
ing styles commonly shared among the successful
literature. Another relevant work has been on a dif-
ferent domain of movies (Yun, 2011), however, the
prediction is based only on external, non-textual in-
formation such as the reputation of actors and direc-
tors, and the power of distribution systems etc, with-
out analyzing the actual content of the movie scripts.
Text quality and readability: Louis (2012) ex-
plored various features that measure the quality of
text, which has some high-level connections to our
work. Combining the insights from Louis (2012)
with our results, we find that the characteristics of
text quality explored in Louis (2012), readability of
text in particular, do not correspond to the prominent
writing style of highly successful literature. There
have been a number of other work that focused on
predicting and measuring readability (e.g., Kate et
al. (2010), Pitler and Nenkova (2008), Schwarm and
Ostendorf (2005), Heilman and Eskenazi (2006) and
Collins-Thompson et al (2004)) employing various
linguistic features.
There is an important difference however, in re-
gard to the nature of the selected text for analysis:
most studies in readability focus on differentiating
good writings from noticeably bad writings, often
involving machine generated text or those written
by ESL students. In contrast, our work essentially
deals with differentiating good writings from even
better writings. After all, all the books that we an-
alyzed are written by expert writers who passed the
scrutinizing eyes of publishers, hence it is reason-
able to expect that the writing quality of even less
successful books is respectful.
Predicting success among academic papers: In
the domain of academic papers, which belongs to
the broad genre of non-fiction, the work of Sawyer
et al (2008) investigated the stylistic characteris-
tics of award winning papers in marketing journals,
and found that the readability plays an important
role. Combined with our study which focuses on fic-
tion and creative writing, it suggests that the recipe
for successful publications can be very different de-
pending on whether it belongs to fiction or nonfic-
tion. The work of Bergsma et al (2012) is also
somewhat relevant to ours in that their work in-
cluded differentiating the writing styles of workshop
papers from major conference papers, where the lat-
ter would be generally considered to be more suc-
cessful.
9 Conclusion
We presented the first quantitative study that learns
to predict the success of literary works based on their
writing styles. Our empirical results demonstrated
that statistical stylometry can be surprisingly effec-
tive in discriminating successful literature, achiev-
ing accuracy up to 84% in the novel domain and
89% in the movie domain. Furthermore, our study
resulted in several insights including: lexical and
syntactic elements of successful styles, the connec-
tion between successful writing style and readabil-
ity, the connection between sentiment / connotation
and the literary success, and last but not least, com-
parative insights between successful writing styles
of fiction and nonfiction.
Acknowledgments This research was supported
in part by the Stony Brook University Office of
the Vice President for Research, and in part by gift
from Google. We thank anonymous reviewers, Steve
Greenspan, and Mike Collins for helpful comments
and suggestions, Alex Berg for the title, and Arun
Nampally for helping with the preliminary work.
1762
References
Omar Ali, Ilias N Flaounas, Tijl De Bie, Nick Mosdell,
Justin Lewis, and Nello Cristianini. 2010. Automat-
ing news content analysis: An application to gender
bias and readability. Journal of Machine Learning
Research-Proceedings Track, 11:36?43.
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and
Anat Rachel Shimoni. 2003. Gender, genre, and writ-
ing style in formal written texts. TEXT-THE HAGUE
THEN AMSTERDAM THEN BERLIN-, 23(3):321?
346.
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric analysis of scientific articles. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 327?337.
Association for Computational Linguistics.
Joshua E Blumenstock. 2008. Automatically assessing
the quality of wikipedia articles.
Kevyn Collins-Thompson, James P. Callan, and James P.
Callan. 2004. A language modeling approach to pre-
dicting reading difficulty. In HLT-NAACL, pages 193?
200.
Cristian Danescu-Niculescu-Mizil and Lillian Lee. 2011.
Chameleons in imagined conversations: A new ap-
proach to understanding coordination of linguistic
style in dialogs. In Proceedings of the Workshop on
Cognitive Modeling and Computational Linguistics,
ACL 2011.
Dan Douglas and Kathleen M Broussard. 2000. Long-
man grammar of spoken and written english. TESOL
Quarterly, 34(4):787?788.
Alvar Ellega?rd. 1962. A Statistical method for determin-
ing authorship: the Junius Letters, 1769-1772, vol-
ume 13. Go?teborg: Acta Universitatis Gothoburgen-
sis.
Hugo J Escalante, Thamar Solorio, and M Montes-y
Go?mez. 2011. Local histograms of character n-
grams for authorship attribution. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
volume 1, pages 288?298.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Characterizing stylistic elements in syntactic struc-
ture. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1522?1533. Association for Computational Lin-
guistics.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and Yejin
Choi. 2013. Connotation lexicon: A dash of senti-
ment beneath the surface meaning. In Proceedings of
the 51th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers), Sofia,
Bulgaria, Angust. Association for Computational Lin-
guistics.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of applied psychology, 32(3):221.
Robert Gunning. 1968. The technique of clear writing.
McGraw-Hill New York.
James W Hall. 2012. Hit Lit: Cracking the Code of
the Twentieth Century?s Biggest Bestsellers. Random
House Digital, Inc.
John Harvey. 1953. The content characteristics of best-
selling novels. Public Opinion Quarterly, 17(1):91?
114.
Michael Heilman and Maxine Eskenazi. 2006. Language
learning: Challenges for intelligent tutoring systems.
In Proceedings of the workshop of intelligent tutor-
ing systems for ill-defined tutoring systems. Eight in-
ternational conference on intelligent tutoring systems,
pages 20?28.
Rohit J Kate, Xiaoqiang Luo, Siddharth Patwardhan,
Martin Franz, Radu Florian, Raymond J Mooney,
Salim Roukos, and Chris Welty. 2010. Learning to
predict readability using diverse linguistic features. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 546?554. Associa-
tion for Computational Linguistics.
Brett Kessler, Geoffrey Numberg, and Hinrich Schu?tze.
1997. Automatic detection of text genre. In Proceed-
ings of the 35th Annual Meeting of the Association for
Computational Linguistics and Eighth Conference of
the European Chapter of the Association for Computa-
tional Linguistics, pages 32?38. Association for Com-
putational Linguistics.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Associ-
ation for Computational Linguistics.
Moshe Koppel and Jonathan Schler. 2003. Exploit-
ing stylistic idiosyncrasies for authorship attribution.
In Proceedings of IJCAI?03 Workshop on Computa-
tional Approaches to Style Analysis and Synthesis, vol-
ume 69, page 72. Citeseer.
Annie Louis. 2012. Automatic metrics for genre-specific
text quality. Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies: Student Research Workshop, page 54.
Jerome McGann. 1998. The Poetics of Sensibility: A
Revolution in Literary Style. Oxford University Press.
1763
Fuchun Peng, Dale Schuurmans, Shaojun Wang, and
Vlado Keselj. 2003. Language independent author-
ship attribution using character level language mod-
els. In Proceedings of the tenth conference on Eu-
ropean chapter of the Association for Computational
Linguistics-Volume 1, pages 267?274. Association for
Computational Linguistics.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: a unified framework for predicting text qual-
ity. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 186?195, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proceedings of the
ACL 2010 Conference Short Papers, pages 38?42. As-
sociation for Computational Linguistics.
Paul Rayson, Andrew Wilson, and Geoffrey Leech.
2001. Grammatical word class variation within the
british national corpus sampler. Language and Com-
puters, 36(1):295?306.
Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi.
2011. Gender attribution: tracing stylometric evi-
dence beyond topic and genre. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning, pages 78?86. Association for Com-
putational Linguistics.
Alan G Sawyer, Juliano Laran, and Jun Xu. 2008.
The readability of marketing journals: Are award-
winning articles better written? Journal of Marketing,
72(1):108?117.
Sarah E. Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?05, pages 523?530, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Arlene E Sierra, Mark A Bisesi, Terry L Rosenbaum, and
E James Potchen. 1992. Readability of the radiologic
report. Investigative radiology, 27(3):236?239.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for information Science and Technology,
60(3):538?556.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In In EMNLP/VLC
2000, pages 63?70.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354.
Association for Computational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1600?1610.
Association for Computational Linguistics.
Chang-Joo Yun. 2011. Performance evaluation of intel-
ligent prediction models on the popularity of motion
pictures. In Interaction Sciences (ICIS), 2011 4th In-
ternational Conference on, pages 118?123. IEEE.
1764
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1469?1473,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Keystroke Patterns as Prosody in Digital Writings:
A Case Study with Deceptive Reviews and Essays
Ritwik Banerjee Song Feng Jun S. Kang
Computer Science
Stony Brook University
{rbanerjee, songfeng, junkang}
@cs.stonybrook.edu
Yejin Choi
Computer Science & Engineering
University of Washington
yejin@cs.washington.edu
Abstract
In this paper, we explore the use of keyboard
strokes as a means to access the real-time writ-
ing process of online authors, analogously to
prosody in speech analysis, in the context of
deception detection. We show that differences
in keystroke patterns like editing maneuvers
and duration of pauses can help distinguish be-
tween truthful and deceptive writing. Empiri-
cal results show that incorporating keystroke-
based features lead to improved performance
in deception detection in two different do-
mains: online reviews and essays.
1 Introduction
Due to the practical importance of detecting deceit, in-
terest in it is ancient, appearing in papyrus dated back
to 900 B.C. (Trovillo, 1939). In more recent years, sev-
eral studies have shown that the deceiver often exhibits
behavior that belies the content of communication, thus
providing cues of deception to an observer. These in-
clude linguistic (e.g., Newman et al. (2003), Hancock
et al. (2004)) as well as paralinguistic (e.g., Ekman et
al. (1991), DePaulo et al. (2003)) cues. Recognizing
deception, however, remains a hard task for humans,
who perform only marginally better than chance (Bond
and DePaulo, 2006; Ott et al., 2011).
Recent studies suggest that computers can be sur-
prisingly effective in this task, albeit in limited domains
such as product reviews. Prior research has employed
lexico-syntactic patterns (Ott et al., 2011; Feng et al.,
2012) as well as online user behavior (Fei et al., 2013;
Mukherjee et al., 2013). In this paper, we study the
effect of keystroke patterns for deception detection in
digital communications, which might be helpful in un-
derstanding the psychology of deception and help to-
ward trustful online communities. This allows us to in-
vestigate differences in the writing and revisional pro-
cesses of truthful and fake writers. Our work thus
shares intuition with HCI research linking keystroke
analysis to cognitive processes (Vizer et al., 2009; Epp
et al., 2011) and psychology research connecting cog-
nitive differences to deception (Ekman, 2003; Vrij et
al., 2006).
Recent research has shown that lying generally im-
poses a cognitive burden (e.g., McCornack (1997), Vrij
et al. (2006)) which increases in real-time scenar-
ios (Ekman, 2003). Cognitive burden has been known
to produce differences in keytroke features (Vizer et
al., 2009; Epp et al., 2011). Previous research has not,
however, directly investigated any quantitative connec-
tion between keystroke patterns and deceptive writing.
In this paper, we posit that cognitive burdens in
deception may lead to measurable characteristics in
keystroke patterns. Our contributions are as follows:
(1) introducing keystroke logs as an extended linguis-
tic signal capturing the real-time writing process (anal-
ogous to prosody in speech analysis) by measuring the
writing rate, pauses and revision rate. (2) showing
their empirical value in deception detection, (3) provid-
ing novel domain-specific insights into deceptive writ-
ing, and (4) releasing a new corpus of deception writ-
ings in new domains.
1
2 Related Work
Prior research has focused mainly on using keystroke
traits as a behvioral biometric. Forsen et al. (1977)
first demonstrated that users can be distinguished by the
way they type their names. Subsequent work showed
that typing patterns are unique to individuals (Leggett
and Williams, 1988), and can be used for authentica-
tion (Cho et al., 2000; Bergadano et al., 2002) and in-
trusion detection (Killourhy and Maxion, 2009).
Keystroke pauses have been linked to linguistic pat-
terns in discourse (e.g. Matsuhashi (1981), van Hell et
al. (2008)) and regarded as indications of cognitive bur-
den (e.g., Johansson (2009), Zulkifli (2013)). In this pa-
per, we present the first empirical study that quantita-
tively measures the deception cues in real-time writing
process as manifested in keystroke logs.
3 Data Collection
As discussed by Gokhman et al. (2012), the crowd-
sourcing approach to soliciting deceptive content sim-
ulates the real world of online deceptive content cre-
ators. We collected the data via Amazon Mechanical
Turk.
2
Turkers were led to a separate website where
keylogging was enabled, and asked to write truthful
and deceptive texts (? 100 words) on one of three top-
1
Available at http://www3.cs.stonybrook.
edu/
?
junkang/keystroke/
2
https://www.mturk.com/mturk
1469
ArrowKey Del MouseUp
0
5
10
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
Fre
que
ncy
 of e
ditin
g ke
ystr
oke
s
Deceptive Truthful
Figure 1: Number of keystrokes corresponding to the three
types of edit patterns (E
3
): (a) use of arrow keys, (b) deletion
(Delete and Backspace) and (c) text selection with mouse.
ics: restaurant review, gay marriage and gun control.
Each Turker was required to agree to their typing be-
ing logged. Since copy/paste operations defeat our pur-
pose of studying keystrokes in the typing process, they
were disabled. This restriction also acts as a hindrance
to plagiarism. All texts were reviewed manually, and
those not meeting the requirements (due to the being
too short, plagiarized content, etc.) were disregarded.
Writing task design: The task was designed such
that each Turker wrote a pair of texts, one truthful and
one deceptive, on the same topic. For restaurant re-
views, they were asked to write a truthful review of
a restaurant they liked, and a deceptive review of a
restaurant they have never been to or did not like. For
the other two topics ? ?gun control? and ?gay marriage?
? we asked their opinion: support, neutral, or against.
Then, they were asked to write a truthful and a decep-
tive essay articulating, respectively, their actual opin-
ion and its opposite.
3
The tasks further were divided
into two ?flows?: writing the truthful text before the de-
ceptive one, and vice versa. Each Turker was assigned
only one flow, and was not allowed to participate in the
other. After completing this, each Turker was asked to
copy their own typing, i.e., re-type the two texts.
Finally, in order to get an idea of the cognitive bur-
den associated with truthful and deceptive writing, we
asked the Turkers which task was easier for them. Of
the 196 participants, 152 answered ?truthful?, 40 an-
swered ?deceptive? and only 4 opted for ?not sure?.
What are logged: We deployed a keylogger to cap-
ture the mouse and keyboard events in the ?text area?.
The events KeyUp, KeyDown and MouseUp, along with
the keycode and timestamp were logged.
4
For the three
topics restaurant review, gay marriage and gun control
we obtained 1000, 800 and 800 texts, respectively.
In the remainder of this paper, k
dn
and k
up
denote
the KeyDown and KeyUp events for a key k. For any
3
To prevent a change in opinion depending on task avail-
ability, Turkers were redirected to other tasks if their opinion
was neutral, or if we had enough essays of their opinion.
4
Printable (e.g., alphanumeric characters) as well as non-
printable keystrokes like (e.g., ?Backspace?), are logged.
Document Sentence Word Key Press
1.5
2.0
2.5
1.5
2.0
2.5
First?only
First+Second
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
Tim
e ta
ken
 (rel.
 to c
opy t
ask
)
Deceptive Truthful
Figure 2: Average normalized timespan ??(e) for documents,
sentences, words and key presses. The top row considers only
the first text, while the bottom row considers both flows.
event e, its timespan, i.e., the time interval between the
beginning and end of e, is denoted by ?(e).
4 Feature Design
Keystroke logging enables the study of two types of in-
formation that go beyond conventional linguistic anal-
ysis. First, it captures editing processes (e.g., deletions,
insertions made by changing cursor position, etc.).
Second, it reveals the temporal aspect of text generation
(e.g., duration, latency). Our exploration of these fea-
tures and their application in deception detection is mo-
tivated by the similarities between text and speech gen-
eration. Editing patterns, for instance, can be viewed as
attempts to veil incoherence in deceptive writing and
temporal patterns like latency or pause can be treated
as analogous to disfluency.
Different people, of course, have varying typing
skills, and some may type faster than others. In or-
der to control for this variation, we normalize all event
timespans ?(e) with respect to the corresponding event
timespan in the copy task:
?
?(e) = ?(e)/?(e
copy
).
4.1 Editing Patterns
In this work, we treat keys that are used only for edit-
ing as different from others. Text editing is done by
employing a small subset of available keys: deletion
keys (?Backspace? and ?Delete?), arrow keys (?, ?,
? and ?) and by using the mouse for text selection
(i.e., the ?MouseUp? event). The three types of editing
keystrokes are collectively denoted by
E
3
= ?|DEL| , |MSELECT| , |ARROW|?
where
(i) |DEL| = number of deletion keystrokes
(ii) |MSELECT| = number of ?MouseUp? events, and
(iii) |ARROW| = number of arrow keystrokes
The editing differences between truthful and deceptive
writing across all three topics are shown in Fig. 1.
4.2 Temporal Aspects
Each event is logged with a keycode and a timestamp.
In order to study the temporal aspects of digital writ-
ing, we calculate the timespan of different linguistic
1470
Topic Features Flow
First + Second First-only
Restaurants
BoW 73.9 78.8
BoW + T
6
74.3 79.1
BoW + T
6
+ E
3
74.6 80.3
?
Gun Control
(Support)
BoW 86.5 80.0
BoW + T
6
86.8 82.5
?
BoW + T
6
+ E
3
88.0
?
83.5
?
Gun Control
(Oppose)
BoW 88.5 88.0
BoW + T
6
89.8 87.5
BoW + T
6
+ E
3
90.8
?
89.1
Gay Marriage
(Support)
BoW 92.5 92.0
BoW + T
6
93.8 92.5
BoW + T
6
+ E
3
94.3
?
92.0
Gay Marriage
(Oppose)
BoW 84.5 86.5
BoW + T
6
85.0 87.0
BoW + T
6
+ E
3
85.3 86.8
Table 1: SVM classifier performance for truthful vs. de-
ceptive writing. Statistically significant improvements over
the baseline are marked * (p < 0.05) and ? (p < 0.1).
E
3
= ?|DEL| , |MSELECT| , |ARROW|? denotes the editing
keystrokes, and T
6
is the set of normalized timespans of
documents, words (plus preceding keystroke), all keystrokes,
spaces, non-whitespace keystrokes and inter-word intervals:
T
6
= {??(D), ??(k), ??(SP), ??(?SP), ??(?W), ??(k
prv
+ W)}
units such as words, sentences and even entire docu-
ments. Further, we separately inspect the timespans
of different parts of speech, function words and con-
tent words. In addition to event timespans, intervals
between successive events (e.g., inter-word and inter-
sentence pauses) and pauses preceding or succeeding
and event (e.g., time interval before and after a function
word) are measured as well.
5 Experimental Results
This section describes our experimental setup and
presents insights based on the obtained results. All
classification experiments use 5-fold cross validation
with 80/20 division for training and testing. In addition
to experimenting on the entire dataset, we also sepa-
rately analyze the texts written first (of the two texts in
each ?flow?). This additional step is taken in order to
eliminate the possibility of a text being primed by its
preceding text.
Deception cues in keystroke patterns: To empiri-
cally check whether keystroke features can help distin-
guish between truthful and deceptive writing, we de-
sign binary SVM classifiers.
5
Unigrams with tf-idf
encoding is used as the baseline. The average baseline
accuracy across all topics is 82.58% when considering
both texts of a flow, and 83.62% when considering only
the first text of each flow. The better performance in the
latter possibly indicates that the second text of a flow
exhibits some amount of lexical priming with the first.
The high accuracy of the baseline is not surprising.
Previous work by Ott et al. (2011) reported similar per-
5
We use the LIBLINEAR (Fan et al., 2008) package.
??(W) ??(kprv + W)
D > T T > D D > T T > D
our best when one
if get quality other
when well even get
were your on service
it?s fresh by been
quality not me their
dishes my has not
the one also with
i?ve had go friendly
on hat we great
they of had an
we other is our
friendly very at are
has love which really
at service from but
wait great dishes favorite
an really or very
go you re about
is but would will
which been just here
Table 2: Top 20 words in restaurant reviews with greatest
timespan difference between deceptive and truthful writing.
formance of unigram models. The focus of our work
is to explore the completely new feature space of ty-
pographic patterns in deception detection. We draw
motivation from parallels between the text generation
and speech generation processes. Prosodic concepts
such as speed, disfluency and coherence can be real-
ized in typographic behavior by analyzing timestamp
of keystrokes, pauses and editing patterns, respectively.
Based on the differences in the temporal aspects of
keystrokes, we extract six timespan features to improve
this baseline. This set, denoted by T
6
, comprises of
(i)
?
?(D) = timespan of entire document
(ii)
?
?(k
prv
+W) = average timespan of word plus pre-
ceding keystroke
(iii)
?
?(k) = average keystroke timespan
(iv)
?
?(SP) = average timespan of spaces
(v)
?
?(?SP) = average timespan of non whitesp-
ace keystrokes
(vi)
?
?(?W) = average interval between words.
The improvements attained by adding T
6
to the base-
line are shown in Table 1. Adding the edit patterns (E
3
)
(cf. ? 4.1) further improves the performance (with the
exception of two cases) by 0.7?3.5%.
Writing speed, pauses and revisions: To study the
temporal aspect of language units across all topics,
we first consider all texts, and then restrict to only
the first of each ?flow?. The timespan measurements
are presented in Fig. 2, showing the average duration
of typing documents, sentences, words and individual
keystrokes. The timespans are measured as the inter-
val between the first and the last keystrokes. The sen-
tence timespan, for instance, does not include the gap
between a sentence end and the first keystroke marking
the beginning of the next.
The sentence timespans for ?gay marriage? and ?gun
1471
DT+TD
120
130
140
150
160
170
All Words
Function Words
Content Words
Nouns Verbs Adjectives
Adverbs
Tim
esp
an (m
s) Deceptive
Truthful
(a)
DT+TD
350
400
450
500
550
Function Words
Content Words
Nouns Verbs Adjectives
Adverbs
Tim
esp
an (m
s) Deceptive
Truthful
(b)
Figure 3: Event timespans in restaurant reviews: (a) language units, and (b) language units including their preceding k
dn
.
control? are lower in truthful writing, even though the
document timespans are higher. This difference implies
that the writer is spending a longer period of time to
think before commencing the next sentence, but once
started, the actual typing proceeds rapidly.
Apart from restaurant reviews, truthful writers have
typed slower. This may be due to exercising better care
while expressing their honest opinion.
For restaurant reviews, the document, sentence and
word timespans are significantly higher in deceptive
writing. This, however, is not the case for documents
and words in the other two topics. We conjecture that
this is because deception is harder to write for prod-
uct reviews, due to their dependence on factual details.
Gun control and gay marriage, on the other hand, are
topics well discussed in media, and it is possible that
the writers are aware of the arguments that go against
their personal belief. The frequency of revisional oc-
currences (i.e., keys used for editing) shown in Fig. 1,
too, supports the thought that writing fake reviews may
be harder than adopting a fake stance on well-known
issues. Deceptive reviews exhibit a higher number of
revisions than truthful ones, but essays show the oppo-
site trend. Our findings align with previous studies (Ott
et al., 2011) which showed that deception cues are do-
main dependent.
Writing speed variations over word categories:
Next, we investigate whether there is any quantitative
difference in the writing rate over different words with
respect to the deceptive and truthful intent of the author.
In an attempt to understand this, we analyze words
which show the highest timespan difference between
deceptive and truthful writings.
Table 2 presents words in the restaurant review
topic for which deceptive writers took a lot longer
than truthful writers, and vice versa. Some word cat-
egories exhibit common trends across all three top-
ics. Highly subjective words, for instance (e.g., ?love?,
?best?, ?great?) are words over which truthful writers
spent more time.
Deceptive and truthful texts differ in the typing rate
of first- and second-person pronouns. Deceptive re-
views reveal more time spent in using 2
nd
-person pro-
nouns, as shown by ?you? and ?your?. This finding
throws some light on how people perceive text cues.
Toma and Hancock (2012) showed that readers per-
form poorly at deception detection because they rely on
unrelated text cues such as 2
nd
-person pronouns. Our
analysis indicates that people associate the use of 2
nd
-
person pronouns more with deception not only while
reading, but while writing as well.
Deceptive reviews also exhibit longer time spans for
1
st
-person pronouns (e.g., ?we?, ?me?), which have
been known to be useful in deception detection (New-
man et al., 2003; Ott et al., 2011). Newman et al.
(2003) attributed the less frequent usage of 1
st
-person
pronouns to psychological distancing. The longer time
taken by deceptive writers in our data is a possible sign
of increased cognitive burden when the writer is unable
to maintain the psychological distance. Deceptive re-
viewers also paused a lot more around relative clauses,
e.g., ?if?, ?when?, and ?which?.
In essays, however, the difference in timespans of
1
st
-person and 2
nd
-person pronouns as well as the
timespan difference in relative clauses were insignifi-
cant (< 50ms).
A broader picture of the temporal difference in using
different types of words is presented in Fig. 3, which
shows deceptive reviewers spending less time on ad-
verbs as compared to truthful writers, but more time on
nouns, verbs, adjectives, function words and content
words. They also exhibited significantly longer pauses
before nouns, verbs and function words.
6 Conclusion
In this paper, we investigated the use of typographic
style in deception detection and presented distinct tem-
poral and revisional aspects of keystroke patterns that
improve the characterization of deceptive writing. Our
study provides novel empirically supported insights
into the writing and editing processes of truthful and
deceptive writers. It also presents the first application
of keylogger data used to distinguish between true and
fake texts, and opens up a new range of questions to
better understand what affects these different keystroke
patterns and what they exhibit. It also suggests new
possibilities for making use of keystroke information
as an extended linguistic signal to accompany writings.
Acknowledgements
This research is supported in part by gift from Google.
1472
References
Francesco Bergadano, Daniele Gunetti, and Claudia Pi-
cardi. 2002. User Authentication through Keystroke
Dynamics. ACM Transactions on Information and
System Security (TISSEC), 5(4):367?397.
Charles F Bond and Bella M DePaulo. 2006. Accu-
racy of Deception Judgments. Personality and So-
cial Psychology Review, 10(3):214?234.
Sungzoon Cho, Chigeun Han, Dae Hee Han, and
Hyung-Il Kim. 2000. Web-based Keystroke Dy-
namics Identity Verification Using Neural Network.
Journal of Organizationl Computing and Electronic
Commerce, 10(4):295?307.
Bella M DePaulo, James J Lindsay, Brian E Mal-
one, Laura Muhlenbruck, Kelly Charlton, and Harris
Cooper. 2003. Cues to Deception. Psychological
Bulletin, 129(1):74.
Paul Ekman, Maureen O?Sullivan, Wallace V Friesen,
and Klaus R Scherer. 1991. Invited Article: Face,
Voice and Body in Detecting Deceit. Journal of
Nonverbal Behavior, 15(2):125?135.
Paul Ekman. 2003. Darwin, Deception, and Facial Ex-
pression. Annals of the New York Academy of Sci-
ences, 1000(1):205?221.
Clayton Epp, Michael Lippold, and Regan L Mandryk.
2011. Identifying Emotional States Using Keystroke
Dynamics. In Proc. of the SIGCHI Conference on
Human Factors in Computing Systems, pages 715?
724. ACM.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. The Jour-
nal of Machine Learning Research, 9:1871?1874.
Geli Fei, Arjun Mukherjee, Bing Liu, Meichun Hsu,
Malu Castellanos, and Riddhiman Ghosh. 2013.
Exploiting Burstiness in Reviews for Review Spam-
mer Detection. In ICWSM, pages 175?184.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Syntactic Stylometry for Deception Detection. In
Proc. 50th Annual Meeting of the ACL, pages 171?
175. ACL.
George E Forsen, Mark R Nelson, and Raymond J
Staron Jr. 1977. Personal Attributes Authentication
Techniques. Technical report, DTIC Document.
Stephanie Gokhman, Jeff Hancock, Poornima Prabhu,
Myle Ott, and Claire Cardie. 2012. In Search of a
Gold Standard in Studies of Deception. In Compu-
tational Approaches to Deception Detection, pages
23?30. ACL.
Jeffrey T Hancock, L Curry, Saurabh Goorha, and
Michael T Woodworth. 2004. Lies in Conversa-
tion: An Examination of Deception Using Auto-
mated Linguistic Analysis. In Annual Conference
of the Cognitive Science Society, volume 26, pages
534?540.
Victoria Johansson. 2009. Developmental Aspects of
Text Production in Writing and Speech. Ph.D. thesis,
Lund University.
Kevin S Killourhy and Roy A Maxion. 2009. Compar-
ing Anomaly-Detection Algorithms for Keystroke
Dynamics. In Dependable Systems & Networks,
2009. DSN?09., pages 125?134. IEEE.
John Leggett and Glen Williams. 1988. Verifying
Identity Via Keystroke Characteristics. Interna-
tional Journal of Man-Machine Studies, 28(1):67?
76.
Ann Matsuhashi. 1981. Pausing and Planning: The
Tempo of Written Discourse Production. Research
in the Teaching of English, pages 113?134.
Steven A McCornack. 1997. The Generation of De-
ceptive Messages: Laying the Groundwork for a Vi-
able Theory of Interpersonal Deception. In John O
Greene, editor, Message Production: Advances in
Communication Theory. Erlbaum, Mahwah, NJ.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Natalie Glance. 2013. What Yelp Fake Review Fil-
ter Might be Doing. In ICSWM, pages 409?418.
Matthew L Newman, James W Pennebaker, Diane S
Berry, and Jane M Richards. 2003. Lying Words:
Predicting Deception from Linguistic Styles. Per-
sonality and Social Psychology Bulletin, 29(5):665?
675.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T
Hancock. 2011. Finding Deceptive Opinion Spam
by Any Stretch of the Imagination. In Proc. 49th
Annual Meeting of the ACL: HLT, pages 309?319.
ACL.
Catalina L Toma and Jeffrey T Hancock. 2012. What
Lies Beneath: The Linguistic Traces of Deception in
Online Dating Profiles. Journal of Communication,
62(1):78?97.
Paul V Trovillo. 1939. A History of Lie Detection.
Journal of Criminal Law and Criminology (1931-
1951), 29:848?881.
Janet G van Hell, Ludo Verhoeven, and Liesbeth M van
Beijsterveldt. 2008. Pause Time Patterns in Writ-
ing Narrative and Expository Texts by Children and
Adults. Discourse Processes, 45(4-5):406?427.
Lisa M Vizer, Lina Zhou, and Andrew Sears. 2009.
Automated Stress Detection Using Keystroke and
Linguistic Features: An Exploratory Study. In-
ternational Journal of Human-Computer Studies,
67(10):870?886.
Aldert Vrij, Ronald Fisher, Samantha Mann, and
Sharon Leal. 2006. Detecting Deception by Ma-
nipulating Cognitive Load. Trends in Cognitive Sci-
ences, 10(4):141?142.
Putri Zulkifli. 2013. Applying Pause Analysis to Ex-
plore Cognitive Processes in the Copying of Sen-
tences by Second Language Users. Ph.D. thesis,
University of Sussex.
1473
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 762?772,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Detecting Visual Text
Jesse Dodge1, Amit Goyal2, Xufeng Han3, Alyssa Mensch4, Margaret Mitchell5, Karl Stratos6
Kota Yamaguchi3, Yejin Choi3, Hal Daume? III2, Alexander C. Berg3 and Tamara L. Berg3
1University of Washington, 2University of Maryland, 3Stony Brook University
4MIT, 5Oregon Health & Science University, 6Columbia University
dodgejesse@gmail.com, amit@umiacs.umd.edu, xufhan@cs.stonybrook.edu
acmensch@mit.edu, mitchmar@ohsu.edu, stratos@cs.columbia.edu
kyamagu@cs.stonybrook.edu, ychoi@cs.stonybrook.edu
me@hal3.name, aberg@cs.stonybrook.edu, tlberg@cs.stonybrook.edu
Abstract
When people describe a scene, they often in-
clude information that is not visually apparent;
sometimes based on background knowledge,
sometimes to tell a story. We aim to sepa-
rate visual text?descriptions of what is being
seen?from non-visual text in natural images
and their descriptions. To do so, we first con-
cretely define what it means to be visual, an-
notate visual text and then develop algorithms
to automatically classify noun phrases as vi-
sual or non-visual. We find that using text
alone, we are able to achieve high accuracies
at this task, and that incorporating features
derived from computer vision algorithms im-
proves performance. Finally, we show that we
can reliably mine visual nouns and adjectives
from large corpora and that we can use these
effectively in the classification task.
1 Introduction
People use language to describe the visual world.
Our goal is to: formalize what ?visual text? is (Sec-
tion 2.2); analyze naturally occurring written lan-
guage for occurrences of visual text (Section 2); and
build models that can detect visual descriptions from
raw text or from image/text pairs (Section 3). This
is a challenging problem. One challenge is demon-
strated in Figure 1, which contains two images that
contain the noun ?car? in their human-written cap-
tions. In one case (the top image), there actually is a
car in the image; in the other case, there is not: the
car refers to the state of the speaker.
The ability to automatically identify visual text is
practically useful in a number of scenarios. One can
Another dream car to
add to the list, this one
spotted in Hanbury St.
Shot out my car win-
dow while stuck in traf-
fic because people in
Cincinnati can?t drive in
the rain.
Figure 1: Two image/caption pairs, both containing the
noun ?car? but only the top one in a visual context.
imagine automatically mining image/caption data
(like that in Figure 1) to train object recognition sys-
tems. However, in order to do so reliably, one must
know whether the ?car? actually appears or not.
When building image search engines, it is common
to use text near an image as features; this is more
useful when this text is actually visual. Or when
training systems to automatically generate captions
of images (e.g., for visually impaired users), we
need good language models for visual text.
One of our goals is to define what it means for a
bit of text to be visual. As inspiration, we consider
image/description pairs automatically crawled from
Flickr (Ordonez et al, 2011). A first pass attempt
might be to say ?a phrase in the description of an
image is visual if you can see it in the corresponding
image.? Unfortunately, this is too vague to be useful;
the biggest issues are discussed in Section 2.2.
762
Based on our analysis, we settled on the follow-
ing definition: A piece of text is visual (with re-
spect to a corresponding image) if you can cut out
a part of that image, paste it into any other image,
and a third party could describe that cut-out part in
the same way. In the car example, the claim is that I
could cut out the car, put it in the middle of any other
image, and someone else might still refer to that car
as ?dream car.? The car in the bottom image in Fig-
ure 1 is not visual because there?s nothing you could
cut out that would retain car-ness.
2 Data Analysis
Before embarking on the road to building models of
visual text, it is useful to obtain a better understand-
ing of what visual text is like, and how it compares to
the more standard corpora that we are used to work-
ing with. We describe the two large data sets that we
use (one visual, one non-visual), then describe the
quantitative differences between them, and finally
discuss our annotation effort for labeling visual text.
2.1 Data sets
We use the SBU Captioned Photo Dataset (Ordonez
et al, 2011) as our primary source of image/caption
data. This dataset contains 1 million images with
user associated captions, collected in the wild by in-
telligent filtering of a huge number of Flickr pho-
tos. Past work has made use of this dataset to re-
trieve whole captions for association with a query
image (Ordonez et al, 2011). Their method first
used global image descriptors to retrieve an initial
matched set, and then applied more local estimates
of content to re-rank this (relatively small) set (Or-
donez et al, 2011). This means that content based
matching was relatively constrained by the bottle-
neck of global descriptors, and local content (e.g.,
objects) had relatively small effect on accuracy.
As an auxiliary source of information for (largely)
non-visual text, we consider a large corpus of text
obtained by concatenating ukWaC1 and the New
York Times Newswire Service (NYT) section of the
Gigaword (Graff, 2003) Corpus. The Web-derived
ukWaC is already tokenized and POS-tagged with
the TreeTagger (Schmid, 1995). NYT is tokenized,
1ukWaC is a freely available Wikipedia-derived corpus from
2009; see http://wacky.sslmit.unibo.it/doku.php.
and POS-tagged using TagChunk (Daume? III and
Marcu, 2005). This consists of 171 million sen-
tences (4 billion words). We refer to this generic
text corpus as Large-Data.
2.2 Formalizing visual text
We begin our analysis by revisiting the definition
of visual text from the introduction, and justifying
this particular definition. In order to arrive at a suf-
ficiently specific definition of ?visual text,? we fo-
cused on the applications of visual text that we care
about. As discussed in the introduction, these are:
training object detectors, building image search en-
gines and automatically generating captions for im-
ages. Our definition is based on access to image/text
pairs, but later we discuss how to talk about it purely
based on text. To make things concrete, consider an
image/text pair like that in the top of Figure 1. And
then consider a phrase in the text, like ?dream car.?
The question is: is ?dream car? visual or not?
One of the challenges in arriving at such a defi-
nition is that the description of an image in Flickr
is almost always written by the photographer of that
image. This means the descriptions often contain in-
formation that is not actually pictured in the image,
or contain references that are only relevant to the
photographer (referring to a person/pet by name).
One might think that this is an artifact of this par-
ticular dataset, but it appears to be generic to all cap-
tions, even those written by a viewer (rather than the
photographer). Figure 2 shows an image from the
Pascal dataset (Everingham et al, 2010), together
with captions written by random people collected
via crowd-sourcing (Rashtchian et al, 2010). There
is much in this caption that is clearly made-up by the
author, presumably to make the caption more inter-
esting (e.g., meta-references like ?the camera? or ?A
photo? as well as ?guesses? about the image, such as
?garage? and ?venison?).
Second, there is a question of how much inference
you are allowed to do when you say that you ?see?
something. For example, in the top image in Fig-
ure 1, the street is pictured, but does that mean that
?Hanbury St.? is visual? What if there were a street
sign that clearly read ?Hanbury St.? in the image?
This problem comes up all the time, when people
say things like ?in London? or ?in France? in their
captions. If it?s just a portrait of people ?in France,?
763
1. A distorted photo of a man cutting up a large cut of meat in a garage.
2. A man smiling at the camera while carving up meat.
3. A man smiling while he cuts up a piece of meat.
4. A smiling man is standing next to a table dressing a piece of venison.
5. The man is smiling into the camera as he cuts meat.
Figure 2: An image from the Pascal data with five captions collected via crowd-sourcing. Measurements on the
SMALL and LARGE dataset show that approximately 70% of noun phrases are visual (bolded), while the rest are
non-visual (underlined). See Section 2.4 for details.
it?s hard to say that this is visual. If you see the Eif-
fel tower in the background, this is perhaps better
(though it could be Las Vegas!), but how does this
compare to a photo taken out of an airplane window
in which you actually do see France-the-country?
This problem becomes even more challenging
when you consider things other than nouns. For in-
stance, when is a verb visual? For instance, the most
common non-copula verb in our data is ?sitting,?
which appears in roughly two usages: (1) ?Took this
shot, sitting in a bar and enjoying a Portugese beer.?
and (2) ?Lexy sitting in a basket on top of her cat
tree.? The first one is clearly not visual; the second
probably is. A more nuanced case is for ?playing,?
as in: ?Girls playing in a boat on the river bank?
(probably visual) versus ?Tuckered out from play-
ing in Nannie?s yard.? The corresponding image for
the latter description shows a sleeping cat.
Our final definition, based on cutting out the po-
tentially visual part of the image, allows us to say
that: (1) ?venison? is not visual (because you cannot
actually tell); (2) ?Hanbury St.? and ?Lexy? are not
visual (you can infer them, in the first case because
there is only one street and in the second case be-
cause there is only one cat); (3) that seeing the real
Eiffel tower in the background does not mean that
?France? is visual (but again, may be inferred); etc.
2.3 Most Pronounced Differences
To get an intuitive sense of how Flickr captions (ex-
pected to be predominantly visual) and generic text
(expected not to be so) differ, we computed some
simple statistics on sentences from these. In gen-
eral, the generic text had twice as many main verbs
as the Flickr data, four times as many auxiliaries or
light verbs, and about 50% more prepositions.
Flickr captions tended to have far more references
to physical objects (versus abstract objects) than the
generic text, according to the WordNet hierarchy.
Approximately 64% of the objects in Flickr were
physical (about 22% abstract and 14% unknown).
Whereas in the generic text, only 30% of the objects
were physical, 53% were abstract (17% unknown).
A third major difference between the corpora is
in terms of noun modifiers. In both corpora, nouns
tend not to have any modifiers, but modifiers are still
more prevalent in Flickr than in generic text. In par-
ticular, 60% of nouns in Flickr have zero modifiers,
but 70% of nouns in generic text have zero modi-
fiers. In Flickr, 30% of nouns have exactly one mod-
ifier, as compared to only 22% for generic text.
The breakdown of what those modifiers look like
is even more pronounced, even when restricted just
to physical objects (modifier types are obtained
through the bootstrapping process discussed in Sec-
tion 3.1). Almost 50% of nominal modifiers in the
Flickr data are color modifiers, whereas color ac-
counts for less than 5% of nominal modifiers in
generic text. In Flickr, 10% of modifiers talk about
beauty, in comparison to less than 5% in generic
text. On the other hand, less than 3% of modifiers
in Flickr reference ethnicity, as compared to almost
20% in generic text; and 20% of Flickr modifiers
reference size, versus 50% in generic text.
2.4 Annotating Visual Text
In order to obtain ground truth data, we rely on
crowdsourcing (via Amazon?s Mechanical Turk).
Each instance is an image, a paired caption, and a
highlighted noun phrase in that caption. The anno-
tation for this instance is a label of ?visual,? ?non-
visual? or ?error,? where the error category is re-
764
served for cases where the noun phrase segmenta-
tion was erroneous. Each worker is given five in-
stances to label and paid one cent per annotation.2
For a small amount of data (803 images contain-
ing 2339 instances), we obtained annotations from
three separate workers per instance to obtain higher
quality data. For a large amount of data (48k im-
ages), we obtained annotations from only a sin-
gle worker. Subsequently, we will refer to these
two data sets as the SMALL and LARGE data sets.
In both data sets, approximately 70% of the noun
phrases were visual, 28% were non-visual and 2%
were erroneous. For simplicity, we group erroneous
and non-visual for all learning and evaluation.
In the SMALL data set, the rate of disagreement
between annotators was relatively low. In 74% of the
annotations, there was no disagreement at all. We
reconciled the annotations using the quality manage-
ment technique of Ipeirotis et al (2010); only 14%
of the annotations need to be changed in order to ob-
tain a gold standard.
One immediate question raised in this process is
whether one needs to actually see the image to per-
form the annotation. In particular, if we expect an
NLP system to be able to classify noun phrases as
visual or non-visual, we need to know whether peo-
ple can do this task sans image. We therefore per-
formed the same annotation on the SMALL data set,
but where the workers were not shown the image.
Their task was to imagine an image for this caption
and then annotate the noun phrase based on whether
they thought it would be pictured or not. We ob-
tained three annotations as before and reconciled
them (Ipeirotis et al, 2010). The accuracy of this
reconciled version against the gold standard (pro-
duced by people who did see the image) was 91%.
This suggests that while people are able to do this
task with some reliability, seeing the image is very
important (recall that always guessing ?visual? leads
to an accuracy of 70%).
3 Visual Features from Raw Text
Our first goal is to attempt to obtain relatively large
knowledge bases of terms that are (predominantly)
visual. This is potentially useful in its own right
2Data available at http://hal3.name/dvt/, with direct links
back to the SBU Captioned Photo Dataset.
(for instance, in the context of search, to determine
which query terms are likely to be pictured). We
have explored two techniques for performing this
task, the first based on bootstrapping (Section 3.1)
and the second based on label propagation (Sec-
tion 3.2). We then use these lists to generate features
for a classifier that predicts whether a noun phrase?
in context?is visual or not (Section 4).
In addition, we consider the task of separating ad-
jectives into different visual categories (Section 3.3).
We have already used the results of this in Sec-
tion 2.3 to understand the differences between our
two corpora. It is also potentially useful for the
purpose of building new object detection systems or
even attribute detection systems, to get a vocabulary
of target detections.
3.1 Bootstrapping for Visual Text
In this section, we learn visual and non-visual nouns
and adjectives automatically based on bootstrapping
techniques. First, we construct a graph between ad-
jectives by computing distributional similarity (Tur-
ney and Pantel, 2010) between them. For comput-
ing distributional similarity between adjectives, each
target adjective is defined as a vector of nouns which
are modified by the target adjective. To be exact, we
use only those adjectives as modifiers which appear
adjacent to a noun (that is, in a JJ NN construction).
For example, in ?small red apple,? we consider only
red as a modifier for noun. We use Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1989)
to weight the contexts, and select the top 1000 PMI
contexts for each adjective.3
Next, we apply cosine similarity to find the top
10 distributionally similar adjectives with respect to
each target adjective based on our large generic cor-
pus (Large-Data from Section 2.1). This creates a
graph with adjectives as nodes and cosine similarity
as weight on the edges. Analogously, we construct a
graph with nouns as nodes (here, adjectives are used
as contexts for nouns).
We then apply bootstrapping (Kozareva et al,
2008) on the noun and adjective graphs by select-
ing 10 seeds for visual and non-visual nouns and
adjectives (see Table 1). We use in-degree (sum of
weights of incoming edges) to compute the score for
3We are interested in descriptive adjectives, which ?typi-
cally ascribe to a noun a value of an attribute? (Miller, 1998).
765
Visual car house tree horse animal
nouns man table bottle
seeds woman computer
Non-visual idea bravery deceit trust
nouns dedication anger humour luck
seeds inflation honesty
Visual brown green wooden striped
adjectives orange rectangular furry
seeds shiny rusty feathered
Non-visual public original whole righteous
adjectives political personal intrinsic
seeds individual initial total
Table 1: Example seeds for bootstrapping.
each node that has connections with known (seeds)
or automatically labeled nodes, previously exploited
to learn hyponymy relations from the web (Kozareva
et al, 2008). Intuitively, in-degree captures the pop-
ularity of new instances among instances that have
already been identified as good instances. We learn
visual and non-visual words together (known as the
mutual exclusion principle in bootstrapping (The-
len and Riloff, 2002; McIntosh and Curran, 2008)):
each word (node) is assigned to only one class.
Moreover, after each iteration, we harmonically de-
crease the weight of the in-degree associated with
instances learned in later iterations. We added 25
new instances at each iteration and ran 500 iterations
of bootstrapping, yielding 11955 visual and 11978
non-visual nouns, and 7746 visual and 7464 non-
visual adjectives.
Based on manual inspection, the learned visual
and non-visual lists look great. In the future, we
would like to do a Mechanical Turk evaluation to
directly evaluate the visual and non-visual nouns
and adjectives. For now, we show the coverage of
these classes in the Flickr data-set: Visual nouns:
53.71%; Non-visual nouns: 14.25%; Visual ad-
jectives: 51.79%; Non-visual adjectives: 14.40%.
Overall, we find more visual nouns and adjectives
are covered in the Flickr data-set, which makes
sense, since the Flickr data-set is largely visual.
Second, we show the coverage of these classes
on the large text corpora (Large-Data from Sec-
tion 2.1): Visual nouns: 26.05%; Non-visual nouns:
41.16%; Visual adjectives: 20.02%; Non-visual ad-
Visual: attend, buy, clean, comb, cook, drink, eat,
fry, pack, paint, photograph, smash, spill, steal,
taste, tie, touch, watch, wear, wipe
Non-visual: achieve, admire, admit, advocate, al-
leviate, appreciate, arrange, criticize, eradicate,
induce, investigate, minimize, overcome, pro-
mote, protest, relieve, resolve, review, support,
tolerate
Table 2: Predicates that are visual and non-visual.
Visual: water, cotton, food, pumpkin, chicken,
ring, hair, mouth, meeting, kind, filter, game, oil,
show, tear, online, face, class, car
Non-visual: problem, poverty, pain, issue, use,
symptom, goal, effect, thought, government,
share, stress, work, risk, impact, concern, obsta-
cle, change, disease, dispute
Table 3: Learned visual/non-visual nouns.
jectives: 40.00%. Overall, more non-visual nouns
and adjectives cover text data, since Large-Data is
a non-visual data-set.
3.2 Label Propagation for Visual Text
To propagate visual labels, we construct a bipartite
graph between visually descriptive predicates and
their arguments. Let VP be the set of nodes that cor-
responds to predicates, and let VA be the set of nodes
that corresponds to arguments. To learn the visually
descriptive words, we set VP to 20 visually descrip-
tive predicates shown in the top of Table 2, and VA
to all nouns that appear in the object argument posi-
tion with respect to the seed predicates. We approx-
imate this by taking nouns on the right hand side
of the predicates within a window of 4 words using
the Web 1T Google N-gram data (Brants and Franz.,
2006). For edge weights, we use conditional prob-
abilities between predicates and arguments so that
w(p? a) := pr(a|p) and w(a? p) := pr(p|a).
In order to collectively induce the visually de-
scriptive words from this graph, we apply the graph
propagation algorithm of Velikovich et al (2010),
a variant of label propagation algorithms (Zhu and
Ghahramani, 2002) that has been shown to be ef-
fective for inducing a web-scale polarity lexicon
based on word co-occurrence statistics. This algo-
766
Color purple blue maroon beige green
Material plastic cotton wooden metallic silver
Shape circular square round rectangular triangular
Size small big tiny tall huge
Surface coarse smooth furry fluffy rough
Direction sideways north upward left down
Pattern striped dotted checked plaid quilted
Quality shiny rusty dirty burned glittery
Beauty beautiful cute pretty gorgeous lovely
Age young mature immature older senior
Ethnicity french asian american greek hispanic
Table 4: Attribute Classes with their seed values
rithm iteratively updates the semantic distance be-
tween each pair of nodes in the graph, then produces
a score for each node that represents how visually
descriptive each word is. To learn the words that
are not visually descriptive, we use the predicates
shown in the bottom of Table 2 as VP instead. Ta-
ble 3 shows the top ranked nouns that are visually
descriptive and not visually descriptive.
3.3 Bootstrapping Visual Adjectives
Our goal in this section is to automatically gener-
ate comprehensive lists of adjectives for different at-
tributes, such as color, material, shape, etc. To our
knowledge, this is the first significant effort of this
type for adjectives: most bootstrapping techniques
focus exclusively on nouns, although Almuhareb
and Poesio (2005) populated lists of attributes us-
ing web-based similarity measures. We found that
in some ways adjectives are easier than nouns, but
require slightly different representations.
One might conjecture that listing attributes by
hand is difficult. Colors names are well known to
be quite varied. For instance, our bootstrapping
approach is able to discover colors like ?grayish,?
?chestnut,? ?emerald,? and ?rufous? that would be
hard to list manually (the last is a reddish-brown
color, somewhat like rust). Although perhaps not
easy to create, the Wikipedia list of colors (http:
//en.wikipedia.org/wiki/List of colors) includes all of these
except ?grayish?. On the other hand, it includes
color terms that might be difficult to make use of as
colors, such as ?bisque,? ?bone? and ?bubbles? (the
last is a very light cyan), which might over-generate
hits. For shape, we find ?oblong,? ?hemispherical,?
?quadrangular? and, our favorite, ?convex?.
We use essentially the same bootstrapping process
as described earlier in Section 3.1, but on a slightly
different data representation. The only difference is
that instead of linking adjectives to their 10 most
similar neighbors, we link them only to 25 neigh-
bors to attempt to improve recall.
We begin with seeds for each attribute class from
Table 4. We conduct a manual evaluation to di-
rectly measure the quality of attribute classes. We
recruited 3 annotators and developed annotation
guidelines that instructed each recruiter to judge
whether a learned value belongs to an attribute class
or not. The annotators assigned ?1? if a learned
value belongs to a class, otherwise ?0?.
We conduct an Information Retrieval (IR) Style
human evaluation. Analogous to an IR evaluation,
here the total number of relevant values for attribute
classes can not be computed. Therefore, we assume
the correct output of several systems as the total re-
call which can be produced by any system. Now,
with the help of our 3 manual annotators, we obtain
the correct output of several systems from the total
output produced by these systems.
First, we measured the agreement on whether
each learned value belongs to a semantic class or
not. We computed ? to measure inter-annotator
agreement for each pair of annotators. We focus
our evaluation on 4 classes: age, beauty, color, and
direction; between Human 2 and Human 3 and be-
tween Human 1 and Human 3, the ? value was 0.48;
between Human 1 and Human 2 it was 0.45. These
numbers are somewhat lower than we would like,
but not terrible. If we evaluate the classes individu-
ally, we find that age has the lowest ?. If we remove
?age,? the pairwise ?s rise to 0.59, 0.57 and 0.55.
Second, we compute Precision (Pr), Recall (Rec)
and F-measure (F1) for different bootstrapping sys-
tems (based on the number of iterations and the
number of new words added in each iteration).
Two parameter settings performed consistently bet-
ter than others (10 iterations with 25 items, and 5 it-
erations with 50 items). The former system achieves
a precision/recall/F1 of 0.53, 0.71, 0.60 against Hu-
man 2; the latter achieves scores of 0.54, 0.72, 0.62.
4 Recognizing Visual Text
We train a logistic regression (aka maximum en-
tropy) model (Daume? III, 2004) to classify text as
visual or non-visual. The features we use fall into
767
the following categories: WORDS (the actual lexi-
cal items and stems); BIGRAMS (lexical bigrams);
SPELL (lexical features such as capitalization pat-
tern, and word prefixes and suffixes); WORDNET
(set of hypernyms according to WordNet); and
BOOTSTRAP (features derived from bootstrapping
or label propagation).
For each of these feature categories, we compute
features inside the phrase being categorized (e.g.,
?the car?), before the phrase (two words to the left)
and after the phrase (two words to the right). We
additionally add a feature that computes the num-
ber of words in a phrase, and a feature that com-
putes the position of the phrase in the caption (first
fifth through last fifth of the description). This leads
to seventeen feature templates that are computed for
each example. In the SMALL data set, there are 25k
features (10k non-singletons); in the LARGE data
set, there are 191k features (79k non-singletons).
To train models on the SMALL data set, we use
1500 instances as training, 200 as development and
the remaining 639 as test data. To train models on
the LARGE data set, we use 45000 instances as train-
ing and the remaining 4401 as development. We
always test on the 639 instances from the SMALL
data, since it has been redundantly annotated. The
development data is used only to choose the regular-
ization parameter for a Gaussian prior on the logis-
tic regression model; this parameter is chosen in the
range {0.01, 0.05, 0.1, 0.5, 1, 2, 4, 8, 16, 32, 64}.
Because of the imbalanced data problem, evalu-
ating according to accuracy is not appropriate for
this task. Even evaluating by precision/recall is not
appropriate, because a baseline system that guesses
that everything is visual obtains 100% recall and
70% precision. Due to these issues, we instead
evaluate according to the area under the ROC curve
(AUC). To check statistical significance, we com-
pute standard deviations using bootstrap resampling,
and consider there to be a significant difference if a
result falls outside of two standard deviations of the
baseline (95% confidence).
Figure 3 shows learning curves for the two data
sets. The SMALL data achieves an AUC score of
71.3 in the full data setting (1700 examples); the
LARGE data needs 12k examples to achieve similar
accuracy due to noise. However, with 49k examples,
we are able to achieve a AUC score of 75.3 using the
101 102 103 104 105
0.55
0.6
0.65
0.7
0.75
0.8
Figure 3: Learning curves for training on SMALL data
(blue solid) and LARGE data (black dashed). X-axis (in
log-scale) is number of training examples; Y-axis is AUC.
large data set. By pooling the data (and weighting
the small data), this boosts results to 76.1. The con-
fidence range on these data is approximately ?1.9,
meaning that this boost is likely not significant.
4.1 Using Image Features
As discussed previously, humans are only able to
achieve 90% accuracy on the visual/non-visual task
when they are not allowed to view the image.
This potentially upper-bounds the performance of a
learned system that can only look at text. In order to
attempt to overcome this, we augment our basic sys-
tem with a number of features computed from the
corresponding images. These features are derived
from the output of state of the art vision algorithms
to detect 121 different objects, stuff and scenes.
As our object detectors, we use standard state
of the art deformable part-based models (Felzen-
szwalb et al, 2010) for 89 common object cate-
gories, including: the original 20 objects from Pas-
cal, 49 objects from Object Bank (Li-Jia Li and Fei-
Fei, 2010), and 20 from Im2Text (Ordonez et al,
2011). We additionally use coarse image parsing
to estimate background elements in each database
image. Six possible background (stuff) categories
are considered: sky, water, grass, road, tree, and
building. For this we use detectors (Ordonez et
al., 2011) which compute color, texton, HoG (Dalal
and Triggs, 2005) and Geometric Context (Hoiem
et al, 2005) as input features to a sliding win-
dow based SVM classifier. These detectors are run
on all database images, creating a large pool of
background elements for retrieval. Finally, we ob-
768
Figure 4: (Left) Highest confidence flower detected in an
image; (Right) All detections in the same image.
tain scene descriptors for each image by comput-
ing scene classification scores for 26 common scene
categories, using the features, methods and training
data from the SUN dataset (Xiao et al, 2010).
Figure 4 shows an example image on which sev-
eral detectors have been run. From each image, we
extract the following features: which object detec-
tors fired; how many times they fired; the confidence
of the most-likely firing; the percentage of the image
(in pixels) that the bounding box corresponding to
this object occupies; and the percentage of the width
(and height) of the image that it occupies.
Unfortunately, object detection is a highly noisy
process. The right image in Figure 4 shows all de-
tections for that image, which includes, for instance,
a chair detection that spans nearly the entire image,
and a person detection in the bottom-right corner.
For an average image, if a single detector (e.g., the
flower detector) fires once, it actually fires 40 times
(?? = 1.8). Moreover, of the 120 detectors, on
an average image over 22 (?? = 5.6) of them fire
at least once (though certainly in an average image
only a few objects are actually present). Exacerbat-
ing this problem, although the confidence scores for
a single detector can be compared, the scores be-
tween different detectors are not at all comparable.
In order to attenuate this problem, we include dupli-
cate copies of all the above features restricted to the
most confident object for each object type.
On the SMALL data set, this adds 400 new fea-
CATEGORY POSITION AUC
Bootstrap Phrase 65.2
+ Spell Phrase 68.6
+ Image - 69.2
+ Words Phrase 70.0
+ Length - 69.8
+ Wordnet Phrase 70.4
+ Wordnet Before 70.6
+ Spell Before 71.8
+ Words Before 72.2
+ Bootstrap Before 72.4
+ Spell After 71.5
Table 5: Results of feature ablation on SMALL data set.
Best result is in bold; results that are not statistically sig-
nificantly worse are italicized.
tures (300 of which are non-singletons4); on the
LARGE data set, this adds 500 new features (480
non-singletons). Overall, the AUC scores trained on
the small data set increase from 71.3 to 73.9 (a sig-
nificant improvement). On the large data set, the in-
crease is only from 76.1 to 76.8, which is not likely
to be significant. In general, the improvement ob-
tained by adding image features is most pronounced
in the setting of small training data, perhaps because
these features are more generic than the highly lexi-
calized features used in the textual model. But once
there is a substantial amount of text data, the noisy
image features become less useful.
4.2 Feature Ablations
In order to ascertain the degree to which each feature
template is useful, we perform an ablation study. We
first perform feature selection at the template level
using the information gain criteria, and then train
models using the corresponding subset of features.
The results on the SMALL data set are shown in
Table 5. Here, the bootstrapping features computed
on words within the phrase to be classified were
judged as the most useful, followed by spelling fea-
tures. Image features were judged third most use-
ful. In general, features in the phrase were most use-
ful (not surprisingly), and then features before the
phrase (presumably to give context, for instance as
in ?out of the window?). Features from after the
phrase were not useful.
4Non-singleton features appear more than once in the data.
769
CATEGORY POSITION AUC
Words Phrase 74.7
+ Image - 74.4
+ Bootstrap Phrase 74.3
+ Spell Phrase 75.3
+ Length - 74.7
+ Words Before 76.2
+ Wordnet Phrase 76.1
+ Spell After 76.0
+ Spell Before 76.8
+ Wordnet Before 77.0
+ Wordnet After 75.6
Table 6: Results of feature ablation on LARGE data set.
Corresponding results on the LARGE data set are
shown in Table 6. Note that the order of features
selected is different because the training data is dif-
ferent. Here, the most useful features are simply the
words in the phrase to be classified, which alone al-
ready gives an AUC score of 74.7, only a few points
off from the best performance of 77.0 once image
features, bootstrap features and spelling features are
added. As before, these features are rated as very
useful for classification performance.
Finally, we consider the effect of using Bootstrap-
based features or label-propagation-based features.
In all the above experiments, the features used
are based on the union of word lists created by
these two techniques. We perform three experi-
ments. Beginning with the system that contains all
features (SMALL=73.9, LARGE=76.8), we first re-
move the bootstrap-based features (SMALL?71.8,
LARGE?75.5) or remove the label-propagation-
based features (SMALL?71.2, LARGE?74.9) or
remove both (SMALL?70.7, LARGE?74.2). From
these results, we can see that these techniques are
useful, but somewhat redundant: if you had to
choose one, you should choose label-propagation.
5 Discussion
As connections between language and vision be-
come stronger, for instance in the contexts of ob-
ject detection (Hou and Zhang, 2007; Kim and Tor-
ralba, 2009; Sivic et al, 2008; Alexe et al, 2010;
Gu et al, 2009), attribute detection (Ferrari and Zis-
serman, 2007; Farhadi et al, 2009; Kumar et al,
2009; Berg et al, 2010), visual phrases (Farhadi and
Sadeghi, 2011), and automatic caption generation
(Farhadi et al, 2010; Feng and Lapata, 2010; Or-
donez et al, 2011; Kulkarni et al, 2011; Yang et
al., 2011; Li et al, 2011; Mitchell et al, 2012), it
becomes increasingly important to understand, and
to be able to detect, text that actually refers to ob-
served phenomena. Our results suggest that while
this is a hard problem, it is possible to leverage large
text resources and state-of-the-art computer vision
algorithms to address it with high accuracy.
Acknowledgments
T.L. Berg and K. Yamaguchi were supported in part
by NSF Faculty Early Career Development (CA-
REER) Award #1054133; A.C. Berg and Y. Choi
were partially supported by the Stony Brook Uni-
versity Office of the Vice President for Research; H.
Daume? III and A. Goyal were partially supported by
NSF Award IIS-1139909; all authors were partially
supported by a 2011 JHU Summer Workshop.
References
B. Alexe, T. Deselaers, and V. Ferrari. 2010. What is an
object? In Computer Vision and Pattern Recognition
(CVPR), 2010 IEEE Conference on, pages 73 ?80.
A. Almuhareb and M. Poesio. 2005. Finding concept at-
tributes in the web. In Corpus Linguistics Conference.
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and characteriza-
tion from noisy web data. In European Conference on
Computer Vision (ECCV).
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76?83, Vancouver,
Canada, June.
N. Dalal and B. Triggs. 2005. Histograms of oriented
gradients for human detection. In CVPR.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of the In-
ternational Conference on Machine Learning (ICML).
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available
at http://pub.hal3.name/#daume04cg-bfgs, implementation
available at http://hal3.name/megam/, August.
770
M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. 2010. The PASCAL
Visual Object Classes Challenge 2010 (VOC2010)
Results. http://www.pascal-network.org/challenges/VOC/
voc2010/workshop/index.html.
Ali Farhadi and Amin Sadeghi. 2011. Recognition us-
ing visual phrases. In Computer Vision and Pattern
Recognition (CVPR).
A. Farhadi, I. Endres, D. Hoiem, and D.A. Forsyth. 2009.
Describing objects by their attributes. In Computer
Vision and Pattern Recognition (CVPR).
A. Farhadi, M. Hejrati, M.A. Sadeghi, P. Young,
C. Rashtchian1, J. Hockenmaier, and D.A. Forsyth.
2010. Every picture tells a story: Generating sentences
from images. In ECCV.
P. F. Felzenszwalb, R. B. Girshick, and D. McAllester.
2010. Discriminatively trained deformable part
models, release 4. http://people.cs.uchicago.edu/?pff/
latent-release4/.
Y. Feng and M. Lapata. 2010. How many words is a
picture worth? automatic caption generation for news
images. In ACL.
V. Ferrari and A. Zisserman. 2007. Learning visual at-
tributes. In Advances in Neural Information Process-
ing Systems (NIPS).
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Chunhui Gu, J.J. Lim, P. Arbelaez, and J. Malik. 2009.
Recognition using regions. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Confer-
ence on, pages 1030 ?1037.
Derek Hoiem, Alexei A. Efros, and Martial Hebert.
2005. Geometric context from a single image. In
ICCV.
Xiaodi Hou and Liqing Zhang. 2007. Saliency detection:
A spectral residual approach. In Computer Vision and
Pattern Recognition, 2007. CVPR ?07. IEEE Confer-
ence on, pages 1 ?8.
P. Ipeirotis, F. Provost, and J. Wang. 2010. Quality man-
agement on amazon mechanical turk. In Proceedings
of the Second Human Computation Workshop (KDD-
HCOMP).
Gunhee Kim and Antonio Torralba. 2009. Unsupervised
Detection of Regions of Interest using Iterative Link
Analysis. In Annual Conference on Neural Informa-
tion Processing Systems (NIPS 2009).
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048?1056, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C
Berg, and T. L Berg. 2011. Babytalk: Understanding
and generating simple image descriptions. In CVPR.
N. Kumar, A.C. Berg, P. Belhumeur, and S.K. Nayar.
2009. Attribute and simile classifiers for face verifi-
cation. In ICCV.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
CONLL.
Eric P. Xing Li-Jia Li, Hao Su and Li Fei-Fei. 2010. Ob-
ject bank: A high-level image representation for scene
classification and semantic feature sparsification. In
NIPS.
Tara McIntosh and James R Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop 2008, pages 97?105, December.
K.J. Miller. 1998. Modifiers in WordNet. In C. Fell-
baum, editor, WordNet, chapter 2. MIT Press.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,
Alex Berg, Tamara Berg, and Hal Daume? III. 2012.
Midge: Generating image descriptions from computer
vision detections. Proceedings of EACL 2012.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing Images Using 1 Million
Captioned Photographs. In NIPS.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using amazon?s mechanical turk. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Association for Computational Linguistics.
H. Schmid. 1995. Improvements in part?of?speech tag-
ging with an application to german. In Proceedings of
the EACL SIGDAT Workshop.
J. Sivic, B.C. Russell, A. Zisserman, W.T. Freeman, and
A.A. Efros. 2008. Unsupervised discovery of visual
object class hierarchies. In Computer Vision and Pat-
tern Recognition, 2008. CVPR 2008. IEEE Conference
on, pages 1 ?8.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of the Empirical Meth-
ods in Natural Language Processing, pages 214?221.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Artificial Intelligence Research (JAIR),
37:141.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
771
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba.
2010. Sun database: Large-scale scene recognition
from abbey to zoo. In CVPR.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gener-
ation of natural images. In EMNLP.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
772
Proceedings of the ACL 2010 Conference Short Papers, pages 269?274,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hierarchical Sequential Learning for Extracting Opinions and their
Attributes
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Abstract
Automatic opinion recognition involves a
number of related tasks, such as identi-
fying the boundaries of opinion expres-
sion, determining their polarity, and de-
termining their intensity. Although much
progress has been made in this area, ex-
isting research typically treats each of the
above tasks in isolation. In this paper,
we apply a hierarchical parameter shar-
ing technique using Conditional Random
Fields for fine-grained opinion analysis,
jointly detecting the boundaries of opinion
expressions as well as determining two of
their key attributes ? polarity and inten-
sity. Our experimental results show that
our proposed approach improves the per-
formance over a baseline that does not
exploit hierarchical structure among the
classes. In addition, we find that the joint
approach outperforms a baseline that is
based on cascading two separate compo-
nents.
1 Introduction
Automatic opinion recognition involves a number
of related tasks, such as identifying expressions of
opinion (e.g. Kim and Hovy (2005), Popescu and
Etzioni (2005), Breck et al (2007)), determining
their polarity (e.g. Hu and Liu (2004), Kim and
Hovy (2004), Wilson et al (2005)), and determin-
ing their strength, or intensity (e.g. Popescu and
Etzioni (2005), Wilson et al (2006)). Most pre-
vious work treats each subtask in isolation: opin-
ion expression extraction (i.e. detecting the bound-
aries of opinion expressions) and opinion attribute
classification (e.g. determining values for polar-
ity and intensity) are tackled as separate steps in
opinion recognition systems. Unfortunately, er-
rors from individual components will propagate in
systems with cascaded component architectures,
causing performance degradation in the end-to-
end system (e.g. Finkel et al (2006)) ? in our
case, in the end-to-end opinion recognition sys-
tem.
In this paper, we apply a hierarchical param-
eter sharing technique (e.g., Cai and Hofmann
(2004), Zhao et al (2008)) using Conditional Ran-
dom Fields (CRFs) (Lafferty et al, 2001) to fine-
grained opinion analysis. In particular, we aim to
jointly identify the boundaries of opinion expres-
sions as well as to determine two of their key at-
tributes ? polarity and intensity.
Experimental results show that our proposed ap-
proach improves the performance over the base-
line that does not exploit the hierarchical structure
among the classes. In addition, we find that the
joint approach outperforms a baseline that is based
on cascading two separate systems.
2 Hierarchical Sequential Learning
We define the problem of joint extraction of opin-
ion expressions and their attributes as a sequence
tagging task as follows. Given a sequence of to-
kens, x = x1 ... xn, we predict a sequence of
labels, y = y1 ... yn, where yi ? {0, ..., 9} are
defined as conjunctive values of polarity labels
and intensity labels, as shown in Table 1. Then
the conditional probability p(y|x) for linear-chain
CRFs is given as (Lafferty et al, 2001)
P (y|x) = 1
Zx
exp
?
i
(
? f(yi, x, i)+?? f ?(yi?1, yi, x, i)
)
where Zx is the normalization factor.
In order to apply a hierarchical parameter shar-
ing technique (e.g., Cai and Hofmann (2004),
Zhao et al (2008)), we extend parameters as fol-
lows.
269
 
  

	 

  

  
 


	 




 
 

	 

 

 
       
	 

    
Figure 1: The hierarchical structure of classes for opinion expressions with polarity (positive, neutral,
negative) and intensity (high, medium, low)
LABEL 0 1 2 3 4 5 6 7 8 9
POLARITY none positive positive positive neutral neutral neutral negative negative negative
INTENSITY none high medium low high medium low high medium low
Table 1: Labels for Opinion Extraction with Polarity and Intensity
? f(yi, x, i) = ?? gO(?, x, i) (1)
+ ?? gP(?, x, i)
+ ?? gS(?, x, i)
?? f ?(yi?1, yi, x, i) = ???,?? g
?
O(?, ??, x, i)
+ ???,?? g
?
P(?, ??, x, i)
+ ???,?? g
?
S(?, ??, x, i)
where gO and g?O are feature vectors defined for
Opinion extraction, gP and g?P are feature vectors
defined for Polarity extraction, and gS and g?S are
feature vectors defined for Strength extraction, and
?, ?? ? {OPINION, NO-OPINION}
?, ?? ? {POSITIVE, NEGATIVE, NEUTRAL, NO-POLARITY}
?, ?? ? {HIGH, MEDIUM, LOW, NO-INTENSITY}
For instance, if yi = 1, then
? f(1, x, i) = ?OPINION gO(OPINION, x, i)
+ ?POSITIVE gP(POSITVE, x, i)
+ ?HIGH gS(HIGH, x, i)
If yi?1 = 0, yi = 4, then
?? f ?(0, 4, x, i)
= ??NO-OPINION,OPINION g
?
O(NO-OPINION, OPINION, x, i)
+ ??NO-POLARITY, NEUTRAL g
?
P(NO-POLARITY, NEUTRAL, x, i)
+ ??NO-INTENSITY, HIGH g
?
S(NO-INTENSITY, HIGH, x, i)
This hierarchical construction of feature and
weight vectors allows similar labels to share the
same subcomponents of feature and weight vec-
tors. For instance, all ? f(yi, x, i) such that
yi ? {1, 2, 3} will share the same compo-
nent ?POSITIVE gP(POSITVE, x, i). Note that there
can be other variations of hierarchical construc-
tion. For instance, one can add ?? gI(?, x, i)
and ??
?,??
g?I(?, ??, x, i) to Equation (1) for ? ?
{0, 1, ..., 9}, in order to allow more individualized
learning for each label.
Notice also that the number of sets of param-
eters constructed by Equation (1) is significantly
smaller than the number of sets of parameters that
are needed without the hierarchy. The former re-
quires (2+ 4+4)+ (2? 2+4? 4+ 4? 4) = 46
sets of parameters, but the latter requires (10) +
(10 ? 10) = 110 sets of parameters. Because a
combination of a polarity component and an in-
tensity component can distinguish each label, it is
not necessary to define a separate set of parameters
for each label.
3 Features
We first introduce definitions of key terms that will
be used to describe features.
? PRIOR-POLARITY & PRIOR-INTENSITY:
We obtain these prior-attributes from the polar-
ity lexicon populated by Wilson et al (2005).
? EXP-POLARITY, EXP-INTENSITY & EXP-SPAN:
Words in a given opinion expression often do
not share the same prior-attributes. Such dis-
continuous distribution of features can make
it harder to learn the desired opinion expres-
sion boundaries. Therefore, we try to obtain
expression-level attributes (EXP-POLARITY and
EXP-INTENSITY) using simple heuristics. In or-
der to derive EXP-POLARITY, we perform simple
270
voting. If there is a word with a negation effect,
such as ?never?, ?not?, ?hardly?, ?against?, then
we flip the polarity. For EXP-INTENSITY, we use
the highest PRIOR-INTENSITY in the span. The text
span with the same expression-level attributes
are referred to as EXP-SPAN.
3.1 Per-Token Features
Per-token features are defined in the form of
gO(?, x, i), gP(?, x, i) and gS(?, x, i). The do-
mains of ?, ?, ? are as given in Section 3.
Common Per-Token Features
Following features are common for all class labels.
The notation ? indicates conjunctive operation of
two values.
? PART-OF-SPEECH(xi):
based on GATE (Cunningham et al, 2002).
? WORD(xi), WORD(xi?1), WORD(xi+1)
? WORDNET-HYPERNYM(xi):
based on WordNet (Miller, 1995).
? OPINION-LEXICON(xi):
based on opinion lexicon (Wiebe et al, 2002).
? SHALLOW-PARSER(xi):
based on CASS partial parser (Abney, 1996).
? PRIOR-POLARITY(xi) ? PRIOR-INTENSITY(xi)
? EXP-POLARITY(xi) ? EXP-INTENSITY(xi)
? EXP-POLARITY(xi) ? EXP-INTENSITY(xi) ?
STEM(xi)
? EXP-SPAN(xi):
boolean to indicate whether xi is in an EXP-SPAN.
? DISTANCE-TO-EXP-SPAN(xi): 0, 1, 2, 3+.
? EXP-POLARITY(xi) ? EXP-INTENSITY(xi) ?
EXP-SPAN(xi)
Polarity Per-Token Features
These features are included only for gO(?, x, i)
and gP(?, x, i), which are the feature functions
corresponding to the polarity-based classes.
? PRIOR-POLARITY(xi), EXP-POLARITY((xi)
? STEM(xi) ? EXP-POLARITY(xi)
? COUNT-OF-Polarity:
where Polarity ? {positive, neutral, negative}.
This feature encodes the number of positive,
neutral, and negative EXP-POLARITY words re-
spectively, in the current sentence.
? STEM(xi) ? COUNT-OF-Polarity
? EXP-POLARITY(xi) ? COUNT-OF-Polarity
? EXP-SPAN(xi) and EXP-POLARITY(xi)
? DISTANCE-TO-EXP-SPAN(xi) ? EXP-POLARITY(xp)
Intensity Per-Token Features
These features are included only for gO(?, x, i)
and gS(?, x, i), which are the feature functions cor-
responding to the intensity-based classes.
? PRIOR-INTENSITY(xi), EXP-INTENSITY(xi)
? STEM(xi) ? EXP-INTENSITY(xi)
? COUNT-OF-STRONG, COUNT-OF-WEAK:
the number of strong and weak EXP-INTENSITY
words in the current sentence.
? INTENSIFIER(xi): whether xi is an intensifier,
such as ?extremely?, ?highly?, ?really?.
? STRONGMODAL(xi): whether xi is a strong modal
verb, such as ?must?, ?can?, ?will?.
? WEAKMODAL(xi): whether xi is a weak modal
verb, such as ?may?, ?could?, ?would?.
? DIMINISHER(xi): whether xi is a diminisher, such
as ?little?, ?somewhat?, ?less?.
? PRECEDED-BY-? (xi),
PRECEDED-BY-? (xi) ? EXP-INTENSITY(xi):
where ? ? { INTENSIFIER, STRONGMODAL, WEAK-
MODAL, DIMINISHER}
? ? (xi) ? EXP-INTENSITY(xi),
? (xi) ? EXP-INTENSITY(xi?1),
? (xi?1) ? EXP-INTENSITY(xi+1)
? EXP-SPAN(xi) ? EXP-INTENSITY(xi)
? DISTANCE-TO-EXP-SPAN(xi) ? EXP-INTENSITY(xp)
3.2 Transition Features
Transition features are employed to help with
boundary extraction as follows:
Polarity Transition Features
Polarity transition features are features that are
used only for g?O(?, ??, x, i) and g?P(?, ??, x, i).
? PART-OF-SPEECH(xi) ? PART-OF-SPEECH(xi+1) ?
EXP-POLARITY(xi)
? EXP-POLARITY(xi) ? EXP-POLARITY(xi+1)
Intensity Transition Features
Intensity transition features are features that are
used only for g?O(?, ??, x, i) and g?S(?, ??, x, i).
? PART-OF-SPEECH(xi) ? PART-OF-SPEECH(xi+1) ?
EXP-INTENSITY(xi)
? EXP-INTENSITY(xi) ? EXP-INTENSITY(xi+1)
4 Evaluation
We evaluate our system using the Multi-
Perspective Question Answering (MPQA) cor-
pus1. Our gold standard opinion expressions cor-
1The MPQA corpus can be obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
271
Positive Neutral Negative
Method Description r(%) p(%) f(%) r(%) p(%) f(%) r(%) p(%) f(%)
Polarity-Only ? Intensity-Only (BASELINE1) 29.6 65.7 40.8 26.5 69.1 38.3 35.5 77.0 48.6
Joint without Hierarchy (BASELINE2) 30.7 65.7 41.9 29.9 66.5 41.2 37.3 77.1 50.3
Joint with Hierarchy 31.8 67.1 43.1 31.9 66.6 43.1 40.4 76.2 52.8
Table 2: Performance of Opinion Extraction with Correct Polarity Attribute
High Medium Low
Method Description r(%) p(%) f(%) r(%) p(%) f(%) r(%) p(%) f(%)
Polarity-Only ? Intensity-Only (BASELINE1) 26.4 58.3 36.3 29.7 59.0 39.6 15.4 60.3 24.5
Joint without Hierarchy (BASELINE2) 29.7 54.2 38.4 28.0 57.4 37.6 18.8 55.0 28.0
Joint with Hierarchy 27.1 55.2 36.3 32.0 56.5 40.9 21.1 56.3 30.7
Table 3: Performance of Opinion Extraction with Correct Intensity Attribute
Method Description r(%) p(%) f(%)
Polar-Only ? Intensity-Only 43.3 92.0 58.9
Joint without Hierarchy 46.0 88.4 60.5
Joint with Hierarchy 48.0 87.8 62.0
Table 4: Performance of Opinion Extraction
respond to direct subjective expression and expres-
sive subjective element (Wiebe et al, 2005).2
Our implementation of hierarchical sequential
learning is based on the Mallet (McCallum, 2002)
code for CRFs. In all experiments, we use a Gaus-
sian prior of 1.0 for regularization. We use 135
documents for development, and test on a dif-
ferent set of 400 documents using 10-fold cross-
validation. We investigate three options for jointly
extracting opinion expressions with their attributes
as follows:
[Baseline-1] Polarity-Only ? Intensity-Only:
For this baseline, we train two separate sequence
tagging CRFs: one that extracts opinion expres-
sions only with the polarity attribute (using com-
mon features and polarity extraction features in
Section 3), and another that extracts opinion ex-
pressions only with the intensity attribute (using
common features and intensity extraction features
in Section 3). We then combine the results from
two separate CRFs by collecting all opinion en-
tities extracted by both sequence taggers.3 This
2Only 1.5% of the polarity annotations correspond to
both; hence, we merge both into the neutral. Similarly, for
gold standard intensity, we merge extremely high into high.
3We collect all entities whose portions of text spans are
extracted by both models.
baseline effectively represents a cascaded compo-
nent approach.
[Baseline-2] Joint without Hierarchy: Here
we use simple linear-chain CRFs without exploit-
ing the class hierarchy for the opinion recognition
task. We use the tags shown in Table 1.
Joint with Hierarchy: Finally, we test the hi-
erarchical sequential learning approach elaborated
in Section 3.
4.1 Evaluation Results
We evaluate all experiments at the opinion entity
level, i.e. at the level of each opinion expression
rather than at the token level. We use three evalua-
tion metrics: recall, precision, and F-measure with
equally weighted recall and precision.
Table 4 shows the performance of opinion ex-
traction without matching any attribute. That is, an
extracted opinion entity is counted as correct if it
overlaps4 with a gold standard opinion expression,
without checking the correctness of its attributes.
Table 2 and 3 show the performance of opinion
extraction with the correct polarity and intensity
respectively.
From all of these evaluation criteria, JOINT WITH
4Overlap matching is a reasonable choice as the annotator
agreement study is also based on overlap matching (Wiebe
et al, 2005). One might wonder whether the overlap match-
ing scheme could allow a degenerative case where extracting
the entire test dataset as one giant opinion expression would
yield 100% recall and precision. Because each sentence cor-
responds to a different test instance in our model, and because
some sentences do not contain any opinion expression in the
dataset, such degenerative case is not possible in our experi-
ments.
272
HIERARCHY performs the best, and the least effec-
tive one is BASELINE-1, which cascades two sepa-
rately trained models. It is interesting that the sim-
ple sequential tagging approach even without ex-
ploiting the hierarchy (BASELINE-2) performs better
than the cascaded approach (BASELINE-1).
When evaluating with respect to the polarity at-
tribute, the performance of the negative class is
substantially higher than the that of other classes.
This is not surprising as there is approximately
twice as much data for the negative class. When
evaluating with respect to the intensity attribute,
the performance of the LOW class is substantially
lower than that of other classes. This result reflects
the fact that it is inherently harder to distinguish
an opinion expression with low intensity from no
opinion. In general, we observe that determining
correct intensity attributes is a much harder task
than determining correct polarity attributes.
In order to have a sense of upper bound, we
also report the individual performance of two sep-
arately trained models used for BASELINE-1: for the
Polarity-Only model that extracts opinion bound-
aries only with polarity attribute, the F-scores with
respect to the positive, neutral, negative classes are
46.7, 47.5, 57.0, respectively. For the Intensity-
Only model, the F-scores with respect to the high,
medium, low classes are 37.1, 40.8, 26.6, respec-
tively. Remind that neither of these models alone
fully solve the joint task of extracting boundaries
as well as determining two attributions simultane-
ously. As a result, when conjoining the results
from the two models (BASELINE-1), the final per-
formance drops substantially.
We conclude from our experiments that the sim-
ple joint sequential tagging approach even with-
out exploiting the hierarchy brings a better perfor-
mance than combining two separately developed
systems. In addition, our hierarchical joint se-
quential learning approach brings a further perfor-
mance gain over the simple joint sequential tag-
ging method.
5 Related Work
Although there have been much research for fine-
grained opinion analysis (e.g., Hu and Liu (2004),
Wilson et al (2005), Wilson et al (2006), Choi
and Claire (2008), Wilson et al (2009)),5 none is
5For instance, the results of Wilson et al (2005) is not
comparable even for our Polarity-Only model used inside
BASELINE-1, because Wilson et al (2005) does not operate
directly comparable to our results; much of previ-
ous work studies only a subset of what we tackle
in this paper. However, as shown in Section 4.1,
when we train the learning models only for a sub-
set of the tasks, we can achieve a better perfor-
mance instantly by making the problem simpler.
Our work differs from most of previous work in
that we investigate how solving multiple related
tasks affects performance on sub-tasks.
The hierarchical parameter sharing technique
used in this paper has been previously used by
Zhao et al (2008) for opinion analysis. However,
Zhao et al (2008) employs this technique only to
classify sentence-level attributes (polarity and in-
tensity), without involving a much harder task of
detecting boundaries of sub-sentential entities.
6 Conclusion
We applied a hierarchical parameter sharing tech-
nique using Conditional Random Fields for fine-
grained opinion analysis. Our proposed approach
jointly extract opinion expressions from unstruc-
tured text and determine their attributes ? polar-
ity and intensity. Empirical results indicate that
the simple joint sequential tagging approach even
without exploiting the hierarchy brings a better
performance than combining two separately de-
veloped systems. In addition, we found that the
hierarchical joint sequential learning approach im-
proves the performance over the simple joint se-
quential tagging method.
Acknowledgments
This work was supported in part by National
Science Foundation Grants BCS-0904822, BCS-
0624277, IIS-0535099 and by the Department of
Homeland Security under ONR Grant N0014-07-
1-0152. We thank the reviewers and Ainur Yesse-
nalina for many helpful comments.
References
S. Abney. 1996. Partial parsing via finite-state cas-
cades. In Journal of Natural Language Engineering,
2(4).
E. Breck, Y. Choi and C. Cardie. 2007. Identifying
Expressions of Opinion in Context. In IJCAI.
on the entire corpus as unstructured input. Instead, Wilson
et al (2005) evaluate only on known words that are in their
opinion lexicon. Furthermore, Wilson et al (2005) simplifies
the problem by combining neutral opinions and no opinions
into the same class, while our system distinguishes the two.
273
L. Cai and T. Hofmann. 2004. Hierarchical docu-
ment categorization with support vector machines.
In CIKM.
Y. Choi and C. Cardie. 2008. Learning with Composi-
tional Semantics as Structural Inference for Subsen-
tential Sentiment Analysis. In EMNLP.
H. Cunningham, D. Maynard, K. Bontcheva and V.
Tablan. 2002. GATE: A Framework and Graphical
Development Environment for Robust NLP Tools
and Applications. In ACL.
J. R. Finkel, C. D. Manning and A. Y. Ng. 2006.
Solving the Problem of Cascading Errors: Approx-
imate Bayesian Inference for Linguistic Annotation
Pipelines. In EMNLP.
M. Hu and B. Liu. 2004. Mining and Summarizing
Customer Reviews. In KDD.
S. Kim and E. Hovy. 2004. Determining the sentiment
of opinions. In COLING.
S. Kim and E. Hovy. 2005. Automatic Detection of
Opinion Bearing Words and Sentences. In Com-
panion Volume to the Proceedings of the Second In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-05).
J. Lafferty, A. McCallum and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In ICML.
A. McCallum. 2002. MALLET: A Machine Learning
for Language Toolkit. http://mallet.cs.umass.edu.
G. A. Miller. 1995. WordNet: a lexical database for
English. In Communications of the ACM, 38(11).
Ana-Maria Popescu and O. Etzioni. 2005. Extracting
Product Features and Opinions from Reviews. In
HLT-EMNLP.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff and T.
Wilson. 2002. Summer Workshop on Multiple-
Perspective Question Answering: Final Report. In
NRRC.
J. Wiebe and T. Wilson and C. Cardie 2005. Annotat-
ing Expressions of Opinions and Emotions in Lan-
guage. In Language Resources and Evaluation, vol-
ume 39, issue 2-3.
T. Wilson, J. Wiebe and P. Hoffmann. 2005. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment
Analysis. In HLT-EMNLP.
T. Wilson, J. Wiebe and R. Hwa. 2006. Recognizing
strong and weak opinion clauses. In Computational
Intelligence. 22 (2): 73-99.
T.Wilson, J. Wiebe and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features
for phrase-level sentiment analysis. Computational
Linguistics 35(3).
J. Zhao, K. Liu and G. Wang. 2008. Adding Redun-
dant Features for CRFs-based Sentence Sentiment
Classification. In EMNLP.
274
Proceedings of the ACL 2010 Conference Short Papers, pages 336?341,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatically generating annotator rationales
to improve sentiment classification
Ainur Yessenalina Yejin Choi Claire Cardie
Department of Computer Science, Cornell University, Ithaca NY, 14853 USA
{ainur, ychoi, cardie}@cs.cornell.edu
Abstract
One of the central challenges in sentiment-
based text categorization is that not ev-
ery portion of a document is equally in-
formative for inferring the overall senti-
ment of the document. Previous research
has shown that enriching the sentiment la-
bels with human annotators? ?rationales?
can produce substantial improvements in
categorization performance (Zaidan et al,
2007). We explore methods to auto-
matically generate annotator rationales for
document-level sentiment classification.
Rather unexpectedly, we find the automat-
ically generated rationales just as helpful
as human rationales.
1 Introduction
One of the central challenges in sentiment-based
text categorization is that not every portion of
a given document is equally informative for in-
ferring its overall sentiment (e.g., Pang and Lee
(2004)). Zaidan et al (2007) address this prob-
lem by asking human annotators to mark (at least
some of) the relevant text spans that support each
document-level sentiment decision. The text spans
of these ?rationales? are then used to construct ad-
ditional training examples that can guide the learn-
ing algorithm toward better categorizationmodels.
But could we perhaps enjoy the performance
gains of rationale-enhanced learningmodels with-
out any additional human effort whatsoever (be-
yond the document-level sentiment label)? We hy-
pothesize that in the area of sentiment analysis,
where there has been a great deal of recent re-
search attentiongiven to various aspects of the task
(Pang and Lee, 2008), this might be possible: us-
ing existing resources for sentiment analysis, we
might be able to construct annotator rationales au-
tomatically.
In this paper, we explore a number of methods
to automatically generate rationales for document-
level sentiment classification. In particular, we in-
vestigate the use of off-the-shelf sentiment analy-
sis components and lexicons for this purpose. Our
approaches for generating annotator rationales can
be viewed as mostly unsupervised in that we do not
require manually annotated rationales for training.
Rather unexpectedly, our empirical results show
that automatically generated rationales (91.78%)
are just as good as human rationales (91.61%) for
document-level sentiment classification of movie
reviews. In addition, complementing the hu-
man annotator rationales with automatic rationales
boosts the performance even further for this do-
main, achieving 92.5% accuracy. We further eval-
uate our rationale-generation approaches on prod-
uct review data for which human rationales are not
available: here we find that even randomly gener-
ated rationales can improve the classification accu-
racy although rationales generated from sentiment
resources are not as effective as for movie reviews.
The rest of the paper is organized as follows.
We first briefly summarize the SVM-based learn-
ing approach of Zaidan et al (2007) that allows the
incorporation of rationales (Section 2). We next
introduce three methods for the automatic gener-
ation of rationales (Section 3). The experimental
results are presented in Section 4, followed by re-
lated work (Section 5) and conclusions (Section
6).
2 Contrastive Learning with SVMs
Zaidan et al (2007) first introduced the notion of
annotator rationales ? text spans highlighted by
human annotators as support or evidence for each
document-level sentiment decision. These ratio-
nales, of course, are only useful if the sentiment
categorization algorithm can be extended to ex-
ploit the rationales effectively. With this in mind,
Zaidan et al (2007) propose the following con-
336
trastive learning extension to the standard SVM
learning algorithm.
Let ~xi be movie review i, and let {~rij} be the
set of annotator rationales that support the posi-
tive or negative sentiment decision for ~xi. For each
such rationale~rij in the set, construct a contrastive
training example ~vij , by removing the text span
associated with the rationale ~rij from the original
review ~xi. Intuitively, the contrastive example ~vij
should not be as informative to the learning algo-
rithm as the original review ~xi, since one of the
supporting regions identified by the human anno-
tator has been deleted. That is, the correct learned
model should be less confident of its classifica-
tion of a contrastive example vs. the corresponding
original example, and the classification boundary
of the model should be modified accordingly.
Zaidan et al (2007) formulate exactly this intu-
ition as SVM constraints as follows:
(?i, j) : yi (~w~xi ? ~w~vij) ? ?(1 ? ?ij)
where yi ? {?1,+1} is the negative/positive sen-
timent label of document i, ~w is the weight vector,
? ? 0 controls the size of the margin between the
original examples and the contrastive examples,
and ?ij are the associated slack variables. After
some re-writing of the equations, the resulting ob-
jective function and constraints for the SVM are as
follows:
1
2 ||~w||
2 + C
?
i
?i + Ccontrast
?
ij
?ij (1)
subject to constraints:
(?i) : yi ~w ? ~xi ? 1 ? ?i, ?i ? 0
(?i, j) : yi ~w ? ~xij ? 1 ? ?ij ?ij ? 0
where ?i and ?ij are the slack variables for ~xi
(the original examples) and ~xij (~xij are named as
pseudo examples and defined as ~xij = ~xi?~vij? ), re-
spectively. Intuitively, the pseudo examples (~xij)
represent the difference between the original ex-
amples (~xi) and the contrastive examples (~vij),
weighted by a parameter ?. C and Ccontrast are
parameters to control the trade-offs between train-
ing errors and margins for the original examples ~xi
and pseudo examples ~xij respectively. As noted in
Zaidan et al (2007),Ccontrast values are generally
smaller than C for noisy rationales.
In the work described below, we similarly em-
ploy Zaidan et al?s (2007) contrastive learning
method to incorporate rationales for document-
level sentiment categorization.
3 Automatically Generating Rationales
Our goal in the current work, is to generate anno-
tator rationales automatically. For this, we rely on
the following two assumptions:
(1) Regions marked as annotator rationales are
more subjective than unmarked regions.
(2) The sentiment of each annotator rationale co-
incides with the document-level sentiment.
Note that assumption 1 was not observed in the
Zaidan et al (2007) work: annotators were asked
only to mark a few rationales, leaving other (also
subjective) rationale sections unmarked.
And at first glance, assumption (2) might seem
too obvious. But it is important to include as there
can be subjective regions with seemingly conflict-
ing sentiment in the same document (Pang et al,
2002). For instance, an author for a movie re-
view might express a positive sentiment toward
the movie, while also discussing a negative sen-
timent toward one of the fictional characters ap-
pearing in the movie. This implies that not all sub-
jective regions will be relevant for the document-
level sentiment classification ? rather only those
regions whose polarity matches that of the docu-
ment should be considered.
In order to extract regions that satisfy the above
assumptions, we first look for subjective regions
in each document, then filter out those regions that
exhibit a sentiment value (i.e., polarity) that con-
flicts with polarity of the document. Assumption
2 is important as there can be subjective regions
with seemingly conflicting sentiment in the same
document (Pang et al, 2002).
Because our ultimate goal is to reduce human
annotation effort as much as possible, we do not
employ supervised learning methods to directly
learn to identify good rationales from human-
annotated rationales. Instead, we opt for methods
that make use of only the document-level senti-
ment and off-the-shelf utilities that were trained
for slightly different sentiment classification tasks
using a corpus from a different domain and of a
different genre. Although such utilities might not
be optimal for our task, we hoped that these ba-
sic resources from the research community would
constitute an adequate source of sentiment infor-
mation for our purposes.
We next describe three methods for the auto-
matic acquisition of rationales.
337
3.1 Contextual Polarity Classification
The first approach employs OpinionFinder (Wil-
son et al, 2005a), an off-the-shelf opinion anal-
ysis utility.1 In particular, OpinionFinder identi-
fies phrases expressing positive or negative opin-
ions. Because OpinionFinder models the task as
a word-based classification problem rather than a
sequence tagging task, most of the identified opin-
ion phrases consist of a single word. In general,
such short text spans cannot fully incorporate the
contextual information relevant to the detection of
subjective language (Wilson et al, 2005a). There-
fore, we conjecture that good rationales should ex-
tend beyond short phrases.2 For simplicity, we
choose to extend OpinionFinder phrases to sen-
tence boundaries.
In addition, to be consistentwith our second op-
erating assumption, we keep only those sentences
whose polarity coincides with the document-level
polarity. In sentences where OpinionFindermarks
multiple opinion words with opposite polarities
we perform a simple voting ? if words with pos-
itive (or negative) polarity dominate, then we con-
sider the entire sentence as positive (or negative).
We ignore sentences with a tie. Each selected sen-
tence is considered as a separate rationale.
3.2 Polarity Lexicons
Unfortunately, domain shift as well as task mis-
match could be a problem with any opinion util-
ity based on supervised learning.3 Therefore, we
next consider an approach that does not rely on su-
pervised learning techniques but instead explores
the use of a manually constructed polarity lexicon.
In particular, we use the lexicon constructed for
Wilson et al (2005b), which contains about 8000
words. Each entry is assigned one of three polarity
values: positive, negative, neutral. We construct
rationales from the polarity lexicon for every in-
stance of positive and negative words in the lexi-
con that appear in the training corpus.
As in the OpinionFinder rationales, we extend
the words found by the PolarityLexicon approach
to sentence boundaries to incorporate potentially
1Available at www.cs.pitt.edu/mpqa/opinionfinderrelease/.
2This conjecture is indirectly confirmed by the fact that
human-annotated rationales are rarely a single word.
3It is worthwhile to note that OpinionFinder is trained on a
newswire corpus whose prevailing sentiment is known to be
negative (Wiebe et al, 2005). Furthermore, OpinionFinder
is trained for a task (word-level sentiment classification) that
is different from marking annotator rationales (sequence tag-
ging or text segmentation).
relevant contextual information. We retain as ra-
tionales only those sentences whose polarity co-
incides with the document-level polarity as deter-
mined via the voting scheme of Section 3.1.
3.3 Random Selection
Finally, we generate annotator rationales ran-
domly, selecting 25% of the sentences from each
document4 and treating each as a separate ratio-
nale.
3.4 Comparison of Automatic vs.
Human-annotated Rationales
Before evaluating the performance of the au-
tomatically generated rationales, we summarize
in Table 1 the differences between automatic
vs. human-generated rationales. All computa-
tions were performed on the same movie review
dataset of Pang and Lee (2004) used in Zaidan et
al. (2007). Note, that the Zaidan et al (2007) an-
notation guidelines did not insist that annotators
mark all rationales, only that some were marked
for each document. Nevertheless, we report pre-
cision, recall, and F-score based on overlap with
the human-annotated rationales of Zaidan et al
(2007), so as to demonstrate the degree to which
the proposed approaches align with human intu-
ition. Overlap measures were also employed by
Zaidan et al (2007).
As shown in Table 1, the annotator rationales
found by OpinionFinder (F-score 49.5%) and the
PolarityLexicon approach (F-score 52.6%) match
the human rationales much better than those found
by random selection (F-score 27.3%).
As expected, OpinionFinder?s positive ratio-
nales match the human rationales at a significantly
lower level (F-score 31.9%) than negative ratio-
nales (59.5%). This is due to the fact that Opinion-
Finder is trained on a dataset biased toward nega-
tive sentiment (see Section 3.1 - 3.2). In contrast,
all other approaches show a balanced performance
for positive and negative rationales vs. human ra-
tionales.
4 Experiments
For our contrastive learning experiments we use
SVM light (Joachims, 1999). We evaluate the use-
fulness of automatically generated rationales on
4We chose the value of 25% to match the percentage of
sentences per document, on average, that contain human-
annotated rationales in our dataset (24.7%).
338
% of sentences Precision Recall F-Score
Method selected ALL POS NEG ALL POS NEG ALL POS NEG
OPINIONFINDER 22.8% 54.9 56.1 54.6 45.1 22.3 65.3 49.5 31.9 59.5
POLARITYLEXICON 38.7% 45.2 42.7 48.5 63.0 71.8 55.0 52.6 53.5 51.6
RANDOM 25.0% 28.9 26.0 31.8 25.9 24.9 26.7 27.3 25.5 29.0
Table 1: Comparison of Automatic vs. Human-annotated Rationales.
five different datasets. The first is the movie re-
view data of Pang and Lee (2004), which was
manually annotated with rationales by Zaidan et
al. (2007)5; the remaining are four product re-
view datasets from Blitzer et al (2007).6 Only
the movie review dataset contains human annota-
tor rationales. We replicate the same feature set
and experimental set-up as in Zaidan et al (2007)
to facilitate comparison with their work.7
The contrastive learning method introduced in
Zaidan et al (2007) requires three parameters: (C,
?, Ccontrast). To set the parameters, we use a grid
search with step 0.1 for the range of values of each
parameter around the point (1,1,1). In total, we try
around 3000 different parameter triplets for each
type of rationales.
4.1 Experiments with the Movie Review Data
We follow Zaidan et al (2007) for the training/test
data splits. The top half of Table 2 shows the
performance of a system trained with no anno-
tator rationales vs. two variations of human an-
notator rationales. HUMANR treats each rationale
in the same way as Zaidan et al (2007). HU-
MANR@SENTENCE extends the human annotator
rationales to sentence boundaries, and then treats
each such sentence as a separate rationale. As
shown in Table 2, we get alost the same per-
formance from these two variations (91.33% and
91.61%).8 This result demonstrates that locking
rationales to sentence boundaries was a reasonable
5Available at http://www.cs.jhu.edu/?ozaidan/rationales/.
6http://www.cs.jhu.edu/?mdredze/datasets/sentiment/.
7We use binary unigram features corresponding to the un-
stemmed words or punctuation marks with count greater or
equal to 4 in the full 2000 documents, then we normalize the
examples to the unit length. When computing the pseudo ex-
amples ~xij = ~xi?~vij? we first compute (~xi ? ~vij) using the
binary representation. As a result, features (unigrams) that
appeared in both vectors will be zeroed out in the resulting
vector. We then normalize the resulting vector to a unit vec-
tor.
8The performance of HUMANR reported by Zaidan et al
(2007) is 92.2% which lies between the performance we get
(91.61%) and the oracle accuracy we get if we knew the best
parameters for the test set (92.67%).
Method Accuracy
NORATIONALES 88.56
HUMANR 91.61?
HUMANR@SENTENCE 91.33? ?
OPINIONFINDER 91.78? ?
POLARITYLEXICON 91.39? ?
RANDOM 90.00?
OPINIONFINDER+HUMANR@SENTENCE 92.50? 4
Table 2: Experimental results for the movie
review data.
? The numbers marked with ? (or ?) are statistically
significantly better than NORATIONALES according to a
paired t-test with p < 0.001 (or p < 0.01).
? The numbers marked with 4 are statistically significantly
better than HUMANR according to a paired t-test with
p < 0.01.
? The numbers marked with ? are not statistically signifi-
cantly worse than HUMANR according to a paired t-test with
p > 0.1.
choice.
Among the approaches that make use of only
automatic rationales (bottom half of Table 2), the
best is OPINIONFINDER, reaching 91.78% accu-
racy. This result is slightly better than results
exploiting human rationales (91.33-91.61%), al-
though the difference is not statistically signifi-
cant. This result demonstrates that automatically
generated rationales are just as good as human
rationales in improving document-level sentiment
classification. Similarly strong results are ob-
tained from the POLARITYLEXICON as well.
Rather unexpectedly, RANDOM also achieves
statistically significant improvement over NORA-
TIONALES (90.0% vs. 88.56%). However, notice
that the performance of RANDOM is statistically
significantly lower than those based on human ra-
tionales (91.33-91.61%).
In our experiments so far, we observed that
some of the automatic rationales are just as
good as human rationales in improving the
document-level sentiment classification. Could
we perhaps achieve an even better result if we
combine the automatic rationales with human
339
rationales? The answer is yes! The accuracy
of OPINIONFINDER+HUMANR@SENTENCE
reaches 92.50%, which is statistically signifi-
cantly better than HUMANR (91.61%). In other
words, not only can our automatically generated
rationales replace human rationales, but they can
also improve upon human rationales when they
are available.
4.2 Experiments with the Product Reviews
We next evaluate our approaches on datasets for
which human annotator rationales do not exist.
For this, we use some of the product review data
from Blitzer et al (2007): reviews for Books,
DVDs, Videos and Kitchen appliances. Each
dataset contains 1000 positive and 1000 negative
reviews. The reviews, however, are substantially
shorter than those in the movie review dataset:
the average number of sentences in each review
is 9.20/9.13/8.12/6.37 respectively vs. 30.86 for
the movie reviews. We perform 10-fold cross-
validation, where 8 folds are used for training, 1
fold for tuning parameters, and 1 fold for testing.
Table 3 shows the results. Rationale-based
methods perform statistically significantly bet-
ter than NORATIONALES for all but the Kitchen
dataset. An interesting trend in product re-
view datasets is that RANDOM rationales are just
as good as other more sophisticated rationales.
We suspect that this is because product reviews
are generally shorter and more focused than the
movie reviews, thereby any randomly selected
sentence is likely to be a good rationale. Quantita-
tively, subjective sentences in the product reviews
amount to 78% (McDonald et al, 2007), while
subjective sentences in the movie review dataset
are only about 25% (Mao and Lebanon, 2006).
4.3 Examples of Annotator Rationales
In this section, we examine an example to com-
pare the automatically generated rationales (using
OPINIONFINDER) with human annotator ratio-
nales for the movie review data. In the following
positive document snippet, automatic rationales
are underlined, while human-annotated ratio-
nales are in bold face.
...But a little niceness goes a long way these days, and
there?s no denying the entertainment value of that thing
you do! It?s just about impossible to hate. It?s an
inoffensive, enjoyable piece ofnostalgia that is sure to leave
audiences smiling and humming, if not singing, ?that thing
you do!? ?quite possibly for days...
Method Books DVDs Videos Kitchen
NORATIONALES 80.20 80.95 82.40 87.40
OPINIONFINDER 81.65? 82.35? 84.00? 88.40
POLARITYLEXICON 82.75? 82.85? 84.55? 87.90
RANDOM 82.05? 82.10? 84.15? 88.00
Table 3: Experimental results for subset of
Product Review data
? The numbers marked with ? (or ?) are statistically
significantly better than NORATIONALES according to a
paired t-test with p < 0.05 (or p < 0.08).
Notice that, although OPINIONFINDER misses
some human rationales, it avoids the inclusion of
?impossible to hate?, which contains only negative
terms and is likely to be confusing for the con-
trastive learner.
5 Related Work
In broad terms, constructing annotator rationales
automatically and using them to formulate con-
trastive examples can be viewed as learning with
prior knowledge (e.g., Schapire et al (2002), Wu
and Srihari (2004)). In our task, the prior knowl-
edge corresponds to our operating assumptions
given in Section 3. Those assumptions can be
loosely connected to recognizing and exploiting
discourse structure (e.g., Pang and Lee (2004),
Taboada et al (2009)). Our automatically gener-
ated rationales can be potentially combined with
other learning frameworks that can exploit anno-
tator rationales, such as Zaidan and Eisner (2008).
6 Conclusions
In this paper, we explore methods to automatically
generate annotator rationales for document-level
sentiment classification. Our study is motivated
by the desire to retain the performance gains of
rationale-enhanced learning models while elimi-
nating the need for additional human annotation
effort. By employing existing resources for sen-
timent analysis, we can create automatic annota-
tor rationales that are as good as human annotator
rationales in improving document-level sentiment
classification.
Acknowledgments
We thank anonymous reviewers for their comments. This
work was supported in part by National Science Founda-
tion Grants BCS-0904822, BCS-0624277, IIS-0535099 and
by the Department of Homeland Security under ONR Grant
N0014-07-1-0152.
340
References
John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Bi-
ographies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Proceedings of
the 45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 440?447, Prague, Czech Repub-
lic, June. Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large-scale support vector
machine learning practical. pages 169?184.
Yi Mao and Guy Lebanon. 2006. Sequential models for sen-
timent prediction. In Proceedings of the ICML Workshop:
Learning in Structured Output Spaces Open Problems in
Statistical Relational Learning Statistical Network Analy-
sis: Models, Issues and New Directions.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells,
and Jeff Reynar. 2007. Structured models for fine-to-
coarse sentiment analysis. In Proceedings of the 45th
Annual Meeting of the Association of Computational Lin-
guistics, pages 432?439, Prague, Czech Republic, June.
Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2004. A sentimental education:
sentiment analysis using subjectivity summarization based
on minimum cuts. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational Lin-
guistics, page 271, Morristown, NJ, USA. Association for
Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and senti-
ment analysis. Found. Trends Inf. Retr., 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up?: sentiment classification using machine
learning techniques. In EMNLP ?02: Proceedings of the
ACL-02 conference on Empirical methods in natural lan-
guage processing, pages 79?86, Morristown, NJ, USA.
Association for Computational Linguistics.
Robert E. Schapire, Marie Rochery, Mazin G. Rahim, and
Narendra Gupta. 2002. Incorporating prior knowledge
into boosting. In ICML ?02: Proceedings of the Nine-
teenth International Conference on Machine Learning,
pages 538?545, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Maite Taboada, Julian Brooke, and Manfred Stede. 2009.
Genre-based paragraph classification for sentiment anal-
ysis. In Proceedings of the SIGDIAL 2009 Conference,
pages 62?70, London, UK, September. Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 1(2):0.
Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Ja-
son Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie,
Ellen Riloff, and Siddharth Patwardhan. 2005a. Opinion-
finder: a system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages 34?
35, Morristown, NJ, USA. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b.
Recognizing contextual polarity in phrase-level sentiment
analysis. In HLT-EMNLP ?05: Proceedings of the con-
ference on Human Language Technology and Empirical
Methods in Natural Language Processing, pages 347?
354, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Xiaoyun Wu and Rohini Srihari. 2004. Incorporating
prior knowledgewith weighted margin support vector ma-
chines. In KDD ?04: Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discov-
ery and data mining, pages 326?333, New York, NY,
USA. ACM.
Omar F. Zaidan and Jason Eisner. 2008. Modeling anno-
tators: a generative approach to learning from annotator
rationales. In EMNLP ?08: Proceedings of the Confer-
ence on Empirical Methods in Natural LanguageProcess-
ing, pages 31?40, Morristown, NJ, USA. Association for
Computational Linguistics.
Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine learning
for text categorization. In NAACLHLT 2007; Proceedings
of the Main Conference, pages 260?267, April.
341
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 309?319,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Finding Deceptive Opinion Spam by Any Stretch of the Imagination
Myle Ott Yejin Choi Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{myleott,ychoi,cardie}@cs.cornell.edu
Jeffrey T. Hancock
Department of Communication
Cornell University
Ithaca, NY 14853
jth34@cornell.edu
Abstract
Consumers increasingly rate, review and re-
search products online (Jansen, 2010; Litvin
et al, 2008). Consequently, websites con-
taining consumer reviews are becoming tar-
gets of opinion spam. While recent work
has focused primarily on manually identifi-
able instances of opinion spam, in this work
we study deceptive opinion spam?fictitious
opinions that have been deliberately written to
sound authentic. Integrating work from psy-
chology and computational linguistics, we de-
velop and compare three approaches to detect-
ing deceptive opinion spam, and ultimately
develop a classifier that is nearly 90% accurate
on our gold-standard opinion spam dataset.
Based on feature analysis of our learned mod-
els, we additionally make several theoretical
contributions, including revealing a relation-
ship between deceptive opinions and imagina-
tive writing.
1 Introduction
With the ever-increasing popularity of review web-
sites that feature user-generated opinions (e.g.,
TripAdvisor1 and Yelp2), there comes an increasing
potential for monetary gain through opinion spam?
inappropriate or fraudulent reviews. Opinion spam
can range from annoying self-promotion of an un-
related website or blog to deliberate review fraud,
as in the recent case3 of a Belkin employee who
1http://tripadvisor.com
2http://yelp.com
3http://news.cnet.com/8301-1001_
3-10145399-92.html
hired people to write positive reviews for an other-
wise poorly reviewed product.4
While other kinds of spam have received consid-
erable computational attention, regrettably there has
been little work to date (see Section 2) on opinion
spam detection. Furthermore, most previous work in
the area has focused on the detection of DISRUPTIVE
OPINION SPAM?uncontroversial instances of spam
that are easily identified by a human reader, e.g., ad-
vertisements, questions, and other irrelevant or non-
opinion text (Jindal and Liu, 2008). And while the
presence of disruptive opinion spam is certainly a
nuisance, the risk it poses to the user is minimal,
since the user can always choose to ignore it.
We focus here on a potentially more insidi-
ous type of opinion spam: DECEPTIVE OPINION
SPAM?fictitious opinions that have been deliber-
ately written to sound authentic, in order to deceive
the reader. For example, one of the following two
hotel reviews is truthful and the other is deceptive
opinion spam:
1. I have stayed at many hotels traveling for both business
and pleasure and I can honestly stay that The James is
tops. The service at the hotel is first class. The rooms
are modern and very comfortable. The location is per-
fect within walking distance to all of the great sights and
restaurants. Highly recommend to both business trav-
ellers and couples.
2. My husband and I stayed at the James Chicago Hotel
for our anniversary. This place is fantastic! We knew
as soon as we arrived we made the right choice! The
rooms are BEAUTIFUL and the staff very attentive and
wonderful!! The area of the hotel is great, since I love
to shop I couldn?t ask for more!! We will definatly be
4It is also possible for opinion spam to be negative, poten-
tially in order to sully the reputation of a competitor.
309
back to Chicago and we will for sure be back to the James
Chicago.
Typically, these deceptive opinions are neither
easily ignored nor even identifiable by a human
reader;5 consequently, there are few good sources
of labeled data for this research. Indeed, in the ab-
sence of gold-standard data, related studies (see Sec-
tion 2) have been forced to utilize ad hoc procedures
for evaluation. In contrast, one contribution of the
work presented here is the creation of the first large-
scale, publicly available6 dataset for deceptive opin-
ion spam research, containing 400 truthful and 400
gold-standard deceptive reviews.
To obtain a deeper understanding of the nature of
deceptive opinion spam, we explore the relative util-
ity of three potentially complementary framings of
our problem. Specifically, we view the task as: (a)
a standard text categorization task, in which we use
n-gram?based classifiers to label opinions as either
deceptive or truthful (Joachims, 1998; Sebastiani,
2002); (b) an instance of psycholinguistic decep-
tion detection, in which we expect deceptive state-
ments to exemplify the psychological effects of ly-
ing, such as increased negative emotion and psycho-
logical distancing (Hancock et al, 2008; Newman et
al., 2003); and, (c) a problem of genre identification,
in which we view deceptive and truthful writing as
sub-genres of imaginative and informative writing,
respectively (Biber et al, 1999; Rayson et al, 2001).
We compare the performance of each approach
on our novel dataset. Particularly, we find that ma-
chine learning classifiers trained on features tradi-
tionally employed in (a) psychological studies of
deception and (b) genre identification are both out-
performed at statistically significant levels by n-
gram?based text categorization techniques. Notably,
a combined classifier with both n-gram and psy-
chological deception features achieves nearly 90%
cross-validated accuracy on this task. In contrast,
we find deceptive opinion spam detection to be well
beyond the capabilities of most human judges, who
perform roughly at-chance?a finding that is consis-
tent with decades of traditional deception detection
research (Bond and DePaulo, 2006).
5The second example review is deceptive opinion spam.
6Available by request at: http://www.cs.cornell.
edu/?myleott/op_spam
Additionally, we make several theoretical con-
tributions based on an examination of the feature
weights learned by our machine learning classifiers.
Specifically, we shed light on an ongoing debate in
the deception literature regarding the importance of
considering the context and motivation of a decep-
tion, rather than simply identifying a universal set
of deception cues. We also present findings that are
consistent with recent work highlighting the difficul-
ties that liars have encoding spatial information (Vrij
et al, 2009). Lastly, our study of deceptive opinion
spam detection as a genre identification problem re-
veals relationships between deceptive opinions and
imaginative writing, and between truthful opinions
and informative writing.
The rest of this paper is organized as follows: in
Section 2, we summarize related work; in Section 3,
we explain our methodology for gathering data and
evaluate human performance; in Section 4, we de-
scribe the features and classifiers employed by our
three automated detection approaches; in Section 5,
we present and discuss experimental results; finally,
conclusions and directions for future work are given
in Section 6.
2 Related Work
Spam has historically been studied in the contexts of
e-mail (Drucker et al, 2002), and the Web (Gyo?ngyi
et al, 2004; Ntoulas et al, 2006). Recently, re-
searchers have began to look at opinion spam as
well (Jindal and Liu, 2008; Wu et al, 2010; Yoo
and Gretzel, 2009).
Jindal and Liu (2008) find that opinion spam is
both widespread and different in nature from either
e-mail or Web spam. Using product review data,
and in the absence of gold-standard deceptive opin-
ions, they train models using features based on the
review text, reviewer, and product, to distinguish
between duplicate opinions7 (considered deceptive
spam) and non-duplicate opinions (considered truth-
ful). Wu et al (2010) propose an alternative strategy
for detecting deceptive opinion spam in the absence
7Duplicate (or near-duplicate) opinions are opinions that ap-
pear more than once in the corpus with the same (or similar)
text. While these opinions are likely to be deceptive, they are
unlikely to be representative of deceptive opinion spam in gen-
eral. Moreover, they are potentially detectable via off-the-shelf
plagiarism detection software.
310
of gold-standard data, based on the distortion of pop-
ularity rankings. Both of these heuristic evaluation
approaches are unnecessary in our work, since we
compare gold-standard deceptive and truthful opin-
ions.
Yoo and Gretzel (2009) gather 40 truthful and 42
deceptive hotel reviews and, using a standard statis-
tical test, manually compare the psychologically rel-
evant linguistic differences between them. In con-
trast, we create a much larger dataset of 800 opin-
ions that we use to develop and evaluate automated
deception classifiers.
Research has also been conducted on the re-
lated task of psycholinguistic deception detection.
Newman et al (2003), and later Mihalcea and
Strapparava (2009), ask participants to give both
their true and untrue views on personal issues
(e.g., their stance on the death penalty). Zhou et
al. (2004; 2008) consider computer-mediated decep-
tion in role-playing games designed to be played
over instant messaging and e-mail. However, while
these studies compare n-gram?based deception clas-
sifiers to a random guess baseline of 50%, we addi-
tionally evaluate and compare two other computa-
tional approaches (described in Section 4), as well
as the performance of human judges (described in
Section 3.3).
Lastly, automatic approaches to determining re-
view quality have been studied?directly (Weimer
et al, 2007), and in the contexts of helpful-
ness (Danescu-Niculescu-Mizil et al, 2009; Kim et
al., 2006; O?Mahony and Smyth, 2009) and credibil-
ity (Weerkamp and De Rijke, 2008). Unfortunately,
most measures of quality employed in those works
are based exclusively on human judgments, which
we find in Section 3 to be poorly calibrated to de-
tecting deceptive opinion spam.
3 Dataset Construction and Human
Performance
While truthful opinions are ubiquitous online, de-
ceptive opinions are difficult to obtain without re-
sorting to heuristic methods (Jindal and Liu, 2008;
Wu et al, 2010). In this section, we report our ef-
forts to gather (and validate with human judgments)
the first publicly available opinion spam dataset with
gold-standard deceptive opinions.
Following the work of Yoo and Gretzel (2009), we
compare truthful and deceptive positive reviews for
hotels found on TripAdvisor. Specifically, we mine
all 5-star truthful reviews from the 20 most popular
hotels on TripAdvisor8 in the Chicago area.9 De-
ceptive opinions are gathered for those same 20 ho-
tels using Amazon Mechanical Turk10 (AMT). Be-
low, we provide details of the collection methodolo-
gies for deceptive (Section 3.1) and truthful opinions
(Section 3.2). Ultimately, we collect 20 truthful and
20 deceptive opinions for each of the 20 chosen ho-
tels (800 opinions total).
3.1 Deceptive opinions via Mechanical Turk
Crowdsourcing services such as AMT have made
large-scale data annotation and collection efforts fi-
nancially affordable by granting anyone with ba-
sic programming skills access to a marketplace of
anonymous online workers (known as Turkers) will-
ing to complete small tasks.
To solicit gold-standard deceptive opinion spam
using AMT, we create a pool of 400 Human-
Intelligence Tasks (HITs) and allocate them evenly
across our 20 chosen hotels. To ensure that opin-
ions are written by unique authors, we allow only a
single submission per Turker. We also restrict our
task to Turkers who are located in the United States,
and who maintain an approval rating of at least 90%.
Turkers are allowed a maximum of 30 minutes to
work on the HIT, and are paid one US dollar for an
accepted submission.
Each HIT presents the Turker with the name and
website of a hotel. The HIT instructions ask the
Turker to assume that they work for the hotel?s mar-
keting department, and to pretend that their boss
wants them to write a fake review (as if they were
a customer) to be posted on a travel review website;
additionally, the review needs to sound realistic and
portray the hotel in a positive light. A disclaimer
8TripAdvisor utilizes a proprietary ranking system to assess
hotel popularity. We chose the 20 hotels with the greatest num-
ber of reviews, irrespective of the TripAdvisor ranking.
9It has been hypothesized that popular offerings are less
likely to become targets of deceptive opinion spam, since the
relative impact of the spam in such cases is small (Jindal and
Liu, 2008; Lim et al, 2010). By considering only the most
popular hotels, we hope to minimize the risk of mining opinion
spam and labeling it as truthful.
10http://mturk.com
311
Time spent t (minutes)
All submissions
count: 400
tmin: 0.08, tmax: 29.78
t?: 8.06, s: 6.32
Length ` (words)
All submissions
`min: 25, `max: 425
?`: 115.75, s: 61.30
Time spent t < 1
count: 47
`min: 39, `max: 407
?`: 113.94, s: 66.24
Time spent t ? 1
count: 353
`min: 25, `max: 425
?`: 115.99, s: 60.71
Table 1: Descriptive statistics for 400 deceptive opinion
spam submissions gathered using AMT. s corresponds to
the sample standard deviation.
indicates that any submission found to be of insuffi-
cient quality (e.g., written for the wrong hotel, unin-
telligible, unreasonably short,11 plagiarized,12 etc.)
will be rejected.
It took approximately 14 days to collect 400 sat-
isfactory deceptive opinions. Descriptive statistics
appear in Table 1. Submissions vary quite dramati-
cally both in length, and time spent on the task. Par-
ticularly, nearly 12% of the submissions were com-
pleted in under one minute. Surprisingly, an inde-
pendent two-tailed t-test between the mean length of
these submissions (?`t<1) and the other submissions
(?`t?1) reveals no significant difference (p = 0.83).
We suspect that these ?quick? users may have started
working prior to having formally accepted the HIT,
presumably to circumvent the imposed time limit.
Indeed, the quickest submission took just 5 seconds
and contained 114 words.
3.2 Truthful opinions from TripAdvisor
For truthful opinions, we mine all 6,977 reviews
from the 20 most popular Chicago hotels on
TripAdvisor. From these we eliminate:
? 3,130 non-5-star reviews;
? 41 non-English reviews;13
? 75 reviews with fewer than 150 characters
since, by construction, deceptive opinions are
11A submission is considered unreasonably short if it con-
tains fewer than 150 characters.
12Submissions are individually checked for plagiarism at
http://plagiarisma.net.
13Language is determined using http://tagthe.net.
at least 150 characters long (see footnote 11 in
Section 3.1);
? 1,607 reviews written by first-time authors?
new users who have not previously posted an
opinion on TripAdvisor?since these opinions
are more likely to contain opinion spam, which
would reduce the integrity of our truthful re-
view data (Wu et al, 2010).
Finally, we balance the number of truthful and
deceptive opinions by selecting 400 of the remain-
ing 2,124 truthful reviews, such that the document
lengths of the selected truthful reviews are similarly
distributed to those of the deceptive reviews. Work
by Serrano et al (2009) suggests that a log-normal
distribution is appropriate for modeling document
lengths. Thus, for each of the 20 chosen hotels, we
select 20 truthful reviews from a log-normal (left-
truncated at 150 characters) distribution fit to the
lengths of the deceptive reviews.14 Combined with
the 400 deceptive reviews gathered in Section 3.1
this yields our final dataset of 800 reviews.
3.3 Human performance
Assessing human deception detection performance
is important for several reasons. First, there are few
other baselines for our classification task; indeed, re-
lated studies (Jindal and Liu, 2008; Mihalcea and
Strapparava, 2009) have only considered a random
guess baseline. Second, assessing human perfor-
mance is necessary to validate the deceptive opin-
ions gathered in Section 3.1. If human performance
is low, then our deceptive opinions are convincing,
and therefore, deserving of further attention.
Our initial approach to assessing human perfor-
mance on this task was with Mechanical Turk. Un-
fortunately, we found that some Turkers selected
among the choices seemingly at random, presum-
ably to maximize their hourly earnings by obviating
the need to read the review. While a similar effect
has been observed previously (Akkaya et al, 2010),
there remains no universal solution.
Instead, we solicit the help of three volunteer un-
dergraduate university students to make judgments
on a subset of our data. This balanced subset, cor-
responding to the first fold of our cross-validation
14We use the R package GAMLSS (Rigby and Stasinopoulos,
2005) to fit the left-truncated log-normal distribution.
312
TRUTHFUL DECEPTIVE
Accuracy P R F P R F
HUMAN
JUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7
JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3
JUDGE 3 53.1% 52.3 70.0 59.9 54.7 36.3 43.6
META
MAJORITY 58.1% 54.8 92.5 68.8 76.0 23.8 36.2
SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9
Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the
first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
experiments described in Section 5, contains all 40
reviews from each of four randomly chosen hotels.
Unlike the Turkers, our student volunteers are not
offered a monetary reward. Consequently, we con-
sider their judgements to be more honest than those
obtained via AMT.
Additionally, to test the extent to which the in-
dividual human judges are biased, we evaluate the
performance of two virtual meta-judges. Specifi-
cally, the MAJORITY meta-judge predicts ?decep-
tive? when at least two out of three human judges
believe the review to be deceptive, and the SKEP-
TIC meta-judge predicts ?deceptive? when any hu-
man judge believes the review to be deceptive.
Human and meta-judge performance is given in
Table 2. It is clear from the results that human
judges are not particularly effective at this task. In-
deed, a two-tailed binomial test fails to reject the
null hypothesis that JUDGE 2 and JUDGE 3 per-
form at-chance (p = 0.003, 0.10, 0.48 for the three
judges, respectively). Furthermore, all three judges
suffer from truth-bias (Vrij, 2008), a common find-
ing in deception detection research in which hu-
man judges are more likely to classify an opinion
as truthful than deceptive. In fact, JUDGE 2 clas-
sified fewer than 12% of the opinions as decep-
tive! Interestingly, this bias is effectively smoothed
by the SKEPTIC meta-judge, which produces nearly
perfectly class-balanced predictions. A subsequent
reevaluation of human performance on this task sug-
gests that the truth-bias can be reduced if judges
are given the class-proportions in advance, although
such prior knowledge is unrealistic; and ultimately,
performance remains similar to that of Table 2.
Inter-annotator agreement among the three
judges, computed using Fleiss? kappa, is 0.11.
While there is no precise rule for interpreting
kappa scores, Landis and Koch (1977) suggest
that scores in the range (0.00, 0.20] correspond
to ?slight agreement? between annotators. The
largest pairwise Cohen?s kappa is 0.12, between
JUDGE 2 and JUDGE 3?a value far below generally
accepted pairwise agreement levels. We suspect
that agreement among our human judges is so
low precisely because humans are poor judges of
deception (Vrij, 2008), and therefore they perform
nearly at-chance respective to one another.
4 Automated Approaches to Deceptive
Opinion Spam Detection
We consider three automated approaches to detect-
ing deceptive opinion spam, each of which utilizes
classifiers (described in Section 4.4) trained on the
dataset of Section 3. The features employed by each
strategy are outlined here.
4.1 Genre identification
Work in computational linguistics has shown that
the frequency distribution of part-of-speech (POS)
tags in a text is often dependent on the genre of the
text (Biber et al, 1999; Rayson et al, 2001). In our
genre identification approach to deceptive opinion
spam detection, we test if such a relationship exists
for truthful and deceptive reviews by constructing,
for each review, features based on the frequencies of
each POS tag.15 These features are also intended to
provide a good baseline with which to compare our
other automated approaches.
4.2 Psycholinguistic deception detection
The Linguistic Inquiry and Word Count (LIWC)
software (Pennebaker et al, 2007) is a popular au-
tomated text analysis tool used widely in the so-
cial sciences. It has been used to detect personality
15We use the Stanford Parser (Klein and Manning, 2003) to
obtain the relative POS frequencies.
313
traits (Mairesse et al, 2007), to study tutoring dy-
namics (Cade et al, 2010), and, most relevantly, to
analyze deception (Hancock et al, 2008; Mihalcea
and Strapparava, 2009; Vrij et al, 2007).
While LIWC does not include a text classifier, we
can create one with features derived from the LIWC
output. In particular, LIWC counts and groups
the number of instances of nearly 4,500 keywords
into 80 psychologically meaningful dimensions. We
construct one feature for each of the 80 LIWC di-
mensions, which can be summarized broadly under
the following four categories:
1. Linguistic processes: Functional aspects of text
(e.g., the average number of words per sen-
tence, the rate of misspelling, swearing, etc.)
2. Psychological processes: Includes all social,
emotional, cognitive, perceptual and biological
processes, as well as anything related to time or
space.
3. Personal concerns: Any references to work,
leisure, money, religion, etc.
4. Spoken categories: Primarily filler and agree-
ment words.
While other features have been considered in past
deception detection work, notably those of Zhou et
al. (2004), early experiments found LIWC features
to perform best. Indeed, the LIWC2007 software
used in our experiments subsumes most of the fea-
tures introduced in other work. Thus, we focus our
psycholinguistic approach to deception detection on
LIWC-based features.
4.3 Text categorization
In contrast to the other strategies just discussed,
our text categorization approach to deception de-
tection allows us to model both content and con-
text with n-gram features. Specifically, we consider
the following three n-gram feature sets, with the
corresponding features lowercased and unstemmed:
UNIGRAMS, BIGRAMS+, TRIGRAMS+, where the
superscript + indicates that the feature set subsumes
the preceding feature set.
4.4 Classifiers
Features from the three approaches just introduced
are used to train Na??ve Bayes and Support Vector
Machine classifiers, both of which have performed
well in related work (Jindal and Liu, 2008; Mihalcea
and Strapparava, 2009; Zhou et al, 2008).
For a document ~x, with label y, the Na??ve Bayes
(NB) classifier gives us the following decision rule:
y? = arg max
c
Pr(y = c) ? Pr(~x | y = c) (1)
When the class prior is uniform, for example
when the classes are balanced (as in our case), (1)
can be simplified to the maximum likelihood classi-
fier (Peng and Schuurmans, 2003):
y? = arg max
c
Pr(~x | y = c) (2)
Under (2), both the NB classifier used by Mihal-
cea and Strapparava (2009) and the language model
classifier used by Zhou et al (2008) are equivalent.
Thus, following Zhou et al (2008), we use the SRI
Language Modeling Toolkit (Stolcke, 2002) to esti-
mate individual language models, Pr(~x | y = c),
for truthful and deceptive opinions. We consider
all three n-gram feature sets, namely UNIGRAMS,
BIGRAMS+, and TRIGRAMS+, with corresponding
language models smoothed using the interpolated
Kneser-Ney method (Chen and Goodman, 1996).
We also train Support Vector Machine (SVM)
classifiers, which find a high-dimensional separating
hyperplane between two groups of data. To simplify
feature analysis in Section 5, we restrict our evalu-
ation to linear SVMs, which learn a weight vector
~w and bias term b, such that a document ~x can be
classified by:
y? = sign(~w ? ~x + b) (3)
We use SVMlight (Joachims, 1999) to train our
linear SVM models on all three approaches and
feature sets described above, namely POS, LIWC,
UNIGRAMS, BIGRAMS+, and TRIGRAMS+. We also
evaluate every combination of these features, but
for brevity include only LIWC+BIGRAMS+, which
performs best. Following standard practice, doc-
ument vectors are normalized to unit-length. For
LIWC+BIGRAMS+, we unit-length normalize LIWC
and BIGRAMS+ features individually before com-
bining them.
314
TRUTHFUL DECEPTIVE
Approach Features Accuracy P R F P R F
GENRE IDENTIFICATION POSSVM 73.0% 75.3 68.5 71.7 71.1 77.5 74.2
PSYCHOLINGUISTIC
LIWCSVM 76.8% 77.2 76.0 76.6 76.4 77.5 76.9
DECEPTION DETECTION
TEXT CATEGORIZATION
UNIGRAMSSVM 88.4% 89.9 86.5 88.2 87.0 90.3 88.6
BIGRAMS+SVM 89.6% 90.1 89.0 89.6 89.1 90.3 89.7
LIWC+BIGRAMS+SVM 89.8% 89.8 89.8 89.8 89.8 89.8 89.8
TRIGRAMS+SVM 89.0% 89.0 89.0 89.0 89.0 89.0 89.0
UNIGRAMSNB 88.4% 92.5 83.5 87.8 85.0 93.3 88.9
BIGRAMS+NB 88.9% 89.8 87.8 88.7 88.0 90.0 89.0
TRIGRAMS+NB 87.6% 87.7 87.5 87.6 87.5 87.8 87.6
HUMAN / META
JUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7
JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3
SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9
Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.
Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false
positive and false negative rates, as suggested by Forman and Scholz (2009). Human performance is repeated here for
JUDGE 1, JUDGE 2 and the SKEPTIC meta-judge, although they cannot be directly compared since the 160-opinion
subset on which they are assessed only corresponds to the first cross-validation fold.
5 Results and Discussion
The deception detection strategies described in Sec-
tion 4 are evaluated using a 5-fold nested cross-
validation (CV) procedure (Quadrianto et al, 2009),
where model parameters are selected for each test
fold based on standard CV experiments on the train-
ing folds. Folds are selected so that each contains all
reviews from four hotels; thus, learned models are
always evaluated on reviews from unseen hotels.
Results appear in Table 3. We observe that auto-
mated classifiers outperform human judges for every
metric, except truthful recall where JUDGE 2 per-
forms best.16 However, this is expected given that
untrained humans often focus on unreliable cues to
deception (Vrij, 2008). For example, one study ex-
amining deception in online dating found that hu-
mans perform at-chance detecting deceptive pro-
files because they rely on text-based cues that are
unrelated to deception, such as second-person pro-
nouns (Toma and Hancock, In Press).
Among the automated classifiers, baseline per-
formance is given by the simple genre identifica-
tion approach (POSSVM) proposed in Section 4.1.
Surprisingly, we find that even this simple auto-
16As mentioned in Section 3.3, JUDGE 2 classified fewer than
12% of opinions as deceptive. While achieving 95% truthful re-
call, this judge?s corresponding precision was not significantly
better than chance (two-tailed binomial p = 0.4).
mated classifier outperforms most human judges
(one-tailed sign test p = 0.06, 0.01, 0.001 for the
three judges, respectively, on the first fold). This
result is best explained by theories of reality mon-
itoring (Johnson and Raye, 1981), which suggest
that truthful and deceptive opinions might be clas-
sified into informative and imaginative genres, re-
spectively. Work by Rayson et al (2001) has found
strong distributional differences between informa-
tive and imaginative writing, namely that the former
typically consists of more nouns, adjectives, prepo-
sitions, determiners, and coordinating conjunctions,
while the latter consists of more verbs,17 adverbs,18
pronouns, and pre-determiners. Indeed, we find that
the weights learned by POSSVM (found in Table 4)
are largely in agreement with these findings, no-
tably except for adjective and adverb superlatives,
the latter of which was found to be an exception by
Rayson et al (2001). However, that deceptive opin-
ions contain more superlatives is not unexpected,
since deceptive writing (but not necessarily imagi-
native writing in general) often contains exaggerated
language (Buller and Burgoon, 1996; Hancock et al,
2008).
Both remaining automated approaches to detect-
ing deceptive opinion spam outperform the simple
17Past participle verbs were an exception.
18Superlative adverbs were an exception.
315
TRUTHFUL/INFORMATIVE DECEPTIVE/IMAGINATIVE
Category Variant Weight Category Variant Weight
NOUNS
Singular 0.008
VERBS
Base -0.057
Plural 0.002 Past tense 0.041
Proper, singular -0.041 Present participle -0.089
Proper, plural 0.091 Singular, present -0.031
ADJECTIVES
General 0.002 Third person
0.026
Comparative 0.058 singular, present
Superlative -0.164 Modal -0.063
PREPOSITIONS General 0.064
ADVERBS
General 0.001
DETERMINERS General 0.009 Comparative -0.035
COORD. CONJ. General 0.094
PRONOUNS
Personal -0.098
VERBS Past participle 0.053 Possessive -0.303
ADVERBS Superlative -0.094 PRE-DETERMINERS General 0.017
Table 4: Average feature weights learned by POSSVM. Based on work by Rayson et al (2001), we expect weights on
the left to be positive (predictive of truthful opinions), and weights on the right to be negative (predictive of deceptive
opinions). Boldface entries are at odds with these expectations. We report average feature weights of unit-normalized
weight vectors, rather than raw weights vectors, to account for potential differences in magnitude between the folds.
genre identification baseline just discussed. Specifi-
cally, the psycholinguistic approach (LIWCSVM) pro-
posed in Section 4.2 performs 3.8% more accurately
(one-tailed sign test p = 0.02), and the standard text
categorization approach proposed in Section 4.3 per-
forms between 14.6% and 16.6% more accurately.
However, best performance overall is achieved by
combining features from these two approaches. Par-
ticularly, the combined model LIWC+BIGRAMS+SVM
is 89.8% accurate at detecting deceptive opinion
spam.19
Surprisingly, models trained only on
UNIGRAMS?the simplest n-gram feature set?
outperform all non?text-categorization approaches,
and models trained on BIGRAMS+ perform even
better (one-tailed sign test p = 0.07). This suggests
that a universal set of keyword-based deception
cues (e.g., LIWC) is not the best approach to de-
tecting deception, and a context-sensitive approach
(e.g., BIGRAMS+) might be necessary to achieve
state-of-the-art deception detection performance.
To better understand the models learned by these
automated approaches, we report in Table 5 the top
15 highest weighted features for each class (truthful
and deceptive) as learned by LIWC+BIGRAMS+SVM
and LIWCSVM. In agreement with theories of reality
monitoring (Johnson and Raye, 1981), we observe
that truthful opinions tend to include more sensorial
and concrete language than deceptive opinions; in
19The result is not significantly better than BIGRAMS+SVM.
LIWC+BIGRAMS+SVM LIWCSVM
TRUTHFUL DECEPTIVE TRUTHFUL DECEPTIVE
- chicago hear i
... my number family
on hotel allpunct perspron
location , and negemo see
) luxury dash pronoun
allpunctLIWC experience exclusive leisure
floor hilton we exclampunct
( business sexual sixletters
the hotel vacation period posemo
bathroom i otherpunct comma
small spa space cause
helpful looking human auxverb
$ while past future
hotel . husband inhibition perceptual
other my husband assent feel
Table 5: Top 15 highest weighted truthful and deceptive
features learned by LIWC+BIGRAMS+SVM and LIWCSVM.
Ambiguous features are subscripted to indicate the source
of the feature. LIWC features correspond to groups
of keywords as explained in Section 4.2; more details
about LIWC and the LIWC categories are available at
http://liwc.net.
particular, truthful opinions are more specific about
spatial configurations (e.g., small, bathroom, on, lo-
cation). This finding is also supported by recent
work by Vrij et al (2009) suggesting that liars have
considerable difficultly encoding spatial information
into their lies. Accordingly, we observe an increased
focus in deceptive opinions on aspects external to
the hotel being reviewed (e.g., husband, business,
316
vacation).
We also acknowledge several findings that, on the
surface, are in contrast to previous psycholinguistic
studies of deception (Hancock et al, 2008; Newman
et al, 2003). For instance, while deception is often
associated with negative emotion terms, our decep-
tive reviews have more positive and fewer negative
emotion terms. This pattern makes sense when one
considers the goal of our deceivers, namely to create
a positive review (Buller and Burgoon, 1996).
Deception has also previously been associated
with decreased usage of first person singular, an ef-
fect attributed to psychological distancing (Newman
et al, 2003). In contrast, we find increased first
person singular to be among the largest indicators
of deception, which we speculate is due to our de-
ceivers attempting to enhance the credibility of their
reviews by emphasizing their own presence in the
review. Additional work is required, but these find-
ings further suggest the importance of moving be-
yond a universal set of deceptive language features
(e.g., LIWC) by considering both the contextual (e.g.,
BIGRAMS+) and motivational parameters underly-
ing a deception as well.
6 Conclusion and Future Work
In this work we have developed the first large-scale
dataset containing gold-standard deceptive opinion
spam. With it, we have shown that the detection
of deceptive opinion spam is well beyond the ca-
pabilities of human judges, most of whom perform
roughly at-chance. Accordingly, we have introduced
three automated approaches to deceptive opinion
spam detection, based on insights coming from re-
search in computational linguistics and psychology.
We find that while standard n-gram?based text cate-
gorization is the best individual detection approach,
a combination approach using psycholinguistically-
motivated features and n-gram features can perform
slightly better.
Finally, we have made several theoretical con-
tributions. Specifically, our findings suggest the
importance of considering both the context (e.g.,
BIGRAMS+) and motivations underlying a decep-
tion, rather than strictly adhering to a universal set
of deception cues (e.g., LIWC). We have also pre-
sented results based on the feature weights learned
by our classifiers that illustrate the difficulties faced
by liars in encoding spatial information. Lastly, we
have discovered a plausible relationship between de-
ceptive opinion spam and imaginative writing, based
on POS distributional similarities.
Possible directions for future work include an ex-
tended evaluation of the methods proposed in this
work to both negative opinions, as well as opinions
coming from other domains. Many additional ap-
proaches to detecting deceptive opinion spam are
also possible, and a focus on approaches with high
deceptive precision might be useful for production
environments.
Acknowledgments
This work was supported in part by National
Science Foundation Grants BCS-0624277, BCS-
0904822, HSD-0624267, IIS-0968450, and NSCC-
0904822, as well as a gift from Google, and the
Jack Kent Cooke Foundation. We also thank, al-
phabetically, Rachel Boochever, Cristian Danescu-
Niculescu-Mizil, Alicia Granstein, Ulrike Gretzel,
Danielle Kirshenblat, Lillian Lee, Bin Lu, Jack
Newton, Melissa Sackler, Mark Thomas, and Angie
Yoo, as well as members of the Cornell NLP sem-
inar group and the ACL reviewers for their insight-
ful comments, suggestions and advice on various as-
pects of this work.
References
C. Akkaya, A. Conrad, J. Wiebe, and R. Mihalcea. 2010.
Amazon mechanical turk for subjectivity word sense
disambiguation. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazons Mechanical Turk, Los Angeles,
pages 195?203.
D. Biber, S. Johansson, G. Leech, S. Conrad, E. Finegan,
and R. Quirk. 1999. Longman grammar of spoken and
written English, volume 2. MIT Press.
C.F. Bond and B.M. DePaulo. 2006. Accuracy of de-
ception judgments. Personality and Social Psychology
Review, 10(3):214.
D.B. Buller and J.K. Burgoon. 1996. Interpersonal
deception theory. Communication Theory, 6(3):203?
242.
W.L. Cade, B.A. Lehman, and A. Olney. 2010. An ex-
ploration of off topic conversation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
317
the North American Chapter of the Association for
Computational Linguistics, pages 669?672. Associa-
tion for Computational Linguistics.
S.F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Pro-
ceedings of the 34th annual meeting on Association
for Computational Linguistics, pages 310?318. Asso-
ciation for Computational Linguistics.
C. Danescu-Niculescu-Mizil, G. Kossinets, J. Kleinberg,
and L. Lee. 2009. How opinions are received by on-
line communities: a case study on amazon.com help-
fulness votes. In Proceedings of the 18th international
conference on World wide web, pages 141?150. ACM.
H. Drucker, D. Wu, and V.N. Vapnik. 2002. Support
vector machines for spam categorization. Neural Net-
works, IEEE Transactions on, 10(5):1048?1054.
G. Forman and M. Scholz. 2009. Apples-to-Apples in
Cross-Validation Studies: Pitfalls in Classifier Perfor-
mance Measurement. ACM SIGKDD Explorations,
12(1):49?57.
Z. Gyo?ngyi, H. Garcia-Molina, and J. Pedersen. 2004.
Combating web spam with trustrank. In Proceedings
of the Thirtieth international conference on Very large
data bases-Volume 30, pages 576?587. VLDB Endow-
ment.
J.T. Hancock, L.E. Curry, S. Goorha, and M. Woodworth.
2008. On lying and being lied to: A linguistic anal-
ysis of deception in computer-mediated communica-
tion. Discourse Processes, 45(1):1?23.
J. Jansen. 2010. Online product research. Pew Internet
& American Life Project Report.
N. Jindal and B. Liu. 2008. Opinion spam and analysis.
In Proceedings of the international conference on Web
search and web data mining, pages 219?230. ACM.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
Machine Learning: ECML-98, pages 137?142.
T. Joachims. 1999. Making large-scale support vec-
tor machine learning practical. In Advances in kernel
methods, page 184. MIT Press.
M.K. Johnson and C.L. Raye. 1981. Reality monitoring.
Psychological Review, 88(1):67?85.
S.M. Kim, P. Pantel, T. Chklovski, and M. Pennacchiotti.
2006. Automatically assessing review helpfulness.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 423?
430. Association for Computational Linguistics.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159.
E.P. Lim, V.A. Nguyen, N. Jindal, B. Liu, and H.W.
Lauw. 2010. Detecting product review spammers us-
ing rating behaviors. In Proceedings of the 19th ACM
international conference on Information and knowl-
edge management, pages 939?948. ACM.
S.W. Litvin, R.E. Goldsmith, and B. Pan. 2008. Elec-
tronic word-of-mouth in hospitality and tourism man-
agement. Tourism management, 29(3):458?468.
F. Mairesse, M.A. Walker, M.R. Mehl, and R.K. Moore.
2007. Using linguistic cues for the automatic recogni-
tion of personality in conversation and text. Journal of
Artificial Intelligence Research, 30(1):457?500.
R. Mihalcea and C. Strapparava. 2009. The lie detector:
Explorations in the automatic recognition of deceptive
language. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 309?312. Association
for Computational Linguistics.
M.L. Newman, J.W. Pennebaker, D.S. Berry, and J.M.
Richards. 2003. Lying words: Predicting deception
from linguistic styles. Personality and Social Psychol-
ogy Bulletin, 29(5):665.
A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly.
2006. Detecting spam web pages through content
analysis. In Proceedings of the 15th international con-
ference on World Wide Web, pages 83?92. ACM.
M.P. O?Mahony and B. Smyth. 2009. Learning to rec-
ommend helpful hotel reviews. In Proceedings of
the third ACM conference on Recommender systems,
pages 305?308. ACM.
F. Peng and D. Schuurmans. 2003. Combining naive
Bayes and n-gram language models for text classifica-
tion. Advances in Information Retrieval, pages 547?
547.
J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gonzales,
and R.J. Booth. 2007. The development and psycho-
metric properties of LIWC2007. Austin, TX, LIWC.
Net.
N. Quadrianto, A.J. Smola, T.S. Caetano, and Q.V.
Le. 2009. Estimating labels from label proportions.
The Journal of Machine Learning Research, 10:2349?
2374.
P. Rayson, A. Wilson, and G. Leech. 2001. Grammatical
word class variation within the British National Cor-
pus sampler. Language and Computers, 36(1):295?
306.
R.A. Rigby and D.M. Stasinopoulos. 2005. Generalized
additive models for location, scale and shape. Jour-
nal of the Royal Statistical Society: Series C (Applied
Statistics), 54(3):507?554.
318
F. Sebastiani. 2002. Machine learning in automated
text categorization. ACM computing surveys (CSUR),
34(1):1?47.
M.A?. Serrano, A. Flammini, and F. Menczer. 2009.
Modeling statistical properties of written text. PloS
one, 4(4):5372.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In Seventh International Conference on
Spoken Language Processing, volume 3, pages 901?
904. Citeseer.
C. Toma and J.T. Hancock. In Press. What Lies Beneath:
The Linguistic Traces of Deception in Online Dating
Profiles. Journal of Communication.
A. Vrij, S. Mann, S. Kristen, and R.P. Fisher. 2007. Cues
to deception and ability to detect lies as a function
of police interview styles. Law and human behavior,
31(5):499?518.
A. Vrij, S. Leal, P.A. Granhag, S. Mann, R.P. Fisher,
J. Hillman, and K. Sperry. 2009. Outsmarting the
liars: The benefit of asking unanticipated questions.
Law and human behavior, 33(2):159?166.
A. Vrij. 2008. Detecting lies and deceit: Pitfalls and
opportunities. Wiley-Interscience.
W. Weerkamp and M. De Rijke. 2008. Credibility im-
proves topical blog post retrieval. ACL-08: HLT,
pages 923?931.
M. Weimer, I. Gurevych, and M. Mu?hlha?user. 2007. Au-
tomatically assessing the post quality in online discus-
sions on software. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 125?128. Association
for Computational Linguistics.
G. Wu, D. Greene, B. Smyth, and P. Cunningham. 2010.
Distortion as a validation criterion in the identification
of suspicious reviews. Technical report, UCD-CSI-
2010-04, University College Dublin.
K.H. Yoo and U. Gretzel. 2009. Comparison of De-
ceptive and Truthful Travel Reviews. Information and
Communication Technologies in Tourism 2009, pages
37?47.
L. Zhou, J.K. Burgoon, D.P. Twitchell, T. Qin, and J.F.
Nunamaker Jr. 2004. A comparison of classifica-
tion methods for predicting deception in computer-
mediated communication. Journal of Management In-
formation Systems, 20(4):139?166.
L. Zhou, Y. Shi, and D. Zhang. 2008. A Statistical Lan-
guage Modeling Approach to Online Deception De-
tection. IEEE Transactions on Knowledge and Data
Engineering, 20(8):1077?1081.
319
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 83?88,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language of Vandalism:
Improving Wikipedia Vandalism Detection via Stylometric Analysis
Manoj Harpalani, Michael Hart, Sandesh Singh, Rob Johnson, and Yejin Choi
Department of Computer Science
Stony Brook University
NY 11794, USA
{mharpalani, mhart, sssingh, rob, ychoi}@cs.stonybrook.edu
Abstract
Community-based knowledge forums, such as
Wikipedia, are susceptible to vandalism, i.e.,
ill-intentioned contributions that are detrimen-
tal to the quality of collective intelligence.
Most previous work to date relies on shallow
lexico-syntactic patterns and metadata to au-
tomatically detect vandalism in Wikipedia. In
this paper, we explore more linguistically mo-
tivated approaches to vandalism detection. In
particular, we hypothesize that textual vandal-
ism constitutes a unique genre where a group
of people share a similar linguistic behav-
ior. Experimental results suggest that (1) sta-
tistical models give evidence to unique lan-
guage styles in vandalism, and that (2) deep
syntactic patterns based on probabilistic con-
text free grammars (PCFG) discriminate van-
dalism more effectively than shallow lexico-
syntactic patterns based on n-grams.
1 Introduction
Wikipedia, the ?free encyclopedia? (Wikipedia,
2011), ranks among the top 200 most visited web-
sites worldwide (Alexa, 2011). This editable ency-
clopedia has amassed over 15 million articles across
hundreds of languages. The English language en-
cyclopedia alone has over 3.5 million articles and
receives over 1.25 million edits (and sometimes up-
wards of 3 million) daily (Wikipedia, 2010). But
allowing anonymous edits is a double-edged sword;
nearly 7% (Potthast, 2010) of edits are vandalism,
i.e. revisions to articles that undermine the quality
and veracity of the content. As Wikipedia contin-
ues to grow, it will become increasingly infeasible
for Wikipedia users and administrators to manually
police articles. This pressing issue has spawned re-
cent research activities to understand and counteract
vandalism (e.g., Geiger and Ribes (2010)). Much of
previous work relies on hand-picked rules such as
lexical cues (e.g., vulgar words) and metadata (e.g.,
anonymity, edit frequency) to automatically detect
vandalism in Wikipedia (e.g., Potthast et al (2008),
West et al (2010)). Although some recent work
has started exploring the use of natural language
processing, most work to date is based on shallow
lexico-syntactic patterns (e.g., Wang and McKeown
(2010), Chin et al (2010), Adler et al (2011)).
We explore more linguistically motivated ap-
proaches to detect vandalism in this paper. Our
hypothesis is that textual vandalism constitutes a
unique genre where a group of people share simi-
lar linguistic behavior. Some obvious hallmarks of
this style include usage of obscenities, misspellings,
and slang usage, but we aim to automatically un-
cover stylistic cues to effectively discriminate be-
tween vandalizing and normal text. Experimental re-
sults suggest that (1) statistical models give evidence
to unique language styles in vandalism, and that (2)
deep syntactic patterns based on probabilistic con-
text free grammar (PCFG) discriminate vandalism
more effectively than shallow lexico-syntactic pat-
terns based on n-grams.
2 Stylometric Features
Stylometric features attempt to recognize patterns
of style in text. These techniques have been tra-
ditionally applied to attribute authorship (Argamon
et al (2009), Stamatatos (2009)), opinion mining
83
(Panicheva et al, 2010), and forensic linguistics
(Turell, 2010). For our purposes, we hypothesize
that different stylistic features appear in regular and
vandalizing edits. For regular edits, honest editors
will strive to follow the stylistic guidelines set forth
by Wikipedia (e.g. objectivity, neutrality and factu-
ality). For edits that vandalize articles, these users
may converge on common ways of vandalizing arti-
cles.
2.1 Language Models
To differentiate between the styles of normal users
and vandalizers, we employ language models to cap-
ture the stylistic differences between authentic and
vandalizing revisions. We train two trigram lan-
guage model (LM) with Good-Turing discounting
and Katz backoff for smoothing of vandalizing ed-
its (based on the text difference between the vandal-
izing and previous revision) and good edits (based
on the text difference between the new and previous
revision).
2.2 Probabilistic Context Free Grammar
(PCFG) Models
Probabilistic context-free grammars (PCFG) capture
deep syntactic regularities beyond shallow lexico-
syntactic patterns. Raghavan et al (2010) reported
for the first time that PCFG models are effective in
learning stylometric signature of authorship at deep
syntactic levels. In this work, we explore the use of
PCFG models for vandalism detection, by viewing
the task as a genre detection problem, where a group
of authors share similar linguistic behavior. We give
a concise description of the use of PCFG models be-
low, referring to Raghavan et al (2010) for more de-
tails.
(1) Given a training corpus D for vandalism de-
tection and a generic PCFG parser Co trained
on a manually tree-banked corpus such as WSJ
or Brown, tree-bank each training document
di ? D using the generic PCFG parser Co.
(2) Learn vandalism language by training a new
PCFG parser Cvandal using only those tree-
banked documents in D that correspond to van-
dalism. Likewise, learn regular Wikipedia lan-
guage by training a new PCFG parser Cregular
using only those tree-banked documents in D
that correspond to regular Wikipedia edits.
(3) For each test document, compare the proba-
bility of the edit determined by Cvandal and
Cregular, where the parser with the higher score
determines the class of the edit.
We use the PCFG implementation of Klein and
Manning (2003).
3 System Description
Our system decides if an edit to an article is vandal-
ism by training a classifier based on a set of features
derived from many different aspects of the edit. For
this task, we use an annotated corpus (Potthast et
al., 2010) of Wikipedia edits where revisions are la-
beled as either vandalizing or non-vandalizing. This
section will describe in brief the features used by
our classifier, a more exhaustive description of our
non-linguistically motivated features can be found
in Harpalani et al (2010).
3.1 Features Based on Metadata
Our classifier takes into account metadata generated
by the revision. We generate features based on au-
thor reputation by recording if the edit is submitted
by an anonymous user or a registered user. If the au-
thor is registered, we record how long he has been
registered, how many times he has previously van-
dalized Wikipedia, and how frequent he edits arti-
cles. We also take into account the comment left by
an author. We generate features based on the charac-
teristics of the articles revision history. This includes
how many times the article has been previously van-
dalized, the last time it was edited, how many times
it has been reverted and other related features.
3.2 Features Based on Lexical Cues
Our classifier also employs a subset of features that
rely on lexical cues. Simple strategies such as count-
ing the number of vulgarities present in the revision
are effective to capture obvious forms of vandalism.
We measure the edit distance between the old and
new revision, the number of repeated patterns, slang
words, vulgarities and pronouns, the type of edit (in-
sert, modification or delete) and other similar fea-
tures.
84
Features P R F1 AUC
Baseline 72.8 41.1 52.6 91.6
+LM 73.3 42.1 53.5 91.7
+PCFG 73.5 47.7 57.9 92.9
+LM+PCFG 73.2 47.3 57.5 93.0
Table 1: Results on naturally unbalanced test data
3.3 Features Based on Sentiment
Wikipedia editors strive to maintain a neutral and
objective voice in articles. Vandals, however, in-
sert subjective and polar statements into articles. We
build two classifiers based on the work of Pang and
Lee (2004) to measure the polarity and objectivity
of article edits. We train the classifier on how many
positive and negative sentences were inserted as well
as the overall change in the sentiment score from the
previous version to the new revision and the num-
ber of inserted or deleted subjective sentences in the
revision.
3.4 Features Based on Stylometric Measures
We encode the output of the LM and PCFG in the
following manner for training our classifier. We
take the log-likelihood of the regular edit and van-
dalizing edit LMs. For our PCFG, we take the dif-
ference between the minimum log-likelihood score
(i.e. the sentences with the minimum log-likelihood)
of Cvandal and Cregular, the difference in the max-
imum log-likelihood score, the difference in the
mean log-likelihood score, the difference in the
standard deviation of the mean log-likelihood score
and the difference in the sum of the log-likelihood
scores.
3.5 Choice of Classifier
We use Weka?s (Hall et al, 2009) implementation
of LogitBoost (Friedman et al, 2000) to perform the
classification task. We use Decision Stumps (Ai and
Langley, 1992) as the base learner and run Logit-
Boost for 500 iterations. We also discretize the train-
ing data using the Multi-Level Discretization tech-
nique (Perner and Trautzsch, 1998).
4 Experimental Results
Data We use the 2010 PAN Wikipedia vandalism
corpus Potthast et al (2010) to quantify the ben-
Feature Score
Total number of author contributions 0.106
How long the author has been registered 0.098
How frequently the author contributed
in the training set 0.097
If the author is registered 0.0885
Difference in the maximum PCFG scores 0.0437
Difference in the mean PCFG scores 0.0377
How many times the article has been reverted 0.0372
Total contributions of author to Wikipedia 0.0343
Previous vandalism count of the article 0.0325
Difference in the sum of PCFG scores 0.0320
Table 2: Top 10 ranked features on the unbalanced test
data by InfoGain
efit of stylometric analysis to vandalism detection.
This corpus comprises of 32452 edits on 28468 ar-
ticles, with 2391 of the edits identified as vandal-
ism by human annotators. The class distribution is
highly skewed, as only 7% of edits corresponds to
vandalism. Among the different types of vandalism
(e.g. deletions, template changes), we focus only on
those edits that inserted or modified text (17145 ed-
its in total) since stylometric features are not relevant
to deletes and template modifications. Note that in-
sertions and modifications are the main source for
vandalism.
We randomly separated 15000 edits for training
of Cvandal and Cregular, and 17444 edits for testing,
preserving the ratio of vandalism to non-vandalism
revisions. We eliminated 7359 of the testing ed-
its to remove revisions that were exclusively tem-
plate modifications (e.g. inserting a link) and main-
tain the observed ratio of vandalism for a total of
10085 edits. For each edit in the test set, we com-
pute the probability of each modified sentence for
Cvandal and Cregular and generate the statistics for
the features described in 3.4. We compare the per-
formance of the language models and stylometric
features against a baseline classifier that is trained
on metadata, lexical and sentiment features using 10
fold stratified cross validation on the test set.
Results Table 1 shows the experimental results.
Because our dataset is highly skewed (97% corre-
sponds to ?not vandalism?), we report F-score and
85
One day rodrigo was in the school and he saw a
girl and she love her now and they are happy to-
gether
So listen Im going to attack ur family with mighty
powers.
He?s also the best granddaddy ever.
Beatrice Rosen (born 29 November 1985 (Happy
birthday)), also known as Batrice Rosen or Ba-
trice Rosenblatt, is a French-born actress. She is
best known for her role as Faith in the second sea-
son of the TV series ?Cuts?.
Table 3: Examples of vandalism detected by base-
line+PCFG features. Baseline features alone could not
detect these vandalism. Notice that several stylistic fea-
tures present in these sentences are unlikely to appear in
normal Wikipedia articles.
AUC rather than accuracy.1 The baseline system,
which includes a wide range of features that are
shown to be highly effective in vandalism detection,
achieves F-score 52.6%, and AUC 91.6%. The base-
line features include all features introduced in Sec-
tion 3.
Adding language model features to the baseline
(denoted as +LM in Table 1) increases the F-score
slightly (53.5%), while the AUC score is almost
the same (91.7%). Adding PCFG based features to
the baseline (denoted as +PCFG) brings the most
substantial performance improvement: it increases
recall substantially while also improving precision,
achieving 57.9% F-score and 92.9% AUC. Combin-
ing both PCFG and language model based features
(denoted as +LM+PCFG) only results in a slight
improvement in AUC. From these results, we draw
the following conclusions:
? There are indeed unique language styles in van-
dalism that can be detected with stylometric
analysis.
? Rather unexpectedly, deep syntax oriented fea-
tures based on PCFG bring a much more sub-
stantial improvement than language models
that capture only shallow lexico-syntactic pat-
terns.
1A naive rule that always chooses the majority class (?not
vandalism?) will receive zero F-score.
All those partaking in the event get absolutely
?fritzeld? and certain attendees have even been
known to soil themselves
March 10,1876 Alexander Grahm Ball dscovered
th telephone when axcidently spilt battery juice on
his expeiriment.
English remains the most widely spoken language
and New York is the largest city in the English
speaking world. Although massive pockets in
Queens and Brooklyn have 20% or less people
who speak English not so good.
Table 4: Examples of vandalism that evaded both our
baseline and baseline+PCFG classifier. Dry wit, for
example, relies on context and may receive a good
score from the parser trained on regular Wikipedia edits
(Cregular).
Feature Analysis Table 2 lists the information
gain ranking of our features. Notice that several of
our PCFG features are in the top ten most informa-
tive features. Language model based features were
ranked very low in the list, hence we do not include
them in the list. This finding will be potentially ad-
vantageous to many of the current anti-vandalism
tools such as vulgarisms, which rely only on shal-
low lexico-syntactic patterns.
Examples To provide more insight to the task, Ta-
ble 3 shows several instances where the addition
of the PCFG derived features detected vandalism
that the baseline approach could not. Notice that
the first example contains a lot of conjunctions that
would be hard to characterize using shallow lexico-
syntactic features. The second and third examples
also show sentence structure that are more informal
and vandalism-like. The fourth example is one that
is harder to catch. It looks almost like a benign edit,
however, what makes it a vandalism is the phrase
?(Happy Birthday)? inserted in the middle.
Table 4 shows examples where all of our systems
could not detect the vandalism correctly. Notice that
examples in Table 4 generally manifest more a for-
mal voice than those in Table 3.
5 Related Work
Wang and McKeown (2010) present the first ap-
proach that is linguistically motivated. Their ap-
86
proach was based on shallow syntactic patterns,
while ours explores the use of deep syntactic pat-
terns, and performs a comparative evaluation across
different stylometry analysis techniques. It is worth-
while to note that the approach of Wang and McKe-
own (2010) is not as practical and scalable as ours in
that it requires crawling a substantial number (150)
of webpages to detect each vandalism edit. From
our pilot study based on 1600 edits (50% of which
is vandalism), we found that the topic-specific lan-
guage models built from web search do not produce
stronger result than PCFG based features. We do
not have a result directly comparable to theirs how-
ever, as we could not crawl the necessary webpages
required to match the size of corpus.
The standard approach to Wikipedia vandalism
detection is to develop a feature based on either the
content or metadata and train a classifier to recog-
nize it. A comprehensive overview of what types
of features have been employed for this task can be
found in Potthast et al (2010). WikiTrust, a repu-
tation system for Wikipedia authors, focuses on de-
termining the likely quality of a contribution (Adler
and de Alfaro, 2007).
6 Future Work and Conclusion
This paper presents a vandalism detection system for
Wikipedia that uses stylometric features to aide in
classification. We show that deep syntactic patterns
based on PCFGs more effectively identify vandal-
ism than shallow lexico-syntactic patterns based on
n-grams or contextual language models. PCFGs do
not require the laborious process of performing web
searches to build context language models. Rather,
PCFGs are able to detect differences in language
styles between vandalizing edits and normal edits to
Wikipedia articles. Employing stylometric features
increases the baseline classification rate.
We are currently working to improve this tech-
nique through more effective training of our PCFG
parser. We look to automate the expansion of the
training set of vandalized revisions to include exam-
ples from outside of Wikipedia that reflect similar
language styles. We also are investigating how we
can better utilize the output of our PCFG parsers for
classification.
7 Acknowledgments
We express our most sincere gratitude to Dr. Tamara
Berg and Dr. Luis Ortiz for their valuable guid-
ance and suggestions in applying Machine Learning
and Natural Language Processing techniques to the
task of vandalism detection. We also recognize the
hard work of Megha Bassi and Thanadit Phumprao
for assisting us in building our vandalism detection
pipeline that enabled us to perform these experi-
ments.
References
B. Thomas Adler and Luca de Alfaro. 2007. A content-
driven reputation system for the wikipedia. In Pro-
ceedings of the 16th international conference on World
Wide Web, WWW ?07, pages 261?270, New York, NY,
USA. ACM.
B. Thomas Adler, Luca de Alfaro, Santiago M. Mola-
Velasco, Paolo Rosso, and Andrew G. West. 2011.
Wikipedia vandalism detection: Combining natural
language, metadata, and reputation features. In CI-
CLing ?11: Proceedings of the 12th International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics.
Wayne Iba Ai and Pat Langley. 1992. Induction of one-
level decision trees. In Proceedings of the Ninth In-
ternational Conference on Machine Learning, pages
233?240. Morgan Kaufmann.
Alexa. 2011. Top 500 sites (retrieved April 2011).
http://www.alexa.com/topsites.
Shlomo Argamon, Moshe Koppel, James W. Pennebaker,
and Jonathan Schler. 2009. Automatically profiling
the author of an anonymous text. Commun. ACM,
52:119?123, February.
Si-Chi Chin, W. Nick Street, Padmini Srinivasan, and
David Eichmann. 2010. Detecting wikipedia vandal-
ism with active learning and statistical language mod-
els. In WICOW ?10: Proceedings of the 4rd Workshop
on Information Credibility on the Web.
J. Friedman, T. Hastie, and R. Tibshirani. 2000. Additive
Logistic Regression: a Statistical View of Boosting.
The Annals of Statistics, 38(2).
R. Stuart Geiger and David Ribes. 2010. The work of
sustaining order in wikipedia: the banning of a vandal.
In Proceedings of the 2010 ACM conference on Com-
puter supported cooperative work, CSCW ?10, pages
117?126, New York, NY, USA. ACM.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
87
Manoj Harpalani, Thanadit Phumprao, Megha Bassi,
Michael Hart, and Rob Johnson. 2010. Wiki
vandalysis- wikipedia vandalism analysis lab report
for pan at clef 2010.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423?430. Association for Computa-
tional Linguistics.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Polina Panicheva, John Cardiff, and Paolo Rosso. 2010.
Personal sense and idiolect: Combining authorship at-
tribution and opinion analysis. In Nicoletta Calzo-
lari (Conference Chair), Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh conference on International Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, may. European Language Resources Associa-
tion (ELRA).
Petra Perner and Sascha Trautzsch. 1998. Multi-interval
discretization methods for decision tree learning. In
In: Advances in Pattern Recognition, Joint IAPR In-
ternational Workshops SSPR 98 and SPR 98, pages
475?482.
Martin Potthast, Benno Stein, and Robert Gerling. 2008.
Automatic vandalism detection in wikipedia. In
ECIR?08: Proceedings of the IR research, 30th Euro-
pean conference on Advances in information retrieval,
pages 663?668, Berlin, Heidelberg. Springer-Verlag.
Martin Potthast, Benno Stein, and Teresa Holfeld. 2010.
Overview of the 1st International Competition on
Wikipedia Vandalism Detection. In Martin Braschler
and Donna Harman, editors, Notebook Papers of
CLEF 2010 LABs and Workshops, 22-23 September,
Padua, Italy, September.
Martin Potthast. 2010. Crowdsourcing a wikipedia van-
dalism corpus. In SIGIR?10, pages 789?790.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proceedings of the
ACL, pages 38?42, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Efstathios Stamatatos. 2009. A survey of modern author-
ship attribution methods. J. Am. Soc. Inf. Sci. Technol.,
60:538?556, March.
M. Teresa Turell. 2010. The use of textual, grammatical
and sociolinguistic evidence in forensic text compari-
son:. International Journal of Speech Language and
the Law, 17(2).
William Yang Wang and Kathleen R. McKeown.
2010. ?got you!?: Automatic vandalism detec-
tion in wikipedia with web-based shallow syntactic-
semantic modeling. In 23rd International Conference
on Computational Linguistics (Coling 2010), page
1146?1154.
Andrew G. West, Sampath Kannan, and Insup Lee. 2010.
Detecting wikipedia vandalism via spatio-temporal
analysis of revision metadata. In EUROSEC ?10: Pro-
ceedings of the Third European Workshop on System
Security, pages 22?28, New York, NY, USA. ACM.
Wikipedia. 2010. Daily edit statistics.
http://stats.wikimedia.org/EN/
PlotsPngDatabaseEdits.htm.
Wikipedia. 2011. Wikipedia. http://www.
wikipedia.org.
88
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359?368,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Collective Generation of Natural Image Descriptions
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg and Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{pkuznetsova,vordonezroma,aberg,tlberg,ychoi}@cs.stonybrook.edu
Abstract
We present a holistic data-driven approach
to image description generation, exploit-
ing the vast amount of (noisy) parallel im-
age data and associated natural language
descriptions available on the web. More
specifically, given a query image, we re-
trieve existing human-composed phrases
used to describe visually similar images,
then selectively combine those phrases
to generate a novel description for the
query image. We cast the generation pro-
cess as constraint optimization problems,
collectively incorporating multiple inter-
connected aspects of language composition
for content planning, surface realization
and discourse structure. Evaluation by hu-
man annotators indicates that our final
system generates more semantically cor-
rect and linguistically appealing descrip-
tions than two nontrivial baselines.
1 Introduction
Automatically describing images in natural lan-
guage is an intriguing, but complex AI task, re-
quiring accurate computational visual recogni-
tion, comprehensive world knowledge, and natu-
ral language generation. Some past research has
simplified the general image description goal by
assuming that relevant text for an image is pro-
vided (e.g., Aker and Gaizauskas (2010), Feng
and Lapata (2010)). This allows descriptions to
be generated using effective summarization tech-
niques with relatively surface level image under-
standing. However, such text (e.g., news articles
or encyclopedic text) is often only loosely related
to an image?s specific content and many natu-
ral images do not come with associated text for
summarization.
In contrast, other recent work has focused
more on the visual recognition aspect by de-
tecting content elements (e.g., scenes, objects,
attributes, actions, etc) and then composing de-
scriptions from scratch (e.g., Yao et al (2010),
Kulkarni et al (2011), Yang et al (2011), Li
et al (2011)), or by retrieving existing whole
descriptions from visually similar images (e.g.,
Farhadi et al (2010), Ordonez et al (2011)). For
the latter approaches, it is unrealistic to expect
that there will always exist a single complete de-
scription for retrieval that is pertinent to a given
query image. For the former approaches, visual
recognition first generates an intermediate rep-
resentation of image content using a set of En-
glish words, then language generation constructs
a full description by adding function words and
optionally applying simple re-ordering. Because
the generation process sticks relatively closely
to the recognized content, the resulting descrip-
tions often lack the kind of coverage, creativ-
ity, and complexity typically found in human-
written text.
In this paper, we propose a holistic data-
driven approach that combines and extends the
best aspects of these previous approaches ? a)
using visual recognition to directly predict indi-
vidual image content elements, and b) using re-
trieval from existing human-composed descrip-
tions to generate natural, creative, and inter-
359
esting captions. We also lift the restriction of
retrieving existing whole descriptions by gather-
ing visually relevant phrases which we combine
to produce novel and query-image specific de-
scriptions. By judiciously exploiting the corre-
spondence between image content elements and
phrases, it is possible to generate natural lan-
guage descriptions that are substantially richer
in content and more linguistically interesting
than previous work.
At a high level, our approach can be moti-
vated by linguistic theories about the connection
between reading activities and writing skills,
i.e., substantial reading enriches writing skills,
(e.g., Hafiz and Tudor (1989), Tsang (1996)).
Analogously, our generation algorithm attains a
higher level of linguistic sophistication by read-
ing large amounts of descriptive text available
online. Our approach is also motivated by lan-
guage grounding by visual worlds (e.g., Roy
(2002), Dindo and Zambuto (2010), Monner and
Reggia (2011)), as in our approach the mean-
ing of a phrase in a description is implicitly
grounded by the relevant content of the image.
Another important thrust of this work is col-
lective image-level content-planning, integrating
saliency, content relations, and discourse struc-
ture based on statistics drawn from a large
image-text parallel corpus. This contrasts with
previous approaches that generate multiple sen-
tences without considering discourse flow or re-
dundancy (e.g., Li et al (2011)). For example,
for an image showing a flock of birds, generating
a large number of sentences stating the relative
position of each bird is probably not useful.
Content planning and phrase synthesis can
be naturally viewed as constraint optimization
problems. We employ Integer Linear Program-
ming (ILP) as an optimization framework that
has been used successfully in other generation
tasks (e.g., Clarke and Lapata (2006), Mar-
tins and Smith (2009), Woodsend and Lapata
(2010)). Our ILP formulation encodes a rich
set of linguistically motivated constraints and
weights that incorporate multiple aspects of the
generation process. Empirical results demon-
strate that our final system generates linguisti-
cally more appealing and semantically more cor-
rect descriptions than two nontrivial baselines.
1.1 System Overview
Our system consists of two parts. For a query
image, we first retrieve candidate descriptive
phrases from a large image-caption database us-
ing measures of visual similarity (?2). We then
generate a coherent description from these can-
didates using ILP formulations for content plan-
ning (?4) and surface realization (?5).
2 Vision & Phrase Retrieval
For a query image, we retrieve relevant candi-
date natural language phrases by visually com-
paring the query image to database images from
the SBU Captioned Photo Collection (Ordonez
et al, 2011) (1 million photographs with asso-
ciated human-composed descriptions). Visual
similarity for several kinds of image content are
used to compare the query image to images from
the database, including: 1) object detections for
89 common object categories (Felzenszwalb et
al., 2010), 2) scene classifications for 26 com-
mon scene categories (Xiao et al, 2010), and
3) region based detections for stuff categories
(e.g. grass, road, sky) (Ordonez et al, 2011).
All content types are pre-computed on the mil-
lion database photos, and caption parsing is per-
formed using the Berkeley PCFG parser (Petrov
et al, 2006; Petrov and Klein, 2007).
Given a query image, we identify content el-
ements present using the above classifiers and
detectors and then retrieve phrases referring to
those content elements from the database. For
example, if we detect a horse in a query im-
age, then we retrieve phrases referring to vi-
sually similar horses in the database by com-
paring the color, texture (Leung and Malik,
1999), or shape (Dalal and Triggs, 2005; Lowe,
2004) of the detected horse to detected horses
in the database images. We collect four types of
phrases for each query image as follows:
[1] NPs We retrieve noun phrases for each
query object detection (e.g., ?the brown cow?)
from database captions using visual similar-
ity between object detections computed as an
equally weighted linear combination of L2 dis-
360
tances on histograms of color, texton (Leung and
Malik, 1999), HoG (Dalal and Triggs, 2005) and
SIFT (Lowe, 2004) features.
[2] VPs We retrieve verb phrases for each
query object detection (e.g. ?boy running?)
from database captions using the same mea-
sure of visual similarity as for NPs, but restrict-
ing the search to only those database instances
whose captions contain a verb phrase referring
to the object category.
[3] Region/Stuff PPs We collect preposi-
tional phrases for each query stuff detection (e.g.
?in the sky?, ?on the road?) by measuring visual
similarity of appearance (color, texton, HoG)
and geometric configuration (object-stuff rela-
tive location and distance) between query and
database detections.
[4] Scene PPs We also collect prepositonal
phrases referring to general image scene context
(e.g. ?at the market?, ?on hot summer days?,
?in Sweden?) based on global scene similarity
computed using L2 distance between scene clas-
sification score vectors (Xiao et al, 2010) com-
puted on the query and database images.
3 Overview of ILP Formulation
For each image, we aim to generate multiple
sentences, each sentence corresponding to a sin-
gle distinct object detected in the given image.
Each sentence comprises of the NP for the main
object, and a subset of the corresponding VP,
region/stuff PP, and scene PP retrieved in ?2.
We consider four different types of operations
to generate the final description for each image:
T1. Selecting the set of objects to describe (one
object per sentence).
T2. Re-ordering sentences (i.e., re-ordering ob-
jects).
T3. Selecting the set of phrases for each sen-
tence.
T4. Re-ordering phrases within each sentence.
The ILP formulation of ?4 addresses T1 & T2,
i.e., content-planning, and the ILP of ?5 ad-
dresses T3 & T4, i.e., surface realization.1
1It is possible to create one conjoined ILP formulation
to address all four operations T1?T4 at once. For com-
4 Image-level Content Planning
First we describe image-level content planning,
i.e., abstract generation. The goals are to (1) se-
lect a subset of the objects based on saliency and
semantically compatibility, and (2) order the se-
lected objects based on their content relations.
4.1 Variables and Objective Function
The following set of indicator variables encodes
the selection of objects and ordering:
ysk =
?
?
?
1, if object s is selected
for position k
0, otherwise
(1)
where k = 1, ..., S encodes the position (order)
of the selected objects, and s indexes one of the
objects. In addition, we define a set of variables
indicating specific pairs of adjacent objects:
yskt(k+1) =
{
1, if ysk = yt(k+1) = 1
0, otherwise
(2)
The objective function, F , that we will maxi-
mize is a weighted linear combination of these
indicator variables and can be optimized using
integer linear programming:
F =
?
s
Fs ?
S?
k=1
ysk ?
?
st
Fst ?
S?1?
k=1
yskt(k+1) (3)
where Fs quantifies the salience/confidence of
the object s, and Fst quantifies the seman-
tic compatibility between the objects s and t.
These coefficients (weights) will be described in
?4.3 and ?4.4. We use IBM CPLEX to optimize
this objective function subject to the constraints
introduced next in ?4.2.
4.2 Constraints
Consistency Constraints: We enforce consis-
tency between indicator variables for indivisual
objects (Eq. 1) and consecutive objects (Eq. 2)
so that yskt(k+1) = 1 iff ysk = 1 and yt(k+1) = 1:
?stk, yskt(k+1) ? ysk (4)
yskt(k+1) ? yt(k+1) (5)
yskt(k+1) + (1? ysk) + (1? yt(k+1)) ? 1 (6)
putational and implementation efficiency however, we opt
for the two-step approach.
361
To avoid empty descriptions, we enforce that the
result includes at least one object:
?
s
ys1 = 1 (7)
To enforce contiguous positions be selected:
?k = 2, ..., S ? 1,
?
s
ys(k+1) ?
?
s
ysk (8)
Discourse constraints: To avoid spurious de-
scriptions, we allow at most two objects of the
same type, where cs is the type of object s:
?c ? objTypes,
?
{s: cs=c}
S?
k=1
ysk ? 2 (9)
4.3 Weight Fs: Object Detection
Confidence
In order to quantify the confidence of the object
detector for the object s, we define 0 ? Fs ? 1
as the mean of the detector scores for that object
type in the image.
4.4 Weight Fst: Ordering and
Compatibility
The weight 0 ? Fst ? 1 quantifies the compat-
ibility of the object pairing (s, t). Note that in
the objective function, we subtract this quan-
tity from the function to be maximized. This
way, we create a competing tension between the
single object selection scores and the pairwise
compatibility scores, so that variable number of
objects can be selected.
Object Ordering Statistics: People have bi-
ases on the order of topic or content flow. We
measure these biases by collecting statistics on
ordering of object names from the 1 million im-
age descriptions in the SBU Captioned Dataset
(Ordonez et al, 2011). Let ford(w1, w2) be
the number of times w1 appeared before w2.
For instance, ford(window, house) = 2895 and
ford(house, window) = 1250, suggesting that
people are more likely to mention a window be-
fore mentioning a house/building2. We use these
ordering statistics to enhance content flow. We
define score for the order of objects using Z-score
for normalization as follows:
F?st =
ford(cs, ct)?mean(ford)
std dev(ford)
(10)
2We take into account synonyms.
We then transform F?st so that F?st ? [0,1], and
then set Fst = 1 ? F?st so that smaller values
correspond to better choices.
5 Surface Realization
Recall that for each image, the computer vi-
sion system identifies phrases from descriptions
of images that are similar in a variety of aspects.
The result is a set of phrases representing four
different types of information (?2). From this
assortment of phrases, we aim to select a subset
and glue them together to compose a complete
sentence that is linguistically plausible and se-
mantically truthful to the content of the image.
5.1 Variables and Objective Function
The following set of variables encodes the selec-
tion of phrases and their ordering in construct-
ing S? sentences.
xsijk =
?
?????
?????
1, if phrase i of type j
is selected
for position k
in sentence s
0, otherwise
(11)
where k = 1, ..., N encodes the ordering of the
selected phrases, and j indexes one of the four
phrases types (object-NPs, action-VPs, region-
PPs, scene-PPs), i = 1, ...,M indexes one of
the M candidate phrases of each phrase type,
and s = 1, ..., S? encodes the sentence (object).
In addition, we define indicator variables for
adjacent pairs of phrases: xsijkpq(k+1) = 1 if
xsijk = xspq(k+1) = 1 and 0 otherwise. Finally,
we define the objective function F as:
F =
?
sij
Fsij ?
N?
k=1
xsijk
?
?
sijpq
Fsijpq ?
N?1?
k=1
xsijkpq(k+1) (12)
where Fsij weights individual phrase goodness
and Fsijpq adjacent phrase goodness. All coeffi-
cients (weights) will be described in Section 5.3
and 5.4.
We optionally prepend the first sentence in a
generated description with a cognitive phrase.3
3We collect most frequent 200 phrases of length 1-
7 that start a caption from the SBU Captioned Photo
Collection.
362
ILP: I think this is a boy?s bike 
lied in saltwater for quite a 
while. 
 
HMM: I liked the way bicycles 
leaning against a wall in 
Copenhagen Denmark in a 
windy sky in a Singapore 
bathroom. Boy?s bike lied in 
saltwater for quite a while in a 
windy sky in a Singapore 
bathroom. Fruit rubbing his 
face in the encrusted snow in a 
windy sky in a Singapore 
bathroom. 
 
Human: You re nobody in 
Oxford, unless you have a old 
bike with a basket 
 
ILP:  
This is a photo of this little flower sprouted up in defiance against grass.  
Bright yellow flowers growing in a rock garden at Volcan Mombacho. 
 
HMM: These was taken on the flowers growing in a rock garden in the field in two sorts. This 
little flower sprouted up in defiance in the field in two sorts. A full open flower sprouted up in 
defiance in the field in gardens. Bright yellow flowers growing in a rock garden in the field. 
 
Human: Yellow flower in my field 
 
ILP: Found trucks parked on first avenue in the east village. 
 
HMM: This is the first cellar door left back bedroom in center 
and clothes dryer to the right to the building in the house. 
This HUGE screen hanging on the wall outside a burned down 
building in the house. My truck parked on first avenue in the 
east village by the glass buildings in the house. 
 
Human: Flat bed Chisholms truck on display at the vintage 
vehicle rall y at Astley Green Colliery near Leigh Lancs 
 
Figure 1: ILP & HMM generated captions. In HMM generated captions, underlined phrases show redundancy
across different objects (due to lack of discourse constraints), and phrases in boldface show awkward topic
flow (due to lack of content planning). Note that in the bicycle image, the visual recognizer detected two
separate bicycles and some fruits, as can be seen in the HMM result. Via collective image-level content
planning (see ?4), some of these erroneous detection can be corrected, as shown in the ILP result. Spurious
and redundant phrases can be suppressed via discourse constraints (see ?5).
These are generic constructs that are often used
to start a description about an image, for in-
stance, ?This is an image of...?. We treat these
phrases as an additional type, but omit corre-
sponding variables and constraints for brevity.
5.2 Constraints
Consistency Constraints: First we enforce
consistency between the unary variables (Eq.
11) and the pairwise variables so that xsijkpqm =
1 iff xsijk = 1 and xspqm = 1:
?ijkpqm, xsijkpqm ? xsijk (13)
xsijkpqm ? xspqm (14)
xsijkpqm + (1? xsijk) + (1? xspqm) ? 1 (15)
Next we include constraints similar to Eq. 8
(contiguous slots are filled), but omit them for
brevity. Finally, we add constraints to ensure at
least two phrases are selected for each sentence,
to promote informative descriptions.
Linguistic constraints: We include linguisti-
cally motivated constraints to generate syntacti-
cally and semantically plausible sentences. First
we enforce a noun-phrase to be selected to en-
sure semantic relevance to the image:
?s,
?
ik
xsiNPk = 1 (16)
Also, to avoid content redundancy, we allow at
most one phrase of each type:
?sj,
?
i
N?
k=1
xsijk ? 1 (17)
Discourse constraints: We allow at most
one prepositional scene phrase for the whole de-
scription to avoid redundancy:
For j = PPscene,
?
sik
xsijk ? 1 (18)
We add constraints that prevent the inclusion of
more than one phrase with identical head words:
?s, ij, pq with the same heads,
N?
k=1
xsijk +
N?
k=1
xspqk ? 1 (19)
5.3 Unary Phrase Selection
Let Msij be the confidence score for phrase
xsij given by the image?phrase matching al-
gorithm (?2). To make the scores across dif-
ferent phrase types comparable, we normalize
them using Z-score: Fsij = norm?(Msij) =
(Msij ? meanj)/devj , and then transform the
values into the range of [0,1].
5.4 Pairwise Phrase Cohesion
In this section, we describe the pairwise phrase
cohesion score Fsijpq defined for each xsijpq in
363
ILP: I like the way the clouds hanging down by 
the ground in Dupnitsa of Avikwalal. 
 
Human: Car was raised on the wall over a bridge 
facing traffic..paramedics were attending the 
driver on the ground 
ILP: This is a photo of this bird hopping 
around eating things off of the ground by 
river. 
Human: IMG_6892 Lookn up in the sky its a 
bird its a plane its ah..... you 
ILP: This is a sporty little red convertible made for 
a great day in Key West FL. This car was in the 4th 
parade of the apartment buildings. 
 
Human: Hard rock casino exotic car show in June 
ILP: Taken in front of my cat sitting in a shoe 
box. Cat likes hanging around in my recliner. 
 
Human: H happily rests his armpit on a 
warm Gatorade bottle of water (a small 
bottle wrapped in a rag) 
Figure 2: In some cases (16%), ILP generated captions were preferred over human written ones!
the objective function (Eq. 12). Via Fsijpq,
we aim to quantify the degree of syntactic and
semantic cohesion across two phrases xsij and
xspq. Note that we subtract this cohesion score
from the objective function. This trick helps the
ILP solver to generate sentences with varying
number of phrases, rather than always selecting
the maximum number of phrases allowed.
N-gram Cohesion Score: We use n-gram
statistics from the Google Web 1-T dataset
(Brants and Franz., 2006) Let Lsijpq be the set
of all n-grams (2 ? n ? 5) across xsij and xspq.
Then the n-gram cohesion score is computed as:
FNGRAMsijpq = 1?
?
l?Lsijpq
NPMI(l)
size(Lsijpq)
(20)
NPMI(ngr) =
PMI(ngr)? PMImin
PMImax ? PMImin
(21)
Where NPMI is the normalized point-wise mu-
tual information.4
Co-occurrence Cohesion Score: To cap-
ture long-distance cohesion, we introduce a co-
occurrence-based score, which measures order-
preserved co-occurrence statistics between the
head words hsij and hspq 5. Let f?(hsij , hspq)
be the sum frequency of all n-grams that start
with hsij , end with hspq and contain a prepo-
sition prep(spq) of the phrase spq. Then the
4We include the n-gram cohesion for the sentence
boundaries as well, by approximating statistics for sen-
tence boundaries with punctuation marks in the Google
Web 1-T data.
5For simplicity, we use the last word of a phrase as
the head word, except VPs where we take the main verb.
co-occurrence cohesion is computed as:
FCOsijpq =
max(f?)? f?(hsij , hspq)
max(f?)?min(f?)
(22)
Final Cohesion Score: Finally, the pairwise
phrase cohesion score Fijpq is a weighted sum of
n-gram and co-occurrence cohesion scores:
Fsijpq =
? ? FNGRAMsijpq + ? ? F
CO
sijpq
?+ ?
(23)
where ? and ? can be tuned via grid search,
and FNGRAMijpq and F
CO
ijpq are normalized ? [0, 1]
for comparability. Notice that Fsijpq is in the
range [0,1] as well.
6 Evaluation
TestSet: Because computer vision is a challeng-
ing and unsolved problem, we restrict our query
set to images where we have high confidence that
visual recognition algorithms perform well. We
collect 1000 test images by running a large num-
ber (89) of object detectors on 20,000 images
and selecting images that receive confident ob-
ject detection scores, with some preference for
images with multiple object detections to obtain
good examples for testing discourse constraints.
Baselines: We compare our ILP approaches
with two nontrivial baselines: the first is an
HMM approach (comparable to Yang et al
(2011)), which takes as input the same set of
candidate phrases described in ?2, but for de-
coding, we fix the ordering of phrases as [ NP
? VP ? Region PP ? Scene PP] and find the
best combination of phrases using the Viterbi
algorithm. We use the same rich set of pairwise
364
Hmm Hmm Ilp Ilp
cognitive phrases: with w/o with w/o
0.111 0.114 0.114 0.116
Table 1: Automatic Evaluation
ILP selection rate
ILP V.S. HMM (w/o cogn) 67.2%
ILP V.S. HMM (with cogn) 66.3%
Table 2: Human Evaluation (without images)
ILP selection rate
ILP V.S. HMM (w/o cogn) 53.17%
ILP V.S. HMM (with cogn) 54.5%
ILP V.S. Retrieval 71.8%
ILP V.S. Human 16%
Table 3: Human Evaluation (with images)
phrase cohesion scores (?5.4) used for the ILP
formulation, producing a strong baseline6.
The second baseline is a recent Retrieval
based description method (Ordonez et al, 2011),
that searches the large parallel corpus of im-
ages and captions, and transfers a caption from
a visually similar database image to the query.
This again is a very strong baseline, as it ex-
ploits the vast amount of image-caption data,
and produces a description high in linguistic
quality (since the captions were written by hu-
man annotators).
Automatic Evaluation: Automatically quan-
tifying the quality of machine generated sen-
tences is known to be difficult. BLEU score
(Papineni et al, 2002), despite its simplicity
and limitations, has been one of the common
choices for automatic evaluation of image de-
scriptions (Farhadi et al, 2010; Kulkarni et al,
2011; Li et al, 2011; Ordonez et al, 2011), as
it correlates reasonably well with human evalu-
ation (Belz and Reiter, 2006).
Table 1 shows the the BLEU @1 against the
original caption of 1000 images. We see that the
ILP improves the score over HMM consistently,
with or without the use of cognitive phrases.
6Including other long-distance scores in HMM decod-
ing would make the problem NP-hard and require more
sophisticated decoding, e.g. ILP.
Grammar Cognitive Relevance
HMM 3.40(?=.82) 3.40(?=.88) 2.25(?=1.37)
ILP 3.56(?=.90) 3.60(?=.98) 2.37(?=1.49)
Hum. 4.36(?=.79) 4.77(?=.66) 3.86(?=1.60)
Table 4: Human Evaluation: Multi-Aspect Rating
(? is a standard deviation)
Human Evaluation I ? Ranking: We com-
plement the automatic evaluation with Mechan-
ical Turk evaluation. In ranking evaluation, we
ask raters to choose a better caption between
two choices7. We do this rating with and with-
out showing the images, as summarized in Ta-
ble 2 & 3. When images are shown, raters evalu-
ate content relevance as well as linguistic quality
of the captions. Without images, raters evaluate
only linguistic quality.
We found that raters generally prefer ILP gen-
erated captions over HMM generated ones, twice
as much (67.2% ILP V.S. 32.8% HMM), if im-
ages are not presented. However the difference is
less pronounced when images are shown. There
could be two possible reasons. The first is that
when images are shown, the Turkers do not try
as hard to tell apart the subtle difference be-
tween the two imperfect captions. The second
is that the relative content relevance of ILP gen-
erated captions is negating the superiority in lin-
guistic quality. We explore this question using
multi-aspect rating, described below.
Note that ILP generated captions are exceed-
ingly (71.8 %) preferred over the Retrieval
baseline (Ordonez et al, 2011), despite the gen-
erated captions tendency to be more prone to
grammatical and cognitive errors than retrieved
ones. This indicates that the generated captions
must have substantially better content relevance
to the query image, supporting the direction of
this research. Finally, notice that as much as
16% of the time, ILP generated captions are pre-
ferred over the original human generated ones
(examples in Figure 2).
Human Evaluation II ? Multi-Aspect Rat-
ing: Table 4 presents rating in the 1?5 scale (5:
perfect, 4: almost perfect, 3: 70?80% good, 2:
7We present two captions in a randomized order.
365
Found MIT boy 
gave me this 
quizical expression. 
One of the most shirt 
in the wall of the 
house. 
Grammar Problems 
Here you can see a 
bright red flower taken 
near our apartment in 
Torremolinos the Costa 
Del Sol. 
Content Irrelevance 
This is a shoulder bag with 
a blended rainbow effect. 
Cognitive Absurdity 
Here you can see a cross 
by the frog in the sky. 
Figure 3: Examples with different aspects of prob-
lems in the ILP generated captions.
50?70% good, 1: totally bad) in three different
aspects: grammar, cognitive correctness,8 and
relevance. We find that ILP improves over HMM
in all aspects, however, the relevance score is no-
ticeably worse than scores of two other criteria.
It turns out human raters are generally more
critical against the relevance aspect, as can be
seen in the ratings given to the original human
generated captions.
Discussion with Examples: Figure 1 shows
contrastive examples of HMM vs ILP gener-
ated captions. Notice that HMM captions
look robotic, containing spurious and redundant
phrases due to lack of discourse constraints, and
often discussing an awkward set of objects due
to lack of image-level content planning. Also
notice how image-level content planning under-
pinned by language statistics helps correct some
of the erroneous vision detections. Figure 3
shows some example mistakes in the ILP gen-
erated captions.
7 Related Work & Discussion
Although not directly focused on image descrip-
tion generation, some previous work in the realm
of summarization shares the similar problem of
content planning and surface realization. There
8E.g., ?A desk on top of a cat? is grammatically cor-
rect, but cognitively absurd.
are subtle, but important differences however.
First, sentence compression is hardly the goal
of image description generation, as human writ-
ten descriptions are not necessarily succinct.9
Second, unlike summarization, we are not given
with a set of coherent text snippet to begin with,
and the level of noise coming from the visual
recognition errors is much higher than that of
starting with clean text. As a result, choosing
an additional phrase in the image description is
much riskier than it is in summarization.
Some recent research proposed very elegant
approaches to summarization using ILP for col-
lective content planning and/or surface realiza-
tion (e.g., Martins and Smith (2009), Woodsend
and Lapata (2010), Woodsend et al (2010)).
Perhaps the most important difference in our
approach is the use of negative weights in the
objective function to create the necessary ten-
sion between selection (salience) and compatibil-
ity, which makes it possible for ILP to generate
variable length descriptions, effectively correct-
ing some of the erroneous vision detections. In
contrast, all previous work operates with a pre-
defined upper limit in length, hence the ILP was
formulated to include as many textual units as
possible modulo constraints.
To conclude, we have presented a collective
approach to generating natural image descrip-
tions. Our approach is the first to systematically
incorporate state of the art computer vision
to retrieve visually relevant candidate phrases,
then produce images descriptions that are sub-
stantially more complex and human-like than
previous attempts.
Acknowledgments T. L. Berg is supported
in part by NSF CAREER award #1054133; A.
C. Berg and Y. Choi are partially supported by
the Stony Brook University Office of the Vice
President for Research. We thank K. Yam-
aguchi, X. Han, M. Mitchell, H. Daume III, A.
Goyal, K. Stratos, A. Mensch, J. Dodge for data
pre-processing and useful initial discussions.
9On a related note, the notion of saliency also differs
in that human written captions often digress on details
that might be tangential to the visible content of the
image. E.g., ?This is a dress my mom made.?, where the
picture does not show a woman making the dress.
366
References
Ahmet Aker and Robert Gaizauskas. 2010. Gen-
erating image descriptions using dependency rela-
tional patterns. In ACL.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of nlg systems.
In EACL 2006, 11st Conference of the European
Chapter of the Association for Computational Lin-
guistics, Proceedings of the Conference, April 3-7,
2006, Trento, Italy. The Association for Computer
Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-
gram version 1. In Linguistic Data Consortium.
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 144?151, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detection. In Pro-
ceedings of the 2005 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recogni-
tion (CVPR?05) - Volume 1 - Volume 01, CVPR
?05, pages 886?893, Washington, DC, USA. IEEE
Computer Society.
Haris Dindo and Daniele Zambuto. 2010. A prob-
abilistic approach to learning a visually grounded
language model through human-robot interaction.
In IROS, pages 790?796. IEEE.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every
picture tells a story: generating sentences for im-
ages. In ECCV.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part based
models. tPAMI, Sept.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption gen-
eration for news images. In ACL.
Fateh Muhammad Hafiz and Ian Tudor. 1989. Ex-
tensive reading and the development of language
skills. ELT Journal, 43(1):4?13.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar,
Siming Li, Yejin Choi, Alexander C Berg, and
Tamara L Berg. 2011. Babytalk: Understand-
ing and generating simple image descriptions. In
CVPR.
Thomas K. Leung and Jitendra Malik. 1999. Rec-
ognizing surfaces using three-dimensional textons.
In ICCV.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Compos-
ing simple image descriptions using web-scale n-
grams. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 220?228, Portland, Oregon, USA, June.
Association for Computational Linguistics.
David G. Lowe. 2004. Distinctive image features
from scale-invariant keypoints. Int. J. Comput.
Vision, 60:91?110, November.
Andre Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
guage Processing, pages 1?9, Boulder, Colorado,
June. Association for Computational Linguistics.
Derek D. Monner and James A. Reggia. 2011. Sys-
tematically grounding language through vision in
a deep, recurrent neural network. In Proceed-
ings of the 4th international conference on Arti-
ficial general intelligence, AGI?11, pages 112?121,
Berlin, Heidelberg. Springer-Verlag.
Vicente Ordonez, Girish Kulkarni, and Tamara L.
Berg. 2011. Im2text: Describing images using 1
million captioned photographs. In Neural Infor-
mation Processing Systems (NIPS).
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In ACL.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, com-
pact, and interpretable tree annotation. In COL-
ING/ACL.
Deb K. Roy. 2002. Learning visually-grounded
words and syntax for a scene description task.
Computer Speech and Language, In review.
Wai-King Tsang. 1996. Comparing the effects of
reading and writing on writing performance. Ap-
plied Linguistics, 17(2):210?233.
Kristian Woodsend and Mirella Lapata. 2010. Au-
tomatic generation of story highlights. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 565?
574, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Kristian Woodsend, Yansong Feng, and Mirella
Lapata. 2010. Title generation with quasi-
synchronous grammar. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 513?523,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
367
Jianxiong Xiao, James Hays, Krista A. Ehinger,
Aude Oliva, and Antonio Torralba. 2010. Sun
database: Large-scale scene recognition from
abbey to zoo. In CVPR.
Yezhou Yang, Ching Teo, Hal Daume III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Proceedings of the
2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 444?454, Edin-
burgh, Scotland, UK., July. Association for Com-
putational Linguistics.
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2t: Image pars-
ing to text description. Proc. IEEE, 98(8).
368
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 171?175,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Syntactic Stylometry for Deception Detection
Song Feng Ritwik Banerjee Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
songfeng, rbanerjee, ychoi@cs.stonybrook.edu
Abstract
Most previous studies in computerized de-
ception detection have relied only on shal-
low lexico-syntactic patterns. This pa-
per investigates syntactic stylometry for
deception detection, adding a somewhat
unconventional angle to prior literature.
Over four different datasets spanning from
the product review to the essay domain,
we demonstrate that features driven from
Context Free Grammar (CFG) parse trees
consistently improve the detection perfor-
mance over several baselines that are based
only on shallow lexico-syntactic features.
Our results improve the best published re-
sult on the hotel review data (Ott et al,
2011) reaching 91.2% accuracy with 14%
error reduction.
1 Introduction
Previous studies in computerized deception de-
tection have relied only on shallow lexico-
syntactic cues. Most are based on dictionary-
based word counting using LIWC (Pennebaker
et al, 2007) (e.g., Hancock et al (2007), Vrij et
al. (2007)), while some recent ones explored the
use of machine learning techniques using sim-
ple lexico-syntactic patterns, such as n-grams
and part-of-speech (POS) tags (Mihalcea and
Strapparava (2009), Ott et al (2011)). These
previous studies unveil interesting correlations
between certain lexical items or categories with
deception that may not be readily apparent to
human judges. For instance, the work of Ott
et al (2011) in the hotel review domain results
in very insightful observations that deceptive re-
viewers tend to use verbs and personal pronouns
(e.g., ?I?, ?my?) more often, while truthful re-
viewers tend to use more of nouns, adjectives,
prepositions. In parallel to these shallow lexical
patterns, might there be deep syntactic struc-
tures that are lurking in deceptive writing?
This paper investigates syntactic stylometry
for deception detection, adding a somewhat un-
conventional angle to prior literature. Over four
different datasets spanning from the product re-
view domain to the essay domain, we find that
features driven from Context Free Grammar
(CFG) parse trees consistently improve the de-
tection performance over several baselines that
are based only on shallow lexico-syntactic fea-
tures. Our results improve the best published re-
sult on the hotel review data of Ott et al (2011)
reaching 91.2% accuracy with 14% error reduc-
tion. We also achieve substantial improvement
over the essay data of Mihalcea and Strapparava
(2009), obtaining upto 85.0% accuracy.
2 Four Datasets
To explore different types of deceptive writing,
we consider the following four datasets spanning
from the product review to the essay domain:
I. TripAdvisor?Gold: Introduced in Ott et
al. (2011), this dataset contains 400 truthful re-
views obtained from www.tripadviser.com and
400 deceptive reviews gathered using Amazon
Mechanical Turk, evenly distributed across 20
Chicago hotels.
171
TripAdvisor?Gold TripAdvisor?Heuristic
Deceptive Truthful Deceptive Truthful
NP?PP ? DT NNP NNP NNP S?ROOT ? VP . NP?S ? PRP VP?S ? VBZ NP
SBAR?NP ? S NP?NP ? $ CD SBAR?S ? WHADVP S NP?NP ? NNS
NP?VP ? NP SBAR PRN?NP ? LRB NP RRB VP?S ? VBD PP WHNP?SBAR ? WDT
NP?NP ? PRP$ NN NP?NP ? NNS S?SBAR ? NP VP NP?NP ? NP PP PP
NP?S ? DT NNP NNP NNP NP?S ? NN S?ROOT ? PP NP VP . NP?S ? EX
VP?S ? VBG PP NP?PP ? DT NNP VP?S ? VBD S NX?NX ? JJ NN
NP?PP ? PRP$ NN NP?PP ? CD NNS NP?S ? NP CC NP NP?NP ? NP PP
VP?S ? MD ADVP VP NP?NP ? NP PRN NP?S ? PRP$ NN VP?S ? VBZ RB NP
VP?S ? TO VP PRN?NP ? LRB PP RRB NP?PP ? DT NNP PP?NP ? IN NP
ADJP?NP ? RBS JJ NP?NP ? CD NNS NP?PP ? PRP$ NN PP?ADJP ? TO NP
Table 1: Most discriminative rewrite rules (r?): hotel review datasets
Figure 1: Parsed trees
II. TripAdvisor?Heuristic: This dataset
contains 400 truthful and 400 deceptive reviews
harvested from www.tripadviser.com, based
on fake review detection heuristics introduced
in Feng et al (2012).1
III. Yelp: This dataset is our own creation
using www.yelp.com. We collect 400 filtered re-
views and 400 displayed reviews for 35 Italian
restaurants with average ratings in the range of
[3.5, 4.0]. Class labels are based on the meta
data, which tells us whether each review is fil-
tered by Yelp?s automated review filtering sys-
tem or not. We expect that filtered reviews
roughly correspond to deceptive reviews, and
displayed reviews to truthful ones, but not with-
out considerable noise. We only collect 5-star
reviews to avoid unwanted noise from varying
1Specifically, using the notation of Feng et al (2012),
we use data created by Strategy-dist? heuristic, with
HS ,S as deceptive and H ?S , T as truthful.
degree of sentiment.
IV. Essays: Introduced in Mihalcea and
Strapparava (2009), this corpus contains truth-
ful and deceptive essays collected using Amazon
Mechanic Turk for the following three topics:
?Abortion? (100 essays per class), ?Best Friend?
(98 essays per class), and ?Death Penalty? (98
essays per class).
3 Feature Encoding
Words Previous work has shown that bag-of-
words are effective in detecting domain-specific
deception (Ott et al, 2011; Mihalcea and Strap-
parava, 2009). We consider unigram, bigram,
and the union of the two as features.
Shallow Syntax As has been used in many
previous studies in stylometry (e.g., Argamon-
Engelson et al (1998), Zhao and Zobel (2007)),
we utilize part-of-speech (POS) tags to encode
shallow syntactic information. Note that Ott
et al (2011) found that even though POS tags
are effective in detecting fake product reviews,
they are not as effective as words. Therefore, we
strengthen POS features with unigram features.
Deep syntax We experiment with four differ-
ent encodings of production rules based on the
Probabilistic Context Free Grammar (PCFG)
parse trees as follows:
? r: unlexicalized production rules (i.e., all
production rules except for those with ter-
minal nodes), e.g., NP2 ? NP3 SBAR.
? r?: lexicalized production rules (i.e., all
production rules), e.g., PRP ? ?you?.
? r?: unlexicalized production rules combined
with the grandparent node, e.g., NP2 ?VP
172
TripAdvisor Yelp Essay
Gold Heur Abort BstFr Death
unigram 88.4 74.4 59.9 70.0 77.0 67.4
words bigram 85.8 71.5 60.7 71.5 79.5 55.5
uni + bigram 89.6 73.8 60.1 72.0 81.5 65.5
pos(n=1) + unigram 87.4 74.0 62.0 70.0 80.0 66.5
shallow syntax pos(n=2) + unigram 88.6 74.6 59.0 67.0 82.0 66.5
+words pos(n=3) + unigram 88.6 74.6 59.3 67.0 82.0 66.5
r 78.5 65.3 56.9 62 67.5 55.5
deep syntax r? 74.8 65.3 56.5 58.5 65.5 56.0
r? 89.4 74.0 64.0 70.1 77.5 66.0
r?? 90.4 75 63.5 71.0 78 67.5
r + unigram 89.0 74.3 62.3 76.5 82.0 69.0
deep syntax r? + unigram 88.5 74.3 62.5 77.0 81.5 70.5
+words r? + unigram 90.3 75.4 64.3 74.0 85.0 71.5
r?? + unigram 91.2 76.6 62.1 76.0 84.5 71.0
Table 2: Deception Detection Accuracy (%).
1 ? NP3 SBAR.
? r??: lexicalized production rules (i.e., all
production rules) combined with the grand-
parent node, e.g., PRP?NP 4 ? ?you?.
4 Experimental Results
For all classification tasks, we use SVM classi-
fier, 80% of data for training and 20% for test-
ing, with 5-fold cross validation.2 All features
are encoded as tf-idf values. We use Berkeley
PCFG parser (Petrov and Klein, 2007) to parse
sentences. Table 2 presents the classification
performance using various features across four
different datasets introduced earlier.3
4.1 TripAdvisor?Gold
We first discuss the results for the TripAdvisor?
Gold dataset shown in Table 2. As reported in
Ott et al (2011), bag-of-words features achieve
surprisingly high performance, reaching upto
89.6% accuracy. Deep syntactic features, en-
coded as r?? slightly improves this performance,
achieving 90.4% accuracy. When these syntactic
features are combined with unigram features, we
attain the best performance of 91.2% accuracy,
2We use LIBLINEAR (Fan et al, 2008) with L2-
regulization, parameter optimized over the 80% training
data (3 folds for training, 1 fold for testing).
3Numbers in italic are classification results reported
in Ott et al (2011) and Mihalcea and Strapparava (2009).
yielding 14% error reduction over the word-only
features.
Given the power of word-based features, one
might wonder, whether the PCFG driven fea-
tures are being useful only due to their lexi-
cal production rules. To address such doubts,
we include experiments with unlexicalized rules,
r and r?. These features achieve 78.5% and
74.8% accuracy respectively, which are signifi-
cantly higher than that of a random baseline
(?50.0%), confirming statistical differences in
deep syntactic structures. See Section 4.4 for
concrete exemplary rules.
Another question one might have is whether
the performance gain of PCFG features are
mostly from local sequences of POS tags, indi-
rectly encoded in the production rules. Compar-
ing the performance of [shallow syntax+words]
and [deep syntax+words] in Table 2, we find sta-
tistical evidence that deep syntax based features
offer information that are not available in simple
POS sequences.
4.2 TripAdvisor?Heuristic & Yelp
The performance is generally lower than that of
the previous dataset, due to the noisy nature
of these datasets. Nevertheless, we find similar
trends as those seen in the TripAdvisor?Gold
dataset, with respect to the relative performance
differences across different approaches. The sig-
173
TripAdvisor?Gold TripAdvisor?Heur
Decep Truth Decep Truth
VP PRN VP PRN
SBAR QP WHADVP NX
WHADVP S SBAR WHNP
ADVP PRT WHADJP ADJP
CONJP UCP INTJ WHPP
Table 3: Most discriminative phrasal tags in PCFG
parse trees: TripAdvisor data.
nificance of these results comes from the fact
that these two datasets consists of real (fake)
reviews in the wild, rather than manufactured
ones that might invite unwanted signals that
can unexpectedly help with classification accu-
racy. In sum, these results indicate the exis-
tence of the statistical signals hidden in deep
syntax even in real product reviews with noisy
gold standards.
4.3 Essay
Finally in Table 2, the last dataset Essay con-
firms the similar trends again, that the deep syn-
tactic features consistently improve the perfor-
mance over several baselines based only on shal-
low lexico-syntactic features. The final results,
reaching accuracy as high as 85%, substantially
outperform what has been previously reported
in Mihalcea and Strapparava (2009). How ro-
bust are the syntactic cues in the cross topic set-
ting? Table 4 compares the results of Mihalcea
and Strapparava (2009) and ours, demonstrat-
ing that syntactic features achieve substantially
and surprisingly more robust results.
4.4 Discriminative Production Rules
To give more concrete insights, we provide
10 most discriminative unlexicalized production
rules (augmented with the grand parent node)
for each class in Table 1. We order the rules
based on the feature weights assigned by LIB-
LINEAR classifier. Notice that the two produc-
tion rules in bolds ? [SBAR?NP? S] and [NP
?VP? NP SBAR] ? are parts of the parse tree
shown in Figure 1, whose sentence is taken from
an actual fake review. Table 3 shows the most
discriminative phrasal tags in the PCFG parse
training: A & B A & D B & D
testing: DeathPen BestFrn Abortion
M&S 2009 58.7 58.7 62.0
r? 66.8 70.9 69.0
Table 4: Cross topic deception detection accuracy:
Essay data
trees for each class. Interestingly, we find more
frequent use of VP, SBAR (clause introduced
by subordinating conjunction), and WHADVP
in deceptive reviews than truthful reviews.
5 Related Work
Much of the previous work for detecting de-
ceptive product reviews focused on related, but
slightly different problems, e.g., detecting dupli-
cate reviews or review spams (e.g., Jindal and
Liu (2008), Lim et al (2010), Mukherjee et al
(2011), Jindal et al (2010)) due to notable dif-
ficulty in obtaining gold standard labels.4 The
Yelp data we explored in this work shares a sim-
ilar spirit in that gold standard labels are har-
vested from existing meta data, which are not
guaranteed to align well with true hidden la-
bels as to deceptive v.s. truthful reviews. Two
previous work obtained more precise gold stan-
dard labels by hiring Amazon turkers to write
deceptive articles (e.g., Mihalcea and Strappa-
rava (2009), Ott et al (2011)), both of which
have been examined in this study with respect
to their syntactic characteristics. Although we
are not aware of any prior work that dealt
with syntactic cues in deceptive writing directly,
prior work on hedge detection (e.g., Greene and
Resnik (2009), Li et al (2010)) relates to our
findings.
6 Conclusion
We investigated syntactic stylometry for decep-
tion detection, adding a somewhat unconven-
tional angle to previous studies. Experimental
results consistently find statistical evidence of
deep syntactic patterns that are helpful in dis-
criminating deceptive writing.
4It is not possible for a human judge to tell with full
confidence whether a given review is a fake or not.
174
References
S. Argamon-Engelson, M. Koppel, and G. Avneri.
1998. Style-based text categorization: What
newspaper am i reading. In Proc. of the AAAI
Workshop on Text Categorization, pages 1?4.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?
1874.
S. Feng, L. Xing, Gogar A., and Y. Choi. 2012.
Distributional footprints of deceptive product re-
views. In Proceedings of the 2012 International
AAAI Conference on WebBlogs and Social Media,
June.
S. Greene and P. Resnik. 2009. More than
words: Syntactic packaging and implicit senti-
ment. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 503?511. Asso-
ciation for Computational Linguistics.
J.T. Hancock, L.E. Curry, S. Goorha, and M. Wood-
worth. 2007. On lying and being lied to: A lin-
guistic analysis of deception in computer-mediated
communication. Discourse Processes, 45(1):1?23.
Nitin Jindal and Bing Liu. 2008. Opinion spam
and analysis. In Proceedings of the international
conference on Web search and web data mining,
WSDM ?08, pages 219?230, New York, NY, USA.
ACM.
Nitin Jindal, Bing Liu, and Ee-Peng Lim. 2010.
Finding unusual review patterns using unexpected
rules. In Proceedings of the 19th ACM Confer-
ence on Information and Knowledge Management,
pages 1549?1552.
X. Li, J. Shen, X. Gao, and X. Wang. 2010. Ex-
ploiting rich features for detecting hedges and
their scope. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning?Shared Task, pages 78?83. Association
for Computational Linguistics.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing
Liu, and Hady Wirawan Lauw. 2010. Detecting
product review spammers using rating behaviors.
In Proceedings of the 19th ACM international con-
ference on Information and knowledge manage-
ment, CIKM ?10, pages 939?948, New York, NY,
USA. ACM.
R. Mihalcea and C. Strapparava. 2009. The lie de-
tector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages
309?312. Association for Computational Linguis-
tics.
Arjun Mukherjee, Bing Liu, Junhui Wang, Natalie S.
Glance, and Nitin Jindal. 2011. Detecting group
review spam. In Proceedings of the 20th Interna-
tional Conference on World Wide Web (Compan-
ion Volume), pages 93?94.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, pages 309?319, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gon-
zales, and R.J. Booth. 2007. The development
and psychometric properties of liwc2007. Austin,
TX, LIWC. Net.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411.
A. Vrij, S. Mann, S. Kristen, and R.P. Fisher. 2007.
Cues to deception and ability to detect lies as a
function of police interview styles. Law and hu-
man behavior, 31(5):499?518.
Ying Zhao and Justin Zobel. 2007. Searching with
style: authorship attribution in classic literature.
In Proceedings of the thirtieth Australasian confer-
ence on Computer science - Volume 62, ACSC ?07,
pages 59?68, Darlinghurst, Australia, Australia.
Australian Computer Society, Inc.
175
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1774?1784,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Connotation Lexicon:
A Dash of Sentiment Beneath the Surface Meaning
Song Feng Jun Seok Kang Polina Kuznetsova Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
songfeng, junkang, pkuznetsova, ychoi@cs.stonybrook.edu
Abstract
Understanding the connotation of words
plays an important role in interpreting sub-
tle shades of sentiment beyond denotative
or surface meaning of text, as seemingly
objective statements often allude nuanced
sentiment of the writer, and even purpose-
fully conjure emotion from the readers?
minds. The focus of this paper is draw-
ing nuanced, connotative sentiments from
even those words that are objective on the
surface, such as ?intelligence?, ?human?,
and ?cheesecake?. We propose induction
algorithms encoding a diverse set of lin-
guistic insights (semantic prosody, distri-
butional similarity, semantic parallelism of
coordination) and prior knowledge drawn
from lexical resources, resulting in the first
broad-coverage connotation lexicon.
1 Introduction
There has been a substantial body of research
in sentiment analysis over the last decade (Pang
and Lee, 2008), where a considerable amount of
work has focused on recognizing sentiment that is
generally explicit and pronounced rather than im-
plied and subdued. However in many real-world
texts, even seemingly objective statements can be
opinion-laden in that they often allude nuanced
sentiment of the writer (Greene and Resnik, 2009),
or purposefully conjure emotion from the readers?
minds (Mohammad and Turney, 2010). Although
some researchers have explored formal and statis-
tical treatments of those implicit and implied sen-
timents (e.g. Wiebe et al (2005), Esuli and Sebas-
tiani (2006), Greene and Resnik (2009), Davidov
et al (2010)), automatic analysis of them largely
remains as a big challenge.
In this paper, we concentrate on understanding
the connotative sentiments of words, as they play
an important role in interpreting subtle shades of
sentiment beyond denotative or surface meaning
of text. For instance, consider the following:
Geothermal replaces oil-heating; it helps re-
ducing greenhouse emissions.1
Although this sentence could be considered as a
factual statement from the general standpoint, the
subtle effect of this sentence may not be entirely
objective: this sentence is likely to have an influ-
ence on readers? minds in regard to their opinion
toward ?geothermal?. In order to sense the subtle
overtone of sentiments, one needs to know that the
word ?emissions? has generally negative connota-
tion, which geothermal reduces. In fact, depend-
ing on the pragmatic contexts, it could be precisely
the intention of the author to transfer his opinion
into the readers? minds.
The main contribution of this paper is a broad-
coverage connotation lexicon that determines the
connotative polarity of even those words with ever
so subtle connotation beneath their surface mean-
ing, such as ?Literature?, ?Mediterranean?, and
?wine?. Although there has been a number of
previous work that constructed sentiment lexicons
(e.g., Esuli and Sebastiani (2006), Wilson et al
(2005a), Kaji and Kitsuregawa (2007), Qiu et
al. (2009)), which seem to be increasingly and
inevitably expanding over words with (strongly)
connotative sentiments rather than explicit senti-
ments alone (e.g., ?gun?), little prior work has di-
rectly tackled this problem of learning connota-
tion,2 and much of the subtle connotation of many
seemingly objective words is yet to be determined.
1Our learned lexicon correctly assigns negative polarity to
emission.
2A notable exception would be the work of Feng et al
1774
POSITIVE NEGATIVE
FEMA, Mandela, Intel, Google, Python, Sony, Pulitzer,
Harvard, Duke, Einstein, Shakespeare, Elizabeth, Clooney,
Hoover, Goldman, Swarovski, Hawaii, Yellowstone
Katrina, Monsanto, Halliburton, Enron, Teflon, Hi-
roshima, Holocaust, Afghanistan, Mugabe, Hutu, Sad-
dam, Osama, Qaeda, Kosovo, Helicobacter, HIV
Table 1: Example Named Entities (Proper Nouns) with Polar Connotation.
A central premise to our approach is that it is
collocational statistics of words that affect and
shape the polarity of connotation. Indeed, the ety-
mology of ?connotation? is from the Latin ?com-
? (?together or with?) and ?notare? (?to mark?).
It is important to clarify, however, that we do not
simply assume that words that collocate share the
same polarity of connotation. Although such an
assumption played a key role in previous work for
the analogous task of learning sentiment lexicon
(Velikovich et al, 2010), we expect that the same
assumption would be less reliable in drawing sub-
tle connotative sentiments of words. As one ex-
ample, the predicate ?cure?, which has a positive
connotation typically takes arguments with nega-
tive connotation, e.g., ?disease?, when used as the
?relieve? sense.3
Therefore, in order to attain a broad cover-
age lexicon while maintaining good precision, we
guide the induction algorithm with multiple, care-
fully selected linguistic insights: [1] distributional
similarity, [2] semantic parallelism of coordina-
tion, [3] selectional preference, and [4] seman-
tic prosody (e.g., Sinclair (1991), Louw (1993),
Stubbs (1995), Stefanowitsch and Gries (2003))),
and also exploit existing lexical resources as an ad-
ditional inductive bias.
We cast the connotation lexicon induction task
as a collective inference problem, and consider ap-
proaches based on three distinct types of algorith-
mic framework that have been shown successful
for conventional sentiment lexicon induction:
Random walk based on HITS/PageRank (e.g.,
Kleinberg (1999), Page et al (1999), Feng
et al (2011) Heerschop et al (2011),
Montejo-Ra?ez et al (2012))
Label/Graph propagation (e.g., Zhu and Ghahra-
(2011) but with practical limitations. See ?3 for detailed dis-
cussion.
3Note that when ?cure? is used as the ?preserve? sense, it
expects objects with non-negative connotation. Hence word-
sense-disambiguation (WSD) presents a challenge, though
not unexpectedly. In this work, we assume the general conno-
tation of each word over statistically prevailing senses, leav-
ing a more cautious handling of WSD as future work.
mani (2002), Velikovich et al (2010))
Constraint optimization (e.g., Roth and Yih
(2004), Choi and Cardie (2009), Lu et al
(2011)).
We provide comparative empirical results over
several variants of these approaches with compre-
hensive evaluations including lexicon-based, hu-
man judgments, and extrinsic evaluations.
It is worthwhile to note that not all words have
connotative meanings that are distinct from deno-
tational meanings, and in some cases, it can be dif-
ficult to determine whether the overall sentiment is
drawn from denotational or connotative meanings
exclusively, or both. Therefore, we encompass any
sentiment from either type of meanings into the
lexicon, where non-neutral polarity prevails over
neutral one if some meanings lead to neutral while
others to non-neutral.4
Our work results in the first broad-coverage
connotation lexicon,5 significantly improving both
the coverage and the precision of Feng et al
(2011). As an interesting by-product, our algo-
rithm can be also used as a proxy to measure the
general connotation of real-world named entities
based on their collocational statistics. Table 1
highlights some example proper nouns included in
the final lexicon.
The rest of the paper is structured as follows.
In ?2 we describe three types of induction algo-
rithms followed by evaluation in ?3. Then we re-
visit the induction algorithms based on constraint
optimization in ?4 to enhance quality and scala-
bility. ?5 presents comprehensive evaluation with
human judges and extrinsic evaluations. Related
work and conclusion are in ?6 and ?7.
4In general, polysemous words do not seem to have con-
flicting non-neutral polarities over different senses, though
there are many exceptions, e.g., ?heat?, or ?fine?. We treat
each word in each part-of-speech as a separate word to reduce
such cases, otherwise aim to learn the most prevalent polar-
ity in the corpus with respect to each part-of-speech of each
word.
5Available at http://www.cs.stonybrook.edu/
?ychoi/connotation.
1775
? 
Pred-Arg 
Arg-Arg 
pred-arg distr sim 
enjoy 
thank 
writing 
profit 
help 
investment 
aid 
reading 
Figure 1: Graph for Graph Propagation (?2.2).
? 
? 
synonyms antonyms 
prevent 
suffer  
enjoy 
thank 
tax  
loss 
writing 
profit 
preventing 
gain 
investment 
bonus  
pred-arg distr sim 
flu  
cold  
Figure 2: Graph for ILP/LP (?2.3, ?4.2).
2 Connotation Induction Algorithms
We develop induction algorithms based on three
distinct types of algorithmic framework that have
been shown successful for the analogous task of
sentiment lexicon induction: HITS & PageRank
(?2.1), Label/Graph Propagation (?2.2), and Con-
straint Optimization via Integer Linear Program-
ming (?2.3). As will be shown, each of these ap-
proaches will incorporate additional, more diverse
linguistic insights.
2.1 HITS & PageRank
The work of Feng et al (2011) explored the use
of HITS (Kleinberg, 1999) and PageRank (Page
et al, 1999) to induce the general connotation
of words hinging on the linguistic phenomena of
selectional preference and semantic prosody, i.e.,
connotative predicates influencing the connotation
of their arguments. For example, the object of
a negative connotative predicate ?cure? is likely
to have negative connotation, e.g., ?disease? or
?cancer?. The bipartite graph structure for this
approach corresponds to the left-most box (labeled
as ?pred-arg?) in Figure 1.
2.2 Label Propagation
With the goal of obtaining a broad-coverage lexi-
con in mind, we find that relying only on the struc-
ture of semantic prosody is limiting, due to rel-
atively small sets of connotative predicates avail-
able.6 Therefore, we extend the graph structure
as an overlay of two sub-graphs (Figure 1) as de-
scribed below:
6For connotative predicates, we use the seed predicate set
of Feng et al (2011), which comprises of 20 positive and 20
negative predicates.
Sub-graph #1: Predicate?Argument Graph
This sub-graph is the bipartite graph that encodes
the selectional preference of connotative predi-
cates over their arguments. In this graph, conno-
tative predicates p reside on one side of the graph
and their co-occurring arguments a reside on the
other side of the graph based on Google Web 1T
corpus.7 The weight on the edges between the
predicates p and arguments a are defined using
Point-wise Mutual Information (PMI) as follows:
w(p? a) := PMI(p, a) = log2
P (p, a)
P (p)P (a)
PMI scores have been widely used in previous
studies to measure association between words
(e.g., Turney (2001), Church and Hanks (1990)).
Sub-graph #2: Argument?Argument Graph
The second sub-graph is based on the distribu-
tional similarities among the arguments. One pos-
sible way of constructing such a graph is simply
connecting all nodes and assign edge weights pro-
portionate to the word association scores, such as
PMI, or distributional similarity. However, such a
completely connected graph can be susceptible to
propagating noise, and does not scale well over a
very large set of vocabulary.
We therefore reduce the graph connectivity by
exploiting semantic parallelism of coordination
(Bock (1986), Hatzivassiloglou and McKeown
7We restrict predicte-argument pairs to verb-object pairs
in this study. Note that Google Web 1T dataset consists of
n-grams upto n = 5. Since n-gram sequences are too short
to apply a parser, we extract verb-object pairs approximately
by matching part-of-speech tags. Empirically, when overlaid
with the second sub-graph, we found that it is better to keep
the connectivity of this sub-graph as uni-directional. That is,
we only allow edges to go from a predicate to an argument.
1776
POSITIVE NEGATIVE NEUTRAL
n. avatar, adrenaline, keynote, debut,
stakeholder, sunshine, cooperation
unbeliever, delay, shortfall, gun-
shot, misdemeanor, mutiny, rigor
header, mark, clothing, outline,
grid, gasoline, course, preview
v. handcraft, volunteer, party, ac-
credit, personalize, nurse, google
sentence, cough, trap, scratch, de-
bunk, rip, misspell, overcharge
state, edit, send, put, arrive, type,
drill, name, stay, echo, register
a. floral, vegetarian, prepared, age-
less, funded, contemporary
debilitating, impaired, swollen,
intentional, jarring, unearned
same, cerebral, west, uncut, auto-
matic, hydrated, unheated, routine
Table 2: Example Words with Learned Connotation: Nouns(n), Verbs(v), Adjectives(a).
(1997), Pickering and Branigan (1998)). In par-
ticular, we consider an undirected edge between a
pair of arguments a1 and a2 only if they occurred
together in the ?a1 and a2? or ?a2 and a1? coor-
dination, and assign edge weights as:
w(a1 ? a2) = CosineSim(??a1,??a2) =
??a1 ? ??a2
||??a1|| ||??a2||
where ??a1 and ??a2 are co-occurrence vectors for a1
and a2 respectively. The co-occurrence vector for
each word is computed using PMI scores with re-
spect to the top n co-occurring words.8 n (=50)
is selected empirically. The edge weights in two
sub-graphs are normalized so that they are in the
comparable range.9
Limitations of Graph-based Algorithms
Although graph-based algorithms (?2.1, ?2.2) pro-
vide an intuitive framework to incorporate various
lexical relations, limitations include:
1. They allow only non-negative edge weights.
Therefore, we can encode only positive (sup-
portive) relations among words (e.g., distri-
butionally similar words will endorse each
other with the same polarity), while miss-
ing on exploiting negative relations (e.g.,
antonyms may drive each other into the op-
posite polarity).
2. They induce positive and negative polarities
in isolation via separate graphs. However, we
expect that a more effective algorithm should
induce both polarities simultaneously.
3. The framework does not readily allow incor-
porating a diverse set of soft and hard con-
straints.
8We discard edges with cosine similarity ? 0, as those
indicate either independence or the opposite of similarity.
9Note that cosine similarity does not make sense for the
first sub-graph as there is no reason why a predicate and an ar-
gument should be distributionally similar. We experimented
with many different variations on the graph structure and
edge weights, including ones that include any word pairs that
occurred frequently enough together. For brevity, we present
the version that achieved the best results here.
2.3 Constraint Optimization
Addressing limitations of graph-based algorithms
(?2.2), we propose an induction algorithm based
on Integer Linear Programming (ILP). Figure 2
provides the pictorial overview. In comparison to
Figure 1, two new components are: (1) dictionary-
driven relations targeting enhanced precision, and
(2) dictionary-driven words (i.e., unseen words
with respect to those relations explored in Figure
1) targeting enhanced coverage. We formulate in-
sights in Figure 2 using ILP as follows:
Definition of sets of words:
1. P+: the set of positive seed predicates.
P?: the set of negative seed predicates.
2. S: the set of seed sentiment words.
3. Rsyn: word pairs in synonyms relation.
Rant: word pairs in antonyms relation.
Rcoord: word pairs in coordination relation.
Rpred: word pairs in pred-arg relation.
Rpred+(?) : Rpred based on P+ (P?).
Definition of variables: For each word i, we
define binary variables xi, yi, zi ? {0, 1}, where
xi = 1 (yi = 1, zi = 1) if and only if i has a pos-
itive (negative, neutral) connotation respectively.
For every pair of word i and j, we define binary
variables dpqij where p, q ? {+,?, 0} and dpqij = 1
if and only if the polarity of i and j are p and q
respectively.
Objective function: We aim to maximize:
F = ?prosody + ?coord + ?neu
where ?prosody is the scores based on semantic
prosody, ?coord captures the distributional similar-
ity over coordination, and ?neu controls the sen-
sitivity of connotation detection between positive
(negative) and neutral. In particular,
?prosody =
Rpred?
i,j
wpredi,j (d++i,j + d??i,j ? d+?i,j ? d?+i,j )
?coord =
Rcoord?
i,j
wcoordi,j (d++i,j + d??i,j + d00i,j)
1777
?neu = ?
Rpred?
i,j
wpredi,j ? zj
Soft constraints (edge weights): The weights in
the objective function are set as follows:
wpred(p, a) = freq(p, a)?
(p,x)?Rpred
freq(p, x)
wcoord(a1, a2) = CosSim(??a1,??a2) =
??a1 ? ??a2
||??a1|| ||??a2||
Note that the same wcoord(a1, a2) has been used
in graph propagation described in Section 2.2. ?
controls the sensitivity of connotation detection
such that higher value of ? will promote neutral
connotation over polar ones.
Hard constrains for variable consistency:
1. Each word i has one of {+,?, ?} as polarity:
?i, xi + yi + zi = 1
2. Variable consistency between dpqij and
xi, yi, zi:
xi + xj ? 1 ? 2d++i,j ? xi + xj
yi + yj ? 1 ? 2d??i,j ? yi + yj
zi + zj ? 1 ? 2d00i,j ? zi + zj
xi + yj ? 1 ? 2d+?i,j ? xi + yj
yi + xj ? 1 ? 2d?+i,j ? yi + xj
Hard constrains for WordNet relations:
1. Cant: Antonym pairs will not have the same
positive or negative polarity:
?(i, j) ? Rant, xi + xj ? 1, yi + yj ? 1
For this constraint, we only consider
antonym pairs that share the same root, e.g.,
?sufficient? and ?insufficient?, as those pairs
are more likely to have the opposite polarities
than pairs without sharing the same root, e.g.,
?east? and ?west?.
2. Csyn: Synonym pairs will not have the oppo-
site polarity:
?(i, j) ? Rsyn, xi + yj ? 1, xj + yi ? 1
3 Experimental Result I
We provide comprehensive comparisons over vari-
ants of three types of algorithms proposed in ?2.
We use the Google Web 1T data (Brants and Franz
(2006)), and POS-tagged ngrams using Stanford
POS Tagger (Toutanova and Manning (2000)). We
filter out the ngrams with punctuations and other
special characters to reduce the noise.
3.1 Comparison against Conventional
Sentiment Lexicon
Note that we consider the connotation lexicon to
be inclusive of a sentiment lexicon for two prac-
tical reasons: first, it is highly unlikely that any
word with non-neutral sentiment (i.e., positive or
negative) would carry connotation of the oppo-
site, i.e., conflicting10 polarity. Second, for some
words with distinct sentiment or strong connota-
tion, it can be difficult or even unnatural to draw a
precise distinction between connotation and senti-
ment, e.g., ?efficient?. Therefore, sentiment lexi-
cons can serve as a surrogate to measure a subset
of connotation words induced by the algorithms,
as shown in Table 3 with respect to General In-
quirer (Stone and Hunt (1963)) and MPQA (Wil-
son et al (2005b)).11
Discussion Table 3 shows the agreement statis-
tics with respect to two conventional sentiment
lexicons. We find that the use of label propaga-
tion alone [PRED-ARG (CP)] improves the per-
formance substantially over the comparable graph
construction with different graph analysis algo-
rithms, in particular, HITS and PageRank ap-
proaches of Feng et al (2011). The two com-
pletely connected variants of the graph propa-
gation on the Pred-Arg graph, [? PRED-ARG
(PMI)] and [? PRED-ARG (CP)], do not neces-
sarily improve the performance over the simpler
and computationally lighter alternative, [PRED-
ARG (CP)]. The [OVERLAY], which is based
on both Pred-Arg and Arg-Arg subgraphs (?2.2),
achieves the best performance among graph-based
algorithms, significantly improving the precision
over all other baselines. This result suggests:
1 The sub-graph #2, based on the semantic par-
allelism of coordination, is simple and yet
very powerful as an inductive bias.
2 The performance of graph propagation varies
significantly depending on the graph topol-
ogy and the corresponding edge weights.
Note that a direct comparison against ILP for top
N words is tricky, as ILP does not rank results.
Only for comparison purposes however, we assign
10We consider ?positive? and ?negative? polarities conflict,
but ?neutral? polarity does not conflict with any.
11In the case of General Inquirer, we use words in POSITIV
and NEGATIV sets as words with positive and negative labels
respectively.
1778
GENINQ EVAL MPQA EVAL
100 1,000 5,000 10,000 ALL 100 1,000 5,000 10,000 ALL
ILP 97.6 94.5 84.5 80.8 80.4 98.0 89.7 84.6 81.2 78.4
OVERLAY 97.0 95.1 78.8 (78.3) 78.3 98.0 93.4 82.1 77.7 77.7? PRED-ARG (PMI) 91.0 91.4 76.1 (76.1) 76.1 88.0 89.1 78.8 75.1 75.1?PRED-ARG (CP) 88.0 85.4 76.2 (76.2) 76.2 87.0 82.6 78.0 76.3 76.3
PRED-ARG (CP) 91.0 91.0 81.0 (81.0) 81.0 88.0 91.5 80.0 78.3 78.3
HITS-ASYMT 77.0 68.8 - - 66.5 86.3 81.3 - - 72.2
PAGERANK-ASYMF 77.0 68.5 - - 65.7 87.2 80.3 - - 72.3
Table 3: Evaluation of Induction Algorithms (?2) with respect to Sentiment Lexicons (precision%).
ranks based on the frequency of words for ILP. Be-
cause of this issue, the performance of top ?1k
words of ILP should be considered only as a con-
servative measure. Importantly, when evaluated
over more than top 5k words, ILP is overall the
top performer considering both precision (shown
in Table 3) and coverage (omitted for brevity).12
4 Precision, Coverage, and Efficiency
In this section, we address three important aspects
of an ideal induction algorithm: precision, cover-
age, and efficiency. For brevity, the remainder of
the paper will focus on the algorithms based on
constraint optimization, as it turned out to be the
most effective one from the empirical results in ?3.
Precision In order to see the effectiveness of the
induction algorithms more sharply, we had used a
limited set of seed words in ?3. However to build a
lexicon with substantially enhanced precision, we
will use as a large seed set as possible, e.g., entire
sentiment lexicons13.
Broad coverage Although statistics in Google
1T corpus represent a very large amount of text,
words that appear in pred-arg and coordination re-
lations are still limited. To substantially increase
the coverage, we will leverage dictionary words
(that are not in the corpus) as described in ?2.3
and Figure 2.
Efficiency One practical problem with ILP is ef-
ficiency and scalability. In particular, we found
that it becomes nearly impractical to run the ILP
formulation including all words in WordNet plus
all words in the argument position in Google Web
1T. We therefore explore an alternative approach
based on Linear Programming in what follows.
12In fact, the performance of PRED-ARG variants for top
10K w.r.t. GENINQ is not meaningful as no additional word
was matched beyond top 5k words.
13Note that doing so will prevent us from evaluating
against the same sentiment lexicon used as a seed set.
4.1 Induction using Linear Programming
One straightforward option for Linear Program-
ming formulation may seem like using the same
Integer Linear Programming formulation intro-
duced in ?2.3, only changing the variable defini-
tions to be real values ? [0, 1] rather than integers.
However, because the hard constraints in ?2.3 are
defined based on the assumption that all the vari-
ables are binary integers, those constraints are not
as meaningful when considered for real numbers.
Therefore we revise those hard constraints to en-
code various semantic relations (WordNet and se-
mantic coordination) more directly.
Definition of variables: For each word i, we de-
fine variables xi, yi, zi ? [0, 1]. i has a positive
(negative) connotation if and only if the xi (yi) is
assigned the greatest value among the three vari-
ables; otherwise, i is neutral.
Objective function: We aim to maximize:
F = ?prosody + ?coord + ?syn + ?ant + ?neu
?prosody =
Rpred+?
i,j
wpred
+
i,j ? xj +
Rpred??
i,j
wpred
?
i,j ? yj
?coord =
Rcoord?
i,j
wcoordi,j ? (dc++i,j + dc??i,j )
?syn = W syn
Rsyn?
i,j
(ds++i,j + ds??i,j )
?ant = W ant
Rant?
i,j
(da++i,j + da??i,j )
?neu = ?
Rpred?
i,j
wpredi,j ? zj
Hard constraints We add penalties to the
objective function if the polarity of a pair of words
is not consistent with its corresponding semantic
relations. For example, for synonyms i and j, we
introduce a penalty W syn (a positive constant) for
ds++i,j , ds??i,j ? [?1, 0], where we set the upper
bound of ds++i,j (ds??i,j ) as the signed distance of
1779
FORMULA POSITIVE NEGATIVE ALLR P F R P F R P F
ILP ?prosody + Csyn + Cant 51.4 85.7 64.3 44.7 87.9 59.3 48.0 86.8 61.8
?prosody + Csyn + Cant + CS 61.2 93.3 73.9 52.4 92.2 66.8 56.8 92.8 70.5
?prosody + ?coord + Csyn + Cant 67.3 75.0 70.9 53.7 84.4 65.6 60.5 79.7 68.8
?prosody + ?coord + Csyn + Cant + CS 62.2 96.0 75.5 51.5 89.5 65.4 56.9 92.8 70.5
LP ?prosody + ?syn + ?ant 24.4 76.0 36.9 23.6 78.8 36.3 24.0 77.4 36.6
?prosody + ?syn + ?ant + ?S 71.6 87.8 78.9 68.8 84.6 75.9 70.2 86.2 77.4
?prosody + ?coord + ?syn + ?ant 67.9 92.6 78.3 64.6 89.1 74.9 66.3 90.8 76.6
?prosody + ?coord + ?syn + ?ant + ?S 78.6 90.5 84.1 73.3 87.1 79.6 75.9 88.8 81.8
Table 4: ILP/LP Comparison on MQPA? (%).
xi and xj (yi and yj) as shown below:
For (i, j) ? Rsyn,
ds++i,j ? xi ? xj , ds++i,j ? xj ? xi
ds??i,j ? yi ? yj , ds??i,j ? yj ? yi
Notice that ds++i,j , ds??i,j satisfying above inequal-
ities will be always of negative values, hence in
order to maximize the objective function, the LP
solver will try to minimize the absolute values of
ds++i,j , ds??i,j , effectively pushing i and j toward
the same polarity. Constraints for semantic coor-
dination Rcoord can be defined similarly. Lastly,
following constraints encode antonym relations:
For (i, j) ? Rant ,
da++i,j ? xi ? (1? xj), da++i,j ? (1? xj)? xi
da??i,j ? yi ? (1? yj), da??i,j ? (1? yj)? yi
Interpretation Unlike ILP, some of the vari-
ables result in fractional values. We consider a
word has positive or negative polarity only if the
assignment indicates 1 for the corresponding po-
larity and 0 for the rest. In other words, we treat
all words with fractional assignments over differ-
ent polarities as neutral. Because the optimal so-
lutions of LP correspond to extreme points in the
convex polytope formed by the constraints, we ob-
tain a large portion of words with non-fractional
assignments toward non-neutral polarities. Alter-
natively, one can round up fractional values.
4.2 Empirical Comparisons: ILP v.s. LP
To solve the ILP/LP, we run ILOG CPLEX Opti-
mizer (CPLEX, 2009)) on a 3.5GHz 6 core CPU
machine with 96GB RAM. Efficiency-wise, LP
runs within 10 minutes while ILP takes several
hours. Table 4 shows the results evaluated against
MPQA for different variations of ILP and LP.
We find that LP variants much better recall and
F-score, while maintaining comparable precision.
Therefore, we choose the connotation lexicon by
LP (C-LP) in the following evaluations in ?5.
5 Experimental Results II
In this section, we present comprehensive intrin-
sic ?5.1 and extrinsic ?5.2 evaluations comparing
three representative lexicons from ?2 & ?4: C-
LP, OVERLAY, PRED-ARG (CP), and two popular
sentiment lexicons: SentiWordNet (Baccianella et
al., 2010) and GI+MPQA.14 Note that C-LP is the
largest among all connotation lexicons, including
?70,000 polar words.15
5.1 Intrinsic Evaluation: Human Judgements
We evaluate 4000 words16 using Amazon Me-
chanical Turk (AMT). Because we expect that
judging a connotation can be dependent on one?s
cultural background, personality and value sys-
tems, we gather judgements from 5 people for
each word, from which we hope to draw a more
general judgement of connotative polarity. About
300 unique Turkers participated the evaluation
tasks. We gather gold standard only for those
words for which more than half of the judges
agreed on the same polarity. Otherwise we treat
them as ambiguous cases.17 Figure 3 shows a part
of the AMT task, where Turkers are presented with
questions that help judges to determine the subtle
connotative polarity of each word, then asked to
rate the degree of connotation on a scale from -
5 (most negative) and 5 (most positive). To draw
14GI+MPQA is the union of General Inquirer and MPQA.
The GI, we use words in the ?Positiv? & ?Negativ? set. For
SentiWordNet, to retrieve the polarity of a given word, we
sum over the polarity scores over all senses, where positive
(negative) values correspond to positive (negative) polarity.
15?13k adj, ?6k verbs, ?28k nouns, ?22k proper nouns.
16We choose words that are not already in GI+MPQA and
obtain most frequent 10,000 words based on the unigram fre-
quency in Google-Ngram, then randomly select 4000 words.
17We allow Turkers to mark words that can be used with
both positive and negative connotation, which results in about
7% of words that are excluded from the gold standard set.
1780
Figure 3: A Part of AMT Task Design.
YES NO
QUESTION % Avg % Avg
?Enjoyable or pleasant? 43.3 2.9 16.3 -2.4
?Of a good quality? 56.7 2.5 6.1 -2.7
?Respectable / honourable? 21.0 3.3 14.0 -1.1
?Would like to do or have? 52.5 2.8 11.5 -2.4
Table 5: Distribution of Answers from AMT.
the gold standard, we consider two different voting
schemes:
? ?V ote: The judgement of each Turker is
mapped to neutral for ?1 ? score ? 1, pos-
itive for score ? 2, negative for score ? 2,
then we take the majority vote.
? ?Score: Let ?(i) be the sum (weighted vote)
of the scores given by 5 judges for word i.
Then we determine the polarity label l(i) of i
as:
l(i) =
?
?
?
positive if ?(i) > 1
negative if ?(i) < ?1
neutral if ?1 ? ?(i) ? 1
The resulting distribution of judgements is shown
in Table 5 & 6. Interestingly, we observe
that among the relatively frequently used English
words, there are overwhelmingly more positively
connotative words than negative ones.
In Table 7, we show the percentage of words
with the same label over the mutual words by the
two lexicon. The highest agreement is 77% by
C-LP and the gold standard by AMTV ote. How
good is this? It depends on what is the natural de-
gree of agreement over subtle connotation among
people. Therefore, we also report the degree of
agreement among human judges in Table 7, where
we compute the agreement of one Turker with re-
spect to the gold standard drawn from the rest of
the Turkers, and take the average across over all
five Turkers18. Interestingly, the performance of
18In order to draw the gold standard from the 4 remaining
Turkers, we consider adjusted versions of ?V ote and ?Score
schemes described above.
POS NEG NEU UNDETERMINED
?V ote 50.4 14.6 24.1 10.9
?Score 67.9 20.6 11.5 n/a
Table 6: Distribution of Connotative Polarity from
AMT.
C-LP SENTIWN HUMAN JUDGES
?V ote 77.0 71.5 66.0
?Score 73.0 69.0 69.0
Table 7: Agreement (Accuracy) against AMT-
driven Gold Standard.
Turkers is not as good as that of C-LP lexicon. We
conjecture that this could be due to generally vary-
ing perception of different people on the connota-
tive polarity,19 while the corpus-driven induction
algorithms focus on the general connotative po-
larity corresponding to the most prevalent senses
of words in the corpus.
5.2 Extrinsic Evaluation
We conduct lexicon-based binary sentiment clas-
sification on the following two corpora.
SemEval From the SemEval task, we obtain a
set of news headlines with annotated scores (rang-
ing from -100 to 87). The positive/negative scores
indicate the degree of positive/negative polarity
orientation. We construct several sets of the posi-
tive and negative texts by setting thresholds on the
scores as shown in Table 8. ?? n? indicates that
the positive set consists of the texts with scores
? n and the negative set consists of the texts with
scores ? ?n.
Emoticon tweets The sentiment Twitter data20
consists of tweets containing either a smiley
emoticon (positive sentiment) or a frowny emoti-
con (negative sentiment). We filter out the tweets
with question marks or more than 30 words, and
keep the ones with at least two words in the union
of all polar words in the five lexicons in Table 8,
and then randomly select 10000 per class.
We denote the short text (e.g., content of tweets
or headline texts from SemEval) by t. w repre-
sents the word in t. W+/W? is the set of posi-
19Pearson correlation coefficient among turkers is 0.28,
which corresponds to a positive small to medium correlation.
Note that when the annotation of turkers is aggregated, we
observe agreement as high as 77% with respect to the learned
connotation lexicon.
20http://www.stanford.edu/?alecmgo/
cs224n/twitterdata.2009.05.25.c.zip
1781
DATA
LEXICON TWEET SEMEVAL
?20 ?40 ?60 ?80
C-LP 70.1 70.8 74.6 80.8 93.5
OVERLAY 68.5 70.0 72.9 76.8 89.6
PRED-ARG (CP) 60.5 64.2 69.3 70.3 79.2
SENTIWN 67.4 61.0 64.5 70.5 79.0
GI+MPQA 65.0 64.5 69.0 74.0 80.5
Table 8: Accuracy on Sentiment Classification
(%).
tive/negative words of the lexicon. We define the
weight of w as s(w). If w is adjective, s(w) = 2;
otherwise s(w) = 1. Then the polarity of each text
is determined as follows:
pol(t) =
?
????
????
positive if
W+?
w?t
s(w) ?
W??
w?t
s(w)
negative if
W+?
w?t
s(w) <
W??
w?t
s(w)
As shown in Table 8, C-LP generally performs
better than the other lexicons on both corpora.
Considering that only very simple classification
strategy is applied, the result by the connotation
lexicon is quite promising.
Finally, Table 1 highlights interesting exam-
ples of proper nouns with connotative polarity,
e.g., ?Mandela?, ?Google?, ?Hawaii? with pos-
itive connotation, and ?Monsanto?, ?Hallibur-
ton?, ?Enron? with negative connotation, sug-
gesting that our algorithms could potentially serve
as a proxy to track the general connotation of real
world entities. Table 2 shows example common
nouns with connotative polarity.
5.3 Practical Remarks on WSD and MWEs
In this work we aim to find the polarity of most
prevalent senses of each word, in part because it
is not easy to perform unsupervised word sense
disambiguation (WSD) on a large corpus in a reli-
able way, especially when the corpus consists pri-
marily of short n-grams. Although the resulting
lexicon loses on some of the polysemous words
with potentially opposite polarities, per-word con-
notation (rather than per-sense connotation) does
have a practical value: it provides a convenient
option for users who wish to avoid the burden of
WSD before utilizing the lexicon. Future work in-
cludes handling of WSD and multi-word expres-
sions (MWEs), e.g., ?Great Leader? (for Kim
Jong-Il), ?Inglourious Basterds? (a movie title).21
21These examples credit to an anonymous reviewer.
6 Related Work
A very interesting work of Mohammad and Tur-
ney (2010) uses Mechanical Turk in order to build
the lexicon of emotions evoked by words. In con-
trast, we present an automatic approach that in-
fers the general connotation of words. Velikovich
et al (2010) use graph propagation algorithms for
constructing a web-scale polarity lexicon for sen-
timent analysis. Although we employ the same
graph propagation algorithm, our graph construc-
tion is fundamentally different in that we integrate
stronger inductive biases into the graph topology
and the corresponding edge weights. As shown
in our experimental results, we find that judicious
construction of graph structure, exploiting multi-
ple complementing linguistic phenomena can en-
hance both the performance and the efficiency of
the algorithm substantially. Other interesting ap-
proaches include one based on min-cut (Dong et
al., 2012) or LDA (Xie and Li, 2012). Our pro-
posed approaches are more suitable for encoding
a much diverse set of linguistic phenomena how-
ever. But our work use a few seed predicates with
selectional preference instead of relying on word
similarity. Some recent work explored the use
of constraint optimization framework for inducing
domain-dependent sentiment lexicon (Choi and
Cardie (2009), Lu et al (2011)). Our work dif-
fers in that we provide comprehensive insights into
different formulations of ILP and LP, aiming to
learn the much different task of learning the gen-
eral connotation of words.
7 Conclusion
We presented a broad-coverage connotation lexi-
con that determines the subtle nuanced sentiment
of even those words that are objective on the sur-
face, including the general connotation of real-
world named entities. Via a comprehensive eval-
uation, we provided empirical insights into three
different types of induction algorithms, and pro-
posed one with good precision, coverage, and effi-
ciency.
Acknowledgments
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research. We thank reviewers for many insightful
comments and suggestions, and for providing us
with several very inspiring examples to work with.
1782
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
J. Kathryn Bock. 1986. Syntactic persistence
in language production. Cognitive psychology,
18(3):355?387.
Thorsten Brants and Alex Franz. 2006. {Web 1T 5-
gram Version 1}.
Yejin Choi and Claire Cardie. 2009. Adapting a po-
larity lexicon using integer linear programming for
domain-specific sentiment classification. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2 -
Volume 2, EMNLP ?09, pages 590?598, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16:22?29, March.
ILOG CPLEX. 2009. High-performance software for
mathematical programming and optimization. U RL
http://www.ilog.com/products/cplex.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the
Fourteenth Conference on Computational Natural
Language Learning, CoNLL ?10, pages 107?116,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Xishuang Dong, Qibo Zou, and Yi Guan. 2012. Set-
similarity joins based semi-supervised sentiment
analysis. In Neural Information Processing, pages
176?183. Springer.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC06), pages 417?422.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 1092?1103. Association for Computa-
tional Linguistics.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 503?511, Boulder, Colorado, June.
Association for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the eighth conference on
European chapter of the Association for Computa-
tional Linguistics, pages 174?181. Association for
Computational Linguistics.
Bas Heerschop, Alexander Hogenboom, and Flavius
Frasincar. 2011. Sentiment lexicon creation from
lexical resources. In Business Information Systems,
pages 185?196. Springer.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive col-
lection of html documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. JOURNAL OF THE ACM,
46(5):604?632.
Bill Louw. 1993. Irony in the text or insincerity in
the writer. Text and technology: In honour of John
Sinclair, pages 157?176.
Yue Lu, Malu Castellanos, Umeshwar Dayal, and
ChengXiang Zhai. 2011. Automatic construction
of a context-aware sentiment lexicon: an optimiza-
tion approach. In Proceedings of the 20th interna-
tional conference on World wide web, pages 347?
356. ACM.
Saif Mohammad and Peter Turney. 2010. Emotions
evoked by common words and phrases: Using me-
chanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 26?34, Los Angeles,
CA, June. Association for Computational Linguis-
tics.
Arturo Montejo-Ra?ez, Eugenio Mart??nez-Ca?mara,
M. Teresa Mart??n-Valdivia, and L. Alfonso Uren?a
Lo?pez. 2012. Random walk weighting over sen-
tiwordnet for sentiment polarity detection on twit-
ter. In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 3?10, Jeju, Korea, July. Association
for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135.
Martin J Pickering and Holly P Branigan. 1998. The
representation of verbs: Evidence from syntactic
priming in language production. Journal of Mem-
ory and Language, 39(4):633?651.
1783
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. Defense Technical Information Center.
John Sinclair. 1991. Corpus, concordance, colloca-
tion. Describing English language. Oxford Univer-
sity Press.
Anatol Stefanowitsch and Stefan Th Gries. 2003. Col-
lostructions: Investigating the interaction of words
and constructions. International journal of corpus
linguistics, 8(2):209?243.
Philip J. Stone and Earl B. Hunt. 1963. A computer
approach to content analysis: studies using the gen-
eral inquirer system. In Proceedings of the May 21-
23, 1963, spring joint computer conference, AFIPS
?63 (Spring), pages 241?256, New York, NY, USA.
ACM.
Michael Stubbs. 1995. Collocations and semantic pro-
files: on the cause of the trouble with quantitative
studies. Functions of language, 2(1):23?55.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in
a maximum entropy part-of-speech tagger. In In
EMNLP/VLC 2000, pages 63?70.
Peter Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The via-
bility of web-derived polarity lexicons. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Eval-
uation (formerly Computers and the Humanities),
39(2/3):164?210.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: a system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP on
Interactive Demonstrations, pages 34?35, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ?05: Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 347?354, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Rui Xie and Chunping Li. 2012. Lexicon construc-
tion: A topic model approach. In Systems and Infor-
matics (ICSAI), 2012 International Conference on,
pages 2299?2303. IEEE.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
1784
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 790?796,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Generalizing Image Captions for Image-Text Parallel Corpus
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg and Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{pkuznetsova,vordonezroma,aberg,tlberg,ychoi}@cs.stonybrook.edu
Abstract
The ever growing amount of web images
and their associated texts offers new op-
portunities for integrative models bridging
natural language processing and computer
vision. However, the potential benefits of
such data are yet to be fully realized due
to the complexity and noise in the align-
ment between image content and text. We
address this challenge with contributions
in two folds: first, we introduce the new
task of image caption generalization, for-
mulated as visually-guided sentence com-
pression, and present an efficient algo-
rithm based on dynamic beam search with
dependency-based constraints. Second,
we release a new large-scale corpus with
1 million image-caption pairs achieving
tighter content alignment between images
and text. Evaluation results show the in-
trinsic quality of the generalized captions
and the extrinsic utility of the new image-
text parallel corpus with respect to a con-
crete application of image caption transfer.
1 Introduction
The vast number of online images with accom-
panying text raises hope for drawing synergistic
connections between human language technolo-
gies and computer vision. However, subtleties and
complexity in the relationship between image con-
tent and text make exploiting paired visual-textual
data an open and interesting problem.
Some recent work has approached the prob-
lem of composing natural language descriptions
for images by using computer vision to retrieve
images with similar content and then transferring
?A house being 
pulled by a boat.? 
?I saw her in the light 
of her reading lamp 
and sneaked back to 
her door with the 
camera.? 
?Sections of the 
bridge sitting in the 
Dyer Construction 
yard south of 
Cabelas Driver.? 
Circumstantial 
information that is not 
visually present 
Visually relevant, 
but with overly 
extraneous details 
Visually truthful, 
but for an uncommon 
situation 
Figure 1: Examples of captions that are not readily
applicable to other visually similar images.
text from the retrieved samples to the query im-
age (e.g. Farhadi et al (2010), Ordonez et al
(2011), Kuznetsova et al (2012)). Other work
(e.g. Feng and Lapata (2010a), Feng and Lapata
(2010b)) uses computer vision to bias summariza-
tion of text associated with images to produce de-
scriptions. All of these approaches rely on ex-
isting text that describes visual content, but many
times existing image descriptions contain signifi-
cant amounts of extraneous, non-visual, or other-
wise non-desirable content. The goal of this paper
is to develop techniques to automatically clean up
visually descriptive text to make it more directly
usable for applications exploiting the connection
between images and language.
As a concrete example, consider the first image
in Figure 1. This caption was written by the photo
owner and therefore contains information related
to the context of when and where the photo was
taken. Objects such as ?lamp?, ?door?, ?camera?
are not visually present in the photo. The second
image shows a similar but somewhat different is-
sue. Its caption describes visible objects such as
?bridge? and ?yard?, but ?Cabelas Driver? are
overly specific and not visually detectable. The
790
Dependency Constraints with Examples Additional Dependency ConstraintsConstraints Sentence Dependency
advcl*(?) Taken when it was running... taken?running acomp*(?), advmod(?), agent*(?), attr(?)
amod(?) A wooden chair in the living room chair? wooden auxpass(?), cc*(?),complm(?), cop*(?)
aux(?) This crazy dog was jumping... jumping?was csubj*/csubjpass*(?),expl(?), mark*(?)
ccomp*(?) I believe a bear was in the box... believe?was infmod*(?), mwe(?), nsubj*/nsubjpass*(?)
prep(?) A view from the balcony view?from npadvmod(?), nn(?), conj*(?), num*(?)
det(?) A cozy street cafe... cafe?A number(?), parataxis(?),?
dobj*(?) A curious cow surveys the road... surveys?road partmod*(?), pcomp*(?), purpcl*(?)
iobj*(?) ...rock gives the water the color gives?water possessive(?), preconj*(?), predet*(?)
neg(?) Not a cloud in the sky... cloud?Not prt(?), quantmod(?), rcmod(?), ref(?)
pobj*(?) This branch was on the ground... on?ground rel*(?), tmod*(?), xcomp*(?), xsubj(?)
Table 1: Dependency-based Constraints
text of the third image, ?A house being pulled by a
boat?, pertains directly to the visual content of the
image, but is unlikely to be useful for tasks such as
caption transfer because the depiction is unusual.1
This phenomenon of information gap between the
visual content of the images and their correspond-
ing narratives has been studied closely by Dodge
et al (2012).
The content misalignment between images and
text limits the extent to which visual detectors
can learn meaningful mappings between images
and text. To tackle this challenge, we introduce
the new task of image caption generalization that
rewrites captions to be more visually relevant and
more readily applicable to other visually similar
images. Our end goal is to convert noisy image-
text pairs in the wild (Ordonez et al, 2011) into
pairs with tighter content alignment, resulting in
new simplified captions over 1 million images.
Evaluation results show both the intrinsic quality
of the generalized captions and the extrinsic util-
ity of the new image-text parallel corpus. The new
parallel corpus will be made publicly available.2
2 Sentence Generalization as Constraint
Optimization
Casting the generalization task as visually-guided
sentence compression with lightweight revisions,
we formulate a constraint optimization problem
that aims to maximize content selection and lo-
cal linguistic fluency while satisfying constraints
driven from dependency parse trees. Dependency-
based constraints guide the generalized caption
1Open domain computer vision remains to be an open
problem, and it would be difficult to reliably distinguish pic-
tures of subtle visual differences, e.g., pictures of ?a water
front house with a docked boat? from those of ?a floating
house pulled by a boat?.
2Available at http://www.cs.stonybrook.edu/
?ychoi/imgcaption/
to be grammatically valid (e.g., keeping articles
in place, preventing dangling modifiers) while re-
maining semantically compatible with respect to a
given image-text pair (e.g., preserving predicate-
argument relations). More formally, we maximize
the following objective function:
F (y;x) = ?(y;x, v) + ?(y;x)
subject to C(y;x, v)
where x = {xi} is the input caption (a sentence),
v is the accompanying image, y = {yi} is the
output sentence, ?(y;x, v) is the content selection
score, ?(y;x) is the linguistic fluency score, and
C(y;x, v) is the set of hard constraints. Let l(yi)
be the index of the word in x that is selected as the
i?th word in the output y so that xl(yi) = yi. Then,
we factorize ?(?) and ?(?) as:
?(y;x, v) =
?
i
?(yi, x, v) =
?
i
?(xl(yi), v)
?(y;x) =
?
i
?(yi, ..., yi?K)
=
?
i
?(xl(yi), ..., xl(yi?K))
where K is the size of local context.
Content Selection ? Visual Estimates:
The computer vision system used consists of 7404
visual classifiers for recognizing leaf level Word-
Net synsets (Fellbaum, 1998). Each classifier is
trained using labeled images from the ImageNet
dataset (Deng et al, 2009) ? an image database
of over 14 million hand labeled images orga-
nized according to the WordNet hierarchy. Image
similarity is represented using a Spatial Pyramid
Match Kernel (SPM) (Lazebnik et al, 2006) with
Locality-constrained Linear Coding (Wang et al,
2010) on shape based SIFT features (Lowe, 2004).
791
  (a) (b)
0 1 2 3 4 5 6 7 80
200400
600800
# of s
enten
ces (
in tho
usan
ds)
0 1 2 3 40
400
800
1200
# of s
enten
ces (
in tho
usan
ds)
Figure 2: Number of sentences (y-axis) for each
average (x-axis in (a)) and maximum (x-axis in
(b)) number of words with future dependencies
Models are linear SVMs followed by a sigmoid to
produce probability for each node.3
Content Selection ? Salient Topics:
We consider Tf.Idf driven scores to favor salient
topics, as those are more likely to generalize
across many different images. Additionally, we
assign a very low content selection score (??) for
proper nouns and numbers and a very high score
(larger then maximum idf or visual score) for the
2k most frequent words in our corpus.
Local Linguistic Fluency:
We model linguistic fluency with 3-gram condi-
tional probabilities:
?(xl(yi), xl(yi?1), xl(yi?2)) (1)
= p(xl(yi)|xl(yi?2), xl(yi?1))
We experiment with two different ngram statis-
tics, one extracted from the Google Web 1T cor-
pus (Brants and Franz., 2006), and the other com-
puted from the 1M image-caption corpus (Or-
donez et al, 2011).
Dependency-driven Constraints:
Table 1 defines the list of dependencies used
as constraints driven from the typed dependen-
cies (de Marneffe and Manning, 2009; de Marn-
effe et al, 2006). The direction of arrows indi-
cate the direction of inclusion requirements. For
example, dep(X ?? Y ), denotes that ?X? must
be included whenever ?Y ? is included. Similarly,
dep(X ?? Y ) denotes that ?X? and ?Y ? must
either be included together or eliminated together.
We determine the uni- or bi-directionality of these
constraints by manually examining a few example
sentences corresponding to each of these typed de-
pendencies. Note that some dependencies such as
det(??) would hold regardless of the particular
3Code was provided by Deng et al (2012).
Method-1 (M1) v.s. Method-2 (M2) M1 winsover M2
SALIENCY ORIG 76.34%
VISUAL ORIG 81.75%
VISUAL SALIENCY 72.48%
VISUAL VISUAL W/O CONSTR 83.76%
VISUAL NGRAM-ONLY 90.20%
VISUAL HUMAN 19.00%
Table 2: Forced Choice Evaluation (LM Corpus =
Google)
lexical items, while others, e.g., dobj(??) may
or may not be necessary depending on the context.
Those dependencies that we determine as largely
context dependent are marked with * in Table 1.
One could consider enforcing all dependency
constraints in Table 1 as hard constraints so that
the compressed sentence must not violate any of
those directed dependency constraints. Doing so
would lead to overly conservative compression
with least compression ratio however. Therefore,
we relax those that are largely context dependent
as soft constraints (marked in Table 1 with *) by
introducing a constant penalty term in the objec-
tive function. Alternatively, the dependency based
constraints can be learned statistically from the
training corpus of paired original and compressed
sentences. Since we do not have such in-domain
training data at this time, we leave this exploration
as future research.
Dynamic Programming with Dynamic Beam:
The constraint optimization we formulated corre-
sponds to an NP-hard problem. In our work, hard
constraints are based only on typed dependencies,
and we find that long range dependencies occur in-
frequently in actual image descriptions, as plotted
in Figure 2. With this insight, we opt for decoding
based on dynamic programming with dynamically
adjusted beam.4 Alternatively, one can find an ap-
proximate solution using Integer Linear Program-
ming (e.g., Clarke and Lapata (2006), Clarke and
Lapata (2007), Martins and Smith (2009)).
3 Evaluation
Since there is no existing benchmark data for im-
age caption generalization, we crowdsource evalu-
ation using Amazon Mechanical Turk (AMT). We
empirically compare the following options:
4The required beam size at each step depends on how
many words have dependency constraints involving any word
following the current one ? beam size is at most 2p, where p
is the max number of words dependent on any future words.
792
Big elm tree over 
the house is no 
their anymore. 
? Tree over the house. 
Abandonned 
houses in the 
forest. 
? Houses in the 
     forest. 
A woman paints a tree in 
bloom near the duck pond 
in the Boston Public 
Garden, April 15, 2006. 
? A tree in bloom . 
Pillbox in field 
behind a pub 
car park. 
? Pub car. 
Flowering tree in 
mixed forest at 
Wakehurst. 
? Flowering tree  
    in forest. 
The insulbrick matches 
the yard. This is outside 
of medina ohio near the 
tonka truck house. 
? The yard. This is 
     outside the house. 
Query Image Retrieved Images 
Figure 3: Example Image Caption Transfer
Method LM strict matching semantic matchingCorpus BLEU P R F BLEU P R F
ORIG N/A 0.063 0.064 0.139 0.080 0.215 0.220 0.508 0.276
SALIENCY Image Corpus 0.060 0.074 0.077 0.068 0.302 0.411 0.399 0.356
VISUAL Image Corpus 0.060 0.075 0.075 0.068 0.305 0.422 0.397 0.360
SALIENCY Google Corpus 0.064 0.070 0.101 0.074 0.286 0.337 0.459 0.340
VISUAL Google Corpus 0.065 0.071 0.098 0.075 0.296 0.354 0.457 0.350
Table 3: Image Description Transfer: performance in BLEU and F1 with strict & semantic matching.
? ORIG: original uncompressed captions
? HUMAN: compressed by humans (See ? 3.2)
? SALIENCY: linguistic fluency + saliency-based
content selection + dependency constraints
? VISUAL: linguistic fluency + visually-guided
content selection + dependency constraints
? x W/O CONSTR: method xwithout dependency
constraints
? NGRAM-ONLY: linguistic fluency only
3.1 Intrinsic Evaluation: Forced Choice
Turkers are provided with an image and two cap-
tions (produced by different methods) and are
asked to select a better one, i.e., the most relevant
and plausible caption that contains the least extra-
neous information. Results are shown in Table 2.
We observe that VISUAL (full model with visually
guided content selection) performs the best, being
selected over SALIENCY (content-selection with-
out visual information) in 72.48% cases, and even
over the original image caption in 81.75% cases.
This forced-selection experiment between VI-
SUAL and ORIG demonstrates the degree of noise
prevalent in the image captions in the wild. Of
course, if compared against human-compressed
captions, the automatic captions are preferred
much less frequently ? in 19% of the cases. In
those 19% cases when automatic captions are pre-
ferred over human-compressed ones, it is some-
times that humans did not fully remove informa-
tion that is not visually present or verifiable, and
other times humans overly compressed. To ver-
ify the utility of dependency-based constraints,
we also compare two variations of VISUAL, with
and without dependency-based constraints. As ex-
pected, the algorithm with constraints is preferred
in the majority of cases.
3.2 Extrinsic Evaluation: Image-based
Caption Retrieval
We evaluate the usefulness of our new image-text
parallel corpus for automatic generation of image
descriptions. Here the task is to produce, for a
query image, a relevant description, i.e., a visu-
ally descriptive caption. Following Ordonez et al
(2011), we produce a caption for a query image
by finding top k most similar images within the
1M image-text corpus (Ordonez et al, 2011) and
then transferring their captions to the query im-
age. To compute evaluation measures, we take the
average scores of BLEU(1) and F-score (unigram-
based with respect to content-words) over k = 5
candidate captions.
Image similarity is computed using two global
(whole) image descriptors. The first is the GIST
feature (Oliva and Torralba, 2001), an image de-
scriptor related to perceptual characteristics of
scenes ? naturalness, roughness, openness, etc.
The second descriptor is also a global image de-
scriptor, computed by resizing the image into a
?tiny image? (Torralba et al, 2008), which is ef-
fective in matching the structure and overall color
of images. To find visually relevant images, we
compute the similarity of the query image to im-
793
Huge wall of glass 
at the Conference 
Centre in 
Yohohama  Japan.  
? Wall of glass  
My footprint in a 
sand box 
? A sand box  
James the cat is 
dreaming of running 
in a wide green 
valley 
? Running in 
a valley (not 
relevant) 
This little boy was so 
cute. He was flying his 
spiderman kite all by 
himself on top of Max 
Patch  
? This little boy was so 
cute. He was flying 
(semantically odd) 
A view of the post office 
building in Manila from 
the other side of the 
Pasig River  
? A view of the post 
office building from 
the side  
Cell phone shot of 
a hat stall in the 
Northeast Market, 
Baltimore, MD.  
? Cell phone shot.  
(visually not 
verifiable) 
Figure 4: Good (left three, in blue) and bad examples (right three, in red) of generalized captions
ages in the whole dataset using an unweighted sum
of gist similarity and tiny image similarity.
Gold standard (human compressed) captions are
obtained using AMT for 1K images. The results
are shown in Table 3. Strict matching gives credit
only to identical words between the gold-standard
caption and the automatically produced caption.
However, words in the original caption of the
query image (and its compressed caption) do not
overlap exactly with words in the retrieved cap-
tions, even when they are semantically very close,
which makes it hard to see improvements even
when the captions of the new corpus are more gen-
eral and transferable over other images. Therefore,
we also report scores based on semantic matching,
which gives partial credits to word pairs based on
their lexical similarity.5 The best performing ap-
proach with semantic matching is VISUAL (with
LM = Image corpus), improving BLEU, Precision,
F-score substantially over those of ORIG, demon-
strating the extrinsic utility of our newly gener-
ated image-text parallel corpus in comparison to
the original database. Figure 3 shows an example
of caption transfer.
4 Related Work
Several recent studies presented approaches to
automatic caption generation for images (e.g.,
Farhadi et al (2010), Feng and Lapata (2010a),
Feng and Lapata (2010b), Yang et al (2011),
Kulkarni et al (2011), Li et al (2011), Kuznetsova
et al (2012)). The end goal of our work differs in
that we aim to revise original image captions into
5We take Wu-Palmer Similarity as similarity mea-
sure (Wu and Palmer, 1994). When computing BLEU with
semantic matching, we look for the match with the highest
similarity score among words that have not been matched be-
fore. Any word matched once (even with a partial credit) will
be removed from consideration when matching next words.
descriptions that are more general and align more
closely to the visual image content.
In comparison to prior work on sentence com-
pression, our approach falls somewhere between
unsupervised to distant-supervised approach (e.g.,
Turner and Charniak (2005), Filippova and Strube
(2008)) in that there is not an in-domain train-
ing corpus to learn generalization patterns directly.
Future work includes exploring more direct su-
pervision from human edited sample generaliza-
tion (e.g., Knight and Marcu (2000), McDonald
(2006)) Galley and McKeown (2007), Zhu et al
(2010)), and the inclusion of edits beyond dele-
tion, e.g., substitutions, as has been explored by
e.g., Cohn and Lapata (2008), Cordeiro et al
(2009), Napoles et al (2011).
5 Conclusion
We have introduced the task of image caption gen-
eralization as a means to reduce noise in the paral-
lel corpus of images and text. Intrinsic and extrin-
sic evaluations confirm that the captions in the re-
sulting corpus align better with the image contents
(are often preferred over the original captions by
people), and can be practically more useful with
respect to a concrete application.
Acknowledgments
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research. Additionally, Tamara Berg is supported
by NSF #1054133 and NSF #1161876. We thank
reviewers for many insightful comments and sug-
gestions.
794
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium.
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 144?151, Sydney, Australia, July. Association
for Computational Linguistics.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1?11, Prague, Czech Republic, June.
Association for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 137?144,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Joao Cordeiro, Gael Dias, and Pavel Brazdil. 2009.
Unsupervised induction of sentence compression
rules. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 15?22, Suntec, Singapore,
August. Association for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2009. Stanford typed dependencies manual.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Language Resources and Evaluation Conference
2006.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hi-
erarchical Image Database. In Conference on Com-
puter Vision and Pattern Recognition.
Jia Deng, Jonathan Krause, Alexander C. Berg, and
L. Fei-Fei. 2012. Hedging your bets: Optimizing
accuracy-specificity trade-offs in large scale visual
recognition. In Conference on Computer Vision and
Pattern Recognition.
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yam-
aguchi, Yejin Choi, Hal Daume III, Alex Berg, and
Tamara Berg. 2012. Detecting visual text. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
762?772, Montre?al, Canada, June. Association for
Computational Linguistics.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young1, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences for images.
In European Conference on Computer Vision.
Christiane D. Fellbaum, editor. 1998. WordNet: an
electronic lexical database. MIT Press.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption genera-
tion for news images. In Association for Computa-
tional Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In Hu-
man Language Technologies.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference, INLG ?08, pages 25?32,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 180?187,
Rochester, New York, April. Association for Com-
putational Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In AAAI/IAAI, pages 703?710.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C Berg, and Tamara L
Berg. 2011. Babytalk: Understanding and gener-
ating simple image descriptions. In Conference on
Computer Vision and Pattern Recognition.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2012. Collective gen-
eration of natural image descriptions. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 359?368, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching. In Conference on Computer Vision and
Pattern Recognition, June.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, pages 220?
228, Portland, Oregon, USA, June. Association for
Computational Linguistics.
David G. Lowe. 2004. Distinctive image features from
scale-invariant keypoints. Int. J. Comput. Vision,
60:91?110, November.
795
Andre Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on
Integer Linear Programming for Natural Language
Processing, pages 1?9, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Ryan T. McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In EACL
2006, 11st Conference of the European Chapter of
the Association for Computational Linguistics, Pro-
ceedings of the Conference, April 3-7, 2006, Trento,
Italy. The Association for Computer Linguistics.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphras-
tic sentence compression with a character-based
metric: Tightening without deletion. In Proceed-
ings of the Workshop on Monolingual Text-To-Text
Generation, pages 84?90, Portland, Oregon, June.
Association for Computational Linguistics.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: a holistic representation of the
spatial envelope. International Journal of Computer
Vision.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Neural Information Pro-
cessing Systems (NIPS).
Antonio Torralba, Rob Fergus, and William T. Free-
man. 2008. 80 million tiny images: a large dataset
for non-parametric object and scene recognition.
Pattern Analysis and Machine Intelligence, 30.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 290?297, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv,
T. Huang, and Yihong Gong. 2010. Locality-
constrained linear coding for image classification.
In Conference on Computer Vision and Pattern
Recognition (CVPR).
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis
Aloimonos. 2011. Corpus-guided sentence genera-
tion of natural images. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 444?454, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1353?1361, Bei-
jing, China, August. Coling 2010 Organizing Com-
mittee.
796
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1544?1554,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
ConnotationWordNet:
Learning Connotation over the Word+Sense Network
Jun Seok Kang Song Feng Leman Akoglu Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
junkang, songfeng, leman, ychoi@cs.stonybrook.edu
Abstract
We introduce ConnotationWordNet, a con-
notation lexicon over the network of words
in conjunction with senses. We formulate
the lexicon induction problem as collec-
tive inference over pairwise-Markov Ran-
dom Fields, and present a loopy belief
propagation algorithm for inference. The
key aspect of our method is that it is
the first unified approach that assigns the
polarity of both word- and sense-level
connotations, exploiting the innate bipar-
tite graph structure encoded in WordNet.
We present comprehensive evaluation to
demonstrate the quality and utility of the
resulting lexicon in comparison to existing
connotation and sentiment lexicons.
1 Introduction
We introduce ConnotationWordNet, a connotation
lexicon over the network of words in conjunction
with senses, as defined in WordNet. A connotation
lexicon, as introduced first by Feng et al (2011),
aims to encompass subtle shades of sentiment a
word may conjure, even for seemingly objective
words such as ?sculpture?, ?Ph.D.?, ?rosettes?.
Understanding the rich and complex layers of con-
notation remains to be a challenging task. As a
starting point, we study a more feasible task of
learning the polarity of connotation.
For non-polysemous words, which constitute a
significant portion of English vocabulary, learning
the general connotation at the word-level (rather
than at the sense-level) would be a natural oper-
ational choice. However, for polysemous words,
which correspond to most frequently used words,
it would be an overly crude assumption that the
same connotative polarity should be assigned for
all senses of a given word. For example, consider
?abound?, for which lexicographers of WordNet
prescribe two different senses:
? (v) abound: (be abundant of plentiful; exist
in large quantities)
? (v) abound, burst, bristle: (be in a state of
movement or action) ?The room abounded
with screaming children?; ?The garden bris-
tled with toddlers?
For the first sense, which is the most commonly
used sense for ?abound?, the general overtone of
the connotation would seem positive. That is, al-
though one can use this sense in both positive and
negative contexts, this sense of ?abound? seems
to collocate more often with items that are good to
be abundant (e.g., ?resources?), than unfortunate
items being abundant (e.g., ?complaints?).
However, as for the second sense, for which
?burst? and ?bristle? can be used interchangeably
with respect to this particular sense,
1
the general
overtone is slightly more negative with a touch of
unpleasantness, or at least not as positive as that of
the first sense. Especially if we look up the Word-
Net entry for ?bristle?, there are noticeably more
negatively connotative words involved in its gloss
and examples.
This word sense issue has been a universal chal-
lenge for a range of Natural Language Processing
applications, including sentiment analysis. Recent
studies have shown that it is fruitful to tease out
subjectivity and objectivity corresponding to dif-
ferent senses of the same word, in order to improve
computational approaches to sentiment analysis
(e.g. Pestian et al (2012), Mihalcea et al (2012)
Balahur et al (2014)). Encouraged by these recent
successes, in this study, we investigate if we can
attain similar gains if we model the connotative
polarity of senses separately.
There is one potential practical issue we would
like to point out in building a sense-level lexical
resource, however. End-users of such a lexicon
may not wish to deal with Word Sense Disam-
1
Hence a sense in WordNet is defined by synset (= syn-
onym set), which is the set of words sharing the same sense.
1544
biguation (WSD), which is known to be often too
noisy to be incorporated into the pipeline with re-
spect to other NLP tasks. As a result, researchers
often would need to aggregate labels across differ-
ent senses to derive the word-level label. Although
such aggregation is not entirely unreasonable, it
does not seem to be the most optimal and princi-
pled way of integrating available resources.
Therefore, in this work, we present the first uni-
fied approach that learns both sense- and word-
level connotations simultaneously. This way, end-
users will have access to more accurate sense-level
connotation labels if needed, while also having ac-
cess to more general word-level connotation la-
bels. We formulate the lexicon induction problem
as collective inference over pairwise-Markov Ran-
dom Fields (pairwise-MRF) and derive a loopy be-
lief propagation algorithm for inference.
The key aspect of our approach is that we ex-
ploit the innate bipartite graph structure between
words and senses encoded in WordNet. Although
our approach seems conceptually natural, previous
approaches, to our best knowledge, have not di-
rectly exploited these relations between words and
senses for the purpose of deriving lexical knowl-
edge over words and senses collectively. In ad-
dition, previous studies (for both sentiment and
connotation lexicons) aimed to produce only ei-
ther of the two aspects of the polarity: word-level
or sense-level, while we address both.
Another contribution of our work is the intro-
duction of loopy belief propagation (loopy-BP)
as a lexicon induction algorithm. Loopy-BP in
our study achieves statistically significantly better
performance over the constraint optimization ap-
proaches previously explored. In addition, it runs
much faster and it is considerably easier to imple-
ment. Last but not least, by using probabilistic rep-
resentation of pairwise-MRF in conjunction with
Loopy-BP as inference, the resulting solution has
the natural interpretation as the intensity of con-
notation. This contrasts to approaches that seek
discrete solutions such as Integer Linear Program-
ming(Papadimitriou and Steiglitz, 1998).
ConnotationWordNet, the final outcome of our
study, is a new lexical resource that has conno-
tation labels over both words and senses follow-
ing the structure of WordNet. The lexicon is pub-
licly available at: http://www.cs.sunysb.
edu/
?
junkang/connotation_wordnet.)
In what follows, we will first describe the net-
memberships)antonyms)Pred1Arg)Arg1Arg)
?)
?)
prevent)
suffer)
enjoy)
achieve)
pain)
losses)
life)
profit)
success)
win)
investment)
injure)accident)
wound)ache)
word)sense)
gain)
hurt)
put)on)
lose)
injure,))wound)
gain,))put)on)
win,)gain,)acquire)
win,)profits,)))winnings)
ConnotaAve)Predicates) Arguments) Senses)
Figure 1: G
WORD+SENSE
with words and senses.
work of words and senses (Section 2), then intro-
duce the representation of the network structure as
pairwise Markov Random Fields, and a loopy be-
lief propagation algorithm as collective inference
(Section 3). We then present comprehensive eval-
uation (Section 4 & 5 & 6), followed by related
work (Section 7) and conclusion (Section 8).
2 Network of Words and Senses
The connotation graph, called G
WORD+SENSE
, is a
heterogeneous graph with multiple types of nodes
and edges. As shown in Figure 1, it contains two
types of nodes; (i) lemmas (i.e., words, 115K)
and (ii) synsets (63K), and four types of edges;
(t
1
) predicate-argument (179K), (t
2
) argument-
argument (144K), (t
3
) argument-synset (126K),
and (t
4
) synset-synset (3.4K) edges.
The predicate-argument edges, first introduced
by Feng et al (2011), depict the selectional prefer-
ence of connotative predicates (i.e., the polarity of
a predicate indicates the polarity of its arguments)
and encode their co-occurrence relations based
on the Google Web 1T corpus. The argument-
argument edges are based on the distributional
similarities among the arguments. The argument-
synset edges capture the synonymy between argu-
ment nodes through the corresponding synsets. Fi-
nally, the synset-synset edges depict the antonym
relations between synset pairs.
In general, our graph construction is similar to
that of Feng et al (2013), but there are a few im-
portant differences. Most notably, we model both
words and synsets explicitly, and exploit the mem-
bership relations between words and senses. We
expect that edges between words and senses will
encourage senses that belong to the same word to
1545
receive the same connotation label. Conversely,
we expect that these edges will also encourage
words that belong to the same sense (i.e., synset
definition) to receive the same connotation label.
Another benefit of our approach is that for var-
ious WordNet relations (e.g., antonym relations),
which are defined over synsets (not over words),
we can add edges directly between corresponding
synsets, rather than projecting (i.e., approximat-
ing) those relations over words. Note that the lat-
ter, which has been employed by several previous
studies (e.g., Kamps et al (2004), Takamura et al
(2005), Andreevskaia and Bergler (2006), Su and
Markert (2009), Lu et al (2011), Kaji and Kit-
suregawa (2007), Feng et al (2013)), could be a
source of noise, as one needs to assume that the
semantic relation between a pair of synsets trans-
fers over the pair of words corresponding to that
pair of synsets. For polysemous words, this as-
sumption may be overly strong.
3 Pairwise Markov Random Fields and
Loopy Belief Propagation
We formulate the task of learning sense- and word-
level connotation lexicon as a graph-based clas-
sification task (Sen et al, 2008). More formally,
we denote the connotation graph G
WORD+SENSE
by
G = (V,E), in which a total of n word and synset
nodes V = {v
1
, . . . , v
n
} are connected with
typed edges e(v
i
, v
j
, t) ? E, where edge types
t ? {pred-arg, arg-arg, syn-arg, syn-syn} de-
pict the four edge types as described in Section
2. A neighborhood function N , where N
v
=
{u| e(u, v) ? E} ? V , describes the underlying
network structure.
In our collective classification formulation, each
node in V is represented as a random variable that
takes a value from an appropriate class label do-
main; in our case, L = {+,?} for positive and
negative connotation. In this classification task,
we denote by Y the nodes the labels of which need
to be assigned, and let y
i
refer to Y
i
?s label.
3.1 Pairwise Markov Random Fields
We next define our objective function. We pro-
pose to use an objective formulation that utilizes
pairwise Markov Random Fields (MRFs) (Kinder-
mann and Snell, 1980), which we adapt to our
problem setting. MRFs are a class of probabilistic
graphical models that are suited for solving infer-
ence problems in networked data. An MRF con-
sists of an undirected graph where each node can
be in any of a finite number of states (i.e., class
labels). The state of a node is assumed to be de-
pendent on each of its neighbors and independent
of other nodes in the graph.
2
In pairwise MRFs,
the joint probability of the graph can be written as
a product of pairwise factors, parameterized over
the edges. These factors are referred to as clique
potentials in general MRFs, which are essentially
functions that collectively determine the graph?s
joint probability.
Specifically, let G = (V,E) denote a network
of random variables, where V consists of the un-
observed variables Y that need to be assigned val-
ues from label set L. Let ? denote a set of clique
potentials that consists of two types of factors:
? For each Y
i
? Y , ?
i
? ? is a prior map-
ping ?
i
: L ? R
?0
, where R
?0
denotes non-
negative real numbers.
? For each e(Y
i
, Y
j
, t) ? E, ?
t
ij
? ? is a com-
patibility mapping ?
t
ij
: L ? L ? R
?0
.
Objective formulation Given an assignment y
to all the unobserved variables Y and x to ob-
served ones X (variables with known labels, if
any), our objective function is associated with the
following joint probability distribution
P (y|x) =
1
Z(x)
?
Y
i
?Y
?
i
(y
i
)
?
e(Y
i
,Y
j
,t)?E
?
t
ij
(y
i
, y
j
)
(1)
where Z(x) is the normalization function. Our
goal is then to infer the maximum likelihood as-
signment of states (i.e., labels) to unobserved vari-
ables (i.e., nodes) that will maximize Equation (1).
Problem Definition Having introduced our
graph-based classification task and objective for-
mulation, we define our problem more formally.
Given
- a connotation graph G = (V,E) of words
and synsets connected with typed edges,
- prior knowledge (i.e., probabilities) of (some
or all) nodes belonging to each class,
- compatibility of two nodes with a given pair
of labels being connected to each other;
Classify the nodes Y
i
? Y , into one of two classes;
L = {+,?}, such that the class assignments y
i
maximize our objective in Equation (1).
We can further rank the network objects by the
probability of their connotation polarity.
2
This assumption yields a pairwise Markov Random Field
(MRF); a special case of general MRFs (Yedidia et al, 2003).
1546
3.2 Loopy Belief Propagation
Finding the best assignments to unobserved vari-
ables in our objective function is the inference
problem. The brute force approach through enu-
meration of all possible assignments is exponen-
tial and thus intractable. In general, exact in-
ference is known to be NP-hard and there is
no known algorithm which can be theoretically
shown to solve the inference problem for gen-
eral MRFs. Therefore in this work, we em-
ploy a computationally tractable (in fact linearly
scalable with network size) approximate infer-
ence algorithm called Loopy Belief Propagation
(LBP) (Yedidia et al, 2003), which we extend to
handle typed graphs like our connotation graph.
Our inference algorithm is based on iterative
message passing and the core of it can be concisely
expressed as the following two equations:
m
i?j
(y
j
) = ?
?
y
i
?L
(
?
t
ij
(y
i
, y
j
) ?
i
(y
i
)
?
Y
k
?N
i
?Y\Y
j
m
k?i
(y
i
)
)
, ?y
j
? L (2)
b
i
(y
i
) = ? ?
i
(y
i
)
?
Y
j
?N
i
?Y
m
j?i
(y
i
),?y
i
? L
(3)
A message m
i?j
is sent from node i to node j
and captures the belief of i about j, which is the
probability distribution over the labels of j; i.e.
what i ?thinks? j?s label is, given the current la-
bel of i and the type of the edge that connects i
and j. Beliefs refer to marginal probability dis-
tributions of nodes over labels; for example b
i
(y
i
)
denotes the belief of node i having label y
i
. ? and
? are the normalization constants, which respec-
tively ensure that each message and each set of
marginal probabilities sum to 1. At every iteration,
each node computes its belief based on messages
received from its neighbors, and uses the compat-
ibility mapping to transform its belief into mes-
sages for its neighbors. The key idea is that after
enough iterations of message passes between the
nodes, the ?conversations? are likely to come to a
consensus, which determines the marginal proba-
bilities of all the unknown variables.
The pseudo-code of our method is given in Al-
gorithm 1. It first initializes all messages to 1
and priors to unbiased (i.e., equal) probabilities
for all nodes except the seed nodes for which the
sentiment is known (lines 3-9). It then proceeds
by making each Y
i
? Y communicate messages
Algorithm 1: CONNOTATION INFERENCE
1 Input: Connotation graph G=(V,E), prior
potentials ?
s
for seed words s ? S, and
compatibility potentials ?
t
ij
2 Output: Connotation label probabilities for
each node i ? V \P
3 foreach e(Y
i
, Y
j
, t) ? E do // initialize msg.s
4 foreach y
j
? L do
5 m
i?j
(y
j
)? 1
6 foreach i ? V do // initialize priors
7 foreach y
j
? L do
8 if i ? S then ?
i
(y
j
)? ?
i
(y
j
) else
?
i
(y
j
)? 1/|L|
9 repeat // iterative message passing
10 foreach e(Y
i
, Y
j
, t) ? E, Y
j
? Y
V \S
do
11 foreach y
j
? L do
12 Use Equation (2)
13 until all messages stop changing
14 foreach Y
i
? Y
V \S
do // compute final beliefs
15 foreach y
i
? L do
16 Use Equation (3)
with their neighbors in an iterative fashion until
the messages stabilize (lines 10-14), i.e. conver-
gence is reached.
3
At convergence, we calculate
the marginal probabilities, that is of assigning Y
i
with label y
i
, by computing the final beliefs b
i
(y
i
)
(lines 15-17). We use these maximum likelihood
probabilities for label assignment; for each node i,
we assign the label L
i
? max
y
i
b
i
(y
i
).
To completely define our algorithm, we need to
instantiate the potentials ?, in particular the priors
and the compatibilities, which we discuss next.
Priors The prior beliefs ?
i
of nodes can be suit-
ably initialized if there is any prior knowledge for
their connotation sentiment (e.g., enjoy is posi-
tive, suffer is negative). As such, our method
is flexible to integrate available side information.
In case there is no prior knowledge available, each
node is initialized equally likely to have any of the
possible labels, i.e.,
1
|L|
as in Algorithm 1 (line 9).
Compatibilities The compatibility potentials
can be thought of as matrices, with entries
3
Although convergence is not theoretically guaranteed, in
practice LBP converges to beliefs within a small threshold of
change (e.g., 10
?6
) fairly quickly with accurate results (Pan-
dit et al, 2007; McGlohon et al, 2009; Akoglu et al, 2013).
1547
?t
ij
(y
i
, y
j
) that give the likelihood of a node hav-
ing label y
i
, given that it has a neighbor with label
y
j
to which it is connected through a type t edge.
A key difference of our method from earlier mod-
els is that we use clique potentials that differ for
edge types, since the connotation graph is hetero-
geneous. This is exactly because the compatibil-
ity of class labels of two adjacent nodes depends
on the type of the edge connecting them: e.g.,
+
syn-arg
?????? + is highly compatible, whereas +
syn-syn
?????? + is unlikely; as syn-arg edges capture
synonymy; i.e., words-sense memberships, while
syn-syn edges depict antonym relations.
A sample instantiation of the compatibilities
is shown in Table 1. Notice that the potentials
for pred-arg, arg-arg, and syn-arg capture ho-
mophily, i.e., nodes with the same label are likely
to connect to each other through these types of
edges.
4
On the other hand, syn-syn edges con-
nect nodes that are antonyms of each other, and
thus the compatibilities capture the reverse rela-
tionship among their labels.
Table 1: Instantiation of compatibility potentials.
Entry ?
t
ij
(y
i
, y
j
) is the compatibility of a node
with label y
i
having a neighbor labeled y
j
, given
the edge between i and j is type t, for small .
t: t
1
A
P + ?
+ 1- 
?  1-
t: t
2
A
A + ?
+ 1-2 2
? 2 1-2
(t
1
) pred-arg (t
2
) arg-arg
t: t
3
A
S + ?
+ 1- 
?  1-
t: t
4
S
S + ?
+  1-
? 1- 
(t
3
) syn-arg (t
4
) syn-syn
(synonym relations) (antonym relations)
Complexity analysis Most demanding compo-
nent of Algorithm 1 is the iterative message pass-
ing over the edges (lines 10-14), with time com-
plexity O(ml
2
r), where m = |E| is the num-
ber of edges in the connotation graph, l = |L|,
the classes, and r, the iterations until convergence.
Often, l is quite small (in our case, l = 2) and
r  m. Thus running time grows linearly with the
number of edges and is scalable to large datasets.
4
arg-arg edges are based on co-occurrence (see Section
2), which does not carry as strong indication of the same con-
notation as e.g., synonymy. Thus, we enforce less homophily
for nodes connected through edges of arg-arg type.
4 Evaluation I: Agreement with
Sentiment Lexicons
ConnotationWordNet is expected to be the super-
set of a sentiment lexicon, as it is highly likely for
any word with positive/negative sentiment to carry
connotation of the same polarity. Thus, we use
two conventional sentiment lexicons, General In-
quirer (GENINQ) (Stone et al, 1966) and MPQA
(Wilson et al, 2005b), as surrogates to measure
the performance of our inference algorithm.
4.1 Variants of Graph Construction
The construction of the connotation graph, de-
noted by G
WORD+SENSE
, which includes words and
synsets, has been described in Section 2. In ad-
dition to this graph, we tried several other graph
constructions, the first three of which have previ-
ously been used in (Feng et al, 2013). We briefly
describe these graphs below, and compare perfor-
mance on all the graphs in the proceeding.
G
WORD
W/ PRED-ARG: This is a (bipartite)
subgraph of G
WORD+SENSE
, which only includes
the connotative predicates and their arguments. As
such, it contains only type t
1
edges. The edges
between the predicates and the arguments can be
weighted by their Point-wise Mutual Information
(PMI)
5
based on the Google Web 1T corpus.
G
WORD
W/ OVERLAY: The second graph is also
a proper subgraph of G
WORD+SENSE
, which in-
cludes the predicates and all the argument words.
Predicate words are connected to their arguments
as before. In addition, argument pairs (a
1
, a
2
) are
connected if they occurred together in the ?a
1
and
a
2
? or ?a
2
and a
1
? coordination (Hatzivassiloglou
and McKeown, 1997; Pickering and Branigan,
1998). This graph contains both type t
1
and t
2
edges. The edges can also be weighted based on
the distributional similarities of the word pairs.
G
WORD
: The third graph is a super-graph of
G
WORD
W/ OVERLAY, with additional edges,
where argument pairs in synonym and antonym
relation are connected to each other. Note that un-
like the connotation graph G
WORD+SENSE
, it does
not contain any synset nodes. Rather, the words
that are synonyms or antonyms of each other are
directly linked in the graph. As such, this graph
contains all edge types t
1
through t
4
.
5
PMI scores are widely used in previous studies to mea-
sure association between words (e.g., (Church and Hanks,
1990), (Turney, 2001), (Newman et al, 2009)).
1548
GWORD+SENSE
W/ SYNSIM: This is a super-
graph of our original G
WORD+SENSE
graph; that
is, it has all the predicate, arguments, and synset
nodes, as well as the four types of edges between
them. In addition, we add edges of a fifth type t
5
between the synset nodes to capture their similar-
ity. To define similarity, we use the glossary def-
initions of the synsets and derive three different
scores. Each score utilizes the count(s
1
, s
2
) of
overlapping nouns, verbs, and adjectives/adverbs
among the glosses of the two synsets s
1
and s
2
.
G
WORD+SENSE
W/ SYNSIM1: We discard edges
with count less than 3. The weighted version has
the counts normalized between 0 and 1.
G
WORD+SENSE
W/ SYNSIM2: We normalize
the counts by the length of the gloss (the
avg of two lengths), that is, p = count /
avg(len gloss(s
1
), len gloss(s
2
))
and discard edges with p < 0.5. The weighted
version contains p values as edge weights.
G
WORD+SENSE
W/ SYNSIM3: To further sparsify
the graph we discard edges with p < 0.6. To
weigh the edges, we use the cosine similarity be-
tween the gloss vectors of the synsets based on the
TF-IDF values of the words the glosses contain.
Note that the connotation inference algorithm,
as given in Algorithm 1, remains exactly the same
for all the graphs described above. The only dif-
ference is the set of parameters used; while G
WORD
W/ PRED-ARG and G
WORD
W/ OVERLAY contain
one and two edge types, respectively and only use
compatibilities (t
1
) and (t
2
), G
WORD
uses all four
as given in Table 1. The G
WORD+SENSE
W/ SYN-
SIM graphs use an additional compatibility matrix
for the synset similarity edges of type t
5
, which is
the same as the one used for t
1
, i.e., similar synsets
are likely to have the same connotation label. This
flexibility is one of the key advantages of our al-
gorithm as new types of nodes and edges can be
added to the graph seamlessly.
4.2 Sentiment-Lexicon based Performance
In this section, we first compare the performance
of our connotation graph G
WORD+SENSE
to graphs
that do not include synset nodes but only words.
Then we analyze the performance when the addi-
tional synset similarity edges are added. First, we
briefly describe our performance measures.
The sentiment lexicons we use as gold standard
are small, compared to the size (i.e., number of
words) our graphs contain. Thus, we first find
the overlap between each graph and a senti-
GENINQ MPQA
P R F F
Variations of G
WORD
W/ PRED-ARG 88.0 67.6 76.5 57.3
W/ PRED-ARG-W 84.9 68.9 76.1 57.8
W/ OVERLAY 87.8 70.4 78.1 58.4
W/ OVERLAY-W 82.2 67.7 74.2 54.2
G
WORD
88.5 83.1 85.7 69.7
G
WORD
-W 75.5 71.5 73.4 53.2
Variations of G
WORD+SENSE
G
WORD+SENSE
88.8 84.1 86.4 70.0
G
WORD+SENSE
-W 76.8 73.0 74.9 54.6
W/ SYNSIM1 87.2 83.3 85.2 67.9
W/ SYNSIM2 83.9 80.8 82.3 65.1
W/ SYNSIM3 86.5 83.2 84.8 67.8
W/ SYNSIM1-W 88.0 84.3 86.1 69.2
W/ SYNSIM2-W 86.4 83.7 85.0 68.5
W/ SYNSIM3-W 86.7 83.4 85.0 68.2
Table 2: Connotation inference performance on
various graphs. ?-W? indicates weighted versions
(see ?4.1). P: precision, R: recall, F: F1-score (%).
ment lexicon. Note that the overlap size may be
smaller than the lexicon size, as some sen-
timent words may be missing from our graphs.
Then, we calculate the number of correct la-
bel assignments. As such, precision is defined as
(correct / overlap), and recall as (correct
/ lexicon size). Finally, F1-score is their har-
monic mean and reflects the overall accuracy.
As shown in Table 2 (top), we first observe that
including the synonym and antonym relations in
the graph, as with G
WORD
and G
WORD+SENSE
, im-
prove the performance significantly, almost by an
order of magnitude, over graphs G
WORD
W/ PRED-
ARG and G
WORD
W/ OVERLAY that do not contain
those relation types. Furthermore, we notice that
the performances on the G
WORD+SENSE
graph are
better than those on the word-only graphs. This
shows that including the synset nodes explicitly in
the graph structure is beneficial. What is more,
it gives us a means to obtain connotation labels
for the synsets themselves, which we use in the
evaluations in the next sections. Finally, we note
that using the unweighted versions of the graphs
provide relatively more robust performance, po-
tentially due to noise in the relative edge weights.
Next we analyze the performance when the new
edges between synsets are introduced, as given in
Table 2 (bottom). We observe that connecting the
synset nodes by their gloss-similarity (at least in
the ways we tried) does not yield better perfor-
mance than on our original G
WORD+SENSE
graph.
Different from earlier, the weighted versions of
the similarity based graphs provide better perfor-
1549
mance than their unweighted counterparts. This
suggests that glossary similarity would be a more
robust means to correlate nodes; we leave it as fu-
ture work to explore this direction for predicate-
argument and argument-argument relations.
4.3 Parameter Sensitivity
Our belief propagation based connotation senti-
ment inference algorithm has one user-specified
parameter  (see Table 1). To study the sensitivity
of its performance to the choice of , we reran our
experiments for  = {0.02, 0.04, . . . , 0.24}
6
and
report the accuracy results on our G
WORD+SENSE
in
Figure 2 for the two lexicons. The results indicate
that the performances remain quite stable across a
wide range of the parameter choice.
precisionrecallF-scoreP
erfor
man
ce
0
20
40
60
80
100
?0.02 0.06 0.10 0.14 0.18 0.22
precisionrecallF-scoreP
erfor
manc
e
0
20
40
60
80
100
?0.02 0.06 0.10 0.14 0.18 0.22
(a) GENINQ EVAL (b) MPQA EVAL
Figure 2: Performance is stable across various .
5 Evaluation II: Human Evaluation on
ConnotationWordNet
In this section, we present the result of human
evaluation we executed using Amazon Mechani-
cal Turk (AMT). We collect two separate sets of
labels: a set of labels at the word-level, and an-
other set at the sense-level. We first describe the
labeling process of sense-level connotation: We
selected 350 polysemous words and one of their
senses, and each Turker was asked to rate the con-
notative polarity of a given word (or of a given
sense), from -5 to 5, 0 being the neutral.
7
For each
word, we asked 5 Turkers to rate and we took the
average of the 5 ratings as the connotative inten-
sity score of the word. We labeled a word as nega-
tive if its intensity score is less than 0 and positive
otherwise. For word-level labels we apply similar
procedure as above.
6
Note that for  > 0.25, compatibilities of ?
t
2
in Table 1
are reversed, hence the maximum of 0.24.
7
Because senses in WordNet can be tricky to understand,
care should be taken in designing the task so that the Turkers
will focus only on the corresponding sense of a word. There-
fore, we provided the part of speech tag, the WordNet gloss
of the selected sense, and a few examples as given in Word-
Net. As an incentive, each Turker was rewarded $0.07 per hit
which consists of 10 words to label.
Lexicon Word-level Sense-level
SentiWordNet 27.22 14.29
OpinionFinder 31.95 -
Feng2013 62.72 -
G
WORD+SENSE
(95%) 84.91 83.43
G
WORD+SENSE
(99%) 84.91 83.71
E-G
WORD+SENSE
(95%) 86.98 86.29
E-G
WORD+SENSE
(99%) 86.69 85.71
Table 3: Word-/Sense-level evaluation results
5.1 Word-Level Evaluation
We first evaluate the word-level assignment of
connotation, as shown in Table 3. The agreement
between the new lexicon and human judges varies
between 84% and 86.98%. Sentiment lexicons
such as SentiWordNet (Baccianella et al (2010))
and OpinionFinder (Wilson et al (2005a)) show
low agreement rate with human, which is some-
what as expected: human judges in this study are
labeling for subtle connotation, not for more ex-
plicit sentiment. OpinionFinder?s low agreement
rate was mainly due to the low hit rate of the words
(successful look-up rate, 33.43%). Feng2013 is
the lexicon presented in (Feng et al, 2013) and it
showed a relatively higher 72.13% hit rate.
Note that belief propagation was run until 95%
and 99% of the nodes were converged in their
beliefs. In addition, the seed words with known
connotation labels originally consist of 20 positive
and 20 negative predicates. We also extended the
seed set with the sentiment lexicon words and de-
note these runs with E- for ?Extended?.
5.2 Sense-Level Evaluation
We also examined the agreement rates on the
sense-level. Since OpinionFinder and Feng2013
do not provide the polarity scores at the sense-
level, we excluded them from this evaluation. Be-
cause sense-level polarity assignment is a harder
(more subtle) task, the performance of all lexicons
decreased to some degree in comparison to that of
word-level evaluations.
5.3 Pair-wise Intensity Ranking
A notable goodness of our induction algorithm is
that the outcome of the algorithm can be inter-
preted as an intensity of the corresponding conno-
tation. But are these values meaningful? We an-
swer this question in this section. We formulate a
pair-wise ranking task as a binary decision task as
follows: given a pair of words, we ask which one
is more positive (or more negative) than the other.
Since we collect human labels based on scales, we
1550
Lexicon Correct Undecided
SentiWordNet 33.77 23.34
G
WORD+SENSE
(95%) 74.83 0.58
G
WORD+SENSE
(99%) 73.01 0.58
E-G
WORD+SENSE
(95%) 73.84 1.16
E-G
WORD+SENSE
(99%) 74.01 1.16
Table 4: Results of pair-wise intensity evaluation,
for intensity difference threshold = 2.0
already have this information at hand. Because
different human judges have different notion of
scales however, subtle differences are more likely
to be noisy. Therefore, we experiment with vary-
ing degrees of differences in their scales, as shown
in Figure 3. Threshold values (ranging from 0.5 to
3.0) indicate the minimum differences in scales for
any pair of words, for the pair to be included in the
test set. As expected, we observe that the perfor-
mance improves as we increase the threshold (as
pairs get better separated). Within range [0.5, 1.5]
(249 pairs examined), the accuracies are as high as
68.27%, which shows that even the subtle differ-
ences of the connotative intensities are relatively
well reflected in the new lexicons.
SentiWordNetGWord+Sense(95%)GWord+Sense(99%)e-GWord+Sense(95%)e-GWord+Sense(99%)A
ccu
rac
y (%
)
40
60
80
Threshold
0.5 1.0 2.0 3.0
Figure 3: Trend of accuracy for pair-wise intensity
evaluation over threshold
The results for pair-wise intensity evaluation
(threshold=2.0, 1,208 pairs) are given in Table 4.
Despite that intensity is generally a harder prop-
erty to measure (than the coarser binary catego-
rization of polarities), our connotation lexicons
perform surprisingly well, reaching up to 74.83%
accuracy. Further study on the incorrect cases re-
veals that SentiWordNet has many pair of words
with the same polarity score (23.34%). Such cases
seems to be due to the limited score patterns of
SentiWordNet. The ratio of such cases are ac-
counted as Undecided in Table 4.
6 Evaluation III: Sentiment Analysis
using ConnotationWordNet
Finally, to show the utility of the resulting lexi-
con in the context of a concrete sentiment analysis
task, we perform lexicon-based sentiment analy-
sis. We experiment with SemEval dataset (Strap-
parava and Mihalcea, 2007) that includes the hu-
man labeled dataset for predicting whether a news
headline is a good news or a bad news, which we
expect to have a correlation with the use of con-
notative words that we focus on in this paper. The
good/bad news are annotated with scores (ranging
from -100 to 87). We construct several data sets by
applying different thresholds on scores. For exam-
ple, with the threshold set to 60, we discard the in-
stances whose scores lie between -60 and 60. For
comparison, we also test the connotation lexicon
from (Feng et al, 2013) and the combined senti-
ment lexicon GENINQ+MPQA.
Note that there is a difference in how humans
judge the orientation and the degree of connota-
tion for a given word out of context, and how the
use of such words in context can be perceived as
good/bad news. In particular, we conjecture that
humans may have a bias toward the use of posi-
tive words, which in turn requires calibration from
the readers? minds (Pennebaker and Stone, 2003).
That is, we might need to tone down the level of
positiveness in order to correctly measure the ac-
tual intended positiveness of the message.
With this in mind, we tune the appropriate cali-
bration from a small training data, by using 1 fold
from N fold cross validation, and using the re-
maining N ? 1 folds as testing. We simply learn
the mixture coefficient ? to scale the contribution
of positive and negative connotation values. We
tune this parameter ?
8
for other lexicons we com-
pare against as well. Note that due to this param-
eter learning, we are able to report better perfor-
mance for the connotation lexicon of (Feng et al,
2013) than what the authors have reported in their
paper (labeled with *) in Table 5.
Table 5 shows the results for N=15, where the
new lexicon consistently outperforms other com-
petitive lexicons. In addition, Figure 4 shows that
the performance does not change much based on
the size of training data used for parameter tuning
(N={5, 10, 15, 20}).
7 Related Work
Several previous approaches explored the use of
graph propagation for sentiment lexicon induction
(Velikovich et al, 2010) and connotation lexicon
8
What is reported is based on ? ? {20, 40, 60, 80}. More
detailed parameter search does not change the results much.
1551
Lexicon
SemEval Threshold
20 40 60 80
Instance Size 955 649 341 86
Feng2013 71.5 77.1 81.6 90.5
GENINQ+MPQA 72.8 77.2 80.4 86.7
G
WORD+SENSE
(95%) 74.5 79.4 86.5 91.9
G
WORD+SENSE
(99%) 74.6 79.4 86.8 91.9
E-G
WORD+SENSE
(95%) 72.5 76.8 82.3 87.2
E-G
WORD+SENSE
(99%) 72.6 76.9 82.5 87.2
Feng2013* 70.8 74.6 80.8 93.5
GENINQ+MPQA* 64.5 69.0 74.0 80.5
Table 5: SemEval evaluation results, for N=15
Feng2013MPQA+GenInqGWord+Sense(95%)GWord+Sense(99%)e-GWord+Sense(95%)e-GWord+Sense(99%)
Acc
ura
cy 
(%)
50
60
70
80
N
5 10 15 20
Figure 4: Trend of SemEval performance over N ,
the number of CV folds
induction (Feng et al, 2013). Our work intro-
duces the use of loopy belief propagation over
pairwise-MRF as an alternative solution to these
tasks. At a high-level, both approaches share the
general idea of propagating confidence or belief
over the graph connectivity. The key difference,
however, is that in our MRF representation, we
can explicitly model various types of word-word,
sense-sense and word-sense relations as edge po-
tentials. In particular, we can naturally encode re-
lations that encourage the same assignment (e.g.,
synonym) as well as the opposite assignment (e.g.,
antonym) of the polarity labels. Note that integra-
tion of the latter is not straightforward in the graph
propagation framework.
There have been a number of previous studies
that aim to construct a word-level sentiment lex-
icon (Wiebe et al, 2005; Qiu et al, 2009) and
a sense-level sentiment lexicon (Esuli and Sebas-
tiani, 2006). But none of these approaches con-
sidered to induce the polarity labels at both the
word-level and sense-level. Although we focus on
learning connotative polarity of words and senses
in this paper, the same approach would be applica-
ble to constructing a sentiment lexicon as well.
There have been recent studies that address
word sense disambiguation issues for sentiment
analysis. SentiWordNet (Esuli and Sebastiani,
2006) was the very first lexicon developed for
sense-level labels of sentiment polarity. In recent
years, Akkaya et al (2009) report a successful em-
pirical result where WSD helps improving senti-
ment analysis, while Wiebe and Mihalcea (2006)
study the distinction between objectivity and sub-
jectivity in each different sense of a word, and
their empirical effects in the context of sentiment
analysis. Our work shares the high-level spirit of
accessing the sense-level polarity, while also de-
riving the word-level polarity.
In recent years, there has been a growing re-
search interest in investigating more fine-grained
aspects of lexical sentiment beyond positive and
negative sentiment. For example, Mohammad and
Turney (2010) study the affects words can evoke
in people?s minds, while Bollen et al (2011) study
various moods, e.g., ?tension?, ?depression?, be-
yond simple dichotomy of positive and negative
sentiment. Our work, and some recent work by
Feng et al (2011) and Feng et al (2013) share this
spirit by targeting more subtle, nuanced sentiment
even from those words that would be considered
as objective in early studies of sentiment analysis.
8 Conclusion
We have introduced a novel formulation of lexicon
induction operating over both words and senses,
by exploiting the innate structure between the
words and senses as encoded in WordNet. In addi-
tion, we introduce the use of loopy belief propaga-
tion over pairwise-Markov Random Fields as an
effective lexicon induction algorithm. A notable
strength of our approach is its expressiveness: var-
ious types of prior knowledge and lexical relations
can be encoded as node potentials and edge po-
tentials. In addition, it leads to a lexicon of bet-
ter quality while also offering faster run-time and
easiness of implementation. The resulting lexi-
con, called ConnotationWordNet, is the first lex-
icon that has polarity labels over both words and
senses. ConnotationWordNet is publicly available
for research and practical use.
Acknowledgments
This research was supported by the Army Re-
search Office under Contract No. W911NF-14-1-
0029, Stony Brook University Office of Vice Pres-
ident for Research, and gifts from Northrop Grum-
man Aerospace Systems and Google. We thank
reviewers for many insightful comments and sug-
gestions.
1552
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1-Volume 1, pages 190?199. Association for Com-
putational Linguistics.
Leman Akoglu, Rishi Chandy, and Christos Faloutsos.
2013. Opinion fraud detection in online reviews by
network effects.
Alina Andreevskaia and Sabine Bergler. 2006. Min-
ing wordnet for a fuzzy sentiment: Sentiment tag
extraction from wordnet glosses. In EACL, pages
209?216.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC, volume 10, pages 2200?2204.
Alexandra Balahur, Rada Mihalcea, and Andr?es Mon-
toyo. 2014. Computational approaches to subjec-
tivity and sentiment analysis: Present and envisaged
methods and applications. Computer Speech & Lan-
guage, 28(1):1?6.
Johan Bollen, Huina Mao, and Alberto Pepe. 2011.
Modeling public mood and emotion: Twitter senti-
ment and socio-economic phenomena. In ICWSM.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 1(16):22?29.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC06, pages 417?422.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 1092?1103. Association for Computa-
tional Linguistics.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash
of sentiment beneath the surface meaning. In The
Association for Computer Linguistics, pages 1774?
1784.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the Joint ACL/EACL Con-
ference, pages 174?181.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive col-
lection of html documents. In EMNLP-CoNLL,
pages 1075?1083.
Jaap Kamps, MJ Marx, Robert J Mokken, and Maarten
De Rijke. 2004. Using wordnet to measure seman-
tic orientations of adjectives.
Ross Kindermann and J. L. Snell. 1980. Markov Ran-
dom Fields and Their Applications.
Yue Lu, Malu Castellanos, Umeshwar Dayal, and
ChengXiang Zhai. 2011. Automatic construction
of a context-aware sentiment lexicon: an optimiza-
tion approach. In Proceedings of the 20th interna-
tional conference on World wide web, pages 347?
356. ACM.
Mary McGlohon, Stephen Bay, Markus G. Anderle,
David M. Steier, and Christos Faloutsos. 2009.
Snare: a link analytic system for graph labeling
and risk detection. In John F. Elder IV, Franoise
Fogelman-Souli, Peter A. Flach, and Mohammed
Zaki, editors, KDD, pages 1265?1274. ACM.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2012. Multilingual subjectivity and sentiment anal-
ysis. In Tutorial Abstracts of ACL 2012, pages 4?4.
Association for Computational Linguistics.
Saif Mohammad and Peter Turney. 2010. Emotions
evoked by common words and phrases: Using me-
chanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 26?34, Los Angeles,
CA, June. Association for Computational Linguis-
tics.
David Newman, Sarvnaz Karimi, and Lawrence Cave-
don. 2009. External evaluation of topic models.
In Australasian Document Computing Symposium,
pages 11?18, Sydney, December.
Shashank Pandit, Duen Horng Chau, Samuel Wang,
and Christos Faloutsos. 2007. Netprobe: a fast and
scalable system for fraud detection in online auction
networks. In WWW, pages 201?210.
Christos H Papadimitriou and Kenneth Steiglitz. 1998.
Combinatorial optimization: algorithms and com-
plexity. Courier Dover Publications.
James W Pennebaker and Lori D Stone. 2003. Words
of wisdom: language use over the life span. Journal
of personality and social psychology, 85(2):291.
John P Pestian, Pawel Matykiewicz, Michelle Linn-
Gust, Brett South, Ozlem Uzuner, Jan Wiebe, K Bre-
tonnel Cohen, John Hurdle, Christopher Brew, et al
2012. Sentiment analysis of suicide notes: A shared
task. Biomedical Informatics Insights, 5(Suppl.
1):3.
Martin J. Pickering and Holly P. Branigan. 1998. The
representation of verbs: Evidence from syntactic
priming in language production. Journal of Mem-
ory and Language, 39:633?651.
1553
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In IJCAI, volume 9, pages
1199?1204.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise
Getoor, Brian Gallagher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
Magazine, 29(3):93?106.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press, Cambridge, MA.
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 70?74. Association for Computational
Linguistics.
Fangzhong Su and Katja Markert. 2009. Subjectiv-
ity recognition on word senses via semi-supervised
mincuts. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 1?9. Association for Com-
putational Linguistics.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 133?140. Association for Computational
Linguistics.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-01), pages 491?502, Freiburg,
Germany.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The via-
bility of web-derived polarity lexicons. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 1065?1072. Asso-
ciation for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Eval-
uation (formerly Computers and the Humanities),
39(2/3):164?210.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: A system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP on
Interactive Demonstrations, pages 34?35. Associa-
tion for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technologies Conference/Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005), Vancouver, CA.
Jonathan S. Yedidia, William T. Freeman, and Yair
Weiss. 2003. Understanding belief propagation and
its generalizations. In Exploring AI in the new mil-
lennium, pages 239?269.
1554
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 78?86,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Gender Attribution: Tracing Stylometric Evidence Beyond Topic and Genre
Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi
Department of Computer Science
Stony Brook University
NY 11794, USA
{rsarawgi, kgajulapalli, ychoi}@cs.stonybrook.edu
Abstract
Sociolinguistic theories (e.g., Lakoff (1973))
postulate that women?s language styles differ
from that of men. In this paper, we explore
statistical techniques that can learn to iden-
tify the gender of authors in modern English
text, such as web blogs and scientific papers.
Although recent work has shown the efficacy
of statistical approaches to gender attribution,
we conjecture that the reported performance
might be overly optimistic due to non-stylistic
factors such as topic bias in gender that can
make the gender detection task easier. Our
work is the first that consciously avoids gender
bias in topics, thereby providing stronger evi-
dence to gender-specific styles in language be-
yond topic. In addition, our comparative study
provides new insights into robustness of var-
ious stylometric techniques across topic and
genre.
1 Introduction
Sociolinguistic theories (e.g., Lakoff (1973)) postu-
late that women?s language styles differ from that
of men with respect to various aspects of communi-
cation, such as discourse behavior, body language,
lexical choices, and linguistic cues (e.g., Crosby
and Nyquist (1977), Tannen (1991), Argamon et al
(2003), Eckert and McConnell-Ginet (2003), Arga-
mon et al (2007)). In this paper, we explore statis-
tical techniques that can learn to identify the gen-
der of authors in modern English text, such as web
blogs and scientific papers, motivated by sociolin-
guistic theories for gender attribution.
There is a broad range of potential applications
across computational linguistics and social science
where statistical techniques for gender attribution
can be useful: e.g., they can help understanding de-
mographic characteristics of user-created web text
today, which can provide new insight to social sci-
ence as well as intelligent marketing and opinion
mining. Models for gender attribution can also help
tracking changes to gender-specific styles in lan-
guage over different domain and time. Gender de-
tectors can be useful to guide the style of writing as
well, if one needs to assume the style of a specific
gender for imaginative writing.
Although some recent work has shown the effi-
cacy of machine learning techniques to gender at-
tribution (e.g., Koppel et al (2002), Mukherjee and
Liu (2010)), we conjecture that the reported perfor-
mance might be overly optimistic under scrutiny due
to non-stylistic factors such as topic bias in gender
that can make the gender detection task easier. In-
deed, recent research on web blogs reports that there
is substantial gender bias in topics (e.g., Janssen and
Murachver (2004), Argamon et al (2007)) as well
as in genre (e.g., Herring and Paolillo (2006)).
In order to address this concern, we perform the
first comparative study of machine learning tech-
niques for gender attribution after deliberately re-
moving gender bias in topics and genre. Further-
more, making the task even more realistic (and chal-
lenging), we experiment with cross-topic and cross-
genre gender attribution, and provide statistical ev-
idence to gender-specific styles in language beyond
topic and genre. Five specific questions we aim to
investigate are:
78
Q1 Are there truly gender-specific characteristics
in language? or are they confused with gender
preferences in topics and genre?
Q2 Are there deep-syntactic patterns in women?s
language beyond words and shallow patterns?
Q3 Which stylometric analysis techniques are ef-
fective in detecting characteristics in women?s
language?
Q4 Which stylometric analysis techniques are ro-
bust against domain change with respect to top-
ics and genre?
Q5 Are there gender-specific language characteris-
tics even in modern scientific text?
From our comparative study of various techniques
for gender attribution, including two publicly avail-
able systems - Gender Genie1 and Gender Guesser2
we find that (1) despite strong evidence for deep
syntactic structure that characterizes gender-specific
language styles, such deep patterns are not as robust
as shallow morphology-level patterns when faced
with topic and genre change, and that (2) there are
indeed gender-specific linguistic signals that go be-
yond topics and genre, even in modern and scientific
literature.
2 Related Work
The work of Lakoff (1973) initiated the research on
women?s language, where ten basic characteristics
of women?s language were listed. Some exemplary
ones are as follows:
1 Hedges: e.g., ?kind of?, ?it seems to be?
2 Empty adjectives: e.g., ?lovely?, ?adorable?,
?gorgeous?
3 Hyper-polite: e.g., ?would you mind ...?, ?I?d
much appreciate if ...?
4 Apologetic: e.g., ?I am very sorry, but I think
that ...?
5 Tag questions: e.g., ?you don?t mind, do you??
1http://bookblog.net/gender/genie.php
2Available at http://www.hackerfactor.com/
GenderGuesser.php
Many sociolinguists and psychologists consequently
investigated on the validity of each of the above
assumptions and extended sociolinguistic theo-
ries on women?s language based on various con-
trolled experiments and psychological analysis (e.g.,
Crosby and Nyquist (1977), McHugh and Ham-
baugh (2010)).
While most theories in socioliguistics and psy-
chology focus on a small set of cognitively identi-
fiable patterns in women?s language (e.g., the use of
tag questions), some recent studies in computer sci-
ence focus on investigating the use of machine learn-
ing techniques that can learn to identify women?s
language from a bag of features (e.g., Koppel et al
(2002), Mukherjee and Liu (2010)).
Our work differs from most previous work in
that we consciously avoid gender bias in topics and
genre in order to provide more accurate analysis
of statistically identifiable patterns in women?s lan-
guage. Furthermore, we compare various techniques
in stylometric analysis within and beyond topics and
genre.
3 Dataset without Unwanted Gender Bias
In this section, we describe how we prepared our
dataset to avoid unwanted gender bias in topics and
genre. Much of previous work has focused on for-
mal writings, such as English literature, newswire
articles and the British Natural Corpus(BNC) (e.g.,
Argamon et al (2003)), while recent studies ex-
panded toward more informal writing such as web
blogs (e.g., Mukherjee and Liu (2010)). In this
work, we chose two very different and prominent
genre electronically available today: web blogs and
scientific papers.
Blogs: We downloaded blogs from popular blog
sites for 7 distinctive topics:3 education, travel, spir-
ituality, entertainment, book reviews, history and
politics. Within each topic, we find 20 articles writ-
ten by male authors, and additional 20 articles writ-
ten by female authors. We took the effort to match
articles written by different gender even at the sub-
topic level. For example, if we take a blog written
about the TV show ?How I met your mother? by a
female author, then we also find a blog written by a
3wordpress.com, blogspot.com & nytimes.
com/interactive/blogs/directory.html
79
male author on the same show. Note that previous
research on web blogs does not purposefully main-
tain balanced topics between gender, thereby bene-
fiting from topic bias inadvertently. From each blog,
we keep the first 450 (+/- 20) words preserving the
sentence boundaries.4 We plan to make this data
publically available.
Scientific Papers: Scientific papers have not been
studied in previous research on gender attribution.
Scientific papers correspond to very formal writ-
ing where gender-specific language styles are not
likely to be conspicuous (e.g., Janssen and Mu-
rachver (2004)).
For this dataset, we collected papers from the re-
searchers in our own Natural Language Processing
community. We randomly selected 5 female and 5
male authors, and collected 20 papers from each au-
thor. We tried to select these authors across a variety
of subtopics within NLP research, so as to reduce
potential topic-bias in gender even in research. It is
also worthwhile to mention that authors in our selec-
tion are highly established ones who have published
over multiple subtopics in NLP.
Similarly as the blog dataset, we keep the first
450 (+/- 20) words preserving the sentence bound-
aries. Some papers are co-authored by researchers
of mixed gender. In those cases, we rely on the gen-
der of the advisory person as she or he is likely to
influence on the abstract and intro the most.
4 Statistical Techniques
In this section, we describe three different types of
statistical language models that learn patterns at dif-
ferent depth. The first kind is based on probabilis-
tic context-free grammars (PCFG) that learn deep
long-distance syntactic patterns (Section 4.1). The
second kind is based on token-level language mod-
els that learn shallow lexico-syntactic patterns (Sec-
tion 4.2). The last kind is based on character-level
language models that learn morphological patterns
on extremely short text spans (Section 4.3). Fi-
nally, we describe the bag-of-word approach using
the maximum entropy classifier (Section 4.4).
4Note that existing gender detection tools require a mini-
mum 300 words for appropriate identification.
4.1 Deep Syntactic Patterns using
Probabilistic Context free Grammar
A probabilistic context-free grammar (PCFG) cap-
tures syntactic regularities beyond shallow ngram-
based lexico-syntactic patterns. Raghavan et al
(2010) recently introduced the use of PCFG for au-
thorship attribution for the first time, and demon-
strated that it is highly effective for learning stylistic
patterns for authorship attribution. We therefore ex-
plore the use of PCFG for gender attribution. We
give a very concise description here, referring to
Raghavan et al (2010) for more details.
(1) Train a generic PCFG parser Go on manually
tree-banked corpus such as WSJ or Brown.
(2) Given training corpus D for gender attribution,
tree-bank each training document di ? D using
the PCFG parser Go.
(3) For each gender ?, train a new gender-specific
PCFG parser G? using only those tree-banked
documents in D that correspond to gender ?.
(4) For each test document, compare the likelihood
of the document determined by each gender-
specific PCFG parser G? , and the gender cor-
responding to the higher score.
Note that PCFG models can be considered as a kind
of language models, where probabilistic context-
free grammars are used to find the patterns in lan-
guage, rather than n-grams. We use the implementa-
tion of Klein and Manning (2003) for PCFG models.
4.2 Shallow Lexico-Syntactic Patterns using
Token-level Language Models
Token-based (i.e. word-based) language models
have been employed in a wide variety of NLP ap-
plications, including those that require stylometric
analysis, e.g., authorship attribution (e.g., Uzner and
Katz (2005)), and Wikipedia vandalism detection
(Wang and McKeown, 2010). We expect that token-
based language models will be effective in learning
shallow lexico-syntactic patterns of gender specific
language styles. We therefore experiment with un-
igram, bigram, and trigram token-level models, and
name them as TLM(n=1), TLM(n=2), TLM(n=3),
respectively, where TLM stands for Token-based
80
lexicon based deep syntax morphology b.o.w. shallow lex-syntax
Gender Gender PCFG CLM CLM CLM ME TLM TLM TLM
Data Type Genie Guesser n=1 n=2 n=3 n=1 n=2 n=3
Male Only 72.1 68.6 53.4 65.8 69.0 63.4 57.6 67.1 67.8 66.2
Female Only 27.1 06.4 74.8 57.6 73.6 76.8 73.8 60.1 64.2 64.2
All 50.0 37.5 64.1 61.70 71.3 70.3 65.8 63.7 66.1 65.4
Table 1: Overall Accuracy of Topic-Balanced Gender Attribution on Blog Data (Experiment-I)
Language Models. We use the LingPipe package5
for experiments.
4.3 Shallow Morphological Patterns using
Character-level Language Models
Next we explore the use of character-level lan-
guage models to investigate whether there are mor-
phological patterns that characterize gender-specific
styles in language. Despite its simplicity, previ-
ous research have reported that character-level lan-
guage models are effective for authorship attribu-
tion (e.g., Peng et al (2003b)) as well as genre
classification (e.g., Peng et al (2003a), Wu et al
(2010)). We experiment with unigram, bigram,
and trigram character-level models, and name them
as CLM(n=1), CLM(n=2), CLM(n=3), respectively,
where CLM stands for Character-based Language
Models. We again make use of the LingPipe pack-
age for experiments.
Note that there has been no previous research
that directly compares the performance of character-
level language models to that of PCFG based models
for author attribution, not to mention for gender at-
tribution.
4.4 Bag of Words using
Maximum Entropy (MaxEnt) Classifier
We include Maximum Entropy classifier using sim-
ple unigram features (bag-of-words) for comparison
purposes, and name it as ME. We use the MALLET
package (McCallum, 2002) for experiments.
5 Experimental Results
Note that our two datasets are created to specifically
answer the following question: are there gender-
specific characteristics in language beyond gender
5Available at http://alias-i.com/lingpipe/
preferences in topics and genre? One way to answer
this question is to test whether statistical models can
detect gender attribution on a dataset that is dras-
tically different from the training data in topic and
genre. Of course, it is a known fact that machine
learning techniques do not transfer well across dif-
ferent domains (e.g., Blitzer et al (2006)). However,
if they can still perform considerably better than ran-
dom prediction, then it would prove that there is in-
deed gender-specific stylometric characteristics be-
yond topic and genre. In what follows, we present
five different experimental settings across two differ-
ent dataset to compare in-domain and cross-domain
performance of various techniques for gender attri-
bution.
5.1 Experiments with Blog Dataset
First we conduct two different experiments using the
blog data in the order of increasing difficulty.
[Experiment-I: Balanced Topic] Using the web
blog dataset introduced in Section 3, we perform
gender attribution (classification) task on balanced
topics. For each topic, 80% of the documents are
used for training and remaining ones are used for
testing, yielding 5-fold cross validation. Both train-
ing and test data have balanced class distributions
so that random guess would yield 50% of accuracy.
The results are given in Table 1. Note that the ?over-
all accuracy? corresponds to the average across the
five folds.
The PCFG model achieves prediction accuracy
64.1%, demonstrating statistical evidence to gender-
specific characteristics in syntactic structure. The
PCFG model outperforms two publicly available
systems - Gender Genie and Gender Guesser, which
are based on a fixed list of indicator words. The dif-
ference is statistically significant (p = 0.01 < 0.05)
81
lexicon based deep syntax morphology b.o.w. shallow lex-syntax
Gender Gender PCFG CLM CLM CLM ME TLM TLM TLM
Topic Genie Guesser n=1 n=2 n=3 n=1 n=2 n=3
Per Topic Accuracy (%) for All Authors
Entertain 50.0 42.5 50.0 52.5 67.5 67.5 60.0 57.5 57.5 57.5
Book 50.0 42.5 65.0 57.5 67.5 72.5 55.0 60.0 67.5 67.5
Politics 35.0 30.0 50.0 47.5 52.5 50.0 45.0 52.5 52.5 52.5
History 40.0 35.0 77.5 65.0 80.0 80.0 55.0 65.0 65.0 65.0
Education 62.5 42.5 55.0 63.0 65.0 70.0 63.0 55.0 57.5 52.5
Travel 62.5 37.5 63.0 65.0 63.0 63.0 63.0 62.5 65.0 65.0
Spirituality 50.0 32.5 53.0 78.0 78.0 78.0 50.0 65.0 70.0 72.5
Avg 50.0 37.5 59.0 61.2 68.3 68.3 55.87 60.0 61.3 61.5
Per Topic Accuracy (%) for Female Authors
Entertain 25.0 10.0 85.0 70.0 50.0 85.0 70.0 75.0 75.0 75.0
Book 15.0 15.0 95.0 80.0 95.0 90.0 85.0 75.0 90.0 90.0
Politics 10.0 05.0 65.0 00.0 05.0 00.0 35.0 30.0 30.0 25.0
History 10.0 05.0 90.0 70.0 80.0 75.0 70.0 50.0 50.0 50.0
Education 45.0 10.0 80.0 95.0 85.0 90.0 100.0 50.0 55.0 50.0
Travel 65.0 00.0 85.0 90.0 100.0 100.0 100.0 85.0 95.0 90.0
Spirituality 20.0 00.0 60.0 65.0 65.0 70.0 45.0 50.0 50.0 50.0
Avg 27.1 06.4 80.0 67.1 68.6 72.9 72.1 59.3 63.6 61.4
Per Topic Accuracy (%) for Male Authors
Entertain 75.0 75.0 15.0 35.0 85.0 50.0 50.0 40.0 40.0 40.0
Book 80.0 70.0 35.0 35.0 40.0 55.0 25.0 45.0 45.0 45.0
Politics 60.0 55.0 35.0 95.0 100.0 100.0 55.0 75.0 75.0 80.0
History 70.0 65.0 65.0 60.0 80.0 85.0 40.0 80.0 80.0 80.0
Education 80.0 75.0 30.0 30.0 45.0 50.0 25.0 60.0 60.0 55.0
Travel 60.0 75.0 40.0 40.0 25.0 25.0 25.0 40.0 35.0 40.0
Spirituality 80.0 65.0 45.0 90.0 90.0 85.0 55.0 80.0 90.0 95.0
Avg 72.1 68.6 37.9 55.0 66.4 64.2 39.3 60.0 60.8 62.1
Table 2: Per-Topic & Per-Gender Accuracy of Cross-Topic Gender Attribution on Blog Data (Experiment-II)
using paired student?s t-test.6
Interestingly, the best performing approaches are
character-level language models, performing sub-
stantially better (71.30% for n=2) than both the
token-level language models (66.1% for n=2) and
the PCFG model (64.10%). The difference between
CLM(n=2) and PCFG is statistically significant (p =
0.015 < 0.05) using paired student?s t-test, while the
difference between TLM(n=2) and PCFG is not.
6We also experimented with the interpolated PCFG model
following Raghavan et al (2010) using various interpolation
dataset, but we were not able to achieve a better result in our
experiments. We omit the results of interpolated PCFG models
for brevity.
As will be seen in the following experiment
(Experiment-II) using the Blog dataset as well, the
performance of PCFG models is very close to that
of unigram language models. As a result, one might
wonder whether PCFG models are learning any use-
ful syntactic pattern beyond terminal productions
that can help discriminating gender-specific styles in
language. This question will be partially answered
in the fourth experiment (Experiment-IV) using
the Scientific Paper dataset, where PCFG models
demonstrate considerably better performance over
the unigram language models.
Following Raghavan et al (2010), we also exper-
82
lexicon based deep syntax morphology b.o.w. shallow lex-syntax
Gender Gender PCFG CLM CLM CLM ME TLM TLM TLM
Data Type Genie Guesser n=1 n=2 n=3 n=1 n=2 n=3
Male Only 85.0 63.0 59.0 96.0 94.0 99.0 62.0 68.0 68.0 68.0
Female Only 9.0 0.0 36.0 10.0 8.0 18.0 61.0 34.0 32.0 32.0
All 47.0 31.5 47.5 53.0 51.0 58.5 61.5 51.0 50.0 50.0
Table 3: Overall Accuracy of Cross-Topic /Cross-Genre Gender Attribution on Scientific Papers (Experiment-III)
imented with ensemble methods that linearly com-
bine the output of different classifiers, but we omit
the results in Table 1, as we were not able to ob-
tain consistently higher performance than the simple
character-level language models in our dataset.
[Experiment-II: Cross-Topic] Next we perform
cross-topic experiments using the same blog dataset,
in order to quantify the robustness of different tech-
niques against topic change. We train on 6 topics,
and test on the remaining 1 topic, making 7-fold
cross validation. The results are shown in Table 2,
where the top one third shows the performance for
all authors, the next one third shows the performance
with respect to only female authors, the bottom one
third shows the performance with respect to only
male authors.
Again, the best performing approaches are based
on character-level language models, achieving upto
68.3% in accuracy. PCFG models and token-level
language models achieve substantially lower accu-
racy of 59.0% and 61.5% respectively. Per-gender
analysis in Table 1 reveals interesting insights into
different approaches. In particular, we find that
Gender Genie and Gender Guesser are biased to-
ward male authors, attributing the majority authors
as male. PCFG and ME on the other hand are bi-
ased toward female authors. Both character-level
and token-level language models show balanced dis-
tribution between gender. We also experimented
with ensemble methods, but omit the results as we
were not able to obtain higher scores than simple
character-level language models.
From these two experiments so far, we find that
PCFG models and word-level language models are
neither as effective, nor as robust as character-level
language models for gender attribution. Despite
overall low performance of PCFG models, this re-
sult suggests that PCFG models are able to learn
gender-specific syntactic patterns, albeit the signals
from deep syntax seem much weaker than those of
very shallow morphological patterns.
5.2 Experiments with Scientific Papers
Next we present three different experiments using
the scientific data, in the order of decreasing diffi-
culty.
[Experiment-III: Cross-Topic & Cross-Genre]
In this experiment, we challenge statistical tech-
niques for gender attribution by changing both top-
ics and genre across training and testing. To do so,
we train models on the blog dataset and test on the
scientific paper dataset. Notice that this is a dramati-
cally harder task than the previous two experiments.
Note also that previous research thus far has not
reported experiments such as this, or even like the
previous one. It is worthwhile to mention that our
goal in this paper is not domain adaptation for gen-
der attribution, but merely to quantify to what degree
the gender-specific language styles can be traced
across different topics and genre, and which tech-
niques are robust against domain change.
The results are shown in Table 5. Precisely as ex-
pected, the performance of all models drop signif-
icantly in this scenario. The two baseline systems
? Gender Genie and Gender Guesser, which are not
designed for formal scientific writings also perform
worse in this dataset. Table 4 discussed in the next
experiment will provide more insight into this by
providing per-gender accuracy of these baseline sys-
tems.
From this experiment, we find a rather surprising
message: although the performance of most statis-
tical approaches decreases significantly, notice that
most approaches perform still better than random
(50%) prediction, achieving upto 61.5% accuracy.
83
lexicon based deep syntax morphology b.o.w. shallow lex-syntax
Gender Gender PCFG CLM CLM CLM ME TLM TLM TLM
Data Type Genie Guesser n=1 n=2 n=3 n=1 n=2 n=3
Per Author Accuracy (%) for All Authors
All 47.0 31.5 76.0 73.0 72.0 76.0 70.50 63.5 62.5 62.5
Per Author Accuracy (%) for Male Authors
A 80.0 55.0 75.0 100.0 100.0 100.0 45.0 45.0 40.0 40.0
B 90.0 75.0 75.0 80.0 70.0 85.0 55.0 45.0 40.0 40.0
C 95.0 55.0 85.0 85.0 90.0 95.0 90.0 90.0 90.0 90.0
D 85.0 65.0 100.0 95.0 100.0 100.0 100.0 100.0 100.0 100.0
E 75.0 65.0 90.0 70.0 85.0 80.0 70.0 70.0 70.0 60.0
Avg 85.0 63.0 85.0 86.0 89.0 92.0 72.0 70.0 68.0 66.0
Per Author Accuracy (%) for Female Authors
F 15.0 0.0 95.0 05.0 30.0 75.0 100.0 85.0 85.0 85.0
G 5.0 0.0 25.0 55.0 70.0 85.0 75.0 80.0 85.0 85.0
H 10.0 0.0 65.0 70.0 45.0 35.0 40.0 35.0 30.0 30.0
I 15.0 0.0 80.0 85.0 45.0 50.0 65.0 35.0 35.0 35.0
J 0.0 0.0 70.0 85.0 85.0 85.0 65.0 50.0 50.0 60.0
Avg 9.0 0.0 67.0 60.0 55.0 66.0 69.0 57.0 57.0 59.0
Table 4: Per-Author Accuracy of Cross-Topic Gender Attribution for Scientific Papers (Experiment-IV)
Considering that the models are trained on dras-
tically different topics and genre, this result sug-
gests that there are indeed gender-specific linguis-
tic signals beyond different topics and genre. This
is particularly interesting given that scientific papers
correspond to very formal writing where gender-
specific language styles are not likely to be conspic-
uous (e.g., Janssen and Murachver (2004)).
[Experiment-IV: Cross-Topic] Next we perform
cross-topic experiment, only using the scientific pa-
per dataset. Because the stylistic difference in genre
is significantly more prominent than the stylistic dif-
ference in topics, this should be a substantially eas-
ier task than the previous experiment. Nevertheless,
previous research to date has not attempted to eval-
uate gender attribution techniques across different
topics. Here we train on 4 authors per gender (8 au-
thors in total), and test on the remaining 2 authors,
making 5-fold cross validation. As before, the class
distributions are balanced in both training and test
data.
The experimental results are shown in Table 4,
where we report per-author, per-gender, and overall
average accuracy. As expected, the overall perfor-
mance increase dramatically, as models are trained
on articles in the same genre. It is interesting to
see how Gender Genie and Gender Guesser are ex-
tremely biased toward male authors, achieving al-
most zero accuracy with respect to articles written
by female authors. Here the best performing models
are PCFG and CLM(n=3), both achieving 76.0% in
accuracy. Token-level language models on the other
hand achieve significantly lower performance.
Remind that in the first two experiments based
on the blog data, PCFG models and token-level lan-
guage models performed similarly. Given that, it is
very interesting that PCFG models now perform just
as good as character-level language models, while
outperforming token-level language models signifi-
cantly. We conjecture following two reasons to ex-
plain this:
? First, scientific papers use very formal lan-
guage, thereby suppressing gender-specific lex-
ical cues that are easier to detect (e.g., empty
words such as ?lovely?, ?gorgeous? (Lakoff,
1973)). In such data, deep syntactic patterns
play a much stronger role in detecting gender
specific language styles. This also indirectly
84
lexicon based deep syntax morphology b.o.w. shallow lex-syntax
Gender Gender PCFG CLM CLM CLM ME TLM TLM TLM
Data Type Genie Guesser n=1 n=2 n=3 n=1 n=2 n=3
Male Only 85.0 63.0 86.0 92.0 92.0 91.0 86.0 86.0 87.0 88.0
Female Only 9.0 0.0 84.0 88.0 87.0 92.0 91.0 83.0 84.0 86.0
All 47.0 31.5 85.0 90.0 88.50 91.50 88.50 85.0 85.5 87.0
Table 5: Overall Accuracy of Topic-Balanced Gender Attribution on Scientific Papers (Experiment-V)
addresses the concern raised in Experiment-I
& II as to whether the PCFG models are learn-
ing any syntactic pattern beyond terminal pro-
ductions that are similar to unigram language
models.
? Second, our dataset is constructed in such a
way that the training and test data do not share
articles written by the same authors. Further-
more, the authors are chosen so that the main
research topics are substantially different from
each other. Therefore, token-based language
models are likely to learn topical words and
phrases, and suffer when the topics change dra-
matically between training and testing.
[Experiment-V: Balanced Topic] Finally, we
present the conventional experimental set up, where
topic distribution is balanced between training and
test dataset. This is not as interesting as the previous
two scenarios, however, we include this experiment
in order to provide a loose upper bound. Because
we choose each different author from each different
sub-topic of research, we need to split articles by the
same author into training and testing to ensure bal-
anced topic distribution. We select 80% of articles
from each author as training data, and use the re-
maining 20% as test data, resulting in 5-fold cross
validation.
This is the easiest task among the three exper-
iments using the scientific paper data, hence the
performance increases substantially. As before,
character-level language models perform the best,
with CLM n=3 reaching extremely high accuracy
of 91.50%. All other statistical approaches perform
very well achieving at least 85% or higher accuracy.
Note that token-level language models perform
very poorly in the previous experimental setting,
while performing close to the top performer in this
experiment. We make the following two conclusions
based on the last two experiments:
? Token-level language models have the ten-
dency of learning topics words, rather than just
stylometric cues.
? When performing cross-topic gender attribu-
tion (as in Experiment-IV), PCFG models are
more robust than token-level language models.
6 Conclusions
We postulate that previous study in gender attribu-
tion might have been overly optimistic due to gen-
der specific preference on topics and genre. We per-
form the first comparative study of machine learn-
ing techniques for gender attribution consciously re-
moving gender bias in topics. Rather unexpect-
edly, we find that the most robust approach is based
on character-level language models that learn mor-
phological patterns, rather than token-level language
models that learn shallow lexico-syntactic patterns,
or PCFG models that learn deep syntactic patterns.
Another surprising finding is that we can trace sta-
tistical evidence of gender-specific language styles
beyond topics and genre, and even in modern scien-
tific papers.
Acknowledgments
We thank reviewers for giving us highly insightful
and valuable comments.
References
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and
Anat Rachel Shimoni. 2003. Gender, genre, and writ-
ing style in formal written texts. Text, 23.
Shlomo Argamon, Moshe Koppel, James W. Pennebaker,
and Jonathan Schler. 2007. Mining the blogosphere:
85
Age, gender and the varieties of selfexpression. In
First Monday, Vol. 12, No. 9.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Methods
in Natural Language Processing, Sydney, Australia.
Faye Crosby and Linda Nyquist. 1977. The female reg-
ister: an empirical study of lakoff?s hypotheses. In
Language in Society, 6, pages 313 ? 322.
Penelope Eckert and Sally McConnell-Ginet. 2003. Lan-
guage and gender. Cambridge University Press.
Susan C. Herring and John C. Paolillo. 2006. Gender
and genre variations in weblogs. In Journal of Soci-
olinguistics, Vol. 10, No. 4., pages 439 ?459.
Anna Janssen and Tamar Murachver. 2004. The relation-
ship between gender and topic in gender-preferential
language use. In Written Communication, 21, pages
344? 367.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423?430. Association for Computa-
tional Linguistics.
Moshe Koppel, Shlomo Argamon, and Anat Shimoni.
2002. Automatically categorizing written texts by
author gender. Literary and Linguistic Computing,
17(4):401?412, June.
Robin T. Lakoff. 1973. Language and woman?s place. In
Language in Society, Vol. 2, No. 1, pages 45 ? 80.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Maureen C. McHugh and Jennifer Hambaugh. 2010.
She said, he said: Gender, language, and power. In
Handbook of Gender Research in Psychology. Volume
1: Gender Research in General and Experimental Psy-
chology, pages 379 ? 410.
Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 207?217,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Fuchun Peng, Dale Schuurmans, Vlado Keselj, and Shao-
jun Wang. 2003a. Language independent authorship
attribution with character level n-grams. In EACL.
Funchun Peng, Dale Schuurmans, and Shaojun Wang.
2003b. Language and task independent text catego-
rization with simple language models. In Proceedings
of the 2003 Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proceedings of the
ACL, pages 38?42, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Deborah Tannen. 1991. You just don?t understand:
Women and men in conversation. Ballantine Books.
Ozlem Uzner and Boris Katz. 2005. A Compara-
tive Study of Language Models for Book And Au-
thor Recognition. In Second International Joint Con-
ference on Natural Language Processing:Full Papers,
pages 1969?980. Association for Computational Lin-
guistics.
William Yang Wang and Kathleen R. McKeown.
2010. ?got you!?: Automatic vandalism detec-
tion in wikipedia with web-based shallow syntactic-
semantic modeling. In 23rd International Conference
on Computational Linguistics (Coling 2010), page
1146?1154.
Zhili Wu, Katja Markert, and Serge Sharoff. 2010. Fine-
grained genre classification using structural learning
algorithms. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 749?759, Uppsala, Sweden, July. Association
for Computational Linguistics.
86
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 220?228,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Composing Simple Image Descriptions using Web-scale N-grams
Siming Li, Girish Kulkarni, Tamara L Berg, Alexander C Berg, and Yejin Choi
Department of Computer Science
Stony Brook University
NY 11794, USA
{silli, gkulkarni, tlberg, aberg, ychoi}@cs.stonybrook.edu
Abstract
Studying natural language, and especially how
people describe the world around them can
help us better understand the visual world. In
turn, it can also help us in the quest to generate
natural language that describes this world in a
human manner. We present a simple yet effec-
tive approach to automatically compose im-
age descriptions given computer vision based
inputs and using web-scale n-grams. Unlike
most previous work that summarizes or re-
trieves pre-existing text relevant to an image,
our method composes sentences entirely from
scratch. Experimental results indicate that it is
viable to generate simple textual descriptions
that are pertinent to the specific content of an
image, while permitting creativity in the de-
scription ? making for more human-like anno-
tations than previous approaches.
1 Introduction
Gaining a better understanding of natural language,
and especially natural language associated with im-
ages helps drive research in both computer vision
and natural language processing (e.g., Barnard et
al. (2003), Pastra et al (2003), Feng and Lapata
(2010b)). In this paper, we look at how to exploit
the enormous amount of textual data electronically
available today, web-scale n-gram data in particular,
in a simple yet highly effective approach to com-
pose image descriptions in natural language. Auto-
matic generation of image descriptions differs from
automatic image tagging (e.g., Leong et al (2010))
in that we aim to generate complex phrases or sen-
tences describing images rather than predicting in-
dividual words. These natural language descriptions
can be useful for a variety of applications, includ-
ing image retrieval, automatic video surveillance,
and providing image interpretations for visually im-
paired people.
Our work contrasts to most previous approaches
in four key aspects: first, we compose fresh sen-
tences from scratch, instead of retrieving (Farhadi et
al. (2010)), or summarizing existing text fragments
associated with an image (e.g., Aker and Gaizauskas
(2010), Feng and Lapata (2010a)). Second, we aim
to generate textual descriptions that are truthful to
the specific content of the image, whereas related
(but subtly different) work in automatic caption gen-
eration creates news-worthy text (Feng and Lapata
(2010a)) or encyclopedic text (Aker and Gaizauskas
(2010)) that is contextually relevant to the image, but
not closely pertinent to the specific content of the
image. Third, we aim to build a general image de-
scription method as compared to work that requires
domain specific hand-written grammar rules (Yao et
al. (2010)). Last, we allow for some creativity in
the generation process which produces more human-
like descriptions than a closely related, very recent
approach that drives annotation more directly from
computer vision inputs (Kulkarni et al, 2011).
In this work, we propose a novel surface realiza-
tion technique based on web-scale n-gram data. Our
approach consists of two steps: (n-gram) phrase se-
lection and (n-gram) phrase fusion. The first step
? phrase selection ? collects candidate phrases that
may be potentially useful for generating the descrip-
tion of a given image. This step naturally accom-
modates uncertainty in image recognition inputs as
220
Hairy goat under a tree 
Fluffy posturing sheep under a tree 
<furry;gray;brown,sheep>,by;near,<rusty;gray;green,tree> 
furry 
gray 
brown 
rusty 
gray 
green 
by 
near 
Figure 1: The big picture of our task to automatically
generate image description.
well as synonymous words and word re-ordering to
improve fluency. The second step ? phrase fusion
? finds the optimal compatible set of phrases us-
ing dynamic programming to compose a new (and
more complex) phrase that describes the image. We
compare the performance of our proposed approach
to three baselines based on conventional techniques:
language models, parsers, and templates.
Despite its simplicity, our approach is highly ef-
fective for composing image descriptions: it gen-
erates mostly appealing and presentable language,
while permitting creative writing at times (see Fig-
ure 5 for example results). We conclude from our
exploration that (1) it is viable to generate simple
textual descriptions that are germane to the specific
image content, and that (2) world knowledge implic-
itly encoded in natural language (e.g., web-scale n-
gram data) can help enhance image content recogni-
tion.
2 Image Recognition
Figure 1 depicts our system flow: a) an image is in-
put into our system, b) image recognition techniques
are used to extract visual content information, c) vi-
sual content is encoded as a set of triples, d) natural
language descriptions are generated.
In this section, we briefly describe the image
recognition system that extracts visual information
and encodes it as a set of triples. For a given image,
the image recognizer extracts objects, attributes and
spatial relationships among objects as follows:
1. Objects: including things (e.g., bird, bus, car)
and stuff (e.g., grass, water, sky, road) are de-
tected.
2. Visual attributes (e.g., feathered, black) are pre-
dicted for each object.
3. Spatial relationships (e.g., on, near, under) be-
tween objects are estimated.
In particular, object detectors are trained using state
of the art mixtures of multi-scale deformable parts
models (Felzenszwalb et al, 2010). Our set of
objects encompasses the 20 PASCAL 2010 object
challenge 1 categories as well as 4 additional cate-
gories for flower, laptop, tiger, and window trained
on images with associated bounding boxes from
Imagenet (Deng et al, 2009). Stuff detectors are
trained to detect regions corresponding to non-part
based object categories (sky, road, building, tree,
water, and grass) using linear SVMs trained on
the low level region features of (Farhadi et al,
2009). These are also trained on images with la-
beled bounding boxes from ImageNet and evaluated
at test time on a coarsely sampled grid of overlap-
ping square regions over whole images. Pixels in
any region with a classification probability above a
fixed threshold are treated as detections.
We select visual attribute characteristics that are
relevant to our object and stuff categories. Our at-
tribute terms include 21 visual modifiers ? adjec-
tives ? related to color (e.g. blue, gray), texture
(e.g. striped, furry), material (e.g. wooden, feath-
ered), general appearance (e.g. rusty, dirty, shiny),
and shape (e.g. rectangular) characteristics. The at-
tribute classifiers are trained on the low level fea-
tures of (Farhadi et al, 2009) using RBF kernel
SVMs. Preposition functions encoding spatial rela-
tionships between objects are hand designed to eval-
uate the spatial relationships between pairs of re-
gions in an image and provide a score for 16 prepo-
sitions (e.g., above, under, against, in etc).
1http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2010/
221
From these three types of visual output, we con-
struct a meaning representation of an image as a
set of triples (one triple for every pair of detected
objects). Each triple encodes a spatial relation be-
tween two objects in the following format: <<adj1,
obj1>, prep, <adj2, obj2>>. The generation pro-
cedure is elaborated in the following two sections.
3 Baseline Approaches to Surface
Realization
This section explores three baseline surface realiza-
tion approaches: language models (?3.1), random-
ized local search (?3.2), and template-based (?3.3).
Our best approach, phrase fusion using web-scale n-
grams follows in ?4.
3.1 Language Model Based Approach
For each triple, as described in ?2, we construct a
sentence. For instance, given the triple <<white,
cloud>, in, <blue, sky>>, we might generate
?There is a white cloud in the blue sky?.
We begin with a simple decoding scheme based
on language models. Let t be a triple, and let V t
be the set of words in t. We perform surface real-
ization by adding function words in-between words
in V t. As a concrete example, suppose we want to
determine whether to insert a function word x be-
tween a pair of words ? ? V t and ? ? V t. Then,
we need to compare the length-normalized probabil-
ity p?(?x?) with p?(??), where p? takes the n?th root
of the probability p for n-word sequences. We in-
sert the new function word x if p?(?x?) ? p?(??)
using the n-gram models, where the probability of
any given sequence w1, ..., wm is approximated by
p(w1, ..., wm) =
m?
i=1
p(wi|wi?(n?1), ..., wi?1)
Note that if we wish to reorder words in V t based on
n-gram based language models, then the decoding
problem becomes an instance of asymmetric trav-
eler?s salesman problem (NP-hard). For brevity, we
retain the original order of words in the given triple.
We later lift this restriction using the web-scale n-
gram based phrase fusion method introduced in ?4.
3.2 Randomized Local Search Approach
A much needed extension to the language model
based surface realization is incorporating parsers to
Begin Loop (until T iterations or convergence)
Choose a position i to revise at random
Choose an edit operation at random
If the edit yields a better score by LM and PCFG
Commit the edit
End Loop
Table 1: Pseudo code for a randomized local search ap-
proach. A possible edit operation includes insertion,
deletion, and replacement. The score of the current sen-
tence is determined by the multiplication LM-based prob-
ability and PCFG-based probability.
enforce long distance regularities for more gram-
matically correct generation. However, optimiz-
ing both language-model-based probabilities and
parser-based probabilities is intractable. Therefore,
we explore a randomized local search approach that
makes greedy revisions using both language models
and parsers. Randomized local search has been suc-
cessfully applied to intractable optimization prob-
lems in AI (e.g., Chisholm and Tadepalli (2002)) and
NLP (e.g., White and Cardie (2002)).
Table 1 shows the skeleton of the algorithm in our
study. Iterating through a loop, it chooses an edit
location and an edit operation (insert, delete, or re-
place) at random. If the edit yields a better score,
then we commit the edit, otherwise we jump to the
next iteration of the loop. We define the score as
score(X) = p?LM (X)p?PCFG(X)
where X is a given sentence (image description),
p?LM (X) is the length normalized probability of X
based on the language model, and p?PCFG(X) is the
length normalized probability of X based on the
probabilistic context free grammar (PCFG) model.
The loop is repeated until convergence or a fixed
number of iterations is reached. Note that this ap-
proach can be extended to simulated annealing to al-
low temporary downward steps to escape from local
maxima. We use the PCFG implementation of Klein
and Manning (2003).
3.3 Template Based Approach
The third approach is a template-based approach
with linguistic constraints, a technique that has of-
ten been used for various practical applications such
as summarization (Zhou and Hovy, 2004) and dia-
222
blue, bike  [2669]  blue, bicycle  [1365]  bike, blue  [1184]  blue, cycle  [324]  cycle, of, the, blue  [172]  cycle, blue  [158]  bicycle, blue  [154]  bike, in, blue  [98]  cycle, of, blue  [64]  bike, with, blue  [43]  
< < blue , bicycle >, near, < shiny , person > >  
bright, boy  [8092]  bright, child  [7840]  bright, girl  [6191]  bright, kid  [5873]  bright, person  [5461 ]  bright, man  [4936]  bright, woman  [2726]  bright, women  [1684]  lady, bright  [1360]  bright, men  [1050]  
person, operating, a, bicycle  [3409]  man, on, a, bicycle  [2842]  cycle, of, child  [2507]  bike, for, men  [2485]  person, riding, a, bicycle  [2118]  cycle, in, women  [1853]  bike, for, women  [1442]  boy, on, a, bicycle  [1378]  cycle, of, women  [1288]  man, on, a, bike  [1283]  
bright person operating a blue bicycle [2541 158 938 5] bright man on a blue bicycle [1914 83 72 88 0]  bright man on a blue bike [1690 24 78 07 2]  bright person riding a blue bicycle [157 881 332 70]  bright boy on a blue bicycle [1522 08 09 24 0]  blue bike for bright men [6964 08 82 50 ]  blue bike for bright women [648120 743 2]  blue cycle of bright child [6368 18 11 20 ]  blue cycle in bright women [1011 02 64 48 ]  
Figure 2: Illustration of phrase fusion composition al-
gorithm using web-scale n-grams. Numbers in square
brackets are n-gram frequencies.
logue systems (Channarukul et al, 2003). Because
the meaning representation produced by the image
recognition system has a fixed pattern of <<adj1,
obj1>, prep, <adj2, obj2>>, it can be templated as
?There is a [adj1] [obj1] [prep] the [adj2] [obj2].?
We also include templates that encode basic dis-
course constraints. For instance, the template that
generated the first sentences in Figure 3 and 4 is:
[PREFIX] [#(x1)] [x1], [#(x2)] [x2], ... and [#(xk)]
[xk], where xi is the name of an object (e.g. ?cow?),
#(xi) is the number of instances of xi (e.g. ?one?),
and PREFIX ? {?This picture shows?, ?This is a pic-
ture of?, etc}.
Although this approach can produce good looking
sentences in a limited domain, there are many limita-
tions. First, a template-based approach does not al-
low creative writing and produces somewhat stilted
prose. In particular, it cannot add interesting new
words, or replace existing content words with better
ones. In addition, such an approach does not allow
any reordering of words which might be necessary to
create a fluent sentence. Finally, hand-written rules
are domain-specific, and do not generalize well to
new domains.
4 Surface Realization by Phrase Fusion
using Web-scale N-gram
We now introduce an entirely different approach
that addresses the limitations of the conventional ap-
proaches discussed in ?3. This approach is based
on web-scale n-gram, also known as Google Web
1T data, which provides the frequency count of each
possible n-gram sequence for 1 ? n ? 5.
4.1 [Step I] ? Candidate Phrase Selection
We first define three different sets of phrases for each
given triple <<adj1, obj1>, prep, <adj2, obj2>>:
? O1 = {(x, f) | x is an n-gram phrase describ-
ing the first object using the words adj1 and
obj1, and f is the frequency of x}
? O2 = {(x, f) | x is an n-gram phrase describ-
ing the second object using the words adj2 and
obj2, and f is the frequency of x}
? R = {(x, f) | x is an n-gram describing the re-
lation between the two objects using the words
obj1 and obj2, and f is the frequency of x}
We find n-gram phrases for O1, O2, andR from the
Google Web 1T data. The search patterns for O1 is:
? [adj1] [?]n?2 [obj1]
? [obj1] [?]n?2 [adj1]
where [?] is a wildcard word, and [?]n?2 denotes
a sequence of n-2 number of wildcard words in a n-
gram sequence. For wildcards, we only allow a lim-
ited set of function words, and verbs in the gerund
form2 for reasons that will become clearer in the
next step ? phrase fusion in ?4.2.
Note that it is the second pattern that allows
interesting re-ordering of words in the final sen-
tence generation. For instance, suppose adj1=green,
obj1=person. Then it is more natural to generate
a phrase using the reverse pattern such as, ?person
in green? or ?person wearing green? than simply
concatenating adj1 and obj1 to generate ?green per-
son?. Similarly, given obj1=bicycle and obj2=man,
generating a phrase using the reverse pattern, e.g.,
?man with a bicycle? would be more natural than
?bicycle with a man?. Our hypothesis is that such
ordering preference is implicitly encoded in the
web-scale n-grams via frequencies.
It is worthwhile to note that our pattern matching
is case sensitive, and we only allow patterns that are
2We treat words with suffix ?ing? as gerund for simplicity.
223
Phrase Fusion Black bull eating fluffy grass Bright apartment in the blue sky Shaggy grass looking at the blue sky 
Local Search the black cow or even by the furry grass.  the shiny building up by the same blue sky. the furry grass be below over one  blue sky. 
Template   This picture shows one cow, one building, one grass and one sky. The black cow is by the shiny building, and by the furry grass, and by the blue sky. The shiny building is by the furry grass, and by the blue sky. The furry grass is below the blue sky.  
Simple decoding the black cow or by the furry grass. the shiny building up by the blue sky. the furry grass be below one  blue sky.  
Image Recognition Output as Tripes: <black;yellow;rusty,cow>,by;near;by,<furry;green;brown,grass>  <shiny;colorful;yellow,building>,by;near;by,<blue;clear;colorful,sky>  <furry;green;brown,grass>,below;beneath;by,<blue;clear;colorful,sky> 
Figure 3: Comparison of image descriptions
all lower-case. From our pilot study, we found that
n-grams with upper case characters are likely from
named entities, which distort the n-gram frequency
distribution that we rely on during the phrase fusion
phase. To further reduce noise, we also discard any
n-gram that contains a character that is not an alpha-
bet.
Accommodating Uncertainty We extend candi-
date phrase selection in order to cope with uncer-
tainty from the image recognition. In particular,
for each object detection obji, we include its top 3
predicted modifiers adji1, adji2, adji3 determined
by the attribute classifiers (see ?2) to expand the
set O1 and O2 accordingly. For instance, given
adji =(shiny or white) and obji = sheep, we can
consider both <shiny,sheep> and <white,sheep>
pairs to predict more compatible pairs of words.
Accommodating Synonyms Additionally, we
augment each modifier adji and each object name
obji with synonyms to further expand our sets
O1, O2, and R. These expanded sets of phrases
enable resulting generations that are more fluent
and creative.
4.2 [Step II] ? Phrase Fusion
Given the expanded sets of phrases O1, O2, and R
described above, we perform phrase fusion to gen-
erate simple image description. In this step, we find
the best combination of three phrases, (x?1, f?1) ?
O1, (x?2, f?2) ? O2, and (x?R, f?R) ? R as follows:
(x?1, x?2, x?R) = argmaxx1,x2,xRscore(x1, x2, xR) (1)
score(x1, x2, xR) = ?(x1)? ?(x2)? ?(xR) (2)
s.t. x?1 and x?R are compatible
& x?2 and x?R are compatible
Two phrases x?i and x?R are compatible if they share
the same object noun obji. We define the phrase-
level score function ?(?) as ?(xi) = fi using the
Google n-gram frequencies. The equation (2) can be
maximized using dynamic programming, by align-
ing the decision sequence as x?1 ? x?R ? x?2.
Once the best combination ? (x?1, x?2, x?R) is de-
termined, we fuse the phrases by replacing the word
obj1 in the phrase x?R with the corresponding phrase
x?1. Similarly, we replace the word obj2 in the phrase
x?R with the other corresponding phrase x?2. Because
the wildcard words ? [?] in ?4.1 allow only a lim-
ited set of function words and gerund, the resulting
phrase is highly likely to be grammatically correct.
Computational Efficiency One advantage of our
phrase fusion method is its efficiency. If we were
to attempt to re-order words with language mod-
els in a naive way, we would need to consider all
possible permutations of words ? an NP-hard prob-
lem (?3.1). However, our phrase fusion method is
clever in that it probes reordering only on selected
pairs of words, where reordering is likely to be use-
ful. In other words, our approach naturally ignores
most word pairs that do not require reordering and
has a time complexity of only O(K2n), where K is
the maximum number of candidate phrases of any
phrase type, and n is the number of phrase types in
each sentence. K can be kept as a small constant by
selecting K-best candidate phrases of each phrase
type. We set K = 10 in this paper.
5 Experimental Results
To construct the training corpus for language mod-
els, we crawled Wikipedia pages that describe our
object set. For evaluation, we use the UIUC PAS-
CAL sentence dataset3 which contains upto five
human-generated sentences that describing 1000 im-
ages. Note that all of the approaches presented in
3http://vision.cs.uiuc.edu/pascal-sentences/
224
Phrase fusion  shiny motorcycle nearby shiny motorcycle.   black women operating a shiny motorcycle.   bright boy on a shiny motorcycle.   girl showing pink on a shiny motorcycle.  
Local search the shiny motorbike or against the shiny motorbike. the shiny motorbike or by the black person. the shiny motorbike or by the shiny person. the shiny motorbike or by the pink person. 
Simple Decoding  the shiny motorbike or against the shiny motorbike. the shiny motorbike or by the black person. the shinny motorbike or by the shiny boy. the shiny motorbike or by the pink person. 
Template  This is a picture of two motorbikes, three persons, one building and one tree. The first shiny motorbike is against the second shiny motorbike, and by the first black person. The second shiny motorbike is by the first black person, and by the second shiny person, and by the third pink person. 
Image Recognition Output as Triples: < < shiny; black; rusty , motorbike >, against; by; in , < shiny; black; rusty , motorbike > > < < shiny; black; rusty , motorbike >, by; near; by , < black; shiny; rusty , person > > < < shiny; black; rusty , motorbike >, by; near; by , < pink; rusty; striped , person > > 
Figure 4: Comparison of image descriptions
Section 3 and 4 attempt to insert function words for
surface realization. In this work, we limit the choice
of function words to only those words that are likely
to be necessary in the final output.4 For instance, we
disallow function words such as ?who? or ?or?.
Before presenting evaluation results, we present
some samples of image descriptions generated by 4
different approaches in Figure 3 and 4. Notice that
only the PHRASE FUSION approach is able to in-
clude interesting and adequate verbs, such as ?eat-
ing? or ?looking? in Figure 3, and ?operating? in
Figure 4. Note that the choice of these action verbs
is based only on the co-occurrence statistics encoded
in n-grams, without relying on the vision compo-
nent that specializes in action recognition. These ex-
amples therefore demonstrate that world knowledge
implicitly encoded in natural language can help en-
hance image content recognition.
Automatic Evaluation: BLEU (Papineni et al,
2002) is a widely used metric for automatic eval-
uation of machine translation that measures the n-
gram precision of machine generated sentences with
respect to human generated sentences. Because our
task can be viewed as machine translation from im-
ages to text, BLEU (Papineni et al, 2002) may seem
4This limitation does not apply to TEMPLATE.
w/o w/ syn
LANGUAGE MODEL 0.094 0.106
TEMPLATE 0.087 0.096
LOCAL SEARCH 0.100 0.111
PHRASE FUSION (any best) 0.149 0.153
PHRASE FUSION (best w/ gerund) 0.146 0.149
Human 0.500 0.510
Table 2: Automatic Evaluation: BLEU measured at 1
Creativ. Fluency Relevan.
LANGUAGE MODEL 2.12 1.96 2.09
TEMPLATE 2.04 1.7 1.96
LOCAL SEARCH 2.21 1.96 2.04
PHRASE FUSION 1.86 1.97 2.11
Table 3: Human Evaluation: the scores range over 1 to 3,
where 1 is very good, 2 is ok, 3 is bad.
like a reasonable choice. However, there is larger
inherent variability in generating sentences from im-
ages than translating a sentence from one language
to another. In fact two people viewing the same pic-
ture may produce quite different descriptions. This
means BLEU could penalize many correctly gener-
ated sentences, and be poorly correlated with human
judgment of quality. Nevertheless we report BLEU
scores in absence of any other automatic evaluation
method that serves our needs perfectly.
The results are shown in Table 2 ? first column
shows BLEU score considering exact matches, sec-
ond column shows BLEU with full credit for syn-
onyms. To give a sense of upper bound and to see
some limitations of the BLEU score, we also com-
pute the BLEU score between human-generated sen-
tences by computing the BLEU score of the first hu-
man sentence with respect to the others.
There is one important factor to consider when in-
terpreting Table 2. The four approaches explored
in this paper are purposefully prolific writers in that
they generate many more sentences than the num-
ber of sentences in the image descriptions written by
humans (available in the UIUC PASCAL dataset).
In this work, we do not perform sentence selection
to reduce the number of sentences in the final out-
put. Rather, we focus on the quality of each gener-
ated sentence. The consequence of producing many
225
Way rusty the golden cow 
Golden cow in the golden sky 
Tree snowing black train 
Black train under the tree Rusty girl sitting at a white table White table in the clear sky 
Rusty girl living in the clear sky 
Blue path up in the clear sky 
Blue path to colored fishing boat 
Blue path up in the clear 
morning sky 
rusty chair for rusty dog.  
rusty dog under the rusty chair.  
rusty dog sitting in a rusty chair. 
Gray cat from a burning gray 
building 
Gray building with a gray cat. 
Gray building in the white sky 
 
Shaggy dog knotting hairy men 
Pink flowering plant the hairy dog 
Pink dog training shaggy dog 
Shaggy dog relaxing on a colored sofa 
 
black women hanging 
from a black tree.  
colored man in the tree. 
1 2 3 4 
5 
6 7 
8 
Figure 5: Sample image descriptions using PHRASE FUSION: some of the unexpected or poetic descriptions are
highlighted in boldface, and some of the interesting incorrect descriptions are underlined.
more sentences in our output is overall lower BLEU
scores, because BLEU precision penalizes spurious
repetitions of the same word, which necessarily oc-
curs when generating more sentences. This is not an
issue for comparing different approaches however,
as we generate the same number of sentences for
each method.
From Table 2, we find that our final approach ?
PHRASE FUSION based on web-scale n-grams per-
forms the best. Notice that there are two different
evaluations for PHRASE FUSION: the first one is
evaluated for the best combination of phrases (Equa-
tion (1)), while the second one is evaluated for the
best combination of phrases that contained at least
one gerund.
Human Evaluation: As mentioned earlier, BLEU
score has some drawbacks including obliviousness
to correctness of grammar and inability to evaluate
the creativity of a composition. To directly quantify
these aspects that could not be addressed by BLEU,
we perform human judgments on 120 instances for
the four proposed methods. Evaluators do not have
any computer vision or natural language generation
background.
We consider the following three aspects to eval-
uate the our image descriptions: creativity, fluency,
and relevance. For simplicity, human evaluators as-
sign one set of scores for each aspect per image. The
scores range from 1 to 3, where 1 is very good, 2 is
ok, and 3 is bad.5 The definition and guideline for
each aspect is:
[Creativity] How creative is the generated sen-
tence?
1 There is creativity either based on unexpected
words (in particular, verbs), or describing
things in a poetic way.
2 There is minor creativity based on re-ordering
words that appeared in the triple
3 None. Looks like a robot talking.
[Fluency] How grammatically correct is the gener-
ated sentence?
1 Mostly perfect English phrase or sentence.
2 There are some errors, but mostly comprehen-
sible.
3 Terrible.
[Relevance] How relevant is the generated descrip-
tion to the given image?
1 Very relevant.
2 Reasonably relevant.
3 Totally off.
5In our pilot study, human annotations on 160 instances
given by two evaluators were identical on 61% of the instances,
and close (difference ? 1) on 92%.
226
Table 3 shows the human evaluation results. In
terms of creativity, PHRASE FUSION achieves the
best score as expected. In terms of fluency and
relevance however, TEMPLATE achieves the best
scores, while PHRASE FUSION performs the second
best. Remember that TEMPLATE is based on hand-
engineered rules with discourse constraints, which
seems to appeal to evaluators more. It would be
straightforward to combine PHRASE FUSION with
TEMPLATE to improve the output of PHRASE FU-
SION with hand-engineered rules. However, our
goal in this paper is to investigate statistically moti-
vated approaches for generating image descriptions
that can address inherent limitations of hand-written
rules discussed in ?3.3.
Notice that the relevance score of TEMPLATE is
better than that of LANGUAGE MODEL, even though
both approaches generate descriptions that consist of
an almost identical set of words. This is presum-
ably because the output from LANGUAGE MODEL
contains grammatically incorrect sentences that are
not comprehendable enough to the evaluators. The
relevance score of PHRASE FUSION is also slightly
worse than that of TEMPLATE, presumably because
PHRASE FUSION often generates poetic or creative
expressions, as shown in Figure 5, which can be con-
sidered a deviation from the image content.
Error Analysis There are different sources of er-
rors. Some errors are due to mistakes in the origi-
nal visual recognition input. For example, in the 3rd
image in Figure 5, the color of sky is predicted to
be ?golden?. In the 4th image, the wall behind the
table is recognized as ?sky?, and in the 6th image,
the parrots are recognized as ?person?.
Other errors are from surface realization. For in-
stance, in the 8th image, PHRASE FUSION selects
the preposition ?under?, presumably because dogs
are typically under the chair rather than on the chair
according to Google n-gram statistics. In the 5th
image, an unexpected word ?burning? is selected to
make the resulting output idiosyncratic. Word sense
disambiguation sometimes causes a problem in sur-
face realization as well. In the 3rd image, the word
?way? is chosen to represent ?path? or ?street? by
the image recognizer. However, a different sense of
way ? ?very? ? is being used in the final output.
6 Related Work
There has been relatively limited work on automat-
ically generating natural language image descrip-
tions. Most work related to our study is discussed
in ?1, hence we highlight only those that are clos-
est to our work here. Yao et al (2010) present a
comprehensive system that generates image descrip-
tions using Head-driven phrase structure (HPSG)
grammar, which requires carefully written domain-
specific lexicalized grammar rules, and also de-
mands a very specific and complex meaning rep-
resentation scheme from the image processing. In
contrast, our approach handles images in the open-
domain more naturally using much simpler tech-
niques.
We use similar vision based inputs ? object detec-
tors, modifier classifiers, and prepositional functions
? to some very recent work on generating simple de-
scriptions for images (Kulkarni et al, 2011), but fo-
cus on improving the sentence generation method-
ology and produce descriptions that are more true
to human generated descriptions. Note that the
BLEU scores reported in their work of Kulkarni et
al. (2011) are not directly comparable to ours, as the
scale of the scores differs depending on the number
of sentences generated per image.
7 Conclusion
In this paper, we presented a novel surface realiza-
tion technique based on web-scale n-gram data to
automatically generate image description. Despite
its simplicity, our method is highly effective in gen-
erating mostly appealing and presentable language,
while permitting creative writing at times. We con-
clude from our study that it is viable to generate
simple textual descriptions that are germane to the
specific image content while also sometimes pro-
ducing almost poetic natural language. Furthermore,
we demonstrate that world knowledge implicitly en-
coded in natural language can help enhance image
content recognition.
Acknowledgments
This work is supported in part by NSF Faculty Early
Career Development (CAREER) Award #1054133.
227
References
A. Aker and R. Gaizauskas. 2010. Generating image
descriptions using dependency relational patterns. In
ACL.
K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,
D. Blei, and M. Jordan. 2003. Matching words and
pictures. JMLR, 3:1107?1135.
Songsak Channarukul, Susan W. McRoy, and Syed S.
Ali. 2003. Doghed: a template-based generator for
multimodal dialog systems targeting heterogeneous
devices. In NAACL.
Michael Chisholm and Prasad Tadepalli. 2002. Learning
decision rules by randomized iterative local search. In
ICML, pages 75?82.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchical Im-
age Database. In CVPR.
A. Farhadi, I. Endres, D. Hoiem, and D. A. Forsyth.
2009. Describing objects by their attributes. In CVPR.
A. Farhadi, M Hejrati, A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. A. Forsyth.
2010. Every picture tells a story: generating sentences
for images. In ECCV.
P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-
manan. 2010. Object detection with discriminatively
trained part based models. tPAMI, Sept.
Y. Feng and M. Lapata. 2010a. How many words is a
picture worth? automatic caption generation for news
images. In ACL.
Yansong Feng and Mirella Lapata. 2010b. Topic models
for image annotation and text illustration. In HLT.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423?430. Association for Computa-
tional Linguistics.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C Berg, and Tamara L Berg.
2011. Babytalk: Understanding and generating simple
image descriptions. In CVPR.
Chee Wee Leong, Rada Mihalcea, and Samer Hassan.
2010. Text mining for automatic image tagging. In
COLING.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation.
Katerina Pastra, Horacio Saggion, and Yorick Wilks.
2003. Nlp for indexing and retrieval of captioned pho-
tographs. In EACL.
Michael White and Claire Cardie. 2002. Selecting sen-
tences for multidocument summaries using random-
ized local search. In ACL Workshop on Automatic
Summarization.
B.Z. Yao, Xiong Yang, Liang Lin, Mun Wai Lee, and
Song-Chun Zhu. 2010. I2t: Image parsing to text de-
scription. Proc. IEEE, 98(8).
Liang Zhou and Eduard Hovy. 2004. Template-
filtered headline summarization. In Text Summariza-
tion Branches Out: Pr ACL-04 Wkshp, July.
228
