Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151?161,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Semi-Supervised Recursive Autoencoders
for Predicting Sentiment Distributions
Richard Socher Jeffrey Pennington? Eric H. Huang Andrew Y. Ng Christopher D. Manning
Computer Science Department, Stanford University, Stanford, CA 94305, USA
?SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA
richard@socher.org {jpennin,ehhuang,ang,manning}@stanford.edu ang@cs.stanford.edu
Abstract
We introduce a novel machine learning frame-
work based on recursive autoencoders for
sentence-level prediction of sentiment label
distributions. Our method learns vector space
representations for multi-word phrases. In
sentiment prediction tasks these represen-
tations outperform other state-of-the-art ap-
proaches on commonly used datasets, such as
movie reviews, without using any pre-defined
sentiment lexica or polarity shifting rules. We
also evaluate the model?s ability to predict
sentiment distributions on a new dataset based
on confessions from the experience project.
The dataset consists of personal user stories
annotated with multiple labels which, when
aggregated, form a multinomial distribution
that captures emotional reactions. Our al-
gorithm can more accurately predict distri-
butions over such labels compared to several
competitive baselines.
1 Introduction
The ability to identify sentiments about personal ex-
periences, products, movies etc. is crucial to un-
derstand user generated content in social networks,
blogs or product reviews. Detecting sentiment in
these data is a challenging task which has recently
spawned a lot of interest (Pang and Lee, 2008).
Current baseline methods often use bag-of-words
representations which cannot properly capture more
complex linguistic phenomena in sentiment analy-
sis (Pang et al, 2002). For instance, while the two
phrases ?white blood cells destroying an infection?
and ?an infection destroying white blood cells? have
the same bag-of-words representation, the former is
a positive reaction while the later is very negative.
More advanced methods such as (Nakagawa et al,
IndicesWords
Semantic Representations
Recursive Autoencoder
i         walked      into         a        parked     car
Sorry, Hugs      You Rock       Teehee    I Understand    Wow, Just Wow
Predicted Sentiment Distribution
Figure 1: Illustration of our recursive autoencoder archi-
tecture which learns semantic vector representations of
phrases. Word indices (orange) are first mapped into a
semantic vector space (blue). Then they are recursively
merged by the same autoencoder network into a fixed
length sentence representation. The vectors at each node
are used as features to predict a distribution over senti-
ment labels.
2010) that can capture such phenomena use many
manually constructed resources (sentiment lexica,
parsers, polarity-shifting rules). This limits the ap-
plicability of these methods to a broader range of
tasks and languages. Lastly, almost all previous
work is based on single, positive/negative categories
or scales such as star ratings. Examples are movie
reviews (Pang and Lee, 2005), opinions (Wiebe et
al., 2005), customer reviews (Ding et al, 2008) or
multiple aspects of restaurants (Snyder and Barzilay,
2007). Such a one-dimensional scale does not accu-
rately reflect the complexity of human emotions and
sentiments.
In this work, we seek to address three issues. (i)
Instead of using a bag-of-words representation, our
model exploits hierarchical structure and uses com-
positional semantics to understand sentiment. (ii)
Our system can be trained both on unlabeled do-
main data and on supervised sentiment data and does
not require any language-specific sentiment lexica,
151
parsers, etc. (iii) Rather than limiting sentiment to
a positive/negative scale, we predict a multidimen-
sional distribution over several complex, intercon-
nected sentiments.
We introduce an approach based on semi-
supervised, recursive autoencoders (RAE) which
use as input continuous word vectors. Fig. 1 shows
an illustration of the model which learns vector rep-
resentations of phrases and full sentences as well as
their hierarchical structure from unsupervised text.
We extend our model to also learn a distribution over
sentiment labels at each node of the hierarchy.
We evaluate our approach on several standard
datasets where we achieve state-of-the art perfor-
mance. Furthermore, we show results on the re-
cently introduced experience project (EP) dataset
(Potts, 2010) that captures a broader spectrum of
human sentiments and emotions. The dataset con-
sists of very personal confessions anonymously
made by people on the experience project website
www.experienceproject.com. Confessions are la-
beled with a set of five reactions by other users. Re-
action labels are you rock (expressing approvement),
tehee (amusement), I understand, Sorry, hugs and
Wow, just wow (displaying shock). For evaluation on
this dataset we predict both the label with the most
votes as well as the full distribution over the senti-
ment categories. On both tasks our model outper-
forms competitive baselines. A set of over 31,000
confessions as well as the code of our model are
available at www.socher.org.
After describing the model in detail, we evalu-
ate it qualitatively by analyzing the learned n-gram
vector representations and compare quantitatively
against other methods on standard datasets and the
EP dataset.
2 Semi-Supervised Recursive
Autoencoders
Our model aims to find vector representations for
variable-sized phrases in either unsupervised or
semi-supervised training regimes. These representa-
tions can then be used for subsequent tasks. We first
describe neural word representations and then pro-
ceed to review a related recursive model based on
autoencoders, introduce our recursive autoencoder
(RAE) and describe how it can be modified to jointly
learn phrase representations, phrase structure and
sentiment distributions.
2.1 Neural Word Representations
We represent words as continuous vectors of param-
eters. We explore two settings. In the first setting
we simply initialize each word vector x ? Rn by
sampling it from a zero mean Gaussian distribution:
x ? N (0, ?2). These word vectors are then stacked
into a word embedding matrix L ? Rn?|V |, where
|V | is the size of the vocabulary. This initialization
works well in supervised settings where a network
can subsequently modify these vectors to capture
certain label distributions.
In the second setting, we pre-train the word vec-
tors with an unsupervised neural language model
(Bengio et al, 2003; Collobert and Weston, 2008).
These models jointly learn an embedding of words
into a vector space and use these vectors to predict
how likely a word occurs given its context. After
learning via gradient ascent the word vectors cap-
ture syntactic and semantic information from their
co-occurrence statistics.
In both cases we can use the resulting matrix of
word vectors L for subsequent tasks as follows. As-
sume we are given a sentence as an ordered list of
m words. Each word has an associated vocabulary
index k into the embedding matrix which we use to
retrieve the word?s vector representation. Mathemat-
ically, this look-up operation can be seen as a sim-
ple projection layer where we use a binary vector b
which is zero in all positions except at the kth index,
xi = Lbk ? Rn. (1)
In the remainder of this paper, we represent a sen-
tence (or any n-gram) as an ordered list of these
vectors (x1, . . . , xm). This word representation is
better suited to autoencoders than the binary number
representations used in previous related autoencoder
models such as the recursive autoassociative mem-
ory (RAAM) model (Pollack, 1990; Voegtlin and
Dominey, 2005) or recurrent neural networks (El-
man, 1991) since sigmoid units are inherently con-
tinuous. Pollack circumvented this problem by hav-
ing vocabularies with only a handful of words and
by manually defining a threshold to binarize the re-
sulting vectors.
152
x1 x3 x4x2
y1=f(W(1)[x3;x4] + b)
y2=f(W(1)[x2;y1] + b)
y3=f(W(1)[x1;y2] + b)
Figure 2: Illustration of an application of a recursive au-
toencoder to a binary tree. The nodes which are not filled
are only used to compute reconstruction errors. A stan-
dard autoencoder (in box) is re-used at each node of the
tree.
2.2 Traditional Recursive Autoencoders
The goal of autoencoders is to learn a representation
of their inputs. In this section we describe how to
obtain a reduced dimensional vector representation
for sentences.
In the past autoencoders have only been used in
setting where the tree structure was given a-priori.
We review this setting before continuing with our
model which does not require a given tree structure.
Fig. 2 shows an instance of a recursive autoencoder
(RAE) applied to a given tree. Assume we are given
a list of word vectors x = (x1, . . . , xm) as described
in the previous section as well as a binary tree struc-
ture for this input in the form of branching triplets
of parents with children: (p ? c1c2). Each child
can be either an input word vector xi or a nontermi-
nal node in the tree. For the example in Fig. 2, we
have the following triplets: ((y1 ? x3x4), (y2 ?
x2y1), (y1 ? x1y2)). In order to be able to apply
the same neural network to each pair of children, the
hidden representations yi have to have the same di-
mensionality as the xi?s.
Given this tree structure, we can now compute the
parent representations. The first parent vector y1 is
computed from the children (c1, c2) = (x3, x4):
p = f(W (1)[c1; c2] + b(1)), (2)
where we multiplied a matrix of parameters W (1) ?
Rn?2n by the concatenation of the two children.
After adding a bias term we applied an element-
wise activation function such as tanh to the result-
ing vector. One way of assessing how well this n-
dimensional vector represents its children is to try to
reconstruct the children in a reconstruction layer:
[
c?1; c?2
]
= W (2)p+ b(2). (3)
During training, the goal is to minimize the recon-
struction errors of this input pair. For each pair, we
compute the Euclidean distance between the original
input and its reconstruction:
Erec([c1; c2]) =
1
2
????[c1; c2]?
[
c?1; c?2
]????2 . (4)
This model of a standard autoencoder is boxed in
Fig. 2. Now that we have defined how an autoen-
coder can be used to compute an n-dimensional vec-
tor representation (p) of two n-dimensional children
(c1, c2), we can describe how such a network can be
used for the rest of the tree.
Essentially, the same steps repeat. Now that y1
is given, we can use Eq. 2 to compute y2 by setting
the children to be (c1, c2) = (x2, y1). Again, after
computing the intermediate parent vector y2, we can
assess how well this vector capture the content of
the children by computing the reconstruction error
as in Eq. 4. The process repeat until the full tree
is constructed and we have a reconstruction error at
each nonterminal node. This model is similar to the
RAAM model (Pollack, 1990) which also requires a
fixed tree structure.
2.3 Unsupervised Recursive Autoencoder for
Structure Prediction
Now, assume there is no tree structure given for
the input vectors in x. The goal of our structure-
prediction RAE is to minimize the reconstruction er-
ror of all vector pairs of children in a tree. We de-
fine A(x) as the set of all possible trees that can be
built from an input sentence x. Further, let T (y) be
a function that returns the triplets of a tree indexed
by s of all the non-terminal nodes in a tree. Using
the reconstruction error of Eq. 4, we compute
RAE?(x) = argmin
y?A(x)
?
s?T (y)
Erec([c1; c2]s) (5)
We now describe a greedy approximation that con-
structs such a tree.
153
Greedy Unsupervised RAE. For a sentence with
m words, we apply the autoencoder recursively. It
takes the first pair of neighboring vectors, defines
them as potential children of a phrase (c1; c2) =
(x1;x2), concatenates them and gives them as in-
put to the autoencoder. For each word pair, we save
the potential parent node p and the resulting recon-
struction error.
After computing the score for the first pair, the
network is shifted by one position and takes as input
vectors (c1, c2) = (x2, x3) and again computes a po-
tential parent node and a score. This process repeats
until it hits the last pair of words in the sentence:
(c1, c2) = (xm?1, xm). Next, it selects the pair
which had the lowest reconstruction error (Erec) and
its parent representation p will represent this phrase
and replace both children in the sentence word list.
For instance, consider the sequence (x1, x2, x3, x4)
and assume the lowestErec was obtained by the pair
(x3, x4). After the first pass, the new sequence then
consists of (x1, x2, p(3,4)). The process repeats and
treats the new vector p(3,4) like any other input vec-
tor. For instance, subsequent states could be either:
(x1, p(2,(3,4))) or (p(1,2), p(3,4)). Both states would
then finish with a deterministic choice of collapsing
the remaining two states into one parent to obtain
(p(1,(2,(3,4)))) or (p((1,2),(3,4))) respectively. The tree
is then recovered by unfolding the collapsing deci-
sions.
The resulting tree structure captures as much of
the single-word information as possible (in order
to allow reconstructing the word vectors) but does
not necessarily follow standard syntactic constraints.
We also experimented with a method that finds bet-
ter solutions to Eq. 5 based on CKY-like beam
search algorithms (Socher et al, 2010; Socher et al,
2011) but the performance is similar and the greedy
version is much faster.
Weighted Reconstruction. One problem with
simply using the reconstruction error of both chil-
dren equally as describe in Eq. 4 is that each child
could represent a different number of previously
collapsed words and is hence of bigger importance
for the overall meaning reconstruction of the sen-
tence. For instance in the case of (x1, p(2,(3,4)))
one would like to give more importance to recon-
structing p than x1. We capture this desideratum
by adjusting the reconstruction error. Let n1, n2 be
the number of words underneath a current poten-
tial child, we re-define the reconstruction error to be
Erec([c1; c2]; ?) =
n1
n1 + n2
????c1 ? c?1
????2 + n2n1 + n2
????c2 ? c?2
????2 (6)
Length Normalization. One of the goals of
RAEs is to induce semantic vector representations
that allow us to compare n-grams of different
lengths. The RAE tries to lower reconstruction error
of not only the bigrams but also of nodes higher in
the tree. Unfortunately, since the RAE computes the
hidden representations it then tries to reconstruct, it
can just lower reconstruction error by making the
hidden layer very small in magnitude. To prevent
such undesirable behavior, we modify the hidden
layer such that the resulting parent representation al-
ways has length one, after computing p as in Eq. 2,
we simply set: p = p||p|| .
2.4 Semi-Supervised Recursive Autoencoders
So far, the RAE was completely unsupervised and
induced general representations that capture the se-
mantics of multi-word phrases.In this section, we
extend RAEs to a semi-supervised setting in order
to predict a sentence- or phrase-level target distribu-
tion t.1
One of the main advantages of the RAE is that
each node of the tree built by the RAE has associ-
ated with it a distributed vector representation (the
parent vector p) which could also be seen as fea-
tures describing that phrase. We can leverage this
representation by adding on top of each parent node
a simple softmax layer to predict class distributions:
d(p; ?) = softmax(W labelp). (7)
Assuming there are K labels, d ? RK is
a K-dimensional multinomial distribution and?
k=1 dk = 1. Fig. 3 shows such a semi-supervised
RAE unit. Let tk be the kth element of the multino-
mial target label distribution t for one entry. The
softmax layer?s outputs are interpreted as condi-
tional probabilities dk = p(k|[c1; c2]), hence the
cross-entropy error is
EcE(p, t; ?) = ?
K?
k=1
tk log dk(p; ?). (8)
1For the binary label classification case, the distribution is
of the form [1, 0] for class 1 and [0, 1] for class 2.
154
R e c o n s t r u c t i o n  e r r o r            C r o s s - e n t r o p y e r r o r
W(1)
W(2) W(l a be l )
Figure 3: Illustration of an RAE unit at a nonterminal tree
node. Red nodes show the supervised softmax layer for
label distribution prediction.
Using this cross-entropy error for the label and the
reconstruction error from Eq. 6, the final semi-
supervised RAE objective over (sentences,label)
pairs (x, t) in a corpus becomes
J = 1N
?
(x,t)
E(x, t; ?) + ?2 ||?||
2, (9)
where we have an error for each entry in the training
set that is the sum over the error at the nodes of the
tree that is constructed by the greedy RAE:
E(x, t; ?) =
?
s?T (RAE?(x))
E([c1; c2]s, ps, t, ?).
The error at each nonterminal node is the weighted
sum of reconstruction and cross-entropy errors,
E([c1; c2]s, ps, t, ?) =
?Erec([c1; c2]s; ?) + (1? ?)EcE(ps, t; ?).
The hyperparameter ? weighs reconstruction and
cross-entropy error. When minimizing the cross-
entropy error of this softmax layer, the error will
backpropagate and influence both the RAE param-
eters and the word representations. Initially, words
such as good and bad have very similar representa-
tions. This is also the case for Brown clusters and
other methods that use only cooccurrence statistics
in a small window around each word. When learn-
ing with positive/negative sentiment, the word em-
beddings get modified and capture less syntactic and
more sentiment information.
In order to predict the sentiment distribution of a
sentence with this model, we use the learned vector
representation of the top tree node and train a simple
logistic regression classifier.
3 Learning
Let ? = (W (1), b(1),W (2), b(1),W label, L) be the set
of our model parameters, then the gradient becomes:
?J
?? =
1
N
?
(x,t)
?E(x, t; ?)
?? + ??. (10)
To compute this gradient, we first greedily construct
all trees and then derivatives for these trees are com-
puted efficiently via backpropagation through struc-
ture (Goller and Ku?chler, 1996). Because the algo-
rithm is greedy and the derivatives of the supervised
cross-entropy error also modify the matrix W (1),
this objective is not necessarily continuous and a
step in the gradient descent direction may not nec-
essarily decrease the objective. However, we found
that L-BFGS run over the complete training data
(batch mode) to minimize the objective works well
in practice, and that convergence is smooth, with the
algorithm typically finding a good solution quickly.
4 Experiments
We first describe the new experience project (EP)
dataset, results of standard classification tasks on
this dataset and how to predict its sentiment label
distributions. We then show results on other com-
monly used datasets and conclude with an analysis
of the important parameters of the model.
In all experiments involving our model, we repre-
sent words using 100-dimensional word vectors. We
explore the two settings mentioned in Sec. 2.1. We
compare performance on standard datasets when us-
ing randomly initialized word vectors (random word
init.) or word vectors trained by the model of Col-
lobert and Weston (2008) and provided by Turian
et al (2010).2 These vectors were trained on an
unlabeled corpus of the English Wikipedia. Note
that alternatives such as Brown clusters are not suit-
able since they do not capture sentiment information
(good and bad are usually in the same cluster) and
cannot be modified via backpropagation.
2http://metaoptimize.com/projects/
wordreprs/
155
Corpus K Instances Distr.(+/-) Avg|W |
MPQA 2 10,624 0.31/0.69 3
MR 2 10,662 0.5/0.5 22
EP 5 31,675 .2/.2/.1/.4/.1 113
EP? 4 5 6,129 .2/.2/.1/.4/.1 129
Table 1: Statistics on the different datasets. K is the num-
ber of classes. Distr. is the distribution of the different
classes (in the case of 2, the positive/negative classes, for
EP the rounded distribution of total votes in each class).
|W | is the average number of words per instance. We use
EP? 4, a subset of entries with at least 4 votes.
4.1 EP Dataset: The Experience Project
The confessions section of the experience project
website3 lets people anonymously write short per-
sonal stories or ?confessions?. Once a story is on
the site, each user can give a single vote to one of
five label categories (with our interpretation):
1 Sorry, Hugs: User offers condolences to author.
2. You Rock: Indicating approval, congratulations.
3. Teehee: User found the anecdote amusing.
4. I Understand: Show of empathy.
5. Wow, Just Wow: Expression of surprise,shock.
The EP dataset has 31,676 confession entries, a to-
tal number of 74,859 votes for the 5 labels above, the
average number of votes per entry is 2.4 (with a vari-
ance of 33). For the five categories, the numbers of
votes are [14, 816; 13, 325; 10, 073; 30, 844; 5, 801].
Since an entry with less than 4 votes is not very well
identified, we train and test only on entries with at
least 4 total votes. There are 6,129 total such entries.
The distribution over total votes in the 5 classes
is similar: [0.22; 0.2; 0.11; 0.37; 0.1]. The average
length of entries is 129 words. Some entries con-
tain multiple sentences. In these cases, we average
the predicted label distributions from the sentences.
Table 1 shows statistics of this and other commonly
used sentiment datasets (which we compare on in
later experiments). Table 2 shows example entries
as well as gold and predicted label distributions as
described in the next sections.
Compared to other datasets, the EP dataset con-
tains a wider range of human emotions that goes far
beyond positive/negative product or movie reviews.
Each item is labeled with a multinomial distribu-
3http://www.experienceproject.com/
confessions.php
tion over interconnected response categories. This
is in contrast to most other datasets (including multi-
aspect rating) where several distinct aspects are rated
independently but on the same scale. The topics
range from generic happy statements, daily clumsi-
ness reports, love, loneliness, to relationship abuse
and suicidal notes. As is evident from the total num-
ber of label votes, the most common user reaction
is one of empathy and an ability to relate to the au-
thors experience. However, some stories describe
horrible scenarios that are not common and hence
receive more offers of condolence. In the following
sections we show some examples of stories with pre-
dicted and true distributions but refrain from listing
the most horrible experiences.
For all experiments on the EP dataset, we split the
data into train (49%), development (21%) and test
data (30%).
4.2 EP: Predicting the Label with Most Votes
The first task for our evaluation on the EP dataset is
to simply predict the single class that receives the
most votes. In order to compare our novel joint
phrase representation and classifier learning frame-
work to traditional methods, we use the following
baselines:
Random Since there are five classes, this gives 20%
accuracy.
Most Frequent Selecting the class which most fre-
quently has the most votes (the class I under-
stand).
Baseline 1: Binary BoW This baseline uses logis-
tic regression on binary bag-of-word represen-
tations that are 1 if a word is present and 0 oth-
erwise.
Baseline 2: Features This model is similar to tra-
ditional approaches to sentiment classification
in that it uses many hand-engineered resources.
We first used a spell-checker and Wordnet to
map words and their misspellings to synsets to
reduce the total number of words. We then re-
placed sentiment words with a sentiment cat-
egory identifier using the sentiment lexica of
the Harvard Inquirer (Stone, 1966) and LIWC
(Pennebaker et al, 2007). Lastly, we used tf-idf
weighting on the bag-of-word representations
and trained an SVM.156
KL Predicted&Gold V. Entry (Shortened if it ends with ...)
.03
.16 .16 .16 .33 .16
6 I reguarly shoplift. I got caught once and went to jail, but I?ve found that this was not a deterrent. I don?t buy
groceries, I don?t buy school supplies for my kids, I don?t buy gifts for my kids, we don?t pay for movies, and I
dont buy most incidentals for the house (cleaning supplies, toothpaste, etc.)...
.03
.38 .04 .06 .35 .14
165 i am a very succesfull buissnes man.i make good money but i have been addicted to crack for 13 years.i moved 1
hour away from my dealers 10 years ago to stop using now i dont use daily but once a week usally friday nights.
i used to use 1 or 2 hundred a day now i use 4 or 5 hundred on a friday.my problem is i am a funcational addict...
.05
.14 .28 .14 .28 .14
7 Hi there, Im a guy that loves a girl, the same old bloody story... I met her a while ago, while studying, she Is so
perfect, so mature and yet so lonely, I get to know her and she get ahold of me, by opening her life to me and so
did I with her, she has been the first person, male or female that has ever made that bond with me,...
.07
.27 .18 .00 .45 .09
11 be kissing you right now. i should be wrapped in your arms in the dark, but instead i?ve ruined everything. i?ve
piled bricks to make a wall where there never should have been one. i feel an ache that i shouldn?t feel because
i?ve never had you close enough. we?ve never touched, but i still feel as though a part of me is missing. ...
.05 23 Dear Love, I just want to say that I am looking for you. Tonight I felt the urge to write, and I am becoming more
and more frustrated that I have not found you yet. I?m also tired of spending so much heart on an old dream. ...
.05 5 I wish I knew somone to talk to here.
.06 24 I loved her but I screwed it up. Now she?s moved on. I?ll never have her again. I don?t know if I?ll ever stop
thinking about her.
.06 5 i am 13 years old and i hate my father he is alwas geting drunk and do?s not care about how it affects me or my
sisters i want to care but the truthis i dont care if he dies
.13 6 well i think hairy women are attractive
.35 5 As soon as I put clothings on I will go down to DQ and get a thin mint blizzard. I need it. It?ll make my soul
feel a bit better :)
.36 6 I am a 45 year old divoced woman, and I havent been on a date or had any significant relationship in 12
years...yes, 12 yrs. the sad thing is, Im not some dried up old granny who is no longer interested in men, I just
can?t meet men. (before you judge, no Im not terribly picky!) What is wrong with me?
.63 6 When i was in kindergarden i used to lock myself in the closet and eat all the candy. Then the teacher found out
it was one of us and made us go two days without freetime. It might be a little late now, but sorry guys it was
me haha
.92 4 My paper is due in less than 24 hours and I?m still dancing round my room!
Table 2: Example EP confessions from the test data with KL divergence between our predicted distribution (light blue,
left bar on each of the 5 classes) and ground truth distribution (red bar and numbers underneath), number of votes. The
5 classes are [Sorry, Hugs ;You Rock; Teehee;I Understand;Wow, Just Wow]. Even when the KL divergence is higher,
our model makes reasonable alternative label choices. Some entries are shortened.
Baseline 3: Word Vectors We can ignore the RAE
tree structure and only train softmax layers di-
rectly on the pre-trained words in order to influ-
ence the word vectors. This is followed by an
SVM trained on the average of the word vec-
tors.
We also experimented with latent Dirichlet aloca-
tion (Blei et al, 2003) but performance was very
low.
Table 3 shows the results for predicting the class
with the most votes. Even the approach that is based
on sentiment lexica and other resources is outper-
formed by our model by almost 3%, showing that
for tasks involving complex broad-range human sen-
timent, the often used sentiment lexica lack in cover-
age and traditional bag-of-words representations are
not powerful enough.
4.3 EP: Predicting Sentiment Distributions
We now turn to evaluating our distribution-
prediction approach. In both this and the previous
Method Accuracy
Random 20.0
Most Frequent 38.1
Baseline 1: Binary BoW 46.4
Baseline 2: Features 47.0
Baseline 3: Word Vectors 45.5
RAE (our method) 50.1
Table 3: Accuracy of predicting the class with most votes.
maximum label task, we backprop using the gold
multinomial distribution as a target. Since we max-
imize likelihood and because we want to predict a
distribution that is closest to the distribution of labels
that people would assign to a story, we evaluate us-
ing KL divergence: KL(g||p) = ?i gi log(gi/pi),
where g is the gold distribution and p is the predicted
one. We report the average KL divergence, where a
smaller value indicates better predictive power. To
get an idea of the values of KL divergence, predict-
157
Avg.Distr. BoW Features Word Vec. RAE0.6
0.7
0.8
0.83 0.81 0.72 0.73 0.70
Figure 4: Average KL-divergence between gold and pre-
dicted sentiment distributions (lower is better).
ing random distributions gives a an average of 1.2 in
KL divergence, predicting simply the average distri-
bution in the training data give 0.83. Fig. 4 shows
that our RAE-based model outperforms the other
baselines. Table 2 shows EP example entries with
predicted and gold distributions, as well as numbers
of votes.
4.4 Binary Polarity Classification
In order to compare our approach to other meth-
ods we also show results on commonly used sen-
timent datasets: movie reviews4 (MR) (Pang and
Lee, 2005) and opinions5 (MPQA) (Wiebe et al,
2005).We give statistical information on these and
the EP corpus in Table 1.
We compare to the state-of-the-art system of
(Nakagawa et al, 2010), a dependency tree based
classification method that uses CRFs with hidden
variables. We use the same training and testing regi-
men (10-fold cross validation) as well as their base-
lines: majority phrase voting using sentiment and
reversal lexica; rule-based reversal using a depen-
dency tree; Bag-of-Features and their full Tree-CRF
model. As shown in Table 4, our algorithm outper-
forms their approach on both datasets. For the movie
review (MR) data set, we do not use any hand-
designed lexica. An error analysis on the MPQA
dataset showed several cases of single words which
never occurred in the training set. Correctly classify-
ing these instances can only be the result of having
them in the original sentiment lexicon. Hence, for
the experiment on MPQA we added the same sen-
timent lexicon that (Nakagawa et al, 2010) used in
their system to our training set. This improved ac-
curacy from 86.0 to 86.4.Using the pre-trained word
vectors boosts performance by less than 1% com-
4www.cs.cornell.edu/people/pabo/
movie-review-data/
5www.cs.pitt.edu/mpqa/
Method MR MPQA
Voting with two lexica 63.1 81.7
Rule-based reversal on trees 62.9 82.8
Bag of features with reversal 76.4 84.1
Tree-CRF (Nakagawa et al?10) 77.3 86.1
RAE (random word init.) 76.8 85.7
RAE (our method) 77.7 86.4
Table 4: Accuracy of sentiment classification on movie
review polarity (MR) and the MPQA dataset.
0 0.2 0.4 0.6 0.8 10.83
0.84
0.85
0.86
0.87
Figure 5: Accuracy on the development split of the MR
polarity dataset for different weightings of reconstruction
error and supervised cross-entropy error: err = ?Erec+
(1? ?)EcE .
pared to randomly initialized word vectors (setting:
random word init). This shows that our method can
work well even in settings with little training data.
We visualize the semantic vectors that the recursive
autoencoder learns by listing n-grams that give the
highest probability for each polarity. Table 5 shows
such n-grams for different lengths when the RAE is
trained on the movie review polarity dataset.
On a 4-core machine, training time for the smaller
corpora such as the movie reviews takes around 3
hours and for the larger EP corpus around 12 hours
until convergence. Testing of hundreds of movie re-
views takes only a few seconds.
4.5 Reconstruction vs. Classification Error
In this experiment, we show how the hyperparame-
ter ? influences accuracy on the development set of
one of the cross-validation splits of the MR dataset.
This parameter essentially trade-off the supervised
and unsupervised parts of the objective. Fig. 5 shows
that a larger focus on the supervised objective is im-
portant but that a weight of ? = 0.2 for the recon-
struction error prevents overfitting and achieves the
highest performance.
158
n Most negative n-grams Most positive n-grams
1 bad; boring; dull; flat; pointless; tv; neither; pretentious; badly;
worst; lame; mediocre; lack; routine; loud; bore; barely; stupid;
tired; poorly; suffers; heavy;nor; choppy; superficial
touching; enjoyable; powerful; warm; moving; culture; flaws;
provides; engrossing; wonderful; beautiful; quiet; socio-political;
thoughtful; portrait; refreshingly; chilling; rich; beautifully; solid;
2 how bad; by bad; dull .; for bad; to bad; boring .; , dull; are bad;
that bad; boring ,; , flat; pointless .; badly by; on tv; so routine; lack
the; mediocre .; a generic; stupid ,; abysmally pathetic
the beautiful; moving,; thoughtful and; , inventive; solid and; a
beautiful; a beautifully; and hilarious; with dazzling; provides the;
provides.; and inventive; as powerful; moving and; a moving; a
powerful
3 . too bad; exactly how bad; and never dull; shot but dull; is more
boring; to the dull; dull, UNK; it is bad; or just plain; by turns
pretentious; manipulative and contrived; bag of stale; is a bad; the
whole mildly; contrived pastiche of; from this choppy; stale mate-
rial.
funny and touching; a small gem; with a moving; cuts, fast; , fine
music; smart and taut; culture into a; romantic , riveting; ... a solid;
beautifully acted .; , gradually reveals; with the chilling; cast of
solid; has a solid; spare yet audacious; ... a polished; both the
beauty;
5 boring than anything else.; a major waste ... generic; nothing i
hadn?t already; ,UNK plotting;superficial; problem ? no laughs.;
,just horribly mediocre .; dull, UNK feel.; there?s nothing exactly
wrong; movie is about a boring; essentially a collection of bits
reminded us that a feel-good; engrossing, seldom UNK,; between
realistic characters showing honest; a solid piece of journalistic;
easily the most thoughtful fictional; cute, funny, heartwarming;
with wry humor and genuine; engrossing and ultimately tragic.;
8 loud, silly, stupid and pointless.; dull, dumb and derivative horror
film.; UNK?s film, a boring, pretentious; this film biggest problem
? no laughs.; film in the series looks and feels tired; do draw easy
chuckles but lead nowhere.; stupid, infantile, redundant, sloppy
shot in rich , shadowy black-and-white , devils an escapist con-
fection that ?s pure entertainment .; , deeply absorbing piece that
works as a; ... one of the most ingenious and entertaining; film is a
riveting , brisk delight .; bringing richer meaning to the story ?s;
Table 5: Examples of n-grams (n = 1, 2, 3, 5, 8) from the test data of the movie polarity dataset for which our model
predicts the most positive and most negative responses.
5 Related Work
5.1 Autoencoders and Deep Learning
Autoencoders are neural networks that learn a re-
duced dimensional representation of fixed-size in-
puts such as image patches or bag-of-word repre-
sentations of text documents. They can be used to
efficiently learn feature encodings which are useful
for classification. Recently, Mirowski et al (2010)
learn dynamic autoencoders for documents in a bag-
of-words format which, like ours, combine super-
vised and reconstruction objectives.
The idea of applying an autoencoder in a recursive
setting was introduced by Pollack (1990). Pollack?s
recursive auto-associative memories (RAAMs) are
similar to ours in that they are a connectionst, feed-
forward model. However, RAAMs learn vector
representations only for fixed recursive data struc-
tures, whereas our RAE builds this recursive data
structure. More recently, (Voegtlin and Dominey,
2005) introduced a linear modification to RAAMs
that is able to better generalize to novel combina-
tions of previously seen constituents. One of the
major shortcomings of previous applications of re-
cursive autoencoders to natural language sentences
was their binary word representation as discussed in
Sec. 2.1.
Recently, (Socher et al, 2010; Socher et al, 2011)
introduced a max-margin framework based on recur-
sive neural networks (RNNs) for labeled structure
prediction. Their models are applicable to natural
language and computer vision tasks such as parsing
or object detection. The current work is related in
that it uses a recursive deep learning model. How-
ever, RNNs require labeled tree structures and use a
supervised score at each node. Instead, RAEs learn
hierarchical structures that are trying to capture as
much of the the original word vectors as possible.
The learned structures are not necessarily syntacti-
cally plausible but can capture more of the semantic
content of the word vectors. Other recent deep learn-
ing methods for sentiment analysis include (Maas et
al., 2011).
5.2 Sentiment Analysis
Pang et al (2002) were one of the first to experiment
with sentiment classification. They show that sim-
ple bag-of-words approaches based on Naive Bayes,
MaxEnt models or SVMs are often insufficient for
predicting sentiment of documents even though they
work well for general topic-based document classi-
fication. Even adding specific negation words, bi-
grams or part-of-speech information to these mod-
els did not add significant improvements. Other
document-level sentiment work includes (Turney,
2002; Dave et al, 2003; Beineke et al, 2004; Pang
and Lee, 2004). For further references, see (Pang
and Lee, 2008).
Instead of document level sentiment classifica-
tion, (Wilson et al, 2005) analyze the contextual
polarity of phrases and incorporate many well de-
signed features including dependency trees. They
also show improvements by first distinguishing be-
159
tween neutral and polar sentences. Our model natu-
rally incorporates the recursive interaction between
context and polarity words in sentences in a unified
framework while simultaneously learning the neces-
sary features to make accurate predictions. Other ap-
proaches for sentence-level sentiment detection in-
clude (Yu and Hatzivassiloglou, 2003; Grefenstette
et al, 2004; Ikeda et al, 2008).
Most previous work is centered around a given
sentiment lexicon or building one via heuristics
(Kim and Hovy, 2007; Esuli and Sebastiani, 2007),
manual annotation (Das and Chen, 2001) or machine
learning techniques (Turney, 2002). In contrast, we
do not require an initial or constructed sentiment lex-
icon of positive and negative words. In fact, when
training our approach on documents or sentences, it
jointly learns such lexica for both single words and
n-grams (see Table 5). (Mao and Lebanon, 2007)
propose isotonic conditional random fields and dif-
ferentiate between local, sentence-level and global,
document-level sentiment.
The work of (Polanyi and Zaenen, 2006; Choi and
Cardie, 2008) focuses on manually constructing sev-
eral lexica and rules for both polar words and re-
lated content-word negators, such as ?prevent can-
cer?, where prevent reverses the negative polarity of
cancer. Like our approach they capture composi-
tional semantics. However, our model does so with-
out manually constructing any rules or lexica.
Recently, (Velikovich et al, 2010) showed how to
use a seed lexicon and a graph propagation frame-
work to learn a larger sentiment lexicon that also in-
cludes polar multi-word phrases such as ?once in a
life time?. While our method can also learn multi-
word phrases it does not require a seed set or a large
web graph. (Nakagawa et al, 2010) introduced an
approach based on CRFs with hidden variables with
very good performance. We compare to their state-
of-the-art system. We outperform them on the stan-
dard corpora that we tested on without requiring
external systems such as POS taggers, dependency
parsers and sentiment lexica. Our approach jointly
learns the necessary features and tree structure.
In multi-aspect rating (Snyder and Barzilay, 2007)
one finds several distinct aspects such as food or ser-
vice in a restaurant and then rates them on a fixed
linear scale such as 1-5 stars, where all aspects could
obtain just 1 star or all aspects could obtain 5 stars
independently. In contrast, in our method a single
aspect (a complex reaction to a human experience)
is predicted not in terms of a fixed scale but in terms
of a multinomial distribution over several intercon-
nected, sometimes mutually exclusive emotions. A
single story cannot simultaneously obtain a strong
reaction in different emotional responses (by virtue
of having to sum to one).
6 Conclusion
We presented a novel algorithm that can accurately
predict sentence-level sentiment distributions. With-
out using any hand-engineered resources such as
sentiment lexica, parsers or sentiment shifting rules,
our model achieves state-of-the-art performance on
commonly used sentiment datasets. Furthermore,
we introduce a new dataset that contains distribu-
tions over a broad range of human emotions. Our
evaluation shows that our model can more accu-
rately predict these distributions than other models.
Acknowledgments
We gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommendations
expressed in this material are those of the author(s) and
do not necessarily reflect the view of DARPA, AFRL, or
the US government. This work was also supported in part
by the DARPA Deep Learning program under contract
number FA8650-10-C-7020.
We thank Chris Potts for help with the EP data set, Ray-
mond Hsu, Bozhi See, and Alan Wu for letting us use
their system as a baseline and Jiquan Ngiam, Quoc Le,
Gabor Angeli and Andrew Maas for their feedback.
References
P. Beineke, T. Hastie, C. D. Manning, and
S. Vaithyanathan. 2004. Exploring sentiment
summarization. In Proceedings of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning Re-
search., 3:993?1022.
160
Y. Choi and C. Cardie. 2008. Learning with composi-
tional semantics as structural inference for subsenten-
tial sentiment analysis. In EMNLP.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML, pages 160?167.
S. Das and M. Chen. 2001. Yahoo! for Amazon: Ex-
tracting market sentiment from stock message boards.
In Proceedings of the Asia Pacific Finance Association
Annual Conference (APFA).
K. Dave, S. Lawrence, and D. M. Pennock. 2003. Min-
ing the peanut gallery: Opinion extraction and seman-
tic classification of product reviews. In Proceedings of
WWW, pages 519?528.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In Proceedings of
the Conference on Web Search and Web Data Mining
(WSDM).
J. L. Elman. 1991. Distributed representations, simple
recurrent networks, and grammatical structure. Ma-
chine Learning, 7(2-3):195?225.
A. Esuli and F. Sebastiani. 2007. Pageranking word-
net synsets: An application to opinion mining. In
Proceedings of the Association for Computational Lin-
guistics (ACL).
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks (ICNN-96).
G. Grefenstette, Y. Qu, J. G. Shanahan, and D. A. Evans.
2004. Coupling niche browsers and affect analysis
for an opinion mining application. In Proceedings
of Recherche d?Information Assiste?e par Ordinateur
(RIAO).
D. Ikeda, H. Takamura, L. Ratinov, and M. Okumura.
2008. Learning to shift the polarity of words for senti-
ment classification. In IJCNLP.
S. Kim and E. Hovy. 2007. Crystal: Analyzing predic-
tive opinions on the web. In EMNLP-CoNLL.
A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng,
and C. Potts. 2011. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of ACL.
Y. Mao and G. Lebanon. 2007. Isotonic Conditional
Random Fields and Local Sentiment Flow. In NIPS.
P. Mirowski, M. Ranzato, and Y. LeCun. 2010. Dynamic
auto-encoders for semantic indexing. In Proceedings
of the NIPS 2010 Workshop on Deep Learning.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-
dency tree-based sentiment classification using CRFs
with hidden variables. In NAACL, HLT.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACL.
B. Pang and L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In ACL, pages 115?124.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In EMNLP.
J. W. Pennebaker, R.J. Booth, and M. E. Francis. 2007.
Linguistic inquiry and word count: Liwc2007 opera-
tors manual. University of Texas.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46:77?105, November.
C. Potts. 2010. On the negativity of negation. In David
Lutz and Nan Li, editors, Proceedings of Semantics
and Linguistic Theory 20. CLC Publications, Ithaca,
NY.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the Good Grief algorithm. In HLT-NAACL.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
R. Socher, C. C. Lin, A. Y. Ng, and C. D. Manning.
2011. Parsing Natural Scenes and Natural Language
with Recursive Neural Networks. In ICML.
P. J. Stone. 1966. The General Inquirer: A Computer
Approach to Content Analysis. The MIT Press.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of ACL, pages
384?394.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification of
reviews. In ACL.
L. Velikovich, S. Blair-Goldensohn, K. Hannan, and
R. McDonald. 2010. The viability of web-derived po-
larity lexicons. In NAACL, HLT.
T. Voegtlin and P. Dominey. 2005. Linear Recursive Dis-
tributed Representations. Neural Networks, 18(7).
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In HLT/EMNLP.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
EMNLP.
161
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532?1543,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
GloVe: Global Vectors for Word Representation
Jeffrey Pennington, Richard Socher, Christopher D. Manning
Computer Science Department, Stanford University, Stanford, CA 94305
jpennin@stanford.edu, richard@socher.org, manning@stanford.edu
Abstract
Recent methods for learning vector space
representations of words have succeeded
in capturing fine-grained semantic and
syntactic regularities using vector arith-
metic, but the origin of these regularities
has remained opaque. We analyze and
make explicit the model properties needed
for such regularities to emerge in word
vectors. The result is a new global log-
bilinear regression model that combines
the advantages of the two major model
families in the literature: global matrix
factorization and local context window
methods. Our model efficiently leverages
statistical information by training only on
the nonzero elements in a word-word co-
occurrence matrix, rather than on the en-
tire sparse matrix or on individual context
windows in a large corpus. The model pro-
duces a vector space with meaningful sub-
structure, as evidenced by its performance
of 75% on a recent word analogy task. It
also outperforms related models on simi-
larity tasks and named entity recognition.
1 Introduction
Semantic vector space models of language repre-
sent each word with a real-valued vector. These
vectors can be used as features in a variety of ap-
plications, such as information retrieval (Manning
et al., 2008), document classification (Sebastiani,
2002), question answering (Tellex et al., 2003),
named entity recognition (Turian et al., 2010), and
parsing (Socher et al., 2013).
Most word vector methods rely on the distance
or angle between pairs of word vectors as the pri-
mary method for evaluating the intrinsic quality
of such a set of word representations. Recently,
Mikolov et al. (2013c) introduced a new evalua-
tion scheme based on word analogies that probes
the finer structure of the word vector space by ex-
amining not the scalar distance between word vec-
tors, but rather their various dimensions of dif-
ference. For example, the analogy ?king is to
queen as man is to woman? should be encoded
in the vector space by the vector equation king ?
queen = man ? woman. This evaluation scheme
favors models that produce dimensions of mean-
ing, thereby capturing the multi-clustering idea of
distributed representations (Bengio, 2009).
The two main model families for learning word
vectors are: 1) global matrix factorization meth-
ods, such as latent semantic analysis (LSA) (Deer-
wester et al., 1990) and 2) local context window
methods, such as the skip-gram model of Mikolov
et al. (2013c). Currently, both families suffer sig-
nificant drawbacks. While methods like LSA ef-
ficiently leverage statistical information, they do
relatively poorly on the word analogy task, indi-
cating a sub-optimal vector space structure. Meth-
ods like skip-gram may do better on the analogy
task, but they poorly utilize the statistics of the cor-
pus since they train on separate local context win-
dows instead of on global co-occurrence counts.
In this work, we analyze the model properties
necessary to produce linear directions of meaning
and argue that global log-bilinear regression mod-
els are appropriate for doing so. We propose a spe-
cific weighted least squares model that trains on
global word-word co-occurrence counts and thus
makes efficient use of statistics. The model pro-
duces a word vector space with meaningful sub-
structure, as evidenced by its state-of-the-art per-
formance of 75% accuracy on the word analogy
dataset. We also demonstrate that our methods
outperform other current methods on several word
similarity tasks, and also on a common named en-
tity recognition (NER) benchmark.
We provide the source code for the model as
well as trained word vectors at http://nlp.
stanford.edu/projects/glove/.
1532
2 Related Work
Matrix Factorization Methods. Matrix factor-
ization methods for generating low-dimensional
word representations have roots stretching as far
back as LSA. These methods utilize low-rank ap-
proximations to decompose large matrices that
capture statistical information about a corpus. The
particular type of information captured by such
matrices varies by application. In LSA, the ma-
trices are of ?term-document? type, i.e., the rows
correspond to words or terms, and the columns
correspond to different documents in the corpus.
In contrast, the Hyperspace Analogue to Language
(HAL) (Lund and Burgess, 1996), for example,
utilizes matrices of ?term-term? type, i.e., the rows
and columns correspond to words and the entries
correspond to the number of times a given word
occurs in the context of another given word.
A main problem with HAL and related meth-
ods is that the most frequent words contribute a
disproportionate amount to the similarity measure:
the number of times two words co-occur with the
or and, for example, will have a large effect on
their similarity despite conveying relatively little
about their semantic relatedness. A number of
techniques exist that addresses this shortcoming of
HAL, such as the COALS method (Rohde et al.,
2006), in which the co-occurrence matrix is first
transformed by an entropy- or correlation-based
normalization. An advantage of this type of trans-
formation is that the raw co-occurrence counts,
which for a reasonably sized corpus might span
8 or 9 orders of magnitude, are compressed so as
to be distributed more evenly in a smaller inter-
val. A variety of newer models also pursue this
approach, including a study (Bullinaria and Levy,
2007) that indicates that positive pointwise mu-
tual information (PPMI) is a good transformation.
More recently, a square root type transformation
in the form of Hellinger PCA (HPCA) (Lebret and
Collobert, 2014) has been suggested as an effec-
tive way of learning word representations.
Shallow Window-Based Methods. Another
approach is to learn word representations that aid
in making predictions within local context win-
dows. For example, Bengio et al. (2003) intro-
duced a model that learns word vector representa-
tions as part of a simple neural network architec-
ture for language modeling. Collobert and Weston
(2008) decoupled the word vector training from
the downstream training objectives, which paved
the way for Collobert et al. (2011) to use the full
context of a word for learning the word represen-
tations, rather than just the preceding context as is
the case with language models.
Recently, the importance of the full neural net-
work structure for learning useful word repre-
sentations has been called into question. The
skip-gram and continuous bag-of-words (CBOW)
models of Mikolov et al. (2013a) propose a sim-
ple single-layer architecture based on the inner
product between two word vectors. Mnih and
Kavukcuoglu (2013) also proposed closely-related
vector log-bilinear models, vLBL and ivLBL, and
Levy et al. (2014) proposed explicit word embed-
dings based on a PPMI metric.
In the skip-gram and ivLBL models, the objec-
tive is to predict a word?s context given the word
itself, whereas the objective in the CBOW and
vLBL models is to predict a word given its con-
text. Through evaluation on a word analogy task,
these models demonstrated the capacity to learn
linguistic patterns as linear relationships between
the word vectors.
Unlike the matrix factorization methods, the
shallow window-based methods suffer from the
disadvantage that they do not operate directly on
the co-occurrence statistics of the corpus. Instead,
these models scan context windows across the en-
tire corpus, which fails to take advantage of the
vast amount of repetition in the data.
3 The GloVe Model
The statistics of word occurrences in a corpus is
the primary source of information available to all
unsupervised methods for learning word represen-
tations, and although many such methods now ex-
ist, the question still remains as to how meaning
is generated from these statistics, and how the re-
sulting word vectors might represent that meaning.
In this section, we shed some light on this ques-
tion. We use our insights to construct a new model
for word representation which we call GloVe, for
Global Vectors, because the global corpus statis-
tics are captured directly by the model.
First we establish some notation. Let the matrix
of word-word co-occurrence counts be denoted by
X , whose entries Xi j tabulate the number of times
word j occurs in the context of word i. Let Xi =
?
k Xik be the number of times any word appears
in the context of word i. Finally, let Pi j = P( j |i) =
Xi j/Xi be the probability that word j appear in the
1533
Table 1: Co-occurrence probabilities for target words ice and steam with selected context words from a 6
billion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion
cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and
small values (much less than 1) correlate well with properties specific of steam.
Probability and Ratio k = solid k = gas k = water k = fashion
P(k |ice) 1.9 ? 10
?4
6.6 ? 10
?5
3.0 ? 10
?3
1.7 ? 10
?5
P(k |steam) 2.2 ? 10
?5
7.8 ? 10
?4
2.2 ? 10
?3
1.8 ? 10
?5
P(k |ice)/P(k |steam) 8.9 8.5 ? 10
?2
1.36 0.96
context of word i.
We begin with a simple example that showcases
how certain aspects of meaning can be extracted
directly from co-occurrence probabilities. Con-
sider two words i and j that exhibit a particular as-
pect of interest; for concreteness, suppose we are
interested in the concept of thermodynamic phase,
for which we might take i = ice and j = steam.
The relationship of these words can be examined
by studying the ratio of their co-occurrence prob-
abilities with various probe words, k. For words
k related to ice but not steam, say k = solid, we
expect the ratio Pik/Pjk will be large. Similarly,
for words k related to steam but not ice, say k =
gas, the ratio should be small. For words k like
water or fashion, that are either related to both ice
and steam, or to neither, the ratio should be close
to one. Table 1 shows these probabilities and their
ratios for a large corpus, and the numbers confirm
these expectations. Compared to the raw probabil-
ities, the ratio is better able to distinguish relevant
words (solid and gas) from irrelevant words (water
and fashion) and it is also better able to discrimi-
nate between the two relevant words.
The above argument suggests that the appropri-
ate starting point for word vector learning should
be with ratios of co-occurrence probabilities rather
than the probabilities themselves. Noting that the
ratio Pik/Pjk depends on three words i, j, and k,
the most general model takes the form,
F (wi ,w j , w?k ) =
Pik
Pjk
, (1)
where w ? Rd are word vectors and w? ? Rd
are separate context word vectors whose role will
be discussed in Section 4.2. In this equation, the
right-hand side is extracted from the corpus, and
F may depend on some as-of-yet unspecified pa-
rameters. The number of possibilities for F is vast,
but by enforcing a few desiderata we can select a
unique choice. First, we would like F to encode
the information present the ratio Pik/Pjk in the
word vector space. Since vector spaces are inher-
ently linear structures, the most natural way to do
this is with vector differences. With this aim, we
can restrict our consideration to those functions F
that depend only on the difference of the two target
words, modifying Eqn. (1) to,
F (wi ? w j , w?k ) =
Pik
Pjk
. (2)
Next, we note that the arguments of F in Eqn. (2)
are vectors while the right-hand side is a scalar.
While F could be taken to be a complicated func-
tion parameterized by, e.g., a neural network, do-
ing so would obfuscate the linear structure we are
trying to capture. To avoid this issue, we can first
take the dot product of the arguments,
F
(
(wi ? w j )
T w?k
)
=
Pik
Pjk
, (3)
which prevents F from mixing the vector dimen-
sions in undesirable ways. Next, note that for
word-word co-occurrence matrices, the distinction
between a word and a context word is arbitrary and
that we are free to exchange the two roles. To do so
consistently, we must not only exchange w ? w?
but also X ? X
T
. Our final model should be in-
variant under this relabeling, but Eqn. (3) is not.
However, the symmetry can be restored in two
steps. First, we require that F be a homomorphism
between the groups (R,+) and (R>0,? ), i.e.,
F
(
(wi ? w j )
T w?k
)
=
F (wTi w?k )
F (wTj w?k )
, (4)
which, by Eqn. (3), is solved by,
F (wTi w?k ) = Pik =
Xik
Xi
. (5)
The solution to Eqn. (4) is F = exp, or,
wTi w?k = log(Pik ) = log(Xik ) ? log(Xi ) . (6)
1534
Next, we note that Eqn. (6) would exhibit the ex-
change symmetry if not for the log(Xi ) on the
right-hand side. However, this term is indepen-
dent of k so it can be absorbed into a bias bi for
wi . Finally, adding an additional bias ?bk for w?k
restores the symmetry,
wTi w?k + bi + ?bk = log(Xik ) . (7)
Eqn. (7) is a drastic simplification over Eqn. (1),
but it is actually ill-defined since the logarithm di-
verges whenever its argument is zero. One reso-
lution to this issue is to include an additive shift
in the logarithm, log(Xik ) ? log(1 + Xik ), which
maintains the sparsity of X while avoiding the di-
vergences. The idea of factorizing the log of the
co-occurrence matrix is closely related to LSA and
we will use the resulting model as a baseline in
our experiments. A main drawback to this model
is that it weighs all co-occurrences equally, even
those that happen rarely or never. Such rare co-
occurrences are noisy and carry less information
than the more frequent ones ? yet even just the
zero entries account for 75?95% of the data in X ,
depending on the vocabulary size and corpus.
We propose a new weighted least squares re-
gression model that addresses these problems.
Casting Eqn. (7) as a least squares problem and
introducing a weighting function f (Xi j ) into the
cost function gives us the model
J =
V
?
i, j=1
f
(
Xi j
) (
wTi w? j + bi + ?bj ? log Xi j
)
2
,
(8)
where V is the size of the vocabulary. The weight-
ing function should obey the following properties:
1. f (0) = 0. If f is viewed as a continuous
function, it should vanish as x ? 0 fast
enough that the limx?0 f (x) log
2
x is finite.
2. f (x) should be non-decreasing so that rare
co-occurrences are not overweighted.
3. f (x) should be relatively small for large val-
ues of x, so that frequent co-occurrences are
not overweighted.
Of course a large number of functions satisfy these
properties, but one class of functions that we found
to work well can be parameterized as,
f (x) =
{
(x/x
max
)
?
if x < x
max
1 otherwise .
(9)
0.2
0.4
0.6
0.8
1.0
0.0
Figure 1: Weighting function f with ? = 3/4.
The performance of the model depends weakly on
the cutoff, which we fix to x
max
= 100 for all our
experiments. We found that ? = 3/4 gives a mod-
est improvement over a linear version with ? = 1.
Although we offer only empirical motivation for
choosing the value 3/4, it is interesting that a sim-
ilar fractional power scaling was found to give the
best performance in (Mikolov et al., 2013a).
3.1 Relationship to Other Models
Because all unsupervised methods for learning
word vectors are ultimately based on the occur-
rence statistics of a corpus, there should be com-
monalities between the models. Nevertheless, cer-
tain models remain somewhat opaque in this re-
gard, particularly the recent window-based meth-
ods like skip-gram and ivLBL. Therefore, in this
subsection we show how these models are related
to our proposed model, as defined in Eqn. (8).
The starting point for the skip-gram or ivLBL
methods is a model Qi j for the probability that
word j appears in the context of word i. For con-
creteness, let us assume that Qi j is a softmax,
Qi j =
exp(wTi w? j )
?V
k=1 exp(w
T
i w?k )
. (10)
Most of the details of these models are irrelevant
for our purposes, aside from the the fact that they
attempt to maximize the log probability as a con-
text window scans over the corpus. Training pro-
ceeds in an on-line, stochastic fashion, but the im-
plied global objective function can be written as,
J = ?
?
i?corpus
j?context(i)
logQi j . (11)
Evaluating the normalization factor of the soft-
max for each term in this sum is costly. To al-
low for efficient training, the skip-gram and ivLBL
models introduce approximations to Qi j . How-
ever, the sum in Eqn. (11) can be evaluated much
1535
more efficiently if we first group together those
terms that have the same values for i and j,
J = ?
V
?
i=1
V
?
j=1
Xi j logQi j , (12)
where we have used the fact that the number of
like terms is given by the co-occurrence matrix X .
Recalling our notation for Xi =
?
k Xik and
Pi j = Xi j/Xi , we can rewrite J as,
J = ?
V
?
i=1
Xi
V
?
j=1
Pi j logQi j =
V
?
i=1
XiH (Pi ,Qi ) ,
(13)
where H (Pi ,Qi ) is the cross entropy of the dis-
tributions Pi and Qi , which we define in analogy
to Xi . As a weighted sum of cross-entropy error,
this objective bears some formal resemblance to
the weighted least squares objective of Eqn. (8).
In fact, it is possible to optimize Eqn. (13) directly
as opposed to the on-line training methods used in
the skip-gram and ivLBL models. One could inter-
pret this objective as a ?global skip-gram? model,
and it might be interesting to investigate further.
On the other hand, Eqn. (13) exhibits a number of
undesirable properties that ought to be addressed
before adopting it as a model for learning word
vectors.
To begin, cross entropy error is just one among
many possible distance measures between prob-
ability distributions, and it has the unfortunate
property that distributions with long tails are of-
ten modeled poorly with too much weight given
to the unlikely events. Furthermore, for the mea-
sure to be bounded it requires that the model dis-
tribution Q be properly normalized. This presents
a computational bottleneck owing to the sum over
the whole vocabulary in Eqn. (10), and it would be
desirable to consider a different distance measure
that did not require this property of Q. A natural
choice would be a least squares objective in which
normalization factors in Q and P are discarded,
?
J =
?
i, j
Xi
(
?
Pi j ? ?Qi j
)
2
(14)
where
?
Pi j = Xi j and ?Qi j = exp(wTi w? j ) are the
unnormalized distributions. At this stage another
problem emerges, namely that Xi j often takes very
large values, which can complicate the optimiza-
tion. An effective remedy is to minimize the
squared error of the logarithms of
?
P and
?
Q instead,
?
J =
?
i, j
Xi
(
log
?
Pi j ? log ?Qi j
)
2
=
?
i, j
Xi
(
wTi w? j ? log Xi j
)
2
. (15)
Finally, we observe that while the weighting factor
Xi is preordained by the on-line training method
inherent to the skip-gram and ivLBL models, it is
by no means guaranteed to be optimal. In fact,
Mikolov et al. (2013a) observe that performance
can be increased by filtering the data so as to re-
duce the effective value of the weighting factor for
frequent words. With this in mind, we introduce
a more general weighting function, which we are
free to take to depend on the context word as well.
The result is,
?
J =
?
i, j
f (Xi j )
(
wTi w? j ? log Xi j
)
2
, (16)
which is equivalent
1
to the cost function of
Eqn. (8), which we derived previously.
3.2 Complexity of the model
As can be seen from Eqn. (8) and the explicit form
of the weighting function f (X ), the computational
complexity of the model depends on the number of
nonzero elements in the matrix X . As this num-
ber is always less than the total number of en-
tries of the matrix, the model scales no worse than
O( |V |
2
). At first glance this might seem like a sub-
stantial improvement over the shallow window-
based approaches, which scale with the corpus
size, |C |. However, typical vocabularies have hun-
dreds of thousands of words, so that |V |
2
can be in
the hundreds of billions, which is actually much
larger than most corpora. For this reason it is im-
portant to determine whether a tighter bound can
be placed on the number of nonzero elements of
X .
In order to make any concrete statements about
the number of nonzero elements in X , it is neces-
sary to make some assumptions about the distribu-
tion of word co-occurrences. In particular, we will
assume that the number of co-occurrences of word
i with word j, Xi j , can be modeled as a power-law
function of the frequency rank of that word pair,
ri j :
Xi j =
k
(ri j )
?
. (17)
1
We could also include bias terms in Eqn. (16).
1536
The total number of words in the corpus is pro-
portional to the sum over all elements of the co-
occurrence matrix X ,
|C | ?
?
i j
Xi j =
|X |
?
r=1
k
r
?
= kH|X |,? , (18)
where we have rewritten the last sum in terms of
the generalized harmonic number Hn,m . The up-
per limit of the sum, |X |, is the maximum fre-
quency rank, which coincides with the number of
nonzero elements in the matrix X . This number is
also equal to the maximum value of r in Eqn. (17)
such that Xi j ? 1, i.e., |X | = k
1/?
. Therefore we
can write Eqn. (18) as,
|C | ? |X |
?
H|X |,? . (19)
We are interested in how |X | is related to |C | when
both numbers are large; therefore we are free to
expand the right hand side of the equation for large
|X |. For this purpose we use the expansion of gen-
eralized harmonic numbers (Apostol, 1976),
Hx,s =
x
1?s
1 ? s
+ ? (s) + O(x
?s
) if s > 0, s , 1 ,
(20)
giving,
|C | ?
|X |
1 ? ?
+ ? (?) |X |
?
+ O(1) , (21)
where ? (s) is the Riemann zeta function. In the
limit that X is large, only one of the two terms on
the right hand side of Eqn. (21) will be relevant,
and which term that is depends on whether ? > 1,
|X | =
{
O(|C |) if ? < 1,
O(|C |
1/?
) if ? > 1.
(22)
For the corpora studied in this article, we observe
that Xi j is well-modeled by Eqn. (17) with ? =
1.25. In this case we have that |X | = O(|C |
0.8
).
Therefore we conclude that the complexity of the
model is much better than the worst case O(V
2
),
and in fact it does somewhat better than the on-line
window-based methods which scale like O(|C |).
4 Experiments
4.1 Evaluation methods
We conduct experiments on the word analogy
task of Mikolov et al. (2013a), a variety of word
similarity tasks, as described in (Luong et al.,
2013), and on the CoNLL-2003 shared benchmark
Table 2: Results on the word analogy task, given
as percent accuracy. Underlined scores are best
within groups of similarly-sized models; bold
scores are best overall. HPCA vectors are publicly
available
2
; (i)vLBL results are from (Mnih et al.,
2013); skip-gram (SG) and CBOW results are
from (Mikolov et al., 2013a,b); we trained SG
?
and CBOW
?
using the word2vec tool
3
. See text
for details and a description of the SVD models.
Model Dim. Size Sem. Syn. Tot.
ivLBL 100 1.5B 55.9 50.1 53.2
HPCA 100 1.6B 4.2 16.4 10.8
GloVe 100 1.6B 67.5 54.3 60.3
SG 300 1B 61 61 61
CBOW 300 1.6B 16.1 52.6 36.1
vLBL 300 1.5B 54.2 64.8 60.0
ivLBL 300 1.5B 65.2 63.0 64.0
GloVe 300 1.6B 80.8 61.5 70.3
SVD 300 6B 6.3 8.1 7.3
SVD-S 300 6B 36.7 46.6 42.1
SVD-L 300 6B 56.6 63.0 60.1
CBOW
?
300 6B 63.6 67.4 65.7
SG
?
300 6B 73.0 66.0 69.1
GloVe 300 6B 77.4 67.0 71.7
CBOW 1000 6B 57.3 68.9 63.7
SG 1000 6B 66.1 65.1 65.6
SVD-L 300 42B 38.4 58.2 49.2
GloVe 300 42B 81.9 69.3 75.0
dataset for NER (Tjong Kim Sang and De Meul-
der, 2003).
Word analogies. The word analogy task con-
sists of questions like, ?a is to b as c is to ??
The dataset contains 19,544 such questions, di-
vided into a semantic subset and a syntactic sub-
set. The semantic questions are typically analogies
about people or places, like ?Athens is to Greece
as Berlin is to ??. The syntactic questions are
typically analogies about verb tenses or forms of
adjectives, for example ?dance is to dancing as fly
is to ??. To correctly answer the question, the
model should uniquely identify the missing term,
with only an exact correspondence counted as a
correct match. We answer the question ?a is to b
as c is to ?? by finding the word d whose repre-
sentation wd is closest to wb ? wa + wc according
to the cosine similarity.
4
2
http://lebret.ch/words/
3
http://code.google.com/p/word2vec/
4
Levy et al. (2014) introduce a multiplicative analogy
evaluation, 3COSMUL, and report an accuracy of 68.24% on
1537
0 100 200 300 400 500 60020
30
40
50
60
70
80
Vector Dimension
Accur
acy [%
]
 
SemanticSyntacticOverall
(a) Symmetric context
2 4 6 8 1040
50
55
60
65
70
45
Window Size
Accur
acy [%
]
 
 
 
SemanticSyntacticOverall
(b) Symmetric context
2 4 6 8 1040
50
55
60
65
70
45
Window Size
Accur
acy [%
]
 
 
 
SemanticSyntacticOverall
(c) Asymmetric context
Figure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are
trained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100.
Word similarity. While the analogy task is our
primary focus since it tests for interesting vector
space substructures, we also evaluate our model on
a variety of word similarity tasks in Table 3. These
include WordSim-353 (Finkelstein et al., 2001),
MC (Miller and Charles, 1991), RG (Rubenstein
and Goodenough, 1965), SCWS (Huang et al.,
2012), and RW (Luong et al., 2013).
Named entity recognition. The CoNLL-2003
English benchmark dataset for NER is a collec-
tion of documents from Reuters newswire articles,
annotated with four entity types: person, location,
organization, and miscellaneous. We train mod-
els on CoNLL-03 training data on test on three
datasets: 1) ConLL-03 testing data, 2) ACE Phase
2 (2001-02) and ACE-2003 data, and 3) MUC7
Formal Run test set. We adopt the BIO2 annota-
tion standard, as well as all the preprocessing steps
described in (Wang and Manning, 2013). We use a
comprehensive set of discrete features that comes
with the standard distribution of the Stanford NER
model (Finkel et al., 2005). A total of 437,905
discrete features were generated for the CoNLL-
2003 training dataset. In addition, 50-dimensional
vectors for each word of a five-word context are
added and used as continuous features. With these
features as input, we trained a conditional random
field (CRF) with exactly the same setup as the
CRF
join
model of (Wang and Manning, 2013).
4.2 Corpora and training details
We trained our model on five corpora of varying
sizes: a 2010 Wikipedia dump with 1 billion to-
kens; a 2014 Wikipedia dump with 1.6 billion to-
kens; Gigaword 5 which has 4.3 billion tokens; the
combination Gigaword5 + Wikipedia2014, which
the analogy task. This number is evaluated on a subset of the
dataset so it is not included in Table 2. 3COSMUL performed
worse than cosine similarity in almost all of our experiments.
has 6 billion tokens; and on 42 billion tokens of
web data, from Common Crawl
5
. We tokenize
and lowercase each corpus with the Stanford to-
kenizer, build a vocabulary of the 400,000 most
frequent words
6
, and then construct a matrix of co-
occurrence counts X . In constructing X , we must
choose how large the context window should be
and whether to distinguish left context from right
context. We explore the effect of these choices be-
low. In all cases we use a decreasing weighting
function, so that word pairs that are d words apart
contribute 1/d to the total count. This is one way
to account for the fact that very distant word pairs
are expected to contain less relevant information
about the words? relationship to one another.
For all our experiments, we set x
max
= 100,
? = 3/4, and train the model using AdaGrad
(Duchi et al., 2011), stochastically sampling non-
zero elements from X , with initial learning rate of
0.05. We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate). Unless otherwise noted, we use a context of
ten words to the left and ten words to the right.
The model generates two sets of word vectors,
W and
?
W . When X is symmetric, W and
?
W are
equivalent and differ only as a result of their ran-
dom initializations; the two sets of vectors should
perform equivalently. On the other hand, there is
evidence that for certain types of neural networks,
training multiple instances of the network and then
combining the results can help reduce overfitting
and noise and generally improve results (Ciresan
et al., 2012). With this in mind, we choose to use
5
To demonstrate the scalability of the model, we also
trained it on a much larger sixth corpus, containing 840 bil-
lion tokens of web data, but in this case we did not lowercase
the vocabulary, so the results are not directly comparable.
6
For the model trained on Common Crawl data, we use a
larger vocabulary of about 2 million words.
1538
the sum W +
?
W as our word vectors. Doing so typ-
ically gives a small boost in performance, with the
biggest increase in the semantic analogy task.
We compare with the published results of a va-
riety of state-of-the-art models, as well as with
our own results produced using the word2vec
tool and with several baselines using SVDs. With
word2vec, we train the skip-gram (SG
?
) and
continuous bag-of-words (CBOW
?
) models on the
6 billion token corpus (Wikipedia 2014 + Giga-
word 5) with a vocabulary of the top 400,000 most
frequent words and a context window size of 10.
We used 10 negative samples, which we show in
Section 4.6 to be a good choice for this corpus.
For the SVD baselines, we generate a truncated
matrix X
trunc
which retains the information of how
frequently each word occurs with only the top
10,000 most frequent words. This step is typi-
cal of many matrix-factorization-based methods as
the extra columns can contribute a disproportion-
ate number of zero entries and the methods are
otherwise computationally expensive.
The singular vectors of this matrix constitute
the baseline ?SVD?. We also evaluate two related
baselines: ?SVD-S? in which we take the SVD of
?
X
trunc
, and ?SVD-L? in which we take the SVD
of log(1+X
trunc
). Both methods help compress the
otherwise large range of values in X .
7
4.3 Results
We present results on the word analogy task in Ta-
ble 2. The GloVe model performs significantly
better than the other baselines, often with smaller
vector sizes and smaller corpora. Our results us-
ing the word2vec tool are somewhat better than
most of the previously published results. This is
due to a number of factors, including our choice to
use negative sampling (which typically works bet-
ter than the hierarchical softmax), the number of
negative samples, and the choice of the corpus.
We demonstrate that the model can easily be
trained on a large 42 billion token corpus, with a
substantial corresponding performance boost. We
note that increasing the corpus size does not guar-
antee improved results for other models, as can be
seen by the decreased performance of the SVD-
7
We also investigated several other weighting schemes for
transforming X ; what we report here performed best. Many
weighting schemes like PPMI destroy the sparsity of X and
therefore cannot feasibly be used with large vocabularies.
With smaller vocabularies, these information-theoretic trans-
formations do indeed work well on word similarity measures,
but they perform very poorly on the word analogy task.
Table 3: Spearman rank correlation on word simi-
larity tasks. All vectors are 300-dimensional. The
CBOW
?
vectors are from the word2vec website
and differ in that they contain phrase vectors.
Model Size WS353 MC RG SCWS RW
SVD 6B 35.3 35.1 42.5 38.3 25.6
SVD-S 6B 56.5 71.5 71.0 53.6 34.7
SVD-L 6B 65.7 72.7 75.1 56.5 37.0
CBOW
?
6B 57.2 65.6 68.2 57.0 32.5
SG
?
6B 62.8 65.2 69.7 58.1 37.2
GloVe 6B 65.8 72.7 77.8 53.9 38.1
SVD-L 42B 74.0 76.4 74.1 58.3 39.9
GloVe 42B 75.9 83.6 82.9 59.6 47.8
CBOW
?
100B 68.4 79.6 75.4 59.4 45.5
L model on this larger corpus. The fact that this
basic SVD model does not scale well to large cor-
pora lends further evidence to the necessity of the
type of weighting scheme proposed in our model.
Table 3 shows results on five different word
similarity datasets. A similarity score is obtained
from the word vectors by first normalizing each
feature across the vocabulary and then calculat-
ing the cosine similarity. We compute Spearman?s
rank correlation coefficient between this score and
the human judgments. CBOW
?
denotes the vec-
tors available on the word2vec website that are
trained with word and phrase vectors on 100B
words of news data. GloVe outperforms it while
using a corpus less than half the size.
Table 4 shows results on the NER task with the
CRF-based model. The L-BFGS training termi-
nates when no improvement has been achieved on
the dev set for 25 iterations. Otherwise all config-
urations are identical to those used by Wang and
Manning (2013). The model labeled Discrete is
the baseline using a comprehensive set of discrete
features that comes with the standard distribution
of the Stanford NER model, but with no word vec-
tor features. In addition to the HPCA and SVD
models discussed previously, we also compare to
the models of Huang et al. (2012) (HSMN) and
Collobert and Weston (2008) (CW). We trained
the CBOW model using the word2vec tool
8
.
The GloVe model outperforms all other methods
on all evaluation metrics, except for the CoNLL
test set, on which the HPCA method does slightly
better. We conclude that the GloVe vectors are
useful in downstream NLP tasks, as was first
8
We use the same parameters as above, except in this case
we found 5 negative samples to work slightly better than 10.
1539
Table 4: F1 score on NER task with 50d vectors.
Discrete is the baseline without word vectors. We
use publicly-available vectors for HPCA, HSMN,
and CW. See text for details.
Model Dev Test ACE MUC7
Discrete 91.0 85.4 77.4 73.4
SVD 90.8 85.7 77.3 73.7
SVD-S 91.0 85.5 77.6 74.3
SVD-L 90.5 84.8 73.6 71.5
HPCA 92.6 88.7 81.7 80.7
HSMN 90.5 85.7 78.7 74.7
CW 92.2 87.4 81.7 80.2
CBOW 93.1 88.2 82.2 81.1
GloVe 93.2 88.3 82.9 82.2
shown for neural vectors in (Turian et al., 2010).
4.4 Model Analysis: Vector Length and
Context Size
In Fig. 2, we show the results of experiments that
vary vector length and context window. A context
window that extends to the left and right of a tar-
get word will be called symmetric, and one which
extends only to the left will be called asymmet-
ric. In (a), we observe diminishing returns for vec-
tors larger than about 200 dimensions. In (b) and
(c), we examine the effect of varying the window
size for symmetric and asymmetric context win-
dows. Performance is better on the syntactic sub-
task for small and asymmetric context windows,
which aligns with the intuition that syntactic infor-
mation is mostly drawn from the immediate con-
text and can depend strongly on word order. Se-
mantic information, on the other hand, is more fre-
quently non-local, and more of it is captured with
larger window sizes.
4.5 Model Analysis: Corpus Size
In Fig. 3, we show performance on the word anal-
ogy task for 300-dimensional vectors trained on
different corpora. On the syntactic subtask, there
is a monotonic increase in performance as the cor-
pus size increases. This is to be expected since
larger corpora typically produce better statistics.
Interestingly, the same trend is not true for the se-
mantic subtask, where the models trained on the
smaller Wikipedia corpora do better than those
trained on the larger Gigaword corpus. This is
likely due to the large number of city- and country-
based analogies in the analogy dataset and the fact
that Wikipedia has fairly comprehensive articles
for most such locations. Moreover, Wikipedia?s
50
55
60
65
70
75
80
85 OverallSyntacticSemantic
Wiki20101B tokens
Accur
acy [%
]
Wiki20141.6B tokens Gigaword54.3B tokens Gigaword5 + Wiki20146B tokens Common Crawl 42B tokens
Figure 3: Accuracy on the analogy task for 300-
dimensional vectors trained on different corpora.
entries are updated to assimilate new knowledge,
whereas Gigaword is a fixed news repository with
outdated and possibly incorrect information.
4.6 Model Analysis: Run-time
The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across mul-
tiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 ma-
chine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X , the time it takes to train the model de-
pends on the vector size and the number of itera-
tions. For 300-dimensional vectors with the above
settings (and using all 32 cores of the above ma-
chine), a single iteration takes 14 minutes. See
Fig. 4 for a plot of the learning curve.
4.7 Model Analysis: Comparison with
word2vec
A rigorous quantitative comparison of GloVe with
word2vec is complicated by the existence of
many parameters that have a strong effect on per-
formance. We control for the main sources of vari-
ation that we identified in Sections 4.4 and 4.5 by
setting the vector length, context window size, cor-
pus, and vocabulary size to the configuration men-
tioned in the previous subsection.
The most important remaining variable to con-
trol for is training time. For GloVe, the rele-
vant parameter is the number of training iterations.
For word2vec, the obvious choice would be the
number of training epochs. Unfortunately, the
code is currently designed for only a single epoch:
1540
1 2 3 4 5 6
60
62
64
66
68
70
72
5 10 15 20 25
1357 10 15 20 25 30 40 50
Accu
racy 
[%]
Iterations (GloVe)
Negative Samples (CBOW)
Training Time (hrs)
 
GloVeCBOW
(a) GloVe vs CBOW
3 6 9 12 15 18 21 24
60
62
64
66
68
70
72
20 40 60 80 100
1 2 3 4 5 6 7 10 12 15 20
GloVeSkip-Gram
Accu
racy 
[%]
Iterations (GloVe)
Negative Samples (Skip-Gram)
Training Time (hrs)
(b) GloVe vs Skip-Gram
Figure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by
the number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram
(b). In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 +
Gigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10.
it specifies a learning schedule specific to a single
pass through the data, making a modification for
multiple passes a non-trivial task. Another choice
is to vary the number of negative samples. Adding
negative samples effectively increases the number
of training words seen by the model, so in some
ways it is analogous to extra epochs.
We set any unspecified parameters to their de-
fault values, assuming that they are close to opti-
mal, though we acknowledge that this simplifica-
tion should be relaxed in a more thorough analysis.
In Fig. 4, we plot the overall performance on
the analogy task as a function of training time.
The two x-axes at the bottom indicate the corre-
sponding number of training iterations for GloVe
and negative samples for word2vec. We note
that word2vec?s performance actually decreases
if the number of negative samples increases be-
yond about 10. Presumably this is because the
negative sampling method does not approximate
the target probability distribution well.
9
For the same corpus, vocabulary, window size,
and training time, GloVe consistently outperforms
word2vec. It achieves better results faster, and
also obtains the best results irrespective of speed.
5 Conclusion
Recently, considerable attention has been focused
on the question of whether distributional word
representations are best learned from count-based
9
In contrast, noise-contrastive estimation is an approxi-
mation which improves with more negative samples. In Ta-
ble 1 of (Mnih et al., 2013), accuracy on the analogy task is a
non-decreasing function of the number of negative samples.
methods or from prediction-based methods. Cur-
rently, prediction-based models garner substantial
support; for example, Baroni et al. (2014) argue
that these models perform better across a range of
tasks. In this work we argue that the two classes
of methods are not dramatically different at a fun-
damental level since they both probe the under-
lying co-occurrence statistics of the corpus, but
the efficiency with which the count-based meth-
ods capture global statistics can be advantageous.
We construct a model that utilizes this main ben-
efit of count data while simultaneously capturing
the meaningful linear substructures prevalent in
recent log-bilinear prediction-based methods like
word2vec. The result, GloVe, is a new global
log-bilinear regression model for the unsupervised
learning of word representations that outperforms
other models on word analogy, word similarity,
and named entity recognition tasks.
Acknowledgments
We thank the anonymous reviewers for their valu-
able comments. Stanford University gratefully
acknowledges the support of the Defense Threat
Reduction Agency (DTRA) under Air Force Re-
search Laboratory (AFRL) contract no. FA8650-
10-C-7020 and the Defense Advanced Research
Projects Agency (DARPA) Deep Exploration and
Filtering of Text (DEFT) Program under AFRL
contract no. FA8750-13-2-0040. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of the DTRA,
AFRL, DEFT, or the US government.
1541
References
Tom M. Apostol. 1976. Introduction to Analytic
Number Theory. Introduction to Analytic Num-
ber Theory.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL.
Yoshua Bengio. 2009. Learning deep architectures
for AI. Foundations and Trends in Machine
Learning.
Yoshua Bengio, R?ejean Ducharme, Pascal Vin-
cent, and Christian Janvin. 2003. A neural prob-
abilistic language model. JMLR, 3:1137?1155.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study.
Behavior Research Methods, 39(3):510?526.
Dan C. Ciresan, Alessandro Giusti, Luca M. Gam-
bardella, and J?urgen Schmidhuber. 2012. Deep
neural networks segment neuronal membranes
in electron microscopy images. In NIPS, pages
2852?2860.
Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language process-
ing: deep neural networks with multitask learn-
ing. In Proceedings of ICML, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. 2011. Natural Language Processing (Al-
most) from Scratch. JMLR, 12:2493?2537.
Scott Deerwester, Susan T. Dumais, George W.
Furnas, Thomas K. Landauer, and Richard
Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for
Information Science, 41.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learn-
ing and stochastic optimization. JMLR, 12.
Lev Finkelstein, Evgenly Gabrilovich, Yossi Ma-
tias, Ehud Rivlin, Zach Solan, Gadi Wolfman,
and Eytan Ruppin. 2001. Placing search in con-
text: The concept revisited. In Proceedings
of the 10th international conference on World
Wide Web, pages 406?414. ACM.
Eric H. Huang, Richard Socher, Christopher D.
Manning, and Andrew Y. Ng. 2012. Improving
Word Representations via Global Context and
Multiple Word Prototypes. In ACL.
R?emi Lebret and Ronan Collobert. 2014. Word
embeddings through Hellinger PCA. In EACL.
Omer Levy, Yoav Goldberg, and Israel Ramat-
Gan. 2014. Linguistic regularities in sparse and
explicit word representations. CoNLL-2014.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical
co-occurrence. Behavior Research Methods, In-
strumentation, and Computers, 28:203?208.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word represen-
tations with recursive neural networks for mor-
phology. CoNLL-2013.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient Estimation of Word
Representations in Vector Space. In ICLRWork-
shop Papers.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013b. Distributed
representations of words and phrases and their
compositionality. In NIPS, pages 3111?3119.
Tomas Mikolov, Wen tau Yih, and Geoffrey
Zweig. 2013c. Linguistic regularities in con-
tinuous space word representations. In HLT-
NAACL.
George A. Miller and Walter G. Charles. 1991.
Contextual correlates of semantic similarity.
Language and cognitive processes, 6(1):1?28.
Andriy Mnih and Koray Kavukcuoglu. 2013.
Learning word embeddings efficiently with
noise-contrastive estimation. In NIPS.
Douglas L. T. Rohde, Laura M. Gonnerman,
and David C. Plaut. 2006. An improved
model of semantic similarity based on lexical
co-occurence. Communications of the ACM,
8:627?633.
Herbert Rubenstein and John B. Goodenough.
1965. Contextual correlates of synonymy. Com-
munications of the ACM, 8(10):627?633.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing
Surveys, 34:1?47.
Richard Socher, John Bauer, Christopher D. Man-
ning, and Andrew Y. Ng. 2013. Parsing With
Compositional Vector Grammars. In ACL.
1542
Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron
Fernandes, and Gregory Marton. 2003. Quanti-
tative evaluation of passage retrieval algorithms
for question answering. In Proceedings of the
SIGIR Conference on Research and Develop-
ment in Informaion Retrieval.
Erik F. Tjong Kim Sang and Fien De Meul-
der. 2003. Introduction to the CoNLL-2003
shared task: Language-independent named en-
tity recognition. In CoNLL-2003.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and gen-
eral method for semi-supervised learning. In
Proceedings of ACL, pages 384?394.
Mengqiu Wang and Christopher D. Manning.
2013. Effect of non-linear deep architecture in
sequence labeling. In Proceedings of the 6th
International Joint Conference on Natural Lan-
guage Processing (IJCNLP).
1543
