Coling 2010: Poster Volume, pages 782?790,
Beijing, August 2010
Opinion Target Extraction in Chinese News Comments 
Tengfei Ma 
 
Xiaojun Wan* 
 
 
Abstract 
News Comments on the web express 
readers? attitudes or opinions about an 
event or object in the corresponding 
news article. And opinion target extrac-
tion from news comments is very impor-
tant for many useful Web applications. 
However, many sentences in the com-
ments are irregular and informal, and 
sometimes the opinion targets are impli-
cit. Thus the task is very challenging and 
it has not been investigated yet. In this 
paper, we propose a new approach to un-
iformly extracting explicit and implicit 
opinion targets from news comments by 
using Centering Theory. The approach 
uses global information in news articles 
as well as contextual information in ad-
jacent sentences of comments. Our expe-
rimental results verify the effectiveness 
of the proposed approach.  
1 Introduction 
With the dramatic development of web 2.0, there 
are more and more news web sites allowing 
users to comment on news events. These 
comments have become valuable resources for 
researchers to make advanced opinion analysis, 
such as tracking the attitudes to a focused event, 
person or corporation. In these advanced opinion 
analysis tasks, opinion target extraction is a 
necessary step. Unfortunately, former works did 
not focus on the domain of news comments. 
Though some researchers and workshops have 
investigated the task of opinion target extraction 
in product reviews and news articles, the 
                                                 
* Contact author 
methods cannot perform well on news comments. 
Actually, target extraction in news comments 
significantly differs from that in product reviews 
and news articles in the following ways. 
1) Products usually have a set of definite 
attributes (e.g. size) and related opinion words 
(e.g. large), and thus researchers can use a small 
fixed set of keywords to recognize frequent fea-
ture words (Zhuang et al, 2006), or leverage the 
associated rules between feature words and opi-
nion words to improve the performance (Hu and 
Liu, 2004; Su et al, 2008; Jin and Ho, 2009; Du 
and Tan, 2009). But news comments are more 
complicated. There are much more potential 
opinion targets in news comments. In other 
words, the candidate targets are in a much more 
open domain.  On the other hand, the opinion 
targets in news comments are not strongly asso-
ciated with the opinion words. We cannot judge 
a target by a special opinion word as easily as in 
product reviews. 
2) The opinionated sentences in news articles 
mostly contain opinion operators (e.g. believe, 
realize), which can be used to find the positions 
of opinion expressions. However, news com-
ments have already been considered to be de-
clared by readers and they do not have many 
operators to indicate the positions of opinion 
targets.  
3) Furthermore, many comment sentences are 
of free style. In many cases, there are even no 
manifest targets in the comment sentences. For 
example, a news article and its relational com-
ment are as follows: 
News: ?????????????????? 
(Dubai will build the highest skyscraper in the 
world)  
Comment:  
??????????????? 
(Really high, but what (is it) used for?) 
Institute of Compute Science and Technology 
The MOE Key Laboratory of Computational Linguistics 
Peking University 
{matengfei, wanxiaojun}@icst.pku.edu.cn 
782
The comment sentence obviously comments 
on ?skyscraper? by human understanding, but in 
the sentence we cannot find the word or an alter-
native. Instead, the real target is included in the 
news article. Now we give two definitions of the 
phenomenon. 
Implicit targets: The implicit targets are 
those opinion targets which do not occur in the 
current sentence. The sentence is called implicit 
sentence. 
Explicit targets: The explicit targets are those 
opinion targets which occur in the current right 
sentence, and the sentence is called explicit sen-
tence. 
In Chinese comments, the phenomena of im-
plicit targets are fairly common. In our dataset, 
the sentences with implicit targets make up near-
ly 30 percents of the total. 
In this paper, we focus on opinion target ex-
traction from news comments and propose a 
novel framework uniformly extracting explicit 
and implicit opinion targets. The method uses 
both information in news articles and informa-
tion in comment contexts to improve the result. 
We extract focused concepts in news articles as 
candidate implicit targets, and exploit a new ap-
proach based on Centering Theory to taking ad-
vantage of comment contexts.  
We evaluate our system on a test corpus con-
taining different topics. The results show that it 
improves the baseline by 8.8%, and the accuracy 
is also 8.1% higher over the popular SVM-based 
method.  
The rest of this paper is organized as follows: 
The next section gives an overview of the related 
work in opinion analysis. Section 3 introduces 
the background of Centering Theory and Section 
4 describes our framework based on Centering 
Theory. In Section 5 we test the results and give 
a discussion on the errors. Finally Section 6 
draws a conclusion. 
2 Related Work 
The early research of opinion mining only fo-
cused on the sentiment classification (Turney et 
al., 2002; Pang et al, 2002). However, for many 
applications only judging the sentiment orienta-
tion is not sufficient (eg. Hu and Liu, 2004). 
Fine-grained opinion analysis has attracted more 
and more attention these years. It mainly in-
cludes these types: opinion holder extraction 
(Kim and Hovy, 2005; Choi et al, 2005), opi-
nion target extraction (Kim and Hovy, 2006; 
Ruppenhofer et al, 2008), and the identification 
of opinion proposals (Bethard et al, 2004) and 
some special opinion expressions (Bloom et al, 
2007). Also, there are some other related tasks, 
such as detecting users? needs and wants (Ka-
nayama and Nasukawa, 2008). However, these 
general systems are different from ours because 
they do not have or use any contextual informa-
tion, and implicit opinion targets are not recog-
nized and handled there. 
A more special domain of feature extraction is 
product and movie reviews. Hu and Liu (2004) 
design a system to mine product features and 
generate opinion summaries of customer reviews. 
Frequent features are extracted by a statistical 
approach, and infrequent features are generated 
by the associated opinion words.  The product 
features are limited in amount and they are 
strongly associated with specific opinion words, 
so researchers can use a fixed set of keywords or 
templates to extract frequent features (Zhuang et 
al., 2006; Popescu and Etzioni, 2005) or try var-
ious methods to augment the database of product 
features and improve the extraction accuracy by 
using the relations between attributes and opi-
nions (Ghani et al, 2006; Su et al, 2008; Jin and 
Ho, 2009; Du and Tan, 2009). However, in news 
comments, the opinion targets are not strongly 
associated with specific opinion words and these 
techniques cannot be used. 
There are also some works focusing on the 
target extraction in news articles, such as 
NTCIR7-MOAT (Seki et al, 2008). Different 
from the news comments, there are opinion indi-
cators in the subjective sentences. However, in 
our task of this paper, the opinion holders are 
pre-assigned as the reviewers, so few opinion 
indicators and holders can be found. 
To our best knowledge, this paper is the first 
work of extracting opinion targets in news com-
ments. We analyze the complex phenomena in 
news comments and propose a framework to 
solve the problems of implicit targets. Our me-
thod synthesizes the information from related 
articles and contexts of comments, and it can 
effectively improve the extracting results. 
783
3 Background of Centering Theory 
Centering Theory (Grosz, Joshi and Weinstein, 
1995) was developed for an original purpose of 
indicating the coherence of a discourse and 
choosing a referring expression. In the theory, 
the term ?centers? of an utterance is used to 
refer to the entities serving to link this utterance 
to another utterance in a discourse. But this is 
not the only function of centers, and there are 
some other useful characteristics of centers to be 
recognized. Our observation shows that a center 
always represents the focus of attention, and the 
salience of a center indicates the significance of 
the component as a commented target. In news 
comments, we consider a comment as a 
discourse and a sentence as an utterance. If an 
utterance has a ?center?, then the center can be 
regarded as the target of the sentence. 
Before introducing the common process of 
choosing the centers in utterances, several defi-
nitions are elaborated as follows: 
Forward-looking center: Given an utter-
ance U, there is a set of forward-looking cen-
ters Cf(U) assigned. The set is a collection of 
all potential centers that may be realized by 
the next utterance. 
Backward-looking center: Each utterance 
is assigned exactly one (in fact at most one) 
backward-looking center Cb. The backward-
looking center of utterance Un+1 connects with 
one of the forward-looking centers of Un. The 
Cb is the real focus of the utterance. 
Rank: The rank is the salience of an ele-
ment of Cf. Ranking of elements in Cf(Un) 
guides determination of Cb(Un+1). The more 
highly ranked an element of Cf(Un ), the more 
likely it is to be Cb(Un+1). The most highly 
ranked element of Cf(Un) that is realized in 
Un+1 is the Cb(Un+1). The rank is affected by 
several factors, the most important of which 
depends on the grammatical role, with SUB-
JECT > OBJECT(S) > OTHER.  
Preferred center: In the set of Cf(Un), the 
element with the highest rank is a preferred 
center Cp(Un). This means that it has the high-
est probability to be Cb(Un+1). 
 
Table 1 is an example of the centers. In the 
example, the target of the first sentence is ?Jack?, 
which is exactly the preferred center; while in 
the second sentence, it is easy to see that ?him? 
gets more attention than ?the company? in this 
environment and thus the backward-looking cen-
ter is more likely to be the target. So we assume 
that if Cb(Un) exists, it can be regarded as the 
opinion target of Un, otherwise the Cp(Un) is the 
target. 
 
Utterance Center 
U1:????????
???????? 
(Jack regards the com-
pany as his life.) 
Cf: ??(Jack)/ 
?? (the company)/ 
??(life) 
Cb: null Cp: ??(Jack) U2: ????????
??????? 
(It attributes to him that 
the company can obtain 
today?s achievement.)
Cf: ??(the company)/ 
??(achievement)/  
?(??) (him(Jack)) 
Cb:?(??) (him(Jack)) Cp:??(the company)
Table 1 Example of different centers. 
4 Proposed Approach 
Due to the problems we introduced in Section 1, 
the techniques of target extraction in other do-
mains are not appropriate in news comments, 
and general approaches encounter the problems 
of free style sentences and implicit targets. For-
tunately, news comments have their own charac-
teristics, which can be used to improve the target 
extraction performance. 
One important characteristic is that though po-
tential opinion targets may be in large quantities, 
most comments focus on several central con-
cepts in the corresponding news article, especial-
ly in the title. So we can extract the focused con-
cepts in the news and use them as potential im-
plicit targets for the comments. 
784
The other useful information comes from the 
fact that the sentences in one comment are usual-
ly coherent. As the comments may be long and 
each comment contains several sentences, the 
sentences within one comment are relevant and 
coherent. So the opinion targets in previous sen-
tences have some influence on that in subsequent 
sentences. Using this kind of contextual informa-
tion, we can eliminate noisy candidates and relax 
the dependence on an unreliable syntactic parser. 
Considering the above characteristics, we 
propose a framework of target extraction based 
on focused concepts recognition and Centering 
Theory, as shown in Figure 1. 
Given a news article and its relevant com-
ments, we first adopt some syntactic rules to 
classify the comment sentences into implicit or 
explicit type. Whether a sentence includes an 
explicit target is mainly decided by whether it 
owns a subject. A few heuristic rules, such as the 
appearance of the subject, the combination of the 
POS, and the position of the predicate, are used 
based on the parse result by using a Chinese 
NLP toolkit1, and the rule-based classification 
can attain an accuracy of 77.33%.  
Then we exploit two different approaches for 
dealing with the two types of sentences, respec-
tively. For the implicit type, we extract the fo-
                                                 
1 LTP, http://ir.hit.edu.cn/demo/ltp/Sharing_Plan.htm 
LTP is an integrated NLP toolkit which contains segmenta-
tion, parsing, semantic role labeling, and etc. 
cused concepts in the news article as candidate 
implicit targets, and rank them by calculating the 
semantic relatedness between the targets and the 
sentence. For the explicit type, all nouns and 
pronouns in the sentence are extracted as candi-
date targets and ranked mainly by their gram-
matical roles. At last, Centering Theory is used 
to choose the best candidate using the ranks and 
contextual information.  
The details of the main parts are explained in 
the following sections. 
4.1 Focused Concepts (FC) Recognition 
As the comments usually point to the news 
article, it is highly probable that the implicit 
targets appear in the news article. Generally, the 
focused concepts of the news article are more 
likely to be the commented targets. Thus, if we 
extract the focused concepts of the news article, 
we will get the candidate implicit targets. 
In general, the focused concepts are named 
entities (Zhang et al 2004) or specific noun 
phrases. Taking the news 
?????????????????(D
ubai will build the highest skyscraper in the 
world)?    ----NEWS1 
as an example, ???(Dubai)? and 
?????(skyscraper)? are the potential opi-
nion targets. ?Dubai? is a named entity, and 
?skyscraper? is a specific noun phrase. In addi-
tion, the focused concepts may also appear in the 
content of the news article, if they attract enough 
attention or have strong relations with the fo-
cused named entities in the title.   
As the number of noun phrases is usually 
large, if we extract the two types of concepts 
together, there must be much noise to impact the 
final result. To be simple and accurate, we first 
extract focused named entities (FNE), and then 
expand them with other focused noun phrases, 
for the reason that the focused noun phrases 
usually have a strong relation with the focused 
named entities.   
 
Entity Type Person, Location, Organization, 
Time 
Title In title or not 
Frequency The number of occurrence 
Relative 
Frequency 
Frequency/the number of total 
words 
News Article News Comments
Sentences 
of 
Implicit 
Type
Sentences 
of 
Explicit 
Type
Focused named 
entity classifier
Focused 
Concepts
Implicit 
candidate 
targets
Explicit 
candidate 
targets
Choosing a best target via 
Centering Theory
RankingWikipedia-based
ESA
Grammar Role 
Analysis
Opinion 
targets
Rule Based 
Classifier
Figure 1: Framework of opinion target ex-
traction in news comments 
785
Distribution 
Entropy 
(Here we take 
N=5 according 
to the length of 
articles)
 
1
log
N
i
Entropy p pi i=
=? , where 
th
i
Occurrence in the i  Sectionp= Occurrence in Total  
Table 2 Features of FNE classification 
Extracting FNEs can be seen as a classifica-
tion problem. In this work, we choose the fea-
tures in Table 2. 
Given a news document, we first recognize all 
named entities with our own named entity re-
cognizer (NER).Then all named entities are clas-
sified based on the above mentioned features. 
The noun phrases in the title are also extracted 
and filtered by their frequency in the news ar-
ticle and co-occurrences with FNEs. The filter-
ing threshold is set to a relatively high value to 
guarantee that not much noise is brought in. 
Thus we can get a small set of focused concepts 
in the news article. 
4.2 Ranking Implicit Targets 
We use the semantic relatedness to decide which 
potential target is most likely to be the right im-
plicit target. There are many methods to calcu-
late the semantic relatedness. We choose the 
Wikipedia-base explicit semantic analysis (ESA) 
(Gabrilovich and Markovitch, 2007), for its 
adaptability and effectiveness for Chinese lan-
guage. The method converts a word or a sen-
tence to a series of wiki concepts, and then cal-
culates the similarity between words or sen-
tences. 
Input:  a Focused Concept t0 in the news Output: a vector C with a length of N.  C= 
<(cj,wj)>, where cj is a Wikipedia concept, and wj is the weight of cj 1. Find all nouns, adjectives and verbs co-occurring
with t0 in the same sentence, and put them into the set S= {ti}. 2. Compute MI (Mutual information) of each ti with t0. 3. Choose 10 words in S with the highest MI (ac-
cording to the total number of words, 10 is a 
proper value). Combine them with t0 into a word vector and assign each word ti a weight of its frequency vi in the news article. The vec-tor V= <(ti,vi)>, |V|?11. 4. Let <kij> be an inverted index entry for ti, where kij quantifies the strength of association of ti with Wikipedia concept cj. Then the vector V can be interpreted as a vector constructed by 
All Wikipedia concepts. Each concept cj has a 
weight wj= i i ijVt v k?? .5. Select N concepts with the highest weights.  
Table 3: Algorithm that converts a focused 
concept to a vector of Wikipedia concepts 
Chinese Wikipedia is not as large as English 
Wikipedia. When some words are not included 
in the database, the original ESA algorithm will 
fail. To solve the problem, we first expand the 
input FC with a few words extracted from the 
news article. The words represent the semantic 
information related to the article, so they are 
more informative than a single concept while 
easily recognized by the Wikipedia database. 
The details of the algorithm are shown in Table 
3. 
On the other hand, when given a comment 
sentence, we segment it to words and remove the 
stop words (e.g. ?? (of)?). Then the serial of 
words are also converted by ESA into a vector of 
Wikipedia concepts. 
After getting the vectors of wiki concepts for 
focused concepts and the comment sentence, we 
use the cosine metric to obtain their relatedness 
scores. In this way, the focused concepts are 
ranked by their relatedness scores with the sen-
tence. 
4.3 Ranking Explicit Targets 
A comment sentence with explicit targets usual-
ly has a complete syntactic structure. According 
to Centering Theory, the ranks of explicit targets 
are decided mainly by their grammatical roles. 
Generally, a subject is most likely to be the opi-
nion target, and the rank can be heuristically as-
signed by SUBJECT > OBJECT(S) > OTHER. 
4.4 Choosing Best Candidate target via 
Centering Theory (CT) 
After getting the candidate targets and their 
ranks, we start the matching step to make use of 
contextual information. The algorithm originates 
from the process of choosing preferred centers 
and backward-looking centers. A subtle adaption 
is that we add some global information in the 
news article as the context when dealing with the 
first sentence in a comment. The details of the 
algorithm are represented in Table 4. 
Now we give an example to show the whole 
process of the framework. The following com-
ment is associated with NEWS1 in Section 4.1. 
U1:???????????????? 
786
(Dubai is developing travel and trades.) 
U2:??????????? 
((It) is an active city.) 
U3:?????????????? 
(In Dubai you can encounter many miracles.) 
First, U1, U2 and U3 are classified as explicit, 
implicit and explicit, respectively. Then for U1 
and U3 we choose noun phrases and pronouns in 
the sentence as candidate targets and rank them 
according to their grammatical roles. U2 chooses 
FC as candidates, and ?Dubai? is more related 
than ?skyscraper?. At last, the final target is cho-
sen by the algorithm in Table 4 and the whole 
process is illustrated in Table5. 
 
Input: A comment with M sentences S={si}, each sentence has a candidate target set Cf(si)={ci}; The Focused Concepts set FC in the news article. 
Output: A target set {ti}, where each ti is the opinion target of sentence si. 1. For each si in S 2.         If i=1 (si is the first sentence) 3.              For each  ci in Cf(si) 4.                      If ci is contained in FC 5.                            Add ci into the set Cb(si) 6.              If Cb(si) is not void  7.                    Choose the highest ranked ele-ment in Cb(si) as ti  8.              Else 
9.                     Choose the highest ranked ele-ment in Cf(si) as ti 10.       Else 
11.             For each  ci in Cf(si)  12.                  If ci realizes (equals or refers to) an element c?i in Cf(si-1) 13.                            Add c?i into the set Cb(si) 14.             If Cb(si) is not void  15.                   Choose the highest ranked ele-
ment in Cb(si) as ti  16.             Else 17.                 Choose the highest ranked element 
in Cf(si) as ti 
Table 4 Algorithm of choosing the best candi-
date target via CT 
 type ranks of candidates target
U1 Explicit ??>??>????
(Dubai >travel >trade) 
??
(Dubai)
U2 Implicit ??>????
(Dubai>skyscraper) 
?? 
(Dubai)
U3 Explicit ?>??>??
(you>miracles>Dubai) 
?? 
(Dubai)
Table 5 Example of the extraction process 
5 Experiments 
5.1 Evaluation Setup 
To evaluate the whole system, we evaluate not 
only the result of the final target extraction but 
also some key steps. This makes the analysis of 
the bottleneck possible. 
We first build a FNE dataset to evaluate the 
FNE classification result. As our target extrac-
tion task focuses on news comments, we collect 
1000 news articles and the associated user com-
ments from http://comment.news.sohu.com, 
which is a famous website offering a platform 
for users to comment on the news. Every news 
articles are annotated with its focused named 
entities, which are also the most possible com-
mented targets.  
Then we build the target dataset to evaluate 
the final target extraction. 9 articles and asso-
ciated comments are randomly chosen from the 
FNE dataset, and each of their comment sen-
tences is annotated with the opinion target. The 
target dataset focuses on 3 different topics: eco-
nomics, technology and sports. Each document 
contains a news article and about 100 relevant 
comments, and there are 1597 comment sen-
tences in total. 
We assume that each comment sentence has 
one opinion target, but 108 sentences have more 
than one focused objects.  In that case, we anno-
tate all targets for evaluation and the result is 
regarded as true if we extract only one of the 
annotated targets. 
In the target dataset, there are 444 sentences 
with implicit targets. This demonstrates that the 
implicit target extraction problem is prevalent 
and worth solving.  
For the final target extraction, we use the ac-
curacy metric to evaluate the result. It is defined 
as follows: 
We do not use the precision and recall metric 
because every comment sentence in our dataset 
must have a target after extracting. The precision 
and the recall are both equal to the accuracy. 
5.2 Evaluation Results 
5.2.1 FNE Results 
Number of sentences with right extractionAccuracy= Number of total sentences
787
We perform a 4:1 cross-validation on the FNE 
dataset using a commonly used classifier SVM-
light2 and gain a mean f-measure of 80.43%. 
Then, to assess the improvement by the FNE 
step and the classification of implicit and explicit 
sentences, we estimate the theoretic upper limit 
of the following three target extractions on the 
target dataset. Test 1 assumes every noun phras-
es or nouns in the sentence can be possible to be 
extracted as the target. So if there is one candi-
date matching the target, we can recognize the 
sentence as extractable. Test 2 adopts the anno-
tation results of the classification of explicit and 
implicit sentences. For the manually annotated 
implicit targets, we adapt the candidate to be FC. 
Then, as same as Test 1, all candidates are de-
termined whether to be the target. In Test 3, we 
follow the ruled-based classification of implicit 
and explicit sentences in our system and then 
judge the sentences whether extractable or not.  
 
 Proportion of extractable sentences 
Test 1 55.0% 
Test 2 69.6% 
Test 3 61.7% 
Table 6 Improvement of the proportion of ex-
tractable sentences by FNE classification and 
explicit/implicit sentence classification 
 
Table 6 shows the proportions of extractable 
sentences in the three tests. It is easy to see that 
the proportion of extractable sentences means 
the theoretic optimization of target extraction. So, 
by Test 2 we can see the extracted FC set is an 
effective complement of the candidate targets, 
while Test 3 demonstrates that the system still 
has much potential to improve the baseline after 
the rule-based classification of explicit and im-
plicit sentences.  
 
5.2.2 Target Extraction Results 
To demonstrate the effectiveness of our ap-
proach, we design two baselines.  
Baseline 1 treats all sentences as explicit type. 
In the method, we extract all noun phrases and 
pronouns in a sentence as candidates and obtain 
their ranks according to their grammatical roles.  
Baseline 2, a SVM-based approach, is offered 
to compare with the popular target extraction 
methods. In this method we regard the target 
                                                 
2 http://svmlight.joachims.org/ 
extraction as a classification problem. We ex-
tract the candidate noun phrases in a sentence 
first, and then use the semantic features to classi-
fy them as targets or not. The features mainly 
include: POS, whether or not a Named Entity, 
the positions in the sentence, the syntactic rela-
tions with the verb, and etc. As it is a supervised 
approach, the result is tested by a 2:1 cross vali-
dation. 
Then we use a method called FC-only (using 
only Focused Concepts) to improve Baseline 1 
by using the global information in news articles. 
For sentences of explicit type, we use the me-
thod in Baseline1. For sentences of implicit type, 
we take focused concepts in news articles as po-
tential targets, and choose the highest ranked 
element as the final target. 
Finally, our proposed approach CT (using 
Centering Theory) uses both Focused Concepts 
and Centering Theory. When the size of Wiki-
pedia concept vector is set to be 800, the com-
parison results of the four approaches are shown 
in Table 7: 
 
Accuracy
Baseline1 34.38% 
Baseline2(SVM-based) 35.13% 
FC-only 37.25% 
CT 43.20% 
Table 7 Comparison results 
FC-only is better than Baseline1, which de-
monstrates that the focused concepts are useful 
to provide information to implicit targets extrac-
tion. 444 implicit sentences are a large propor-
tion of the total corpus. And the focused con-
cepts do represent the global information and 
have influence on the target extraction. 
Centering Theory is naturally another im-
provement. It mainly takes advantage of the in-
formation of contexts within a comment, using a 
rule of coherence to decide the center of atten-
tion. And the result indicates that it is very help-
ful.  
Compared with the SVM-based approach, our 
approach is also much better. The SVM-based 
approach is only a little higher than Baseline 1. 
It seems that the manually annotated information 
is not very useful in target extraction in news 
comments. The reason may be that the target 
rules are complicated and exist not only in the 
current sentence. Using global and contextual 
788
information is a more economic and effective 
way to improve the result.  
In the Wikipedia-based ESA algorithm,there 
is a parameter of N, which is the vector size of 
the expanded vector. It is important to choose a 
proper parameter value to achieve a high accura-
cy and meanwhile keep a low computational 
complexity. The accuracy curves for FC and CT 
with different values of N are represented in 
Figure 2. Apparently, when N exceeds 600, the 
extraction performance almost does not change 
any more. So we finally take 800 as the value of 
N . 
5.3 Error Analysis  
Generally there are two major types of errors 
in the extraction results. One common error is 
that the target is not in our extracted candidate 
nouns or noun phrases. For example: 
??????????????????.? (It 
is a disaster of Chinese beverage that Coca Cola 
buys HuiYuan.) 
The sentence comments on the event of ?Coca 
Cola buys HuiYuan? but not a single concept 
?Coca Cola? or ?HuiYuan?. But our system can-
not recognize this type of targets properly. Also 
there are some cases that the noun phrases 
missed to be extracted by the LTP toolkit. It 
causes that the target is not matched by the can-
didates.  
Another error originates from the wrong clas-
sification of explicit and implicit sentences. For 
example, 
?????????????????.? (Re-
turning profits to civilians can get through the 
crisis of little companies.) 
In this sentence, ?????(Returning profits 
to civilians)? is the opinion target and the sen-
tence has a explicit target. But the rules based on 
the Chinese parser failed to recognize the phrase 
as a subject and thus the sentence is considered 
as implicit type by our approach. And lastly the 
target is extracted incorrectly. 
In 5.2.1, we test the theoretic upper limit of 
the target extraction and prove the potential ef-
fectiveness of two steps. The tests also can be 
used to estimate the proportion of the types of 
errors and analyze the bottleneck. In Test 2, 
there are 298 un-extractable sentences among 
the annotated explicit sentences. It shows that 
there is at least 18.6% loss in accuracy caused by 
the candidate recognition, which accounts for the 
first error type. As for the second error type, its 
proportion can be computed by the reduction 
from Test 2 to Test 3, which is 7.9%. 
6 Conclusion and Future Work  
In this paper, we propose a novel approach to 
extracting opinion targets in Chinese news 
comments. In order to solve the problem of im-
plicit target extraction, we extract focused con-
cepts and rank their importance by computing 
the semantic relatedness with sentences via Wi-
kipedia. In addition, we apply Centering Theory 
to the target extraction system, for utilizing con-
textual information. The experiment results 
demonstrate that our approach is effective.  
Currently, the result does not reach an abso-
lutely high accuracy. One bottleneck is that Chi-
nese parsing results are far from satisfactory. 
Actually this bottleneck has impacted the gener-
al target extraction long, such as the low perfor-
mances of all participants in the target extraction 
task of NTCIR7-MOAT-CS. We hope to im-
prove our results by avoid this disadvantage. 
Moreover, the phenomenon of implicit opinion 
targets exists not only in Chinese but also in 
English and other languages, while sometimes it 
is similar to zero anaphora. So the approach in 
this paper can be extended to news comments in 
other languages.  
Acknowledgement 
This work was supported by NSFC (60873155), 
Beijing Nova Program (2008B03), NCET 
(NCET-08-0006) and National High-tech R&D 
Program (2008AA01Z421). We thank the ano-
nymous reviewers for their useful comments. 
Figure 2: Accuracy vs. vector size N
789
References 
Bethard, Steven, Hong Yu, Ashley Thornton, Vasi-
leios Hatzivassiloglou, and Dan Jurafsky. 2004. 
Automatic Extraction of Opinion Propositions and 
their Holders. In Proceedings of AAAI Spring 
Symposium on Exploring Attitude and Affect in 
Text: Theories and Applications. 
Choi, Yejin, Claire Cardie, Ellen Riloff, and Sidd-
harth Patwardhan. 2005. Identifying Sources of 
Opinions with Conditional Random Fields and Ex-
traction Patterns. In Proceeding of HLT/EMNLP? 
05. 
Ding Xiaowen, Bing Liu, Philip S. Yu. 2008. A Ho-
listic Lexicon based Approach to Opinion Mining.  
Proceeding of the international conference on Web 
Search and Web Data Mining (WSDM?08), 231-
239. 
Du, Weifu. and Songbo Tan. 2009. An Iterative Rein-
forcement Approach for Fine-Grained Opinion 
Mining. The 2009 Annual Conference of the North 
American Chapter of the ACL 
Gabrilovich, Evgeniy. and Shaul Markovitch. 2007. 
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings 
of the 20th International Joint Conference on Ar-
tificial Intelligence (IJCAI). 
Ghani, Rayid, Katharina Probst, Yan Liu, Marko 
Krema, and Andrew Fano. 2006. Text Mining for 
Product Attribute Extraction. The Twelfth ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining. 
Grosz, Barbara J., Scott Winstein, and Aravind K. 
Joshi. (1995). Centering: A Framework for Model-
ing the Local Coherence of Discourse. In Compu-
tational Linguistics, 21(2). 
Hu, Minqing and Bing Liu. 2004. Mining Opinion 
Features in Customer Reviews. In Proceedings of 
Nineteenth National Conference on Artificial Intel-
ligence (AAAI-2004) 
Jin, Wei and Hung Hay Ho. 2009. A Novel Lexica-
lized HMM-based Learning Framework for Web 
Opinion Mining. In Proceedings of the 26th Inter-
national Conference on Machine Learning (ICML 
2009). 
Jin, Wei and Hung Hay Ho, Rohini K. Srihari. 2009. 
OpinionMiner: A Novel Machine Learning System 
for Web Opinion Mining and Extraction. In The 
15th ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining. 
Kim, Soo-Min. and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders, and Topics Expressed 
in Online News Media Text. In ACL Workshop on 
Sentiment and Subjectivity in Text. 
Kim, Soo-Min. and Eduard Hovy. 2005. Identifying 
Opinion Holders for Question Answering in Opi-
nion Texts. In Proceedings of AAAI-05 Workshop 
on Question Answering in Restricted Domains. 
Pang, Bo and Lillian Lee, and Vaithyanathan, S. 2002. 
Thumbs up? Sentiment classification using ma-
chine learning techniques. In EMNLP 2002. 
Popescu, Ana-Maria. and Oren Etzioni. 2005. Ex-
tracting Product Features and Opinions from Re-
views. In Proceeding of 2005 Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP?05), 339-346. 
Riloff, Ellen and Janyce Wiebe. 2003. Learning Ex-
traction Patterns for Subjective Expressions. Pro-
ceedings of the 2003 Conference on EMNLP.  
Ruppenhofer, Josef, Swapna Somasundaran, and Ja-
nyce Wiebe. 2008. Finding the Sources and Tar-
gets of Subjective Expressions. In LREC08. 
Seki, Yohei, David K. Evans, Lun-Wei Ku, Le Sun, 
Hsin-Hsi Chen, and Noriko Kando. 2008. Over-
view of Multilingual Opinion Analysis Task at 
NTCIR-7. The 7th NTCIR workshop (2007/2008). 
Su Qi, Xinying Xu, Honglei Guo, Zhili Guo, XianWu, 
Xiaoxun Zhang, Bin Swen and Zhong Su. 2008. 
Hidden Sentiment Association in Chinese WebO-
pinion Mining. In The 17th International World 
Wide Web Conference (WWW). 
Turney, Peter D. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings of the 40th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL). 
Zhang, Li, Yue Pan, and Tong Zhang. 2004. Focused 
Named Entity Recognition using Machine Learn-
ing. The 27th Annual International ACM SIGIR 
Conference. 
Zhuang, Li, Feng Jing. and Xiao-yan Zhu. 2006. 
Movie Review Mining and Summarization. In Pro-
ceedings of the 15th ACM International Confe-
rence on Information and Knowledge Management 
(CIKM?06), 43-50. 
790
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 736?746,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatically Determining a Proper Length for Multi-document
Summarization: A Bayesian Nonparametric Approach
Tengfei Ma and Hiroshi Nakagawa
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
{matf@r., nakagawa@}dl.itc.u-tokyo.ac.jp
Abstract
Document summarization is an important task
in the area of natural language processing,
which aims to extract the most important in-
formation from a single document or a clus-
ter of documents. In various summarization
tasks, the summary length is manually de-
fined. However, how to find the proper sum-
mary length is quite a problem; and keeping
all summaries restricted to the same length
is not always a good choice. It is obvi-
ously improper to generate summaries with
the same length for two clusters of docu-
ments which contain quite different quantity
of information. In this paper, we propose
a Bayesian nonparametric model for multi-
document summarization in order to automat-
ically determine the proper lengths of sum-
maries. Assuming that an original document
can be reconstructed from its summary, we
describe the ?reconstruction? by a Bayesian
framework which selects sentences to form
a good summary. Experimental results on
DUC2004 data sets and some expanded data
demonstrate the good quality of our sum-
maries and the rationality of the length deter-
mination.
1 Introduction
Text summarization is the process of generating a
short version of a given text to indicate its main top-
ics. As the number of documents on the web expo-
nentially increases, text summarization has attracted
increasing attention, because it can help people get
the most important information within a short time.
In most of the existing summarization systems,
people need to first define a constant length to re-
strict all the output summaries. However, in many
cases it is improper to require all summaries are of
the same length. Take the multi-document summa-
rization as an example, generating the summaries
of the same length for a 5-document cluster and a
50-document cluster is intuitively improper. More
specifically, consider two different clusters of doc-
uments: one cluster contains very similar articles
which all focus on the same event at the same time;
the other contains different steps of the event but
each step has its own topics. The former cluster may
need only one or two sentences to explain its infor-
mation, while the latter needs to include more.
Research on summary length dates back in the
late 90s. Goldstein et al (1999) studied the char-
acteristics of a good summary (single-document
summarization for news) and showed an empiri-
cal distribution of summary length over document
size. However, the length problem has been grad-
ually ignored later, since researchers need to fix
the length so as to estimate different summarization
models conveniently. A typical instance is the Doc-
ument Understanding Conferences (DUC)1, which
provide authoritative evaluation for summarization
systems. The DUC conferences collect news arit-
cles as the input data and define various summariza-
tion tasks, such as generic multi-document summa-
rization, query-focused summarization and update
summarization. In all the DUC tasks, the output is
restricted within a length. Then human-generated
1After 2007, the DUC tasks are incorporated into the Text
Analysis Conference (TAC).
736
summaries are provided to evaluate the results of dif-
ferent summarization systems. Limiting the length
of summaries contributed a lot to the development
of summarization techniques, but as we discussed
before, in many cases keeping the summaries of the
same size is not a good choice.
Moreover, even in constant-length summariza-
tion, how to define a proper size of summaries for
the summarization tasks is quite a problem. Why
does DUC2007 main task require 250 words while
Update task require 100 words? Is it reasonable?
A short summary may sacrifice the coverage, while
a long summary may cause redundance. Automati-
cally determining the best size of summaries accord-
ing to the input documents is valuable, and it may
deepen our understanding of summarization.
In this work, we aim to find the proper length
for document summarization automatically and gen-
erate varying-length summaries based on the doc-
ument itself. The varying-length summarization is
more robust for unbalanced clusters. It can also
provide a recommended size as the predefined sum-
mary length for general constant-length summariza-
tion systems. We advance a Bayesian nonparametric
model of extractive multi-document summarization
to achieve this goal. As far as we are concerned, it is
the first model that can learn appropriate lengths of
summaries.
Bayesian nonparametric (BNP) methods are pow-
erful tools to determine the size of latent vari-
ables (Gershman and Blei, 2011). They let the data
?speak for itself? and allow the dimension of la-
tent variables to grow with the data. In order to
integrate the BNP methods into document summa-
rization, we follow the assumption that the original
documents should be recovered from the reconstruc-
tion of summaries (Ma and Wan, 2010; He et al,
2012). We use the Beta process as a prior to gen-
erate binary vectors for selecting active sentences
that reconstruct the original documents. Then we
construct a Bayesian framework for summarization
and use the variational approximation for inference.
Experimental results on DUC2004 dataset demon-
strate the effectiveness of our model. Besides, we
reorganize the original documents to generate some
new datasets, and examine how the summary length
changes on the new data. The results prove that our
summary length determination is rational and neces-
sary on unbalanced data.
2 Related Work
2.1 Research on Summary Length
Summary length is an important aspect for gener-
ating and evaluating summaries. Early research on
summary length (Goldstein et al, 1999) focused on
discovering the properties of human-generated sum-
maries and analyzing the effect of compression ratio.
It demonstrated that an evaluation of summarization
systems must take into account both the compres-
sion ratios and the characteristics of the documents.
Radev and Fan (2000) compared the readability and
speedup in reading time of 10% summaries and 20%
summaries2 for topic sets with different number of
documents. Sweeney et al (2008) developed an in-
cremental summary containing additional sentences
that provide context. Kaisser et al (2008) studied
the impact of query types on summary length of
search results. Other than the content of original
documents, there are also some other factors affect-
ing summary length especially in specific applica-
tions. For example, Sweeney and Crestani (2006)
studied the relation between screen size and sum-
mary length on mobile platforms. The conclusion of
their work is the optimal summary size always falls
into the shorter one regardless of the screen size.
In sum, the previous works on summary length
mostly put their attention on the empirical study of
the phenomenon, factors and impacts of summary
length. None of them automatically find the best
length, which is our main task in this paper. Nev-
ertheless, they demonstrated the importance of sum-
mary length in summarization and the reasonability
of determining summary length based on content of
news documents (Goldstein et al, 1999) or search
results (Kaisser et al, 2008). As our model is mainly
applied for generic summarization of news articles,
we do not consider the factor of screen size in mo-
bile applications.
2.2 BNP Methods in Document Summarization
Bayesian nonparametric methods provide a
Bayesian framework for model selection and
adaptation using nonparametric models (Gershman
210% and 20% are the compression rates, and the documents
are from search results in information retrieval systems.
737
and Blei, 2011). A BNP model uses an infinite-
dimensional parameter space, but invokes only a
finite subset of the available parameters on any
given finite data set. This subset generally grows
with the data set. Thus BNP models address the
problem of choosing the number of mixture compo-
nents or latent factors. For example, the hierarchical
Dirichlet process (HDP) can be used to infer the
number of topics in topic models or the number of
states in the infinite Hidden Markov model (Teh et
al., 2006).
Recently, some BNP models are also involved in
document summarization approaches (Celikyilmaz
and Hakkani-Tu?r, 2010; Chang et al, 2011; Darling
and Song, 2011). BNP priors such as the nested Chi-
nese restaurant process (nCRP) are associated with
topic analysis in these models. Then the topic dis-
tributions are used to get the sentence scores and
rank sentences. BNP here only impacts the number
and the structure of the latent topics, but the sum-
marization framework is still constant-length. Our
BNP summarization model differs from the previous
models. Besides using the HDP for topic analysis,
our approach further integrates the beta process into
sentence selection. The BNP method in our model
are directly used to determine the number of sum-
mary sentences but not latent topics.
3 BNP Summarization
In this section, we first introduce the BNP priors
which will be used in our model. Then we propose
our model called BNP summarization.
3.1 The Beta Process and the Bernoulli process
The beta process(BP) (Thibaux and Jordan, 2007;
Paisley and Carin, 2009) and the related Indian buf-
fet process(IBP) (Griffiths and Ghahramani, 2005)
are widely applied to factor/feature analysis. By
defining the infinite dimensional priors, these factor
analysis models need not to specify the number of
latent factors but automatically determine it.
Definition of BP (Paisley et al, 2010): Let B
0
be
a continuous measure on a space ? and B
0
(?) = ?.
If Bk is defined as follows,
Bk =
N
?
k=1
?k??
k
,
?k ? Beta(
??
N
,?(1?
?
N
))
?k ?
1
?
B
0
(1)
(where ??
k
is the atom at the location ?k; and ? is a
positive scalar), then as N ? ?, Bk ? B and B is
a beta process: B ? BP (?B
0
).
Finite Approximation: The beta process is de-
fined on an infinite parameter space, but sometimes
we can also use its finite approximation by sim-
ply setting N to a large number (Paisley and Carin,
2009).
Bernoulli Process: The beta process is conju-
gate to a class of Bernoulli processes, denoted by
X ? Bep(B). If B is discrete, of the form in
(1), then X =
?
k bk??k where the bk are indepen-
dent Bernoulli variables with the probability p(bk =
1) = ?k. Due to the conjugation between the
beta process priors and Bernoulli process, the pos-
terior of B given M samples X
1
, X
2
, ...XM where
Xi ? Bep(B)fori = 1, , ,M. is also a beta process
which has updated parameters:
B|X
1
, X
2
, ..., XM
? BP (?+M, ??+MB0 +
1
c+M
?
iXi) (2)
Application of BP: Furthermore, marginalizing
over the beta process measure B and taking ? =
1, provides a predictive distribution on indicators
known as the Indian buffet process (IBP) (Thibaux
and Jordan, 2007). The beta process or the IBP is
often used in a feature analysis model to generate
infinite vectors of binary indicator variables(Paisley
and Carin, 2009), which indicates whether a feature
is used to represent a sample. In this paper, we use
the beta process as the prior to select sentences.
3.2 Framework of BNP Summarization
Most existing approaches for generic extractive
summarization are based on sentence ranking. How-
ever, these methods suffer from a severe problem
that they cannot make a good trade-off between
the coverage and minimum redundancy (He et al,
738
2012). Some global optimization algorithms are de-
veloped, instead of greedy search, to select the best
overall summaries (Nenkova and McKeown, 2012).
One approach to global optimization of summariza-
tion is to regard the summarization as a reconstruc-
tion process (Ma and Wan, 2010; He et al, 2012)
. Considering a good summary must catch most of
the important information in original documents, the
original documents are assumed able to be recov-
ered from summaries with some information loss.
Then the summarization problem is turned into find-
ing the sentences that cause the least reconstruction
error (or information loss). In this paper, we fol-
low the assumption and formulate summarization as
a Bayesian framework.
First we review the models of (Ma and Wan,
2010) and (He et al, 2012). Given a cluster of
M documents x
1
, x
2
, ..., xM and the sentence set
contained in the documents as S = [s
1
, s
2
, ..., sN ],
we denote all corresponding summary sentences as
V = [v
1
, ..., vn], where n is the number of summary
sentences and N is the number of all sentences in
the cluster. A document xi and a sentence vi or si
here are all represented by weighted term frequency
vectors in the space Rd, where d is the number of
total terms (words).
Following the reconstruction assumption, a can-
didate sentence vi can be approximated by the
linear combination of summary sentences: si 
?n
j=1 w
?
jvj , where w
?
j is the weight for summary
sentence vj . Thus the document can also be ap-
proximately represented by a linear combination of
summary sentences (because it is the sum of the sen-
tences).
xi 
n
?
j=1
wjvj . (3)
Then the work in (He et al, 2012) aims to find
the summary sentence set that can minimize the re-
construction error
?N
i=1 ||si ?
?n
j=1 w
?
jvj ||
2; while
the work in (Ma and Wan, 2010) defines the prob-
lem as finding the sentences that minimize the dis-
tortion between documents and its reconstruction
dis(xi,
?n
j=1 wjvj) where this distortion function
can also be a squared error function.
Now we consider the reconstruction for each doc-
ument, if we see the document xi as the dependent
variable, and the summary sentence set S as the
independent variable, the problem to minimize the
reconstruction error can be seen as a linear regres-
sion model. The model can be easily changed to a
Bayesian regression model by adding a zero-mean
Gaussian noise  (Bishop, 2006), as follows.
xi =
n
?
j=1
wjvj + i (4)
where the weights wj are also assigned a Gaussian
prior.
The next step is sentence selection. As our sys-
tem is an extractive summarization model, all the
summary sentences are from the original document
cluster. So we can use a binary vector zi =<
zi1, ..., ziN >
T to choose the active sentences V
(i.e. summary sentences) from the original sen-
tence set S. The Equation (4) is turned into xi =
?N
j=1 ?ij ?zijsj+i. Using a beta process as a prior
for the binary vector zi, we can automatically infer
the number of active component associated with zi.
As to the weights of the sentences, we use a random
vector ?i which has the multivariate normal distri-
bution because of the conjugacy. ?i ? RN is an
extension to the weights {w
1
, ...wn} in (4).
Integrating the linear reconstruction (4) and the
beta process3 (1), we get the complete process of
summary sentence selection as follows.
xi = S(?i ? zi) + i
S = [s
1
, s
2
, ..., sN ]
zij ? Bernoulli(?j)
?j ? Beta(
??
N
,?(1?
?
N
))
?i ? N (0, ?
2
?I)
i ? N (0, ?
2
 I) (5)
where N is the number of sentences in the whole
document cluster. The symbol ? represents the ele-
mentwise multiplication of two vectors.
One problem of the reconstruction model is that
the word vector representation of the sentences are
sparse, which dramatically increase the reconstruc-
tion error. So we bring in topic models to reduce the
3We use the finite approximation because the number of sen-
tences is large but finite
739
dimension of the data. We use a HDP-LDA (Teh et
al., 2006) to get topic distributions for each sentence,
and we represent the sentences and documents as
the topic weight vectors instead of word weight vec-
tors. Finally xi is a K-dimensional vector and S is
a K ?N matrix, where K is the number of topics in
topic models.
4 Variational Inference
In this section, we derive a variational Bayesian al-
gorithm for fast inference of our sentence selec-
tion model. Variational inference (Bishop, 2006)
is a framework for approximating the true posterior
with the best from a set of distributions Q : q? =
argminq?QKL(q(Z)|p(Z|X)). Suppose q(Z) can
be partitioned into disjoint groups denoted by Zj ,
and the q distribution factorizes with respect to these
groups: q(Z) =
?M
j=1 q(Zj). We can obtain a gen-
eral expression for the optimal solution q?j (Zj) given
by
ln q?j (Zj) = Ei =j [ln p(X,Z)] + const. (6)
where Ei =j [ln p(X,Z)] is the expectation of the log-
arithm of the joint probability of the data and latent
variables, taken over all variables not in the parti-
tion. We will therefore seek a consistent solution
by first initializing all of the factors qj(Zj) appro-
priately and then cycling through the factors and re-
placing each in turn with a revised estimate given by
(6) evaluated using the current estimates for all of
the other factors.
Update for Z
p(zij |?j , xi, S, ?i) ? p(xi|zij , sj , ?i)p(zij |?j)
We use q(zij) to approximate the posterior:
q(zij)
? exp{E[ln(p(xi|zij , z
?j
i , S, ?i)) + ln(p(zij |?))]}
? exp{E[ln(?j)]}?
exp{E[?
1
2?2
(
x
?j
i ? sjzij?ij
)T (
x
?j
i ? sjzij?ij
)
]}
? exp{ln(?j)}?
exp{?
(
?2ij ? z
2
ij ? s
T
j sj ? 2?ij ? zij ? sj
T
? x
?j
i
)
2?2
}
(7)
where x?ji = xi ? S
?j(?
?j
i ? z
?j
i ), and the symbol
? indicates the expectation value. The ?2ij can be
extended to this form:
?2ij = ?ij
2
+?
j
i (8)
where ?ji means the j
th diagonal element of ?i
which is defined by Equation 13.
As zi is a binary vector, we only calculate the
probability of zij = 1 and zij = 0.
q(zij = 1) ? exp{ln(?j)} ?
exp{?
1
2?2
(
?2ij ? s
T
j sj ? 2?ij ? sj
T
? x
?j
i
)
}
q(zij = 0) ? exp{ln(1? ?j)} (9)
The expectations can be calculated as
ln(?j) = ?(
??
N
+ nj)? ?(?+M) (10)
ln(1? ?j) = ?(?(1?
?
N
)+M ?nj)??(?+M)
(11)
where nj =
?M
i=1 zij .
Update for ?
p(?j |Z) ? p(?j |?, ?,N)p(Z|?j)
Because of the conjugacy of the beta to Bernoulli
distribution, the posterior of ? is still a beta distribu-
tion:
?j ? Beta(
??
N
+ nj , ?(1?
?
N
) +M ? nj) (12)
Update for ?
p(?i|xi, Z, S) ? p(xi|?i, Z, S)p(?i|?
2
?)
The posterior is also a normal distribution with mean
?i and covariance ?i.
?i =
(
1
?2
S?i
T
S?i +
1
?2?
I
)
?1
(13)
?i = ?i
(
1
?2
S?i
T
xi
)
(14)
Here S?i ? S ? z?i and z?i ? [zi, ..., zi]T is a K ? N
matrix with the vector zi repeated K(the number of
the latent topics) times.
S?i = S ? z?i (15)
740
S?i
T
S?i = (S
TS) ? (zi ? zi
T +Bcovi) (16)
Bcovi = diag[zi1(1? zi1), ..., ziN (1? ziN )] (17)
Update for ?2
p(?2 |?, X, Z, S) ? p(X|?, Z, S, ?
2
 )p(?
2
 )
By using a conjugate prior, inverse gamma prior
InvGamma(u, v), the posterior can be calculated
as a new inverse gamma distribution with parame-
ters
u? = u+MK/2
v? = v +
1
2
M
?
i=1
(||xi ? S(zi ? ?i)||+ ?i)
(18)
where
?i =
?N
j=1(z
2
ij ? ?
2
ij ? s
T
j sj ? zij
2
? ?ij
2
? sTj sj)
+
?
j =l zij ? zil ??i,jl ? s
T
j sl
Update for ?2?
p(?2?|?) ? p(?|?
2
?)p(?
2
?)
By using a conjugate prior, inverse gamma prior
InvGamma(e, f), the posterior can be calculated
as a new inverse gamma distribution with parame-
ters
e? = e+MN/2
f ? = f +
1
2
M
?
i=1
(
(?)T?+ trace(??i)
)
(19)
5 Experiments
To test the capability of our BNP summarization sys-
tems, we design a series of experiments. The aim of
the experiments mainly includes three aspects:
1. To demonstrate the summaries extracted by our
model have good qualities and the summary
length determined by the model is reasonable.
2. To give examples where varying summary
length is necessary.
3. To observe the distribution of summary length.
We evaluate the performance on the dataset of
DUC2004 task2. The data contains 50 document
clusters, with 10 news articles in each cluster. Be-
sides, we construct three new datasets from the
DUC2004 dataset to further prove the advantage of
variable-length summarization. We separate each
cluster in the original dataset into two parts where
each has 5 documents, hence getting the Separate
Dataset; Then we randomly combine two origi-
nal clusters in the DUC2004 dataset, and get two
datasets called Combined1 and Combined2. Thus
each of the clusters in the combined datasets include
20 documents with two different themes.
5.1 Evaluation of Summary Qualities
First, we implement our BNP summarization model
on the DUC2004 dataset, with summary length not
limited. At the topic analysis step, we use the HDP
model and follow the inference in (Teh et al, 2006).
For the sentence selection step, we use the varia-
tional inference described in Section 4, where the
parameters in the beta process (5) are set as ? =
1, ? = 1. The summaries that we finally generate
have an average length of 164 words. We design sev-
eral popular unsupervised summarization systems
and compare them with our model.
? The Random model selects sentences randomly
for each document cluster.
? The MMR (Carbonell and Goldstein, 1998)
strives to reduce redundancy while maintaining
relevance. For generic summarization, we re-
place the query relevance with the relevance to
documents.
? The Lexrank model (Erkan and Radev, 2004) is
a graph-based method which choose sentences
based on the concept of eigenvector centrality.
? The Linear Representation model (Ma and
Wan, 2010) has the same assumption as ours
and it can be seen as an approximation of the
constant-length version of our model.
741











	


	



    
	


