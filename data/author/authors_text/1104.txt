Multi-Human Dialogue Understanding for Assisting
Artifact-Producing Meetings
John Niekrasz and Alexander Gruenstein and Lawrence Cavedon
Center for the Study of Language and Information (CSLI)
Stanford University
Cordura Hall, Stanford, CA, 94305-4115, USA
http://www-csli.stanford.edu/semlab/
{niekrasz, alexgru, lcavedon}@csli.stanford.edu
Abstract
In this paper we present the dialogue-
understanding components of an architec-
ture for assisting multi-human conversa-
tions in artifact-producing meetings: meet-
ings in which tangible products such as
project planning charts are created. Novel
aspects of our system include multimodal
ambiguity resolution, modular ontology-
driven artifact manipulation, and a meeting
browser for use during and after meetings.
We describe the software architecture and
demonstrate the system using an example
multimodal dialogue.
1 Introduction
Recently, much attention has been focused on
the domain of multi-person meeting under-
standing. Meeting dialogue presents a wide
range of challenges including continuous multi-
speaker automatic speech recognition (ASR),
2D whiteboard gesture and handwriting recog-
nition, 3D body and eye tracking, and multi-
modal multi-human dialogue management and
understanding. A significant amount of re-
search has gone toward understanding the prob-
lems facing the collection, organization, and
visualization of meeting data (Moore, 2002;
Waibel et al, 2001), and meeting corpora like
the ICSI Meeting Corpus (Janin et al, 2003) are
being made available. Continuing research in
the multimodal meeting domain has since blos-
somed, including ongoing work from projects
such as AMI1 and M42, and efforts from sev-
eral institutions.
Previous work on automatic meeting un-
derstanding has mostly focused on surface-
level recognition, such as speech segmentation,
for obvious reasons: understanding free multi-
human speech at any level is an extremely diffi-
cult problem for which best performance is cur-
rently poor. In addition, the primary focus for
1http://www.amiproject.org/
2http://www.m4project.org/
applications has been on off-line tools such as
post-meeting multimodal information browsing.
In parallel to such efforts we are applying
dialogue-management techniques to attempt to
understand and monitor meeting dialogues as
they occur, and to supplement multimodal
meeting records with information relating to the
structure and purpose of the meeting.
Our efforts are focused on assisting artifact-
producing meetings, i.e. meetings for which the
intended outcome is a tangible product such as
a project management plan or a budget. The
dialogue-understanding system helps to create
and manipulate the artifact, delivering a final
product at the end of the meeting, while the
state of the artifact is used as part of the dia-
logue context under which interpretation of fu-
ture utterances is performed, serving a num-
ber of useful roles in the dialogue-understanding
process:
? The dialogue manager employs generic di-
alogue moves with plugin points to be de-
fined by specific artifact types, e.g. project
plan, budget;
? The artifact state helps resolve ambiguity
by providing evidence for multimodal fu-
sion and constraining topic-recognition;
? The artifact type can be used to bias ASR
language-models;
? The constructed artifact provides a inter-
face for a meeting browser that supports
directed queries about discussion that took
place in the meeting, e.g. ?Why did we
decide on that date??
In addition, we focus our attention on the
handling of ambiguities produced on many
levels, including those produced during au-
tomatic speech recognition, multimodal com-
munication, and artifact manipulation. The
present dialogue manager uses several tech-
niques to do this, including the maintenance of
Multimodal
Integrator
3-D
Gesture
Recognizer
2-D
Drawing
Recognizer
ASR
CIA
NL
Parser
Information State
Ontology
KB DMT,
Active Node,
Salience List,
etc.
Dialogue
Manager
Meeting Browser
Hypothesis
Repository (CMU)
(OGI)
(OGI)
(MIT)
Figure 1: The meeting assistant architecture,
highlighting the dialogue-management compo-
nents.
multiple dialogue-move hypotheses, fusion with
multimodal gestures, and the incorporation of
artifact-specific plug-ins.
The software architecture we use for manag-
ing multi-human dialogue is an enhancement of
a dialogue-management toolkit previously used
at CSLI in a range of applications, including
command-and-control of autonomous systems
(Lemon et al, 2002) and intelligent tutoring
(Clark et al, 2002). In this paper, we detail the
dialogue-management components (Section 3),
which support a larger project involving mul-
tiple collaborating institutions (Section 2) to
build a multimodal meeting-understanding sys-
tem capable of integrating speech, drawing and
writing on a whiteboard, and physical gesture
recognition.
We also describe our toolkit for on-line and
off-line meeting browsing (Section 4), which al-
lows a meeting participant, observer, or devel-
oper to visually and interactively answer ques-
tions about the history of a meeting, the pro-
cesses performed to understand it, and the
causal relationships between dialogue and ar-
tifact manipulation.
2 Meeting Assistant Architecture
The complete meeting assistant architecture is
a highly collaborative effort from several insti-
tutions. Its overall architecture, focusing on our
contributions to the system is illustrated in Fig-
ure 1.
The components for drawing and writing
recognition and multimodal integration (Kaiser
et al, 2003) were developed at The Oregon
Graduate Institute (OGI) Center for Human-
Computer Communication3; the component for
physical gesture recognition (Ko et al, 2003)
was developed at The Massachusetts Institute
of Technology (MIT) AI Lab4. Integration be-
tween all components was performed by project
members at those sites and at SRI Interna-
tional5, and integration between our CSLI Con-
versational Intelligence Architecture and OGI?s
Multimodal Integrator (MI) was performed by
members of both teams. ASR is done using
CMU Sphinx6, from which the n-best list of re-
sults are passed to SRI?s Gemini parser (Dowd-
ing et al, 1993). Gemini incorporates a suite
of techniques for handling noisy input, includ-
ing fragment detection, and its dynamic gram-
mar capabilities are used to register new lexical
items, such as names of tasks that may be out-
of-grammar.
An example of a multimodal meeting conver-
sation that the meeting assistant currently sup-
ports can be found in Figure 2.7 There are two
meeting participants in a conference room with
an electronic whiteboard which can record their
pen strokes and a video camera that tracks their
body movements; A is standing at the white-
board and drawing while B is sitting at the
table. A gloss of how the system behaves in
response to each utterance and gesture follows
each utterance; these glosses will be explained
in greater detail throughout the rest of the pa-
per. The drawing made on the whiteboard is
in Figure 3(a), and the chart artifact as it was
constructed by the system is displayed in Figure
3(b).
3 Conversational Intelligence
Architecture
To meet the challenges presented by multi-
person meeting dialogue, we have extended
and enhanced our previously used Conversa-
tional Intelligence Architecture (CIA). The CIA
is a modular and highly configurable multi-
application system: a separation is made be-
tween generic dialogue processes and those spe-
cific to a particular domain. Creating a new
application may involve writing new dialogue
moves and configuring the CIA to use these. We
3http://www.cse.ogi.edu/CHCC/
4http://www.ai.mit.edu/
5http://www.sri.com/
6http://www.speech.cs.cmu.edu/sphinx/
7A video demonstration will be available soon at
http://www-csli.stanford.edu/semlab/calo/
A: So, lets uh figure out what uh needs uh needs to be done. Let?s
look at the schedule. [draws a chart axes] utterance and gesture
information fused, a new milestone chart artifact is created
B: So, if all goes well, we?ve got funding for five years. system sets
unit on axis to ?years?
A: Yeah. Let?s see one, two ... [draws five tick marks on the x-axis]
system assumes tick marks are years
B: Well, the way I see it, uh we?ve got three tasks. dialogue man-
ager hypothesizes three tasks should be added, waits for multimodal
confirmation
A: Yeah right [draws three task lines horizontally on the axis] multi-
modal confirmation is given, information about task start and end
dates is fused from the drawing
A: Let?s call this task line demo [touches the top line with the
pen], call this task line signoff [touches the middle line with the
pen], and call this task line system [touches the bottom line with
the pen]. each utterance causes the dialogue manager to hypoth-
esize three distinct hypotheses, in each task a different hypothesis
is named, the gestures disambiguate these in the multimodal inte-
grator
B: So we have two demos to get done.
A: uh huh
B: Darpatech is at the end of month fifteen [A draws a diamond
at month fifteen on the demo task line] dialogue manager hy-
pothesizes a milestone called ?darpatech? at month fifteen; gesture
confirms this and pinpoints appropriate task line
B: And the final demonstrations are at the end of year five [A draws
a diamond at year five on the demo task line] same processing as
previous
A: Hmm, so when do the signoffs need to happen do you think?
dialogue manager expects next utterance to be an answer
B: Six months before the demos [A draws two diamonds on the sig-
noff task line, each one about 6 months before the demo mile-
stones drawn above] answer arrives; dialogue manager hypothe-
sizes two new milestones which are confirmed by gesture
A: And we?ll need the systems by then too [A draws two diamonds
on the system task line] dialogue manager hypothesizes two more
milestones, confirmed by gesture
B: That?s a bit aggressive I think. Let?s move the system milestone
back six months. [B points finger at rightmost system milestone.
A crosses it out and draws another one six months earlier] di-
alogue manager hypothesizes a move of the milestone, 3D gesture
and drawing confirm this
Figure 2: Example conversation understood by
the system.
(a) The whiteboard in-
put captured by OGI?s
Charter gesture recog-
nizer
(b) The artifact as
maintained in the
dialogue system
Figure 3: Ink-captured vs ?idealized? artifact
output.
have successfully used this ?toolkit? approach
in our previous applications at CSLI to inter-
face novel devices without modifying the core
dialogue manager.
The present application is however very dif-
ferent to our previous applications, and those
commonly encountered in the literature, which
typically involve a single human user interact-
ing with a dialogue-enabled artificial agent. In
the meeting environment, the dialogue manager
should at most very rarely interpose itself into
the discussion?to do so would be disruptive
to the interaction between the human partic-
ipants. This requirement prohibits ambiguity
and uncertainty from being resolved with, say,
a clarification question, which is the usual strat-
egy in conversational interfaces. Instead, uncer-
tainty must be maintained in the system until
it can be resolved by leveraging context, using
evidence from another modality, or by a future
utterance.
The meeting-understanding domain has thus
prompted several extensions to our existing
CIA, many of which we expect will be applied
in other conversational domains. These include:
? Support for handling multiple competing
speech parses; (Section 3.2)
? A generic artifact ontology which enables
designing generically useful artifact-savvy
dialogue applications; (Section 3.3)
? Support for the generation and subsequent
confirmation of dialogue-move hypotheses
in a multimodal integration framework;
(Section 3.4)
? The acceptance of non-verbal unimodal
gestures into the dialogue-move repertoire.
(Section 3.5)
? A preliminary mechanism for supporting
uncertainty across multiple conversational
moves; (Section 3.6)
Before discussing these new features in detail,
the following section introduces the CIA and
its persisting core dialogue-management com-
ponents.
3.1 Core Components: Information
State and Context
The core dialogue management components of
the CIA maintain dialogue context using the
information-state and dialogue-move approach
(Larsson and Traum, 2000) where each con-
tributed utterance modifies the current context,
or information state, of the dialogue. Each new
utterance is then interpreted within the current
context (see (Lemon et al, 2002) for a detailed
description).
A number of data structures are employed
in this process. The central dialogue state-
maintaining structure is the Dialogue Move Tree
(DMT). The DMT represents the historical con-
text of a dialogue. An incoming utterance, clas-
sified by dialogue move, is interpreted in con-
text by attaching itself to an appropriate active
node on the DMT; e.g., an answer attaches to
an active corresponding question node. Cur-
rently, active nodes are kept on an Active Node
List , which is ordered so that those most likely
to be relevant to the current conversation are
at the front of the list. Incoming utterances
are displayed to each node in turn, and at-
tach to the first appropriate node (determined
by information-state-update functions). Other
structures include the context-specific Salience
List , which maintains recently used terms for
performing anaphora resolution, and a Knowl-
edge Base containing application specific infor-
mation, which may be leveraged to interpret in-
coming utterances.8
We now present the various enhancements
made to the CIA for use in the meeting domain.
3.2 ASR and Robust Parsing
The first step in understanding any dialogue is
recognizing and interpreting spoken utterances.
In the meeting domain, we are presented with
the particularly difficult task of doing this for
spontaneous human-human speech. We there-
fore chose to perform ASR using a statisti-
cal language model (LM) and employ CMU?s
Sphinx to generate an n-best list of recogni-
tion results. The recognition engine uses a tri-
gram LM trained on the complete set of pos-
sible utterances expected given a small hand-
crafted scenario like that in the example dia-
logue. Despite the task?s limited domain, the re-
alized speech is very disfluent, generating an ex-
tremely broad range of possible utterances that
the system must handle. The resulting n-best
list is therefore often extremely varied.
To handle the ASR results of disfluent utter-
ances, we employ SRI?s Gemini robust language
parser (Dowding et al, 1993). In particular,
we use Gemini to retrieve the longest strings
of valid S and NP fragments in each ASR re-
sult. Currently, we reject all but the parsed S
fragments?and NP fragments when expected
8Command-and-control applications have also made
use of an Activity Tree, which represents activities being
carried out by the dialogue-enabled device (Gruenstein,
2002); however, this application currently makes no use
of this.
by the system (e.g. an answer to a question
containing an NP gap). The parser uses generic
syntactic rules, but is constrained semantically
by sorts specific to the domain. In Section 3.4,
we describe how the dialogue manager handles
the multiple parses for a single utterance and
how it uses the uncertainty they represent.
3.3 Artifact Knowledge Base and
Ontology
In the present version of the CIA, all static do-
main knowledge about meeting artifacts is de-
fined in a modularized class-based ontology. In
conjunction with the ontology, we also maintain
a dynamic knowledge base (KB) which holds
the current state of any artifacts. This is stored
as a collection of instances of the ontological
classes, and both components are maintained
together using the Prote?ge?-20009 ontology and
knowledge-base toolkit (Grosso et al, 1999).
The principal base classes in the artifact on-
tology are designed to be both architecturally
elegant and intuitive. To this end, we charac-
terize the world of artifacts as being made up
of three essential classes: entities which repre-
sent the tangible objects themselves, relations
which represent how the entities relate to one
another, and events which change the state of
entities or relations. Events are the most im-
portant tool aiding the dialogue management
algorithm. They comprehensively characterize
the set of actions which can change the current
state of an artifact. They may be classified into
three categories: insert changes which insert a
new entity or relation instance into the KB, re-
move changes which remove an instance, and
value changes which modify the value of a slot
in an instance. All changes to the KB can be
characterized as one of these three atomic events
or a combination of them.
3.4 Hypothesizers: A plugin
architecture for artifact-driven
multimodal integration
Abmiguities and uncertainties are both ram-
pant in multimodal meeting dialogues, and in
artifact-producing meetings, the majority per-
tain to artifacts and the utterances performed
to change them. In this section we explain how
the CIA?s dialogue manager uses the artifact on-
tology, and the repertoire of event classes in it,
to formulate sets of artifact-changing dialogue-
move hypotheses from single utterances. We
9http://protege.stanford.edu/
also demonstrate how it uses the current state
of the artifact in the KB to constrain the in-
terpretation of utterances in context, and how
multimodal gestures help to resolve ambiguous
interpretations.
To begin, each dialogue-move hypothesis con-
sists of the following elements: (1) the DMT
node associated with this hypothesis, (2) the
parse that gave rise to the hypothesis, (3) the
probability of the hypothesis, (4) an isUnimodal
flag indicating whether or not the dialogue move
requires confirmation from other modalities, (5)
a list of artifact-change events to be made to
the KB, and (6) the information state update
function to be invoked if this hypothesis is con-
firmed by the multimodal integrator. Each of
these elements participate in the generation and
confirmation process as detailed below.
First, consider the utterance Darpatech is at
the end of month fifteen. from the example dia-
logue. This utterance is much more likely to
indicate the creation of a new milestone if a
task line is pertinent to the current dialogue
context, e.g. the user has just created a new
task line. In our system, the ambiguous or un-
certain utterance, the current dialogue context,
and the current state of the chart is delegated to
artifact-type specific components called hypoth-
esizers. Hypothesizers take the above as input,
and using the set of events available to its cor-
responding artifact in the ontology, they pro-
grammatically generate a list of dialogue-move
hypotheses appropriate in the given context?
or they can return the empty list to indicate
that there is no reasonable interpretation of the
utterance given the current context.
Hypothesizers work directly with the DMT
architecture: as an incoming utterance is se-
quentially presented to each active node in the
DMT, the dialogue context and the proposed
active node are passed into a hypothesizer cor-
responding to the particular artifact associated
with that node. If the hypothesizer can create
one or more valid hypotheses, then the utter-
ance is attached to the DMT as a child of that
active node.10
In a multimodal domain, some hypotheses re-
quire confirmation in other modalities before
the dialogue manager can confidently update
10There are, in fact, other rules as well which allow for
attachment. For example, questions?which don?t im-
mediately generate hypotheses?can also be attached to
various nodes depending on the dialogue context. While
the emphasis here is on hypothesizers, these are just one
part of the dialogue processing toolkit
the information state. In this particular system,
in fact, the dialogue manager does not directly
update the KB?s current artifact state; rather, it
hypothesizes a set of dialogue-move hypotheses
and assigns each a confidence derived from ASR
confidence, the fragmentedness of the parse, and
confidence in the proposed attachment to a con-
versational thread. Each conversational move is
then provided a Hypothesis Repository for stor-
ing the hypotheses associated with it. When
dialogue processing is completed for a partic-
ular conversational move, i.e. when all pos-
sible attachments of all possible parses on the
n-best list have been made, the set of hypothe-
ses is sent to the Multimodal Integrator (MI)
for potential fusion with gesture. Depending on
the information from other modalities, the MI
confirms or rejects the hypotheses?moreover, a
confirmed hypothesis might be augmented with
information provided by other modalities. Such
an augmentation occurs for the utterance We
have three tasks from the example dialogue. In
this situation, the dialogue manager hypothe-
sizes that the user may be creating three new
task lines on the chart. When the user actually
draws the three task lines, the MI infers the
start and stop date based on where the lines
start and stop on the axis. In this case, it not
only confirms the dialogue manager?s hypoth-
esis, but augments it to reflect the additional
date information yielded from the whiteboard
input.
3.5 Unimodal Gestures
In addition to the Information State updates
based on both speech and gesture, multimodal
meeting dialogue can often include gestures in
which a participant makes a change to an ar-
tifact using a unimodal gesture not associated
with an utterance. For example, a user may
draw a diamond on a task line but say nothing.
Even in the absence of speech, this can be unam-
biguously understood as the creation of a mile-
stone at a particular point on the line. These
unimodally produced changes to the chart must
be noted by the dialogue manager, as they are
potential targets for later conversation. To ac-
commodate this, we introduce a new DMT node
of type Unimodal Gesture, thus implicitly in-
cluding gesture as a communicative act that can
stand on its own in a conversation
3.6 Uncertain DMT Node Attachment
Since hypotheses are not always immediately
confirmed, uncertainty must be maintained
Figure 4: A snapshot from the meeting browser.
across multiple dialogue moves. The system ac-
complishes this by extending the CIA to main-
tain multiple competing Information States. In
particular, the DMT has been extended to al-
low for the same parse to attach in multiple
locations?these multiple attachments are even-
tually pruned as more evidence is accumulated
in the form of further speech or gestures?that
is, as hypotheses are confirmed or rejected over
time.
4 Meeting Viewer Toolkit
Throughout an artifact-producing meeting, the
dialogue system processes a complex chronolog-
ical sequence of events and information states
that form structures rich in information useful
to dialogue researchers and the dialogue partic-
ipants themselves. To harness the power of this
information, we have constructed a toolkit for
visualizing and investigating the meeting infor-
mation state and its history.
Central to the toolkit is our meeting history
browser, which can be seen in Figure 4, dis-
playing a portion of the example dialogue, with
the results of a search for ?demo? highlighted.
This record of the meeting is available both dur-
ing the meeting and afterwards to assist users
in answering questions they might have about
the meeting. Many kinds of questions can be
answered in the browser, like those a manager
might ask the day after a meeting: ?Why did we
move the deadline on that task 6 months later??,
?Did I approve setting that deadline so early??,
and ?What were we thinking when we put that
milestone at month fifteen??. A meeting partic-
ipant might have questions as the meeting oc-
curs, like ?What did the chart look like 5 min-
utes ago??, ?What did we say to make the sys-
tem move that milestone??, and ?What did Mr.
Smith say at the beginning of the meeting??.
To help answer these questions, the browser
performs many of the functions found in current
multimodal meeting browsers. For example,
it provides concise display of a meeting tran-
scription, advanced searching capabilities, sav-
ing and loading of meeting sessions, and person-
alization of its own display characteristics. As
a novel addition to these basic behaviors, the
browser is also designed to display artifacts and
the causal relationships between artifacts and
the utterances that cause them to change.
To effectively convey this information, the
record of components monitored by the history
toolkit is presented to the user through a win-
dow which chronologically displays the visual
embodiment of those components. Recognized
utterances are shown as text, parses are shown
as grouped string fragments, and artifacts and
their sub-components are shown in their pro-
totypical graphical form. The window orga-
nizes these visual representations of the meet-
ing?s events and states into chronological tracks,
each of which monitors a unified conceptual part
of the meeting. The user is then able to link the
elements causally.
Beyond the history browser, the toolkit also
displays the current state of all artifacts in an
artifact-state window (e.g. Figure 3(b)). In the
window, the user not only confirms the state of
the artifact but can also gain insight into the
currently interpreted dialogue context by mon-
itoring how the artifact is highlighted. In the
figure, the third task is highlighted because it is
the most recently talked-about task. A meeting
participant can therefore see that subsequent
anaphoric references to an unknown task will
be resolved to the third one.
Another GUI component of the toolkit is
a small hypothesis window which shows the
current set of unresolved artifact-changing hy-
potheses. It does this by displaying an artifact
for each hypothesis, reflecting the artifact?s fu-
ture state given confirmation of the hypothe-
sis. The hypothesis? probability and associated
parse is displayed under the artifact. The user
may even directly click a hypothesis to confirm
it. The hypothesized future states are however
not displayed in the artifact-state window or
artifact-history browser, which show only the
results of confirmed actions.
In addition to being a GUI front-end, the
toolkit maintains a fully generic architecture for
recording the history of any object in the sys-
tem software. These objects can be anything
from the utterances of a participant, to the state
history of an artifact component, or the record
of hypotheses formulated by the dialogue man-
ager. This generic functionality provides the
toolkit the ability to answer a wide variety of
questions for the user about absolutely any as-
pect of the dialogue context history.
5 Future Work
Work is currently proceeding in a number of
directions. Firstly, we plan to incorporate fur-
ther techniques for robust language understand-
ing, including word-spotting and other topic-
recognition techniques, within the context of
the constructed artifact. We also plan to in-
vestigate using the current state of the artifact
to further bias the ASR language model. We
also plan on generalizing the uncertainty man-
agement within the dialogue manager, allowing
multiple competing hypotheses to be supported
over multiple dialogue moves. Topic and other
ambiguity management techniques will be used
to statistically filter and bias hypotheses, based
on artifact state.
We are currently expanding the meeting
browser to categorize utterances by dialogue
act, and to recognize and categorize aggrega-
tions as multi-move strategies, such as negoti-
ations. This will allow at-a-glance detection of
where disagreements took place, and where is-
sues may have been left unresolved. A longer-
term aim of the project is to provide further
support to the participants in the meeting, e.g.
by detecting opportunities to provide useful in-
formation (e.g. schedules, when discussing who
to allocate to a task; documents pertinent to a
topic under discussion) to meeting participants
automatically. Evaluation criteria are currently
being designed that include both standard mea-
sures, such as word error rate, and measures in-
volving recognition of meeting-level phenomena,
such as detecting agreement on action-items.
Evaluation will be performed using both corpus-
based approaches (e.g. for evaluating recog-
nition of meeting phenomena) and real (con-
trolled) meetings with human subjects.
6 Acknowledgements
We would like to gratefully acknowledge Phil
Cohen?s group at OGI, especially Ed Kaiser,
Xiaoguang Li, and Matt Wesson, and David
Demirdjian at MIT. This work was funded by
DARPA grant NBCH-D-03-0010(1).
References
B. Clark, E. Owen Bratt, O. Lemon, S. Pe-
ters, H. Pon-Barry, Z. Thomsen-Gray, and
P. Treeratpituk. 2002. A general purpose ar-
chitecture for intelligent tutoring systems. In
International CLASS Workshop on Natural,
Intelligent and Effective Interaction in Multi-
modal Dialogue Systems.
J. Dowding, J.M. Gawron, D. Appelt, J. Bear,
L. Cherny, R. Moore, and D. Moran. 1993.
Gemini: a natural language system for
spoken-language understanding. In Proc.
ACL 93.
W. E. Grosso, H. Eriksson, R. W. Fergerson,
J. H. Gennari, S. W. Tu, and M. A. Musen.
1999. Knowledge modeling at the millen-
nium: (the design and evolution of Prote?ge?-
2000). In Proc. KAW 99.
A. Gruenstein. 2002. Conversational interfaces:
A domain-independent architecture for task-
oriented dialogues. Master?s thesis, Stanford
University.
A. Janin, D. Baron, J. Edwards, D. Ellis,
D. Gelbart, N. Morgan, B. Peskin, T. Pfau,
E. Shriberg, A. Stolcke, and C. Wooters.
2003. The ICSI meeting corpus. In Proc.
ICASSP 2003.
E. Kaiser, A. Olwal, D. McGee, H. Benko,
A. Corradini, X. Li, P. Cohen, and S. Feiner.
2003. Mutual disambiguation of 3D multi-
modal interaction in augmented and virtual
reality. In Proc. ICMI 2003.
T. Ko, D. Demirdjian, and T. Darrell. 2003.
Untethered gesture acquisition and recogni-
tion for a multimodal conversational system.
In Proc. ICMI 2003.
S. Larsson and D. Traum. 2000. Informa-
tion state and dialogue management in the
TRINDI dialogue move engine toolkit. Natu-
ral Language Engineering, 6.
O. Lemon, A. Gruenstein, and S. Peters. 2002.
Collaborative activities and multi-tasking in
dialogue systems. Traitment Automatique
des Langues, 43(2).
D. Moore. 2002. The IDIAP smart meeting
room. Technical Report IDIAP Communica-
tion 02-07.
A. Waibel, M. Bett, F. Metze, K. Ries,
T. Schaaf, T. Schultz, H. Soltau, H. Yu, and
K. Zechner. 2001. Advances in automatic
meeting record creation and access. In Proc.
ICASSP 2001.
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 24?25,
Vancouver, October 2005.
A Flexible Conversational Dialog System for MP3 Player
Fuliang Weng
1
 Lawrence Cavedon
2
 Badri Raghunathan
1
 Danilo Mirkovic
2 
Ben Bei
1
Heather Pon-Barry
1
 Harry Bratt
3
 Hua Cheng
2
 Hauke Schmidt
1
 Rohit Mishra
4
 Brian Lathrop
4
Qi Zhang
1
   Tobias Scheideck
1
   Kui Xu
1
    Tess Hand-Bender
1
   Sandra Upson
1
     Stanley Peters
2
Liz Shriberg
3
 Carsten Bergmann
4
Research and Technology Center, Robert Bosch Corp., Palo Alto, California
1
Center for Study of Language and Information, Stanford University, Stanford, California
2
Speech Technology and Research Lab, SRI International, Menlo Park, California
3
Electronics Research Lab, Volkswagen of America, Palo Alto, California
4
{Fuliang.weng,badri.raghunathan,hauke.Schmidt}@rtc.bosch.com
{lcavedon,huac,peters}@csli.Stanford.edu
{harry,ees}@speech.sri.com
{rohit.mishra,carsten.bergmann}@vw.com
1 Abstract
In recent years, an increasing number of new de-
vices have found their way into the cars we drive.
Speech-operated devices in particular provide a
great service to drivers by minimizing distraction,
so that they can keep their hands on the wheel and
their eyes on the road. This presentation will dem-
onstrate our latest development of an in-car dialog
system for an MP3 player designed under a joint
research effort from Bosch RTC, VW ERL, Stan-
ford CSLI, and SRI STAR Lab funded by NIST
ATP [Weng et al2004] with this goal in mind.
This project has developed a number of new tech-
nologies, some of which are already incorporated
in the system.  These include: end-pointing with
prosodic cues, error identification and recovering
strategies, flexible multi-threaded, multi-device
dialog management, and content optimization and
organization strategies. A number of important
language phenomena are also covered in the sys-
tem?s functionality. For instance, one may use
words relying on context, such as ?this,? ?that,? ?it,?
and ?them,? to reference items mentioned in par-
ticular use contexts. Different types of verbal revi-
sion are also permitted by the system, providing a
great convenience to its users. The system supports
multi-threaded dialogs so that users can diverge to
a different topic before the current one is finished
and still come back to the first after the second
topic is done. To lower the cognitive load on the
drivers, the content optimization component orga-
nizes any information given to users based on on-
tological structures, and may also refine users?
queries via various strategies. Domain knowledge
is represented using OWL, a web ontology lan-
guage recommended by W3C, which should
greatly facilitate its portability to new domains.
The spoken dialog system consists of a number of
components (see Fig. 1 for details). Instead of the
hub architecture employed by Communicator pro-
jects [Senef et al 1998], it is developed in Java and
uses a flexible event-based, message-oriented mid-
dleware. This allows for dynamic registration of
new components. Among the component modules
in Figure 1, we use the Nuance speech recognition
engine with class-based ngrams and dynamic
grammars, and the Nuance Vocalizer as the TTS
engine. The Speech Enhancer removes noises and
echo. The Prosody module will provide additional
features to the Natural Language Understanding
(NLU) and Dialogue Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation.
Parallel to the deep analysis, a topic classifier as-
signs top n topics to the utterance, which are used
in the cases where the dialog manager cannot make
24
any sense of the parsed structure. The NLU mod-
ule also supports dynamic updates of the knowl-
edge base.
The CSLI DM module mediates and manages in-
teraction. It uses the dialogue-move approach to
maintain dialogue context, which is then used to
interpret incoming utterances (including fragments
and revisions), resolve NPs, construct salient re-
sponses, track issues, etc. Dialogue states can also
be used to bias SR expectation and improve SR
performance, as has been performed in previous
applications of the DM. Detailed descriptions of
the DM can be found in [Lemon et al2002; Mirk-
ovic & Cavedon 2005].
The Knowledge Manager (KM) controls access to
knowledge base sources (such as domain knowl-
edge and device information) and their updates.
Domain knowledge is structured according to do-
main-dependent ontologies. The current KM
makes use of OWL, a W3C standard, to represent
the ontological relationships between domain enti-
ties. Prot?g? (http://protege.stanford.edu), a do-
main-independent ontology tool, is used to
maintain the ontology offline. In a typical interac-
tion, the DM converts a user?s query into a seman-
tic frame (i.e. a set of semantic constraints) and
sends this to the KM via the content optimizer.
The Content Optimization module acts as an in-
termediary between the dialogue management
module and the knowledge management module
during the query process. It receives semantic
frames from the DM, resolves possible ambigui-
ties, and queries the KM. Depending on the items
in the query result as well as the configurable
properties, the module selects and performs an ap-
propriate optimization strategy.
Early evaluation shows that the system has a
task completion rate of 80% on 11 tasks of MP3
player domain, ranging from playing requests to
music database queries. Porting to a restaurant se-
lection domain is currently under way.
References
Seneff, Stephanie, Ed Hurley, Raymond Lau, Christine Pao,
Philipp Schmid, and Victor Zue, GALAXY-II: A Reference
Architecture for Conversational System Development, In-
ternational Conference on Spoken Language Processing
(ICSLP), Sydney, Australia, December 1998.
Lemon, Oliver, Alex Gruenstein, and Stanley Peters, Collabo-
rative activities and multi-tasking in dialogue systems,
Traitement Automatique des Langues (TAL), 43(2), 2002.
Mirkovic, Danilo, and Lawrence Cavedon, Practical Multi-
Domain, Multi-Device Dialogue Management, Submitted
for publication, April 2005.
Weng, Fuliang, Lawrence Cavedon, Badri Raghunathan, Hua
Cheng, Hauke Schmidt, Danilo Mirkovic, et al, Develop-
ing a conversational dialogue system for cognitively over-
loaded users, International Conference on Spoken
Language Processing (ICSLP), Jeju, Korea, October 2004.
25
Managing Dialogue Interaction: A Multi-Layered Approach
Oliver Lemon
School of Informatics
University of Edinburgh
2 Buccleugh Place
Edinburgh EH8 9LW, UK
olemon@inf.ed.ac.uk
Lawrence Cavedon
CSLI
Stanford University
220 Panama St
Stanford, CA 94306, USA
lcavedon@csli.stanford.edu
Barbara Kelly
Department of Linguistics
UCSB
Santa Barbara
CA 93106-3100, USA
bfk0@umail.ucsb.edu
Keywords: dialogue management architecture, in-
teraction, communication channel management
Abstract
We present evidence for the importance
of low-level phenomena in dialogue in-
teraction and use this to motivate a
multi-layered approach to dialogue pro-
cessing. We describe an architecture
that separates content-level communica-
tive processes from interaction-level phe-
nomena (such as feedback, grounding,
turn-management), and provide details of
specific implementations of a number of
such phenomena.
1 Introduction
Real dialogue between human participants involves
phenomena that do not so much contribute to the
content of communication as relate directly to the
interactive process between the participants. This
includes turn management, providing feedback, ut-
terance fillers, error and false-start management, and
utterance timing.
Recent work on dialogue and natural language
processing in general has acknowledged the pres-
ence of such phenomena in natural speech, and in
some cases the importance of its role in dialogue in-
teraction. However, treatment of such phenomena
has generally been part of the standard processing
model; for example, some parsers are able to han-
dle fillers such as ?um?, while recent versions of the
TRIPS system (Allen et al, 2001) uses incremental
parsing and other techniques to handle a range of re-
lated phenomena.
We believe that greater focus on ?interaction
level? phenomena is appropriate and will lead to
benefits in building dialogue systems for more ro-
bust natural interaction. In this paper, we outline a
two-layer architecture for dialogue systems, where
one layer uses a range of ?shallow? processing tech-
niques to maintain a smooth interaction between the
dialogue participants.
1.1 Managing interaction
The inspiration for a clean separation into a two-
layer architecture comes from two sources. Clark
(1996) distinguishes between two separate commu-
nication tracks, which he calls communicative and
meta-communicative. These are simultaneously oc-
curring communications, the first dealing with the
information at hand, and the other relating to the
performance itself. Dialogue participants use what
Clark refers to as signals to refer to the performance
itself: e.g. timing, delays, re-phrasing, mistakes, re-
pairs, etc.1
A second motivation is work on architectures for
robots and autonomous agents embedded in com-
plex, dynamic, unpredictable environments. Sev-
eral researchers in this area have argued for multi-
layered architectures for agents that plan action se-
quences to achieve some goal or task, but need to
react quickly to change in the environment (e.g.
1Clark?s distinction does not necessarily carry over directly
to the design of a dialogue system architecture, but it motivates
focus on the low-level communication channel.
(Firby, 1994; Mu?ller, 1996)). In such architectures,
the role of the bottom layer is to monitor the envi-
ronment and initiate appropriate actions within the
broader context of the goal-directed plan, which is
provided by the higher layer of the architecture. The
layers operate independently and asynchronously,
but communicate as necessary: e.g. goals and plans
are passed down to the execution layer, while obser-
vations or problems (which may trigger replanning)
are passed up to the planning layer.
We view the process of natural interaction with
a dialogue participant as analogous to the interac-
tion with a dynamic environment: dialogue phenom-
ena arise which need to be negotiated (as a new
obstacle must be avoided by a robot). In the case
of a human user involved in activity-oriented dia-
logue, timeliness is particularly important in order
to keep the user engaged and focussed?otherwise,
performance of the joint activity may be adversely
affected. In particular, dialogic interaction is a con-
tinuous process which cannot be broken without the
risk of some breakdown: signal-level phenomena
must be handled as smoothly as possible, without
necessarily resorting to content-level processes, in
order to maintain a tight interaction between the par-
ticipants.
1.2 A multi-layered architecture
Motivated partially by some of the same issues we
discuss here, Allen et al (2001) describe a new ar-
chitecture for their TRIPS system that breaks dia-
logue management into multiple asynchronous com-
ponents. We concur with their concerns but focus on
a different architectural shift.
We outline below an architecture that sepa-
rates interaction-focussed techniques from context-
management and conversation planning. An initial
version of the architecture has been implemented at
the Center for the Study of Language and Informa-
tion (CSLI) at Stanford University.
This breakdown into separate architectural levels
is analogous to the multi-level agent/robot architec-
tures. However, many of the same motivations per-
tain, especially those related to design considera-
tions (e.g. separating different types of phenomena
into different layers) and performance (e.g. high-
level planning from low-level execution and mon-
itoring running in parallel2). Further, the manner
in which Mu?ller and Firby?s systems handle reac-
tive tasks (e.g. obstacle avoidance, object tracking,
etc.) completely at the low-level whenever possible
reflects our view of how certain dialogue interaction
phenomena are best handled. Much like these sys-
tems, dialogue communicative goals are produced
at the higher level and imposed as constraints on
the lower-level. Environment-level processes fill in
the detail of these goals and handle contingencies
which may otherwise prevent the achievement of
these goals.
A number of interaction-management techniques
are present in the current implementation, including:
  A back-up recognition pass, using statistical
processing to extend grammar-based coverage
and provide immediate user ?help? feedback
for unrecognized utterances (Hockey et al,
2003);
  Turn management?timing of system output is
governed by monitoring the speech channel and
the (prioritized) agenda of speech outputs. If
the system need to take the turn, it grabs it using
only low-level processing;
  Handling user barge-in?user speech interrupts
system output and automatically grabs the turn;
  Immediate Grounding of recognized com-
mands (e.g. system says ?OK? immediately af-
ter recognizing the user: ?fly to the tower?);
  NP selection ? choosing anaphoric or salient
noun-phrases at the point of generation;
  Incremental aggregation of system-generated
utterances ? appropriately condensing and
forming elliptical system output at the point of
generation.
While this accounts for only a small number of
signals that arise during natural dialogue, the ar-
chitecture provides a framework for incorporat-
ing further techniques?in particular, using shallow
2Note: we are talking about very different parallel threads
here than those which occur in multi-modal fusion, such as oc-
curs in the SmartKom (Wahlster, 2002) system.
processing?for making use of such signals to pro-
vide more natural and robust interactions between
dialogue systems and human participants.
In the next section, we describe work from the lin-
guistic and psychology literature that demonstrates
the importance of asynchronous interaction-level
processing. In Section 3, we propose a specific ar-
chitecture that provides a framework for integrat-
ing various processes for channel-management. In
Sections 4 and 5, we describe specifics of the CSLI
implementation, outlining first the more abstract di-
alogue management layer, followed by techniques
employed at the interaction layer. In Section 6, we
discuss further possibilities and in Section 7 we con-
clude.
2 The Importance of Channel Phenomena
The standard processing model for dialogue systems
involves a sequence of modules from speech recog-
nition to speech synthesis, as illustrated in Figure 1,
which essentially illustrates (a simplification of) the
original TRIPS architecture, as described in (Fergu-
son and Allen, 1998). Typically, each module is self-
contained and relatively independent of other mod-
ules.
Recent findings in the psycholinguistic literature
have suggested various shortcomings of this mod-
ular approach. For example, work on alignment
indicates that conversation participants? processing
interacts on multiple levels, contravening the strict
modular model (Pickering and Garrod, 2003). This
is one of the considerations we address below, but
we are primarily concerned with other interaction-
level phenomena.
One of our prime motivations for an interaction
level processing layer is to ensure timely response
to interaction. Parsing and processing takes time?
this can be alleviated by incremental parsing tech-
niques, but meta-communication signals typically
do not need to be interpreted and processed to the
same extent as communicative utterances, and in-
stead require immediate attention that precludes full
processing.
For example, researchers have looked at the use
of um and uh in conversation and found that these
are often used as place-holders for a speaker who
wants to maintain their speaking turn (Clark and Fox
Tree, 2002). The detection of fillers such as these
generally acts to inhibit (to some extent) the listener
from interrupting or taking the turn from the current
speaker. Hence, not only should such discontinu-
ities not be ignored but they must also be processed
immediately in order to maintain the ongoing inter-
action.
Conversely, listeners also use what is known as
back-channel feedback to indicate to the speaker that
they are listening and paying attention. For En-
glish, back-channels include uh-huh, mhm and yeah.
Back-channels differ from other devices used to
keep a conversation flowing, such as repetitions and
collaborative finishes, in that they tend to be non-
specific to the current utterance. Moreover, back-
channel feedback is often produced without think-
ing, in response to simple prosodic clues such as a
speaker pause, a lowering of speaker pitch, or a rise
in speaker intonation (Ward and Tsukahara, 1999).
Most importantly, however, back-channel feed-
back is important to the speaker.3 Bavelas et al
(2000) investigated how a speaker in a conversation
(in this case someone narrating a story) is affected
when listener responses are inhibited. They found
that speakers with distracted and unresponsive lis-
teners did not finish their stories effectively, measur-
ably faltering at what should have been the dramatic
ending. Speakers needed an interlocutor?s feedback
to be able to maintain fluency and continue the dia-
logue effectively. Bavelas et alalso found that re-
sponse latency in one-on-one conversations is ex-
tremely short and may be simultaneous: listeners
can provide back-channels without fully listening to
the conversation partner and without being respon-
sible for taking up a speaking turn.
These results indicate that the nature of interac-
tion between participants is crucial to the collabora-
tive act of dialogue?signals and feedback that carry
effectively no communicative content are still im-
portant for keeping the interaction smooth and to en-
sure that the participants stay attentive and focussed
on the task at hand. When the dialogue task involves,
say, a human user being guided through a safety-
critical activity by an automated system, then such
issues are of particular importance.
3Allwood (1995) refers to such feedback morphemes as the
most important cohesion device in spoken language.
Speech Input Speech Output
Speech Recog GenerationNL Parser Speech SynthDialogueManager
Problem-Solving
Manager
Discourse
context
Figure 1: Traditional Dialogue System Architecture
Conversely, communicative behavior contains
signals regarding a participant?s attention, and in
particular may indicate a loss of focus. In tuto-
rial settings?one of the dialogue applications we
are specifically concerned with?this can be used
to determine students? confidence in their responses.
For example, phenomena such as timing between re-
sponses, hesitance markers, and intonation can all
be implicit clues that a student is having a problem
(Graesser et al, 1995).
3 A Two-Level Architecture
The traditional architecture for dialogue systems ba-
sically involves a linear approach to processing, as
illustrated in Figure 1. In this standard architecture,
modules tend to be self-contained and only loosely
dependent. Evidence outlined above, particularly
that related to alignment, suggests that this tightly
encapsulated approach will deal poorly with the in-
teractive nature of real dialogue. Allen et als (2001)
revised TRIPS architecture introduces a more non-
linear approach to dialogue processing, with asyn-
chronous processes managing interpretation, gener-
ation, and interface to behavioral aspects.
We augment the TRIPS approach by combining
multiple processes for interpreting utterances (e.g.
structured parsing versus statistical techniques) and
for generating responses (e.g. generation from se-
mantic representation versus template-based). More
fundamental to the architectural distinction we pro-
pose, the processing of an utterance and generat-
ing an appropriate response may proceed without
full processing by the Dialogue Management com-
ponent: information gleaned from an utterance will
always be passed up to the Dialogue Manager, but to
ensure timely response, an appropriate response may
be produced directly from a low-level component.
Other processes included at the interaction layer de-
tect non-communicative information, such as gaps
or delays in the user?s speech.
Figure 2 illustrates various aspects of the specific
two-level architecture we are developing. The lower
level interfaces directly with the user and, impor-
tantly, is driven by this interaction. For example the
low level includes a Turn Manager which manipu-
lates the speech channel to ensure that:
  user inputs are respected without interruption
(except when necessary);
  turn passes to the appropriate participant, based
on the highest priority Agenda item and the di-
alogue move that generated it;
  generated outputs are natural and timely;
  recognized user inputs are acknowledged
quickly using simple feedback utterances.
The upper level is responsible for modeling other
aspects of the conversational context, as well as
communicative goals and intentions. The con-
tent (i.e. logical forms) of user utterances are pro-
cessed using the dialogue model (e.g. updates and
adding nodes to the Dialogue Move Tree (Lemon et
al., 2002b)), and system utterances are constructed
which are in line with the system?s communicative
Dialogue
Move
Tree
Activity
Model
Context
Mgr ConversationPlanner
Agent
- intentions
- goals
- plans
- observations
Content layer:
- utterance planning
- communicative intentions
- grounding
- content management
- interaction with agent arch
Speech
recogition
and
Parsing
Backup
Shallow
Processor
(Helper)
Speech channel
Turn
Mgr
TTS
Generation
ModuleOutput
Agenda
Attention
Monitor
Interaction layer
- timing
- form
- engagement
- acknowledgement
Generation:
- anaphora
- pronouns
- aggregation
- echoing
ack
Figure 2: System Architecture
goals and intentions, whether they be imparting in-
formation to the user or requesting clarification or
further information.
The higher level also interacts with the rest of the
agent architecture, mediated by an Activity Model
(i.e. a representation of the agent activities about
which dialogue may occur (Gruenstein, 2002)). The
agent may wish to communicate its own goals, the
progress of its activities, or report on any observa-
tions it makes regarding its environment.
As with multi-layered agent architectures, the
two levels operate semi-autonomously and asyn-
chronously: the lower level is driven by tight in-
teraction with the user, while the upper level is
driven by longer-range communicative goals from
its activities and responses to user utterances. How-
ever, various types of information exchange connect
the two levels. For instance, user utterances rec-
ognized at the lower level must clearly be passed
to the content-management level to be parsed and
then incorporated into the dialogue context, while
high-level communication goals must be passed to
the lower level?s Output Agenda for generation and
speech-synthesis.
The Output Agenda plays a crucial role in medi-
ating utterances to be communicated, whether they
be system-initiated or responses, and generated from
the planner or a low-level component. The Output
Agenda is a prioritized list, where an utterance?s pri-
ority is influenced by a number of factors, such as:
whether it is in response to an error or misunder-
standing (i.e. ?Pardon?); the importance of the com-
municative content (i.e. an urgent observation); and
the dialogue move that generated it (e.g. answering a
question). The Agenda runs asynchronously, aggre-
gating multiple utterances when appropriate as well
as influencing speaker turn (see below).
Of perhaps greater interest, the interaction level
can be used to monitor user engagement and at-
tention in other ways ? e.g. time between utter-
ances, speaking rate, use of speech fillers ? to de-
tect potential problems as soon as possible, and to
provide early warning to the content layer that the
user may have, for example, misunderstood some
instruction. This can be used to generate a clarifi-
cation or grounding sub-dialogue, in order to estab-
lish mutual understanding before proceeding (thus
improving robustness of the system as a whole).
Conversely, expectations at the upper-layer can
influence processing at the interaction layer: for ex-
ample, open points of attachment on the Dialogue
Move Tree represent types of utterances the system
expects from the user, and these are used to prime
the recognition of incoming utterances for faster
processing, as well as influencing the turn.
In engineering terms, this division of labour is
attractive in that the clarity and modularity of dia-
logue management is enhanced. Rather than conflat-
ing, for example, turn-management with utterance
planning in a single generation component of a dia-
logue system, the separation into multiple levels of
processing allows different turn-taking and utterance
planning strategies to be developed independently,
and various combinations to be experimented with.
In the rest of the paper, we discuss our dialogue
management architecture and, in particular, the tech-
niques employed so far at each of the two levels de-
scribed here to enhance user experience and improve
overall system performance. The current implemen-
tation based on the above architecture is still being
refined; we focus on the features that have already
been implemented.
4 Top-Level Context Management
The approach to dialogue modeling we have imple-
mented is based on the theory of dialogue games
(Carlson, 1983; Power, 1979), and, for task-oriented
dialogues, discourse segments (Grosz and Sidner,
1986). These accounts rely on the observation that
answers generally follow questions, commands are
usually acknowledged, and so on, so that dialogues
can be partially described as consisting of adjacency
pairs of such dialogue moves. The notion of ?attach-
ment? of dialogue moves on a Dialogue Move Tree
(DMT) (Lemon et al, 2002b) embodies this idea.
An Activity Tree represents hierarchical and tem-
poral information about the task-state of the dia-
logue. Activities are the joint tasks managed by the
dialogue: e.g. booking a flight or moving a robot?
again, see (Lemon et al, 2002b) for details. Nodes
on the Activity Tree can be in various states (active,
complete, failed,  ), and any change in the state of
a node (typically because of an action by the agent)
is placed onto the system?s Output Agenda for po-
tential verbal report to the user, via the low-level
message selection and generation module.
This level of the architecture is where conversa-
tion planning and generation of system-initiated top-
ics occur. Any planned communication (whether it
be system-initiated or in response to a user utter-
ance) is put on to the Output Agenda, where it is
scheduled for generation.4 Conversely, true ground-
ing ? i.e. acknowledging that an utterance is un-
derstood within the context of the rest of the dia-
logue ? only occurs after the utterance has been in-
terpreted with respect to the DMT. Since a simple
acknowledgment may already have been generated
4The order in which outputs are generated, or even whether
they end up generated at all, depends on the priority of the cor-
responding information as well other interactions with the user.
after recognition, output after interpretation is only
needed if a response is required (e.g. the user asked
a question), or if a problem is detected (e.g. an am-
biguity must be resolved).
Since system communication is planned here, this
layer is also the one that interacts with the rest of the
agent architecture: any goals, state-changes, or ob-
servations that the agent may wish to communicate
are added as communicative goals, typically via the
Activity Model. For command-and-control applica-
tions (e.g. guiding a robot or UAV), system-initiated
utterances tend to be fairly short and simple and
conversation-planning is minimal; however, for our
dialogue-enabled tutorial application (Clark et al,
2001), conversation-planning is quite complex and
the system may generate multiple, relatively long ut-
terances on its own initiative.
5 Low-level Conversation Management:
Maintaining the Communication
Channel
We currently employ a range of shallow processing
techniques to maintain a smooth interaction with the
human dialogue participant. By ?shallow process-
ing? we mean processing that does not necessarily
result in or concern itself with the semantic repre-
sentation or pragmatic interpretation of the utterance
in the context of the dialogue. In particular, informa-
tion at this level is not processed in the context of the
Dialogue Move Tree or the Activity Tree.
In the following, we describe a number of the low-
level processing techniques currently implemented
in our system. Future work will address more of the
interaction phenomena described earlier.
5.1 Case study 1: Helper Feedback
In cases where a user utterance is not recognized, the
input is passed to a statistical recognizer of wider
coverage. This recognizer is often able to detect
lexical items and grammatical structures in the in-
put that are not covered by the first (grammar-based)
recognizer. In these cases, the results of the second
recognition pass are used to inform the user of the
system?s shortcomings, for example: ?The system
heard you say ?Look around for a red car?, but the
system does not know the word ?around?. You could
say ?Look for a red car? ?.
None of these utterances is planned or represented
at the top level of dialogue management. They are
produced simply to inform the user of a communi-
cation breakdown and to try to keep the communi-
cation flowing. If the user were to indulge in meta-
dialogue about the help message, then that message
would need to be represented in the high-level con-
text. However, we present the help message as being
generated by a different ?helper? agent, which dis-
appears (from the GUI) as soon as the help message
is produced, thus discouraging the user from engag-
ing it in dialogue.
User tests have shown that the use of this low level
module (which can be installed independently of the
high-level dialogue manager) significantly improves
task completion (both percentage of tasks completed
and time taken). By the fifth task, 100% of users
with the helper completed the task as compared with
80% of those without, and those without the helper
took on average 53% longer to complete the tasks.
For full details of the evaluation see (Hockey et al,
2003).
5.2 Case study 2: Turn Taking
Here we use a turn-marker at the low-level of dia-
logue processing. The turn can be marked as user,
system or none, and is set in a variety of ways. If
the user begins to speak (start-of-speech signal is re-
ceived from the recognizer) the turn becomes user
and any system audio output is stopped. If the sys-
tem needs to take the turn (e.g. if it has urgent in-
formation it needs to communicate), but turn is set
to user, and the user is not speaking, the system will
output ?Just a moment? and so take the turn before
generating its required utterance. Again, note that
this turn-grabbing utterance is not planned or repre-
sented at the top-level of dialogue moves. It does
not need to enter into such high-level plans or rep-
resentations because it is required only in order to
manipulate and maintain the channel, and does not
carry any content of its own.
The demonstration system displays a turn marker
on the GUI, allowing observers to monitor the
changing possession of the turn.
5.3 Case study 3: Incremental aggregation
Aggregation (Appelt, 1985) combines and com-
presses utterances to make them more concise, avoid
repetitious language structure, and make the sys-
tem?s speech more natural and understandable over-
all. In our system, this process is carried out not
at the level of content planning, but at the lower-
level of processing, where content logical forms are
manipulated (possibly combined) and converted into
strings for speech synthesis. Indeed, it is impor-
tant that aggregation functions at this lower level,
because the process needs access to:
  the message to be uttered (A),
  what has just been said (B),
  what is to be said next (C),
and the precise surface form of B is only represented
at the low-level. High-level processing only plans
the content of the utterance to be generated, and
passes it down, and so cannot determine the details
of the eventual surface form of the generated utter-
ance.
Aggregation techniques on a prewritten body of
text combine and compress sentences that have al-
ready been determined and ordered. In a complex
dialogue system however, aggregation should pro-
duce similarly natural output, but must function in-
crementally because utterances are generated on the
fly. In fact, when constructing an utterance we often
have no information about the utterances that will
follow it, and thus the best we can do is to com-
press it or ?retro-aggregate? it with utterances that
preceded it (see the example below). Only occasion-
ally does the Output Agenda contain enough unsaid
utterances to perform reasonable ?pre-aggregation?.
At the low-level of processing, the generator re-
ceives an item (on the Output Agenda) to be con-
verted into synthesized speech. This item consists
of a dialogue move type along with some content
(e.g. wh-answer, location(tower)).
Each dialogue move type (e.g. report, wh-
question, wh-answer) has its own aggregation rules,
stored in the class for that logical form (LF) type. In
each type, rules specify which other dialogue move
types can aggregate with it, and exactly how ag-
gregation works. The rules note identical portions
of LFs and unify them, and then combine the non-
identical portions appropriately.
For example, the LF that represents the phrase ?I
will fly to the tower and I will land at the parking
lot?, will be converted to one representing ?I will fly
to the tower and land at the parking lot? according
to the compression rules. Similarly, ?I will fly to the
tower and fly to the hospital? gets converted to ?I
will fly to the tower and the hospital?.
In contrast, the ?retro-aggregation? rules result in
sequences of system utterances such as,
Sys: I have cancelled flying to the base
Sys: and the tower
Sys: and landing at the school
Again, this process happens only at the low-level
processing stage of content realization, and needs
no access to the high-level representations of di-
alogue structure, history, and plans. A separate
thread running in the Output Agenda component
asynchronously performs aggregation as needed and
appropriate.
5.4 Case study 4: Choosing NPs
Another low-level process in utterance realization is
choosing appropriate NPs ? anaphoric expressions
such as ?it? or ?there?, or NPs which ?echo? those
already used by the human operator. Again, this rou-
tine does not need access to the high-level dialogue
management representations, but only to the list of
NPs employed in the dialogue thus far (the Salience
List).
Echoing is achieved by accessing the Salience
List whenever generating referential terms, and us-
ing whatever noun-phrase (if any) the user has pre-
viously employed to refer to the object in question.
Anaphoric phrases are generated whenever the ref-
erence object is the same as the one at the top of the
Salience List.
As in the case of aggregation, the top level content
generation algorithm does not manage the details of
utterance realization ? this is better handled at the
instant that the content logical form is to be trans-
lated into a string for the speech synthesizer. Other-
wise the top level would have to replan utterances af-
ter every intervening dialogue move. This example
shows how respecting the multi-level architecture is
desirable from an engineering point of view.
6 Current Implementation and Further
Possibilities
An initial version of the CSLI dialogue system based
on the described architecture has been implemented,
and is able to engage in conversations such as illus-
trated in Figure 3.
The system has been applied to both command-
and-control and tutorial applications; this is of inter-
est since the former generally involves user-initiated
conversations while in the latter, conversation tends
to be system-initiated. The Output Agenda me-
diates by handling both standard logical forms or
generation-templates.
Only a small number of the interaction-level phe-
nomena that arise in human-human dialogue have
been implemented, but we believe a number of them
could be treated within our framework. For in-
stance, processes at the lower level could detect mis-
communication and channel breakdown, and send a
request to the top level to replan the long-range dia-
logue strategy. This is particularly relevant in the tu-
torial setting, where low-level processes could detect
problems with user attention and responsiveness,
and prompt a switch to a different high-level strat-
egy. Particularly important for safety-critical appli-
cations, but of general use, would be low-level moni-
toring of channel noise and other environmental fac-
tors such as user gestures and gaze. Again, certain
combinations of these inputs would have high-level
consequences for interpretation and dialogue plan-
ning.
Recent work makes use of Icarus (Shapiro, 2001),
a reactive planning system that learns and adapts to
user behavior, to cover timing and realization of sys-
tem turns as well as handling delays in the user in-
put. In future, we anticipate that this will allow,
for instance, turn-taking facilities to be more eas-
ily adapted as personalities or situations require: for
example, after noticing a particular event the system
may be more likely to interrupt a speaker, or may
adapt to become less prone to interruption when in-
teracting with a speaker who responds poorly to sys-
tem barge-in.
7 Conclusion
We have presented evidence from the psycholinguis-
tic literature that demonstrates the importance of
Figure 3: Demonstration system, November 2002
Utterance Dialogue Move
Operator (O): Fly around the school Command
Helper (H): I heard you say ?Fly around the school? but the
system does not know the word ?around?. You could say ?y
to the school?
Helper feedback
O: Fly to the school Command
Robot (R): OK. Confirmation
O: Then pick up the medical supplies at the hospital Take turn, Command
R: OK. Wait a minute Confirmation, System grab turn
R: I am supposed to always y high Report Global Constraint
R: Shall I y to the hospital at low altitude anyway? Negotiate constraint relaxation, Re-
lease Turn
O: Yes. Do it anyway Yn-Answer
R: OK. Wait a minute Confirmation, System grab turn
R: There is a re at the tower Report observation
R: I am ying to the hospital Report Current Activity
O: Why? Why-question, Release Turn
R: Because I am picking up the medical sup  Why-Answer, User barge-in
O: Stop that. Command (Barge-in)
R: OK. I have cancelled ying there Confirmation, Report Activity State,
Anaphoric NP
O: Fight the re at the tower Command
R: OK. I am ying to the lake Confirmation, Report Activity State
O: Why are you doing that? Why-question, Release Turn
R: Because I am picking up water there. Why-Answer, Anaphoric NP
interaction-level phenomena in dialogue. Phenom-
ena such as fillers, timing, and feedback must be
dealt with properly and in a timely manner; other-
wise, the interaction and even the collaborative ac-
tivity being performed may be adversely affected.
Our approach is to cleanly separate the architec-
ture of dialogue systems to provide for an asyn-
chronous layer that is designed to handle interaction
signals, analogously to agent/robot architectures that
include a layer to manage interaction with a dynamic
environment. This allows processing outside the full
context of a dialogue history when required for pro-
cessing speed, while allowing the context to still in-
fluence such processes when able.
A system has been implemented based on this
architecture, containing a range of low-level pro-
cesses, which we have described here in some detail:
shallow-helper feedback; turn-management; aggre-
gation; NP selection. Current work is directed to-
wards incorporating techniques to manage further
phenomena?such as predictors of uncertainty and
loss of attention?in both command-and-control and
tutoring applications.
Acknowledgements
This research was partially supported by the Wallen-
berg Foundation?s WITAS project, Linko?ping Uni-
versity, Sweden, and by grant number N00014-02-
1-0417 from the Department of the US Navy. The
dialogue system was implemented while the first au-
thor was employed at CSLI, Stanford University.
References
James F. Allen, Bradford W. Miller, Eric K. Ringger, and
Teresa Sikorski. 1996. A robust system for natural
spoken dialogue. In Proceedings of ACL.
James Allen, George Ferguson, and Amanda Stent. 2001.
An architecture for more realistic conversational sys-
tems. In Proceedings of Intelligent User Interfaces
2001, Santa Fe, NM.
Jens Allwood. 1995. An activity based approach to prag-
matics. In Gothenburg Papers in Theoretical Linguis-
tics 76, Dept. of Linguistics, Uni. of Go?teborg.
Douglas E. Appelt. 1985. Planning english referring ex-
pressions. Artificial Intelligence, 26(1):1 ? 33.
J. B. Bavelas, L. Coates, and T. Johnson. 2000. Listeners
and co-narrators. Journal of Personality and Social
Psychology, 79:941?952.
Lauri Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis. D. Reidel.
Herbert H. Clark and Jean E. Fox Tree. 2002. Using uh
and um in spontaneous speaking. Cognition, 84:73?
111.
Brady Clark, John Fry, Matt Ginzton, Stanley Pe-
ters, Heather Pon-Barry, and Zachary Thomsen-Gray.
2001. Automated tutoring dialogues for training in
shipboard damage control. In Proceedings of SIGdial
2001.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
George Ferguson and James Allen. 1998. TRIPS: An in-
telligent integrated problem-solving assistant. In Pro-
ceedings 15th National Conference on Artificial Intel-
ligence (AAAI-98), pages 567?573, Madison, WI.
James Firby. 1994. Task networks for controlling con-
tinuous processes. In Proceedings 2nd Int?l Conf. on
AI Planning Systems, pages 49?54.
A. C. Graesser, N. K. Person, and J. P. Magliano. 1995.
Collaborative dialogue patterns in naturalistic one-to-
one tutoring. Applied Cognitive Psychology, 9:1?28.
Barbara Grosz and Candace Sidner. 1986. Attentions,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Alexander H. Gruenstein. 2002. Conversational inter-
faces: A domain-independent architecture for task-
oriented dialogues. Masters thesis, Computer Science
Department, Stanford University.
Beth-Ann Hockey, Oliver Lemon, Ellen Campana, Laura
Hiatt, Gregory Aist, Jim Hieronymus, Alexander Gru-
enstein, and John Dowding. 2003. Targeted help for
spoken dialogue systems: intelligent feed back im-
proves naive users? performance. In Proceedings Eu-
ropean Assoc. for Computational Linguistics (EACL
03).
Oliver Lemon, Alexander Gruenstein, Alexis Battle, and
Stanley Peters. 2002a. Multi-tasking and collabo-
rative activities in dialogue systems. In Proceedings
of 3rd SIGdial Workshop on Discourse and Dialogue,
pages 113 ? 124, Philadelphia.
Oliver Lemon, Alexander Gruenstein, and Stanley Pe-
ters. 2002b. Collaborative activities and multi-tasking
in dialogue systems. Traitement Automatique des
Langues (TAL), 43(2):131 ? 154. Special Issue on Di-
alogue.
Jorge P. Mu?ller. 1996. The Design of Intelligent Agents?
A Layered Approach. Springer Verlag, Heidelberg,
Germany.
Martin Pickering and Simon Garrod. 2003. Toward a
mechanistic psychology of dialogue. Brain and Be-
havioral Science. to appear.
Richard Power. 1979. The organization of purposeful
dialogues. Linguistics, 17:107?152.
Daniel Shapiro. 2001. Value-driven agents. Ph.D. thesis,
Department of Management Science and Engineering,
Stanford University.
Jan van Kuppevelt, Ulrich Heid, and Hans Kamp. 2000.
Best practice in spoken language dialogue system en-
gineering. Natural Language Engineering, 6.
Wolfgang Wahlster. 2002. SmartKom: fusion and fission
of speech, gestures, and facial expressions. In Pro-
ceedings of the 1st International Workshop on Man-
Machine Symbiotic Systems, pages 213?225, Kyoto,
Japan.
N. Ward and W. Tsukahara. 1999. A responsive dialog
system. In Y. Wilks, editor, Machine Conversations,
pages 169?174. Kluwer.
Proceedings of the Workshop on BioNLP, pages 46?54,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extraction of Named Entities from Tables in Gene Mutation Literature
Wern Wong?, David Martinez??, Lawrence Cavedon??
??NICTA Victoria Research Laboratory
?Dept of Computer Science and Software Engineering
The University of Melbourne
{wongwl,davidm,lcavedon}@csse.unimelb.edu.au
Abstract We investigate the challenge of extract-
ing information about genetic mutations from ta-
bles, an important source of information in scien-
tific papers. We use various machine learning algo-
rithms and feature sets, and evaluate performance in
extracting fields associated with an existing hand-
created database of mutations. We then show how
classifying tabular information can be leveraged for
the task of named entity detection for mutations.1
Keywords Information extraction; tables;
biomedical applications.
1 Introduction
We are interested in applying information extraction
and text mining techniques to aiding the construc-
tion of databases of biomedical information, in par-
ticular information about genetic mutations. Such
databases are currently constructed by hand: a long,
involved, time-consuming and human-intensive pro-
cess. Each paper considered for inclusion in the
database must be read, the interesting data identified
and then entered by hand into a database.2
However, the biomedical domain throws up many
new and serious challenges to information extraction
and text mining. Unusual terminology and under-
developed standards for nomenclature present prob-
lems for tokenisation and add complexity to stan-
dard information extraction tasks, such as named en-
tity recognition (NER). A lack of resources (at least
1A short version of this paper was presented at the Aus-
tralasian Document Computing Symposium, 2008. All copy-
rights from that event were retained by the authors.
2Karamis et al(2008) illustrate how even simple tools can
have an impact on improving the database-curation process.
compared to other domains), such as collections of
annotated full-text documents and relevance judge-
ments for various tasks, are a bottleneck to develop-
ing and evaluating the core techniques required.
In this paper, we report on work performed on
extracting information from tables in biomedical
research papers. Tables present a succinct and
information-rich format for providing information,
and are particularly important when reporting re-
sults in biological and medical research papers.
For example, the Human Genome Variation Society
(HGVS), in its general recommendations for muta-
tion nomenclature, recommends making use of tab-
ular listings when several changes are described in
a manuscript.3 A specific premise of our work is
that the highly-structured nature of tabular informa-
tion allows leverage of some techniques that are not
so sensitive to the well-reported problems inherent
in biomedical terminology, which complicate NER
tasks in this domain. In particular, we describe
initial techniques for extending NER performance
through the analysis of tables: columns/rows are
classified as containing items of the entities of inter-
est, thereby allowing those entities to be recognized
as of the target type. Since a significant amount of
such entities may be found in tables in biomedical
scientific papers, this can have positive impact on
the performance of base NER techniques.
NER tools specifically targeted at recognising
mutations have been developed (e.g. (Horn et al,
2004; Baker and Witte, 2006; Caporaso et al, 2007;
Lee et al, 2007)); however, they only detect a sub-
class of mutations, so-called single-point mutations,
3http://www.hgvs.org/mutnomen/recs.html#general
46
i.e. those that affect a single base. MutationFinder
(Caporaso et al, 2007) is the only publicly available
tool, built with around 700 automatically-generated
rules (both for different nomenclatures and natural
language). However, most of the mutations that
we find in our dataset are not point mutations or
do not follow point-mutation nomenclature, limiting
the usefulness of MutationFinder (and related tools)
over our document collection.
In the next section, we describe the setting of our
task, the Mismatch Repair (MMR) Database, and
outline the task of extraction from tables. In Sec-
tion 3, we describe the preparation of our document
collection, and in Section 4, we analyse the amount
of mutation-related information that is in the associ-
ated tables. Section 5 describes the main task, which
is classifying table rows and columns as containing
mutations, and Section 6 leverages this technique to
detect mutations of interest to the MMR Database.
We discuss the results in Section 7.
2 Background
In this section, we discuss the MMR database?the
setting for our task and from which we construct
our document collection?and previous approaches
to table processing.
2.1 The MMR Database
Our extraction task is grounded in the specific con-
text of the Mismatch Repair (MMR) Database com-
piled at the Memorial University of Newfoundland
(Woods et al, 2007)?a database of known genetic
mutations related to hereditary non-polyposis col-
orectal cancer (HNPCC), a hereditary form of bowel
cancer. The MMR Database contains information
on genetic mutations known to be related to HN-
PCC, along with links to the research papers from
which the database has been constructed.4 From the
database and its links to papers, we were able to con-
struct a collection of tables related to HNPCC muta-
tions, and then use the MMR database records them-
selves as a gold standard for evaluating our tech-
niques. As of May 2008, the MMR database con-
tained a total of 5,491 records on mutations that oc-
4I.e. a team of geneticists manually trawled the biomedical
literature for information on HNPCC-related mutation informa-
tion, and added links to any papers relevant to those mutations
in the context of HNPCC.
cur on any one of four genes that have been identi-
fied as related to colon cancer. An example record
from the MMR database is the following:
MLH1 | Exon13 | c.1491delG | Yamamoto et al | 9500462
Respectively, this record contains: gene; exon;
mutation; citation of the paper the information was
sourced from;5 and the paper?s PubMedID. These
fields are important because they contain informa-
tion researchers are directly interested in (gene,
exon, mutation) and the paper said information was
found in. Note that if a gene/mutation pair is refer-
enced in multiple papers, then there are correspond-
ingly multiple entries in the database. Conversely, if
a single paper mentions multiple (relevant) genes,
then that paper is mentioned in multiple database
records.
2.2 Table Processing
An important but less-researched sub-problem in
text mining is information extraction from tables.
This is particularly important in the biomedical do-
main since much important data is present in tabu-
lar form, such as experimental results, relations be-
tween entities, and other information that may not
be contained elsewhere in the text. For example, the
table shown in Figure 1 (taken from an article in our
collection) contains much of the same data that was
present in database records, in a similar format.
Tabular information extraction can be divided into
two broad sub-tasks:
? table detection: identifying tables within docu-
ments;
? table processing: extraction of data from tables.
Several systems have been developed to handle both
tasks, some are designed only to handle table de-
tection, and others focus only on extracting data.
Both machine learning and heuristic / rule-based ap-
proaches have been proposed.
Table detection techniques depend heavily on the
input format. Most work that tackles this problem
tends to assume one homogeneous input format, but
tables generally come in one of two varieties:6
5This field has been abbreviated. We have also omitted fields
such as ?internal id?.
6We don?t consider the possibility of processing bitmaps or
other images from scanned documents.
47
Figure 1: Sample table containing mutation information related to HNPCC
? raw text tables: generally ASCII text in
monospace font, delimited by whitespace
and/or special characters;
? rich text tables: those formatted using LaTeX,
PDF, HTML and other such formats.
Tables in plain text tend to be more difficult to
detect, as the detection system must be sensitive to
whitespace and symbols used to align cells in tables.
Efforts to handle rich text formats generally focus on
HTML-based representations. Raw HTML is easier
to parse than raw LaTeX or PDF, and most formats
are easily converted to HTML. HTML tables can
theoretically be trivially detected using <table>
tags. However, Lerman et al(2004) note that in
HTML files taken from the web, only a fraction of
tabular data was presented using <table> tags, and
those tags were also used to format multi-column
text, images and other non-table applications. Hurst
(2001) attests that less than 30% of HTML tables on
the web contain actual tabular content; for many, the
HTML table tags are often used simply for format-
ting purposes.
Zanibbi et al(2004) present a survey of table
recognition in general. Of greatest relevance to us
here are approaches that adopt a machine learning
approach to detecting and/or extracting table data.
Cohen et al(2002) use features based on HTML
table tags, number of rows and columns of spe-
cific lengths, and ratios of single-square cells to to-
tal number of cells, to perform table detection, and
then form a geometric representation of the data us-
ing algorithms based on table-rendering techniques
implemented by browsers.
Pinto, Wei, and their colleagues have used condi-
tional random fields (CRFs) to both detect and pro-
cess tables simultaneously. Pinto et al(2003) com-
pare the output of their CRF system with a previ-
ous effort using hidden Markov machines (HMMs).
These systems use features such as: presence of
whitespace of varying length (different lengths of
whitespace are used as separate features); domain-
specific lexical features (such as month names, year
strings, specified keywords); separator characters
(e.g. ?+?, ?-?, etc). In subsequent work they develop
a system for performing question answering over ta-
ble data (Wei et al, 2004) by treating each extracted
data cell as a discrete document.
To our knowledge, no previous system has at-
tempted to extract data from tables in biomedical
literature. This is possibly because of a combina-
tion of the lack of resources for this domain (e.g.
48
collections of full-text documents; relevance judge-
ments), as well as the lesser focus on text mining
in general in this area. As will be seen in the next
section, the vagaries of the construction of our col-
lection of tables means we were effectively able to
ignore the issue of table detection and focus directly
on the problem of processing.
3 Experimental Setting
Our experiments were designed to identify mentions
of mutations in the biomedical literature, focusing
on tabular content. In this section, we first describe
our target dataset, built from the hand-curated MMR
database (Woods et al, 2007); we then explain the
table extraction process; finally, we introduce the
task design.
3.1 Mutation Mention Dataset
We relied on the MMR Database and MEDLINE in
order to build our test collection. First we collected
all the information available in the hand-curated
MMR records, obtaining a total of 5,491 mutations
linked to 719 distinct PubMedIDs7.
Our next step was to crawl the full-text articles
from MEDLINE. We used an automatic crawler that
followed the links from the PubMed interface, and
downloaded those papers that had a full-text HTML
version, and which contained at least one content ta-
ble.
The tables were then extracted from the full text
HTML files. It is important to note that the tables
were already present as links to separate HTML files
rather than being presented as inline tables, making
this process easier. Papers that did not contain tables
in HTML format were discarded.
Our final collection consisted of 70 papers out of
the original 719 PubMedIDs. Some of the papers
were not available in full text, and for others our
crawling script failed to extract the full version. Our
approach was conservative, and our collection could
be augmented in the future, but we decided to fo-
cus on this dataset for the experiments presented in
this paper. This set of articles is linked to 717 MMR
records (mutations), which constitutes our gold stan-
dard hand-curated annotation. The collection con-
tains 197 tables in all.
7Data was downloaded from the web interface in May 2008.
3.2 Table extraction
Once scraped, the tables were then pre-processed
into a form that more readily allowed experimenta-
tion. The tables were therefore split into three parts:
column headers, row headers, and data cells. This
was done based on the HTML formatting, which
was consistent throughout the data set as the tables
were automatically generated.
The first step was to deconstruct the HTML ta-
bles into nested lists of cells based on HTML ta-
ble tags. Inconsistencies introduced by colspan and
rowspan attributes were resolved by replicating a
cell?s contents across its spanned lengths. That is, a
cell with colspan=3 would be duplicated across the
three columns, and likewise for cells spanning mul-
tiple rows. Single-cell rows at the top or bottom of a
table were assumed to be captions and discarded.
The remaining HTML was stripped, save for the
following tags which contained important informa-
tion:
? img tags were replaced by their alternate text,
where available. Such images often represent
a mathematical symbol, which is important in-
formation to retain;
? hr tags proved to be an important indicator for
dividing header cells from data cells.
Tables were broken up into row headers, column
headers, and data cells by making use of the hr tags,
denoting horizontal lines, to detect column headers.
Such tags tend to be present as a separator between
column header cells and data cells; in fact, the only
tables in our collection that did not have the separa-
tors did not have column headers either. The hr tags
were subsequently stripped after this use. Detecting
row headers was performed by checking if the top
left cell of the table was blank, a pattern which oc-
curred in all row-major tables. The vast majority of
tables had column headers rather than row headers,
although some had both and a small proportion had
only row headers. We acknowledge that this pro-
cessing may be specific to the vagaries of the specific
format of the HTML generation used by PubMed
(from which we sourced the tables). However, our
whole task is specific to this domain; further, our fo-
cus is on the extraction task rather than the actual
detection of row/column headers.
49
Class Class Freq. Cell Freq.
Gene 64 1,618
Exon 48 1,004
Codon 23 435
Mutation 90 2,174
Statistic 482 8,788
Other 576 14,324
Total 1,283 28,343
Table 1: Frequency per class and number of cells in the
collection.
3.3 Task Design
In order to extract mutation information from
tables, we first performed classification of full
columns/rows into relevant entities. The content of a
column (or row, depending on whether the table was
row- or column-oriented) tends to be homogeneous;
this allowed us to build classifiers that can identify
full vectors of relevant entities in a single step. We
refer to this task as table vector classification.
We identified the following classes as relevant:
Gene, Exon, Mutation, Codon, and Statistic. The
first four were chosen directly from the MMR
Database. We decided to include ?Statistic? after in-
specting the tabular dataset, since we found that this
provides relevant information about the importance
of a given mutation. Of the five classes, Mutation
is the most informative for our final information ex-
traction goal.
The next step was to hand-annotate the headers
of the 197 tables in our collection by using the five
classes and the class ?Other? as the tagset. Some
headers belonged to more that one class, since the
classes were collapsed into a single field of the ta-
ble. The frequency per class and the number of cells,
across the collection of tables, is shown in Table 1.
3.4 Evaluation
We evaluated our systems in two ways:
? Header classification: performance of different
systems on predicting the classes of each col-
umn/row of the tables;
? Mutation extraction: recall of our system over
the subset of the hand-curated MMR database.
Evaluation for the header classification step was
performed using precision, recall and f-score, micro-
averaged amongst the classes. Micro-averaging in-
volves multiplying the score of a class by the number
of instances of the class in the gold standard, and di-
viding by the total number of instances. For the ma-
chine learning algorithms, evaluation was performed
using 10-fold cross-validation. For mutation extrac-
tion we focus on a single class, and produce recall
and a lower-bound on precision.
4 Mutation Mentions in Tables
In order to determine the value of processing tab-
ular data for mutation-mining purposes, we ob-
tained a sample of 100 documents that were hand-
annotated by curators prior to their introduction in
the database?the curators highlighted relevant mu-
tations found in each paper. We found that for 59
of the documents, only the tabular parts of the paper
were selected; 33 of the documents had only textual
parts highlighted; and for 8 documents both tables
and text were selected. This is an indicator of the
importance of tabular data in this context.
Our next step was to measure the amount of in-
formation that we could potentially extract from the
tables in our collection. Since we are interested in
mutations, we extracted all cells from the vectors
that were manually annotated as ?Mutation? in or-
der to compare them to the goldstandard, and mea-
sure the recall. This comparison was not straight-
forward, because mutation mentions have different
nomenclatures. Ideally we would normalise the dif-
ferent references into a standard form, and then per-
form the comparison. However, normalisation is a
complex process in itself, and we resorted to evalu-
ation by hand at this point.
We found that 198 of the 717 goldstandard muta-
tions were present in tables (28%). This is a signif-
icant amount, taking into account that their extrac-
tion should be much easier than parsing the raw text.
We also tested MutationFinder over the full text, and
found that only 6 of the goldstandard mutations were
retrieved (0.8%), which indicates that point mutation
identification is not sufficient for this task.
Finally, we measured the amount of information
that could be extracted by a simple string look-up
system separately over the tabular and textual parts
50
of the articles. We were looking for mutation men-
tions that correspond exactly to the goldstandard
record from each article, which meant that mentions
in different nomenclatures would be missed. We
found that a total of 177 mentions (24.7%) could be
found with the same spelling; of those 142 (80.1%)
were found in tables only, and the remaining 35
(20.9%) were found in both tables and text; i.e., no
mention was found in text only.
These results indicate that we can find relevant in-
formation in tables that is not easy to detect in run-
ning text.
5 Table Vector Classification
We built automatic classifiers to detect relevant en-
tities in tables. Two separate approaches were at-
tempted for vector classification: applying heuristic
rules, and machine learning (ML) techniques. These
are described here, along with an analysis of their
performance.
5.1 Heuristic Baseline
As a baseline method, we approached the task of
classifying headers by matching the header string to
the names of the classes in a case-insensitive man-
ner. When the class name was found as a substring
of the header, the class would be assigned to it. For
example, a header string such as ?Target Mutation?
would be assigned the class ?Mutation?. Some head-
ers had multiple annotations (E.g. ?Gene/Exon?).
For better recall, we also matched synonyms for
the class ?Mutation? (the terms ?Variation? and
?Missense?) and the class ?Statistic? (the terms
?No.?, ?Number? and ?%?). For the remaining
classes we did not identify other obvious synonyms.
The results are shown in Table 2. Precision
was reasonably high for the ?Codon?, ?Exon? and
?Statistic? classes. However, this was not the case
for ?Mutation?, and this illustrates that different
types of information are provided under this head-
ing; illustrative examples include the heading ?Mu-
tation Detected? on a ?Gene? vector, or the heading
?Germline Mutation? referring to ?Statistics?. The
recall was also low for ?Mutation? and most other
classes, showing that more sophisticated approaches
are required in order to exploit the information con-
tained in the tables. Notice also that the micro-
Class Precision Recall FScore
Gene 0.537 0.620 0.575
Exon 0.762 0.615 0.681
Codon 0.850 0.654 0.739
Mutation 0.283 0.301 0.292
Statistic 0.911 0.324 0.478
Other 0.581 0.903 0.707
Micro Avg. 0.693 0.614 0.651
Table 2: Naive Baseline results across the different
classes and micro-averaged
Class Precision Recall FScore
Gene 0.537 0.611 0.571
Exon 0.762 0.615 0.681
Codon 0.850 0.654 0.739
Mutation 0.600 0.452 0.515
Statistic 0.911 0.340 0.495
Other 0.579 0.910 0.708
Micro Avg. 0.715 0.633 0.672
Table 3: Results integrating MutationFinder across the
different classes and micro-averaged
average is highly biased by the classes ?Statistic?
and ?Others?, since they contain most of the test in-
stances.
Our second step was to build a more informed
classifier for the class ?Mutation? using the point
mutation NER system MutationFinder (Caporaso et
al., 2007). We applied this tool to the text in the
table-cells, and identified which table-vectors con-
tained at least one mutation mention. These vectors
were also classified as mutations. The results are
shown in Table 3. This approach caused the ?Muta-
tion? results to improve, but the overall f-score val-
ues are still in the range 50%-70%.
We considered other heuristic rules that could
be applied, such as looking for different kinds of
patterns for each class: for instance, numbers for
?Exon?, or the normalised form c.N[symbol]N for
mutation, or trying to match against term lists (e.g.
using Gene dictionaries). Future work will explore
extending the ML approach below with features
such as these.
51
5.2 Classification Techniques
For the ML experiments we used the Weka (Witten
and Frank, 2005) toolkit, as it contains a wide se-
lection of in-built algorithms. We selected a variety
of well-known approaches in order to obtain a better
picture of the overall performance. As a baseline, we
applied the majority class from the training data to
all test instances. We applied the following ML sys-
tems:8 Naive Bayes (NB); Support Vector Machines
(SVM); Propositional Rule Learner (JRip); and De-
cision Trees (J48). We did not tune the parameters,
and relied on the default settings.
In order to define our feature sets, we used the
text in the headers and cells of the tables, without
tokenisation. Other possible sources of information,
such as captions or the running text referring to the
table were not employed at this stage. We applied
four feature sets:
? Basic (Basic): Four basic features, consisting
of the header string, the average and median
cell lengths, and a binary feature indicating
whether the data in the cells was numeric.
? Cell Bag-of-Words (C bow): Bag of words
over the tokens in the table cells.
? Header Bag-of-Words (H bow): Bag of
words over the tokens in the header strings.
? Header + Cell Bag-of-Words (HC bow):
Combination of bags of words formed by the
tokens in headers and cells, represented as sep-
arate types of features.
The micro-averaged results of the different learn-
ing methods and feature sets are shown in Table 4.
Regarding the feature sets, we can see that the best
performance is obtained by using the headers as bag-
of-words, while the content of the cells seems to be
too sparse to guide the learning methods. SVM is
the best algorithm for this dataset, with JRip and J48
following, and NB performing worst of the four in
most cases.
Overall, the results show that the ML approach
is superior to the baselines when using the header
bag of words feature to classify the relevant entities.
8We applied a number of other ML algorithms as well, but
these showed significantly lesser performance.
Method Feature SetsBasic C bow H bow HC bow
Mj. Cl. 0.288
NB 0.614 0.454 0.678 0.581
SVM 0.717 0.599 0.839 0.816
JRip 0.564 0.493 0.790 0.749
J48 0.288 0.532 0.793 0.782
Table 4: Results for ML Algorithms - Micro-Averaged
FScores. Mj.Cl.: Majority Class. The best results per
column are given in bold.
Class Precision Recall FScore
Gene 0.778 0.737 0.757
Exon 0.786 0.707 0.745
Codon 0.833 0.882 0.857
Mutation 0.656 0.679 0.667
Statistic 0.919 0.853 0.885
Other 0.82 0.884 0.850
Micro Avg 0.839 0.841 0.839
Table 5: Results for SVM and the feature set H bow per
class and micro-averaged.
SVM is able to reach a high f-score of 83.9%, which
has been found to be significantly better than the best
baseline after applying a paired t-test (p-value under
0.0001).
We break down the results per class in Table 5,
using the outputs from SVM and feature-set H bow.
We can see that all classes show an improvement
over the heuristic baselines. There is a big increase
for the classes ?Gene? and ?Statistic?, and all classes
except mutation are above 70% f-score. ?Muta-
tion? is the most difficult class to predict, but it
still reaches 66.7% f-score, which can be helpful for
some tasks, as we explore in the next section.
6 Automatic Mutation Extraction
We applied the results of our classifier to a practi-
cal application, i.e., the detection of mutations in
the literature for the MMR Database project. Ta-
ble vector classification allows us to extract lists of
candidate mutation names from tables to be added
to the database. We would like a system with high
recall that identifies all relevant candidates, but also
acceptable precision so that not all the tables need to
52
System Mut. Found TP % in MMR Rec.
Automatic 1,702 153 9.0 77.3
Gold standard 1,847 198 10.7 100
Table 6: Results for Mutation detection. TP indicates the
number of true positives, ?% in MMR? shows the per-
centage of positives found in the database.
be hand-checked.
In order to test the viability of this approach, we
measured the results of the system in detecting the
existing hand-curated mutations in MMR. We cal-
culated the recall in retrieving those mutations, and
also the rate of false positives; however, note that
we also consider as false positives those valid muta-
tions that were not relevant for MMR, and therefore
the reported precision is artificially low.
Results for the automatic extraction and the gold-
standard annotation are given in Table 6. As ex-
pected, there is a high rate of false positives in the
goldstandard and automatic systems; this shows that
most of the mutations detected are not relevant for
the MMR database. More interestingly, we were
able to retrieve 77.3% of relevant mutation mentions
automatically using the ML approach, which corre-
sponds to 21.3% of all the hand-curated data.
The vector classifier discriminates 1,702 mutation
cells out of a total of 27,700 unique cells in the table
collection, and it effectively identifies 153 out of the
198 relevant mutations present in the tabular data.
This means that we only need to hand-check 6.1%
of the tabular content to retrieve 77.3% of relevant
mutations, saving the curators a significant amount
of time. The classifiers could also be biased towards
higher recall by parameter tuning?this is an area for
further investigation.
Finally, after the evaluation process we observed
that many false mutation candidates could be re-
moved by discarding those that do not contain two
consecutive digits or any of the following n-grams:
?c.?, ?p.?, ?>?, ?del?, ?ins?, ?dup?. This heuristic re-
duces the number of mutation candidates from 1,702
to 989 with no cost in recall.
7 Discussion
While this is early work, our preliminary results on
the task of identifying relevant entities from gene
mutation literature show that targeting tables can be
a fruitful approach for text mining. By relying on
ML methods and simple bag-of-words features, we
were able to achieve good performance over a num-
ber of selected entities, well above header word-
matching baselines. This allowed us to identify lists
of mentions of relevant entities with minimal effort.
An advantage of our approach is that the annotation
of examples for training and evaluation is consider-
ably easier, since many entities can be annotated in
a single step, opening the way to faster annotation of
other entities of interest in the biomedical domain.
The approach of using table vector classification
for the named entity task also has promise. In partic-
ular, the wide variety and non-standard terminology
of biomedical entities (i.e. genes, proteins, muta-
tions) is one of the challenges to NER in this do-
main. However, since a column of homogeneous
information may include representatives of the het-
erogeneous nomenclature schemes, classification of
a whole column or row potentially helps nullify the
effect of the terminological variability.
For future work, we plan to study different types
of features for better representing the entities tar-
geted in this work. Specially for mutation mentions,
we observed that the presence of certain ngrams (e.g.
?del?) can be a strong indicator for this class. An-
other issue we plan to address is that of the normal-
isation of mutation mentions into a standard form,
for which we have started developing a collection
of regular expressions. Another of our goals is to
increase the size of our dataset of articles by im-
proving our web crawler, and by hand-annotating
the retrieved table vectors for further experimenta-
tion. Finally, we also aim to explore the potential of
using tabular data for NER of different entities in the
biomedical domain, such as gene mentions.
Acknowledgements NICTA is funded by the Aus-
tralian Government as represented by the Depart-
ment of Broadband, Communications and the Dig-
ital Economy and the Australian Research Council
through the ICT Centre of Excellence program.
Thanks to Mike Woods and his colleagues at the
Memorial University of Newfoundland for making
the MMR database and their curation data available
to us. Eric Huang wrote several of the scripts men-
tioned in Section 3 for creating the table collection.
53
References
C. J. O. Baker and R. Witte. 2006. Mutation mining?a
prospector?s tale. J. of Information Systems Frontiers,
8(1):45?57.
J. G. Caporaso, W. A. Baumgartner Jr., D. A. Randolph,
K. B. Cohen, and L. Hunter. 2007. Mutationfinder: A
high-performance system for extracting point mutation
mentions from text. Bioinformatics, 23(14):1862?
1865.
W. W. Cohen, M. Hurst, and L. S. Jensen. 2002. A flex-
ible learning system for wrapping tables and lists in
html documents. In WWW ?02: Proc. 11th Int?l Conf.
on World Wide Web, pages 232?241, Honolulu.
F. Horn, A. L. Lau, and F. E. Cohen. 2004. Auto-
mated extraction of mutation data from the literature:
Application of MuteXt to g protein-coupled recep-
tors and nuclear hormone receptors. Bioinformatics,
20(4):557?568.
M. Hurst. 2001. Layout and language: Challenges
for table understanding on the web. Technical report,
WhizBang!Labs.
N. Karamanis, R. Seal, I. Lewin, P. McQuilton, A. Vla-
chos, C. Gasperin, R. Drysdale, and T. Briscoe. 2008.
Natural language processing in aid of flybase curators.
BMC Bioinformatics, 9:193?204.
Lawrence C. Lee, Florence Horn, and Fred E. Cohen.
2007. Automatic extraction of protein point muta-
tions using a graph bigram association. PLoS Com-
putational Biology, 3(2):e16+, February.
K. Lerman, L. Getoor, S. Minton, and C. Knoblock.
2004. Using the structure of web sites for automatic
segmentation of tables. In SIGMOD?04, pages 119?
130, Paris.
D. Pinto, A. McCallum, X. Wei, and W. B. Croft. 2003.
Table extraction using conditional random fields. In
SIGIR ?03, pages 235?242.
X. Wei, W.B. Croft, and D. Pinto. 2004. Question
answering performance on table data. Proceedings
of National Conference on Digital Government Re-
search.
I. H. Witten and E. Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
M.O. Woods, P. Williams, A. Careen, L. Edwards,
S. Bartlett, J. McLaughlin, and H. B. Younghusband.
2007. A new variant database for mismatch repair
genes associated with lynch syndrome. Hum. Mut.,
28:669?673.
R. Zanibbi, D. Bolstein, and J. R. Cordy. 2004. A survey
of table recognition. Int?l J. on Document Analysis
and Recognition, 7(1).
54
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862?871,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Classifying Dialogue Acts in One-on-one Live Chats
Su Nam Kim,? Lawrence Cavedon? and Timothy Baldwin?
? Dept of Computer Science and Software Engineering, University of Melbourne
? School of Computer Science and IT, RMIT University
sunamkim@gmail.com, lcavedon@gmail.com, tb@ldwin.net
Abstract
We explore the task of automatically classify-
ing dialogue acts in 1-on-1 online chat forums,
an increasingly popular means of providing
customer service. In particular, we investi-
gate the effectiveness of various features and
machine learners for this task. While a sim-
ple bag-of-words approach provides a solid
baseline, we find that adding information from
dialogue structure and inter-utterance depen-
dency provides some increase in performance;
learners that account for sequential dependen-
cies (CRFs) show the best performance. We
report our results from testing using a corpus
of chat dialogues derived from online shop-
ping customer-feedback data.
1 Introduction
Recently, live chats have received attention due to
the growing popularity of chat services and the in-
creasing body of applications. For example, large
organizations are increasingly providing support or
information services through live chat. One advan-
tage of chat-based customer service over conven-
tional telephone-based customer service is that it
becomes possible to semi-automate aspects of the
interaction (e.g. conventional openings or canned
responses to standard questions) without the cus-
tomer being aware of it taking place, something that
is not possible with speech-based dialogue systems
(as synthesised speech is still easily distinguishable
from natural speech). Potentially huge savings can
be made by organisations providing customer help
services if we can increase the degree of automation
of live chat.
Given the increasing impact of live chat services,
there is surprisingly little published computational
linguistic research on the topic. There has been sub-
stantially more work done on dialogue and dialogue
corpora, mostly in spoken dialogue (e.g. Stolcke et
al. (2000)) but also multimodal dialogue systems in
application areas such as telephone support service
(Bangalore et al, 2006) and tutoring systems (Lit-
man and Silliman, 2004). Spoken dialogue analysis
introduces many complications related to the error
inherent in current speech recognition technologies.
As an instance of written dialogue, an advantage of
live chats is that recognition errors are not such an is-
sue, although the nature of language used in chat is
typically ill-formed and turn-taking is complicated
by the semi-asynchronous nature of the interaction
(e.g. Werry (1996)).
In this paper, we investigate the task of automatic
classification of dialogue acts in 1-on-1 live chats,
focusing on ?information delivery? chats since these
are proving increasingly popular as part of enter-
prise customer-service solutions. Our main chal-
lenge is to develop effective features and classifiers
for classifying aspects of 1-on-1 live chat. Much of
the work on analysing dialogue acts in spoken di-
alogues has relied on non-lexical features, such as
prosody and acoustic features (Stolcke et al, 2000;
Julia and Iftekharuddin, 2008; Sridhar et al, 2009),
which are not available for written dialogues. Pre-
vious dialogue-act detection for chat systems has
used bags-of-words (hereafter, BoW) as features
for dialogue-act detection; this simple approach
has shown some promise (e.g. Bangalore et al
(2006), Louwerse and Crossley (2006) and Ivanovic
(2008)). Other features such as keywords/ontologies
(Purver et al, 2005; Forsyth, 2007) and lexical cues
(Ang et al, 2005) have also been used for dialogue
act classification.
862
In this paper, we first re-examine BoW features
for dialogue act classification. As a baseline, we
use the work of Ivanovic (2008), which explored 1-
grams and 2-grams with Boolean values in 1-on-1
live chats in the MSN Online Shopping domain (this
dataset is described in Section 5). Although this
work achieved reasonably high performance (up to
a micro-averaged F-score of around 80%), we be-
lieve that there is still room for improvement using
BoW only. We extend this work by using ideas from
related research such as text categorization (Debole
and Sebastiani, 2003), and explore variants of BoW
based on analysis of live chats, along with feature
weighting. Finally, our main aim is to explore new
features based on dialogue structure and dependen-
cies between utterances1 that can enhance the use of
BoW for dialogue act classification. Our hypothesis
is that, for task-oriented 1-on-1 live chats, the struc-
ture and interactions among utterances are useful in
predicting future dialogue acts: for example, conver-
sations typically start with a greeting, and questions
and answers typically appear as adjacency pairs in
a conversation. Therefore, we propose new features
based on structural and dependency information de-
rived from utterances (Sections 4.2 and 4.3).
2 Related Work
While there has been significant work on classify-
ing dialogue acts, the bulk of this has been for spo-
ken dialogue. Most such work has considered: (1)
defining taxonomies of dialogue acts; (2) discover-
ing useful features for the classification task; and (3)
experimenting with different machine learning tech-
niques. We focus here on (2) and (3); we return to
(1) in Section 3.
For classifying dialogue acts in spoken dialogue,
various features such as dialogue cues, speech char-
acteristics, and n-grams have been proposed. For
example, Samuel et al (1998) utilized the charac-
teristics of spoken dialogues and examined speaker
direction, punctuation marks, cue phrases and n-
grams for classifying spoken dialogues. Jurafsky et
al. (1998) used prosodic, lexical and syntactic fea-
tures for spoken dialogue classification. More re-
cently, Julia and Iftekharuddin (2008) and Sridhar et
1An utterance is the smallest unit to deliver a participant?s
message(s) in a turn.
al. (2009) achieved high performance using acous-
tic and prosodic features. Louwerse and Cross-
ley (2006), on the other hand, used various n-gram
features?which could be adapted to both spoken
and written dialogue?and tested them using the
Map Task Corpus (Anderson et al, 1991). Extend-
ing the discourse model used in previous work, Ban-
galore et al (2006) used n-grams from the previous
1?3 utterances in order to classify dialogue acts for
the target utterance.
There has been substantially less effort on clas-
sifying dialogue acts in written dialogue: Wu et al
(2002) and Forsyth (2007) have used keyword-based
approaches for classifying online chats; Ivanovic
(2008) tested the use of n-gram features for 1-on-1
live chats with MSN Online Shopping assistants.
Various machine learning techniques have been
investigated for the dialogue classification task.
Samuel et al (1998) used transformation-based
learning to classify spoken dialogues, incorporat-
ing Monte Carlo sampling for training efficiency.
Stolcke et al (2000) used Hidden Markov Mod-
els (HMMs) to account for the structure of spo-
ken dialogues, while Wu et al (2002) also used
transformation- and rule-based approaches plus
HMMs for written dialogues. Other researchers
have used Bayesian based approaches, such as
naive Bayes (e.g. (Grau et al, 2004; Forsyth,
2007; Ivanovic, 2008)) and Bayesian networks (e.g.
(Keizer, 2001; Forsyth, 2007)). Maximum entropy
(e.g. (Ivanovic, 2008)), support vector machines
(e.g. (Ivanovic, 2008)), and hidden Markov models
(e.g. (Bui, 2003)) have also all been applied to auto-
matic dialogue act classification.
3 Dialogue Acts
A number of dialogue act taxonomies have been pro-
posed, designed mainly for spoken dialogue. Many
of these use the Dialogue Act Markup in Several
Layers (DAMSL) scheme (Allen and Core, 1997).
DAMSL was originally applied to the TRAINS cor-
pus of (transcribed) spoken task-oriented dialogues,
but various adaptations of it have since been pro-
posed for specific types of dialogue. The Switch-
board corpus (Godfrey et al, 1992) defines 42 types
of dialogue acts from human-to-human telephone
conversations. The HCRCMap Task corpus (Ander-
863
son et al, 1991) defines a set of 128 dialogue acts to
model task-based spoken conversations.
For casual online chat dialogues, Wu et al (2002)
define 15 dialogue act tags based on previously-
defined dialogue act sets (Samuel et al, 1998;
Shriberg et al, 1998; Jurafsky et al, 1998; Stolcke
et al, 2000). Forsyth (2007) defines 15 dialogue acts
for casual online conversations, based on 16 conver-
sations with 10,567 utterances. Ivanovic (2008) pro-
poses 12 dialogue acts based on DAMSL for 1-on-1
online customer service chats.
Ivanovic?s set of dialogue acts for chat dia-
logues has significant overlap with the dialogue act
sets of Wu et al (2002) and Forsyth (2007) (e.g.
GREETING, EMOTION/EXPRESSION, STATEMENT,
QUESTION). In our work, we re-use the set of dia-
logue acts proposed in Ivanovic (2008), due to our
targeting the same task of 1-on-1 IM chats, and in-
deed experimenting over the same dataset. The def-
initions of the dialogue acts are provided in Table 1,
along with examples.
4 Feature Selection
In this section, we describe our initial dialogue-act
classification experiments using simple BoW fea-
tures, and then introduce two groups of new fea-
tures based on structural information and dependen-
cies between utterances.
4.1 Bag-of-Words
n-gram-based BoW features are simple yet effec-
tive for identifying similarities between two utter-
ances, and have been used widely in previous work
on dialogue act classification for online chat di-
alogues (Louwerse and Crossley, 2006; Ivanovic,
2008). However, chats containing large amounts of
noise such as typos and emoticons pose a greater
challenge for simple BoW approaches. On the other
hand, keyword-based features (Forsyth, 2007) have
achieved high performance; however, keyword-
based approaches are more domain-dependent. In
this work, we chose to start with a BoW approach
based on our observation that commercial live chat
services contain relatively less noise; in particular,
the commercial agent tends to use well-formed, for-
mulaic prose.
Previously, Ivanovic (2008) explored Boolean 1-
gram and 2-gram features to classify MSN Online
Shopping live chats, where a user requests assis-
tance in purchasing an item, in response to which the
commercial agent asks the customer questions and
makes suggestions. Ivanovic (2008) achieved solid
performance over this data (around 80% F-score).
While 1-grams performed well (as live chat utter-
ances are generally shorter than, e.g., sentences in
news articles), we expect 2- and 3-grams are needed
to detect formulaic expressions, such as No problem
and You are welcome. We would also expect a pos-
itive effect from combining n-grams due to increas-
ing the coverage of feature words. We thus test 1-,
2- and 3-grams individually, as well as the combi-
nation of 1- and 2-grams together (i.e. 1+2-grams)
and 1-, 2- and 3-grams (i.e. 1+2+3-grams); this re-
sults in five BoW sets. Also, unlike Ivanovic (2008),
we test both raw words and lemmas; we expect the
use of lemmas to perform better than raw words as
our data is less noisy. As the feature weight, in addi-
tion to simple Boolean, we also experiment with TF,
TF?IDF and Information Gain (IG).
4.2 Structural Information
Our motivation for using structural information as
a feature is that the location of an utterance can be
a strong predictor of the dialogue act. That is, dia-
logues are sequenced, comprising turns (i.e. a given
user is sending text), each of which is made up of
one or more messages (i.e. strings sent by the user).
Structured classification methods which make use of
this sequential information have been applied to re-
lated tasks such as tagging semantic labels of key
sentences in biomedical domains (Chung, 2009) and
post labels in web forums (Kim et al, 2010).
Based on the nature of live chats, we observed that
the utterance position in the chat, as well as in a turn,
plays an important role when identifying its dialogue
act. For example, an utterance such as Hello will oc-
cur at the beginning of a chat while an utterance such
as Have a nice day will typically appear at the end.
The position of utterances in a turn can also help
identify the dialogue act; i.e. when there are several
utterances in a turn, utterances are related to each
other, and thus examining the previous utterances in
the same turn can help correctly predict the target
utterance. For example, the greeting (Welcome to ..)
and question (How may I help you?) could occur in
864
Dialogue Act, Definition and Examples
CONVENTIONAL CLOSING: Various ways of ending a conversation e.g. Bye Bye
CONVENTIONAL OPENING: Greeting and other ways of starting a conversation e.g. Hello Customer
DOWNPLAYER: A backwards-linking label often used after THANKS to down play the contribution
e.g. You are welcome, my pleasure
EXPRESSIVE: An acknowledgement of a previous utterance or an indication of the speaker?s mood.
e.g. haha, : ?) wow
NO ANSWER: A backward-linking label in the form of a negative response to a YESNO-QUESTION e.g. no, nope
OPEN QUESTION: A question that cannot be answered with only a yes or no. The answer is usually
some form of explanation or statement. e.g. how do I use the international version?
REQUEST: Used to express a speaker?s desire that the learner do something ? either performing some action
or simply waiting. e.g. Please let me know how I can assist you on MSN Shopping today.
RESPONSE ACK: A backward-linking acknowledgement of the previous utterance. Used to confirm
that the previous utterance was received/accepted. e.g. Sure
STATEMENT: Used for assertions that may state a belief or commit the speaker to doing something
e.g. I am sending you the page which will pop up in a new window on your screen.
THANKS: Conventional thanks e.g. Thank you for contacting us.
YES ANSWER: A backward-linking label in the form of an affirmative response to a YESNO-QUESTION e.g. yes, yeah
YESNO QUESTION: A closed question which can be answered in the affirmative or negative.
e.g. Did you receive the page, Customer?
Table 1: The set of dialogue acts used in this research, taken from Ivanovic (2008)
the same turn. We also noticed that identifying the
utterance author can help classify the dialogue act
(previously used in Ivanovic (2008)).
Based on these observations, we tested the follow-
ing four structural features:
? Author information,
? Relative position in the chat,
? Author + Relative position,
? Author + Turn-relative position among utter-
ances in a given turn.
We illustrate our structural features in Table 2,
which shows an example of a 1-on-1 live chat. The
participants are the agent (A) and customer (C); Uxx
indicates an utterance (U) with ID number xx. This
conversation has 42 utterances in total. The relative
position is calculated by dividing the utterance num-
ber by the total number of utterances in the dialogue;
the turn-relative position is calculated by dividing
the utterance position by the number of utterances
in that turn. For example, for utterance 4 (U4), the
relative position is 442 , while its turn-relative position
is 23 since U4 is the second utterance among U3,4,5
that the customer makes in a single turn.
4.3 Utterance Dependency
In recent work, Kim et al (2010) demonstrated the
importance of dependencies between post labels in
web forums. The authors introduced series of fea-
tures based on structural dependencies among posts.
They used relative position, author information and
automatically predicted labels from previous post(s)
as dependency features for assigning a semantic la-
bel to the current target post.
Similarly, by examining our chat corpus, we ob-
served significant dependencies between utterances.
First, 1-on-1 (i.e. agent-to-user) dialogues often con-
tain dependencies between adjacent utterances by
different authors. For example, in Table 2, when the
agent asks Is that correct?, the expected response
from the user is a Yes or No. Another example is
that when the agent makes a greeting, such as Have
a nice day, then the customer will typically respond
with a greeting or closing remark, and not a Yes or
No. Second, the flow of dialogues is in general co-
hesive, unless the topic of utterances changes dra-
matically (e.g. U5: Are you still there?, U22: brb
in 1 min in Table 2). Third, we observed that be-
tween utterances made by the same author (either
agent or user), the target utterance relies on previous
utterances made by the same author, especially when
865
ID Utterance
A:U1 Hello Customer, welcome to MSN Shopping.
A:U2 My name is Krishna and I am your
online Shopping assistant today.
C:U3 Hello!
C:U4 I?m trying to find a sports watch.
C:U5 are you still there?
A:U6 I understand that you are looking for sports
watch.
A:U7 Is that correct?
C:U8 yes, that is correct.
..
C:U22 brb in 1 min
C:U23 Thank you for waiting
..
A:U37 Thank you for allowing us to assist
you regarding wrist watch.
A:U38 I hope you found our session today helpful.
A:U39 If you have any additional questions or
you need additional information,
please log in again to chat with us.
We are available 24 hours a day, 7 days a
week for your help.
A:U40 Thank you for contacting MSN Shopping.
A:U41 Have a nice day! Good Bye and Take Care.
C:U42 You too.
Table 2: An example of a 1-on-1 live chat, with turn and
utterance structure
the agent and user repeatedly question and answer.
With these observations, we checked the likelihood
of dialogue act pairings between two adjacent utter-
ances, as well as between two adjacent utterances
made by the same author. Overall, we found strong
co-occurrence (as measured by number of occur-
rences of labels across adjacency pairs) between cer-
tain pairs of dialogue acts (e.g. (YESNO QUESTION
?YES ANSWER/NO ANSWER) and (REQUEST
?YES ANSWER)). STATEMENT, on the other
hand, can associate with most other dialogue acts.
Based on this, we designed the following five ut-
terance dependency features; by combining these,
we obtain 31 feature sets.
1. Dependency of utterances regardless of author
(a) Dialogue act of previous utterance
(b) Accumulated dialogue act(s) of previous
utterances
(c) Accumulated dialogue acts of previous ut-
terances in a given turn
2. Dependency of utterances made by a single au-
thor
(a) Dialogue act of previous utterance
by same author; a dialogue act can be in
the same turn or in the previous turn
(b) Accumulated dialogue acts of previous
utterances by same author; dialogue acts
can be in the same turn or in the previous
turn
To capture utterance dependency, Bangalore et al
(2006) previously used n-gram BoW features from
the previous 1?3 utterances. In contrast, instead of
using utterances which indirectly encode dialogue
acts, we directly use the dialogue act classifications,
as done in Stolcke et al (2000). The motivation is
that, due to the high performance of simple BoW
features, using dialogue acts directly would cap-
ture the dependency better than indirect information
from utterances, despite introducing some noise. We
do not build a probabilistic model of dialogue tran-
sitions the way Stolcke et al (2000) does, but follow
an approach similar to that used in Kim et al (2010)
in using predicted dialogue act(s) labels learned in
previous step(s) as a feature.
5 Experiment Setup
As stated earlier, we use the data set from Ivanovic
(2008) for our experiments; it contains 1-on-1 live
chats from an information delivery task. This dataset
contains 8 live chats, including 542 manually-
segmented utterances. The maximum and minimum
number of utterances in a dialogue are 84 and 42,
respectively; the maximum number of utterances in
a turn is 14. The live chats were manually tagged
with the 12 dialogue acts described in Section 3.
The utterance distribution over the dialogue acts is
described in Table 3.
For our experiments, we calculated TF, TF?IDF
and IG (Information Gain) over the utterances,
which were optionally lemmatized with the morph
tool (Minnen et al, 2000). We then built a dialogue
act classifier using three different machine learn-
ers: SVM-HMM (Joachims, 1998),2 naive Bayes
2http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html
866
Dialogue Act Utterance number
CONVENTIONAL CLOSING 15
CONVENTIONAL OPENING 12
DOWNPLAYER 15
EXPRESSIVE 5
NO ANSWER 12
OPEN QUESTION 17
REQUEST 28
RESPONSE ACK 27
STATEMENT 198
THANKS 79
YES ANSWER 35
YESNO QUESTION 99
Table 3: Dialogue act distribution in the corpus
Index Learner Ours Ivanovic
Feature Acc. Feature Acc.
Word SVM 1+2+3/B .790 1/B .751
NB 1/B .673 1/B .673
CRF 1/IG .839 1/B .825
Lemma SVM 1+2+3/IG .777 N/A N/A
NB 1/B .672 N/A N/A
CRF 1/B .862 N/A N/A
Table 4: Best accuracy achieved by the different learn-
ers over different feature sets and weighting methods (1
= 1-gram; 1+2+3 = 1/2/3-grams; B = Boolean; IG = in-
formation gain)
from the WEKA machine learning toolkit (Wit-
ten and Frank, 2005), and Conditional Random
Fields (CRF) using CRF++.3 Note that we chose
to test CRF and SVM-HMM as previous work (e.g.
(Samuel et al, 1998; Stolcke et al, 2000; Chung,
2009)) has shown the effectiveness of structured
classification models on sequential dependencies.
Thus, we expect similar effects with CRF and SVM-
HMM. Finally, we ran 8-fold cross-validation using
the feature sets described above (partitioning across
the 8 sessions). All results are presented in terms
of classification accuracy. The accuracy of a zero-R
(i.e. majority vote) baseline is 0.36.
6 Evaluation
6.1 Testing Bag-of-Words Features
Table 4 shows the best accuracy achieved by the dif-
ferent learners, in combination with BoW represen-
3http://crfpp.sourceforge.net/
n-gram Boolean TF TF?IDF IG
1 .731 .511 .517 .766
2 .603 .530 .601 .614
3 .474 .463 .472 .482
1+2 .756 .511 .522 .777
1+2+3 .773 .511 .528 .777
Table 5: Accuracy of different feature representations and
weighting methods for SVM-HMM
tations and feature weighting methods. Note that the
CRF learner ran using 1-grams only, as CRF++ does
not accept large numbers of features. As a bench-
mark, we also tested the method in Ivanovic (2008)
and present the best performance over words (rather
than lemmas). Overall, we found using just 1-grams
produced the best performance for all learners, al-
though SVM achieved the best performance when
using all three n-gram orders (i.e. 1+2+3). Since the
utterances are very short, 2-grams or 3-grams alone
are too sparse to be effective. Among the feature
weighting methods, Boolean and IG achieved higher
accuracy than TF and TF?IDF. Likewise, due to the
short utterances, simple Boolean values were often
the most effective. However, as IG was computed
using the training data, it also achieved high perfor-
mance. When comparing the learners, we found that
CRF produced the best performance, due to its abil-
ity to capture inter-utterance dependencies. Finally,
we confirmed that using lemmas results in higher ac-
curacy.
Table 5 shows the accuracy over all feature sets;
for brevity, we show this for SVM only since the
pattern is similar across all learners.
6.2 Using Structural Information
In this section, we describe experiments using struc-
tural information?i.e. author and/or position?with
BoWs. As with the base BoW technique, we used
1-gram lemmas with Boolean values, based on the
results from Section 6.1. Table 6 shows the results:
Pos indicates the relative position of an utterance in
the whole dialogue, Author means author informa-
tion, and Posturn indicates the relative position of
the utterance in a turn. All methods outperformed
the baseline; methods that surpassed the results for
the simple BoW method (for the given learner) at a
867
Feature Learners
CRF SVM NB
BoW .862 .731 .672
BoW+Author .860 .655 .649
BoW+Pos .862 .721 .655
BoW+Posabsolute .863 .631 .524
BoW+Author+Pos .875 .700 .642
BoW+Author+Posturn .871 .651 .631
Table 6: Accuracy with structural information
level of statistical significance (based on randomised
estimation, p < 0.05) are boldfaced.
Overall, using CRFs with Author and Position in-
formation produced better performance than using
BoW alone. Clearly, the ability of CRFs to natively
optimise over structural dependencies provides an
advantage over other learners.
Relative position cannot of course be measured
directly in an actual online application; hence Ta-
ble 6 also includes the use of ?absolute position? as
a feature. We see that, for CRF, the absolute posi-
tion feature shows an insignificant drop in accuracy
as compared to the use of relative position. (How-
ever, we do see a significant drop in performance
when using this feature with SVM and NB.)
6.3 Using Utterance Dependency
We next combined the inter-utterance dependency
features with the BoW features. Since we use the
dialogue acts directly in utterance dependency, we
first experimented using gold-standard dialogue act
labels. We also tested using the dialogue acts which
were automatically learned in previous steps.
Table 7 shows performance using both the gold-
standard and learned dialogue acts. The differ-
ent features listed are as follows: LabelList/L in-
dicates those corresponding to all utterances in
a dialogue preceding the target utterance; Label-
Prev/P indicates a dialogue act from a previous
utterance; LabelAuthor/A indicates a dialogue act
from a previous utterance by the same author;
and LabelPrevt/LabelAuthort indicates the previ-
ous utterance(s) and previously same-authored ut-
terance(s) in a turn, respectively. Since the accuracy
for SVM and NB using learned labels is similar to
that using gold standard labels, for brevity we report
Features Dialogue Acts
Goldstandard Learned
CRF HMM NB CRF
BoW .862 .731 .672 .862
BoW+LabelList(L) .795 .435 .225 .803
BoW+LabelPrev(P) .875 .661 .364 .876
BoW+LabelAuthor(A) .865 .633 .559 .865
BoW+LabelPrevt(Pt) .873 .603 .557 .873
BoW+LabelAuthort(At) .862 .587 .535 .851
BoW+L+P .804 .428 .227 .808
BoW+L+A .799 .404 .225 .804
BoW+L+Pt .803 .413 .229 .804
BoW+L+At .808 .408 .216 .801
BoW+P+A .873 .631 .517 .869
BoW+P+Pt .878 .579 .539 .875
BoW+P+At .871 .603 .519 .867
BoW+A+Pt .847 .594 .519 .849
BoW+A+At .869 .594 .530 .871
BoW+Pt+At .871 .592 .519 .867
BoW+L+P+A .812 .419 .231 .804
BoW+L+P+Pt .816 .423 .229 .812
BoW+L+P+At .808 .397 .225 .806
BoW+L+A+Pt .810 .388 .225 .810
BoW+L+A+At .812 .415 .216 .801
BoW+L+Pt+At .810 .375 .205 .816
BoW+P+A+Pt .875 .602 .522 .876
BoW+P+A+At .862 .609 .511 .864
BoW+P+Pt+At .873 .594 .515 .867
BoW+A+Pt+At .865 .594 .517 .864
BoW+L+P+A+Pt .817 .410 .231 .810
BoW+L+P+A+At .814 .411 .223 .810
BoW+L+P+Pt+At .816 .382 .205 .806
BoW+L+A+Pt+At .812 .406 .203 .808
BoW+P+A+Pt+At .865 .583 .513 .865
BoW+L+P+A+Pt+At .816 .399 .205 .803
Table 7: Accuracy for the different learners with depen-
dency features
the performance for CRF using learned labels only.
Results that exceed the BoW accuracy at a level of
statistical significance (p < 0.05) are boldfaced.
Utterance dependency features worked well in
combination with CRF only. Individually, Prev and
Prevt (i.e. BoW+P+Pt) helped to achieve higher ac-
curacies, and the Author feature was also benefi-
cial. However, List decreased the performance, as
the flow of dialogues can change, and when a larger
history of dialogue acts is included, it tends to in-
troduce noise. Comparing use of gold-standard and
learned dialogue acts, the reduction in accuracy was
not statistically significant, indicating that we can
868
Feature CRF SVM NB
C+LabelList .9557 .4613 .2565
C+LabelPrev .9649 .6365 .5720
C+LabelAuthor .9686 .6310 .5424
C+LabelPrevt .9686 .5738 .5738
C+LabelAuthort .9561 .6125 .5332
Table 8: Accuracy with Structural and Dependency Infor-
mation: C means lemmatized Unigram+Position+Author
achieve high performance on dialogue act classifi-
cation even with interactively-learned dialogue acts.
We believe this demonstrates the robustness of the
proposed techniques.
Finally, we tested the combination of features
from structural and dependency information. That
is, we used a base feature (unigrams with Boolean
value), relative position, author information, com-
bined with each of the different dependency features
? LabelList, LabelPrev, LabelAuthor, LabelPrevt
and LabelAuthort.
Table 8 shows the performance when using these
combinations, for each dependency feature. As we
would expect, CRFs performed well with the com-
bined features since CRFs can incorporate the struc-
tural and dependency information; the achieved the
highest accuracy of 96.86%.
6.4 Error Analysis and Future Work
Finally, we analyzed the errors of
the best-performing feature set (i.e.
BoW+Position+Author+LabelAuthor). In Ta-
ble 9, we present a confusion matrix of errors,
for CONVENTIONAL CLOSING (Cl), CON-
VENTIONAL OPENING (Op), DOWNPLAYER
(Dp), EXPRESSIVE (Ex), NO ANSWER (No),
OPEN QUESTION (Qu), REQUEST (Rq), RE-
SPONSE ACK (Ack), STATEMENT (St), THANKS
(Ta), YES ANSWER (Yes), and YESNO QUESTION
(YN). Rows indicate the correct dialogue acts and
columns indicate misclassified dialogue acts.
Looking over the data, STATEMENT is a common
source of misclassification, as it is the majority class
in the data. In particularly, a large number of RE-
QUEST and RESPONSE ACK utterances were tagged
as STATEMENT. We did not include punctuation
such as question marks in our feature sets; includ-
ing this would likely improve results further.
In future work, we plan to investigate methods for
automatically cleansing the data to remove typos,
and taking account of temporal gaps that can some-
times arise in online chats (e.g. in Table 2, there is
a time gap between C:U22 brb in 1 min and C:U23
Thank you for waiting).
7 Conclusion
We have explored an automated approach for classi-
fying dialogue acts in 1-on-1 live chats in the shop-
ping domain, using bag-of-words (BoW), structural
information and utterance dependency features. We
found that the BoW features perform remarkably
well, with slight improvements when using lemmas
rather than words. Including structural and inter-
utterance dependency information further improved
performance. Of the learners we experimented with,
CRFs performed best, due to their ability to natively
capture sequential dialogue act dependencies.
Acknowledgements
This research was supported in part by funding from
Microsoft Research Asia.
References
J.Allen and M.Core. Draft of DAMSL: Dialog Act
Markup in Several Layers. The Multiparty Dis-
course Group. University of Rochester, Rochester,
USA. 1997.
A. Anderson, M. Bader, E. Bard, E. Boyle G.M. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,
J. Miller, C. Sotillo, H.S. Thompson, R. and Weinert.
The HCRC Map Task Corpus. Language and Speech.
1991, 34, pp. 351?366.
J. Ang, Y. Liu and E. Shriberg. Automatic Dialog Act
Segmentation and Classification in Multiparty Meet-
ings. IEEE International Conference on Acoustics,
Speech, and Signal Processing. 2005, pp, 1061?1064.
S. Bangalore, G. Di Fabbrizio and A. Stent. Learning
the Structure of Task-Driven Human-Human Dialogs.
Proceedings of the 21st COLING and 44th ACL. 2006,
pp. 201?208.
H. H. Bui. A general model for online probabilistic plan
recognition. IJCAI. 2003, pp. 1309?1318.
G.Y Chung. Sentence retrieval for abstracts of random-
ized controlled trials. BMC Medical Informatics and
Decision Making. 2009, 9(10), pp. 1?13.
F. Debole and F. Sebastiani. Supervised term weighting
for automated text categorization. 18th ACM Sympo-
sium on Applied Computing. 2003, pp. 784?788.
869
Cl Op Dp Ex No Qu Rq Ack St Ta Yes YN
Op 0 0 0 0 0 0 0 0 0 0 0 2
Cl 0 0 0 0 0 0 0 0 1 1 0 0
Dp 0 0 0 0 0 0 0 0 0 0 0 0
Ex 0 0 0 0 0 0 0 0 0 0 0 0
No 0 0 0 0 0 0 0 0 0 0 0 0
Qu 0 0 0 0 0 0 0 0 0 0 0 0
Rq 0 0 0 0 0 0 0 0 3 0 0 0
Ack 0 0 1 0 1 0 0 0 5 0 0 0
St 0 0 0 0 0 0 1 0 0 0 0 0
Ta 1 0 0 0 0 0 0 0 0 0 0 0
Yes 0 0 0 0 0 0 0 0 0 0 0 0
YN 0 1 0 0 0 0 0 0 0 0 0 0
Table 9: Confusion matrix for errors from the CRF with BoW+Position+Author+LabelAuthor (rows = correct clas-
sification; columns = misclassification; CONVENTIONAL CLOSING = Cl; CONVENTIONAL OPENING = Op; DOWN-
PLAYER = Dp; EXPRESSIVE = Ex; NO ANSWER = No; OPEN QUESTION = Qu; REQUEST = Rq; RESPONSE ACK
= Ack; STATEMENT = St; THANKS = Ta; YES ANSWER = Yes; and YESNO QUESTION = YN)
E. N. Forsyth. Improving Automated Lexical and Dis-
course Analysis of Online Chat Dialog. Master?s the-
sis. Naval Postgraduate School, 2007.
J. Godfrey and E. Holliman and J. McDaniel. SWITCH-
BOARD: Telephone speech corpus for research and
development. Proceedings of IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing. 1992, pp. 517?520.
S. Grau, E. Sanchis, M. Jose and D. Vilar. Dialogue act
classification using a Bayesian approach. Proceedings
of the 9th Conference on Speech and Computer. 2004.
P. A. Heeman and J. Allen. The Trains 93 Dialogues.
Trains Technical Note 94-2. Computer Science Dept.,
University of Rochester, March 1995.
T. Joachims. Text categorization with support vector ma-
chines: Learning with many relevant features. Pro-
ceedings of European Conference on Machine Learn-
ing. 1998, pp. 137?142.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker and P. Maloor.
MATCH: An Architecture for Multimodal Dialogue
Systems. Proceedings of 40th ACL. 2002, pp. 376?
383.
F. N. Julia and K. M. Iftekharuddin. Dialog Act clas-
sification using acoustic and discourse information of
MapTask Data. Proceedings of the International Joint
Conference on Neural Networks. 2008, pp. 1472?
1479.
D. Jurafsky, E. Shriberg, B Fox and T. Curl. Lexical,
Prosodic, and Syntactic Cues for Dialog Acts. Pro-
ceedings of ACL/COLING-98 Workshop on Discourse
Relations and Discourse Markers. 1998, pp. 114?120.
E. Ivanovic. Automatic instant messaging dialogue us-
ing statistical models and dialogue acts. Master?s The-
sis. The University of Melbourne. 2008.
S. Keizer. A Bayesian Approach to Dialogue Act Clas-
sification. 5th Workshop on Formal Semantics and
Pragmatics of Dialogue. 2001, pp. 210?218.
S.N. Kim and L. Wang and T. Baldwin. Tagging and
Linking Web Forum Posts. Fourteenth Conference on
Computational Natural Language Learning. 2010.
J. Lafferty, A. McCallum and F. Pereira. Conditional
random fields: Probabilistic models for segmenting
and labeling sequence data. Proceedings of ICML.
2001, pp. 282?289.
D. J. Litman and S. Silliman. ITSPOKE: An Intelligent
Tutoring Spoken Dialogue SYstem. Proceedings of
the HLT/NAACL. 2004.
M. M. Louwerse and S. Crossley. Dialog Act Classifica-
tion Using N -Gram Algorithms. FLAIRS Conference,
2006, pp. 758?763.
G. Minnen, J. Carroll and D. Pearce. Applied morpho-
logical processing of English Natural Language Engi-
neering 2000, 7(3), pp. 77?80.
M. Purver, J. Niekrasz and S. Peters. Ontology-Based
Discourse Understanding for a Persistent Meeting As-
sistant. Proc. CHI 2005 Workshop on The Virtuality
Continuum Revisited. 2005.
K. Samuel, Sandra Carberry and K. Vijay-Shanker. Dia-
logue Act Tagging with Transformation-Based Learn-
ing. Proceedings of COLING/ACL 1998. 1998, pp.
1150-1156.
E. Shriberg, R. Bates, P. Taylor, A. Stolcke, D. Jurafsky,
K. Ries, N. Coccaro, R. Martin, M. Meteer and C. Van
870
Ess-Dykema. Can Prosody Aid the Automatic Clas-
sification of Dialog Acts in Conversational Speech?.
Language and Speech. 1998, 41(3-4), pp. 439?487.
V. R. Sridhar, S. Bangalore and S. Narayanan. Combin-
ing lexical, syntactic and prosodic cues for improved
online dialog act tagging. Computer Speech and Lan-
guage. 2009, 23(4), pp. 407?422.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema
and M. Meteer. Dialogue Act Modeling for Automatic
Tagging and Recognition of Conversational Speech.
Computational Linguistics. 2000, 26(3), pp. 339?373.
A. Stolcke and E. Shriberg. Markovian Combination of
Language and Prosodic Models for better Speech Un-
derstanding and Recognition . Invited talk at the IEEE
Workshop on Speech Recognition and Understanding,
Madonna di Campiglio, Italy, December 2001 2001,
C. C. Werry. Linguistic and interactional features of In-
ternet Relay Chat. In S. C. Herring (ed.). Computer-
Mediated Communication. Benjamins, 1996.
I. Witten and E. Frank. Data Mining: Practical Machine
Learning Tools and Techniques. Morgan Kaufmann,
2005.
T. Wu, F. M. Khan, T. A. Fisher, L. A. Shuler and W. M.
Pottenger. Posting act tagging using transformation-
based learning. Proceedings of the Workshop on Foun-
dations of Data Mining and Discovery, IEEE Interna-
tional Conference on Data Mining. 2002.
871
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 89?97,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Generating Shifting Sentiment for a Conversational Agent
Simon Whitehead
University of Melbourne, Australia
srwhitehead@gmail.com
Lawrence Cavedon
RMIT University, Australia
lawrence.cavedon@rmit.edu.au
Abstract
We investigate techniques for generating al-
ternative output sentences with varying sen-
timent, using (an approximation to) the
Valentino method, based on SentiWordNet, of
Guerini et al We extend this method by filter-
ing out unacceptable candidate sentences, us-
ing bigrams sourced from different corpora to
determine whether lexical substitutions are ap-
propriate in the given context. We also com-
pare the generated candidates against human
judgements of whether the desired sentiment
shift has occurred: our results suggest limi-
tations with the overall knowledge-based ap-
proach, and we propose potential directions
for improvement.
1 Introduction
The design of more natural or believable conver-
sational agents (Bates, 1994; Pelachaud and Bilvi,
2003) requires the need for such agents to communi-
cate affectively, by the display of emotion or attitude
towards objects, other agents, or states of affairs.
More engaging or influential agents may seek to ac-
tually affect their conversational partner at a deeper
level, for example, by influencing their emotional
state (van der Sluis and Mellish, 2008). Previous
work in this area has explored the use of gestures and
facial expression (Caridakis et al, 2007) and rhythm
and prosody of speech (Zovato et al, 2008) for ex-
pressing affect; however there has been little work
on generation of affective language in dialogue.
Our general approach is inspired by (Fleischman
and Hovy, 2002)?s work on generating different
surface-level versions of utterance content, depend-
ing on an agent?s appraisals towards objects, char-
acters and events in its environment. While their
approach is effective, it relies on manual creation
of lexical alternatives, customized to the application
domain. We are interested in approaches that will
scale, and can be applied domain-independently.
While our ultimate aim is generation of language
that relects emotional state, in this work we in-
vestigate the automatic generation of varying ?sen-
timent? in output utterances; we focus on senti-
ment mainly due to the recent development of use-
ful resources for this task. (Guerini et al, 2008)?s
Valentino system is an approach to automatically
generating candidate output utterances with differ-
ent sentiment from an original; the authors suggest
ECAs as a possible application scenario for their
techniques. We explore this suggestion, implement-
ing a lexical substitution (McCarthy and Navigli,
2007) approach to dialogue generation with sen-
timent, using the Valentino approach and associ-
ated resources. Lexical substitution approaches raise
well-known challenges, and we investigate a number
of techniques to address these in Section 4; for ex-
ample, using bigrams and grammatical relations to
determine which substitutions are acceptable based
on their context in a sentence.1
Our techniques show improvement over naive lex-
ical substitution; however, an evaluation with human
subjects suggests that a deeper problem is that even
?acceptable? candidate sentences generated by the
method do not match human judgements with re-
spect to sentiment shift: i.e., alternatives labeled as
more positive (resp., negative) than the original by
the system are often seen as a sentiment shift in the
opposite direction by human judges (Section 5).
2 Background: Valentino
The Valentino2 system (Guerini et al, 2008) is a
tool developed from WordNet and SentiWordNet
1Guerini et al suggest this as an area for further work.
2VALENced Text INOculator
89
designed to produce more positively or negatively
slanted versions of text. Input to the system consists
of a short sentence, and a target valence (between
-1 and 1), which indicates the desired polarity and
magnitude of sentiment in the modified output text.
Valentino uses a number of strategies for adding, re-
moving, or substituting certain words in order to al-
ter the overall sentiment of the sentence. Table 1
shows examples of Valentino output for different tar-
get valences, with modifications in italics.
To perform the word-substitution, (Guerini et al,
2008) created a resource of OVVTs3: vectors of se-
mantically related terms which may substitute for
one another. The OVVTs were constructed us-
ing structural analysis of WordNet, and are divided
into adjectives, nouns, and verbs. (Guerini et al,
2008) also constructed a separate resource of Mod-
ifier OVVTs which list adverbs that can be used to
modify verbs. Modifier OVVTs were created using
verbs extracted from certain FrameNet4 categories,
then recording which adverbs occur next to these
verbs in the British National Corpus (BNC). Each
term in the Valentino resource was assigned a senti-
ment valence, which corresponds to the SentiWord-
Net score of its parent WordNet synset. Table 2
shows part of an OVVT containing the noun ?man?.5
Term POS Sense Valence
hunk n 1 0.375
man n 1 0
dude n 1 -0.125
beau n 2 -0.125
Table 2: (Abridged) example of an OVVT
To generate a modified sentence, (Guerini et al,
2008) apply the following strategies to each word6
until the sentence valence (total of term valences)
meets the target:
1. Paraphrase: Lemmas with only one sense
are replaced by their WordNet gloss, which is
scored for sentiment using the OVVTs;
3We assume OVVT stands for Ordered Vector of Valenced
Terms; this is not explicit in (Guerini et al, 2008).
4http://framenet.icsi.berkeley.edu/
5All our examples and evaluations are using a version of the
OVVTs made available by Marco Guerini on May 13, 2009.
6Actually, to the lemma of each word.
2. Use of most frequent senses: The OVVTs are
searched using only the most frequent senses;
3. Adjective modification: Adjectives are re-
placed with their stronger/weaker alternatives
such that the target valence is not exceeded;
4. Verb modification: Verbs are modified by in-
serting, removing, or replacing intensifier or
downtoner adverbs.
The final sentence is rendered as surface text by
transforming each of the inserted lemmas back into
the original morphology.
(Guerini et al, 2008) suggest their system?s po-
tential application to dialogue generation in an ECA,
enabling emotional variation. However, they do not
present an evaluation of Valentino?s effectiveness.
We expect that not all output utterances generated
using their method will be sensible in the context of
a believable ECA, for the following reasons:
Unconventional Word Usage: Upon inspection,
we found the OVVTs often contain several
words which are no longer conventionally used
(e.g. ?beau?). For an ECA to be believable, we
hypothesise that such unpopular words should
not be considered as potential candidates for
substitution.
Incorrect Grammatical Context: The naive ver-
sion of the Valentino method assumes that all
words in an OVVT can be substituted for one
another regardless of their context in the sen-
tence (see Table 3); Guerini et al propose this
as an area for future work. We explore semi-
informed solutions using bigrams and gram-
matical relations to eliminate syntactically in-
correct substitutions.
... Williams was not interested (in) girls
... Williams was not concerned (with) girls
... Williams was not fascinated (by) girls
Table 3: Illustration of grammatical context issues
3 Implementation
We implemented a lexical substitution approach to
varying valence, closely following the Valentino ap-
proach described in (Guerini et al, 2008). We did
90
Valence Sentence
n/a Bob admitted that John is absolutely the best guy
1.0 Bob wholeheartedly admitted that John is absolutely a superb hunk
0.5 Bob openly admitted that John is highly the redeemingest signor
0.0 Bob admitted that John is highly a well-behaved sir
-0.5 Bob sadly confessed that John is nearly a well-behaved beau
-1.0 Bob harshly confessed that John is pretty an acceptable eunuch
Table 1: Example of Valentino sentiment shifting (Guerini et al, 2008)
not implement all the above strategies?in partic-
ular, we did not implement paraphrasing, adverb
modification, or morphology synthesis; rather we
focused on developing techniques that would ad-
dress the lexical substitution issues described above.
As with Valentino, we calculate sentence valence
by summing the valences of all terms in the sentence
which are present in the OVVTs7. However, as a
variation on Valentino, we aggregated sentence shift
into five broad categories: ?major positive shift?;
?minor positive shift?; ?no shift?; ?minor negative
shift?; ?major negative shift?.
Since most OVVTs contain only lemmas, we first
performed lemmatisation using the MorphAdorner8
package. To locate a term in the OVVTs, we first
search for the original word morphology, then if no
match is found we try using the lemma.
As with (Guerini et al, 2008), we included candi-
dates frommultiple senses of a matching word; how-
ever, rather than stopping at the third most frequent
sense, we explored up to sense forty so as to increase
the number of possible substitutions for terms.9 We
performed a very naive version of word sense dis-
ambiguation (WSD) (see below), but lack of WSD
was an issue (discussed later).
Alternative sentences were generated by modify-
ing at most a single word; this reduces the explo-
sion in the number of alternatives, but the methods
described could just as easily apply to alternatives
constructed by varying multiple words.
The novel aspect of our implementation was the
?candidate filtering? techniques: i.e. techniques
for deciding whether to accept a candidate replace-
7Since we ignore adverbs, we do not include these when
scoring a sentence.
8http://morphadorner.northwestern.edu/
9Increasing this further increased the number of alternatives
but did not improve performance.
ment term as substitute in a given sentence; this was
specifically designed to address the issues above. In
the next section, we describe filtering techniques us-
ing simple bigrams and grammatical relations, and
evaluate the effectiveness of each.
4 Evaluation: Candidate Filtering
The data set we used for this evaluation consisted of
25 sentences, randomly extracted from the BNC.10
The sentences were sourced from the BNC to avoid
any bias which may have been introduced had the
test sentences been created manually. We required
that each test sentence satisfy the following condi-
tions11:
1. The sentence must contain between 6 and 10
words (to reflect length of a typical dialogue
utterance);
2. The sentence must contain at least one term
which is found in the OVVTs (otherwise it
would be pointless for evaluation purposes);
the term may have any valence.12
Our second filtering technique requires informa-
tion about the grammatical relations between terms
in a sentence (illustrated in Figure 1). For this, we
used a version of the BNC which was pre-processed
with the RASP parser (Briscoe et al, 2006).
Our gold standard for candidate acceptability was
created using the first author?s judgements.13 In or-
10The size of our test data set was capped at 25 due to the time
required to create the gold standard (i.e., judging 1030 substitu-
tions consistently).
11These constraints reduced our sample set from the ?4.6
million sentences in the BNC to approx. 627,000 sentences.
12The sentence can theoretically be valence-shifted by sub-
stituting that term, regardless of the term?s valence.
13With more time we would of course have preferred to use
multiple annotators. However, the judgement task was simple
91
der to be judged as an ACCEPT by the annotator,
a generated sentence needed to satisfy the following
criteria (otherwise it was labelled REJECT):
1. Semantic Equivalence: The new sentence
should convey reasonably equivalent semantics
compared to the original: e.g., phrases such as
?young boy? and ?small boy? were considered
acceptably close;14
2. Grammatical Correctness: The new sentence
should not contain grammatical errors. For the
gold standard, terms were manually converted
into their original morphological form before
annotation (e.g., if the lemma ?speak? replaced
an instance of ?shouted?, then it was converted
to ?spoke?).
4.1 Evaluation Methodology
To evaluate each candidate selection method, we
performed the following procedure for each of our
25 test sentences:
1. Find all matching15 terms and retrieve the va-
lence score of each;
2. For each matching term:
(a) Retrieve the corresponding list of alterna-
tive terms from the OVVTs;
(b) Generate several different candidate sen-
tences by substituting each alternative
term into the original sentence;
(c) Apply the chosen candidate selection
technique to each generated sentence, and
label each as ACCEPT or REJECT (for
step 3);
3. Compare all system classifications to our gold
standard (automatically), and mark each as ei-
ther a true positive (TP), false positive (FP),
true negative (TN), or false negative (FN).
We then used the TP, FP, TN and FN counts to
compute the accuracy, precision, recall and F-score
enough for us to believe it to be reliable.
14A fairly liberal view of ?semantic equivalence? was taken;
for example, for our purposes we consider all sentences in Table
1 to be more-or-less semantically equivalent.
15A matching term is defined as a term which has a corre-
sponding entry in the OVVTs.
across all generated sentences. These metrics are
used to compare the relative performance between
each of our candidate selection methods.
We describe each of our techniques and the re-
sults; we present all the measurements in a single
table (Table 5).16
4.2 Candidate filtering using bigrams
For each candidate sentence generated, we exam-
ined the bigrams including the newly substituted
term. If both17 bigrams appear in the BNC, we take
this as an indication that the substitution is accept-
able, and we accept the candidate sentence. Other-
wise, the candidate is rejected. We pre-processed the
BNC to extract 8,463,295 unique bigrams, formatted
as lemma/pos lemma/pos pairs, where lemma
is the lemmatised word, and pos is the WordNet
POS. As a simple attempt to address word-sense dis-
ambiguation, we discriminated on POS18 when ex-
tracting and matching these bigrams. For example,
?drive/n home/n? and ?drive/v home/n?
would be considered separate bigrams, as the term
?drive? occurs with different POS in each. We chose
to lemmatise all bigrams due to the relatively small
size of the BNC. Also, we did not consider bigrams
which are interrupted by sentence punctuation, as
this indicates a phrase break.
We take this bigram approach as our base-
line.19 This simple technique has reasonable accu-
racy (0.752: see Table 5) but this is due largely to the
high number of true negatives produced. The false
negatives are mainly caused by the BNC?s relatively
limited bigram coverage.
To address this issue, we sourced our bigrams
from the Google Web 1T Corpus, which covers
approximately one trillion words of English text
sourced from publicly accessible web pages. Com-
pared with the BNC, it has much greater coverage,
containing ?314 million bigrams. However, Web
1T does not contain POS information, and due to
its size we did not lemmatise the bigrams. Using a
16Note that had we performed no filtering, all TN?s would
become FP?s and all FN?s would befome TP?s.
17For terms beginning/ending a sentence (or phrase sur-
rounded by punctuation), we only examine one bigram.
18We differentiated only adjectives, nouns, verbs, and ad-
verbs; all other POS were considered equivalent for the pur-
poses of bigram extraction.
19A lower baseline would be to perform no filtering.
92
smaller corpus, these differences may reduce cov-
erage and bigram matching accuracy. However we
hypothesise that using the Web 1T corpus, such lim-
itations should be outweighed by its sheer size.
From Table 5, we see a substantial increase in re-
call over our previous baseline, which supports our
hypothesis that using a larger corpus would increase
true positives and reduce false negatives. However,
the increased coverage of the Web 1T corpus brings
with it more opportunities for false positives, the
number of which has increased dramatically from
our baseline, causing a reduction in precision and
accuracy. Despite this, due to increased recall, we
achieved an improvement in overall F-score.
Due to its web-based nature, the Web 1T corpus
will contain more errors than a corpus sourced from
published print, such as the BNC. Bigrams which
occur infrequently may be a source of noise. We
hypothesized that a substitution is acceptable if its
replacement bigrams occur in some reasonable pro-
portion to the original bigrams. Hence, we experi-
mented with bigram frequency ratios, where a can-
didate is accepted only if its ratio exceeds a given
threshold The ratio is calculated as fr/fo, where
fr and fo represent the replacement and original
bigram frequencies, respectively. We repeated our
Web 1T bigrams experiment for several ratio thresh-
olds between 0 and 0.9, and measured the changes in
accuracy, precision and recall. Our results showed
that frequency ratio thresholding can reduce false
positives, leading to slightly increased precision for
certain ratios. However, true positives are also re-
duced, and we sacrifice significant recall for only
minor gains in precision.
4.3 Filter using grammatical
relations
Candidate selection using bigrams is a somewhat
na??ve approach, as it considers only the surface text
without regard for the underlying grammatical rela-
tions (GRs) between terms. To illustrate, consider
the example shown in Table 4.
We observed that alternatives for ?lovely? such as
?picturesque? and ?scenic? were falsely rejected us-
ing BNC bigrams.20 As bigrams, ?picturesque fam-
ily? and ?scenic family? seem like unnatural ways
20These candidates were accepted using the Web 1T corpus.
Context on their lovely family holidays
Term lovely
Alt.s handsome, picturesque, pretty,
splendid, scenic, resplendent, ...
Table 4: Sample context & replacements for ?lovely?
of describing a family. However, in this context
?lovely? modifies ?holiday?, not ?family?: this dis-
tinction is not picked up using simple bigrams. To
address this limitation, we extended our bigram can-
didate selection technique to consider grammatical
relations (GRs).
Our GR technique uses an input sentence in
RASP format. We only change one term per sen-
tence as before; however we first extract the term?s
GRs from the RASP annotation. We convert each
binary21 GR into a GR-bigram using the original or-
dering of terms in the sentence. Figure 1 illustrates
the GRs for our example sentence, and how such
translate into GR-bigrams.
?On     their     lovely     family     holidays? 
ncmod 
ncmod 
detmod/poss 
GR-bigrams extracted for ?lovely?:  
  1. ?lovely holidays? 
Figure 1: Grammatical relations and GR-bigrams
By converting GRs into bigrams, we can take ad-
vantage of Web 1T?s extensive coverage. However,
due to our restrictions on GR types, it is possible to
obtain zero GR-bigrams for some words in a sen-
tence. This happens when the word has no modifier
or comparative relations associated with it. For these
words, we revert to our bigram selection technique.
Our results for candidate selection using GRs are
again shown in Table 5. Surprisingly, this technique
performs worse than using regular bigrams for all
metrics when compared to our baseline. We suspect
our GR selection technique performs no better than
21We only examine binary comparative and modifier GR
types, as RASP provides many other syntactic relations which
we deemed not relevant to our task.
93
Web 1T bigrams simply due to the corpus? extensive
coverage, which leads to a similar amount of false
positives.
Selection 
Technique 
BNC 
Bigrams 
Web 1T 
Bigrams 
Web 1T 
GRs 
True positives 22 55 150% 54 145% 
False positives 45 155 244% 169 276% 
True negatives 288 178 -38% 164 -43% 
False negatives 57 24 -58% 25 -56% 
Accuracy 0.752 0.566 -25% 0.529 -30% 
Precision 0.328 0.262 -20% 0.242 -26% 
Recall 0.278 0.696 150% 0.684 145% 
F-score 0.301 0.381 26% 0.358 19% 
 
Table 5: Collated results for all experiments
4.4 Error Analysis
To explain our experimental results, we first look at
how the performance changes between our different
versions relative to the baseline (i.e., BNC Bigrams):
see Table 5. Note first that, while all methods in-
creased the number of true positives and decreased
false negatives, any performance gains were simply
drowned out by the massive increases in false posi-
tives that occurred: this is the main cause of our low
precision and recall. For the following discussion,
we focus on the use of Web IT bigrams, which was
the best performing filtering technique.
Since false positives are the most important
source of error to avoid in an ECA, we focus on
these. We examined the false positive instances
and categorised each error into the following four
groups. The distribution of errors into these cate-
gories is shown in Table 6.
Category No. FP % of all FP
Change in Meaning 76 49.03%
Incorrect WSD 42 27.10%
Phrase/Metaphor 31 20.00%
Grammatical 6 3.87%
Total 155 100%
Table 6: Distribution of classification errors
4.4.1 Change in meaning
A major limitation of the OVVT resource is that
several of the alternative terms simply cause too
much semantic change even when the correct sense
of the original term is detected. For example, some
alternatives for ?winner? are words such as ?sleeper?,
?upsetter?, and ?walloper?. In the context of the
phrase ?Cash prizes will be offered to the winners?,
we will almost always prefer the generic ?winner?.
We suspect this limitation arises due to the meth-
ods used to construct the OVVTs; in particular the
use of the WordNet hyponym and hypernym re-
lations. For example, the ?thing? category in Word-
Net encompasses a multitude of more specific terms,
such as ?ornament?, ?structure?, ?surface?, and ?in-
stallation?. These terms all made their way into the
OVVT for ?thing?, yet they are rarely appropriate
substitutions for ?thing?. Conversely, we may not
wish to replace any specific terms with the more
generic ?thing? as this removes too much meaning.
As this kind of error accounted for almost half
of our false positives, addressing this limitation
may lead to significant gains in performance. This
likely requires a more conservative approach to con-
structing the OVVTs themselves, e.g., by incorpo-
rating corpus-based information, as per (Guerini et
al., 2008)?s approach to constructing the Modifier-
OVVTs): the technique for mining appropriate verb-
adverb pairings from the BNC could be generalised
to include other POS types.
Related to the problem of semantic change is the
idea of context-dependent semantics. For example,
certain qualifiers have opposing effects depending
on the appraisal of the subject: consider a ?long
term illness? compared to a ?long term vacation?.
One possible solution to this problem is to modify
the way valences are calculated to take into account
which terms modify one another.
4.4.2 Incorrect word-sense disambiguation
The WSD approach used in our work adapted
from (Guerini et al, 2008) is only a crude approx-
imation to a complex problem; the WSD-related
problems could at least be alleviated by incorpo-
rating a more sophisticated WSD approach into the
pipeline. However, even if we could determine the
correct sense of each word, we are still left with the
limitation that the OVVTs are not exhaustive in their
coverage, with several word senses missing.
94
4.4.3 Phrases and metaphors
Several false positives were caused by phrases
such as ?long term?. Metaphors were a similar
cause for error, e.g. ?stepping stone?. Phrase and
metaphor detection should improve our technique?s
performance, especially since the OVVTs contain
several phrases; however, these are known difficult
challenges in themselves.
4.4.4 Grammatical errors
A grammatical error occurs when the alternative
term is acceptable semantically, yet further syntactic
modification to the sentence is needed to preserve
correct grammar: see Table 3.
An extension of our bigram approach could be to
use a larger window around replaced words to assess
the suitability of a substitution. Recent work has
shown this technique could be used to rank poten-
tial substitutions in order of acceptability (Hawker,
2007) and is worth considering as future work.
4.4.5 Limitations of bigrams and corpus
coverage
In some cases, our bigram selection technique is
ineffective when the term being changed is flanked
by stop words. In a corpus of sufficient size and cov-
erage, the majority of terms will occur next to stop
words far more often than they occur next to other,
less common terms. Hence, bigrams containing stop
words were a common source of false positives.
This limitation could be addressed in future work
by extending our grammatical relation technique to
include ternary GRs, which provide relations for
noun-verb phrases such as ?solution to fitness? and
?solution to health?. Given these, we could accept
or reject based on the presence of the accompany-
ing trigrams in the Web 1T corpus. As described in
(Hawker, 2007), use of an even larger window, such
as 4-grams and 5-grams around replaced terms may
also address this issue, however the size of the Web
1T corpus for larger N-grams presents serious pro-
cessing challenges.22
5 Evaluation: Sentiment Shift
The technqiues described above attempt to create ac-
ceptable candidates to shift sentiment. However, this
22(Hassan et al, 2007) describes a successful approach to lex-
ical substitution that combines multiple knowledge sources.
leaves open the question as to whether the technique
has its desired effect: i.e. appropriately shifting sen-
timent. We designed an experiment which aims to
measure correlation between human judgements of
the sentiment shift in our generated candidates, and
our system?s representation of sentiment shift.
We presented subjects with an original sentence,
along with one of the generated candidates. Our
six subjects had no specialised knowledge of the
task and were all native English speakers. Sub-
jects were asked to judge the modified sentence for
change in sentiment relative to the original accord-
ing to the five shift categories described earlier (i.e.,
major/minor positive/negative/no shift). In order
to avoid bias and to clarify the task, we explained
that sentiment should be separated from changes in
meaning, or the reader?s opinions about the sen-
tences. Instead, we urged subjects to ask themselves
the question: ?Is the author of the second sentence
saying what they?re saying in a more positive or
more negative way, compared to the first sentence??
The sentences used were extracted from the BNC
at random, using the restrictions listed above. We
extracted 250 sentences to be used as the originals,
each of which was used as input to our sentiment
shifting system. For each original sentence, we pro-
duced all possible candidates using our best per-
forming candidate selection method, Web 1T Bi-
grams. We also limited our generation to changing
one term per sentence, as to not produce a combi-
natorial explosion in the number of candidates gen-
erated. This produced approximately 3000 modi-
fied candidates, including several candidates with no
sentiment shift.
Upon inspection, we found many generated can-
didates contained the types of errors described
above. Hence, we manually extracted original and
modified sentences until we had a total of 50 origi-
nals, and 100 shifted sentences. In selecting which
sentences to keep, we chose ones which sounded
the most natural, or had the least amount of seman-
tic change from the original. Manual selection was
performed in order to prevent introducing any bias
into judgements when a subject is confronted with
a grammatically incorrect or unnatural sentence. We
also aimed for a fairly even distribution of the shifted
95
sentences into the five sentiment shift intervals.23
5.1 Results and analysis
We performed a pairwise Kendall?s Tau rank cor-
relation (Kendall and Gibbons, 1962), which com-
pares each human?s judgements with the system?s
sentiment shift, for all 100 generated sentences.
Kendall?s Tau measures the correlation between two
distributions on a scale of -1 to 1, with 1 indicating
total agreement; -1 indicating total disagreement;
and 0 indicating no (or random) correlation.
We measured the correlation using the five senti-
ment shift intervals, and also using judgement po-
larities, i.e. whether a score is positive, nega-
tive or zero. We only report on polarity results as
the finer-grained comparison showed similar results
with slightly less correlation.
Our results are shown in Table 7; Kendall?s Tau
correlations are shown above the shaded diagonal,
while the corresponding p-values for statistical sig-
nificance are shown below the diagonal.
  Kendall's Tau Correlation 
  sys h1 h2 h3 h4 h5 h6 
p-
va
lu
e 
sys  0.075 0.024 -0.099 0.034 0.022 -0.078 
h1 0.413  0.276 0.423 0.417 0.339 0.249 
h2 0.790 0.002  0.406 0.348 0.361 0.198 
h3 0.273 0.000 0.000  0.418 0.300 0.343 
h4 0.708 0.000 0.000 0.000  0.325 0.277 
h5 0.810 0.000 0.000 0.001 0.000  0.189 
h6 0.393 0.006 0.029 0.000 0.002 0.040  
 
Table 7: Kendall?s Tau rank correlation between system
(sys) and human (hi) judgement polarities
Although the correlation observed between inter-
annotator judgements of polarity was fairly low, it
is statistically significant in all cases using a confi-
dence level of p < 0.05. While this indicates there
was some agreement between human annotators, the
relatively low correlation indicates that judging sen-
timent is a fairly subjective task. However, we saw
no correlation between the human judgements and
our system?s representation of sentiment shift.
23Note: the judgement of which sentiment-shift category a
sentence-pair fell into was made by the system (and subjects);
the manual intervention in the experiment design was to remove
unacceptable sentence-pairs.
The poor correlation between human and system
polarities can possibly be attributed to a number
of reasons. (Guerini et al, 2008) mention that in
SentiWordNet, several of the WordNet synsets are
valenced incorrectly, with many having a valence of
zero, which we also observed in the OVVT resource.
Our survey results suggest that SentiWordNet in its
current form is not ideally suited to the task of gen-
erating sentiment in text using the Valentino method.
SentiWordNet may be effective when classifying
the sentiment of large texts; the valence scores can
be considered to reflect the degree to which each
word represents a sentiment ?feature?. However, it
is somewhat unrealistic to assume that every term
will have the same effect on sentiment in all con-
texts; assigning words a ?universal? sentiment score
seems non-intuitive, and a finer-grained representa-
tion of sentiment is needed for short texts such as
dialogue utterances.
In sentiment generation, when choosing a re-
placement term from a set of alternatives, we are
more interested in each candidate?s effect on senti-
ment, relative to the other candidates. While a re-
source of semantically clustered terms is needed for
this task (such as the OVVTs), terms within each
cluster need to be ranked for sentiment in a localised
way, taking account of positivity or negativity rela-
tive to other terms in the cluster. Upon inspection
of several OVVTs, this ranking is a straightforward
task for a human to perform (if time-consuming).
However, the context of a substitution often de-
termines its effects of sentiment. Hence, we ar-
gue that future work in sentiment generation using
knowledge-based techniques should extend existing
resources to encompass ranking of candidates in a
contextual way, rather than ranking them statically
out of context. For example, an MRE-style (Traum
et al, 2003) approach could be used which goes be-
yond scoring the overall sentiment of an utterance,
but considers how sentiment (or attitude) is directed
towards agents, objects and events.
Acknowledgements: We thankMarco Guerini for kindly
providing us with the OVVTs resource, Tim Baldwin for
helpful suggestions for the evaluation in Section 5, and
the referees for valuable feedback. Cavedon?s contribu-
tion was partially supported by the Australian Research
Council under Linkage Grant LP0882013.
96
References
Joseph Bates. 1994. The Role of Emotion in Believable
Agents. Communications of the ACM, 37(7):122?125.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The Second Release of the RASP System. In Proceed-
ings of ACL, pages 77?80, Sydney.
G. Caridakis, A. Raouzaiou, E. Bevacqua, M. Mancini,
K. Karpouzis, L. Malatesta, and C. Pelachaud. 2007.
Virtual Agent Multimodal Mimicry of Humans. Lan-
guage Resources and Evaluation, 41(3):367?388.
Michael Fleischman and Eduard Hovy. 2002. Towards
Emotional Variation in Speech-Based Natural Lan-
guage Generation. In Proceedings of the Second In-
ternational Natural Language Generation Conference,
pages 57?64, New York.
Marco Guerini, Carlo Strapparava, and Oliviero Stock.
2008. Valentino: A Tool for Valence Shifting of Natu-
ral Language Texts. In Proceedings of the 6th Interna-
tional Conference on Language Resources and Evalu-
ation, Marrakech.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. Unt: Subfinder:
combining knowledge sources for automatic lexical
substitution. In Proc. Fourth Int. Workshop on Se-
mantic Evaluations (SemEval 2007), pages 410?413,
Prague.
Tobias Hawker. 2007. USYD: WSD and Lexical Sub-
stitution Using the Web1T Corpus. In Proc. 4th Int.
Workshop on Semantic Evaluations (SemEval 2007),
pages 446?453, Prague.
M.G. Kendall and J.D. Gibbons. 1962. Rank Correlation
Methods. Griffin London.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In
Proc. Fourth Int. Workshop on Semantic Evaluations
(SemEval 2007), pages 48?53, Prague.
Catherine Pelachaud and Massimo Bilvi. 2003. Compu-
tational Model of Believable Conversational Agents.
In Communication in Multiagent Systems, volume
2650 of Lecture Notes in Computer Science, pages
300?317. Springer.
David Traum, Michael Fleischman, and Eduard Hovy.
2003. NL Generation for Virtual Humans in a Com-
plex Social Environment. In In Proceedings of the
AAAI Spring Symposium on Natural Language Gener-
ation in Spoken and Written Dialogue, pages 151?158,
Palo Alto.
Ielka van der Sluis and Chris Mellish. 2008. Towards
Affective Natural Language Generation: Empirical In-
vestigations. In Proceedings of the Symposium on Af-
fective Language in Human and Machine, AISB, pages
9?16, Aberdeen.
E. Zovato, F. Tini Brunozzi, and M. Danieli. 2008. Inter-
play between pragmatic and acoustic level to embody
expressive cues in a Text to Speech system. In Pro-
ceedings of the Symposium on Affective Language in
Human and Machine, AISB, pages 88?91, Aberdeen.
97
Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 19?24,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
?Hello Emily, how are you today??
Personalised dialogue in a toy to engage children
Carole Adam
RMIT University
Melbourne, Australia.
carole.adam.rmit@gmail.com
Lawrence Cavedon
RMIT University
Melbourne, Australia.
lawrence.cavedon@rmit.edu.au
Lin Padgham
RMIT University
Melbourne, Australia.
lin.padgham@rmit.edu.au
Abstract
In line with the growing interest in conver-
sational agents as companions, we are de-
veloping a toy companion for children that
is capable of engaging interactions and of
developing a long-term relationship with
them, and is extensible so as to evolve with
them. In this paper, we investigate the im-
portance of personalising interaction both
for engagement and for long-term relation-
ship development. In particular, we pro-
pose a framework for representing, gath-
ering and using personal knowledge about
the child during dialogue interaction. 1
1 Introduction
In recent years there has been an increasing in-
terest in so-called Companion agents: agents that
are intelligent, and built to interact naturally (via
speech and other modalities) with their user over a
prolonged period of time, personalising the inter-
action to them and developing a relationship with
them. The EU Companions project2 is the most
well known such project, with applications such
as a companion for the elderly (Field et al, 2009),
and a health and fitness companion (Stahl et al,
2009). In our work, together with industry part-
ners, we are developing a speech-enabled compan-
ion toy for children. While there are many ?smart
toys? on the market, as far as we are aware our
work is unique in attempting to develop a ?com-
panion toy? for a child, evolving with them over
a long period of time. As with other projects on
intelligent companions, a crucial task is to build a
long-term relationship with the user, by a series of
interactions over time, that the user experiences as
engaging and valuable.
1A slightly longer version of this paper is currently under
review elsewhere. If both papers are accepted for publication
we will modify to ensure that they expand different aspects.
2See www.companions-project.org.
According to models of the ?enjoyability? of
human-computer interaction (Brandtzaeg et al,
2006), there are three main features making an in-
teractive system engaging for the user: the user
should feel in control of the interaction (which
includes being able to customise it and getting
timely feedback); the demands on the user should
be adapted to their capabilities, i.e. the interaction
should be challenging and surprising but not over-
whelming; and the system should support social
interaction rather than isolating the user. Another
important aspect of any engaging interaction is for
it to be personalised, i.e. customised to the par-
ticular interlocutor and their environment. Other
important features for engagement include coher-
ence of the dialogue, emotional management, and
personality. In this paper we focus specifically on
the issue of appropriate personalisation of interac-
tions with a child, and how to realise this.
Existing personalised systems mainly have a
task-oriented focus, i.e. they aim at building a
user profile and using it to facilitate the user?s task
(e.g. Web navigation assistants or product recom-
mendation systems (Abbattista et al, 2003)), and
at being user-configurable. On the contrary we
aim at personalising the interaction to build a re-
lationship and engage a child. The main novelties
of our system are that: it is not task-oriented; it
is specifically designed for children; and its be-
haviour is derived from actual interaction data. In-
deed, in order to understand the kinds of person-
alisation occurring in natural dialogues with chil-
dren, we have analysed corpora of children?s dia-
logues (MacWhinney, 1995; MacWhinney, 2000).
We have then developed a framework that enables
the implementation of a number of these person-
alised behaviours within our intelligent toy.
The contribution of this paper is the identifi-
cation of different kinds of personalisation be-
haviours in dialogue with children, based on actual
data, plus the framework to realise these within
an implemented system.
19
2 Personalisation behaviours
2.1 Corpus analysis
We have analysed examples of children-adult di-
alogues (mainly from the CHILDES database
(MacWhinney, 1995; MacWhinney, 2000); one
dialogue from a forthcoming study performed
with a puppet as part of this project) in order to
determine the types of behaviours that adults use
to personalise their interaction with a child.
Relation to self
A first observation is that children often try to re-
late conversation to themselves. This is illustrated
by this conversation between a girl (G) and her
mother (M) about a visit to the doctor.
G What?s polio?
M An illness that makes you crippled. That?s why you get
all those injections and... A long time ago, kiddies, kid-
dies used to die with all that things.
G will I ?
M hmm. You aren?t going to die.
Personal questions
Adults also often ask the child questions about
themselves. This dialogue illustrates a conversa-
tion between an adult (A) and a child (C) about C?s
holidays. Notice that the questions are adapted to
the context (ask about holidays in summer).
A Did you go on vacation over the summer? Did you?
A Where?d you go? To the beach?
C Yes.
A Yeah? Did you go by yourself? No. Why laugh? You
could go by yourself.
A Do you have brothers and sisters?
C Just a little sister.
A A sister? Did she go too? On vacation?
Child control
Even if the adult is asking the questions, the child
retains some control over the interaction. The fol-
lowing dialogue between a boy (B) and his grand-
mother (G) shows how the adult follows the child
when he switches away from a disliked topic. This
dialogue also shows the adult commenting on the
child?s tastes based on her knowledge of them.
G how are you getting on in school?
B we?re not going to go shopping today.
G eh?
B shopping today.
G ...
B and chips.
G going to have chips?
B mm.
G you likes that.
Reciprocity
Another way for the adult to learn personal infor-
mation about the child without asking questions
is to confide personal information first, which en-
courages the child to reciprocate. In this dialogue
between a child (C) and a puppet (P) controlled
by an adult, P confides personal information (its
tastes), which leads the child to do the same.
P My favourite drink is lemon. Lemon soft drink. I like
that.
C Mine is orange juice.
P mmhm. Orange one? You like the orange one?
C Orange juice (nodding)
Recalling shared activities
Another form of personalisation is recalling past
shared activities. In the following dialogue, a
mother (M) reads a book to her child (C); when
a picture of a snowman appears in the book she
recalls the child recently making one with her.
M what did we make outside here today?
C um I don?t know.
M did we make a man?
C yeah.
M a snowman?
C yeah.
Child?s preferences
Another way to personalise interaction is to recall
a child?s preferences. For example this dialogue
involves a child (C) and an interrogator (I) wanting
to record a story. Here the child corrects incorrect
knowledge; this update should be remembered.
I Do you wanna tell a story?
C No. I won?t.
I No, you don?t.
I You told me down there that you like stories.
C No, I hate stories.
Child?s agenda
Parents may also use knowledge about a child?s
agenda (i.e. planned future activities, school, etc.)
and make relevant and timely comments about it.
In this dialogue a mother (M) and her friend (F)
talk with a boy (B) about his next school day, when
he is supposed to see chicken eggs hatching.
F Oh you?re going to see the little chicks tomorrow are
you. You?ll have to tell me what it?s like. I haven?t
never seen any.
B I I haven?t either.
F I haven?t.
M We?ve seen them on the tellie, haven?t we?
F I haven?t seen those little ones.
M haven?t you?
F So you?ll have to tell me.
M Have you seen them on the tellie?
B mm [= yes].
We notice again that when the mother?s friend
confides some information (she never saw that),
the child reciprocates (he neither). Moreover the
mother again shows memory of past activities
(seeing something on television).
20
2.2 Personalisation strategies
Based on our analysis of adult-children dialogue
corpora, we have designed a number of strategies
to allow our toy to generate these kinds of person-
alised interactions with the child. These strategies
fit into two categories: strategies for gathering
personal information, and strategies for exploiting
personal information.
Information gathering
The Toy can gather and then use different types of
information: (1) personal information (e.g. fam-
ily, friends, pets); (2) preferences (e.g. favourite
movie, favourite food); (3) agenda (plays foot-
ball on Saturday, has maths every Thursday);
(4) activity-specific information (preferred stories,
current level of quiz difficulty); (5) interaction en-
vironment (e.g. time, day, season, weather).
The easiest strategy to gather this information
is to explicitly query the child. These queries have
to be made opportunistically, e.g. when matching
the current conversational topic, so as to seam-
lessly integrate information gathering into a con-
versation. Other strategies include confiding per-
sonal information to make the child reciprocate
and confide similar information; or extracting per-
sonal information from spontaneous child?s input.
These strategies are useful so as to avoid asking
too many questions, which would dirupt the con-
versation flow and could annoy the child.
Information exploitation
One of the challenges for using the gathered per-
sonal information in a conversation is to deter-
mine the appropriate opportunities to do so. The
personal information can be used to engage the
child in various ways, reproducing the types of be-
haviours illustrated above. In particular, our toy
has the following information exploiting strate-
gies: (1) use child?s name; (2) insert comments
using personal information; (3) ask about daily ac-
tivities; (4) adapt interaction (e.g. greetings) to the
context (e.g. time of day); (5) take child?s prefer-
ences into account in topic or activity selection.
3 The Toy architecture: overview
This section outlines the general architecture of
the toy. The integration of our personalisation
framework is detailed in Section 4.
The central component of the Toy is the Dia-
logue Manager (DM) which is made up of two
components: the input/output manager (IOM) re-
ceives input from Automatic Speech Recognition
(ASR)3 and sends output to Text-to-Speech (TTS);
the Semantic Interaction Manager (SIM) receives
input from IOM, generates the toy?s response and
sends it back to IOM (see Figure 1).
Figure 1: Architecture of the Toy
Our current approach to ASR and utterance pro-
cessing is grammar-based: on sending an out-
put utterance for synthesis, the DM loads into the
speech recogniser a parameterised grammar speci-
fying the set of expected user responses to this out-
put. The DM is multi-domain and extensible via
domain modules, designed to handle utterances
about a particular domain, and encapsulating data
required for this: a knowledge-base segment; a set
of conversational fragments (see Section 3.2.2); a
collection of the topics it is designed to handle;
and an entry grammar to assign a topic to inputs.
3.1 Input Output Manager
The IOM is implemented using a BDI agent-
oriented methodology, with dialogue processing
?strategies? built as plans. For example, there are
plans designed to handle errors or low-confidence
results from speech recognition; plans to handle
utterance content and update the information state;
and plans to manage concurrent conversational
threads and select which of a number of candidate
responses to output.
3.2 Semantic Interaction Manager
The Semantic Interaction Manager (SIM) is a
component designed to manage flexible conver-
sational flow. The SIM maintains an agenda of
things to say. When an input is received from
the IOM, it is pre-processed to generate an in-
put analysis that informs the further stages of the
3We have mainly used SRI?s Dynaspeak system which is
designed for small computational platforms.
21
SIM plan. In particular the input is then either
dispatched to an existing ongoing activity if it
matches its expected answers, or an appropriate
new activity is created. The chosen activity se-
lects a conversational fragment in the topic net-
work corresponding to its topic, and writes it in
the conversational agenda. Finally the output is
generated from the agenda and sent to the IOM.
3.2.1 The conversational agenda
The conversational agenda maintained by the SIM
has two main parts. The history represents the past
interaction and stores past questions under discus-
sion (QUD) (Ginzburg, 1997) with their received
answer. The stack represents the future interac-
tion and lists QUD to be asked next, in order. The
agenda also stores the current ongoing activities
(Section 3.2.3), making it possible to switch back
and forth between them.
3.2.2 Conversational fragments
In our system, we use pre-scripted pieces of dia-
logue that we call conversational fragments. The
designers of domain modules will provide a topic
network describing its domain, with nodes being
the possible topics, having links with other topics,
and providing a pool of fragments to possibly use
when talking about this topic. Each fragment has
an applicability condition, and provides the text of
an output as well as a list of expected answer pat-
terns with associated processing (e.g. giving feed-
back) applied when the child?s response matches.
This representation obviates the need for full
natural language generation (NLG) by provid-
ing semi-scripted outputs, and also informs the
grammar-based ASR by providing a list of ex-
pected child answers. Moreover it allows the Toy
to generate quite flexible interactions by switching
between topics and using fragments in any order.
3.2.3 Activities
When interacting with the child, the Toy suggests
possible activities (e.g. quiz, story) about the avail-
able topics. Each type of activity uses specific
types of fragments (e.g. quiz questions with ex-
pected (in)correct answers; story steps with ex-
pected questions) and has particular success and
failure conditions (e.g. a number of (in)correct an-
swers for a quiz; or reaching the end for a story).
This concept of activity helps to keep the dia-
logue cohesive, while allowing flexibility. It also
meets the requirement that an engaging interaction
should be demanding for the child while staying
controlled by them. Indeed a number of activities
can be listed in the agenda at the same time, be-
ing resumed or paused to allow switching between
them (e.g. to follow the child?s topic requests or to
insert personalised contributions).
4 The toy personalisation framework
We now describe our framework for implementing
the personalisation strategies specified earlier.
4.1 The personalisation frame
All the information that our toy needs to person-
alise an interaction is gathered using a structure
called the personalisation frame. This structure is
tailored to the requirements imposed by our archi-
tecture, namely the grammar-based speech recog-
nition and the absence of natural language pro-
cessing. It consists of: (1) a static list of per-
sonal information fields (e.g. child name, age); (2)
a static indexed list of rules specifying when it is
appropriate to insert personal comments or ques-
tions in the interaction; (3) a dynamic child pro-
file, storing the current values of (some) personal
information fields, updated during interaction.
Personal information fields (PIFs)
Each personal information field contains: a list of
possible values for this field (informing the ASR
grammar); and a grammar of specific ways in
which the child may spontaneously provide infor-
mation relevant to this field (allowing the toy to
interpret such input and extract the value).
For example the field ?favourite animal? has a
list of animals as its values, and its grammar con-
tains patterns such as ?My favourite animal is X?
or ?I love X? (where the variable X ranges over
the possible values of this field).
Personalisation rules
Each personalisation rule specifies the opportunity
that triggers it, and the text of the output. The
text of personalisation comments and questions is
scripted, and used to automatically generate con-
versation fragments from the frame. Comment
rules also specify the list of personal information
fields that are used in the text of the comment,
while Question rules specify the name of the field
set by their answer and a grammar of expected an-
swers, with their interpretation in terms of which
value the corresponding field should receive.
22
For example, there may be a comment rule re-
ferring to the field pet type, enabling the output
?I know you have a pet type? when the keyword
pet type is detected. There may also be a ques-
tion rule for asking ?What is your favourite ani-
mal?? when talking about the zoo; expected an-
swers would include ?I like A?; so if the child an-
swers ?I like tigers? then the favourite animal
field would receive the value ?tigers? as a result.
Opportunities
Personalisation must be integrated into the con-
versational management so as not to disrupt dia-
logue (i.e. the toy should still maintain a coherent
interaction). It is thus important to accurately de-
tect appropriate opportunities to insert personali-
sation side-talk. There are three types of oppor-
tunities that can trigger the personalisation rules:
(1) keyword opportunities (a particular keyword
appears in the child?s input, e.g. the child uses the
word ?mother?); (2) topic opportunities (the in-
teraction is focused on a particular topic, e.g. the
child is talking about koalas); (3) activity op-
portunities (a particular activity is in a particular
state, e.g. start of a story).
The following sections describe how this per-
sonalisation frame is used in the Conversation
Manager process to personalise the conversation
that is generated: we first outline the full process,
before giving details about the steps where the per-
sonalisation frame is used.
4.2 Personalised input handling
The following algorithm is the result of the inte-
gration of personalisation into the response gen-
eration plan of the SIM. Steps manipulating the
personalisation frame will be detailed below.
1. Initialisation (load child profile,
update environment description);
2. Input reception (from IOM):
3. Input analysis (preprocess input,
detect opportunities);
4. Profile update;
5. Input dispatching (to selected
activity);
6. Activity progressing (fragment
selection);
7. Personalisation generation (generate
fragment from best applicable
triggered rule);
8. Agenda processing (prioritisation
of activity vs personalisation
fragments);
9. Personalisation of output (detection
of opportunities, modification of
output);
10. Output generation (sent to IOM);
11. End turn (save profile).
Fragment selection (step 6)
Fragment selection is personalised in two ways.
First, some fragments have applicability condi-
tions concerning the interaction context and the
child?s profile. For example a fragment such as
?Hi, what?s your name?? is only applicable if
the toy does not know the child?s name. A greet-
ing fragment such as ?Hi! How was school to-
day?? is only applicable at the end of a school
day. Other greeting fragments are available for dif-
ferent contexts. Second, some fragments have an
adaptable content, using variables referring to the
child?s profile and to the context. These fragments
are only applicable if the value of these variables is
known and can be used to instantiate the variable
when generating output. For example a fragment
with the text ?Hello child name! How are you??
is applicable once the child?s name is known. Or
a fragment saying ?I know you have a pet type
called pet name.? will be instantiated as ?I know
you have a cat called Simba?.
Personalisation fragments generation (step 7)
When an opportunistic rule in the personalisation
frame is triggered, its applicability is checked:
comment rules are only applicable if the fields
used have a value; question rules are only appli-
cable if the field set has no value. Then the appli-
cable rule of highest priority is used to generate a
personalisation fragment. Its topic is the current
topic provided in the input analysis; its type is ei-
ther ?personal question? or ?personal comment?;
and its text is as specified in the rule. Comment
fragments have no additional applicability condi-
tion and no expected answers. Question fragments
receive the generic expected answers specified in
the rule, instantiated with the possible values of
the field set by this question; the process associ-
ated with them consists in setting the value of this
field to the value extracted from the input; the de-
fault process stores the full input as the value.
For example with the question rule ?What is
your favourite animal?? shown above, the gener-
ated fragment will expect answers such as ?I like
tigers?, ?I like giraffes?, etc. If a matching an-
swer is received, then the corresponding animal is
stored as the child?s favourite one in their profile.
Prioritisation (step 8)
When updating the agenda, several situations can
arise depending on the relative priorities of person-
alising the interaction vs continuing the current ac-
23
tivity. If the activity can be disrupted at this point,
personalisation question fragments can be added
on top of the stack in the agenda, while comments
can be appended to the activity output; if the per-
sonalisation fragment has too low priority to dis-
rupt the activity, then it is dropped. This priority
of personalisation depends in particular on the pri-
ority of the triggered rule and on the time since
personalisation was last inserted.
5 Conclusion
This framework for personalisation has been im-
plemented in our toy using the Java-based agent
platform Gorite (Ronnquist, 2008). The following
is an example of the type of dialogues we intend
to generate with it (T is the Toy and C a child); it
illustrates a number of our personalisation strate-
gies. Our infrastructure technically allows it to
happen, however we still need to add more con-
tent (domain modules and fragments in them) to
make it actually happen; we are working on writ-
ing such content, as well as on providing tools to
make it easier for designers to write this content in
the future.
C Hi there
T Hello Emily! [Use child?s name]
T What did you do on such a sunny week-end?
[Context-adaptation]
C I went to the zoo!
T Great! That must have been fun. Which animals did
you see?
C I saw tigers
T Tigers are cats, but much bigger than your cat Simba.
What else did you see?
C Koalas!
T Oh, that?s your favourite animal, isn?t it?
[Personal comment]
C Yes
T You must have been really happy. My favourite ani-
mal is orangutan, did you see any? [Confide
personal information]
C No
T Pity... Allright, enough chatting. I know you love
stories. Do you want to hear a story about tigers?
[Choose favourite activity]
The ASR is not functional yet due to the spe-
cific challenges of recognizing children voices, so
for now we are only experimenting with a textual
interface. This may look similar to a chatbot but
has additional functionalities such as playing ac-
tivities, and maintaining a context of interaction,
including the history of the past interaction (in or-
der not to repeat itself), physical context (to tai-
lor interaction to the date, time, weather...), and
a profile of the user (to personalise interaction to
them). Contrarily to a chatbot which is designed
for short-term interactions, we expect such a com-
panion agent to be able to develop a long-term re-
lationship with the user. This will be tested with a
Wizard of Oz setting before our industrial partner
provides us with a children-specific ASR.
The dialogue above is obviously not as rich as
child-mother interactions from the CHILDES cor-
pus; in particular it lacks the recognition of emo-
tions and expression of empathy that is essential
in human interactions. Therefore future directions
for research include detecting the child?s emotions
(we have been experimenting with OpenEar (Ey-
ben et al, 2009) to detect emotions from voice);
reasoning about detected emotions, using an exist-
ing BDI model of emotions (Adam, 2007); helping
the child to cope with them, in particular by show-
ing empathy; and endowing the toy with its own
personality (Goldberg, 1993).
6 Acknowledgements
This project is supported by the Australian Re-
search Council, and RealThing Pty Ltd. under
Linkage Grant LP0882013
References
F. Abbattista, G. Catucci, M. Degemmis, P. Lops, G. Semer-
aro, and F. Zambetta. 2003. A framework for the devel-
opment of personalized agents. In KES.
C. Adam. 2007. Emotions: from psychological theories to
logical formalisation and implementation in a BDI agent.
Ph.D. thesis, INP Toulouse, France.
P. B. Brandtzaeg, A. Folstad, and J. Heim. 2006. Enjoyment:
Lessons from karasek. In M. A. Blythe, K. Overbeeke,
A. F. Monk, and P. C. Wright, editors, Funology: From
Usability to Enjoyment. Springer.
F. Eyben, M. Wollmer, and B. Schuller. 2009. openEAR:
Introducing the Munich open-source emotion and affect
recognition toolkit. In ACII, Amsterdam.
D. Field, R. Catizone, W. Cheng, A. Dingli, S. Worgan, L. Ye,
and Y. Wilks. 2009. The senior companion: a semantic
web dialogue system. (demo). In AAMAS.
J. Ginzburg. 1997. Resolving questions I and II. Linguistics
and Philosophy, 17 and 18.
L. R. Goldberg. 1993. The structure of phenotypic personal-
ity traits. American Psychologist, 48:26?34.
B. MacWhinney. 1995. The CHILDES Database.
B. MacWhinney. 2000. The CHILDES project: Tools for
analyzing talk. Lawrence Erlbaum Associates.
R. Ronnquist. 2008. The goal oriented teams (gorite) frame-
work. In Programming Multi-Agent Systems, volume
LNCS 4908, pages 27?41. Springer.
O. Stahl, B. Gamback, M. Turunen, and J. Hakulinen. 2009.
A mobile health and fitness companion demonstrator. In
EACL.
24
