Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2028?2038,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Learning Spatial Knowledge for Text to 3D Scene Generation
Angel X. Chang, Manolis Savva and Christopher D. Manning
Stanford University
{angelx,msavva,manning}@cs.stanford.edu
Abstract
We address the grounding of natural lan-
guage to concrete spatial constraints, and
inference of implicit pragmatics in 3D en-
vironments. We apply our approach to the
task of text-to-3D scene generation. We
present a representation for common sense
spatial knowledge and an approach to ex-
tract it from 3D scene data. In text-to-
3D scene generation, a user provides as in-
put natural language text from which we
extract explicit constraints on the objects
that should appear in the scene. The main
innovation of this work is to show how
to augment these explicit constraints with
learned spatial knowledge to infer missing
objects and likely layouts for the objects
in the scene. We demonstrate that spatial
knowledge is useful for interpreting natu-
ral language and show examples of learned
knowledge and generated 3D scenes.
1 Introduction
To understand language, we need an understanding
of the world around us. Language describes the
world and provides symbols with which we rep-
resent meaning. Still, much knowledge about the
world is so obvious that it is rarely explicitly stated.
It is uncommon for people to state that chairs are
usually on the floor and upright, and that you usu-
ally eat a cake from a plate on a table. Knowledge
of such common facts provides the context within
which people communicate with language. There-
fore, to create practical systems that can interact
with the world and communicate with people, we
need to leverage such knowledge to interpret lan-
guage in context.
Spatial knowledge is an important aspect of the
world and is often not expressed explicitly in nat-
ural language. This is one of the biggest chal-
Figure 1: Generated scene for ?There is a room
with a chair and a computer.? Note that the system
infers the presence of a desk and that the computer
should be supported by the desk.
lenges in grounding language and enabling natu-
ral communication between people and intelligent
systems. For instance, if we want a robot that can
follow commands such as ?bring me a piece of
cake?, it needs to be imparted with an understand-
ing of likely locations for the cake in the kitchen
and that the cake should be placed on a plate.
The pioneering WordsEye system (Coyne and
Sproat, 2001) addressed the text-to-3D task and is
an inspiration for our work. However, there are
many remaining gaps in this broad area. Among
them, there is a need for research into learning spa-
tial knowledge representations from data, and for
connecting them to language. Representing un-
stated facts is a challenging problem unaddressed
by prior work and the focus of our contribution.
This problem is a counterpart to the image descrip-
tion problem (Kulkarni et al., 2011; Mitchell et al.,
2012; Elliott and Keller, 2013), which has so far
remained largely unexplored by the community.
We present a representation for this form of spa-
tial knowledge that we learn from 3D scene data
and connect to natural language. We will show
how this representation is useful for grounding
language and for inferring unstated facts, i.e., the
pragmatics of language describing physical envi-
ronments. We demonstrate the use of this repre-
sentation in the task of text-to-3D scene genera-
2028
Room
Table
Plate
Cake
color(red)?There is a room with a table and a cake. There is a red chair to the right of the table.?
a) Scene TemplateInput Text
supports(o0,o1) supports(o0,o2)
right(o2,o1)
o3cake
c) 3D Scene
o0room
o1table o2chair
supports(o1,o4)
supports(o4,o3)o4plate
Parse
Infer
Ground
Layout
b) Geometric Scene
Render
ViewChair
Figure 2: Overview of our spatial knowledge representation for text-to-3D scene generation. We parse
input text into a scene template and infer implicit spatial constraints from learned priors. We then ground
the template to a geometric scene, choose 3Dmodels to instantiate and arrange them into a final 3D scene.
tion, where the input is natural language and the
desired output is a 3D scene.
We focus on the text-to-3D task to demonstrate
that extracting spatial knowledge is possible and
beneficial in a challenging scenario: one requiring
the grounding of natural language and inference of
rarelymentioned implicit pragmatics based on spa-
tial facts. Figure 1 illustrates some of the inference
challenges in generating 3D scenes from natural
language: the desk was not explicitly mentioned
in the input, but we need to infer that the computer
is likely to be supported by a desk rather than di-
rectly placed on the floor. Without this inference,
the user would need to be much more verbose with
text such as ?There is a room with a chair, a com-
puter, and a desk. The computer is on the desk, and
the desk is on the floor. The chair is on the floor.?
Contributions We present a spatial knowledge
representation that can be learned from 3D scenes
and captures the statistics of what objects occur
in different scene types, and their spatial posi-
tions relative to each other. In addition, we model
spatial relations (left, on top of, etc.) and learn a
mapping between language and the geometric con-
straints that spatial terms imply. We show that
using our learned spatial knowledge representa-
tion, we can infer implicit constraints, and generate
plausible scenes from concise natural text input.
2 Task Definition and Overview
We define text-to-scene generation as the task of
taking text that describes a scene as input, and gen-
erating a plausible 3D scene described by that text
as output. More concretely, based on the input
text, we select objects from a dataset of 3D models
and arrange them to generate output scenes.
The main challenge we address is in transform-
ing a scene template into a physically realizable 3D
scene. For this to be possible, the system must be
able to automatically specify the objects present
and their position and orientation with respect to
each other as constraints in 3D space. To do so, we
need to have a representation of scenes (?3). We
need good priors over the arrangements of objects
in scenes (?4) and we need to be able to ground
textual relations into spatial constraints (?5). We
break down our task as follows (see Figure 2):
Template Parsing (?6.1): Parse the textual de-
scription of a scene into a set of constraints on the
objects present and spatial relations between them.
Inference (?6.2): Expand this set of constraints by
accounting for implicit constraints not specified in
the text using learned spatial priors.
Grounding (?6.3): Given the constraints and pri-
ors on the spatial relations of objects, transform the
scene template into a geometric 3D scenewith a set
of objects to be instantiated.
Scene Layout (?6.4): Arrange the objects and op-
timize their placement based on priors on the rel-
ative positions of objects and explicitly provided
spatial constraints.
3 Scene Representation
To capture the objects present and their arrange-
ment, we represent scenes as graphs where nodes
are objects in the scene, and edges are semantic re-
lationships between the objects.
We represent the semantics of a scene using a
scene template and the geometric properties using
a geometric scene. One critical property which is
captured by our scene graph representation is that
of a static support hierarchy, i.e., the order in which
bigger objects physically support smaller ones: the
floor supports tables, which support plates, which
can support cakes. Static support and other con-
straints on relationships between objects are rep-
resented as edges in the scene graph.
2029
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
p(Scene|Knife+Table)
Kitchen Counter Dining TableLiving Room Kitchen
Figure 3: Probabilities of different scene types
given the presence of ?knife? and ?table?.
Figure 4: Probabilities of support for some most
likely child object categories given four different
parent object categories, from top left clockwise:
dining table, bookcase, room, desk.
3.1 Scene Template
A scene template T = (O, C, C
s
) consists of a
set of object descriptions O = {o
1
, . . . , o
n
} and
constraints C = {c
1
, . . . , c
k
} on the relationships
between the objects. A scene template also has a
scene type C
s
.
Each object o
i
, has properties associated with
it such as category label, basic attributes such as
color and material, and number of occurrences in
the scene. For constraints, we focus on spatial re-
lations between objects, expressed as predicates of
the form supported_by(o
i
, o
j
) or left(o
i
, o
j
)where
o
i
and o
j
are recognized objects.1 Figure 2a shows
an example scene template. From the scene tem-
plate we instantiate concrete geometric 3D scenes.
To infer implicit constraints on objects and spa-
tial support we learn priors on object occurrences
in 3D scenes (?4.1) and their support hierarchies
(?4.2).
3.2 Geometric Scene
We refer to the concrete geometric representation
of a scene as a ?geometric scene?. It consists of
a set of 3D model instances ? one for each ob-
ject ? that capture the appearance of the object. A
transformation matrix that represents the position,
orientation, and scaling of the object in a scene is
also necessary to exactly position the object. We
generate a geometric scene from a scene template
by selecting appropriate models from a 3D model
database and determining transformations that op-
1Our representation can also support other relationships
such as larger(o
i
, o
j
).
timize their layout to satisfy spatial constraints. To
inform geometric arrangement we learn priors on
the types of support surfaces (?4.2) and the relative
positions of objects (?4.4).
4 Spatial Knowledge
Our model of spatial knowledge relies on the idea
of abstract scene types describing the occurrence
and arrangement of different categories of objects
within scenes of that type. For example, kitchens
typically contain kitchen counters on which plates
and cups are likely to be found. The type of scene
and category of objects condition the spatial rela-
tionships that can exist in a scene.
We learn spatial knowledge from 3D scene data,
basing our approach on that of Fisher et al. (2012)
and using their dataset of 133 small indoor scenes
created with 1723 Trimble 3D Warehouse mod-
els (Fisher et al., 2012).
4.1 Object Occurrence Priors
We learn priors for object occurrence in different
scene types (such as kitchens, offices, bedrooms).
P
occ
(C
o
|C
s
) =
count(C
o
in C
s
)
count(C
s
)
This allows us to evaluate the probability of dif-
ferent scene types given lists of object occurring
in them (see Figure 3). For example given input of
the form ?there is a knife on the table? then we are
likely to generate a scene with a dining table and
other related objects.
4.2 Support Hierarchy Priors
We observe the static support relations of objects
in existing scenes to establish a prior over what ob-
jects go on top of what other objects. As an exam-
ple, by observing plates and forks on tables most
of the time, we establish that tables are more likely
to support plates and forks than chairs. We esti-
mate the probability of a parent category C
p
sup-
porting a given child category C
c
as a simple con-
ditional probability based on normalized observa-
tion counts.2
P
support
(C
p
|C
c
) =
count(C
c
on C
p
)
count(C
c
)
We show a few of the priors we learn in Figure 4
as likelihoods of categories of child objects being
statically supported by a parent category object.
2The support hierarchy is explicitly modeled in the scene
dataset we use.
2030
Figure 5: Predicted positions using learned rela-
tive position priors for chair given desk (top left),
poster-room (top right), mouse-desk (bottom left),
keyboard-desk (bottom right).
4.3 Support Surface Priors
To identify which surfaces on parent objects sup-
port child objects, we first segment parent models
into planar surfaces using a simple region-growing
algorithm based on (Kalvin and Taylor, 1996). We
characterize support surfaces by the direction of
their normal vector, limited to the six canonical
directions: up, down, left, right, front, back. We
learn a probability of supporting surface normal di-
rection S
n
given child object category C
c
. For ex-
ample, posters are typically found on walls so their
support normal vectors are in the horizontal di-
rections. Any unobserved child categories are as-
sumed to haveP
surf
(S
n
= up|C
c
) = 1 sincemost
things rest on a horizontal surface (e.g., floor).
P
surf
(S
n
|C
c
) =
count(C
c
on surface with S
n
)
count(C
c
)
4.4 Relative Position Priors
We model the relative positions of objects based
on their object categories and current scene type:
i.e., the relative position of an object of category
C
obj
is with respect to another object of category
C
ref
and for a scene type C
s
. We condition on the
relationship R between the two objects, whether
they are siblings (R = Sibling) or child-parent
(R = ChildParent).
P
relpos
(x, y, ?|C
obj
, C
ref
, C
s
, R)
When positioning objects, we restrict the search
space to points on the selected support surface.
The position x, y is the centroid of the target ob-
ject projected onto the support surface in the se-
mantic frame of the reference object. The ? is the
angle between the front of the two objects. We rep-
resent these relative position and orientation pri-
ors by performing kernel density estimation on the
Relation P (relation)
inside(A,B) V ol(A?B)
V ol(A)
outside(A,B) 1 - V ol(A?B)
V ol(A)
left_of(A,B) V ol(A? left_of (B))
V ol(A)
right_of(A,B) V ol(A? right_of (B))
V ol(A)
near(A,B) 1(dist(A,B) < t
near
)
faces(A,B) cos(front(A), c(B)? c(A))
Table 1: Definitions of spatial relation using
bounding boxes. Note: dist(A,B) is normalized
against the maximum extent of the bounding box
ofB. front(A) is the direction of the front vector
of A and c(A) is the centroid of A.
Keyword Top Relations and Scores
behind (back_of, 0.46), (back_side, 0.33)
adjacent (front_side, 0.27), (outside, 0.26)
below (below, 0.59), (lower_side, 0.38)
front (front_of, 0.41), (front_side, 0.40)
left (left_side, 0.44), (left_of, 0.43)
above (above, 0.37), (near, 0.30)
opposite (outside, 0.31), (next_to, 0.30)
on (supported_by, 0.86), (on_top_of, 0.76)
near (outside, 0.66), (near, 0.66)
next (outside, 0.49), (near, 0.48)
under (supports, 0.62), (below, 0.53)
top (supported_by, 0.65), (above, 0.61)
inside (inside, 0.48), (supported_by, 0.35)
right (right_of, 0.50), (lower_side, 0.38)
beside (outside, 0.45), (right_of, 0.45)
Table 2: Map of top keywords to spatial relations
(appropriate mappings in bold).
observed samples. Figure 5 shows predicted posi-
tions of objects using the learned priors.
5 Spatial Relations
We define a set of formal spatial relations that we
map to natural language terms (?5.1). In addi-
tion, we collect annotations of spatial relation de-
scriptions from people, learn a mapping of spatial
keywords to our formal spatial relations, and train
a classifier that given two objects can predict the
likelihood of a spatial relation holding (?5.2).
5.1 Predefined spatial relations
For spatial relations we use a set of predefined rela-
tions: left_of, right_of, above, below, front, back,
supported_by, supports, next_to, near, inside, out-
side, faces, left_side, right_side.3 These are mea-
sured using axis-aligned bounding boxes from the
viewer?s perspective; the involved bounding boxes
are compared to determine volume overlap or clos-
est distance (for proximity relations; see Table 1).
3Wedistinguish left_of(A,B) asA being left of the left edge
of the bounding box of B vs left_side(A,B) as A being left of
the centroid of B.
2031
Feature # Description
delta(A,B) 3 Delta position (x, y, z) between the centroids of A and B
dist(A,B) 1 Normalized distance (wrt B) between the centroids of A and B
overlap(A, f(B)) 6 Fraction of A inside left/right/front/back/top/bottom regions wrt B: V ol(A?f(B))
V ol(A)
overlap(A,B) 2 V ol(A?B)
V ol(A)
and V ol(A?B)
V ol(B)
support(A,B) 2 supported_by(A,B) and supports(A,B)
Table 3: Features for trained spatial relations predictor.
Figure 6: Our data collection task.
Since these spatial relations are resolvedwith re-
spect to the view of the scene, they correspond to
view-centric definitions of spatial concepts.
5.2 Learning Spatial Relations
We collect a set of text descriptions of spatial rela-
tionships between two objects in 3D scenes by run-
ning an experiment on Amazon Mechanical Turk.
We present a set of screenshots of scenes in our
dataset that highlight particular pairs of objects and
we ask people to fill in a spatial relationship of the
form ?The __ is __ the __? (see Fig 6). We col-
lected a total of 609 annotations over 131 object
pairs in 17 scenes. We use this data to learn pri-
ors on view-centric spatial relation terms and their
concrete geometric interpretation.
For each response, we select one keyword from
the text based on length. We learn a mapping of
the top 15 keywords to our predefined set of spa-
tial relations. We use our predefined relations on
annotated spatial pairs of objects to create a binary
indicator vector that is set to 1 if the spatial relation
holds, or zero otherwise. We then create a simi-
lar vector for whether the keyword appeared in the
annotation for that spatial pair, and then compute
the cosine similarity of the two vectors to obtain
a score for mapping keywords to spatial relations.
Table 2 shows the obtained mapping. Using just
the top mapping, we are able to map 10 of the 15
Above
Above On
Left Right
Front Behind
Figure 7: High probability regions where the cen-
ter of another object would occur for some spatial
relations with respect to a table: above (top left),
on (top right), left (mid left), right (mid right), in
front (bottom left), behind (bottom right).
keywords to an appropriate spatial relation. The 5
keywords that are not well mapped are proximity
relations that are not well captured by our prede-
fined spatial relations.
Using the 15 keywords as our spatial relations,
we train a log linear binary classifier for each key-
word over features of the objects involved in that
spatial relation (see Table 3). We then use this
model to predict the likelihood of that spatial re-
lation in new scenes.
Figure 7 shows examples of predicted likeli-
hoods for different spatial relations with respect to
an anchor object in a scene. Note that the learned
spatial relations are much stricter than our prede-
fined relations. For instance, ?above? is only used
to referred to the area directly above the table, not
to the region above and to the left or above and in
front (which our predefined classifier will all con-
sider to be above). In our results, we showwe have
more accurate scenes using the trained spatial re-
lations than the predefined ones.
2032
Dependency Pattern Example Text
{tag:VBN}=verb >nsubjpass {}=nsubj >prep ({}=prep >pobj {}=pobj) The chair[nsubj] is made[verb] of[prep] wood[pobj].
attribute(verb,pobj)(nsubj,pobj) material(chair,wood)
{}=dobj >cop {} >nsubj {}=nsubj The chair[nsubj] is red[dobj].
attribute(dobj)(nsubj,dobj) color(chair,red)
{}=dobj >cop {} >nsubj {}=nsubj >prep ({}=prep >pobj {}=pobj) The table[nsubj] is next[dobj] to[prep] the chair[pobj].
spatial(dobj)(nsubj, pobj) next_to(table,chair)
{}=nsubj >advmod ({}=advmod >prep ({}=prep >pobj {}=pobj)) There is a table[nsubj] next[advmod] to[prep] a chair[pobj].
spatial(advmod)(nsubj, pobj) next_to(table,chair)
Table 4: Example dependency patterns for extracting attributes and spatial relations.
6 Text to Scene generation
We generate 3D scenes from brief scene descrip-
tions using our learned priors.
6.1 Scene Template Parsing
During scene template parsing we identify the
scene type, the objects present in the scene, their
attributes, and the relations between them. The
input text is first processed using the Stanford
CoreNLP pipeline (Manning et al., 2014). The
scene type is determined by matching the words
in the utterance against a list of known scene types
from the scene dataset.
To identify objects, we look for noun phrases
and use the head word as the category, filtering
with WordNet (Miller, 1995) to determine which
objects are visualizable (under the physical object
synset, excluding locations). We use the Stanford
coreference system to determine when the same
object is being referred to.
To identify properties of the objects, we extract
other adjectives and nouns in the noun phrase. We
alsomatch dependency patterns such as ?X ismade
of Y? to extract additional attributes. Based on the
object category and attributes, and other words in
the noun phrase mentioning the object, we identify
a set of associated keywords to be used later for
querying the 3D model database.
Dependency patterns are also used to extract
spatial relations between objects (see Table 4 for
some example patterns). We use Semgrex patterns
to match the input text to dependencies (Cham-
bers et al., 2007). The attribute types are deter-
mined from a dictionary using the text express-
ing the attribute (e.g., attribute(red)=color, at-
tribute(round)=shape). Likewise, spatial relations
are looked up using the learned map of keywords
to spatial relations.
As an example, given the input ?There is a room
with a desk and a red chair. The chair is to the left
of the desk.? we extract the following objects and
spatial relations:
Objects category attributes keywords
o
0
room room
o
1
desk desk
o
2
chair color:red chair, red
Relations: left(o
2
, o
1
)
6.2 Inferring Implicits
From the parsed scene template, we infer the pres-
ence of additional objects and support constraints.
We can optionally infer the presence of addi-
tional objects from object occurrences based on the
scene type. If the scene type is unknown, we use
the presence of known object categories to pre-
dict the most likely scene type by using Bayes?
rule on our object occurrence priors P
occ
to get
P (C
s
|{C
o
}) ? P
occ
({C
o
}|C
s
)P (C
s
). Once we
have a scene type C
s
, we sample P
occ
to find ob-
jects that are likely to occur in the scene. We re-
strict sampling to the top n = 4 object categories.
We can also use the support hierarchy priors
P
support
to infer implicit objects. For instance, for
each object o
i
we find the most likely supporting
object category and add it to our scene if not al-
ready present.
After inferring implicit objects, we infer the sup-
port constraints. Using the learned text to prede-
fined relation mapping from ?5.2, we can map the
keywords ?on? and ?top? to the supported_by re-
lation. We infer the rest of the support hierarchy
by selecting for each object o
i
the parent object o
j
that maximizes P
support
(C
o
j
|C
o
i
).
6.3 Grounding Objects
Once we determine from the input text what ob-
jects exist and their spatial relations, we select 3D
models matching the objects and their associated
properties. Each object in the scene template is
grounded by querying a 3D models database with
2033
?There is a desk and a keyboard and a monitor .?
Input Text Basic + Support Hierarchy + Relative Positions
?There is a coffee table and there is a lamp behind the coffee table. There is a chair in front of the coffee table.?
UPDATE UPDATE
No Relations Predefined Relations Learned Relations
Figure 8: Top Generated scenes for randomly placing objects on the floor (Basic), with inferred Support
Hierarchy, and with priors on Relative Positions. Bottom Generated scenes with no understanding of
spatial relations (No Relations), scoring using Predefined Relations and Learned Relations.
the appropriate category and keywords.
We use a 3D model dataset collected from
Google 3DWarehouse by prior work in scene syn-
thesis and containing about 12490 mostly indoor
objects (Fisher et al., 2012). These models have
text associated with them in the form of names and
tags. In addition, we semi-automatically annotated
models with object category labels (roughly 270
classes). We used model tags to set these labels,
and verified and augmented them manually.
In addition, we automatically rescale models so
that they have physically plausible sizes and orient
them so that they have a consistent up and front
direction (Savva et al., 2014). We then indexed all
models in a database that we query at run-time for
retrieval based on category and tag labels.
6.4 Scene Layout
Once we have instantiated the objects in the scene
by selecting models, we aim to optimize an over-
all layout score L = ?
obj
L
obj
+ ?
rel
L
rel
that is
a weighted sum of object arrangement L
obj
score
and constraint satisfaction L
rel
score:
L
obj
=
?
o
i
P
surf
(S
n
|C
o
i
)
?
o
j
?F (o
i
)
P
relpos
(?)
L
rel
=
?
c
i
P
rel
(c
i
)
where F (o
i
) are the sibling objects and parent ob-
ject of o
i
. We use ?
obj
= 0.25 and ?
rel
= 0.75 for
the results we present.
We use a simple hill climbing strategy to find a
reasonable layout. We first initialize the positions
Figure 9: Generated scene for ?There is a room
with a desk and a lamp. There is a chair to the
right of the desk.? The inferred scene hierarchy is
overlayed in the center.
of objects within the scene by traversing the sup-
port hierarchy in depth-first order, positioning the
children from largest to first and recursing. Child
nodes are positioned by first selecting a supporting
surface on a candidate parent object through sam-
pling of P
surf
. After selecting a surface, we sam-
ple a position on the surface based on P
relpos
. Fi-
nally, we check whether collisions exist with other
objects, rejecting layouts where collisions occur.
We iterate by randomly jittering and repositioning
objects. If there are any spatial constraints that are
not satisfied, we also remove and randomly repo-
sition the objects violating the constraints, and it-
erate to improve the layout. The resulting scene is
rendered and presented to the user.
7 Results and Discussion
We show examples of generated scenes, and com-
pare against naive baselines to demonstrate learned
priors are essential for scene generation. We
2034
Figure 10: Generated scene for ?There is a room
with a poster bed and a poster.?
Figure 11: Generated scene for ?living room?.
also discuss interesting aspects of using spatial
knowledge in view-based object referent resolu-
tion (?7.2) and in disambiguating geometric inter-
pretations of ?on? (?7.3).
Model Comparison Figure 8 shows a compari-
son of scenes generated by our model versus sev-
eral simpler baselines. The top row shows the im-
pact of modeling the support hierarchy and the rel-
ative positions in the layout of the scene. The bot-
tom row shows that the learned spatial relations
can give a more accurate layout than the naive
predefined spatial relations, since it captures prag-
matic implicatures of language, e.g., left is only
used for directly left and not top left or bottom
left (Vogel et al., 2013).
Figure 12: Left: chair is selected using ?the chair
to the right of the table? or ?the object to the right of
the table?. Chair is not selected for ?the cup to the
right of the table?. Right: Different view results
in different chair being selected for the input ?the
chair to the right of the table?.
7.1 Generated Scenes
Support Hierarchy Figure 9 shows a generated
scene along with the input text and support hier-
archy. Even though the spatial relation between
lamp and desk was not mentioned, we infer that the
lamp is supported by the top surface of the desk.
Disambiguation Figure 10 shows a generated
scene for the input ?There is a room with a poster
bed and a poster?. Note that the system differen-
tiates between a ?poster? and a ?poster bed? ? it
correctly selects and places the bed on the floor,
while the poster is placed on the wall.
Inferring objects for a scene type Figure 11
shows an example of inferring all the objects
present in a scene from the input ?living room?.
Some of the placements are good, while others can
clearly be improved.
7.2 View-centric object referent resolution
After a scene is generated, the user can refer to ob-
jects with their categories andwith spatial relations
between them. Objects are disambiguated by both
category and view-centric spatial relations. We use
the WordNet hierarchy to resolve hyponym or hy-
pernym referents to objects in the scene. In Fig-
ure 12 (left), the user can select a chair to the right
of the table using the phrase ?chair to the right of
the table? or ?object to the right of the table?. The
user can then change their viewpoint by rotating
and moving around. Since spatial relations are re-
solved with respect to the current viewpoint, a dif-
ferent chair is selected for the same phrase from a
different viewpoint in the right screenshot.
7.3 Disambiguating ?on?
As shown in ?5.2, the English preposition ?on?,
when used as a spatial relation, corresponds
strongly to the supported_by relation. In our
trained model, the supported_by feature also has
a high positive weight for ?on?.
Our model for supporting surfaces and hierar-
chy allows interpreting the placement of ?A on
B? based on the categories of A and B. Fig-
ure 13 demonstrates four different interpretations
for ?on?. Given the input ?There is a cup on the
table? the system correctly places the cup on the
top surface of the table. In contrast, given ?There
is a cup on the bookshelf?, the cup is placed on a
supporting surface of the bookshelf, but not nec-
essarily the top one which would be fairly high.
2035
Figure 13: From top left clockwise: ?There is a
cup on the table?, ?There is a cup on the book-
shelf?, ?There is a poster on the wall?, ?There is
a hat on the chair?. Note the different geometric
interpretations of ?on?.
Given the input ?There is a poster on the wall?, a
poster is pasted on the wall, while with the input
?There is a hat on the chair? the hat is placed on
the seat of the chair.
7.4 Limitations
While the system shows promise, there are still
many challenges in text-to-scene generation. For
one, we did not address the difficulties of resolving
objects. A failure case of our system stems from
using a fixed set of categories to identify visualiz-
able objects. For example, the sense of ?top? refer-
ring to a spinning top, and other uncommon object
types, are not handled by our system as concrete
objects. Furthermore, complex phrases including
object parts such as ?there?s a coat on the seat of
the chair? are not handled. Figure 14 shows some
Figure 14: Left: A water bottle instead of wine
bottle is selected for ?There is a bottle of wine on
the table in the kitchen?. In addition, the selected
table is inappropriate for a kitchen. Right: A floor
lamp is incorrectly selected for the input ?There is
a lamp on the table?.
example cases where the context is important in
selecting an appropriate object and the difficulties
of interpreting noun phrases.
In addition, we rely on a few dependency pat-
terns for extracting spatial relations so robustness
to variations in spatial language is lacking. We
only handle binary spatial relations (e.g., ?left?,
?behind?) ignoringmore complex relations such as
?around the table? or ?in the middle of the room?.
Though simple binary relations are some of the
most fundamental spatial expressions and a good
first step, handling more complex expressions will
do much to improve the system.
Another issue is that the interpretation of sen-
tences such as ?the desk is covered with paper?,
which entails many pieces of paper placed on the
desk, is hard to resolve. With a more data-driven
approach we can hope to link such expressions to
concrete facts.
Finally, we use a traditional pipeline approach
for text processing, so errors in initial stages
can propagate downstream. Failures in depen-
dency parsing, part of speech tagging, or coref-
erence resolution can result in incorrect interpre-
tations of the input language. For example, in
the sentence ?there is a desk with a chair in front
of it?, ?it? is not identified as coreferent with
?desk? so we fail to extract the spatial relation
front_of(chair, desk).
8 Related Work
There is related prior work in the topics of mod-
eling spatial relations, generating 3D scenes from
text, and automatically laying out 3D scenes.
8.1 Spatial knowledge and relations
Prior work that required modeling spatial knowl-
edge has defined representations specific to the
task addressed. Typically, such knowledge is man-
ually provided or crowdsourced ? not learned from
data. For instance, WordsEye (Coyne et al., 2010)
uses a set of manually specified relations. The
NLP community has explored grounding text to
physical attributes and relations (Matuszek et al.,
2012; Krishnamurthy and Kollar, 2013), gener-
ating text for referring to objects (FitzGerald et
al., 2013) and connecting language to spatial re-
lationships (Vogel and Jurafsky, 2010; Golland et
al., 2010; Artzi and Zettlemoyer, 2013). Most
of this work focuses on learning a mapping from
text to formal representations, and does not model
2036
implicit spatial knowledge. Many priors on real
world spatial facts are typically unstated in text and
remain largely unaddressed.
8.2 Text to Scene Systems
Early work on the SHRDLU system (Winograd,
1972) gives a good formalization of the linguis-
tic manipulation of objects in 3D scenes. By re-
stricting the discourse domain to a micro-world
with simple geometric shapes, the SHRDLU sys-
tem demonstrated parsing of natural language in-
put for manipulating scenes. However, generaliza-
tion to more complex objects and spatial relations
is still very hard to attain.
More recently, a pioneering text-to-3D scene
generation prototype system has been presented by
WordsEye (Coyne and Sproat, 2001). The authors
demonstrated the promise of text to scene genera-
tion systems but also pointed out some fundamen-
tal issues which restrict the success of their system:
much spatial knowledge is required which is hard
to obtain. As a result, users have to use unnatural
language (e.g., ?the stool is 1 feet to the south of
the table?) to express their intent. Follow up work
has attempted to collect spatial knowledge through
crowd-sourcing (Coyne et al., 2012), but does not
address the learning of spatial priors.
We address the challenge of handling natural
language for scene generation, by learning spatial
knowledge from 3D scene data, and using it to in-
fer unstated implicit constraints. Our work is simi-
lar in spirit to recent work on generating 2D clipart
for sentences using probabilistic models learned
from data (Zitnick et al., 2013).
8.3 Automatic Scene Layout
Work on scene layout has focused on determining
good furniture layouts by optimizing energy func-
tions that capture the quality of a proposed layout.
These energy functions are encoded from design
guidelines (Merrell et al., 2011) or learned from
scene data (Fisher et al., 2012). Knowledge of ob-
ject co-occurrences and spatial relations is repre-
sented by simple models such as mixtures of Gaus-
sians on pairwise object positions and orientations.
We leverage ideas from this work, but they do not
focus on linking spatial knowledge to language.
9 Conclusion and Future Work
We have demonstrated a representation of spatial
knowledge that can be learned from 3D scene data
and how it corresponds to natural language. We
also showed that spatial inference and grounding is
critical for achieving plausible results in the text-
to-3D scene generation task. Spatial knowledge is
critically useful not only in this task, but also in
other domains which require an understanding of
the pragmatics of physical environments.
We only presented a deterministic approach for
mapping input text to the parsed scene template.
An interesting avenue for future research is to
automatically learn how to parse text describing
scenes into formal representations by using more
advanced semantic parsing methods.
We can also improve the representation used for
spatial priors of objects in scenes. For instance, in
this paper we represented support surfaces by their
orientation. We can improve the representation by
modeling whether a surface is an interior or exte-
rior surface.
Another interesting line of future work would
be to explore the influence of object identity in de-
termining when people use ego-centric or object-
centric spatial reference models, and to improve
resolution of spatial terms that have different in-
terpretations (e.g., ?the chair to the left of John? vs
?the chair to the left of the table?).
Finally, a promising line of research is to explore
using spatial priors for resolving ambiguities dur-
ing parsing. For example, the attachment of ?next
to? in ?Put a lamp on the table next to the book? can
be readily disambiguated with spatial priors such
as the ones we presented.
Acknowledgments
We thank the anonymous reviewers for their
thoughtful comments. We gratefully acknowl-
edge the support of the Defense Advanced Re-
search Projects Agency (DARPA) Deep Explo-
ration and Filtering of Text (DEFT) Program under
Air Force Research Laboratory (AFRL) contract
no. FA8750-13-2-0040. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associ-
ation for Computational Linguistics.
2037
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
and Christopher D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing.
Bob Coyne and Richard Sproat. 2001. WordsEye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques.
Bob Coyne, Richard Sproat, and Julia Hirschberg.
2010. Spatial relations in text-to-scene conversion.
InComputational Models of Spatial Language Inter-
pretation, Workshop at Spatial Cognition.
BobCoyne, Alexander Klapheke, MasoudRouhizadeh,
Richard Sproat, and Daniel Bauer. 2012. Annota-
tion tools and knowledge representation for a text-to-
scene system. Proceedings of COLING 2012: Tech-
nical Papers.
Desmond Elliott and Frank Keller. 2013. Image de-
scription using visual dependency representations.
In Proceedings of Empirical Methods in Natural
Language Processing (EMNLP).
Matthew Fisher, Daniel Ritchie, Manolis Savva,
Thomas Funkhouser, and Pat Hanrahan. 2012.
Example-based synthesis of 3D object arrangements.
ACM Transactions on Graphics (TOG).
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Pro-
ceedings of Empirical Methods in Natural Language
Processing (EMNLP).
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP).
Alan D. Kalvin and Russell H. Taylor. 1996. Super-
faces: Polygonal mesh simplification with bounded
error. Computer Graphics and Applications, IEEE.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connecting
natural language to the physical world. Transactions
of the Association for Computational Linguistics.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In Computer Vision
and Pattern Recognition (CVPR), 2011 IEEE Con-
ference on.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations.
Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In International Conference onMa-
chine Learning (ICML).
Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh
Agrawala, and Vladlen Koltun. 2011. Interactive
furniture layout using interior design guidelines. In
ACM Transactions on Graphics (TOG).
George A. Miller. 1995. WordNet: a lexical database
for English. Communications of the ACM.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, andHal Daum? III. 2012.
Midge: Generating image descriptions from com-
puter vision detections. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Manolis Savva, Angel X. Chang, Gilbert Bernstein,
Christopher D. Manning, and Pat Hanrahan. 2014.
On being the right scale: Sizing large collections of
3D models. Stanford University Technical Report
CSTR 2014-03.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of ACL.
Adam Vogel, Christopher Potts, and Dan Jurafsky.
2013. Implicatures and nested beliefs in approxi-
mate Decentralized-POMDPs. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics.
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive psychology.
C. Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE Intenational Conference on
Computer Vision (ICCV).
2038
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 17?21,
Baltimore, Maryland USA, June 26 2014. c?2014 Association for Computational Linguistics
Semantic Parsing for Text to 3D Scene Generation
Angel X. Chang, Manolis Savva and Christopher D. Manning
Computer Science Department, Stanford University
angelx,msavva,manning@cs.stanford.edu
Figure 1: Generated scene for ?There is a room
with a chair and a computer.? Note that the system
infers the presence of a desk and that the computer
should be supported by the desk.
1 Introduction
We propose text-to-scene generation as an appli-
cation for semantic parsing. This is an applica-
tion that grounds semantics in a virtual world that
requires understanding of common, everyday lan-
guage. In text to scene generation, the user pro-
vides a textual description and the system gener-
ates a 3D scene. For example, Figure 1 shows the
generated scene for the input text ?there is a room
with a chair and a computer?. This is a challeng-
ing, open-ended problem that prior work has only
addressed in a limited way.
Most of the technical challenges in text to
scene generation stem from the difficulty of map-
ping language to formal representations of vi-
sual scenes, as well as an overall absence of real
world spatial knowledge from current NLP sys-
tems. These issues are partly due to the omis-
sion in natural language of many facts about the
world. When people describe scenes in text, they
typically specify only important, relevant informa-
tion. Many common sense facts are unstated (e.g.,
chairs and desks are typically on the floor). There-
fore, we focus on inferring implicit relations that
are likely to hold even if they are not explicitly
stated by the input text.
Text to scene generation offers a rich, interactive
environment for grounded language that is famil-
iar to everyone. The entities are common, every-
day objects, and the knowledge necessary to ad-
dress this problem is of general use across many
domains. We present a system that leverages user
interactionwith 3D scenes to generate training data
for semantic parsing approaches.
Previous semantic parsing work has dealt with
grounding text to physical attributes and rela-
tions (Matuszek et al., 2012; Krishnamurthy and
Kollar, 2013), generating text for referring to ob-
jects (FitzGerald et al., 2013) and with connect-
ing language to spatial relationships (Golland et
al., 2010; Artzi and Zettlemoyer, 2013). Seman-
tic parsing methods can also be applied to many
aspects of text to scene generation. Furthermore,
work on parsing instructions to robots (Matuszek
et al., 2013; Tellex et al., 2014) has analogues in
the context of discourse about physical scenes.
In this extended abstract, we formalize the text
to scene generation problem and describe it as a
task for semantic parsing methods. To motivate
this problem, we present a prototype system that
incorporates simple spatial knowledge, and parses
natural text to a semantic representation. By learn-
ing priors on spatial knowledge (e.g., typical posi-
tions of objects, and common spatial relations) our
system addresses inference of implicit spatial con-
straints. The user can interactively manipulate the
generated scene with textual commands, enabling
us to refine and expand learned priors.
Our current system uses deterministic rules to
map text to a scene representation but we plan to
explore training a semantic parser from data. We
can leverage our system to collect user interactions
for training data. Crowdsourcing is a promising
avenue for obtaining a large scale dataset.
17
Objects:PLATE, FORK
ON(FORK, TABLE)ON(PLATE, TABLE)ON(CAKE, PLATE)
?There is a piece of cake on a table.?
Scene Generation
3D ModelsSpatial KB
Objects:CAKE, TABLE
ON(CAKE, TABLE)SemanticParsing
INTERACTION
Scene Inference
ObjectSelection
Figure 2: Illustration of our system architecture.
2 Task Definition
We define text to scene generation as the task of
taking text describing a scene as input, and gen-
erating a plausible 3D scene described by that
text as output. More concretely, we parse the
input text into a scene template, which places
constraints on what objects must be present and
relationships between them. Next, using priors
from a spatial knowledge base, the system expands
the scene template by inferring additional implicit
constraints. Based on the scene template, we select
objects from a dataset of 3D models and arrange
them to generate an output scene.
After a scene is generated, the user can interact
with the scene using both textual commands and
mouse interactions. During interaction, semantic
parsing can be used to parse the input text into
a sequence of scene interaction commands. See
Figure 2 for an illustration of the system archi-
tecture. Throughout the process, we need to ad-
dress grounding of language to: 1) actions to be
performed, 2) objects to be instantiated or manip-
ulated, and 3) constraints on the objects.
2.1 Scene Template
A scene template T = (O, C) consists of a set
of object descriptions O = {o
1
, . . . , o
n
} and con-
straints C = {c
1
, . . . , c
k
} on the relationships be-
tween the objects. For each object o
i
, we identify
properties associated with it such as category la-
bel, basic attributes such as color and material, and
number of occurrences in the scene. Based on the
object category and attributes, and other words in
the noun phrase mentioning the object, we iden-
tify a set of associated keywords to be used later
for querying the 3D model database. Spatial rela-
tions between objects are extracted as predicates of
the form on(o
i
, o
j
) or left(o
i
, o
j
) where o
i
and o
j
are recognized objects.
As an example, given the input ?There is a room
with a desk and a red chair. The chair is to the left
of the desk.? we extract the following objects and
spatial relations:
Objects category attributes keywords
o
0
room room
o
1
desk desk
o
2
chair color:red chair, red
Relations: left(o
2
, o
1
)
2.2 Scene Interaction Commands
During interaction, we parse textual input provided
by the user into a sequence of commands with rele-
vant parts of the scene as arguments. For example,
given a scene S, we use the input text to identify a
subset of relevant objects matchingX = {O
s
, C
s
}
where O
s
is the set of object descriptions and C
s
is the set of object constraints. Commands can
then be resolved against this argument to manip-
ulate the scene state: Select(X), Remove(X),
Insert(X), Replace(X,Y ), Move(X,?X),
Scale(X,?X), and Orient(X,?X). X and Y
are semantic representations of objects, while?X
is a change to be applied to X , expressed as either
a target condition (?put the lamp on the table?) or
a relative change (?move the lamp to the right?).
These basic operations demonstrate possible
scene manipulations through text. This set of op-
erations can be enlarged to cover manipulation of
parts of objects (?make the seat of the chair red?),
and of the viewpoint (?zoom in on the chair?).
2.3 Spatial Knowledge
One of the richest sources of spatial knowledge
is 3D scene data. Prior work by (Fisher et al.,
2012) collected 133 small indoor scenes created
with 1723 3D Warehouse models. Based on their
approach, we create a spatial knowledge base with
priors on the static support hierarchy of objects in
scenes1, their relative positions and orientations.
We also define a set of spatial relations such as left,
right, above, below, front, back, on top of, next to,
near, inside, and outside. Table 1 gives examples
of the definitions of these spatial relations.
We use a 3D model dataset collected from
Google 3DWarehouse by prior work in scene syn-
1A static support hierarchy represents which objects are
likely to support which other objects on their surface (e.g.,
the floor supports tables, tables support plates).
18
Relation P (relation)
inside(A,B) V ol(A?B)
V ol(A)
right(A,B) V ol(A? right (B))
V ol(A)
near(A,B) 1(dist(A,B) < t
near
)
Table 1: Definitions of spatial relation using object
bounding box computations.
thesis and containing about 12490 mostly indoor
objects (Fisher et al., 2012). These models have
text associated with them in the form of names and
tags, and category labels. In addition, we assume
the models have been scaled to physically plausi-
ble sizes and oriented with consistent up and front
direction (Savva et al., 2014). All models are in-
dexed in a database so they can be queried at run-
time for retrieval.
3 System Description
We present how the parsed representations are
used by our system to demonstrate the key issues
that have to be addressed during text to scene gen-
eration. Our current implementation uses a sim-
ple deterministic approach to map text to the scene
template and user actions on the scene. We use the
Stanford CoreNLP pipeline2 to process the input
text and use rules to match dependency patterns.
3.1 Scene generation
During scene generation, we want to construct the
most likely scene given the input text. We first
parse the text into a scene template and use it to
select appropriate models from the database. We
then perform object layout and arrangement given
the priors on spatial knowledge.
Scene Template Parsing We use the Stanford
coreference system to determine when the same
object is being referred to. To identify objects,
we look for noun phrases and use the head word
as the category, filtering with WordNet (Miller,
1995) to determine which objects are visualizable
(under the physical object synset, excluding loca-
tions). To identify properties of the objects, we ex-
tract other adjectives and nouns in the noun phrase.
We also match syntactic dependency patterns such
as ?X is made of Y? to extract more attributes and
keywords. Finally, we use dependency patterns to
extract spatial relations between objects.
2http://nlp.stanford.edu/software/corenlp.shtml
Figure 3: Select ?a blue office chair? and ?a
wooden desk? from the models database
Object Selection Once we have the scene tem-
plate, we use the keywords associated with each
object to query the model database. We select ran-
domly from the top 10 results for variety and to
allow the user to regenerate the scene with differ-
ent models. This step can be enhanced to take into
account correlations between objects (e.g., a lamp
on a table should not be a floor lamp model). See
Figure 3 for an example of object selection.
Object Layout Given the selected models, the
source scene template, and priors on spatial rela-
tions, we find an arrangement of the objects within
the scene that maximizes the probability of the lay-
out under the given scene template.
3.2 Scene Interaction
Here we address parsing of text after a scene has
been generated and during interaction sessions.
Command Parsing We deterministically map
verbs to possible actions as shown in Table 2.
Multiple actions are possible for some verbs (e.g.,
?place? and ?put? can refer to either Move or
Insert). To differentiate between these, we as-
sume new objects are introduced with the indefi-
nite article ?a? whereas old ones are modified with
the definite article ?the?.
Object Resolution To allow interaction with the
scene, wemust resolve references to objects within
a scene. Objects are disambiguated by category
and view-centric spatial relations. In addition to
matching objects by their categories, we use the
WordNet hierarchy to handle hyponym or hyper-
nym referents. Depending on the current view,
spatial relations such as ?left? or ?right? can refer
to different objects (see Figure 4).
Scene Modification Based on the action we
need to appropriately modify the current scene.
19
verb Action Example Text Example Parse
generate Generate generate a room with a desk and a lamp Generate( {room,desk,lamp} , {}) )
select Select select the chair on the right of the table Select({lamp},{right(lamp,table)})
add, insert Insert add a lamp to the table Insert({lamp},{on(lamp,table)})
delete, remove Remove remove the lamp Remove({lamp})
move Move move the chair to the left Move({chair},{left(chair)})
place, put Move, Insert put the lamp on the table Move({lamp},{on(lamp,table)})
replace Replace replace the lamp with a vase Replace({lamp},{vase})
Table 2: Mapping of verbs to possible actions.
Figure 4: Left: chair is selected by ?chair to the
right of the table? or ?object to the right of the ta-
ble?, but not selected by ?cup to the right of the
table?. Right: Different view results in a different
chair selection for ?chair to the right of the table?.
Figure 5: Left: initial scene. Right: after input
?Put a lamp on the table?.
We do this by maximizing the probability of a new
scene template given the requested action and pre-
vious scene template (see Figure 5 for an example).
4 Future Directions
We described a system prototype to motivate ap-
proaching text to scene generation as a semantic
parsing application. While this prototype illus-
trates inference of implicit constraints using prior
knowledge, it still relies on hand coded rules for
mapping text to the scene representation. This is
similar to most previous work on text to scene gen-
eration (Winograd, 1972; Coyne and Sproat, 2001)
and limits handling of natural language. More re-
cently, (Zitnick et al., 2013) used data to learn how
to ground sentences to a CRF representing 2D cli-
part scenes. Similarly, we plan to investigate using
data to learn how to ground sentences to 3D scenes.
Spatial knowledge can be helpful for resolving
ambiguities during parsing. For instance, from
spatial priors of object positions and reasoning
with physical constraints we can disambiguate the
attachment of ?next to? in ?there is a book on the
table next to the lamp?. The book and lamp are
likely on the table and thus next_to(book, lamp)
should be more likely.
User interaction is a natural part of text to scene
generation. We can leverage such interaction to
obtain data for training a semantic parser. Every
time the user issues a command, the user can indi-
cate whether the result of the interaction was cor-
rect or not, and optionally provide a rating. By
keeping track of these scene interactions and the
user ratings we can construct a corpus of tuples
containing: user action, parsed scene interaction,
scene operation, scene state before and after the
operation, and rating by the user. By building up
such a corpus over multiple interactions and users,
we obtain data for training semantic parsers.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associ-
ation for Computational Linguistics.
Bob Coyne and Richard Sproat. 2001. WordsEye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques.
Matthew Fisher, Daniel Ritchie, Manolis Savva,
Thomas Funkhouser, and Pat Hanrahan. 2012.
Example-based synthesis of 3D object arrangements.
ACM Transactions on Graphics.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Pro-
ceedings of the Conference on EMNLP.
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 conference on
EMNLP.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connecting
20
natural language to the physical world. Transactions
of the Association for Computational Linguistics.
Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In International Conference onMa-
chine Learning.
Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,
and Dieter Fox. 2013. Learning to parse natural
language commands to a robot control system. In
Experimental Robotics.
G.A. Miller. 1995. WordNet: a lexical database for
english. CACM.
Manolis Savva, Angel X. Chang, Gilbert Bernstein,
Christopher D. Manning, and Pat Hanrahan. 2014.
On being the right scale: Sizing large collections of
3D models. Stanford University Technical Report
CSTR 2014-03.
Stefanie Tellex, Pratiksha Thaker, Joshua Joseph, and
Nicholas Roy. 2014. Learning perceptually
grounded word meanings from unaligned parallel
data. Machine Learning.
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive psychology.
C. Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE Intenational Conference on
Computer Vision (ICCV).
21
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 14?21,
Baltimore, Maryland, USA, June 27, 2014. c?2014 Association for Computational Linguistics
Interactive Learning of Spatial Knowledge
for Text to 3D Scene Generation
Angel X. Chang, Manolis Savva and Christopher D. Manning
Computer Science Department, Stanford University
{angelx,msavva,manning}@cs.stanford.edu
Abstract
We present an interactive text to 3D scene
generation system that learns the expected
spatial layout of objects from data. A user
provides input natural language text from
which we extract explicit constraints on
the objects that should appear in the scene.
Given these explicit constraints, the sys-
tem then uses prior observations of spa-
tial arrangements in a database of scenes
to infer the most likely layout of the ob-
jects in the scene. Through further user
interaction, the system gradually adjusts
and improves its estimates of where ob-
jects should be placed. We present exam-
ple generated scenes and user interaction
scenarios.
1 Introduction
People possess the power of visual imagination
that allows them to turn descriptions of scenes into
imagery. The conceptual simplicity of generating
pictures from descriptions has spurred the desire
to make systems capable of this task. However, re-
search into computational systems for creating im-
agery from textual descriptions has seen only lim-
ited success.
Most current 3D scene design systems require
the user to learn complex manipulation interfaces
through which objects are constructed and pre-
cisely positioned within scenes. However, arrang-
ing objects in scenes can much more easily be
achieved using natural language. For instance, it
is much easier to say ?Put a cup on the table?,
rather than having to search for a 3D model of a
cup, insert it into the scene, scale it to the correct
size, orient it, and position it on a table ensuring
it maintains contact with the table. By making
3D scene design more accessible to novice users
we empower a broader demographic to create 3D
scenes for use cases such as interior design, virtual
storyboarding and personalized augmented reality.
Unfortunately, several key technical challenges
restrict our ability to create text to 3D scene sys-
tems. Natural language is difficult to map to for-
mal representations of spatial knowledge and con-
straints. Furthermore, language rarely mentions
common sense facts about the world, that contain
critically important spatial knowledge. For exam-
ple, people do not usually mention the presence of
the ground or that most objects are supported by it.
As a consequence, spatial knowledge is severely
lacking in current computational systems.
Pioneering work in mapping text to 3D scene
representations has taken two approaches to ad-
dress these challenges. First, by restricting the dis-
course domain to a micro-world with simple geo-
metric shapes, the SHRDLU system demonstrated
parsing of natural language input for manipulating
the scene, and learning of procedural knowledge
through interaction (Winograd, 1972). However,
generalization to scenes with more complex ob-
jects and spatial relations is very hard to attain.
More recently, the WordsEye system has fo-
cused on the general text to 3D scene generation
task (Coyne and Sproat, 2001), allowing a user
to generate a 3D scene directly from a textual de-
scription of the objects present, their properties and
their spatial arrangement. The authors of Words-
Eye demonstrated the promise of text to scene gen-
eration systems but also pointed out some funda-
mental issues which restrict the success of their
system: a lot of spatial knowledge is required
which is hard to obtain. As a result, the user has to
use unnatural language (e.g. ?the stool is 1 feet to
the south of the table?) to express their intent.
For a text to scene system to understand more
natural text, it must be able to infer implicit in-
formation not explicitly stated in the text. For in-
stance, given the sentence ?there is an office with
a red chair?, the system should be able to infer
14
that the office also has a desk in front of the chair.
This sort of inference requires a source of prior
spatial knowledge. We propose learning this spa-
tial knowledge from existing 3D scene data. How-
ever, since the number of available scenes is small,
it is difficult to have broad coverage. Therefore,
we also rely on user interaction to augment and
grow the spatial knowledge. Luckily, user inter-
action is also natural for scene design since it is an
inherently interactive process where user input is
needed for refinement.
Our contributions address the fundamental chal-
lenges of establishing and interactively expanding
a spatial knowledge base. We build on prior work
in data-driven scene synthesis (Fisher et al., 2012)
to automatically extract general spatial knowledge
from data: knowledge of what objects occur in
scenes, and their expected spatial relations. Our
system then uses this knowledge to generate scenes
from natural text inferring implicit constraints. It
then leverages user interaction to allow refinement
of the scene, and improve the spatial knowledge
base. We demonstrate that user interaction is criti-
cal in expanding and improving spatial knowledge
learned from data.
2 Background
A key insight for enabling text to scene generation
is that linguistic and non-linguistic spatial knowl-
edge is critical for this task and can be learned di-
rectly from data representing the physical world
and from interactions of people with such data.
User feedback allows us to interactively update
spatial knowledge, an idea that we illustrate here
in the domain of spatial relations. Early work on
the PUT system (Clay andWilhelms, 1996) and the
SHRDLU system (Winograd, 1972) gives a good
formalization of the interactive linguistic manipu-
lation of objects in 3D scenes. Recently, there has
been promising work on generating 2D clipart for
sentences using probabilistic models with place-
ment priors learned from data (Zitnick et al., 2013).
2.1 Text to Scene Systems
Prior work on text to 3D scene generation has re-
sulted in systems such as WordsEye (Coyne and
Sproat, 2001) and other similar approaches (Sev-
ersky and Yin, 2006). These systems are typi-
cally not designed to be fully interactive and do not
leverage user interaction to improve their results.
Furthermore, they mostly rely on manual annota-
tion of 3Dmodels and on hand crafted rules to map
text to object placement decisions, which makes
them hard to extend and generalize. More re-
cent work has used crowdsourcing platforms, such
as Amazon Mechanical Turk, to collect necessary
annotations (Coyne et al., 2012). However, this
data collection is treated as a separate pre-process
and the user still has no influence on the system?s
knowledge base. We address one part of this is-
sue: learning simple spatial knowledge from data
and interactively updating it through user feed-
back. We also infer unstated implicit constraints
thus allowing for more natural text input.
2.2 Automatic Scene Layout
Prior work on scene layout has focused largely on
room interiors and determining good furniture lay-
outs by optimizing energy functions that capture
the quality of a proposed layout. These energy
functions are encoded from interior design guide-
lines (Merrell et al., 2011) or learned from input
scene data (Fisher et al., 2012). Knowledge of ob-
ject co-occurrences and spatial relations is repre-
sented by simple models such as mixtures of Gaus-
sians on pairwise object positions and orientations.
Methods to learn scene structure have been demon-
strated using various data sources including sim-
ulation of human agents in 3D scenes (Jiang et
al., 2012; Jiang and Saxena, 2013), and analysis
of supporting contact points in scanned environ-
ments (Rosman and Ramamoorthy, 2011).
However, prior work has not explored methods
for enabling users of scene generation algorithms
to interactively refine and improve an underlying
spatial knowledge model ? a capability which is
critically important. Our work focuses on demon-
strating an interactive system which allows a user
to manipulate and refine such spatial knowledge.
Such a system is useful regardless of the algorithm
used to get the input spatial knowledge.
2.3 Interactive Learning
In many tasks, user interaction can provide feed-
back to an automated system and guide it towards
a desired goal. There is much prior work in various
domains including interactive systems for refin-
ing image search algorithms (Fogarty et al., 2008)
and for manipulating social network group cre-
ation (Amershi et al., 2012). We focus on the do-
main of text to 3D scene generation where despite
the success of data-driven methods there has been
little work on interactive learning systems.
15
3 Approach Overview
What should an interactive text to scene system
look like from the perspective of a user? The user
should be able to provide a brief scene description
in natural language as input. The system parses
this text to a set of explicitly provided constraints
on what objects should be present, and how they
are arranged. This set of constraints should be au-
tomatically expanded by using prior knowledge so
that ?common sense? facts are reflected in the gen-
eral scene ? an example is the static support hier-
archy for objects in the scene (i.e. plate goes on
table, table goes on ground). The system gener-
ates a candidate scene and then the user is free to
interact with it by direct control or through textual
commands. The system can then leverage user in-
teraction to update its spatial knowledge and inte-
grate newly learned constraints or relations. The
final output is a 3D scene that can be viewed from
any position and rendered by a graphics engine. In
this paper we select an initial viewpoint such that
objects are in the frame and view-based spatial re-
lations are satisfied.
How might we create such a system? Spatial
knowledge is critical for this task. We need it to
understand spatial language, to plausibly position
objects within scenes and to allow users to manip-
ulate them. We learn spatial knowledge from ex-
ample scene data to ensure that our approach can
be generalized to different scenarios. We also learn
from user interaction to refine and expand existing
spatial knowledge. In ?5 we describe the spatial
knowledge used by our system.
We define our problem as the task of taking text
describing a scene as input, and generating a plau-
sible 3D scene described by that text as output.
More concretely, based on the input text, we se-
lect objects from a dataset of 3D models (?4) and
arrange them to generate output scenes. See Fig-
ure 1 for an illustration of the system architecture.
We break the system down into several subtasks:
Constraint Parsing (?6): Parse the input textual
description of a concrete scene into a set of con-
straints on the objects present and spatial relations
between them. Automatically expand this set of
constraints to account for implicit constraints not
specified in the text.
SceneGeneration (?7): Using above constraints
and prior knowledge on the spatial arrangement of
objects, construct a scene template. Next, sample
Objects:PLATE, FORK
ON(FORK, TABLE)ON(PLATE, TABLE)ON(CAKE, PLATE)
?There is a piece of cake on a table.?
Create SceneIdentify missing objects
3D ModelsSpatial KB
Objects:CAKE, TABLE
ON(CAKE, TABLE)
Identify objects and relationships
INTERACTION
CONSTRAINTPARSING
Figure 1: Diagram illustrating the architecture of
our system.
the template and select a set of objects to be in-
stantiated. Finally, optimize the placement of the
objects to finalize the arrangement of the scene.
Interaction and Learning (?8): Provide means
for a user to interactively adjust the scene through
direct manipulation and textual commands. Use
any such interaction to update the system?s spatial
knowledge so it better captures the user?s intent.
4 Object Knowledge from 3D Models
To generate scenes we need to have a collection
of 3D models for representing physical objects.
We use a 3D model dataset collected from Google
3D Warehouse by prior work in scene synthe-
sis and containing about 12490 mostly indoor ob-
jects (Fisher et al., 2012). These models have text
associated with them in the form of names and
tags. In addition, we semi-automatically annotated
models with object category labels (roughly 270
classes). We used model tags to set these labels,
and verified and augmented them manually.
In addition, we automatically rescale models so
that they have physically plausible sizes and orient
them so that they have a consistent up and front
direction (Savva et al., 2014). Due to the num-
ber of models in the database, not all models were
rescaled and re-oriented. We then indexed all mod-
els in a database that we query at run-time for re-
trieval based on category and tag labels.
5 Spatial Knowledge
Here we describe how we learn spatial knowledge
from existing scene data. We base our approach
on that of (Fisher et al., 2012) and use their dataset
16
of 133 small indoor scenes created with 1723 3D
Warehouse models. Relative object-to-object po-
sition and orientation priors can also be learned
from the scene data but we have not yet incorpo-
rated them in the results for this paper.
5.1 Support Hierarchy
We observe the static support relations of objects
in existing scenes to establish a prior over what ob-
jects go on top of what other objects. As an exam-
ple, by observing plates and forks on tables most
of the time, we establish that tables are more likely
to support plates and forks than chairs. We esti-
mate the probability of a parent category C
p
sup-
porting a given child category C
c
as a simple con-
ditional probability based on normalized observa-
tion counts.
P
support
(C
p
|C
c
) =
count(C
c
on C
p
)
count(C
c
)
5.2 Supporting surfaces
To identify which surfaces on parent objects sup-
port child objects, we first segment parent models
into planar surfaces using a simple region-growing
algorithm based on (Kalvin and Taylor, 1996). We
characterize support surfaces by the direction of
their normal vector limited to the six canonical di-
rections: up, down, left, right, front, back. We then
learn a probability of supporting surface normal
direction S
n
given child object category C
c
. For
example, posters are typically found on walls so
their support normal vectors are in the horizontal
directions. Any unobserved child categories are
assumed to have P
surf
(S
n
= up|C
c
) = 1 since
most things rest on a horizontal surface (e.g. floor).
P
surf
(S
n
|C
c
) =
count(C
c
on surface with S
n
)
count(C
c
)
5.3 Spatial Relations
For spatial relations we use a set of predefined re-
lations: left, right, above, below, front, back, on
top of, next to, near, inside, and outside. These
are measured using axis-aligned bounding boxes
from the viewer?s perspective. More concretely,
the bounding boxes of the two objects involved in
a spatial relation are compared to determine vol-
ume overlap or closest distance (for proximity re-
lations). Table 1 gives a few examples of the defi-
nitions of these spatial relations.
Since these spatial relations are resolvedwith re-
spect to the current view of the scene, they corre-
spond to view-centric definitions of these spatial
Relation P (relation)
inside(A,B) V ol(A?B)
V ol(A)
outside(A,B) 1 - V ol(A?B)
V ol(A)
left(A,B) V ol(A? left (B))
V ol(A)
right(A,B) V ol(A? right (B))
V ol(A)
near(A,B) 1(dist(A,B) < t
near
)
Table 1: Definitions of spatial relation using object
bounding box computations. Note that dist(A,B)
is normalized with respect to the maximum extent
of the bounding box of B.
concepts. An interesting line of future work would
be to explore when ego-centric and object-centric
spatial reference models are more likely in a given
utterance, and resolve the spatial term accordingly.
6 Constraint Parsing
During constraint parsing we take the input text
and identify the objects and the relations between
them. For each object, we also identify proper-
ties associated with it such as category label, ba-
sic attributes such as color and material, and num-
ber of occurrences in the scene. Based on the ob-
ject category and attributes, and other words in
the noun phrase mentioning the object, we iden-
tify a set of associated keywords to be used later
for querying the 3D model database. Spatial re-
lations between objects are extracted as predicates
of the form on(A,B) or left(A,B) where A and B are
recognized objects.
As an example, given the input ?There is a
room with a desk and a red chair. The chair is
to the left of the desk.? we extract the following
objects and spatial relations:
Objects:
index category attributes keywords
0 room room
1 desk desk
2 chair color:red chair, red
Relations: left(chair, desk)
The input text is processed using the Stanford
CoreNLP pipeline1. We use the Stanford corefer-
ence system to determine when the same object is
being referred to. To identify objects, we look for
noun phrases and use the head word as the cate-
gory, filtering with WordNet (Miller, 1995) to de-
termine which objects are visualizable (under the
1http://nlp.stanford.edu/software/corenlp.shtml
17
Dependency Pattern Example Text
tag:VBN=verb >nsubjpass =nsubj >prep (=prep >pobj =pobj) The chair[nsubj] is made[verb] of[prep] wood[pobj]
tag:VB=verb >dobj =dobj >prep (=prep >pobj =pobj) Put[verb] the cup[dobj] on[prep] the table[pobj]
Table 2: Example dependency patterns for extracting spatial relations.
Figure 2: Generated scene for ?There is a room
with a desk and a lamp. There is a chair to the
right of the desk.? The inferred scene hierarchy is
overlayed in the center.
physical object synset, excluding locations). To
identify properties of the objects, we extract other
adjectives and nouns in the noun phrase. We also
match dependency patterns such as ?X is made of
Y? to extract more attributes and keywords. Fi-
nally, we use dependency patterns to extract spa-
tial relations between objects (see Table 2 for some
example patterns).
We used a fairly simple deterministic approach
to map text to the scene template and user actions
on the scene. An interesting avenue for future re-
search is to automatically learn how to map text
using more advanced semantic parsing methods.
7 Scene Generation
During scene generation we aim to find the most
likely scene given the input utterance, and prior
knowledge. Once we have determined from the
input text what objects exist and their spatial re-
Figure 3: Generated scene for ?There is a room
with a poster bed and a poster.?
Figure 4: Generated scene for ?There is a room
with a table and a sandwich.? Note that the plate is
not explicitly stated, but is inferred by the system.
lations in the scene, we select 3D models match-
ing the objects and their associated properties. We
sample the support hierarchy prior P
support
to ob-
tain the support hierarchy for the scene.
We then initialize the positions of objects within
the scene by traversing the support hierarchy in
depth-first order, positioning the largest available
child node and recursing. Child nodes are posi-
tioned by selecting a supporting surface on a can-
didate parent object through sampling ofP
surf
and
ensuring no collisions exist with other objects. If
there are any spatial constraints that are not satis-
fied, we remove and randomly reposition the ob-
jects violating the constraints, and iterate to im-
prove the layout. The resulting scene is rendered
and presented to the user.
Figure 2 shows a rendering of a generated scene
along with the support hierarchy and input text.
Even though the spatial relation between lamp and
desk was not mentioned explicitly, we infer that
the lamp is supported by the top surface of the
desk. In Figure 3 we show another example of
a generated scene for the input ?There is a room
with a poster bed and a poster?. Note that the sys-
tem differentiates between a ?poster? and a ?poster
bed? ? it correctly selects and places the bed on the
floor, while the poster is placed on the wall.
Figure 4 shows an example of inferring missing
objects. Even though the plate was not explicitly
mentioned in the input, we infer that the sandwich
is more likely to be supported by a plate rather than
directly placed on the table. Without this infer-
18
Figure 5: Left: chair is selected using ?the chair to
the right of the table? or ?the object to the right of
the table?. Chair is not selected for ?the cup to the
right of the table?. Right: Different view results
in different chair being selected for the input ?the
chair to the right of the table?.
ence, the user would need to bemuchmore verbose
with text such as ?There is a room with a table, a
plate and a sandwich. The sandwich is on the plate,
and the plate is on the table.?
8 Interactive System
Once a scene is generated, the user can view the
scene and manipulate it using both simple action
phrases and mouse interaction. The system sup-
ports traditional 3D scene interaction mechanisms
such as navigating the viewpoint with mouse and
keyboard, selection and movement of object mod-
els by clicking. In addition, a user can give simple
textual commands to select and modify objects, or
to refine the scene. For example, a user can re-
quest to ?remove the chair? or ?put a pot on the
table? which requires the system to resolve refer-
ents to objects in the scene (see ?8.1). The system
tracks user interactions throughout this process and
can adjust its spatial knowledge accordingly. In
the following sections, we give some examples of
how the user can interact with the system and how
the system learns from this interaction.
8.1 View centric spatial relations
During interaction, the user can refer to objects
with their categories and with spatial relations be-
tween them. Objects are disambiguated by both
category and view-centric spatial relations. We use
the WordNet hierarchy to resolve hyponym or hy-
pernym referents to objects in the scene. In the left
screenshot in Figure 5, the user can select a chair
to the right of the table using the phrase ?chair to
the right of the table? or ?object to the right of the
table?. The user can then change their viewpoint
by rotating and moving around. Since spatial rela-
tions are resolved with respect to the current view-
point, we see that a different chair is selected for
Figure 6: Left: initial scene. Right: after input
?Put a lamp on the table?.
the same phrase from the different viewpoint in the
right screenshot.
8.2 Scene Editing with Text
By using simple textual commands the user can
edit the scene. For example, given the initial scene
on the left in Figure 6, the user can then issue the
command ?put a lamp on the table? which results
in the scene on the right. The system currently al-
lows for adding objects to new positions and re-
moving existing objects. Currently, repositioning
of objects is performed only with direct control,
but in the future we also plan to support reposi-
tioning of objects by using textual commands.
8.3 Learning Support Hierarchy
After a user requests that a lamp be placed on a ta-
ble, the system updates its prior on the likelihood
of a lamp being supported by a table. Based on
prior observations the likelihood of lamps being
placed on tables was very low (4%) since very few
lamps were observed on tables in the scene dataset.
However, after the user interaction, we recompute
the prior including the scene that the user has cre-
ated and the probability of lamp on table increases
to 12% (see Figure 7).
8.4 Learning Object Names
Often, objects or parts may not have associated la-
bels that the user would use to refer to the objects.
In those cases, the system can inform the user that
it cannot resolve a given name, and the user can
then select the object or part of the object they were
referring to and annotate it with a label. For in-
stance, in Figure 8, the user annotated the differ-
ent parts of the room as ?floor?, ?wall?, ?window?,
and ?door?. Before annotation, the system did not
know any labels for these parts of the room. After
annotation, the user can select these parts using the
associated names. In addition, the system updates
its spatial knowledge base and can now predict that
the probability of a poster being placed on a wall
19
0% 25% 50% 75% 100%
Before
After
Nightstand
Nightstand
Room
Room
Table
Table
Desk
Desk
Figure 7: Probability of supporting parent categories for lamps before and after the user explicitly requests
a lamp on a table.
Figure 8: The user clicks and selects parts of the scene, annotating them as ?floor?, ?wall?, ?window?,
?door?. After annotation, the user can also refer to these parts with the associated names. The system
spatial knowledge base is updated accordingly.
is 40%, and that the probability of a table being
placed on the floor is 23%. Note that these prob-
abilities are based on multiple observations of the
annotated room. Accumulating annotations such
as these and propagating labels to new models is
an effective way to expand spatial knowledge.
9 Future Work
We described a preliminary interactive text to 3D
scene generation system that can learn from prior
data and user interaction. We hope to improve
the system by incorporating more feedback mech-
anisms for the user, and the learning algorithm.
If the user requests a particular object be se-
lected but the system gets the referent wrong, the
user could then indicate the error and provide a cor-
rection. We can then use this feedback as a source
of training data to improve the interpretation of text
to the desired user action. For example, if the user
asks to ?select the red bowl? and the system could
not resolve ?red bowl? to the correct object, the
user could intervene by clicking on the correct ref-
erent object. Simple interactions such as this are
incredibly powerful for providing additional data
for learning. Though we did not focus on this as-
pect, a dialogue-based interaction pattern is natural
for our system. The user can converse with the sys-
tem to iteratively refine the scene and the system
can ask for clarifications at any point ? when and
how the system should inquire for more informa-
tion is interesting future research.
To evaluate whether the generated scenes are
satisfactory, we can ask people to rate them against
input text descriptions. We can also study usage
of the system in concrete tasks to see how often
users need to provide corrections and manually
manipulate the scene. A useful baseline to com-
pare against would be a traditional scenemanipula-
tion system. By doing these studies at a large scale,
for instance by making the interface available on
20
the web, we can crowdsource the accumulation of
user interactions and gathering of spatial knowl-
edge. Simultaneously, running formal user stud-
ies to better understand preference for text-based
versus direct interactions during different actions
would be very beneficial for more informed design
of text-to-scene generation systems.
10 Conclusion
We have demonstrated the usefulness of an inter-
active text to 3D scene generation system. Spatial
knowledge is essential for text to 3D scene gener-
ation. While it is possible to learn spatial knowl-
edge purely from data, it is hard to have complete
coverage of all possible scenarios. Interaction and
user feedback is a good way to improve coverage
and to refine spatial knowledge. In addition, in-
teraction is a natural mode of user involvement in
scene generation and creative tasks.
Little prior work has addressed the need for in-
teraction or the need for recovering implicit spatial
constraints. We propose that the resolution of un-
mentioned spatial constraints, and leveraging user
interaction to acquire spatial knowledge are criti-
cal for enabling natural text to scene generation.
User interaction is essential for text to scene
generation since the process is fundamentally
under-constrained. Most natural textual descrip-
tions of scenes will not mention many visual as-
pects of a physical scene. However, it is still pos-
sible to automatically generate a plausible starting
scene for refinement.
Our work focused on showing that user interac-
tion is both natural and useful for a text to scene
generation system. Furthermore, refining spatial
knowledge through interaction is a promising way
of acquiring more implicit knowledge. Finally,
any practically useful text to scene generation will
by necessity involve interaction with users who
have particular goals and tasks in mind.
References
Saleema Amershi, James Fogarty, and Daniel Weld.
2012. Regroup: interactive machine learning for on-
demand group creation in social networks. In Pro-
ceedings of the SIGCHI Conference on Human Fac-
tors in Computing Systems.
Sharon Rose Clay and Jane Wilhelms. 1996. Put:
Language-based interactive manipulation of objects.
Computer Graphics and Applications, IEEE.
Bob Coyne and Richard Sproat. 2001. WordsEye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques.
BobCoyne, Alexander Klapheke,MasoudRouhizadeh,
Richard Sproat, and Daniel Bauer. 2012. Annota-
tion tools and knowledge representation for a text-to-
scene system. Proceedings of COLING 2012: Tech-
nical Papers.
Matthew Fisher, Daniel Ritchie, Manolis Savva,
Thomas Funkhouser, and Pat Hanrahan. 2012.
Example-based synthesis of 3D object arrangements.
ACM Transactions on Graphics (TOG).
James Fogarty, Desney Tan, Ashish Kapoor, and Simon
Winder. 2008. CueFlik: interactive concept learn-
ing in image search. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems.
Yun Jiang and Ashutosh Saxena. 2013. Infinite la-
tent conditional random fields for modeling environ-
ments through humans.
Yun Jiang, Marcus Lim, and Ashutosh Saxena. 2012.
Learning object arrangements in 3D scenes using hu-
man context. In Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12).
AlanDKalvin andRussell HTaylor. 1996. Superfaces:
Polygonal mesh simplification with bounded error.
Computer Graphics and Applications, IEEE.
Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh
Agrawala, and Vladlen Koltun. 2011. Interactive
furniture layout using interior design guidelines. In
ACM Transactions on Graphics (TOG).
G.A. Miller. 1995. WordNet: a lexical database for
english. CACM.
Benjamin Rosman and Subramanian Ramamoorthy.
2011. Learning spatial relationships between ob-
jects. The International Journal of Robotics Re-
search.
Manolis Savva, Angel X. Chang, Gilbert Bernstein,
Christopher D. Manning, and Pat Hanrahan. 2014.
On being the right scale: Sizing large collections of
3D models. Stanford University Technical Report
CSTR 2014-03.
Lee M Seversky and Lijun Yin. 2006. Real-time au-
tomatic 3D scene generation from natural language
voice and text descriptions. In Proceedings of the
14th annual ACM international conference on Mul-
timedia.
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive psychology.
C Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE Intenational Conference on
Computer Vision (ICCV).
21
