Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 29?32,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Novel Word Segmentation Approach for
Written Languages with Word Boundary Markers
Han-Cheol Cho
?
, Do-Gil Lee
?
, Jung-Tae Lee
?
, Pontus Stenetorp
?
, Jun?ichi Tsujii
?
and Hae-Chang Rim
?
?
Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan
?
Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
{hccho,pontus,tsujii}@is.s.u-tokyo.ac.jp, {dglee,jtlee,rim}@nlp.korea.ac.kr
Abstract
Most NLP applications work under the as-
sumption that a user input is error-free;
thus, word segmentation (WS) for written
languages that use word boundary mark-
ers (WBMs), such as spaces, has been re-
garded as a trivial issue. However, noisy
real-world texts, such as blogs, e-mails,
and SMS, may contain spacing errors that
require correction before further process-
ing may take place. For the Korean lan-
guage, many researchers have adopted a
traditional WS approach, which eliminates
all spaces in the user input and re-inserts
proper word boundaries. Unfortunately,
such an approach often exacerbates the
word spacing quality for user input, which
has few or no spacing errors; such is the
case, because a perfect WS model does
not exist. In this paper, we propose a
novel WS method that takes into consider-
ation the initial word spacing information
of the user input. Our method generates
a better output than the original user in-
put, even if the user input has few spacing
errors. Moreover, the proposed method
significantly outperforms a state-of-the-art
Korean WS model when the user input ini-
tially contains less than 10% spacing er-
rors, and performs comparably for cases
containing more spacing errors. We be-
lieve that the proposed method will be a
very practical pre-processing module.
1 Introduction
Word segmentation (WS) has been a fundamen-
tal research issue for languages that do not have
word boundary markers (WBMs); on the con-
trary, other languages that do have WBMs have re-
garded the issue as a trivial task. Texts segmented
with such WBMs, however, could contain a hu-
man writer?s intentional or un-intentional spacing
errors; and even a few spacing errors can cause
error-propagation for further NLP stages.
For written languages that have WBMs, such as
for the Korean language, the majority of recent
research has been based on a traditional WS ap-
proach (Nakagawa, 2004). The first step of the
traditional approach is to eliminate all spaces in
the user input, and then re-locate the proper places
to insert WBMs. One state-of-the-art Korean WS
model (Lee et al, 2007) is known to achieve a per-
formance of 90.31% word-unit precision, which is
comparable with other WS models for the Chinese
or Japanese language.
Still, there is a downside to the evaluation
method. If the user input has a few or no spac-
ing errors, traditional WS models may cause more
spacing errors than it correct because they produce
the same output regardless the word spacing states
of the user input.
In this paper, we propose a new WS method that
takes into account the word spacing information
from the user input. Our proposed method first
generates the best word spacing states for the user
input by using a traditional WS model; however
the method does not immediately apply the out-
put. Secondly, the method estimates a threshold
based on the word spacing quality of the user in-
put. Finally, the method uses the new word spac-
ing states that have probabilities that are higher
than the threshold.
The most important contribution of the pro-
posed method is that, for most cases, the method
generates an output that is better than the user in-
put. The experimental results show that the pro-
posed method produces a better output than the
user input even if the user input has less than 1%
spacing errors in terms of the character-unit pre-
cision. Moreover, the proposed method outper-
forms (Lee et al, 2007) significantly, when the
29
user input initially contains less than 10% spacing
errors, and even performs comparably, when the
input contains more than 10% errors. Based on
these results, we believe that the proposed method
would be a very practical pre-processing module
for other NLP applications.
The paper is organized as follows: Section 2 ex-
plains the proposed method. Section 3 shows the
experimental results. Finally, the last section de-
scribes the contributions of the proposed method.
2 The Proposed Method
The proposed method consists of three steps: a
baseline WS model, confidence and threshold es-
timation, and output optimization. The following
sections will explain the steps in detail.
2.1 Baseline Word Segmentation Model
We use the tri-gram Hidden Markov Model
(HMM) of (Lee et al, 2007) as the baseline WS
model; however, we adopt the Maximum Like-
lihood (ML) decoding strategy to independently
find the best word spacing states. ML-decoding
allows us to directly compare each output to the
threshold. There is little discrepancy in accuracy
when using ML-decoding, as compared to Viterbi-
decoding, as mentioned in (Merialdo, 1994).
1
Let o
1,n
be a sequence of n-character user input
without WBMs, x
t
be the best word spacing state
for o
t
where 1 ? t ? n. Assume that x
t
is either 1
(space after o
t
) or 0 (no space after o
t
). Then each
best word spacing state x?
t
for all t can be found by
using Equation 1.
x?
t
= argmax
i?(0,1)
P (x
t
= i|o
1,n
) (1)
= argmax
i?(0,1)
P (o
1,n
, x
t
= i) (2)
= argmax
i?(0,1)
?
x
t?2
,x
t?1
P (x
t
= i|x
t?2
, o
t?1
, x
t?1
, o
t
)
?
?
x
t?1
P (o
t+1
|o
t?1
, x
t?1
, o
t
, x
t
= i)
?
?
x
t+1
P (o
t+2
|o
t
, x
t
= i, o
t+1
, x
t+1
) (3)
Equation 2 is derived by applying the Bayes?
rule and by eliminating the constant denominator.
Moreover, the equation is simplified, as is Equa-
tion 3, by using the Markov assumption, and by
1
In the preliminary experiment, Viterbi-decoding showed
a 0.5% higher word-unit precision.
eliminating the constant parts. Every part of Equa-
tion 3 can be calculated by adding the probabilities
of all possible combinations of x
t?2
, x
t?1
, x
t+1
and x
t+2
values.
The model is trained by using the relative fre-
quency information of the training data, and a
smoothing technique is applied to relieve the data-
sparseness problem which is the linear interpola-
tion of n-grams that are used in (Lee et al, 2007).
2.2 Confidence and Threshold Estimation
We set a variable threshold that is proportional to
the word spacing quality of the user input, Confi-
dence. Formally, we can define the threshold T as
a function of a confidence C, as in Equation 4.
T = f(C) (4)
Then, we define the confidence as is done in
Equation 5. Because calculating such a variable
is impossible, we estimate the value by substi-
tuting the word spacing states produced by the
baseline WS model, x
WS
1,n
, with the correct word
spacing states, x
correct
1,n
, as is done in Equation 6.
This estimation is based on the assumption that
the word spacing states of the WS model is suf-
ficiently similar to the correct word spacing states
in the character-unit precision.
2
C =
# of x
input
t
same to x
correct
t
# of x
input
t
(5)
?
# of x
input
t
same to x
WS
t
# of x
input
t
(6)
?
n
?
?
?
?
n
?
k=1
P (x
input
k
|o
1,n
) (7)
To handle the estimation error for short sen-
tences, we use the probability generating word
spacing states of the user input with the length nor-
malization as shown in Equation 7.
Figure 1 shows that the estimated confidence of
Equation 7 is almost linearly proportional to the
true confidence of Equation 5, thus suggesting that
the threshold T can be defined as a function of the
estimated confidence of Equation 7.
3
2
In the experiment with the development data, the base-
line WS model shows about 97% character-unit precision.
3
The development data is generated by randomly intro-
ducing spacing errors into correctly spaced sentences. We
think that this reflects various intentional and un-intentional
error patterns of individuals.
30
20%30%
40%50%
60%70%
80%90%
100%
100% 96% 92% 88% 84% 80%
Estim
ated C
onfid
ence
True Confidence
Figure 1: The relationship between estimated con-
fidence and true confidence
To keep the focus on the research subject of this
paper, we simply assume f(x) = x as in Equation
8, for the threshold function f .
T ? f(C) = C (8)
In the experimental results, we confirm that
even this simple threshold function can be help-
ful in improving the performance of the proposed
method against traditional WS models.
2.3 Output Optimization
After completing the two steps described in Sec-
tion 2.1 and 2.2, we have acquired the new spacing
states for the user input generated by the baseline
WS model, and the threshold measuring the word
spacing quality of the user input.
The proposed method only applies a part of the
new word spacing states to the user input, which
have probabilities that are higher than the thresh-
old; further the method discards the other new
word spacing states that have probabilities that are
lower than the threshold. By rejecting the unreli-
able output of the baseline WS model in this way,
the proposed method can effectively improve the
performance when the user input contains a rela-
tively small number of spacing errors.
3 Experimental Results
Two types of experiments have been performed.
In the first experiment, we investigate the level of
performance improvement based on different set-
tings of the user input?s word spacing error rate.
Because it is nearly impossible to obtain enough
test data for any error rate, we generate pseudo test
data in the same way that we generate develop-
ment data.
4
In the second experiment, we attempt
4
See Footnote 3.
figuring out whether the proposed method really
improves the word spacing quality of the user in-
put in a real-world setting.
3.1 Performance Improvement according to
the Word Spacing Error Rate of User
Input
For the first experiment, we use the Sejong corpus
5
from 1998-1999 (1,000,000 Korean sentences) for
the training data, and ETRI corpus (30,000 sen-
tences) for the test data (ETRI, 1999). To gener-
ate the test data that have spacing errors, we make
twenty one copies of the test data and randomly
insert spacing errors from 0% to 20% in the same
way in which we made the development data. We
feel that this strategy can model both the inten-
tional and un-intentional human error patterns.
In Figure 2, the x-axis indicates the word spac-
ing error rate of the user input in terms of the
character-unit precision, and the y-axis shows the
word-unit precision of the output. Each graph de-
picts the word-unit precision of the test corpus,
a state-of-the-art Korean WS model (Lee et al,
2007), the baseline WS model, and the proposed
method.
Although Lee?s model is known to perform
comparably with state-of-the-art Chinese and
Japanese WS models, it does not necessarily sug-
gest that the word spacing quality of the model?s
output is better than the user input. In Figure 2,
Lee?s model exacerbates the user input when it has
spacing errors that are lower than 3%.
The proposed method, however, produces a bet-
ter output, even if the user input has 1% spacing er-
rors. Moreover, the proposed method shows a con-
siderably better performance within the 10% spac-
ing error range, as compared to Lee?s model, al-
though the baseline WS model itself does not out-
performs Lee?s model. The performance improve-
ment in this error range is fairly significant be-
cause we found that the spacing error rate of texts
collected for the second experiment was about
9.1%.
3.2 Performance Comparison with Web Text
having Usual Error Rate
In the second experiment, we attempt finding out
whether the proposed method can be beneficial un-
der real-world circumstances. Web texts, which
consist of 1,000 erroneous sentences from famous
5
Details available at: http://www.sejong.or.kr/eindex.php
31
84%
86%
88%
90%
92%
94%
96%
98%
100%
0% 2% 4% 6% 8% 10% 12% 14% 16% 18% 20%
w
or
d-u
nit
 
 
pr
ec
isio
n
word spacing error rate of user input (in character-unit precision)
Test corpus Lee's model Baseline WS model Proposed method
Figure 2: Performance improvement according to the word spacing error rate of user input
Method Web Text
Test Corpus 70.89%
Lee?s Model 70.45%
Baseline WS Model 69.13%
Proposed Method 73.74%
Table 1: Performance comparison with Web text
Web portals and personal blogs, were collected
and used as the test data. Since the test data tend
to have a similar error rate to the narrow standard
deviation, we computed the overall performance
over the average word spacing error rate, which is
9.1%. The baseline WS model is trained on the
Sejong corpus, described in Section 3.1.
The test result is shown in Table 1. The
overall performance of Lee?s model, the baseline
WS model and the proposed method decreased
by roughly 18%. We hypothesize that the per-
formance degradation probably results from the
spelling errors of the test data, and the inconsis-
tencies that exist between the training data and the
test data. However, the proposed method still im-
proves the word spacing quality of the user input
by 3%, while the two traditional WS models de-
grades the quality. Such a result indicates that
the proposed method is effective for real-world
environments, as we had intended. Furthermore,
we also believe that the performance can be im-
proved if a proper training corpus is provided, or
if a spelling correction method is integrated.
4 Conclusion
In this paper, we proposed a new WS method that
uses the word spacing information of the user in-
put, for languages with WBMs. By utilizing the
user input, the proposed method effectively refines
the output of the baseline WS model and improves
the overall performance.
The most important contribution of this work is
that it produces an output that is better than the
user input even if it contains few spacing errors.
Therefore, the proposed method can be applied as
a pre-processing module for practical NLP appli-
cations without introducing a risk that would gen-
erate a worse output than the user input. Moreover,
the performance is notably better than a state-of-
the-art Korean WS model (Lee et al, 2007) within
the 10% spacing error range, which human writers
seldom exceed. It also performs comparably, even
if the user input contains more than 10% spacing
errors.
5 Acknowledgment
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
References
ETRI. 1999. Pos-tag guidelines. Technical report.
Electronics and Telecomminications Research Insti-
tute.
Do-Gil Lee, Hae-Chang Rim, and Dongsuk Yook.
2007. Automatic Word Spacing Using Probabilistic
Models Based on Character n-grams. IEEE Intelli-
gent Systems, 22(1):28?35.
Bernard Merialdo. 1994. Tagging English text with a
probabilistic model. Comput. Linguist., 20(2):155?
171.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level
information. In COLING ?04, page 466, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
32
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1544?1555,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Jointly Learning Word Representations and Composition Functions
Using Predicate-Argument Structures
Kazuma Hashimoto?, Pontus Stenetorp?, Makoto Miwa?, and Yoshimasa Tsuruoka?
?The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{hassy,pontus,tsuruoka}@logos.t.u-tokyo.ac.jp
?Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan
makoto-miwa@toyota-ti.ac.jp
Abstract
We introduce a novel compositional lan-
guage model that works on Predicate-
Argument Structures (PASs). Our model
jointly learns word representations and
their composition functions using bag-
of-words and dependency-based con-
texts. Unlike previous word-sequence-
based models, our PAS-based model com-
poses arguments into predicates by using
the category information from the PAS.
This enables our model to capture long-
range dependencies between words and
to better handle constructs such as verb-
object and subject-verb-object relations.
We verify this experimentally using two
phrase similarity datasets and achieve re-
sults comparable to or higher than the pre-
vious best results. Our system achieves
these results without the need for pre-
trained word vectors and using a much
smaller training corpus; despite this, for
the subject-verb-object dataset our model
improves upon the state of the art by as
much as ?10% in relative performance.
1 Introduction
Studies on embedding single words in a vector
space have made notable successes in capturing
their syntactic and semantic properties (Turney
and Pantel, 2010). These embeddings have also
been found to be a useful component for Natural
Language Processing (NLP) systems; for exam-
ple, Turian et al. (2010) and Collobert et al. (2011)
demonstrated how low-dimensional word vectors
learned by Neural Network Language Models
(NNLMs) are beneficial for a wide range of NLP
tasks.
Recently, the main focus of research on vector
space representation is shifting from word repre-
sentations to phrase representations (Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011; Mitchell and Lapata, 2010; Socher et al.,
2012). Combining the ideas of NNLMs and se-
mantic composition, Tsubaki et al. (2013) intro-
duced a novel NNLM incorporating verb-object
dependencies. More recently, Levy and Goldberg
(2014) presented a NNLM that integrated syntac-
tic dependencies. However, to the best of our
knowledge, there is no previous work on integrat-
ing a variety of syntactic and semantic dependen-
cies into NNLMs in order to learn composition
functions as well as word representations. The fol-
lowing question thus arises naturally:
Can a variety of dependencies be used to
jointly learn both stand-alone word vectors
and their compositions, embedding them in
the same vector space?
In this work, we bridge the gap between
purely context-based (Levy and Goldberg, 2014;
Mikolov et al., 2013b; Mnih and Kavukcuoglu,
2013) and compositional (Tsubaki et al., 2013)
NNLMs by using the flexible set of categories
from Predicate-Argument-Structures (PASs).
More specifically, we propose a Compositional
Log-Bilinear Language Model using PASs (PAS-
CLBLM), an overview of which is shown in
Figure 1. The model is trained by maximizing
the accuracy of predicting target words from their
bag-of-words and dependency-based context,
which provides information about selectional
preference. As shown in Figure 1 (b), one of the
advantages of the PAS-CLBLM is that the model
can treat not only word vectors but also composed
vectors as contexts. Since the composed vectors
1544
Figure 1: An overview of the proposed model: PAS-CLBLM. (a) The PAS-LBLM predicts target words
from their bag-of-words and dependency-based context words. (b) The PAS-CLBLM predicts target
words using not only context words but also composed vector representations derived from another level
of predicate-argument structures. Underlined words are target words and we only depict the bag-of-
words vector for the PAS-CLBLM.
are treated as input to the language model in
the same way as word vectors, these composed
vectors are expected to become similar to word
vectors for words with similar meanings.
Our empirical results demonstrate that the pro-
posed model has the ability to learn meaning-
ful representations for adjective-noun, noun-noun,
and (subject-) verb-object dependencies. On three
tasks of measuring the semantic similarity be-
tween short phrases (adjective-noun, noun-noun,
and verb-object), the learned composed vectors
achieve scores (Spearman?s rank correlation ?)
comparable to or higher than those of previ-
ous models. On a task involving more complex
phrases (subject-verb-object), our learned com-
posed vectors achieve state-of-the-art performance
(? = 0.50) with a training corpus that is an order
of magnitude smaller than that used by previous
work (Tsubaki et al., 2013; Van de Cruys et al.,
2013). Moreover, the proposed model does not
require any pre-trained word vectors produced by
external models, but rather induces word vectors
jointly while training.
2 Related Work
There is a large body of work on how to represent
the meaning of a word in a vector space. Distri-
butional approaches assume that the meaning of
a word is determined by the contexts in which it
appears (Firth, 1957). The context of a word is of-
ten defined as the words appearing in a window
of fixed-length (bag-of-words) and a simple ap-
proach is to treat the co-occurrence statistics of a
word w as a vector representation for w (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010); al-
ternatively, dependencies between words can be
used to define contexts (Goyal et al., 2013; Erk
and Pado?, 2008; Thater et al., 2010).
In contrast to distributional representations,
NNLMs represent words in a low-dimensional
vector space (Bengio et al., 2003; Collobert et al.,
2011). Recently, Mikolov et al. (2013b) and Mnih
and Kavukcuoglu (2013) proposed highly scalable
models to learn high-dimensional word vectors.
Levy and Goldberg (2014) extended the model of
Mikolov et al. (2013b) by treating syntactic depen-
dencies as contexts.
Mitchell and Lapata (2008) investigated a vari-
ety of compositional operators to combine word
vectors into phrasal representations. Among these
operators, simple element-wise addition and mul-
tiplication are now widely used to represent short
phrases (Mitchell and Lapata, 2010; Blacoe and
Lapata, 2012). The obvious limitation with these
simple approaches is that information about word
order and syntactic relations is lost.
To incorporate syntactic information into com-
position functions, a variety of compositional
models have been proposed. These include recur-
sive neural networks using phrase-structure trees
(Socher et al., 2012; Socher et al., 2013b) and
models in which words have a specific form of
parameters according to their syntactic roles and
composition functions are syntactically dependent
on the relations of input words (Baroni and Zam-
parelli, 2010; Grefenstette and Sadrzadeh, 2011;
Hashimoto et al., 2013; Hermann and Blunsom,
2013; Socher et al., 2013a).
More recently, syntactic dependency-based
1545
compositional models have been proposed (Pa-
perno et al., 2014; Socher et al., 2014; Tsub-
aki et al., 2013). One of the advantages of these
models is that they are less restricted by word or-
der. Among these, Tsubaki et al. (2013) intro-
duced a novel compositional NNLM mainly fo-
cusing on verb-object dependencies and achieved
state-of-the-art performance for the task of mea-
suring the semantic similarity between subject-
verb-object phrases.
3 PAS-CLBLM: A Compositional
Log-Bilinear Language Model Using
Predicate-Argument Structures
In some recent studies on representing words as
vectors, word vectors are learned by solving word
prediction tasks (Mikolov et al., 2013a; Mnih and
Kavukcuoglu, 2013). More specifically, given tar-
get words and their context words, the basic idea
is to train classifiers to discriminate between each
target word and artificially induced negative tar-
get words. The feature vector of the classifiers are
calculated using the context word vectors whose
values are optimized during training. As a result,
vectors of words in similar contexts become simi-
lar to each other.
Following these studies, we propose a novel
model to jointly learn representations for words
and their compositions by training word predic-
tion classifiers using PASs. In this section, we
first describe the predicate-argument structures as
they serve as the basis of our model. We then
introduce a Log-Bilinear Language Model us-
ing Predicate-Argument Structures (PAS-LBLM)
to learn word representations using both bag-of-
words and dependency-based contexts. Finally,
we propose integrating compositions of words into
the model. Figure 1 (b) shows the overview of the
proposed model.
3.1 Predicate-Argument Structures
Due to advances in deep parsing technologies,
syntactic parsers that can produce predicate-
argument structures are becoming accurate and
fast enough to be used for practical applications.
In this work, we use the probabilistic HPSG
parser Enju (Miyao and Tsujii, 2008) to obtain the
predicate-argument structures of individual sen-
tences. In its grammar, each word in a sentence
is treated as a predicate of a certain category with
zero or more arguments. Table 1 shows some ex-
Category predicate arg1 arg2
adj arg1 heavy rain
noun arg1 car accident
verb arg12 cause rain accident
prep arg12 at eat restaurant
Table 1: Examples of predicates of different cate-
gories from the grammar of the Enju parser. arg1
and arg2 denote the first and second arguments.
amples of predicates of different categories.1 For
example, a predicate of the category verb arg12
expresses a verb with two arguments. A graph can
be constructed by connecting words in predicate-
argument structures in a sentence; in general, these
graphs are acyclic.
One of the merits of using predicate-argument
structures is that they can capture dependencies
between more than two words, while standard syn-
tactic dependency structures are limited to depen-
dencies between two words. For example, one of
the predicates in the phrase ?heavy rain caused car
accidents? is the verb ?cause?, and it has two ar-
guments (?rain? and ?accident?). Furthermore, ex-
actly the same predicate-argument structure (pred-
icate: cause, first argument: rain, second argu-
ment: accident) is extracted from the passive form
of the above phrase: ?car accidents were caused
by heavy rain?. This is helpful when capturing
semantic dependencies between predicates and ar-
guments, and in extracting facts or relations de-
scribed in a sentence, such as who did what to
whom.
3.2 A Log-Bilinear Language Model Using
Predicate-Argument Structures
3.2.1 PAS-based Word Prediction
The PAS-LBLM predicts a target word given its
PAS-based context. We assume that each word
w in the vocabulary V is represented with a d-
dimensional vector v(w). When a predicate of
category c is extracted from a sentence, the PAS-
LBLM computes the predicted d-dimensional vec-
tor p(w
t
) for the target word w
t
from its context
words w
1
, w
2
, . . . , w
m
:
p(w
t
) = f
(
m
?
i=1
h
c
i
? v(w
i
)
)
, (1)
1The categories of the predicates in the Enju parser are
summarized at http://kmcs.nii.ac.jp/
?
yusuke/
enju/enju-manual/enju-output-spec.html.
1546
where hc
i
? R
d?1 are category-specific weight
vectors and ? denotes element-wise multiplica-
tion. f is a non-linearity function; in this work
we define f as tanh.
As an example following Figure 1 (a), when
the predicate ?cause? is extracted with its first
and second arguments ?rain? and ?accident?, the
PAS-LBLM computes p(cause) ? Rd following
Eq. (1):
p(cause) = f(h
verb arg12
arg1
? v(rain)+
h
verb arg12
arg2
? v(accident)).
(2)
In Eq. (2), the predicate is treated as the target
word, and its arguments are treated as the con-
text words. In the same way, an argument can be
treated as a target word:
p(rain) = f(h
verb arg12
verb
? v(cause)+
h
verb arg12
arg2
? v(accident)).
(3)
Relationship to previous work. If we omit the
the category-specific weight vectors hc
i
in Eq. (1),
our model is similar to the CBOW model in
Mikolov et al. (2013a). CBOW predicts a tar-
get word given its surrounding bag-of-words con-
text, while our model uses its PAS-based context.
To incorporate the PAS information in our model
more efficiently, we use category-specific weight
vectors. Similarly, the vLBL model of Mnih and
Kavukcuoglu (2013) uses different weight vec-
tors depending on the position relative to the tar-
get word. As with previous neural network lan-
guage models (Collobert et al., 2011; Huang et al.,
2012), our model and vLBL can use weight ma-
trices rather than weight vectors. However, as dis-
cussed by Mnih and Teh (2012), using weight vec-
tors makes the training significantly faster than us-
ing weight matrices. Despite the simple formula-
tion of the element-wise operations, the category-
specific weight vectors efficiently propagate PAS-
based context information as explained next.
3.2.2 Training Word Vectors
To train the PAS-LBLM, we use a scoring function
to evaluate how well the target word w
t
fits the
given context:
s(w
t
, p(w
t
)) = v?(w
t
)
T
p(w
t
), (4)
where v?(w
t
) ? R
d?1 is the scoring weight vector
for w
t
. Thus, the model parameters in the PAS-
LBLM are (V, ?V ,H). V is the set of word vec-
tors v(w), and ?V is the set of scoring weight vec-
tors v?(w). H is the set of the predicate-category-
specific weight vectors hc
i
.
Based on the objective in the model of Collobert
et al. (2011), the model parameters are learned by
minimizing the following hinge loss:
N
?
n=1
max(1? s(w
t
, p(w
t
)) + s(w
n
, p(w
t
)), 0),
(5)
where the negative sample w
n
is a randomly sam-
pled word other than w
t
, and N is the number
of negative samples. In our experiments we set
N = 1. Following Mikolov et al. (2013b), nega-
tive samples were drawn from the distribution over
unigrams that we raise to the power 0.75 and then
normalize to once again attain a probability distri-
bution. We minimize the loss function in Eq. (5)
using AdaGrad (Duchi et al., 2011). For further
training details, see Section 4.5.
Relationship to softmax regression models.
The model parameters can be learned by maximiz-
ing the log probability of the target word w
t
based
on the softmax function:
p(w
t
|context) =
exp(s(w
t
, p(w
t
)))
?
|V|
i=1
exp(s(w
i
, p(w
t
)))
. (6)
This is equivalent to a softmax regression model.
However, when the vocabulary V is large, com-
puting the softmax function in Eq. (6) is compu-
tationally expensive. If we do not need probabil-
ity distributions over words, we are not necessar-
ily restricted to using the probabilistic expressions.
Recently, several methods have been proposed to
efficiently learn word representations rather than
accurate language models (Collobert et al., 2011;
Mikolov et al., 2013b; Mnih and Kavukcuoglu,
2013), and our objective follows the work of Col-
lobert et al. (2011). Mikolov et al. (2013b) and
Mnih and Kavukcuoglu (2013) trained their mod-
els using word-dependent scoring weight vectors
which are the arguments of our scoring function
in Eq. (4). During development we also trained
our model using the negative sampling technique
of Mikolov et al. (2013b); however, we did not ob-
serve any significant performance difference.
Intuition behind the PAS-LBLM. Here we
briefly explain how each class of the model pa-
rameters of the PAS-LBLM contributes to learning
word representations at each stochastic gradient
1547
decent step. The category-specific weight vectors
provide the PAS information for context word vec-
tors which we would like to learn. During train-
ing, context word vectors having the same PAS-
based syntactic roles are updated similarly. The
word-dependent scoring weight vectors propagate
the information on which words should, or should
not, be predicted. In effect, context word vectors
making similar contributions to word predictions
are updated similarly. The non-linear function f
provides context words with information on the
other context words in the same PAS. In this way,
word vectors are expected to be learned efficiently
by the PAS-LBLM.
3.3 Learning Composition Functions
As explained in Section 3.1, predicate-argument
structures inherently form graphs whose nodes are
words in a sentence. Using the graphs, we can in-
tegrate relationships between multiple predicate-
argument structures into our model.
When the context word w
i
in Eq. (1), excluding
predicate words, has another predicate-argument
of category c? as a dependency, we replace v(w
i
)
with the vector produced by the composition func-
tion for the predicate category c?. For example,
as shown in Figure 1 (b), when the first argument
?rain? of the predicate ?cause? is also the argu-
ment of the predicate ?heavy?, we first compute
the d-dimensional composed vector representation
for ?heavy? and ?rain?:
g
c
?
(v(heavy), v(rain)), (7)
where c? is the category adj arg1, and g
c
? is a func-
tion to combine input vectors for the predicate-
category c?. We can use any composition func-
tion that produces a representation of the same
dimensionality as its inputs, such as element-
wise addition/multiplication (Mitchell and Lap-
ata, 2008) or neural networks (Socher et al.,
2012). We then replace v(rain) in Eq. (2) with
g
c
?
(v(heavy), v(rain)). When the second argu-
ment ?accident? in Eq. (2) is also the argument
of the predicate ?car?, v(accident) is replaced
with g
c
??
(v(car), v(accident)). c
?? is the predi-
cate category noun arg1. These multiple relation-
ships of predicate-argument structures should pro-
vide richer context information. We refer to the
PAS-LBLM with composition functions as PAS-
CLBLM.
3.4 Bag-of-Words Sensitive PAS-CLBLM
Both the PAS-LBLM and PAS-CLBLM can take
meaningful relationships between words into ac-
count. However, at times, the number of context
words can be limited and the ability of other mod-
els to take ten or more words from a fixed con-
text in a bag-of-words (BoW) fashion could com-
pensate for this sparseness. Huang et al. (2012)
combined local and global contexts in their neural
network language models, and motivated by their
work, we integrate bag-of-words vectors into our
models. Concretely, we add an additional input
term to Eq. (1):
p(w
t
) = f
(
m
?
i=1
h
c
i
? v(w
i
) + h
c
BoW
? v(BoW)
)
,
(8)
where hc
BoW
? R
d?1 are additional weight vec-
tors, and v(BoW) ? Rd?1 is the average of the
word vectors in the same sentence. To construct
the v(BoW) for each sentence, we average the
word vectors of nouns and verbs in the same sen-
tence, excluding the target and context words.
4 Experimental Settings
4.1 Training Corpus
We used the British National Corpus (BNC) as our
training corpus, extracted 6 million sentences from
the original BNC files, and parsed them using the
Enju parser described in Section 3.1.
4.2 Word Sense Disambiguation Using
Part-of-Speech Tags
In general, words can have multiple syntactic us-
ages. For example, the word cause can be a
noun or a verb depending on its context. Most
of the previous work on learning word vectors
ignores this ambiguity since word sense disam-
biguation could potentially be performed after the
word vectors have been trained (Huang et al.,
2012; Kartsaklis and Sadrzadeh, 2013). Some re-
cent work explicitly assigns an independent vec-
tor for each word usage according to its part-of-
speech (POS) tag (Hashimoto et al., 2013; Kart-
saklis and Sadrzadeh, 2013). Alternatively, Baroni
and Zamparelli (2010) assigned different forms of
parameters to adjectives and nouns.
In our experiments, we combined each word
with its corresponding POS tags. We used the
base-forms provided by the Enju parser rather than
1548
Figure 2: Two PAS-CLBLM training samples.
the surface-forms, and used the first two charac-
ters of the POS tags. For example, VB, VBP,
VBZ, VBG, VBD, VBN were all mapped to VB.
This resulted in two kinds of cause: cause NN and
cause VB and we used the 100,000 most frequent
lowercased word-POS pairs in the BNC.
4.3 Selection of Training Samples Based on
Categories of Predicates
To train the PAS-LBLM and PAS-CLBLM, we
could use all predicate categories. However, our
preliminary experiments showed that these cate-
gories covered many training samples which are
not directly relevant to our experimental setting,
such as determiner-noun dependencies. We thus
manually selected the categories used in our ex-
periments. The selected predicates are listed in
Table 1: adj arg1, noun arg1, prep arg12, and
verb arg12. These categories should provide
meaningful information on selectional preference.
For example, the prep arg12 denotes prepositions
with two arguments, such as ?eat at restaurant?
which means that the verb ?eat? is related to the
noun ?restaurant? by the preposition ?at?. Prepo-
sitions are one of the predicates whose arguments
can be verbs, and thus prepositions are important
in training the composition functions for (subject-)
verb-object dependencies as described in the next
paragraph.
Another point we had to consider was how
to construct the training samples for the PAS-
CLBLM. We constructed compositional training
samples as explained in Section 3.3 when c? was
adj arg1, noun arg1, or verb arg12. Figure 2
shows two examples in addition to the example
in Figure 1 (b). Using such training samples, the
PAS-CLBLM could, for example, recognize from
the two predicate-argument structures, ?eat food?
and ?eat at restaurant?, that eating foods is an ac-
tion that occurs at restaurants.
Model Composition Function
Add
l
v(w
1
) + v(w
2
)
Add
nl
tanh(v(w
1
) + v(w
2
))
Wadd
l
m
c
adj
? v(w
1
) + m
c
arg1
? v(w
2
)
Wadd
nl
tanh(m
c
adj
?v(w
1
)+m
c
arg1
?v(w
2
))
Table 2: Composition functions used in this work.
The examples are shown as the adjective-noun de-
pendency between w
1
=?heavy? and w
2
=?rain?.
4.4 Selection of Composition Functions
As described in Section 3.3, we are free to se-
lect any composition functions in Eq. (7). To
maintain the fast training speed of the PAS-
LBLM, we avoid dense matrix-vector multiplica-
tion in our composition functions. In Table 2,
we list the composition functions used for the
PAS-CLBLM. Add
l
is element-wise addition and
Add
nl
is element-wise addition with the non-
linear function tanh. The subscripts l and nl de-
note the words linear and non-linear. Similarly,
Wadd
l
is element-wise weighted addition and
Wadd
nl
is element-wise weighted addition with
the non-linear function tanh. The weight vec-
tors mc
i
? R
d?1 in Table 2 are predicate-category-
specific parameters which are learned during train-
ing. We investigate the effects of the non-linear
function tanh for these composition functions.
In the formulations of the backpropagation algo-
rithm, non-linear functions allow the input vectors
to weakly interact with each other.
4.5 Initialization and Optimization of Model
Parameters
We assigned a 50-dimensional vector for each
word-POS pair described in Section 4.2 and ini-
tialized the vectors and the scoring weight vec-
tors using small random values. In part inspired
by the initialization method of the weight matrices
in Socher et al. (2013a), we initialized all values
in the compositional weight vectors of the Wadd
l
and Wadd
nl
as 1.0. The context weight vectors
were initialized using small random values.
We minimized the loss function in Eq. (5) us-
ing mini-batch SGD and AdaGrad (Duchi et al.,
2011). Using AdaGrad, the SGD?s learning rate
is adapted independently for each model parame-
ter. This is helpful in training the PAS-LBLM and
PAS-CLBLM, as they have conditionally depen-
dent model parameters with varying frequencies.
1549
The mini-batch size was 32 and the learning rate
was 0.05 for each experiment, and no regulariza-
tion was used. To verify the semantics captured by
the proposed models during training and to tune
the hyperparameters, we used the WordSim-3532
word similarity data set (Finkelstein et al., 2001).
5 Evaluation on Phrase Similarity Tasks
5.1 Evaluation Settings
The learned models were evaluated on four tasks
of measuring the semantic similarity between
short phrases. We performed evaluation using the
three tasks (AN, NN, and VO) in the dataset3 pro-
vided by Mitchell and Lapata (2010), and the SVO
task in the dataset4 provided by Grefenstette and
Sadrzadeh (2011).
The datasets include pairs of short phrases ex-
tracted from the BNC. AN, NN, and VO con-
tain 108 phrase pairs of adjective-noun, noun-
noun, and verb-object. SVO contains 200 pairs of
subject-verb-object phrases. Each phrase pair has
multiple human-ratings: the higher the rating is,
the more semantically similar the phrases. For ex-
ample, the subject-verb-object phrase pair of ?stu-
dent write name? and ?student spell name? has a
high rating. The pair ?people try door? and ?peo-
ple judge door? has a low rating.
For evaluation we used the Spearman?s rank
correlation ? between the human-ratings and the
cosine similarity between the composed vector
pairs. We mainly used non-averaged human-
ratings for each pair, and as described in Section
5.3, we also used averaged human-ratings for the
SVO task. Each phrase pair in the datasets was an-
notated by more than two annotators. In the case
of averaged human ratings, we averaged multiple
human-ratings for each phrase pair, and in the case
of non-averaged human-ratings, we treated each
human-rating as a separate annotation.
With the PAS-CLBLM, we represented each
phrase using the composition functions listed in
Table 2. When there was no composition present,
we represented the phrase using element-wise ad-
dition. For example, when we trained the PAS-
CLBLM with the composition function Wadd
nl
,
2http://www.cs.technion.ac.il/
?
gabr/
resources/data/wordsim353/
3http://homepages.inf.ed.ac.uk/
s0453356/share
4http://www.cs.ox.ac.uk/activities/
compdistmeaning/GS2011data.txt
Model AN NN VO
PAS-CLBLM (Add
l
) 0.52 0.44 0.35
PAS-CLBLM (Add
nl
) 0.52 0.46 0.45
PAS-CLBLM (Wadd
l
) 0.48 0.39 0.34
PAS-CLBLM (Wadd
nl
) 0.48 0.40 0.39
PAS-LBLM 0.41 0.44 0.39
word2vec 0.52 0.48 0.42
BL w/ BNC 0.48 0.50 0.35
HB w/ BNC 0.41 0.44 0.34
KS w/ ukWaC n/a n/a 0.45
K w/ BNC n/a n/a 0.41
Human agreement 0.52 0.49 0.55
Table 3: Spearman?s rank correlation scores ? for
the three tasks: AN, NN, and VO.
the composed vector for each phrase was com-
puted using the Wadd
nl
function, and when we
trained the PAS-LBLM, we used the element-wise
addition function. To compute the composed vec-
tors using the Wadd
l
and Wadd
nl
functions, we
used the categories of the predicates adj arg1,
noun arg1, and verb arg12 listed in Table 1.
As a strong baseline, we trained the Skip-gram
model of Mikolov et al. (2013b) using the pub-
licly available word2vec5 software. We fed the
POS-tagged BNC into word2vec since our models
utilize POS tags and trained 50-dimensional word
vectors using word2vec. For each phrase we then
computed the representation using vector addition.
5.2 AN, NN, and VO Tasks
Table 3 shows the correlation scores ? for the AN,
NN, and VO tasks. Human agreement denotes the
inter-annotator agreement. The word2vec baseline
achieves unexpectedly high scores for these three
tasks. Previously these kinds of models (Mikolov
et al., 2013b; Mnih and Kavukcuoglu, 2013) have
mainly been evaluated for word analogy tasks and,
to date, there has been no work using these word
vectors for the task of measuring the semantic sim-
ilarity between phrases. However, this experimen-
tal result suggests that word2vec can serve as a
strong baseline for these kinds of tasks, in addi-
tion to word analogy tasks.
In Table 3, BL, HB, KS, and K denote the work
of Blacoe and Lapata (2012), Hermann and Blun-
som (2013), Kartsaklis and Sadrzadeh (2013), and
Kartsaklis et al. (2013) respectively. Among these,
5https://code.google.com/p/word2vec/
1550
Averaged Non-averaged
Model Corpus SVO-SVO SVO-V SVO-SVO SVO-V
PAS-CLBLM (Add
l
) 0.29 0.34 0.24 0.28
PAS-CLBLM (Add
nl
) 0.27 0.32 0.24 0.28
PAS-CLBLM (Wadd
l
) BNC 0.25 0.26 0.21 0.23
PAS-CLBLM (Wadd
nl
) 0.42 0.50 0.34 0.41
PAS-LBLM 0.21 0.06 0.18 0.08
word2vec BNC 0.12 0.32 0.12 0.28
Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/a
Tsubaki et al. (2013) ukWaC n/a 0.47 n/a n/a
Van de Cruys et al. (2013) ukWaC n/a n/a 0.32 0.37
Human agreement 0.75 0.62
Table 4: Spearman?s rank correlation scores ? for the SVO task. Averaged denotes the ? calculated by
averaged human ratings, and Non-averaged denotes the ? calculated by non-averaged human ratings.
only Kartsaklis and Sadrzadeh (2013) used the
ukWaC corpus (Baroni et al., 2009) which is an or-
der of magnitude larger than the BNC. As we can
see in Table 3, the PAS-CLBLM (Add
nl
) achieves
scores comparable to and higher than those of the
baseline and the previous state-of-the-art results.
In relation to these results, the Wadd
l
and Wadd
nl
variants of the PAS-CLBLM do not achieve great
improvements in performance. This indicates that
simple word vector addition can be sufficient to
compose representations for phrases consisting of
word pairs.
5.3 SVO Task
Table 4 shows the correlation scores ? for the SVO
task. The scores ? for this task are reported for
both averaged and non-averaged human ratings.
This is due to a disagreement in previous work
regarding which metric to use when reporting re-
sults. Hence, we report the scores for both settings
in Table 4. Another point we should consider is
that some previous work reported scores based on
the similarity between composed representations
(Grefenstette and Sadrzadeh, 2011; Van de Cruys
et al., 2013), and others reported scores based on
the similarity between composed representations
and word representations of landmark verbs from
the dataset (Tsubaki et al., 2013; Van de Cruys et
al., 2013). For completeness, we report the scores
for both settings: SVO-SVO and SVO-V in Table 4.
The results show that the weighted addition
model with the non-linear function tanh (PAS-
CLBLM (Wadd
nl
)) is effective for the more com-
plex phrase task. While simple vector addition is
sufficient for phrases consisting of word pairs, it is
clear from our experimental results that they fall
short for more complex structures such as those
involved in the SVO task.
Our PAS-CLBLM (Wadd
nl
) model outperforms
the previous state-of-the-art scores for the SVO
task as reported by Tsubaki et al. (2013) and
Van de Cruys et al. (2013). As such, there are three
key points that we would like to emphasize:
(1) the difference of the training corpus size,
(2) the necessity of the pre-trained word vectors,
(3) the modularity of deep learning models.
Tsubaki et al. (2013) and Van de Cruys et al.
(2013) used the ukWaC corpus. This means our
model works better, despite using a considerably
smaller corpora. It should also be noted that, like
us, Grefenstette and Sadrzadeh (2011) used the
BNC corpus.
The model of Tsubaki et al. (2013) is based on
neural network language models which use syn-
tactic dependencies between verbs and their ob-
jects. While their novel model, which incorpo-
rates the idea of co-compositionality, works well
with pre-trained word vectors produced by exter-
nal models, it is not clear whether the pre-trained
vectors are required to achieve high scores. In
contrast, we have achieved state-of-the-art results
without the use of pre-trained word vectors.
Despite our model?s scalability, we trained 50-
dimensional vector representations for words and
their composition functions and achieved high
scores using this low dimensional vector space.
1551
model d AN NN VO SVO
Add
l
50 0.52 0.44 0.35 0.24
1000 0.51 0.51 0.43 0.31
Add
nl
50 0.52 0.46 0.45 0.24
1000 0.51 0.50 0.45 0.31
Wadd
l
50 0.48 0.39 0.34 0.21
1000 0.50 0.49 0.43 0.32
Wadd
nl
50 0.48 0.40 0.39 0.34
1000 0.51 0.48 0.48 0.34
Table 5: Comparison of the PAS-CLBLM between
d = 50 and d = 1000.
This maintains the possibility to incorporate re-
cently developed deep learning composition func-
tions into our models, such as recursive neural
tensor networks (Socher et al., 2013b) and co-
compositional neural networks (Tsubaki et al.,
2013). While such complex composition functions
slow down the training of compositional models,
richer information could be captured during train-
ing.
5.4 Effects of the Dimensionality
To see how the dimensionality of the word vectors
affects the scores, we trained the PAS-CLBLM for
each setting using 1,000-dimensional word vectors
and set the learning rate to 0.01. Table 5 shows
the scores for all four tasks. Note that we only re-
port the scores for the setting non-averaged SVO-
SVO here. As shown in Table 5, the scores consis-
tently improved with a few exceptions. The scores
? = 0.51 for the NN task and ? = 0.48 for the
VO task are the best results to date. However, the
score ? = 0.34 for the SVO task did not improve
by increasing the dimensionality. This means that
simply increasing the dimensionality of the word
vectors does not necessarily lead to better results
for complex phrases.
5.5 Effects of Bag-of-Words Contexts
Lastly, we trained the PAS-CLBLM without the
bag-of-words contexts described in Section 3.4
and used 50-dimensional word vectors. As can be
seen in Table 6, large score improvements were
observed only for the VO and SVO tasks by in-
cluding the bag-of-words contexts and the non-
linearity function. It is likely that the results de-
pend on how the bag-of-words contexts are con-
structed. However, we leave this line of analysis
as future work. Both adjective-noun and noun-
model BoW AN NN VO SVO
Add
l
w/ 0.52 0.44 0.35 0.24
w/o 0.48 0.46 0.38 0.23
Add
nl
w/ 0.52 0.46 0.45 0.24
w/o 0.50 0.47 0.41 0.15
Wadd
l
w/ 0.48 0.39 0.34 0.21
w/o 0.47 0.39 0.38 0.21
Wadd
nl
w/ 0.48 0.40 0.39 0.34
w/o 0.52 0.42 0.33 0.26
Table 6: Scores of the PAS-CLBLM with and
without BoW contexts.
noun phrase are noun phrases, and (subject-) verb-
object phrases can be regarded as complete sen-
tences. Therefore, different kinds of context infor-
mation might be required for both groups.
6 Qualitative Analysis on Composed
Vectors
An open question that remains is to what ex-
tent composition affects the representations pro-
duced by our PAS-CLBLM model. To evalu-
ate this we assigned a vector for each composed
representation. For example, the adjective-noun
dependency ?heavy rain? would be assigned an
independent vector. We added the most fre-
quent 100,000 adjective-noun, noun-noun, and
(subject-) verb-object tuples to the vocabulary and
the resulting vocabulary contained 400,000 to-
kens (100,000+3?100,000). A similar method
for treating frequent neighboring words as single
words was introduced by Mikolov et al. (2013b).
However, some dependencies, such as (subject-)
verb-object phrases, are not always captured when
considering only neighboring words.
Table 7 (No composition) shows some examples
of predicate-argument dependencies with their
closest neighbors in the vector space according
to the cosine similarity. The table shows that the
learned vectors of multiple words capture seman-
tic similarity. For example, the vector of ?heavy
rain? is close to the vectors of words which ex-
press the phenomena heavily raining. The vector
of ?new york? captures the concept of a major city.
The vectors of (subject-) verb-object dependencies
also capture the semantic similarity, which is the
main difference to previous approaches, such as
that of Mikolov et al. (2013b), which only consider
neighboring words. These results suggest that the
PAS-CLBLM can learn meaningful composition
1552
Query No composition Composition
rain rain
(AN) thunderstorm sunshine
heavy downpour storm
rain blizzard drizzle
much rain chill
general manager executive
(AN) vice president director
chief executive director representative
executive project manager officer
managing director administrator
second war war
(NN) plane crash world
world riot race
war last war holocaust
great war warfare
oslo york
(NN) paris toronto
new birmingham paris
york moscow edinburgh
madrid glasgow
make order make
(VO) carry survey allow
make pay tax demand
payment pay produce
impose tax bring
achieve objective solve
(VO) bridge gap alleviate
solve improve quality overcome
problem deliver information resolve
encourage development circumvent
hold meeting take
(SVO) event take place get
meeting end season win
take discussion take place put
place do work gain
Table 7: Nearest neighbor vectors for multiple
words. POS-tags are not shown for simplicity.
category predicate arg1 arg2
adj arg1 2.38 6.55 -
noun arg1 3.37 5.60 -
verb arg12 6.78 2.57 2.18
Table 8: L2-norms of the 50-dimensional weight
vectors of the composition function Wadd
nl
.
functions since the composition layers receive the
same error signal via backpropagation.
We then trained the PAS-CLBLM using Wadd
nl
to learn composition functions. Table 7 (Compo-
sition) shows the nearest neighbor words for each
composed vector, and as we can see, the learned
composition function emphasizes the head words
and captures some sort of semantic similarity. We
then inspected the L2-norms of the weight vectors
of the composition function. As shown in Table 8,
head words are strongly emphasized. Emphasiz-
ing head words is helpful in representing com-
posed meanings, but in the case of verbs it may
not always be sufficient. This can be observed in
Table 3 and Table 4, which demonstrates that verb-
related tasks are more difficult than noun-phrase
tasks.
While No composition captures the seman-
tic similarity well using independent parameters,
there is the issue of data sparseness. As the size of
the vocabulary increases, the number of tuples of
word dependencies increases rapidly. In this ex-
periment, we used only the 300,000 most frequent
tuples. In contrast to this, the learned composi-
tion functions can capture similar information us-
ing only word vectors and a small set of predicate
categories.
7 Conclusion and Future Work
We have presented a compositional log-bilinear
language model using predicate-argument struc-
tures that incorporates both bag-of-words and
dependency-based contexts. In our experiments
the learned composed vectors achieve state-of-the-
art scores for the task of measuring the semantic
similarity between short phrases. For the subject-
verb-object phrase task, the result is achieved
without any pre-trained word vectors using a cor-
pus an order of magnitude smaller than that of the
previous state of the art. For future work, we will
investigate how our models and the resulting vec-
tor representations can be helpful for other unsu-
pervised and/or supervised tasks.
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments and suggestions. This work was
supported by JSPS KAKENHI Grant Number
13F03041.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209?226.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
1553
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
William Blacoe and Mirella Lapata. 2012. A Com-
parison of Vector-based Representations for Seman-
tic Composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546?556.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493?2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121?2159.
Katrin Erk and Sebastian Pado?. 2008. A Structured
Vector Space Model for Word Meaning in Context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897?906.
Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi,
Rivlin Ehud, Solan Zach, Wolfman Gadi, and Rup-
pin Eytan. 2001. Placing Search in Context: The
Concept Revisited. In Proceedings of the Tenth In-
ternational World Wide Web Conference.
John Rupert Firth. 1957. A synopsis of linguistic
theory 1930-55. In Studies in Linguistic Analysis,
pages 1?32.
Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-
maya Sachan, Shashank Srivastava, and Eduard
Hovy. 2013. A Structured Distributional Seman-
tic Model : Integrating Structure with Semantics. In
Proceedings of the Workshop on Continuous Vector
Space Models and their Compositionality, pages 20?
29.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1394?
1404.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Simple Cus-
tomization of Recursive Neural Networks for Se-
mantic Relation Classification. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1372?1376.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 894?904.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving Word Represen-
tations via Global Context and Multiple Word Proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873?882.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior Disambiguation of Word Tensors for Con-
structing Sentence Vectors. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1590?1601.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating Disambiguation from
Composition in Distributional Semantics. In Pro-
ceedings of 17th Conference on Natural Language
Learning (CoNLL), pages 114?123.
Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
302?308.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of Work-
shop at the International Conference on Learning
Representations.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed Represen-
tations of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems 26, pages 3111?3119.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1439.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Advances in Neural Information Pro-
cessing Systems 26, pages 2265?2273.
Andriy Mnih and Yee Whye Teh. 2012. A fast
and simple algorithm for training neural probabilis-
tic language models. In John Langford and Joelle
Pineau, editors, Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12),
ICML ?12, pages 1751?1758.
Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
1554
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 90?99.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Compo-
sitionality through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1201?1211.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013a. Parsing with Compo-
sitional Vector Grammars. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
455?465.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1631?1642.
Richard Socher, Quoc V. Le, Christopher D. Manning,
and Andrew Y. Ng. 2014. Grounded Compositional
Semantics for Finding and Describing Images with
Sentences. Transactions of the Association for Com-
putational Linguistics, 2:207?218.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing Semantic Representations
Using Syntactically Enriched Vector Models. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 948?
957.
Masashi Tsubaki, Kevin Duh, Masashi Shimbo, and
Yuji Matsumoto. 2013. Modeling and Learning Se-
mantic Co-Compositionality through Prototype Pro-
jections and Neural Networks. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 130?140.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384?394.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A Tensor-based Factorization Model of
Semantic Compositionality. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1142?1151.
1555
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 102?107,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
BRAT: a Web-based Tool for NLP-Assisted Text Annotation
Pontus Stenetorp1? Sampo Pyysalo2,3? Goran Topic?1
Tomoko Ohta1,2,3 Sophia Ananiadou2,3 and Jun?ichi Tsujii4
1Department of Computer Science, The University of Tokyo, Tokyo, Japan
2School of Computer Science, University of Manchester, Manchester, UK
3National Centre for Text Mining, University of Manchester, Manchester, UK
4Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,smp,goran,okap}@is.s.u-tokyo.ac.jp
sophia.ananiadou@manchester.ac.uk
jtsujii@microsoft.com
Abstract
We introduce the brat rapid annotation tool
(BRAT), an intuitive web-based tool for text
annotation supported by Natural Language
Processing (NLP) technology. BRAT has
been developed for rich structured annota-
tion for a variety of NLP tasks and aims
to support manual curation efforts and in-
crease annotator productivity using NLP
techniques. We discuss several case stud-
ies of real-world annotation projects using
pre-release versions of BRAT and present
an evaluation of annotation assisted by se-
mantic class disambiguation on a multi-
category entity mention annotation task,
showing a 15% decrease in total annota-
tion time. BRAT is available under an open-
source license from: http://brat.nlplab.org
1 Introduction
Manually-curated gold standard annotations are
a prerequisite for the evaluation and training of
state-of-the-art tools for most Natural Language
Processing (NLP) tasks. However, annotation is
also one of the most time-consuming and finan-
cially costly components of many NLP research
efforts, and can place heavy demands on human
annotators for maintaining annotation quality and
consistency. Yet, modern annotation tools are
generally technically oriented and many offer lit-
tle support to users beyond the minimum required
functionality. We believe that intuitive and user-
friendly interfaces as well as the judicious appli-
cation of NLP technology to support, not sup-
plant, human judgements can help maintain the
quality of annotations, make annotation more ac-
cessible to non-technical users such as subject
?These authors contributed equally to this work
Figure 1: Visualisation examples. Top: named en-
tity recognition, middle: dependency syntax, bot-
tom: verb frames.
domain experts, and improve annotation produc-
tivity, thus reducing both the human and finan-
cial cost of annotation. The tool presented in
this work, BRAT, represents our attempt to realise
these possibilities.
2 Features
2.1 High-quality Annotation Visualisation
BRAT is based on our previously released open-
source STAV text annotation visualiser (Stene-
torp et al 2011b), which was designed to help
users gain an understanding of complex annota-
tions involving a large number of different se-
mantic types, dense, partially overlapping text an-
notations, and non-projective sets of connections
between annotations. Both tools share a vector
graphics-based visualisation component, which
provide scalable detail and rendering. BRAT in-
tegrates PDF and EPS image format export func-
tionality to support use in e.g. figures in publica-
tions (Figure 1).
102
Figure 2: Screenshot of the main BRAT user-interface, showing a connection being made between the
annotations for ?moving? and ?Citibank?.
2.2 Intuitive Annotation Interface
We extended the capabilities of STAV by imple-
menting support for annotation editing. This was
done by adding functionality for recognising stan-
dard user interface gestures familiar from text ed-
itors, presentation software, and many other tools.
In BRAT, a span of text is marked for annotation
simply by selecting it with the mouse by ?drag-
ging? or by double-clicking on a word. Similarly,
annotations are linked by clicking with the mouse
on one annotation and dragging a connection to
the other (Figure 2).
BRAT is browser-based and built entirely using
standard web technologies. It thus offers a fa-
miliar environment to annotators, and it is pos-
sible to start using BRAT simply by pointing a
standards-compliant modern browser to an instal-
lation. There is thus no need to install or dis-
tribute any additional annotation software or to
use browser plug-ins. The use of web standards
also makes it possible for BRAT to uniquely iden-
tify any annotation using Uniform Resource Iden-
tifiers (URIs), which enables linking to individual
annotations for discussions in e-mail, documents
and on web pages, facilitating easy communica-
tion regarding annotations.
2.3 Versatile Annotation Support
BRAT is fully configurable and can be set up to
support most text annotation tasks. The most ba-
sic annotation primitive identifies a text span and
assigns it a type (or tag or label), marking for e.g.
POS-tagged tokens, chunks or entity mentions
(Figure 1 top). These base annotations can be
connected by binary relations ? either directed or
undirected ? which can be configured for e.g. sim-
ple relation extraction, or verb frame annotation
(Figure 1 middle and bottom). n-ary associations
of annotations are also supported, allowing the an-
notation of event structures such as those targeted
in the MUC (Sundheim, 1996), ACE (Doddington
et al 2004), and BioNLP (Kim et al 2011) In-
formation Extraction (IE) tasks (Figure 2). Addi-
tional aspects of annotations can be marked using
attributes, binary or multi-valued flags that can
be added to other annotations. Finally, annotators
can attach free-form text notes to any annotation.
In addition to information extraction tasks,
these annotation primitives allow BRAT to be
configured for use in various other tasks, such
as chunking (Abney, 1991), Semantic Role La-
beling (Gildea and Jurafsky, 2002; Carreras
and Ma`rquez, 2005), and dependency annotation
(Nivre, 2003) (See Figure 1 for examples). Fur-
ther, both the BRAT client and server implement
full support for the Unicode standard, which al-
low the tool to support the annotation of text us-
ing e.g. Chinese or Devana?gar?? characters. BRAT
is distributed with examples from over 20 cor-
pora for a variety of tasks, involving texts in seven
different languages and including examples from
corpora such as those introduced for the CoNLL
shared tasks on language-independent named en-
tity recognition (Tjong Kim Sang and De Meul-
der, 2003) and multilingual dependency parsing
(Buchholz and Marsi, 2006).
BRAT also implements a fully configurable sys-
tem for checking detailed constraints on anno-
tation semantics, for example specifying that a
TRANSFER event must take exactly one of each
of GIVER, RECIPIENT and BENEFICIARY argu-
ments, each of which must have one of the types
PERSON, ORGANIZATION or GEO-POLITICAL
ENTITY, as well as a MONEY argument of type
103
Figure 3: Incomplete TRANSFER event indicated
to the annotator
MONEY, and may optionally take a PLACE argu-
ment of type LOCATION (LDC, 2005). Constraint
checking is fully integrated into the annotation in-
terface and feedback is immediate, with clear vi-
sual effects marking incomplete or erroneous an-
notations (Figure 3).
2.4 NLP Technology Integration
BRAT supports two standard approaches for inte-
grating the results of fully automatic annotation
tools into an annotation workflow: bulk anno-
tation imports can be performed by format con-
version tools distributed with BRAT for many
standard formats (such as in-line and column-
formatted BIO), and tools that provide standard
web service interfaces can be configured to be in-
voked from the user interface.
However, human judgements cannot be re-
placed or based on a completely automatic analy-
sis without some risk of introducing bias and re-
ducing annotation quality. To address this issue,
we have been studying ways to augment the an-
notation process with input from statistical and
machine learning methods to support the annota-
tion process while still involving human annotator
judgement for each annotation.
As a specific realisation based on this approach,
we have integrated a recently introduced ma-
chine learning-based semantic class disambigua-
tion system capable of offering multiple outputs
with probability estimates that was shown to be
able to reduce ambiguity on average by over 75%
while retaining the correct class in on average
99% of cases over six corpora (Stenetorp et al
2011a). Section 4 presents an evaluation of the
contribution of this component to annotator pro-
ductivity.
2.5 Corpus Search Functionality
BRAT implements a comprehensive set of search
functions, allowing users to search document col-
Figure 4: The BRAT search dialog
lections for text span annotations, relations, event
structures, or simply text, with a rich set of search
options definable using a simple point-and-click
interface (Figure 4). Additionally, search results
can optionally be displayed using keyword-in-
context concordancing and sorted for browsing
using any aspect of the matched annotation (e.g.
type, text, or context).
3 Implementation
BRAT is implemented using a client-server ar-
chitecture with communication over HTTP using
JavaScript Object Notation (JSON). The server is
a RESTful web service (Fielding, 2000) and the
tool can easily be extended or adapted to switch
out the server or client. The client user interface is
implemented using XHTML and Scalable Vector
Graphics (SVG), with interactivity implemented
using JavaScript with the jQuery library. The
client communicates with the server using Asyn-
chronous JavaScript and XML (AJAX), which
permits asynchronous messaging.
BRAT uses a stateless server back-end imple-
mented in Python and supports both the Common
Gateway Interface (CGI) and FastCGI protocols,
the latter allowing response times far below the
100 ms boundary for a ?smooth? user experience
without noticeable delay (Card et al 1983). For
server side annotation storage BRAT uses an easy-
to-process file-based stand-off format that can be
converted from or into other formats; there is no
need to perform database import or export to in-
terface with the data storage. The BRAT server in-
104
Figure 5: Example annotation from the BioNLP Shared Task 2011 Epigenetics and Post-translational
Modifications event extraction task.
stallation requires only a CGI-capable web server
and the set-up supports any number of annotators
who access the server using their browsers, on any
operating system, without separate installation.
Client-server communication is managed so
that all user edit operations are immediately sent
to the server, which consolidates them with the
stored data. There is no separate ?save? operation
and thus a minimal risk of data loss, and as the
authoritative version of all annotations is always
maintained by the server, there is no chance of
conflicting annotations being made which would
need to be merged to produce an authoritative ver-
sion. The BRAT client-server architecture also
makes real-time collaboration possible: multiple
annotators can work on a single document simul-
taneously, seeing each others edits as they appear
in a document.
4 Case Studies
4.1 Annotation Projects
BRAT has been used throughout its development
during 2011 in the annotation of six different cor-
pora by four research groups in efforts that have
in total involved the creation of well-over 50,000
annotations in thousands of documents compris-
ing hundreds of thousands of words.
These projects include structured event an-
notation for the domain of cancer biology,
Japanese verb frame annotation, and gene-
mutation-phenotype relation annotation. One
prominent effort making use of BRAT is the
BioNLP Shared Task 2011,1 in which the tool was
used in the annotation of the EPI and ID main
task corpora (Pyysalo et al 2012). These two
information extraction tasks involved the annota-
tion of entities, relations and events in the epige-
netics and infectious diseases subdomains of biol-
ogy. Figure 5 shows an illustration of shared task
annotations.
Many other annotation efforts using BRAT are
still ongoing. We refer the reader to the BRAT
1http://2011.bionlp-st.org
Mode Total Type Selection
Normal 45:28 13:49
Rapid 39:24 (-6:04) 09:35 (-4:14)
Table 1: Total annotation time, portion spent se-
lecting annotation type, and absolute improve-
ment for rapid mode.
website2 for further details on current and past an-
notation projects using BRAT.
4.2 Automatic Annotation Support
To estimate the contribution of the semantic class
disambiguation component to annotation produc-
tivity, we performed a small-scale experiment in-
volving an entity and process mention tagging
task. The annotation targets were of 54 dis-
tinct mention types (19 physical entity and 35
event/process types) marked using the simple
typed-span representation. To reduce confound-
ing effects from annotator productivity differ-
ences and learning during the task, annotation was
performed by a single experienced annotator with
a Ph.D. in biology in a closely related area who
was previously familiar with the annotation task.
The experiment was performed on publication
abstracts from the biomolecular science subdo-
main of glucose metabolism in cancer. The texts
were drawn from a pool of 1,750 initial candi-
dates using stratified sampling to select pairs of
10-document sets with similar overall statistical
properties.3 Four pairs of 10 documents (80 in to-
tal) were annotated in the experiment, with 10 in
each pair annotated with automatic support and 10
without, in alternating sequence to prevent learn-
ing effects from favouring either approach.
The results of this experiment are summarized
in Table 1 and Figure 6. In total 1,546 annotations
were created in normal mode and 1,541 annota-
2http://brat.nlplab.org
3Document word count and expected annotation count,
were estimated from the output of NERsuite, a freely avail-
able CRF-based NER tagger: http://nersuite.nlplab.org
105
0500
1000
1500
2000
2500
3000
Normal Mode Rapid Mode
Tim
e(
se
co
nd
s)
Figure 6: Allocation of annotation time. GREEN
signifies time spent on selecting annotation type
and BLUE the remaining annotation time.
tions in rapid mode; the sets are thus highly com-
parable. We observe a 15.4% reduction in total
annotation time, and, as expected, this is almost
exclusively due to a reduction in the time the an-
notator spent selecting the type to assign to each
span, which is reduced by 30.7%; annotation time
is otherwise stable across the annotation modes
(Figure 6). The reduction in the time spent in se-
lecting the span is explained by the limiting of the
number of candidate types exposed to the annota-
tor, which were decreased from the original 54 to
an average of 2.88 by the semantic class disam-
biguation component (Stenetorp et al 2011a).
Although further research is needed to establish
the benefits of this approach in various annotation
tasks, we view the results of this initial experi-
ment as promising regarding the potential of our
approach to using machine learning to support an-
notation efforts.
5 Related Work and Conclusions
We have introduced BRAT, an intuitive and user-
friendly web-based annotation tool that aims to
enhance annotator productivity by closely inte-
grating NLP technology into the annotation pro-
cess. BRAT has been and is being used for several
ongoing annotation efforts at a number of aca-
demic institutions and has so far been used for
the creation of well-over 50,000 annotations. We
presented an experiment demonstrating that inte-
grated machine learning technology can reduce
the time for type selection by over 30% and over-
all annotation time by 15% for a multi-type entity
mention annotation task.
The design and implementation of BRAT was
informed by experience from several annotation
tasks and research efforts spanning more than
a decade. A variety of previously introduced
annotation tools and approaches also served to
guide our design decisions, including the fast an-
notation mode of Knowtator (Ogren, 2006), the
search capabilities of the XConc tool (Kim et al
2008), and the design of web-based systems such
as MyMiner (Salgado et al 2010), and GATE
Teamware (Cunningham et al 2011). Using ma-
chine learning to accelerate annotation by sup-
porting human judgements is well documented in
the literature for tasks such as entity annotation
(Tsuruoka et al 2008) and translation (Mart??nez-
Go?mez et al 2011), efforts which served as in-
spiration for our own approach.
BRAT, along with conversion tools and exten-
sive documentation, is freely available under the
open-source MIT license from its homepage at
http://brat.nlplab.org
Acknowledgements
The authors would like to thank early adopters of
BRAT who have provided us with extensive feed-
back and feature suggestions. This work was sup-
ported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan), the UK Biotechnology
and Biological Sciences Research Council (BB-
SRC) under project Automated Biological Event
Extraction from the Literature for Drug Discov-
ery (reference number: BB/G013160/1), and the
Royal Swedish Academy of Sciences.
106
References
Steven Abney. 1991. Parsing by chunks. Principle-
based parsing, 44:257?278.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149?164.
Stuart K. Card, Thomas P. Moran, and Allen Newell.
1983. The psychology of human-computer interac-
tion. Lawrence Erlbaum Associates, Hillsdale, New
Jersey.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic Role
Labeling. In Proceedings of the 9th Conference on
Natural Language Learning, pages 152?164. Asso-
ciation for Computational Linguistics.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, Angus
Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program: Tasks, data, and evaluation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation, pages 837?
840.
Roy Fielding. 2000. REpresentational State Trans-
fer (REST). Architectural Styles and the Design
of Network-based Software Architectures. Univer-
sity of California, Irvine, page 120.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC Bioinformatics,
9(1):10.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP Shared Task 2011. In Pro-
ceedings of BioNLP Shared Task 2011 Workshop,
pages 1?6, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
LDC. 2005. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Events. Technical
report, Linguistic Data Consortium.
Pascual Mart??nez-Go?mez, Germa?n Sanchis-Trilles,
and Francisco Casacuberta. 2011. Online learn-
ing via dynamic reranking for computer assisted
translation. In Alexander Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
volume 6609 of Lecture Notes in Computer Science,
pages 93?105. Springer Berlin / Heidelberg.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Proceedings of the
8th International Workshop on Parsing Technolo-
gies, pages 149?160.
Philip V. Ogren. 2006. Knowtator: A prote?ge? plug-in
for annotated corpus construction. In Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Companion Volume:
Demonstrations, pages 273?275, New York City,
USA, June. Association for Computational Linguis-
tics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Junichi Tsujii, and Sophia Ananiadou. 2012.
Overview of the ID, EPI and REL tasks of BioNLP
Shared Task 2011. BMC Bioinformatics, 13(suppl.
8):S2.
David Salgado, Martin Krallinger, Marc Depaule,
Elodie Drula, and Ashish V Tendulkar. 2010.
Myminer system description. In Proceedings of the
Third BioCreative Challenge Evaluation Workshop
2010, pages 157?158.
Pontus Stenetorp, Sampo Pyysalo, Sophia Ananiadou,
and Jun?ichi Tsujii. 2011a. Almost total recall: Se-
mantic category disambiguation using large lexical
resources and approximate string matching. In Pro-
ceedings of the Fourth International Symposium on
Languages in Biology and Medicine.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011b. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 112?120, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Beth M. Sundheim. 1996. Overview of results of
the MUC-6 evaluation. In Proceedings of the Sixth
Message Understanding Conference, pages 423?
442. Association for Computational Linguistics.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recogni-
tion. In Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 142?147.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2008. Accelerating the annotation of
sparse named entities by dynamic sentence selec-
tion. BMC Bioinformatics, 9(Suppl 11):S8.
107
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 136?145,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
SimSem: Fast Approximate String Matching in Relation to Semantic
Category Disambiguation
Pontus Stenetorp?? Sampo Pyysalo? and Jun?ichi Tsujii?
? Tsujii Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
? Aizawa Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
? Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,smp}@is.s.u-tokyo.ac.jp
jtsujii@microsoft.com
Abstract
In this study we investigate the merits of
fast approximate string matching to address
challenges relating to spelling variants and to
utilise large-scale lexical resources for seman-
tic class disambiguation. We integrate string
matching results into machine learning-based
disambiguation through the use of a novel set
of features that represent the distance of a
given textual span to the closest match in each
of a collection of lexical resources. We col-
lect lexical resources for a multitude of se-
mantic categories from a variety of biomedi-
cal domain sources. The combined resources,
containing more than twenty million lexical
items, are queried using a recently proposed
fast and efficient approximate string match-
ing algorithm that allows us to query large
resources without severely impacting system
performance. We evaluate our results on six
corpora representing a variety of disambigua-
tion tasks. While the integration of approxi-
mate string matching features is shown to sub-
stantially improve performance on one corpus,
results are modest or negative for others. We
suggest possible explanations and future re-
search directions. Our lexical resources and
implementation are made freely available for
research purposes at: http://github.com/ninjin/
simsem
1 Introduction
The use of dictionaries for boosting performance has
become commonplace for Named Entity Recogni-
tion (NER) systems (Torii et al, 2009; Ratinov and
Roth, 2009). In particular, dictionaries can give an
initial improvement when little or no training data
is available. However, no dictionary is perfect, and
all resources lack certain spelling variants and lag
behind current vocabulary usage and thus are un-
able to cover the intended domain in full. Further,
due to varying dictionary curation and corpus anno-
tation guidelines, the definition of what constitutes
a semantic category is highly unlikely to precisely
match for any two specific resources (Wang et al,
2009). Ideally, for applying a lexical resource to an
entity recognition or disambiguation task to serve as
a definition of a semantic category there would be
a precise match between the definitions of the lexi-
cal resource and target domain, but this is seldom or
never the case.
Most previous work studying the use of dictionary
resources in entity mention-related tasks has focused
on single-class NER, in particular this is true for
BioNLP where it has mainly concerned the detec-
tion of proteins. These efforts include Tsuruoka and
Tsujii (2003), utilising dictionaries for protein de-
tection by considering each dictionary entry using a
novel distance measure, and Sasaki et al (2008), ap-
plying dictionaries to restrain the contexts in which
proteins appear in text. In this work, we do not
consider entity mention detection, but instead focus
solely on the related task of disambiguating the se-
mantic category for a given continuous sequence of
characters (a textual span), doing so we side-step the
issue of boundary detection in favour of focusing on
novel aspects of semantic category disambiguation.
Also, we are yet to see a high-performing multi-class
biomedical NER system, this motivates our desire to
include multiple semantic categories.
136
2 Methods
In this section we introduce our approach and the
structure of our system.
2.1 SimSem
Many large-scale language resources are available
for the biomedical domain, including collections
of domain-specific lexical items (Ashburner et al,
2000; Bodenreider, 2004; Rebholz-Schuhmann et
al., 2010). These resources present obvious opportu-
nities for semantic class disambiguation. However,
in order to apply them efficiently, one must be able
to query the resources taking into consideration both
lexical variations in dictionary entries compared to
real-world usage and the speed of look-ups.
We can argue that each resource offers a differ-
ent view of what constitutes a particular semantic
category. While these views will not fully overlap
between resources even for the same semantic cate-
gory, we can expect a certain degree of agreement.
When learning to disambiguate between semantic
categories, a machine learning algorithm could be
expected to learn to identify a specific semantic cat-
egory from the similarity between textual spans an-
notated for the category and entries in a related lex-
ical resource. For example, if we observe the text
?Carbonic anhydrase IV? marked as PROTEIN and
have an entry for ?Carbonic anhydrase 4? in a lexical
resource, a machine learning method can learn to as-
sociate the resource with the PROTEIN category (at
specific similarity thresholds) despite syntactic dif-
ferences.
In this study, we aim to construct such a system
and to demonstrate that it outperforms strict string
matching approaches. We refer to our system as
SimSem, as in ?Similarity? and ?Semantic?.
2.2 SimString
SimString1 is a software library utilising the CP-
Merge algorithm (Okazaki and Tsujii, 2010) to en-
able fast approximate string matching. The software
makes it possible to find matches in a collection with
over ten million entries using cosine similarity and
a similarity threshold of 0.7 in approximately 1 mil-
lisecond with modest modern hardware. This makes
it useful for querying a large collection of strings to
1http://www.chokkan.org/software/simstring/
find entries which may differ from the query string
only superficially and may still be members of the
same semantic category.
As an example, if we construct a SimString
database using an American English wordlist2 and
query it using the cosine measure and a threshold of
0.7. For the query ?reviewer? SimString would re-
turn the following eight entries: review, viewer, pre-
view, reviewer, unreviewed, televiewer, and review-
eress. We can observe that most of the retrieved en-
tries share some semantic similarity with the query.
2.3 Machine Learning
For the machine learning component of our system
we use the L2-regularised logistic regression im-
plementation of the LIBLINEAR3 software library
(Fan et al, 2008). We do not normalise our feature
vectors and optimise our models? penalty parameter
using k-fold cross-validation on the training data. In
order to give a fair representation of the performance
of other systems, we use a rich set of features that are
widely applied for NER (See Table 1).
Our novel SimString features are generated as fol-
lows. We query each SimString database using the
cosine measure with a sliding similarity threshold,
starting at 1.0 and ending at 0.7, lowering the thresh-
old by 0.1 per query. If a query is matched, we gen-
erate a feature unique for that database and thresh-
old, we also generate the same feature for each step
from the current threshold to the cut-off of 0.7 (a
match at e.g. 0.9 similarity also implies matches at
0.8 and 0.7).
The cut-off is motivated by the fact that very
low thresholds introduces a large degree of noise.
For example, for our American English wordlist
the query ?rejection? using threshold 0.1 and the
cosine measure will return 13,455 results, among
them ?questionableness? which only have a single
sequence ?ion? in common.
It is worthwhile to note that during our prelimi-
nary experiments we failed to establish a consistent
benefit from contextual features across our develop-
ment sets. Thus, contextual features are not included
in our feature set and instead our study focuses only
2/usr/share/dict/web2 under FreeBSD 8.1-RELEASE, based
on Webster?s Second International dictionary from 1934
3We used version 1.7 of LIBLINEAR for our experiments
137
Feature Type Input Value(s)
Text Text Flu Flu
Lower-cased Text DNA dna
Prefixes: sizes 3 to 5 Text bull bul, . . .
Suffixes: sizes 3 to 5 Text bull ull, . . .
Stem (Porter, 1993) Text performing perform
Is a pair of digits Bool 42 True
Is four digits Bool 4711 True
Letters and digits Bool C4 True
Digits and hyphens Bool 9-12 True
Digits and slashes Bool 1/2 True
Digits and colons Bool 3,1 True
Digits and dots Bool 3.14 True
Upper-case and dots Bool M.C. True
Initial upper-case Bool Pigeon True
Only upper-case Bool PMID True
Only lower-case Bool pure True
Only digits Bool 131072 True
Only non-alpha-num Bool #*$! True
Contains upper-case Bool gAwn True
Contains lower-case Bool After True
Contains digits Bool B52 True
Contains non-alpha-num Bool B52;s True
Date regular expression4 Bool 1989-01-30 True
Pattern Text 1B-zz 0A-aa
Collapsed Pattern Text 1B-zz 0A-a
Table 1: Basic features used for classification
the features that are generated solely from the tex-
tual span which has been annotated with a semantic
category (span-internal features) and the comparison
of approximate and strict string matching.
3 Resources
This section introduces and discusses the prepro-
cessing and statistics of the lexical and corpus re-
sources used in our experiments.
3.1 Lexical Resources
To generate a multitude of SimString databases cov-
ering a wide array of semantic categories we employ
several freely available lexical resources (Table 2).
The choice of lexical resources was initially made
with the aim to cover commonly annotated domain
semantic categories: the CHEBI and CHEMICAL
subsets of JOCHEM for chemicals, LINNAEUS for
species, Entrez Gene and SHI for proteins. We then
4A simple regular expression matching dates:
?(19|20)\d\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])$
from http://www.regular-expressions.info/dates.html
expanded the selection based on error analysis to in-
crease our coverage of a wider array of semantic cat-
egories present in our development data.
We used the GO version from March 2011, ex-
tracting all non-obsolete terms from the ontology
and separating them into the three GO subontolo-
gies: biological process (BP), cellular component
(CC) and molecular function (MF). We then created
an additional three resources by extracting all exact
synonyms for each entry. Lastly, we expanded these
six resources into twelve resources by applying the
GO term variant generation technique described by
Beisswanger et al (2008).
UMLS, a collection of various resources, contain
135 semantic categories (e.g. Body Location or Re-
gion and Inorganic Chemical) which we use to cre-
ate a database for each category.
For Entrez Gene we extracted all entries for the
following types: gene locus, protein name, protein
description, nomenclature symbol and nomenclature
fullname, creating a SimString database for each.
This leaves some parts of Entrez Gene unutilised,
but we deemed these categories to be sufficient for
our experiments.
The Turku Event Corpus is a resource created by
applying an automated event extraction system on
the full release of PubMed from 2009. As a pre-
condition for the event extraction system to operate,
protein name recognition is necessary; for this cor-
pus, NER has been performed by the corpus curators
using the BANNER (Leaman and Gonzalez, 2008)
NER system trained on GENETAG (Tanabe et al,
2005). We created a database (PROT) containing
all protein annotations, extracted all event triggers
(TRIG) and created a database for each of the event
types covered by the event extraction system.
For the AZDC corpus, we extracted each anno-
tated textual span since the corpus covers only a sin-
gle semantic category. Similarly, the LINNAEUS
dictionary was converted into a single database since
it covers the single category ?species?.
Table 3 contains the statistics per dictionary re-
source and the number of SimString databases cre-
ated for each resource. Due to space requirements
we leave out the full details for GO BP, GO CC,
GO MF, UMLS, Entrez Gene and TURKU TRIG,
and instead give the total entries for all the databases
generated from these resources.
138
Name Abbreviation Semantic Categories Publication
Gene Ontology GO Multiple Ashburner et al (2000)
Protein Information Resource PIR Proteins Wu et al (2003)
Unified Medical Language System UMLS Multiple Bodenreider (2004)
Entrez Gene ? Proteins Maglott et al (2005)
Automatically generated dictionary SHI Proteins Shi and Campagne (2005)
Jochem JOCHEM Multiple Hettne et al (2009)
Turku Event Corpus TURKU Proteins and biomolecular events Bjo?rne et al (2010)
Arizona Disease Corpus AZDC Diseases Chowdhury and Lavelli (2010)
LINNAEUS Dictionary LINNAEUS Species Gerner et al (2010)
Webster?s International Dictionary WID Multiple ?
Table 2: Lexical resources gathered for our experiments
Resource Unique Entries Databases
GO BP 67,411 4
GO CC 5,993 4
GO MF 55,595 4
PIR 691,577 1
UMLS 5,902,707 135
Entrez Gene 3,602,757 5
SHI 61,676 1
CHEBI 187,993 1
CHEMICAL 1,527,751 1
TURKU PROT 4,745,825 1
TURKU TRIG 130,139 10
AZDC 1,195 1
LINNAEUS 3,119,005 1
WID 235,802 1
Total: 20, 335, 426 170
Table 3: Statistics per dictionary resource
3.2 Corpora
To evaluate our approach we need a variety of cor-
pora annotated with multiple semantic categories.
For this purpose we selected the six corpora listed
in Table 4.
The majority of our corpora are available in the
common stand-off style format introduced for the
BioNLP 2009 Shared Task (BioNLP?09 ST) (Kim
et al, 2009). The remaining two, NLPBA and
CALBC CII, were converted into the BioNLP?09 ST
format so that we could process all resources in the
same manner for our experimental set-up.
In addition to physical entity annotations, the
GREC, EPI, ID and GENIA corpora incorporate
event trigger annotations (e.g. Gene Regulatory
Event (GRE) for GREC). These trigger expressions
carry with them a specific semantic type (e.g. ?in-
teract? can carry the semantic type BINDING for
GENIA), allowing us to enrich the data sets with
additional semantic categories by including these
types in our dataset as distinct semantic categories.
This gave us the following increase in semantic cat-
egories: GREC one, EPI 15, ID ten, GENIA nine.
The original GREC corpus contains an exception-
ally wide array of semantic categories. While this
is desirable for evaluating the performance of our
approach under different task settings, the sparsity
of the data is a considerable problem; the majority
of categories do not permit stable evaluation as they
have only a handful of annotations each. To alleviate
this problem we used the five ontologies defined in
the GREC annotation guidelines5, collapsing the an-
notations into five semantic super categories to cre-
ate a resource we refer to as Super GREC. This pre-
processing conforms with how the categories were
used when annotating the GREC corpus (Thompson
et al, 2009). This resource contains sufficient anno-
tations for each semantic category to enable evalua-
tion on a category-by-category basis. Also, for the
purpose of our experiments we removed all ?SPAN?
type annotations since they themselves carry no se-
mantic information (cf. GREC annotation guide-
lines).
CALBC CII contains 75,000 documents, which
is more than enough for our experiments. In order
to maintain balance in size between the resources in
our experiments, we sampled a random 5,000 docu-
ments and used these as our CALBC CII dataset.
5http://www.nactem.ac.uk/download.php?target=GREC/
Event annotation guidelines.pdf
139
Name Abbreviation Publication
BioNLP/NLPBA 2004 Shared Task Corpus NLPBA Kim et al (2004)
Gene Regulation Event Corpus GREC Thompson et al (2009)
Collaborative Annotation of a Large Biomedical Corpus CALBC CII Rebholz-Schuhmann et al (2010)
Epigenetics and Post-Translational Modifications EPI Ohta et al (2011)
Infectious Diseases Corpus ID Pyysalo et al (2011)
Genia Event Corpus GENIA Kim et al (2011)
Table 4: Corpora used for evaluation
3.3 Corpus Statistics
In this section we present statistics for each of our
datasets. For resources with a limited number of se-
mantic categories we use pie charts to illustrate their
distribution (Figure 1). For the other corpora we use
tables to illustrate this. Tables for the corpora for
which pie charts are given has been left out due to
space requirements.
The NLPBA corpus (Figure 1a) with 59,601 to-
kens annotated, covers five semantic categories, with
a clear majority of protein annotations. While
NLPBA contains several semantic categories, they
are closely related, which is expected to pose chal-
lenges for disambiguation. This holds in particular
for proteins, DNA and RNA, which commonly share
names.
Our collapsed version of GREC, Super GREC
(see Figure 1b), contains 6,777 annotated tokens and
covers a total of six semantic categories: Regulatory
Event (GRE), nucleic acids, proteins, processes, liv-
ing system and experimental. GREC is an interest-
ing resource in that its classes are relatively distinct
and four of them are evenly distributed.
CALBC CII is balanced among its annotated cat-
egories, as illustrated in Figure 1c. The 6,433 to-
kens annotated are of the types: proteins and genes
(PRGE), species (SPE), disorders (DISO) and chem-
icals and drugs (CHED). We note that we have in-
troduced lexical resources covering each of these
classes (Section 3.1).
For the BioNLP?11 ST resources EPI (Table 5),
GENIA (Figure 1d and contains 27,246 annotated
tokens) and ID (Table 6), we observe a very skewed
distribution due to our decision to include event
types as distinct classes; The dominating class for
all the datasets are proteins. For several of these
categories, learning accurate disambiguation is ex-
Type Ratio Annotations
Acetylation 2.3% 294
Catalysis 1.4% 186
DNA demethylation 0.1% 18
DNA methylation 2.3% 301
Deacetylation 0.3% 43
Deglycosylation 0.2% 26
Dehydroxylation 0.0% 1
Demethylation 0.1% 12
Dephosphorylation 0.0% 3
Deubiquitination 0.1% 13
Entity 6.6% 853
Glycosylation 2.3% 295
Hydroxylation 0.9% 116
Methylation 2.5% 319
Phosphorylation 0.9% 112
Protein 77.7% 10,094
Ubiquitination 2.3% 297
Total: 12,983
Table 5: Semantic categories in EPI
pected to be very challenging if not impossible due
to sparsity: For example, Dehydroxylation in EPI
has a single annotation.
ID is of particular interest since it contains a con-
siderable amount of annotations for more than one
physical entity category, including in addition to
protein also organism and a minor amount of chem-
ical annotations.
4 Experiments
In this section we introduce our experimental set-up
and discuss the outcome of our experiments.
4.1 Experimental Set-up
To ensure that our results are not biased by over-
fitting on a specific set of data, all data sets were
separated into training, development and test sets.
140
(a) NLPBA
(b) Super GREC
(c) CALBC CII
(d) GENIA
Figure 1: Semantic category distributions
NLPBA defines only a training and test set, GREC
and CALBC CII are provided as resources and lack
any given division, and for the BioNLP?11 ST data
the test sets are not distributed. Thus, we combined
all the available data for each dataset and separated
the documents into fixed sets with the following ra-
tios: 1/2 training, 1/4 development and 1/4 test.
Type Ratio Annotations
Binding 1.0% 102
Chemical 6.8% 725
Entity 0.4% 43
Gene expression 3.3% 347
Localization 0.3% 36
Negative regulation 1.6% 165
Organism 25.5% 2,699
Phosphorylation 0.5% 54
Positive regulation 2.5% 270
Process 8.0% 843
Protein 43.1% 4,567
Protein catabolism 0.0% 5
Regulation 1.8% 188
Regulon-operon 1.1% 121
Transcription 0.4% 47
Two-component-system 3.7% 387
Total: 10,599
Table 6: Semantic categories in ID
We use a total of six classifiers for our experi-
ments. First, a naive baseline (Naive): a majority
class voter with a memory based on the exact text
of the textual span. The remaining five are ma-
chine learning classifiers trained using five differ-
ent feature sets: gazetteer features constituting strict
string matching towards our SimString databases
(Gazetteer), SimString features generated from our
SimString databases (SimString), the span internal
features listed in Table 1 (Internal), the span inter-
nal and gazetteer features (Internal-Gazetteer) and
the span internal and SimString features (Internal-
SimString).
We evaluate performance using simple instance-
level accuracy (correct classifications / all classifica-
tions). Results are represented as learning curves for
each data set.
4.2 Results
From our experiments we find that ? not surpris-
ingly ? the performance of the Naive, Gazetteer and
SimString classifiers alone is comparatively weak.
Their performance is illustrated in Figure 2. We can
briefly summarize the results for these methods by
noting that the SimString classifier outperforms the
Gazetteer by a large margin for every dataset.6 From
6Due to space restrictions we do not include further analysis
or charts.
141
Figure 2: SimString, Gazetteer and Naive for ID
Figure 3: Learning curve for NLPBA
here onwards we focus on the performance of the In-
ternal classifier in combination with Gazetteer and
SimString features.
For NLPBA (Figure 3), GENIA (Figure 4) and ID
(Figure 5) our experiments show no clear systematic
benefit from either SimString or Gazetteer features.
For Super GREC (Figure 6) and EPI (Figure 7)
classifiers with Gazetteer and SimString features
consistently outperform the Internal classifier, and
the SimString classifier further shows some benefit
over Gazetteer for EPI.
The only dataset for which we see a clear benefit
from SimString features over Gazetteer and Internal
is for CALBC CII (Figure 8).
5 Discussion and Conclusions
While we expected to see clear benefits from both
using Gazetteers and SimString features, our exper-
Figure 4: Learning curve for GENIA
Figure 5: Learning curve for ID
iments returned negative results for the majority of
the corpora. For NLPBA, GENIA and ID we are
aware that most of the instances are either proteins
or belong to event trigger classes for which we may
not have had adequate lexical resources for disam-
biguation. By contrast, for Super GREC there are
several distinct classes for which we expected lex-
ical resources to have fair coverage for SimString
and Gazetteer features. While an advantage over In-
ternal was observed for Super GREC, SimString fea-
tures showed no benefit over Gazetteer features. The
methods exhibited the expected result on only one of
the six corpora, CALBC CII, where there is a clear
advantage for Gazetteer over Internal and a further
clear advantage for SimString over Gazetteer.
Disappointingly, we did not succeed in establish-
ing a clear improvement for more than one of the six
corpora. Although we have not been successful in
142
Figure 6: Learning curve for Super GREC
Figure 7: Learning curve for EPI
proving our initial hypothesis we argue that our re-
sults calls for further study due to several concerns
raised by the results remaining unanswered. It may
be that our notion of distance to lexical resource en-
tries is too naive. A possible future direction would
be to compare the query string to retrieved results us-
ing a method similar to that of Tsuruoka and Tsujii
(2003). This would enable us to retain the advantage
of fast approximate string matching, thus being able
to utilise larger lexical resources than if we were to
calculate sophisticated alignments for each lexical
entry.
Study of the confusion matrices revealed that
some event categories such as negative regulation,
positive regulation and regulation for ID are com-
monly confused by the classifiers. Adding addi-
tional resources or contextual features may alleviate
these problems.
Figure 8: Learning curve for CALBC CII
To conclude, we have found a limited advantage
but failed to establish a clear, systematic benefit
from approximate string matching for semantic class
disambiguation. However, we have demonstrated
that approximate string matching can be used to gen-
erate novel features for classifiers and allow for the
utilisation of large scale lexical resources in new and
potentially interesting ways. It is our hope that by
making our findings, resources and implementation
available we can help the BioNLP community to
reach a deeper understanding of how best to incor-
porate our proposed features for semantic category
disambiguation and related tasks.
Our system and collection of resources are freely
available for research purposes at http://github.com/
ninjin/simsem
Acknowledgements
The authors would like to thank Dietrich Rebholz-
Schuhmann and the CALBC organisers for allowing
us the use of their data. and Jari Bjo?rne for answer-
ing questions regarding the Turku Event Corpus. We
would also like to thank the anonymous reviewers
and Luke McCrohon for their insightful and exten-
sive feedback, which has considerably helped us to
improve this work. Lastly the first author would
like to thank Makoto Miwa and Jun Hatori for their
timely and helpful advice on machine learning meth-
ods.
This work was supported by the Swedish Royal
Academy of Sciences and by Grant-in-Aid for Spe-
cially Promoted Research (MEXT, Japan).
143
References
M. Ashburner, C.A. Ball, J.A. Blake, D. Botstein, H. But-
ler, J.M. Cherry, A.P. Davis, K. Dolinski, S.S. Dwight,
J.T. Eppig, et al 2000. Gene ontology: tool for the
unification of biology. The Gene Ontology Consor-
tium. Nature genetics, 25(1):25.
E. Beisswanger, M. Poprat, and U. Hahn. 2008. Lexical
Properties of OBO Ontology Class Names and Syn-
onyms. In 3rd International Symposium on Semantic
Mining in Biomedicine.
J. Bjo?rne, F. Ginter, S. Pyysalo, J. Tsujii, and
T. Salakoski. 2010. Scaling up biomedical event ex-
traction to the entire PubMed. In Proceedings of the
2010 Workshop on Biomedical Natural Language Pro-
cessing, pages 28?36. Association for Computational
Linguistics.
O Bodenreider. 2004. The unified medical language sys-
tem (umls): integrating biomedical terminology. Nu-
cleic Acids Research, 32:D267?D270.
M.F.M. Chowdhury and A. Lavelli. 2010. Disease Men-
tion Recognition with Specific Features. ACL 2010,
page 83.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
M. Gerner, G. Nenadic, and C.M. Bergman. 2010.
LINNAEUS: A species name identification system for
biomedical literature. BMC bioinformatics, 11(1):85.
K.M. Hettne, R.H. Stierum, M.J. Schuemie, P.J.M. Hen-
driksen, B.J.A. Schijvenaars, E.M. Mulligen, J. Klein-
jans, and J.A. Kors. 2009. A dictionary to identify
small molecules and drugs in free text. Bioinformat-
ics, 25(22):2983.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction
to the bio-entity recognition task at JNLPBA. In Pro-
ceedings of the International Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA), pages 70?75.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Yue Wang, Toshihasi Takagi, and Aki-
nori Yonezawa. 2011. Overview of genia event
task in bionlp shared task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
R. Leaman and G. Gonzalez. 2008. BANNER: an exe-
cutable survey of advances in biomedical named entity
recognition. In Pacific Symposium on Biocomputing,
volume 13, pages 652?663. Citeseer.
D. Maglott, J. Ostell, K.D. Pruitt, and T. Tatusova. 2005.
Entrez Gene: gene-centered information at NCBI. Nu-
cleic Acids Research, 33(suppl 1):D54.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Naoaki Okazaki and Jun?ichi Tsujii. 2010. Simple and
efficient algorithm for approximate dictionary match-
ing. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 851?859, Beijing, China, August.
M.F. Porter. 1993. An algorithm for suffix stripping.
Program: electronic library and information systems,
14(3):130?137.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning, pages 147?155.
Association for Computational Linguistics.
D. Rebholz-Schuhmann, A.J.J. Yepes, E.M. Van Mul-
ligen, N. Kang, J. Kors, D. Milward, P. Corbett,
E. Buyko, E. Beisswanger, and U. Hahn. 2010.
CALBC silver standard corpus. Journal of bioinfor-
matics and computational biology, 8(1):163?179.
Y. Sasaki, Y. Tsuruoka, J. McNaught, and S. Ananiadou.
2008. How to make the most of NE dictionaries in
statistical NER. BMC bioinformatics, 9(Suppl 11):S5.
L. Shi and F. Campagne. 2005. Building a protein name
dictionary from full text: a machine learning term ex-
traction approach. BMC bioinformatics, 6(1):88.
L. Tanabe, N. Xie, L. Thom, W. Matten, and W.J.
Wilbur. 2005. GENETAG: a tagged corpus for
gene/protein named entity recognition. BMC bioinfor-
matics, 6(Suppl 1):S3.
P. Thompson, S.A. Iqbal, J. McNaught, and S. Anani-
adou. 2009. Construction of an annotated corpus
to support biomedical information extraction. BMC
bioinformatics, 10(1):349.
144
M. Torii, Z. Hu, C.H. Wu, and H. Liu. 2009. BioTagger-
GM: a gene/protein name recognition system. Jour-
nal of the American Medical Informatics Association,
16(2):247.
Y. Tsuruoka and J. Tsujii. 2003. Boosting precision and
recall of dictionary-based protein name recognition.
In Proceedings of the ACL 2003 workshop on Natural
language processing in biomedicine-Volume 13, pages
41?48. Association for Computational Linguistics.
Yue Wang, Jin-Dong Kim, Rune Saetre, Sampo Pyysalo,
and Jun?ichi Tsujii. 2009. Investigating heteroge-
neous protein annotations toward cross-corpora uti-
lization. BMC Bioinformatics, 10(1):403.
C.H. Wu, L.S.L. Yeh, H. Huang, L. Arminski, J. Castro-
Alvear, Y. Chen, Z. Hu, P. Kourtesis, R.S. Ledley, B.E.
Suzek, et al 2003. The protein information resource.
Nucleic Acids Research, 31(1):345.
145
Proceedings of BioNLP Shared Task 2011 Workshop, pages 112?120,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP Shared Task 2011: Supporting Resources
Pontus Stenetorp: Goran Topic? Sampo Pyysalo
Tomoko Ohta Jin-Dong Kim; and Jun?ichi Tsujii$
Tsujii Laboratory, Department of Computer Science, University of Tokyo, Tokyo, Japan
:Aizawa Laboratory, Department of Computer Science, University of Tokyo, Tokyo, Japan
; Database Center for Life Science,
Research Organization of Information and Systems, Tokyo, Japan
$Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,goran,smp,okap}@is.s.u-tokyo.ac.jp
jdkim@dbcls.rois.ac.jp
jtsujii@microsoft.com
Abstract
This paper describes the supporting resources
provided for the BioNLP Shared Task 2011.
These resources were constructed with the
goal to alleviate some of the burden of sys-
tem development from the participants and al-
low them to focus on the novel aspects of con-
structing their event extraction systems. With
the availability of these resources we also seek
to enable the evaluation of the applicability of
specific tools and representations towards im-
proving the performance of event extraction
systems. Additionally we supplied evaluation
software and services and constructed a vi-
sualisation tool, stav, which visualises event
extraction results and annotations. These re-
sources helped the participants make sure that
their final submissions and research efforts
were on track during the development stages
and evaluate their progress throughout the du-
ration of the shared task. The visualisation
software was also employed to show the dif-
ferences between the gold annotations and
those of the submitted results, allowing the
participants to better understand the perfor-
mance of their system. The resources, evalu-
ation tools and visualisation tool are provided
freely for research purposes and can be found
at http://sites.google.com/site/bionlpst/
1 Introduction
For the BioNLP?09 Shared Task (Kim et al, 2009),
the first in the ongoing series, the organisers pro-
vided the participants with automatically generated
syntactic analyses for the sentences from the anno-
tated data. For evaluation purposes, tools were made
publicly available as both distributed software and
online services. These resources were well received.
A majority of the participants made use of one or
more of the syntactic analyses, which have remained
available after the shared task ended and have been
employed in at least two independent efforts study-
ing the contribution of different tools and forms of
syntactic representation to the domain of informa-
tion extraction (Miwa et al, 2010; Buyko and Hahn,
2010). The evaluation software for the BioNLP?09
Shared Task has also been widely adopted in subse-
quent studies (Miwa et al, 2010; Poon and Vander-
wende, 2010; Bjo?rne et al, 2010).
The reception and research contribution from pro-
viding these resources encouraged us to continue
providing similar resources for the BioNLP Shared
Task 2011 (Kim et al, 2011a). Along with the
parses we also encouraged the participants and ex-
ternal groups to process the data with any NLP (Nat-
ural Language Processing) tools of their choice and
make the results available to the participants.
We provided continuous verification and evalua-
tion of the participating systems using a suite of in-
house evaluation tools. Lastly, we provided a tool
for visualising the annotated data to enable the par-
ticipants to better grasp the results of their experi-
ments and to help gain a deeper understanding of
the underlying concepts and the annotated data. This
paper presents these supporting resources.
2 Data
This section introduces the data resources provided
by the organisers, participants and external groups
for the shared task.
112
Task Provider Tool
CO University of Utah Reconcile
CO University of Zu?rich UZCRS
CO University of Turku TEES
REL University of Turku TEES
Table 1: Supporting task analyses provided, TEES
is the Turku Event Extraction System and UZCRS
is the University of Zu?rich Coreference Resolution
System
2.1 Supporting task analyses
The shared task included three Supporting Tasks:
Coreference (CO) (Nguyen et al, 2011), Entity re-
lations (REL) (Pyysalo et al, 2011b) and Gene re-
naming (REN) (Jourde et al, 2011). In the shared
task schedule, the supporting tasks were carried out
before the main tasks (Kim et al, 2011b; Pyysalo
et al, 2011a; Ohta et al, 2011; Bossy et al, 2011)
in order to allow participants to make use of analy-
ses from the systems participating in the Supporting
Tasks for their main task event extraction systems.
Error analysis of BioNLP?09 shared task sub-
missions indicated that coreference was the most
frequent feature of events that could not be cor-
rectly extracted by any participating system. Fur-
ther, events involving statements of non-trivial rela-
tions between participating entities were a frequent
cause of extraction errors. Thus, the CO and REL
tasks were explicitly designed to support parts of
the main event extraction tasks where it had been
suggested that they could improve the system per-
formance.
Table 1 shows the supporting task analyses pro-
vided to the participants. For the main tasks, we
are currently aware of one group (Emadzadeh et al,
2011) that made use of the REL task analyses in their
system. However, while a number of systems in-
volved coreference resolution in some form, we are
not aware of any teams using the CO task analyses
specifically, perhaps due in part to the tight sched-
ule and the somewhat limited results of the CO task.
These data will remain available to allow future re-
search into the benefits of these resources for event
extraction.
2.2 Syntactic analyses
For syntactic analyses we provided parses for all
the task data in various formats from a wide range
of parsers (see Table 2). With the exception of
the Pro3Gres1 parser (Schneider et al, 2007), the
parsers were set up and run by the task organisers.
The emphasis was put on availability for research
purposes and variety of parsing models and frame-
works to allow evaluation of their applicability for
different tasks.
In part following up on the results of Miwa et al
(2010) and Buyko and Hahn (2010) regarding the
impact on performance of event extraction systems
depending on the dependency parse representation,
we aimed to provide several dependency parse for-
mats. Stanford Dependencies (SD) and Collapsed
Stanford Dependencies (SDC), as described by de
Marneffe et al (2006), were generated by convert-
ing Penn Treebank (PTB)-style (Marcus et al, 1993)
output using the Stanford CoreNLP Tools2 into the
two dependency formats. We also provided Confer-
ence on Computational Natural Language Learning
style dependency parses (CoNLL-X) (Buchholz and
Marsi, 2006) which were also converted from PTB-
style output, but for this we used the conversion
tool3 from Johansson and Nugues (2007). While
this conversion tool was not designed with convert-
ing the output from statistical parsers in mind (but
rather to convert between treebanks), it has previ-
ously been applied successfully for this task (Miyao
et al, 2008; Miwa et al, 2010).
The text from all documents provided were split
into sentences using the Genia Sentence Splitter4
(S?tre et al, 2007) and then postprocessed using a
set of heuristics to correct frequently occurring er-
rors. The sentences were then tokenised using a to-
kenisation script created by the organisers intended
to replicate the tokenisation of the Genia Tree Bank
(GTB) (Tateisi et al, 2005). This tokenised and
sentence-split data was then used as input for all
parsers.
We used two deep parsers that provide phrase
structure analysis enriched with deep sentence struc-
1https://files.ifi.uzh.ch/cl/gschneid/parser/
2http://nlp.stanford.edu/software/corenlp.shtml
3http://nlp.cs.lth.se/software/treebank converter/
4http://www-tsujii.is.s.u-tokyo.ac.jp/y-matsu/geniass/
113
Name Format(s) Model Availability BioNLP?09
Berkeley PTB, SD, SDC, CoNLL-X News Binary, Source No
C&C CCG, SD Biomedical Binary, Source Yes
Enju HPSG, PTB, SD, SDC, CoNLL-X Biomedical Binary No
GDep CoNLL-X Biomedical Binary, Source Yes
McCCJ PTB, SD, SDC, CoNLL-X Biomedical Source Yes
Pro3Gres Pro3Gres Combination ? No
Stanford PTB, SD, SDC, CoNLL-X Combination Binary, Source Yes
Table 2: Parsers, the formats for which their output was provided and which type of model that was used. The
availability column signifies public availability (without making an explicit request) for research purposes
tures, for example predicate-argument structure for
Head-Driven Phrase Structure Grammar (HPSG).
First we used the C&C Combinatory Categorial
Grammar (CCG) parser5 (C&C) by Clark and Cur-
ran (2004) using the biomedical model described in
Rimell and Clark (2009) which was trained on GTB.
Unlike all other parsers for which we supplied SD
and SDC dependency parses, the C&C output was
converted from its native format using a separate
conversion script provided by the C&C authors. Re-
grettably we were unable to provide CoNLL-X for-
mat output for this parser due to the lack of PTB-
style output. The other deep parser used was the
HPSG parser Enju6 by Miyao and Tsujii (2008), also
trained on GTB.
We also applied the frequently adopted Stanford
Parser7 (Klein and Manning, 2003) using a mixed
model which includes data from the biomedical do-
main, and the Charniak Johnson re-ranking parser8
(Charniak and Johnson, 2005) using the self-trained
biomedical model from McClosky (2009) (McCCJ).
For the BioNLP?09 shared task it was observed
that the Bikel parser9 (Bikel, 2004), which used a
non-biomedical model and can be argued that it uses
the somewhat dated Collins? parsing model (Collins,
1996), did not contribute towards event extraction
performance as strongly as other parses supplied for
the same data. We therefore wanted to supply a
parser that can compete with the ones above in a do-
main which is different from the biomedical domain
to see whether conclusions could be drawn as to the
5http://svn.ask.it.usyd.edu.au/trac/candc/
6http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
7http://nlp.stanford.edu/software/lex-parser.shtml
8ftp://ftp.cs.brown.edu/pub/nlparser/
9http://www.cis.upenn.edu/dbikel/software.html
importance of using a biomedical model. For this
we used the Berkeley parser10 (Petrov et al, 2006).
Lastly we used a native dependency parser, the GE-
NIA Dependency parser (GDep) by Sagae and Tsujii
(2007).
At least one team (Choudhury et al, 2011) per-
formed experiments on some of the provided lexi-
cal analyses and among the 14 submissions for the
EPI and ID tasks, 13 submissions utilised tools for
which resources were provided by the organisers of
the shared task. We intend to follow up on whether
or not the majority of the teams ran the tools them-
selves or used the provided analyses.
2.3 Other analyses
The call for analyses was open to all interested par-
ties and all forms of analysis. In addition to the Sup-
porting Task analyses (CO and REL) and syntactic
analyses provided by various groups, the University
of Antwerp CLiPS center (Morante et al, 2010) re-
sponded to the call providing negation/speculation
analyses in the BioScope corpus format (Szarvas et
al., 2008).
Although this resource was not utilised by the par-
ticipants for the main task, possibly due to a lack of
time, it is our hope that by keeping the data available
it can lead to further development of the participat-
ing systems and analysis of BioScope and BioNLP
ST-style hedging annotations.
3 Tools
This section presents the tools produced by the or-
ganisers for the purpose of the shared task.
10http://code.google.com/p/berkeleyparser/
114
1 10411007-E1 Regulation <Exp>regulate[26-34] <Theme>TNF-alpha[79-88] ?
?<Excerpt>[regulate] an enhancer activity in the third intron of [TNF-alpha]
2 10411007-E2 Gene_expression <Exp>activity[282-290] <Theme>TNF-alpha[252-261] ?
?<Excerpt>[TNF-alpha] gene displayed weak [activity]
3 10411007-E3 +Regulation <Exp>when[291-295] <Theme>E2 <Excerpt>[when]
Figure 1: Text output from the BioNLP?09 Shared Event Viewer with line numbering and newline markings
Figure 2: An illustration of collective (sentence 1)
and distributive reading (sentence 2). ?Theme? is
abbreviated as ?Th? and ?Protein? as ?Pro? when
there is a lack of space
3.1 Visualisation
The annotation data in the format specified by the
shared task is not intended to be human-readable ?
yet researchers need to be able to visualise the data
in order to understand the results of their experi-
ments. However, there is a scarcity of tools that can
be used for this purpose. There are three available
for event annotations in the BioNLP ST format that
we are aware of.
One is the BioNLP?09 Shared Task Event
Viewer11, a simple text-based annotation viewer: it
aggregates data from the annotations, and outputs it
in a format (Figure 1) that is meant to be further pro-
cessed by a utility such as grep.
Another is What?s Wrong with My NLP12, which
visualises relation annotations (see Figure 3a) ? but
is unable to display some of the information con-
tained in the Shared Task data. Notably, the distribu-
tive and collective readings of an event are not dis-
tinguished (Figure 2). It also displays all annotations
on a single line, which makes reading and analysing
longer sentences, let alne whole documents, some-
what difficult.
The last one is U-Compare13 (Kano et al, 2009),
11http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/
downloads.shtml
12http://code.google.com/p/whatswrong/
13http://u-compare.org/bionlp2009.html
which is a comprehensive suite of tools designed for
managing NLP workflows, integrating many avail-
able services. However, the annotation visualisation
component, illustrated in Figure 3b, is not optimised
for displaying complex event structures. Each anno-
tation is marked by underlining its text segment us-
ing a different colour per annotation type, and a role
in an event is represented by a similarly coloured arc
between the related underlined text segments. The
implementation leaves some things to be desired:
there is no detailed information added in the display
unless the user explicitly requests it, and then it is
displayed in a separate panel, away from the text it
annotates. The text spacing makes no allowance for
the annotations, with opaque lines crossing over it,
with the effect of making both the annotations and
the text hard to read if the annotations are above a
certain degree of complexity.
As a result of the difficulties of these existing
tools, in order to extract a piece of annotated text
and rework it into a graph that could be embedded
into a publication, users usually read off the annota-
tions, then create a graph from scratch using vector
drawing or image editing software.
To address these issues, we created a visualisa-
tion tool named stav (stav Text Annotation Visual-
izer), that can read the data formatted according to
the Shared Task specification and aims to present it
to the user in a form that can be grasped at a glance.
Events and entities are annotated immediately above
the text, and the roles within an event by labelled
arcs between them (Figure 3c). In a very complex
graph, users can highlight the object or association
of interest to follow it even more easily. Special fea-
tures of annotations, such as negation or speculation,
are shown by unique visual cues, and more in-depth,
technical information that is usually not required can
be requested by floating the mouse cursor over the
annotation (as seen in Figure 5).
We took care to minimise arc crossovers, and to
115
(a) Visualisation using What?s Wrong with My NLP
(b) Visualisation using U-Compare
(c) Visualisation using stav
Figure 3: Different visualisations of complex textual annotations of Dickensheets et al (1999)
116
Figure 4: A screenshot of the stav file-browser
keep them away from the text itself, in order to main-
tain text readability. The text is spaced to accommo-
date the annotations between the rows. While this
does end up using more screen real-estate, it keeps
the text legible, and annotations adjacent to the text.
The text is broken up into lines, and each sentence
is also forced into a new line, and given a numer-
ical identifier. The effect of this is that the text is
laid out vertically, like an article would be, but with
large spacing to accomodate the annotations. The
arcs are similarly continued on successive lines, and
can easily be traced ? even in case of them spanning
multiple lines, by the use of mouseover highlight-
ing. To preserve the distributionality information of
the annotation, any event annotations are duplicated
for each event, as demonstrated in the example in
Figure 2.
stav is not limited to the Shared Task datasets with
appropriate configuration settings, it could also vi-
sualise other kinds of relational annotations such as:
frame structures (Fillmore, 1976) and dependency
parses (de Marneffe et al, 2006).
To achieve our objectives above, we use the Dy-
namic Scalable Vector Graphics (SVG) functional-
ity (i.e. SVG manipulated by JavaScript) provided
by most modern browsers to render the WYSIWYG
(What You See Is What You Get) representation of
the annotated document. An added benefit from
this technique is that the installation process, if any,
is very simple: although not all browsers are cur-
rently supported, the two that we specifically tested
against are Safari14 and Google Chrome15; the for-
mer comes preinstalled with the Mac OS X oper-
ating system, while the latter can be installed even
by relatively non-technical users. The design is kept
modular using a dispatcher pattern, in order to al-
low the inclusion of the visualiser tool into other
JavaScript-based projects. The client-server archi-
tecture also allows centralisation of data, so that ev-
ery user can inspect an uploaded dataset without the
hassle of downloading and importing into a desktop
application, simply by opening an URL which can
uniquely identify a document, or even a single an-
notation. A screenshot of the stav file browser can
be seen in Figure 4.
3.2 Evaluation Tools
The tasks of BioNLP-ST 2011 exhibit very high
complexity, including multiple non-trivial subprob-
lems that are partially, but not entirely, independent
of each other. With such tasks, the evaluation of par-
ticipating systems itself becomes a major challenge.
Clearly defined evaluation criteria and their precise
implementation is critical not only for the compari-
son of submissions, but also to help participants fol-
low the status of their development and to identify
the specific strengths and weaknesses of their ap-
proach.
A further challenge arising from the complexity
of the tasks is the need to process the relatively in-
tricate format in which annotations are represented,
which in turn carries a risk of errors in submissions.
To reduce the risk of submissions being rejected or
the evaluation showing poor results due to format-
ting errors, tools for checking the validity of the file
format and annotation semantics are indispensable.
For these reasons, we placed emphasis in the or-
ganisation of the BioNLP-ST?11 on making tools for
format checking, validation and evaluation available
to the participants already during the early stages of
system development. The tools were made avail-
able in two ways: as downloads, and as online ser-
vices. With downloaded tools, participants can per-
form format checking and evaluation at any time
without online access, allowing more efficient op-
timisation processes. Each task in BioNLP-ST also
14http://www.apple.com/safari
15http://www.google.com/chrome
117
Figure 5: An example of a false negative illustrated by the evaluation tools in co-ordination with stav
maintained an online evaluation tool for the develop-
ment set during the development period. The online
evaluation is intended to provide an identical inter-
face and criteria for submitted data as the final on-
line submission system, allowing participants to be
better prepared for the final submission. With on-
line evaluation, the organisers could also monitor
submissions to ensure that there were no problems
in, for example, the evaluation software implemen-
tations.
The system logs of online evaluation systems
show that the majority of the participants submit-
ted at least one package with formatting errors, con-
firming the importance of tools for format checking.
Further, most of the participants made use of the on-
line development set evaluation at least once before
their final submission.
To enhance the evaluation tools we drew upon the
stav visualiser to provide a view of the submitted re-
sults. This was done by comparing the submitted
results and the gold data to produce a visualisation
where errors are highlighted, as illustrated in Fig-
ure 5. This experimental feature was available for
the EPI and ID tasks and we believe that by doing so
it enables participants to better understand the per-
formance of their system and work on remedies for
current shortcomings.
4 Discussion and Conclusions
Among the teams participating in the EPI and ID
tasks, a great majority utilised tools for which re-
sources were made available by the organisers. We
hope that the continued availability of the parses will
encourage further investigation into the applicability
of these and similar tools and representations.
As for the analysis of the supporting analyses pro-
vided by external groups and the participants, we are
so far aware of only limited use of these resources
among the participants, but the resources will re-
main available and we are looking forward to see
future work using them.
To enable reproducibility of our resources, we
provide a publicly accessible repository containing
the automated procedure and our processing scripts
used to produce the released data. This repository
also contains detailed instructions on the options and
versions used for each parser and, if the software li-
cense permits it, includes the source code or binary
that was used to produce the processed data. For the
cases where the license restricts redistribution, in-
structions and links are provided on how to obtain
the same version that was used. We propose that us-
ing a multitude of parses and formats can benefit not
just the task of event extraction but other NLP tasks
as well.
We have also made our evaluation tools and visu-
alisation tool stav available along with instructions
on how to run it and use it in coordination with the
shared task resources. The responses from the par-
ticipants in relation to the visualisation tool were
very positive, and we see this as encouragement to
advance the application of visualisation as a way to
better reach a wider understanding and unification
of the concept of events for biomedical event extrac-
tion.
All of the resources described in this paper are
available at http://sites.google.com/site/bionlpst/.
118
Acknowledgements
We would like to thank Jari Bjo?rne of the Uni-
versity of Turku BioNLP group; Gerold Schneider,
Fabio Rinaldi, Simon Clematide and Don Tuggener
of the Univerity of Zurich Computational Linguis-
tics group; Roser Morante of University of Antwerp
CLiPS center; and Youngjun Kim of the Univer-
sity of Utah Natural Language Processing Research
Group for their generosity with their time and exper-
tise in providing us with supporting analyses.
This work was supported by Grant-in-Aid for
Specially Promoted Research (MEXT, Japan) and
the Royal Swedish Academy of Sciences.
References
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
J. Bjo?rne, F. Ginter, S. Pyysalo, J. Tsujii, and
T. Salakoski. 2010. Complex event extraction at
PubMed scale. Bioinformatics, 26(12):i382.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, pages 149?164. Association
for Computational Linguistics.
E. Buyko and U. Hahn. 2010. Evaluating the impact
of alternative dependency graph encodings on solv-
ing event extraction tasks. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 982?992. Association for
Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Pallavi Choudhury, Michael Gamon, Chris Quirk, and
Lucy Vanderwende. 2011. MSR-NLP entry in
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
S. Clark and J.R. Curran. 2004. Parsing the WSJ us-
ing CCG and log-linear models. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 103. Association for Com-
putational Linguistics.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 184?191, Santa
Cruz, California, USA, June. Association for Compu-
tational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
H.L. Dickensheets, C. Venkataraman, U. Schindler, and
R.P. Donnelly. 1999. Interferons inhibit activation of
STAT6 by interleukin 4 in human monocytes by in-
ducing SOCS-1 gene expression. Proceedings of the
National Academy of Sciences of the United States of
America, 96(19):10800.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. A generalizable and efficient ma-
chine learning approach for biological event extraction
from text. In Proceedings of the BioNLP 2011 Work-
shop Companion Volume for Shared Task, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Charles J. Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy of
Sciences, 280(1):20?32.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task 2011
- Bacteria Gene Interactions and Renaming. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Yoshinobu Kano, William Baumgartner, Luke McCro-
hon, Sophia Ananiadou, Kevin Cohen, Larry Hunter,
and Jun?ichi Tsujii. 2009. U-Compare: share and
compare text mining tools with UIMA. Bioinformat-
ics, 25(15):1997?1998, May.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
119
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
D. Klein and C.D. Manning. 2003. Fast exact infer-
ence with a factored model for natural language pars-
ing. Advances in neural information processing sys-
tems, pages 3?10.
M.P Marcus, B. Santorini, and M.A Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Tree Bank. Computational Linguistics,
pages 313?318.
D. McClosky. 2009. Any Domain Parsing: Automatic
Domain Adaptation for Natural Language Parsing.
Ph.D. thesis, Ph. D. thesis, Department of Computer
Science, Brown University.
M. Miwa, S. Pyysalo, T. Hara, and J. Tsujii. 2010. Eval-
uating Dependency Representation for Event Extrac-
tion. In In the 23rd International Conference on Com-
putational Linguistics (COLING 2010), pages 779?
787.
Y. Miyao and J. Tsujii. 2008. Feature forest models for
probabilistic HPSG parsing. Computational Linguis-
tics, 34(1):35?80.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
Proceedings of ACL-08: HLT, pages 46?54, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
R. Morante, V. Van Asch, and W. Daelemans. 2010.
Memory-based resolution of in-sentence scopes of
hedge cues. CoNLL-2010: Shared Task, page 40.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Overview of the Protein Coreference task in BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 433?440. Association for Computa-
tional Linguistics.
H. Poon and L. Vanderwende. 2010. Joint inference
for knowledge extraction from biomedical literature.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 813?
821. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the Entity Relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Laura Rimell and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, 42(5):852 ? 865.
Biomedical Natural Language Processing.
R. S?tre, K. Yoshida, A. Yakushiji, Y. Miyao, Y. Matsub-
yashi, and T. Ohta. 2007. AKANE system: protein-
protein interaction pairs in BioCreAtIvE2 challenge,
PPI-IPS subtask. In Proceedings of the Second
BioCreative Challenge Workshop, pages 209?212.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL 2007 Shared
Task.
G. Schneider, M. Hess, and P. Merlo. 2007. Hybrid
long-distance functional dependency parsing. Unpub-
lished PhD thesis, Institute of Computational Linguis-
tics, University of Zurich.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope corpus: annotation for negation, uncer-
tainty and their scope in biomedical texts. In Proceed-
ings of the Workshop on Current Trends in Biomedical
Natural Language Processing, pages 38?45. Associa-
tion for Computational Linguistics.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005.
Syntax Annotation for the GENIA corpus. In Proceed-
ings of the IJCNLP, pages 222?227.
120
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 100?108,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
New Resources and Perspectives for Biomedical Event Extraction
Sampo Pyysalo1, Pontus Stenetorp2, Tomoko Ohta1, Jin-Dong Kim3 and Sophia Ananiadou1
1National Centre for Text Mining and University of Manchester,
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester, UK
2Tokyo University, 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan
3Database Center for Life Science, 2-11-16 Yayoi, Bunkyo-ku, Tokyo, Japan
Abstract
Event extraction is a major focus of re-
cent work in biomedical information extrac-
tion. Despite substantial advances, many chal-
lenges still remain for reliable automatic ex-
traction of events from text. We introduce a
new biomedical event extraction resource con-
sisting of analyses automatically created by
systems participating in the recent BioNLP
Shared Task (ST) 2011. In providing for the
first time the outputs of a broad set of state-of-
the-art event extraction systems, this resource
opens many new opportunities for studying
aspects of event extraction, from the identifi-
cation of common errors to the study of ef-
fective approaches to combining the strengths
of systems. We demonstrate these opportuni-
ties through a multi-system analysis on three
BioNLP ST 2011 main tasks, focusing on
events that none of the systems can success-
fully extract. We further argue for new per-
spectives to the performance evaluation of do-
main event extraction systems, considering a
document-level, ?off-the-page? representation
and evaluation to complement the mention-
level evaluations pursued in most recent work.
1 Introduction
Biomedical information extraction efforts are in-
creasingly focusing on event extraction using struc-
tured representations that allow associations of arbi-
trary numbers of participants in specific roles (e.g.
Theme, Cause) to be captured (Ananiadou et al,
2010). Domain event extraction has been advanced
in particular by the BioNLP Shared Task (ST) events
(Kim et al, 2011a; Kim et al, 2011b), which have
introduced common task settings, datasets, and eval-
uation criteria for event extraction. Participants in
these shared tasks have introduced dozens of sys-
tems for event extraction, and the resulting methods
have been applied to automatically analyse the entire
available domain literature (Bjo?rne et al, 2010) and
applied in support of applications such as semantic
literature search (Ohta et al, 2010; Van Landeghem
et al, 2011b) and pathway curation support (Kemper
et al, 2010).
It is possible to assess recent advances in event ex-
traction through results for a task considered both in
the BioNLP ST 2009 and 2011. By the primary eval-
uation criteria, the highest performance achieved in
the 2009 task was 51.95% F-score, and a 57.46% F-
score was reached in the comparable 2011 task (Kim
et al, 2011b). These results demonstrate significant
advances in event extraction methods, but also indi-
cate that the task continues to hold substantial chal-
lenges. This has led to a call from task participants
for further analysis of the data and results, accompa-
nied by a proposal to release analyses from individ-
ual systems to facilitate such analysis (Quirk et al,
2011).
In this study, we explore new perspectives into the
analyses and performance of event extraction meth-
ods. We build primarily on a new resource compiled
with the support of the majority of groups participat-
ing in the BioNLP ST 2011, consisting of analyses
from systems for the three main tasks sharing the
text-bound event representation. We demonstrate
the use of this resource through an evaluation fo-
cusing on events that cannot be extracted even by
the union of combined systems, identifying partic-
ular remaining challenges for event extraction. We
further propose and evaluate an alternate, document-
level perspective to event extraction, demonstrat-
ing that when only unique events are considered for
100
Figure 1: Example event annotations. The ?crossed-out? event type identifies an event marked as negated. Event
illustrations created using the STAV visualization tool (Stenetorp et al, 2011).
each document, the measured performance and even
ranking of systems participating in the shared task is
notably altered.
2 Background
In this work, we focus on the definition of the
event extraction task first introduced in the BioNLP
Shared Task 2009.1 The task targets the extrac-
tion of events, represented as n-ary associations of
participants (entities or other events), each marked
as playing a specific role such as Theme or Cause
in the event. Each event is assigned a type such
as BINDING or PHOSPHORYLATION from a fixed,
task-specific set. Events are further typically associ-
ated with specific trigger expressions that state their
occurrence in text. As physical entities such as pro-
teins are also identified in the setting with specific
spans referring to the real-world entities in text, the
overall task is ?text-bound? in the sense of requiring
not only the extraction of targeted statements from
text, but also the identification of specific regions of
text expressing each piece of extracted information.
Events can further be marked with modifiers iden-
tifying additional features such as being explicitly
negated or stated in a speculative context. Figure 1
shows an illustration of event annotations.
This BioNLP ST 2009 formulation of the event
extraction task was followed also in three 2011 main
tasks: the GE (Kim et al, 2011c), ID (Pyysalo et al,
2011a) and EPI (Ohta et al, 2011) tasks. A vari-
ant of this representation that omits event triggers
was applied in the BioNLP ST 2011 bacteria track
(Bossy et al, 2011), and simpler, binary relation-
type representations were applied in three support-
ing tasks (Nguyen et al, 2011; Pyysalo et al, 2011b;
Jourde et al, 2011). Due to the challenges of con-
sistent evaluation and processing for tasks involv-
1While far from the only formulation proposed in the litera-
ture, this specific task setting is the most frequently considered
and arguably a de facto standard for domain event extraction.
ing different representations, we focus in this work
specifically on the three 2011 main tasks sharing a
uniform representation: GE, ID and EPI.
3 New Resources for Event Extraction
In this section, we present the new collection of au-
tomatically created event analyses and demonstrate
one use of the data through an evaluation of events
that no system could successfully extract.
3.1 Data Compilation
Following the BioNLP ST 2011, the MSR-NLP
group called for the release of outputs from various
participating systems (Quirk et al, 2011) and made
analyses of their system available.2 Despite the ob-
vious benefits of the availability of these resources,
we are not aware of other groups following this ex-
ample prior to the time of this publication.
To create the combined resource, we approached
each group that participated in the three targeted
BioNLP ST 2011 main tasks to ask for their support
to the creation of a dataset including analyses from
their event extraction systems. This suggestion met
with the support of all but a few groups that were
approached.3 The groups providing analyses from
their systems into this merged resource are summa-
rized in Table 1, with references to descriptions of
the systems used to create the included analyses. We
compiled for each participant and each task both the
final test set submission and a comparable submis-
sion for the separate development set.
As the gold annotations for the test set are only
available for evaluation through an online interface
(in order to avoid overfitting and assure the compa-
rability of results), it is important to provide also de-
velopment set analyses to permit direct comparison
2http://research.microsoft.com/bionlp/
3We have yet to hear back from a few groups, but none has
yet explicitly denied the release of their data. Should any re-
maining group accept the release of their data, we will release a
new, extended version of the resource.
101
Task System
Team GE EPI ID BB BI CO REL REN description
UTurku 1 1 1 1 1 1 1 1 Bjo?rne and Salakoski (2011)
ConcordU 1 1 1 1 1 1 Kilicoglu and Bergler (2011)
UMass 1 1 1 Riedel and McCallum (2011)
Stanford 1 1 1 McClosky et al (2011)
FAUST 1 1 1 Riedel et al (2011)
MSR-NLP 1 1 Quirk et al (2011)
CCP-BTMG 1 1 Liu et al (2011)
BMI@ASU 1 Emadzadeh et al (2011)
TM-SCS 1 Bui and Sloot (2011)
UWMadison 1 Vlachos and Craven (2011)
HCMUS 1 1 Le Minh et al (2011)
PredX 1 -
VIBGhent 1 Van Landeghem et al (2011a)
Table 1: BioNLP ST 2011 participants contributing to the combined resource.
Events
Task Gold FN Recall
GE (task 1) 3250 1006 69.05%
EPI (CORE task) 601 129 78.54%
ID (CORE task) 691 183 73.52%
Table 2: Recall for the union of analyses from systems
included in the combined dataset.
against gold annotations. The inclusion of both de-
velopment and test set annotations also allows e.g.
the study of system combination approaches where
the combination parameters are estimated on devel-
opment data for final testing on the test set (Kim et
al., 2011a).
3.2 Evaluation
We demonstrate the use of the newly compiled
dataset through a manual evaluation of GE, EPI and
ID main task development set gold standard events
that are not extracted by any of the systems for
which analyses were available.4 We perform eval-
uation on the GE subtask 1 and the EPI and ID
task CORE subtasks, as all participating systems ad-
dressed the extraction targets of these subtasks.
We first evaluated each of the analyses against the
development set of the respective task using the of-
ficial shared task evaluation software, using options
for the evaluation tools to list the sets of true posi-
tive (TP), false positive (FP) and false negative (FN)
4The final collection includes analyses from the systems of
two groups that agreed to the release of their data after the com-
pletion of this analysis, but we expect the results to largely hold
also for the final collection.
events. We then selected for each of the three tasks
the set of events that were included in the FN list
for all systems. This gives the results for the re-
call of the union of all systems shown in Table 2.
The recall of the system union is approximately 30%
points higher than that of any individual GE system
(Kim et al, 2011c) and 25% points higher for EPI
and ID (Ohta et al, 2011; Pyysalo et al, 2011a),
suggesting potential remaining benefits from system
combination. Nevertheless, a substantial fraction of
the total set of gold events remains inaccessible also
to this system union.
We then selected a random set of 100 events from
each of the three sets of events that were not re-
covered by any system (i.e. 300 events in total) and
performed a manual evaluation to identify frequent
properties of these events that could contribute to
extraction failures. In brief, we first performed a
brief manual evaluation to identify common charac-
teristics of these events, and then evaluated the 300
events individually to identify the set of these char-
acteristics that apply to each event.
The results of the evaluation for common cases
are shown in Table 3. We find that the most fre-
quent property of the unrecoverable events is that
they involve implicit arguments (Gerber and Chai,
2010), a difficult challenge that has not been ex-
tensively considered in domain event extraction. A
closely related issue are events involving arguments
in a sentence different from that containing the trig-
ger (?cross-sentence?), connected either implicitly
or through explicit coreference (?coreference?). Al-
102
Type GE EPI ID Total
Implicit argument 18 33 15 66
Cross-sentence 14 40 4 58
Weak trigger 28 14 11 53
Coreference 12 20 18 50
Static Relation 6 28 6 40
Error in gold 17 4 9 30
Ambiguous type 2 9 11 22
Shared trigger 2 12 1 15
Table 3: Manual evaluation results for features of events
that could not be recovered by any system.
though coreference was considered as as separate
task in BioNLP ST 2011 (Nguyen et al, 2011), it is
clear that it involves many remaining challenges for
event extraction systems. Similarly, events where
explicit arguments are connected to other arguments
through ?static? relations such as part-of (e.g. ?A
binds the X domain of B?) represent a known chal-
lenge (Pyysalo et al, 2011b). These results sug-
gest that further advances in event extraction perfor-
mance could be gained by the integration of systems
for the analysis of coreference and static relations,
approaches for which some success has already been
demonstrated in recent efforts (Van Landeghem et
al., 2010; Yoshikawa et al, 2011; Miwa et al, 2012).
?Weak? trigger expressions that must be inter-
preted in context to determine whether they express
an event, as well as a related class of events whose
type must be disambiguated with reference to con-
text (?ambiguous type?) are comparatively frequent
in the three tasks, while EPI in particular involves
many cases where a trigger is shared between mul-
tiple events ? an issue for approaches that assume
each token can be assigned at most a single class.
Finally, we noted a number of cases that we judged
to be errors in the gold annotation; the number
is broadly in line with the reported inter-annotator
agreement for the data (see e.g. Ohta et al (2011)).
While there is an unavoidable subjective com-
ponent to evaluations such as this, we note that a
similar evaluation performed following the BioNLP
Shared Task 2009 using test set data reached broadly
comparable results (Kim et al, 2011a). The newly
compiled dataset represents the first opportunity for
those without direct access to the test set data and
submissions to directly assess the task results, as
demonstrated here. We hope that this resource will
encourage further exploration of both the data, the
system analyses and remaining challenges in event
extraction.
4 New Perspectives to Event Extraction
As discussed in Section 2, the BioNLP ST event ex-
traction task is ?text-bound?: each entity and event
annotation is associated with a specific span of text.
Contrasted to the alternative approach where anno-
tations are document-level only, this approach has
a number of important benefits, such as allowing
machine learning methods for event extraction to
be directly trained on fully and specifically anno-
tated data without the need to apply frequently error-
prone heuristics (Mintz et al, 2009) or develop ma-
chine learning methods addressing the mapping be-
tween text expressions and document-level annota-
tions (Riedel et al, 2010). Many of the most suc-
cessful event extraction approaches involve direct
training of machine learning methods using the text-
bound annotations (Riedel and McCallum, 2011;
Bjo?rne and Salakoski, 2011; McClosky et al, 2011).
However, while the availability of text-bound anno-
tations in data provided to task participants is clearly
a benefit, there are drawbacks to the choice of ex-
clusive focus on text-bound annotations in system
output, including issues relating to evaluation and
the applicability of methods to the task. In the fol-
lowing section, we discuss some of these issues and
propose alternatives to representation and evaluation
addressing them.
4.1 Evaluation
The evaluation of the BioNLP ST is instance-based
and text-bound: each event in gold annotation and
each event extracted by a system is considered in-
dependently, separating different mentions of the
?same? real-world event. This is the most detailed
(sensitive) evaluation setting permitted by the data,
and from a technical perspective a reasonable choice
for ranking systems performing the task.
However, from a practical perspective, this eval-
uation setting arguably places excessively strict de-
mands on systems, and may result in poor correla-
tion between measured performance and the practi-
cal value of systems. Our motivating observations
are that specific real-world events tend to be men-
103
tioned multiple times in a single publication ? espe-
cially the events that are of particular importance in
the study ? and that there are few practical applica-
tions for which it is necessary to find each such re-
peated mention. For example, in literature search for
e.g. pathway or database curation support, one typi-
cal information need is to identify biomolecular re-
actions involving a specific protein. Event extraction
can support such needs either by summarizing all
events involving the protein that could be extracted
from the literature (Van Landeghem et al, 2011b), or
by retrieving documents (perhaps showing relevant
text snippets) containing such events (Ohta et al,
2010). For the former to meet the information need,
it may be sufficient that each different event is ex-
tracted once from the entire literature; for the latter,
once from each relevant document. For uses such
as these, there is no obvious need for, or, indeed,
no very obvious benefit from the ability of extrac-
tion systems to separately enumerate every mention
of every event in every publication. It is easy to en-
vision other practical use cases where instance-level
extraction performance is at best secondary and, we
argue, difficult to identify ones where it is of critical
importance.
For applications such as these, the important
question is the reliability of the system at identify-
ing events either on the level of documents or on the
level of (a relevant subset of) the literature, rather
than on the level of individual mentions. For a more
complete and realistic picture of the practical value
of event extraction methods, measures other than
instance-level should thus also be considered.
4.2 Task setting
While applications can benefit from the ability of
IE systems to identify a specific span of text sup-
porting extracted information,5 the requirement of
the BioNLP ST setting that the output of event ex-
traction systems must identify specific text spans for
each entity and event makes it complex or impossi-
ble to address the task using a number of IE methods
that might otherwise represent feasible approaches
to event extraction.
5For example, for curation support tasks, this allows the hu-
man curator to easily check the correctness of extracted infor-
mation and helps to select ?evidence sentences?, as included in
many databases.
For example, Patwardhan and Riloff (2007) and
Chambers and Jurafsky (2011) consider an IE ap-
proach where the extraction targets are MUC-4 style
document-level templates (Sundheim, 1991), the
former a supervised system and the latter fully un-
supervised. These methods and many like them for
tasks such as ACE (Doddington et al, 2004) work
on the document level, and can thus not be readily
applied or evaluated against the existing annotations
for the BioNLP shared tasks. Enabling the appli-
cation of such approaches to the BioNLP ST could
bring valuable new perspectives to event extraction.
4.3 Alternative evaluation
We propose a new mode of evaluation that otherwise
follows the primary BioNLP ST evaluation criteria,
but incorporates the following two exceptions:
1. remove the requirement to match trigger spans
2. only require entity texts, not spans, to match
The first alternative criterion has also been previ-
ously considered in the GE task evaluation (Kim et
al., 2011c); the latter has, to the best of our knowl-
edge, not been previously considered in domain
event extraction. We additionally propose to con-
sider only the minimal set of events that are unique
on the document level (under the evaluation criteria),
thus eliminating effects from repeated mentions of a
single event on evaluated performance. We created
tools implementing this mode of evaluation with ref-
erence to the BioNLP ST 2011 evaluation tools.
While this type of evaluation has, to the best of
our knowledge, not been previously applied specif-
ically in biomedical event extraction, it is closely
related (though not identical) to evaluation criteria
applied in MUC, ACE, and the in-domain PPI re-
lation extraction tasks in BioCreative (Krallinger et
al., 2008).
4.4 Alternative representation
A true conversion to a document-level, ?off the
page? representation would require manual anno-
tation efforts to identify the real-world entities and
events referred to in text (Doddington et al, 2004).
However, it is possible to reasonably approximate
such a representation through an automatic heuristic
conversion.
104
BioNLP Shared Task
T1 Protein 0 5 CIITA
T2 Protein 21 28 TAFII32
T3 Binding 6 15 interacts
E1 Binding:T3 Theme:T1 Theme2:T2
T4 Protein 54 61 TAFII32
T5 Protein 66 71 CIITA
T6 Binding 33 45 interactions
E2 Binding:T6 Theme:T4 Theme2:T5
Document level
T1 Protein CIITA
T2 Protein TAFII32 
E1 Binding Theme:T1 Theme2:T2
CIITA interacts with TAFII32 ... interactions between TAFII32 and CIITA are
Pro Binding Protein Binding Protein ProTh Th2 Theme
Theme2
...
Figure 2: Illustration of BioNLP Shared Task annotation format and the proposed document-level (?off-the-page?)
format.
We first introduce a non-textbound annotation for-
mat that normalizes over differences in e.g. argu-
ment order and eliminates duplicate events. The for-
mat largely follows that of the shared task but re-
moves any dependencies and references to text off-
sets (see Figure 2). The conversion process into this
representation involves a number of steps. First, we
merge duplicate pairs of surface strings and types,
as different mentions of the same entity in different
parts of the text are no longer distinguishable in the
representation. In the original format, equivalence
relations (Kim et al, 2011a) are annotated only for
specific mentions. When ?raising? the annotations
to the document level, equivalence relations are rein-
terpreted to cover the full document by extending
the equivalence to all mentions that share the surface
form and type with members of existing equivalence
classes. Finally, we implemented an event equiv-
alence comparison to remove duplicate annotations
from each document. The result of the conversion
to this alternate representation is thus an ?off-the-
page? summary of the unique set of events in the
document.
This data can then be used for training and com-
parison of methods analogously to the original anno-
tations, but without the requirement that all analyses
include text-bound annotations.
4.5 Experimental Results
We next present an evaluation using the alternative
document-level event representation and evaluation,
comparing its results to those for the primary shared
task evaluation criteria. As comparatively few of the
Primary criteria New criteria
Group Rec. Prec. F Rec. Prec. F
FAUST 49.41 64.75 56.04 53.10 67.56 59.46
UMass 48.49 64.08 55.20 52.55 66.57 58.74
UTurku 49.56 57.65 53.30 54.23 60.11 57.02
MSR-NLP 48.64 54.71 51.50 53.55 58.24 55.80
ConcordU 43.55 59.58 50.32 47.42 60.85 53.30
UWMadison 42.56 61.21 50.21 46.09 62.50 53.06
Stanford 42.36 61.08 50.03 46.48 63.22 53.57
BMI@ASU 36.91 56.63 44.69 41.15 61.44 49.29
CCP-BTMG 31.57 58.99 41.13 34.82 66.89 45.80
TM-SCS 32.73 45.84 38.19 38.02 50.87 43.51
HCMUS 10.12 27.17 14.75 14.50 40.05 21.29
Table 4: Comparison of BioNLP ST 2011 GE task 1 re-
sults.
shared task participants attempted subtasks 2 and 3
for GE or the FULL task setting for EPI and ID, we
consider only GE subtask 1 and the EPI and ID task
CORE extraction targets in these experiments. We
refer to the task overviews for the details of the sub-
tasks and the primary evaluation criteria (Kim et al,
2011c; Pyysalo et al, 2011a; Ohta et al, 2011).
Tables 4, 5 and 6 present the results for the
GE, EPI and ID tasks, respectively. For GE, we
see consistently higher F-scores for the new crite-
ria, in most cases reflecting primarily an increase
in recall, but also involving increases in precision.
The F-score differences range between 3-4% for
most high-ranking systems, with more substantial
increases for lower-ranking systems. Notable in-
creases in precision are seen for some systems (e.g.
HCMUS), indicating that the systems comparatively
frequently extract correct information, but associ-
ated with the wrong spans of text.
105
Primary criteria New criteria
Group Rec. Prec. F Rec. Prec. F
UTurku 68.51 69.20 68.86 74.20 69.14 71.58
FAUST 59.88 80.25 68.59 67.04 76.82 71.60
MSR-NLP 55.70 77.60 64.85 59.24 77.66 67.21
UMass 57.04 73.30 64.15 65.76 69.65 67.65
Stanford 56.87 70.22 62.84 62.74 67.12 64.86
CCP-BTMG 45.06 63.37 52.67 54.62 63.17 58.58
ConcordU 40.28 76.71 52.83 48.41 76.57 59.32
Table 5: Comparison of BioNLP ST 2011 EPI CORE
task results.
For EPI (Table 5), we find comparable differences
in F-score to those for GE, but there is a signifi-
cant difference in the precision-recall balance: the
majority of systems show over 5% points higher re-
call under the new criteria, but many show substan-
tial losses in precision, while for GE precision was
also systematically increased. This effect was not
unexpected: we judge this to reflect primarily the
increased number of opportunities to extract each
unique event (higher recall) combined with the com-
paratively higher effect from errors from the reduc-
tion in the total number of unique correct extraction
targets (lower precision). It is not clear from our
analysis why a comparable effect was not seen for
GE. Interestingly, most systems show a better pre-
cision/recall balance under the new criteria than the
old, despite not optimizing for these criteria.
For ID (Table 6), we find a different effect also on
F-score, with all but one system showing reduced
performance under the new criteria, with some very
clear drops in performance; the only system to ben-
efit is UTurku. Analysis suggests that this effect
traces primarily to a notable reduction in the number
of simple PROCESS events that take no arguments6
when considering unique events on the document
level instead of each event mention independently.7
Conversely, the Stanford system, which showed the
highest instance-level performance in the extraction
of PROCESS type events (see Pyysalo et al (2011a)),
shows a clear loss in precision.
6The ID task annotation criteria call for mentions of some
high-level biological processes such as ?infection? to be anno-
tated as PROCESS even if no explicit participants are mentioned
(Pyysalo et al, 2011a).
7It is interesting to note that there was an error in the
UTurku system implementation causing it to fail to output any
events without arguments (Jari Bjo?rne, personal communica-
tion), likely contributing to the effect seen here.
Primary criteria New criteria
Group Rec. Prec. F Rec. Prec. F
FAUST 50.84 66.35 57.57 50.11 65.33 56.72
UMass 49.67 62.39 55.31 49.34 60.98 54.55
Stanford 49.16 56.37 52.52 42.00 50.80 45.98
ConcordU 50.91 43.37 46.84 43.42 37.18 40.06
UTurku 39.23 49.91 43.93 48.03 51.84 49.86
PredX 23.67 35.18 28.30 20.94 30.69 24.90
Table 6: Comparison of BioNLP ST 2011 ID CORE task
results.
The clear differences in performance and the
many cases in which the system rankings under the
two criteria differ demonstrate that the new evalua-
tion criteria can have a decisive effect in which ap-
proaches to event extraction appear preferred. While
there may be cases for which the original shared task
criteria are preferred, there is at the very minimum
a reasonable argument to be made that the emphasis
these criteria place on the extraction of each instance
of simple events is unlikely to reflect the needs of
many practical applications of event extraction.
While these experimental results demonstrate that
the new evaluation criteria emphasize substantially
different aspects of the performance of the systems
than the original criteria, they cannot per se serve
as an argument in favor of one set of criteria over
another. We hope that these results and the accom-
panying tools will encourage increased study and
discussion of evaluation criteria for event extraction
and more careful consideration of the needs of spe-
cific applications of the technology.
5 Discussion and Conclusions
We have presented a new resource combining analy-
ses from the systems participating in the GE, ID and
EPI main tasks of the BioNLP Shared Task 2011,
compiled with the collaboration of groups partic-
ipating in these tasks. We demonstrated one use
of the resource through an evaluation of develop-
ment set events that none of the participating sys-
tems could recover, finding that events involving
implicit arguments, coreference and participants in
more than once sentence continue to represent chal-
lenges to the event extraction systems that partici-
pated in these tasks.
We further argued in favor of new perspectives to
the evaluation of domain event extraction systems,
106
emphasizing in particular the need for document-
level, ?off-the-page? representations and evaluation
to complement the text-bound, instance-level eval-
uation criteria that have so far been applied in the
shared task evaluation. We proposed a variant of
the shared task standoff representation for support-
ing such evaluation, and introduced evaluation tools
implementing the proposed criteria. An evaluation
supported by the introduced resources demonstrated
that the new criteria can in cases provide substan-
tially different results and rankings of the systems,
confirming that the proposed evaluation can serve
as an informative complementary perspective into
event extraction performance.
In future work, we hope to further extend the cov-
erage of the provided system outputs as well as their
analysis to cover all participants of all tasks in the
BioNLP Shared Task 2011. We also aim to use the
compiled resource in further study of appropriate
criteria for the evaluation of event extraction meth-
ods and deeper analysis of the remaining challenges
in event extraction.
To encourage further study of all aspects of event
extraction, all resources and tools introduced in this
study are provided freely to the community from
http://2011.bionlp-st.org.
Acknowledgments
We wish to thank the members of all groups con-
tributing to the combined resource, and in particular
the members of the MSR-NLP group for providing
both the initial suggestion for its creation as well as
the first publicly released analyses from their sys-
tem. We would also like to thank the anonymous
reviewers for their many insightful comments.
This work was funded in part by UK Biotechnol-
ogy and Biological Sciences Research Council (BB-
SRC) under project Automated Biological Event Ex-
traction from the Literature for Drug Discovery (ref-
erence number: BB/G013160/1), by the Ministry of
Education, Culture, Sports, Science and Technology
of Japan under the Integrated Database Project and
by the Swedish Royal Academy of Sciences.
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381?390.
Jari Bjo?rne and Tapio Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of the
BioNLP Shared Task 2011 Workshop.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Complex event extraction
at PubMed scale. Bioinformatics, 26(12):i382?390.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Maarten
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 56?64.
Quoc-Chinh Bui and Peter. M.A. Sloot. 2011. Extract-
ing biological events from text using simple syntactic
patterns. In Proceedings of BioNLP Shared Task 2011
Workshop, pages 143?146.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the ACL-HLT 2011, pages 976?986.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ACE) program?tasks, data, and evaluation. In Pro-
ceedings of LREC, volume 4, pages 837?840.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double layered learning for bio-
logical event extraction from text. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 153?154.
Matthew Gerber and Joyce Chai. 2010. Beyond nom-
bank: A study of implicit arguments for nominal predi-
cates. In Proceedings of ACL 2010, pages 1583?1592.
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task
2011 ? Bacteria gene interactions and renaming. In
Proceedings of BioNLP Shared Task 2011 Workshop,
pages 65?73.
Brian Kemper, Takuya Matsuzaki, Yukiko Matsuoka,
Yoshimasa Tsuruoka, Hiroaki Kitano, Sophia Anani-
adou, and Jun?ichi Tsujii. 2010. PathText: a text min-
ing integrator for biological pathway visualizations.
Bioinformatics, 26(12):i374?i381.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
general semantic interpretation approach to biologi-
cal event extraction. In Proceedings of the BioNLP
Shared Task 2011 Workshop.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2011a. Extracting
bio-molecular events from literature - the BioNLP?09
shared task. Computational Intelligence, 27(4):513?
540.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011b.
107
Overview of BioNLP Shared Task 2011. In Proceed-
ings of BioNLP Shared Task, pages 1?6.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori
Yonezawa. 2011c. Overview of the Genia Event task
in BioNLP Shared Task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop.
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, Alfonso Valencia, et al 2008. Overview
of the protein-protein interaction annotation extrac-
tion task of BioCreative II. Genome Biology, 9(Suppl
2):S4.
Quang Le Minh, Son Nguyen Truong, and Quoc Ho Bao.
2011. A pattern approach for biomedical event anno-
tation. In Proceedings of BioNLP Shared Task 2011
Workshop, pages 149?150.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching
approach for information extraction from biomedical
text. In Proceedings of the BioNLP Shared Task 2011
Workshop.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event extraction as dependency parsing.
In Proceedings of ACL-HLT 2011, pages 1626?1635.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-IJCNLP
2009, pages 1003?1011.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012. Boosting automatic event extraction from the
literature using domain adaptation and coreference
resolution. Bioinformatics.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii.
2011. Overview of BioNLP 2011 Protein Coreference
Shared Task. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 74?82.
Tomoko Ohta, Takuya Matsuzaki, Naoaki Okazaki,
Makoto Miwa, Rune S?tre, Sampo Pyysalo, and
Jun?ichi Tsujii. 2010. Medie and info-pubmed: 2010
update. BMC Bioinformatics, 11(Suppl 5):P7.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP Shared Task 2011
Workshop.
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of EMNLP-
CoNLL 2007, pages 717?727.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the entity relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of BioNLP Shared Task 2011 Workshop,
pages 83?88.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP entry in
BioNLP Shared Task 2011. In Proceedings of BioNLP
Shared Task 2011 Workshop, pages 155?163.
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction. In
Proceedings of EMNLP 2011, pages 1?12.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. Machine Learning and Knowledge Dis-
covery in Databases, pages 148?163.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Chris Manning. 2011. Model
combination for event extraction in BioNLP 2011. In
Proceedings of the BioNLP Shared Task 2011 Work-
shop.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP Shared Task 2011 Work-
shop.
Beth M. Sundheim. 1991. Third message understanding
evaluation and conference (MUC-3): Phase 1 status
report. In Proceedings of the Speech and Natural Lan-
guage Workshop, pages 301?305.
Sofie Van Landeghem, Sampo Pyysalo, Tomoko Ohta,
and Yves Van de Peer. 2010. Integration of static re-
lations to enhance event extraction from text. In Pro-
ceedings of BioNLP 2010, pages 144?152.
Sofie Van Landeghem, Thomas Abeel, Bernard De Baets,
and Yves Van de Peer. 2011a. Detecting entity rela-
tions as a supporting task for bio-molecular event ex-
traction. In Proceedings of BioNLP Shared Task 2011
Workshop, pages 147?148.
Sofie Van Landeghem, Filip Ginter, Yves Van de Peer,
and Tapio Salakoski. 2011b. Evex: a pubmed-scale
resource for homology-based generalization of text
mining predictions. In Proceedings of BioNLP 2011
Workshop, pages 28?37.
Andreas Vlachos and Mark Craven. 2011. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 36?40.
Katsumasa Yoshikawa, Sebastian Riedel, Tsutomu Hi-
rao, Masayuki Asahara, and Yuji Matsumoto. 2011.
Coreference based event-argument relation extraction
on biomedical text. Journal of Biomedical Semantics,
2(Suppl 5):S6.
108
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 47?56, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Bridging the Gap Between Scope-based and Event-based
Negation/Speculation Annotations: A Bridge Not Too Far
Pontus Stenetorp1 Sampo Pyysalo2,3 Tomoko Ohta2,3
Sophia Ananiadou2,3 and Jun?ichi Tsujii2,3,4
1Department of Computer Science, University of Tokyo, Tokyo, Japan
2School of Computer Science, University of Manchester, Manchester, United Kingdom
3National Centre for Text Mining, University of Manchester, Manchester, United Kingdom
4Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,smp,okap}@is.s.u-tokyo.ac.jp
sophia.ananiadou@manchester.ac.uk
jtsujii@microsoft.com
Abstract
We study two approaches to the marking of
extra-propositional aspects of statements in
text: the task-independent cue-and-scope rep-
resentation considered in the CoNLL-2010
Shared Task, and the tagged-event representa-
tion applied in several recent event extraction
tasks. Building on shared task resources and
the analyses from state-of-the-art systems rep-
resenting the two broad lines of research, we
identify specific points of mismatch between
the two perspectives and propose ways of ad-
dressing them. We demonstrate the feasibility
of our approach by constructing a method that
uses cue-and-scope analyses together with a
small set of features motivated by data anal-
ysis to predict event negation and speculation.
Evaluation on BioNLP Shared Task 2011 data
indicates the method to outperform the nega-
tion/speculation components of state-of-the-
art event extraction systems.
The system and resources introduced in this
work are publicly available for research pur-
poses at: https://github.com/ninjin/eepura
1 Introduction
Understanding extra-propositional aspects of texts
is key to deeper understanding of statements con-
tained in natural language texts. Extra-propositional
aspects such as the polarity of key statements have
long been acknowledged to be critical for user-
facing applications such as information retrieval
(Friedman et al, 1994; Hersh, 1996). In recogni-
tion of this need, a number of recent information
extraction (IE) resources involving structured repre-
sentations of text statements have explicitly included
some marking of certainty and polarity (LDC, 2005;
Kim et al, 2009; Saur and Pustejovsky, 2009; Kim
et al, 2011a; Thompson et al, 2011).
Although extra-propositional aspects are recog-
nised as important, there is no clear consensus on
how to address their annotation and extraction from
text. Some comparatively early efforts focused on
the detection of negation cue phrases associated with
specific (previously detected) terms through regu-
lar expression-based rules (Chapman et al, 2001).
A number of later efforts identified the scope of
negation cues with phrases in constituency analy-
ses in sentence structure (Huang and Lowe, 2007).
Drawing in part on this work, the BioScope corpus
(Vincze et al, 2008) applied a representation where
both cues and their associated scopes are marked as
contiguous spans of text (Figure 1 bottom). This ap-
proach was also applied in the CoNLL-2010 Shared
Task (Farkas et al, 2010), in which 13 participat-
ing groups proposed approaches for Task 2, which
required the identification of uncertainty cues and
their associated scopes in text. In the following,
we will term this task-independent, linguistically-
motivated approach as the cue-and-scope represen-
tation (please see Vincze et al (2008) for details re-
garding the representation).
For IE efforts, more task-oriented representations
are commonly applied. In an effort to formalise
and drive research for extracting structured repre-
sentations of statements regarding molecular biol-
ogy, the ongoing series of BioNLP shared tasks
have addressed biomedical Event Extraction (EE)
(Kim et al, 2009; Kim et al, 2011a). The extra-
propositional targets of negation and speculation
47
Figure 1: Example illustrating cue-and-scope and
event-based negation marking. ?Crossing-out?
marks events as negated. PRO, TH and NEG are ab-
breviations for PROTEIN, THEME and NEGATION,
respectively.
of extracted events were already included in the
first task in the series, using a representation where
events can be assigned ?flags? to mark them as being
negated, speculated, or both (Figure 1 upper). Due
to space limitations we refer the reader to Kim et al
(2009) for a detailed explanation of the representa-
tion; similar representations have been applied also
in previous event extraction tasks (LDC, 2005).
There are a number of ways in which task-
oriented, event-based approaches could benefit from
the existing linguistically-oriented cue-and-scope
methods for identifying extra-propositional aspects
of text statements. However, there has been sur-
prisingly little work exploring the combination of
the approaches, and comparatively few methods ad-
dressing the latter task in detail. Only three out
of the 24 participants in the BioNLP Shared Task
2009 submitted results for the non-mandatory nega-
tion/speculation task, and although negation and
speculation were also considered in three main tasks
for the 2011 follow-up event (Kim et al, 2011a),
the trend continued, with only two participants ad-
dressing the negation/speculation aspects of the task.
We are aware of only two studies exploring the rela-
tionship between the cue-and-scope and event-based
representations: in a manual analysis of scope over-
lap with tagged events, Vincze et al (2011) identi-
fied a number of issues and mismatches in annota-
tion scope and criteria, which may explain in part
the lack of methods combining these two lines of
research. Kilicoglu and Bergler (2010) approached
the problem from the opposite direction and used an
existing EE system to extract cue-and-scope annota-
tions in the CoNLL-2010 Shared Task.
In this work, we take a high-level perspective,
seeking to bridge the linguistically oriented frame-
work and the more application-oriented event frame-
work to overcome the mismatches demonstrated
by Vincze et al (2011). Specifically, we aim to
determine how cue-and-scope recognition systems
can be used to produce a state-of-the-art nega-
tion/speculation detection system for the EE task.
2 Resources
Several existing resources can support the investiga-
tion of the relationship between the linguistically-
oriented and task-oriented perspectives on nega-
tion/speculation detection. In this study, we make
use of the following resources.
First, we study the three BioNLP 2011 Shared
Task corpora that include annotation for negation
and speculation: the GE, EPI and ID main task cor-
pora (Table 1). Second, we make use of support-
ing analyses provided for these corpora in response
to a call sent by the BioNLP Shared Task organis-
ers to the developers of third-party systems (Stene-
torp et al, 2011). Specifically, we use the output
of the BiographTA NeSp Scope Labeler (here re-
ferred to as CLiPS-NESP) (Morante and Daelemans,
2009; Morante et al, 2010) provided by the Univer-
sity of Antwerp CLiPS center. This system provides
cue-and-scope analyses for negation and speculation
and was demonstrated to have state-of-the-art per-
formance at the relevant CoNLL-2010 Shared Task.
Finally, we make use of the event analyses created
by systems that participated in the BioNLP Shared
Task, made available to the research community for
the majority of the shared task submissions (Pyysalo
et al, 2012). These analyses represent the state-
of-the-art in event extraction and their capability to
detect event structures as well as marking them for
negation and speculation.
The above three resources present us with many
opportunities to relate scope-based annotations to
three highly relevant event-based corpora containing
negation/speculation annotations.
3 Manual Analysis
To gain deeper insight into the data and the chal-
lenges in combining the cue-and-scope and event-
oriented perspectives, we performed a manual anal-
ysis of the corpus annotations using the manually
48
Name Negated Events Speculated Events Negated Spans Speculated Spans Publication
EPI 103 (5.6%) 70 (3.8%) 561 1,032 Ohta et al (2011)
GE 759 (7.4%) 623 (6.0%) 1,308 1,968 Kim et al (2011b)
ID 69 (3.3%) 26 (1.2%) 415 817 Pyysalo et al (2011)
Table 1: Corpora used for our experiments along with annotation statistics for their respective training sets.
The parenthesised values are the relative proportion of negated/speculated event annotations.
Occ. (Ratio) EPI ID
Covered 26 (15.03%) 52 (56.52%)
Not-covered 135 (78.03%) 38 (41.30%)
Error-in-gold 12 (6.94%) 2 (2.18%)
Morphological 48 (27.75%) 11 (11.96%)
Hypothesis 44 (25.43%) 15 (16.30%)
Ellipsis 5 (2.89%) 0 (0.00%)
Argument-only 2 (1.16%) 10 (10.87%)
Table 2: Results from the Manual Data Analysis of
the EPI and ID test sets.
created BioNLP Shared Task training data event an-
notations, and the automatic annotations created for
this data by the CLiPS-NESP system. The test
data was held out and was not directly examined
at any point of our study. We performed the anal-
ysis specifically on the EPI and ID corpora, as the
GE corpus training set texts overlap with the train-
ing data for the CLiPS-NESP system (BioScope cor-
pus), and results on this data would thus not reflect
the performance of the system on unseen data, and
a comparison of the GE and BioScope gold anno-
tations was previously performed by Vincze et al
(2011).
The analysis was performed by an experienced
annotator with a doctoral degree in a related field
in biology, who individually examined each of the
events marked as negated and speculated in the
EPI and ID training corpora. For the analysis,
the CLiPS-NESP system output was super-imposed
onto the BioNLP Shared Task event annotations.
The annotator was asked to assign three primary
flags for each event that was marked as negated or
speculated: Covered if the event trigger was covered
by span(s) of the correct type with a correct cue in
the cue-and-span analysis, Not-covered if not Cov-
ered, and Error-in-gold if the negation/speculation
flag on the event annotation was itself incorrect. We
also identified a number of additional properties that
initial analysis suggested to frequently characterise
instances where the coverage of the cue-and-scope
system is lacking: Morphological was assigned if
the negation/speculation of an event could be in-
ferred only from the morphology of the word ex-
pressing the event, rather than from cue words in its
context (e.g. unphosphorylated, non-glycosylated);
Hypothesis for cases where speculation is marked
for events stated as hyphotheses1 under consider-
ation, e.g. ?We analysed the methylation status of
MGMT?; Ellipsis for cases where the modified ex-
pression is elided (e.g. ?A was phosphorylated but B
was not?); and Argument-only if the CLiPS-NESP
output had marked the argument of an event as
negated rather than the event trigger (we use argu-
ment in the sense it is used in the BioNLP Shared
Tasks, for example, in Figure 1 upper, the two argu-
ments of the event are ?fMimR? and ?fimA?).
The results of the analysis are summarised in Ta-
ble 2. We find that that the system shows a clear dif-
ference in coverage depending on the dataset. For
the ID dataset, a majority of the annotations are cov-
ered by the appropriate spans, while only a small mi-
nority are covered for EPI. Instead, the EPI dataset
contains a significant portion of events where extra-
propositional aspects can only be distinguished by
the morphology of the word expressing the event
(all Morphological cases were negation) as well as
events marked as speculated due to being expressed
as hypotheses under study.
The analysis thus identified specific ways in
which the applicability of negation-detection sys-
tems using a span-and-scope representation could be
improved for some tasks.
1While it is arguable whether such cases represent specula-
tion (Vincze et al, 2008), separation from affirmatively made
claims is clearly motivated for many applications.
49
Event-based
Scope-based
Negation/speculationdetection
Eventextraction
Oursystem
Figure 2: An illustration of our approach.
4 Methods
We next introduce the methods we apply for as-
signing negation and speculation flags to extracted
events.
4.1 Approach
To focus on the extra-propositional aspects of event
extraction, we only consider the assignment of the
negation and speculation flags, not the extraction of
the event structures that these mark. To our knowl-
edge, no previous work studying this subtask in iso-
lation from event extraction exists. Thus, in order to
be able to relate the performance of the methods we
consider to the performance of previously proposed
approaches, it is necessary to base the negation and
speculation detection on an event extraction analy-
sis. For this reason, we construct our methods us-
ing system outputs for systems participating in the
BioNLP Shared Task 2011, in effect creating a nega-
tion/speculation processing stage for a pipeline sys-
tem where the previous stage is the completion of
event analysis without negation/speculation detec-
tion (Figure 2).
Our methods thus take extracted events as input
and attempt to enrich the output with negation and
speculation annotations. This enables us to produce
a general system with the potential to be applied
together with any existing event extraction system.
Additionally, this allows us to directly compare our
system output with that of the negation/speculation
components of previously proposed monolithic sys-
tems by removing the existing negation and spec-
ulation output from submissions including this and
recreating these annotations using our methods.
4.2 Rule-based Methods
The most straightforward way of carrying over in-
formation from scope-based to event-based annota-
tions is to consider any event structure for which the
word or words stating the event (i.e. the event trig-
ger) is within the scope of a negation or speculation
be negated or speculated (respectively). We imple-
mented this simple heuristic as our initial rule-based
method.
One relatively common category of cases where
this heuristic fails that was identified in analysis re-
lates to events that take other events as arguments.
Consider, for example, the case illustrated in Fig-
ure 3. The speculation span is correctly identified as
covering the statement ?FimR modulates mfa1 ex-
pression?, and the event expressed through ?mod-
ulates? is identified as speculated. However, the
nested event, the expression of mfa1, is not spec-
ulated. To cover this case, we implemented what
we refer to as the root-heuristic, which prevents the
propagation of negation/speculation marking from
scopes to events that are the arguments of another
event contained in the same scope. The second rule-
based method we consider incorporates this addi-
tional heuristic.
Preliminary development set experiments indi-
cated that while the root-heuristic could improve
precision, the performance of the rule-based meth-
ods remained poor, in particular on the EPI dataset.
The results of the manual analysis (Section 3) sug-
gested this to trace in particular to two main issues,
namely differences between annotation criteria be-
tween BioScope and the shared task data (as noted
also by Vincze et al (2011)) and events which are
negated not by external cues but by morphological
alternations of the event trigger, such as ?unphos-
phorylated? expressing the absence of phosphory-
lation. As it would have been difficult to system-
atically incorporate both morphology and context
into the rule-based method without compromising
the generality of the approach, we opted to move to a
machine learning framework for further method de-
velopment. This allows us to continue to make use
of the existing cue-and-scope annotations while ex-
ploring the effects of other aspects of the text and
maintaining generality through retraining.
4.3 Machine Learning-based Methods
In developing a machine learning-based approach to
the negation/speculation task, we aimed to identify
and evaluate a minimal set of features directly mo-
50
Feature Example Value(s)
Heuristic ROOT/NON-ROOT
Heuristic-Cue possibility
Heuristic-Span One, possibility, . . .
Trigger-Text non-phosphorylated
Trigger-Prefixes no, non, non-, . . .
Trigger-Preceding-Context is, that, . . .
Trigger-Proceeding-Context mfa1, expression, . . .
Table 3: Machine learning features. The fea-
tures are categorised into three groups: features
based on cue-and-scope based heuristics (top), non-
contextual features derived from the event trigger
(middle), and features derived from the context of
the event trigger (bottom). These three feature sets
are abbreviated as E, M and C, respectively.
Figure 3: Example of a speculation span containing
two events, of which only one is speculated (marked
by a dashed border).
tivated by the analysis of the data and to use the
cue-and-scope analyses as much as possible. In par-
ticular, we wanted to avoid features requiring com-
putationally expensive analyses such as full pars-
ing or replicating the type of analyses performed by
the CLiPS-NESP system, focusing rather on specific
points where its output does not meet the needs of
the event-based approach.
We introduced features representing the heuristics
described in Section 4.2, marking each case as be-
ing either a root or non-root event in its scope (if
any). Drawing further on the cue-and-scope analy-
sis, we included as features the cue word and bag-of-
words features for all tokens in the scope (using sim-
ple white-space tokenisation). To address the issues
identified in manual analysis, we introduced features
for the event trigger text as well as character-based
prefixes of lengths 2 to 7 of the, intended primarily
to capture morphological negation.
All features presented above are derived only
from those parts of the sentence already marked ei-
ther by the event extraction or the cue-and-scope
system. However, due to the differences in anno-
tation guidelines for speculation annotations, we ex-
pect that the scope-based system will fail to mark
a significant portion of the speculation annotations.
To allow the system to learn to detect these, we in-
troduce a minimal set of contextual features, limited
to a bag-of-words representation of the three words
preceding and following the event trigger.
5 Experiments
We perform two sets of experiments, the first to eval-
uate our approach on gold annotations to give a fair
upper-limit to how well our negation/speculation de-
tection system could perform under ideal settings,
and the second to enrich the output of an event ex-
traction system with negation and speculation an-
notations, to evaluate real-world performance and
to allow direct comparison of our methods with
those incorporated in monolithic event extraction
and negation/speculation detection systems.
5.1 Corpora
For our experiments we used the GE, EPI and ID
corpora of the BioNLP Shared Task 2011 (Table 1).
We note that while the GE training set texts overlap
with the BioScope corpus used to train the CLiPS-
NESP system, the GE test set does not, and thus test
set results are not expected to be overfit.
We noted when performing development set
experiments that training machine learning-based
methods on the negation/speculation annotations of
the event-annotated corpora was problematic due to
the sparseness of these flags in the annotation. To
address this issue, we merge the training data of the
three corpora in all experiments with machine learn-
ing methods.
5.2 Baseline methods
We use the event analyses created by the UTurku
(Bjo?rne and Salakoski, 2011) and UConcordia (Kil-
icoglu and Bergler, 2011) systems for the BioNLP
2011, the only systems that included negation and
speculation analyses. To investigate the impact on a
system that did not include a negation/speculation
component, we further consider analyses created
51
Negation (R/P/F) EPI GE ID
H 29.23/31.67/30.40 53.92/52.84/53.38 44.00/31.88/36.97
HR 27.69/32.73/30.00 53.24/71.89/61.18 44.00/37.93/40.74
M 47.69/20.00/28.18 43.00/25.25/31.82 46.00/26.74/33.82
ME 60.00/66.10/62.90 58.36/70.08/63.69 54.00/69.23/60.67
MC 40.00/74.29/52.00 58.36/76.34/66.15 52.00/61.90/56.52
MCE 58.46/73.08/64.96 61.77/83.03/70.84 58.00/70.73/63.74
Table 4: Results for Negation for our two heuristics and the four combinations of machine learning features.
Speculation (R/P/F) EPI GE ID
H 13.46/6.48/8.75 33.77/18.12/23.58 54.17/6.50/11.61
HR 11.54/5.66/7.59 32.79/29.45/31.03 54.17/7.98/13.90
M 1.92/0.62/0.93 25.65/10.84/15.24 45.83/10.58/17.19
ME 3.85/12.50/5.88 22.08/42.24/29.00 29.17/28.00/28.57
MC 51.92/52.94/52.43 27.27/50.30/35.37 37.50/31.03/33.96
MCE 48.08/51.02/49.50 31.82/53.85/40.00 33.33/42.11/37.21
Table 5: Results for Speculation for our two heuristics and the four combinations of ML features.
by the FAUST system, which achieved the high-
est performance at two of the three tasks consid-
ered (Riedel et al, 2011). The UTurku system is
a pipeline ML-based EE system, while the UCon-
cordia system is strictly rule-based. FAUST is an
ML-based model combination system incorporating
information from the parser-based Stanford system
(McClosky et al, 2011) and the jointly-modelled
UMass system (Riedel and McCallum, 2011).
We also performed preliminary experiments for
the other released submissions to the BioNLP 2011
Shared Task, but due to space limitations focus only
on the three above-mentioned systems.
5.3 Evaluation criteria
We use the primary evaluation criteria of the
BioNLP 2011 Shared Task (Kim et al, 2011a) to
assure comparability, reporting all results using the
standard precision, recall and their harmonic mean
(F-score).
5.4 Methods
We apply the rule-based simple heuristic method
and its root extension (Section 4.2) as well as Sup-
port Vector Machines (SVM) trained with the fea-
tures introduced in Section 4.3. For the SVM, we
separately evaluate models based on all permuta-
tions of the feature sets introduced in Table 3. In the
results tables we abbreviate the feature set names as
done in Table 3 and use H for the heuristic method
and R for its root extension. As our machine learn-
ing component we use LIBLINEAR (Fan et al,
2008) with a L2-regularised L2-loss SVM model.
We optimise the SVM regularisation parameter C
using 10-fold cross-validation on the training data.
We use the training, development and test set par-
tition provided by the shared task organisers. In line
with standard ML methodology the test set was held
out during development and was only used when
carrying out the final experiments prior to submit-
ting the manuscript.
6 Results and Discussion
Our initial experiments, building on gold event data
(Tables 4 and 5), support our manual analysis, show-
ing nearly uniform performance improvement with
additional features. First, we find that the root-
heuristic gives an improvement over the original
heuristic in four out of six cases. To justify our us-
age of the cue-and-scope based heuristic feature (E)
we find that adding it as a feature improves on the M
feature set and the MC feature set, showing that even
given context, the cue-and-scope perspective is still
useful. The only anomaly is for speculation on the
EPI dataset, where adding this heuristic feature ac-
tually hampers performance, possibly relating to the
52
Negation (R/P/F) EPI GE ID
UConcordia 16.92/61.11/26.51 18.43/43.44/25.88 22.00/23.91/22.92
UConcordia* 20.00/70.59/31.17 20.14/42.96/27.42 28.00/31.58/29.68
UTurku 12.31/38.10/18.60 22.87/48.85/31.15 26.00/44.83/32.91
UTurku* 43.08/48.28/45.53 21.16/38.56/27.33 26.00/41.94/32.10
FAUST* 29.23/59.38/39.18 21.50/41.18/28.25 28.00/46.67/35.00
Table 6: Results of the Negation enrichment experiment.
Speculation (R/P/F) EPI GE ID
UConcordia 5.77/8.33/6.82 21.10/38.46/27.25 8.33/2.00/3.23
UConcordia* 1.92/4.55/2.70 12.99/29.20/17.98 8.33/2.22/3.51
UTurku 30.77/48.48/37.65 17.86/32.54/23.06 12.50/18.75/15.00
UTurku* 46.15/47.06/46.60 11.04/26.56/15.60 8.33/3.33/4.76
FAUST* 36.54/48.72/41.76 10.39/26.50/14.93 12.50/12.50/12.50
Table 7: Results of the Speculation enrichment experiment.
(R/P/F) EPI ID
UConcordia 20.83/42.14/27.88 49.00/40.27/44.21
UConcordia* 20.83/42.94/28.05 49.20/41.78/45.19
UTurku 52.69/53.98/53.33 37.85/48.62/42.57
UTurku* 54.72/53.86/54.29 37.79/47.76/42.19
FAUST 28.88/44.51/35.03 48.03/65.97/55.59
FAUST* 31.64/45.17/37.21 49.20/64.66/55.88
Table 8: Overall scores for the EPI and ID data sets.
sparseness of useful annotations due to the differing
annotation guidelines, as noted in manual analysis.
The numbers from these initial experiments serve as
an upper bound when we proceed to our enrichment
experiments, as they do not suffer from the possibil-
ity of producing false positives negation/speculation
annotations for false positive event structures.
In addition to the above in preliminary experi-
ments we also considered two features inspired by
findings made by Vincze et al (2011). A distance-
based feature, measuring the distance in tokens be-
tween the cue-word and the event trigger, and also
trigger suffixes to capture some cases of morpholog-
ical speculation (?induced? vs. ?inducible?). How-
ever, we failed to establish any consistent benefits
from these features and only for the EPI dataset did
the suffix features improve performance.
For the enrichment evaluation, adding nega-
F EPI GE ID
UConcordia 57.43 60.68 67.28
UTurku 81.31 66.27 55.84
FAUST 74.91 66.14 67.13
Table 9: Estimated F-score upper-bound for an ora-
cle system precision assigning negation/speculation
annotations to events predicted by an up-stream EE
system.
tion/speculation flags to the output of event extrac-
tion systems (Tables 6 and 7), our results are some-
what more modest. For negation we see an improve-
ment in four out of six cases, and for speculation in
two out of six. Despite the fact that a major limi-
tation to our approach are the false positive events
that are propagated from the original EE system, we
manage to improve the global score for all data sets
where a global score is provided by the organisers
(Table 8). We improve a full point in F-score for
UTurku on EPI, but only sub-percentage for Faust
on ID, the latter most likely since ID contains fewer
negation and speculation annotations and the global
scores are microaverages over all annotations.
As a final analysis we estimate the upper-bound
in F-score performance for all three EE systems
(Table 9). We do so by assuming that the recall
for events marked by negation and speculation is
53
equal to that of the overall recall of the up-stream
EE system and that negation/speculation annotations
assigned by an oracle. What we can see is that
there is still room for improvement, both for our
enrichment approach and for the EE system?s inter-
nal negation/speculation components, although re-
call of the EE output is a limiting factor we can
expect further efforts towards improving the extra-
propositional aspects of the system to yield perfor-
mance improvements.
7 Conclusions and Future Work
In this study, we have considered two broad lines
of research on extra-propositional aspects of key
statements in text, one using the task-independent,
linguistically-motivated cue-and-scope representa-
tion applied in the recent CoNLL-2010 Shared Task,
and the other using the task-oriented flagged-event
representation applied e.g. in the ACE and BioNLP
Shared Task evaluations. We presented a detailed
manual analysis exploring points of disagreement
and evaluated in detail rule-based and machine
learning-based methods joining state-of-the-art sys-
tems representing the two approaches.
Our manual analysis identified a number of phe-
nomena that limit the applicability of existing cue-
and-scope based systems to the event extraction
task, such as negation expressed through morpho-
logical change of words expressing events (e.g. un-
phosphorylated). To address these issues, we pro-
posed a combination of heuristics and simple lexical
features, carefully selected to address differences in
perspective between the cue-and-scope and event-
based frameworks and aiming to complement cue-
and-scope analyses for creating task-oriented out-
puts.
To test our approach, we created a method suit-
able for use as a component of an event extraction
pipeline that incorporates information from a previ-
ously proposed state-of-the-art cue-and-scope based
negation/speculation detection system and a mini-
mal set of features in an SVM-based system that was
shown to enhance and in several cases improve upon
the output of existing EE systems. Experiments on
the BioNLP Shared Task 2011 EPI and ID datasets
demonstrated that the combined approach could im-
prove the results of the best-performing systems at
the original task in 5 out of 6 cases, outperforming
the highest results reported for any system for these
two tasks.
There exist several potential targets for future
work on improving our introduced system and
to join cue-and-scope and event-based approaches.
Since none of the existing EE corpora was con-
structed with the aim to solely cover negation and
speculation annotations and taking into account our
finding that merging datasets to compensate for data
sparseness is beneficial, it might be worth consid-
ering other possible corpora or resources and how
they can be used for training our machine learning
system.
Also, it would be worthwhile to attempt to com-
bine an existing EE system capable of detect-
ing negation/speculation with our proposed method.
Combining the two could yield an ensemble, im-
proving upon an already strong system by bridging
the differences in perspectives and tapping into the
potential benefits of both approaches.
The system and all resources introduced in this
work are publicly available for research purposes at:
https://github.com/ninjin/eepura
Acknowledgements
The authors would like to thank the anonymous re-
viewers for their many insightful comments and sug-
gestions for improvements.
This work was funded in part by UK Biotechnol-
ogy and Biological Sciences Research Council (BB-
SRC) under project Automated Biological Event Ex-
traction from the Literature for Drug Discovery (ref-
erence number: BB/G013160/1), by the Ministry of
Education, Culture, Sports, Science and Technology
of Japan under the Integrated Database Project and
by the Swedish Royal Academy of Sciences.
References
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing Biomedical Event Extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 183?191.
Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gre-
gory F. Cooper, and Bruce G. Buchanan. 2001. A
simple algorithm for identifying negated findings and
diseases in discharge summaries. Journal of biomedi-
cal informatics, 34(5):301?310.
54
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 1?12.
Carol Friedman, Philip O. Alderson, John H.M. Austin,
James J. Cimino, and Stephen B. Johnson. 1994. A
general natural-language text processor for clinical ra-
diology. Journal of the American Medical Informatics
Association, 1(2):161?174.
William R. Hersh. 1996. Information retrieval: a health
care perspective. Springer.
Yuang Huang and Henry J. Lowe. 2007. A novel hybrid
approach to automated negation detection in clinical
radiology reports. Journal of the American Medical
Informatics Association, 14(3):304?311.
Halil Kilicoglu and Sabine Bergler. 2010. A High-
Precision Approach to Detecting Hedges and their
Scopes. In Proceedings of the Fourteenth Conference
on Computational Natural Language Learning, pages
70?77.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
General Semantic Interpretation Approach to Biolog-
ical Event Extraction. In Proceedings of the BioNLP
2011 Workshop Companion Volume for Shared Task,
pages 173?182.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 Shared Task on Event Extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011a.
Overview of BioNLP Shared Task 2011. In Proceed-
ings of the BioNLP 2011 Workshop Companion Vol-
ume for Shared Task, pages 1?6.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of Genia Event
Task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 7?15.
LDC. 2005. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Events. Technical re-
port, Linguistic Data Consortium.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event Extraction as Dependency Parsing
for BioNLP 2011. In Proceedings of BioNLP 2011,
pages 41?45.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the Workshop on Current Trends in
Biomedical Natural Language Processing, pages 28?
36.
Roser Morante, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based resolution of in-sentence
scopes of hedge cues. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning ? Shared Task, CoNLL 2010: Shared
Task, pages 40?47.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, pages 16?25.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 26?35.
Sampo Pyysalo, Pontus Stenetorp, Tomoka Ohta, Jin-
Dong Kim, and Sophia Ananiadou. 2012. New Re-
sources and Perspectives for Biomedical Event Extrac-
tion. In Proceedings of BioNLP 2012 Workshop. to
appear.
Sebastian Riedel and Andrew McCallum. 2011. Robust
Biomedical Event Extraction with Dual Decomposi-
tion and Minimal Domain Adaptation. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 46?50.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher D. Manning. 2011.
Model Combination for Event Extraction in BioNLP
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, pages 51?55.
Roser Saur and James Pustejovsky. 2009. Fact-
Bank: a corpus annotated with event factuality.
Language Resources and Evaluation, 43:227?268.
10.1007/s10579-009-9089-9.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, pages 112?120.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedical
event corpus with meta-knowledge annotation. BMC
Bioinformatics, 12(1):393.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The Bio-
55
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
Veronika Vincze, Gyorgy Szarvas, Gyorgy Mora,
Tomoko Ohta, and Richard Farkas. 2011. Linguis-
tic scope-based and biological event-based specula-
tion and negation annotations in the BioScope and Ge-
nia Event corpora. Journal of Biomedical Semantics,
2(Suppl 5):S8.
56
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 99?103,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP Shared Task 2013: Supporting Resources
Pontus Stenetorp 1 Wiktoria Golik 2 Thierry Hamon 3
Donald C. Comeau 4 Rezarta Islamaj Dog?an 4 Haibin Liu 4 W. John Wilbur 4
1 National Institute of Informatics, Tokyo, Japan
2 French National Institute for Agricultural Research (INRA), Jouy-en-Josas, France
3 University Paris 13, Paris, France
4 National Center for Biotechnology Information, National Library of Medicine,
National Institutes of Health, Bethesda, MD, USA
pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr
{comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov
Abstract
This paper describes the technical con-
tribution of the supporting resources pro-
vided for the BioNLP Shared Task 2013.
Following the tradition of the previous
two BioNLP Shared Task events, the task
organisers and several external groups
sought to make system development easier
for the task participants by providing auto-
matically generated analyses using a vari-
ety of automated tools. Providing analy-
ses created by different tools that address
the same task also enables extrinsic evalu-
ation of the tools through the evaluation of
their contributions to the event extraction
task. Such evaluation can improve under-
standing of the applicability and benefits
of specific tools and representations. The
supporting resources described in this pa-
per will continue to be publicly available
from the shared task homepage
http://2013.bionlp-st.org/
1 Introduction
The BioNLP Shared Task (ST), first organised in
2009, is an ongoing series of events focusing on
novel challenges in biomedical domain informa-
tion extraction. In the first BioNLP ST, the or-
ganisers provided the participants with automat-
ically generated syntactic analyses from a variety
of Natural Language Processing (NLP) tools (Kim
et al, 2009) and similar syntactic analyses have
since then been a key component of the best per-
forming systems participating in the shared tasks.
This initial work was followed up by a similar ef-
fort in the second event in the series (Kim et al,
2011), extended by the inclusion of software tools
and contributions from the broader BioNLP com-
munity in addition to task organisers (Stenetorp et
al., 2011).
Although no formal study was carried out to es-
timate the extent to which the participants utilised
the supporting resources in these previous events,
we note that six participating groups mention us-
ing the supporting resources in published descrip-
tions of their methods (Emadzadeh et al, 2011;
McClosky et al, 2011; McGrath et al, 2011;
Nguyen and Tsuruoka, 2011; Bjo?rne et al, 2012;
Vlachos and Craven, 2012). These resources have
been available also after the original tasks, and
several subsequent studies have also built on the
resources. Van Landeghem et al (2012) applied a
visualisation tool that was made available as a part
of the supporting resources, Vlachos (2012) em-
ployed the syntactic parses in a follow-up study
on event extraction, Van Landeghem et al (2013)
used the parsing pipeline created to produce the
syntactic analyses, and Stenetorp et al (2012) pre-
sented a study of the compatibility of two different
representations for negation and speculation anno-
tation included in the data.
These research contributions and the overall
positive reception of the supporting resources
prompted us to continue to provide supporting re-
sources for the BioNLP Shared Task 2013. This
paper presents the details of this technical contri-
bution.
2 Organisation
Following the practice established in the
BioNLP ST 2011, the organisers issued an
open call for supporting resources, welcoming
contributions relevant to the task from all authors
of NLP tools. In the call it was mentioned that
points such as availability for research purposes,
support for well-established formats and access
99
Name Annotations Availability
BioC Lemmas and syntactic constituents Source
BioYaTeA Terms, lemmas, part-of-speech and syntactic constituencies Source
Cocoa Entities Web API
Table 1: Summary of tools/analyses provided by external groups.
to technical documentation would considered
favourable (but not required) and each supporting
resource provider was asked to write a brief
description of their tools and how they could
potentially be applied to aid other systems in the
event extraction task. This call was answered
by three research groups that offered to provide
a variety of semantic and syntactic analyses.
These analyses were provided to the shared
task participants along with additional syntactic
analyses created by the organisers.
However, some of the supporting resource
providers were also participants in the main event
extraction tasks, and giving them advance access
to the annotated texts for the purpose of creating
the contributed analyses could have given those
groups an advantage over others. To address this
issue, the texts were made publicly available one
week prior to the release of the annotations for
each set of texts. During this week, the supporting
analysis providers annotated the texts using their
automated tools and then handed the analyses to
the shared task organisers, who made them avail-
able to the task participants via the shared task
homepage.
3 Analyses by External Groups
This section describes the tools that were applied
to create supporting resources by the three exter-
nal groups. These contributions are summarised in
Table 1.
BioC Don Comeau, Rezarta Islamaj, Haibin
Liu and John Wilbur of the National Center for
Biotechnology Information provided the output of
the shallow parser MedPost (Smith et al, 2004)
and the BioLemmatizer tool (Liu et al, 2012),
supplied in the BioC XML format1 for annota-
tion interchange (Comeau et al, 2013). The BioC
format address the problem of interoperability be-
tween different tools and platforms by providing a
unified format for use by various tools. Both Med-
Post and BioLemmatizer are specifically designed
1http://bioc.sourceforge.net/
for biomedical texts. The former annotates parts-
of-speech and performs sentence splitting and to-
kenisation, while the latter performs lemmatisa-
tion. In order to make it easier for participants
to get started with the BioC XML format, the
providers also supplied example code for parsing
the format in both the Java and C++ programming
languages.
BioYaTeA Wiktoria Golik of the French Na-
tional Institute for Agricultural Research (INRA)
and Thierry Hamon of University Paris 13 pro-
vided analyses created by BioYaTeA2 (Golik et
al., 2013). BioYaTeA is a modified version of the
YaTeA term extraction tool (Aubin and Hamon,
2006) adapted to the biomedical domain. Working
on a noun-phrase level, BioYaTeA provides anno-
tations such as lemmas, parts-of-speech, and con-
stituent analysis. The output formats used were a
simple tabular format as well as BioYaTeA-XML,
an XML representation specific to the tool.
Cocoa S. V. Ramanan of RelAgent Private Ltd
provided the output of the Compact cover anno-
tator (Cocoa) for biological noun phrases.3 Co-
coa provides noun phrase-level entity annotations
for over 20 different semantic categories such as
macromolecules, chemicals, proteins and organ-
isms. These annotations were made available for
the annotated texts for the shared task along with
the opportunity for the participants to use the Co-
coa web API to annotate any text they may con-
sider beneficial for their system. The data format
used by Cocoa is a subset of the standoff format
used for the shared task entity annotations, and it
should thus be easy to integrate into existing event
extraction systems.
4 Analyses by Task Organisers
This section describes the syntactic parsers ap-
plied by the task organisers and the pre-processing
2http://search.cpan.org/?bibliome/
Lingua-BioYaTeA/
3http://npjoint.com/
100
Name Model Availability
Enju Biomedical Binary
Stanford Combination Binary, Source
McCCJ Biomedical Source
Table 2: Parsers used for the syntactic analyses.
and format conversions applied to their output.
The applied parsers are listed in Table 2.
4.1 Syntactic Parsers
Enju Enju (Miyao and Tsujii, 2008) is a deep
parser based on the Head-Driven Phrase Struc-
ture Grammar (HPSG) formalism. Enju analyses
its input in terms of phrase structure trees with
predicate-argument structure links, represented in
a specialised XML-format. To make the analyses
of the parser more accessible to participants, we
converted its output into the Penn Treebank (PTB)
format using tools included with the parser. The
use of the PTB format also allow for its output to
be exchanged freely for that of the other two syn-
tactic parsers and facilitates further conversions
into dependency representations.
McCCJ The BLLIP Parser (Charniak and John-
son, 2005), also variously known as the Charniak
parser, the Charniak-Johnson parser, or the Brown
reranking parser, has been applied in numerous
biomedical domain NLP efforts, frequently using
the self-trained biomedical model of McClosky
(2010) (i.e. the McClosky-Charniak-Johnson or
McCCJ parser). The BLLIP Parser is a con-
stituency (phrase structure) parser and the applied
model produces PTB analyses as its native out-
put. These analyses were made available to par-
ticipants without modification.
Stanford The Stanford Parser (Klein and Man-
ning, 2002) is a widely used publicly available
syntactic parser. As for the Enju and BLLIP
parsers, a model trained on a dataset incorporating
biomedical domain annotations is available also
for the Stanford parser. Like the BLLIP parser,
the Stanford parser is constituency-based and pro-
duces PTB analyses, which were provided to task
participants. The Stanford tools additionally in-
corporate methods for automatic conversion from
this format to other representations, discussed fur-
ther below.
4.2 Pre-processing and Conversions
To create the syntactic analyses from the Enju,
BLLIP and Stanford Parser systems, we first ap-
plied a uniform set of pre-processing steps in order
to normalise over differences in e.g. tokenisation
and thus ensure that the task participants can eas-
ily swap the output of one system for another. This
pre-processing was identical to that applied in the
BioNLP 2011 Shared Task, and included sentence
splitting of the annotated texts using the Genia
Sentence Splitter,4 the application of a set of post-
processing heuristics to correct frequently occur-
ring sentence splitting errors, and Genia Treebank-
like tokenisation (Tateisi et al, 2004) using a to-
kenisation script created by the shared task organ-
isers. 5
Since several studies have indicated that repre-
sentations of syntax and aspects of syntactic de-
pendency formalism differ in their applicability to
support information extraction tasks (Buyko and
Hahn, 2010; Miwa et al, 2010; Quirk et al, 2011),
we further converted the output of each of the
parsers from the PTB representation into three
other representations: CoNNL-X, Stanford De-
pendencies and Stanford Collapsed Dependencies.
For the CoNLL-X format we employed the con-
version tool of Johansson and Nugues (2007), and
for the two Stanford Dependency variants we used
the converter provided with the Stanford CoreNLP
tools (de Marneffe et al, 2006). These analyses
were provided to participants in the output for-
mats created by the respective tools, i.e. the TAB-
separated column-oriented format CoNLL and the
custom text-based format of the Stanford Depen-
dencies.
5 Results and Discussion
Just like in previous years the supporting resources
were well-received by the shared task participants
and as many as five participating teams mentioned
utilising the supporting resources in their initial
submissions (at the time of writing, the camera-
ready versions were not yet available). This level
of usage of the supporting resources by the partici-
pants is thus comparable to what was observed for
the 2011 shared task.
Following in the tradition of the 2011 support-
4https://github.com/ninjin/geniass
5https://github.com/ninjin/bionlp_
st_2013_supporting/blob/master/tls/
GTB-tokenize.pl
101
ing resources, to aim for reproducibility, the pro-
cessing pipeline containing pre/post-processing
and conversion scripts for all the syntactic parses
has been made publicly available under an open
licence.6 The repository containing the pipeline
also contains detailed instructions on how to re-
produce the output and how it can potentially be
applied to other texts.
Given the experience of the organisers in
analysing medium-sized corpora with a variety of
syntactic parsers, many applied repeatedly over
several years, we are also happy to report that the
robustness of several publicly available parsers has
recently improved noticeably. Random crashes,
corrupt outputs and similar failures appear to be
transitioning from being expected to rare occur-
rences.
In this paper, we have introduced the supporting
resources provided for the BioNLP 2013 Shared
Task by the task organisers and external groups.
These resources included both syntactic and se-
mantic annotations and were provided to allow the
participants to focus on the various novel chal-
lenges of constructing event extraction systems by
minimizing the need for each group to separately
perform standard processing steps such as syntac-
tic analysis.
Acknowledgements
We would like to give special thanks to Richard
Johansson for providing and allowing us to dis-
tribute an improved and updated version of his for-
mat conversion tool.7 We would also like to ex-
press our appreciation to the broader NLP com-
munity for their continued efforts to improve the
availability of both code and data, thus enabling
other researchers to stand on the shoulders of gi-
ants.
This work was partially supported by the
Quaero programme funded by OSEO (the French
agency for innovation). The research of Donald
C. Comeau, Rezarta Islamaj Dog?an, Haibin Liu
and W. John Wilbur was supported by the Intra-
mural Research Program of the National Institutes
of Health (NIH), National Library of Medicine
(NLM).
6https://github.com/ninjin/bionlp_st_
2013_supporting
7https://github.com/ninjin/
pennconverter
References
Sophie Aubin and Thierry Hamon. 2006. Improving
term extraction with terminological resources. In
Advances in Natural Language Processing, pages
380?387. Springer.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 Shared Task.
BMC Bioinformatics, 13(Suppl 11):S4.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating
the Impact of Alternative Dependency Graph Encod-
ings on Solving Event Extraction Tasks. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 982?992,
Cambridge, MA, October.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Ver-
spoor, Thomas C. Wiegers, Cathy H. Wu, and
W. John Wilbur. 2013. BioC: A minimalist ap-
proach to interoperability for biomedical text pro-
cessing. submitted.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double layered learning for bio-
logical event extraction from text. In Proceedings
of the BioNLP Shared Task 2011 Workshop, pages
153?154. Association for Computational Linguis-
tics.
Wiktoria Golik, Robert Bossy, Zorana Ratkovic, and
Claire Ne?dellec. 2013. Improving Term Extraction
with Linguistic Analysis in the Biomedical Domain.
In Special Issue of the journal Research in Comput-
ing Science, Samos, Greece, March. 14th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proc. of the 16th Nordic Conference
on Computational Linguistics (NODALIDA), pages
105?112.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
102
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of Genia Event
Task in BioNLP Shared Task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop.
Dan Klein and Christopher D Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. Advances in neural information pro-
cessing systems, 15(2003):3?10.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. BioLemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
David McClosky, Mihai Surdeanu, and Christopher D
Manning. 2011. Event Extraction as Dependency
Parsing for BioNLP 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 41?45.
Association for Computational Linguistics.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Brown University.
Liam R McGrath, Kelly Domico, Courtney D Cor-
ley, and Bobbie-Jo Webb-Robertson. 2011. Com-
plex biological event extraction from full text us-
ing signatures of linguistic and semantic features.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 130?137. Association for Compu-
tational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating Dependency Rep-
resentations for Event Extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 779?787,
Beijing, China, August.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Nhung TH Nguyen and Yoshimasa Tsuruoka. 2011.
Extracting bacteria biotopes with semi-supervised
named entity recognition and coreference resolution.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 94?101. Association for Compu-
tational Linguistics.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP Entry in
BioNLP Shared Task 2011. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 155?
163, Portland, Oregon, USA, June.
Larry Smith, Thomas Rindflesch, and W. John Wilbur.
2004. MedPost: a part-of-speech tagger for bio
medical text. Bioinformatics, 20(14):2320?2321.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 112?120, Portland, Oregon,
USA, June.
Pontus Stenetorp, Sampo Pyysalo, Tomoko Ohta,
Sophia Ananiadou, and Jun?ichi Tsujii. 2012.
Bridging the gap between scope-based and event-
based negation/speculation annotations: a bridge not
too far. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, pages 47?56. Association for Computa-
tional Linguistics.
Y Tateisi, T Ohta, and J Tsujii. 2004. Annotation of
predicate-argument structure on molecular biology
text. Proceedings of the Workshop on the 1st In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-04).
Sofie Van Landeghem, Kai Hakala, Samuel Ro?nnqvist,
Tapio Salakoski, Yves Van de Peer, and Filip Gin-
ter. 2012. Exploring biomolecular literature with
EVEX: connecting genes through events, homology,
and indirect associations. Advances in Bioinformat-
ics, 2012.
Sofie Van Landeghem, Jari Bjo?rne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, et al 2013. Large-scale event extrac-
tion from literature with multi-level gene normaliza-
tion. PloS one, 8(4):e55814.
Andreas Vlachos and Mark Craven. 2012. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. BMC Bioinfor-
matics, 13(Suppl 11):S5.
Andreas Vlachos. 2012. An investigation of imita-
tion learning algorithms for structured prediction. In
Workshop on Reinforcement Learning, page 143.
103
