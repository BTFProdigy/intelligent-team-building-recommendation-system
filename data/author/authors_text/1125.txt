A Natural Language Processing Infrastructure for Turkish 
A. C. Cem SAY 
Department of Computer Engineering, 
Bogazi?i University,  
Bebek,stanbul 
say@boun.edu.tr 
 
?zlem ?ETNOLU 
Faculty of Engineering and Natural Sciences, 
Sabanc? University,  
Tuzla,stanbul 
ozlemc@sabanciuniv.edu 
eniz DEMR 
Department of Computer Engineering, 
Bogazi?i University 
Bebek,stanbul 
sdemir@cse.yeditepe.edu.tr 
Fatih ??N 
Department of Computer Engineering, 
Bogazi?i University 
Bebek,stanbul 
fatih_ogun@yahoo.com 
 
Abstract 
We built an open-source software platform in-
tended to serve as a common infrastructure that 
can be of use in the development of new applica-
tions involving the processing of Turkish. The 
platform incorporates a lexicon, a morphological 
analyzer/generator, and a DCG parser/generator 
that translates Turkish sentences to predicate 
logic formulas, and a knowledge base frame-
work. Several developers have already utilized 
the platform for a variety of applications, includ-
ing conversation programs and an artificial per-
sonal assistant, tools for automatic analysis of 
rhyme and meter in Turkish folk poems, a proto-
type sentence-level translator between Albanian, 
Turkish, and English, natural language interfaces 
for generating SQL queries and JAVA code, as 
well as a text tagger used for collecting statistics 
about Turkish morpheme order for a speech rec-
ognition algorithm. The results indicate the 
adaptability of the infrastructure to different 
kinds of applications and how it facilitates im-
provements and modifications.  
Introduction 
The obvious potential of natural language processing 
technology for economic, social and cultural pro-
gress can be realized more comprehensively if NLP 
techniques applicable to a wider selection of the lan-
guages of the world are developed. Before the full-
scale treatment of a new language can start, a con-
siderable amount of effort has to be invested to 
computerize the lexical, morphological and syntactic 
specifics of that language, which would be required 
by any nontrivial application.  
We built an open-source software platform in-
tended to serve as a common infrastructure that can 
be of use in the development of new applications 
involving the processing of Turkish. The platform, 
named TOY (?etinolu 2001), is essentially a big 
set of predicates in the logic programming language 
Prolog. The choice of Prolog, which was designed 
specifically with computational linguistics applica-
tions in mind, as the implementation language for 
our software has natural consequences for the 
knowledge representation setup to be used by other 
programs built on our platform. Prolog is based on 
first-order predicate calculus, it allows knowledge 
items to be represented in terms of logic-style facts 
and rules, and a built-in theorem prover drives the 
execution of Prolog queries. 
The TOY program?s internal organization into 
source files reflects the three different levels (see 
Figure 1) on which text-based NLP applications can 
be based. In terms of that figure, processing at a 
?deeper? level necessitates all components of ?shal-
lower? levels.  
In this paper, we describe this infrastructure and 
how it was adapted to a variety of applications. Sec-
tion 2 gives a brief overview of the infrastructure. 
Section 3 presents the applications based on it. 
1 Infrastructure 
The TOY platform is formed of a lexicon that con-
tains most of the Turkish morphemes (either root or 
suffix), a Turkish morphological analyzer/generator, 
a DCG parser/generator for Turkish, and a semantic 
processor which interfaces the aforementioned sub-
units with the underlying knowledge base for 
knowledge addition and extraction. 
 
 Figure 1.  TOY?s Internal Organization. 
1.1    Lexicon 
A complete lexicon is supposed to contain entries 
for all morphemes (meaningful units that make up 
words) for the language in question. There are two 
kinds of morphemes: roots and affixes. In Turkish, 
all affixes follow the root, that is, they are suffixes. 
Our lexicon contains entries for over 29000 roots 
and 157 suffixes. A single morpheme may have 
more than one entry, corresponding to its different 
allomorphs. A morpheme definition example for the 
word ??ocuk? (?child?) is shown in Figure 2. 
 
Figure 2. Morpheme Definition. 
 
The semantic representation slot gives a descrip-
tion of the contribution of the morpheme to the 
meaning of full-size sentences in which it appears: 
Since the meanings of sentences are represented by 
predicate logic formulas in our setup, roots contrib-
ute ?partial? versions of such formulas, with ?holes? 
to be filled by the contributions of the other words of 
the sentence. For instance, the semantic representa-
tion entry of the noun ??ocuk? is ?ocuk(_). In the 
sentence ?Ali ?ocuktur? (?Ali is a child?), the value 
of the missing argument is supplied by the name Ali, 
resulting in the formula ?ocuk('Ali') for the overall 
sentence. 
For noun entries, the commonsense knowledge 
slot contains a pointer to the location of the thing 
described by this noun in the taxonomy tree (see 
Figure 4) used by the program. (The entry for 
??ocuk?, for instance, indicates that it can be 
reached by descending from the root along the ?con-
crete entity?-?animate-?human being? arcs.) For 
verbs, this slot contains a set of restrictions on the 
various argument slots of the verb. As an example, 
the agent of the verb ?ye-? (?eat?) is restricted to be 
a living thing, and its theme is restricted to be a 
solid.  
 
1.2    Morphological Analyzer/ Generator 
In the infrastructure, possible legal orderings of 
Turkish morphemes are represented by a large finite 
state diagram, a small part of which can be seen in 
Figure 3. The morphological component of the plat-
form is a finite state transducer that makes use of the 
lexicon for traversing the arcs of the diagram 
(adopted, with changes, from Kemal Oflazer?s work 
on Turkish morphology (Oflazer 1993)) to associate 
a character string with the list of meaning contribu-
tions of its morphemes. This traversal is complicated 
by the vowel harmony constraint of Turkish mor-
phology. This rule means that, when adding a suffix 
to a word, the allomorph to be added is a function of 
the last vowel of the word to be extended. For in-
stance, the plural suffix has the two allomorphs ?-
ler? and ?-lar?. The Turkish word for ?children? is 
??ocuklar?, while ?olives? is ?zeytinler?, since 
?back vowels? like ?a? and ?u? require a back vowel 
in the suffix, whereas ?front vowels? like ?e? and ?i? 
require a front vowel there. The program keeps track 
of the vowels during the transduction to enforce 
these constraints. Words of foreign origin which vio-
late vowel harmony are flagged appropriately in the 
lexicon. 
 
Figure 3. A Subgraph of the Morphological FST 
Employed by TOY. 
 
Like most Prolog predicates the morphological 
component is reversible, that is, the same piece of 
code can be used both to analyze a given word to 
obtain its underlying constituents, and to generate a 
word when given a list of such constituents. Another 
built-in feature of Prolog makes it very easy for the 
program to compute all results associated with a par-
ticular input when more than one legal output is 
possible, as in the case of the analysis of the 
morphologically ambiguous word ?yedi?, where our 
morphological parser produces, through backtrack-
ing, two alternative analyses: the third-
person/singular past tense inflection of the verb ?ye-
? (?eat?), and the Turkish number word for ?seven?. 
1.3    DCG Parser/Generator 
We encoded a subset of the syntax rules of Turkish 
in Prolog?s DCG notation. The DCG formalism al-
lows the computation of the meaning formula of a 
constituent to be performed in parallel with its syn-
tactic parsing; each DCG rule is written to indicate 
how the partial meanings of the elements on its 
right-hand side fit together to produce the semantic 
expression for the constituent on the left-hand side.  
Certain language constructs correspond naturally 
to the notion of quantification in predicate calculus. 
For instance, the sentence ?Bir ?ocuk zeytin yedi.? 
(?A child ate (an/some) olive(s)?) can be represented 
by the logical formula  
?X?Y( ?ocuk(X) ? zeytin(Y)  ? ye(Event, X,Y, Loca-
tion, Time, Goal, Source, Instrument, defi-
nite_past, none, positive) ). 
In the Prolog program, existentially quantified 
expressions like this one have the form 
some(X,Restrictor,Scope), where X is the quantified 
variable, and Restrictor and Scope are the two sides 
of the conjunction (Covington, 1994) 
some(X,?ocuk(X),some(Y,zeytin(Y),ye(EventMarker,
X,Y,Location,Time,Goal,Source,Instrument, 
definite_past,none,positive))) 
The successful DCG parsing of a sentence also re-
sults in a field being instantiated to a symbol repre-
senting the sentence?s mood. Possible values for the 
mood field are ?statement,? ?yes_no_question,? and 
?wh_question.? 
This component of the program is also designed 
to be reversible, that is, it can produce the corre-
sponding sentence when given a logical formula, but 
yet another peculiarity of the language complicates 
the solution: Turkish word order is (almost) free, 
which basically means that the sentence constituents 
can be shuffled around without changing the mean-
ing. Therefore, a single semantic formula usually 
corresponds to several different sentences, even 
without taking synonymity of words into account. 
Our software has the capability of producing multi-
ple alternative sentences as output in such cases. 
1.4    Anaphora Resolver 
In general, the full-scale processing of all but very 
simple sentences necessitates information that is not 
present in the sentence itself, the most obvious ex-
amples being question sentences. This additional 
information is either pre-encoded in the knowledge 
base as part of a big store of commonsense knowl-
edge, or, when the agent is involved in a dialogue or 
a story understanding task, it is gleaned from the 
other sentences in the input. 
One example where the computation of the mean-
ing of a declarative sentence requires access to 
knowledge obtained from previous sentences is the 
process of anaphora resolution. Resolving an ana-
phor is the job of finding out which discourse 
marker (unique internal name) to use for the entity 
referred to by this phrase in the knowledge base. 
There is no ?correct? algorithm for this task because 
of the inherent ambiguity of natural language (Lenat 
1995). Our resolver selects a discourse marker for an 
anaphoric reference making use of the taxonomy 
tree (see Figure 4), semantic type information in its 
dictionary, pointers to the locations in this tree, and 
the positions of the original referents in their sen-
tences.  
Our resolver treats only definite clauses as ana-
phors and resolves direct anaphors. Gelbukh and 
Sidorov (1999) propose ways of solving indirect 
anaphors. 
Figure 4. TOY?s Taxonomy Tree. 
 
An anaphor and its antecedent can be related if 
semantic type of the anaphor contains the semantic 
type of the antecedent or vice versa or their types 
intersect. Resolution of indirect anaphors will be 
added to TOY?s anaphora resolver in the future. This 
taxonomy tree will also be used for this purpose.   
1.5    Knowledge Base Interface 
This module translates predicate logic formulas cre-
ated by the DCG parser to Prolog facts and rules. As 
an example, the sentence ?Ali ?ocuktur? (?Ali is a 
child?) is eventually translated to the Prolog fact 
?ocuk('Ali'), whose form enables it to take part in 
automatic proofs involving this knowledge item 
when necessary. In general, nouns and adjectives are 
represented as single-argument predicates standing 
for the invoked property. 
Verbs other than ?to be? have a considerably 
more complicated representation. The Prolog 
equivalent of the sentence ?Ali gitti? (?Ali left?) is 
 
Figure 5. Prolog Representation Example. 
Of course, from the point of view of the com-
puter, (or, for that matter, of anybody who cannot 
speak Turkish,) a formula like ?ocuk('Ali') is just as 
opaque as ?Ali ?ocuktur?. When we look up a 
strange word in the dictionary, we comprehend its 
meaning by mentally linking it in appropriate ways 
to the words appearing in its description. If a suffi-
ciently large subgraph of this network of concepts 
that exists in our minds is replicated in the computer, 
it would be able to give the same response to an in-
put sentence as a human utilizing the same network. 
For instance, the Prolog rule 
?ocuk(X):- insan(X), k???k(X). 
(where ?insan? means ?human? and ?k???k? means 
?small? in Turkish) relates these three concepts in a 
way similar to the picture in most people?s minds. 
The translation of a Turkish sentence to the corre-
sponding predicate logic formula by the DCG rules 
is just an intermediate step in the processing of that 
sentence. ?Understanding? a sentence necessitates a 
computation involving both this formula and the 
current contents of the knowledge base, possibly 
resulting in a change to the knowledge base, and the 
generation of an appropriate response. 
Skolemization is used in the automatic transfor-
mation of the logical formulas of declarative sen-
tences to actual Prolog code by means of replacing 
all the existentially quantified variables by special 
expressions called Skolem functions. The purpose of 
this operation is to assign a discourse marker to 
every entity which is mentioned but not named in 
the sentence. These markers are the atomic symbols 
used by the computer to model the world being de-
scribed and referred to during the conversation, and 
keeping track of them is an essential part of the dia-
logue processing task. 
For wh-question sentences, the DCG parser cre-
ates formulas in the form of Prolog predicates. For 
instance, the sentence ?Kim zeytin yedi?? (?Who ate 
(an/some) olive(s)??) is translated to the formula 
which(X,insan(X),some(Y,zeytin(Y),ye(Event,X,Y, 
Loc, Time,Goal,Source, Instrument,      
        definite_past, none, positive)) 
whose form matches the already available logic pro-
gram which(Item,Property1Item,Property2Item). See 
the next section for a discussion of these ?question-
word? routines. 
2       Applications Based on TOY 
In this section, we will present some applications 
that were developed using the TOY infrastructure. 
Each subsection will briefly explain the application, 
the TOY components used, and the modifications 
done on the infrastructure. 
 
 
2.1    Conversational agent ? TOYagent 
Smith (1994) classifies dialogue styles that can be 
adopted by the computer during human-computer 
interaction into four modes, depending on the degree 
of control that the computer has on the dialogue: 
Directive, suggestive, declarative, and passive. 
TOYagent?s original approach mostly suits the pas-
sive mode, where the user has complete control, and 
the computer passively acknowledges user state-
ments, and provides information only as a response 
to direct user requests. 
TOYagent (Demir 2003) enables users to make 
on-line additions to the lexicon without the need to 
know Prolog. When faced with a word that it is un-
able to parse morphologically, TOYagent engages in 
a (mostly menu-driven) subdialogue with the user to 
identify the root, category, and morphophonemic 
properties of the word, and adds the appropriate en-
tries to the lexicon. The meanings of these new 
words can be incorporated to the system by the logic 
program synthesis facility, which enables the user to 
provide natural language descriptions for new predi-
cates in terms of existing predicates. These descrip-
tions are automatically converted to Prolog clauses 
and added to the knowledge base of the program for 
future use.  
The original dialogue algorithm embedded in 
TOYagent can be summarized as follows: 
1. Read a sentence (this may cause a ?word learn-
ing? subdialogue if one or more words in the sen-
tence cannot be parsed by the morphological 
analyzer) 
2. Analyze the sentence using the DCG parser, re-
solving anaphors if necessary. If the syntactic 
parse is unsuccessful, report this to the user and 
GOTO 1.  
3. If the mood is ?statement?, then the user is mak-
ing a declarative statement; use the built-in theo-
rem prover to try to prove the logical formula 
corresponding to the sentence. There are two pos-
sibilities: (In the following, all the ?canned? re-
sponses are in Turkish, of course.) 
        a. If the formula can be proven using the cur-
rent contents of the knowledge base, the informa-
tion contained in the sentence is already there; 
respond with ?Thanks, I know that? 
      b. If Prolog fails to prove the formula with its 
current knowledge, then negate the formula and 
try to prove this negation. There are two possibili-
ties: 
            i. If this new formula can be proven using the 
current contents of the knowledge base, the infor-
mation contained in the sentence is contradictory 
with what we already know; respond with ?I do 
not think so? 
            ii. If Prolog fails to prove this new formula 
with its current knowledge, create the necessary 
discourse and event markers and assert the Prolog 
clauses representing the input sentence to the 
knowledge base, responding with ?Thanks for the 
information?  
4. If the mood is ?yes_no_question?, the user has 
asked a yes-no question; use the built-in the prover 
to try to prove the sentence?s logical formula. 
There are two possibilities: 
      a. If the formula can be proven using the current 
contents of the knowledge base, respond with 
?Yes? 
    b. If Prolog fails to prove the formula with its 
current knowledge, then negate the formula and 
try to prove this negation. There are two possibili-
ties: 
            i. If this new formula can be proven using the 
current contents of the knowledge base, respond 
with ?No? 
            ii. If Prolog fails to prove this new formula 
with its current knowledge, respond with ?I do not 
know.? 
5. If the mood is ?wh_question?, the user has asked 
a wh-question; use the built-in theorem prover on 
the sentence?s logical formula. The associated 
program of each question word scans the knowl-
edge base and produces the relevant answer. The 
answer can be printed out directly, or, if required, 
in the form of a grammatical sentence generated 
by a procedure that first prepares a new logical 
formula from the produced knowledge items and 
then uses the syntax and morphology components 
to form the statement corresponding to this for-
mula. GOTO 1. 
The following conversation fragments, in which 
user entries are shown in boldface, illustrate several 
aspects of TOYagent. (The English translations are 
not part of TOYagent?s input-output, and have been 
added ?manually.?) 
As an example to ?online? learning of lexical en-
tries, we deleted the word ?ana? (?mother?) from the 
lexicon, and carried out the following dialogue with 
the program: 
 Her ana g?zeldir. (Every mother is beautiful) 
?ana? kelimesini bilmiyorum. (I do not know the 
word ?ana?) 
 Kelimenin k?k? nedir? (What is the stem of this 
word?) 
 1: a   
2: an  
3: ana  
 Hangisi: 3. 
 (Please enter: 3.) 
?ana? kelimesinin tipi nedir? (What is the type of 
the word ?ana??) 
 
 1: Cins isim (1: Common noun) 
 2: S?fat        (2: Adjective) 
 3: ?zel isim(3: Proper noun) 
 Hangisi: 3. 
(Please enter: 3) 
 S?zl?e eklendi. (It has been added to the lexicon) 
 ?ana? ne demek? Anlat?r m?s?n? (Could you ex-
plain the word ?ana??) 
 ?ocuu olan bir bayan anad?r. (A female who has a 
child is a mother) 
 Teekk?rler, ?rendim. (Thanks for the information) 
 
Since the unknown word could have appeared in 
an inflected form in the input sentence, TOYagent 
first asks about the actual stem. A Prolog rule corre-
sponding to the relevant universally quantified logic 
formula is prepared and asserted for each of the 
user?s sentences read in this dialogue. The last line is 
the program?s response to the original input sen-
tence. 
Aye bir anad?r. (Aye is a mother) 
Teekk?rler, ?rendim. (Thanks for the information) 
Aye g?zel midir? (Is Aye beautiful?) 
Evet. (Yes)  
Neden? (Why?) 
Her ana g?zeldir. (Every mother is beautiful) 
Aye anad?r. (Aye is a mother) 
 
The affirmative answer requires stepping through 
the implication translated to Prolog during the proc-
essing of the original user entry. The question 
?why?? is answered by translating the Prolog formu-
las used for the previous answer back to sentence 
form. 
Definite noun phrases are treated as anaphors: 
Canan k???k bir ?ocuk. (Canan is a small child) 
Teekk?rler, ?rendim. (Thanks for the information) 
?ocuk kahvalt?da zeytin yedi. (The child ate olives 
for breakfast) 
Teekk?rler, ?rendim. (Thanks for the information) 
Kim zeytin yedi? (Who ate olives?) 
Canan zeytin yedi. (Canan ate olives) 
Ka? kii kahvalt?da zeytin yedi? (How many people 
ate olives for breakfast?) 
Bir kii kahvalt?da zeytin yedi. (One person ate ol-
ives for breakfast) 
 
The definite noun phrase in the second user entry 
(?the child?) is matched to the most recently men-
tioned child. As mentioned earlier, question words 
have small Prolog programs corresponding to them. 
The answer extracted from the knowledge base is 
presented in the form of a grammatically correct 
sentence. (The fact that every child is also a person 
is one of the commonsense items that have been 
preencoded in the knowledge base.) 
A rudimentary capability of commonsense rea-
soning about time is implemented: The ?time? ar-
gument in verb predicates has a substructure with 
slots for the beginning and ending points of the in-
terval corresponding to the event. (In the present 
version, only a small subset of the verbal lexicon 
entries have their time subslots manually encoded 
for this purpose.) Hours are used as the unit interval. 
Kemal k???k bir ?ocuk. B?t?n k???k ?ocuklar 10 
saat uyurlar. (Kemal is a small child. All small chil-
dren sleep for 10 hours) 
Teekk?rler, ?rendim. (Thanks for the information) 
Kemal saat 23?te uyudu. (Kemal fell asleep at 23 
hours) 
Teekk?rler, ?rendim. (Thanks for the information) 
Kemal ne zaman uyudu? (When did Kemal fall 
asleep?)  
Kemal yirmi??te uyudu. (Kemal fell asleep at twenty 
three) 
Kemal ne zaman uyand?? (When did Kemal wake 
up?)  
Kemal dokuzda uyand?. (Kemal woke up at nine) 
 
Note that the program is able to do the ?modulo 
24? calculation required for producing the appropri-
ate answer. 
To find pronominal references in the absence of 
gender information, the semantic network is utilized. 
In the following excerpt, the pronoun ?o? 
(?he/she/it?) is correctly deduced to correspond to 
??ay? (?tea?), since the network does not allow 
?Kemal?, a human name, to be the agent of the word 
?bit-? (?to be consumed entirely?), which can have 
only inanimate material at that role. 
Kemal kahvalt?da ne i?ti? (What did Kemal drink for 
breakfast?) 
Bilmiyorum. (I do not know) 
Kemal ?ay i?ti ise o bitmitir. (If Kemal drank tea, 
(he/she/it) must have been consumed entirely) 
Teekk?rler, ?rendim. (Thanks for the information) 
Kemal ?ay i?ti. (Kemal drank tea) 
Teekk?rler, ?rendim.(Thanks for the information) 
?ay bitmi midir? (Has the tea been consumed en-
tirely?)  
Evet (Yes) 
 
The latest release of TOYagent (??n 2003) is 
able to manage conversations with multiple agents, 
can adapt different ?attitudes? about whether to be-
lieve what a user says depending on the user?s pro-
file, and has the capability of detecting and pointing 
out inconsistencies among the statements made by 
different users. This version also supports an op-
tional ?inquisitive? dialogue mode, where the com-
puter questions the user about the values of currently 
empty slots in the verb predicates corresponding to 
previous user statements. 
2.2     Turkish Natural Language Interface 
For SQL Queries (NALAN-TS) 
NALAN-TS (Maden, Demir and ?zcan 2003) is a 
Turkish natural language query interface for SQL 
databases, formed of a syntactic parser, semantic 
analyzer, meaning extractor, SQL constructor and 
executer. It is a dictionary based application and in-
cludes Turkish and database dictionaries. 
  
 
Figure 6. NALAN-TS Flow Diagram. 
 
The shaded modules in Figure 6 were taken com-
pletely from the TOY infrastructure, except for a 
few modifications like the addition of new Turkish 
syntax rules and a different format for the semantic 
representation of the words in the dictionary. TOY?s 
knowledge base interface is taken as the basis by 
NALAN-TS.  
2.3   Turkish Speaking Assistant -TUSA 
TUSA (eker, 2003) is a natural language interface 
for an online personal calendar. The morphological 
analyzer/generator of TOY was taken as a basis in 
this project with modifications made for utilizing.  
2.4   Generating Java Class Skeleton Using a 
Natural Language Interface- TUJA 
TUJA (?zcan, eker and Karadeniz 2004) is a natu-
ral language interface for generating Java source 
code and creating an object-oriented semantic net-
work. This program uses TOY?s morphological ana-
lyzer/generator as the starting point. 
2.5     Other Applications 
Ballhysa (2000) used TOY to produce a prototypical 
sentence-level translator between Albanian, English, 
and Turkish. (To our knowledge, this is the first 
NLP work ever done on Albanian) Dutaac? (2002) 
used the morphological component to tag a Turkish 
corpus of nearly ten million words to collect statis-
tics and compared the performance of an N-gram 
model of speech recognition based on morphemes 
with those based on words or syllables. Tekeli 
(2002) made use of the word-level components to 
build an ?ELIZA-like? (Covington 1994) dialogue 
program which caricaturizes Fatih Terim, a famous 
soccer coach and an idiosyncratic Turkish speaker. 
The program?s ?bag of tricks? includes coming up 
with rhyming responses to user sentences. Bilsel 
(2000) developed a ?poem expert? for analyzing 
Turkish folk poems for their rhyme and meter prop-
erties, a demanding task which is part of the high-
school curriculum in Turkey. 
Conclusion 
Our work on TOY is continuing on many fronts: The 
DCG component is currently being extended to 
cover both a bigger subset of Turkish syntax, and 
some types of agrammatical sentences. We hope that 
TOY will be useful in the development of many 
other applications in the near future. 
References 
Can Tekeli 2002. TERIM_SON. B.S. Thesis, Department 
of Computer Engineering, Bogazici University. 
Douglas Lenat 1995. CYC: A Large-Scale Investment in 
Knowledge Infrastructure. In ?Comm. ACM, 38/11?, 
pages 33-38. 
Eda Bilsel 2000. Poem Analyzer, B.S. Thesis, Department 
of Computer Engineering, Bogazici University. 
Elton Ballhysa 2000. Albanian-Turkish-English Transla-
tor. B.S. Thesis, Department of Computer Engineering, 
Bogazici University. 
Ender ?zcan, adi E. eker and Zeynep. I. Karadeniz 
2004. Generating Java Class Skeleton Using A Natural 
Language Interface. In ?First International Workshop 
on Natural Language Understanding and Cognitive Sci-
ence-NLUCS?. 
Fatih ??n 2003. Design and Implementation of an Im-
proved Conversational Agent Infrastructure for Turk-
ish. M.S. Thesis, Department of Computer Engineering, 
Bogazici University. 
Helin Dutaac? 2002. Statistical Language Models for 
Large Vocabulary Turkish Speech Recognition. M.S. 
Thesis, Department of Computer Engineering, Bogazici 
University.  
Ibrahim Maden, eniz Demir and Ender ?zcan 2003. 
Turkish Natural Language Interface for Generating 
SQL Queries. In ?TBD 20. Ulusal Biliim Kurultayi?. 
Kemal Oflazer 1993. Two-Level Description of Turkish 
Morphology. In ?Proc. Second Turkish Symposium on 
Artificial Intelligence and Neural Networks?, Bogazici 
University Press, pages 86-93, Istanbul.  
Michael A .Covington 1994. Natural Language Process-
ing for Prolog Programmers. Prentice Hall, Englewood 
Cliffs, NJ. 
?zlem ?etinolu 2001. A Prolog Based Natural Lan-
guage Processing Infrastructure for Turkish. M.S. The-
sis, Department of Computer Engineering, Bogazici 
University. 
Ronnie W. Smith 1994. Spoken Variable Initiative Dia-
log: An Adaptable Natural-Language Interface. In 
?IEEE Expert: Intelligent Systems and Their Applica-
tions 9/1?,pages 45-50. 
adi E. eker 2003.  Design and Implementation of a 
Personal Calendar with a Natural Language Interface 
in Turkish. M.S. Thesis, Department of Computer En-
gineering, Yeditepe University. 
eniz Demir 2003. Improved Treatment of Word Meaning  
in a Turkish Conversational Agent. M.S.     Thesis, De-
partment of Computer Engineering, Bogazici Univer-
sity. 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 153?160,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Morphology-Syntax Interface for Turkish LFG
?Ozlem C?etinog?lu
Faculty of Engineering and Natural Sciences
Sabanc? University
34956, Istanbul, Turkey
ozlemc@su.sabanciuniv.edu
Kemal Oflazer
Faculty of Engineering and Natural Sciences
Sabanc? University
34956, Istanbul, Turkey
oflazer@sabanciuniv.edu
Abstract
This paper investigates the use of sublexi-
cal units as a solution to handling the com-
plex morphology with productive deriva-
tional processes, in the development of
a lexical functional grammar for Turkish.
Such sublexical units make it possible to
expose the internal structure of words with
multiple derivations to the grammar rules
in a uniform manner. This in turn leads to
more succinct and manageable rules. Fur-
ther, the semantics of the derivations can
also be systematically reflected in a com-
positional way by constructing PRED val-
ues on the fly. We illustrate how we use
sublexical units for handling simple pro-
ductive derivational morphology and more
interesting cases such as causativization,
etc., which change verb valency. Our pri-
ority is to handle several linguistic phe-
nomena in order to observe the effects of
our approach on both the c-structure and
the f-structure representation, and gram-
mar writing, leaving the coverage and
evaluation issues aside for the moment.
1 Introduction
This paper presents highlights of a large scale lex-
ical functional grammar for Turkish that is being
developed in the context of the ParGram project1
In order to incorporate in a manageable way, the
complex morphology and the syntactic relations
mediated by morphological units, and to handle
lexical representations of very productive deriva-
tions, we have opted to develop the grammar using
sublexical units called inflectional groups.
Inflectional groups (IGs hereafter) represent the
inflectional properties of segments of a complex
1http://www2.parc.com/istl/groups/nltt/
pargram/
word structure separated by derivational bound-
aries. An IG is typically larger than a morpheme
but smaller than a word (except when the word has
no derivational morphology in which case the IG
corresponds to the word). It turns out that it is
the IGs that actually define syntactic relations be-
tween words. A grammar for Turkish that is based
on words as units would have to refer to informa-
tion encoded at arbitrary positions in words, mak-
ing the task of the grammar writer much harder.
On the other hand, treating morphemes as units in
the grammar level implies that the grammar will
have to know about morphotactics making either
the morphological analyzer redundant, or repeat-
ing the information in the morphological analyzer
at the grammar level which is not very desirable.
IGs bring a certain form of normalization to the
lexical representation of a language like Turkish,
so that units in which the grammar rules refer to
are simple enough to allow easy access to the in-
formation encoded in complex word structures.
That IGs delineate productive derivational pro-
cesses in words necessitates a mechanism that re-
flects the effect of the derivations to semantic rep-
resentations and valency changes. For instance,
English LFG (Kaplan and Bresnan, 1982) repre-
sents derivations as a part of the lexicon; both
happy and happiness are separately lexicalized.
Lexicalized representations of adjectives such as
easy and easier are related, so that both lexicalized
and phrasal comparatives would have the same
feature structure; easier would have the feature
structure
(1)  





PRED ?easy?
ADJUNCT

PRED ?more?

DEG-DIM pos
DEGREE comparative






Encoding derivations in the lexicon could be ap-
plicable for languages with relatively unproduc-
tive derivational phenomena, but it certainly is not
153
possible to represent in the grammar lexicon,2 all
derived forms as lexemes for an agglutinative lan-
guage like Turkish. Thus one needs to incorpo-
rate such derivational processes in a principled
way along with the computation of the effects on
derivations on the representation of the semantic
information.
Lexical functional grammar (LFG) (Kaplan and
Bresnan, 1982) is a theory representing the syn-
tax in two parallel levels: Constituent structures
(c-structures) have the form of context-free phrase
structure trees. Functional structures (f-structures)
are sets of pairs of attributes and values; attributes
may be features, such as tense and gender, or func-
tions, such as subject and object. C-structures de-
fine the syntactic representation and f-structures
define more semantic representation. Therefore
c-structures are more language specific whereas
f-structures of the same phrase for different lan-
guages are expected to be similar to each other.
The remainder of the paper is organized as fol-
lows: Section 2 reviews the related work both on
Turkish, and on issues similar to those addressed
in this paper. Section 3 motivates and presents IGs
while Section 4 explains how they are employed
in a LFG setting. Section 5 summarizes the ar-
chitecture and the current status of the our system.
Finally we give conclusions in Section 6.
2 Related Work
Gu?ngo?rdu? and Oflazer (1995) describes a rather
extensive grammar for Turkish using the LFG
formalism. Although this grammar had a good
coverage and handled phenomena such as free-
constituent order, the underlying implementation
was based on pseudo-unification. But most cru-
cially, it employed a rather standard approach to
represent the lexical units: words with multiple
nested derivations were represented with complex
nested feature structures where linguistically rel-
evant information could be embedded at unpre-
dictable depths which made access to them in rules
extremely complex and unwieldy.
Bozs?ahin (2002) employed morphemes overtly
as lexical units in a CCG framework to account
for a variety of linguistic phenomena in a pro-
totype implementation. The drawback was that
morphotactics was explicitly raised to the level of
the sentence grammar, hence the categorial lexi-
con accounted for both constituent order and the
morpheme order with no distinction. Oflazer?s de-
pendency parser (2003) used IGs as units between
which dependency relations were established. An-
other parser based on IGs is Eryig?it and Oflazer?s
2We use this term to distinguish the lexicon used by the
morphological analyzer.
(2006) statistical dependency parser for Turkish.
C?ak?c? (2005), used relations between IG-based
representations encoded within the Turkish Tree-
bank (Oflazer et al, 2003) to automatically induce
a CCG grammar lexicon for Turkish.
In a more general setting, Butt and King (2005)
have handled the morphological causative in Urdu
as a separate node in c-structure rules using LFG?s
restriction operator in semantic construction of
causatives. Their approach is quite similar to ours
yet differs in an important way: the rules explicitly
use morphemes as constituents so it is not clear if
this is just for this case, or all morphology is han-
dled at the syntax level.
3 Inflectional Groups as Sublexical Units
Turkish is an agglutinative language where a se-
quence of inflectional and derivational morphemes
get affixed to a root (Oflazer, 1994). At the syntax
level, the unmarked constituent order is SOV, but
constituent order may vary freely as demanded by
the discourse context. Essentially all constituent
orders are possible, especially at the main sen-
tence level, with very minimal formal constraints.
In written text however, the unmarked order is
dominant at both the main sentence and embedded
clause level.
Turkish morphotactics is quite complicated: a
given word form may involve multiple derivations
and the number of word forms one can generate
from a nominal or verbal root is theoretically in-
finite. Turkish words found in typical text aver-
age about 3-4 morphemes including the stem, with
an average of about 1.23 derivations per word,
but given that certain noninflecting function words
such as conjuctions, determiners, etc. are rather
frequent, this number is rather close to 2 for in-
flecting word classes. Statistics from the Turkish
Treebank indicate that for sentences ranging be-
tween 2 words to 40 words (with an average of
about 8 words), the number of IGs range from 2
to 55 IGs (with an average of 10 IGs per sentence)
(Eryig?it and Oflazer, 2006).
The morphological analysis of a word can be
represented as a sequence of tags corresponding
to the morphemes. In our morphological analyzer
output, the tag ?DB denotes derivation boundaries
that we also use to define IGs. If we represent the
morphological information in Turkish in the fol-
lowing general form:
root+IG
 
  DB+IG

  DB+        DB+IG
 
.
then each IG
 
denotes the relevant sequence of in-
flectional features including the part-of-speech for
the root (in IG
 
) and for any of the derived forms.
A given word may have multiple such representa-
tions depending on any morphological ambiguity
brought about by alternative segmentations of the
154
Figure 1: Modifier-head relations in the NP eski
kitaplar?mdaki hikayeler
word, and by ambiguous interpretations of mor-
phemes.
For instance, the morphological analysis of
the derived modifier cezaland?r?lacak (lit-
erally, ?(the one) that will be given punishment?)
would be :3
ceza(punishment)+Noun+A3sg+Pnon+Nom
?DB+Verb+Acquire
?DB+Verb+Caus
?DB+Verb+Pass+Pos
?DB+Adj+FutPart+Pnon
The five IGs in this word are:
1. +Noun+A3sg+Pnon+Nom
2. +Verb+Acquire
3. +Verb+Caus
4. +Verb+Pass+Pos
5. +Adj+FutPart+Pnon
The first IG indicates that the root is a singular
noun with nominative case marker and no posses-
sive marker. The second IG indicates a deriva-
tion into a verb whose semantics is ?to acquire?
the preceding noun. The third IG indicates that a
causative verb (equivalent to ?to punish? in En-
glish), is derived from the previous verb. The
fourth IG indicates the derivation of a passive verb
with positive polarity from the previous verb. Fi-
nally the last IG represents a derivation into future
participle which will function as a modifier in the
sentence.
The simple phrase eski kitaplar?mdaki hikayeler
(the stories in my old books) in Figure 1 will help
clarify how IGs are involved in syntactic relations:
Here, eski (old) modifies kitap (book) and not
hikayeler (stories),4 and the locative phrase eski
3The morphological features other than the obvious part-
of-speech features are: +A3sg: 3sg number-person agree-
ment, +Pnon: no possesive agreement, +Nom: Nominative
case, +Acquire: acquire verb, +Caus: causative verb,
+Pass: passive verb, +FutPart: Derived future participle,
+Pos: Positive Polarity.
4Though looking at just the last POS of the words one
sees an +Adj +Adj +Noun sequence which may imply
that both adjectives modify the noun hikayeler
kitaplar?mda (in my old books) modifies hikayeler
with the help of derivational suffix -ki. Morpheme
boundaries are represented by ?+? sign and mor-
phemes in solid boxes actually define one IG. The
dashed box around solid boxes is for word bound-
ary. As the example indicates, IGs may consist of
one or more morphemes.
Example (2) shows the corresponding f-
structure for this NP. Supporting the dependency
representation in Figure 1, f-structure of adjective
eski is placed as the adjunct of kitaplar?mda, at
the innermost level. The semantics of the relative
suffix -ki is shown as ?rel  OBJ? where the f-
structure that represents the NP eski kitaplar?mda
is the OBJ of the derived adjective. The new f-
structure with a PRED constructed on the fly, then
modifies the noun hikayeler. The derived adjective
behaves essentially like a lexical adjective. The ef-
fect of using IGs as the representative units can be
explicitly seen in c-structure where each IG cor-
responds to a separate node as in Example (3).5
Here, DS stands for derivational suffix.
(2)
 















PRED ?hikaye?
ADJUNCT
 









PRED ?rel kitap?
OBJ
 




PRED ?kitap?
ADJUNCT

PRED ?eski?
ATYPE attributive

CASE loc, NUM pl





ATYPE attributive










CASE NOM, NUM PL
















(3) NP
 
 
 
 




AP
 
 
 
 




NP






AP
A
eski
NP
N
kitaplar?mda
DS
ki
NP
N
hikayeler
Figure 2 shows the modifier-head relations for
a more complex example given in Example (4)
where we observe a chain/hierarchy of relations
between IGs
(4) mavi
blue
renkli
color-WITH
elbiselideki
dress-WITH-LOC-REL
kitap
book
5Note that placing the sublexical units of a word in sepa-
rate nodes goes against the Lexical Integrity principle of LFG
(Dalrymple, 2001). The issue is currently being discussed
within the LFG community (T. H. King, personal communi-
cation).
155
?the book on the one with the blue colored
dress?
Figure 2: Syntactic Relations in the NP mavi ren-
kli elbiselideki kitap
Examples (5) and (6) show respectively the con-
stituent structure (c-structure) and the correspond-
ing feature structure (f-structure) for this noun
phrase. Within the tree representation, each IG
corresponds to a separate node. Thus, the LFG
grammar rules constructing the c-structures are
coded using IGs as units of parsing. If an IG con-
tains the root morpheme of a word, then the node
corresponding to that IG is named as one of the
syntactic category symbols. The rest of the IGs
are given the node name DS (to indicate deriva-
tional suffix), no matter what the content of the IG
is.
The semantic representation of derivational suf-
fixes plays an important role in f-structure con-
struction. In almost all cases, each derivation that
is induced by an overt or a covert affix gets a OBJ
feature which is then unified with the f-structure of
the preceding stem already constructed, to obtain
the feature structure of the derived form, with the
PRED of the derived form being constructed on
the fly. A PRED feature thus constructed however
is not meant to necessarily have a precise lexical
semantics. Most derivational suffixes have a con-
sistent (lexical) semantics6, but some don?t, that
is, the precise additional lexical semantics that the
derivational suffix brings in, depends on the stem
it is affixed to. Nevertheless, we represent both
cases in the same manner, leaving the determina-
tion of the precise lexical semantics aside.
If we consider Figure 2 in terms of dependency
relations, the adjective mavi (blue) modifies the
noun renk (color) and then the derivational suf-
fix -li (with) kicks in although the -li is attached
to renk only. Therefore, the semantics of the
phrase should be with(blue color), not blue
with(color). With the approach we take, this
difference can easily be represented in both the f-
structure as in the leftmost branch in Example (5)
6e.g., the ?to acquire? example earlier
and the c-structure as in the middle ADJUNCT
f-structure in Example (6). Each DS in c-structure
gives rise to an OBJject in c-structure. More pre-
cisely, a derived phrase is always represented as
a binary tree where the right daughter is always
a DS. In f-structure DS unifies with the mother f-
structure and inserts PRED feature which subcat-
egorizes for a OBJ. The left daughter of the bi-
nary tree is the original form of the phrase that is
derived, and it unifies with the OBJ of the mother
f-structure.
(5)
NP






AP






NP






AP






NP






AP




NP




AP
A
mavi
NP
N
renk
DS
li
NP
N
elbise
DS
li
DS
de
DS
ki
NP
N
kitap
4 Inflectional Groups in Practice
We have already seen how the IGs are used to con-
struct on the fly PRED features that reflect the
lexical semantics of the derivation. In this section
we describe how we handle phenomena where the
derivational suffix in question does not explicitly
affect the semantic representation in PRED fea-
ture but determines the semantic role so as to unify
the derived form or its components with the appro-
priate external f-structure.
4.1 Sentential Complements and Adjuncts,
and Relative Clauses
In Turkish, sentential complements and adjuncts
are marked by productive verbal derivations into
nominals (infinitives, participles) or adverbials,
while relative clauses with subject and non-subject
(object or adjunct) gaps are formed by participles
which function as adjectivals modifying a head
noun.
Example (7) shows a simple sentence that will
be used in the following examples.
156
(6)  




































PRED ?kitap?
ADJUNCT
 






























PRED ?rel zero-deriv?
OBJ
 

























PRED ?zero-deriv with?
OBJ
 



















PRED ?with elbise?
OBJ
 














PRED ?elbise?
ADJUNCT
 








PRED ?with renk?
OBJ
 



PRED ?renk?
ADJUNCT

PRED ?mavi?

CASE nom, NUM sg, PERS 3




ATYPE attributive









CASE nom, NUM sg, PERS 3















ATYPE attributive




















CASE loc, NUM sg, PERS 3


























ATYPE attributive































CASE NOM, NUM SG, PERS 3





































(7) K?z
Girl-NOM
adam?
man-ACC
arad?.
call-PAST
?The girl called the man?
In (8), we see a past-participle form heading a
sentential complement functioning as an object for
the verb so?yledi (said).
(8) Manav
Grocer-NOM
k?z?n
girl-GEN
adam?
man-ACC
arad?g??n?
call-PASTPART-ACC
so?yledi.
say-PAST
?The grocer said that the girl called the man?
Once the grammar encounters such a sentential
complement, everything up to the participle IG is
parsed, as a normal sentence and then the partici-
ple IG appends nominal features, e.g., CASE, to
the existing f-structure. The final f-structure is for
a noun phrase, which now is the object of the ma-
trix verb, as shown in Example (9). Since the par-
ticiple IG has the right set of syntactic features of
a noun, no new rules are needed to incorporate the
derived f-structure to the rest of the grammar, that
is, the derived phrase can be used as if it is a sim-
ple NP within the rules. The same mechanism is
used for all kinds of verbal derivations into infini-
tives, adverbial adjuncts, including those deriva-
tions encoded by lexical reduplications identified
by multi-word construct processors.
(9)  






























PRED ?so?yle manav, ara?
SUBJ

PRED ?manav?
CASE nom, NUM sg, PERS 3

OBJ
 















PRED ?ara k z, adam?
SUBJ

PRED ?k z?
CASE gen, NUM sg, PERS 3

OBJ

PRED ?adam?
CASE acc, NUM sg, PERS 3

CHECK

PART pastpart
	
CASE acc, NUM sg, PERS 3, VTYPE main
CLAUSE-TYPE nom
















TNS-ASP

TENSE past
	
NUM SG, PERS 3, VTYPE MAIN































Relative clauses also admit to a similar mech-
anism. Relative clauses in Turkish are gapped
sentences which function as modifiers of nominal
heads. Turkish relative clauses have been previ-
ously studied (Barker et al, 1990; Gu?ngo?rdu? and
Engdahl, 1998) and found to pose interesting is-
sues for linguistic and computational modeling.
Our aim here is not to address this problem in its
generality but show with a simple example, how
our treatment of IGs encoding derived forms han-
dle the mechanics of generating f-structures for
such cases.
Kaplan and Zaenen (1988) have suggested a
general approach for handling long distance de-
pendencies. They have extended the LFG notation
and allowed regular expressions in place of sim-
ple attributes within f-structure constraints so that
phenomena requiring infinite disjunctive enumer-
ation can be described with a finite expression. We
basically follow this approach and once we derive
the participle phrase we unify it with the appro-
priate argument of the verb using rules based on
functional uncertainty. Example (10) shows a rel-
ative clause where a participle form is used as a
modifier of a head noun, adam in this case.
(10) Manav?n
Grocer-GEN
k?z?n
girl-GEN
[]

obj-gap
arad?g??n?
call-PASTPART-ACC
so?yledig?i
say-PASTPART
adam

man-NOM
?The man the grocer said the girl called?
This time, the sentence is parsed with a gap with
an appropriate functional uncertainty constraint,
and when the participle IG is encountered the sen-
tence f-structure is derived into an adjective and
the gap in the derived form, the object here, is
then unified with the head word as marked with
co-indexation in Example (11).
The example sentence (10) includes Example
(8) as a relative clause with the object extracted,
hence the similarity in the f-structures can be ob-
served easily. The ADJUNCT in Example (11)
157
is almost the same as the whole f-structure of Ex-
ample (9), differing in TNS-ASP and ADJUNCT-
TYPE features. At the grammar level, both the rel-
ative clause and the complete sentence is parsed
with the same core sentence rule. To understand
whether the core sentence is a complete sentence
or not, the finite verb requirement is checked.
Since the requirement is met by the existence of
TENSE feature, Example (8) is parsed as a com-
plete sentence. Indeed the relative clause also in-
cludes temporal information as ?pastpart? value of
PART feature, of the ADJUNCT f-structure, de-
noting a past event.
(11)  





































PRED ?adam?  
ADJUNCT
 































PRED ?so?yle manav, ara?
SUBJ

PRED ?manav?
CASE gen, NUM sg, PERS 3
	
OBJ
 














PRED ?ara kz, adam?
SUBJ

PRED ?kz?
CASE gen, NUM sg, PERS 3
	
OBJ

PRED ?adam?

 
CHECK

PART pastpart

CASE acc, NUM sg, PERS 3, VTYPE main
CLAUSE-TYPE nom















CHECK

PART pastpart

NUM sg, PERS 3, VTYPE main
ADJUNCT-TYPE relative
































CASE NOM, NUM SG, PERS 3






































4.2 Causatives
Turkish verbal morphotactics allows the produc-
tion multiply causative forms for verbs.7 Such
verb formations are also treated as verbal deriva-
tions and hence define IGs. For instance, the mor-
phological analysis for the verb arad? (s/he called)
is
ara+Verb+Pos+Past+A3sg
and for its causative aratt? (s/he made (someone
else) call) the analysis is
ara+Verb?DB+Verb+Caus+Pos+Past+A3sg.
In Example (12) we see a sentence and its
causative form followed by respective f-structures
for these sentences in Examples (13) and (14). The
detailed morphological analyses of the verbs are
given to emphasize the morphosyntactic relation
between the bare and causatived versions of the
verb.
(12) a. K?z
Girl-NOM
adam?
man-ACC
arad?.
call-PAST
?The girl called the man?
b. Manav
Grocer-NOM
k?za
girl-DAT
adam?
man-ACC
aratt?.
call-CAUS-PAST
?The grocer made the girl call the man?
7Passive, reflexive, reciprocal/collective verb formations
are also handled in morphology, though the latter two are not
productive due to semantic constraints. On the other hand
it is possible for a verb to have multiple causative markers,
though in practice 2-3 seem to be the maximum observed.
(13)  













PRED ?ara k z, adam?
SUBJ

PRED ?k z?
CASE nom, NUM sg, PERS 3

OBJ

PRED ?adam?
CASE acc, NUM sg, PERS 3

TNS-ASP

TENSE past
	
NUM SG, PERS 3,VTYPE MAIN














(14)  






























PRED ?caus manav, k z, adam, ara k z , adam?
SUBJ

PRED ?manav?
	
OBJ

PRED ?k z?
	
 
OBJTH

PRED ?adam?
	

XCOMP
 









PRED ?ara k z , adam?
SUBJ

PRED ?k z?
CASE dat, NUM sg, PERS 3

 
OBJ

PRED ?adam?
CASE acc, NUM sg, PERS 3


VTYPE main










TNS-ASP

TENSE past
	
NUM SG, PERS 3,VTYPE MAIN































The end-result of processing an IG which has a
verb with a causative form is to create a larger f-
structure whose PRED feature has a SUBJect, an
OBJect and a XCOMPlement. The f-structure of
the first verb is the complement in the f-structure
of the causative form, that is, its whole structure is
embedded into the mother f-structure in an encap-
sulated way. The object of the causative (causee
- that who is caused by the causer ? the sub-
ject of the causative verb) is unified with the sub-
ject the inner f-structure. If the original verb is
transitive, the object of the original verb is fur-
ther unified with the OBJTH of the causative
verb. All of grammatical functions in the inner
f-structure, namely XCOMP, are also represented
in the mother f-structure and are placed as argu-
ments of caus since the flat representation is re-
quired to enable free word order in sentence level.
Though not explicit in the sample f-structures,
the important part is unifying the object and for-
mer subject with appropriate case markers, since
the functions of the phrases in the sentence are de-
cided with the help of case markers due to free
word order. If the verb that is causativized sub-
categorizes for an direct object in accusative case,
after causative formation, the new object unified
with the subject of the causativized verb should
be in dative case (Example 15). But if the verb
in question subcategorizes for a dative or an abla-
tive oblique object, then this object will be trans-
formed into a direct object in accusative case after
causativization (Example 16). That is, the causati-
vation will select the case of the object of the
causative verb, so as not to ?interfere? with the ob-
ject of the verb that is causativized. In causativized
intransitive verbs the causative object is always in
accusative case.
158
(15) a. adam
man-NOM
kad?n?
woman-ACC
arad?.
call-PAST
?the man called the woman?
b. adama
man-DAT
kad?n?
woman-ACC
aratt?.
call-CAUS-PAST
?(s/he) made the man call the woman?
(16) a. adam
man-NOM
kad?na
woman-DAT
vurdu.
hit-PAST
?the man hit the woman?
b. adam?
man-ACC
kad?na
woman-DAT
vurdurdu.
hit-CAUS-PAST
?(s/he) made the man hit the woman?
All other derivational phenomena can be solved in
a similar way by establishing the appropriate se-
mantic representation for the derived IG and its
effect on the semantic representation.
5 Current Implementation
The implementation of the Turkish LFG gram-
mar is based on the Xerox Linguistic Environ-
ment (XLE) (Maxwell III and Kaplan, 1996), a
grammar development platform that facilitates the
integration of various modules, such as tokeniz-
ers, finite-state morphological analyzers, and lex-
icons. We have integrated into XLE, a series of
finite state transducers for morphological analysis
and for multi-word processing for handling lexi-
calized, semi-lexicalized collocations and a lim-
ited form of non-lexicalized collocations.
The finite state modules provide the rele-
vant ambiguous morphological interpretations for
words and their split into IGs, but do not provide
syntactically relevant semantic and subcategoriza-
tion information for root words. Such information
is encoded in a lexicon of root words on the gram-
mar side.
The grammar developed so far addresses many
important aspects ranging from free constituent or-
der, subject and non-subject extractions, all kinds
of subordinate clauses mediated by derivational
morphology and has a very wide coverage NP sub-
grammar. As we have also emphasized earlier, the
actual grammar rules are oblivious to the source of
the IGs, so that the same rule handles an adjective
- noun phrase regardless of whether the adjective
is lexical or a derived one. So all such relations in
Figure 28 are handled with the same phrase struc-
ture rule.
The grammar is however lacking the treatment
of certain interesting features of Turkish such as
suspended affixation (Kabak, 2007) in which the
inflectional features of the last element in a co-
ordination have a phrasal scope, that is, all other
8Except the last one which requires some additional treat-
ment with respect to definiteness.
coordinated constituents have certain default fea-
tures which are then ?overridden? by the features
of the last element in the coordination. A very sim-
ple case of such suspended affixation is exempli-
fied in (17a) and (17b). Note that although this is
not due to derivational morphology that we have
emphasized in the previous examples, it is due to
a more general nature of morphology in which af-
fixes may have phrasal scopes.
(17) a. k?z
girl
adam
man-NOM
ve
and
kad?n?
woman-ACC
arad?.
call-PAST
?the girl called the man and the woman?
b. k?z
girl
[adam
[man
ve
and
kad?n]-?
woman]-ACC
arad?.
call-PAST
?the girl called the man and the woman?
Suspended affixation is an example of a phe-
nomenon that IGs do not seem directly suitable
for. The unification of the coordinated IGs have to
be done in a way in which non-default features of
the final constituent is percolated to the upper node
in the tree as is usually done with phrase struc-
ture grammars but unlike coordination is handled
in such grammars.
6 Conclusions and Future Work
This paper has described the highlights of our
work on developing a LFG grammar for Turkish
employing sublexical constituents, that we have
called inflectional groups. Such a sublexical con-
stituent choice has enabled us to handle the very
productive derivational morphology in Turkish in
a rather principled way and has made the grammar
more or less oblivious to morphological complex-
ity.
Our current and future work involves extending
the coverage of the grammar and lexicon as we
have so far included in the grammar lexicon only
a small subset of the root lexicon of the morpho-
logical analyzer, annotated with the semantic and
subcategorization features relevant to the linguis-
tic phenomena that we have handled. We also in-
tend to use the Turkish Treebank (Oflazer et al,
2003), as a resource to extract statistical informa-
tion along the lines of Frank et al (2003) and
O?Donovan et al (2005).
Acknowledgement
This work is supported by TUBITAK (The Scien-
tific and Technical Research Council of Turkey)
by grant 105E021.
159
References
Chris Barker, Jorge Hankamer, and John Moore, 1990.
Grammatical Relations, chapter Wa and Ga in Turk-
ish. CSLI.
Cem Bozs?ahin. 2002. The combinatory morphemic
lexicon. Computational Linguistics, 28(2):145?186.
Miriam Butt and Tracey Holloway King. 2005.
Restriction for morphological valency alternations:
The Urdu causative. In Proceedings of The 10th
International LFG Conference, Bergen, Norway.
CSLI Publications.
Ruken C?ak?c?. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of the ACL
Student Research Workshop, pages 73?78, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Mary Dalrymple. 2001. Lexical Functional Gram-
mar, volume 34 of Syntax and Semantics. Academic
Press, New York.
Gu?ls?en Eryig?it and Kemal Oflazer. 2006. Statisti-
cal dependency parsing for turkish. In Proceedings
of EACL 2006 - The 11th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Trento, Italy. Association for Computa-
tional Linguistics.
Anette Frank, Louisa Sadler, Josef van Genabith, and
Andy Way. 2003. From treebank resources to LFG
f-structures:automatic f-structure annotation of tree-
bank trees and CFGs extracted from treebanks. In
Anne Abeille, editor, Treebanks. Kluwer Academic
Publishers, Dordrecht.
Zelal Gu?ngo?rdu? and Elisabeth Engdahl. 1998. A rela-
tional approach to relativization in Turkish. In Joint
Conference on Formal Grammar, HPSG and Cate-
gorial Grammar, Saarbru?cken, Germany, August.
Zelal Gu?ngo?rdu? and Kemal Oflazer. 1995. Parsing
Turkish using the Lexical Functional Grammar for-
malism. Machine Translation, 10(4):515?544.
Bar?s? Kabak. 2007. Turkish suspended affixation. Lin-
guistics, 45. (to appear).
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In Joan Bresnan, editor, The
Mental Representation of Grammatical Relations,
pages 173?281. MIT Press, Cambridge, MA.
Ronald M. Kaplan and Annie Zaenen. 1988. Long-
distance dependencies, constituent structure, and
functional uncertainty. In M. Baitin and A. Kroch,
editors, Alternative Conceptions of Phrase Struc-
ture. University of Chicago Press, Chicago.
John T. Maxwell III and Ronald M. Kaplan. 1996.
An efficient parser for LFG. In Miriam Butt and
Tracy Holloway King, editors, The Proceedings of
the LFG ?96 Conference, Rank Xerox, Grenoble.
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef
van Genabith, and Andy Way. 2005. Large-scale
induction and evaluation of lexical resources from
the Penn-II and Penn-III Treebanks. Computational
Linguistics, 31(3):329?365.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r,
and Go?khan Tu?r. 2003. Building a Turkish tree-
bank. In Anne Abeille, editor, Building and Exploit-
ing Syntactically-annotated Corpora. Kluwer Aca-
demic Publishers.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2):137?148.
Kemal Oflazer. 2003. Dependency parsing with an
extended finite-state approach. Computational Lin-
guistics, 29(4):515?544.
160
Integrating Morphology with Multi-word Expression Processing in Turkish
Kemal Oflazer and ?zlem ?etinog?lu
Human Language and Speech Technology Laboratory
Sabanc University
Istanbul, Turkey
{oflazer,ozlemc}@sabanciuniv.edu
Bilge Say
Informatics Institute
Middle East Technical University
Ankara, Turkey
bsay@ii.metu.edu.tr
Abstract
This paper describes a multi-word expression pro-
cessor for preprocessing Turkish text for various
language engineering applications. In addition to
the fairly standard set of lexicalized collocations
and multi-word expressions such as named-entities,
Turkish uses a quite wide range of semi-lexicalized
and non-lexicalized collocations. After an overview
of relevant aspects of Turkish, we present a descrip-
tion of the multi-word expressions we handle. We
then summarize the computational setting in which
we employ a series of components for tokenization,
morphological analysis, and multi-word expression
extraction. We finally present results from runs over
a large corpus and a small gold-standard corpus.
1 Introduction
Multi-word expression extraction is an important
component in language processing that aims to
identify segments of input text where the syntactic
structure and the semantics of a sequence of words
(possibly not contiguous) are usually not composi-
tional. Idiomatic forms, support verbs, verbs with
specific particle or pre/post-position uses, morpho-
logical derivations via partial or full word duplica-
tions are some examples of multi-word expressions.
Further, expressions such as time-date expressions
or proper nouns which can be described with sim-
ple (usually finite state) grammars, and whose inter-
nal structure is of no real importance to the overall
analysis of the sentence, can also be considered un-
der this heading. Marking multi-word expressions
in text usually reduces (though not significantly)
the number of actual tokens that further processing
modules use as input, although this reduction may
depend on the domain the text comes from. It can
also reduce the multiplicative ambiguity as morpho-
logical interpretations of tokens are reduced when
they are coalesced into multi-word expressions with
usually a single interpretation.
Turkish presents some interesting issues for multi-
word expression processing as it makes substan-
tial use of support verbs with lexicalized direct or
oblique objects subject to various morphological
constraints. It also uses partial and full reduplica-
tion of forms of various parts-of-speech, across their
whole domain to form what we call non-lexicalized
collocations, where it is the duplication and contrast
of certain morphological patterns that signal a col-
location rather than the specific root words used.
In this paper, we describe a multi-word expression
processor for preprocessing Turkish text for vari-
ous language engineering applications. In the next
section after a very short overview of relevant as-
pects of Turkish, we present a rather comprehen-
sive description of the multi-word expressions we
handle. We then summarize the structure of the
multi-word expression processor which employs a
series of components for tokenization, morpholog-
ical analysis, conservative non-statistical morpho-
logical disambiguation, and multi-word expression
extraction. We finally present results from runs over
a large corpus and a small gold-standard corpus.
1.1 Related Work
Recent work on multi-word expression extraction,
use three basic approaches: statistical, rule-based,
and hybrid. Statistical approaches require a corpus
that contains significant numbers of occurrences of
multi-word expressions. But even if the corpus con-
sists of millions of words, usually, the frequencies
of multi-word expressions are too low for statisti-
cal extraction. Baldwin and Villavicencio (2002)
indicate that ?two-thirds of verb-particle construc-
tions occur at most three times in the overall corpus,
meaning that any extraction method must be able
to handle extremely sparse data.? They use a rule-
based method to extract multi-word expressions in
the form of a head verb and a single obligatory
preposition employing a tagger augmented with an
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 64-71
existing chunking system with which they first iden-
tify the particle chunked and then turn back for the
verb part of the construction.
Piao et al (2003) employ their semantic field an-
notator USAS, containing 37,000 words and a tem-
plate list of 16,000 multi-word units, all constructed
manually from various resources, in order to extract
multi-word expressions. The evaluation indicates a
high precision (over 90%) but the estimated recall is
about 40%. Deeper investigation on the corpus has
indicated that two-thirds of the multi-word expres-
sions occur in the corpus once or twice, verifying
the fact that the statistical methods filtering low fre-
quencies would fail.
Urizar et al (2000) describe a Basque terminol-
ogy extraction system which covered multi-word
term extraction as a subset. As Basque is a highly
inflected agglutinative language like Turkish, mor-
phological information is exploited to better define
multi-word patterns. Their lemmatizer/tagger EU-
SLEM, consists of a tokenizer followed by two sub-
systems for the treatment of single word and multi-
word expressions, and a disambiguator. The pro-
posed term extraction tool uses the tagged input as
the input of a shallow parsing phase which consists
of regular expressions representing morphosyntac-
tic patterns. The final step uses statistical measures
to eliminate incorrect candidates.
The basic disadvantages of rule-based approaches
are that they usually lack flexibility, and it is a
time-consuming and never ending process to try to
cover a high percentage of the multi-word expres-
sions in a language with rules and predefined lists.
The LINGO group which defines multi-word ex-
pressions as ?a pain in the neck for NLP? (Sag et al,
2002), suggests hybrid approaches using rule based
approaches to identify possible multi-word expres-
sions out of a corpus and using statistical methods
to enhance the results obtained.
2 Multi-word expressions in Turkish
Turkish is an Ural-Altaic language, having aggluti-
native word structures with productive inflectional
and derivational processes. Most derivational phe-
nomena take place within a word form, but there are
certain derivations involving partial or full redupli-
cations that are best considered under the notion of
multi-word expressions.
Turkish word forms consist of morphemes concate-
nated to a root morpheme or to other morphemes,
much like beads on a string. Except for a very
few exceptional cases, the surface realizations of
the morphemes are conditioned by various morpho-
phonemic processes such as vowel harmony, vowel
and consonant elisions. The morphotactics of word
forms can be quite complex when multiple deriva-
tions are involved. For instance, the derived mod-
ifier sag?lamlas?t?rd?g??m?zdaki1 would be
represented as:2
saglam+Adj
?DB+Verb+Become
?DB+Verb+Caus+Pos
?DB+Adj+PastPart+P1sg
?DB+Noun+Zero+A3sg+Pnon+Loc
?DB+Adj
This word starts out with an adjective root and af-
ter five derivations, ends up with the final part-of-
speech adjective which determines its role in the
sentence.
Turkish employs multi-word expressions in essen-
tially four different forms:
1. Lexicalized Collocations where all compo-
nents of the collocations are fixed,
2. Semi-lexicalized Collocations where some
components of the collocation are fixed and
some can vary via inflectional and derivational
morphology processes and the (lexical) seman-
tics of the collocation is not compositional,
3. Non-lexicalized Collocations where the collo-
cation is mediated by a morphosyntactic pat-
tern of duplicated and/or contrasting compo-
nents ? hence the name non-lexicalized, and
4. Multi-word Named-entities which are multi-
word proper names for persons, organizations,
places, etc.
2.1 Lexicalized Collocations
Under the notion of lexicalized collocations, we
consider the usual fixed multi-word expressions
1Literally, ?(the thing existing) at the time we caused (some-
thing) to become strong?. Obviously this is not a word that one
would use everyday. Turkish words (excluding noninflecting
frequent words such as conjunctions, clitics, etc.) found in typ-
ical text average about 10 letters in length.
2Please refer to the list of morphological features given in
Appendix A for the semantics of some of the non-obvious sym-
bols used here.
whose resulting syntactic function and semantics
are not readily predictable from the structure and
the morphological properties of the constituents.
Here are some examples of the multi-word expres-
sions that we consider under this grouping:3 ,4
(1) hi? olmazsa
? hi?(never)+Adverb
ol(be)+Verb+Neg+Aor+Cond+A3sg
? hi?_olmazsa+Adverb
?at least? (literally ?if it never is?)
(2) ipe sapa gelmez
? ip(rope)+Noun+A3sg+Pnon+Dat
sap(handle)+Noun+A3sg+Pnon+Dat
gel(come)+Verb+Neg+Aor+A3sg
? ipe_sapa_gelmez+Adj
?worthless? (literally ?(he) does not come to rope
and handle?)
2.2 Semi-lexicalized Collocations
Multi-word expressions that are considered under
this heading are compound and support verb forma-
tions where there are two or more lexical items the
last of which is a verb or is a derivation involving
a verb. These are formed by a lexically adjacent,
direct or oblique object, and a verb, which for the
purposes of syntactic analysis, may be considered
as single lexical item: e.g., sayg dur- (literally to
stand (in) respect ? to pay respect), kafay ye- (lit-
erally to eat the head ? to get mentally deranged),
etc.5 Even though the other components can them-
selves be inflected, they can be assumed to be fixed
for the purposes of the collocation, and the colloca-
tion assumes its morphosyntactic features from the
last verb which itself may undergo any morpholog-
ical derivation or inflection process. For instance in
(3) kafay ye-
? kafa(head)+Noun+A3sg+Pnon+Acc
ye(eat)+Verb...
3In every group we first list the morphological features of
all the tokens, one on every line (with the glosses for the roots),
and then provide the morphological features of the multi-word
construct and then provide glosses and literal meanings.
4Please refer to the list of morphological features given in
Appendix A for the semantics of some of the non-obvious sym-
bols used here.
5Here we just show the roots of the verb with - denoting the
rest of the suffixes for any inflectional and derivational markers.
? kafay_ye+Verb...
?get mentally deranged? ( literally ?eat the head?)
the first part of the collocation, the accusative
marked noun kafay, is the fixed part and the part
starting with the verb ye- is the variable part which
may be inflected and/or derived in myriads of ways.
For example the following are some possible forms
of the collocation:
? kafay? yedim ?I got mentally deranged?
? kafay? yiyeceklerdi ?they were about to get
mentally deranged?
? kafay? yiyenler ?those who got mentally de-
ranged?
? kafay? yedi

gi ?the fact that (s/he) got mentally
deranged?
Under certain circumstances, the ?fixed? part may
actually vary in a rather controlled manner subject
to certain morphosyntactic constraints, as in the id-
iomatic verb:
(4) kafa(y) ?ek-
? kafa(head)+Noun+A3sg+Pnon+Acc
?ek(pull)+Verb...
? kafa_?ek+Verb...
?consume alcohol? (but literally ?to pull the head?)
(5) kafalar ?ek-
? kafa+Noun+A3pl+Pnon+Acc
?ek+Verb...
? kafa_?ek+Verb...
?consume alcohol? (but literally ?to pull the
heads?)
where the fixed part can be in the nominative or the
accusative case, and if it is in the accusative case, it
may be marked plural, in which case the verb has to
have some kind of plural agreement (i.e., first, sec-
ond or third person plural), but no possessive agree-
ment markers are allowed.
In their simplest forms, it is sufficient to recognize
a sequence of tokens one of whose morphologi-
cal analyses matches the corresponding pattern, and
then coalesce these into a single multi-word expres-
sion representation. However, some or all variants
of these and similar semi-lexicalized collocations
present further complications brought about by the
relative freeness of the constituent order in Turkish,
and by the interaction of various clitics with such
collocations.6
When such multi-word expressions are coalesced
into a single morphological entity, the ambiguity in
morphological interpretation is reduced as we see in
the following example:
(6) devam etti
? devam(continuation)+Noun+A3sg
+Pnon+Nom
*deva(therapy)+Noun+A3sg+P1sg+Nom
et(make)+Verb+Pos+Past+A3sg
*et(meat)+Noun+A3sg+Pnon+Nom
?DB+Verb+Past+A3sg
? devam_et+Verb+Pos+Past+A3sg
?(he) continued? (literally ?made a continuation?)
Here, when this semi-lexicalized collocation is rec-
ognized, other morphological interpretations of the
components (marked with a * above) can safely be
removed, contributing to overall morphological am-
biguity reduction.
2.3 Non-lexicalized Collocations
Turkish employs quite a number of non-lexicalized
collocations where the sentential role of the collo-
cation has (almost) nothing to do with the parts-of-
speech and the morphological features of the indi-
vidual forms involved. Almost all of these colloca-
tions involve partial or full duplications of the forms
involved and can actually be viewed as morphologi-
cal derivational processes mediated by reduplication
across multiple tokens.
The morphological feature representations of such
multi-word expressions follow one of the patterns:
1) ? ?
2) ? Z ?,
3) ? + X ? + Y
4) ?1 + X ?2 + X
where ? is the duplicated string comprising the
root, its part-of-speech and possibly some additional
morphological features encoded by any suffixes. X
and Y are further duplicated or contrasted morpho-
logical patterns and Z is a certain clitic token. In
6The question and the emphasis clitics which are written as
separate tokens, can occasionally intervene between the com-
ponents of a semi-lexicalized collocation. We omit the details
of these due to space restrictions.
duplications of type 4, it is possible that ?1 is dif-
ferent from ?2.
Below we present list of the more interesting non-
lexicalized expressions along with some examples
and issues.
? When a noun appears in duplicate following the
first pattern above, the collocation behaves like a
manner adverb, modifying a verb usually to the
right. Although this pattern does not necessarily
occur with every possible noun, it may occur with
many (countable) nouns without much of a further
semantic restriction. Such a sequence has to be co-
alesced into a representation indicating this deriva-
tional process as we see below.
(7) ev ev (? ?)
? ev(house)+Noun+A3sg+Pnon+Nom
ev+Noun+A3sg+Pnon+Nom
? ev+Noun+A3sg+Pnon+Nom?DB+Adverb+By
?house by house? (literally ?house house?)
? When an adjective appears in duplicate, the col-
location behaves like a manner adverb (with the se-
mantics of -ly adverbs in English), modifying a verb
usually to the right. Thus such a sequence has to
be coalesced into a representation indicating this
derivational process.
(8) yava?s yava?s (? ?)
? yava?s(slow)+Adj
yava?s+Adj
? yava?s+Adj?DB+Adverb+Ly
?slowly? (literally ?slow slow?)
This kind of duplication can also occur when the
adjective is a derived adjective as in
(9) hzl hzl (? ?)
? hz(speed)+Noun+A3sg+Pnon+Nom
?DB+Adj+With
hz+Noun+A3sg+Pnon+Nom
?DB+Adj+With
? hz+Noun+A3sg+Pnon+Nom
?DB+Adj+With?DB+Adverb+Ly
?rapidly? (literally ?with-speed with-speed?)
? Turkish has a fairly large set of onomatopoeic
words which always appear in duplicate and func-
tion as manner adverbs. The words by themselves
have no other usage and literal meaning, and mildly
resemble sounds produced by natural or artificial
objects. In these cases, the root word almost al-
ways is reduplicated but need not be, but both words
should be of the part-of-speech category +Dup that
we use to mark such roots.
(10) harl hurul (?1 + X ?2 + X )
? harl+Dup
hurul+Dup
? harl_hurul+Adverb+Resemble
?making rough noises? (no literal meaning)
? Duplicated verbs with optative mood and third
person singular agreement function as manner ad-
verbs, indicating that another verb is executed in a
manner indicated by the duplicated verb:
(11) ko?sa ko?sa (? ?)
? ko?s(run)+Verb+Pos+Opt+A3sg
ko?s(run)+Verb+Pos+Opt+A3sg
? ko?s+Verb+Pos+?DB+Adverb+ByDoingSo
?by running? (literally ?let him run let him run?)
? Duplicated verbs in aorist mood with third person
agreement and first positive then negative polarity,
function as temporal adverbs with the semantics ?as
soon as one has verbed?
(12) uyur uyumaz (? + X ? + Y )
? uyu+Verb+Pos+Aor+A3sg
uyu+Verb+Neg+Aor+A3sg
? uyu+Verb+Pos+?DB+Adverb+AsSoonAs
?as soon as (he) sleeps? ( literally ?(he) sleeps (he)
does not sleep?)
It should be noted that for most of the non-
lexicalized collocations involving verbs (like (11)
and (12) above), the verbal portion before the in-
flectional marking mood can have additional deriva-
tional markers and all such markers have to dupli-
cate.
(13) sa

glamla?strr sa

glamla?strmaz (?+X ?+Y )
? saglam+Adj?DB+Verb+Become
?DB+Verb+Caus?DB+Verb+Pos+Aor+A3sg
saglam+Adj?DB+Verb+Become
?DB+Verb+Caus?DB+Verb+Neg+Aor+A3sg
? saglam+Adj?DB+Verb+Become+
?DB+Verb+Caus+Pos
?DB+Adverb+AsSoonAs
?as soon as (he) fortifies (causes to become strong)?
Another interesting point is that non-lexicalized col-
locations can interact with semi-lexicalized collo-
cations since they both usually involve verbs. For
instance, when the verb of the semi-lexicalized col-
location example in (5) is duplicated in the form of
the non-lexicalized collocation in (12), we get
(14) kafalar? ?eker ?ekmez
In this case, first the non-lexicalized collocation has
to be coalesced into
(15) kafalar? ?ek+Verb+Pos
?DB+Adverb+AsSoonAs
and then the semi-lexicalized collocation kicks in,
to give
(16) kafa_?ek+Verb+Pos
?DB+Adverb+AsSoonAs
(?as soon as (we/you/they) get drunk?)
Finally, the following non-lexicalized collocation
involving adjectival forms involving duplication and
a question clitic is an example of the last type of
non-lexicalized collocation.
(17) g?zel mi g?zel (? Z ?)
? g?zel+Adj
mi+Ques
g?zel+Adj
? g?zel+Adj+Very
?very beautiful? (literally ?beautiful (is it?) beauti-
ful?)
2.4 Named-entities
Another class of multi-word expressions that we
process is the class of multi-word named-entities
denoting persons, organizations and locations. We
essentially treat these just like the semi-lexicalized
collocation discussed earlier, in that, when such
named-entities are used in text, all but the last com-
ponent are fixed and the last component will usually
undergo certain morphological processes demanded
by the syntactic context as in
Figure 1: The architecture of the multi-word expres-
sion extraction processor
(18) T?rkiye B?y?k Millet Meclisi?nde ....7
Here, the last component is case marked and this
represents a case marking on the whole named-
entity. We package this as
(19) T?rkiye_B?y?k_Millet_Meclisi
+Noun+Prop+A3sg+Pnon+Loc
To recognize these named entities we use a rather
simple approach employing a rather extensive
database of person, organization and place names,
developed in the context of a previous project, in-
stead of using a more sophisticated named-entity
extraction scheme.
3 The Structure of the Multi-word
Expression Processor
Our multi-word expression processor is a multi-
stage system as depicted in Figure 1. The first
component is a standard tokenizer which splits in-
put text into constituent tokens. These then go into
7In the Turkish Grand National Assembly.
a wide-coverage morphological analyzer (Oflazer,
1994) implemented using Xerox finite state technol-
ogy (Karttunen et al, 1997), which generates, for all
tokens, all possible morphological analyses. This
module also performs unknown processing by pos-
tulating possible noun roots and then trying to parse
the rest of a word as a sequence of possible Turk-
ish suffixes. The morphological analysis stage also
performs a very conservative non-statistical mor-
phological disambiguation to remove some very un-
likely parses based on unambiguous contexts. Fig-
ure 2 shows a sample Turkish text that comes out of
morphological processing, about to go into multi-
word expression extraction.
Kistin kist+Noun+A3sg+P2sg+Nom
kist+Noun+A3sg+Pnon+Gen
saglgm saglk+Noun+A3sg+P1sg+Acc
sag+Adj?DB+Noun+Ness+
A3sg+P1sg+Acc
skntya sknt+Noun+A3sg+Pnon+Dat
sokacak sok+Verb+Pos+Fut+A3sg
sok+Verb+Pos?DB+Adj
+FutPart+Pnon
herhangi herhangi+Adj
bir bir+Det
bir+Num+Card
bir+Adj
bir+Adverb
etkisi etki+Noun+A3sg+P3sg+Nom
s?z s?z+Noun+A3sg+Pnon+Nom
konusu konu+Noun+A3sg+P3sg+Nom
degil deg+Verb?DB+Verb+Pass
+Pos+Imp+A2sg
degil+Conj
degil+Verb+Pres+A3sg
. .+Punc
Figure 2: Output of the morphological analyzer
The multi-word expression extraction processor has
three stages with the output of one stage feeding into
the next stage:
1. The first stage handles lexicalized collocations
and multi-word named entities.
2. The second stage handles non-lexicalized col-
locations.
3. The third stage handles semi-lexicalized col-
locations. The reason semi-lexicalized collo-
cations are handled last, is that any duplicate
verb formations have to be processed before
compound verbs are combined with their lexi-
calized complements (cf. examples (14) ? (16)
above).
The output of the multi-word expression extraction
processor for the relevant segments in Figure 2 is
given in Figure 3.
The multi-word expression extraction processor has
been implemented in Perl. The rule bases for
the three stages are maintained separately and then
compiled offline into regular expressions which are
then used by Perl at runtime.
...
skntya_sokacak skntya_sok+Verb
+Pos+Fut+A3sg
skntya_sok+Verb
+Pos?DB+Adj
+FutPart+Pnon
herhangi_bir herhangi_bir+Det
...
s?z_konusu s?z_konu+Noun+A3sg
+P3sg+Nom
...
Figure 3: Output of the multi-word expression ex-
traction processor
Table 1 presents statistics on the current rule base
of our multi-word expression extraction processor:
For named entity recognition, we use a list of about
Rule Type Number of Rules
Lexicalized Colloc. 363
Semi-lexicalized Colloc. 731
Non-lexicalized Colloc. 16
Table 1: Rules base statistics
60,000 first and last names, a list of about 16,000
multi-word organization and place names.
4 Evaluation
To improve and evaluate our multi-word expression
extraction processor, we used two corpora of news
text. We used a corpus of about 730,000 tokens to
incrementally test and improve our semi-lexicalized
rule base, by searching for compound verb forma-
tions, etc. Once such rules were extracted, we tested
our processor on this corpus, and on a small corpus
of about 4200 words to measure precision and re-
call. Table 2 provides some statistics on these cor-
pora.
Table 3 shows the result of multi-word expression
extraction on the large (training) corpus. It should
be noted that we only mark multi-word named-
entities, not all. Thus many references to persons by
Corpus Number of Avg. Analyses
Tokens per Token
Large Corpus 729,955 1.760
Small Corpus 4,242 1.702
Table 2: Corpora Statistics
their last name are not marked, hence the low num-
ber of named-entities extracted.8 As a result of
MW Type Number Extracted
Lexicalized Colloc. 3,883
Semi-lexicalized Colloc. 9,173
Non-lexicalized Colloc. 220
Named-Entities 4,480
Total 17,750
Table 3: Multi-word expression extraction statistics
on the large corpus
this extraction, the average number of morphologi-
cal parses per token go from 1.760 down to 1.745.
Table 4 shows the result of multi-word expression
extraction on the small corpus. We also manu-
MW Type Number Extracted
Lexicalized Colloc. 15
Semi-lexicalized Colloc. 62
Non-lexicalized Colloc. 0
Named-Entities 99
Total 176
Table 4: Multi-word expression extraction statistics
on the small corpus
ally marked up the small corpus into a gold-standard
corpus to test precision and recall. The results in
Table 4 correspond to an overall recall of 65.2%
and a precision of 98.9%, over all classes of multi-
word expressions. When we consider all classes
except named-entities, we have a recall of 60.1%
and a precision of 100%. An analysis of the er-
rors and missed multi-word expressions indicates
that the test corpus had a certain variant of a com-
pound verb construction that we had failed to ex-
tract from the larger corpus we used for compil-
ing rules. Failing to extract the multi-word expres-
sions for that compound verb accounted for most
of the drop in recall. Since we are currently using
a rather naive named-entity extraction scheme,9 re-
8Since this is a very large corpus, we have no easy way of
obtaining accurate precision and recall figures.
9As opposed to a general purpose statistical NE extractor
that we have developed earlier (T?r et al, 2003).
call is rather low as there are quite a number of for-
eign multi-word named-entities (persons and orga-
nizations mostly) that do not exist in our database
of named-entities. On the other hand, since named-
entity extraction for English is a relatively mature
technology, we can easily integrate an existing tool
to improve our recall.
5 Conclusions
This paper has described a multi-word expression
extraction system for Turkish for handling vari-
ous types of multi-word expressions such as semi-
lexicalized and non-lexicalized collocations which
depend on the recognition of certain morphologi-
cal patterns across tokens. Our results indicate that
with about 1100 rules (most of which were extracted
from a large ?training corpus? searching for patterns
involving a certain small set of support verbs), we
were able get alost 100% precision and around
60% recall on a small ?test? corpus. We expect that
with additional rules from dictionaries and other
sources we will improve recall significantly.
6 Acknowledgments
We thank Orhan Bilgin for helping us compile the
multi-word expressions.
References
Timothy Baldwin and Aline Villavicencio. 2002.
Extracting the unextractable: A case study on
verb-particles. In Proceedings of the Sixth Confer-
ence on Computational Natural Language Learning
(CoNLL 2002), pages 99?105.
Lauri Karttunen, Tamas Gaal, and Andre Kempe.
1997. Xerox Finite-State Tool. Technical report,
Xerox Research Centre Europe.
Kemal Oflazer. 1994. Two-level description of
Turkish morphology. Literary and Linguistic Com-
puting, 9(2):137?148.
Scott S. L. Piao, Paul Rayson, Dawn Archer, An-
drew Wilson, and Tony McEnery. 2003. Extracting
multiword expressions with a semantic tagger. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Pro-
ceedings of the Third International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLING 2002), pages 1?15.
G?khan T?r, Dilek Zeynep Hakkani-T?r, and Ke-
mal Oflazer. 2003. A statistical information extrac-
tion systems for Turkish. Natural Language Engi-
neering, 9(2).
R. Urizar, N. Ezeiza, and I. Alegria. 2000. Mor-
phosyntactic structure of terms in Basque for auto-
matic terminology extraction. In Proceedings of the
ninth EURALEX International Congress.
A Morphosyntactic Features For Turkish
This section lists the features and their semantics for
the morphological representations used in the text.
?DB marks a derivation boundary.
? Parts-of-speech:+Noun, +Adjective, +Adverb,
+Verb, +Dup (for onomatopoeic words which al-
ways appear in duplicate), +Question (yes/no ques-
tion marker clitic), +Number, +Determiner
? Agreement: +A[1-3][sg-pl], e.g., +A3pl.
? Possessive agreement: +P[1-3][sg-pl] and
+Pnon, e.g., +P1sg
? Case: +Nominative, +Accusative, +Locative,
+Ablative, +Instrumental, +Genitive, +Dative.
? Miscellaneous Verbal Features: +Causative,
+Passive, +Positive Polarity, +Negative Polar-
ity, +Optative Mood, +Aorist Aspect, +Become,
+Conditional Mood, +Imperative Mood, +Past
tense
? Miscellaneous POS Subtypes: Adverbs: +By
(as in ?house by house?), +ByDoingSo, (as in
?he came by running?), +Resemble (as in ?he
made sounds resembling ..?), +Ly (as in ?slowly?)
+AsSoonAs (as in ?he came down as soon as he
woke up?); Adjectives: +With (as in ?the book
with red cover?), +FutPart ? future participle ?
as in (?the boy who will come?); Nouns:+Proper
Noun, +Ness (as in ?sick-ness?), +FutPart ?
future participle fact ? as in (?I know that he will
come?) ; Numbers: +Cardinal
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 550?560,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ParGramBank: The ParGram Parallel Treebank
Sebastian Sulger and Miriam Butt
University of Konstanz, Germany
{sebastian.sulger|miriam.butt}@uni-konstanz.de
Tracy Holloway King
eBay Inc., USA
tracyking@ebay.com
Paul Meurer
Uni Research AS, Norway
paul.meurer@uni.no
Tibor Laczko? and Gyo?rgy Ra?kosi
University of Debrecen, Hungary
{laczko.tibor|rakosi.gyorgy}@arts.unideb.hu
Cheikh Bamba Dione and Helge Dyvik and Victoria Rose?n and Koenraad De Smedt
University of Bergen, Norway
dione.bamba@lle.uib.no, {dyvik|victoria|desmedt}@uib.no
Agnieszka Patejuk
Polish Academy of Sciences
aep@ipipan.waw.pl
O?zlem C?etinog?lu
University of Stuttgart, Germany
ozlem@ims.uni-stuttgart.de
I Wayan Arka* and Meladel Mistica+
*Australian National University and Udayana University, Indonesia
+Australian National University
wayan.arka@anu.edu.au, meladel.mistica@gmail.com
Abstract
This paper discusses the construction of
a parallel treebank currently involving ten
languages from six language families. The
treebank is based on deep LFG (Lexical-
Functional Grammar) grammars that were
developed within the framework of the
ParGram (Parallel Grammar) effort. The
grammars produce output that is maxi-
mally parallelized across languages and
language families. This output forms the
basis of a parallel treebank covering a
diverse set of phenomena. The treebank
is publicly available via the INESS tree-
banking environment, which also allows
for the alignment of language pairs. We
thus present a unique, multilayered paral-
lel treebank that represents more and dif-
ferent types of languages than are avail-
able in other treebanks, that represents
deep linguistic knowledge and that allows
for the alignment of sentences at sev-
eral levels: dependency structures, con-
stituency structures and POS information.
1 Introduction
This paper discusses the construction of a parallel
treebank currently involving ten languages that
represent several different language families, in-
cluding non-Indo-European. The treebank is based
on the output of individual deep LFG (Lexical-
Functional Grammar) grammars that were deve-
loped independently at different sites but within
the overall framework of ParGram (the Parallel
Grammar project) (Butt et al, 1999a; Butt et al,
2002). The aim of ParGram is to produce deep,
wide coverage grammars for a variety of lan-
guages. Deep grammars provide detailed syntactic
analysis, encode grammatical functions as well as
550
other grammatical features such as tense or aspect,
and are linguistically well-motivated. The Par-
Gram grammars are couched within the linguis-
tic framework of LFG (Bresnan, 2001; Dalrymple,
2001) and are constructed with a set of grammati-
cal features that have been commonly agreed upon
within the ParGram group. ParGram grammars are
implemented using XLE, an efficient, industrial-
strength grammar development platform that in-
cludes a parser, a generator and a transfer sys-
tem (Crouch et al, 2012). XLE has been devel-
oped in close collaboration with the ParGram
project. Over the years, ParGram has continu-
ously grown and includes grammars for Ara-
bic, Chinese, English, French, German, Georgian,
Hungarian, Indonesian, Irish, Japanese, Mala-
gasy, Murrinh-Patha, Norwegian, Polish, Spanish,
Tigrinya, Turkish, Urdu, Welsh and Wolof.
ParGram grammars produce output that has
been parallelized maximally across languages ac-
cording to a set of commonly agreed upon uni-
versal proto-type analyses and feature values. This
output forms the basis of the ParGramBank paral-
lel treebank discussed here. ParGramBank is con-
structed using an innovative alignment methodol-
ogy developed in the XPAR project (Dyvik et al,
2009) in which grammar parallelism is presup-
posed to propagate alignment across different pro-
jections (section 6). This methodology has been
implemented with a drag-and-drop interface as
part of the LFG Parsebanker in the INESS infras-
tructure (Rose?n et al, 2012; Rose?n et al, 2009).
ParGramBank has been constructed in INESS and
is accessible in this infrastructure, which also of-
fers powerful search and visualization.
In recent years, parallel treebanking1 has gained
in importance within NLP. An obvious applica-
tion for parallel treebanking is machine transla-
tion, where treebank size is a deciding factor for
whether a particular treebank can support a par-
ticular kind of research project. When conduct-
ing in-depth linguistic studies of typological fea-
tures, other factors such as the number of in-
cluded languages, the number of covered phe-
nomena, and the depth of linguistic analysis be-
come more important. The treebanking effort re-
ported on in this paper supports work of the lat-
ter focus, including efforts at multilingual depen-
dency parsing (Naseem et al, 2012). We have
1Throughout this paper ?treebank? refers to both phrase-
structure resources and their natural extensions to depen-
dency and other deep annotation banks.
created a parallel treebank whose prototype in-
cludes ten typologically diverse languages and re-
flects a diverse set of phenomena. We thus present
a unique, multilayered parallel treebank that rep-
resents more languages than are currently avail-
able in other treebanks, and different types of lan-
guages as well. It contains deep linguistic knowl-
edge and allows for the parallel and simultane-
ous alignment of sentences at several levels. LFG?s
f(unctional)-structure encodes dependency struc-
tures as well as information that is equivalent to
Quasi-Logical Forms (van Genabith and Crouch,
1996). LFG?s c(onstituent)-structure provides in-
formation about constituency, hierarchical rela-
tions and part-of-speech. Currently, ParGramBank
includes structures for the following languages
(with the ISO 639-3 code and language fam-
ily): English (eng, Indo-European), Georgian (kat,
Kartvelian), German (deu, Indo-European), Hun-
garian (hun, Uralic), Indonesian (ind, Austrone-
sian), Norwegian (Bokma?l) (nob, Indo-European),
Polish (pol, Indo-European), Turkish (tur, Altaic),
Urdu (urd, Indo-European) and Wolof (wol, Niger-
Congo). It is freely available for download under
the CC-BY 3.0 license via the INESS treebanking
environment and comes in two formats: a Prolog
format and an XML format.2
This paper is structured as follows. Section
2 discusses related work in parallel treebanking.
Section 3 presents ParGram and its approach to
parallel treebanking. Section 4 focuses on the tree-
bank design and its construction. Section 5 con-
tains examples from the treebank, focusing on ty-
pological aspects and challenges for parallelism.
Section 6 elaborates on the mechanisms for paral-
lel alignment of the treebank.
2 Related Work
There have been several efforts in parallel tree-
banking across theories and annotation schemes.
Kuhn and Jellinghaus (2006) take a mini-
mal approach towards multilingual parallel tree-
banking. They bootstrap phrasal alignments over
a sentence-aligned parallel corpus of English,
French, German and Spanish and report concrete
treebank annotation work on a sample of sen-
tences from the Europarl corpus. Their annotation
2http://iness.uib.no. The treebank is in the
public domain (CC-BY 3.0). The use of the INESS platform
itself is not subject to any licensing. To access the treebank,
click on ?Treebank selection? and choose the ParGram collec-
tion.
551
scheme is the ?leanest? possible scheme in that it
consists solely of a bracketing for a sentence in
a language (where only those units that play the
role of a semantic argument or modifier in a larger
unit are bracketed) and a correspondence relation
of the constituents across languages.
Klyueva and Marec?ek (2010) present a small
parallel treebank using data and tools from two
existing treebanks. They take a syntactically an-
notated gold standard text for one language and
run an automated annotation on the parallel text
for the other language. Manually annotated Rus-
sian data are taken from the SynTagRus treebank
(Nivre et al, 2008), while tools for parsing the cor-
responding text in Czech are taken from the Tec-
toMT framework (Popel and Z?abokrtsky?, 2010).
The SMULTRON project is concerned with con-
structing a parallel treebank of English, German
and Swedish. The sentences have been POS-tagged
and annotated with phrase structure trees. These
trees have been aligned on the sentence, phrase
and word level. Additionally, the German and
Swedish monolingual treebanks contain lemma in-
formation. The treebank is distributed in TIGER-
XML format (Volk et al, 2010).
Megyesi et al (2010) discuss a parallel English-
Swedish-Turkish treebank. The sentences in each
language are annotated morphologically and syn-
tactically with automatic tools, aligned on the
sentence and the word level and partially hand-
corrected.3
A further parallel treebanking effort is Par-
TUT, a parallel treebank (Sanguinetti and Bosco,
2011; Bosco et al, 2012) which provides depen-
dency structures for Italian, English and French
and which can be converted to a CCG (Combina-
tory Categorial Grammar) format.
Closest to our work is the ParDeepBank, which
is engaged in the creation of a highly paral-
lel treebank of English, Portuguese and Bulgar-
ian. ParDeepBank is couched within the linguistic
framework of HPSG (Head-Driven Phrase Struc-
ture Grammar) and uses parallel automatic HPSG
grammars, employing the same tools and imple-
mentation strategies across languages (Flickinger
et al, 2012). The parallel treebank is aligned on
the sentence, phrase and word level.
In sum, parallel treebanks have so far fo-
cused exclusively on Indo-European languages
3The paper mentions Hindi as the fourth language, but
this is not yet available: http://stp.lingfil.uu.
se/?bea/turkiska/home-en.html.
(with Turkish providing the one exception) and
generally do not extend beyond three or four
languages. In contrast, our ParGramBank tree-
bank currently includes ten typologically differ-
ent languages from six different language families
(Altaic, Austronesian, Indo-European, Kartvelian,
Niger-Congo, Uralic).
A further point of comparison with ParDeep-
Bank is that it relies on dynamic treebanks, which
means that structures are subject to change dur-
ing the further development of the resource gram-
mars. In ParDeepBank, additional machinery is
needed to ensure correct alignment on the phrase
and word level (Flickinger et al, 2012, p. 105).
ParGramBank contains finalized analyses, struc-
tures and features that were designed collabora-
tively over more than a decade, thus guaranteeing
a high degree of stable parallelism. However, with
the methodology developed within XPAR, align-
ments can easily be recomputed from f-structure
alignments in case of grammar or feature changes,
so that we also have the flexible capability of
allowing ParGramBank to include dynamic tree-
banks.
3 ParGram and its Feature Space
The ParGram grammars use the LFG formalism
which produces c(onstituent)-structures (trees)
and f(unctional)-structures as the syntactic anal-
ysis. LFG assumes a version of Chomsky?s Uni-
versal Grammar hypothesis, namely that all lan-
guages are structured by similar underlying prin-
ciples (Chomsky, 1988; Chomsky, 1995). Within
LFG, f-structures encode a language universal
level of syntactic analysis, allowing for crosslin-
guistic parallelism at this level of abstraction. In
contrast, c-structures encode language particular
differences in linear word order, surface morpho-
logical vs. syntactic structures, and constituency
(Dalrymple, 2001). Thus, while the Chomskyan
framework is derivational in nature, LFG departs
from this view by embracing a strictly representa-
tional approach to syntax.
ParGram tests the LFG formalism for its uni-
versality and coverage limitations to see how far
parallelism can be maintained across languages.
Where possible, analyses produced by the gram-
mars for similar constructions in each language are
parallel, with the computational advantage that the
grammars can be used in similar applications and
that machine translation can be simplified.
552
The ParGram project regulates the features and
values used in its grammars. Since its inception
in 1996, ParGram has included a ?feature com-
mittee?, which collaboratively determines norms
for the use and definition of a common multilin-
gual feature and analysis space. Adherence to fea-
ture committee decisions is supported technically
by a routine that checks the grammars for com-
patibility with a feature declaration (King et al,
2005); the feature space for each grammar is in-
cluded in ParGramBank. ParGram also conducts
regular meetings to discuss constructions, analy-
ses and features.
For example, Figure 1 shows the c-structure
of the Urdu sentence in (1) and the c-structure
of its English translation. Figure 2 shows the f-
structures for the same sentences. The left/upper
c- and f-structures show the parse from the En-
glish ParGram grammar, the right/lower ones from
Urdu ParGram grammar.4,5 The c-structures en-
code linear word order and constituency and thus
look very different; e.g., the English structure is
rather hierarchical while the Urdu structure is flat
(Urdu is a free word-order language with no evi-
dence for a VP; Butt (1995)). The f-structures, in
contrast, are parallel aside from grammar-specific
characteristics such as the absence of grammati-
cal gender marking in English and the absence of
articles in Urdu.6
(1) ? Aj J
K. Q?K
QK A 	JK @ ?

	
G
	
?A??
kisAn=nE apnA
farmer.M.Sg=Erg self.M.Sg
TrEkTar bEc-A
tractor.M.Sg sell-Perf.M.Sg
?Did the farmer sell his tractor??
With parallel analyses and parallel features, maxi-
mal parallelism across typologically different lan-
guages is maintained. As a result, during the con-
struction of the treebank, post-processing and con-
version efforts are kept to a minimum.
4The Urdu ParGram grammar makes use of a translitera-
tion scheme that abstracts away from the Arabic-based script;
the transliteration scheme is detailed in Malik et al (2010).
5In the c-structures, dotted lines indicate distinct func-
tional domains; e.g., in Figure 1, the NP the farmer and the
VP sell his tractor belong to different f-structures: the former
maps onto the SUBJ f-structure, while the latter maps onto the
topmost f-structure (Dyvik et al, 2009). Section 6 elaborates
on functional domains.
6The CASE feature also varies: since English does not
distinguish between accusative, dative, and other oblique
cases, the OBJ is marked with a more general obl CASE.
Figure 1: English and Urdu c-structures
We emphasize the fact that ParGramBank is
characterized by a maximally reliable, human-
controlled and linguistically deep parallelism
across aligned sentences. Generally, the result of
automatic sentence alignment procedures are par-
allel corpora where the corresponding sentences
normally have the same purported meaning as
intended by the translator, but they do not nec-
essarily match in terms of structural expression.
In building ParGramBank, conscious attention is
paid to maintaining semantic and constructional
parallelism as much as possible. This design fea-
ture renders our treebank reliable in cases when
the constructional parallelism is reduced even at f-
structure. For example, typological variation in the
presence or absence of finite passive constructions
represents a case of potential mismatch. Hungar-
ian, one of the treebank languages, has no produc-
tive finite passives. The most common strategy in
translation is to use an active construction with a
topicalized object, with no overt subject and with
3PL verb agreement:
(2) A fa?-t ki-va?g-t-a?k.
the tree-ACC out-cut-PAST-3PL
?The tree was cut down.?
In this case, a topicalized object in Hungarian has
to be aligned with a (topical) subject in English.
Given that both the sentence level and the phrase
level alignments are human-controlled in the tree-
bank (see sections 4 and 6), the greatest possible
parallelism is reliably captured even in such cases
of relative grammatical divergence.
553
Figure 2: Parallel English and Urdu f-structures
4 Treebank Design and Construction
For the initial seeding of the treebank, we focused
on 50 sentences which were constructed manu-
ally to cover a diverse range of phenomena (tran-
sitivity, voice alternations, interrogatives, embed-
ded clauses, copula constructions, control/raising
verbs, etc.). We followed Lehmann et al (1996)
and Bender et al (2011) in using coverage of
grammatical constructions as a key component for
grammar development. (3) lists the first 16 sen-
tences of the treebank. An expansion to 100 sen-
tences is scheduled for next year.
(3) a. Declaratives:
1. The driver starts the tractor.
2. The tractor is red.
b. Interrogatives:
3. What did the farmer see?
4. Did the farmer sell his tractor?
c. Imperatives:
5. Push the button.
6. Don?t push the button.
d. Transitivity:
7. The farmer gave his neighbor an old
tractor.
8. The farmer cut the tree down.
9. The farmer groaned.
e. Passives and traditional voice:
10. My neighbor was given an old tractor
by the farmer.
11. The tree was cut down yesterday.
12. The tree had been cut down.
13. The tractor starts with a shudder.
f. Unaccusative:
14. The tractor appeared.
g. Subcategorized declaratives:
15. The boy knows the tractor is red.
16. The child thinks he started the tractor.
The sentences were translated from English
into the other treebank languages. Currently, these
languages are: English, Georgian, German, Hun-
garian, Indonesian, Norwegian (Bokma?l), Polish,
Turkish, Urdu and Wolof. The translations were
done by ParGram grammar developers (i.e., expert
linguists and native speakers).
The sentences were automatically parsed with
ParGram grammars using XLE. Since the pars-
ing was performed sentence by sentence, our re-
sulting treebank is automatically aligned at the
sentence level. The resulting c- and f-structures
were banked in a database using the LFG Parse-
banker (Rose?n et al, 2009). The structures were
disambiguated either prior to banking using XLE
or during banking with the LFG Parsebanker and
its discriminant-based disambiguation technique.
The banked analyses can be exported and down-
loaded in a Prolog format using the LFG Parse-
banker interface. Within XLE, we automatically
convert the structures to a simple XML format and
make these available via ParGramBank as well.
The Prolog format is used with applications
which use XLE to manipulate the structures, e.g.
for further semantic processing (Crouch and King,
2006) or for sentence condensation (Crouch et al,
2004).
554
5 Challenges for Parallelism
We detail some challenges in maintaining paral-
lelism across typologically distinct languages.
5.1 Complex Predicates
Some languages in ParGramBank make extensive
use of complex predicates. For example, Urdu uses
a combination of predicates to express concepts
that in languages like English are expressed with
a single verb, e.g., ?memory do? = ?remember?,
?fear come? = ?fear?. In addition, verb+verb com-
binations are used to express permissive or as-
pectual relations. The strategy within ParGram is
to abstract away from the particular surface mor-
phosyntactic expression and aim at parallelism
at the level of f-structure. That is, monoclausal
predications are analyzed via a simple f-structure
whether they consist of periphrastically formed
complex predicates (Urdu, Figure 3), a simple
verb (English, Figure 4), or a morphologically de-
rived form (Turkish, Figure 5).
In Urdu and in Turkish, the top-level PRED
is complex, indicating a composed predicate. In
Urdu, this reflects the noun-verb complex predi-
cate sTArT kar ?start do?, in Turkish it reflects a
morphological causative. Despite this morphosyn-
tactic complexity, the overall dependency struc-
ture corresponds to that of the English simple verb.
(4) ?


?
f
A

KQ ? HPA

J ? ? ? Q

 ? K
Q

K P?

J K
 @P

X
DrAIvar TrEkTar=kO
driver.M.Sg.Nom tractor.M.Sg=Acc
sTArT kartA hE
start.M.Sg do.Impf.M.Sg be.Pres.3Sg
?The driver starts the tractor.?
(5) su?ru?cu? trakto?r-u? c?al?s?-t?r-?yor
driver.Nom tractor-Acc work-Caus-Prog.3Sg
?The driver starts the tractor.?
The f-structure analysis of complex predicates
is thus similar to that of languages which do not
use complex predicates, resulting in a strong syn-
tactic parallelism at this level, even across typo-
logically diverse languages.
5.2 Negation
Negation also has varying morphosyntactic sur-
face realizations. The languages in ParGramBank
differ with respect to their negation strategies.
Languages such as English and German use inde-
pendent negation: they negate using words such as
Figure 3: Complex predicate: Urdu analysis of (4)
Figure 4: Simple predicate: English analysis of (4)
adverbs (English not, German nicht) or verbs (En-
glish do-support). Other languages employ non-
independent, morphological negation techniques;
Turkish, for instance, uses an affix on the verb, as
in (6).
555
Figure 5: Causative: Turkish analysis of (5)
(6) du?g?me-ye bas-ma
button-Dat push-Neg.Imp
?Don?t push the button.?
Within ParGram we have not abstracted away
from this surface difference. The English not in
(6) functions as an adverbial adjunct that modifies
the main verb (see top part of Figure 6) and infor-
mation would be lost if this were not represented
at f-structure. However, the same cannot be said of
the negative affix in Turkish ? the morphological
affix is not an adverbial adjunct. We have there-
fore currently analyzed morphological negation as
adding a feature to the f-structure which marks the
clause as negative, see bottom half of Figure 6.
5.3 Copula Constructions
Another challenge to parallelism comes from co-
pula constructions. An approach advocating a uni-
form treatment of copulas crosslinguistically was
advocated in the early years of ParGram (Butt et
al., 1999b), but this analysis could not do justice to
the typological variation found with copulas. Par-
GramBank reflects the typological difference with
three different analyses, with each language mak-
ing a language-specific choice among the three
possibilities that have been identified (Dalrymple
et al, 2004; Nordlinger and Sadler, 2007; Attia,
2008; Sulger, 2011; Laczko?, 2012).
The possible analyses are demonstrated here
with respect to the sentence The tractor is red.
The English grammar (Figure 7) uses a raising ap-
proach that reflects the earliest treatments of cop-
ulas in LFG (Bresnan, 1982). The copula takes
a non-finite complement whose subject is raised
to the matrix clause as a non-thematic subject of
the copula. In contrast, in Urdu (Figure 8), the
Figure 6: Different f-structural analyses for nega-
tion (English vs. Turkish)
copula is a two-place predicate, assigning SUBJ
and PREDLINK functions. The PREDLINK function
is interpreted as predicating something about the
subject. Finally, in languages like Indonesian (Fig-
ure 9), there is no overt copula and the adjective is
the main predicational element of the clause.
Figure 7: English copula example
556
Figure 8: Urdu copula example
Figure 9: Indonesian copula example
5.4 Summary
This section discussed some challenges for main-
taining parallel analyses across typologically di-
verse languages. Another challenge we face is
when no corresponding construction exists in a
language, e.g. with impersonals as in the English
It is raining. In this case, we provide a translation
and an analysis of the structure of the correspond-
ing translation, but note that the phenomenon be-
ing exemplified does not actually exist in the lan-
guage. A further extension to the capabilities of
the treebank could be the addition of pointers from
the alternative structure used in the translation to
the parallel aligned set of sentences that corre-
spond to this alternative structure.
6 Linguistically Motivated Alignment
The treebank is automatically aligned on the sen-
tence level, the top level of alignment within Par-
GramBank. For phrase-level alignments, we use
the drag-and-drop alignment tool in the LFG Parse-
banker (Dyvik et al, 2009). The tool allows the
alignment of f-structures by dragging the index
of a subsidiary source f-structure onto the index
of the corresponding target f-structure. Two f-
structures correspond if they have translationally
matching predicates, and the arguments of each
predicate correspond to an argument or adjunct in
the other f-structure. The tool automatically com-
putes the alignment of c-structure nodes on the
basis of the manually aligned corresponding f-
structures.7
7Currently we have not measured inter-annotator agree-
ment (IAA) for the f-structure alignments. The f-structure
alignments were done by only one person per language pair.
We anticipate that multiple annotators will be needed for this
This method is possible because the c-structure
to f-structure correspondence (the ? relation) is
encoded in the ParGramBank structures, allow-
ing the LFG Parsebanker tool to compute which c-
structure nodes contributed to a given f-structure
via the inverse (??1) mapping. A set of nodes
mapping to the same f-structure is called a ?func-
tional domain?. Within a source and a target
functional domain, two nodes are automatically
aligned only if they dominate corresponding word
forms. In Figure 10 the nodes in each func-
tional domain in the trees are connected by whole
lines while dotted lines connect different func-
tional domains. Within a functional domain, thick
whole lines connect the nodes that share align-
ment; for simplicity the alignment is only indi-
cated for the top nodes. The automatically com-
puted c-structural alignments are shown by the
curved lines. The alignment information is stored
as an additional layer and can be used to ex-
plore alignments at the string (word), phrase (c-
)structure, and functional (f-)structure levels.
We have so far aligned the treebank pairs
English-Urdu, English-German, English-Polish
and Norwegian-Georgian. As Figure 10 illustrates
for (7) in an English-Urdu pairing, the English ob-
ject neighbor is aligned with the Urdu indirect ob-
ject (OBJ-GO) hamsAyA ?neighbor?, while the En-
glish indirect object (OBJ-TH) tractor is aligned
with the Urdu object TrEkTar ?tractor?. The c-
structure correspondences were computed auto-
matically from the f-structure alignments.
(7) AK
X Q?K
QK A 	K @QK ?? ?
G
A???f ?

	
?K @ ?

	
G
	
?A??
kisAn=nE apnE
farmer.M.Sg=Erg self.Obl
hamsAyE=kO purAnA
neighbor.M.Sg.Obl=Acc old.M.Sg
TrEkTar di-yA
tractor.M.Sg give-Perf.M.Sg
?The farmer gave his neighbor an old tractor.?
The INESS platform additionally allows for the
highlighting of connected nodes via a mouse-over
technique. It thus provides a powerful and flexible
tool for the semi-automatic alignment and subse-
task in the future, in which case we will measure IAA for this
step.
557
Figure 10: Phrase-aligned treebank example English-Urdu: The farmer gave his neighbor an old tractor.
quent inspection of parallel treebanks which con-
tain highly complex linguistic structures.8
7 Discussion and Future Work
We have discussed the construction of ParGram-
Bank, a parallel treebank for ten typologically
different languages. The analyses in ParGram-
Bank are the output of computational LFG Par-
Gram grammars. As a result of ParGram?s cen-
trally agreed upon feature sets and prototypical
analyses, the representations are not only deep
in nature, but maximally parallel. The representa-
tions offer information about dependency relations
as well as word order, constituency and part-of-
speech.
In future ParGramBank releases, we will pro-
vide more theory-neutral dependencies along with
the LFG representations. This will take the form of
triples (King et al, 2003). We also plan to provide
a POS-tagged and a named entity marked up ver-
sion of the sentences; these will be of use for more
general NLP applications and for systems which
use such markup as input to deeper processing.
8One reviewer inquires about possibilities of linking
(semi-)automatically between languages, for example using
lexical resources such as WordNets or Panlex. We agree that
this would be desirable, but unrealizable, since many of the
languages included in ParGramBank do not have a WordNet
resource and are not likely to achieve an adequate one soon.
Third, the treebank will be expanded to include
100 more sentences within the next year. We also
plan to include more languages as other ParGram
groups contribute structures to ParGramBank.
ParGramBank, including its multilingual sen-
tences and all annotations, is made freely avail-
able for research and commercial use under the
CC-BY 3.0 license via the INESS platform, which
supports alignment methodology developed in the
XPAR project and provides search and visualiza-
tion methods for parallel treebanks. We encourage
the computational linguistics community to con-
tribute further layers of annotation, including se-
mantic (Crouch and King, 2006), abstract knowl-
edge representational (Bobrow et al, 2007), Prop-
Bank (Palmer et al, 2005), or TimeBank (Mani
and Pustejovsky, 2004) annotations.
References
Mohammed Attia. 2008. A Unified Analysis of Cop-
ula Constructions. In Proceedings of the LFG ?08
Conference, pages 89?108. CSLI Publications.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2011. Grammar Engineering and Linguistic Hy-
pothesis Testing: Computational Support for Com-
plexity in Syntactic Analysis. In Emily M. Bender
and Jennifer E. Arnold, editors, Languages from a
Cognitive Perspective: Grammar, Usage and Pro-
cessing, pages 5?30. CSLI Publications.
558
Daniel G. Bobrow, Cleo Condoravdi, Dick Crouch,
Valeria de Paiva, Lauri Karttunen, Tracy Holloway
King, Rowan Nairn, Lottie Price, and Annie Zaenen.
2007. Precision-focused Textual Inference. In Pro-
ceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Cristina Bosco, Manuela Sanguinetti, and Leonardo
Lesmo. 2012. The Parallel-TUT: a multilingual and
multiformat treebank. In Proceedings of the Eighth
International Conference on Language Resources
and Evaluation (LREC-2012), pages 1932?1938, Is-
tanbul, Turkey. European Language Resources As-
sociation (ELRA).
Joan Bresnan. 1982. The Passive in Lexical Theory. In
Joan Bresnan, editor, The Mental Representation of
Grammatical Relations, pages 3?86. The MIT Press.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell Publishing.
Miriam Butt, Stefanie Dipper, Anette Frank, and
Tracy Holloway King. 1999a. Writing Large-
Scale Parallel Grammars for English, French and
German. In Proceedings of the LFG99 Conference.
CSLI Publications.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nin?o, and Fre?de?rique Segond. 1999b. A Grammar
Writer?s Cookbook. CSLI Publications.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project. In Proceedings of the
COLING-2002 Workshop on Grammar Engineering
and Evaluation, pages 1?7.
Miriam Butt. 1995. The Structure of Complex Predi-
cates in Urdu. CSLI Publications.
Noam Chomsky. 1988. Lectures on Government and
Binding: The Pisa Lectures. Foris Publications.
Noam Chomsky. 1995. The Minimalist Program. MIT
Press.
Dick Crouch and Tracy Holloway King. 2006. Seman-
tics via F-structure Rewriting. In Proceedings of the
LFG06 Conference, pages 145?165. CSLI Publica-
tions.
Dick Crouch, Tracy Holloway King, John T. Maxwell
III, Stefan Riezler, and Annie Zaenen. 2004. Ex-
ploiting F-structure Input for Sentence Condensa-
tion. In Proceedings of the LFG04 Conference,
pages 167?187. CSLI Publications.
Dick Crouch, Mary Dalrymple, Ronald M. Kaplan,
Tracy Holloway King, John T. Maxwell III, and
Paula Newman, 2012. XLE Documentation. Palo
Alto Research Center.
Mary Dalrymple, Helge Dyvik, and Tracy Holloway
King. 2004. Copular Complements: Closed or
Open? In Proceedings of the LFG ?04 Conference,
pages 188?198. CSLI Publications.
Mary Dalrymple. 2001. Lexical Functional Gram-
mar, volume 34 of Syntax and Semantics. Academic
Press.
Helge Dyvik, Paul Meurer, Victoria Rose?n, and Koen-
raad De Smedt. 2009. Linguistically Motivated Par-
allel Parsebanks. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8), pages 71?82, Milan, Italy. EDU-
Catt.
Dan Flickinger, Valia Kordoni, Yi Zhang, Anto?nio
Branco, Kiril Simov, Petya Osenova, Catarina Car-
valheiro, Francisco Costa, and Se?rgio Castro. 2012.
ParDeepBank: Multiple Parallel Deep Treebank-
ing. In Proceedings of the 11th International Work-
shop on Treebanks and Linguistic Theories (TLT11),
pages 97?107, Lisbon. Edic?o?es Colibri.
Tracy Holloway King, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ronald Kaplan. 2003. The
PARC700 Dependency Bank. In Proceedings of the
EACL03: 4th International Workshop on Linguisti-
cally Interpreted Corpora (LINC-03).
Tracy Holloway King, Martin Forst, Jonas Kuhn, and
Miriam Butt. 2005. The Feature Space in Paral-
lel Grammar Writing. In Emily M. Bender, Dan
Flickinger, Frederik Fouvry, and Melanie Siegel, ed-
itors, Research on Language and Computation: Spe-
cial Issue on Shared Representation in Multilingual
Grammar Engineering, volume 3, pages 139?163.
Springer.
Natalia Klyueva and David Marec?ek. 2010. To-
wards a Parallel Czech-Russian Dependency Tree-
bank. In Proceedings of the Workshop on Anno-
tation and Exploitation of Parallel Corpora, Tartu.
Northern European Association for Language Tech-
nology (NEALT).
Jonas Kuhn and Michael Jellinghaus. 2006. Multilin-
gual Parallel Treebanking: A Lean and Flexible Ap-
proach. In Proceedings of the LREC 2006, Genoa,
Italy. ELRA/ELDA.
Tibor Laczko?. 2012. On the (Un)Bearable Lightness
of Being an LFG Style Copula in Hungarian. In Pro-
ceedings of the LFG12 Conference, pages 341?361.
CSLI Publications.
Sabine Lehmann, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herve? Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP ?
Test Suites for Natural Language Processing. In
Proceedings of COLING, pages 711 ? 716.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian
Sulger, Tina Bo?gel, Atif Gulzar, Ghulam Raza, Sar-
mad Hussain, and Miriam Butt. 2010. Transliter-
ating Urdu for a Broad-Coverage Urdu/Hindi LFG
Grammar. In Proceedings of the Seventh Con-
ference on International Language Resources and
Evaluation (LREC 2010), Valletta, Malta.
559
Inderjeet Mani and James Pustejovsky. 2004. Tem-
poral Discourse Models for Narrative Structure. In
Proceedings of the 2004 ACL Workshop on Dis-
course Annotation, pages 57?64.
Bea?ta Megyesi, Bengt Dahlqvist, E?va A?. Csato?, and
Joakim Nivre. 2010. The English-Swedish-Turkish
Parallel Treebank. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC?10), Valletta, Malta. Euro-
pean Language Resources Association (ELRA).
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective Sharing for Multilingual Depen-
dency Parsing. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 629?637,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Joakim Nivre, Igor Boguslavsky, and Leonid Iomdin.
2008. Parsing the SynTagRus Treebank. In Pro-
ceedings of COLING08, pages 641?648.
Rachel Nordlinger and Louisa Sadler. 2007. Verb-
less Clauses: Revealing the Structure within. In An-
nie Zaenen, Jane Simpson, Tracy Holloway King,
Jane Grimshaw, Joan Maling, and Chris Manning,
editors, Architectures, Rules and Preferences: A
Festschrift for Joan Bresnan, pages 139?160. CSLI
Publications.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: Modular NLP Framework. In Proceedings
of the 7th International Conference on Advances in
Natural Language Processing (IceTAL 2010), pages
293?304.
Victoria Rose?n, Paul Meurer, and Koenraad de Smedt.
2009. LFG Parsebanker: A Toolkit for Building and
Searching a Treebank as a Parsed Corpus. In Pro-
ceedings of the 7th International Workshop on Tree-
banks and Linguistic Theories (TLT7), pages 127?
133, Utrecht. LOT.
Victoria Rose?n, Koenraad De Smedt, Paul Meurer, and
Helge Dyvik. 2012. An Open Infrastructure for Ad-
vanced Treebanking. In META-RESEARCH Work-
shop on Advanced Treebanking at LREC2012, pages
22?29, Istanbul, Turkey.
Manuela Sanguinetti and Cristina Bosco. 2011. Build-
ing the Multilingual TUT Parallel Treebank. In Pro-
ceedings of Recent Advances in Natural Language
Processing, pages 19?28.
Sebastian Sulger. 2011. A Parallel Analysis of have-
Type Copular Constructions in have-Less Indo-
European Languages. In Proceedings of the LFG
?11 Conference. CSLI Publications.
Josef van Genabith and Dick Crouch. 1996. Direct and
Underspecified Interpretations of LFG f-structures.
In Proceedings of the 16th International Conference
on Computational Linguistics (COLING-96), vol-
ume 1, pages 262?267, Copenhagen, Denmark.
Martin Volk, Anne Go?hring, Torsten Marek,
and Yvonne Samuelsson. 2010. SMUL-
TRON (version 3.0) ? The Stock-
holm MULtilingual parallel TReebank.
http://www.cl.uzh.ch/research/paralleltreebanks en.
html.
560
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 85?93,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Lemmatization and Lexicalized Statistical Parsing of Morphologically Rich
Languages: the Case of French
Djam? Seddah
Alpage Inria & Univ. Paris-Sorbonne
Paris, France
Grzegorz Chrupa?a
Spoken Language System, Saarland Univ.
Saarbr?cken, Germany
?zlem ?etinog?lu and Josef van Genabith
NCLT & CNGL, Dublin City Univ.
Dublin, Ireland
Marie Candito
Alpage Inria & Univ. Paris 7
Paris, France
Abstract
This paper shows that training a lexicalized
parser on a lemmatized morphologically-rich
treebank such as the French Treebank slightly
improves parsing results. We also show that
lemmatizing a similar in size subset of the En-
glish Penn Treebank has almost no effect on
parsing performance with gold lemmas and
leads to a small drop of performance when au-
tomatically assigned lemmas and POS tags are
used. This highlights two facts: (i) lemmati-
zation helps to reduce lexicon data-sparseness
issues for French, (ii) it also makes the pars-
ing process sensitive to correct assignment of
POS tags to unknown words.
1 Introduction
Large parse-annotated corpora have led to an explo-
sion of interest in statistical parsing methods, includ-
ing the development of highly successful models for
parsing English using the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1994)). Over the
last 10 years, parsing performance on the PTB has
hit a performance plateau of 90-92% f-score using
the PARSEVAL evaluation metric. When adapted to
other language/treebank pairs (such as German, He-
brew, Arabic, Italian or French), to date these mod-
els have performed much worse.
A number of arguments have been advanced
to explain this performance gap, including limited
amounts of training data, differences in treebank an-
notation schemes, inadequacies of evaluation met-
rics, linguistic factors such as the degree of word or-
der variation, the amount of morphological informa-
tion available to the parser as well as the effects of
syncretism prevalent in many morphologically rich
languages.
Even though none of these arguments in isola-
tion can account for the systematic performance gap,
a pattern is beginning to emerge: morphologically
rich languages tend to be susceptible to parsing per-
formance degradation.
Except for a residual clitic case system, French
does not have explicit case marking, yet its mor-
phology is considerably richer than that of English,
and French is therefore a candidate to serve as an
instance of a morphologically rich language (MRL)
that requires specific treatment to achieve reasonable
parsing performance.
Interestingly, French also exhibits a limited
amount of word order variation occurring at dif-
ferent syntactic levels including (i) the word level
(e.g. pre or post nominal adjective, pre or post ver-
bal adverbs); (ii) phrase level (e.g. possible alterna-
tions between post verbal NPs and PPs). In order
to avoid discontinuous constituents as well as traces
and coindexations, treebanks for this language, such
as the French Treebank (FTB, (Abeill? et al, 2003))
or the Modified French Treebank (MFT, (Schluter
and van Genabith, 2007)), propose a flat annota-
tion scheme with a non-configurational distinction
between adjunct and arguments.
Finally, the extraction of treebank grammars from
the French treebanks, which contain less than a third
of the annotated data as compared to PTB, is subject
to many data sparseness issues that contribute to a
performance ceiling, preventing the statistical pars-
ing of French to reach the same level of performance
as for PTB-trained parsers (Candito et al, 2009).
This data sparseness bottleneck can be summa-
rized as a problem of optimizing a parsing model
along two axes: the grammar and the lexicon. In
both cases, the goal is either to get a more compact
grammar at the rule level or to obtain a consider-
85
ably less sparse lexicon. So far, both approaches
have been tested for French using different means
and with different degrees of success.
To obtain better grammars, Schluter and van Gen-
abith (2007) extracted a subset of an early release
of the FTB and carried out extensive restructuring,
extensions and corrections (referred to as the Modi-
fied French Treebank MFT) to support grammar ac-
quisition for PCFG-based LFG Parsing (Cahill et
al., 2004) while Crabb? and Candito (2008) slightly
modified the original FTB POS tagset to optimize
the grammar with latent annotations extracted by the
Berkeley parser (BKY, (Petrov et al, 2006)).
Moreover, research oriented towards adapting
more complex parsing models to French showed
that lexicalized models such as Collins? model 2
(Collins, 1999) can be tuned to cope effectively with
the flatness of the annotation scheme in the FTB,
with the Charniak model (Charniak, 2000) perform-
ing particularly well, but outperformed by the BKY
parser on French data (Seddah et al, 2009).
Focusing on the lexicon, experiments have been
carried out to study the impact of different forms of
word clustering on the BKY parser trained on the
FTB. Candito et al (2009) showed that using gold
lemmatization provides a significant increase in per-
formance. Obviously, less sparse lexical data which
retains critical pieces of information can only help a
model to perform better. This was shown in (Candito
and Crabb?, 2009) where distributional word clus-
ters were acquired from a 125 million words corpus
and combined with inflectional suffixes extracted
from the training data. Training the BKY parser
with 1000 clusters boosts its performance to the cur-
rent state-of-the-art with a PARSEVAL F1 score of
88.28% (baseline was 86.29 %).
We performed the same experiment using the
CHARNIAK parser and recorded only a small im-
provement (from 84.96% to 85.51%). Given the
fact that lexical information is crucial for lexicalized
parsers in the form of bilexical dependencies, this
result raises the question whether this kind of clus-
tering is in fact too drastic for lexicalized parsers as
it may give rise to head-to-head dependencies which
are too coarse. To answer this question, in this paper
we explore the impact of lemmatization, as a (rather
limited) constrained form of clustering, on a state-
of-the-art lexicalized parser (CHARNIAK). In order
to evaluate the influence of lemmatization on this
parser (which is known to be highly tuned for En-
glish) we carry out experiments on both the FTB and
on a lemmatized version of the PTB. We used gold
lemmatization when available and an automatic sta-
tistical morphological analyzer (Chrupa?a, 2010) to
provide more realistic parsing results.
The idea is to verify whether lemmatization will help
to reduce data sparseness issues due to the French
rich morphology and to see if this process, when
applied to English will harm the performance of a
parser optimized for the limited morphology of En-
glish.
Our results show that the key issue is the way un-
seen tokens (lemmas or words) are handled by the
CHARNIAK parser. Indeed, using pure lemma is
equally suboptimal for both languages. On the other
hand, feeding the parser with both lemma and part-
of-speech slightly enhances parsing performance for
French.
We first describe our data sets in Section 2, intro-
duce our data driven morphology process in Section
3, then present experiments in Section 4. We dis-
cuss our results in Section 5 and compare them with
related research in Section 6 before concluding and
outlining further research.
2 Corpus
THE FRENCH TREEBANK is the first annotated and
manually corrected treebank for French. The data is
annotated with labeled constituent trees augmented
with morphological annotations and functional an-
notations of verbal dependents. Its key properties,
compared with the PTB, are the following :
Size: The FTB consists of 350,931 tokens and
12,351 sentences, that is less than a third of the size
of PTB. The average length of a sentence is 28.41
tokens. By contrast, the average sentence length in
the Wall Street Journal section of the PTB is 25.4
tokens.
A Flat Annotation Scheme: Both the FTB and the
PTB are annotated with constituent trees. However,
the annotation scheme is flatter in the FTB. For in-
stance, there are no VPs for finite verbs and only one
sentential level for clauses or sentences whether or
not they are introduced by a complementizer. Only
the verbal nucleus (VN) is annotated and comprises
86
the verb, its clitics, auxiliaries, adverbs and nega-
tion.
Inflection: French morphology is richer than En-
glish and leads to increased data sparseness for sta-
tistical parsing. There are 24,098 lexical types in
the FTB, with an average of 16 tokens occurring for
each type.
Compounds: Compounds are explicitly annotated
and very frequent in the treebank: 14.52% of to-
kens are part of a compound. Following Candito
and Crabb? (2009), we use a variation of the tree-
bank where compounds with regular syntactic pat-
terns have been expanded. We refer to this instance
as FTB-UC.
Lemmatization: Lemmas are included in the tree-
bank?s morphological annotations and denote an ab-
straction over a group of inflected forms. As there
is no distinction between semantically ambiguous
lexemes at the word form level, polysemic homo-
graphs with common inflections are associated with
the same lemma (Abeill? et al, 2003). Thus, except
for some very rare cases, a pair consisting of a word
form and its part-of-speech unambiguously maps to
the same lemma.
2.1 Lemmatizing the Penn Treebank
Unlike the FTB, the PTB does not have gold lem-
mas provided within the treebank. We use the finite
state morphological analyzer which comes within
the English ParGram Grammar (Butt et al, 1999) for
lemmatization. For open class words (nouns, verbs,
adjectives, adverbs) the word form is sent to the mor-
phological analyzer. The English ParGram morpho-
logical analyzer outputs all possible analyses of the
word form. The associated gold POS from the PTB
is used to disambiguate the result. The same process
is applied to closed class words where the word form
is different from the lemma (e.g. ?ll for will). For the
remaining parts of speech the word form is assigned
to the lemma.
Since gold lemmas are not available for the PTB,
a large-scale automatic evaluation of the lemmatizer
is not possible. Instead, we conducted two manual
evaluations. First, we randomly extracted 5 sam-
ples of 200 <POS,word> pairs from Section 23 of
the PTB. Each data set is fed into the lemmatiza-
tion script, and the output is manually checked. For
the 5x200 <POS,word> sets the number of incorrect
lemmas is 1, 3, 2, 0, and 2. The variance is small
indicating that the results are fairly stable. For the
second evaluation, we extracted each unseen word
from Section 23 and manually checked the accuracy
of the lemmatization. Of the total of 1802 unseen
words, 394 words are associated with an incorrect
lemma (331 unique) and only 8 with an incorrect
<POS,lemma> pair (5 unique). For an overall un-
seen word percentage of 3.22%, the lemma accu-
racy is 77.70%. If we assume that all seen words
are correctly lemmatized, overall accuracy would be
99.28%.
2.2 Treebank properties
In order to evaluate the influence of lemmatization
on comparable corpora, we extracted a random sub-
set of the PTB with properties comparable to the
FTB-UC (mainly with respect to CFG size and num-
ber of tokens). We call this PTB subset S.PTB. Ta-
ble 1 presents a summary of some relevant features
of those treebanks.
FTBUC S.PTB PTB
# of tokens 350,931 350,992 1,152,305
# of sentences 12,351 13,811 45,293
average length 28,41 25.41 25.44
CFG size 607,162 638,955 2,097,757
# unique CFG rules 43,413 46,783 91,027
# unique word forms 27,130 26,536 47,678
# unique lemmas 17,570 20,226 36,316
ratio words/lemma 1.544 1.311 1.312
Table 1: French and Penn Treebanks properties
Table 1 shows that the average number of word
forms associated with a lemma (i.e. the lemma ratio)
is higher in the FTB-UC (1.54 words/lemma) than in
the PTB (1.31). Even though the PTB ratio is lower,
it is still large enough to suggest that even the limited
English morphology should be taken into account
when aiming at reducing lexicon sparseness.
Trying to learn French and English morphology
in a data driven fashion in order to predict lemma
from word forms is the subject of the next section.
3 Morphology learning
In order to assign morphological tags and lemmas
to words we use the MORFETTE model (Chrupa?a,
2010), which is a variation of the approach described
in (Chrupa?a et al, 2008).
87
MORFETTE is a sequence labeling model which
combines the predictions of two classification mod-
els (one for morphological tagging and one for
lemmatization) at decoding time, using beam search.
3.1 Overview of the Morfette model
The morphological classes correspond simply to the
(fine-grained) POS tags. Lemma classes are edit
scripts computed from training data: they specify
which string manipulations (such as character dele-
tions and insertions) need to be performed in order
to transform the input string (word form) into the
corresponding output string (lemma).
The best sequence of lemmas and morphological
tags for input sentence x is defined as:
(?l, m?) = arg max
(l,m)
P (l,m|x)
The joint probability is decomposed as follows:
P (l0...li,m0...mi|x) =PL(li|mi,x)PM (mi|x)
? P (m0...mi?1, l0...li?1|x)
where PL(li|mi,x) is the probability of lemma class
l at position i according to the lemma classifier,
PM (mi|x) is the probability of the tag m at posi-
tion i according to the morphological tag classifier,
and x is the sequence of words to label.
While Chrupa?a et al (2008) use Maximum En-
tropy training to learn PM and PL, here we learn
them using Averaged Perceptron algorithm due to
Freund and Schapire (1999). It is a much simpler
algorithm which in many scenarios (including ours)
performs as well as or better than MaxEnt.
We also use the general Edit Tree instantiation of
the edit script as developed in (Chrupa?a, 2008). We
find the longest common substring (LCS) between
the form w and the lemma w?. The portions of the
string in the word form before (prefix) and after (suf-
fix) the LCS need to be modified in some way, while
the LCS (stem) stays the same. If there is no LCS,
then we simply record that we need to replace w
with w? . As for the modifications to the prefix and
the suffix, we apply the same procedure recursively:
we try to find the LCS between the prefix of w and
the prefix of w?. If we find one, we recurse; if we do
not, we record the replacement; we do the same for
the suffix.
3.2 Data Set
We trained MORFETTE on the standard splits of the
FTB with the first 10% as test set, the next 10% for
the development set and the remaining for training
(i.e. 1235/1235/9881 sentences). Lemmas and part-
of-speech tags are given by the treebank annotation
scheme.
As pointed out in section 2.1, PTB?s lemmas have
been automatically generated by a deterministic pro-
cess, and only a random subset of them have been
manually checked. For the remainder of this paper,
we treat them as gold, regardless of the errors in-
duced by our PTB lemmatizer.
The S.PTB follows the same split as the FTB-UC,
first 10% for test, next 10% for dev and the last 80%
for training (i.e. 1380/1381/11050 sentences).
MORFETTE can optionally use a morphological
lexicon to extract features. For French, we used the
extended version of Lefff (Sagot et al, 2006) and for
English, the lexicon used in the Penn XTAG project
(Doran et al, 1994). We reduced the granularity of
the XTAG tag set, keeping only the bare categories.
Both lexicons contain around 225 thousands word
form entries.
3.3 Performance on French and English
Table 2 presents results of MORFETTE applied to the
development and test sets of our treebanks. Part-of-
speech tagging performance for French is state-of-
the-art on the FTB-UC, with an accuracy of 97.68%,
on the FTB-UC test set, only 0.02 points (absolute)
below the MaxEnt POS tagger of Denis and Sagot
(2009). Comparing MORFETTE?s tagging perfor-
mance for English is a bit more challenging as we
only trained on one third of the full PTB and evalu-
ated on approximately one section, whereas results
reported in the literature are usually based on train-
ing on sections 02-18 and evaluating on either sec-
tions 19-21 or 22-24. For this setting, state-of-the-
art POS accuracy for PTB tagging is around 97.33%.
On our PTB sample, MORFETTE achieves 96.36%
for all words and 89.64 for unseen words.
Comparing the lemmatization performance for both
languages on the same kind of data is even more dif-
ficult as we are not aware of any data driven lem-
matizer on the same data. However, with an overall
accuracy above 98% for the FTB-UC (91.5% for un-
88
seen words) and above 99% for the S.PTB (95% for
unseen words), lemmatization performs well enough
to properly evaluate parsing on lemmatized data.
FTBUC S.PTB
DEV All Unk. (4.8) All Unk. (4.67)
POS acc 97.38 91.95 96.36 88.90
Lemma acc 98.20 92.52 99.11 95.51
Joint acc 96.35 87.16 96.26 87.05
TEST All Unk. (4.62) All Unk. (5.04)
POS acc 97.68 90.52 96.53 89.64
Lemma acc 98.36 91.54 99.13 95.72
Joint acc 96.74 85.28 96.45 88.49
Table 2: POS tagging and lemmatization performance on
the FTB and on the S.PTB
4 Parsing Experiments
In this section, we present the results of two sets
of experiments to evaluate the impact of lemmatiza-
tion on the lexicalized statistical parsing of two lan-
guages, one morphologically rich (French), but with
none of its morphological features exploited by the
CHARNIAK parser, the other (English) being quite
the opposite, with the parser developed mainly for
this language and PTB annotated data. We show that
lemmatization results in increased performance for
French, while doing the same for English penalizes
parser performance.
4.1 Experimental Protocol
Data The data sets described in section 3.2 are used
throughout. The version of the CHARNIAK parser
(Charniak, 2000) was released in August 2005 and
recently adapted to French (Seddah et al, 2009).
Metrics We report results on sentences of length
less than 40 words, with three evaluation met-
rics: the classical PARSEVAL Labeled brackets F1
score, POS tagging accuracy (excluding punctua-
tion tags) and the Leaf Ancestor metric (Sampson
and Babarczy, 2003) which is believed to be some-
what more neutral with respect to the treebank an-
notation scheme than PARSEVAL (Rehbein and van
Genabith, 2007).
Treebank tag sets Our experiments involve the in-
clusion of POS tags directly in tokens. We briefly
describe our treebank tag sets below.
? FTB-UC TAG SET: ?CC? This is the tag set de-
veloped by (Crabb? and Candito, 2008) (Table
4), known to provide the best parsing perfor-
mance for French (Seddah et al, 2009). Like in
the FTB, preterminals are the main categories,
but they are also augmented with a WH flag
for A, ADV, PRO and with the mood for verbs
(there are 6 moods). No information is propa-
gated to non-terminal symbols.
ADJ ADJWH ADV ADVWH CC CLO CLR CLS CS DET
DETWH ET I NC NPP P P+D P+PRO PONCT PREF PRO
PROREL PROWH V VIMP VINF VPP VPR VS
Table 4: CC tag set
? THE PTB TAG SET This tag set is described
at length in (Marcus et al, 1994) and contains
supplementary morphological information (e.g.
number) over and above what is represented in
the CC tag set for French. Note that some infor-
mation is marked at the morphological level in
English (superlative, ?the greatest (JJS)?) and
not in French (? le plus (ADV) grand (ADJ)?).
CC CD DT EX FW IN JJ JJR JJS LS MD NN NNP NNPS
NNS PDT POS PRP PRP$ RB RBR RBS RP SYM TO UH
VB VBD VBG VBN VBP VBZ WDT WP WP$ WRB
Table 5: PTB tag set
4.2 Cross token variation and parsing impact
From the source treebanks, we produce 5 versions
of tokens: tokens are generated as either simple
POS tag, gold lemma, gold lemma+gold POS, word
form, and word form+gold POS. The token versions
successively add more morphological information.
Parsing results are presented in Table 3.
Varying the token form The results show that
having no lexical information at all (POS-only) re-
sults in a small drop of PARSEVAL performance for
French compared to parsing lemmas, while the cor-
responding Leaf Ancestor score is actually higher.
For English having no lexical information at all
leads to a drop of 2 points in PARSEVAL. The so-
called impoverished morphology of English appears
to bring enough morphological information to raise
tagging performance to 95.92% (from POS-only to
word-only).
For French the corresponding gain is only 2 points
of POS tagging accuracy. Moreover, between these
89
Tokens
POS-only
lemma-only
word-only
(1)lemma-POS
(1)word-POS
French Treebank UC
F1 score Pos acc. leaf-Anc.
84.48 100 93.97
84.77 94.23 93.76
84.96 96.26 94.08
86.83(1) 98.79 94.65
86.13(2) 98.4 94.46
Sampled Penn Treebank
F1 score Pos acc. leaf-Anc.
85.62 100 94.02
87.69 89.22 94.92
88.64 95.92 95.10
89.59(3) 99.97 95.41
89.53(4) 99.96 95.38
Table 3: Parsing performance on the FTB-UC and the S.PTB with tokens variations using gold lemmas and gold POS.
( p-value (1) & (2) = 0.007; p-value (3) & (4) = 0.146. All other configurations are statistically significant.)
two tokens variations, POS-only and word-only,
parsing results gain only half a point in PARSEVAL
and almost nothing in leaf Ancestor.
Thus, it seems that encoding more morphology
(i.e. including word forms) in the tokens does not
lead to much improvement for parsing French as op-
posed to English. The reduction in data sparseness
due to the use of lemmas alone is thus not sufficient
to counterbalance the lack of morphological infor-
mation.
However, the large gap between POS tagging
accuracy seen between lemma-only and word-only
for English indicates that the parser makes use of
this information to provide at least reasonable POS
guesses.
For French, only 0.2 points are gained for PAR-
SEVAL results between lemma-only to word-only,
while POS accuracy benefits a bit more from includ-
ing richer morphological information.
This raises the question whether the FTB-UC pro-
vides enough data to make its richer morphology in-
formative enough for a parsing model.
Suffixing tokens with POS tags It is only when
gold POS are added to the lemmas that one can see
the advantage of a reduced lexicon for French. In-
deed, performance peaks for this setting (lemma-
POS). The situation is not as clear for English, where
performance is almost identical when gold POS are
added to lemmas or words. POS Tagging is nearly
perfect, thus a performance ceiling is reached. The
very small differences between those two configura-
tions (most noticeable with the Leaf Ancestor score
of 95.41 vs. 95.38) indicates that the reduced lemma
lexicon is actually of some limited use but its impact
is negligible compared to perfect tagging.
While the lemma+POS setting clearly boosts per-
formance for parsing the FTB, the situation is less
clear for English. Indeed, the lemma+POS and the
word+POS gold variations give almost the same re-
sults. The fact that the POS tagging accuracy is close
to 100% in this mode shows that the key parameter
for optimum parsing performance in this experiment
is the ability to guess POS for unknown words well.
In fact, the CHARNIAK parser uses a two letter
suffix context for its tagging model, and when gold
POS are suffixed to any type of token (being lemma
or word form), the PTB POS tagset is used as a sub-
stitute for lack of morphology.
It should also be noted that the FTB-UC tag set
does include some discriminative features (such as
PART, INF and so on) but those are expressed by
more than two letters, and therefore a two letter
suffix tag cannot really be useful to discriminate
a richer morphology. For example, in the PTB,
the suffix BZ, as in VBZ, always refers to a verb,
whereas the FTB pos tag suffix PP, as in NPP
(Proper Noun) is also found in POS labels such as
VPP (past participle verb).
4.3 Realistic Setup: Using Morfette to help
parsing
Having shown that parsing French benefits from a
reduced lexicon is not enough as results imply that a
key factor is POS tag guessing. We therefore test our
hypothesis in a more realistic set up. We use MOR-
FETTE to lemmatize and tag raw words (instead of
the ?gold? lemma-based approach described above),
and the resulting corpus is then parsed using the cor-
responding training set.
In order to be consistent with PARSEVAL POS eval-
uation, which does not take punctuation POS into
account, we provide a summary of MORFETTE?s
performance for such a configuration in (Table 6).
Results shown in Table 7 confirm our initial hy-
90
POS acc Lemma acc Joint acc
FTB-UC 97.34 98.12 96.26
S.PTB 96.15 99.04 96.07
Table 6: PARSEVAL Pos tagging accuracy of treebanks
test set
pothesis for French. Indeed, parsing performance
peaks with a setup involving automatically gener-
ated lemma and POS pairs, even though the differ-
ence with raw words+auto POS is not statistically
significant for the PARSEVAL F1 metric1. Note that
parser POS accuracy does not follow this pattern. It
is unclear exactly why this is the case. We specu-
late that the parser is helped by the reduced lexicon
but that performance suffers when a <lemma,POS>
pair has been incorrectly assigned by MORFETTE,
leading to an increase in unseen tokens. This is con-
firmed by parsing the same lemma but with gold
POS. In that case, parsing performance does not suf-
fer too much from CHARNIAK?s POS guessing on
unseen data.
For the S.PTB, results clearly show that both the
automatic <lemma,POS> and <word,POS> config-
urations lead to very similar results (yet statistically
significant with a F1 p-value = 0.027); having the
same POS accuracy indicates that most of the work
is done at the level of POS guessing for unseen
tokens, and in this respect the CHARNIAK parser
clearly takes advantage of the information included
in the PTB tag set.
F1 score Pos acc. leaf-Anc.
S.PTB
auto lemma only 87.11 89.82 94.71
auto lemma+auto pos (a) 88.15 96.21 94.85
word +auto pos (b) 88.28 96.21 94.88
F1 p-value: (a) and (b) 0.027
auto lemma+gold pos 89.51 99.96 95,36
FTB-UC
auto lemma only 83.92 92.98 93.53
auto lemma+auto pos (c) 85.06 96.04 94.14
word +auto pos (d) 84.99 96.47 94.09
F1 p-value: (c) and (d) 0.247
auto lemma+gold pos 86.39 97.35 94.68
Table 7: Realistic evaluation of parsing performance
1Statistical significance is computed using Dan Bikel?s
stratified shuffling implementation: www.cis.upenn.edu/
~dbikel/software.html.
5 Discussion
When we started this work, we wanted to explore
the benefit of lemmatization as a means to reduce
data sparseness issues underlying statistical lexical-
ized parsing of small treebanks for morphologically
rich languages, such as the FTB. We showed that
the expected benefit of lemmatization, a less sparse
lexicon, was in fact hidden by the absence of inflec-
tional information, as required by e.g. the CHAR-
NIAK parser to provide good POS guesses for un-
seen words. Even the inclusion of POS tags gen-
erated by a state-of-the-art tagger (MORFETTE) did
not lead to much improvement compared to a parser
run in a regular bare word set up.
An unexpected effect is that the POS accuracy
of the parser trained on the French data does not
reach the same level of performance as our tag-
ger (96.47% for <word, auto POS> vs. 97.34% for
MORFETTE). Of course, extending the CHARNIAK
tagging model to cope with lemmatized input should
be enough, because its POS guessing model builds
on features such as capitalization, hyphenation and
a two-letter suffix (Charniak, 2000). Those features
are not present in our current lemmatized input and
thus cannot be properly estimated.
CHARNIAK also uses the probability that a given
POS is realized by a previously unobserved word.
If any part of a <lemma,POS> pair is incorrect, the
number of unseen words in the test set would be
higher than the one estimated from the training set,
which only contained correct lemmas and POS tags
in our setting. This would lead to unsatisfying POS
accuracy. This inadequate behavior of the unknown
word tagging model may be responsible for the POS
accuracy result for <auto lemma> (cf. Table 7, lines
<auto lemma only> for both treebanks).
We believe that this performance degradation (or
in this case the somewhat less than expected im-
provement in parsing results) calls for the inclusion
of all available lexical information in the parsing
model. For example, nothing prevents a parsing
model to condition the generation of a head upon
a lemma, while the probability to generate a POS
would depend on both morphological features and
(potentially) the supplied POS.
91
6 Related Work
A fair amount of recent research in parsing morpho-
logically rich languages has focused on coping with
unknowns words and more generally with the small
and limited lexicons acquired from treebanks. For
instance, Goldberg et al (2009) augment the lex-
icon for a generative parsing model by including
lexical probabilities coming from an external lexi-
con. These are estimated using an HMM tagger with
Baum-Welch training. This method leads to a sig-
nificant increase of parsing performance over pre-
viously reported results for Modern Hebrew. Our
method is more stratified: external lexical resources
are included as features for MORFETTE and there-
fore are not directly seen by the parser besides gen-
erated lemma and POS.
For parsing German, Versley and Rehbein (2009)
cluster words according to linear context features.
The clusters are then integrated as features to boost a
discriminative parsing model to cope with unknown
words. Interestingly, they also include all possible
information: valence information, extracted from a
lexicon, is added to verbs and preterminal nodes are
annotated with case/number. This leads their dis-
criminative model to state-of-the-art results for pars-
ing German.
Concerning French, Candito and Crabb? (2009)
present the results of different clustering methods
applied to the parsing of FTB with the BKY parser.
They applied an unsupervised clustering algorithm
on the 125 millions words ?Est Republicain? corpus
to get a reduced lexicon of 1000 clusters which they
then augmented with various features such as capi-
talization and suffixes. Their method is the best cur-
rent approach for the probabilistic parsing of French
with a F1 score (<=40) of 88.29% on the standard
test set. We run the CHARNIAK parser on their clus-
terized corpus. Table 8 summarizes the current state-
of-the-art for lexicalized parsing on the FTB-UC.2
Clearly, the approach consisting in extending clus-
ters with features and suffixes seems to improve
CHARNIAK?s performance more than our method.
2For this comparison, we also trained the CHARNIAK parser
on a disinflected variation of the FTB-UC. Disinflection is a de-
terministic, lexicon based process, standing between stemming
and lemmatization, which preserves POS assignment ambigui-
ties (Candito and Crabb?, 2009).
In that case, the lexicon is drastically reduced, as
well as the amount of out of vocabulary words
(OOVs). Nevertheless, the relatively low POS ac-
curacy, with only 36 OOVs, for this configuration
confirms that POS guessing is the current bottleneck
if a process of reducing the lexicon increases POS
assignment ambiguities.
tokens F1 Pos acc % of OOVs
raw word (a) 84.96 96.26 4.89
auto <lemma,pos> (b) 85.06 96.04 6.47
disinflected (c) 85.45 96.51 3.59
cluster+caps+suffixes (d) 85.51 96.89 0.10
Table 8: CHARNIAK parser performance summary on the
FTB-UC test set (36340 tokens). Compared to (a), all F1 re-
sults, but (b), are statistically significant (p-values < 0.05), dif-
ferences between (c) & (d), (b) & (c) and (b) & (d) are not
(p-values are resp. 0.12, 0.41 and 0.11). Note that the (b) &
(d) p-value for all sentences is of 0.034, correlating thus the
observed gap in parsing performance between these two con-
figuration.
7 Conclusion
We showed that while lemmatization can be of
some benefit to reduce lexicon size and remedy data
sparseness for a MRL such as French, the key factor
that drives parsing performance for the CHARNIAK
parser is the amount of unseen words resulting from
the generation of <lemma,POS> pairs for the FTB-
UC. For a sample of the English PTB, morphologi-
cal analysis did not produce any significant improve-
ment.
Finally, even if this architecture has the potential to
help out-of-domain parsing, adding morphological
analysis on top of an existing highly tuned statisti-
cal parsing system can result in suboptimal perfor-
mance. Thus, in future we will investigate tighter
integration of the morphological features with the
parsing model.
Acknowledgments
D. Seddah and M. Candito were supported by the ANR
Sequoia (ANR-08-EMER-013); ?. ?etinog?lu and J.
van Genabith by the Science Foundation Ireland (Grant
07/CE/I1142) as part of the Centre for Next Generation
Localisation at Dublin City University; G. Chrupa?a by
BMBF project NL-Search (contract 01IS08020B).
92
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,
2003. Building a Treebank for French. Kluwer, Dor-
drecht.
Miriam Butt, Mar?a-Eugenia Ni?o, and Fr?d?rique
Segond. 1999. A Grammar Writer?s Cookbook. CSLI
Publications, Stanford, CA.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 320?
327, Barcelona, Spain.
Marie Candito and Beno?t Crabb?. 2009. Im-
proving generative statistical parsing with semi-
supervised word clustering. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 138?141, Paris, France, Octo-
ber. Association for Computational Linguistics.
Marie Candito, Benoit Crabb?, and Djam? Seddah. 2009.
On statistical parsing of french with supervised and
semi-supervised strategies. In EACL 2009 Workshop
Grammatical inference for Computational Linguistics,
Athens, Greece.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics (NAACL 2000), pages 132?
139, Seattle, WA.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with morfette. In
In Proceedings of LREC 2008, Marrakech, Morocco.
ELDA/ELRA.
Grzegorz Chrupa?a. 2008. Towards a machine-learning
architecture for lexical functional grammar parsing.
Ph.D. thesis, Dublin City University.
Grzegorz Chrupa?a. 2010. Morfette: A tool for su-
pervised learning of morphology. http://sites.
google.com/site/morfetteweb/. Version
0.3.1.
Michael Collins. 1999. Head Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Pascal Denis and Beno?t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Proc.
of PACLIC, Hong Kong, China.
Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srini-
vas, and Martin Zaidel. 1994. Xtag system: A wide
coverage grammar for english. In Proceedings of the
15th conference on Computational linguistics, pages
922?928, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine learning, 37(3):277?296.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In Proc. of EACL-09, pages 327?335, Athens, Greece.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for german.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague.
Benoit Sagot, Lionel Cl?ment, Eric V. de La Clergerie,
and Pierre Boullier. 2006. The lefff 2 syntactic lexi-
con for french: Architecture, acquisition, use. Proc. of
LREC 06, Genoa, Italy.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djam? Seddah, Marie Candito, and Benoit Crabb?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
93
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 135?145,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
(Re)ranking Meets Morphosyntax: State-of-the-art Results
from the SPMRL 2013 Shared Task?
Anders Bjo?rkelund?, O?zlem C?etinog?lu?, Richa?rd Farkas?, Thomas Mu?ller??, and Wolfgang Seeker?
?Institute for Natural Language Processing , University of Stuttgart, Germany
?Department of Informatics, University of Szeged, Hungary
?Center for Information and Language Processing, University of Munich, Germany
{anders,ozlem,muellets,seeker}@ims.uni-stuttgart.de
rfarkas@inf.u-szeged.hu
Abstract
This paper describes the IMS-SZEGED-CIS
contribution to the SPMRL 2013 Shared Task.
We participate in both the constituency and
dependency tracks, and achieve state-of-the-
art for all languages. For both tracks we make
significant improvements through high quality
preprocessing and (re)ranking on top of strong
baselines. Our system came out first for both
tracks.
1 Introduction
In this paper, we present our contribution to the 2013
Shared Task on Parsing Morphologically Rich Lan-
guages (MRLs). MRLs pose a number of interesting
challenges to today?s standard parsing algorithms,
for example a free word order and, due to their rich
morphology, greater lexical variation that aggravates
out-of-vocabulary problems considerably (Tsarfaty
et al, 2010).
Given the wide range of languages encompassed
by the term MRL, there is, as of yet, no clear con-
sensus on what approaches and features are gener-
ally important for parsing MRLs. However, devel-
oping tailored solutions for each language is time-
consuming and requires a good understanding of
the language in question. In our contribution to the
SPMRL 2013 Shared Task (Seddah et al, 2013), we
therefore chose an approach that we could apply to
all languages in the Shared Task, but that would also
allow us to fine-tune it for individual languages by
varying certain components.
?Authors in alphabetical order.
For the dependency track, we combined the n-
best output of multiple parsers and subsequently
ranked them to obtain the best parse. While this
approach has been studied for constituency parsing
(Zhang et al, 2009; Johnson and Ural, 2010; Wang
and Zong, 2011), it is, to our knowledge, the first
time this has been applied successfully within de-
pendency parsing. We experimented with different
kinds of features in the ranker and developed fea-
ture models for each language. Our system ranked
first out of seven systems for all languages except
French.
For the constituency track, we experimented
with an alternative way of handling unknown words
and applied a products of Context Free Grammars
with Latent Annotations (PCFG-LA) (Petrov et al,
2006), whose output was reranked to select the best
analysis. The additional reranking step improved
results for all languages. Our system beats vari-
ous baselines provided by the organizers for all lan-
guages. Unfortunately, no one else participated in
this track.
For both settings, we made an effort to automat-
ically annotate our data with the best possible pre-
processing (POS, morphological information). We
used a multi-layered CRF (Mu?ller et al, 2013) to
annotate each data set, stacking with the information
provided by the organizers when this was beneficial.
The high quality of our preprocessing considerably
improved the performance of our systems.
The Shared Task involved a variety of settings as
to whether gold or predicted part-of-speech tags and
morphological information were available, as well
as whether the full training set or a smaller (5k sen-
135
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
MarMoT 97.38/92.22 97.02/87.08 97.61/90.92 98.10/91.80 97.09/97.67 98.72/97.59 94.03/87.68 98.12/90.84 97.27/97.13
Stacked 98.23/89.05 98.56/92.63 97.83/97.62
Table 1: POS/morphological feature accuracies on the development sets.
tences) training set was used for training. Through-
out this paper we focus on the settings with pre-
dicted preprocessing information with gold segmen-
tation and the full1 training sets. Unless stated other-
wise, all given numbers are drawn from experiments
in this setting. For all other settings, we refer the
reader to the Shared Task overview paper (Seddah et
al., 2013).
The remainder of the paper is structured as fol-
lows: We present our preprocessing in Section 2 and
afterwards describe both our systems for the con-
stituency (Section 3) and for the dependency tracks
(Section 4). Section 5 discusses the results on the
Shared Task test sets. We conclude with Section 6.
2 Preprocessing
We first spent some time on preparing the data sets,
in particular we automatically annotated the data
with high-quality POS and morphological informa-
tion. We consider this kind of preprocessing to be an
essential part of a parsing system, since the quality
of the automatic preprocessing strongly affects the
performance of the parsers.
Because our tools work on CoNLL09 format, we
first converted the training data from the CoNLL06
format to CoNLL09. We thus had to decide whether
to use coarse or fine part-of-speech (POS) tags. In
a preliminary experiment we found that fine tags are
the better option for all languages but Basque and
Korean. For Korean the reason seems to be that the
fine tag set is huge (> 900) and that the same infor-
mation is also provided in the feature column.
We predict POS tags and morphological features
jointly using the Conditional Random Field (CRF)
tagger MarMoT2 (Mu?ller et al, 2013).
MarMoT incrementally creates forward-
backward lattices of increasing order to prune
the sizable space of possible morphological analy-
ses. We use MarMoT with the default parameters.
1Although, for Hebrew and Swedish only 5k sentences were
available for training, and the two settings thus coincide.
2https://code.google.com/p/cistern/
Since morphological dictionaries can improve au-
tomatic POS tagging considerably, we also created
such dictionaries for each language. For this, we an-
alyzed the word forms provided in the data sets with
language-specific morphological analyzers except
for Hebrew and German where we just extracted the
morphological information from the lattice files pro-
vided by the organizers. For the other languages
we used the following tools: Arabic: AraMorph
a reimplementation of Buckwalter (2002), Basque:
Apertium (Forcada et al, 2011), French: an IMS
internal tool,3 Hungarian: Magyarlanc (Zsibrita et
al., 2013), Korean: HanNanum (Park et al, 2010),
Polish: Morfeusz (Wolin?ski, 2006), and Swedish:
Granska (Domeij et al, 2000).
The created dictionaries were shared with the
other Shared Task participants. We used these dic-
tionaries as additional features for MarMoT.
For some languages we also integrated the pre-
dicted tags provided by the organizers into the fea-
ture model. These stacked models gave improve-
ments for Swedish, Polish and Basque (cf. Table 1
for accuracies).
For the full setting the training data was annotated
using 5-fold jackknifing. In the 5k setting, we addi-
tionally added all sentences not present in the parser
training data to the training data sets of the tagger.
This is similar to the predicted 5k files provided by
the organizers, where more training data than the 5k
was also used for prediction.
Table 3 presents a comparison between our graph-
based baseline parser using the preprocessing ex-
plained in this section (denoted mate) and the
preprocessing provided by the organizers (denoted
mate?). Our preprocessing yields improvements
for all languages but Swedish. The worse perfor-
mance for Swedish is due to the fact that the pre-
dictions provided by the organizers were produced
by models that were trained on a much larger data
3The French morphology was written by Zhenxia Zhou,
Max Kisselew and Helmut Schmid. It is an extension of Zhou
(2007) and implemented in SFST (Schmid, 2005).
136
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Berkeley 78.24 69.17 79.74 81.74 87.83 83.90 70.97 84.11 74.50
Replaced 78.70 84.33 79.68 82.74 89.55 89.08 82.84 87.12 75.52
Product 80.30 86.21 81.42 84.56 90.49 89.80 84.15 88.32 79.25
Reranked 81.24 87.35 82.49 85.01 90.49 91.07 84.63 88.40 79.53
Table 2: PARSEVAL scores on the development sets.
set. The comparison with other parsers demonstrates
that for some languages (e.g., Hebrew or Korean)
the improvements due to better preprocessing can
be greater than the improvements due to a better
parser. For instance, for Hebrew the parser trained
on the provided preprocessing is more than three
points (LAS) behind the three parsers trained on
our own preprocessing. However, the difference be-
tween these three parsers is less than a point.
3 Constituency Parsing
The phrase structure parsing pipeline is based on
products of Context Free Grammars with Latent An-
notations (PCFG-LA) (Petrov et al, 2006) and dis-
criminative reranking. We further replace rare words
by their predicted morphological analysis.
We preprocess the treebank trees by removing the
morphological annotation of the POS tags and the
function labels of all non-terminals. We also reduce
the 177 compositional Korean POS tags to their first
atomic tag, which results in a POS tag set of 9 tags.
PCFG-LAs are incrementally built by split-
ting non-terminals, refining parameters using EM-
training and reversing splits that only cause small
increases in likelihood.
Running the Berkeley Parser4 ? the reference im-
plementation of PCFG-LAs ? on the data sets results
in the PARSEVAL scores given in Table 2 (Berke-
ley). The Berkeley parser only implements a simple
signature-based unknown word model that seems to
be ineffective for some of the languages, especially
Basque and Korean.
We thus replace rare words (frequency < 20) by
the predicted morphological tags of Section 2 (or the
true morphological tag for the gold setup). The intu-
ition is that our discriminative tagger has a more so-
phisticated unknown word treatment than the Berke-
ley parser, taking for example prefixes, suffixes and
4http://code.google.com/p/
berkeleyparser/
the immediate lexical context into account. Further-
more, the morphological tag contains most of the
necessary syntactic information. An exception, for
instance, might be the semantic information needed
to disambiguate prepositional attachment. We think
that replacing rare words by tags has an advan-
tage over constraining the pre-terminal layer of the
parser, because the parser can still decide to assign
a different tag, for example in cases were the tag-
ger produces errors due to long-distance dependen-
cies. The used frequency threshold of 20 results
in token replacement rates of 18% (French) to 57%
(Korean and Polish), which correspond to 209 (for
Polish) to 3221 (for Arabic) word types that are not
replaced. The PARSEVAL scores for the described
method are again given in Table 2 (Replaced). The
method yields improvements for all languages ex-
cept for French where we observe a drop of 0.06.
The improvements range from 0.46 for Arabic to
1.02 for Swedish, 3.1 for Polish and more than 10
for Basque and Korean.
To further improve results, we employ the
product-of-grammars procedure (Petrov, 2010),
where different grammars are trained on the same
data set but with different initialization setups. We
trained 8 grammars and used tree-level inference.
In Table 2 (Product) we can see that this leads to
improvements from 0.72 for Hungarian to 3.73 for
Swedish.
On the 50-best output of the product parser,
we also carry out discriminative reranking. The
reranker is trained for the maximum entropy objec-
tive function of Charniak and Johnson (2005) and
use the standard feature set ? without language-
specific feature engineering ? from Charniak and
Johnson (2005) and Collins (2000). We use a
slightly modified version of the Mallet toolkit (Mc-
Callum, 2002) for reranking.
Improvements range from negligible differences
(< .1) for Hebrew and Polish to substantial differ-
ences (> 1.) for Basque, French, and Hungarian.
137
mate parser
best-first
parser
turboparser
merged list
of 50-100 best
trees/sentence
merged list
scored by
all parsers
ranker
ptb trees
Parsing Ranking
IN OUT
scores
scores
scores
features
Figure 1: Architecture of the dependency ranking system.
For our final submission, we used the reranker
output for all languages except French, Hebrew, Pol-
ish, and Swedish. This decision was based on an
earlier version of the evaluation setting provided by
the organizers. In this setup, reranking did not help
or was even harmful for these four languages. The
figures in Table 2 use the latest evaluation script and
are thus consistent with the test set results presented
in Section 5.
After the submission deadline the Shared Task
organizers made us aware that we had surprisingly
low exact match scores for Polish (e.g., 1.22 for
the reranked setup). The reason seems to be that
the Berkeley parser cannot produce unary chains of
length > 2. The gold development set contains 1783
such chains while the prediction of the reranked sys-
tem contains none. A particularly frequent unary
chain with 908 occurences in the gold data is ff ?
fwe ? formaczas. As this chain cannot be pro-
duced the parser leaves out the fwe phrase. Inserting
new fwe nodes between ff and formacszas nodes
raises the PARSEVAL scores of the reranked model
from 88.40 to 90.64 and the exact match scores to
11.34. This suggests that the Polish results could be
improved substantially if unary chains were properly
dealt with, for example by collapsing unary chains.5
4 Dependency Parsing
The core idea of our dependency parsing system
is the combination of the n-best output of several
5Thanks to Slav Petrov for pointing us to the unary chain
length limit.
parsers followed by a ranking step on the com-
bined list. Specifically, we first run two parsers that
each output their 50-best analyses for each sentence.
These 50-best analyses are merged together into one
single n-best list of between 50 and 100 analyses
(depending on the overlap between the n-best lists
of the two parsers). We then use the two parsers
plus an additional one to score each tree in the n-
best lists according to their parsing model, thus pro-
viding us with three different scores for each tree in
the n-best lists. The n-best lists are then given to
a ranker, which ranks the list using the three scores
and a small set of additional features in order to find
the best overall analysis. Figure 1 shows a schematic
of the process.
As a preprocessing step, we reduced the depen-
dency label set for the Hungarian training data.
The Hungarian dependency data set encodes ellipses
through composite edge labels which leads to a pro-
liferation of edge labels (more than 400). Since
many of these labels are extremely rare and thus hard
to learn for the parsers, we reduced the set of edge la-
bels during the conversion. Specifically, we retained
the 50 most frequent labels, while reducing the com-
posite labels to their base label.
For producing the initial n-best lists, we use
the mate parser6 (Bohnet, 2010) and a variant of
the EasyFirst parser (Goldberg and Elhadad, 2010),
which we here call best-first parser.
The mate parser is a state-of-the-art graph-based
dependency parser that uses second-order features.
6https://code.google.com/p/mate-tools
138
The parser works in two steps. First, it uses dy-
namic programming to find the optimal projective
tree using the Carreras (2007) decoder. It then
applies the non-projective approximation algorithm
proposed by McDonald and Pereira (2006) in or-
der to produce non-projective parse trees. The non-
projective approximation algorithm is a greedy hill
climbing algorithm that starts from the optimal pro-
jective parse and iteratively tries to reattach all to-
kens, one at a time, everywhere in the sentence as
long as the tree property holds. It halts when the in-
crease in the score of the tree according to the pars-
ing model is below a certain threshold.
n-best lists are obtained by applying the non-
projective approximation algorithm in a non-greedy
manner, exploring multiple possibilities. All trees
are collected in a list, and when no new trees are
found, or newer trees have a significantly lower
score than the currently best one, search halts. The
n best trees are then retrieved from the list. It
should be noted that, in the standard case, the non-
projective approximation algorithm may find a local
optimum, and that there may be other trees that have
a higher score which were not explored. Thus the
best parse in the greedy case may not necessarily
be the one with the highest score in the n-best list.
Since the parser is trained with the greedy version
of the non-projective approximation algorithm, the
greedily chosen output parse tree is of special in-
terest. We thus flag this tree as the baseline mate
parse, in order to use that for features in the ranker.
The baseline mate parse is also our overall baseline
in the dependency track.
The best-first parser deviates from the EasyFirst
parser in several small respects: The EasyFirst de-
coder creates dependency links between the roots of
adjacent substructures, which gives an O(n log n)
complexity, but restricts the output to projective
trees. The best-first parser is allowed to choose as
head any node of an adjacent substructure instead of
only the root, which increases complexity to O(n2),
but accounts for a big part of possible non-projective
structures. We additionally implemented a swap-
operation (Nivre, 2009; Tratz and Hovy, 2011) to
account for the more complex structures. The best-
first parser relies on a beam-search strategy7 to pur-
7Due to the nature of the decoder, the parser can produce
sue multiple derivations, which we also use to pro-
duce the n-best output.
In the scoring step, we additionally apply the tur-
boparser8 (Martins et al, 2010), which is based on
linear programming relaxations.9 We changed all
three parsers such that they would return a score for
a given tree. We use this to extract scores from each
parser for all trees in the n-best lists. It is impor-
tant to have a score from every parser for every tree,
as previously observed by Zhang et al (2009) in the
context of constituency reranking.
4.1 Ranking
Table 3 shows the performance of the individual
parsers measured on the development sets. It also
displays the oracle scores over the different n-best
lists, i.e., the maximal possible score over an n-best
list if the best tree is always selected.
The mate parser generally performs best followed
by turboparser, while the best-first parser comes last.
But we can see from the oracle scores that the best-
first parser often shows comparable or even higher
oracle scores than mate, and that the combination
of the n-best lists always adds substantial improve-
ments to the oracle scores. These findings show that
the mate and best-first parsers are providing differ-
ent sets of n-best lists. Moreover, all three parsers
rely on different parsing algorithms and feature sets.
For these reasons, we hypothesized that the parsers
contribute different views on the parse trees and that
their combination would result in better overall per-
formance.
In order to leverage the diversity between the
parsers we experimented with ranking10 on the
n-best lists. We used the same ranking model in-
troduced in Section 3 here as well. The model is
trained to select the best parse according to the la-
beled attachment score (LAS). The training data for
the ranker was created by 5-fold jackknifing on the
training sets. The feature sets for the ranker for
spurious ambiguities in the beam. If this occurs, only the one
with the higher score is kept.
8http://www.ark.cs.cmu.edu/TurboParser/
9Ideally we would also extract n-best lists from the tur-
boparser, however time prevented us from making the necessary
modifications.
10We refrain from calling it reranking in this setting, since
we are using merged n-best lists and the initial ranking is not
entirely clear to begin with.
139
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Baseline results for individual parsers
mate? 88.50/83.50 88.18/84.49 92.71/90.85 83.63/75.89 87.07/82.84 86.06/82.39 91.17/85.81 83.65/77.16
mate 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05
bf 87.61/85.32 84.07/75.90 87.45/83.92 92.90/91.10 86.10/79.57 83.85/75.94 86.54/83.97 90.10/83.75 82.27/75.36
turbo 87.82/85.35 88.88/83.84 88.24/84.57 93.59/91.54 85.74/78.95 86.86/82.80 88.35/86.23 90.97/85.55 83.24/76.15
Oracle scores for n-best lists
mate 90.85/88.74 93.39/89.85 90.99/87.81 97.14/95.84 89.05/83.03 91.41/88.19 94.86/92.96 95.19/91.67 87.19/81.66
bf 91.47/89.46 91.68/86.46 91.38/88.68 97.40/96.60 91.04/85.67 87.64/81.79 94.90/92.94 96.25/93.74 87.60/82.46
merged 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96
Table 3: Baseline performance and n-best oracle scores (UAS/LAS) on the development sets. mate? uses the prepro-
cessing provided by the organizers, the other parsers use the preprocessing described in Section 2.
each language were optimized manually via cross-
validation on the training sets. The features used for
each language, as well as a default (baseline) fea-
ture set, are shown in Table 4. We now outline the
features we used in the ranker:
Score from the base parsers ? denoted B, M,
T, for the best-first, mate, and turbo parsers, re-
spectively. We also have indicator features whether
a certain parse was the best according to a given
parser, denoted GB, GM, GT, respectively. Since
the mate parser does not necessarily assign the high-
est score to the baseline mate parse, the GM fea-
ture is a ternary feature which indicates whether a
parse is the same as the baseline mate parse, or bet-
ter, or worse. We also experimented with transfor-
mations and combinations of the scores from the
parsers. Specifically, BMProd denotes the product
of B and M; BMeProd denotes the sum of B and M
in e-space, i.e., eB+M ; reBMT, reBT, reMT denote
the normalized product of the corresponding scores,
where scores are normalized in a softmax fashion
such that all features take on values in the interval
(0, 1).
Projectivity features (Hall et al, 2007) ? the
number of non-projective edges in a tree, denoted
np. Whether a tree is ill-nested, denoted I. Since ill-
nested trees are extremely rare in the treebanks, this
helps the ranker filter out unlikely candidates from
the n-best lists. For a definition and further discus-
sion of ill-nestedness, we refer to (Havelka, 2007).
Constituent features ? from the constituent track
we also have constituent trees of all sentences which
can be used for feature extraction. Specifically, for
every head-dependent pair, we extract the path in the
constituent tree between the nodes, denoted ptbp.
Case agreement ? on head-dependent pairs that
both have a case value assigned among their mor-
phological features, we mark whether it is the same
case or not, denoted case.
Function label uniqueness ? on each training set
we extracted a list of function labels that generally
occur at most once as the dependent of a node, e.g.,
subjects or objects. Features are then extracted from
all nodes that have one or more dependents of each
label aimed at capturing mistakes such as double
subjects on a verb. This template is denoted FL.
In addition to the features mentioned above, we
experimented with a variety of feature templates, in-
cluding features drawn from previous work on de-
pendency reranking (Hall, 2007), e.g., lexical and
POS-based features over edges, ?subcategorization?
frames (i.e., the concatenation of POS-tags that are
headed by a certain node in the tree), etc, although
these features did not seem to help. For German we
created feature templates based on the constraints
used in the constraint-based parser by Seeker and
Kuhn (2013). This includes, e.g., violations in case
or number agreement between heads and depen-
dents, as well as more complex features that con-
sider labels on entire verb complexes. None of these
features yielded any clear improvements though. We
also experimented with features that target some
specific constructions (and specifics of annotation
schemes) which the parsers typically cannot fully
see, such as coordination, however, also here we saw
no clear improvements.
4.2 Effects of Ranking
In Table 5, we show the improvements from using
the ranker, both with the baseline and optimized fea-
tures sets for the ranker. For the sake of comparison,
140
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Baseline 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05
Ranked-dflt 88.54/86.32 89.99/85.43 88.85/85.39 94.06/92.36 87.28/80.44 88.16/84.54 88.71/86.65 92.26/87.12 84.51/77.83
Ranked 88.93/86.74 89.95/85.61 89.37/85.96 94.20/92.68 87.63/81.02 88.38/84.77 89.20/87.12 93.02/87.69 85.04/78.57
Oracle 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96
Table 5: Performance (UAS/LAS) of the reranker on the development sets. Baseline denotes our baseline. Ranked-dflt
and Ranked denote the default and optimized ranker feature sets, respectively. Oracle denotes the oracle scores.
default B, M, T, GB, GM, GT, I
Arabic B, M, T, GB, GM, I, ptbp, reBMT
Basque B, M, T, GB, GM, GT, I, ptbp, I, reMT, case
French B, M, T, GB, GM, GT, I, ptbp
German B, M, T, GM, I, BMProd, FL
Hebrew B, M, T, GB, GM, GT, I, ptbp, FL, BMeProd
Hungarian B, M, T, GB, GM, GT, I, ptbp, reBM, FL
Korean B, M, T, GB, GM, GT, I, ptbp, reMT, FL
Polish B, M, T, GB, GM, GT, I, ptbp, np
Swedish B, M, T, GB, GM, GT, I, ptbp, reBM, FL
Table 4: Feature sets for the dependency ranker for each
language. default denotes the default ranker feature set.
the baseline mate parses as well as the oracle parses
on the merged n-best lists are repeated from Table 3.
We see that ranking clearly helps, both with a tai-
lored feature set, as well as the default feature set.
The improvement in LAS between the baseline and
the tailored ranking feature sets ranges from 1.1%
(French) to 1.6% (Hebrew) absolute, with the excep-
tion of Hungarian, where improvements on the dev
set are more modest (contrary to the test set results,
cf. Section 5). Even with the default feature set, the
improvements range from 0.5% (French) to 1.1%
(Hebrew) absolute, again setting Hungarian aside.
We believe that this is an interesting result consid-
ering the simplicity of the default feature set.
5 Test Set Results
In this section we outline our final results on the test
sets. As previously, we focus on the setting with
predicted tags in gold segmentation and the largest
training set. We also present results on Arabic and
Hebrew for the predicted segmentation setting. For
the gold preprocessing and all 5k settings, we refer
the reader to the Shared Task overview paper (Sed-
dah et al, 2013).11
In Table 7, we present our results in the con-
11Or the results page online: http://www.spmrl.org/
spmrl2013-sharedtask-results.html
stituency track. Since we were the only participat-
ing team in the constituency track, we compare our-
selves with the best baseline12 provided by the or-
ganizers. Our system outperforms the baseline for
all languages in terms of PARSEVAL F1. Follow-
ing the trend on the development sets, reranking is
consistently helping across languages.13 Despite the
lack of other submissions in the shared task, we be-
lieve our numbers are generally strong and hope that
they can serve as a reference for future work on con-
stituency parsing on these data sets.
Table 8 displays our results in the dependency
track. We submitted two runs: a baseline, which
is the baseline mate parse, and the reranked trees.
The table also compares our results to the best per-
forming other participant in the shared task (denoted
Other) as well as the MaltParser (Nivre et al, 2007)
baseline provided by the shared task organizers (de-
noted ST Baseline). We obtain the highest scores
for all languages, with the exception of French. It is
also clear that we make considerable gains over our
baseline, confirming our results on the development
sets reported in Section 4. It is also noteworthy that
our baseline (i.e., the mate parser with our own pre-
processing) outperforms the best other system for 5
languages.
Arabic Hebrew
Other 90.75/8.48 88.33/12.20
Dep. Baseline 91.13/9.10 89.27/15.01
Dep. Ranked 91.74/9.83 89.47/16.97
Constituency 92.06/9.49 89.30/13.60
Table 6: Unlabeled TedEval scores (accuracy/exact
match) for the test sets in the predicted segmentation set-
ting. Only sentences of length ? 70 are evaluated.
12It should be noted that the Shared Task organizers com-
puted 2 different baselines on the test sets. The best baseline
results for each language thus come from different parsers.
13We remind the reader that our submission decisions are not
based on figures in Table 2, cf. Section 3.
141
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
ST Baseline 79.19 74.74 80.38 78.30 86.96 85.22 78.56 86.75 80.64
Product 80.81 87.18 81.83 80.70 89.46 90.58 83.49 87.55 83.99
Reranked 81.32 87.86 82.86 81.27 89.49 91.85 84.27 87.76 84.88
Table 7: Final PARSEVAL F1 scores for constituents on the test set for the predicted setting. ST Baseline denotes the
best baseline (out of 2) provided by the Shared Task organizers. Our submission is underlined.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
ST Baseline 83.18/80.36 79.77/70.11 82.49/77.98 81.51/77.81 76.49/69.97 80.72/70.15 85.72/82.06 82.19/75.63 80.29/73.21
Other 85.78/83.20 89.19/84.25 89.19/85.86 90.80/88.66 81.05/73.63 88.93/84.97 85.84/82.65 88.12/82.56 87.28/80.88
Baseline 86.96/84.81 89.32/84.25 87.87/84.37 90.54/88.37 85.88/79.67 89.09/85.31 87.41/85.51 90.30/85.51 86.85/80.67
Ranked 88.32/86.21 89.88/85.14 88.68/85.24 91.64/89.65 86.70/80.89 89.81/86.13 88.47/86.62 91.75/87.07 88.06/82.13
Table 8: Final UAS/LAS scores for dependencies on the test sets for the predicted setting. Other denotes the highest
scoring other participant in the Shared Task. ST Baseline denotes the MaltParser baseline provided by the Shared Task
organizers.
Table 6 shows the unlabeled TedEval (Tsarfaty et
al., 2012) scores (accuracy/exact match) on the test
sets for the predicted segmentation setting for Ara-
bic and Hebrew. Note that these figures only include
sentences of length less than or equal to 70. Since
TedEval enables cross-framework comparison, we
compare our submissions from the dependency track
to our submission from the constituency track. In
these runs we used the same systems that were used
for the gold segmentation with predicted tags track.
The predicted segmentation was provided by the
Shared Task organizers. We also compare our re-
sults to the best other system from the Shared Task
(denoted Other).
Also here we obtain the highest results for both
languages. However, it is unclear what syntactic
paradigm (dependencies or constituents) is better
suited for the task. All in all it is difficult to assess
whether the differences between the best and second
best systems for each language are meaningful.
6 Conclusion
We have presented our contribution to the 2013
SPMRL Shared Task. We participated in both the
constituency and dependency tracks. In both tracks
we make use of a state-of-the-art tagger for POS and
morphological features. In the constituency track,
we use the tagger to handle unknown words and em-
ploy a product-of-grammars-based PCFG-LA parser
and parse tree reranking. In the dependency track,
we combine multiple parsers output as input for a
ranker.
Since there were no other participants in the con-
stituency track, it is difficult to draw any conclusions
from our results. We do however show that the ap-
plication of product grammars, our handling of rare
words, and a subsequent reranking step outperforms
a baseline PCFG-LA parser.
In the dependency track we obtain the best re-
sults for all languages except French among 7 partic-
ipants. Our reranking approach clearly outperforms
a baseline graph-based parser. This is the first time
multiple parsers have been used in a dependency
reranking setup.
Aside from minor decisions made on the basis
of each language, our approach is language agnos-
tic and does not target morphology in any particu-
lar way as part of the parsing process. We show
that with a strong baseline and with no language
specific treatment it is possible to achieve state-of-
the-art results across all languages. Our architec-
ture for the dependency parsing track enables the use
of language-specific features in the ranker, although
we only had minor success with features that target
morphology. However, it may be the case that ap-
proaches from previous work on parsing MRLs, or
the approaches taken by other teams in the Shared
Task, can be successfully combined with ours and
improve parsing accuracy even more.
Acknowledgments
Richa?rd Farkas is funded by the European Union and
the European Social Fund through the project Fu-
turICT.hu (grant no.: TA?MOP-4.2.2.C-11/1/KONV-
142
2012-0013). Thomas Mu?ller is supported by a
Google Europe Fellowship in Natural Language
Processing. The remaining authors are funded by
the Deutsche Forschungsgemeinschaft (DFG) via
the SFB 732, projects D2 and D8 (PI: Jonas Kuhn).
We also express our gratitude to the treebank
providers for each language: Arabic (Maamouri et
al., 2004; Habash and Roth, 2009; Habash et al,
2009; Green and Manning, 2010), Basque (Aduriz
et al, 2003), French (Abeille? et al, 2003), He-
brew (Sima?an et al, 2001; Tsarfaty, 2010; Gold-
berg, 2011; Tsarfaty, 2013), German (Brants et al,
2002; Seeker and Kuhn, 2012), Hungarian (Csendes
et al, 2005; Vincze et al, 2010), Korean (Choi
et al, 1994; Choi, 2013), Polish (S?widzin?ski and
Wolin?ski, 2010), and Swedish (Nivre et al, 2006).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeille?, editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. D??az de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In TLT-03, pages 201?204.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, edi-
tors, Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT 2002), pages 24?41, So-
zopol, Bulgaria.
Tim Buckwalter. 2002. Buckwalter Arabic Morpholog-
ical Analyzer Version 1.0. Linguistic Data Consor-
tium, University of Pennsylvania, 2002. LDC Catalog
No.: LDC2002L49.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961, Prague, Czech Republic, June.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 173?180.
Key-Sun Choi, Young S Han, Young G Han, and Oh W
Kwon. 1994. Kaist tree bank project for korean:
Present and future development. In Proceedings of
the International Workshop on Sharable Natural Lan-
guage Resources, pages 7?14. Citeseer.
Jinho D. Choi. 2013. Preparing korean data for
the shared task on parsing morphologically rich lan-
guages. ArXiv e-prints.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of the Sev-
enteenth International Conference on Machine Learn-
ing, ICML ?00, pages 175?182.
Do?ra Csendes, Jano?s Csirik, Tibor Gyimo?thy, and Andra?s
Kocsor. 2005. The Szeged treebank. In Va?clav Ma-
tous?ek, Pavel Mautner, and Toma?s? Pavelka, editors,
Text, Speech and Dialogue: Proceedings of TSD 2005.
Springer.
Rickard Domeij, Ola Knutsson, Johan Carlberger, and
Viggo Kann. 2000. Granska-an efficient hybrid sys-
tem for Swedish grammar checking. In In Proceed-
ings of the 12th Nordic Conference in Computational
Linguistics.
Mikel L Forcada, Mireia Ginest??-Rosell, Jacob Nord-
falk, Jim O?Regan, Sergio Ortiz-Rojas, Juan An-
tonio Pe?rez-Ortiz, Felipe Sa?nchez-Mart??nez, Gema
Ram??rez-Sa?nchez, and Francis M Tyers. 2011. Aper-
tium: A free/open-source platform for rule-based ma-
chine translation. Machine Translation.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 742?750, Los Angeles, California, June.
Association for Computational Linguistics.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
Spence Green and Christopher D. Manning. 2010. Bet-
ter arabic parsing: Baselines, evaluations, and anal-
ysis. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 394?402, Beijing, China, August. Coling 2010
Organizing Committee.
Nizar Habash and Ryan Roth. 2009. Catib: The
columbia arabic treebank. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 221?
224, Suntec, Singapore, August. Association for Com-
putational Linguistics.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
143
Keith Hall, Jiri Havelka, and David A. Smith. 2007.
Log-Linear Models of Non-Projective Trees, k-best
MST Parsing and Tree-Ranking. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 962?966, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Keith Hall. 2007. K-best Spanning Tree Parsing. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 392?399, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Jiri Havelka. 2007. Beyond Projectivity: Multilin-
gual Evaluation of Constraints and Measures on Non-
Projective Structures. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 608?615, Prague, Czech Republic,
June. Association for Computational Linguistics.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 665?668, Los An-
geles, California, June. Association for Computational
Linguistics.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo Parsers: Depen-
dency Parsing by Approximate Variational Inference.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 34?
44, Cambridge, MA, October. Association for Compu-
tational Linguistics.
Andrew Kachites McCallum. 2002. ?mal-
let: A machine learning for language toolkit?.
http://mallet.cs.umass.edu.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 81?88, Trento, Italy. Asso-
ciation for Computational Linguistics.
Thomas Mu?ller, Helmut Schmid, and Hinrich Schu?tze.
2013. Efficient Higher-Order CRFs for Morphological
Tagging. In In Proceedings of EMNLP.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13:95?135, 6.
Joakim Nivre. 2009. Non-Projective Dependency Pars-
ing in Expected Linear Time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
351?359, Suntec, Singapore, August. Association for
Computational Linguistics.
S Park, D Choi, E-k Kim, and KS Choi. 2010. A plug-in
component-based Korean morphological analyzer. In
Proceedings of HCLT2010.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, pages 433?440. Associa-
tion for Computational Linguistics.
Slav Petrov. 2010. Products of Random Latent Variable
Grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Los Angeles, California, June. Associa-
tion for Computational Linguistics.
Helmut Schmid. 2005. A programming language for
finite state transducers. In FSMNLP.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Can-
dito, Jinho Choi, Richa?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola, Yoav Goldberg, Spence
Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan
Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolin?ski, and Alina Wro?blewska.
2013. Overview of the SPMRL 2013 Shared Task: A
Cross-Framework Evaluation of Parsing Morphologi-
cally Rich Languages. In Proceedings of the 4th Work-
shop on Statistical Parsing of Morphologically Rich
Languages: Shared Task, Seattle, WA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 3132?3139, Istanbul, Turkey. European
Language Resources Association (ELRA).
Wolfgang Seeker and Jonas Kuhn. 2013. Morphological
and Syntactic Case in Statistical Dependency Parsing.
Computational Linguistics, 39(1):23?55.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique des
Langues.
144
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Text,
Speech and Dialogue: 13th International Conference
(TSD), Lecture Notes in Artificial Intelligence, pages
197?204, Brno, Czech Republic. Springer.
Stephen Tratz and Eduard Hovy. 2011. A Fast, Ac-
curate, Non-Projective, Semantically-Enriched Parser.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1257?1268, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Reut Tsarfaty, Djame? Seddah, Yoav Goldberg, Sandra
Kuebler, Yannick Versley, Marie Candito, Jennifer
Foster, Ines Rehbein, and Lamia Tounsi. 2010. Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL) What, How and Whither. In Proc. of the
SPMRL Workshop of NAACL-HLT, pages 1?12, Los
Angeles, CA, USA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Joint Evaluation of Morphological Segmen-
tation and Syntactic Parsing. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
6?10, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A Unified Morpho-Syntactic
Scheme of Stanford Dependencies. Proceedings of
ACL.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian dependency treebank. In LREC.
Zhiguo Wang and Chengqing Zong. 2011. Parse Rerank-
ing Based on Higher-Order Lexical Dependencies. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 1251?1259, Chi-
ang Mai, Thailand, November. Asian Federation of
Natural Language Processing.
Marcin Wolin?ski. 2006. Morfeusz - A practical tool for
the morphological analysis of Polish. In Intelligent in-
formation processing and web mining, pages 511?520.
Springer.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-Best Combination of Syntactic Parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552?1560, Singapore, August. Association for Com-
putational Linguistics.
Zhenxia Zhou. 2007. Entwicklung einer franzo?sischen
Finite-State-Morphologie. Diplomarbeit, Institute for
Natural Language Processing, University of Stuttgart.
Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.
2013. Magyarlanc 2.0: Szintaktikai elemze?s e?s fel-
gyors??tott szo?faji egye?rtelmu?s??te?s. In IX. Magyar
Sza?m??to?ge?pes Nyelve?szeti Konferencia.
145
