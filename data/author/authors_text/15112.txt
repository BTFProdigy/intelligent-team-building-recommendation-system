Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 95?100,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Joint Identification and Segmentation of Domain-Specific Dialogue Acts for
Conversational Dialogue Systems
Fabrizio Morbini and Kenji Sagae
Institute for Creative Technologies
University of Southern California
12015 Waterfront Drive, Playa Vista, CA 90094
{morbini,sagae}@ict.usc.edu
Abstract
Individual utterances often serve multiple
communicative purposes in dialogue. We
present a data-driven approach for identifica-
tion of multiple dialogue acts in single utter-
ances in the context of dialogue systems with
limited training data. Our approach results in
significantly increased understanding of user
intent, compared to two strong baselines.
1 Introduction
Natural language understanding (NLU) at the level
of speech acts for conversational dialogue systems
can be performed with high accuracy in limited do-
mains using data-driven techniques (Bender et al,
2003; Sagae et al, 2009; Gandhe et al, 2008, for
example), provided that enough training material is
available. For most systems that implement novel
conversational scenarios, however, enough exam-
ples of user utterances, which can be annotated as
NLU training data, only become available once sev-
eral users have interacted with the system. This situ-
ation is typically addressed by bootstrapping from a
relatively small set of hand-authored utterances that
perform key dialogue acts in the scenario or from
utterances collected from wizard-of-oz or role-play
exercises, and having NLU accuracy increase over
time as more users interact with the system and more
utterances are annotated for NLU training.
While this can be effective in practice for ut-
terances that perform only one of several possible
system-specific dialogue acts (often several dozens),
longer utterances that include multiple dialogue acts
pose a greater challenge: the many available combi-
nations of dialogue acts per utterance result in sparse
coverage of the space of possibilities, unless a very
large amount of data can be collected and anno-
tated, which is often impractical. Users of the dia-
logue system, whose utterances are collected for fur-
ther NLU improvement, tend to notice that portions
of their longer utterances are ignored and that they
are better understood when they express themselves
with simpler sentences. This results in generation of
data heavily skewed towards utterances that corre-
spond to a single dialogue act, making it difficult to
collect enough examples of utterances with multiple
dialogue acts to improve NLU, which is precisely
what would be needed to make users feel more com-
fortable with using longer utterances.
We address this chicken-and-egg problem with a
data-driven NLU approach that segments and iden-
tifies multiple dialogue acts in single utterances,
even when only short (single dialogue act) utter-
ances are available for training. In contrast to previ-
ous approaches that assume the existence of enough
training data for learning to segment utterances,
e.g. (Stolcke and Shriberg, 1996), or to align spe-
cific words to parts of the formal representation,
e.g. (Bender et al, 2003), our framework requires a
relatively small dataset, which may not contain any
utterances with multiple dialogue acts. This makes it
possible to create new conversational dialogue sys-
tem scenarios that allow and encourage users to ex-
press themselves with fewer restrictions, without an
increased burden in the collection and annotation of
NLU training data.
2 Method
Given (1) a predefined set of possible dialogue acts
for a specific dialogue system, (2) a set of utterances
95
each annotated with a single dialogue act label, and
(3) a classifier trained on this annotated utterance-
label set, which assigns for a given word sequence a
dialogue act label with a corresponding confidence
score, our task is to find the best sequence of dia-
logue acts that covers a given input utterance. While
short utterances are likely to be covered entirely by a
single dialogue act that spans all of its words, longer
utterances may be composed of spans that corre-
spond to different dialogue acts.
bestDialogueActEndingAt(Text,pos) begin
if pos < 0 then
return ?pos, ?null, 1??;
end
S = {};
for j = 0 to pos do
?c, p? = classify(words(Text, j, pos));
S = S ? {?j, ?c, p??};
end
return argmax
?k,?c,p???S
{p ? p
?
: ?h, ?c
?
, p
?
?? =
bestDialogueActEndingAt(Text, k ? 1)};
end
Algorithm 1: The function classify(T ) calls the
single dialogue act classifier subsystem on the in-
put text T and returns the highest scoring dia-
logue act label c with its confidence score p. The
function words(T, i, j) returns the string formed
by concatenating the words in T from the ith to
the jth included. To obtain the best segmenta-
tion of a given text, one has to work its way back
from the end of the text: start by calling ?k, ?c, p??
= bestDialogueActEndingAt(Text, numWords),
where numWords is the number of words
in Text. If k > 0 recursively call
bestDialogueActEndingAt(Text, k ? 1) to obtain
the optimal dialogue act ending at k ? 1.
Algorithm 1 shows our approach for using a sin-
gle dialogue act classifier to extract the sequence of
dialogue acts with the highest overall score from a
given utterance. The framework is independent of
the particular subsystem used to select the dialogue
act label for a given segment of text. The constraint
is that this subsystem should return, for a given se-
quence of words, at least one dialogue act label and
its confidence level in a normalized range that can
be used for comparisons with subsequent runs. In
the work reported in this paper, we use an existing
data-driven NLU module (Sagae et al, 2009), de-
veloped for the SASO virtual human dialogue sys-
tem (Traum et al, 2008b), but retrained using the
data described in section 3. This NLU module per-
forms maximum entropy multiclass classification,
using features derived from the words in the input
utterance, and using dialogue act labels as classes.
The basic idea is to find the best segmentation
(that is, the one with the highest score) of the portion
of the input text up to the ith word. The base case Si
would be for i = 1 and it is the result of our classi-
fier when the input is the single first word. For any
other i > 1 we construct all word spans Tj,i of the
input text, containing the words from j to i, where
1 ? j ? i, then we classify each of the Tj,i and
pick the best returned class (dialogue act label) Cj,i
(and associated score, which in the case of our maxi-
mum entropy classifier is the conditional probability
Score(Cj,i) = P (Cj,i|Tj,i)). Then we assign to the
best segmentation ending at i, Si, the label Ck,i iff:
k = argmax
1?h?i
(
Score(Ch,i) ? Score(Sh?1)
)
(1)
Algorithm 1 calls the classifier O(n2) where n
is the number of words in the input text. Note
that, as in the maximum entropy NLU of Bender et
al. (2003), this search uses the ?maximum approxi-
mation,? and we do not normalize over all possible
sequences. Therefore, our scores are not true proba-
bilities, although they serve as a good approximation
in the search for the best overall segmentation.
We experimented with two other variations of
the argument of the argmax in equation 1: (1) in-
stead of considering Score(Sh?1), consider only
the last segment contained in Sh?1; and (2) instead
of using the product of the scores of all segments,
use the average score per segment: (Score(Ch,i) ?
Score(Sh?1))1/(1+N(Sh?1)) where N(Si) is the
number of segments in Si. These variants produce
similar results; the results reported in the next sec-
tion were obtained with the second variant.
3 Evaluation
3.1 Data
To evaluate our approach we used data collected
from users of the TACQ (Traum et al, 2008a) dia-
96
logue system, as described by Artstein et al (2009).
Of the utterances in that dataset, about 30% are an-
notated with multiple dialogue acts. The annotation
also contains for each dialogue act the correspond-
ing segment of the input utterance.
The dataset contains a total of 1,579 utterances.
Of these, 1,204 utterances contain only a single di-
alogue act, and 375 utterances contain multiple dia-
logue acts, according to manual dialogue act anno-
tation. Within the set of utterances that contain mul-
tiple dialogue acts, the average number of dialogue
acts per utterance is 2.3.
The dialogue act annotation scheme uses a total
of 77 distinct labels, with each label corresponding
to a domain-specific dialogue act, including some
semantic information. Each of these 77 labels is
composed at least of a core speech act type (e.g.
wh-question, offer), and possibly also attributes that
reflect semantics in the domain. For example, the
dialogue act annotation for the utterance What is
the strange man?s name? would be whq(obj:
strangeMan, attr: name), reflecting that
it is a wh-question, with a specific object and at-
tribute. In the set of utterances with only one speech
act, 70 of the possible 77 dialogue act labels are
used. In the remaining utterances (which contain
multiple speech acts per utterance), 59 unique dia-
logue act labels are used, including 7 that are not
used in utterances with only a single dialogue act
(these 7 labels are used in only 1% of those utter-
ances). A total of 18 unique labels are used only
in the set of utterances with one dialogue act (these
labels are used in 5% of those utterances). Table 1
shows the frequency information for the five most
common dialogue act labels in our dataset.
The average number of words in utterances with
only a single dialogue act is 7.5 (with a maximum
of 34, and minimum of 1), and the average length of
utterances with multiple dialogue acts is 15.7 (max-
imum of 66, minimum of 2). To give a better idea of
the dataset used here, we list below two examples of
utterances in the dataset, and their dialogue act an-
notation. We add word indices as subscripts in the
utterances for illustration purposes only, to facilitate
identification of the word spans for each dialogue
act. The annotation consists of a word interval and a
Single DA Utt. [%] Multiple DA Utt. [%]
Wh-questions 51 Wh-questions 31
Yes/No-questions 14 Offers to agent 24
Offers to agent 9 Yes answer 11
Yes answer 7 Yes/No-questions 8
Greeting 7 Thanks 7
Table 1: The frequency of the dialogue act classes most
used in the TACQ dataset (Artstein et al, 2009). The
left column reports the statistics for the set of utterances
annotated with a single dialogue act the right those for the
utterances annotated with multiple dialogue acts. Each
dialogue act class typically contains several more specific
dialogue acts that include domain-specific semantics (for
example, there are 29 subtypes of wh-questions that can
be performed in the domain, each with a separate domain-
specific dialogue act label).
dialogue act label1.
1. ? 0 his 1 name, 2 any 3 other 4 informa-
tion 5 about 6 him, 7 where 8 he 9 lives
10? is labeled with: [0 2] whq(obj:
strangeMan, attr: name), [2 7]
whq(obj: strangeMan) and [7 10]
whq(obj: strangeMan, attr:
location).
2. ? 0 I 1 can?t 2 offer 3 you 4 money 5 but 6 I 7 can
8 offer 9 you 10 protection 11? is labeled with:
[0 5] reject, [5 11] offer(safety).
3.2 Setup
In our experiments, we performed 10-fold cross-
validation using the dataset described above. For
the training folds, we use only utterances with a sin-
gle dialogue act (utterances containing multiple dia-
logue acts are split into separate utterances), and the
training procedure consists only of training a max-
imum entropy text classifier, which we use as our
single dialogue act classifier subsystem.
For each evaluation fold we run the procedure de-
scribed in Section 2, using the classifier obtained
from the corresponding training fold. The segments
present in the manual annotation are then aligned
with the segments identified by our system (the
1Although the dialogue act labels could be thought of as
compositional, since they include separate parts, we treat them
as atomic labels.
97
alignment takes in consideration both the word span
and the dialogue act label associated to each seg-
ment). The evaluation then considers as correct only
the subset of dialogue acts identified automatically
that were successfully aligned with the same dia-
logue act label in the gold-standard annotation.
We compared the performance of our proposed
approach to two baselines; both use the same max-
imum entropy classifier used internally by our pro-
posed approach.
1. The first baseline simply uses the single dia-
logue act label chosen by the maximum entropy
classifier as the only dialogue act for each ut-
terance. In other words, this baseline corre-
sponds to the NLU developed for the SASO di-
alogue system (Traum et al, 2008b) by Sagae
et al (2009)2. This baseline is expected to have
lower recall for those utterances that contain
multiple dialogue acts, but potentially higher
precision overall, since most utterances in the
dataset contain only one dialogue act label.
2. For the second baseline, we treat multiple dia-
logue act detection as a set of binary classifica-
tion tasks, one for each possible dialogue act la-
bel in the domain. We start from the same train-
ing data as above, and create N copies, where
N is the number of unique dialogue acts labels
in the training set. Each utterance-label pair in
the original training set is now present in all N
training sets. If in the original training set an ut-
terance was labeled with the ith dialogue act la-
bel, now it will be labeled as a positive example
in the ith training set and as a negative exam-
ple in all other training sets. Binary classifiers
for each N dialogue act labels are then trained.
During run-time, each utterance is classified by
all N models and the result is the subset of di-
alogue acts associated with the models that la-
beled the example as positive. This baseline is
excepted to be much closer in performance to
our approach, but it is incapable of determining
what words in the utterance correspond to each
dialogue act3.
2We do not use the incremental processing version of the
NLU described by Sagae et al, only the baseline NLU, which
consist only of a maximum entropy classifier.
3This corresponds to the transformation of a multi-label
P [%] R [%] F [%]
Single this 73 77 75
2ndbl 86 71 78
1stbl 82 77 80
Multiple this 87 66 75
2ndbl 85 55 67
1stbl 91 39 55
Overall this 78 72 75
2ndbl 86 64 73
1stbl 84 61 71
Table 2: Performance on the TACQ dataset obtained by
our proposed approach (denoted by ?this?) and the two
baseline methods. Single indicates the performance when
tested only on utterances annotated with a single dialogue
act. Multiple is for utterances annotated with more than
one dialogue act, and Overall indicates the performance
over the entire set. P stands for precision, R for recall,
and F for F-score.
3.3 Results
Table 2 shows the performance of our approach and
the two baselines. All measures show that the pro-
posed approach has considerably improved perfor-
mance for utterances that contain multiple dialogue
acts, with only a small increase in the number of er-
rors for the utterances containing only a single dia-
logue act. In fact, even though more than 70% of
the utterances in the dataset contain only a single di-
alogue act, our approach for segmenting and iden-
tifying multiple dialogue acts increases overall F-
score by about 4% when compared to the first base-
line and by about 2% when compared to the sec-
ond (strong) baseline, which suffers from the addi-
tional deficiency of not identifying what spans cor-
respond to what dialogue acts. The differences in
F-score over the entire dataset (shown in the Over-
all portion of Table 2) are statistically significant
(p < 0.05). As a drawback of our approach, it
is on average 25 times slower than our first base-
line, which is incapable of identifying multiple di-
alogue acts in a utterance4. Our approach is still
about 15% faster than our second baseline, which
classification problem into several binary classifiers, described
as PT4 by Tsoumakas and Katakis (?).
4In our dataset, our method takes on average about 102ms
to process an utterance that was originally labeled with multiple
dialogue acts, and 12ms to process one annotated with a single
dialogue act.
98
0100
200
300
400
500
0 10 20 30 40 50 60 70
Ex
ecu
tio
nt
im
e[
ms
]
Hi
sto
gra
m
(nu
mb
er
of
utt
era
nc
es)
Number of words in input text
this
1stbl
2ndbl
histogram
Figure 1: Execution time in milliseconds of the classifier
with respect to the number of words in the input text.
identifies multiple speech acts, but without segmen-
tation, and with lower F-score. Figure 1 shows the
execution time versus the length of the input text. It
also shows a histogram of utterance lengths in the
dataset, suggesting that our approach is suitable for
most utterances in our dataset, but may be too slow
for some of the longer utterances (with 30 words or
more).
Figure 2 shows the histogram of the average error
(absolute value of word offset) in the start and end
of the dialogue act segmentation. Each dialogue act
identified by Algorithm 1 is associated with a start-
ing and ending index that corresponds to the por-
tion of the input text that has been classified with
the given dialogue act. During the evaluation, we
find the best alignment between the manual annota-
tion and the segmentation we computed. For each
of the aligned pairs (i.e. extracted dialogue act and
dialogue act present in the annotation) we compute
the absolute error between the starting point of the
extracted dialogue act and the starting point of the
paired annotation. We do the same for the ending
point and we average the two error figures. The
result is binned to form the histogram displayed in
figure 2. The figure also shows the average error
and the standard deviation. The largest average er-
ror happens with the data annotated with multiple
dialogue acts. In that case, the extracted segments
have a starting and ending point that in average are
misplaced by about ?2 words.
4 Conclusion
We described a method to segment a given utter-
ance into non-overlapping portions, each associated
0 1 2 3 4 5 6 7 8 9 10
Average error in the starting and ending indexes of each speech act segment
All data: ?=1.07 ?=1.69
Single speech act: ?=0.72 ?=1.12
Multiple speech acts: ?=1.64 ?=2.22
Figure 2: Histogram of the average absolute error in the
two extremes (i.e. start and end) of segments correspond-
ing to the dialogue acts identified in the dataset.
with a dialogue act. The method addresses the prob-
lem that, in development of new scenarios for con-
versational dialogue systems, there is typically not
enough training data covering all or most configu-
rations of how multiple dialogue acts appear in sin-
gle utterances. Our approach requires only labeled
utterances (or utterance segments) corresponding to
a single dialogue act, which tends to be the easiest
type of training data to author and to collect.
We performed an evaluation using existing data
annotated with multiple dialogue acts for each utter-
ance. We showed a significant improvement in over-
all performance compared to two strong baselines.
The main drawback of the proposed approach is the
complexity of the segment optimization that requires
calling the dialogue act classifier O(n2) times with
n representing the length of the input utterance. The
benefit, however, is that having the ability to identify
multiple dialogue acts in utterances takes us one step
closer towards giving users more freedom to express
themselves naturally with dialogue systems.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily re-
flect the position or the policy of the United States
Government, and no official endorsement should be
inferred. We would also like to thank the anonymous
reviewers for their helpful comments.
99
References
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009. Viability of a simple dialogue
act scheme for a tactical questioning dialogue system.
In DiaHolmia 2009: Proceedings of the 13th Work-
shop on the Semantics and Pragmatics of Dialogue,
page 43?50, Stockholm, Sweden, June.
Oliver Bender, Klaus Macherey, Franz Josef Och, and
Hermann Ney. 2003. Comparison of alignment tem-
plates and maximum entropy models for natural lan-
guage understanding. In Proceedings of the tenth
conference on European chapter of the Association
for Computational Linguistics - Volume 1, EACL ?03,
pages 11?18, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Sudeep Gandhe, David DeVault, Antonio Roque, Bilyana
Martinovski, Ron Artstein, Anton Leuski, Jillian
Gerten, and David R. Traum. 2008. From domain
specification to virtual humans: An integrated ap-
proach to authoring tactical questioning characters.
In Proceedings of Interspeech, Brisbane, Australia,
September.
Kenji Sagae, Gwen Christian, David DeVault, and
David R. Traum. 2009. Towards natural language
understanding of partial speech recognition results in
dialogue systems. In Short Paper Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (NAACL HLT) 2009 conference.
Andreas Stolcke and Elizabeth Shriberg. 1996. Au-
tomatic linguistic segmentation of conversational
speech. In Proc. ICSLP, pages 1005?1008.
David R. Traum, Anton Leuski, Antonio Roque, Sudeep
Gandhe, David DeVault, Jillian Gerten, Susan Robin-
son, and Bilyana Martinovski. 2008a. Natural lan-
guage dialogue architectures for tactical questioning
characters. In Army Science Conference, Florida,
12/2008.
David R. Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008b. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal vir-
tual agents. In IVA, pages 117?130.
100
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 137?139,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Mixed-Initiative Conversational Dialogue System for Healthcare
Fabrizio Morbini and Eric Forbell and David DeVault and Kenji Sagae and
David R. Traum and Albert A. Rizzo
Institute for Creative Technologies
University of Southern California
Los Angeles, CA 90094, USA
{morbini,forbell,devault,sagae,traum,rizzo}@ict.usc.edu
Abstract
We present a mixed initiative conversational
dialogue system designed to address primar-
ily mental health care concerns related to
military deployment. It is supported by a
new information-state based dialogue man-
ager, FLoReS (Forward-Looking, Reward
Seeking dialogue manager), that allows both
advanced, flexible, mixed initiative interac-
tion, and efficient policy creation by domain
experts. To easily reach its target population
this dialogue system is accessible as a web ap-
plication.
1 Introduction
The SimCoach project is motivated by the challenge
of empowering troops and their significant others in
regard to their healthcare, especially with respect to
issues related to the psychological toll of military
deployment. SimCoach virtual humans are not de-
signed to act as therapists, but rather to encourage
users to explore available options and seek treatment
when needed by fostering comfort and confidence in
a safe and anonymous environment where users can
express their concerns to an artificial conversational
partner without fear of judgment or possible reper-
cussions.
SimCoach presents a rich test case for all compo-
nents of a dialogue system. The interaction with the
virtual human is delivered via the web for easy ac-
cess. As a trade-off between performance and qual-
ity, the virtual human has access to a limited set of
pre-rendered animations.
The Natural Language Understanding (NLU)
module needs to cope with both chat and military
Figure 1: Bill Ford, a SimCoach character. SimCoach
virtual humans are accessible through a web browser.
The user enters natural language input in the text field
on the bottom of the screen. The simcoach responds with
text, speech and character animation. The text area to the
right shows a transcript of the dialogue.
slang and a broad conversational domain. The dia-
logue policy authoring module needs to support non-
dialogue experts given that important parts of the di-
alogue policy are contributed by experts in psycho-
metrics and mental health issues in the military, and
others with familiarity with the military domain.
The dialogue manager (DM) must be able to take
initiative when building rapport or collecting the in-
formation it needs, but also respond appropriately
when the user takes initiative.
2 Supporting Mixed Initiative Dialogues
There is often a tension between system initiative
and performance of the system?s decision-making
for understanding and actions. A strong system-
initiative policy reduces the action state space since
137
user actions are only allowed at certain points in
the dialogue. System initiative also usually makes
it easier for a domain expert to design a dialogue
policy that will behave as desired.1 Such systems
can work well if the limited options available to the
user are what the user wants to do, but can be prob-
lematic otherwise, especially if the user has a choice
of whether or not to use the system. In particular,
this approach may not be well suited to an appli-
cation like SimCoach. At the other extreme, some
systems allow the user to say anything at any time,
but have fairly flat dialogue policies, e.g., (Leuski et
al., 2006). These systems can work well when the
user is naturally in charge, such as in interviewing
a character, but may not be suitable for situations
in which a character is asking the user questions, or
mixed initiative is desired.
True mixed initiative is notoriously difficult for a
manually constructed call-flow graph, in which the
system might want to take different actions in re-
sponse to similar stimuli, depending on local utili-
ties. Reinforcement learning approaches (Williams
and Young, 2007; English and Heeman, 2005) can
be very useful at learning local policy optimizations,
but they require large amounts of training data and a
well-defined global reward structure, are difficult to
apply to a large state-space and remove some of the
control, which can be undesirable (Paek and Pierac-
cini, 2008).
Our approach to this problem is a forward-looking
reward seeking agent, similar to that described in
(Liu and Schubert, 2010), though with support for
complex dialogue interaction and its authoring. Au-
thoring involves design of local subdialogue net-
works with pre-conditions and effects, and also qual-
itative reward categories (goals), which can be in-
stantiated with specific reward values. The dialogue
manager, called FLoReS, can locally optimize pol-
icy decisions, by calculating the highest overall ex-
pected reward for the best sequence of subdialogues
from a given point. Within a subdialogue, authors
can craft the specific structure of interaction.
Briefly, the main modules that form FLoReS are:
? The information state, a propositional knowl-
1Simple structures, such as a call flow graph (Pieraccini and
Huerta, 2005) and branching narrative for interactive games
(Tavinor, 2009) will suffice for authoring.
edge base that keeps track of the current state
of the conversation. The information state sup-
ports missing or unknown information by al-
lowing atomic formulas to have 3 possible val-
ues: true, false and null.
? A set of inference rules that allows the sys-
tem to add new knowledge to its information
state, based on logical reasoning. Forward in-
ference facilitates policy authoring by provid-
ing a mechanism to specify information state
updates that are independent of the specific di-
alogue context.2
? An event handling system, that allows the in-
formation state to be updated based on user in-
put, system action, or other classes of author-
defined events (such as system timeouts).
? A set of operators. Operators represent lo-
cal dialogue structure (trees), and can also be
thought of as reusable subdialogues. Each state
within the subdialogue can include a reward
for reaching that state. Rewards are functions
of the goals of the system, and are the main
method used to decide what to do when there is
more than one applicable operator. Operators
have preconditions and effects. Effects specify
changes to the information state. The precondi-
tions define when an operator can be activated.
3 Sample Dialogue
In this demo, the user will interact with the Sim-
Coach character Bill Ford, using a standard web
browser and typing text. The virtual human, driven
by FLoReS, will respond using pre-rendered anima-
tions encoded as H.264 video, delivered via a stan-
dard web server. Table 1 shows an excerpt from a
sample conversation with Bill Ford that illustrates
some of the features of this dialogue manager.
The excerpt starts from a rapport building
smalltalk sub-dialogue on the topic of barbecuing
which is interrupted by a user question about con-
fidentiality. The system responds to the user inter-
ruption and then re-starts the interrupted smalltalk
because it is still the most valuable conversation con-
tinuation available at that moment.
2For example: every time the user says that s/he has night-
mares we want to update the information state to include that
s/he also has sleeping problems.
138
Dialogue transcript Notes
Ask anybody about me, and
they?ll tell you that I love to
BBQ
BBQ Smalltalk The character is equipped
with a few operators for
smalltalk about a few topics.
BBQ is one of them.
Is this conversation se-
cret?
We don?t share your info
with anyone who can per-
sonally identify you. The
techs can see what we say,
but just to tell that the site is
working. But they have no
idea who said it, just what
was said
Did that help you?
Yes it did.
Great.
Confidentiality QA
Here the system is inter-
rupted by a user question and
it decides that answering it is
the best course of action.
Like I was saying, I love to
BBQ
BBQ Smalltalk After answering the question,
the best course of action is to
awaken the paused operator
about the BBQ smalltalk.
What is PTSD?
PTSD, or post-traumatic
stress disorder is an anxiety
condition associated with
serious traumatic events.
It can come with survivor
guilt, reliving the trauma in
dreams, numbness, and lack
of involvement with reality.
What is PTSD QA
Again the BBQ smalltalk is
interrupted by another ques-
tion from the user.
So, is PTSD something
you?re worried about. I only
ask, because you?ve been
asking about it. ...
PTSD Topic Interest QA
After answering the second
question the system decides
to ignore the paused operator
and load a follow-up operator
related to the important topic
raised by the user?s question.
The selection is based on the
expected reward that talking
about PTSD can bring to the
system.
Table 1: An excerpt of a conversation with Bill Ford that
shows opportunistic mixed initiative behavior.
Next, the user asks a question about the impor-
tant topic of post-traumatic stress disorder (PTSD).
That allows operators related to the PTSD topic to
become available and at the next chance the most
rewarding operator is no longer the smalltalk sub-
dialogue but one that stays on the PTSD topic.
4 Conclusion
We described the SimCoach dialogue system which
is designed to facilitate access to difficult health con-
cerns faced by military personnel and their fami-
lies. To easily reach its target population, the sys-
tem is available on the web. The dialogue is driven
by FLoReS, a new information-state and plan-based
DM with opportunistic action selection based on ex-
pected rewards that supports non-expert authoring.
Acknowledgments
The effort described here has been sponsored by the
U.S. Army. Any opinions, content or information
presented does not necessarily reflect the position or
the policy of the United States Government, and no
official endorsement should be inferred.
References
M.S. English and P.A. Heeman. 2005. Learning mixed
initiative dialogue strategies by using reinforcement
learning on both conversants. In HLT-EMNLP.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proceedings of the 7th SIG-
dial Workshop on Discourse and Dialogue, pages 18?
27.
Daphne Liu and Lenhart K. Schubert. 2010. Combin-
ing self-motivation with logical planning and inference
in a reward-seeking agent. In Joaquim Filipe, Ana
L. N. Fred, and Bernadette Sharp, editors, ICAART (2),
pages 257?263. INSTICC Press.
Tim Paek and Roberto Pieraccini. 2008. Automating
spoken dialogue management design using machine
learning: An industry perspective. Speech Commu-
nication, 50(89):716 ? 729. Evaluating new methods
and models for advanced speech-based interactive sys-
tems.
Roberto Pieraccini and Juan Huerta. 2005. Where do we
go from here? Research and commercial spoken dia-
log systems. In Proceedings of the 6th SIGdial Work-
shop on Discourse and Dialogue, Lisbon, Portugal,
September.
Grant Tavinor. 2009. The art of videogames. New Di-
rections in Aesthetics. Wiley-Blackwell, Oxford.
J.D. Williams and S. Young. 2007. Scaling POMDPs for
spoken dialog management. IEEE Trans. on Audio,
Speech, and Language Processing, 15(7):2116?2129.
139
Proceedings of the SIGDIAL 2013 Conference, pages 193?202,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Verbal indicators of psychological distress in interactive dialogue with a
virtual human
David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum,
Stefan Scherer, Albert (Skip) Rizzo, Louis-Philippe Morency
University of Southern California, Institute for Creative Technologies
Playa Vista, CA
devault@ict.usc.edu
Abstract
We explore the presence of indicators of
psychological distress in the linguistic be-
havior of subjects in a corpus of semi-
structured virtual human interviews. At
the level of aggregate dialogue-level fea-
tures, we identify several significant dif-
ferences between subjects with depres-
sion and PTSD when compared to non-
distressed subjects. At a more fine-grained
level, we show that significant differences
can also be found among features that
represent subject behavior during specific
moments in the dialogues. Finally, we
present statistical classification results that
suggest the potential for automatic assess-
ment of psychological distress in individ-
ual interactions with a virtual human dia-
logue system.
1 Introduction
One of the first steps toward dealing with psy-
chological disorders such as depression and PTSD
is diagnosing the problem. However, there is of-
ten a shortage of trained health care professionals,
or of access to those professionals, especially for
certain segments of the population such as mili-
tary personnel and veterans (Johnson et al, 2007).
One possible partial remedy is to use virtual hu-
man characters to do a preliminary triage screen-
ing, so that mental healthcare providers can focus
their attention on those who are most likely to need
help. The virtual human would engage an indi-
vidual in an interview and analyze some of their
behavioral characteristics. In addition to serving
a triage function, this automated interview could
produce valuable information to help the health-
care provider make their expert diagnosis.
In this paper, we investigate whether features
in the linguistic behavior of participants in a con-
versation with a virtual human could be used
for recognizing psychological distress. We focus
specifically on indicators of depression and post-
traumatic stress disorder (PTSD) in the verbal be-
havior of participants in a Wizard-of-Oz corpus.
The results and analysis presented here are part
of a broader effort to create an automated, interac-
tive virtual human dialogue system that can detect
indicators of psychological distress in the multi-
modal communicative behavior of its users. Re-
alizing this vision requires a careful and strate-
gic design of the virtual human?s dialogue behav-
ior, and in concert with the system?s behavior, the
identification of robust ?indicator? features in the
verbal and nonverbal responses of human intervie-
wees. These indicators should be specific behavior
patterns that are empirically correlated with spe-
cific psychological disorders, and that can inform
a triage screening process or facilitate the diagno-
sis or treatment performed by a clinician.
In this paper, we report on several kinds of such
indicators we have observed in a corpus of 43
Wizard-of-Oz interactions collected with our pro-
totype virtual human, Ellie, pictured in Figure 1.
We begin in Section 2 with a brief discussion of
background and related work on the communica-
tive behavior associated with psychological dis-
tress. In Section 3, we describe our Wizard-of-Oz
data set. Section 4 presents an analysis of indicator
features we have explored in this data set, identi-
fying several significant differences between sub-
jects with depression and PTSD when compared
to non-distressed subjects. In Section 5 we present
statistical classification results that suggest the po-
tential for automatic assessment of psychological
distress based on individual interactions with a vir-
tual human dialogue system. We conclude in Sec-
tion 6.
2 Background and Related Work
There has been a range of psychological and clin-
ical research that has identified differences in the
193
Figure 1: Ellie.
communicative behavior of patients with specific
psychological disorders such as depression. In this
section, we briefly summarize some closely re-
lated work.
Most work has observed the behavior of patients
in human-human interactions, such as clinical in-
terviews and doctor-patient interactions. PTSD is
generally less well studied than depression.
Examples of the kinds of differences that have
been observed in non-verbal behavior include dif-
ferences in rates of mutual gaze and other gaze
patterns, downward angling of the head, mouth
movements, frowns, amount of gesturing, fidget-
ing, emotional expressivity, and voice quality; see
Scherer et al (2013) for a recent review.
In terms of verbal behavior, our exploration of
features here is guided by several previous obser-
vations in the literature. Cohn and colleagues have
identified increased speaker-switch durations and
decreased variability of vocal fundamental fre-
quency as indicators of depression, and have ex-
plored the use of these features for classification
(Cohn et al, 2009). That work studied these fea-
tures in human-human clinical interviews, rather
than in virtual human interactions as reported here.
In clinical studies, acute depression has been as-
sociated with decreased speech, slow speech, de-
lays in delivery, and long silent pauses (Hall et al,
1995). Aggregate differences in lexical frequen-
cies have also been observed. For example, in
written essays, Rude et al (2004) observed that
depressed participants used more negatively va-
lenced words and used the first-person pronoun ?I?
more frequently than never-depressed individuals.
Heeman et al (2010) observed differences in chil-
dren with autism in how long they pause before
speaking and in their use of fillers, acknowledg-
ments, and discourse markers. Some of these fea-
tures are similar to those studied here, but looked
at children communicating with clinicians rather
than a virtual human dialogue system.
Recent work on machine classification has
demonstrated the ability to discriminate between
schizophrenic patients and healthy controls based
on transcriptions of spoken narratives (Hong et al,
2012), and to predict patient adherence to med-
ical treatment from word-level features of dia-
logue transcripts (Howes et al, 2012). Automatic
speech recognition and word alignment has also
been shown to give good results in scoring narra-
tive recall tests for identification of cognitive im-
pairment (Prud?hommeaux and Roark, 2011; Lehr
et al, 2012).
3 Data Set
In this section, we introduce the Wizard-of-Oz
data set that forms the basis for this paper. In
this virtual human dialogue system, the charac-
ter Ellie depicted in Figure 1 carries out a semi-
structured interview with a single user. The sys-
tem was designed after a careful analysis of a
set of face-to-face interviews in the same do-
main. The face-to-face interviews make up the
large human-human Distress Assessment Inter-
view Corpus (DAIC) that is described in Scherer
et al (2013). Drawing on observations of inter-
viewer behavior in the face-to-face dialogues, El-
lie was designed to serve as an interviewer who
is also a good listener, providing empathetic re-
sponses, backchannels, and continuation prompts
to elicit more extended replies to specific ques-
tions. The data set used in this paper is the re-
sult of a set of 43 Wizard-of-Oz interactions where
the virtual human interacts verbally and nonver-
bally in a semi-structured manner with a partici-
pant. Excerpts from the transcripts of two interac-
tions in this Wizard-of-Oz data set are provided in
the appendix in Figure 5.1
3.1 Procedure
The participants were recruited via Craigslist and
were recorded at the USC Institute for Creative
1A sample demonstration video of an interaction be-
tween the virtual agent and a human actor can be seen here:
http://www.youtube.com/watch?v=ejczMs6b1Q4
194
Technologies. In total 64 participants interacted
with the virtual human. All participants who met
requirements (i.e. age greater than 18, and ad-
equate eyesight) were accepted. In this paper,
we focus on a subset of 43 of these participants
who were told that they would be interacting with
an automated system. (The other participants,
which we exclude from our analysis, were aware
that they were interacting with a human-controlled
system.) The mean age of the 43 participants in
our data set was 36.6 years, with 23 males and 20
females.
We adhered to the following procedure for data
collection: After a short explanation of the study
and giving consent, participants completed a series
of questionnaires. These questionnaires included
the PTSD Checklist-Civilian version (PCL-C) and
the Patient Health Questionnaire, depression mod-
ule (PHQ-9) (Scherer et al, 2013) along with other
questions. Then participants engage in an inter-
view with the virtual human, Ellie. After the di-
alogue concludes, participants are then debriefed
(i.e. the wizard control is revealed), paid $25 to
$35, and escorted out.
The interaction between the participants and El-
lie was designed as follows: Ellie explains the pur-
pose of the interaction and that she will ask a series
of questions. She then tries to build rapport with
the participant in the beginning of the interaction
with a series of casual questions about Los Ange-
les. Then the main interview begins, including a
range of questions such as:
What would you say are some of your
best qualities?
What are some things that usually put
you in a good mood?
Do you have disturbing thoughts?
What are some things that make you re-
ally mad?
How old were you when you enlisted?
What did you study at school?
Ellie?s behavior was controlled by two human
?wizards? in a separate room, who used a graph-
ical user interface to select Ellie?s nonverbal be-
havior (e.g. head nods, smiles, back-channels)
and verbal utterances (including the interview
questions, verbal back-channels, and empathy re-
sponses). This Wizard-of-Oz setup allows us to
prove the utility of the protocol and collect training
data for the eventual fully automatic interaction.
The speech for each question was pre-recorded us-
ing an amateur voice actress (who was also one of
the wizards). The virtual human?s performance of
these utterances is animated using the SmartBody
animation system (Thiebaux et al, 2008).
3.2 Condition Assessment
The PHQ-9 and PCL-C scales provide researchers
with guidelines on how to assess the participants?
conditions based on the responses. Among the
43 participants, 13 scored above 10 on the PHQ-
9, which corresponds to moderate depression and
above (Kroenke et al, 2001). We consider these
13 participants as positive for depression in this
study. 20 participants scored positive for PTSD,
following the PCL-C classification. The two pos-
itive conditions overlap strongly, as the evalu-
ated measurements PHQ-9 and PCL-C correlate
strongly (Pearson?s r > 0.8, as reported in Scherer
et al (2013)).
4 Feature Analysis
4.1 Transcription and timing of speech
We have a set D = {d1, ..., d43} of 43 dialogues.
The user utterances in each dialogue were tran-
scribed using ELAN (Wittenburg et al, 2006),
with start and end timestamps for each utterance.2
At each pause of 300ms or longer in the user?s
speech, a new transcription segment was started.
The resulting speech segments were subsequently
reviewed and corrected for accuracy.
For each dialogue di ? D, this process resulted
in a sequence of user speech segments. We repre-
sent each segment as a tuple ?s, e, t?, where s and e
are the starting and ending timestamps in seconds,
and t is the manual text transcription of the corre-
sponding audio segment. The system speech seg-
ments, including their starting and ending times-
tamps and verbatim transcripts of system utter-
ances, were recovered from the system log files.
To explore aggregate statistical features based
on user turn-taking behavior in the dialogues, we
employ a simple approach to identifying turns
within the dialogues. First, all user and system
speech segments are sorted in increasing order of
2ELAN is a tool that supports annotation of
video and audio, from the Max Planck Insti-
tute for Psycholinguistics, The Language Archive,
Nijmegen, The Netherlands. It is available at
http://tla.mpi.nl/tools/tla-tools/elan/.
195
Segment level features
(a) mean speaking rate of each user segment
(b) mean onset time of first segment in each user turn
(c) mean onset time of non-first segments in user turns
(d) mean length of user segments
(e) mean minimum valence in user segments
(f) mean mean valence in user segments
(g) mean maximum valence in user segments
(h) mean number of filled pauses in user segments
(i) mean filled pause rate in user segments
Dialogue level features
(j) total number of user segments
(k) total length of all user segments
Figure 2: List of context-independent features.
their starting timestamps. All consecutive seg-
ments with the same speaker are then designated
as constituting a single turn. While this simple
scheme does not provide a detailed treatment of
relevant phenomena such as overlapping speech,
backchannels, and the interactive process of ne-
gotiating the turn in dialogue (Yang and Heeman,
2010), it provides a conceptually simple model for
the definition of features for aggregate statistical
analysis.
4.2 Context-independent feature analysis
We begin by analyzing a set of shallow features
which we describe as context-independent, as they
apply to user speech segments independently of
what the system has recently said. Most of these
are features that apply to many or all user speech
segments. We describe our context-independent
features in Section 4.2.1, and present our results
for these features in Section 4.2.2.
4.2.1 Context-independent features
We summarize our context-independent features
in Figure 2.
Speaking rate and onset times Based on previ-
ous clinical observations related to slowed speech
and increased onset time for depressed individuals
(Section 2), we defined features for speaking rate
and onset time of user speech segments.
We quantify the speaking rate of a user speech
segment ?s, e, t?, where t = ?w1, ..., wN ?, as
N/(e ? s). Feature (a) is the mean value of
this feature across all user speech segments within
each dialogue.
Onset time is calculated using the notion of user
turns. For each user turn, we extracted the first
user speech segment in the turn fu = ?su, eu, tu?,
and the most recent system speech segment ls =
?ss, es, ts?. We define the onset time of such a first
user segment as su ? es, and for each dialogue,
feature (b) is the intra-dialogue mean of these on-
set times.
In order to also quantify pause length between
user speech segments within a turn, we define fea-
ture (c), a similar feature that measures the mean
onset time between non-first user speech segments
within a user turn in relation to the preceding user
speech segment.
Length of user segments As one way to quan-
tify the amount of speech, feature (d) reports the
mean length of all user speech segments within a
dialogue (measured in words).
Valence features for user speech Features (e)-
(g) are meant to explore the idea that distressed
users might use more negative or less positive vo-
cabulary than non-distressed subjects. As an ex-
ploratory approach to this topic, we used Senti-
WordNet 3.0 (Baccianella and Sebastiani, 2010),
a lexical sentiment dictionary, to assign valence
to individual words spoken by users in our study.
The dictionary contains approximately 117,000
entries. In general, each word w may appear in
multiple entries, corresponding to different parts
of speech and word senses. To assign a single va-
lence score v(w) to each word in the dictionary, in
our features we compute the average score across
all parts of speech and word senses:
v(w) =
?
e?E(w) PosScoree(w)?NegScoree(w)
|E(w)|
where E(w) is the set of entries for the word w,
PosScoree(w) is the positive score for w in entry
e, and NegScoree(w) is the negative score for w
in entry e. This is similar to the ?averaging across
senses? method described in Taboada et al (2011).
We use several different measures of the va-
lence of each speech segment with transcript t =
?w1, ..., wn?. We compute the min, mean, and max
valence of each transcript:
minimum valence of t = minwi?t v(wi)
mean valence of t = 1n
?
wi?t v(wi)
maximum valence of t = maxwi?t v(wi)
Features (e)-(f) then are intra-dialogue mean
196
values for these three segment-level valence mea-
sures.
Filled pauses Another feature that we explored
is the presence of filled pauses in user speech seg-
ments. To do so, we counted the number of times
any of the tokens uh, um, uhh, umm, mm, or mmm
appeared in each speech segment. For each dia-
logue, feature (h) is the mean number of these to-
kens per user speech segment. In order to account
for the varying length of speech segments, we also
normalize the raw token counts in each segment
by dividing them by the length of the segment, to
produce a filled pause rate for the segment. Fea-
ture (i) is the mean value of the filled pause rate
for all speech segments in the dialogue.
Dialogue level features We also included two
dialogue level measures of how ?talkative? the
user is. Feature (j) is the total number of user
speech segments throughout the dialogue. Feature
(k) is the total length (in words) of all speech seg-
ments throughout the dialogue.
Standard deviation features For the classifica-
tion experiments reported in Section 5, we also in-
cluded a standard deviation variant of each of the
features (a)-(i) in Figure 2. These variants are de-
fined as the intra-dialogue standard deviation of
the underlying value, rather than the mean. We
discuss examples of standard deviation features
further in Section 5.
4.2.2 Results for context-independent
features
We summarize the observed significant effects in
our Wizard-of-Oz corpus in Table 1.
Onset time We report our findings for individu-
als with and without depression and PTSD for fea-
ture (b) in Table 1 and in Figure 3. The units are
seconds. While an increase in onset time for in-
dividuals with depression has previously been ob-
served in human-human interaction (Cohn et al,
2009; Hall et al, 1995), here we show that this
effect transfers to interactions between individuals
with depression and virtual humans. We find that
mean onset time is significantly increased for indi-
viduals with depression in their interactions with
our virtual human Ellie (p = 0.018, Wilcoxon
rank sum test).
Additionally, while to our knowledge onset time
for individuals with PTSD has not been reported,
we also found a significant increase in onset time
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
No depr.
??
Depr.
?
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
?PTSD
?
PTSD
?
Figure 3: Onset time.
for individuals with PTSD (p = 0.019, Wilcoxon
rank sum test).
Filled pauses We report our findings for individ-
uals with and without depression and PTSD under
feature (h) in Table 1 and in Figure 4. We observed
a significant reduction in this feature for both in-
dividuals with depression (p = 0.012, Wilcoxon
rank sum test) and PTSD (p = 0.014, Wilcoxon
rank sum test). We believe this may be related
to the trend we observed toward shorter speech
segments from distressed individuals (though this
trend did not reach significance). There is a pos-
itive correlation, ? = 0.55 (p = 0.0001), be-
tween mean segment length (d) and mean number
of filled pauses in segments (h).
Other features We did not observe significant
differences in the values of the other context-
independent features in Figure 2.
4.3 Context-dependent features
Our data set alows us to zoom in and look at
specific contextual moments in the dialogues, and
observe how users respond to specific Ellie ques-
tions. As an example, one of Ellie?s utterances,
which has system ID happy lasttime, is:
happy lasttime = Tell me about the last
time you felt really happy.
In our data set of 43 dialogues, this question was
asked in 42 dialogues, including 12 users positive
for depression and 19 users positive for PTSD.
197
Feature Depression (13 yes, 30 no) PTSD (20 yes, 23 no)
(b) mean onset time of first
segment in each user turn
?
Depr.: 1.72 (0.89)
No Depr.: 1.08 (0.56)
p = 0.018
?
PTSD.: 1.56 (0.80)
No PTSD.: 1.03 (0.57)
p = 0.019
(h) mean number of filled pauses
in user segments
?
Depr.: 0.32 (0.19)
No Depr.: 0.48 (0.23)
p = 0.012
?
PTSD: 0.36 (0.24)
No PTSD: 0.49 (0.21)
p = 0.014
Table 1: Results for context-independent features. For each feature and condition, we provide the mean
(standard deviation) for individuals with and without the condition. P-values for individual Wilcoxon
rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased feature values
for positive individuals. A down arrow (?) indicates a significant trend toward decreased feature values
for positive individuals.
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
No depr.
?
Depr.
?
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
?PTSD PTSD
?
Figure 4: Number of filled pauses per speech seg-
ment.
This question is one of 95 topic setting utter-
ances in Ellie?s repertoire. (Ellie has additional
utterances that serve as continuation prompts,
backchannels, and empathy responses, which can
be used as a topic is discussed.)
To define context-dependent features, we asso-
ciate with each user segment the most recent of
Ellie?s topic setting utterances that has occurred in
the dialogue. We then focus our analysis on those
user segments and turns that follow specific topic
setting utterances. In Table 2, we present some ex-
amples of our findings for context-dependent fea-
tures for happy lasttime.3
3While we provide significance test results here at the p <
0.05 level, it should be noted that because of the large number
of context-dependent features that may be defined in a small
corpus such as ours, we report these merely as observations in
our corpus. We do not claim that these results transfer beyond
The contextual feature labeled (g?) in Table 2 is
the mean of the maximum valence feature across
all segments for which happy lasttime is the most
recent topic setting system utterance. We provide
a full example of this feature calculation in Fig-
ure 5 in the appendix.
As the figure shows, we find that users with
both PTSD and depression show a sharp reduc-
tion in the mean maximum valence in their speech
segments that respond to this question. This sug-
gests that in these virtual human interactions, this
question plays a useful role in eliciting differen-
tial responses from subjects with these psycholog-
ical disorders. We observed three additional ques-
tions which showed a significant difference in the
mean maximum valence feature. One example is
the question, How would your best friend describe
you?.
With feature (b?) in Table 2, we find an in-
creased onset time in responses to this question for
subjects with depression.4 Feature (d?) shows that
subjects with PTSD exhibit shorter speech seg-
ments in their responses to this question.
We observed a range of findings of this sort for
various combinations of Ellie?s topic setting utter-
ances and specific context-dependent features. In
future work, we would like to study the optimal
combinations of context-dependent questions that
yield the most information about the user?s distress
status.
this data set.
4In comparing Table 2 with Table 1, this question seems
to induce a higher mean onset time for distressed users than
the average system utterance does. This does not seem to be
the case for non-distressed users.
198
Feature Depression (12 yes, 30 no) PTSD (19 yes, 23 no)
(g?) mean maximum valence
in user segments following
happy lasttime
?
Depr.: 0.15 (0.07)
No Depr.: 0.26 (0.12)
p = 0.003
?
PTSD: 0.16 (0.08)
No PTSD: 0.28 (0.11)
p = 0.0003
(b?) mean onset time of first
segments in user turns
following happy lasttime
?
Depr.: 2.64 (2.70)
No Depr.: 0.94 (1.80)
p = 0.030
n.s.
PTSD: 2.18 (2.48)
No PTSD: 0.80 (1.76)
p = 0.077
(d?) mean length of user
segments following
happy lasttime
n.s.
Depr.: 5.95 (1.80)
No Depr.: 10.03 (6.99)
p = 0.077
?
PTSD: 6.82 (5.12)
No PTSD: 10.55 (6.68)
p = 0.012
Table 2: Example results for context-dependent features. For each feature and condition, we provide
the mean (standard deviation) for individuals with and without the condition. P-values for individual
Wilcoxon rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased
feature values for positive individuals. A down arrow (?) indicates a significant trend toward decreased
feature values for positive individuals.
5 Classification Results
In this section, we present some suggestive clas-
sification results for our data set. We construct
three binary classifiers that use the kinds of fea-
tures described in Section 4 to predict the pres-
ence of three conditions: PTSD, depression, and
distress. For the third condition, we define dis-
tress to be present if and only if PTSD, depres-
sion, or both are present. Such a notion of distress
that collapses distinctions between disorders may
be the most appropriate type of classification for a
potential application in which distressed users of
any type are prioritized for access to health care
professionals (who will make a more informed as-
sessment of specific conditions).
For each individual dialogue, each of the three
classifiers emits a single binary label. We train
and evaluate the classifiers in a 10-fold cross-
validation using Weka (Hall et al, 2009).
While our data set of 43 dialogues is perhaps
of a typical size for a study of a research proto-
type dialogue system, it remains very small from
a machine learning perspective. We report here
two kinds of results that help provide perspective
on the prospects for classification of these condi-
tions. The first kind looks at classification based
on all the context-independent features described
in Section 4.2.1. The second looks at classifica-
tion based on individual features from this set.
In the first set of experiments, we trained a
Na??ve Bayes classifier for each condition using
all the context-independent features. We present
our results in Table 3, comparing each classifier to
a baseline that always predicts the majority class
(i.e. no condition for PTSD, no condition for de-
pression, and with condition for distress).
We note first that the trained classifiers all out-
perform the baseline in terms of weighted F-score,
weighted precision, weighted recall, and accuracy.
The accuracy improvement over baseline is sub-
stantial for PTSD (20.9% absolute improvement)
and distress (23.2% absolute improvement). The
accuracy improvement over baseline is more mod-
est for depression (2.3% absolute). We believe
one factor in the relative difficulty of classifying
depression more accurately is the relatively small
number of depressed participants in our study
(13).
While it has been shown in prior work (Cohn et
al., 2009) that depression can be classified above
baseline performance using features observed in
clinical human-human interactions, here we have
shown that classification above baseline perfor-
mance is possible in interactions between human
participants and a virtual human dialogue system.
Further, we have shown classification results for
PTSD and distress as well as depression.
We tried incorporating context-dependent fea-
tures, and also unigram features, but found that
neither improved performance. We believe our
data set is too small for effective training with
these very large extended feature sets.
199
Disorder Model Weighted F-score Weighted Precision Weighted Recall Accuracy
PTSD Na??ve Bayes 0.738 0.754 0.744 74.4%
Majority Baseline 0.373 0.286 0.535 53.5%
Depression Na??ve Bayes 0.721 0.721 0.721 72.1%
Majority Baseline 0.574 0.487 0.698 69.8%
Distress Na??ve Bayes 0.743 0.750 0.744 74.4%
Majority Baseline 0.347 0.262 0.512 51.2%
Table 3: Classification results.
In our second set of experiments, we sought to
gain understanding of which features were pro-
viding the greatest value to classification perfor-
mance. We therefore retrained Na??ve Bayes classi-
fiers using only one feature at a time. We summa-
rize here some of the highest performing features.
For depression, we found that the feature stan-
dard deviation in onset time of first segment in
each user turn yielded very strong performance
by itself. In our corpus, we observed that de-
pressed individuals show a greater standard devia-
tion in the onset time of their responses to Ellie?s
questions (p = 0.024, Wilcoxon rank sum test).
The value of this feature in classification comple-
ments the clinical finding that depressed individu-
als show greater onset times in their responses to
interview questions (Cohn et al, 2009).
For distress, we found that the feature mean
maximum valence in user segments was the most
valuable. We discussed findings for a context-
dependent version of this feature in Section 4.3.
This finding for distress can be related to previ-
ous observations that individuals with depression
use more negatively valenced words (Rude et al,
2004).
For PTSD, we found that the feature mean num-
ber of filled pauses in user segments was among
the most informative.
Based on our observation of classification per-
formance using individual features, we believe
there remains much room for improvement in fea-
ture selection and training. A larger data set would
enable feature selection approaches that use held
out data, and would likely result in both increased
performance and deeper insights into the most
valuable combination of features for classification.
6 Conclusion
In this paper, we have explored the presence of in-
dicators of psychological distress in the linguistic
behavior of subjects in a corpus of semi-structured
virtual human interviews. In our data set, we
have identified several significant differences be-
tween subjects with depression and PTSD when
compared to non-distressed subjects. Drawing on
these features, we have presented statistical classi-
fication results that suggest the potential for auto-
matic assessment of psychological distress in indi-
vidual interactions with a virtual human dialogue
system.
Acknowledgments
This work is supported by DARPA under con-
tract (W911NF-04-D-0005) and the U.S. Army
Research, Development, and Engineering Com-
mand. The content does not necessarily reflect the
position or the policy of the Government, and no
official endorsement should be inferred.
References
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Jeffery F. Cohn, Tomas Simon Kruez, Iain Matthews,
Ying Yang, Minh Hoai Nguyen, Margara Tejera
Padilla, Feng Zhou, and Fernando De la Torre.
2009. Detecting depression from facial actions and
vocal prosody. In Affective Computing and Intelli-
gent Interaction (ACII), September.
Judith A. Hall, Jinni A. Harrigan, and Robert Rosen-
thal. 1995. Nonverbal behavior in clinician-patient
interaction. Applied and Preventive Psychology,
4(1):21 ? 37.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Peter A Heeman, Rebecca Lunsford, Ethan Selfridge,
Lois Black, and Jan Van Santen. 2010. Autism and
200
interactional aspects of dialogue. In Proceedings
of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 249?252.
Association for Computational Linguistics.
Kai Hong, Christian G. Kohler, Mary E. March, Am-
ber A. Parker, and Ani Nenkova. 2012. Lexi-
cal differences in autobiographical narratives from
schizophrenic patients and healthy controls. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 37?
47, Jeju Island, Korea, July. Association for Compu-
tational Linguistics.
Christine Howes, Matthew Purver, Rose McCabe,
Patrick G. T. Healey, and Mary Lavelle. 2012.
Predicting adherence to treatment for schizophrenia
from dialogue transcripts. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 79?83, Seoul,
South Korea, July. Association for Computational
Linguistics.
Shannon J Johnson, Michelle D Sherman, Jeanne S
Hoffman, Larry C James, Patti L Johnson, John E
Lochman, Thomas N Magee, David Riggs, Jes-
sica Henderson Daniel, Ronald S Palomares, et al
2007. The psychological needs of US military ser-
vice members and their families: A preliminary re-
port. American Psychological Association Presi-
dential Task Force on Military Deployment Services
for Youth, Families and Service Members.
Kurt Kroenke, Robert L. Spitzer, and Janet B. W.
Williams. 2001. The phq-9. Journal of General
Internal Medicine, 16(9):606?613.
Maider Lehr, Emily Prud?hommeaux, Izhak Shafran,
and Brian Roark. 2012. Fully automated neuropsy-
chological assessment for detecting mild cognitive
impairment. In Interspeech 2012: 13th Annual Con-
ference of the International Speech Communication
Association, Portland, Oregon, September.
Emily Prud?hommeaux and Brian Roark. 2011. Ex-
traction of narrative recall patterns for neuropsycho-
logical assessment. In Interspeech 2011: 12th An-
nual Conference of the International Speech Com-
munication Association, pages 3021?3024, Flo-
rence, Italy, August.
Stephanie Rude, Eva-Maria Gortner, and James Pen-
nebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121?1133.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In IEEE Conference on Automatic
Face and Gesture Recognition.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307, June.
Marcus Thiebaux, Stacy Marsella, Andrew N. Mar-
shall, and Marcelo Kallmann. 2008. Smartbody:
behavior realization for embodied conversational
agents. In Proceedings of the 7th international
joint conference on Autonomous agents and multi-
agent systems - Volume 1, AAMAS ?08, pages 151?
158, Richland, SC. International Foundation for Au-
tonomous Agents and Multiagent Systems.
Peter Wittenburg, Hennie Brugman, Albert Russel,
Alex Klassmann, and Han Sloetjes. 2006. Elan: a
professional framework for multimodality research.
In Proceedings of LREC 2006, Fifth International
Conference on Language Resources and Evaluation.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue. Computer Speech &
Language, 24(2):175 ? 189.
201
Appendix A. Wizard-of-Oz Dialogue Excerpts
Example user with PTSD and depression Example non-distressed user
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.014 when i play with my dog
Ellie when was the last time
that happened?
User 0.000 this morning
Ellie aw
Ellie tell me more about that
User 0.312 i don?t know i just
User 0.010 whenever i get
User 0.312 super bored or i just
need some time to
User 0.312 for myself i just play
with the dog
Ellie that?s good
User 0.000 yeah
0.1371 = mean
maximum valence in
user segments following
happy lasttime
Ellie (ideal weekendC) tell
me how you spend your
ideal weekend
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.562 um the last time i felt
really happy was
User 0.000 hm
User 0.000 today
Ellie tell me more about that
User 0.688 uh just from the moment
i woke up it was a
beautiful sunny day
User -0.062 i
User 0.565 went to see some friends
we had a good time
went to school
User 0.565 had some good grades
on some papers um
wrote a good essay
User 0.292 feel pretty accomplished
and
User 0.312 i feel like my day is just
User 0.565 a good day
Ellie that?s so good to hear
0.3487 = mean
maximum valence in
user segments following
happy lasttime
Ellie (BF describe) how
would your best friend
describe you?
Figure 5: Examples of maximum valence feature.
202
Proceedings of the SIGDIAL 2013 Conference, pages 372?374,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Roundtable: An Online Framework for Building Web-based         Conversational Agents 
 Eric Forbell and Nicolai Kalisch and Fabrizio Morbini and Kelly Christoffersen  and Kenji Sagae and David Traum and Albert A. Rizzo Institute for Creative Technologies University of Southern California Los Angeles, CA 90094, USA {lastname}@ict.usc.edu 
   
Abstract 
We present an online system that provides a complete web-based sandbox for creating, testing and publishing embodied conversation-al agents. The tool, called Roundtable, em-powers many different types of authors and varying team sizes to create flexible interac-tions by automating many editing workflows while limiting complexity and hiding architec-tural concerns. Finished characters can be pub-lished directly to web servers, enabling highly interactive applications.  1 Introduction To support the creation of a virtual guide system called SimCoach (Rizzo et al 2011) designed to help military service personnel and their families understand behavioral healthcare issues and learn about support resources, a core virtual human architecture that included a new dialogue man-agement approach was developed (Morbini et al, 2012b). SimCoach is an embodied, conversa-tional virtual human guide delivered via the web and is supported by a flexible information state dialogue manager called FLoReS designed to support mixed initiative dialogue with conversa-tional systems. Morbini et al (2012a) provide a detailed description of the dialogue manager. Although FLoReS supports a wide variety of virtual human character behaviors, these must be specified in dialogue policies that must be au-thored manually. Initially, authoring for this dia-logue manager required coding of policies using a custom programming language. Therefore sig-nificant training for content authors was neces-sary, as well as substantial support from dialogue 
system developers in managing resources such as training data for the language understanding sys-tem. To improve the accessibility of the system to non-technical subject matter experts and other creative staff, it became clear that additional tools were necessary. In this demonstration, we present Roundtable: a web-based authoring envi-ronment for virtual human characters that is de-signed for use by subject matter experts who are qualified for content authoring in targeted do-mains, but who may not possess technical skills in programming or experience in dialogue sys-tem design.  2 Supporting rapid authoring of dia-logue agents for the web Roundtable is a complete web-based authoring system enabling the end-to-end creation, valida-tion, testing and web publishing of virtual human characters using the SimCoach virtual human architecture. The system provides features that empower many types of authors, team sizes and makeups. The system allows an author to select from a set of preconfigured 3D character models, model the dialogue policy through behavior tem-plates and more direct subdialogue editing, train and test the natural language understanding com-ponent, render animation performances associat-ed with character behaviors and utterances, and test both text-based and fully animated interac-tions. Finally, the complete character dataset can be exported and deployed to a live, highly avail-able server environment, where interaction data can be monitored and periodically collected for analysis and refinement, all from within the same browser environment (Figure 1). The entire sys-tem, from authoring to end-user interaction with 
372
the virtual human character, is web-based and requires only a current web browser for content authors and end users.   (a)
 (b)
  (c)
  (d)
  Figure 1:  Selected modules from the Roundtable character authoring system (a) character project browser; (b) dialogue policy editor; (c) training data manager (d) action and animation asset man-ager 
At the core of the authoring application is an object-oriented information model and set of management systems that span the following roles: ? Dialogue content management, respon-sible for persistence, search, validation and retrieval operations of all dialogue el-ements including subdialogue networks; information state variables and effects; goals and effects; and dialogue action an-notations that provide the mapping to the action database. ? Training data management, concerned with managing training items for a data-driven natural language understanding module, as well as providing support for running regressions when updating the training set.   ? Action management, provides data op-erations for managing potentially large sets of virtual human performance-related assets, including utterance text, speech au-dio when not system-generated, annotated nonverbal behavior schedules, as well as non-performance actions which include web-hosted videos, digested web articles, or any arbitrary HTML effect. ? Deployment management, enabling rapid deployment of locally tested charac-ters to highly available web servers as well as review and data warehousing functions for both analytic and refinement purposes. The information model is implemented in a re-lational database that fully specifies, relates and allows inquiry and validation of authored infor-mation. Additionally, a complete web application programming interface (API) powers the Roundtable application, providing a transactional framework for data operations as well as user privilege enforcement, but which also allows application expansion. The information model also serves to decouple the authoring representation from the data struc-tures necessary to drive dialogue behavior at runtime. Prior to realizing an authored character in the FLoReS engine, project dialogue data ele-ments are exported into the format expected by the runtime target, a process that we expect to expand in the future to support different dialogue managers and language understanding configura-tions.  
373
 Figure 2: The interactive virtual human character published to the web, accessible by current brows-ers.  3 Demo script This demonstration will show how to build a simple conversational virtual human character using Roundtable, from acquiring an account (http://authoring.simcoach.org, free for academic research) to obtaining the URL for the newly created character, and all of the steps in between. The workflow to build a character is as follows: 1. In the project module (Figure 1a) we create a new character by providing a unique name and selecting an existing 3D character mod-el.  2. Opening the newly created project brings up the interaction module (Figure 1b) where we choose from a list of available subdialogue templates that can be used for common dia-logue behaviors (question-answer, greeting, etc.). The provided Greeting and Goodbye templates are used to define the character?s conversational behavior when initiating and ending an interaction, respectively. Invoking the Question-Answer template, we can quickly define how the character will re-spond to a specific question or statement. Each template requires a name and sample text for any user or system utterance.  3. Following the template-based subdialogue generation, we create training data for the natural language understanding component by providing possible user utterances associ-ated with each user dialogue act in the tem-plates used (Figure 1c).  4. The last task is to refine system utterances, which are generated automatically during the step of policy authoring, and generate anima-tion data. From the action module, we can search and inspect all system actions. For any system action, with a single button click, 
we can synthesize audio and render anima-tions (Figure 1d).  5. Finally, we navigate to the test module, compile our character project, and are then able to chat with the new character to ensure expected behavior. At this point, the charac-ter is ready to be deployed, with its unique URL, and is immediately accessible on the web (Figure 2). 4 Conclusion  We described the Roundtable online authoring framework that has been designed to support non-expert users in rapidly creating embodied, conversational virtual characters of varying complexities.  The tool, being web-based, re-quires zero configuration to get started and au-thored virtual characters can be deployed to In-ternet-facing web servers immediately, expand-ing the reach of many dialogue-driven applica-tions.  Acknowledgments The effort described here has been sponsored by the U.S. Army. Any opinions, content or infor-mation presented does not necessarily reflect the position or the policy of the United States Gov-ernment, and no official endorsement should be inferred. References  A. Rizzo, B. Lange, J.G. Buckwalter, E. Forbell, J. Kim, K. Sagae, J. Williams, B.O. Rothbaum, J. Difede, G. Reger, T. Parsons, and P. Kenny. An in-telligent virtual human system for providing healthcare information and support. In J.D. West-wood et al, editor, Technology and Informatics. IOS Press, 2011. Fabrizio Morbini, David Devault, Kenji Sagae, Jillian Gerten, Angela Nazarian and David Traum FLo-ReS: A Forward Looking, Reward Seeking, Dia-logue Manager in proceedings of International Workshop on Spoken Dialog Systems (IWSDS-2012), Ermenonville, France, November 2012b. Fabrizio Morbini, Eric Forbell, David DeVault, Kenji Sagae, David Traum and Albert Rizzo. A Mixed-Initiative Conversational Dialogue System for Healthcare. Demonstration in SIGdial 2012, the 13th Annual SIGdial meeting on Discourse and Dialogue, Seoul, South Korea, 2012a.    
374
Proceedings of the SIGDIAL 2013 Conference, pages 394?403,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Which ASR should I choose for my dialogue system?
Fabrizio Morbini, Kartik Audhkhasi, Kenji Sagae, Ron Artstein,
Dog?an Can, Panayiotis Georgiou, Shri Narayanan, Anton Leuski and David Traum
University of Southern California
Los Angeles, California, USA
{morbini,sagae,artstein,leuski,traum}@ict.usc.edu
{audhkhas,dogancan}@usc.edu {georgiou,shri}@sipi.usc.edu
Abstract
We present an analysis of several pub-
licly available automatic speech recogniz-
ers (ASRs) in terms of their suitability for
use in different types of dialogue systems.
We focus in particular on cloud based
ASRs that recently have become available
to the community. We include features
of ASR systems and desiderata and re-
quirements for different dialogue systems,
taking into account the dialogue genre,
type of user, and other features. We then
present speech recognition results for six
different dialogue systems. The most in-
teresting result is that different ASR sys-
tems perform best on the data sets. We
also show that there is an improvement
over a previous generation of recognizers
on some of these data sets. We also inves-
tigate language understanding (NLU) on
the ASR output, and explore the relation-
ship between ASR and NLU performance.
1 Introduction
Dialogue system developers who are not also
speech recognition experts are in a better posi-
tion than ever before in terms of the ease of in-
tegrating existing speech recognizers in their sys-
tems. While there have been commercial solutions
and toolkits for a number of years, there were a
number of problems in getting these systems to
work. For example, early toolkits relied on spe-
cific machine hardware, software, and firmware
to function properly, often had a difficult instal-
lation process, and moreover often didn?t work
well for complex dialogue domains, or challeng-
ing acoustic environments. Fortunately the situ-
ation has greatly improved in recent years. Now
there are a number of easy to use solutions, in-
cluding open-source systems (like PocketSphinx),
as well as cloud-based approaches.
While this increased choice of quality recogniz-
ers is of great benefit to dialogue system develop-
ers, it also creates a dilemma ? which recognizer
to use? Unfortunately, the answer is not simple ?
it depends on a number of issues, including the
type of dialogue domain, availability and amount
of training data, availability of internet connectiv-
ity for the runtime system, and speed of response
needed. In this paper we assess several freely
available speech recognition engines, and exam-
ine their suitability and performance in several di-
alogue systems. Here we extend the work done in
Yao et al (2010) focusing in particular on cloud
based freely available ASR systems. We include
2 local ASRs for reference, one of which was also
used in the earlier work for easy comparison.
2 Speech Recognizer Features and
Engines
The following are some of the major criteria for
selection of a speech recognizer.
Customization Some of the available speech
recognizers allow the users to tune the recognizer
to the environment it will operate in, by providing
a specialized lexicon, trained language models or
acoustic models. Customization is especially im-
portant for dialogue systems whose input contains
specialized vocabulary (see section 4).
Output options A basic recognizer will output
a string of text, representing its best hypothesis
about the transcription of the speech input. Some
recognizers offer additional outputs which are use-
ful for dialogue systems: ranked n-best hypothe-
ses allow later processing to use context for dis-
ambiguation, and incremental results allow the
system to react while the user is still speaking.
Performance characteristics Dialogue systems
differ in their requirements for response speed; a
394
System Customization Output options Open Source PerformanceN-best Incremental Speed Installation
Pocketsphinx Full Yes Yes Yes realtime Local
Apple No Noa No No network Cloud
Google No Yes Yesb No network Cloud
AT&T Partialc Yes No No network Cloud
Otosense-Kaldi Full Yes No Yesd variablee Local
aSingle output annotated with alternative hypotheses. bOnly for web-delivered applications in a Google Chrome browser.
cCustom language models. dRelease scheduled for Fall 2013. eUser controls trade-off between speed and output quality.
Table 1: Speech recognizer features important for use in dialogue systems
speech recognizer that runs locally can help by
avoiding network latencies.
Output quality Typically, a dialogue system
would want the best recognition accuracy pos-
sible given the constraints. Ultimately, dialogue
systems want the output that would yield the best
performance for Natural Language Understand-
ing and other downstream processes. As a rule,
better speech recognition leads to better language
understanding, though this is not necessarily the
case for specific applications (see section 5).
We evaluated 5 freely available speech recog-
nizers. Their features are summarized in Table 1.
We did not include the MIT WAMI toolkit1 as we
are focused on speech services that can directly
be used by stand alone applications as opposed to
web delivered ones. We did not include commer-
cial recognizers such as Nuance, because licensing
terms can be difficult for research institutions, and
in particular, disallow publishing benchmarks.
Pocketsphinx is a version of the CMU Sphinx
ASR system optimized to run also on embedded
systems (Huggins-Daines et al, 2006). Pocket-
sphinx is fast, runs locally, and requires relatively
modest computational resources. It provides n-
best lists and lattices, and supports incremental
output. It also provides a voice activity detec-
tion functionality for continuous ASR. This ASR
is fully customizable and trainable, but users are
expected to provide language models suitable for
their applications. A few acoustic models are pro-
vided, and can be adapted using the CMUSphinx
tools.2
1http://wami.csail.mit.edu/
2http://cmusphinx.sourceforge.net/wiki/tutorialadapt
Apple Dictation is the OS level feature in both
MacOSX and iOS.3 It is integrated into the text in-
put system pipeline so a user can replace her key-
board with a microphone for entering text in any
application. Dictation is often associated with the
Siri personal assistant feature of iOS. While it is
likely that Dictation and Siri share the same ASR
technology, Dictation only does speech recogni-
tion. Apple states that Dictation learns the charac-
teristics of the user?s voice and adapts to her accent
(Apple Inc, 2012). Dictation requires an internet
connection to send recorded user speech to Ap-
ple?s servers and receive ASR results. Processing
starts as soon as the user starts speaking so the de-
lay of getting the recognition results after the user
finishes speaking is minimal.
To integrate Dictation into a dialogue system,
a system designer needs to include any system de-
fined text input control into her application and use
the control APIs to observe text changes. The user
would need to press a key when starting to speak
and push the key again once she is done speak-
ing. The ASR result is a text string annotated with
alternative interpretations of individual words or
phrases in the text. There is an API for extract-
ing those interpretations from the result. While the
Dictation feature is reasonably fast and easy to in-
tegrate, dialogue system developers have no con-
trol over the ASR process, which must be treated
as a black box. Apple dictation is limited in that
no customization is possible, no partial recogni-
tion results are provided, and there is an unspeci-
fied limit on the number of utterances dictated for
a period of time, which is not a problem for inter-
action between a single user and a dialogue sys-
tem, but may be an issue in dialogue systems that
support multiple concurrent users.
3Dictation was introduced in iOS 5.0 and MacOSX 10.8.
395
Google Speech API provides support for the
HTML 5 speech input feature.4 It is a cloud based
service in which a user submits audio data using
an HTML POST request and receives as reply the
ASR output in the form of an n-best list. The au-
dio data is limited to roughly 10 seconds in length,
longer clips are rejected and return no ASR results.
The user can (1) customize the number of hy-
potheses returned by the ASR, (2) specify which
language the audio file contains and (3) enable a
filter to remove profanities from the output text.
As is the case with Apple Dictation, ASR must be
treated as a black box, and no task customization
is possible for dialogue system developers. Users
cannot specify or provide custom language models
or acoustic models. The service returns only the fi-
nal hypothesis, there is no incremental output.5 In
addition, results for the same inputs may change
unpredictably, since Google may update or other-
wise change its service and models, and models
may be adapted using specific audio data supplied
by users. In our experiments, we observed accu-
racy improvements when submitting the same au-
dio files over repeated trials over two weeks.
AT&T Watson is the ASR engine available
through the AT&T Speech Mashup service.6 It is
a cloud based service that can be accessed through
HTML POST requests, like the Google Speech
API. AT&T Watson is designed to support the de-
mands of online spoken dialogue systems, and can
be customized with data specific to a dialogue sys-
tem. Additionally, in our tests we did not observe
any limitation in the maximum length of the in-
put audio data. However, AT&T does not provide
a default general-purpose language model, and
application-specific models must be built within
the Speech Mashup service using user-provided
text data. The acoustic model must be selected
from a list provided by the AT&T service, and
acoustic models can be further customized within
the Speech Mashup service. The ASR returns an
n-best list of hypotheses but does not provide in-
cremental output.
Otosense-Kaldi Another ASR we employed
was the Kaldi-based OtoSense-Kaldi engine de-
4https://www.google.com/speech-api/v1/recognize
5The demo page shows continuous speech understanding
with incremental results but requires Google Chrome to run
and is specific to web delivered applications:
http://www.google.com/intl/en/chrome/demos/speech.html
6https://service.research.att.com/smm
veloped at SAIL.7 OtoSense-Kaldi8 is an on-line,
multi-threaded architecture based on the Kaldi
toolkit (Povey et al, 2011) that allows for dynam-
ically configurable and distributed ASR.
3 Dialogue Systems, Users, and Data
All spoken dialogue systems are similar in some
respects, in that there is speech by a user (or users)
that needs to be recognized, and this speech is
punctuated by speech from the system. More-
over, the speech is not fully independent, but ut-
terances are connected to other utterances, e.g. an-
swers to questions, or clarifications. There are,
however many ways in which systems can differ,
that have implications for which speech recogniz-
ers are most appropriate. Some of the dimensions
to consider are:
Type of microphone(s) One of the biggest im-
pacts on ASR is the acoustic environment. Will
the audio be clean, coming from a close-talking
head or lapel-mounted microphone, or will it need
to be picked up from a broader directional micro-
phone or microphone array?
Number of speakers/microphones Will there
be one designated microphone per person, or will
speaker identification need to be performed? Will
audio from the system confuse the ASR?
Push to talk or continuous speech Will the
user clearly identify the start and end of speech,
or will the system need to detect speech acousti-
cally?
Type of Users Will there be designated long-
term users, where user-training or system model
adaptation is feasible, or will there be many un-
known users, where training is not feasible? See
also section 3.1 for more on user types.
Genre What kinds of things will people be say-
ing to the system? Is it mostly commands or short
answers to questions, or more open-ended conver-
sation? See section 3.2 for more on genre issues.
Training Data Is within-domain training data
available, and if so how much?
3.1 Types of Users
The type of user is important for the overall
design of the system and has implications for
7http://sail.usc.edu
8OtoSense-Kaldi will be released (BSD license) in 2013.
396
ASR performance as well. One important as-
pect is the broad physical differences among
speakers, such as male vs female, adult vs child
(e.g. Bell and Gustafson, 2003), or language pro-
ficiency/accent, that will have implications for the
acoustics of what is said, and ASR results. Other
aspects of users have implications for what will
be said, and how successful the interface may
be, overall. Many (e.g. Hassel and Hagen, 2006;
Jokinen and Kanto, 2004) have looked at the dif-
ferences between novice and expert users. Ai et
al. (2007a) also points out a difference between
real users and recruited subjects. Real users also
come in many different flavors, depending on their
purposes. E.g. are they interacting with the system
for fun, to do a specific task that they need to get
done, to learn something (specific or general), or
with some other purpose in mind?
We considered the following classes of users,
ordered from easiest to hardest to get to acceptable
performance and robustness levels:
Demonstrators are generally the easiest for a sys-
tem to understand ? a demonstrator is trained in
use of the system, knows what can and can?t be
said, is motivated toward success, and is gener-
ally interested in showing off the most impres-
sive/successful aspects of the system to an audi-
ence rather than using it for its own sake.
Trained/Expert Users are similar to demonstra-
tors, but use the system to achieve specific results
rather than just to show off its capabilities. This
means that users may be forced down lines that
are not ideal for the system, if these are necessary
to accomplish the task.
Motivated Users do not have the training of ex-
pert users, and may say many things that the sys-
tem can not handle as opposed to equivalent ex-
pressions that could be handled. However moti-
vated users do want the system to succeed, and in
general are willing to do whatever they think is
necessary to improve system performance. Unlike
expert users, motivated users might be incorrect
about what will help the system (e.g. hyperarticu-
lation in response to system misunderstanding).
Casual Users are interested in finding out what
the system can do, but do not have particular moti-
vations to help or hinder the system. Casual Users
may also leave in the middle of an interaction, if it
is not engaging enough.
Red Teams are out to test or ?break? the system,
or show it as not-competent, and may try to do
things the system can?t understand or react well
to, even when an alternative formulation is known
to work.
3.2 Types of Dialogue System Genres
Dialogue Genres can be distinguished along many
lines, e.g. the number and relationship of partic-
ipants, specific conversational rules, purposes of
the participants, etc. We distinguish here four gen-
res of dialogue system that have been in use at
the Institute for Creative Technologies and that we
have available corpora for (there are many other
types of dialogue genres, including tutoring, ca-
sual conversation, interviewing,. . . ). Each genre
has implications for the internal representations
and system architectures needed to engage in that
genre of dialogue.
Simple Question-answering This genre in-
volves strong user-initiative and weak global di-
alogue coherence. The user can ask any ques-
tion to the system at any time, and the system
should respond, with an appropriate answer if
able, or with some other reply indicating either
inability or unwillingness to provide the answer.
This genre allows modeling dialogue at a surface-
text level (Gandhe, 2013), without internal se-
mantic representations of the input, and where
the result of ?understanding? input is the system?s
expected output. The NCPEditor9 (Leuski and
Traum, 2011) is a toolkit that provides an author-
ing environment, classification, and dialogue ca-
pability for simple question-answering characters.
The SGT Blackwell, SGT Star, and Twins systems
described below are all systems in this genre.
Advanced Question-answering This genre is
similar to the simple question-answering charac-
ters, in that the main task of the user is to elicit
information from the system character. The differ-
ence is that there is more long-range and interme-
diate dialogue coherence, in that questions can be
answered several utterances after they have been
asked, there can be intervening sub-dialogues, and
characters sometimes take the initiative to pursue
their own goals rather than just responding to the
user. Because of the requirements for somewhat
deeper understanding, and relation of input to con-
9Available free for academic research purposes from
https://confluence.ict.usc.edu/display/VHTK/Home
397
text and character goals and policies, there is a
need of at least a shallow semantic representa-
tion and representation of the dialogue informa-
tion state, and the character must distinguish un-
derstanding of the input from the character out-
put (since the latter will depend on the dialogue
policy and information state, not just the under-
standing of input). The tactical questioning archi-
tecture (Gandhe et al, 2009)10 provides author-
ing and run-time support for advanced question-
answering characters, and has been used to build
over a dozen characters for purposes such as train-
ing tactical questioning, training culture, and psy-
chology experiments (Gandhe et al, 2011). The
Amani character described below is in this genre.
Slot-filling Probably the most common type of
dialogue system (at least in the research commu-
nity) is slot-filling. Here the dialogue is fairly
structured, with an initial greeting phase, then one
or more tasks, which all start with the user se-
lecting the task, and the system taking over ini-
tiative to ?fill? and possibly confirm the needed
slots, before retrieving some information from a
database, or performing a simple service.11 This
genre also requires a semantic representation, at
least of the slots and acceptable values. Gener-
ally, the set of possible values is large enough, that
some form of NLG is needed (at least template
filling), rather than authoring of all full sentences.
There are a number of toolkits and development
frameworks that are well suited to slot-filling sys-
tems, e.g. Ravenclaw (Bohus and Rudnicky, 2003)
or Trindikit (Larsson and Traum, 2000). The Ra-
diobots system, described below is in this genre.
Negotiation and Planning In this genre, the
system is more of an equal partner with the user,
than a servant, as in the slot-filling systems. The
system must not merely understand user requests,
but must also evaluate whether they meet the sys-
tem goals, what the consequences and precondi-
tions of requests are, and whether there are better
alternatives. For this kind of inference, a more de-
tailed semantic representation is required than just
filling in slots. While we are not aware of publicly
available software that makes this kind of system
easy to construct, there have been several built us-
ing an information-state approach, or the soar cog-
10Soon to be released as part of the virtual human toolkit.
11Mixed-initiative versions of this genre exist, where the
user can also provide unsolicited information, which reduces
the number of system queries needed.
nitive architecture. The TRIPS system (Allen et
al., 2001) also has many similarities.
3.3 ICT Dialogue Systems Tested
We tested the recognizers described in section 2
on data sets collected from six different dialogue
domains. Five are the same ones tested in Yao et
al. (2010), to which we added the Twins set. De-
tails on the size of the training and development
sets may be found in Yao et al (2010), here we
report only the numbers relevant to the Twins do-
main and to the NLU analysis, which are not in
Yao et al (2010).
SGT Blackwell was created as a virtual human
technology demonstration for the 2004 Army Sci-
ence Conference. This is a question-answering
character, with no internal semantic representation
and the primary NLU task merged with Dialogue
management as selecting the best response.
The original users were ICT demonstrators.
However, there were also some experiments with
recruited participants (Leuski et al, 2006a; Leuski
et al, 2006b). Later SGT Blackwell became a part
of the ?best design in America? triennial at the
Cooper-Hewitt Museum in New York City, and
the data set here is from visitors to the museum,
who are mostly casual users, but range from expert
to red-team. Users spoke into a mounted direc-
tional microphone (see Robinson et al, 2008 for
more details).
SGT STAR (Artstein et al, 2009a) is a question-
answering character similar to SGT Blackwell, al-
though designed to talk about Army careers rather
than general knowledge. The users are Army per-
sonnel who went to job fairs and visited schools in
the mobile Army adventure vans, speaking using
headset microphones, and performing for an audi-
ence. The users are somewhere between demon-
strators and expert users. They are speaking to
SGT STAR for the benefit of an audience, but their
primary purpose is to convey information to the
audience in a memorable way (through dialogue
with SGT STAR) rather than to show off the high-
lights of the character.
The Twins are two life-size virtual characters
who serve as guides at the Museum of Science
in Boston (Swartout et al, 2010). The charac-
ters promote interest in Science, Technology, En-
gineering and Mathematics (STEM) in children
between the ages of 7 and 14. They are question-
398
answering characters, but unlike SGTs Blackwell
and Star, the response is a whole dialogue se-
quence, potentially involving interchange from
both characters, rather than a single character turn.
There are two types of users for the Twins:
demonstrators, who are museum staff members,
using head-mounted microphones, and museum
visitors, who use a Shure 522 table-top mounted
microphone (Traum et al, 2012). More on analy-
sis of the museum data can be found in (Aggarwal
et al, 2012). We also investigated speech recog-
nition and NLU performance in this domain in
Morbini et al (2012).
This dataset contains 14K audio files each an-
notated with one of the 168 possible response se-
quences. The division in training development and
test is the same used in Morbini et al (2012) (10K
for training, the rest equally divided between de-
velopment and test).
Amani (Artstein et al, 2009b; Artstein et al,
2011) is an advanced question-answering char-
acter used as a prototype for systems meant to
train soldiers to perform tactical questioning. The
users are in between real users and test subjects:
they were cadets at the U.S. Military Academy in
April 2009, who interacted with Amani as a uni-
versity course exercise on negotiation techniques.
They used head-mounted microphones to talk with
Amani.
This dataset comprises of 1.8K audio files each
annotated with one of the 105 possible NLU se-
mantic classes.
Radiobots (Roque et al, 2006) is a training pro-
totype that responds to military calls for artillery
fire in a virtual reality urban combat environment.
This is a domain in the slot-filling genre, where
there is a preferred protocol for the order in which
information is provided and confirmed. Users are
generally trainees, learning how to do calls for fire,
they are motivated users with some training. The
semantic processing involved tagging each word
with the dialogue act and parameter that it was as-
sociated with (Ai et al, 2007b).
This data set was collected during the develop-
ment of the system in 2006 at Fort Sill, Oklahoma,
during two evaluation sessions from recruited vol-
unteer trainees who performed calls for specific
missions (Robinson et al, 2006). These subjects
used head-mounted microphones rather than the
ASTI simulated radios from later data collection.
SASO-EN (Traum et al, 2008) is a negotiation
training prototype in which two virtual characters
negotiate with a human ?trainee? about moving a
medical clinic. The genre is negotiation and plan-
ning, where the human participant must try to form
a coalition, and the characters reason about utili-
ties of different proposals, as well as causes and
effects. The output of NLU is a frame represen-
tation including both semantic elements, like the-
matic argument structure, and pragmatic elements,
such as addressee and referring expressions. Fur-
ther contextual interpretation is performed by each
of the virtual characters to match the (possibly par-
tial) representation to actions and states in their
task model, resolve other referring expressions,
and determine a full set of dialogue acts (Traum,
2003). Speech was collected at the USC Insti-
tute for Creative Technologies (ICT) during 2006?
2009, mostly from visitors and new hires, who
acted as test subjects.
This dataset has 4K audio files each anno-
tated with one of the 117 different NLU semantic
classes.
4 ASR Performance
We tested each of the Datasets described in Sec-
tion 3.3 with some of the recognizers described
in Section 2. All recognizers were tested on the
Amani, SASO-EN, and Twins domains, and we
also tested a natural language understanding com-
ponent on these data sets (Section 5). For SGT
Blackwell, SGT STAR, and Radiobots, we report
the performance on the same development set used
in Yao et al (2010). For Amani and SASO-EN
(where we also report the NLU performance), we
run a 10-fold cross-validation in which 9 folds
where used to train the NLU and ASR language
model and the 10th was used for testing. For the
Twins dialogue system, we used the same partition
into training, development and testing reported in
Morbini et al (2012) and the results reported here
are from the development set. Due to differences
in training/testing regimens, performance of sys-
tems are only comparable within each domain.
Table 2 summarizes the performance of the var-
ious ASR engines on the evaluation data sets. Per-
formance is measured as Word Error Rate and was
obtained using the NIST SCLITE tool.12
Note that only Otosense-Kaldi in the Twins do-
main had adapted acoustic models. In the remain-
12http://www.itl.nist.gov/iad/mig/tools/
399
Speech recognizer Evaluation data setAmani Radiobots SASO-EN SGT Blackwell SGT Star Twins
Pocketsphinx 39.7 11.8 28.4 51 28.6 81
Apple 28 ? 30.9 ? ? 29
AT&T 29 12.1 16.3 27.3 21.7 28.8
Google 23.8 36.3 20 18 26 20.6
Otosense-Kaldi 33.7 ? 22.1 ? ? 18.7
Table 2: Word Error Rates (%) for the various dialogue systems and ASR systems tested.
ing cases only the language model was adapted.
Looking at the results on the development set re-
ported in Yao et al (2010), we have improvements
in 3 out of 5 domains: Amani (?11.8% Google),
SASO-EN (?11.7% AT&T) and SGT Blackwell
(?13% Google). In Radiobots and SGT Star the
performance achieved with just language model
adaptation, when permitted, is worse: +4.8% and
+1.7% respectively.
We find that there is no single best performing
speech recognizer: results vary greatly between
the evaluation test sets. In 4 of the 6 datasets over-
all, and 2 of the 3 datatests tested with Otosense-
Kaldi, the best performer is a cloud-based ser-
vice (Google or AT&T). There are two datasets
for which a local, fully customizable recognizer
performs better than the cloud-based services. Ra-
diobots, consisting of military calls for artillery
fire, has a fairly limited and very specialized vo-
cabulary, and indeed the two recognizers with cus-
tom language models (Pocketsphinx and AT&T)
perform much better than the non-customizable
recognizer (Google).
The Twins dataset is unique in that for the
Otosense-Kaldi system we custom-trained acous-
tic and language models, while standard WSJ
acoustic models and adapted language models
were used for the other dialogue systems. In
both cases the models were triphone based with
a Linear Discriminant Analysis (LDA) front end,
and Maximum Likelihood Linear Transforma-
tion (MLLT) and Maximum Mutual Information
(MMI) training. This reflects on the very good
performance in the Twins domain, decent perfor-
mance on the SASO-EN domain (reasonable mis-
match of WSJ and SASO-EN) and very degraded
performance in Amani (highly mismatched Amani
and WSJ domains). The observed degradation in
performance is accentuated by the MMI discrim-
inative training on the mismatched-WSJ data. As
with PocketSphinx and Watson, and unlike with
Apple Dictation and Google Speech API, with
Kaldi we fully control experimental conditions
and can guarantee no contamination of the train-
test data.
In summary, our evaluation shows that cus-
tomizable recognizers are useful when the ex-
pected speech is highly specialized, or when sub-
stantial resources are available for tuning the rec-
ognizer.
5 NLU Accuracy & Relation between
ASR and NLU
While the different genres of system have different
types of output for NLU: response text, dialogue
act and parameter tags, speech acts, or semantic
frames, many of them can be coerced into a se-
lection task, in which the NLU selects the right
output from a set of possible outputs. This allows
any multiclass classification algorithm to be used
for NLU. A possible drawback is that for some
inputs, the right output might not be available in
the set considered by the training data, even if it
might easily be constructed from known parts us-
ing a generative approach.
A second issue is that even though we can cast
the problem as multi-class classification, classifi-
cation accuracy is not always the most appropriate
metric of NLU quality. For question-answering
characters, getting an appropriate and relevant re-
ply is more important than picking the exact re-
ply selected by a human domain designer or an-
notator: there might be multiple good answers, or
even the best available answer might not be very
good. For that reason, the question-answering
characters allow an ?off-topic? answer and Error-
return plots (Artstein, 2011) might be necessary
to choose an optimal threshold. For the SASO-EN
system, slot-filler metrics such as precision, recall,
and f-score are more appropriate than frame accu-
400
racy, because some frames may have many slots
in common and few that are different (e.g. just a
different addressee). Nonetheless, we begin our
analysis within this common framework. For sim-
plicity, we start with just three domains: Twins,
Amani, and SASO-EN. SGT STAR and Blackwell
are very similar to Twins in terms of NLU. Ra-
diobots is more challenging to coerce to multiclass
classification.
Conventional wisdom in the speech and lan-
guage processing community is that performance
of ASR and NLU are closely tied: improved
speech recognition leads to better language under-
standing, while deficiencies in speech recognition
cause difficulty in understanding. This conven-
tional wisdom is borne out by decades of experi-
ence with speech and dialogue systems, though we
are not aware of attempts to systematically demon-
strate it. The present study shows that the expected
relation between speech recognition and language
understanding holds for the systems we tested.
Accepted assumptions about the relation be-
tween speech recognition and language under-
standing have been repeatedly challenged. Direct
challenges are typically limited to specific appli-
cations. Wang et al (2003) show that for a slot-
filling NLU, ASR can be specifically tuned to rec-
ognize those words that are relevant to the slot-
filling task, resulting in improved understanding
despite a decrease in performance on overall word
recognition. However, Boros et al (1996) found
that when not optimizing the ASR for the specific
slot filling task there is a nearly linear correlation
between word accuracy and NLU accuracy. Al-
shawi (2003) and Huang and Cox (2006) show that
in call-routing applications the word level can be
dispensed with altogether and calls routed based
on phonetic information alone without noticeable
loss in performance. These challenges suggest that
the speech-language divide is not as clean as the
theory suggests.
To investigate the relation between ASR and
NLU, we ran each ASR output from each of
the 5 recognizers through an understanding com-
ponent to obtain an NLU output (each dataset
had a separate NLU component, which was held
constant for all speech recognizers). ASR and
NLU performance are conventionally measured on
scales of opposite polarity: better performance
shows up as lower word error rates but higher
NLU accuracies. For the correlations we invert the
conventional ASR scale and use word accuracy, so
that higher numbers signify better performance on
both scales.13
Figure 1 shows the results obtained in the 3 di-
alogue systems by the various ASR systems. The
figures plot ASR performance against NLU per-
formance; NLU results on manual transcriptions
are included for comparison. There are too few
data points for the correlations between ASR and
NLU performance to be significant, but the trends
are positive, as expected.
Our experiments lend supporting evidence to
the claim that in general, ASR performance is pos-
itively linked to NLU performance (special cases
notwithstanding). The 3 datasets exhibit posi-
tive correlations between speech recognition and
language understanding performance. Thus, we
claim that the basis of the conventional wisdom
is sound: speech recognition directly affects lan-
guage understanding. This conclusion holds when
the speech recognizer has been optimized to pro-
duce the most accurate transcript, rather than for a
specific NLU.
6 Conclusion and Future Work
We have extended here the ASR system evaluation
published in Yao et al (2010) including some new
cloud based ASR services that achieve very good
performance showing an improvement of around
12%. We also showed that ASR and NLU perfor-
mance are correlated.
One possible avenue of future work is to ex-
tract importance weights for each word from the
learnt NLU models and use these weights to try
to explain those cases that diverge from the corre-
lation between ASR and NLU performance. This
may also give us a better measure than WER for
assessing ASR performance in dialogue systems.
Another avenue of future work involves examin-
ing different types of NLU engines, and different
metrics for the different dialogue system genres,
which, again, may lead to a more relevant assess-
ment of ASR performance.
Acknowledgments
The effort described here has been sponsored by
the U.S. Army. Any opinions, content or informa-
tion presented does not necessarily reflect the posi-
13We define ?accuracy? as 1 minus WER, so this number
can in principle dip below zero if there are more errors than
words.
401
Amani
r = 0.54, df = 3, p = 0.345
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
50 60 70 80 90 100
51
54
57
60
63
66
69
?
?
?
? ?
?
SASO-EN
r = 0.77, df = 3, p = 0.130
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
60 70 80 90 100
30
40
50
60
70
80
90
?
???
?
?
Twins
r = 0.99, df = 3, p = 0.002
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
0 20 40 60 80 100
40
50
60
70
80
90
100
?
?
?
?
?
Figure 1: Relation between ASR and NLU performance (red dots are manual transcriptions)
tion or the policy of the United States Government,
and no official endorsement should be inferred.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, Athana-
sios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David Traum. 2012. The Twins cor-
pus of museum visitor questions. In LREC-2012,
Istanbul, Turkey, May.
Hua Ai, Antoine Raux, Dan Bohus, Maxine Eskenazi,
and Diane Litman. 2007a. Comparing spoken dia-
log corpora collected with recruited subjects versus
real users. In SIGdial 2007.
Hua Ai, Antonio Roque, Anton Leuski, and David
Traum. 2007b. Using information state to improve
dialogue move identification in a spoken dialogue
system. In Proceedings of the 10th Interspeech Con-
ference, Antwerp, Belgium, August.
James F. Allen, George Ferguson, and Amanda Stent.
2001. An architecture for more realistic conversa-
tional systems. In IUI, pages 1?8.
Hiyan Alshawi. 2003. Effective utterance classifica-
tion with unsupervised phonotactic models. In HLT-
NAACL 2003, pages 1?7, Edmonton, Alberta, May.
Apple Inc. 2012. Mac basics: Dictation (Technote
HT5449), November.
R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and
D. Traum. 2009a. Semi-formal evaluation of con-
versational characters. In O. Grumberg, M. Kamin-
ski, S. Katz, and S. Wintner, editors, Languages:
From Formal to Natural. Essays Dedicated to Nis-
sim Francez on the Occasion of His 65th Birthday,
volume 5533 of Lecture Notes in Computer Science,
pages 22?35. Springer, Berlin.
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009b. Viability of a simple dia-
logue act scheme for a tactical questioning dialogue
system. In DiaHolmia 2009: Proceedings of the
13th Workshop on the Semantics and Pragmatics of
Dialogue, page 43?50, Stockholm, Sweden, June.
Ron Artstein, Michael Rushforth, Sudeep Gandhe,
David Traum, and MAJ Aram Donigian. 2011.
Limits of simple dialogue acts for tactical question-
ing dialogues. In 7th IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
Barcelona, Spain, July.
Ron Artstein. 2011. Error return plots. In 12th SIG-
dial Workshop on Discourse and Dialogue, Port-
land, OR, June.
Linda Bell and Joakim Gustafson. 2003. Child and
adult speaker adaptation during error resolution in a
publicly available spoken dialogue system. In IN-
TERSPEECH 2003.
Dan Bohus and Alexander I. Rudnicky. 2003. Raven-
claw: dialog management using hierarchical task de-
composition and an expectation agenda. In INTER-
SPEECH 2003.
M. Boros, W. Eckert, F. Gallwitz, G. Grz, G. Han-
rieder, and H. Niemann. 1996. Towards understand-
ing spontaneous speech: Word accuracy vs. concept
accuracy. In In Proceedings of (ICSLP 96), pages
1009?1012.
Sudeep Gandhe, Nicolle Whitman, David R. Traum,
and Ron Artstein. 2009. An integrated authoring
tool for tactical questioning dialogue systems. In 6th
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Pasadena, California, July.
Sudeep Gandhe, Michael Rushforth, Priti Aggarwal,
and David R. Traum. 2011. Evaluation of
an integrated authoring tool for building advanced
question-answering characters. In Proceedings of
Interspeech-11, Florence, Italy, 08/2011.
Sudeep Gandhe. 2013. Rapid prototyping and evalu-
ation of dialogue systems for virtual humans. Ph.D.
thesis, University of Southern California.
402
Liza Hassel and Eli Hagen. 2006. Adaptation of an
automotive dialogue system to users? expertise and
evaluation of the system. Language Resources and
Evaluation, 40(1):67?85.
Quiang Huang and Stephen Cox. 2006. Task-
independent call-routing. Speech Communication,
48(3?4):374?389.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Acoustics,
Speech and Signal Processing, 2006. ICASSP 2006
Proceedings. 2006 IEEE International Conference
on, volume 1, pages I?I.
Kristiina Jokinen and Kari Kanto. 2004. User ex-
pertise modeling and adaptivity in a speech-based
e-mail system. In Donia Scott, Walter Daelemans,
and Marilyn A. Walker, editors, ACL, pages 87?94.
ACL.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language En-
gineering, 6:323?340, September. Special Issue on
Spoken Language Dialogue System Engineering.
Anton Leuski and David R. Traum. 2011. NPCEditor:
Creating virtual human dialogue using information
retrieval techniques. AI Magazine, 32:42?56.
Anton Leuski, Brandon Kennedy, Ronakkumar Patel,
and David Traum. 2006a. Asking questions to
limited domain virtual characters: How good does
speech recognition have to be? In 25th Army Sci-
ence Conference.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006b. Building effective ques-
tion answering characters. In Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue,
pages 18?27.
Fabrizio Morbini, Kartik Audhkhasi, Ron Artstein,
Maarten Van Segbroeck, Kenji Sagae, Panayio-
tis S. Georgiou, David R. Traum, and Shrikanth S.
Narayanan. 2012. A reranking approach for recog-
nition and classification of speech input in conversa-
tional dialogue systems. In SLT, pages 49?54. IEEE.
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka?s?
Burget, Ondr?ej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motl??c?ek, Yanmin Qian, Petr
Schwarz, Jan Silovsky?, Georg Stemmer, and Karel
Vesely?. 2011. The Kaldi speech recognition
toolkit. In IEEE 2011 Workshop on Automatic
Speech Recognition and Understanding, December.
S.M. Robinson, A. Roque, A. Vaswani, D. Traum,
C. Hernandez, and B. Millspaugh. 2006. Evalua-
tion of a spoken dialogue system for virtual reality
call for fire training. In 25th Army Science Confer-
ence, Orlando, Florida, USA.
S. Robinson, D. Traum, M. Ittycheriah, and J. Hen-
derer. 2008. What would you ask a conversational
agent? Observations of human-agent dialogues in
a museum setting. In Proc. of Sixth International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
A. Roque, A. Leuski, V. Rangarajan, S. Robinson,
A. Vaswani, S. Narayanan, and D. Traum. 2006.
Radiobot-CFF: A spoken dialogue system for mil-
itary training. In Proc. of Interspeech, Pittsburgh,
Pennsylvania, USA.
W. Swartout, D. Traum, R. Artstein, D. Noren, P. De-
bevec, K. Bronnenkant, J. Williams, A. Leuski,
S. Narayanan, D. Piepol, C. Lane, J. Morie, P. Ag-
garwal, M. Liewer, J. Chiang, J. Gerten, S. Chu,
and K. White. 2010. Ada and Grace: Toward
realistic and engaging virtual museum guides. In
J. Allbeck, N. Badler, T. Bickmore, C. Pelachaud,
and A. Safonova, editors, Intelligent Virtual Agents:
10th International Conference, IVA 2010, Philadel-
phia, PA, USA, September 20?22, 2010 Proceed-
ings, volume 6356 of Lecture Notes in Artificial In-
telligence, pages 286?300. Springer, Heidelberg.
David R. Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal
virtual agents. In IVA, pages 117?130.
David Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William Swartout. 2012.
Ada and grace: Direct interaction with museum
visitors. In The 12th International Conference on
Intelligent Virtual Agents (IVA), Santa Cruz, CA,
September.
David Traum. 2003. Semantics and pragmatics of
questions and answers for dialogue agents. In pro-
ceedings of the International Workshop on Compu-
tational Semantics, pages 380?394.
Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is
word error rate a good indicator for spoken lan-
guage understanding accuracy. In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing (ASRU ?03), pages 577?582.
Xuchen Yao, Pravin Bhutada, Kallirroi Georgila, Kenji
Sagae, Ron Artstein, and David R. Traum. 2010.
Practical evaluation of speech recognizers for vir-
tual human dialogue systems. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, LREC. European Language
Resources Association.
403
Proceedings of the SIGDIAL 2014 Conference, pages 69?73,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Improving Classification-Based Natural Language Understanding with
Non-Expert Annotation
Fabrizio Morbini and Eric Forbell and Kenji Sagae
Institute for Creative Technologies
University of Southern California
Los Angeles, CA 90094, USA
{morbini,forbell,sagae}@ict.usc.edu
Abstract
Although data-driven techniques are com-
monly used for Natural Language Under-
standing in dialogue systems, their effi-
cacy is often hampered by the lack of ap-
propriate annotated training data in suffi-
cient amounts. We present an approach
for rapid and cost-effective annotation of
training data for classification-based lan-
guage understanding in conversational di-
alogue systems. Experiments using a web-
accessible conversational character that in-
teracts with a varied user population show
that a dramatic improvement in natural
language understanding and a substantial
reduction in expert annotation effort can
be achieved by leveraging non-expert an-
notation.
1 Introduction
Robust Natural Language Understanding (NLU)
remains a challenge in conversational dialogue
systems that allow arbitrary natural language input
from users. Although data-driven approaches are
now commonly used to address the NLU problem
as one of classification, e.g. (Heintze et al., 2010;
Leuski and Traum, 2010; Moreira et al., 2011),
where input utterances are mapped automatically
into system-specific categories, the dependence of
such approaches on training data annotated with
semantic classes or dialogue acts creates a chicken
and egg problem: user utterances are needed to
create the annotated training data necessary for
NLU by classification, but these cannot be col-
lected without a working system that users can in-
teract with.
Common solutions to this problem include the
use of Wizard-of-Oz data collection, where a hu-
man expert manually provides the functionality of
data-driven modules while data is collected from
users, or the use of scenario authors who attempt
to anticipate user input to create an initial set of
training data. While these options offer practical
ways around the training data acquisition prob-
lem, they typically require substantial work from
system experts and provide suboptimal solutions:
data-driven approaches work best when utterances
in the training data are drawn from the same distri-
bution as those encountered in actual system use,
but the conditions under which training data is col-
lected (a human expert filling in for systems mod-
ules, or a human expert generating possible user
utterances) are quite different from those where
users interact with the final system. High qual-
ity results are often obtained through an iterative
process where an initial training set is authored
by a scenario designer, but NLU resources are
gradually updated based on real user data over
time (Gandhe et al., 2011). Although this can ulti-
mately produce training data composed primarily
of real user utterances, and therefore result in bet-
ter performance from data-driven models, an ex-
pert annotator is required to perform manual clas-
sification of user utterances. This is a laborious
process that assumes availability and willingness
of the annotator for as long as it takes to collect
enough user utterances, which may range from
weeks to months or even years, depending on the
size of the domain and the number and type of ut-
terance categories.
The main question we address is whether an-
notation by non-experts can be leveraged to speed
up utterance classification and lower its cost. We
present a technique that frames the annotation of
training data as a human intelligence task suit-
able for crowdsourcing. Although there are sim-
ilarities between our technique and active learning
(e.g. see (Gambck et al., 2011)), an important dif-
ference is that our technique does not reduce the
annotation effort by reducing the size of the data
to be labeled, but by casting the annotation task
into a simpler problem. This allows us to take ad-
vantage of the entire data generated by the users.
Through an experiment with a conversational dia-
69
logue system deployed on the web, we show that a
dramatic improvement in the quality of NLU can
be achieved with non-expert data annotation, re-
ducing the time required of an expert annotator by
70%.
2 Improving understanding with data
Our approach for creating accurate utterance clas-
sifiers for NLU in conversational dialogue systems
is based on a simple strategy, which we describe
next in general terms. NLU is assumed to be per-
formed through multiclass classification.
The first step is to create a small initial train-
ing dataset T
0
either through Wizard-of-Oz data
collection or by generation of utterances by a sys-
tem developer or content author. This training set
is used to train a NLU model M
0
. Although this
model is likely to be inadequate, it allows users
to interact with an initial version of the system.
As input utterances are collected from real users,
these utterances are annotated with their desired
NLU output labels. Periodically, at time i, we add
to the initial training dataset T
0
the annotated user
utterances accumulated up to that point. We train
a new NLU model M
i
using this augmented train-
ing set, T
i
.
1
We also keep aside a small fraction
of utterances to test the performance of the NLU
models, that is, at each time i we also have an eval-
uation set E
i
and the union of E
i
and T
i
is the en-
tire set of user utterances collected up to time i. As
more utterances are added and annotated, an NLU
model M
i
is expected to surpass the initial model
M
0
. In general, we replace the running NLU
model M
r
whenever we have a better perform-
ing M
i
model. This straightforward process can
be used to obtain increasingly more accurate lan-
guage understanding, at the cost of data annotation
in the form of labelling utterances with categories
that are defined according to the needs of the spe-
cific system and the specific domain. The cate-
gories may be based on dialogue acts, e.g. (Core
and Allen, 1997; Bunt et al., 2010), user informa-
tion needs, e.g. (Moreira et al., 2011), or stand
in for entire semantic frames, e.g. (DeVault and
Traum, 2013). The technical nature of the task of
categorizing utterances in schemes such as these
usually means that substantial time is required of
an expert annotator.
2.1 Annotation as a human intelligence task
Although the task of annotating NLU training data
involves assigning categories with technical defi-
1
For every time i and j with i < j it holds that T
i
? T
j
.
nitions to utterances, and therefore would appear
to require knowledge of these technical defini-
tions, in fact the task requires primarily the type
of language understanding that is common to all
native speakers of a given language. Our main hy-
pothesis is that this annotation can be structured
as a trivial task that requires no specific exper-
tise, and that annotations performed this way can
have a substantial impact on the quality of utter-
ance classification. We define the NLU annotation
task as follows.
Before annotation begins, each utterance cate-
gory in the system is associated with one or more
canonical utterance(s) that capture the meaning
and communicative intent of that category. These
canonical utterances allow for trivial generation
of surface text from NLU categories by simple
lookup.
Given a set of dialogues between the system
and individual users, each dialogue is split into
segments. Segments are typically composed of
a user utterance and the preceding system utter-
ance. In cases where users produce multiple utter-
ances without intervening system utterances, seg-
ments may consist of a system utterance followed
by consecutive user utterances, so that there is ex-
actly one segment corresponding to each user ut-
terance u in the dialogue and each segment starts
with a system utterance and ends with u.
Annotators work on one dialogue at a time, and
within a dialogue they are presented with one seg-
ment at a time, following the order in which the
utterances appear in the dialogue. Assignment
of a system category to the utterance targeted in
a segment is a multiple choice task, involving n
choices, one of which is none of the above. The re-
maining n?1 choices are generated automatically
by running the current NLU utterance classifier on
the target utterance, and generating the canonical
surface realizations of each of the n ? 1 highest
scoring categories. The annotator is then asked to
choose the option containing language that means
the same as the targeted utterance in the segment.
The choice of n depends on the quality of the cur-
rent NLU model, and must be made to balance the
length of the list of options with the expected num-
ber of times the appropriate choice is not in the list
of options. The more none of the above annota-
tions are performed, the less effective the annota-
tion exercise will be in reducing the effort required
by an annotation expert.
The utterances annotated as none of the above
are given to an expert for annotation. The impact
70
System Hello and welcome.
...
User Are you married?
System Yeah, forty-four years this September. I can?t be-
lieve Linda?s stuck with me this long.
System Okay, let?s get to it. Why are you here today?
User I?ve been having trouble sleeping lately.
System This must be hard for you.
System Are you feeling stressed at all?
User Yes, quite a bit
System So, I?ve got some questions I need to ask you, to get
to know you better.
System Just double checking, you haven?t been sleeping
good. Is that right?
User That?s right
System Have you been having any nightmares?
User Every once in a while, but mostly I can?t stay asleep
...
Figure 1: Excerpt of a sample interaction with a
Simcoach character over the web. User input is
typed into a text box on a web page.
of this procedure on how quickly user utterances
are annotated as training examples for the NLU
and how much expert time is saved depends on
the quality of the initial classification model and
the choice of n. These are practical factors that are
expected to vary greatly from one dialogue system
to another. In the next section, we present one ap-
plication of this procedure to an existing conversa-
tional dialogue system deployed on the web, and
show examples of dialogue segments and annota-
tion options.
3 Experiment
To test our hypothesis that language understand-
ing can be improved with much reduced expert ef-
fort, we applied the framework described above to
a system that implements a conversational char-
acter that talks with users about issues relating
to mental and behavioral disorders and presents
health care options. The system is publicly ac-
cessible at http://www.simcoach.org, and receives
traffic on the order of one hundred users per week.
Of these, about one quarter engage the system in
a meaningful dialogue with multiple turns, with
the dialogues containing on average 16 user utter-
ances. Because our process depends crucially on
user traffic to generate data for annotation, a web-
accessible system is ideally suited for it. An ex-
cerpt from a typical interaction with the system is
shown in Figure 1. The system and the NLU clas-
sifier based on Maximum Entropy models (Berger
et al., 1996) are described respectively in (Rizzo et
al., 2011) and (Sagae et al., 2009).
3.1 Data collection
Starting with an initial system deployed with an
NLU model trained with data generated by an au-
thor attempting to anticipate user behavior, we ap-
plied the approach described in section 2 to im-
prove NLU accuracy over a period of approxi-
mately five months. The initial accuracy of the
NLU classifier was 62%, measured as the number
of utterances classified correctly divided by the to-
tal number of user utterances. This accuracy fig-
ure was obtained only after the five months of data
annotation, using the heldout set of manually an-
notated dialogues.
Although the data annotation procedure as de-
scribed in section 2 could in principle be per-
formed continuously as user data come in, we
instead performed all of our annotation in three
rounds, the first consisting of approximately 2,000
user utterances, the second one month later, con-
sisting of an additional 1,000 utterances. The last
round, collected about two months later, contained
about 2,000 utterances. We used five annotators
2
working in parallel, and the average speed of each
annotator exceeded 500 utterances per hour.
The total number of NLU utterance classes in
the system is 378, although only 120 classes were
used by annotators in all rounds of annotation to
cover all of the utterances collected
3
. In our an-
notation exercise we set the number of multiple
choice items at n = 6, including 5 choices gener-
ated from categories chosen by the NLU classifier,
and one none of the above choice. Figure 2 shows
a sample dialogue segment with the corresponding
multiple choice items. During annotation, clicking
on a multiple choice item advances the annotation
by presenting the next segment containing a user
utterance to be annotated.
3.2 Results
Of the utterances in the three rounds of data col-
lection, respectively 29%, 34% and 17% were
marked by annotators as none of the above. These
were given to a developer of the NLU system who
assigned a category to each of them. In this ex-
pert annotation step the choice is not restricted to
a small set of options, and may be any of the cat-
egories in the system. Given this rate of use of
2
The non-expert annotators belonged to the same team
that developed the system but did not participate in the de-
velopment of the NLU module and the NLU classes used in
the particular dialogue system used.
3
This difference is a further evidence of the difficulty of
correctly anticipating how the end users will interact with the
dialogue system.
71
System Okay, let?s get to it. Why are you here today?
User I?ve been having trouble sleeping lately.
Which of the following options correspond most
closely to the last user utterance? If none of them have
the same general meaning as the user utterance, select
?none of the above.?
(a) I have been in a bad mood lately
(b) I have nightmares often
(c) I haven?t been sleeping well
(d) My family is worried about me
(e) I eat too much
(f) None of the above
Figure 2: Example of a dialogue segment with cor-
responding multiple choice items. The annotation
task consists of choosing the item that has approx-
imately the same meaning and communicative in-
tent as the targeted utterance (the user utterance).
the none of the above category, the need for ex-
pert annotation is not eliminated, but the amount
of expert effort necessary is reduced by over 70%.
The NLU classification accuracy figures ob-
tained after each round of annotation are shown in
Table 1. In the table, Our Approach represents the
results obtained by the technique described here.
A large improvement is observed after the first
round of annotation, with a more modest improve-
ment observed after the other two rounds. The ini-
tial jump in accuracy after round 1 is explained
by the fact that the initial model based on a sys-
tem author?s expectation of what users may say to
the system (approximately 3,000 utterances) is im-
proved using utterances that users did in fact pro-
duce in real interactions with the system. Clearly,
a more well-matched distribution of utterances in
the training data produces higher accuracy.
To assess the value of our approach, we com-
pare it with two other reasonable experimental
conditions: a baseline where only expert annota-
tion is used (Expert Only), and a condition where
no expert annotation is used (No Expert). The Ex-
pert Only condition is meant to represent what can
be achieved with the same workload for the expert
used in Our Approach. This is achieved by random
selection of user utterances to create a set with
the same number of utterances set aside for ex-
pert annotation in Our Approach. The expert then
annotates each of these utterances to create train-
ing data. For the No Expert condition, we used
only utterances annotated by non-experts, leaving
out completely utterances labeled as none of the
NLU accuracy after
each annotation round [%]
Base 1st 2nd 3rd
round round round
Our Approach 62 70 73 78
Expert Only 62 64 68 70
No Expert 62 64 65 71
Table 1: NLU accuracy obtained using the initial
training dataset T
0
, after one round of annotation
with T
1
(2,013 utterances), after two rounds of an-
notation with T
2
(additional 948 utterances), and
after three rounds with T
3
(additional 1806 utter-
ances). Accuracy is estimated on the same heldout
set of dialogues E
3
for all conditions, accounting
for roughly 10% of the annotated data.
above. Both Expert Only and No Expert condi-
tions achieve significantly lower performance than
the approach described here. This indicates that
expert annotation is important, but also that cheap
and fast non-expert annotation can provide sub-
stantial improvements to NLU.
4 Conclusion
We described a framework for annotation of train-
ing data by non-experts that can provide dramatic
improvements to natural language understanding
in dialogue systems that perform NLU through ut-
terance classification. Our approach transforms
the annotation NLU training data into a task that
can be performed by anyone with language profi-
ciency. Annotation is structured as a simple mul-
tiple choice task, easily delivered over the web.
Using our approach with a conversational char-
acter on the web, we improved NLU accuracy
from 62% to 78% using only less than 30% of the
effort it would be required of an expert to annotate
data without non-expert annotation.
Acknowledgments
We thank Kelly Christoffersen, Nicolai Kalisch
and Tomer Mor-Barak for data annotation and up-
dates to the SimCoach system, David Traum for
insightful discussions, and the anonymous review-
ers. The effort described here has been sponsored
by the U.S. Army. Any opinions, content or infor-
mation presented does not necessarily reflect the
position or the policy of the United States Govern-
ment, and no official endorsement should be in-
ferred.
72
References
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):39?71, March.
Harry Bunt, Jan Alexandersson, Jean Carletta, Jae-
Woong Choe, Alex Chengyu Fang, Koiti Hasida,
Kiyong Lee, Volha Petukhova, Andrei Popescu-
Belis, Laurent Romary, Claudia Soria, and David
Traum. 2010. Towards an iso standard for dia-
logue act annotation. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Bente Maegaard,
Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike
Rosner, and Daniel Tapias, editors, Proceedings
of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Val-
letta, Malta, may. European Language Resources
Association (ELRA).
Mark G. Core and James F. Allen. 1997. Coding di-
alogues with the DAMSL annotation scheme. In
David Traum, editor, Working Notes: AAAI Fall
Symposium on Communicative Action in Humans
and Machines, pages 28?35, Menlo Park, Califor-
nia. AAAI, American Association for Artificial In-
telligence.
David DeVault and David Traum. 2013. A method
for the approximation of incremental understanding
of explicit utterance meaning using predictive mod-
els in nite domains. In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, GA, June.
Bjrn Gambck, Fredrik Olsson, and Oscar Tckstrm.
2011. Active learning for dialogue act classification.
In INTERSPEECH, pages 1329?1332. ISCA.
Sudeep Gandhe, Michael Rushforth, Priti Aggar-
wal, and David Traum. 2011. Evaluation of
an integrated authoring tool for building advanced
question-answering characters. In 12th Annual Con-
ference of the International Speech Communication
Association (InterSpeech 2011), Florence, Italy, Au-
gust.
Silvan Heintze, Timo Baumann, and David Schlangen.
2010. Comparing local and sequential models
for statistical incremental natural language under-
standing. In Raquel Fern?andez, Yasuhiro Kata-
giri, Kazunori Komatani, Oliver Lemon, and Mikio
Nakano, editors, SIGDIAL Conference, pages 9?16.
The Association for Computer Linguistics.
Anton Leuski and David R. Traum. 2010. Practical
language processing for virtual humans. In Twenty-
Second Annual Conference on Innovative Applica-
tions of Artificial Intelligence (IAAI-10).
Catarina Moreira, Ana Cristina Mendes, Lu??sa Coheur,
and Bruno Martins. 2011. Towards the rapid devel-
opment of a natural language understanding mod-
ule. In Proceedings of the 10th International Con-
ference on Intelligent Virtual Agents, IVA?11, pages
309?315, Berlin, Heidelberg. Springer-Verlag.
Albert A. Rizzo, Belinda Lange, John G. Buckwalter,
E. Forbell, Julia Kim, Kenji Sagae, Josh Williams,
Barbara O. Rothbaum, JoAnn Difede, Greg Reger,
Thomas Parsons, and Patrick Kenny. 2011. An in-
telligent virtual human system for providing health-
care information and support. In Studies in Health
Technology and Informatics.
Kenji Sagae, Gwen Christian, David DeVault, and
David R. Traum. 2009. Towards natural language
understanding of partial speech recognition results
in dialogue systems. In Short Paper Proceedings of
the North American Chapter of the Association for
Computational Linguistics - Human Language Tech-
nologies (NAACL HLT) 2009 conference.
73
Proceedings of the SIGDIAL 2014 Conference, pages 254?256,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
A Demonstration of Dialogue Processing in SimSensei Kiosk
Fabrizio Morbini, David DeVault, Kallirroi Georgila,
Ron Artstein, David Traum, Louis-Philippe Morency
USC Institute for Creative Technologies
12015 Waterfront Dr., Playa Vista, CA 90094
{morbini,devault,kgeorgila,artstein,traum,morency}@ict.usc.edu
Abstract
This demonstration highlights the dia-
logue processing in SimSensei Kiosk, a
virtual human dialogue system that con-
ducts interviews related to psychologi-
cal distress conditions such as depression,
anxiety, and post-traumatic stress disorder
(PTSD). The dialogue processing in Sim-
Sensei Kiosk allows the system to con-
duct coherent spoken interviews of human
users that are 15-25 minutes in length,
and in which users feel comfortable talk-
ing and openly sharing information. We
present the design of the individual dia-
logue components, and show examples of
natural conversation flow between the sys-
tem and users, including expressions of
empathy, follow-up responses and contin-
uation prompts, and turn-taking.
1 Introduction
This demonstration highlights the dialogue pro-
cessing in SimSensei Kiosk, a virtual human di-
alogue system that conducts interviews related to
psychological distress conditions such as depres-
sion, anxiety, and post-traumatic stress disorder
(PTSD) (DeVault et al., 2014). SimSensei Kiosk
has two main functions ? a virtual human called
Ellie (pictured in Figure 1), who converses with a
user in a spoken, semi-structured interview, and a
multimodal perception system which analyzes the
user?s behavior in real time to identify indicators
of psychological distress.
The system has been designed and devel-
oped over two years using a series of face-to-
face, Wizard-of-Oz, and automated system stud-
ies involving more than 350 human participants
(Scherer et al., 2013; DeVault et al., 2013; DeVault
et al., 2014). Agent design has been guided by
two overarching goals: (1) the agent should make
Figure 1: Ellie, the virtual human interviewer in
SimSensei Kiosk.
the user feel comfortable talking and openly shar-
ing information, and at the same time (2) the agent
should create interactional situations that support
the automatic assessment of verbal and nonver-
bal behaviors correlated with psychological dis-
tress. During an interview, the agent presents a
set of questions which have been shown in user
testing to support these goals. Since the main in-
terview questions and their order are mostly fixed,
dialogue management concentrates on providing
appropriate verbal feedback behaviors to keep the
user engaged, maintain a natural and comfort-
able conversation flow, and elicit continuations
and elaborations from the user.
The agent is implemented using a modular ar-
chitecture (Hartholt et al., 2013). Dialogue pro-
cessing, which is the focus of this demonstration,
is supported by individual modules for speech
recognition, language understanding and dialogue
management (see Section 2). The agent?s lan-
guage and speech are executed by selecting from
pre-recorded audio clips. Additional agent mod-
ules include nonverbal behavior generation, which
matches appropriately timed body movements to
the agent?s speech; character animation in a vir-
tual 3D environment; and rendering in a game en-
254
gine. The perception system analyzes audio and
video in real time to identify features such as head
position, gaze direction, smile intensity, and voice
quality. DeVault et al. (2014) provides details on
all the agent?s modules.
2 Overview of Dialogue Processing
2.1 ASR and NLU components
Unlike many task-oriented dialogue domains, in-
terview dialogues between SimSensei Kiosk and
participants are naturally open-ended. People tend
to respond to interview stimuli such as ?what?s
one of your most memorable experiences?? with
idiosyncratic stories and events from their lives.
The variability in the vocabulary and content of
participants? answers to such questions is so large
that it makes the ASR task very challenging. Fur-
thermore, continuous ASR is employed to ensure
that participants feel comfortable interacting with
the system without being distracted by having to
use a push-to-talk microphone. The use of con-
tinuous ASR necessitates the development of spe-
cific policies for turn-taking (see Section 2.2). In
this demonstration, voice activity detection and
speech recognition are performed using Pocket-
Sphinx (Huggins-Daines et al., 2006).
Because of the open-ended participants? re-
sponses, for NLU, we cannot simply construct a
small semantic ontology and expect to cover the
majority of meanings that will be expressed by
users. Thus, this is an application in which the
dialogue policy needs to be able to create a sense
of engagement, continuation, and empathy despite
relatively shallow and limited understanding of
user speech. SimSensei Kiosk currently uses 4
statistically trained utterance classifiers to capture
different aspects of user utterance meaning.
The first NLU classifier identifies generic di-
alogue act types, including statements, yes-no
questions, wh-questions, yes and no answers, and
several others. This classifier is trained using
the Switchboard DAMSL corpus (Jurafsky et al.,
1997) using a maximum entropy model.
The second NLU classifier assigns positive,
negative, or neutral valence to utterances, in or-
der to guide Ellie?s expression of empathy. We
use SentiWordNet 3.0 (Baccianella et al., 2010), a
lexical sentiment dictionary, to assign valence to
individual words spoken by users (as recognized
by the ASR); the valence assigned to an utterance
is based primarily on the mean valence scores of
Opening Rapport Building Phase
Ellie What are some things you really like about LA?
(top level question)
User I love the weather, I love the palm trees, I love the
beaches, there?s a lot to do here.
Diagnostic Phase
Ellie Have you noticed any changes in your behavior or
thoughts lately? (top level question)
User Yes.
Ellie Can you tell me about that? (continuation prompt)
User I?m having a lot more nightmares now uh can?t
sleep have haven?t really been eating uh trying to
eat. . . I have to force down food um just feeling
like an emotional wreck.
Ellie I?m sorry to hear that. (empathy response)
Ellie What are you like when you don?t sleep well?
(follow-up question)
User Irritable, emotional, it just adds to my overall
stress um [long pause]
Ellie What. . . (Ellie speaks after the participant?s long
pause)
User Can?t concentrate uh I uh. . . (the participant starts
speaking while Ellie is speaking)
Ellie I?m sorry please continue. (Ellie realizes that she
has interrupted the participant and apologizes)
Positive Closing Phase
Ellie How would your best friend describe you? (top
level question)
User As caring, as fun because most of the time when
I?m around my best friends I?m happy and I?m fun
loving. I joke around with them a lot and uh I do
better when I?m around my friends. . .
Figure 2: Examples of Ellie?s interview phases.
the individual words in the utterance.
The third NLU classifier supports domain-
specific small talk by recognizing a handful of
specific anticipated responses to Ellie?s rapport-
building questions. For example, when Ellie asks
users where they are from, this classifier detects
the names of commonly mentioned cities and re-
gions using keyphrase spotting.
The fourth NLU classifier identifies domain-
specific dialogue acts, and supports Ellie?s follow-
up responses to specific questions, such as ?how
close are you to your family??. This maximum
entropy classifier is trained using face-to-face and
Wizard-of-Oz data to detect specific responses
such as assertions of closeness.
2.2 Dialogue Management
Ellie currently uses about 100 fixed utterances in
total in the automated system. She employs 60 top
level interview questions (e.g., ?do you travel a
255
lot??), plus some follow-up questions (e.g., ?what
do you enjoy about traveling??) and a range of
backchannels (e.g., ?uh huh?), empathy responses
(e.g., ?that?s great?, ?I?m sorry?), and continua-
tion prompts (e.g., ?tell me more about that?).
The dialogue policy is implemented using the
FLoReS dialogue manager (Morbini et al., 2012).
The policy groups interview questions into three
phases (opening rapport building, diagnostic, pos-
itive closing ? ensuring that the participant leaves
with positive feelings). Questions are generally
asked in a fixed order, with some branching based
on responses to specific questions.
Rule-based subpolicies determine what Ellie?s
follow-up responses will be for each of her top-
level interview questions. The rules for follow-ups
are defined in relation to the four NLU classifiers
and the duration of user speech (measured in sec-
onds). These rules trigger continuation prompts
and empathy responses under specific conditions.
The turn-taking policy supports our design goal
to encourage users to openly share information
and to speak at length in response to Ellie?s open-
ended questions. In this domain, users often pause
before or during their responses to think about
their answers to Ellie?s personal questions. The
turn-taking policy is designed to provide ample
time for users to consider their responses, and to
let users take and keep the initiative as much as
possible. Ellie?s turn-taking decisions are based
on thresholds for user pause duration, i.e., how
much time the system should wait after the user
has stopped speaking before Ellie starts speaking.
These thresholds are tuned to the face-to-face and
Wizard-of-Oz data to minimize Ellie?s interrup-
tion rate, and are extended dynamically when El-
lie detects that she has interrupted the participant.
This is to take into account the fact that some peo-
ple tend to use longer pauses than others.
Examples of the three interview phases and of
Ellie?s subdialogue policies (top level and follow-
up questions, continuation prompts, empathy re-
sponses, and turn-taking) are given in Figure 2.
3 Demonstration Summary
The demonstration will feature a live interac-
tion between Ellie and a participant, showing El-
lie?s real-time understanding and consequent pol-
icy actions. Live dialogues will highlight Ellie?s
strategies for questioning, follow-up continuation
prompts, displays of empathy, and turn-taking,
similar to the example in Figure 2. The demon-
stration will illustrate how these elements work to-
gether to enable Ellie to carry out extended inter-
views that also provide information relevant to the
automatic assessment of indicators of distress.
Acknowledgments
The effort described here is supported by DARPA
under contract W911NF-04-D-0005 and the U.S.
Army. Any opinion, content or information pre-
sented does not necessarily reflect the position or
the policy of the United States Government, and
no official endorsement should be inferred.
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiWordNet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Proceed-
ings of LREC.
D. DeVault, K. Georgila, R. Artstein, F. Morbini, D.
Traum, S. Scherer, A. Rizzo, and L.-P. Morency.
2013. Verbal indicators of psychological distress in
interactive dialogue with a virtual human. In Pro-
ceedings of SIGDIAL.
D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast,
A. Gainer, K. Georgila, J. Gratch, A. Hartholt, M.
Lhommet, G. Lucas, S. Marsella, F. Morbini, A.
Nazarian, S. Scherer, G. Stratou, A. Suri, D. Traum,
R. Wood, Y. Xu, A. Rizzo, and L.-P. Morency. 2014.
SimSensei Kiosk: A virtual human interviewer for
healthcare decision support. In Proceedings of AA-
MAS.
A. Hartholt, D. Traum, S. Marsella, A. Shapiro, G.
Stratou, A. Leuski, L.-P. Morency, and J. Gratch.
2013. All together now, introducing the virtual hu-
man toolkit. In Proceedings of IVA.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
Sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Proceedings
of ICASSP.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL Shallow-Discourse-Function
Annotation Coders Manual, Draft 13.
F. Morbini, D. DeVault, K. Sagae, J. Gerten, A. Nazar-
ian, and D. Traum. 2012. FLoReS: A forward look-
ing reward seeking dialogue manager. In Proceed-
ings of IWSDS.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In Proceedings of IEEE Conference
on Automatic Face and Gesture Recognition.
256
