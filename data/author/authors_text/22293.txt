Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 289?299,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Coreference Resolution and Named-Entity Linking
with Multi-pass Sieves
Hannaneh Hajishirzi Leila Zilles Daniel S. Weld Luke Zettlemoyer
Department of Computer Science and Electrical Engineering
University of Washington
{hannaneh,lzilles,lsz,weld}@cs.washington.edu
Abstract
Many errors in coreference resolution come
from semantic mismatches due to inadequate
world knowledge. Errors in named-entity
linking (NEL), on the other hand, are of-
ten caused by superficial modeling of entity
context. This paper demonstrates that these
two tasks are complementary. We introduce
NECO, a new model for named entity linking
and coreference resolution, which solves both
problems jointly, reducing the errors made on
each. NECO extends the Stanford determinis-
tic coreference system by automatically link-
ing mentions to Wikipedia and introducing
new NEL-informed mention-merging sieves.
Linking improves mention-detection and en-
ables new semantic attributes to be incorpo-
rated from Freebase, while coreference pro-
vides better context modeling by propagat-
ing named-entity links within mention clus-
ters. Experiments show consistent improve-
ments across a number of datasets and ex-
perimental conditions, including over 11% re-
duction in MUC coreference error and nearly
21% reduction in F1 NEL error on ACE 2004
newswire data.
1 Introduction
Coreference resolution and named-entity linking are
closely related problems, but have been largely stud-
ied in isolation. This paper demonstrates that they
are complementary by introducing a simple joint
model that improves performance on both tasks.
Coreference resolution is the task of determining
when two textual mentions name the same individ-
[Michael Eisner]1 and [Donald Tsang]2 announced the
grand opening of [[Hong Kong]3 Disneyland]4 yester-
day. [Eisner]1 thanked [the President]2 and welcomed
[fans]5 to [the park]4.
Figure 1: A text passage illustrating interactions between
coreference resolution and NEL.
ual. The biggest challenge in coreference resolu-
tion ? accounting for 42% of errors in the state-
of-the-art Stanford system ? is the inability to rea-
son effectively about background semantic knowl-
edge (Lee et al, 2013). For example, consider the
sentence in Figure 1. ?President? refers to ?Donald
Tsang? and ?the park? refers to ?Hong Kong Dis-
neyland,? but automated algorithms typically lack
the background knowledge to draw such inferences.
Incorporating knowledge is challenging, and many
efforts to do so have actually hurt performance,
e.g. (Lee et al, 2011; Durrett and Klein, 2013).
Named-entity linking (NEL) is the task of match-
ing textual mentions to corresponding entities in a
knowledge base, such as Wikipedia or Freebase.
Such links provide rich sources of semantic knowl-
edge about entity attributes ? Freebase includes
president as Tsang?s title and Disneyland as hav-
ing the attribute park. But NEL is itself a chal-
lenging problem, and finding the correct link re-
quires disambiguating based on the mention string
and often non-local contextual features. For exam-
ple, ?Michael Eisner? is relatively unambiguous but
the isolated mention ?Eisner? is more challenging.
However, these mentions could be clustered with
a coreference model, allowing for improved NEL
through link propagation from the easier mentions.
289
We present NECO, a new algorithm for jointly solv-
ing named entity linking and coreference resolu-
tion. Our work is related to that of Ratinov and
Roth (2012), which also uses knowledge derived
from an NEL system to improve coreference. How-
ever, NECO is the first joint model we know of, is
purely deterministic with no learning phase, does
automatic mention detection, and improves perfor-
mance on both tasks.
NECO extends the Stanford?s sieve-based model,
in which a high recall mention detection phase is
followed by a sequence of cluster merging opera-
tions ordered by decreasing precision (Raghunathan
et al, 2010; Lee et al, 2013). At each step, it
merges two clusters only if all available information
about their respective entities is consistent. We use
NEL to increase recall during the mention detection
phase and introduce two new cluster-merging sieves,
which compare the Freebase attributes of entities.
NECO also improves NEL by initially favoring high
precision linking results and then propagating links
and attributes as clusters are formed.
In summary we make the following contributions:
? We introduce NECO, a novel, joint approach
to solving coreference and NEL, demonstrating
that these tasks are complementary by achiev-
ing joint error reduction.
? We present experiments showing improved per-
formance at coreference resolution, given both
gold and automatic mention detection: e.g.,
6.2 point improvement in MUC recall on ACE
2004 newswire text and 3.1 point improvement
in MUC precision the CoNLL 2011 test set.
? NECO also leads to better performance at
named-entity linking, given both gold and au-
tomatic linking, improving F1 from 61.7% to
69.2% on a newly labeled test set.1
2 Background
We make use of existing models for coreference res-
olution and named entity linking.
1Our corpus and the source code for NECO can be down-
loaded from https://www.cs.washington.edu/
research-projects/nlp/neco.
2.1 Coreference Resolution
Coreference resolution is the the task of identifying
all text spans (called mentions) that refer to the same
entity, forming mention clusters.
Stanford?s SieveModel is a state-of-the-art coref-
erence resolver comprising a pipeline of ?sieves?
that merge coreferent mentions according to deter-
ministic rules. Mentions are automatically predicted
by selecting all noun phrases (NP), pronouns, and
named entities. Each sieve either merges a cluster
to its single best antecedent from a list of previous
clusters, or declines to merge.
Higher precision sieves are applied earlier in the
pipeline according to the following order, looking at
different aspects of the text, including: (1) speaker
identification, (2-3) exact and relaxed string matches
between mentions, (4) precise constructs, including
appositives, acronyms and demonyms, (5-9) differ-
ent notions of strict and relaxed head matches be-
tween mentions, and finally (10) a number of syn-
tactic and distance cues for pronoun resolution.
2.2 Named Entity Linking
Named-entity linking (NEL) is the task of identi-
fying mentions in a text and linking them to the
entity they name in a knowledge base, usually
Wikipedia. NECO uses two existing NEL sys-
tems: GLOW (Ratinov et al, 2011) and Wikipedi-
aMiner (Milne and Witten, 2008).
WikipediaMiner links mentions based on a notion
of semantic similarity to Wikipedia pages, consider-
ing all substrings up to a fixed length. Since there
are often many possible links, it disambiguates by
choosing the entity whose Wikipedia page is most
semantically related to the nearby context of the
mention. The semantic scoring function includes n-
gram statistics and also counts shared links to other
unambiguous mentions in the text.
GLOW finds mentions by selecting all the NPs
and named entities in the text. Linking is framed
as an integer linear programming optimization prob-
lem that takes into account using similar local con-
straints but also includes global constraints such as
entity link co-occurrence.
Both systems return confidence values. To main-
tain high precision, NECO uses an ensemble of
290
? Let Exemplar(c) be a representative mention of the cluster c, computed as defined below
? Let cj be an antecedent cluster of ci if cj has a mention which is before the first mention of ci
? Let l(m) be a Wikipedia page linked to mention m or ? if there is no link
? Let l(c) be a Wikipedia page linked to mention Exemplar(c) or ? if there is no link
1. Initialize Linked Mentions:
(a) Let MNEL = {mi | i = 1 . . . p} be the NEL output mentions, mi, each with a link l(mi)
(b) Let MCR = {mi | i = 1 . . . q} be the mentions mi from coreference mention detection
(c) Let M ?MCR ?MNEL (Sec. 3.1)
(d) Update entity links for all m ?M and prune M (Sec. 3.2)
(e) Extract attributes from Wikipedia and Freebase for all m ?M (Sec. 3.3)
(f) Let C ?M be singleton mention clusters where Exemplar(ci) = mi, l(ci) = l(mi)
2. Merge Clusters: For every sieve S (including NEL sieves, Sec. 3.6) and cluster ci ? C
(a) For every cluster cj , j = [i? 1 . . . 1] (traverse the preceding clusters in reverse order)
i. NEL constraints: Prevent merge if l(ci) 6= l(cj) (Sec. 3.4)
ii. If all rules of sieve S are satisfied for clusters ci and cj
A. ck ? Merge(ci, cj), including entity link and attribute updates (Sec. 3.5)
B. C ? C ? {ck} \ {ci, cj}
3. Output: Coreference clusters C and linked Wikipedia pages l(ci)?ci ? C
Figure 2: NECO: A joint algorithm for named-entity linking and coreference resolution.
GLOW and WikipediaMiner, selecting only high
confidence links.
3 Joint Coreference and Linking
We introduce a joint model for coreference resolu-
tion and NEL. Building on the Stanford sieve ar-
chitecture, our algorithm incrementally constructs
clusters of mentions using deterministic coreference
rules under NEL constraints.
Figure 2 presents the complete algorithm. The in-
put to NECO is a document and the output is a set C
of coreference clusters, with links l(c) to Wikipedia
pages for a subset of the clusters c ? C. Step 1
detects mentions, merging the outputs of the base
systems (Sec. 3.1). Step 2 repeatedly merges coref-
erence clusters, while ensuring that NEL constraints
(Sec. 3.4) are satisfied. It uses the original Stan-
ford sieves and also two new NEL-informed sieves
(Sec. 3.6). NEL links are propagated to new clusters
as they are formed (Sec. 3.5).
3.1 Mention Detection
In Steps 1(a-c) in Fig. 2, NECO combines mentions
from the base coreference and NEL systems.
Let MCR be the set of mentions returned by us-
ing Stanford?s rule-based mention detection algo-
rithm (Lee et al, 2013). Let MNEL be the set of
mentions output by the two NEL systems. NECO
creates an initial set of mentions, M , by taking the
union of all the mentions in MNEL and MCR. In
practice, taking the union increases diversity in the
mention pool. For example, it is often the case that
MNEL will include sub-phrases such as ?Suharto?
when they are part of a larger mention ?ex-dictator
Suharto? that is detected in MCR.
3.2 Mention Entity Links and Pruning
Step 1(d) in Fig. 2 assigns Wikipedia links to a sub-
set of the detected mentions.
For mentions m output by the base NEL sys-
tems, we assign an exact link l(m) if the entire
mention span is linked. Mentions m? that differ
from an exact linked mention m by only a pre- or
post-fix stop word are similarly assigned exact links
l(m?) = l(m). For example, the mention ?the pres-
ident? will be assigned the same link as ?president?
but ?The governor of Alaska Sarah Palin? would not
be assigned an exact link to Sarah Palin.
For mentions m? that do not receive an exact link,
we assign a head link h(m?) if the head word2 m has
been linked, by setting h(m?) = l(m). For instance,
the head link for the mention ?President Clinton?
(with ?Clinton? as head word) will be the Wikipedia
title of Bill Clinton. We use head links for the
Relaxed NEL sieve (Sec. 3.6).
Next, we define L(m) to be the set con-
2A head word is assigned to every mention with the Stanford
parser head finding rules (Klein and Manning, 2003).
291
country president city area
company state region location
place agency power unit
body market park province
manager organization owner trial
site prosecutor attorney county
senator stadium network building
attraction government department person
origin plant airport kingdom
capital operation author period
nominee candidate film venue
Figure 3: The most commonly used fine-grained at-
tributes from Freebase and Wikipedia (out of over 500
total attributes).
taining l(m) and l(m?) for all sub-phrases m?
of m. We add the sub-phrase links only
if their confidence is higher than the confi-
dence for l(m). For instance, assuming ap-
propriate confidence values, L(m) would in-
clude the pages for {List of governors of
Alaska, Alaska, Sarah Palin} given the
mention ?The governor of Alaska Sarah Palin.? We
will use L(m) for NEL constraints and filtering
(Sec. 3.4).
After updating the entity links for all mentions,
NECO prunes spurious mentions that begin or
end with a stop word where the remaining sub-
expression of the mention exists in M . It also re-
moves time expressions and numbers from M if they
are not included in MNEL.
3.3 Mention Attributes
Step 1(e) in Fig. 2 also assigns attributes for a
mention m linked to Wikipedia page l(m), at both
coarse and fine-grained levels, based on information
from the Freebase entry corresponding to exact link
l(m) or head link h(m).
The coarse attributes include gender, type, and
NER classes such as PERSON, LOCATION, and OR-
GANIZATION. These attributes are part of the orig-
inal Stanford coreference system and are used to
avoid merging conflicting clusters. We use the Free-
base values for these attributes when available. For
instance, if the linked entity contains the Freebase
type location or organization, we include the coarse
type to LOCATION or ORGANIZATION respectively.
In order to account for both links to specific peo-
ple (Barack Obama) and generic links to positions
held by people (President), we include the type PER-
SON if the linked entity has any of the Freebase types
person, job title, or government office or title. If no
coarse Freebase types are available for an attribute,
we default to predicted NER classes.
We add fine-grained attributes from Freebase and
Wikipedia by importing additional type information.
We use all of the Freebase notable types, a set of
hundreds of commonly used Freebase types, rang-
ing from us president to tropical cyclone and syn-
thpop album. We also include all of the Wikipedia
categories, on average six per entity. For example,
the mention ?Indonesia? is assigned fine-grained at-
tributes such as book subject, military power, and
olympic participating country. Since many of these
fine-grained attributes are extremely specific, we use
the last word of each attribute to define an addi-
tional fine-grained attribute (see Fig. 3). These fine-
grained attributes are used in the Relaxed NEL sieve
(Sec. 3.6).
3.4 NEL Constraints
While applying sieves to merge clusters in Figure 2
Step 2(a), NECO uses NEL constraints to eliminate
some otherwise acceptable merges.
We avoid merging inconsistent clusters that link
to different entities. Clusters ci and cj are incon-
sistent if both are linked (i.e., both clusters have
non-null entity assignments) and l(ci) 6= l(cj) or
h(ci) 6= h(cj). Also, in order to consider an an-
tecedent cluster c as a merge candidate, we require a
pair of entities in the set of linked entities L(c) to be
related to one another in Freebase. Two entities are
related in Freebase if they both appear in a relation;
for example, Bill Clinton and Arkansas are
related because Bill Clinton has a ?governor-of? re-
lation with Arkansas.
3.5 Merging Clusters and Update Entity Links
When two clusters ci and cj are merged to form a
new cluster ck, the entity link information L(ck),
l(ck), and h(ck) must be updated (Step 2 of Fig. 2).
We set L(ck) to the union of the linked entities found
in l(ci) and l(cj) and merge coarse attributes at this
point.
In order to set the exact and head entity links
l(ck) and h(ck), we use the exemplar mention
292
Exemplar(ck) that denotes the most representative
mention of the cluster. Exemplar(c) is selected
according to a set of rules in the Stanford system,
based on textual position and mention type (proper
noun vs. common). We augment this function by
considering information from exact and head en-
tity links as well. Mentions appearing earlier in
text, proper mentions, and mentions that have ex-
act or head named-entity links are preferred to those
which do not. Given exemplars, we set l(ck) =
l(Exemplar(ck)) and h(ck) = h(Exemplar(ck)).
3.6 NEL Sieves
Finally, we introduce two new sieves that use NEL
information at the beginning and end of the Stan-
ford sieves pipeline in the merging stage (Step 2 of
Fig. 2).
Exact NEL sieve The Exact NEL sieve merges
two clusters ci and cj if both are linked and their
links match, l(ci) = l(cj). For example, all men-
tions that have been linked to Barack Obama will
become members of the same coreference cluster.
Because the Exact NEL sieve has high precision, we
place it at the very beginning of the pipeline.
Relaxed NEL sieve The Relaxed NEL sieve uses
fine-grained attributes of the linked mentions to
merge proper nouns with common nouns when they
share attributes. For example, this sieve is able to
merge the proper mention ?Disneyland? with the
?the mysterious park?, because park is one of the
fine-grained attributes assigned to Disneyland.
More formally, let mi = Exemplar(ci) and
mj = Exemplar(cj). For every common noun
mention mi, we merge ci with an antecedent clus-
ter cj if (1) mj is a linked proper noun, (2) if mi or
the title of its linked Wikipedia page is in the list of
fine-grained attributes of mj , or (3) if h(mj) is re-
lated to the head link h(mi) according to Freebase
as defined above.
Because this sieve has low precision, we only
allow merges between mentions that have a maxi-
mum distance of three sentences between one an-
other. We add the Relaxed NEL sieve near the end
of the pipeline, just before pronoun resolution.
4 Experimental Setup
Core Components and Baselines The Stanford
sieve-based coreference system (Lee et al, 2013),
the GLOW NEL system (Ratinov et al, 2011), and
WikipediaMiner (Milne and Witten, 2008) provide
core functionality for our joint model, and are also
the state-of-the-art baselines against which we mea-
sure performance.
Parameter Settings Based on performance on the
development set, we set the GLOW?s confidence pa-
rameter to 1.0 and WikipediaMiner?s to 0.4 to assure
high-precision NEL. We also optimized for the set of
fine-grained attributes to import from Wikipedia and
Freebase, and the best way to incorporate the NEL
constraints into the sieve architecture.
Datasets We report results on the following
three datasets: ACE????-NWIRE, CONLL????,
and ACE????-NWIRE-NEL. ACE????-NWIRE, the
newswire subset of the ACE 2004 corpus (NIST,
2004), includes 128 documents. The CONLL????
coreference dataset includes text from five different
domains: broadcast conversation (BC), broadcast
news (BN), magazine (MZ), newswire (NW), and
web data (WB) (Pradhan et al, 2011). The broadcast
conversation and broadcast news domains consist of
transcripts, whereas magazine and newswire contain
more standard written text. The development data
includes 303 documents and the test data includes
322 documents.
We created ACE????-NWIRE-NEL by taking a
subset of ACE????-NWIRE and annotating with
gold-standard entity links. We segment and link all
the expressions in text that refer to Wikipedia pages,
allowing for nested linking. For instance, both the
phrase ?Hong Kong Disneyland,? and the sub-phrase
?Hong Kong? are linked. This dataset includes 12
documents and 350 linked entities.
Metrics We evaluate our system using MUC (Vi-
lain et al, 1995), B3 (Bagga and Baldwin, 1998),
and pairwise scores. MUC is a link-based met-
ric which measures how many clusters need to be
merged to cover the gold clusters and favors larger
clusters; B3 computes the proportion of intersec-
tion between predicted and gold clusters for every
mention and favors singletons (Recasens and Hovy,
2010). We computed the scores using the Stanford
293
Method MUC B3 Pairwise
P R F1 P R F1 P R F1
Stanford Sieves 39.9 46.2 42.8 67.9 71.8 69.8 44.2 29.7 35.6
NECO 46.8 52.5 49.5 70.4 72.6 71.5 51.5 34.6 41.4
No NEL Mentions 46.1 48.3 47.2 71.4 70.0 70.9 49.7 30.9 38.1
No Mention Pruning 43.6 45.6 44.6 70.5 69.9 70.2 46.2 29.4 35.9
No Attributes 45.9 47.4 46.6 71.8 69.7 70.7 48.6 27.0 34.7
No Constraints 42.3 49.3 45.5 68.3 72.3 70.2 44.2 28.6 34.7
Table 1: Coreference results on ACE????-NWIRE with predicted mentions and automatic linking.
coreference software for ACE2004 and using the
CoNLL scorer for the CoNLL 2011 dataset.
5 Experimental Results
We first look at NECO?s performance at coreference
resolution and then evaluate its ability at NEL.
5.1 Coref. Results with Predicted Mentions
Overall System Performance on ACE Data Ta-
ble 1 shows NECO?s performance at coreference
resolution on ACE-???? compared to the Stanford
sieve implementation (Lee et al, 2013). The table
shows that NECO has both significantly improved
precision and recall compared to the Stanford base-
line, across all metrics. We generally observe larger
gains in MUC due to better mention detection and
the Relaxed NEL Sieve.
Contribution of System Components Table 1
also details the performance of four variants of our
system that ablate various components and features.
Specifically, we consider the following cases:
? No NEL Mentions: We discard additional
mentions, MNEL, provided by NEL (Sec. 3.1).
This increases B3 precision at the expense of
recall. Inspection shows that some of the errors
introduced by MNEL are actually due to cor-
rectly linked entities that were not annotated as
mentions in the dataset, but also some improp-
erly linked mentions.
? No Mention Pruning: We disable the initial
step of updating mention boundaries and re-
moving spurious mentions (Sec. 3.2). As ex-
pected, removing this step drops precision and
recall significantly, even compared to the No
NEL Mentions variant.
? No Attributes: Ablating coarse and fine-
grained attributes (Sec. 3.3) drops F1 and re-
call measures across all metrics. To under-
stand this effect, note that NECO uses at-
tributes in two different settings. Updating
coarse attributes tends to increase precision be-
cause it prevents dangerous merges, such as
merging ?Staples? with the mention ?it? in
a situation when ?Staples? refers to the per-
son entity Todd Staples. Fine-grained at-
tributes also help with recall, when merging
a specific name of an entity with a mention
that uses a more general term; for instance,
?Hong Kong Disneyland? can be merged with
?the mysterious park? because ?park? is a fine-
grained attribute for Disneyland. However,
when fine-grained attributes are used, precision
sometimes drops (e.g., when ?president? might
merge with ?Bush? when it should really merge
with ?Clinton?).
? No NEL Constraints: Removing these con-
straints (Sec. 3.4) drops precision dramatically
leading to drop in F1. In the case of incor-
rect linking, however, NEL constraints can af-
fect recall. For instance, NEL constraints might
prevent merging ?Staples? with ?Todd Staples?
if the former were linked to the company and
the latter to the politician.
Overall System Performance on CoNLL Data
We also compare our full system (with added NEL
sieves, constraints, and mention pruning3) with the
Stanford sieve coreference system on CoNLL data
3Due to CoNLL annotation guidelines, a named entity is
added to the mention list if it is not inside a larger mention with
an exact named entity link.
294
MUC B3
Category: Method P R F1 P R F1
BC: NECO 62.1 64.7 63.4 69.8 57.8 63.2
BC: Stanford Sieves 60.9 65.0 62.9 69.2 58.0 63.1
BN: NECO 69.3 59.4 64.0 78.8 60.8 68.6
BN: Stanford Sieves 68.0 58.9 63.1 79.0 60.2 68.3
MZ: NECO 67.6 62.9 65.2 78.4 61.1 68.7
MZ: Stanford Sieves 66.0 63.4 64.9 77.9 61.5 68.7
NW: NECO 62.0 54.5 58.0 74.9 57.4 65.0
NW: Stanford Sieves 60.0 54.2 56.9 75.3 57.0 64.9
Table 3: Coreference results on the individual categories of CoNLL 2011 development data. (BC=broadcast conver-
sation, BN=broadcast news, MZ=magazine, NW=newswire)
MUC B3
Method P R F1 P R F1
Development Data
NECO 64.1+ 59.4 61.7+ 74.7 58.7 65.7
Stanford 62.7 59.0 60.8 74.8 58.3 65.6
NECO* 56.4+ 50.0 53.0+ 72.6 51.6 60.3
Stanford* 53.5 50.0 51.6 71.8 51.3 59.9
Test Data
NECO 61.2+ 58.4 59.8+ 72.2 56.4 63.3
Stanford 59.2 58.8 59.0 71.3 56.1 62.8
NECO* 55.1+ 51.7 53.3+ 70.0 50.8 58.8
Stanford* 52.0 52.3+ 52.1 68.9 50.8 58.5
Table 2: Coreference results on CoNLL 2011 develop-
ment and test data, using predicted mentions. Rows de-
noted with * indicate runs using the fully automated Stan-
ford CoreNLP pipeline rather than the predicted annota-
tions provided with the CoNLL data. Given the relatively
close results, we ran the Mann-Whitney U test for this
table; values with the + superscript are significant with
p < 0.05.
(Table 2). We ran NECO and the baseline in two set-
tings: in the first, we use the standard predicted an-
notations (for POS, parses, NER, and speaker tags)
provided with the CoNLL data, and in the second,
we use the automated Stanford CoreNLP pipeline
to predict this information. On both the develop-
ment and test sets, we gain about 1 point in MUC
F1 as well as a smaller improvement in B3. Closer
inspection indicates that our system increases pre-
cision primarily due to mention pruning and NEL
constraints. Due to the differences in mention anno-
tation guidelines between ACE and CoNLL, perfor-
mance on ACE benefits more from improved men-
tion detection from NEL. Moreover, the ACE cor-
pus is all newswire text, which contains more enti-
ties that can benefit from linking. CoNLL, on the
other hand, contains a wider variety of texts, some
of which do not mention many named entities in
Wikipedia.
To examine the performance of our system on the
different domains covered by the CoNLL data, we
also test our system on each domain separately (Ta-
ble 3). We found NEL provided the biggest im-
provement for the news domains, broadcast news
(BN) and newswire (NW). These domains espe-
cially benefit from the improved mention detection
and pruning provided by NEL, and strong linking
benefitted both precision and recall in these do-
mains. We found that the magazine (MZ) section
of the corpus benefited the least from NEL, as there
were relatively few entities that our NEL systems
were able to connect to Wikipedia.
5.2 Coreference Results with Gold Linking
Some of the errors introduced in our system are due
to incorrect or incomplete links discovered by the
automatic linking system. To assess the effect of
NEL performance on NECO, we tested on a por-
tion of ACE????-NWIRE dataset for which we hand-
labeled correct links for the gold and predicted men-
tions. ?NECO + Gold NEL? denotes a version of our
system which uses gold links instead of those pre-
dicted by NEL. As shown in Table 4, gold linking
significantly improves the performance of our sys-
tem across all measures. This suggests that further
work to improve automatic NEL may have substan-
tial reward.
Gold linking improves precision for two main rea-
295
Method MUC B3 Pairwise
P R F1 P R F1 P R F1
Gold Mentions
NECO + Gold NEL 85.8 75.5 80.3 91.4 81.2 86.0 89.1 68.0 77.1
NECO 84.6 74.0 78.9 90.5 80.4 85.2 83.9 66.0 73.9
Stanford Sieves 84.5 72.2 77.8 89.9 77.7 83.4 89.9 57.3 68.1
Predicted Mentions
NECO + Gold NEL 56.4 58.8 57.5 78.2 78.3 78.3 68.0 54.3 60.4
NECO 51.3 53.5 52.4 76.5 76.4 76.5 61.2 45.6 52.2
Stanford Sieves 43.9 46.4 45.1 74.4 74.2 74.3 51.3 36.1 42.4
Table 4: Coreference results on ACE????-NWIRE-NEL with gold and predicted mentions and gold or automatic linking.
Method MUC B3 Pairwise
P R F1 P R F1 P R F1
NECO 85.0 76.6 80.6 87.6 76.4 81.6 79.3 56.1 65.8
Stanford Sieves 84.6 75.1 79.6 87.3 74.1 80.2 79.4 50.1 61.4
Haghighi and Klein (2009) 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7
Poon and Domingos (2008) 71.3 70.5 70.9 - - - 62.6 38.9 48.0
Finkel and Manning (2008) 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9
Table 5: Coreference results on ACE????-NWIRE with gold mentions and automatic linking.
sons. First, it reduces the coreference errors caused
by incorrect NEL links. For instance, gold link-
ing replaces the erroneous link generated by our
NEL systems for ?Nasser al-Kidwa? to the correct
Wikipedia entity. As another example, two men-
tions of ?Rutgers? will not be merged if one links
to the university and the other links to their football
team. Second, gold linking leads to better mention
detection and better linked mentions. For instance,
under gold linking, the whole mention, ?The gover-
nor of Alaska, Sarah Palin,? is linked to the politi-
cian, while automatic linking systems only link the
substring containing her name, ?Sarah Palin.? Still,
gold NEL cannot compensate for all coreference er-
rors in cases of generic or unlinked entities.
5.3 Coreference Results with Gold Mentions
Many of the previous papers evaluate coreference
resolution assuming gold mentions so we also run
under that condition (Table 5) using ACE????-
NWIRE data. As the table shows, with gold mentions
our system outperforms Haghighi and Klein (2009),
Poon and Domingos (2008), Finkel and Man-
ning (2008) and the Stanford sieve algorithm across
all metrics. Our method shows a relatively smaller
gain in precision, because this condition adds no
benefit to our technique of using NEL information
for pruning mentions.
5.4 Improving Named Entity Linking
While our previous experiments show that named-
entity linking can improve coreference resolution,
we now address the question of whether coreference
techniques can help NEL. We compare NECO with
a baseline ensemble4 composed of GLOW (Ratinov
et al, 2011) and WikipediaMiner (Milne and Witten,
2008) on our ACE????-NWIRE-NEL dataset (Table
6). Our system gains about 8% in absolute recall
and 5% in absolute precision. For instance, our sys-
tem correctly adds links from ?Bullock? to the en-
tity Sandra Bullock because coreference reso-
lution merges two mentions. In another example, it
correctly links ?company? to Nokia. Overall, there
is a 21% relative reduction in F1 error.
4We take the union of all the links returned by GLOW and
WikipediaMiner, but if they link a mention to two different en-
tities, we use only the output of WikipediaMiner.
296
Method F1 Precision Recall
NECO 70.6 72.0 69.2
Baseline NEL 64.4 67.4 61.7
Table 6: NEL performance of our system and the ensem-
ble baseline linker on ACE????-NWIRE-NEL.
5.5 Error Analysis
We analyzed 90 precision and recall errors and
present our findings in Table 7. Spurious mentions
accounted for the majority of non-semantic errors.
Despite the improvements that come from NEL, a
large portion of coreference errors can still be at-
tributed to incomplete semantic information, includ-
ing precision errors caused by incorrect linking. For
instance, the mention ?Disney? sometimes refers to
the company, and other times refers to the amuse-
ment park; however, the NEL systems we used had
difficulty disambiguating these cases, and NECO of-
ten incorrectly merges such mentions. Overly gen-
eral fine-grained attributes caused precision errors in
cases where many proper noun mentions were po-
tential antecedents for a common noun. Although
attributes such as country are useful for resolving a
generic ?country? mention, this information is insuf-
ficient when two distinct mentions such as ?China?
and ?Russia? both have the country attribute.
However, many recall errors are also caused by
the lack of fine-grained attributes. Finding the ideal
set of fine-grained attributes remains an open prob-
lem.
6 Related Work
Coreference resolution has a fifty year history which
defies brief summarization; see Ng (2010) for a
recent survey. Section 2.1 described the Stanford
multi-pass sieve algorithm, which is the foundation
for NECO.
Earlier coreference resolution systems used shal-
low semantics and pioneered knowledge extraction
from online encyclopedias (Ponzetto and Strube,
2006; Daume? III and Marcu, 2005; Ng, 2007). Some
recent work shows improvement in coreference res-
olution by incorporating semantic information from
Web-scale structured knowledge bases. Haghighi
and Klein (2009) use a rule-based system to extract
fine-grained attributes for mentions by analyzing
precise constructs (e.g., appositives) in Wikipedia
articles. Subsequently, Haghighi and Klein (2010)
used a generative approach to learn entity types from
an initial list of unambiguous mention types. Bansal
and Klein (2012) use statistical analysis of Web n-
gram features including lexical relations.
Rahman and Ng (2011) use YAGO to extract type
relations for all mentions. These methods incor-
porate knowledge about all possible meanings of a
mention. If a mention has multiple meanings, ex-
traneous information might be associated with it.
Zheng et al (2013) use a ranked list of candidate en-
tities for each mention and maintain the ranked list
when mentions are merged. Unlike previous work,
our method relies on NEL systems to disambiguate
possible meanings of a mention and capture high-
precision semantic knowledge from Wikipedia cate-
gories and Freebase notable types.
Ratinov and Roth (2012) investigated using NEL
to improve coreference resolution, but did not con-
sider a joint approach. They extracted attributes
from Wikipedia categories and used them as fea-
tures in a learned mention-pair model, but did not
do mention detection. Unfortunately, it is difficult
to compare directly to the results of both systems,
since they reported results on portions of ACE and
CoNLL datasets using gold mentions. However,
our approach provides independent evidence for the
benefit of NEL, and joint modeling in particular,
since it outperforms the state-of-the-art Stanford
sieve system (winner of the CoNLL 2011 shared
task (Pradhan et al, 2011)) and other recent com-
parable approaches on benchmark datasets.
Our work also builds on a long trajectory of
work in named entity resolution stemming from
SemTag (Dill et al, 2003). Section 2.2 discussed
GLOW and WikipediaMiner (Ratinov et al, 2011;
Milne and Witten, 2008). Kulkarni et al (2009)
present an elegant collective disambiguation model,
but do not exploit the syntactic nuances gleaned by
within-document coreference resolution. Hachey et
al. (2013) provide an insightful summary and evalu-
ation of different approaches to NEL.
7 Conclusions
Observing that existing coreference resolution and
named-entity linking have complementary strengths
297
Error Type Percentage Example
Extra mentions 31.1 The other thing Paula really important is that they talk a lot about the
fact ...
Pronoun 27.7 However , [all 3 women gymnasts , taking part in the internationals for
the first time], performed well , because they had strong events and their
movements had difficulty .
Contextual
semantic
16.6 [The Chinese side] hopes that each party concerned continues to make
constructive efforts to ...Considering the requirements of the Korean side
, ... the Chinese government decided to ...
NEL semantic 13.3 The most important thing about Disney is that it is a global brand. ... The
subway to Disney has already been constructed.
Attributes 11.1 The Hong Kong government turned over to Disney Corporation [200
hectares of land ...]. ... this area has become a prohibited zone in Hong
Kong.
Table 7: Examples of different error categories and the relative frequency of each. For every example, the mention to
be resolved is underlined, and the correct antecedent is italicized. For precision errors, the wrongly merged mention
is bolded. For recall errors, the missed mention is surrounded by [brackets].
and weaknesses, we present a joint approach. We
introduce NECO, a novel algorithm which solves
the problems jointly, demonstrating improved per-
formance on both tasks.
We envision several ways to improve the joint
model. While the current implementation of NECO
only introduces NEL once, we could also integrate
predictions with different levels of confidence into
different sieves. It would be interesting to more
tightly integrate the NEL system so it operates on
clusters rather than individual mentions ? after
each sieve merges an unlinked cluster, the algorithm
would retry NEL with the new context information.
NECO uses a relatively modest number of Freebase
attributes. While using more semantic knowledge
holds the promise of increased recall, the challenge
is maintaining precision. Finally, we would also like
to explore the extent to which a joint probabilistic
model (e.g., (Durrett and Klein, 2013)) might be
used to learn how to best make this tradeoff.
8 Acknowledgements
The research was supported in part by grants
from DARPA under the DEFT program through
the AFRL (FA8750-13-2-0019) and the CSSG
(N11AP20020), the ONR (N00014-12-1-0211), and
the NSF (IIS-1115966). Support was also provided
by a gift from Google, an NSF Graduate Research
Fellowship, and the WRF / TJ Cable Professor-
ship. The authors thank Greg Durrett, Heeyoung
Lee, Mitchell Koch, Xiao Ling, Mark Yatskar, Ken-
ton Lee, Eunsol Choi, Gabriel Schubiner, Nicholas
FitzGerald, Tom Kwiatkowski, and the anonymous
reviewers for helpful comments and feedback on the
work.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In International Confer-
ence on Language Resources and Evaluation Work-
shop on Linguistics Coreference.
Mohit Bansal and Dan Klein. 2012. Coreference se-
mantics from web features. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing.
Stephen Dill, Nadav Eiron, David Gibson, Daniel Gruhl,
R. Guha, Anant Jhingran, Tapas Kanungo, Sridhar Ra-
jagopalan, Andrew Tomkins, John A. Tomlin, and Ja-
son Y. Zien. 2003. SemTag and Seeker: bootstrapping
the semantic web via automated semantic annotation.
In Proceedings of the 12th International Conference
on World Wide Web.
298
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies.
Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-
nibal, and James R. Curran. 2013. Evaluating entity
linking with Wikipedia. Artificial Intelligence Jour-
nal, 194.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Hu-
man Language Technologies: Annual Conference of
the North American Chapter of the Association for
Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation of
Wikipedia entities in Web text. In Proceedings of the
2009 Conference on Knowledge Discovery and Data
Mining.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings of
the Conference on Computational Natural Language
Learning.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4).
Dan Milne and Ian H. Witten. 2008. Learning to link
with Wikipedia. In Proceedings of the ACM Confer-
ence on Information and Knowledge Management.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics.
NIST. 2004. The ACE 2004 evaluation planXPToolkit
architecture.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, Wordnet and
Wikipedia for coreference resolution. In Proceedings
of the North American Association for Natural Lan-
guage Processing on Human Language Technologies.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies.
Lev Ratinov and Dan Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to Wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics.
Marta Recasens and Eduard Hovy. 2010. Coreference
resolution across corpora: languages, coding schemes,
and preprocessing information. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of the 6th conference on Message Understanding.
Jiaping Zheng, Luke Vilnis, Sameer Singh, Jinho D.
Choi, and Andrew McCallum. 2013. Dynamic
knowledge-base alignment for coreference resolution.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning.
299
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 386?396,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Multi-Resolution Language Grounding with Weak Supervision
R. Koncel-Kedziorski, Hannaneh Hajishirzi, and Ali Farhadi
University of Washington
{kedzior,hannaneh,farhadi}@washington.edu
Abstract
Language is given meaning through its
correspondence with a world representa-
tion. This correspondence can be at mul-
tiple levels of granularity or resolutions.
In this paper, we introduce an approach
to multi-resolution language grounding in
the extremely challenging domain of pro-
fessional soccer commentaries. We define
and optimize a factored objective function
that allows us to leverage discourse struc-
ture and the compositional nature of both
language and game events. We show that
finer resolution grounding helps coarser
resolution grounding, and vice versa. Our
method results in an F1 improvement of
more than 48% versus the previous state
of the art for fine-resolution grounding
1
.
1 Introduction
Language is inextricable from its context. A hu-
man language user interprets an utterance in the
context of, among other things, their perception of
the world. Grounded language acquisition algo-
rithms imitate this setup: language is given mean-
ing through its correspondence with a rich world
representation. A solution to the acquisition prob-
lem must resolve several ambiguities: the seg-
mentation of the text into meaningful units (spans
of words that refer to events); determining which
events are being referenced; and finding the proper
alignment of events to these units.
Historically, language grounding was only pos-
sible over simple controlled domains and rigidly
structured language. Current research in grounded
1
Source code and data are available at http://ssli.
ee.washington.edu/tial/projects/multires/
Figure 1: An example of the multiple resolutions at which
soccer commentaries refer to events: The utterance level
alignments are shown in the black dashed boxes. The first
utterance can be further broken into the fragment-level align-
ments shown; the second cannot be decomposed further.
language acquisition is moving into real-world en-
vironments (Yu and Siskind, 2013). Grounding
sports commentaries in game events is a specific
instance of this problem that has attracted attention
(Liang et al., 2009; Snyder and Barzilay, 2007;
Hajishirzi et al., 2012), in part because of the com-
plexity of both the language and the world repre-
sentation involved.
The language employed in soccer commentaries
is difficult to ground due to its dense informa-
tion structure, novel vocabulary and word senses,
and colorful, non-traditional syntax. These chal-
lenges conspire to foil most language processing
techniques including automated parsers and word-
sense disambiguation systems.
In addition to the structural problems presented
by the language of soccer commentaries, the prob-
lem of reference is further complicated by the fact
that for game events (and other real-world phe-
nomena) there is no standardized meaningful lin-
guistic unit. Utterances ranging from a single
word to multiple sentences can be used to refer to a
single event. For example, in Figure 1 the first four
words of commentary (I) refer to a single event, as
does the entirety of (II).
386
Figure 2: An example of the different levels of granularity present in the soccer data. The dashed boxes on the left denote ut-
terances made by the commentators. Solid boxes denote fragments that cannot be decomposed into finer resolution alignments.
The table on the right is a portion of the detailed listing of game events.
Turning our attention to Figure 2, sometimes a
fragment refers to a combination of events and no
further decomposition is available, such as the first
fragment of commentary (I). Moreover, it is some-
times desirable to construct a complex of events
by determining all the events corresponding to a
particular collection of words. For instance, we
would want to be able to align the whole of (I)
with all the events in the corresponding dashed
box. This suggests studying language grounding
at multiple levels of granularity (resolutions).
We use resolution to describe the continuum of
meaningful units which exist in human language
2
.
These resolutions interact in a complicated way,
with clues from different resolutions sometimes
combining to produce an effect and sometimes
negating one another. With enough training data,
one could hope to learn the details of the interac-
tions of various resolutions. However, the expense
of producing or obtaining supervised training data
at multiple resolutions is prohibitive.
To address all these complications, we in-
troduce weakly-supervised multi-resolution lan-
guage grounding. Our method makes use of a
factorized objective function which allows us to
model the complex interplay of resolutions. Our
language model takes advantage of the discourse
structure of the commentaries, making it robust
enough to handle the unique language of the soc-
cer domain. Finally, our method relies only on
2
Though it is tempting to discritize meaning in text, Chafe
(1988) shows that readers imbue text with meaningful intona-
tional patterns drawn from the potentially continuous space of
auditory signals.
loose temporal co-occurrence of events and utter-
ances as supervision and does not require expen-
sive annotated training data.
To test our method we augment the Profes-
sional Soccer Commentary Dataset (Hajishirzi et
al., 2012) with fragment-level event alignment an-
notations. This dataset is composed of commen-
taries for soccer matches paired with event logs
produced by Opta Sportsdata and includes human
annotated gold alignments
3
. We achieve an F1 im-
provement of over 48% on fragment-level align-
ment versus a previous state-of-the-art. We are
also able to leverage the interplay of fragment- and
utterance- level alignments to improve the previ-
ous state-of-the-art utterance-alignment system.
2 Challenges
Syntactic Limitations: Syntax is used to struc-
ture the information provided by an utterance, and
so it seems intuitive that syntactic relations could
be leveraged in this task. For example, consider
utterance (III) in Figure 2. The multi-resolution
grounding of (III) would provide a segmentation
of the utterance ? or a division of the utterance into
the fragments which refer to separate events. In
(III), there is an obvious syntactic correlate to the
correct segmentation: each verb phrase within the
conjunction headed by ?and? identifies a separate
event. Parsing (III) to an event-based semantics
like that of Davidson (1967), one could associate
each verb in an utterance with a game event and
achieve the desired segmentation.
3
Our updated dataset is available at http://ssli.
ee.washington.edu/tial/projects/multires/
387
Unfortunately, there is a preponderance of ex-
amples such as (II) in Figure 2, where 4 verbs
are used to describe a single ?miss? event. (II) il-
lustrates just one of the many difficulties of using
syntactic information ? elsewhere, events are ref-
erenced without an explicit verb whatsoever (such
as the use of the phrase ?into the books? to refer to
a foul event). What is needed instead is a language
model that is powerful enough to proscribe some
structure yet robust enough to allow the world rep-
resentation to determine which pieces of language
are referring to which referent or set of referents.
Complex Interplay between Resolutions:
Language refers at a variety of resolutions, and
the relationship between nested reference scopes
is complex. A single or few words can indicate
entities or properties; full phrases are often needed
to denote an action; complex events like a missed
shot may take up to several phrases of narration
to properly describe. A soccer commentator
does not encode every detail necessary for proper
alignment and segmentation into their utterances,
but rather only enough to make clear to another
with similar world knowledge what is meant.
A language grounding method is at a severe
disadvantage when faced with such implicit
information.
Instead, a successful method can make heavy
use of the limited lexical, phrasal, and discourse
structural cues provided in an utterance, as the dif-
ferent resolutions rely on these different contex-
tual clues to meaning. At finer resolutions one can
rely more on the lexical meanings of the words;
at medium resolutions, compositionality can be
leveraged; at coarser resolutions, discourse fea-
tures come into play. These cues interact in a com-
plicated way, providing additional challenge.
Consider again Figure 2. In (III), the tempo-
ral discourse marker ?and? marks the division be-
tween the fragments referring to each event. In
(I) the same word (used again as a temporal dis-
course marker) is used to elaborate on the single
?foul? event being described in the second frag-
ment. A human (with sufficient understanding of
soccer) knows that, despite being separated by the
discourse marker, the phrases ?bring him down?
and ?set piece? both refer to the foul. A language
grounding algorithm that can model the interac-
tion between such word-level and utterance-level
cues can successfully segment both (I) and (III).
Supervision: For language grounding generally,
and multi-resolution grounding specifically, su-
pervised training data is expensive to produce.
Also, the various grounding domains of interest
are highly independent of one another (Liang et
al., 2009). In the face of these issues, the ideal
correspondence between language and world rep-
resentation would be learned with as little supervi-
sion as possible.
3 Problem Definition
We define the problem of multi-resolution lan-
guage grounding as follows: Given a temporal
evolution of a world state (a sequence of events)
and an overlapping natural language text (a se-
quence of utterances), we want to learn the best
correspondences between the language and the
world at different levels of granularity (Figure 2).
To set up notations, for each utterance repre-
sented as a set of words W = {w
1
, w
2
, . . . , w
n
},
we want a segmentation which expresses the re-
lationship of the words to the events which they
describe.
Let S denote a set of all possible segmentations
of W . Then S = {S|S is a segmentation of W}.
A segmentation S is in turn a set of non-
overlapping fragments (S = {s
i
}), where each
fragment is a consecutive sequence of words from
the utterance W . For example, for utterance (III)
from Figure 2, one possible (incorrect) segmenta-
tion is S = {s
1
, s
2
, s
3
} for s
1
={Chamakh rises
highest}, s
2
={and aims a header}, and s
3
= {to-
wards goal which is narrowly wide}.
An alignment consists of a segmentation S and
a mapping E from fragments of S to the set of all
events E. For example, the segmentation S could
be mapped as E = {?s
1
, e
2
?, ?s
2
, e
3
?, ?s
3
, e
1
?},
with e
1
being an Aerial Challenge, e
2
being a
missed attempt on goal, and e
3
being an out of
bounds penalty. Let E = S ? E denote the set
of all possible alignments.
As we show in Figure 2, events are composed
of the various attributes Time, Type, Pass Events,
Outcome, and Player. For example, the aerial
event in Figure 2 has the attributes and values
type:aerial, outcome:successful, pass events:head
pass, and player:Chamakh.
Finally, we denote the values for the attributes
of each e
j
as e
a
j
, where a ranges over the different
attributes of events as represented in the data.
We define the multi-resolution grounding of W
388
into E as the best segmentation S and alignment
E that maximize the joint probability distribution:
arg max
S?S,E?E
P (S,E|W ) (1)
This optimization
4
can be accomplished
through the use of supervised learning. However,
training data is expensive and tedious to produce
for the grounding problem, especially at multiple
resolutions. Additionally, the complexity of the
language in this domain would result in very
sparse associations.
Yet if we knew some of the correct fine-
resolution alignments, we could use that informa-
tion to produce good coarse resolution alignments,
and vice versa. Therefore, we formulate a fac-
torized form of the above objective which allows
us to learn features specific to aligning at the ut-
terance, fragment, and attribute resolutions. Our
method can be optimized with only weak super-
vision (loose temporal alignments between utter-
ances and a set of events occurring within a win-
dow of the utterance time).
We can evaluate such a correspondence in sev-
eral ways. For each utterance, can we predict the
correct events to which this utterance refers? This
is the problem of utterance-level alignment.
We can also evaluate based on events: for each
event, can we identify the minimal text span(s)
which refers to this event? We want a tight corre-
spondence because loose, overlapping alignments
are not semantically satisfying. However, we do
not want to under associate: human language
makes reference at a variety of levels (the word
level, the phrase level, the utterance level, and be-
yond). It is important to correctly identify all and
only the words which correspond to a given event.
This is the fragment-level alignment problem. We
show that good fragment-level alignments will im-
prove utterance-level alignment, and vice versa.
Since events are composed of their attributes,
we can imagine a very fine resolution grounding of
individual words to individual attributes. In fact,
our solution involves producing such a grounding
and composing the fragment- and utterance-level
alignments therefrom.
W
S	 ?
E	 ?
w1	 ? w2	 ? w3	 ? w4	 ? w5	 ?
Chamakh	 ? raises	 ? highest	 ? and	 ? aims	 ??	 ?
S1	 ? S2	 ? Sn	 ?
E1	 ? E2	 ? E3	 ? E4	 ?
{w3,w4,w5}	 ?
{w2}	 ?
{w1}	 ?
{w3,w4,w5}	 ?
{w1,w2}	 ? 	 ?	 ?{w4,w5}	 ?
{w1,w2,w3}	 ?
pass	 ? aerial	 ? miss	 ? out	 ?
Figure 3: Factor graph for P (S,E|W ). Here the w
i
are the
words of utterance W , S
j
are the possible segmentations of
W , and E
k
are different events.
4 Our Method
We have formulated the grounding problem as an
optimization of the joint probability distribution
P (S,E|W ), which returns the best segmentation
and accompanying event alignments given an ut-
terance W . Optimizing this function in the do-
main of real world language, however, is a diffi-
cult problem. Utterances are long here, and there
are many events which could be grounded to each.
Furthermore, the cardinality of the set of possible
segmentations is combinatorially large.
Therefore we decompose Equation 1 using the
factor graph depicted in Figure 3. We write the
joint probability distribution as a product of the
following two potential functions:
P (S,E|W )
def
=
1
Z
?
s?S
?
align
(E, s) ? ?
seg
(s,W )
(2)
where ?
align
is a function for scoring the align-
ment E for fragment s and ?
seg
scores how good
a fragment s is for the utterance W , and Z is for
normalization.
To optimize Equation 2 it is not practical to
search the space of possible S,E combinations
(this space is combinatorially large). However, we
can optimize the factored form using dynamic pro-
gramming. We first describe how to find values
for each of the potentials in sections 4.1 and 4.2.
In section 4.3 we describe the dynamic program-
ming approach to optimization.
4.1 Event Alignments Given Segmentation
The potential function ?
align
(E, s) takes as inputs
a fragment s from segmentation S and a candidate
alignment E for S and returns a score for E with
4
As this and future equations are conditioned on the set
of all events E, we omit this variable from the equations for
notational simplicity.
389
regards to s. It is here that we produce the multi-
resolution alignments; s can vary in size from a
single word to a whole utterance. ?
align
decom-
poses as the following:
?
align
(E, s) = ?
prior
(E) ??
affinity
(s, E) (3)
where the priors (?
prior
) are confidence scores for
an alignment E with the whole utterance as given
by Hajishirzi et al. (2012), which fits an exemplar
SVM to each utterance/event pair. An exemplar
SVM is an SVM fit with one positive and many
negative instances, allowing us to define an ex-
ample by what it is not (Malisiewicz et al., 2011;
Shrivastava et al., 2011).
?
affinity
scores the affinity between a fragment
s and the event e
j
to which it is aligned. We use
the term affinity as a measure of the goodness of
an alignment. Intuitively, a fragment s will have
a higher affinity for an event e
j
if s describes that
event well. Formally, the affinity between s and e
j
amounts to a product of the affinity between each
word w
i
? s and e
j
. Since e
j
is defined by a col-
lection of attributes, we can compose a score for
w
i
with e
j
from the affinity between w
i
and each
attribute a of e
j
.
?
affinity
(s, E) =
?
w
i
?s,e
j
?E
?
atr.
(w
i
, e
j
)
=
?
w
i
?s,e
j
?E
max
a
?(w
i
, e
a
j
) (4)
where e
j
is the event to which s is aligned in align-
mentE, ?
atr.
(w
i
, e
j
) is the affinity betweenw
i
and
event e
j
, and ?(w
i
, e
a
j
) is the affinity between w
i
and attribute a of e
j
.
In order to determine the affinity of a word and
an event attribute, we create attribute:value clas-
sifiers ? one for each attribute:value pair that oc-
curs in any event. For example, for goals we create
a type:goal classifier, and for unsuccessful events
we create an outcome:unsuccessful classifier.
For the categorical attributes Type, Outcome,
and Pass Events, we fit a linear SVM (Fan et al.,
2008) using the utterance-level alignments pro-
vided by ?
prior
(the exemplar SVMs) to deter-
mine the positive and negative examples. For in-
stance, we use all the utterances which are aligned
with an event whose type value is ?pass? as posi-
tive examples for our type:pass classifier, and all
other utterances as negative examples.
The weight assigned to each dimension in a
linear SVM describes the relative importance of
that dimension in the classification process. The
dimensions of our attribute:value SVMs are the
words of the corpus, normalized for case and mi-
nus punctuation and stop words. Therefore, the
affinity of a word w
i
and the attribute:value e
a
j
is
the weight of the dimension corresponding to w
i
in the e
a
j
attribute:value classifier. Following oth-
ers (Liang et al., 2009; Kate and Mooney, 2007),
we use string matches to determine the affinity be-
tween a word and the Player attribute.
In order to make comparisons between the im-
portance of a word in the decision process for dif-
ferent classifiers, we normalize the weight vectors
for each. These attribute:value classifiers produce
our finest resolution alignments, allowing us to de-
fine a correspondence between a single word and
a single attribute of any event.
By considering e
j
in terms of its attributes, we
are able to compose a score for e
j
with fragment
s. This is a kind of double-sided compositional
semantics, where both the meaningful signs (s)
and their extensions (e
j
) are composed of finer-
resolution atomic parts (w
i
and e
a
j
, respectively).
4.2 Segmentations Given Utterances
The potential function ?
seg
(s,W ) from Equation
2 returns a score for a fragment within an utter-
ance. A segmentation can be thought of as the
collection of bigrams ?w
i
, w
i+1
? where w
i
is the
last word of a fragment which is being used to de-
scribe one event and w
i+1
is the first word of a
fragment being used to describe a different event.
We will refer to such bigrams as splitpoints.
The function ?
seg
should favor fragments that
begin and end at good splitpoint and whose inter-
mediate bigrams are bad splitpoints. We formalize
this as follows:
?
seg
(s,W ) ?
?(w
k?1
, w
k
) ? ?(w
k+m
, w
k+m+1
)
?
m?1
j=0
?(w
k+j
, w
k+j+1
)
where fragment s is a span ofm consecutive words
{w
k
, ..., w
k+m
} from W , and ? is a score for how
good of a splitpoint ?w
i
, w
i+1
? would make (ex-
plained below).
Ideally, ? will be a classifier which can tell
us if a given bigram is a good splitpoint for the
utterance W . However, ours being an attempt
at weakly-supervised learning, we have no la-
beled examples of correct splitpoints from which
390
to work. Instead, we employ linguistic knowledge
to create a proxy of labels. We will use this proxy
to train a classifier to discover the features of good
splitpoints which can be generalized and produce
a more robust system.
The proxy labeling scheme we developed is
based on conservative components common to a
variety of theories of discourse. Discourse theo-
ries aim to model the relationships which exist be-
tween adjacent utterances in a coherent discourse.
Since we consider a sports commentary to be a co-
herent discourse, we can leverage results from dis-
course theory in producing our proxy labels.
Temporal Discourse: Events in a soccer match
occur in a temporal sequence, and so it is reason-
able to assume that the language used to describe
them will employ temporal discourse relations to
distinguish fragments describing separate events.
Pitler et al. (2008) have constructed a list of dis-
course relations which can be easily automatically
identified, including temporal discourse relations.
These are indicated by the presence of discourse
markers ? alternately known as cue phrases. We
hypothesize that cue phrases can be used to iden-
tify splitpoints and use them in our proxy labeling
scheme. This method is not restricted to tempo-
rally related discourse: some contingency, expan-
sion, and comparison relations are also analyzed
as ?easily identifiable?. As such, our segmentation
process can also be used to ground language into
a world state where these relations would hold.
Prosodic Discourse: We also make use of
prosodic discourse cues. Pierrehumbert and
Hirschberg (1990) claim that intonational phrases
play an important role in discourse segmentation.
Therefore, we hypothesize that the edges of in-
tonational phrases are very likely to correspond
with correct splitpoints. Viewing the commen-
tary transcriptions as a noisy channel of the ac-
tual speech signal, we can identify the intona-
tional phrase boundaries with the punctuation in-
serted in the transcription process. Chafe (1988)
confirms that punctuation in written language has
a strong correspondence with intonational phrase
boundaries, and an assumption like ours has been
successfully implemented in speech synthesis sys-
tems (Black and Lenzo, 2000). Thus, we include
bigrams containing punctuation as splitpoints in
our proxy labels.
Feature Description for splitpoint classifier
Is w
i
/w
i+1
a discourse marker?
Is w
i
/w
i+1
punctuation?
Is w
i
/w
i+1
a player name?
Part of speech of w
i
/w
i+1
Is one of w
i
/w
i+1
a dependent of the other?
Are w
i
and w
i+1
dependents of the same governor?
Dependency relations that hold across splitpoint
Height of w
i
/w
i+1
in the dependency tree
Difference in height of w
i
/w
i+1
in dependency tree
?(w
i
, e
j
) of all words left versus right of splitpoint
Symmetric difference of best affinity scores for w
i
/w
i+1
Are best affinity scores from the same event?
Table 1: Feature description for splitpoint classifier
w1	 ? w2	 ? w3	 ?
Chamakh	 ? rises	 ? highest	 ??	 ??(w1,e1) ?(w2,e1) ?(w3,e1)?(w1,w2 )E1:	 ?Unsuccessful	 ?Cross	 ?Pass	 ?E2:	 ?Successful	 ?	 ?Aerial	 ?Head	 ?Pass	 ?
E3:	 ?Missed	 ?	 ?Head	 ?Pass	 ?	 ?
1/?(w1,w2 ) ?(w3,e2 )?(w3,e3)?(w2,e2 )?(w2,e3)?(w1,e2 )?(w1,e3) ?(w1,w2 )
Figure 4: We use a trellis to allow for dynamic programming
optimization of the objective function
Splitpoint Classifier: All other bigrams besides
those above are labeled as negative examples, and
a linear SVM is fit to the data. The features for the
classifier include structural, discourse, and statis-
tical features. We make use of dependency parse
information from the Stanford dependency parser
(De Marneffe and Manning, 2008). The full fea-
tures list is explained in Table 1.
4.3 Optimization
We want to maximize the function in Equation 1,
and we have explained that we can approximate
this by maximizing the factored form in Equation
2. By the above methods, we can produce values
for the functions ?
align
and ?
seg
. What remains is
to optimize Equation 2.
We take advantage of the factorization by using
a dynamic programming approach to optimiza-
tion. Figure 4 illustrates the setup. For each word
w
i
of the utterance, we create a column of nodes
in our trellis, with one row for each event e
j
? E.
The nodes represent the affinity of a given wordw
i
with event e
j
. The weights on these nodes come
from ?
atr.
(w
i
, e
j
) described in section 4.2.
The nodes in column w
i
are connected to the
nodes in column w
i+1
by edges whose weights
391
Method Precision Recall F1
Liang et al. (2009) 0.513 0.393 0.445
Our approach 0.603 0.481 0.535
Table 2: Fragment-level alignments starting from gold
utterance-level alignments
Method Precision Recall F1
Liang et al. (2009) 0.211 0.135 0.165
Our approach 0.235 0.255 0.245
Table 3: Fragment-level alignments starting from raw data
are drawn from the splitpoint classifier response
?(w
i
, w
i+1
). We label the edges between adja-
cent nodes corresponding to different events with
the responses from the splitpoint classifier, and the
inverse of these responses for edges connecting
nodes corresponding to the same event.
We then use the Viterbi algorithm (Viterbi,
1967) to find the maximum scoring path through
this trellis. The maximum scoring path optimizes
Equation 2, and serves as our approximation of the
optimization of Equation 1. We choose the top k
diverse paths through the trellis and use the associ-
ations therein as our alignments. See Figure 5 for
a detailed example of how our Viterbi path coin-
cides with the responses from the attribute:value
classifiers.
5 Experiments
One justification for multi-resolution language
grounding would be if finer-resolution grounding
improves coarser-resolution grounding and vice
versa. If so, we expect that better utterance-level
alignments will improve fragment-level align-
ments, and that in turn those fragment-level align-
ments will improve utterance-level alignments.
We evaluate both of these hypotheses.
5.1 Experimental Setup
Dataset: We use the publicly available Profes-
sional Soccer Commentary (PSC) dataset intro-
duced in Hajishirzi et al. (2012). This dataset is
composed of professional commentaries from the
2010-2011 season of the English Premier League,
along with a human-annotated data feed produced
for each game by Opta Sportsdata (Opta, 2012)
which describes all events occurring around the
ball. Events include passes, shots, misses, cards,
Method Precision Recall F1
Liang et al. (2009) 0.327 0.418 0.367
Hajishirzi et al. (2012) 0.355 0.576 0.439
Our approach 0.407 0.520 0.457
Table 4: Utterance-level alignment results
tackles, and other relevant game details. Each
event category is defined precisely and the feed
is annotated by professionals according to strict
event description guidelines.
The PSC also provides ground truth alignment
of full utterances to events in the data feed, and for
this work we have augmented it with ground truth
fragment-level annotations
5
.
We use data from 7 games of the PSC. These
games consist of 778 utterances totaling 13,692
words. There are 12,275 events. This data is la-
beled with ground truth utterance- and fragment-
alignments.
Metric: There are 1,295 correct utterance-to-
event alignments. For evaluation we use precision,
recall, and F1 of our utterance-level alignments.
The evaluation of fragment-level alignments is
less straight forward. This is due to the two fea-
tures of a correct fragment alignment: picking
the correct fragment boundaries and associating
the fragment with the correct event. We evaluate
fragment-level alignment on a per word basis. We
consider precision in this task to be the number of
correct word to event alignments versus the total
number of alignments produced by a system. Re-
call is the number of correct word to event align-
ments versus the total gold word to event align-
ments, of which there are 18,147.
Comparisons: We compare to two previous
works: Liang et al. (2009), which produces
both segmentation and alignment results; and Ha-
jishirzi et al. (2012), which produces state-of-the-
art alignments. When evaluating segmentation,
we compare how well the systems perform start-
ing from the raw dataset, and starting from gold
utterance-level alignments. This allows us to iso-
late the segmentation process from the overall sys-
tem architectures. It also gives us some insight
into the effect of event priors on the segmentation
and alignment processes.
5
The full dataset is available at http://ssli.
ee.washington.edu/tial/projects/multires/
392
Chamakh	 ? rises	 ? highest	 ? and	 ? aims	 ? a	 ? header	 ? towards	 ?goal	 ? which	 ? is	 ? narrowly	 ?wide	 ?pass:head	 ?pass	 ?
outcome:unsuccessful	 ?
type:out	 ?
pass:head	 ?pass	 ?
outcome:successful	 ?
type:miss	 ?
pass:head	 ?pass	 ?
outcome:successful	 ?
type:aerial	 ?
.01	 ? .02	 ?0	 ? .05	 ? 0	 ?.23	 ? 0	 ? .02	 ? .1	 ?.33	 ? .02	 ?.04	 ? 0	 ?
0	 ? .03	 ?0	 ? .01	 ? 0	 ?.04	 ? 0	 ? .01	 ? .04	 ?.33	 ? .01	 ?.02	 ? 0	 ?
.01	 ? .07	 ?0	 ? 0	 ? 0	 ?0	 ? 0	 ? .03	 ? .28	 ?.33	 ? .01	 ?.01	 ? 0	 ? .01	 ? .02	 ?0	 ? .05	 ? 0	 ?.23	 ? 0	 ? .02	 ? .1	 ?.33	 ? .02	 ?.04	 ? 0	 ?
0	 ? .06	 ?0	 ? 0	 ? 0	 ?.03	 ? 0	 ? .01	 ? .03	 ?.33	 ? .01	 ?.01	 ? 0	 ?
.02	 ? .15	 ?0	 ? .01	 ? 0	 ?.11	 ? 0	 ? .06	 ? .50	 ?.33	 ? .02	 ?.07	 ? 0	 ?
.01	 ? .02	 ?0	 ? .05	 ? 0	 ?.23	 ? 0	 ? .02	 ? .1	 ?.33	 ? .02	 ?.04	 ? 0	 ?
0	 ? .06	 ?0	 ? 0	 ? 0	 ?.03	 ? 0	 ? .01	 ? .03	 ?.33	 ? .01	 ?.02	 ? 0	 ?
.04	 ? .02	 ?0	 ? .05	 ? 0	 ?.1	 ? 0	 ? .03	 ? .13	 ?.33	 ? .03	 ?.06	 ? 0	 ?
Figure 5: A successful grounding at multiple resolutions. Thin blue lines separate
the attribute:value pairs corresponding to the three events. Values of ?(w
i
, e
j
)
are shown on each node. The shaded bands indicate the gold fragment-level align-
ments. Thick line connecting the green nodes indicates the classifier responses
used in the Viterbi best path through our trellis. The red dashed edge indicates a
high response from the splitpoint classifier. This figure is best viewed in color.
Method Precision Recall F1
Ours 0.235 0.255 0.245
- ?
affinity
0.213 0.133 0.164
- ?
seg
0.205 0.232 0.218
Table 5: Ablation studies for fragment-level
alignments by removing ?
affinity
and ?
seg
from our model by replacing them with uni-
form function.
Method Precision Recall F1
Ours 0.407 0.520 0.457
- ?
affinity
0.446 0.189 0.265
- ?
seg
0.376 0.563 0.451
Table 6: Ablation studies for utterance-level
alignments by removing ?
affinity
and ?
seg
from our model by replacing them with uni-
form function.
5.2 Results
We evaluate our method on its alignments at the
fragment-level and at the utterance-level. The re-
sults are as follows:
Fragment-level: Our results for segmentation can
be seen in Tables 2 and 3. Table 2 shows the results
achieved on the fragment-level alignment task us-
ing human-labeled utterance to event alignments.
In this setting, all and only the correct events for
each utterance are present. Still, there are sev-
eral ambiguities in the data. Some fragments are
aligned in the gold data with multiple events, and
some are aligned to no event. Our method out-
performs the previous by a large margin in terms
of both precision and recall. We show below how
this is due to our system?s accommodation of dis-
course structure when making segmentation deci-
sions and the factored form of our optimization.
Table 3 shows the results for fragment-level
alignment by applying each system starting from
the raw data. Here, in addition to the ambiguities
mentioned above, the problem is further compli-
cated by the fact that some correct events are miss-
ing from the alignments produced by each system
and some incorrect events are included in these
alignments (see Error Analysis below for details).
Still our method achieves a significant improve-
ment, with a 48% increase in F1 versus prior work.
Table 5 shows ablation results for the effect of
the factors used in our optimization for fragment-
level alignments. These results demonstrate the
value of each factor in the fragment-level align-
ment process. We cannot ascribe the benefit of this
method to one factor or another alone ? it is their
concert that improves performance.
Utterance-level: We have posited that good finer-
resolution alignments will improve the coarser-
resolution utterance to event alignments. Our re-
sults confirm this hypothesis. Table 4 shows our
results on these alignments. We are able to im-
prove F1 versus a state-of-the-art system which
is tuned to maximize its F1 score. The major-
ity of our improvement comes from the increased
precision of our system, due to the influence of
the finer-resolution fragment-level alignments on
these coarser, utterance-level alignments. We pro-
vide a detailed example of this below. Ablation
results are shown in Table 6.
5.3 Qualitative Analysis
A qualitative analysis of our system reveals the
power of our factored objective, double-sided
compositional approach, and leveraging of dis-
course structure. Figure 5 shows the best path
through the trellis of the example sentence used
in the introduction. For explanatory purposes,
we have split every event into its three compo-
nent attributes. This allows us to see how the
attribute:value classifiers combine to produce an
alignment.
Discourse Structure: The fragment-level align-
ment we have produced for this utterance is per-
fect: it correctly identifies the single splitpoint and
correctly identifies each fragment with the associ-
ated event.
The identification of the splitpoint ?and? comes
from the fact that this word has, among other uses,
a discourse connective meaning. Thus, the edges
393
in our trellis between different events are weighted
higher than edges between the same event in the
edges between the nodes for ?highest? and ?and?,
encouraging the Viterbi path to change events at
this point.
Compositionality: We can see effect of the com-
positional approach we have taken ? composing
?
affinity
(s, e
j
) from the attribute:value classifier
scores of each ?(w
i
, e
a
j
) ? by looking at how the
best path makes use of different attributes of the
same event. For the ?miss? event aligned with
the second part of the sentence, we can see that
the best path makes use of both values from the
type:miss and pass event:head pass classifiers.
Affinities: A few interesting associations are
worth pointing out. First, we note that the word
?header? has a stronger affinity for the type:miss
attribute than it does for the pass events:head pass
attribute. On first blush, this seems like a mistake
in our classifier. However, we can see that even
in this single trellis all three events have the pass
events:head pass attribute. The utterance-level
alignment uses this association already, align-
ing utterances containing the word ?header? with
events that have a pass events:head pass attribute.
At a finer-resolution, it is necessary to make a
different distinction between events. Our method
finds that the presence of the word ?header? is a
stronger indicator of an event with a type:miss at-
tribute, and thus this association is made.
Words that are better for the coarser-resolution
association with the pass events:head pass at-
tribute are ?towards? and ?goal?. Out of the 10
utterances containing the word ?towards? in the
dataset, 3 of these are aligned with at least 1 pass
events:head pass event, making this strong asso-
ciation a correct one. The word ?goal? also has
an affinity for the pass events:head pass attribute
due to the fact that many events with this attribute
are attempts on goal. This correlates with domain
knowledge about soccer, because, although there
may be other uses of their head by a player in the
game, shots on goal are events which will nearly
always be commented upon by an announcer.
Factorization: We have shown that finer-
resolution fragment-level alignments can improve
utterance-level alignments. From the exemplar
SVMs, we are given an utterance-level alignment
of the three events shown in the trellis with the
utterance. This alignment is incorrect: the gold
utterance alignment only includes the bottom two
events. But by building an utterance-level align-
ment from the results of our fragment level align-
ment, we are left with only the two correct events.
We prune the topmost event due to its failure to
participate in a finer-resolution alignment.
5.4 Error Analysis
The majority of the errors made on our fragment-
level alignments come in one of two flavors:
Firstly, we sometimes erroneously identify a frag-
ment as referring to an event when in truth it refers
to no event. Commentators often describe facts
about players or the weather or previous games
which have no extension in the current game.
However, our system cannot distinguish such lan-
guage from the language referring to this game.
This is a good avenue for future exploration.
The second set of errors we make in fragmen-
tation are caused by bad event priors. Our current
setup cannot increase recall: we can only improve
the precision of the utterance-level alignments we
are given. Therefore, if an event is overlooked
in the first-pass of utterance-level alignments, we
cannot reintroduce it through a fragment align-
ment. This is a direction for future work as well.
6 Related Work
Early semantic parsing work made use of fully su-
pervised training (Zettlemoyer and Collins, 2005;
Ge and Mooney, 2006; Snyder and Barzilay,
2007), but more recent work has focused on re-
ducing the amount of supervision required (Artzi
and Zettlemoyer, 2013). A few unsupervised ap-
proaches exist (Poon and Domingos, 2009; Poon,
2013), but these are specific to translating lan-
guage into queries in highly structured database
and cannot be applied to our more flexible domain.
There are few datasets as detailed as the Profes-
sional Soccer Commentary Dataset. Early work
in understanding soccer commentaries focused on
RoboCup soccer (Chen and Mooney, 2008; Chen
et al., 2010; Bordes et al., 2010; Hajishirzi et
al., 2011) where simple language describes each
event, and events are in a one-to-one correspon-
dence with utterances. Another dataset used for
language grounding is the Weather Report Dataset
(Liang et al., 2009). Here, again, however, we
have mostly single utterances paired with single
events, and many alignments are made via nu-
merical string matching rather than learning lex-
394
ical cues. The NFL Recap dataset (Snyder and
Barzilay, 2007) is also laden with numerical fact
matching, and does not include the fragment-level
segmentation annotation that the PSC dataset pro-
vides.
Impressive advances have been made grounding
language in instructions. Branavan et al. (2009)
and Vogel and Jurafsky (2010) work in the do-
main of computer technical support instructions,
mapping language to actions using reinforcement
learning. Matuszek et al. (2012b) parses sim-
ple language to robot control instructions. Our
work focuses on dealing with a richer space, both
in terms of the language used and the world-
representation into which it is grounded, and lever-
aging the multiple resolutions of reference.
An exciting direction of research, closer to our
own, aims to ground natural language in visual
perception systems. Matuszek et al. (2012a) at-
tempts to learn a joint model of language and ob-
ject characteristics of a workplace environment.
Yu and Siskind (2013) grounds moderately rich
language in automatically annotated video clips.
Again, the contribution of our work versus the
above is in the complexity of the language with
which we deal and our multi-resolution model.
7 Conclusion
The problem of grounding complex natural hu-
man language such as soccer commentaries is
extremely difficult at all resolutions, and it is
most challenging at finer resolutions where data is
sparsest and small errors cannot be as easily nor-
malized. Our work will help open new avenues of
research into this difficult and exciting problem.
This paper presents a new method for the multi-
resolution grounding of complex natural language
in a detailed world representation. Our factor
graph allows us to decompose the grounding prob-
lem into the more tractable subproblems of seg-
menting the language into fragments and aligning
the fragments with the world representation. In
the segmentation phase, we make use of linguis-
tic theories of discourse to create a proxy of labels
from which we learn statistical and structural fea-
tures of good splitpoints. In the alignment phase,
we bootstrap the learning of finer-grained corre-
spondences between the language and the world
representation with rough alignments from a state-
of-the-art system. We combine these phases in a
dynamic programming setup which allows us to
efficiently optimize our objective.
We have shown that factoring the acquisition
problem into separate alignment and segmentation
phases improves performance on several evalua-
tion metrics. We achieve considerable improve-
ments over the previous state of the art on finer-
resolution alignments in the domain of profes-
sional soccer commentaries, and we show that we
can leverage groundings at one resolution to im-
prove alignments in another.
Several extensions of this work are possible. We
would like to annotate more games to improve
our dataset. We could improve our model by en-
coding the dynamics of the environment. We did
not attempt to learn this information in our pro-
cess, but it is likely that modeling the event tran-
sition probabilities could provide better results. A
larger future work would extend the method out-
lined herein to produce templates for automated
commentary generation.
Acknowledgments
This research was supported in part by a grant
from the NSF (IIS-1352249), and the Royalty Re-
search Fund (RRF) at the University of Washing-
ton. The authors also wish to thank Gina-Anne
Levow, Yoav Artzi, Ben Hixon, and the anony-
mous reviewers for their valuable feedback on this
work.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Alan Black and Kevin Lenzo. 2000. Building voices
in the festival speech synthesis system.
Antoine Bordes, Nicolas Usunier, and Jason Weston.
2010. Label ranking under ambiguous supervision
for learning semantic correspondences. In Proceed-
ings of The 27th International Conference on Ma-
chine Learning, pages 103?110.
S. R. K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the Association for Computational Linguistics and
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 82?90.
Wallace Chafe. 1988. Punctuation and the prosody
of written language. Written communication,
5(4):395?426.
395
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: a test of grounded language ac-
quisition. In Proceedings of the 25th International
Conference on Machine Learning, pages 128?135.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. Journal
of Artificial Intelligence Research (JAIR), 37:397?
435.
Donald Davidson. 1967. The logical form of action
sentences. The logic of Decision and Action.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. Stanford typed dependencies manual.
URL http://nlp. stanford. edu/software/dependencies
manual. pdf.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Ruifang Ge and Raymond J. Mooney. 2006. Discrim-
inative reranking for semantic parsing. In Proceed-
ings of the 44th Annual Meeting of the Association
for Computational Linguistics.
Hannaneh Hajishirzi, Julia Hockenmaier, Erik T.
Mueller, and Eyal Amir. 2011. Reasoning about
robocup soccer narratives. In Proceedings of the
27th conference on Uncertainty in Artificial Intelli-
gence, pages 291?300.
Hannaneh Hajishirzi, Mohammad Rastegari, Ali
Farhadi, and Jessica K Hodgins. 2012. Semantic
understanding of professional soccer commentaries.
In Proceedings of the 28th conference on Uncer-
tainty in Artificial Intelligence.
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambiguous supervi-
sion. In Proceedings of the Twenty-Second AAAI
Conference on Artificial Intelligence, pages 895?
900.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of the Joint Conference of the
47th Annual Meeting of the Association for Com-
putational Linguistics and 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 91?99.
Tomasz Malisiewicz, Abhinav Gupta, and Alexei A.
Efros. 2011. Ensemble of exemplar-svms for object
detection and beyond. In Proceedings of the 13th
International Conference on Computer Vision.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012a. A Joint
Model of Language and Perception for Grounded
Attribute Learning. In Proc. of the 2012 Interna-
tional Conference on Machine Learning, Edinburgh,
Scotland, June.
Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,
and Dieter Fox. 2012b. Learning to parse natural
language commands to a robot control system. In
Proc. of the 13th International Symposium on Ex-
perimental Robotics (ISER), June.
Opta. 2012. http://www.optasports.com.
Janet Pierrehumbert and Julia Hirschberg. 1990. The
meaning of intonational contours in the interpreta-
tion of discourse. Intentions in Communication,
271.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind K Joshi. 2008.
Easily identifiable discourse relations. Technical
Reports (CIS), page 884.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10.
Hoifung Poon. 2013. Grounded unsupervised seman-
tic parsing. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics.
Abhinav Shrivastava, Tomasz Malisiewicz, Abhinav
Gupta, and Alexei A. Efros. 2011. Data-driven
visual similarity for cross-domain image matching.
ACM Transaction of Graphics (TOG) (Proceedings
of ACM SIGGRAPH ASIA), 30(6).
Benjamin Snyder and Regina Barzilay. 2007.
Database-text alignment via structured multilabel
classification. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 1713?1718.
Andrew J Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. Information Theory, IEEE Transac-
tions on, 13(2):260?269.
Adam Vogel and Daniel Jurafsky. 2010. Learning to
follow navigational directions. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 806?814.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, volume 1, pages 53?63.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence, pages 658?
666.
396
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523?533,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning to Solve Arithmetic Word Problems with Verb Categorization
Mohammad Javad Hosseini
1
, Hannaneh Hajishirzi
1
, Oren Etzioni
2
, and Nate Kushman
3
1
{hosseini, hannaneh}@washington.edu,
2
OrenE@allenai.org,
3
nkushman@csail.mit.edu
1
University of Washington,
2
Allen Institute for AI,
3
Massachusetts Institute of Technology
Abstract
This paper presents a novel approach to
learning to solve simple arithmetic word
problems. Our system, ARIS, analyzes
each of the sentences in the problem state-
ment to identify the relevant variables and
their values. ARIS then maps this infor-
mation into an equation that represents
the problem, and enables its (trivial) so-
lution as shown in Figure 1. The pa-
per analyzes the arithmetic-word problems
?genre?, identifying seven categories of
verbs used in such problems. ARIS learns
to categorize verbs with 81.2% accuracy,
and is able to solve 77.7% of the problems
in a corpus of standard primary school test
questions. We report the first learning re-
sults on this task without reliance on pre-
defined templates and make our data pub-
licly available.
1
1 Introduction
Designing algorithms to automatically solve math
and science problems is a long-standing AI chal-
lenge (Feigenbaum and Feldman, 1963). For NLP,
mathematical word problems are particularly at-
tractive because the text is concise and relatively
straightforward, while the semantics reduces to
simple equations.
Arithmetic word problems begin by describing
a partial world state, followed by simple updates
or elaborations and end with a quantitative ques-
tion. For a child, the language understanding part
is trivial, but the reasoning may be challenging;
for our system, the opposite is true. ARIS needs to
1
Our data is available at https://www.cs.
washington.edu/nlp/arithmetic.
Arithmetic word Problem
Liz had 9 black kittens. She gave some of her kittens to
Joan. Joan now has 11 kittens. Liz has 5 kittens left and 3
have spots. How many kittens did Joan get?
State Transitions1	
Liz	

N: 9	

E: Kitten	

A: Black	

Liz gave some of her kittens to Joan.	

s2	
 Liz	

N: 9-L1	
E: Kitten	

A: Black	

Joan	

N:  J0+L1	
E: Kitten	

A: Black	

give	

Equation: 9? x = 5
Solution: x = 4 kittens
Figure 1: Example problem and solution.
make sense of multiple sentences, as shown in Fig-
ure 2, without a priori restrictions on the syntax or
vocabulary used to describe the problem. Figure
1 shows an example where ARIS is asked to infer
how many kittens Joan received based on facts and
constraints expressed in the text, and represented
by the state diagram and corresponding equation.
While the equation is trivial, the text could have
involved assembling toy aircraft, collecting coins,
eating cookies, or just about any activity involving
changes in the quantities of discrete objects.
This paper investigates the task of learning to
solve such problems by mapping the verbs in the
problem text into categories that describe their im-
pact on the world state. While the verbs category
is crucial (e.g., what happens if ?give? is replaced
by ?receive? in Figure 1?), some elements of the
problem are irrelevant. For instance, the fact that
three kittens have spots is immaterial to the solu-
tion. Thus, ARIS has to determine what informa-
tion is relevant to solving the problem.
To abstract from the problem text, ARIS maps
the text to a state representation which consists of
523
a set of entities, their containers, attributes, quan-
tities, and relations. A problem text is split into
fragments where each fragment corresponds to an
observation or an update of the quantity of an en-
tity in one or two containers. For example in Fig-
ure 1, the sentence ?Liz has 5 kittens left and 3
have spots? has two fragments of ?Liz has 5 kit-
tens left? and ?3 have spots?.
The verb in each sentence is associated with one
or two containers, and ARIS has to classify each
verb in a sentence into one of seven categories
that describe the impact of the verb on the con-
tainers (Table 1). ARIS learns this classifier based
on training data as described in section 4.2.
To evaluate ARIS, we compiled a corpus of
about 400 arithmetic (addition and subtraction)
word problems and utilized cross validation to
both train ARIS and evaluate its performance
over this corpus. We compare its performance
to the template-based learning method developed
independently and concurrently by Kushman et
al. (2014). We find that our approach is much
more robust to domain diversity between the train-
ing and test sets.
Our contributions are three-fold: (a) We present
ARIS, a novel, fully automated method that learns
to solve arithmetic word problems; (b) We intro-
duce a method to automatically categorize verbs
for sentences from simple, easy-to-obtain train-
ing data; our results refine verb senses in Word-
Net (Miller, 1995) for arithmetic word problems;
(c) We introduce a corpus of arithmetic word prob-
lems, and report on a series of experiments show-
ing high efficacy in solving addition and subtrac-
tion problems based on verb categorization.
2 Related Work
Understanding semantics of a natural language
text has been the focus of many researchers in nat-
ural language processing (NLP). Recent work fo-
cus on learning to align text with meaning repre-
sentations in specific, controlled domains. A few
methods (Zettlemoyer and Collins, 2005; Ge and
Mooney, 2006) use an expensive supervision in
the form of manually annotated formal representa-
tions for every sentence in the training data. More
recent work (Eisenstein et al., 2009; Kate and
Mooney, 2007; Goldwasser and Roth, 2011; Poon
and Domingos, 2009; Goldwasser et al., 2011;
Kushman and Barzilay, 2013) reduce the amount
of required supervision in mapping sentences to
meaning representations while taking advantage
of special properties of the domains. Our method,
on the other hand, requires small, easy-to-obtain
training data in the form of verb categories that
are shared among many different problem types.
Our work is also closely related to the grounded
language acquisition research (Snyder and Barzi-
lay, 2007; Branavan et al., 2009; Branavan et al.,
2012; Vogel and Jurafsky, 2010; Chen et al., 2010;
Hajishirzi et al., 2011; Chambers and Jurafsky,
2009; Liang et al., 2009; Bordes et al., 2010)
where the goal is to align a text into underlying en-
tities and events of an environment. These meth-
ods interact with an environment to obtain super-
vision from the real events and entities in the envi-
ronment. Our method, on the other hand, grounds
the problem into world state transitions by learn-
ing to predict verb categories in sentences. In addi-
tion, our method combines the representations of
individual sentences into a coherent whole to form
the equations. This is in contrast with the previous
work that study each sentence in isolation from the
other sentences.
Previous work on studying math word and logic
problems uses manually aligned meaning repre-
sentations or domain knowledge where the seman-
tics for all the words is provided (Lev, 2007; Lev
et al., 2004). Most recently, Kushman et al. (2014)
introduced an algorithm that learns to align al-
gebra problems to equations through the use of
templates. This method applies to broad range of
math problems, including multiplication, division,
and simultaneous equations, while ARIS only han-
dles arithmetic problems (addition and subtrac-
tion). However, our empirical results show that
for the problems it handles, ARIS is much more
robust to diversity in the problem types between
the training and test data.
3 Arithmetic Problem Representation
We address solving arithmetic word problems that
include addition and subtraction. A problem text
is split into fragments where each fragment is rep-
resented as a transition between two world states
in which the quantities of entities are updated or
observed (Figure 2). We refer to these fragments
as sentences. We represent the world state as a tu-
ple ?E,C,R? consisting of entities E, containers
C, and relations R among entities, containers, at-
tributes, and quantities.
Entities: An entity is a mention in the text corre-
524
N: W0-13 	
E: tree	
A: walnut	

Liz had 9 black kittens. She gave some of her kittens to Joan. Joan has now 11 kittens. Liz has 5 kitten left and 3 has spots. How many kittens did Joan get?	

Liz had 9 	
black kittens	

s0	

s1	
Liz	

N: 9	
E: Kitten	
A: Black	
 She gave some of her kittens to Joan	

s2	
 Liz	

N: 9-L1	
E: Kitten	
A: Black	

Joan	

N:  J0+L1	
E: Kitten	
A: Black	
	

Joan has now 11 kittens	

 Liz has 5 kitten left	
 And 3 has spots	

Liz	

N: 9-L1	
E: Kitten	
A: Black	

Joan	

N: 11	
E: Kitten	
A: Black	

s3	
 Liz	

N: 5	
E: Kitten	
A: Black	

Joan	

N: 11	
E: Kitten	
A: Black	

s4	

Liz	

N: 5	
E: Kitten	
A: Black	

Joan	

N: 11	
E: Kitten	
A: Black	
unknown	

N:3	
E: Kitten	

s5	

There are 42 walnut trees and 12 orange trees currently in the park. Park workers cut down 13 walnut trees that were damaged. How many walnut trees will be in the park when the workers are finished?	

There are 42 walnut trees and 12 orange trees currently in the park. 	

s0	

s1	
Park	

N: 42	
E: tree	
A: walnut	
 Park workers cut down 13 walnut trees that were damaged	

N: 12	
E: tree	
A: orange	

s2	
 Park	
 N: 42-13 	
E: tree	
A: walnut	

N: 12	
E: tree	
A: orange	

Workers	

Figure 2: A figure sketching different steps of our method ? a sequence of states.
sponding to an object whose quantity is observed
or is changing throughout the problem. For in-
stance, kitten and tree are entities in Fig-
ure 2. In addition, every entity has attributes that
modify the entity. For instance, black is an at-
tribute of kittens, and walnut is an attribute
of tree (more details on attributes in section 4.1).
Relations describing attributes are invariant to the
state changes. For instance kittens stay black
throughout the problem of Figure 1.
Containers: A container is a mention in the
text representing a set of entities. For instance,
Liz, Joan, park, and workers are containers
in Figure 2. Containers usually correspond to the
person possessing entities or a location contain-
ing entities. For example, in the sentence ?There
are 43 blue marbles in the basket. John found 32
marbles.?, basket and John are containers of
marbles.
Quantities: Containers include entities with their
corresponding quantities in a particular world
state. Quantities can be known numbers (e.g. 9),
unknown variables (e.g. L
1
), or numerical expres-
sions over unknown quantities and numbers (e.g.
9?L
1
). For instance, in state 2 of Figure 2, the nu-
merical expression corresponding to Liz is 9?L
1
and corresponding to Joan is J
0
+ L
1
, where J
0
is a variable representing the number of kittens
that Joan has started with.
Hereinafter, we will refer to a generic entity as
e, container as c, number as num, attribute as a.
We represent the relation between a container, an
entity, and a number in the form of a quantity ex-
Category Example
Observation There were 28 bales of hay in the barn.
Positive Joan went to 4 football games this year.
Negative John lost 3 of the violet balloons.
Positive
Transfer
Mike?s dad borrowed 7 nickels from
Mike.
Negative
Transfer
Jason placed 131 erasers in the drawer.
Construct Karen added 1/4 of a cup of walnuts to a
batch of trail mix.
Destroy The rabbits ate 4 of Dan?s potatoes.
Table 1: Examples for different verb categories in sen-
tences. Entities are underlined; containers are italic, and
verbs are bolded.
pression N(c,e). Figure 2 shows the quantity
relations in different world states.
State transitions: Sentences depict progression
of the world state (Figure 2) in the form of ob-
servations of updates of quantities. We assume
that every sentence w consists of a verb v, an en-
tity e, a quantity num (might be unknown), one
or two containers c
1
, c
2
, and attributes a. The
presence of the second container, c
2
, will be dic-
tated by the category of the verb, as we discuss
below. Sentences abstract transitions (s
t
? s
t+1
)
between states in the form of an algebraic opera-
tion of addition or subtraction. For every sentence,
we model the state transition according to the verb
category and containers in the sentence. There are
three verb categories for sentences with one con-
tainer: Observation: the quantity is initialized in
the container, Positive: the quantity is increased
in the container, and Negative: the quantity is de-
creased in the container. Moreover, there are four
categories for sentences with two containers: Pos-
525
itive transfer: the quantity is transferred from the
second container to the first one, Negative trans-
fer: the quantity is transferred from the first con-
tainer to the second one, Construct: the quantity
is increased for both containers, and Destroy: the
quantity is decreased for both containers.
Figure 2 shows how the state transitions are
determined by the verb categories. The sen-
tence ?Liz has 9 black kittens? initializes the
quantity of kittens in the container Liz
to 9. In addition, the sentence ?She gave
some of her kittens to Joan.? shows the
negative transfer of L
1
kittens from Liz to
Joan represented as N(Liz,kitten)=9-L
1
and N(Joan,kitten)=J
0
+ L
1
.
Given a math word problem, ARIS grounds the
world state into entities (e.g., kitten), contain-
ers (e.g., Liz), attributes (e.g., black), and quan-
tities (e.g., 9) (Section 4.1). In addition, ARIS
learns state transitions by classifying verb cate-
gories in sentences (Section 4.2). Finally, from the
world state and transitions, it generates an arith-
metic equation which can be solved to generate the
numeric answer to the word problem.
4 Our Method
In this section we describe how ARIS maps an
arithmetic word problem into an equation (Fig-
ure 2). ARIS consists of three main steps (Fig-
ure 3): (1) grounding the problem into entities and
containers, (2) training a model to classify verb
categories in sentences, and (3) solving the prob-
lem by updating the world states with the learned
verb categories and forming equations.
4.1 Grounding into Entities and Containers
ARIS automatically identifies entities, attributes,
containers, and quantities corresponding to every
sentence fragment (details in Figure 3 step 1). For
every problem, this module returns a sequence of
sentence fragments ?w
1
, . . . , w
T
, w
x
?where every
w
t
consists of a verb v
t
, an entity e
t
, its quantity
num
t
, its attributes a
t
, and up to two containers
c
t
1
, c
t
2
. w
x
corresponds to the question sentence
inquiring about an unknown entity. ARIS applies
the Stanford dependency parser, named entity rec-
ognizer and coreference resolution system to the
problem text (de Marneffe et al., 2006; Finkel et
al., 2005; Raghunathan et al., 2010). It uses the
predicted coreference relationships to replace pro-
nouns (including possessive pronouns) with their
coreferenent links. The named entity recognition
output is used to identify numbers and people.
Entities: Entities are references to some object
whose quantity is observed or changing through-
out the problem. So to determine the set of
entities, we define h as the set of noun types
which have a dependent number (in the depen-
dency parse) somewhere in the problem text. The
set of entities is then defined as all noun phrases
which are headed by a noun type in h. For in-
stance kitten in the first sentence of Figure 1
is an entity because it is modified by the number
9, while kitten in the second sentence of Fig-
ure 1 is an entity because kitten was modified
by a number in the first sentence. Every number
in the text is associated with one entity. Num-
bers which are dependents of a noun are associ-
ated with its entity. Bare numbers (not dependent
on a noun) are associated with the previous entity
in the text. The entity in the last sentence is identi-
fied as the question entity e
x
. Finally, ARIS splits
the problem text into T + 1 sentence fragments
?w
1
, . . . w
T
, w
x
? such that each fragment contains
a single entity and it?s containers. For simplicity
we refer to these fragments as a sentences.
Containers: Each entity is associated with one
or two container noun phrases using the algorithm
described in in Figure 3 step 1c. As we saw earlier
with numbers, arithmetic problems often include
sentences with missing information. For example
in Figure 2, the second container in the the sen-
tence ?Park workers had to cut down 13 walnut
trees that were damaged.? is not explicitly men-
tioned. To handle this missing information, we
use the circumscription assumption (McCarthy,
1980). The circumscription assumption formal-
izes the commonsense assumption that things are
as expected unless otherwise specified. In this set-
ting, we assume that the set of containers are fixed
in a problem. Thus if the container(s) for a given
entity cannot be identified they are set to the con-
tainer(s) for the previous entity with the same head
word. For example in Figure 2 we know from the
previous sentence that trees were in the park.
Therefore, we assume that the unmentioned con-
tainer is the park.
Attributes: ARIS selects attributes A as modifiers
for every entity from the dependency parser (de-
tails in Figure 3 step 1a). For example black is
an attribute of the entity kitten and is an ad-
jective modifier in the parser. These attributes are
526
1. Grounding into entities and containers: for every problem p in dataset (Section 4.1)
(a) ?e
1
, . . . , e
T
, e
x
?
p
? extract all entities and the question entity
i. Extract all numbers and noun phrases (NP).
ii. h ? all noun types which appear with a number as a dependant (in the dependency parse tree) somewhere
in the problem text.
iii. e
t
? all NPs which are headed by a noun type in h.
iv. num
t
? the dependant number of e
t
if one exists. Bare numbers (not directly dependant on any noun
phrase) are associated with the previous entity in the text. All other num
t
are set to unknown.
v. e
x
? the last identified entity.
vi. a
t
? adjective and noun modifiers of e
t
. Update implicit attributes using the previously observed attributes.
vii. v
t
? the verb with the shortest path to e
t
in the dependency parse tree.
(b) ?w
1
, . . . , w
T
, w
x
?
p
? split the problem text into fragments based on the entities and verbs
(c) ?c
t
1
, c
t
2
, . . . , c
T
1
, c
T
2
, c
x
?
p
? the list of containers for each entity
i. c
t
1
? the subject of w
t
.
If w
t
contains There is/are, c
t
1
is the first adverb of place to the verb.
ii. c
t
2
? An NP that is direct object of the verb. If not found, c
t
2
is the object of the first adverbial phrase of
the verb.
iii. Circumscription assumption: When c
t
1
or c
t
2
are not found, they are set to the previous containers.
2. Training for sentence categorization (Section 4.2)
(a) instances
1
, instances
2
? ?
(b) for every sentence w
t
? ?w
1
, . . . , w
T
, w
x
?
p
in the training set:
i. features
t
? extract features (similarity based, WordNet based, structural) (Section 4.2.1)
ii. l
t
1
, l
t
2
? determine labels for containers c
t
1
and c
t
2
based on the verb category of w
t
.
iii. append ?features
t
, l
t,1
?, ?features
t
, l
t,2
? to instances
1
, instances
2
.
(c) M
1
,M
2
? train two SVMs for instances
1
, instances
2
3. Solving: for every problem p in the test set (Section 4.3)
(a) Identifying verb categories in sentences
i. for every sentence w
t
? ?w
1
, . . . , w
T
, w
x
?
p
:
A. features
t
? extract features (similarity based, WordNet based, structural).
B. l
t
1
, l
t
2
? classify w
t
for both containers c
t
1
and c
t
2
using models M
1
,M
2
.
(b) State progression: Form ?s
0
, . . . , s
T
? (Section 4.3.1)
i. s
0
? null.
ii. for t ? ?1, . . . , T ?: s
t
? progress(s
t?1
, w
t
).
A. if e
t
= e
x
and a
t
= a
x
:
if w
t
is an observation: N
t
(c
t
1
, e
t
) = num
t
.
else: update N
t
(c
t
1
, e
t
) and N
t
(c
t
2
, e
t
) given verb categories l
t
1
, l
t
2
.
B. copy N
t?1
(c, e) to N
t
(c, e) for all other (c, e) pairs.
(c) Forming equations and solution (Section 4.3.2)
i. Mark each w
t
that matches with w
x
if:
a) c
t
1
matches with c
x
and verb categories are equal or verbs are similar.
b) c
t
2
matches with c
x
and the verbs are in opposite categories.
ii. x? the unknown quantity if w
x
matches with a sentence introducing an unknown number
iii. If the question asks about an unknown variable x or a start variable (w
x
contains ?begin? or ?start?):
For some container c, find two states s
t
(quantity expression contains x) and s
t+1
(quantity is a known
number). Then, form an equation for x: N
t
(c, e
x
) = N
t+1
(c, e
x
).
iv. else: form equation as x = N
t
(c
x
, e
x
).
v. Solve the equation and return the absolute value of x.
Figure 3: ARIS: a method for solving arithmetic word problems.
used to prune the irrelevant information in pro-
gressing world states.
Arithmetic problems usually include sentences
with no attributes for the entities. For example,
the attribute black has not been explicitly men-
tioned for the kitten in the second sentence. In
particular, ARIS updates an implicit attribute using
the previously observed attribute. For example, in
?Joan went to 4 football games this year. She went
to 9 games last year.?, ARIS assigns football as
an attribute of the game in both sentences.
4.2 Training for Verb Categories
This step involves training a model to identify verb
categories for sentences. This entails predicting
one label (increasing, decreasing) for each (verb,
container) pair in the sentence. Each possible set-
ting of these binary labels corresponds to one of
the seven verb categories discussed earlier. For ex-
ample, if c
1
is increasing and c
2
is decreasing this
is a positive transfer verb.
Our dataset includes word problems from dif-
ferent domains (more details in Section 5.2). Each
verb in our dataset is labeled with one of the 7 cat-
527
egories from Table 1.
For training, we compile a list of sentences from
all the problems in the dataset and split sentences
into training and test sets in two settings. In the
first setting no instance from the same domain
appears in the training and test sets in order to
study the robustness of our method to new prob-
lem types. In the second setting no verb is re-
peated in the training and test sets in order to study
how well our method predicts categories of unseen
verbs.
For every sentence w
t
in the problems, we build
two data instances, (w
t
, c
1
) and (w
t
, c
2
), where c
1
and c
2
are containers extracted from the sentence.
For every instance in the training data, we assign
training labels using the verb categories of the sen-
tences instead of labeling every sentence individu-
ally. The verb can be increasing or decreasing cor-
responding to every container in the sentence. For
positive (negative) and construction (destruction)
verbs, both instances are labeled positive (nega-
tive). For transfer positive (negative) verbs, the
first instance is labeled positive (negative) and the
second instance is labeled negative (positive). For
observation verbs, both instances are labeled pos-
itive. We assume that the observation verbs are
known (total of 5 verbs). Finally, we train Support
Vector Machines given the extracted features and
training labels explained above (Figure 3 step 2).
In the following, we describe the features used for
training.
4.2.1 Features
There are three sets of features: similarity based,
Wordnet-based, and structural features. The first
two sets of features focus on the verb and the third
set focuses on the dependency structure of the sen-
tence. All of our features are unlexicalized. This
allows ARIS to handle verbs in the test questions
which are completely different from those seen in
the training data.
Similarity-based Features: For every instance
(w, c), the feature vector includes similarity be-
tween the verb of the sentence w and a list of seed
verbs. The list of seed verbs is automatically se-
lected from a set V containing the 2000 most com-
mon English verbs using `
1
regularized feature se-
lection technique. We select a small set of seed
verbs to avoid dominating the other feature types
(structural and WordNet-based features).
The goal is to automatically select verbs from
V that are most discriminative for each of the 7
verb categories in Table 1. We define 7 classifi-
cation tasks: ?Is a verb a member of each cate-
gory?? Then, we select the three most represen-
tative verbs for each category. To do so, we ran-
domly select a set of 65 verbs V
l
, from all the verbs
in our dataset (118 in total) and manually anno-
tate the verb categories. For every classification
task, the feature vector X includes the similarity
scores (Equation 1) between the verb v and all the
verbs in the V . We train an `
1
regularized regres-
sion model (Park and Hastie, 2007) over the fea-
ture vector X to learn each category individually.
The number of original (similarity based) features
in X is relatively large, but `
1
regularization pro-
vides a sparse weight vector. ARIS then selects the
three most common verbs (without replacement)
among the features (verbs) with non-zero weights.
This accounts for 21 total seed verbs to be used for
the main classification task. We find that in prac-
tice using this selection technique leads to better
performance than using either all the verbs in V or
using just the 65 randomly selected verbs.
Our method computes the similarity between
two verbs v
1
and v
2
from the similarity between all
the senses (from WordNet) of these verbs (Equa-
tion 1). We compute the similarity between two
senses using linear similarity (Lin, 1998). The
similarity between two synsets sv
1
and sv
2
are pe-
nalized according to the order of each sense for the
corresponding verb. Intuitively, if a synset appears
earlier in the set of synsets of a verb, it is more
likely to be considered as the correct meaning.
Therefore, later occurrences of a synset should re-
sult in reduced similarity scores. The similarity
between two verbs v
1
and v
2
is the maximum sim-
ilarity between two synsets of the verbs:
sim(v
1
, v
2
) = max
sv:synsets(v)
lin-sim(sv
1
, sv
2
)
log(p
1
+ p
2
)
(1)
where sv
1
, sv
2
are two synsets, p
1
, p
2
are the posi-
tion of each synset match, and lin-sim is the linear
similarity. Our experiments show better perfor-
mance using linear similarity compared to other
common similarity metrics (e.g., WordNet path
similarity and Resnik similarity (Resnik, 1995)).
WordNet-based Features: We use WordNet
verb categories in the feature vector. For each
part of speech in WordNet, the synsets are or-
ganized into different categories. There are
15 categories for verbs. Some examples in-
528
clude ?verb.communication?, ?verb.possession?,
and ?verb.creation?. In addition, WordNet in-
cludes the frequency measure f
c
sv
indicating how
often the sense sv has appeared in a reference cor-
pus. For each category i, we define the feature f
i
as the ratio of the frequency of the sense sv
i
over
the total frequency of the verb i.e., f
i
= f
c
sv
i
/f
c
v
.
Structural Features: For structural features, we
use the dependency relations between the verb and
the sentence elements since they can be a good
proxy of the sentence structure. ARIS uses a bi-
nary vector including 35 dependency relations be-
tween the verb and other elements. For example,
in the sentence ?Joan picked 2 apples from the ap-
ple tree?, the dependency between (?picked? and
?tree?) and (?picked? and ?apples?) are depicted as
?prep-from? and ?dobj? relations in the dependency
parser, respectively. In addition, we include the
length of the path in the dependency parse from
the entity to the verb.
4.3 Solving the Problem
So far, ARIS grounds every problem into entities,
containers, and attributes, and learns verb cate-
gories in sentences. Solving the problem consists
of two main steps: (1) progressing states based on
verb categories in sentences and (2) forming the
equation.
4.3.1 State Progression with Verb Categories
This step (Figure 3 step 3b) involves forming
states ?s
1
, . . . , s
T
? by updating quantities in every
container using learned verb categories (Figure 3
step 3a). ARIS initializes s
0
to an empty state. It
then iteratively updates the state s
t
by progressing
the state s
t?1
given the sentence w
t
with the verb
v, entity e, number num, and containers c
1
and c
2
.
For a given sentence t, ARIS attempts to match
e
t
and c
t
to entities and categories in s
t?1
. An
entity/category is matched if has the same head
word and same set of attributes as an existing en-
tity/category. If an entity or category cannot be
matching to one in s
t?1
, then a new one is created
in s
t
.
The progress subroutine prunes the irrelevant
sentences by checking if the entity e and its at-
tributes a agree with the question entity e
x
and its
attributes a
x
in the question. For example both
game entities agree with the question entity in the
problem ?Joan went to 4 football games this year.
She went to 9 games last year. How many football
games did Joan go??. The first entity has an ex-
plicit football attribute, and the second entity
has been assigned the same attribute (Section 4.1).
Even if the question asks about games without
mentioning football, the two sentences will
match the question. Note that the second sentence
would have not been matched if there was an ex-
plicit mention of the ?basketball game? in the sec-
ond sentence.
For the matched entities, ARIS initializes or up-
dates the values of the containers c
1
, c
2
in the state
s
t
. ARIS uses the learned verb categories in sen-
tences (Section 4.2) to update the values of con-
tainers. For an observation sentence w
t
, the value
of c
1
in the state s
t
is assigned to the observed
quantity num. For other sentence types, if the
container c does not match to a container the pre-
vious state, its value is initialized with a start vari-
able C
0
. For example, the container Joan is ini-
tialized with J
0
at the state s
1
(Figure 2). Other-
wise, the values of c
1
and c
2
are updated according
to the verb category in the sentence. For instance,
if the verb category in the sentence is a positive
transfer then N
t
(c
1
, e) = N
t?1
(c
1
, e)? num and
N
t
(c
2
, e) = N
t?1
(c
2
, e) + num where N
t
(c, e)
represents the quantity of e in the container c at
state s
t
(Figure 2).
4.3.2 Forming Equations and Solution
The question entity e
x
can match either to an en-
tity in the final state, or to some unknown gener-
ated during the state progression. Concretely, the
question sentence w
x
asks about the quantity x of
the entity e
x
in a container c
x
at a particular state
s
u
or a transition after the sentence w
u
(Figure 3
step 3c).
To determine if e
x
matches to an unknown vari-
able, we define a matching subroutine between
the question sentence w
x
and every sentence w
t
to check entities, containers, and verbs (Figure 3
step 3(c)i). We consider two cases. 1) When
w
x
contains the words ?begin?, or ?start?, the un-
known variable is about the initial value of an en-
tity, and it is set to the start variable of the con-
tainer c
x
(Figure 3 step 3(c)iii). For example, in
?Bob had balloons. He gave 9 to his friends. He
now has 4 balloons. How many balloons did he
have to start with??, the unknown variable is set to
the start variable B
0
. 2) When the question verb
is not one of the defined set of observation verbs,
ARIS attempts to match e
x
with an unknown in-
troduced by one of the state transitions (Figure 3
529
step 3(c)iii). For example, the second sentence
in Figure 1 introduces an unknown variable over
kittens. The matching subroutine matches this
entity with the question entity since the question
container, i.e. Joan, matches with the second
container and verb categories are complementary.
In order to solve for the unknown variable x,
ARIS searches through consecutive states s
t
and
s
t+1
, where in s
t
, the quantity of e
x
for a container
c is an expression over x, and in s
t+1
, the quan-
tity is a known number for a container matched
to c. It then forms an equation by comparing the
quantities for containers matched between the two
states. In the previous example, the equation will
be B
0
? 9 = 4 by comparing states s
2
and s
3
,
where the numerical expression over balloons
is B
0
?9 in the state s
2
, and the quantity is a known
number in the state s
3
.
When neither of the two above cases apply,
ARIS matches e
x
to an entity in the final state,
s
T
and returns its quantity, (Figure 3 step 3(c)iv).
In the football example of the previous sec-
tion, the equation will be x = N
t
(c
x
, e
x
), where
N
t
(c
x
, e
x
) is the quantity in the final state.
Finally, the equation will be solved for the un-
known variable x and the absolute value of the un-
known variable is returned.
5 Experiments
To experimentally evaluate our method we build
a dataset of arithmetic word problems along with
their correct solutions. We test our method on the
accuracy of solving arithmetic word problems and
identifying verb categories in sentences.
5.1 Experimental Setup
Datasets: We compiled three diverse datasets
MA1, MA2, IXL (Table 2) of Arithmetic word
problems on addition and subtraction for third,
fourth, and fifth graders. These datasets have sim-
ilar problem types, but have different characteris-
tics. Problem types include combinations of ad-
ditions, subtractions, one unknown equations, and
U.S. money word problems. Problems in MA2 in-
clude more irrelevant information compared to the
other two datasets, and IXL includes more infor-
mation gaps. In total, they include 395 problems,
13,632 words, 118 verbs, and 1,483 sentences.
Tasks and Baselines: We evaluate ARIS on two
tasks: 1) solving arithmetic word problems in the
three datasets and 2) classifying verb categories in
Source #Tests Avg.# Sentences
MA1 math-aids.com 134 3.5
IXL ixl.com 140 3.36
MA2 math-aids.com 121 4.48
Table 2: Properties of the datasets.
MA1 IXL MA2 Total
3-fold Cross validation
ARIS 83.6 75.0 74.4 77.7
ARIS
2
83.9 75.4
+
69.8
+
76.5
+
KAZB 89.6 51.1 51.2 64.0
Majority 45.5 71.4 23.7 48.9
Gold sentence categorization
Gold ARIS 94.0 77.1 81.0 84.0
Table 3: Accuracy of solving arithmetic word problems in
three datasets MA1, IXL, and MA2. This table compares
our method, ARIS, ARIS
2
with the state-of-the-art KAZB. All
methods are trained on two (out of three) datasets and tested
on the other one. ARIS
2
is trained when no verb is repeated
in the training and test sets. Gold ARIS uses gold verb cat-
egories. The improvement of ARIS (boldfaced) and ARIS
2
(denoted by
+
) are significant over KAZB and the majority
baseline with p < 0.05.
sentences. We use the percentage of correct an-
swers to the problems as the evaluation metric for
the first task and accuracy as the evaluation metric
for the second task. We use Weka?s SVM (Wit-
ten et al., 1999) with default parameters for clas-
sification which is trained with verb categories in
sentences (as described in Section 4.2).
For the first task, we compare ARIS with
KAZB (Kushman et al., 2014), majority baseline,
ARIS
2
, and Gold ARIS. KAZB requires training
data in the form of equation systems and numeri-
cal answers to the problems. The majority base-
line classifies every instance as increasing. In
ARIS
2
(a variant of ARIS) the system is trained in
a way that no verb is repeated in the training and
test sets. Gold ARIS uses the ground-truth sen-
tence categories instead of predicted ones. For the
second task, we compare ARIS with a baseline that
uses WordNet verb senses.
5.2 Results
We evaluate ARIS in solving arithmetic word
problems in the three datasets and then evaluate its
ability in classifying verb categories in sentences.
5.2.1 Solving Arithmetic Problems
Table 3 shows the accuracy of ARIS in solv-
ing problems in each dataset (when trained on
the other two datasets).Table 3 shows that ARIS
530
significantly outperforms KAZB and the major-
ity baseline. As expected, ARIS shows a larger
gain on the two more complex datasets MA2 and
IXL; our method shows promising results in deal-
ing with irrelevant information (dataset MA2) and
information gaps (dataset IXL). This is because
ARIS learns to classify verb categories in sen-
tences and does not require observing similar pat-
terns/templates in the training data. Therefore,
ARIS is more robust to differences between the
training and test datasets and can generalize across
different dataset types. As discussed in the ex-
perimental setup, the datasets have mathematically
similar problems, but differ in the natural language
properties such as in the sentence length and irrel-
evant information (Table 2).
Table 3 also shows that the sentence categoriza-
tion is performed with high accuracy even if the
problem types and also the verbs are different. In
particular, there are a total of 118 verbs among
which 64 verbs belong to MA datasets and 54 are
new to IXL. To further study this, we train our
method ARIS
2
in which no verb can be repeated
in the training and test sets. ARIS
2
still signifi-
cantly outperforms KAZB. In addition, we observe
only a slight change in accuracy between ARIS
and ARIS
2
.
To further understand our method, we study the
effect of verb categorization in sentences in solv-
ing problems. Table 3 shows the results of Gold
ARIS in solving arithmetic word problems with
gold sentence categorizations. In addition, com-
paring ARIS with Gold ARIS suggests that our
method is able to reliably identify verb categories
in sentences.
We also perform an experiment where we pool
all of the problems in the three datasets and
randomly choose 3 folds for the data (instead
of putting each original dataset into it?s own
fold). We compare our method with KAZBin
this scenario. In this setting, our method?s accu-
racy is 79.5% while KAZB?s accuracy is 81.8%.
As expected, our method?s performance has not
changed significantly from the previous setting,
while KAZB?s performance significantly improves
because of the reduced diversity between the train-
ing and test sets in this scenario.
5.2.2 Sentence Categorization
Table 4 compares accuracy scores of sentence
categorization for our method with different fea-
tures, a baseline that uses WordNet verb senses,
and the majority baseline that assigns every (verb,
container) pair as increasing. Similar to ARIS
2
,
we randomly split verbs into three equal folds
and assign the corresponding sentences to each
fold. No verb is shared between training and test
sets. We then directly evaluate the accuracy of
the SVM?s verb categorization (explained in Sec-
tion 4.2). This table shows that ARIS performs
well in classifying sentence categories even with
new verbs in the test set. This suggests that our
method can generalize well to predict verb cate-
gories for unseen verbs.
Table 4 also details the performance of four
variants of our method that ablate various features
of ARIS. The table shows that similarity, contex-
tual, and WordNet features are all important to
the performance of ARIS in verb categorization,
whereas the WordNet features are less important
for solving the problems. In addition, it shows that
similarity features play more important roles. We
also performed another experiment to study the ef-
fect of the proposed feature selection method for
similarity-based features. The accuracy of ARIS
in classifying sentence categories is 69.7% when
we use all the verbs in V in the similarity feature
vector. This shows that our feature selection algo-
rithm for selecting seed verbs is important towards
categorizing verbs.
Finally, Table 4 shows that our method signif-
icantly outperforms the baseline that only uses
WordNet verb sense. An interesting observation
is that the majority baseline in fact outperforms
WordNet verb senses in verb categorization, but
is significantly worse in solving arithmetic word
problems. In addition, we evaluate the accuracy
of predicting only verb categories by assigning the
verb label according to the majority of its labels
in the sentence categories. The accuracy of verb
categories is 78.2% confirming that ARIS is able
to successfully categorize verbs.
5.2.3 Error Analysis
We analyzed all 63 errors of Gold ARIS and
present our findings in Table 5. There are five ma-
jor classes of errors. In the first category, some in-
formation is not mentioned explicitly and should
be entailed. For example, ?washing cars? is the
source of ?making money?. Despite the improve-
ments that come from ARIS, a large portion of the
errors can still be attributed to irrelevant informa-
tion. For example, ?short? is not a ?toy?. The third
category refers to errors that require knowledge
531
Categorization Solution
ARIS 81.2
+
76.5
+
No similarity features 68.8 65.4
No WordNet features 75.3 78.0
+
No structural features 75.5 72.4
+
Baseline (WordNet) 67.8 68.4
Majority Baseline 73.4 48.9
Table 4: Ablation study and baseline comparisons: this ta-
ble reports the accuracy of verb categorization in sentences
and solutions for ARIS with ablating features. It also pro-
vides comparisons to WordNet and majority baselines. The
improvement of ARIS (boldfaced) and ablations denoted by
+
are statistically significant over the baselines (with p < 0.05)
for both tasks.
Error type Example
Entailment,
Implicit
Action (26%)
Last week Tom had $74. He washed cars
over the weekend and now has $86. How
much money did he make washing cars?
Irrelevant
Information
(19%)
Tom bought a skateboard for $9.46, and
spent $9.56 on marbles. Tom also spent
$14.50 on shorts. In total, how much did
Tom spend on toys?
Set Comple-
tion (13%)
Sara?s school played 12 games this year.
They won 4 games. How many games did
they lose?
Parsing
Issues (21%)
Sally had 27 Pokemon cards. Dan gave
her 41 new Pokemon cards. How many
Pokemon cards does Sally have now?
Others (21%) In March it rained 0.81 inches. It rained
0.35 inches less in April than in March.
How much did it rain in April?
Table 5: Examples of different error categories and relative
frequencies. The cause of error is bolded.
about set completions. For example, the ?played?
games can be split into ?win? and ?lost? games.
Finally, parsing and coreference mistakes are an-
other source of errors for ARIS.
6 Discussions and Conclusion
In this paper we introduce ARIS, a method for
solving arithmetic word problems. ARIS learns
to predict verb categories in sentences using syn-
tactic and (shallow) semantic features from small,
easy-to-obtain training data. ARIS grounds the
world state into entities, sets, quantities, attributes,
and their relations and takes advantage of the cir-
cumscription assumption and successfully fills in
the information gaps. Finally, ARIS makes use
of attributes and discards irrelevant information in
the problems. Together these provide a new rep-
resentation and a learning algorithm for solving
arithmetic word problems.
This paper is one step toward building a sys-
tem that can solve any math and logic word
problem. Our empirical evaluations show that
our method outperforms a template-based learn-
ing method (developed recently by Kushman et al.
(2014)) on solving addition and subtraction prob-
lems with diversity between the training and test
sets. In particular, our method generalizes bet-
ter to data from different domains because ARIS
only relies on learning verb categories which al-
leviates the need for equation templates for arith-
metic problems. In this paper, we have focused
on addition and subtraction problems. However,
KAZB can deal with more general types of prob-
lems such as multiplication, division, and simulta-
neous equations.
We have observed a complementary behavior
between our method and that of Kushman et al.
This suggests a hybrid approach that can bene-
fit from the strengths of both methods while be-
ing applicable to more general problems while ro-
bust to the errors specific to each. In addition, we
plan to focus on incrementally collecting domain
knowledge to deal with missing information gaps.
Another possible direction is to improve parsing
and coreference resolution.
Acknowledgments
The research was supported by the Allen Institute
for AI, and grants from the NSF (IIS-1352249)
and UW-RRF (65-2775). We thank Ben Hixon
and the anonymous reviewers for helpful com-
ments and the feedback on the work.
References
Antoine Bordes, Nicolas Usunier, and Jason Weston. 2010.
Label ranking under ambiguous supervision for learning
semantic correspondences. In Proc. International Confer-
ence on Machine Learning (ICML).
SRK Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina
Barzilay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proc. of the Annual Meeting of the
Association for Computational Linguistics and the Inter-
national Joint Conference on Natural Language Process-
ing of the AFNLP (ACL-AFNLP).
SRK Branavan, Nate Kushman, Tao Lei, and Regina Barzi-
lay. 2012. Learning high-level planning from text. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint Con-
ference on Natural Language Processing of the AFNLP
(ACL-AFNLP).
532
David Chen, Joohyun Kim, and Raymond Mooney. 2010.
Training a multilingual sportscaster: Using perceptual
context to learn language. Journal of Artificial Intelli-
gence Research, 37.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proc. Language
Resources and Evaluation Conference (LREC).
Jacob Eisenstein, James Clarke, Dan Goldwasser, and Dan
Roth. 2009. Reading to learn: Constructing features
from semantic abstracts. In Proc. Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP).
Edward A. Feigenbaum and Julian Feldman, editors. 1963.
Computers and Thought. McGraw Hill, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Ruifang Ge and Raymond J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Proc. of the Annual
Meeting of the Association for Computational Linguistics
(ACL).
Dan Goldwasser and Dan Roth. 2011. Learning from natural
instructions. In Proceedings of International Joint Con-
ference on Artificial Intelligence (IJCAI).
Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth.
2011. Confidence driven unsupervised semantic parsing.
In Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Hannaneh Hajishirzi, Julia Hockenmaier, Erik T. Mueller,
and Eyal Amir. 2011. Reasoning about robocup soccer
narratives. In Proc. Conference on Uncertainty in Artifi-
cial Intelligence (UAI).
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambiguous supervision. In
Proc. Conference of the Association for the Advancement
of Artificial Intelligence (AAAI).
Nate Kushman and Regina Barzilay. 2013. Using seman-
tic unification to generate regular expressions from natu-
ral language. In Proceeding of the Annual Meeting of the
North American Chapter of the Association for Computa-
tional Linguistics.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina
Barzilay. 2014. Learning to automatically solve algebra
word problems. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Iddo Lev, Bill MacCartney, Christopher D. Manning, , and
Roger Levy. 2004. Solving logic puzzles: From robust
processing to precise semantics. In Workshop on Text
Meaning and Interpretation at Association for Computa-
tional Linguistics (ACL).
Iddo Lev. 2007. Packed Computation of Exact Meaning Rep-
resentations. Ph.D. thesis, CS, Stanford University.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learn-
ing semantic correspondences with less supervision. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint Con-
ference on Natural Language Processing of the AFNLP
(ACL-AFNLP).
Dekang Lin. 1998. An information-theoretic definition of
similarity. In Proc. International Conference on Machine
Learning (ICML).
John McCarthy. 1980. Circumscription?a form of non-
monotonic reasoning. Artificial Intelligence, 13.
George A Miller. 1995. Wordnet: a lexical database for en-
glish. Communications of the ACM, 38.
Mee Young Park and Trevor Hastie. 2007. L1-regularization
path algorithm for generalized linear models. Journal of
the Royal Statistical Society: Series B (Statistical Method-
ology), 69.
Hoifung Poon and Pedro Domingos. 2009. Unsupervised se-
mantic parsing. In Proc. Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangara-
jan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky,
and Christopher Manning. 2010. A multi-pass sieve for
coreference resolution. In Proc. Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Philip Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In International joint
conference on Artificial intelligence (IJCAI).
Benjamin Snyder and Regina Barzilay. 2007. Database-text
alignment via structured multilabel classification. In Pro-
ceedings of International Joint Conference on Artificial
Intelligence (IJCAI).
Adam Vogel and Daniel Jurafsky. 2010. Learning to follow
navigational directions. In Proc. of the Annual Meeting of
the Association for Computational Linguistics (ACL).
Ian H Witten, Eibe Frank, Leonard E Trigg, Mark A Hall, Ge-
offrey Holmes, and Sally Jo Cunningham. 1999. Weka:
Practical machine learning tools and techniques with java
implementations.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proc. Confer-
ence on Uncertainty in Artificial Intelligence (UAI).
533
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 207?216,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Using Group History to Identify Character-directed Utterances in
Multi-child Interactions
Hannaneh Hajishirzi, Jill F. Lehman, and Jessica K. Hodgins
hannaneh.hajishirzi, jill.lehman, jkh@disneyresearch.com
Abstract
Addressee identification is an element of all
language-based interactions, and is critical for
turn-taking. We examine the particular prob-
lem of identifying when each child playing an
interactive game in a small group is speak-
ing to an animated character. After analyzing
child and adult behavior, we explore a family
of machine learning models to integrate au-
dio and visual features with temporal group
interactions and limited, task-independent lan-
guage. The best model performs identification
about 20% better than the model that uses the
audio-visual features of the child alone.
1 Introduction
Multi-party interaction between a group of partic-
ipants and an autonomous agent is an important
but difficult task. Key problems include identify-
ing when speech is present, who is producing it, and
to whom it is directed, as well as producing an ap-
propriate response to its intended meaning. Solving
these problems is made more difficult when some
or all of the participants are young children, who
have high variability in language, knowledge, and
behavior. Prior research has tended to look at single
children (Oviatt, 2000; Black et al, 2009) or multi-
person groups of adults (Bohus and Horvitz, 2009a).
We are interested in interactions between animated
or robotic characters and small groups of four to ten
year old children. The interaction can be brief but
should be fun.
Here we focus specifically on the question of de-
ciding whether or not a child?s utterance is directed
to the character, a binary form of the addressee
identification (AID) problem. Our broad goals in
this research are to understand how children?s be-
havior in group interaction with a character differs
from adults?, how controllable aspects of the charac-
ter and physical environment determine participants?
behavior, and how an autonomous character can take
advantage of these regularities.
We collected audio and video data of groups of up
to four children and adults playing language-based
games with animated characters that were under lim-
ited human control. An autonomous character can
make two kinds of AID mistakes: failing to detect
when it is being spoken to, and acting as if it has
been spoken to when it has not. The former can be
largely prevented by having the character use exam-
ples of the language that it can recognize as part of
the game. Such exemplification cannot prevent the
second kind of mistake, however. It occurs, for ex-
ample, when children confer to negotiate the next
choice, respond emotionally to changes in the game
state, or address each other without making eye con-
tact. As a result, models that use typical audio-
visual features to decide AID will not be adequate
in multi-child environments. By including tempo-
ral conversational interactions between group mem-
bers, however, we can both detect character-directed
utterances and ignore the remainder about 20% bet-
ter than simple audio-visual models alone, with less
than 15% failure when being spoken to, and about
20% failure when not addressed.
2 Related Work
Our models explore the use of multimodal features
that represent activities among children and adults
interacting with a character over time. Prior research
has tended to look at single children or multi-person
207
groups of adults and has typically used a less inclu-
sive set of features (albeit in decisions that go be-
yond simple AID).
Use of multimodal features rests on early work
by Duncan and Fiske who explored how gaze and
head and body orientation act as important predic-
tors of AID in human-human interactions (Duncan
and Fiske, 1977). Bakx and colleagues showed that
accuracy can be improved by augmenting facial ori-
entation with acoustic features in an agent?s interac-
tions with an adult dyad (Bakx et al, 2003). Oth-
ers have studied the cues that people use to show
their interest in engaging in a conversation (Gra-
vano and Hirschberg, 2009) and how gesture sup-
ports selection of the next speaker in turn-taking
(Bergmann et al, 2011). Researchers have also
looked at combining visual features with lexical fea-
tures like the parseability of the utterance (Katzen-
maier et al, 2004), the meaning of the utterance, flu-
ency of speech, and use of politeness terms (Terken
et al, 2007), and the dialog act (Matsusaka et al,
2007). However, all use hand-annotated data in their
analysis without considering the difficulty of auto-
matically deriving the features. Finally, prosodic
features have been combined with visual and lexi-
cal features in managing the order of speaking and
predicting the end-of-turn in multi-party interactions
(Lunsford and Oviatt, 2006; Chen and Harper, 2009;
Clemens and Diekhaus, 2009).
Work modeling the temporal behavior of the
speaker includes the use of adjacent utterances (e.g.,
question-answer) to study the dynamics of the dialog
(Jovanovic et al, 2006), the prediction of addressee
based on the addressee and dialog acts in previous
time steps (Matsusaka et al, 2007), and the use of
the speaker?s features over time to predict the qual-
ity of an interaction between a robot and single adult
(Fasel et al, 2009).
Horvitz and Bohus have the most complete (and
deployed) model, combining multimodal features
with temporal information using a system for multi-
party dynamic interaction between adults and an
agent (Bohus and Horvitz, 2009a; Bohus and
Horvitz, 2009b). In (Bohus and Horvitz, 2009a)
the authors describe the use of automatic sensors for
voice detection, face detection, head position track-
ing, and utterance length. They do not model tem-
poral or group interactions in determining AID, al-
though they do use a temporal model for the inter-
action as a whole. In (Bohus and Horvitz, 2009b)
the authors use the speaker?s features for the cur-
rent and previous time steps, but do not jointly track
the attention or behavior of all the participants in the
group. Moreover, their model assumes that the sys-
tem is engaged with at most one participant at a time,
which may be a valid conversational expectation for
adults but is unlikely to hold for children. In (Bo-
hus and Horvitz, 2011), the authors make a similar
assumption regarding turn-taking, which is built on
top of the AID module.
3 User Study
We use a Wizard of Oz testbed and a scripted mix
of social dialog and interactive game play to explore
the relationship between controllable features of the
character and the complexity of interacting via lan-
guage with young children. The games are hosted by
two animated characters (Figure 1, left). Oliver, the
turtle, is the main focus of the social interactions and
also handles repair subdialogs when a game does not
run smoothly. Manny, the bear, provides comic re-
lief and controls the game board, making him the
focus of participants? verbal choices during game
play. The game appears on a large flat-screen dis-
play about six feet away from participants who stand
side-by-side behind a marked line. Audio and video
are captured, the former with both close-talk micro-
phones and a linear microphone array.
Oliver and Manny host two games designed to be
fun and easy to understand with little explicit in-
struction. In Madlibs, participants help create a short
movie by repeatedly choosing one everyday object
from a set of three. The objects can be seen on the
board and Oliver gives examples of appropriate re-
ferring phrases when prompting for a choice. In Fig-
ure 1, for example, he asks, ?Should our movie have
a robot, a monster, or a girl in it?? After five sets of
objects are seen, the choices appear in silly contexts
in a short animation; for instance, a robot babysit-
ter may serve a chocolate pickle cake for lunch. In
Mix-and-Match (MnM), participants choose apparel
and accessories to change a girl?s image in unusual
ways (Figure 1, right). MnM has six visually avail-
able objects and no verbal examples from Oliver, ex-
cept in repair subdialogs. It is a faster-paced game
208
Figure 1: Manny and Oliver host Madlibs and a family play Mix-and-Match
with the immediate reward of a silly change to the
babysitter?s appearance whenever a referring phrase
is accepted by the wizard.
The use of verbal examples in Madlibs is expected
to influence the children?s language, potentially in-
creasing the accuracy of speech recognition and ref-
erent resolution in an autonomous system. The cost
of exemplification is slower pacing because children
must wait while the choices are named. To compen-
sate, we offer only a small number of choices per
turn. Removing exemplification, as in MnM, creates
faster pacing and more variety of choice each turn,
which is more fun but also likely to increase three
types of problematic phenomena: out-of-vocabulary
choices (?the king hat? rather than ?the crown?),
side dialogs to establish a referring lexical item or
phrase (?Mommy, what is that thing??), and the use
of weak naming strategies based on physical fea-
tures (?that green hand?).
The two games are part of a longer scripted se-
quence of interactions that includes greetings, good-
byes, and appropriate segues. Overall, the language
that can be meaningfully directed to the characters is
constrained to a small social vocabulary, yes/no re-
sponses, and choices that refer to the objects on the
board. The wizard?s interface reflects these expec-
tations with buttons that come and go as a function
of the game state. For example, yes and no buttons
are available to the wizard after Oliver asks, ?Will
you help me?? while robot, monster, and girl but-
tons are available after he asks, ?Should our movie
have a robot, a monster, or a girl in it?? The wiz-
ard also has access to persistent buttons to indicate a
long silence, unclear speech, multiple people speak-
ing, or a clear reference to an object not on the board.
These buttons launch Oliver?s problem-specific re-
pair behaviors. The decomposition of functional-
ity in the interface anticipates replacing the wizard?s
various roles as voice activity detector, addressee
identifier, speech recognizer, referent resolver, and
dialog manager in an autonomous implementation.
Although meaningful language to the characters
is highly constrained, language to other participants
can be about anything. In particular, both games
establish an environment in which language among
participants is likely to be about negotiating the turn
(?Brad, do you want to change anything??), nego-
tiating the choice (?Billy, don?t do the boot?) or
commenting on the result (?her feet look strange?).
Lacking examples of referring phrases by Oliver,
MnM also causes side dialogs to discuss how ob-
jects should be named. Naming discussions, choice
negotiation, and comments define the essential dif-
ficulty in AID for our testbed; they are all likely to
include references to objects on the board without
the intention of changing the game state.
3.1 Data collection and annotation
Twenty-seven compensated children (14 male, 13
female) and six adult volunteers participated. Chil-
dren ranged in age from four to ten with a mean of
6.4 years. All children spoke English as a first lan-
guage. Groups consisted of up to four people and
always contained either a volunteer adult or the ex-
perimenter the first time through the activities. If the
experimenter participated, she did not make game
choices. Volunteer adults were instructed to sup-
port their children?s participation in whatever way
felt natural for their family. When time permitted,
children were given the option of playing one or both
209
games again. Those who played a second time were
allowed to play alone or in combination with others,
with or without an adult. Data was collected for 25
distinct groups, the details of which are provided in
Table 5 in the Appendix.
Data from all sessions was hand-annotated with
respect to language, gesture, and head orientation.
Labels were based on an initial review of the videos,
prior research on AID and turn-taking in adults, and
the ability to detect candidate features in our phys-
ical environment. A second person segmented and
labeled approximately one third of each session for
inter-annotator comparison. The redundant third
was assigned randomly from the beginning, middle,
or end of the session in order to balance across social
interactions, Madlibs choices, and MnM choices.
Labels were considered to correspond to the same
audio or video sequence if the segments overlapped
by at least 50%.
For language annotations, audio from the close-
talk microphones was used with the video and seg-
mented into utterances based on pauses of at least
50 msec. Typical mispronunciations for young chil-
dren (e.g., word initial /W/ for /R/) were transcribed
as normal words in plain text; non-standard errors
were transcribed phonologically. Every utterance
was also labeled as being directed to the character
(CHAR) or not to the character (NCHAR). Second
annotators segmented the audio and assigned ad-
dressee but did not re-transcribe the speech. Inter-
annotator agreement for segmentation was 95%
(? = .91), with differences resulting from only
one annotator segmenting properly around pauses
or only one being able to distinguish a given child?s
voice among the many who were talking. For seg-
ments coded by both annotators, CHAR/NCHAR
agreement was 94% (? = .89).
For gesture annotations, video segments were
marked for instances of pointing, emphasis, and
head shaking yes and no. Emphatic gestures were
defined as hand or arm movements toward the screen
that were not pointing or part of grooming motions.
Annotators agreed on the existence of gestures 74%
of the time (? = .49), but when both annotators in-
terpreted movement as a gesture, they used the same
label 98% of the time (? = .96).
For orientation, video was segmented when the
head turned away from the screen and when it turned
back. Rather than impose an a priori duration or an-
gle, annotators were told to use the turn-away label
when the turn was associated with meaningful in-
teraction with a person or object, but not for brief,
incidental head movements. Adults could also have
segments that were labeled as head-incline if they
bent to speak to children. Annotators agreed on the
existence of these orientation changes 83% of the
time (? = .62); disagreements may represent simple
differences in accuracy or differences in judgments
about whether a movement denoted a shift in atten-
tion. Orientation changes coded by both annotators
had the same label 92% of the time (? = .85).
The annotated sessions are a significant portion
of the training and test data used for our models.
Although these data reflect some idiosyncracy due
to human variability in speech perception, gesture
recognition, and, possibly, the attribution of inten-
tion to head movements, they show extremely good
agreement with regard to whether participants were
talking to the character. Even very young chil-
dren in group situations give signals in their speech
and movements that allow other people to determine
consistently to whom they are speaking.
3.2 Analysis of behavior
As intended, children did most of the talking
(1371/1895 utterances, 72%), spoke to the charac-
ters the majority of the time (967/1371, 71%), and
made most of the object choices (666/683, 98%).
Adults generally acted in support roles, with 88%
of all adult utterances (volunteers and experimenter)
directed to the children.
The majority of children?s CHAR utterances
(71%) were object choices. Although the wizard
in our study was free to accept any unambiguous
phrase as a valid choice, an automated system must
commit to a fixed lexicon. In general, the larger
the lexicon, the smaller the probability that a ref-
erence will be out-of-vocabulary, but the greater the
probability that a reference could be considered am-
biguous and require clarification. The lexical entry
for each game object contains the simple descrip-
tion given to the illustrator (?alien hands,? ?pickle?)
and related terms from WordNet (Fellbaum, 1998)
likely to be known by young children (see Table 3 in
the Appendix for examples). In anticipation of weak
naming strategies, MnM entries also contain salient
210
visual features based on the artwork (like color), as
well as the body part the object would replace, where
applicable. Entries for Madlibs objects average 2.75
words; entries for MnM average 5.8. With these
definitions, only 37/666 (6%) of character-directed
choices would have been out-of-vocabulary for a
word-spotting speech recognizer with human accu-
racy. However, Oliver?s use of exemplification has a
strong effect. In Madlibs, 98% of children?s choices
were unambiguous repetitions of example phrases.
In MnM, 92% of choices contained words in the lex-
icon, but only 28% indexed a unique object.
Recognition of referring phrases should be a fac-
tor in making AID decisions only if it helps to discri-
mate CHAR from NCHAR utterances. Object refer-
ences occurred in 62% of utterances to the charac-
ters and only 25% of utterances addressed to other
participants, but again, Oliver?s exemplification mat-
tered. About 20% of NCHAR utterances from chil-
dren in both games and from adults in Madlibs con-
tained object references. In MnM, however, a third
of adults? NCHAR utterances contained object ref-
erences as they responded to children?s requests for
naming advice.
Language is not the only source of information
available from our testbed. We know adults use both
eye gaze and gesture to modulate turn-taking and
signal addressee in advance of speech. Because non-
verbal mechanisms for establishing joint attention
occur early in language development, even children
as young as four might use such signals consistently.
Although we use head movement as an approxima-
tion of eye gaze, we positioned participants side-by-
side to make such movements necessary for eye con-
tact. Unfortunately, the game board constituted too
strong a ?situational attractor? (Bakx et al, 2003).
As in their kiosk environment, our adults oriented
toward the screen much of the time (68%) they were
talking to other participants. Children violated con-
versational convention more often, orienting toward
the screen for 82% of NCHAR utterances.
Gesture information is also available from the
video data and reveals distinct patterns of usage
for children and adults. The average number of
gestures/utterance was more than twice as high in
adults. Children were more likely to use empha-
sis gestures when they were talking to the charac-
ters; adults hardly used them at all. Children?s ges-
tures overlapped with their speech almost 80% of
the time, but adult?s gestures overlapped with their
speech only half the time. Moreover, when children
pointed while talking they were talking to the char-
acters, but when adults pointed while talking they
were talking to the children. Finally, adults shook
their heads when they were talking to children but
not when they were talking to the characters, while
children shook their heads when talking to both.
To maintain an engaging experience, object refer-
ences addressed to the character should be treated as
possible choices, while object references addressed
to other participants should not produce action. In-
teractions that violate this rule too often will be
frustrating rather than fun. While exemplification
in Madlibs virtually eliminated out-of-vocabulary
choices, it could not eliminate detectable object ref-
erences that were not directed to the characters. In
both games, such references were often accompa-
nied by other signs that the character was being ad-
dressed, like orientation toward the board and point-
ing. Using all the cues available, human annotators
were almost always able to agree on who was being
addressed. The next section looks at how well an
autonomous agent can perform AID using only the
cues it can sense, if it could sense them with human
levels of accuracy.
4 Models for Addressee Classification
We cast the problem of automatically identifying
whether an utterance is addressed to the character
(and so may result in a character action) as a binary
classification problem. We build and test a family
of models based on distinct sources of information
in order to understand where the power is coming
from and make it easier for other researchers to com-
pare to our approach. All models in the family are
constructed from Support Vector Machines (SVM)
(Cortes and Vapnik, 1995), and use the multimodal
features in Table 1 to map each 500 msec time slice
of a child?s speech to CHAR or NCHAR. This ba-
sic feature vector combines a subset of the hand-
annotated data (Audio and Visual) with automati-
cally generated data (Prosodic and System events).
We use a time slice rather than a lexical or semantic
boundary for forcing a judgment because in a real-
time interaction decisions must be made even when
211
Audio speech: presence/absence
Prosodic pitch: low/medium/highspeech power: low/medium/high
System event character prompt: presence/absence
Visual orientation: head turn away/backgesture: pointing/emphasis
Table 1: Basic features
lexical or semantic events do not occur.
We consider three additional sources of informa-
tion: group behavior, history, and lexical usage.
Group behavior ? the speech, prosody, head orien-
tation, and gestures of other participants ? is impor-
tant because most of the speech that is not directed
to the characters is directed to a specific person in
the group. History is important both because the side
conversations unfold gradually and because it allows
us to capture the changes to and continuity of the
speaker?s features across time slices. Finally, we use
lexical features to represent whether the participant?s
speech contains words from a small, predefined vo-
cabulary of question words, greetings, and discourse
markers (see Appendix). Because the behavioral
analysis showed significant use of words referring
to choice objects during both CHAR and NCHAR
utterances, we do not consider those words in deter-
mining AID. Indeed, we expect the AID decision to
simplify the task of the speech recognizer by helping
that component ignore NCHAR utterances entirely.
The full set of models is described by adding to
the basic vector zero or more of group (g), word (w),
or history (h) features. We use the notation g[+/-
]w[+/-]h[(time parameters)/-] to indicate the pres-
ence or absence of a knowledge source. The time
parameters vary and will be explained in the con-
text of particular models, below. Although we have
explored a larger portion of the total model space,
we limit our discussion here to representative mod-
els (including the best model) that will demonstrate
the effect of each kind of information on the two
main goals of AID: responding to CHAR utterances
and not responding to NCHAR utterances. There
are eight models of interest, the first four of which
isolate individual knowledge sources:
The Basic model (g-w-h-) is an SVM classifier
trained to generate binary CHAR/NCHAR values
based solely on the features in Table 1. It represents
the ability to predict whether a child is talking to the
A1,t	 ?
GN	 ?
P1	 ?
P2	 ?
P3	 ?
P4	 ?
t-??1,t,t+1	 ?
P1	 ?P1	 ?
t+K	 ?t-??M	 ?
T	 ?
t-??N-??1,t-??N,	 ?t-??N+1	 ?
P1	 ?
P1	 ?
P2	 ?
P3	 ?
P4	 ?
G1	 ?
Individual	 ?sub-??model	 ?
Par?cipants	 ?sub-??models	 ?
Figure 2: The two-layer Group-History model maps
group and individual behavior over a fixed window of
time slices to a CHAR/NCHAR decision at time t. The
decision at time t (A1,t) is based on the participant?s ba-
sic features (P1), the output of the individual?s submodel
(T ) ? which encapsulates the history of the individual
for M previous and K subsequent time slices ? and the
output of N participant submodels, each of which con-
tributes a value based on three times slices.
character independent of speech recognition and fo-
cused on only 500 msecs of that child?s behavior.
The Group model (g+w-h-) incorporates group
information, but ignores temporal and lexical be-
havior. This SVM is trained on an extended feature
vector that includes the basic features for the other
participants in the group together with the speaker?s
feature vector at each time slice.
The History model (g-w-h(N ,K)) considers only
the speaker?s basic features, but includesN previous
and K subsequent time slices surrounding the slice
for which we make the CHAR/NCHAR decision.1
The Word model (g-w+h-) extends the basic vec-
tor to include features for the presence or absence of
question words, greetings, and discourse markers.
The next three models combine pairs of knowl-
edge sources. The Group-Word (g+w+h-) and
History-Word (g-w+h(N ,K)) models are straight-
1A History model combining the speaker?s basic vector over
the previous and current time slices (N = 4 and K = 0) out-
performed a Conditional Random Fields (Lafferty et al, 2001)
model with N + 1 nodes representing consecutive time slices
where the last node is conditioned on the previous N nodes.
212
forward extensions of their respective base models,
created by adding lexical features to the basic vec-
tors. The Group-History model (g+w-h(N ,K,M ))
is more complex. It is possible to model group in-
teractions over time by defining a new feature vector
that includes all the participants? basic features over
multiple time slices. As we increase the number of
people in a group and/or the number of time slices to
explore the model space, however, the sheer size of
this simple combination of feature vectors becomes
unwieldy. Instead we make the process hierarchical
by defining the Group-History as a two-layer SVM.
Figure 2 instantiates the Group-History model for
participant P1 playing in a group of four. In the con-
figuration shown, the decision for P1?s utterance at
time t is based on behavior during N previous and
K subsequent time slices, meaning each decision is
delayed by K time slices with respect to real time.
The CHAR/NCHAR decision for time slice t de-
pends on P1?s basic feature vector at time t, the out-
put from the Individual submodel for time t, and the
outputs from the Participants submodel for each of
the time slices through t. A concrete instantiation of
the model can be seen in Figure 4 in the Appendix.
The Individual submodel is an SVM that assigns a
score to the composite of P1?s basic feature vectors
across a window of time (here,M+K+1). The Par-
ticipants submodel is an SVM that assigns a score to
the basic features of all members during each three
slice sliding subwindow in the full interval. More
intuitively: the Individual submodel finds correla-
tions among the child?s observable behaviors over
a window of time; the Participants submodel cap-
tures relationships between members? behaviors that
co-occur over small subwindows; and the Group-
History model combines the two to find regularities
that unfold among participants over time, weighted
toward P1?s own behavior.
The final model of interest, Group-History-Word
(g+w+h(N ,K,M ,Q)), incorporates the knowledge
from all sources of information. A Lexical submodel
is added to the Individual and Participants submod-
els described above. The Lexical submodel is an
SVM classifier trained on the combination of ba-
sic and word features for the current and Q previ-
ous time slices. The second layer SVM is trained on
the scores of the Individual, Participants, and Lex-
ical submodels as well as the combined basic and
Model Max f1 AUC TPR TNR
Basic features
g-w-h- 0.879 0.504 0.823 0.604
g+w-h- 0.903 0.588 0.872 0.650
g-w-h(8,1) 0.897 0.626 0.867 0.697
g+w-h(4,1,8) 0.903 0.645 0.849 0.730
Basic + Word features
g-w+h- 0.904 0.636 0.901 0.675
g+w+h- 0.906 0.655 0.863 0.728
g-w+h(8,1) 0.901 0.661 0.886 0.716
g+w+h(4,1,8,4) 0.913 0.701 0.859 0.786
Table 2: Comparison of models
word feature vector for the child.
5 Results and Discussions
We used the LibSVM implementation (Chang and
Lin, 2011) for evaluation, holding out one child?s
data at a time during training, and balancing the
data set to compensate for the uneven distribution
of CHAR and NCHAR utterances in the corpus. As
previously noted, we used a time slice of 500 msec
in all results reported here. Where history is used,
we consider only models with a single time slice of
look-ahead (K = 1) to create minimal additional de-
lay in the character?s response.
Table 2 reports average values, for each model
and over all sets of remaining children, in terms of
Max F1, true positive rate (TPR), true negative rate
(TNR), and area under the TPR-TNR curve (AUC).
TPR represents a model?s ability to recognize utter-
ances directed to the character; low TPR means chil-
dren will not be able to play the game effectively.
TNR indicates a model?s ability to ignore utterances
directed to other participants; low TNR means that
the character will consider changing the game state
when it hasn?t been addressed.
Table 2 (top) shows comparative performance
without the need for any speech recognition. F1
and TPR are generally high for all models. Using
only the basic features, however, gives a relatively
low TNR and an AUC that is almost random. The
History model, (g-w-h(8,1)), increased performance
across all measures compared to the basic features
(g-w-h-). We found that the History model?s per-
formance was best when four seconds of the past
were considered. Group information within a single
time slice also improves performance over the ba-
sic features, but the Group-History model has the
213
best overall tradeoff in missed CHAR versus ig-
nored NCHAR utterances (AUC). Group-History?s
best performance is achieved using two seconds of
group information from the past via the Participants
submodel and four seconds of the speaker?s past
from the Individual submodel.
Comparing the top and bottom halves of Table 2
shows that all models benefit from accurate recogni-
tion of a small set of task-independent words. The
table shows that word spotting improves both TPR
and TNR when added to the Basic model, but tends
to improve only TNR when added to models with
group and history features. Improved TNR probably
results from the ability to detect NCHAR utterances
when participants are facing the characters and/or
pointing during naming discussions and comments.2
Table 2 shows results averaged over each held out
child. We then recast this information to show, by
model, the percentage of children that would expe-
rience TPR and TNR higher than given thresholds.
Figure 3 shows a small portion of a complete graph
of this type; in this case the percentage of children
who would experience greater than 0.6 for TPR and
greater than 0.5 for TNR under each model. TPR
and TNR lines for a model have the same color and
share a common pattern.
Better models have higher TPR and TNR for more
children. The child who has to keep restating his or
her choice (poor TPR) will be frustrated, as will the
child who has the character pre-emptively take his
or her choice away by ?overhearing? side discus-
sions (poor TNR). While we do not know for any
child (or any age group) how high a TPR or TNR is
required to prevent frustration, Figure 3 shows that
without lexical information the Group-History and
Group models have the best balance for the thresh-
olds. Group-History gives about 85% of the children
a TPR ? 0.7 for a TNR ? 0.5. The simpler Group
model, which has no 500 msec delay for lookahead,
can give a better TPR for the same TNR but for only
75% of the children. When we add lexical knowl-
edge the case for Group-History becomes stronger,
as it gives more than 85% of children a TPR ? 0.7
for a TNR ? 0.6, while Group gives 85% of chil-
dren about the same TPR with a TNR ? 0.5.
2Results showing the affect of including object choice words
in the w+ models are given in Figure 4 in the Appendix.
Figure 3: The percentage of children experiencing dif-
ferent TPR/TNR tradeoffs in models with (bottom) and
without (top) lexical knowledge. The g-w-h- model does
not fall in the region of interest unless lexical features are
used.
6 Conclusions and Future Work
The behavior of the characters, types of games,
group make up, and physical environment all con-
tribute to how participants communicate over time
and signal addressee. We can manipulate some re-
lationships (e.g., by organizing the spatial layout to
promote head movement or having the character use
examples of recognizable language) and take ad-
vantage of others by detecting relevant features and
learning how they combine as behavior unfolds. Our
best current model uses group and history informa-
tion as well as basic audio-visual features to achieve
a max F1 of 0.91 and an AUC of 0.70. Although
this model does not yet perform as well as human
annotators, it may be possible to improve it by tak-
ing advantage of additional features that the behav-
ioral data tells us are predictive (e.g., whether the
speaker is an adult or child). Such additional sources
of information are likely to be important as we re-
place the annotated data with automatic sensors for
speech activity, orientation, and gesture recognition,
and embed addressee identification in the larger con-
text of turn-taking and full autonomous interaction.
214
References
I. Bakx, K. van Turnhout, and J. Terken. 2003. Facial
orientation during multi-party interaction with infor-
mation kiosks. pages 701?704.
K. Bergmann, H. Rieser, and S. Kopp. 2011. Regu-
lating dialogue with gestures towards an empirically
grounded simulation with conversational agents. In
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue (SIGDIAL), pages 88?97.
Matthew Black, Jeannette Chang, Jonathan Chang, and
Shrikanth S. Narayanan. 2009. Comparison of child-
human and child-computer interactions based on man-
ual annotations. In Proceedings of the Workshop on
Child, Computer and Interaction, Cambridge, MA.
D. Bohus and E. Horvitz. 2009a. Dialog in the open
world: Platform and applications. In Proceedings
of the International Conference on Multimodal Inter-
faces (ICMI), pages 31?38.
D. Bohus and E. Horvitz. 2009b. Learning to predict en-
gagement with a spoken dialog system in open-world
settings. In Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL), pages
244?252.
D. Bohus and E. Horvitz. 2011. Multiparty turn taking
in situated dialog: Study, lessons, and directions. In
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue (SIGDIAL), pages 98?109.
C. Chang and C. Lin. 2011. LIBSVM: A library for sup-
port vector machines. ACM Transaction on Intelligent
Systems and Technologies, 2:27:1?27:27.
L. Chen and M. Harper. 2009. Multimodal floor con-
trol shift detection. In Proceedings of the International
Conference on Multimodal Interfaces (ICMI).
C. Clemens and C. Diekhaus. 2009. Prosodic turn-
yielding cues with and without optical feedback. In
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue (SIGDIAL), pages 107?110.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine Learning Journal, 20.
S. Duncan and D. W. Fiske. 1977. Face-to-Face Interac-
tion: Research, Methods and Theory. Lawrence Erl-
baum.
I. Fasel, M. Shiomi, T. Kanda, N. Hagita, P. Chadu-
taud, and H. Ishiguro. 2009. Multi-modal features
for real-time detection of human-robot interaction cat-
egories. In Proceedings of the International Confer-
ence on Multimodal Interfaces (ICMI), pages 15?22.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
A. Gravano and J. Hirschberg. 2009. Turn-yielding
cues in task-oriented dialogue. In Annual Meeting of
the Special Interest Group on Discourse and Dialogue
(SIGDIAL), pages 253?261.
N. Jovanovic, H.J.A. op den Akker, and A. Nijholt. 2006.
Addressee identification in face-to-face meetings. In
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL), pages
169?176.
M. Katzenmaier, R. Steifelhagen, and T. Schultz. 2004.
Identifying the addressee in human-human-robot inter-
actions based on head pose and speech. In Proceed-
ings of the International Conference on Multimodal
Interfaces (ICMI), pages 144?151.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the International Conference onMa-
chine Learning (ICML), pages 282?289.
R. Lunsford and S. Oviatt. 2006. Human perception
of intended addressee during computer assisted meet-
ings. In Proceedings of the International Conference
on Multimodal Interfaces (ICMI), pages 20?27.
Y. Matsusaka, M. Enomoto, and Y. Den. 2007. Simul-
taneous prediction of dialog acts and address types in
three party conversations. In Proceedings of the Inter-
national Conference on Multimodal Interfaces (ICMI),
pages 66?73.
Sharon Oviatt. 2000. Talking to thimble jellies: chil-
dren?s conversational speech with animated characters.
pages 877?880.
J. Terken, I. Joris, and L. de Valk. 2007. Multimodal
cues for addressee hood in triadic communication with
a human information retrieval agent. In Proceedings
of the International Conference on Multimodal Inter-
faces (ICMI).
215
7 Appendix
Object Choice Words
antler, antlers, horn, horns, ear,
ears, head, brown
astronaut, astronauts, space,
spaceman, spacemans, space-
men, helmet, head
bear, bears claw, claws, paw,
paws, hand, hands, brown
bunny, rabbit, bunnies, rabbits,
slipper, slippers, foot, feet,
white
Task-independent Words
Discourse marker hmm, mm, mmm, ok, eww,
shh, oopsy
Question words what, let, where, who, which,
when
Greetings hi, hello, bye, goodbye
Table 3: Excerpts from the dictionary for task-specific
and task-independent words
Model Max f1 AUC TPR TNR
Greeting, question & discourse words
g-w+h- 0.904 0.636 0.901 0.675
g+w+h- 0.906 0.655 0.863 0.728
g-w+h(8,1) 0.901 0.661 0.886 0.716
g+w+h(4,1,8,4) 0.913 0.701 0.859 0.786
With object reference words added
g-w+h- 0.894 0.576 0.777 0.768
g+w+h- 0.898 0.623 0.782 0.773
g-w+h(7,1) 0.910 0.642 0.838 0.783
g+w+h(4,1,8,4) 0.912 0.685 0.834 0.799
Table 4: The effect of adding object reference words
A1,4	 ?
G1	 ?
5	 ?
T	 ? P1	 ?G2	 ?
Individual	 ?sub-??model	 ?
Par?cipants	 ?sub-??models	 ?
4	 ?3	 ?
P4	 ?
P3	 ?
P2	 ?
P1	 ? P1	 ? P1	 ? P1	 ?
P4	 ?
P3	 ?
P2	 ?
P1	 ?
P4	 ?
P3	 ?
P2	 ?
P1	 ?
4	 ?3	 ?2	 ?
P4	 ?
P3	 ?
P2	 ?
P1	 ?
P4	 ?
P3	 ?
P2	 ?
P1	 ?
P4	 ?
P3	 ?
P2	 ?
P1	 ?
5	 ?4	 ?3	 ?
Figure 4: A concrete representation for the Group-
History model with N = 2, M = 1, and K = 1 at
time step t = 4. The value at t = 4 is delayed one time
slice of real time.
Session
Type
Group: participant(age) Duration
full p1(5), experimenter 9 min
full p2(7), p3(6), p6(adult) 9 min
full p4(7), p5 (4), p6(adult) 9 min
replay p2(7), p3(6), p4(7), p5(4) 8 min
full p7(10), experimenter 8 min
replay p7(10) 6 min
full p8(9), p9(8), experimenter 9 min
full p10(10), p11(5), experimenter 11 min
full p12(6), p14(adult) 11 min
full p13(4), p14(adult) 11 min
full p15(4), experimenter 8 min
full p16(9), p17(7), experimenter 12 min
replay p16(9), experimenter 3 min
full p18(8), p19(6), p20(8),
p21(adult)
12 min
full p22(5), experimenter 9 min
replay p22(5), experimenter 3 min
full p25(6), experimenter 9 min
full p26(8), p27(4), experimenter 11 min
replay p26(8), experimenter 6 min
full p28(7), p29(adult) 12 min
full p30(5), experimenter 11 min
replay p30(5), experimenter 4 min
full p31(6), p32(5), p33(adult) 10 min
full p34(4), p35(adult) 9 min
replay p34(4), p35(adult) 4 min
Table 5: Details for sessions used in the analysis (does
not include five sessions with corrupted data)
216
