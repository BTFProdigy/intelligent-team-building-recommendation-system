  
Shallow Semantic Parsing of Chinese 
 
 
Honglin Sun1 
Center for Spoken Language Research 
University of Colorado at Boulder 
Daniel Jurafsky2 
Center for Spoken Language Research 
University of Colorado at Boulder 
 
Abstract 
 
In this paper we address the question of assigning 
semantic roles to sentences in Chinese. We show 
that good semantic parsing results for Chinese can 
be achieved with a small 1100-sentence training set. 
In order to extract features from Chinese, we 
describe porting the Collins parser to Chinese, 
resulting in the best performance currently reported 
on Chinese syntactic parsing; we include our head-
rules in the appendix. Finally, we compare English 
and Chinese semantic-parsing performance. While 
slight differences in argument labeling make a 
perfect comparison impossible, our results 
nonetheless suggest significantly better 
performance for Chinese. We show that much of 
this difference is due to grammatical differences    
between English and Chinese, such as the 
prevalence of passive in English, and the strict 
word order constraints on adjuncts in Chinese. 
 
 
1  Introduction 
 
Thematic roles (AGENT, THEME,  LOCATION, etc) 
provide a natural level of shallow semantic 
representation for a sentence.  A number of algorithms 
have been proposed for automatically assigning such 
shallow semantic structure to English sentences. But 
little is understood about how these algorithms may 
perform in other languages, and in general the role of 
language-specific idiosyncracies in the extraction of 
semantic content and how to train these algorithms 
when large hand-labeled training sets are not available. 
In this paper we address the question of assigning 
semantic roles to sentences in Chinese.   Our  work  is  
1 Currently at Department of Computer Science, Queens 
College, City University of New York. Email: sunh@qc.edu. 
2 Currently at Department of Linguistics, Stanford University. 
Email: jurafsky@stanford.edu. 
 
based on the SVM-based algorithm proposed for 
English by Pradhan et al(2003).  We first describe our 
creation of a small 1100-sentence Chinese corpus 
labeled according to principles from the English and 
(in-progress) Chinese PropBanks. We then introduce 
the features used by our SVM classifier, and show their 
performance on semantic parsing for both seen and 
unseen verbs, given hand-corrected (Chinese TreeBank) 
syntactic parses.  We then describe our port of the 
Collins (1999) parser to Chinese.  Finally, we apply our 
SVM semantic parser to a matching English corpus, 
and discuss the differences between English and 
Chinese that lead to significantly better performance on 
Chinese. 
 
2  Semantic Annotation and the Corpus 
 
Work on semantic parsing in English has generally 
related on the PropBank, a portion of the Penn 
TreeBank in which the arguments of each verb are 
annotated with semantic roles. Although a project to 
produce a Chinese PropBank is underway (Xue and 
Palmer 2003), this data is not expected to be available 
for another year. For these experiments, we therefore 
hand-labeled a small corpus following the Penn 
Chinese Propbank labeling guidelines (Xue, 2002). In 
this section, we first describe the semantic roles we 
used in the annotation and then introduce the data for 
our experiments. 
 
2.1  Semantic roles  
Semantic roles in the English (Kingsbury et al2002) 
and Chinese (Xue 2002) PropBanks are grouped into 
two major types: 
(1) arguments, which represent central participants in 
an event. A verb may require one, two or more 
arguments and they are represented with a contiguous 
sequence of numbers prefixed by arg, as arg0, arg1.  
(2) adjuncts, which are optional for an event but supply 
more information about an event, such as time, location, 
reason, condition, etc. An adjunct role is represented 
with argM plus a tag. For example, argM-TMP stands 
for temporal, argM-LOC for location. 
   In our corpus three argument roles and 15 adjunct 
roles appear. The whole set of roles is given at Table 1. 
 
Table1  The list of semantic roles 
Role Freq 
train 
Freq 
Test 
Note 
arg0  556 63  
arg1 872 91  
arg2  23 5  
argM-ADV 191 32 adverbial 
argM-BFY 26 2 beneficiary(e.g. give 
support [to the plan]) 
argM-CMP 35 3 object to be compared
argM-CND 14 1 condition 
argM-CPN 7 3 companion (e.g. talk 
[with you]) 
argM-DGR 53 4 degree 
argM-FRQ 3 0 frequency 
argM-LOC 207 31 location 
argM-MNR 10 1 manner 
argM-PRP 11 0 purpose or reason 
argM-RNG  7 2 range(e.g. help you [in 
this aspect]) 
argM-RST    15 1 result(e.g. increase [to 
$100]) 
argM-SRC
   
11 1 source(e.g. increase 
[from $50] to $100) 
argM-TMP 376 45 temporal 
argM-TPC 12 2 topic 
 
2.2 The training and test sets 
We created our training and test corpora by choosing 
10 Chinese verbs, and then selecting all sentences 
containing these 10 verbs from the 250K-word Penn 
Chinese Treebank 2.0. We chose the 10 verbs by 
considering frequency, syntactic diversity, and word 
sense.  We chose words that were frequent enough to 
provide sufficient training data. The frequencies of the 
10 verbs range from 41 to 230, with an average of 114. 
We chose verbs that were representative of the variety 
of verbal syntactic behavior in Chinese, including verbs 
with one, two, and three arguments, and verbs with 
various patterns of argument linking. Finally, we chose 
verbs that varied in their number of word senses.  
In total, we selected 1138 sentences.  The first author 
then labeled each verbal argument/adjunct in each 
sentence with a role label. We created our training and 
test sets by splitting the data for each verb into two 
parts: 90% for training and 10% for test. Thus there are 
1025 sentences in the training set and 113 sentences in 
the test set, and each test set verb has been seen in the 
training set. The list of verbs chosen and their number 
of senses, argument numbers and frequencies are given 
in Table 2. 
 
Table 2    List of verbs for experiments 
Verb # of 
senses 
Arg 
number 
Freq 
??/set up 1 2 106 
??/emerge 1 1 80 
??/publish 1 2 113 
??/give 2 3/2 41 
??/build into 2 2/3 113 
??/enter 1 2 123 
??/take place 1 2 230 
??/pass 3 2 75 
??/hope 1 2 90 
??/increase 1 2 167 
 
3  Semantic Parsing 
 
3.1 Architecture and Classifier 
Following the architecture of earlier semantic parsers 
like Gildea and Jurafsky (2002), we treat the semantic 
parsing task as a 1-of-N classification problem.  For 
each (non-aux/non-copula) verb in each sentence, our 
classifier examines each node in the syntactic parse tree 
for the sentence and assigns it a semantic role label. 
Most constituents are not arguments of the verb, and so 
the most common label is NULL.  Our architecture is 
based on a Support Vector Machine classifier,  
following Pradhan et al (2003). Since SVMs are binary 
classifiers, we represent this 1-of-19 classification 
problem (18 roles plus NULL) by training 19 binary 
one-versus-all classifiers.  
Following Pradhan et al (2003), we used tinySVM 
along with YamCha (Kudo and Matsumoto 2000, 2001) 
as the SVM training and test software.  The system 
uses a polynominal kernel with degree 2; the cost per 
unit violation of the margin, C=1; tolerance of the 
termination criterion e=0.001. 
 
3.2 Features 
The literature on semantic parsing in English relies on 
a number of features extracted from the input sentence 
and its parse. These include the constituent?s syntactic 
phrase type, head word, and governing category, the 
syntactic path in the parse tree connecting it to the verb,  
whether the constitutent is before or after the verb,  the 
subcategorization bias of the verb, and the voice 
(active/passive) of the verb. We investigated each of 
these features in Chinese; some acted quite similarly to 
English, while others showed interesting differences. 
Features that acted similarly to English include the 
target verb, the  phrase type,  the syntactic category of 
the constituent. (NP, PP, etc), and the subcategorization 
of the target verb.  The sub-categorization feature 
represents the phrase structure rule for the verb phrase 
containing the target verb (e.g., VP -> VB NP, etc). 
Five features (path, position, governing category, 
headword, and voice) showed interesting patterns that 
are discussed below. 
 
3.2.1 Path in the syntactic parse tree. The path 
feature represents the path from a constituent to the 
target verb in the syntactic parse tree, using "^" for 
ascending a parse tree, and "!" for descending. This 
feature manifests the syntactic relationship between the 
constituent and the target verb. For example the path 
?NP^IP!VP!VP!VV? indicates that the constituent is an 
?NP? which is the subject of the predicate verb. In 
general, we found the path feature to be sparse. In our 
test set, 60% of path types and 39% of path tokens are 
unseen in the training. The distributions of paths are 
very uneven. In the whole corpus, paths for roles have 
an average frequency of 14.5 while paths for non-roles 
have an average of 2.7. Within the role paths, a small 
number of paths account for majority of the total 
occurrences; among the 188 role path types, the top 20 
paths account for 86% of the tokens. Thus, although 
the path feature is sparse, its sparsity may not be a 
major problem in role recognition.  Of the 291 role 
tokens in our test set, only 9 have unseen paths, i.e., 
most of the unseen paths are due to non-roles. 
 
Table 3   The positional distribution of roles 
 
Role 
 
 
Before 
verb 
 
 
After 
verb 
 
 
Total 
 
arg0 
arg1 
arg2 
argM-ADV 
argM-BFY 
argM-CMP 
argM-CND 
argM-CPN 
argM-DGR 
argM-FRQ 
argM-LOC 
argM-MNR 
argM-PRP 
argM-RNG 
argM-RST 
argM-SRC 
argM-TMP 
argM-TPC 
 
547
319
223
28
38
15
10
233
11
11
9
12
408
14
 
72 
644 
28 
 
 
 
 
 
57 
3 
5 
 
 
 
16 
 
13 
 
 
619
963
28
223
28
38
15
10
57
3
238
11
11
9
16
12
421
14
 
Total 
 
1878
 
838 
 
2716   
 
 
3.2.2 Position before or after the verb. The position 
feature indicates that a constituent is before or after the 
target verb. In our corpus, 69% of the roles are before 
the verb while 31% are after the verb. As in English, 
the position is a useful cue for role identity. For 
example, 88% of arg0s are before the verb, 67% of 
arg1s are after the verb and all the arg2s are after the 
verb. Adjuncts have even a stronger bias. Ten of the 
adjunct types can only occur before the verb, while 
three are always after the verb. The two most common 
adjunct roles, argM-LOC and argM-TMP are almost 
always before the verb, a sharp difference from English. 
The details are shown seen in Table 3.  
 
3.2.3 Governing Category. The governing category 
feature is only applicable for NPs.  In the original 
formulation for English in Gildea and Jurafsky (2002), 
it answers the question: Is the NP governed by IP or 
VP? An NP governed by an IP is likely to be a subject, 
while an NP governed by a VP is more likely to be an 
object.  For Chinese, we added a third option in which 
the governing category of an NP is neither IP nor VP, 
but an NP. This is caused by the ?DE? construction, in 
which a clause is used as a modifier of an NP. For 
instance, in the example indicated in Figure 1, for the 
last NP, ??????????(?international Olympic 
conference?)  the parent node is NP, from where it goes 
down to the target verb ????(?taking place?).   
                                                
NP 
                                          
                         
                             CP 
                         
           VP                                                        NP 
                                              DEC                  
 
          ?????              ?       ???????? 
     in Paris take place          DE      intl Olympic conf. 
 
  ?the international Olympic Conference held in Paris? 
Figure 1  Example of  DE construction 
 
Since the governing category information is encoded in 
the path feature, it may be redundant; indeed this 
redundancy might explain why the governing category 
feature was used in Gildea & Jurafsky(2002) but not in 
Gildea and Palmer(2002). Since the ?DE? construction 
caused us to modify the feature for Chinese, we 
conducted several experiments to test whether the 
governing category feature is useful or whether it is 
redundant with the path and position features. Using 
the paradigm to be described in section 3.4, we found a 
small improvement using governing category, and so 
we include it in our model. 
 
3.2.4 Head word and its part of speech. The head 
word is a useful but sparse feature. In our corpus, of the 
2716 roles, 1016 head words (type) are used, in which 
646 are used only once. The top 20 words are given in 
Table 4. 
 
 
Table 4   Top 20 head words for roles 
Word Freq Word Freq
?/in  214 ??/China 25 
??/meeting 43 ?/for 23 
??/today 41 ??/statement 19 
?/at 40 ??/speech 18 
?/already 38 ??/stage 17 
??/enterprise 35 ??/government 16 
??/company 32 ??/present 16 
?/than  31 ??/bank 15 
?/will  30 ??/recently 14 
??/ceremony
  
28
  
??/base 14 
In the top 20 words, 4 are prepositions (??/in??
/at??/than??/for?) and 3 are temporal nouns(??
? /today??? /present??? /recently?) and 2 are 
adverbs(?? /already, ? /will?). These closed class 
words are highly correlated with specific semantic 
roles. For example,"?/for" occurs 195 times as the 
head of a constituent, of which 172 are non-roles, 19 
are argM-BFYs, 3 are arg1s and 1 is an argM-TPC."?
/in" occurs 644 times as a head, of which 430 are non-
roles, 174 are argM-LOCs, 24 are argM-TMPs, 9 are 
argM-RNGs, and 7 are argM-CND. "? /already" 
occurs 135 times as a head, of which 97 are non-roles 
and 38 are argM-ADVs. "??/today" occurs 69 times 
as a head, of which 41 are argM-TMPs and 28 are non-
roles. 
Within the open class words, some are closely 
correlated to the target verb. For example, "??
/meeting; conference" occurs 43 times as a head for 
roles, of which 24 are for the target "??/take place" 
and 19 for "??/pass". "??/ceremony" occurs 28 
times and all are arguments of "??"(take place)."?
?/statement" occurs 19 times, 18 for "??/release; 
publish" and one for "??/hope".   
These statistics emphasize the key role of the 
lexicalized head word feature in capturing the 
collocation between verbs and their arguments. Due to 
the sparsity of the head word feature, we also use the 
part-of-speech of the head word, following Surdeanu et 
al (2003). For example, ?7? 26?/July 26? may not 
be seen in the training,  but its POS, NT(temporal 
noun) , is a good indicator that it is a temporal. 
 
3.2.5  Voice. The passive construction in English gives 
information about surface location of arguments. In 
Chinese the marked passive voice is indicated by the 
use of the preposition "?/by" (POS tag LB in Penn 
Chinese Treebank). This passive, however, is seldom 
used in Chinese text. In our entire 1138-sentence 
corpus, only 13 occurrences of "LB" occur, and only 
one (in the training set) is related to the target verb. 
Thus we do not use the voice feature in our system. 
 
3.3 Experimental Results for Seen Verbs 
We now test the performance of our classifier, trained 
on the 1025-sentence training set and tested on the 113-
sentence test set introduced in Section 2.2. Recall that 
in this ?stratified? test set, each verb has been seen in 
the training data. The last row in Table 5 shows the 
current best performance of our system on this test set. 
The preceding rows show various subsets of the feature 
set, beginning with the path feature. 
 
Table 5  Semantic parsing results on seen verbs 
feature set                             P              R           F 
                                             (%)         (%)        (%) 
path                                     71.8        59.4       65.0 
path + pt                              72.9        62.9       67.5 
path + position                    72.5  60.8  66.2 
path + head POS                 77.6  63.3        69.7 
path + sub-cat                      80.8       63.6        71.2 
path + head word                 85.0       66.0  74.3 
path + target verb                85.8  68.4  76.1 
path + pt + gov + position 
        + subcat + target 
        + head word  
        + head POS                  91.7       76.0        83.1 
 
As Table 5 shows, the most important feature is path, 
followed by target verb and head word.  In general, the 
lexicalized features are more important than the other 
features. The combined feature set outperforms any 
other feature sets with less features and it has an F-
score of 83.1. The performance is better for the 
arguments (i.e., only ARG0-2), 86.7 for arg0 and 89.4 
for arg1. 
 
3.4 Experimental Results for Unseen Verbs 
To test the performance of the semantic parser on 
unseen verbs, we used cross-validation, selecting one 
verb as test and the other 9 as training, and iterating 
with each verb as test. All the results are given in Table 
6. The results for some verbs are almost equal to the 
performance on seen verbs. For example for ???? 
and ????, the F-scores are over 80. However, for 
some verbs, the results are much worse. The worst case 
is the verb ????, which has an F-score of 11.  This is 
due to the special syntactic characteristics of this verb. 
This verb can only have one argument and this 
argument most often follows the verb, in object 
position. In the surface structure, there is often an NP 
before the verb working as its subject, but semantically 
this subject cannot be analyzed as arg0.  For example: 
(1)??/China ?/not ?/will ??/emerge ??/food 
??/crisis.  (A food crisis won't emerge in China.) 
(2)??/Finland ??/economy ??/emerge  ?/AUX  
?? /post-war ? /most ?? /serious ? /AUX ??
/depression.  (The most severe post-war depression 
emerged  in the Finland economy.) 
The subjects, ???/China? in (1) and ???/Finland 
??/economy?, are locatives, i.e. argM-LOC, and the 
objects, ???/food ??/crisis? in (1) and ???/post-
war ?/most ??/serious ?/AUX ??/depression? in 
(2), are analyzed as arg0. But the parser classified the 
subjects as arg0 and the objects as arg1. These are 
correct for most common verbs but wrong for this 
particular verb. It is difficult to know how common this 
problem would be in a larger, test set. The fact that we 
considered diversity of syntactic behavior when 
selecting verbs certainly helps make this test set reflect 
the difficult cases.  
If most verbs prove not to be as idiosyncratic as ???
/emerge?, the real performance of the parser on unseen 
verbs may be better than the average given here. 
Table 6   Experimental Results for Unseen Verbs 
    target               P(%) R(%) F(%) 
??/publish 90.7 72.9 80.8 
??/increase 49.6 34.3 40.5 
??/take place 90.1 63.3 74.4 
??/build into 65.2 55.5 60.0 
??/give 65.7 37.9 48.1 
??/pass 85.9 77.0 81.2 
??/emerge 12.6 10.2 11.3 
??/enter 81.9 58.8 68.4 
??/set up 79.0 61.1 68.9 
??/hope 77.7 35.9 49.1 
Average          69.8 50.7 58.3 
Another important difficulty in processing unseen 
verbs is the fact that roles in PropBank are defined in a 
verb-dependent way. This may be easiest to see with an 
English example. The roles arg2, arg3, arg4 have 
different meaning for different verbs; underlined in the 
following are some examples of arg2: 
(a) The state gave  CenTrust 30 days to sell the Rubens. 
(b) Revenue increased 11 to 2.73 billion from 2.46 
billion. 
(c) One of Ronald Reagan 's attributes as President was 
that he rarely gave his blessing to the claptrap that 
passes for consensus in various international 
institutions. 
In (a), arg2 represents the goal of ?give?, in (b), it 
represents the amount of increase, and in (c) it 
represents yet another role. These complete different 
semantic relations are given the same semantic label. 
For unseen verbs, this makes it difficult for the 
semantic parser to know what would count as an arg2.  
 
4 Using Automatic Parses 
 
The results in the last section are based on the use of 
perfect (hand-corrected) parses drawn from the Penn 
Chinese Treebank. In practical use, of course, 
automatic parses will not be as accurate. In this section 
we describe experiments on semantic parsing when 
given automatic parses produced by an automatic 
parser, the Collins (1999) parser, ported to Chinese. 
We first describe how we ported the Collins parser to 
Chinese and then present the results of the semantic 
parser with features drawn from the automatic parses.  
 
4.1 The Collins parser for Chinese 
The Collins parser is a state-of-the-art statistical parser 
that has high performance on English (Collins, 1999) 
and Czech(Collins et al 1999). There have been 
attempts in applying other algorithms in Chinese 
parsing (Bikel and Chiang, 2000; Chiang and Bikel 
2002; Levy and Manning 2003), but there has been no 
report on applying the Collins parser on Chinese. 
The Collins parser is a lexicalized statistical parser 
based on a head-driven extended PCFG model; thus the 
choice of head node is crucial to the success of the 
parser. We analyzed the Penn Chinese Treebank data 
and worked out head rules for the Chinese Treebank 
grammar (we were unable to find any published head 
rules for Chinese in the literature). There are two major 
differences in the head rules between English and 
Chinese. First, NP heads in Chinese are rigidly 
rightmost, that is to say, no modifiers of an NP can 
follow the head. In contrast, in English a modifier may 
follow the head. Second, just as with NPs in Chinese, 
the head of ADJP is rigidly rightmost. In English, by 
contrast, the head of an ADJP is mainly the leftmost 
constituent. Our head rules for the Chinese Treebank 
grammar are given in the Appendix. 
In addition to the head rules, we modified the POS tags 
for all punctuation.  This is because all cases of 
punctuation in the Penn Chinese Treebank are assigned 
the same POS tag ?PU?. The Collins parser, on the 
other hand, expects the punctuation tags in the English 
TreeBank format, where the tag for a punctuation mark 
is the punctuation mark itself. We therefore replaced 
the POS tags for all punctuation marks in the Chinese 
data to conform to the conventions in English. 
Finally, we made one further augmentation also related 
to punctuation. Chinese has one punctuation mark that 
does not exist in English. This commonly used mark,  
?semi-stop?, is used in Chinese to link coordinates 
within a sentence (for example between elements of a 
list). This function is represented in English by a 
comma. But the comma in English is ambiguous; in 
addition to its use in coordination and lists, it can also 
represent the end of a clause. In Chinese, by contrast 
the semi-stop has only the conjunction/list function. 
Chinese thus uses the regular comma only for 
representing clause boundaries. We investigated two 
ways to model the use of the Chinese semi-stop: (1) 
just converting the semi-stop to the comma, thus 
conflating the two functions as in English; and (2) by 
giving the semi-stop the POS tag ?CC?, a conjunction. 
We compared parsing results with these two methods; 
the latter (conjunction) method gained 0.5% net 
improvement in F-score over the former one. We 
therefore include it in our Collins parser port. 
We trained the Collins parser on the Penn Chinese 
Treebank(CTB) Release 2 with 250K words, first 
removing from the training set any sentences that occur 
in the test set for the semantic parsing experiments. We 
then tested on the test set used in the semantic parsing 
which includes 113 sentences(TEST1). The results of 
the syntactic parsing on the test set are shown in Table 
7. 
 
Table 7     Results for syntactic parsing, trained on 
CTB Release 2, tested on test set in semantic parsing 
    LP(%) LR(%) F1(%) 
overall            81.6 82.1 81.0 
len<=40          86.1 85.5 86.7 
 
To compare the performance of the Collins parser on 
Chinese with those of other parsers, we conducted an 
experiment in which we used the same training and test 
data (Penn Chinese Treebank Release 1, with 100K 
words) as used in those reports. In this experiment, we 
used articles 1-270 for training and 271-300 as 
test(TEST2). Table 8 shows the results and the 
comparison with other parsers. 
Table 8 only shows the performance on sentences ? 40 
words. Our performance on all the sentences TEST2 is 
P/R/F=82.2/83.3/82.7.  It may seem surprising that the 
overall F-score on TEST2 (82.7) is higher than the 
overall F-score on TEST1 (81.0) despite the fact that 
our TEST1 system had more than twice as much 
training as our TEST2 system.  The reason lies in the 
makeup of the two test sets; TEST1 consists of 
randomly selected long sentences; TEST2 consists of 
sequential text, including many short sentences. The 
average sentence length in TEST1 is 35.2 words, vs. 
22.1 in TEST2. TEST1 has 32% long sentences (>40 
words) while TEST2 has only 13%.  
 
Table 8      Comparison with other parsers: TEST2 
 ? 40 words 
 LP(%) LR(%) F1(%)
Bikel & Chiang 2000      77.2 76.2 76.7 
Chiang & Bikel 2002      81.1 78.8 79.9 
Levy & Manning 2003   78.4 79.2 78.8 
  Collins parser                86.4 85.5 85.9 
 
4.2 Semantic parsing using Collins parses 
In the test set of 113 sentences, there are 3 sentences in 
which target verbs are given the wrong POS tags, so 
they can not be used for semantic parsing. For the 
remaining 100 sentences, we used the feature set 
containing eight features (path, pt, gov,  position, 
subcat, target, head word and head POS) , the same as 
that used in the experiment on perfect parses.  The 
results are shown in Table 9. 
 
Table 9  Result for semantic parsing using automatic 
syntactic parses 
 P(%) R(%) F(%) 
110 sentences 86.0 70.8 77.6 
113 sentences 86.0 69.2 76.7 
 
Compared to the F-score using hand-corrected 
syntactic parses from the TreeBank, using automatic 
parses decreases the F-score by 6.4. 
 
5  Comparison with English 
 
Recent research on English semantic parsing has 
achieved quite good results by relying on the large 
amounts of training data available in the Propbank and 
Framenet (Baker et al 1998) databases.  But in 
extending the semantic parsing approach to other 
languages, we are unlikely to always have large data 
sets available. Thus it is crucial to understand how 
small amounts of data affect semantic parsing. At the 
same time, there have been no comparisons between 
English and other languages with respect to semantic 
parsing. It is thus not clear what language-specific 
issues may arise in general with the automatic mapping 
of syntactic structures to semantic relations. In this 
section, we compare English and Chinese by using the 
same semantic parser, similar verbs and similar 
amounts of data. Our goals are two-folds: (1) to 
compare the performance of the parser on English and 
Chinese; and (2) to understand differences between 
English and Chinese that affect automatic mapping 
between syntax and semantics. At first, we introduce 
the data used in the experiments and then we  present 
the results and give analysis. 
 
5.1 The English data  
In order to create an English corpus which matched our 
small Chinese corpus, we selected 10 English verbs 
which corresponded to our 10 Chinese verbs in 
meaning and frequency; exact translations of the 
Chinese when possible, or the closest possible word 
when an extract translation did not exist. The English 
verbs and their Chinese correspondents are given in 
Table 10. 
Table 10   English verbs chosen for experiments 
English   Freq  Chinese English Freq Chinese
build 46 ?? hold 120 ?? 
emerge 30 ?? hope 63 ?? 
enter 108 ?? increase 231 ?? 
found 248 ?? pass 143 ?? 
give 124 ?? publish 77 ?? 
 
Table 12       The comparison between adjuncts in English and Chinese 
 English Chinese 
Role Before  
verb 
After 
verb 
Freq in 
test 
P     R     F 
(%) 
Before 
verb 
After 
verb 
Freq in 
test 
P      R       F 
      (%) 
argM-ADV 22 43 5 0      0      0 223 0 37 91.3    56.8   70 
argM-LOC 25 82 11 80   36.4   50 233 5 31 90.0    87.1  88.5
argM-MNR 22 75 14 0      0      0 11 0 1 0        0        0 
argM-TMP 119 164 37 66.7   27    38.5 408 13 44 96.7   65.9   78.4
 
After the verbs were chosen, we extracted every 
sentence containing these verbs from section 02 to 
section 21 of the Wall Street Journal data from the 
Penn English Propbank. The number of sentences for 
each verb is given in Table 10. 
 
5.2 Experimental Results 
As in our Chinese experiments, we used our SVM-
based classifier, using N one-versus-all classifiers. 
Table 11 shows the performance on our English test set 
(with Chinese for comparison), beginning with the path 
feature, and incrementally adding features until in the 
last row we combine all 8 features together.  
 
Table 11       Experimental results of English 
 Chinese English 
feature set R/F/P P/R/F 
path 71.8/59.4/65.0 78.2/48.3/59.7 
path + pt 72.9/62.9/67.5 77.4/51.2/61.6 
path + position 72.5/60.8/66.2 75.7/50.9/60.8 
path + hd POS 77.6/63.3/69.7 79.1/49.7/61.0 
path + sub-cat 80.8/63.6/71.2 79.9/45.3/57.8 
path + hd word 85.0/66.0/74.3 84.0/47.7/60.8 
path + target 85.8/68.4/76.1 85.7/49.1/62.5 
COMBINED 91.7/76.0/83.1 84.1/62.2/71.5 
 
It is immediately clear from Table 11 that using similar 
verbs, the same amount of data, the same classifier, the 
same number of roles, and the same features, the 
results from English are much worse than those for 
Chinese. While some part of the difference is probably 
due to idiosyncracies of particular sentences in the 
English and Chinese data, other aspects of the 
difference might be accounted for systematically, as we 
discuss in the next section. 
 
5.3 Discussion: English/Chinese differences  
We first investigated whether the differences between 
English and Chinese could be attributed to particular 
semantic roles.  We found that this was indeed the case. 
The great bulk of the error rate difference between 
English and Chinese was caused by the 4 adjunct 
classes argM-ADV, argM-LOC, argM-MNR, and 
argM-TMP, which together account for 19.6% of the 
role tokens in our English corpus. The average F-score 
in English for the four roles is 36.7, while in Chinese 
the F-score for the four roles is 78.6. Why should these 
roles be so much more difficult to identify in English 
than Chinese? We believe the answer lies in the 
analysis of the position feature in section 3.2.2. This is 
repeated, with error rate information in Table 12. We 
see there that adjuncts in English have no strong 
preference for occurring before or after the verb. 
Chinese adjuncts, by contrast, are well-known to have 
an extremely strong preference to be preverbal, as 
Table 12 shows. The relatively fixed word order of 
adjuncts makes it much easier in Chinese to map these 
roles from surface syntactic constituents than in 
English. 
If the average F-score of the four adjuncts in English is 
raised to the level of that in Chinese, the overall F-
score on English would be raised from 71.5 to 79.7, 
accounting for 8.2 of the 11.6 difference in F-scores 
between the two languages.  
We next investigated the one feature from our original 
English-specific feature set that we had dropped in our 
Chinese system: passive. Recall that we dropped this 
feature because marked passives are extremely rare in 
Chinese. When we added this feature back into our 
English system, the performance rose from 
P/R/F=84.1/62.2/71.5 to 86.4/65.1/74.3.  As might be 
expected, this effect of voice is mainly reflected in an 
improvement on arg0 and arg1, as Table 13 shows 
below: 
 
Table 13.  Improvement in English semantic parsing 
with the addition of the voice feature 
 -voice +voice 
 P      R      F P      R        F 
arg0 88.9  75.3  81.5 94.4   80     86.6 
arg1 86.5  82.8  84.6 88.5  86.2   87.3 
A third source of English-Chinese differences is the 
distribution of roles; the Chinese data has 
proportionally more adjuncts (ARGMs), while the 
English data has proportionally more oblique 
arguments (ARG2, ARG3, ARG4).  Oblique arguments 
are more difficult to process than other arguments, as 
was discussed in section 3.4.  This difference is most 
likely to be caused by labeling factors rather than by 
true structural differences between English in Chinese. 
In summary, the higher performance in our Chinese 
system is due to 3 factors: the importance of passive in 
English; the strict word-order constraints of Chinese 
adverbials, and minor labeling differences. 
 
6  Conclusions  
 
We can draw a number of conclusions from our 
investigation of semantic parsing in Chinese. First, 
reasonably good performance can be achieved with a 
very small (1100 sentences) training set. Second, the 
features that we extracted for English semantic parsing 
worked well when applied to Chinese.  Many of these 
features required creating an automatic parse; in doing 
so we showed that the Collins (1999) parser when 
ported to Chinese achieved the best reported 
performance on Chinese syntactic parsing.  Finally, we 
showed that semantic parsing is significantly easier in 
Chinese than in English. We show that this 
counterintuitive result seems to be due to the strict 
constraints on adjunct ordering in Chinese, making 
adjuncts easier to find and label. 
 
Acknowledgements 
 
This work was partially supported by the National 
Science Foundation via a KDD Supplement to NSF 
CISE/IRI/Interactive Systems Award  IIS-9978025. 
Many thanks to Ying Chen for her help on the Collins 
parser port, and to Nianwen Xue and Sameer Pradhan 
for providing the data. Thanks to Kadri Hacioglu, 
Wayne Ward, James Martin, Martha Palmer, and three 
anonymous reviewers for helpful  advice. 
 
 
Appendix: Head rules for Chinese  
 
Parent     Direction          Priority List 
ADJP      Right        ADJP  JJ  AD 
ADVP     Right        ADVP AD CS JJ NP PP P VA VV 
CLP         Right       CLP  M  NN  NP 
CP           Right        CP  IP  VP 
DNP        Right        DEG   DNP  DEC   QP 
DP           Left          M(r)   DP  DT  OD 
DVP        Right        DEV  AD  VP 
IP            Right        VP  IP  NP 
LCP        Right        LCP  LC 
LST        Right        CD  NP  QP 
NP          Right        NP  NN  IP  NR  NT 
PP           Left          P   PP 
PRN        Left          PU 
QP           Right       QP  CLP  CD 
UCP        Left          IP  NP  VP 
VCD       Left          VV  VA  VE 
VP          Left          VE VC VV VNV VPT VRD   
                                VSB VCD VP 
VPT         Left         VA  VV 
VRD        Left         VVl VA 
VSB         Right      VV  VE 
 
References 
 
Baker, Collin F., Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkekey FrameNet Project. In 
Proceeding of COLING/ACL. 
 
Bikel, Daniel and David Chiang. 2000. Two Statistical 
Parsing models Applied to the Chinese Treebank. In 
Proceedings of the Second Chinese Language 
Processing Workshop, pp. 1-6. 
 
Chiang, David and Daniel Bikel. 2002. Recovering 
Latent Information in Treebanks. In Proceedings of 
COLING-2002, pp.183-189. 
  
Collins, Michael. 1999. Head-driven Statistical Models 
for Natural Language Parsing. Ph.D. dissertation, 
University of Pennsylvannia.  
 
Collins, Michael, Jan Hajic, Lance Ramshaw and 
Christoph Tillmann. 1999. A Statistical Parser for 
Czech. In Proceedings of the 37th Meeting of the ACL, 
pp. 505-512. 
 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics, 28(3):245-288. 
 
Gildea, Daniel and Martha Palmer. 2002. The 
Necessity of Parsing for Predicate Argument 
Recognition, In Proceedings of the 40th Meeting of the 
ACL, pp. 239-246. 
 
Kingsbury, Paul, Martha Palmer, and Mitch Marcus. 
2002. Adding semantic annotation to the Penn 
Treebank. In Proceedings of HLT-02. 
 
Kudo, Taku and Yuji Matsumoto. 2000. Use of support 
vector learning for chunk Identification. In 
Proceedings of the 4th Conference on CoNLL, pp. 
142-144. 
 
Kudo, Taku and Yuji Matsumoto. 2001 Chunking with 
Support Vector Machines. In Proceeding of the 2nd 
Meeting of the NAACL. pp.192-199. 
 
Levy, Roger and Christopher Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank? 
ACL 2003, pp. 439-446. 
 
Pradhan, Sameer, Kadri Hacioglu,. Wayne Ward, 
James Martin, and Daniel Jurafsky. 2003. ?Semantic 
Role Parsing: Adding Semantic Structure to 
Unstructured Text?. In the Proceedings of the 
International Conference on Data Mining (ICDM-
2003), Melbourne, FL, 2003 
 
Surdeanu, Mihai, Sanda Harabagiu, John Williams and 
Paul Aarseth. 2003. Using Predicate-Argument 
Structures for Information Extraction, In Proceedings 
of ACL. 
 
Xue, Nianwen. 2002. Guidelines for the Penn Chinese 
Proposition Bank (1st Draft), UPenn. 
 
Xue, Nianwen, Fu-Dong Chiou and Martha Palmer. 
2002. Building a large-scale annotated Chinese corpus. 
In Proceedings of COLING-2002. 
 
Xue, Nianwen, Martha Palmer. 2003. Annotating the 
propositions in the Penn Chinese Treebank. In 
Proceedings of the 2nd SIGHAN Workshop on Chinese 
Language Processing. 
Parsing Arguments of Nominalizations in English and Chinese?
Sameer Pradhan, Honglin Sun,
Wayne Ward, James H. Martin
Center for Spoken Language Research,
University of Colorado, Boulder, CO 80303
{spradhan,sunh,whw,martin}@cslr.colorado.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
In this paper, we use a machine learning frame-
work for semantic argument parsing, and apply
it to the task of parsing arguments of eventive
nominalizations in the FrameNet database. We
create a baseline system using a subset of fea-
tures introduced by Gildea and Jurafsky (2002),
which are directly applicable to nominal pred-
icates. We then investigate new features which
are designed to capture the novelties in nom-
inal argument structure and show a significant
performance improvement using these new fea-
tures. We also investigate the parsing perfor-
mance of nominalizations in Chinese and com-
pare the salience of the features for the two lan-
guages.
1 Introduction
The field of NLP had seen a resurgence of research in
shallow semantic analysis. The bulk of this recent work
views semantic analysis as a tagging, or labeling prob-
lem, and has applied various supervised machine learn-
ing techniques to it (Gildea and Jurafsky (2000, 2002);
Gildea and Palmer (2002); Surdeanu et al (2003); Ha-
cioglu and Ward (2003); Thompson et al (2003); Prad-
han et al (2003)). Note that, while all of these systems
are limited to the analysis of verbal predicates, many un-
derlying semantic relations are expressed via nouns, ad-
jectives, and prepositions. This paper presents a prelimi-
nary investigation into the semantic parsing of eventive
nominalizations (Grimshaw, 1990) in English and Chi-
nese.
2 Semantic Annotation and Corpora
For our experiments, we use the FrameNet database
(Baker et al, 1998) which contains frame-specific se-
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IS-9978025
mantic annotation of a number of predicates in English.
Predicates are grouped by the semantic frame that they
instantiate, depending on the sense of their usage, and
their arguments assume one of the frame elements or
roles specific to that frame. The predicate can be a verb,
noun, adjective, prepositional phrase, etc. FrameNet
contains about 500 different frame types and about 700
distinct frame elements. The following example illus-
trates the general idea. Here, the predicate ?complain?
instantiates a ?Statement? frame once as a nominal
predicate and once as a verbal predicate.
Did [Speaker she] make an official [Predicate:nominal com-
plaint] [Addressee to you] [Topic about the attack.]
[Message?Justice has not been done?] [Speaker he]
[Predicate:verbal complained.]
Nominal predicates in FrameNet include ultra-nominals
(Barker and Dowty, 1992), nominals and nominal-
izations. For the purposes of this study, a human analyst
went through the nominal predicates in FrameNet and
selected those that were identified as nominalizations
in NOMLEX (Macleod et al, 1998). Out of those,
the analyst then selected ones that were eventive
nominalizations.
These data comprise 7,333 annotated sentences, with
11,284 roles. There are 105 frames with about 190 dis-
tinct frame role1 types. A stratified sampling over predi-
cates was performed to select 80% of this data for train-
ing, 10% for development and another 10% for testing.
For the Chinese semantic parsing experiments, we se-
lected 22 nominalizations from the Penn Chinese Tree-
bank and tagged all the sentences containing these predi-
cates with PropBank (Kingsbury and Palmer, 2002) style
arguments ? ARG0, ARG1, etc. These consisted of 630
sentences. These are then split into two parts: 503 (80%)
for training and 127 (20%) for testing.
1We will use the terms role and arguments interchangeably
3 Baseline System
The primary assumption in our system is that a seman-
tic argument aligns with some syntactic constituent. The
goal is to identify and label constituents in a syntactic
tree that represent valid semantic arguments of a given
predicate. Unlike PropBank, there are no hand-corrected
parses available for the sentences in FrameNet, so we
cannot quantify the possible mis-alignment of the nomi-
nal arguments with syntactic constituents. The arguments
that do not align with any constituent are simply missed
by the current system.
3.1 Features We created a baseline system using
all and only those features introduced by Gildea and
Jurafsky that are directly applicable to nominal pred-
icates. Most of the features are extracted from the
syntactic parse of a sentence. We used the Charniak
parser (Chaniak, 2001) to parse the sentences in order to
perform feature extraction. The features are listed below:
Predicate ? The predicate lemma is used as a feature.
Path ? The syntactic path through the parse tree from the
parse constituent being classified to the predicate.
Constituent type ? This is the syntactic category (NP, PP,
S, etc.) of the constituent corresponding to the semantic
argument.
Position ? This is a binary feature identifying whether
the constituent is before or after the predicate.
Head word ? The syntactic head of the constituent.
3.2 Classifier and Implementation We formulate the
parsing problem as a multi-class classification problem
and use a Support Vector Machine (SVM) classifier in the
ONE vs ALL (OVA) formalism, which involves training
n classifiers for a n-class problem ? including the NULL
class. We use TinySVM2 along with YamCha3 (Kudo
and Matsumoto (2000, 2001)) as the SVM training and
test software.
3.3 Performance We evaluate our system on three
tasks: i) Argument Identification: Identifying parse con-
stituents that represent arguments of a given predicate, ii)
Argument Classification: Labeling the constituents that
are known to represent arguments with the most likely
roles, and iii) Argument Identification and Classification:
Finding constituents that represent arguments of a pred-
icate, and labeling them with the most likely roles. The
baseline performance on the three tasks is shown in Ta-
ble 1.
4 New Features
To improve the baseline performance we investigated ad-
ditional features that would provide useful information in
identifying arguments of nominalizations. Following is a
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
Task P R F?=1 A
(%) (%) (%)
Id. 81.7 65.7 72.8
Classification - - - 70.9
Id. + Classification 65.7 42.1 51.4
Table 1: Baseline performance on all three tasks.
description of each feature along with an intuitive justifi-
cation. Some of these features are not instantiated for a
particular constituent. In those cases, the respective fea-
ture values are set to ?UNK?.
1. Frame ? The frame instantiated by the particular sense
of the predicate in a sentence. This is an oracle feature.
2. Selected words/POS in constituent ? Nominal predi-
cates tend to assign arguments, most commonly through
postnominal of-complements, possessive prenominal
modifiers, etc. We added the values of the first and last
word in the constituent as two separate features. Another
two features represent the part of speech of these words.
3. Ordinal constituent position ? Arguments of nouns
tend to be located closer to the predicate than those
for verbs. This feature captures the ordinal position
of a particular constituent to the left or right of the
predicate on a left or right tree traversal, eg., first PP
from the predicate, second NP from the predicate, etc.
This feature along with the position will encode the
before/after information for the constituent.
4. Constituent tree distance ? Another way of quan-
tifying the position of the constituent is to identify its
index in the list of constituents that are encountered
during linear traversal of the tree from the predicate to
the constituent.
5. Intervening verb features ? Support verbs play an
important role in realizing the arguments of nominal
predicates. We use three classes of intervening verbs:
i) auxiliary verbs ? ones with part of speech AUX, ii)
light verbs ? a small set of known light verbs: took, take,
make, made, give, gave, went and go, and iii) other verbs
? with part of speech VBx. We added three features for
each: i) a binary feature indicating the presence of the
verb in between the predicate and the constituent ii) the
actual word as a feature, and iii) the path through the
tree from the constituent to the verb, as the subject of
intervening verbs sometimes tend to be arguments of
nominalizations. The following example could explain
the intuition behind this feature:
[Speaker Leapor] makes general [Predicate assertions] [Topic
about marriage]
6. Predicate NP expansion rule ? This is the noun
equivalent of the verb sub-categorization feature used by
Gildea and Jurafsky (2002). This is the expansion rule
instantiated by the parser, for the lowermost NP in the
tree, encompassing the predicate. This would tend to
cluster NPs with a similar internal structure and would
thus help finding argumentive modifiers.
7. Noun head of prepositional phrase constituents
? Instead of using the standard head word rule for
prepositional phrases, we use the head word of the first
NP inside the PP as the head of the PP and replace the
constituent type PP with PP-<preposition>.
8. Constituent sibling features ? These are six features
representing the constituent type, head word and part of
speech of the head word of the left and right siblings
of the constituent in consideration. These are used
to capture arguments represented by the modifiers of
nominalizations.
9. Partial-path from constituent to predicate ? This
is the path from the constituent to the lowest common
parent of the constituent and the predicate. This is used
to generalize the path statistics.
10. Is predicate plural ? A binary feature indicating
whether the predicate is singular or plural as they tend to
have different argument selection properties.
11. Genitives in constituent ? This is a binary feature
which is true if there is a genitive word (one with the part
of speech POS, PRP, PRP$ or WP$) in the constituent,
as these tend to be markers for nominal arguments as in
[Speaker Burma ?s] [Phenomenon oil] [Predicate search] hits
virgin forests
12. Constituent parent features ? Same as the sibling
features, except that that these are extracted from the
constituent?s parent.
13. Verb dominating predicate ? The head word of the
first VP ancestor of the predicate.
14. Named Entities in Constituent ? As in Surdeanu
et al (2003), this is represented as seven binary fea-
tures extracted after tagging the sentence with BBN?s
IdentiFinder (Bikel et al, 1999) named entity tagger.
5 Feature Analysis and Best System
Performance
5.1 English For the task of argument identification,
features 2, 3, 4, 5 (the verb itself, path to light-verb and
presence of a light verb), 6, 7, 9, 10 an 13 contributed pos-
itively to the performance. The Frame feature degrades
performance significantly. This could be just an artifact
of the data sparsity. We trained a new classifier using all
the features that contributed positively to the performance
and the F?=1 score increased from the baseline of 72.8%
to 76.3% (?2; p < 0.05).
For the task of argument classification, adding the
Frame feature to the baseline features, provided the most
significant improvement, increasing the classification
accuracy from 70.9% to 79.0% (?2; p < 0.05). All
other features added one-by-one to the baseline did
not bring any significant improvement to the baseline,
which might again be owing to the comparatively small
training and test data sizes. All the features together
produced a classification accuracy of 80.9%. Since the
Frame feature is an oracle, we were interested in finding
out what all the other features combined contributed.
We ran an experiment with all features, except Frame,
added to the baseline, and this produced an accuracy of
73.1%, which however, is not a statistically significant
improvement over the baseline of 70.9%.
For the task of argument identification and classifi-
cation, features 8 and 11 (right sibling head word part
of speech) hurt performance. We trained a classifier
using all the features that contributed positively to the
performance and the resulting system had an improved
F?=1 score of 56.5% compared to the baseline of 51.4%
(?2; p < 0.05).
We found that a significant subset of features that con-
tribute marginally to the classification performance, hurt
the identification task. Therefore, we decided to perform
a two-step process in which we use the set of features that
gave optimum performance for the argument identifica-
tion task and identify all likely argument nodes. Then, for
those nodes, we use all the available features and classify
them into one of the possible classes. This ?two-pass?
system performs slightly better than the ?one-pass? men-
tioned earlier. Again, we performed the second pass of
classification with and without the Frame feature.
Table 2 shows the improved performance numbers.
Task P R F?=1 A
(%) (%) (%)
Id. 83.8 70.0 76.3
Classification (w/o Frame) - - - 73.1
Classification (with Frame) - - - 80.9
Id. + Classification 69.4 47.6 56.5
(one-pass, w/o Frame)
Id. + Classification 62.2 53.1 57.3
(two-pass, w/o Frame)
Id. + Classification 69.4 59.2 63.9
(two-pass, with Frame)
Table 2: Best performance on all three tasks.
5.2 Chinese For the Chinese task, we use the one-pass
algorithm as used for English. A baseline system was
created using the same features as used for English (Sec-
tion 3). We evaluate this system on just the combined task
of argument identification and classification. The base-
line performance is shown in Table 3.
To improve the system?s performance over the base-
line, we added all the features discussed in Section 4, ex-
cept features Frame ? as the data was labeled in a Prop-
Bank fashion, there are no frames involved as in Frame-
Net; Plurals and Genitives ? as they are not realized the
same way morphologically in Chinese, and Named En-
tities ? owing to the unavailability of a Chinese Named
Entity tagger. We found that of these features, 2, 3, 4, 6, 7
and 13 hurt the performance when added to the baseline,
but the other features helped to some degree, although
not significantly. The improved performance is shown in
Table 3
Features P R F?=1
(%) (%)
Baseline 86.2 32.2 46.9
Baseline 83.9 44.1 57.8
+ more features
Table 3: Parsing performance for Chinese on the com-
bined task of identifying and classifying semantic argu-
ments.
An interesting linguistic phenomenon was observed
which explains part of the reason why recall for Chinese
argument parsing is so low. In Chinese, arguments
which are internal to the NP which encompasses the
nominalized predicate, tend to be multi-word, and are
not associated with any node in the parse tree. These
violates our basic assumption of the arguments aligning
with parse tree constituents, and are guaranteed to be
missed. In the case of English however, these tend to be
single word arguments which are represented by a leaf
in the parse tree and stand a chance of getting classified
correctly.
6 Conclusion
In this paper we investigated the task of identifying and
classifying arguments of eventive nominalizations in
FrameNet. The best system generates an F1 score of
57.3% on the combined task of argument identification
and classification using automatically extracted features
on a test set of about 700 sentences using a classifier
trained on about 6,000 sentences.
As noted earlier, the bulk of past research in this area
has focused on verbal predicates. Two notable exceptions
to this include the work of (Hull and Gomez, 1996) ? a
rule based system for identifying the semantic arguments
of nominal predicates, and the work of (Lapata, 2002)
on interpreting the relation between the head of a nom-
inalized compound and its modifier noun. Unfortunately,
meaningful comparisons to these efforts are difficult due
to differing evaluation metrics.
We would like to thank Ralph Weischedel and Scott Miller of
BBN Inc. for letting us use BBN?s named entity tagger ? Iden-
tiFinder; Ashley Thornton for identifying the sentences from
FrameNet with predicates that are eventive nominalizations.
References
[Baker et al1998] Collin F. Baker, Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley FrameNet project. In
COLING/ACL-98, pages 86?90, Montreal.
[Barker and Dowty1992] Chris Barker and David Dowty. 1992.
Non-verbal thematic proto-roles. In NELS-23, Amy Schafer,
ed., GSLA, Amherst, pages 49?62.
[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34:211?231.
[Chaniak2001] Eugene Chaniak. 2001. Immediate-head pars-
ing for language models. In ACL, Toulouse, France.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky.
2000. Automatic labeling of semantic roles. In ACL, pages
512?520, Hong Kong, October.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky.
2002. Automatic labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer.
2002. The necessity of syntactic parsing for predicate ar-
gument recognition. In ACL, PA.
[Grimshaw1990] Jane Grimshaw. 1990. Argument Structure.
The MIT Press, US.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward.
2003. Target word detection and semantic role chunking us-
ing support vector machines. In HLT, Edmonton, Canada.
[Hull and Gomez1996] Richard D. Hull and Fernando Gomez.
1996. Semantic interpretation of nominalizations. In AAAI
Conference, Oregon, pages 1062?1068.
[Kingsbury and Palmer2002] Paul Kingsbury and Martha Pal-
mer. 2002. From Treebank to PropBank. In LREC-2002,
Las Palmas, Canary Islands, Spain.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto.
2000. Use of support vector learning for chunk identifica-
tion. In CoNLL-2000, pages 142?144.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto.
2001. Chunking with support vector machines. In NAACL.
[Lapata2002] Maria Lapata. 2002. The disambiguation of nom-
inalizations. Computational Linguistics, 28(3):357?388.
[Macleod et al1998] C. Macleod, R. Grishman, A. Meyers,
L. Barrett, and R. Reeves. 1998. Nomlex: A lexicon of
nominalizations.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, Wayne
Ward, James Martin, and Dan Jurafsky. 2003. Semantic role
parsing: Adding semantic structure to unstructured text. In
ICDM, Melbourne, Florida.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, John
Williams, and Paul Aarseth. 2003. Using predicate-
argument structures for information extraction. In ACL, Sap-
poro, Japan.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, and
Christopher D. Manning. 2003. A generative model for se-
mantic role labeling. In ECML.
  
 
The Effect of Rhythm on Structural Disambiguation in Chinese 
 
Honglin Sun                     Dan Jurafsky 
Center for Spoken Language Research 
University of Colorado at Boulder 
{honglin.sun, jurafsky}@colorado.edu 
 
 
 
Abstract 
 
The length of a constituent (number of 
syllables in a word or number of words in a 
phrase), or rhythm, plays an important role 
in Chinese syntax. This paper systematically 
surveys the distribution of rhythm in 
constructions in Chinese from the statistical 
data acquired from a shallow tree bank. 
Based on our survey, we then used the 
rhythm feature in a practical shallow parsing 
task by using rhythm as a statistical feature 
to augment a PCFG model. Our results show 
that using the probabilistic rhythm feature 
significantly improves the performance of 
our shallow parser. 
 
1 Introduction 
 
Syntactic research indicates that prosodic features, 
including stress, rhythm, intonation, and others, 
have an impact on syntactic structure. For example, 
normally in a coordination construction like ?A 
and B?, A and B are interchangeable, that is to say, 
you can say ?B and A? and the change of word 
order does not change the meaning. However, 
sometimes A and B are not interchangeable. Quirk 
et al(1985) gives the following examples: 
   man and woman          * woman and man 
   ladies and gentleman   *gentleman and ladies 
Obviously, the examples above cannot be 
explained by gender preference. A reasonable 
explanation is that the length of the words (perhaps 
in syllables) is playing a role; the first constituent 
tends to be shorter than the second constituent. 
This feature of the length in syllables of a 
constituent plays an even more important role in 
Chinese syntax than in English (Feng, 2000). For 
example, in the verb-object construction in 
Chinese, there is a preference for the object to be 
equal to or longer than the verb. Thus while both 
???(plant)  and  ????(plant) are verbs and have 
the same meaning,  ? ? /plant ? /tree? is 
grammatical while ? ? ? /plant ? /tree? is 
ungrammatical. However, both verbs allow bi-
syllabic nouns as objects (e.g., ????(fruit tree), 
????(cotton) etc.). The noun phrases formed by 
?noun + verb? give us another example in which 
rhythm feature places constraints on syntax, as 
indicated in the following examples 
(ungrammatical with *): 
    ??/cotton   ??/planting 
     *??/cotton   ?/planting 
     *?/flower       ??/planting 
     *?/flower       ?/planting 
???/cotton   ??/planting? is grammatical but 
???/cotton   ?/planting? , ??/flower   ??
/planting? and ??/flower   ?/planting? are all 
ungrammatical, although ??? /cotton? and ??
/flour? , ???/planting? and ??/planting? have 
the same POS and the same or similar meaning. 
The only difference lies in that they have different 
number of syllables or different length. 
This paper systematically surveys the effect of 
rhythm on Chinese syntax from the statistical data 
from a shallow tree bank. Based on the observation 
that rhythm places constraints on syntax in Chinese, 
we try to deploy a feature based on rhythm to 
improve disambiguation in a probabilistic parser 
by mixing the rhythm feature into a statistical 
parsing model. 
The rest of the paper is organized as follows: we 
present specific statistical analyses of rhythm 
feature in Chinese syntax in Section 2. Section 3 
introduces the content chunk parsing which is the 
task in our experiment. Section 4 presents the 
statistical model used in our experiment in which a 
probabilistic rhythm feature is integrated. Section 5 
gives the experimental results and finally Section 6 
draws some conclusions. 
   
2 Analysis of Rhythmic Constraints 
 
We divide our analysis of the use of rhythm in 
Chinese phrases into two categories, based on two 
types of phrases in Chinese: (1) simple phrases, 
containing only words, i.e. all the child nodes are 
POS tag in the derivation tree; and (2) complex 
phrases in which at least one constituent is a phrase 
itself, i.e. it has at least one child node with phrase 
type symbol (like NP, VP) in its derivation tree. 
Below we will give the statistical analysis of the 
distribution of rhythm feature in different 
constructions from both simple and complex 
phrases. The corpus from which the statistical data 
is drawn contains 200K words of newspaper text 
from the People?s Daily. The texts are word-
segmented, POS tagged and labeled with content 
chunks. The content chunk is a phrase containing 
only content words, akin to a generalization of a 
BaseNP. These content chunks are parsed into 
binary shallow trees. More details about content 
chunks can be found in Section 3. 
 
2.1 Rhythm feature in simple phrases 
 
Simple phrases contain two lexical words (since, as 
discussed above, our parse trees are binary). The 
rhythm feature of each word is defined to be the 
number of syllables in it. Thus the rhythm feature 
for a word can take on one of the following three 
values: (1) monosyllabic; (2) bi-syllabic; and (3) 
multi-syllabic, meaning with three syllables or 
more. 
Since each binary phrase contains two words, 
the set of rhythm features for a simple phrase is: 
F = { (0?0), (0,1), (0,2), (1,0), (1,1), (1,2), (2,0), 
(2,1), (2,2) } 
where 0, 1, 2 represent monosyllabic, bi-syllabic 
and multi-syllabic respectively. 
In the following sections, we will present three 
case studies on the distributions of rhythm feature 
in different constructions: (1) verbs as modifier or 
head in NP; (2) the contrast between NPs and VPs 
formed by ? verb + noun? sequences; (3) ?noun + 
verb? sequences. 
 
2.1.1  Case 1: Verb as modifier/head  in NP 
 
In Chinese, verbs can function as modifier or head 
in a noun phrase without any change of forms. For 
example, in ???/fruit tree ??/growing?, ??
?? is the head while in ??? /growing ??
/technique?, ???? is a modifier. However, in 
such constructions, there are strong constraints on 
the length of both verbs and nouns. Table 1 gives 
the distributions of the rhythm feature in the rule 
?NP -> N V?(?N? and ?V? represent noun and verb 
respectively)  in which the verb is the head and 
?NP -> V N? in which the verb is a modifier.
Table 1  Distribution of rhythm feature in NP with verb as modifier or head 
 [0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] Total 
NP -> V N 0 4 0 0 1275 4 0 88 0 1371 
NP -> N V 13 10 0 401 2328 91 0 44 2 2889 
   
Table 2  Distribution of rhythm feature in NP and VP formed by ?V N? 
 [0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] Total 
VP -> V N 826 640 49 80 1221 121 0 11 1 2777 
NP -> V N 13 10 0 401 2328 91 0 44 2 2889 
 
Table 3  Distribution of rhythm feature in phrases  formed by ?N V? sequence 
 [0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] Total 
NP -> N V 0 4 0 0 1275 4 0 88 0 1371 
NC -> N V 384 578 42 1131 3718 143 90 435 15 6536 
S    -> N V 28 1 2 17 347 22 2 43 8 470 
 
Table 1 indicates that in both rules, the rhythm 
pattern [1,1], ie. ?bi-syllabic + bi-syllabic?, 
prevails. In the rule ?NP -> V N?, this pattern 
accounts for 93% among the nine possible patterns 
while in the rule ?NP -> N V?, this pattern 
accounts for 81%. We can also find that in both 
cases, [0,2] and [2,0]  are prohibited, that is to say, 
both verbs and nouns cannot be longer than two 
syllables.  
 
2.1.2  Case 2: Contrast between NP and VP 
formed by ?V N? sequence 
 
The sequence ?V N?(?verb + noun?) can constitute 
an NP or a VP. The rhythm patterns in the two 
types of phrases are significantly different, 
however, as shown in Table 2. We see that in the 
NP case, verbs are mainly bi-syllabic. The total 
number of examples with bi-syllabic verbs in NP is 
2820, accounting for 98% of all the cases. On the 
other hand, mono-syllabic verbs are less likely to 
appear in this position. The total number of 
examples with mono-syllabic verbs in NP is 23, 
accounting for only 0.8% of all the cases. That is to 
say, the likelihood of bi-syllabic verbs appearing in 
this syntactic position is 122 times the likelihood 
of mono-syllabic verbs. On the other hand, there is 
no big difference between bi-syllabic verbs and 
mono-syllabic verbs in the VP formed by ?V + N?.  
The ratios of bi-syllabic and mono-syllabic verbs 
in VP are 48 % and 55% respectively. The 
statistical facts tell us that for a ?verb + noun? 
sequence, if the verb is not bi-syllabic then it is 
very unlikely to be an NP. Figure 1 depicts more 
clearly the difference between NP and VP formed 
by ?V N? sequence in the distribution of rhythm 
feature. 
0
500
1000
1500
2000
2500
0,0 0,2 1,1 2,0 2,2
VP-> v n
NP-> v n
 
Figure 1   Distributions of rhythm feature in NP 
and VP formed by ?verb + noun? 
.  
2.1 3  Case 3: ?N  V? sequence 
 
An ?N V?(?noun + verb?) sequence can be mainly 
divided into three types by the dominating phrasal 
category:  
(1) NP(noun phrase), e.g.  ???/fruit tree ??
/growth?;  
(2) S(subject-verb construction), e.g. ? ? ?
/colored flag ??/flutter?;  
(3)NC(non-constituent), eg. ???/economy ??
/develop? in ???/China ?/DE ??/economy 
??/develop ?/DE ?/very ?/fast?. (?China?s  
economy develops very fast?) 
Table 3 gives the distribution of rhythm feature in 
the three types of cases. 
We see in Table 3, in rule ?NP -> N V?, that the 
verb cannot be mono-syllabic since the first row is 
0 in all the patterns in which verb is mono-
syllabic([0,0], [1,0],[2,0]). The ?bi-syllabic + bi-
syllabic? ([1,1]) pattern accounts for 93% 
(1275/1371) of the total number. Let?s look at the 
cases with mono-syllabic verbs in all the three 
types. The total number of such examples is 1652 
in the corpus (adding all the numbers in columns 
[0,0], [1,0] and [2,0] on the three rows). Among 
these 1652 cases, there is not one example in 
which the ?N V? is an NP. The sequence has a 
probability of 3%(47/1652) to be an S and 97 
%(1605/1652) of being an NC(non-constituent). 
 
2.2 Rhythm feature in complex phrases 
 
Just as we saw with two word simple phrases, the 
rhythm feature also has an effect on complex 
phrases where at least one component is a phrase, 
i.e. spanning over two words or more. For example, 
for the following fragment of a sentence:   
      ?/stride   ?/into   ??/the Three Gorges     
      ??/project    ??/gate 
   ?enter into the gate of the Three Gorges Project? 
according to PCFG, the parse as indicated in 
Figure 2 (a) is incorrectly assigned the greatest  
                                   
                                   VP 
                                              NP 
 
                              VP 
                                            NP 
                                       
       ?       ?      ??           ??          ?? 
(a) 
                             
                               VP 
 
                                                            NP 
 
       VP                       NP 
  
     ?         ?        ??         ??         ?? 
(b) 
Figure 2  (a) incorrect parse and 
     (b) correct parse 
 
probability but the correct parse is that given in 
Figure 2 (b). One major error in (a) is that it 
applies the rule ?NP-> VP N? (i.e. ??  ??  ?
? ? modifying ? ? ? ?). This rule has 216 
occurrences in the corpus, of which 168 times it 
contains a VP of 2 words, 30 times a VP of 3 
words and 18 times a VP of more than 3 words. 
These statistics indicate that this rule prefers to 
choose a short VP acting as the modifier of a noun, 
as in ?NP(VP( ? /grow  ? /grain) ?? /large 
family)? and ?NP(VP(?/learn ??/Lei Feng) ?
?/model)?. But in the example in Figure 2(a), the 
VP contains 3 words, so it is less likely to be a 
modifier in an NP.    
When a phrase works as a constituent in a larger 
phrase, its rhythm feature is defined as the number 
of words in it. Thus a phrase may take on one of 
the three values for the rhythm feature: (1) two 
words; (3) three words; and (3) more than three 
words. Similar to that in the simple phrases, we 
may use 0, 1, 2 to represent the three values 
respectively. Therefore, for every construction 
containing two constituents, its rhythm feature can 
be described by a 3?3 matrix uniformly. For 
example, in the examples for rule ?NP -> VP N? 
above, the feature value for  ?NP(VP(?/grow  ?
/grain) ??/large family)? is [0, 1] in which 0 
indicates the VP contains 2 words and 1 represents 
that the noun is bi-syllabic. The rule helps to 
interpret the meaning of the feature value, i.e. the 
value is for a word or a phrase. For example, for 
rule ?VP -> V N?, feature value [0, 1] means that 
the verb is mono-syllabic and the noun is bi-
syllabic, while for rule ?NP-> VP N?, feature [0,1] 
means that the VP contains two words and the 
noun is bi-syllabic. 
 
3   Content Chunk Parsing  
 
We have chosen the task of content chunk parsing 
to test the usefulness of our rhythm feature to 
Chinese text. In this section we address two 
questions: (1) What is a content chunk? (2) Why 
are we interested in content chunk parsing? 
A content chunk is a phrase formed by a sequence 
of content words, including nouns, verbs, 
adjectives and content adverbs. There are three 
kinds of cases for the mapping between content 
word sequences and content chunks: 
(1) A content word sequence is a content chunk. A 
special case of this is that a whole sentence is a 
content chunk when all the words in it are content 
words, eg. [[??/Prospect  ??/company]NP [?
?/release [??/advanced [??/computer [??
/typesetting ? ? /system]NP]NP]NP]VP 
(?Prospect Company released an advanced 
computer typesetting system.?). 
 (2) A content word sequence is not a content 
chunk. For example, in ???/China ?/AUX ??
/economy ? ? /develop ? /AUX ? /very ?
/fast?(?China?s economy develops very fast.?), ??
? /economy ?? /develop? is a content word 
sequence, but it?s not a phrase in the sentence.  
(3) A part of a content word sequence is a content 
chunk. For example, in ? ? ? /private ? ?
/economy ?? /develop ? /AUX ?? /trend ?
/very ? /good?(?The developmental trend of 
private economy is very good.?), ???/private ?
? /economy ?? /develop? is a content word 
sequence, but it?s not a phrase; only ???/private 
??/economy? in it is a phrase.  
The purpose of content chunk parsing is to 
recognize phrases in a sequence of content words. 
Specifically speaking, the content chunking 
contains two subtasks: (1) to recognize the 
maximum phrase in a sequence of content words; 
(2) to analyze the hierarchical structure within the 
phrase down to words. Like baseNP 
chunking(Church, 1988; Ramshaw & Marcus 
1995), content chunk parsing is also a kind of 
shallow parsing. Content chunk parsing is deeper 
than baseNP chunking in two aspects: (1) a content 
chunk may contain verb phrases and other phrases 
even a full sentence as long as the all the 
components are content words; (2) it may contain 
recursive NPs. Thus the content chunk can supply 
more structural information than a baseNP.  
The motives for content chunk parsing are two-
fold: (1) Like other shallow parsing tasks, it can 
simplify the parsing task. This can be explained in 
two aspects. First, it can avoid the ambiguities 
brought up by functional words. In Chinese, the 
most salient syntactic ambiguities are prepositional 
phrases and the ?DE? construction. For 
prepositional phrases, the difficulty lies in how to 
determine the right boundary, because almost any 
constituent can be the object of a preposition. For 
?DE? constructions, the problem is how to 
determine its left boundary, since almost any 
constituent can be followed by ?DE? to form a 
?DE? construction. Second, content chunk parsing 
can simplify the structure of a sentence. When a 
content chunk is acquired, it can be replaced by its 
head word, thus reducing the length of the original 
sentence. If we get a parse from the reduced 
sentence with a full parser, then we can get a parse 
for the original sentence by replacing the head-
word nodes with the content chunks from which 
the head-words are extracted. (2) The content 
chunk parsing may be useful for applications like 
information extraction and question answering. 
When using template matching, a content chunk 
may be just the correct level of shallow structure 
for matching with an element in a template.  
 
4   PCFG + PF Model 
 
In the experiment we propose a statistical model 
integrating probabilistic context-free grammar 
(PCFG) model with a simple probabilistic features 
(PF) model. In this section we first give the 
definition for the statistical model and then we will 
give the method for parameter estimation. 
 
4.1   Definition 
 
According to PCFG, each rule r used to expand a 
node n in a parse is assigned a probability, i.e.: 
      )|())(( APnrP ?=                                            (1) 
where A -> ?  is a CFG rule. The probability of a 
parse T is the product of each rule used to expand 
each node n in T: 
     ?
?
=
Tn
nrPSTP ))(()|(                                 (2) 
We expand PCFG by the way that when a left 
hand side category A is expanded into a string ?, a 
feature set FS related to ? is also generated. Thus, a 
probability is assigned for expansion of each node 
n when a rule r is applied: 
)|,())(( AFSPnrP ?=                         (3) 
where A -> ?  is a CFG rule and FS is a feature set 
related to ?. From Equation (3) we get: 
 )|(*),|())(( APAFSPnrP ??=     (4) 
where P(FS| ?, A) is probabilistic feature(PF) 
model and P(? | A) is PCFG model. PF model 
describes the probability of each feature in feature 
set FS taking on specific values when a CFG rule 
A -> ?  is given. To make the model more practical 
in parameter estimation, we assume the features in 
feature set FS are independent from each other, 
thus: 
?
?
=
FSFi
AFiPAFSP ),|(),|( ??                 (5) 
Under this PCFG+PF model, the goal of a parser 
is to choose a parse that maximizes the following 
score: 
    )|,(maxarg)|(
1
AFS iii
n
iT
PSTScore ??
=
=      (6) 
  Our model is thus a simplification of more 
sophisticated models which integrate PCFGs with 
features, such as those in Magerman(1995), 
Collins(1997) and Goodman(1997). Compared 
with these models, our model is more practical 
when only small training data is available, since 
we assume the independence between features. For 
example, in Goodman?s probabilistic feature 
grammar (PFG), each symbol in a PCFG is 
replaced by a set of features, so it can describe 
specific constraints on the rule. In the PFG model 
the generation of each feature is dependent on all 
the previously generated features, thus likely 
leading to severe sparse data problem in parameter 
estimation. Our simplified model assumes 
independence between the features, thus data 
sparseness problem can be significantly alleviated.  
 
4.2  Parameter Estimation 
 
Let F be a feature associated with a string ?, where 
the possible values for F are f1,f2,?,fn, E is the set 
of observations of rule A ? ? in the training 
corpus, and thus E can be divided into n disjoint 
subsets: E1,E2,?,En, corresponding to f1,f2,?,fn 
respectively. The probability of F taking on a value 
of fi given A ? ? can be estimated as follows, 
according to MLE: 
E
E
AfFP ii == ),|( ?                     (7) 
This indicates that feature F adds constraints on 
CFG rule A ? ? by dividing ?, the state space of 
A ? ?, into n disjoint subspaces ?1, ?2,?, ?n, and 
each case of F taking a value of fi given A ? ? is 
viewed as a random event.  
 
5  Experimental Results 
 
5.1 Training and Test Data  
 
A Chinese corpus of 200K words extracted from 
the People?s Daily are segmented, POS-tagged and 
hand-labeled with content chunks in which all the 
trees are binary. The corpus is divided into two 
parts: (1) 180K for training set and (2) 20K for test 
set.  
 
5.2 Metrics and results 
 
We take two kinds of criteria to measure the 
system?s performance: labeled and unlabeled. 
According to the labeled criterion, a recognized 
phrase is correct only if a phrase with the same 
starting position, ending position and the same 
label is found in the gold standard. According to 
the unlabeled criterion, a recognized phrase is 
correct as long as a phrase with the same starting 
position and ending position is found in the gold 
standard.  
 
Table 4   Experimental  Results  
 Labeled Unlabeled 
 P R F P R F 
PCFG 49.91 64.96 56.45 53.33 80.73 65.66 
PCFG+RF in simple phrases 53.25 68.46 59.90 57.46    81.21 67.30 
PCFG +RF in all the phrases 56.47 72.08 63.33 60.07 83.57 69.90 
 
Table 5    Effect of rhythm feature on structural disambiguation 
   Word sequence Rule P(?|A) RF P(RF=[0,1] 
|A,?) 
P(RF=(0,1),?)|A) 
?     ?? 
country  sacrifice 
NC? N V  0.120273 [0,1] 0.08843   0.010636 
?     ?? S   ?  N V 0.161679 [0,1] 0.00292   0.000344 
?    ?? NP? N V 0.063159 [0,1] 0.00213   0.000184 
?    ?? V  ? N V 0.011573 [0,1] 0.0   0.0 
 
Within each criterion, precision, recall and F-
measure are given as metrics for the system?s 
performance. Precision represents how many 
phrases are correct among the phrases recognized, 
recall represents how many phrases in the gold 
standard are correctly recognized, and F-measure 
is defined as follows: 
callecision
callecisionmeasureF
RePr
2RePr
+
??=?  
Table 4 gives the experimental results in three 
different conditions: the first row gives the result 
of PCFG model; the second row gives the result of 
PCFG model integrated with rhythm feature model 
(RF) where only the features of simple phrases are 
considered; the last row gives the result of PCFG 
model plus RF where the rhythm features in all the 
phrases are considered. The results indicate that 
the rhythm features in both simple and complex 
phrases contribute to the improvement of 
performance over PCFG model. We see that the 
rhythm feature improves the labeled F-measure 
6.88 percent and the unlabeled F-measure 4.24 
percent over the unaugmented PCFG model.  
 
5.3 Effect of rhythm feature on parsing 
 
The experiment shows that the rhythm feature can 
help the performance of a parser in Chinese. 
Specifically, the effects of rhythm feature on 
parsing are shown in two ways: 
(1) Help for the disambiguation of phrasal type.  
Table 5 shows the difference of the results 
between PCFG model and PCFG + RF model for 
the sequence ??/country ??/sacrifice? in the 
sentence ?? /the ? /school ? /have 900 ??
/students ?/for ?/country ??/sacrifice? (`900 
students from this school gave their lives for their 
country?). 
In the sentence above, ??/country? is the object 
of preposition ? ? /for?, ? ? /country ? ?
/sacrifice? is not a constituent. But the 
unaugmented PCFG model incorrectly parses it as 
a S(subject-predicate construction). Contrarily, 
according to PCFG+RF model, the type with 
greatest probability is the (correct) NC(non-
constituent) parse. 
(2) Help for pruning.  
Let?s give an example to explain it. For the 
sentence ??? /solve ?? /resident ? /eat ?
/vegetable ? ? /problem ? ? /very ? ?
/difficult?(?It?s very difficult to solve the vegetable 
problem for the residents.?), the number of edges 
generated by the PCFG is 1236, but the number 
decreases to 348 after the rhythm feature is applied, 
thus pruning 73% of the edges. As indicated in 
Table 1, in the rule ?NP -> N V?, P(RF = [1,0] ) = 
0, so ?[??/N ?/V]NP? is pruned after adding 
RF. Similarly, in rule ?NP -> V N?, P(RF = [0, 1] ) 
= 0.003, so ?[?/V ?/N]NP? is pruned since it has 
very low probability. With these two edges pruned, 
more potential edges containing them will not be 
generated. 
 
6 Conclusion 
 
In this paper, we systematically survey the 
distribution of rhythm (number of syllables per 
word or numbers of words per phrase for a 
constituent) in different constructions in Chinese. 
Our analysis suggests that rhythm places strong 
constraints on Chinese syntax. Based on this 
observation, we used the rhythm feature in a 
practical shallow parsing task in which a PCFG 
model is augmented with a probabilistic 
representation of the rhythm feature. The 
experimental results show that the probabilistic 
rhythm feature aids in disambiguation in Chinese 
and thus helps to improve the performance of a 
Chinese parser. We can expect that the 
performance of the parser may further improve 
when more features are considered under the 
probabilistic feature (PF) model. 
 
Acknowledgments 
 
This research was partially supported by the NSF, 
via a KDD extension to NSF IIS-9978025.  
 
References 
 
Church,,K.,1988.A stochastic parts program and 
noun phrase parser for unrestricted text. In 
Proceedings of the Second Conference on 
Applied Natural Language Processing, pp.136-
143.  
 
Collins, M. 1997. Three generative lexicalized 
models for statistical parsing, in Proceedings of 
the 35th Annual Meeting of the ACL, pp. 16-23. 
 
Feng, Shengli. 2000. The Rhythmic syntax of 
Chinese(in Chinese), Shanghai Education Press. 
 
Goodman, J. 1997. Probabilistic Feature 
Grammars, In Proceedings of the International 
Workshop on Parsing Technologies, September 
1997 
 
Magerman, D. 1995. Statistical decision-tree 
models for parsing, in Proceedings of the 33rd 
Annual Meeting of the Association for 
Computational Linguistics, pp.276-283. 
 
Quirk et al 1985. A Comprehensive Grammar of 
English Languge, Longman. 
 
Ramshaw L., and Marcus M. 1995. Text chunking 
using transformation-based learning. In Proc-
eedings of the Third Workshop on Very Large 
Corpora.pp.86-95.  
