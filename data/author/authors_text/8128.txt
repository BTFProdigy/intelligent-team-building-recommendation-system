A Corpus-Based Evaluation of Centering 
and Pronoun Resolution 
Joel R. Tetreault  ? 
University of Rochester 
In this paper we compare pronoun resolution algorithms and introduce a centering algorithm (Left- 
Right Centering) that adheres to the constraints and rules of centering theory and is an alternative 
to Brennan, Friedman, and Pollard's (1987) algorithm. We then use the Left-Right Centering 
algorithm to see if two psycholinguistic claims on Cf-list ranking will actually improve pronoun 
resolution accuracy. Our results from this investigation lead to the development of a new syntax- 
based ranking of the Cf-list and corpus-based evidence that contradicts the psycholinguistic 
claims. 
1. Introduction 
The aims of this paper are to compare implementations of pronoun resolution algo- 
rithms automatically on a common corpus and to see if results from psycholinguistic 
experiments can be used to improve pronoun resolution. Many hand-tested corpus 
evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; 
Strube and Hahn 1999), but these have the drawback of being carried out on small 
corpora. While manual evaluations have the advantage of allowing the researcher to 
examine the data closely, they are problematic because they can be time consuming, 
generally making it difficult to process corpora that are large enough to provide reli- 
able, broadly based statistics. With a system that can run various pronoun resolution 
algorithms, one can easily and quickly analyze large amounts of data and generate 
more reliable results. In this study, this ability to alter an algorithm slightly and test 
its performance is central. 
We first show the attractiveness of the Left-Right Centering algorithm (henceforth 
LRC) (Tetreault 1999) given its incremental processing of utterances, psycholinguistic 
plausibility, and good performance in finding the antecedents of pronouns. The al- 
gorithm is tested against hree other leading pronoun resolution algorithms: Hobbs's 
naive algorithm (1978), S-list (Strube 1998), and BFP (Brennan, Friedman, and Pollard 
1987). Next we use the conclusions from two psycholinguistic experiments on ranking 
the Cf-list, the salience of discourse ntities in prepended phrases (Gordon, Grosz, and 
Gilliom 1993) and the ordering of possessor and possessed in complex NPs (Gordon 
et al 1999), to try to improve the performance of LRC. 
We begin with a brief review of the four algorithms to be compared (Section 2). We 
then discuss the results of the corpus evaluation (Sections 3 and 4). Finally, we show 
that the results from two psycholinguistic experiments, thought to provide a better 
ordering of the Cf-list, do not improve LRC's performance when they are incorporated 
(Section 5). 
* Department of Computer Science, Rochester, NY 14627. E-mail: tetreaul@cs.rochester.edu 
(~) 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 4 
2. Algorithms 
2.1 Hobbs's Algorithm 
Hobbs (1978) presents two algorithms: a naive one based solely on syntax, and a more 
complex one that includes semantics in the resolution method. The naive one (hence- 
forth, the Hobbs algorithm) is the one analyzed here. Unlike the other three algorithms 
analyzed in this project, the Hobbs algorithm does not appeal to any discourse models 
for resolution; rather, the parse tree and grammatical rules are the only information 
used in pronoun resolution. 
The Hobbs algorithm assumes a parse tree in which each NP node has an N type 
node below it as the parent of the lexical object. The algorithm is as follows: 
1. Begin at the NP node immediately dominating the pronoun. 
2. Walk up the tree to the first NP or S encountered. Call this node X, and 
call the path used to reach it p. 
3. Traverse all branches below node X to the left of path p in a left-to- 
right, breadth-first manner. Propose as the antecedent any NP node 
that is encountered which has an NP or S node between it and X. If no 
antecedent is found, proceed to Step 4. 
4. If node X is the highest S node in the sentence, traverse the surface parse 
trees of previous sentences in order of recency, the most recent first; each 
tree is traversed in a left-to-right, breadth-first manner, and when an NP 
node is encountered, propose it as the antecedent. If X is not the highest 
S node in the sentence, continue to Step 5. 
5. From node X, go up the tree to the first NP or S node encountered. Call 
this new node X, and call the path traversed to reach it p. 
6. If X is an NP node and if the path p to X did not pass through the 51 
node that X immediately dominates, propose X as the antecedent. 
7. Traverse all branches below node X to the left of path p in a left-to- 
right, breadth-first manner. Propose any NP node encountered as the 
antecedent. 
8. If X is an S node, traverse all branches of node X to the right of path p 
in a left-to-right, breadth-first manner, but do not go below any NP or S 
node encountered. Propose any NP node encountered as the antecedent. 
9. Go to Step 4. 
A match is "found" when the NP in question matches the pronoun in number, 
gender, and person. The algorithm amounts to walking the parse tree from the pronoun 
in question by stepping through each NP and S on the path to the top S and running a 
breadth-first earch on NP's children left of the path. If a referent cannot be found in the 
current utterance, then the breadth-first strategy is repeated on preceding utterances. 
Hobbs did a hand-based evaluation of his algorithm on three different texts: a 
history chapter, a novel, and a news article. Four pronouns were considered: he, she, 
it, and they. Cases where it refers to a nonrecoverable entity (such as the time or 
weather) were not counted. The algorithm performed successfully on 88.3% of the 300 
pronouns in the corpus. Accuracy increased to 91.7% with the inclusion of selectional 
constraints. 
2.2 Centering Theory and BFP's Algorithm 
Centering theory is part of a larger theory of discourse structure developed by Grosz 
and Sidner (1986). These researchers assert that discourse structure has three compo- 
508 
Tetreault Centering and Pronoun Resolution 
nents: (1) a linguistic structure, which is the structure of the sequence of utterances; 
(2) the intentional structure, which is a structure of discourse-relevant purposes; and 
(3) the attentional state, which is the state of focus. The attentional state models the 
discourse participants' focus of attention determined by the other two structures at 
any one time. Also, it has global and local components hat correspond to the two 
levels of discourse coherence. Centering models the local component of attentional 
state--namely, how the speaker's choice of linguistic entities affects the inference load 
placed upon the hearer in discourse processing. For example, referring to an entity 
with a pronoun signals that the entity is more prominently in focus. 
As described by Brennan, Friedman, and Pollard (1987) (henceforth, BFP) and 
Walker, Iida, and Cote (1994), entities called centers link an utterance with other utter- 
ances in the discourse segment. Each utterance within a discourse has one backward- 
looking center (Cb) and a set of forward-looking centers (Cf). The Cf set for an 
utterance /do is the set of discourse ntities evoked by that utterance. The Cf set is 
ranked according to discourse salience; the most accepted ranking is by grammatical 
role (by subject, direct object, indirect object). The highest-ranked lement of this list is 
called the preferred center (Cp). The Cb represents he most highly ranked element of 
the previous utterance that is found in the current utterance. Essentially, it serves as a 
link between utterances. Abrupt changes in discourse topic are reflected by a change 
of Cb between utterances. In discourses where the change of Cb is minimal, the Cp of 
the utterance represents a prediction of what the Cb will be in the next utterance. 
Grosz, Joshi, and Weinstein (1986, 1995) proposed the following constraints of 
centering theory: 
Constraints 
For each utterance Ui, in a discourse segment D, consisting of utterances of U1 ...  Urn: 
. 
2. 
3. 
There is precisely one Cb. 
Every element of the Cf-list for Ui must be realized in Ui. 
The center, Cb(Ui, D), is the highest-ranked lement of Cf(Ui_I, D) that is 
realized in Ui. 
In addition, they proposed the following rules: 
Rules 
For each utterance U/, in a discourse segment D, consisting of utterances of U1 ...  Urn: 
. 
. 
If some element of Cf(Ui-1, D) is realized as a pronoun in Ui, then so is 
Cb(Ui, D). 
Transition states (defined below) are ordered such that a sequence of 
Continues is preferred over a sequence of Retains, which are preferred 
over sequences of Shifts. 
The relationship between the Cb and Cp of two utterances determines the co- 
herence between the utterances. Centering theory ranks the coherence of adjacent 
utterances with transitions that are determined by the following criteria: 
1. whether or not the Cb is the same from Un-1 to Un; 
2. whether or not this entity coincides with the Cp of U,. 
509 
Computational Linguistics Volume 27, Number 4 
Table 1 
Centering transition table. 
Cb(U.) = Cb(U._O Cb(U.) # Cb(U._~) 
Cb(U,,) = Cp(U,~) Continue Smooth Shift 
Cb(Un) = Cp(Un-1) Retain Rough Shift 
BFP and Walker, Iida, and Cote (1994) identified a finer gradation in the Shift tran- 
sition, stating that Retains were preferred over Smooth Shifts, which were preferred 
over Rough Shifts. Table 1 shows the criteria for each transition. 
Given these constraints and rules, BFP proposed the following pronoun-binding 
algorithm based on centering: 
I. Generate all possible Cb - C f  combinations. 
2. Filter combinations by contraindices and centering rules. 
3. Rank remaining combinations by transitions. 
Walker (1989) compared Hobbs and BFP on three small data sets using hand 
evaluation. The results indicated that the two algorithms performed equivalently over 
a fictional domain of 100 utterances; and Hobbs outperformed BFP over domains 
consisting of newspaper articles (89% to 79%) and a task domain (Tasks) (51% to 
49%). 
2.3 The S-List Approach 
The third approach (Strube 1998) discards the notions of backward- and forward- 
looking centers but maintains the notion of modeling the attentional state. This method, 
the Sqist (salience list), was motivated by the BFP algorithm's problems with incre- 
mentality and computational overhead (it was also difficult o coordinate the algorithm 
with intrasentential resolution). 
2.3.1 The S-List. The model has one structure, the S-list, which "describes the atten- 
tional state of the hearer at any given point in processing a discourse" (Strube 1998, 
page 1252). At first glance, this definition is quite similar to that of a Cfqist; however, 
the two differ in ranking and composition. First, the S-list can contain elements from 
both the current and previous utterance while the Cf-list contains elements from the 
previous utterance alone. Second, the S-list's elements are ranked not by grammatical 
role but by information status and then by surface order. 
The elements of the S-list are separated into three information sets--hearer-old 
discourse entities (OLD), mediated discourse entities (MED), and hearer-new dis- 
course entities (NEW)--all of which are based on Prince's (1981) familiarity scale. The 
three sets are further subdivided: OLD consists of evoked and unused entities; MED 
consists of inferrables, containing inferrables, and anchored brand-new discourse 
intrasentential entities; NEW consists olely of brand-new entities. 
What sorts of NPs fall into these categories? Pronouns and other referring expres- 
sions, as well as previously mentioned proper names, are evoked. Unused entities are 
proper names. Inferrables are entities that are linked to some other entity in the hearer's 
knowledge, but indirectly. Anchored brand-new discourse ntities have as their anchor 
an entity that is OLD. 
510 
Tetreault Centering and Pronoun Resolution 
The three sets are ordered by their information status. OLD entities are preferred 
over MED entities, which are preferred over NEW entities. Within each set, the or- 
dering is by utterance and position in utterance. Basically, an entity of utterance x
is preferred over an entity of utterance y if utterance x follows utterance y. If the 
entities are in the same utterance, they are ranked by position in the sentence: an 
entity close to the beginning of the sentence is preferred over one that is farther 
away. 
2.3.2 Algorithm. The resolution algorithm presented here comes from Strube (1998) 
and personal communication with Michael Strube. 
For each ut terance  (U1 . . .  UN): for each entity within Ui: 
1. If Ui is a pronoun, then find a referent by looking through the S-list left 
to right for one that matches in gender, number, person, and binding 
constraints. Mark entity as EVOKED. I 
2. If Ui is preceded by an indefinite article, then mark Ui as BRAND-NEW. 
3. If Ui is not preceded by a determiner, then mark Ui as UNUSED. 
4. Else mark Ui as ANCHORED BRAND-NEW. 
5. Insert Ui into the S-list given the ranking described above. 
6. Upon completion of Ui remove all entities from the S-list that were not 
realized in Ui. 
In short, the S-list method continually inserts new entities into the S-list in their 
proper positions and "cleanses" the list after each utterance to purge entities that are 
unlikely to be used again in the discourse. Pronoun resolution is a simple lookup in 
the S-list. 
Strube did perform a hand test of the S-list algorithm and the BFP algorithm on 
three short stories by Hemingway and three articles from the New York Times. BFP, 
with intrasentential centering added, successfully resolved 438 pronouns out of 576 
(76%). The S-list approach performed much better (85%). 
2.4 Left-Right Centering Algorithm 
Left-Right Centering (Tetreault 1999) is an algorithm built upon centering theory's 
constraints and rules as detailed in Grosz, Joshi, and Weinstein (1995). The creation 
of the LRC algorithm is motivated by BFP's limitation as a cognitive model  in that it 
makes no provision for incremental resolution of pronouns (Kehler 1997). Psycholin- 
guistic research supports the claim that listeners process utterances one word at a time. 
Therefore, when a listener hears a pronoun, he or she will try to resolve it immediately; 
if new information appears that makes the original choice incorrect (such as a violation 
of binding constraints), the listener will go back and find a correct antecedent. 
Responding to the lack of incremental processing in the BFP model, we have 
constructed an incremental resolution algorithm that adheres to centering constraints. 
It works by first searching for an antecedent in the current utterance; 2 if one is not 
found, then the previous Cf-lists (starting with the previous utterance) are searched 
1 In the original S-list formulation, pronouns are not the only entities that can be marked as EVOKED; 
nominal anaphora nd previously mentioned proper names (to name just two) can also be EVOKED 
(Strube and Hahn 1999). In our implementation, pronouns are the only entities that can fall in this 
category. 
2 In this project, a sentence is considered an utterance. 
511 
Computational Linguistics Volume 27, Number 4 
left to right for an antecedent: 
1. Preprocessing--from previous utterance: Cb(Un-1) and Cf(Un_l) are  
available. 
2. Process utterance--parse and extract incrementally from Un all refer- 
ences to discourse ntities. For each pronoun do: 
(a) Search for an antecedent intrasententially in Cf-partial(Un) 3 that 
meet feature and binding constraints. 
If one is found, proceed to the next pronoun within utterance. Else 
go to (b). 
(b) Search for an antecedent intersententially in Cf(Un-1) that meets 
feature and binding constraints. 
3. Create Cf--create Cf-list of Un by ranking discourse ntities of Un ac- 
cording to grammatical function. Our implementation used a left-to-right 
breadth-first walk of the parse tree to approximate sorting by grammat- 
ical function. 
It should be noted that while BFP makes use of Rule 2 of centering theory, LRC 
does not since Rule 2's role in pronoun resolution is not yet known (see Kehler \[1997\] 
for a critique of its use by BFP). 
The preference for searching intrasententially before intersententially is motivated 
by the fact that large sentences are not broken up into clauses as Kameyama (1998) 
proposes. By looking through the Cf-partial, clause-by-clause c ntering is roughly ap- 
proximated. In addition, the antecedents of reflexive pronouns are found by searching 
Cf-partial right to left because their referents are usually found in the minimal S. 
There are two important points to be made about centering and pronoun resolu- 
tion. First, centering is not a pronoun resolution method; the fact that pronouns can 
be resolved is simply a side effect of the constraints and rules. Second, ranking by 
grammatical role is very naive. In a perfect world, the Cf-list would consist of entities 
ranked by a combination ofsyntax and semantics. In our study, ranking is based solely 
on syntax. 
3. Evaluation of Algorithms 
3.1 Data 
All four algorithms were compared on two domains taken from the Penn Treebank 
annotated corpus (Marcus, Santorini, and Marcinkiewicz 1993). The first domain con- 
sists of 3,900 utterances (1,694 unquoted pronouns) in New York Times articles provided 
by Ge, Hale, and Charniak (1998), who annotated the corpus with coreference infor- 
mation. The corpus consists of 195 different newspaper articles. Sentences are fully 
bracketed and have labels that indicate part of speech and number. Pronouns and 
their antecedent entities are all marked with the same tag to facilitate coreference 
verification. In addition, the subject NP of each S subconstituent is marked. 
The second domain consists of 553 utterances (511 unquoted pronouns) in three 
fictional texts taken from the Penn Treebank corpus, which we annotated in the same 
manner as Ge, Hale, and Charniak's corpus. The second domain differs from the first 
in that the sentences are generally shorter and less complex, and contain more hes 
and shes. 
3 Cf-partial isa list of all processed discourse entities in Un. 
512 
Tetreault Centering and Pronoun Resolution 
3.2 Method 
The evaluation (Byron and Tetreault 1999) consisted of two steps: (1) parsing Penn 
Treebank utterances and (2) running the four algorithms. The parsing stage involved 
extracting discourse ntities from the Penn Treebank utterances. Since we were solely 
concerned with pronouns having NP antecedents, we extracted only NPs. For each 
NP we generated a "filecard" that stored its syntactic information. This information 
included agreement properties, syntactic type, parent nodes, depth in tree, position 
in utterance, presence or absence of a determiner, gender, coreference tag, utterance 
number, whether it was quoted, commanding verb, whether it was part of a title, 
whether it was reflexive, whether it was part of a possessive NP, whether it was in 
a prepended phrase, and whether it was part of a conjoined sentence. The entities 
were listed in each utterance in order of mention except in the case of conjoined NPs. 
Conjoined entities uch as John and Mary were realized as three entities: the singular 
entities John and Mary and the plural John and Mary. The plural entity was placed 
ahead of the singular ones in the Cf-list, on the basis of research by Gordon et al 
(1999). 
Conjoined utterances were broken up into their subutterances. For example, the 
utterance United Illuminating isbased in New Haven, Conn., and Northeast isbased in Hartford, 
Conn. was replaced by the two utterances United Illuminating isbased in New Haven, Conn. 
and Northeast is based in Hartford, Conn. This strategy was inspired by Kameyama's 
(1998) methods for dealing with complex sentences; it improves the accuracy of each 
algorithm by 1% to 2%. 
The second stage involved running each algorithm on the parsed forms of the 
Penn Treebank utterances. For all algorithms, we used the same guidelines as Strube 
and Hahn (1999): no world knowledge was assumed, only agreement criteria (gender, 
number) and binding constraints were applied. Unlike Strube and Hahn, we did not 
make use of sortal constraints. The number of each NP could be extracted from the 
Penn Treebank annotations, but gender had to be hand-coded. A database of all NPs 
was tagged with their gender (masculine, feminine, neuter). NPs such as president 
or banker were marked as androgynous since it is possible to refer to them with a 
gendered pronoun. Entities within quotes were removed from the evaluation since 
the S-list algorithm and BFP do not allow resolution of quoted text. 
We depart from Walker's (1989) and Strube and Hahn's (1999) evaluations by not 
defining any discourse segments. Walker defines a discourse segment as a paragraph 
(unless the first sentence of the paragraph as a pronoun in subject position or unless 
it has a pronoun with no antecedent among the preceding NPs that match syntactic 
features). Instead, we divide our corpora only by discourses (newspaper article or 
story). Once a new discourse is encountered, the history list for each algorithm (be it 
the Cf-list or S-list) is cleared. Using discourse segments should increase the efficiency 
of all algorithms ince it constrains the search space significantly. 
Unlike Walker (1989), we do not account for false positives or error chains; instead, 
we use a "location'-based valuation procedure. Error chains occur when a pronoun 
P6 refers to a pronoun P6 that was resolved incorrectly to entity Ek (where P6 and Pil 
evoke the same entity El). So P/2 would corefer incorrectly with Ek. In our evaluation, 
a coreference is deemed correct if it corefers with an NP that has the same coreference 
tag. So in the above situation, Pi2 would be deemed correct since it was matched to 
an expression that should realize the correct entity. 
3.3 Algorithm Modifications 
The BFP algorithm had to be modified slightly to compensate for underspecifications 
in its intrasentential resolution. We follow the same method as Strube and Hahn (1999); 
513 
Computational Linguistics Volume 27, Number 4 
that is, we first try to resolve pronouns intersententially using the BFP algorithm. If
there are pronouns left unresolved, we search for an antecedent left to right in the 
same utterance. Strube and Hahn use Kameyama's (1998) specifications for complex 
sentences to break up utterances into smaller components. We keep the utterances 
whole (with the exception of splitting conjoined utterances). 
As an aside, the BFP algorithm can be modified (Walker 1989) so that intrasen- 
tential antecedents are given a higher preference. To quote Walker, the alteration (sug- 
gested by Carter \[1987\]) involves electing intrasentential c ndidates "only in the cases 
where no discourse center has been established or the discourse center has been re- 
jected for syntactic or selectional reasons" (page 258). Walker applied the modification 
and was able to boost BFP's accuracy to 93% correct over the fiction corpus, 84% on 
Newsweek articles, and 64% on Tasks (up from 90%, 79%, and 49%, respectively). BFP 
with Carter's modification may seem quite similar to LRC except for two points. First, 
LRC seeks antecedents intrasententially regardless of the status of the discourse center. 
Second, LRC does not use Rule 2 in constraining possible antecedents intersententially, 
while BFP does so. 
Because the S-list approach incorporates both semantics and syntax in its famil- 
iarity ranking scheme, a shallow version that uses only syntax is implemented in this 
study. This means that inferrables are not represented and entities rementioned asNPs 
may be underrepresented in the ranking. 
Both the BFP and S-list algorithms were modified so that they have the ability to 
look back through all past Cf/S-lists. This puts the two algorithms on equal footing 
with the Hobbs and LRC algorithms, which allow one to look back as far as possible 
within the discourse. 
Hobbs (1978) makes use of selectional constraints ohelp refine the search space for 
neutral pronouns uch as it. We do not use selectional constraints in this syntax-only 
study. 
3.4 Results 
Two naive algorithms were created to serve as a baseline for results. The first, "most 
recent," keeps a history list of all entities een within the discourse unit. The most 
recent entity that matches in gender, number, and binding constraints i selected as 
the antecedent for the pronoun. This method correctly resolves 60% of pronouns in 
both domains. 
A slightly more complex baseline involves using the LRC algorithm but random- 
izing all Cf-lists considered. So, in the intrasentential component, he ranking of the 
entities in Cf-partial is random. Previous Cf-lists are also randomized after being pro- 
cessed. This method actually does well (69%) compared with the "intelligent" algo- 
rithms, in part because of its preference for intrasentential entities. 
Tables 2 and 3 include results for the different algorithms over the two domains. 
"Success rate" as defined by Mitkov (2000) is the number of successfully resolved pro- 
nouns divided by the total number of pronouns. Two variations of LRC are included 
as further baselines. LRCsurf ranks its Cf-list by surface order only. LRC ranks the 
Cf-list by grammatical function. LRC-F is the best instantiation of LRC and involves 
moving entities in a prepended phrase to the back of the Cf-list (which is still ranked 
by grammatical function). LRC-P ranks its entities the same way as LRC-F except hat 
it then moves all pronouns to the head of the Cf-list (maintaining original order). This 
algorithm was meant o be a hybrid of the S-list and LRC algorithms with the hope 
that performance would be increased by giving weight to pronouns ince they would 
be more likely to continue the backward-looking center. 
514 
Tetreault Centering and Pronoun Resolution 
Table 2 
Pronoun resolution algorithms for New York Times articles. 
Algorithm Right Success Rate % Right Intra % Right Inter 
BFP 1004 59.4 75.1 48.0 
Random Cf 1175 69.4 70.2 66.7 
S-list 1211 71.7 74.1 67.5 
LRCsurf 1266 74.7 72.0 81.6 
LRC 1268 74.9 72.0 82.0 
Hobbs 1298 76.8 74.2 82.0 
LRC-F 1362 80.4 77.7 87.3 
LRC-P 1362 80.4 77.7 87.3 
Table 3 
Pronoun resolution algorithms for fictional texts. 
Algorithm Right Success Rate % Right Intra % Right Inter 
BFP 241 46.4 81.8 43.8 
S-list 337 66.1 84.4 56.5 
Random Cf 367 71.1 84.3 62.5 
LRCsurf 372 72.1 84.3 64.2 
LRC 372 72.1 84.3 64.2 
LRC-P 378 74.0 84.3 66.2 
Hobbs 414 80.1 85.8 75.2 
LRC-F 420 81.1 86.0 76.2 
4. Discussion 
For this study, we use McNemar's test to test whether the difference in performance of
two algorithms is significant. We adopt the standard statistical convention of p < 0.05 
for determining whether the relative performance is indeed significant. 
First, we consider LRC in relation to the classical algorithms: Hobbs, BFP, and 
S-list. We found a significant difference in the performance of all four algorithms (e.g., 
LRC and S-list: p < 0.00479), though Hobbs and LRC performed the closest in terms 
of getting the same pronouns right. These two algorithms perform similarly for two 
reasons. First, both search for referents intrasententially and then intersententially. In 
the New York Times corpus, over 71% of all pronouns have intrasentential referents, 
so clearly an algorithm that favors the current utterance will perform better. Second, 
both search their respective data structures in a salience-first manner. Intersententially, 
both examine previous utterances in the same manner: breadth-first based on syntax. 
Intrasententially, Hobbs does slightly better since it first favors antecedents close to 
the pronoun before searching the rest of the tree. LRC favors entities near the head 
of the sentence under the assumption that they are more salient. These algorithms' 
similarities in intra- and intersentential evaluation are reflected in the similarities in 
their percentage correct for the respective categories. 
Although S-list performed worse than LRC over the New York Times corpus, it 
did fare better over the fictional texts. This is due to the high density of pronouns in 
these texts, which S-list would rank higher in its salience list since they are hearer-old. 
It should be restated that a shallow version (syntax only) of the S-list algorithm is 
implemented here. 
515 
Computational Linguistics Volume 27, Number 4 
The standing of the BFP algorithm should not be surprising given past studies. 
For example, Strube (1998) found that the S-list algorithm performed at 91% correct on 
three New York Times articles, while the best version of BFP performed at 81%. This 10% 
difference is reflected in the present evaluation as well. The main drawback for BFP 
was its preference for intersentential resolution. Also, BFP, as formally defined, does 
not have an intrasentential processing mechanism. For the purposes of the project, 
the LRC intrasentential technique was used to resolve pronouns that could not be 
resolved by the BFP (intersentential) lgorithm. It is unclear whether this is the optimal 
intrasentential gorithm for BFP. 
LRC-F is much better than LRC alone considering its improvement of over 5% 
in the newspaper article domain and over 7% in the fictional domain. This increase 
is discussed in the following section. The hybrid algorithm (LRC-P) has the same 
accuracy rate as LRC-F, though each gets 5 instances right that the other does not. 
5. Examining Psycholinguistic Claims of Centering 
Having established LRC as a fair model of centering iven its performance and incre- 
mental processing of utterances, we can use it to test empirically whether psycholin- 
guistic claims about the ordering of the Cf-list are reflected in an increase in accuracy 
in resolving pronouns. The reasoning behind the following corpus tests is that if the 
predictions made by psycholinguistic experiments fail to increase performance or even 
lower performance, then this suggests that the claims may not be useful. As Suri, Mc- 
Coy, and DeCristofaro (1999, page 180) point out: "the corpus analysis reveals how 
language is actually used in practice, rather than depending on a small set of dis- 
courses presented to the human subjects." 
In this section, we use our corpus evaluation to provide counterevidence to the 
claims made about using genitives and prepended phrases to rank the Cf-list, and we 
propose a new Cf-list ranking based on these results. 
5.1 Moving Prepended Phrases 
Gordon, Grosz, and Gilliom (1993) carried out five self-paced reading time experiments 
that provided evidence for the major tenets of centering theory: that the backward- 
looking center (Cb) should be realized as a pronoun and that the grammatical subject of 
an utterance is most likely to be the Cb if possible. Their final experiment showed that 
surface position also plays a role in ranking the Cf-list. They observed that entities 
in surface-initial nonsubject positions in the previous sentence had about the same 
repeated-name p nalty as an entity that had been the noninitial subject of the previous 
sentence. These results can be interpreted to mean that entities in subject position and 
in prepended phrases (nonsubject surface-initial positions) are equally likely to be 
the Cb. 
So the claim we wished to test was whether sentence-initial nd subject position 
can serve equally well as the Cb. To evaluate this claim, we changed our parser to find 
the subject of the utterance. By tagging the subject, we know what entities constitute 
the prepended phrase (since they precede the subject). We developed two different 
methods of locating the subject. The first simply takes the first NP that is the subject 
of the first S constituent. It is possible that this S constituent is not the top-level S 
structure and may even be embedded in a prepended phrase. This method is called 
LRC-F since it takes the first subject NP found. The second method (LRC-S) selects 
the NP that is the subject of the topqevel S structure. If one cannot be found, then the 
system defaults to the first method. The result of both tagging methods is that all NPs 
preceding the chosen subject are marked as being in a prepended phrase. 
516 
Tetreault Centering and Pronoun Resolution 
Table 4 
Prepended phrase movement experiments over New York Times articles. 
Prepended Movement Standard Sort 
Algorithm Norm Pre Norm Pre 
LRC-F 76.21% 80.40% 79.63% 75.50% 
LRC-S 75.97% 80.08% 78.81% 74.85% 
Eight different corpus trials were carried out involving the two different parsing 
algorithms (LRC-F and LRC-S) and two different ordering modifications: (1) ranking 
the Cf-list after processing and (2) modifying the order of entities before processing 
the utterance. The standard Cf-list consists of ranking entities by grammatical role 
and surface order. As a result, prepended phrases would still be ranked ahead of the 
main subject. The modified Cf-list consists of ranking the main clause by grammatical 
role and placing all entities in the prepended phrase after all entities from the main 
clause. The second method involves reordering the utterance before processing. This 
technique was motivated mostly by the order we selected for pronoun resolution: an 
antecedent is first searched for in the Cf-partial, then in the past Cf-lists, and finally 
in the entities of the same utterance not in the Cf-partial. Pronouns in prepended 
phrases frequently refer to the subject of the same utterance as well as to entities in 
the previous utterance. Moving the prepended entities after the main clause entities 
before evaluation achieves the same result as looking in the main clause before the 
intersentential search. 
Table 4 contains the results of the trials over the New York Times domain. "Prepended 
movement" refers to ranking the Cf-list with prepended entities moved to the end of 
the main clause; "Standard sort" refers to maintaining the order of the Cf-list. "Norm" 
means that prepended entities were not moved before the utterance was processed. 
"Pre" means that the entities were placed behind the main clause. 
All statistics (within the respective algorithms) were deemed significant relative 
to each other using McNemar's test. However, it should be noted that between the 
best performers for LRC-F and LRC-S (movement of prepended phrases before and 
after Cf-list, column 2), the difference in performance is insignificant (p ~ 0.624). This 
indicates that the two algorithms fare the same. The conclusion is that if an algorithm 
prefers the subject and marks entities in prepended phrases as less salient, it will 
resolve pronouns better. 
5.2 Ranking Complex NPs 
The second claim we wished to test involved ranking possessor and possessed enti- 
ties realized in complex NPs. Walker and Prince (1996) developed the complex NP 
assumption that "In English, when an NP evokes multiple discourse ntities, such as 
a subject NP with a possessive pronoun, we assume that the Cf ordering is from left 
to right within the higher NP" (page 8). So the Cf-list for the utterance Her mother 
knows Queen Elizabeth would be {her, mother, Elizabeth}. Walker and Prince note that 
the theory is just a hypothesis but motivate its plausibility with a complex example. 
However, a series of psycholinguistic experiments carried out by Gordon et al 
(1999) refute Walker and Prince's claim that the entities are ordered left to right. Gordon 
et al found that subjects had faster reading rates for small discourses in which a 
pronoun referred to the possessed entity rather than the possessor entity. 
517 
Computational Linguistics Volume 27, Number 4 
Table 5 
Results of evaluating pronoun resolution algorithms for New York Times articles. 
Algorithm WP +Gen +Pos 
LRC-F 80.04% 79.99% 78.41% 
LRC-S 80.40% 80.34% 78.83% 
LRC 76.15% 76.03% 74.47% 
Table 6 
Results of evaluating pronoun resolution algorithms for Fiction texts. 
Algorithm WP +Gen +Pos 
LRC-F 79.73% 79.58% 79.42% 
LRC-S 81.08% 81.08% 80.88% 
LRC 80.31% 80.15% 79.81% 
Hobbs (1978) also assumes Gordon et al's interpretation in his pronoun algorithm. 
He assumes that possessor entities are nested deeper in the parse tree, so when the 
algorithm does a breadth-first earch of the tree, it considers the possessed NP to be 
the most prominent. 
To see which claim is correct, we altered the Cf-list ranking to put possessed 
entities before possessor entities. The original LRC ordered them left to right as Walker 
and Prince (WP) suggest. Tables 5 and 6 include results for both domains. "+gen" 
indicates that only complex NPs containing enitive pronouns were reversed; "+pos" 
indicates that all possessive NPs were reversed, matching Gordon et al's study. The 
results indicate for both domains that Walker and Prince's theory works better, though 
marginally (for all domains and algorithms, ignificance l vels between WP and +gen 
are under 0.05). For the New York Times domain, the difference in the actual number 
correct between LRC-S with WP and LRC-S with +pos is 1,362 to 1,337 or 25 pronouns, 
which is substantial (p (1.4e-06) over a corpus of 1,691 pronouns. Likewise, for the 
fictional texts, 1 extra pronoun is resolved incorrectly when using Gordon et al's 
method. 
Looking at the difference in what each algorithm gets right and wrong, it seems 
that type of referring expression and mention count play a role in which entity should 
be selected from the complex NP. If an entity has been mentioned previously or is 
realized as a pronoun, it is more likely to be the referent of a following pronoun. 
This would lend support o Strube and Hahn's S-list and functional centering theories 
(Strube and Hahn 1996), which maintain that type of referring expression and previous 
mention influence the salience of each entity with the S-list or Cf-list. 
6. Conclusions 
In this paper we first presented a new pronoun resolution algorithm, Left-Right Center- 
ing, which adheres to the constraints of centering theory and was inspired by the need 
to remedy a lack of incremental processing in Brennan, Friedman, and Pollard's (1987) 
method. Second, we compared LRC's performance with that of three other leading 
pronoun resolution algorithms, each one restricted to using only syntactic informa- 
tion. This comparison is significant in its own right because these algorithms have not 
been previously compared, in computer-encoded form, on a common corpus. Coding 
518 
Tetreault Centering and Pronoun Resolution 
all the algorithms allows one to quickly test them on a large corpus and eliminates 
human error. Third, we tried to improve LRC's performance by incorporating theories 
on Cf-list construction derived from psycholinguistic experiments. Our corpus-based 
evaluation showed that prepended phrases hould not be ranked prominently in the 
Cf-list as Gordon, Grosz, and Gilliom (1993) suggest. Our results also showed that 
Walker and Prince's (1996) complex NP assumption performs marginally better than 
the opposite theory based on experimental results. We believe that corpus-based anal- 
yses such as this one not only increase performance in resolution algorithms but also 
can aid in validating the results of psycholinguistic studies, which are usually based 
on small sequences of utterances. 
7. Future Work 
The next step is to research ways of breaking up complex utterances and applying 
centering to these utterances. An overlooked area of research, the incorporation of 
quoted phrases into centering and pronoun resolution, should be explored. Research 
into how transitions and the backward-looking center can be used in a pronoun res- 
olution algorithm should also be carried out. Strube and Hahn (1996) developed a
heuristic of ranking transition pairs by cost to evaluate different Cf-ranking schemes. 
Perhaps this heuristic ould be used to constrain the search for antecedents. 
It should be noted that all the algorithms analyzed in this paper are syntax based 
(or modified to be syntax based). Incorporating semantic information such as sortal 
constraints would be the next logical development for the system. We believe that 
purely syntax-based resolution algorithms probably have an upper bound of perfor- 
mance in the mid 80s and that developing an algorithm that achieves 90% or better 
accuracy over several domains requires emantic knowledge. In short, the results pre- 
sented here suggest that purely syntactic methods cannot be pushed much farther, and 
the upper limit reached can serve as a baseline for approaches that combine syntax 
and semantics. 
There are several other psycholinguistic experiments hat can be verified using our 
computational corpus-based approach. The effects of parallelism and other complex 
NPs such as plurals still need to be investigated computationally. 
Acknowledgments 
I am grateful to Barbara Grosz for aiding me 
in the development of the LRC algorithm 
and for discussing centering theory issues. I 
am also grateful to Donna Byron, who was 
responsible for much brainstorming, 
cross-checking of results, and coding of the 
Hobbs algorithm. I am thankful as well for 
Jenny Rogers's work in annotating the 
fictional texts from the Penn Treebank in the 
same style used by Ge, Hale, and Chamiak 
(1998). Special thanks go to Michael Strube, 
James Allen, Lenhart Schubert, and Mark 
Core for advice and brainstorming. I would 
also like to thank Eugene Charniak and 
Niyu Ge for the annotated, parsed Penn 
Treebank corpus, which proved 
invaluable. 
Partial support for the research reported 
in this paper was provided by the National 
Science Foundation under Grants 
IRI-90-09018, IRI-94-04756, and 
CDA-94-01024 to Harvard University and 
by DARPA Research Grant F30602-98-2-0133 
to the University of Rochester. 
References 
Brennan, Susan E., Marilyn W. Friedman, 
and Carl J. Pollard. 1987. A centering 
approach to pronouns. In Proceedings o/the 
25th Annual Meeting of the Association for 
Computational Linguistics, pages 155-162. 
Byron, Donna K. and Joel R. Tetreault. 1999. 
A flexible architecture for reference 
resolution. In Proceedings ofthe 9th 
Conference ofthe European Chapter of the 
Association for Computational Linguistics, 
pages 229-232. 
Carter, David M. 1987. Interpreting Anaphors 
in Natural Language Texts. Ellis Horwood, 
Chichester, UK. 
519 
Computational Linguistics Volume 27, Number 4 
Ge, Niyu, John Hale, and Eugene Charniak. 
1998. A statistical approach to anaphora 
resolution. In Proceedings of the Sixth 
Workshop on Very Large Corpora, 
pages 161-170. 
Gordon, Peter C., Barbara J. Grosz, and 
Laura Gilliom. 1993. Pronouns, names 
and the centering of attention in 
discourse. Cognitive Science, 17(3):311-348. 
Gordon, Peter C., Randall Hendrick, Kerry 
Ledoux, and Chin Lung Yang. 1999. 
Processing of reference and the structure 
of language: An analysis of complex noun 
phrases. Language and Cognitive Processes, 
14(4):353-379. 
Grosz, Barbara J., Aravind K. Joshi, and 
Scott Weinstein. 1986. Towards a 
computational theory of discourse 
interpretation. Preliminary draft. 
Grosz, Barbara J., Aravind K. Joshi, and 
Scott Weinstein. 1995. Centering: A 
framework for modeling the local 
coherence of discourse. Computational 
Linguistics, 21(2):203-225. 
Grosz, Barbara J. and Candace L. Sidner. 
1986. Attention, intentions, and the 
structure of discourse. Computational 
Linguistics, 12(3):175-204. 
Hobbs, Jerry R. 1978. Resolving pronoun 
references. Lingua, 44:311-338. 
Kameyama, Megumi. 1998. Intrasentential 
centering: A case study. In M. A. Walker, 
A. K. Joshi, and E. F. Prince, editors, 
Centering Theory in Discourse, 89-112. 
Oxford University Press. 
Kehler, Andrew. 1997. Current theories of 
centering for pronoun interpretation: A 
critical evaluation. Computational 
Linguistics, 23(3):467-475. 
Marcus, Mitchell P., Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Building 
a large annotated corpus of English: The 
Penn Treebank. Computational Linguistics, 
19(2):313-330. 
Mitkov, Ruslan. 1998. Robust pronoun 
resolution with limited knowledge. In 
Proceedings of the 36th Meeting of the 
Association for Computational Linguistics and 
the 17th International Conference on 
Computational Linguistics, pages 869- 
875. 
Mitkov, Ruslan. 2000. Towards a more 
consistent and comprehensive evaluation 
of anaphora resolution algorithms and 
systems. In Proceedings qf the Discourse 
Anaphora nd Reference Resolution 
Conference (DAARC2000), 96-107. 
Prince, Ellen F. 1981. Towards a taxonomy 
of given-new information. In P. Cole, 
editor, Radical Pragmatics, 223-255. 
Academic Press. 
Strube, Michael. 1998. Never look back: An 
alternative to centering. In Proceedings of 
the 36th Meeting of the Association for 
Computational Linguistics and the 17th 
International Conference on Computational 
Linguistics, volume 2, pages 1251-1257. 
Strube, Michael and Udo Hahn. 1996. 
Functional centering. In Proceedings of the 
34th Annual Meeting of the Association for 
Computational Linguistics, pages 270-277. 
Strube, Michael and Udo Hatch. 1999. 
Functional centering: Grounding 
referential coherence in information 
structure. Computational Linguistics, 
25(3):309-344. 
Suri, Linda Z., Kathleen F. McCoy, and 
Jonathan D. DeCristofaro. 1999. A 
methodology for extending focusing 
frameworks. Computational Linguistics, 
25(2):173-194. 
Tetreault, Joel. 1999. Analysis of 
syntax-based pronoun resolution 
methods. In Proceedings of the 37th Annual 
Meeting of the Association for Computational 
Linguistics, pages 602-605. 
Walker, Marilyn A. 1989. Evaluating 
discourse processing algorithms. In 
Proceedings of the 27th Annual Meeting of the 
Association for Computational Linguistics, 
pages 251-261. 
Walker, Marilyn A., Masayo Iida, and 
Sharon Cote. 1994. Japanese discourse 
and the process of centering. 
Computational Linguistics, 20(2):193-233. 
Walker, Marilyn A. and Ellen F. Prince. 
1996. A bilateral approach to givenness: A
hearer-status algorithm and a centering 
algorithm. In T. Fretheim and J. K. 
Gundel, editors, Reference and Referent 
Accessibility, John Benjamins, Amsterdam, 
pages 291-306. 
520 
Discourse Annotation in the Monroe Corpus
Joel Tetreault   , Mary Swift   , Preethum Prithviraj   , Myroslava Dzikovska  , James Allen  
 
Department of Computer Science, University of Rochester, Rochester, NY, 14620, USA
tetreaul,swift,prithvir,james@cs.rochester.edu

Human Communications Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW
mdzikovs@inf.ed.ac.uk
Abstract
We describe a method for annotating spoken dia-
log corpora using both automatic and manual an-
notation. Our semi-automated method for corpus
development results in a corpus combining rich se-
mantics, discourse information and reference anno-
tation, and allows us to explore issues relating these.
1 Introduction
Discourse information plays an important part in
natural language systems performing tasks such
as text summarization, question-answering systems
and collaborative planning. But the type of dis-
course information that is relevant varies widely de-
pending on domain, genre, number of participants,
whether it is written or spoken, etc. Therefore em-
pirical analysis is necessary to determine common-
alities in the variations of discourse and develop
general purpose algorithms for discourse analysis.
The heightened interest in human language tech-
nologies in the last decade has sparked several dis-
course annotation projects. Though there has been
a lot of research, many of the projects focus on a
few specific areas of discourse relevant to their re-
spective system. For example, a text summarization
system working on texts from the web would not
need to know about dialogue modeling or ground-
ing or prosody. In contrast, for a spoken dialogue
system that collaborates with a user, such informa-
tion is crucial but the organization of web pages is
not.
In this paper we describe our work in the Monroe
Project, an effort targeting the production and use of
a linguistically rich annotated corpus of a series of
task-oriented spoken dialogs in an emergency res-
cue domain. Our project differs from past projects
involving reference annotation and discourse seg-
mentation in that the semantics and discourse infor-
mation is generated automatically. Most other work
in this area has had minimal semantics or speech
act tagging, if anything at all, which can be quite
labor intensive to annotate. In addition, our domain
is spoken language, which is rarely annotated for
the information we are providing. We describe our
research on reference resolution and discourse seg-
mentation using the annotated corpus and the soft-
ware tools we have developed to help us with differ-
ent aspects of the annotation tasks.
2 Aims of Monroe Project
2.1 Parser Development
One of the aims of the Monroe Project was to de-
velop a wide coverage grammar for spoken dia-
logue. Since parsing is just an initial stage of natural
language understanding, the project was focused not
just on obtaining syntactic trees alone (as is done
in many other parsed corpora, for example, Penn
TreeBank (Marcus et al, 1993) or Tiger (Brants
and Plaehn, 2000)). Instead, we aimed to develop a
parser and grammar for the production of syntactic
parses and semantic representations useful in dis-
course processing.
The parser produces a domain-independent se-
mantic representation with information necessary
for referential and discourse processing, in par-
ticular, domain-independent representations of de-
terminers and quantifiers (to be resolved by our
reference module), domain-independent represen-
tations for discourse adverbials, and tense, aspect
and modality information. This necessitated the de-
velopment of a domain-independent logical form
syntax and a domain-independent ontology as a
source of semantic types for our representations
(Dzikovska et al, 2004). In subsequent sections
we discuss how the parser-generated representations
are used as a basis for discourse annotation.
2.2 Reference Resolution Development
In spoken dialogue, choice of referring expression
is influential and influenced by the main entities be-
ing discussed and the intentions of the speaker. If
an entity is mentioned frequently, and thus is very
important to the current topic, it is usually pronom-
inalized. Psycholinguistic studies show that salient
terms are usually evoked as pronouns because of the
lighter inference load they place on the listener. Be-
cause pronouns occur frequently in discourse, it is
very important to know what they resolve to, so the
entire sentence can be processed correctly. A cor-
pus annotated for reference relations allows one to
compare the performance of different reference al-
gorithms.
2.3 Discourse Segmentation
Another research area that can benefit from a
discourse-annotated corpus is discourse structure.
There has been plenty of theoretical work such as
(Grosz and Sidner, 1986), (Moser and Moore, 1996)
which shows that just as sentences can be decom-
posed into smaller constituents, a discourse can be
decomposed into smaller units called discourse seg-
ments. Though there are many different ways to
segment discourse, the common themes are that
some sequences are more closely related than oth-
ers (discourse segments) and that a discourse can be
organized as a tree, with the leaves being the indi-
vidual utterances and the interior nodes being dis-
course segments. The embeddedness of a segment
effects which previous segments, and thus their enti-
ties, are accessible. As a discourse progresses, seg-
ments close and unless they are close to the root of
the tree (have a low embedding) may not be acces-
sible.
Discourse segmentation has implications for spo-
ken dialogue systems. Properly detecting discourse
structure can lead to improved reference resolution
accuracy since competing antecedents in inacces-
sible clauses may be removed from consideration.
Discourse segmentation is often closely related to
plan and intention recognition, so recognizing one
can lead to better detection of the other. Finally,
segmentation reduces the size of the history or con-
text maintained by a spoken dialogue system, thus
decreasing the search space for referents.
3 Monroe Corpus Construction
The Monroe domain is a series of task-oriented di-
alogs between human participants (Stent, 2001) de-
signed to encourage collaborative problem-solving
and mixed-initiative interaction. It is a simulated
rescue operation domain in which a controller re-
ceives emergency calls and is assisted by a system
or another person in formulating a plan to handle
emergencies ranging from requests for medical as-
sistance to civil disorder to snow storms. Available
resources include maps, repair crews, plows, ambu-
lances, helicopters and police.
Each dialog consisted of the execution of one
task which lasted about ten minutes. The two par-
ticipants were told to construct a plan as if they
were in an emergency control center. Each ses-
sion was recorded to audio and video, then broken
up into utterances under the guidelines of (Heeman
and Allen, 1994). Finally, the segmented audio files
were transcribed by hand. The entire Monroe cor-
pus consists of 20 dialogs. The annotation work we
report here is based on 5 dialogs totaling 1756 utter-
ances 1.
Discourse annotation of the Monroe Corpus con-
sisted of three phases: first, a semi-automated anno-
tation loop that resulted in parser-generated syntac-
tic and semantic analyses for each sentence. Sec-
ond, the corpus was manually annotated for refer-
ence information for pronouns and coreferential in-
formation for definite noun phrases. Finally, dis-
course segmentation was conducted manually. In
the following sections we discuss each of the three
phases in more detail.
3.1 Building the Parsed Corpus
To build the annotated corpus, we needed to first
have a parsed corpus as a source of discourse en-
tities. We built a suite of tools to rapidly develop
parsed corpora (Swift et al, 2004). These are Java
GUI for annotating speech repairs, a LISP tool to
parse annotated corpora and merge in changes, and
a Java tool interface to manually check the automat-
ically generated parser analyses (the CorpusTool).
Our goal in building the parsed corpus is to obtain
the output suitable for further annotation for refer-
ence and discourse information. In particular, the
parser achieves the following:
  Identifies the referring expressions. These are
definite noun phrases, but also verb phrases
and propositions which can be referred to by
deictic pronouns such as that. All entities are
assigned a unique variable name which can be
used to identify the referent later.
  Identifies implicit entities. These are implicit
subjects of imperatives, and also some implicit
arguments of relational nouns (e.g., the implied
object in the phrase the weight) and of adver-
bials (e.g., the implied reference time in That
happened before).
  Identifies speech acts. These are based on the
syntactic form of the utterance only, but they
provide an initial analysis which can later be
extended in annotation.
Examples of the logical form representation for
the sentence So the heart attack person can?t go
1The 5 Monroe dialogs are: s2, s4, s12, s16, s17
(TERM :VAR V3283471
:LF (LF::THE V3283471 (:* LF::PERSON PERSON) :ASSOC-WITH (V3283440))
:SEM ($ F::PHYS-OBJ (F::SPATIAL-ABSTRACTION F::SPATIAL-POINT)
(F::GROUP -) (F::MOBILITY F::NON-SELF-MOVING)
(F::FORM F::SOLID-OBJECT) (F::ORIGIN F::HUMAN)
(F::OBJECT-FUNCTION F::OCCUPATION) (F::INTENTIONAL +)
(F::INFORMATION -) (F::CONTAINER -) (F::KR-TYPE KR::PERSON)
(F::TRAJECTORY -))
:INPUT (THE HEART ATTACK PERSON))
Figure 1: Excerpt from full logical form for dialog s2 utterance 173
(UTT :TYPE UTT :SPEAKER :USER :ROOT V3286907
:TERMS
((LF::SPEECHACT V3286907 SA TELL :CONTENT V3283686 :MODS (V3283247))
(LF::F V3283247 (:* LF::CONJUNCT SO) :OF V3286907)
(LF::F V3283686 (:* LF::MOVE GO) :THEME V3283471 :MODS (V3284278)
:TMA ((TENSE PRES) (MODALITY (:* LF::ABILITY CAN)) (NEGATION +)))
(LF::THE V3283471 (:* LF::PERSON PERSON) :ASSOC-WITH (V3283440))
(LF::KIND V3283440 (:* LF::MEDICAL-CONDITION HEART-ATTACK))
(LF::F V3284278 (:* LF::TO-LOC THERE) :OF V3283686 :VAL V3286383)
(LF::IMPRO V3286383 (OR LF::PHYS-OBJECT LF::REFERENTIAL-SEM)
:CONTEXT-REL THERE))
Figure 2: Abbreviated LF representation for So the heart attack person can?t go there
Figure 3: CorpusTool Abbreviated LF View
there (dialog s2, utterance 173) is shown in Fig-
ures 1 and 2. Figure 1 shows the full term for the
noun phrase the heart attack person. It contains
the term identifier :VAR V3283471, the logical
form (:LF), the set of semantic features associated
with the term (:SEM), and the list of words associ-
ated with the term (:INPUT). The semantic features
are the domain-independent semantic properties of
words encoded in our lexicon. We use them to ex-
press selectional restrictions (Dzikovska, 2004) and
we are currently investigating their use in reference
resolution. For discourse annotation, we primarily
rely on the logical forms.
The abbreviated logical form for the sentence is
shown in Figure 2. It contains the speech act for
the utterance, SA TELL, in the first term. There
is a domain-independent term for the discourse
adverbial So2, and the term for the main event,
(LF::Move GO), which contains the tense and
modal information in the :TMA field. The phrase
the heart attack person is represented by two terms
linked together with the :ASSOC-WITH relation-
ship, to be resolved during discourse processing.
Finally, there is a term for the adverbial modifier
there, which also results in the implicit pronoun (the
2So is identified as a conjunct because it is a connective, and
its meaning cannot be identified more specifically by the parser
without pragmatic reasoning
last term in the representation) denoting a place to
which the movement is directed. The terms provide
the basic building blocks to be used in the discourse
annotation, and their unique identifiers are used as
reference indices, as discussed in the next section.
The corpus-building process consists of three
stages: initial annotation, parsing and hand-
checking. The initial annotation prepares the sen-
tences as suitable inputs to the TRIPS parser. It is
necessary because handling speech repairs and ut-
terance segmentation is a difficult task, which our
parser cannot do automatically at this point. There-
fore, we start with segmenting the discourse turns
into utterances and marking the speech repairs us-
ing our tool. We also mark incomplete and ungram-
matical utterances which cannot be successfully in-
terpreted.
Once the corpus is annotated for repairs, we use
our automated LISP testing tool to parse the en-
tire corpus. Our parser skips over the repairs we
marked, and ignores incomplete and ungrammati-
cal utterances. Then, it marks utterances ?AUTO-
GOOD? and ?AUTO-BAD? as a guideline for an-
notators. As a first approximation, the utterances
where there is a parse covering the entire utterance
are marked as ?AUTO-GOOD? and those where
there is not are marked as ?AUTO-BAD?. Then
these results are hand-checked by human annotators
using our CorpusTool to inspect the analyses and ei-
ther mark them as ?GOOD?, or mark the incorrect
parses as ?BAD?, and add a reason code explain-
ing the problem with the parse. Note that we use a
strict criterion for accuracy so only utterances that
have both a correct syntactic structure and a cor-
rect logical form can be marked as ?GOOD?. The
CorpusTool allows annotators to view the syntactic
and semantic representations at different levels of
granularity. The top-level LF tree shown in Figure
3 allows a number of crucial aspects of the repre-
sentation to be checked quickly. Note that the entity
identifiers are color-coded, which is a great help for
checking variable mappings. If everything shown
in the top-level representation is correct, the full LF
with all terms expanded can be viewed. Similarly,
levels of the parse tree can be hidden or expanded
as needed.
After the initial checking stage, we analyze the
utterances marked ?BAD? and make changes in the
grammar and lexicon to address the BAD utterances
whenever possible. Occasionally, when the prob-
lems are due to ambiguity, the parser is able to parse
the utterance, but the interpretation it selects is not
the correct one among possible alternatives. In this
case, we manually select the correct parse and add
it to the gold-standard corpus.
Once the changes have been made, we re-parse
the corpus. Our parsing tool determines automat-
ically which parses have been changed and marks
them to be re-checked by the human annotators.
The CorpusTool has the functionality to quickly
locate the utterances marked as changed for re-
checking. This allows us to quickly conduct several
iterations of re-checking and re-parsing, bringing
the coverage in the completed corpus high enough
so that it may now be annotated for reference infor-
mation. The hand-checking scheme was found to be
quite reliable, with a kappa of 0.79. Currently, 85%
of the grammatical sentences are marked as GOOD
in the gold-standard coverage of the 5 dialogs in the
Monroe corpus.
Several iterations of the check and re-parse cy-
cle were needed to achieve parsing accuracy suit-
able for discourse annotation. Once the suitable ac-
curacy level has been reached, the reference annota-
tion process starts.
3.2 Adding Reference Information
As in the parser development phase, we built a Java
tool for annotating the parsed corpora for reference.
First, the relevant terms were extracted from the
LF representation of the semantic parse. These in-
cluded all verbs, noun phrases, implicit pronouns,
etc. Next, the sentences were manually marked for
reference using the tool (PronounTool).
There are many different ways to mark how en-
tities refer. Our annotation scheme is based on the
GNOME project scheme (Poesio, 2000) which an-
notates referential links between entities as well as
their respective discourse and salience information.
The main difference in our approach is that we do
not annotate discourse units and certain semantic
features, and most of the basic syntactic and seman-
tic features are produced automatically for us in the
parsing phase.
We use standoff annotation to separate our coref-
erence annotation from the syntactic and semantic
parse annotations. The standoff file for pronouns
consists of two fields for each pronoun to handle
the reference information: relation, which specifies
how the entities are related; and refers-to, which
specifies the id of the term the referential entity in
question points to.
The focus for our work has been on coreferential
pronouns and noun phrases, although we also anno-
tated the classes of all other pronouns. Typically,
the non-coreferential pronouns are difficult to an-
notate reliably since there are a myriad of different
categories for bridging relations and for specifying
Figure 4: CorpusTool Parse View
Figure 5: Pronoun Tool
demonstrative relations (Poesio and Viera, 1998).
Because our focus was on coreferential entities, we
had our annotators annotate only the main relation
type for the non-coreferential pronouns since these
could be done more reliably. The relations we used
are listed below:
Identity both entities refer to the same object (corefer-
ence)
Dummy non-referential pronouns (expletive or pleonas-
tic)
Indexicals expressions that refer to the discourse speak-
ers or temporal relations (ie. I, you, us, now)
Action pronouns which refer to an action or event
Demonstrative pronouns that refer to an utterance or se-
ries of utterances
Functional pronouns that are indirectly related to an-
other entity, most commonly bridging and one
anaphora
Set plural pronouns that refer to a collection of men-
tioned entities
Hard pronouns that are too difficult to annotate
Entities in identity, action and functional relations
had refers-to fields that pointed to the id of a spe-
cific term (or terms if the entity was a plural com-
posed of other entities). Dummy had no refers-to
set since they were not included in the evaluation.
Demonstrative pronouns had refers-to fields point-
ing to either utterance numbers or a list of utterance
numbers in the case of a discourse segment. Finally,
there were some pronouns for which it was too dif-
ficult to decide what they referred to, if anything.
These typically were found in incomplete sentences
without a verb to provide semantic information.
After the annotation phase, a post-processing
phase identifies all the noun phrases that refer to
the same entity, and generates a unique chain-id for
this entity. This is similar to the    field in the
GNOME scheme. The advantage of doing this pro-
cessing is that it is possible for a referring expres-
sion to refer to a past instantiation that was not the
last mentioned instantiation, which is usually what
is annotated. As a result, it is necessary to mark all
coreferential instantiations with the same identifica-
tion tag.
Figure 5 shows a snapshot of the PronounTool in
use for the pronoun there in the second utterance of
our example. The top pane has buttons to skip to the
next or previous utterance with a pronoun or noun
phrase. The lower pane has the list of extracted en-
tities for easy viewing. The ?Relation? box is a drop
down menu consisting of the relations listed above.
In this case, the identity relation has been selected
for there. The next step is to select an entity from
the context that the pronoun refers to. By clicking
on the ?Refers To? box, a context window pops up
with all the entities organized in order of appear-
ance in the discourse. The user selects the entity
and clicks ?Select? and the antecedent id is added
to the refers-to field.
Our aim with this part of the project (still in a
preliminary stage) is to investigate whether a shal-
low discourse segmentation (which is generated au-
tomatically) is enough to aid in pronominal refer-
ence resolution. Previous work has focused on us-
ing complex nested tree structures to model dis-
course and dialogue. While this method may be
the best way to go ultimately, empirical work has
shown that it has been difficult to put into practice.
There are many different schemes to choose from,
for example Rhetorical Structure Theory (Mann and
Thompson, 1986) or the stack model (Grosz and
Sidner, 1986) and manually annotating with these
schemes has variable reliability. Finally, annotating
these schemes requires real-world knowledge, rea-
soning, and knowledge of salience and semantics,
all of which make automatic segmentation difficult.
However, past studies such as Tetreault and Allen
(2003) show that for reference resolution, a highly-
structured tree may be too constraining, so a shal-
lower approach may be acceptable for studying the
effect of discourse segmentation on resolution.
3.3 Discourse Segmentation
Our preliminary segmentation scheme is as follows.
In a collaborative domain, participants work on a
task until completion. During the conversation, the
participants raise questions, supply answers, give
orders or suggestions and acknowledge each other?s
information and beliefs. In our corpus, these speech
acts and discourse cues such as so and then are
tagged automatically for reliable annotation. We
use this information to decide when to begin and
end a discourse segment.
Roberts (1996) suggests that questions are good
indicators of the start of a discourse segment be-
UTT1 S so gabriela
UTT2 U yes
UTT3 S at the rochester airport there
has been a bomb attack
UTT4 U oh my goodness
UTT5 S but it?s okay
UTT6 U where is i
UTT7 U just a second
UTT8 U i can?t find the rochester air-
port
UTT9 S [ i ] it?s
UTT10 U i think i have a disability with
maps
UTT11 U have i ever told you that before
UTT12 S it?s located on brooks avenue
UTT13 U oh thank you
UTT14 S [ i ] do you see it
UTT15 U yes
Figure 6: Excerpt from dialog s2
cause they open up a topic under discussion. An an-
swer followed by a series of acknowledgments usu-
ally signal a segment close. Currently we annotate
these segments manually by maintaining a ?hold-
out? file for each dialog which contains a list of all
the segments and their start, end and type informa-
tion.
For example, given the discourse as shown in
Figure 6, the discourse segments would be Figure
7. The starts of both segments are adjacent to sen-
tences that are questions.
(SEGMENT :START utt6
:END utt13
:TYPE clarification
:COMMENTS ?has aside in middle?)
(SEGMENT :START utt10
:END utt11
:TYPE aside
:COMMENTS ?same person aside.?)
Figure 7: Discourse annotation for s2 excerpt
4 Results
Spoken dialogue is a very difficult domain to work
with because utterances are often marred with dis-
fluencies, speech repairs, and are incomplete or un-
grammatical. Speakers will interrupt each other. As
a result, many empirical methods that work well in
very formal, structured domains such as newspaper
texts or manuals tend to suffer. For example, many
leading pronoun resolution methods perform around
80% accuracy over a corpus of syntactically-parsed
Wall Street Journal articles (e.g., (Tetreault, 2001)
and (Ge et al, 1998)), but in spoken dialogue the
performance of these algorithms drops significantly
(Byron, 2002).
However, by including semantic and discourse
information, one is able to improve performance.
Our preliminary results show that using the seman-
tic feature lists associated with each entity as a fil-
ter for reference increases performance to 59% from
44%. Adding discourse segmentation boosts that
figure to 66% over some parts of the corpus.
5 Conclusion
We have presented a description of our corpus an-
notation in the Monroe domain. It is novel in that it
incorporates rich semantic information with refer-
ence and discourse information, a rarity for spoken
dialogue domains which are typically very difficult
to annotate. We expedite the annotation process and
make it more reliable by semi-automating the pars-
ing with checking and also by using two tools tai-
lored for our domain to speed up annotation. The re-
sulting corpus has several applications ranging from
overall system development to the testing of theo-
ries and algorithms of reference and discourse. Our
preliminary results demonstrate the usefulness of
the corpus.
6 Acknowledgments
Partial support for this project was provided by
ONR grant no. N00014-01-1-1015, ?Portable Di-
alog Interfaces? and NSF grant 0328810 ?Continu-
ous Understanding?.
References
T. Brants and O. Plaehn. 2000. Interactive corpus
annotation. In LREC ?00.
D. Byron. 2002. Resolving pronominal reference
to abstract entities. In ACL ?02, pages 80?87,
Philadelphia, USA.
M. O. Dzikovska, M. D. Swift, and J. F. Allen.
2004. Building a computational lexicon and on-
tology with framenet. In LREC workshop on
Building Lexical Resources from Semantically
Annotated Corpora. Lisbon, Portugal, May.
M. Dzikovska. 2004. A Practical Semantic Repre-
sentation for Natural Language Parsing. Ph.D.
thesis, U. Rochester.
N. Ge, J. Hale, and E. Charniak. 1998. A statistical
approach to anaphora resolution. Proceedings of
the Sixth Workshop on Very Large Corpora.
B. Grosz and C. Sidner. 1986. Attention, inten-
tions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
P. Heeman and J. Allen. 1994. The TRAINS93 di-
alogues. Technical Report TRAINS TN 94-2, U.
Rochester.
W. Mann and S. Thompson. 1986. Rhetori-
cal structure theory: Descripton and construc-
tion of text. Technical Report ISI/RS-86-174,
USC/Information Sciences Institute, October.
M. P. Marcus, B Santorini, and M. A.
Marcinkiewicz. 1993. Building a large an-
notated corpus of English: The Penn Treebank.
Computational Linguistics, 19:313?330.
M. Moser and J.D. Moore. 1996. Toward a synthe-
sis of two accounts of discourse structure. Com-
putational Linguistics, 22(3):409?419.
M. Poesio and R. Viera. 1998. A corpus-based in-
vestigation of definite description use. Computa-
tional Linguistics, 24(2):183?216.
M. Poesio. 2000. Annotating a corpus to develop
and evaluate discourse entity realization algo-
rithms: issues and preliminary results. In LREC
?00, Athens.
C. Roberts. 1996. Information structure in dis-
course. Papers in Semantics, 49:43?70. Ohio
State Working Papers in Linguistics.
A. Stent. 2001. Dialogue Systems as Conversa-
tional Partners. Ph.D. thesis, U. Rochester.
M. Swift, M. Dzikovska, J. Tetreault, and James F.
Allen. 2004. Semi-automatic syntactic and se-
mantic corpus annotation with a deep parser. In
LREC?04, Lisbon.
J. Tetreault and J. F. Allen. 2003. An empiri-
cal evaluation of pronoun resolution and clausal
structure. In 2003 International Symposium on
Reference Resolution and its Applications to
Question Answering and Summarization, pages
1?8, Venice, Italy.
J. Tetreault. 2001. A corpus-based evaluation
of centering and pronoun resolution. Computa-
tional Linguistics, 27(4):507?520.
Incremental Parsing with Reference Interaction
Scott C. Stoness, Joel Tetreault, James Allen
Department of Computer Science
University of Rochester
Rochester, NY, USA
stoness@cs.rochester.edu
tetreaul@cs.rochester.edu, james@cs.rochester.edu
Abstract
We present a general architecture for incremen-
tal interaction between modules in a speech-to-
intention continuous understanding dialogue sys-
tem. This architecture is then instantiated in the
form of an incremental parser which receives suit-
ability feedback on NP constituents from a refer-
ence resolution module. Oracle results indicate
that perfect NP suitability judgments can provide a
labelled-bracket error reduction of as much as 42%
and an efficiency improvement of 30%. Prelimi-
nary experiments in which the parser incorporates
feedback judgments based on the set of referents
found in the discourse context achieve a maximum
error reduction of 9.3% and efficiency gain of 4.6%.
The parser is also able to incrementally instantiate
the semantics of underspecified pronouns based on
matches from the discourse context. These results
suggest that the architecture holds promise as a plat-
form for incremental parsing supporting continuous
understanding.
1 Introduction
Humans process language incrementally, as has
been shown by classic psycholinguistic discussions
surrounding the garden-path phenomenon and pars-
ing preferences (Altmann and Steedman, 1988;
Konieczny, 1996; Phillips, 1996). Moreover, a va-
riety of eye-tracking experiments (Cooper, 1974;
Tanenhaus and Spivey, 1996; Allopenna et al,
1998; Sedivy et al, 1999) suggest that complex se-
mantic and referential constraints are incorporated
on an incremental basis in human parsing decisions.
Computational parsers, however, still tend to op-
erate an entire sentence at a time, despite the ad-
vent of speech-to-intention dialogue systems such
as Verbmobil (Kasper et al, 1996; Noth et al, 2000;
Pinkal et al, 2000), Gemini (Dowding et al, 1993;
Dowding et al, 1994; Moore et al, 1995) and TRIPS
(Allen et al, 1996; Ferguson et al, 1996; Fergu-
son and Allen, 1998). Naturalness, robustness, and
interactivity are goals of such systems, but control
flow is typically the sequential execution of mod-
ules, each operating on the output of its predeces-
sor; only after the entire sentence has been parsed
do higher-level modules such as intention recogni-
tion and reference resolution get involved.
In contrast to this sequential model is the con-
tinuous understanding approach, in which all lev-
els of language analysis occur simultaneously, from
speech recognition to intention recognition. As well
as being psycholinguistically motivated, continuous
understanding models offer potential computational
advantages, including accuracy and efficiency im-
provements for real-time spoken language under-
standing and better support for the spontaneities of
natural human speech. Continuous understanding
is necessary if the system is to respond before the
entire utterance is analyzed, a prerequisite for in-
cremental confirmation and clarification. The major
computational advantage of continuous understand-
ing models is that high-level expectations and feed-
back should be able to influence the search of lower-
level processes, thus leading to a focused search
through hypotheses that are plausible at all levels
of processing.
One of the major current applications of parsers
that operate incrementally is for language modelling
in speech recognition (Brill et al, 1998; Jelinek and
Chelba, 1999). This work is important not only
for its ability to improve performance on the speech
recognition task; it also models the interactions be-
tween speech recognition and parsing in a contin-
uous understanding system. Our research attempts
to further the quest for continuous understanding by
moving one step up the hierarchy, building an incre-
mental parser which is the advisee rather than the
advisor.
We begin by presenting a general architecture
for incremental interaction between the parser and
higher-level modules, and then discuss a specific in-
stantiation of this general architecture in which a
reference resolution module provides feedback to
the parser on the suitability of noun phrases. Ex-
periments with incremental feedback from a refer-
Client(parser)
Mediator
Advisor(reference)
Inform
Inform Feedback
ModifyChart
Figure 1: A General Architecture for Incremental
Parsing
ence resolution module and an NP suitability oracle
are reported, and the ability of the implementation
to incrementally instantiate semantically underspec-
ified pronouns is outlined. We believe this research
provides an important start towards developing end-
to-end continuous understanding models.
2 An Incremental Parsing Architecture
Many current parsers fall into the class of history-
based grammars (Black et al, 1992). The indepen-
dence assumptions of these models make the pars-
ing problem both stochastically and computation-
ally tractable, but represent a simplification and may
therefore be a source of error. In a continuous un-
derstanding framework, higher-level modules may
have additional information that suggests loci for
improvement, recognizing either invalid indepen-
dence assumptions or errors in the underlying prob-
ability model.
We have designed a general incremental parsing
architecture (Figure 1) in which the Client, a dy-
namic programming parser, performs its calcula-
tions, the results of which are incrementally passed
on via a Mediator to an Advisor with access to
higher-level information. This higher-level Advi-
sor sends feedback to the Mediator which has ac-
cess to the Client?s chart, and which then surrepti-
tiously changes and/or adds to the chart in order to
make the judgments conform more closely to those
of the Advisor. The parser, whose chart has (unbe-
knownst to it) been changed, then simply calculates
chart expansions for the next word, na??vely expand-
ing the currently available (and possibly modified)
hypotheses.
This architecture is general in that neither the Me-
diator nor the Advisor have been specified; either
of these modules can be instantiated in any number
of ways within the general framework. The typical
dynamic programming component will function in
very much the same way that it does in the vanilla
algorithm, except that the chart in which partial re-
sults are recorded may be modified between time
steps. The Client can be any system which uses dy-
namic programming to efficiently encode indepen-
dence assumptions, so long as it provides the Me-
diator with the ability to modify chart probabilities
and add chart entries; otherwise the original parser
can remain untouched. By having the Mediator per-
form these modifications rather than the Advisor,
we preserve modularity: in this architecture the Ad-
visor need not be aware of the specific implementa-
tion of the Client, although depending on the type
of advice provided, it may need access to the under-
lying grammar. The Mediator isolates the Advisor
and Client from each other as well as determining
how the feedback will be introduced into into the
Client?s chart.
Stoness (2004) identifies two broad categories of
subversion - our term for the Mediator?s surrepti-
tious modification of the Client?s chart - as outlined
below:
? Heuristic Subversion: the Mediator uses the
Advisor?s feedback as heuristic information,
affecting the search sequence but not the prob-
abilities calculated for a given hypothesis; and
? Chart Subversion: the Mediator is free to
modify the Client?s chart as necessary, but does
not directly affect the search sequence of the
Client (except insofar as this is accomplished
by the modifications to the chart).
The two types of subversion have very different
properties. Heuristic subversion will affect the set
of analyses which is output by the parser, but each
of those analyses will have exactly the same proba-
bility score as under the original parser; the effects
of the Advisor are essentially limited to determin-
ing which hypotheses remain within the beam, or
the order in which hypotheses are expanded, de-
pending on whether the underlying parser uses a
beam search or an agenda. Chart subversion, on the
other hand, will actually change the scores assigned
analyses, resulting in a new probability distribution.
Heuristic subversion is considerably less powerful,
but more stable; the effects of chart subversion can
be fairly chaotic, especially if care is not taken to
avoid feedback loops. Stoness (2004) outlines con-
ditions under which the effects of chart subversion
are predictable, becoming broadly equivalent to an
incremental version of a post-hoc re-ranking of the
Client?s output hypotheses.
Further details on the general architecture, in-
cluding properties of various modes of feedback in-
tegration, a discussion of the relationship between
incremental parsing and parse re-ranking, the pos-
sibilities of multiple Advisors working in combina-
tion, and provisions in the model for asynchronous
feedback are available in a University of Rochester
Technical Report (Stoness, 2004).
3 Instantiating the Architecture
Working in the context of TRIPS, an existing task-
oriented dialogue system, we have modified the
existing parser and reference resolution modules
so that they communicate incrementally with each
other. This models the early incorporation of refer-
ence resolution information seen in humans (Cham-
bers et al, 1999; Allopenna et al, 1998), and al-
lows reference resolution information to affect pars-
ing decisions.
For example, in ?Put the apple in the box in the
corner? there is an attachment ambiguity. Reference
resolution can determine the number of matches for
the noun phrase ?the apple? incrementally; if there
is a single match, the parser would expect this to
be a complete NP, and prefer the reading where the
box is in the corner. If reference returns multiple
matches for ?the apple?, the parser would expect
disambiguating information, and prefer a reading
where additional information about the apple is pro-
vided: in this case, an the NP ?the apple in the box?.
With solid feedback from reference, it should be
possible to remove some of the ambiguity inherent
in the search process within the parser. This will
simultaneously guide the search to the most likely
region of the search space, improving accuracy, and
delay the search of unlikely regions, improving effi-
ciency. Of course, this comes at the cost of some
communication overhead and additional reference
resolution. Ideally, the overall improvement in the
parser?s search space would be enough to cover
the additional incremental operation costs of other
modules.
3.1 An Incremental Parser
The pre-existing parser in the dialogue system was a
pure bottom-up chart parser with a hand-built gram-
mar suited for parsing task-oriented dialogue. The
grammar consisted of a context-free backbone with
a set of associated features and semantic restric-
tions, including agreement, hard subcategorization
constraints, and soft selectional restriction prefer-
ences. The parser has been modified so that when-
ever a constituent is built, it can be sent forward to
the Mediator, allowing for the possibility of feed-
back. The architecture and experiments described in
this paper were performed in a synchronous mode,
but the parser can also operate in an incrementally
asynchronous mode, where it continues to build the
chart in parallel with other modules? operations;
probability adjustments to the chart then cascade to
dependent constituents.
3.2 Interaction with Reference
When the parser builds a potential referring expres-
sion (e.g. any NP), it is immediately passed on to the
Advisor, the reference resolution module described
in Tetreault et. al. (2004) modified for incremental
interaction. This module then determines all pos-
sible discourse referents, providing the parser with
a ranked classification based on the salience of the
referents and the (incremental) syntactic environ-
ment.
The reference module keeps a dynamically up-
dated list of currently salient discourse entities
against which incoming incrementally constructed
NP constituents are matched. Before any utterances
are processed, the module loads a static database
of relevant place names in the domain; all other
possible referents are discourse entities which have
been spoken of during the course of the dialogue.
For efficiency, the dynamic portion of the context
list is limited to the ten most recent contentful ut-
terances; human-annotated antecedent data for this
corpus shows that 99% of all pronoun antecendents
fall within this threshold. After each sentence is
fully parsed the context list is updated with new dis-
course entities introduced in the utterance; ideally,
these context updates would also be incremental,
but this feature was omitted in the current version
for simplicity.
The matching process is based on that described
by Byron (2000), and differs from that of many
other reference modules in that every entity and
NP-constituent has a (possibly underspecified) se-
mantic feature vector, and it is both the logical and
semantic forms which determine successful match-
ings. Adding semantic information increases the ac-
curacy of the reference resolution from 44% to 58%
(Tetreault and Allen, 2004), and consequently im-
proves the feedback provided to the parser.
The Mediator receives the set of all possible ref-
erents, including the semantic content of the refer-
ent and a classification of whether the referent is the
single salient entity in focus, has previously been
mentioned, or is a relevant place name.
3.3 Mediator
The Mediator interprets the information received
from reference and determines how the parser?s
chart should be modified. If the NP matches noth-
ing in the discourse context, no match is returned;
otherwise each referent is annotated with its type
and discourse distance, and this set is run through a
classifier to reduce it to a single tag. The resulting
tag is the reference resolution tag, or R. The NP
constituents are also classified by definiteness and
number, giving an NP tag N .
For each classifier, we trained a probability model
which calculated Pr, the probability that a noun
phrase constituent c would be in the final parse, con-
ditioned on R and N , or
Pr = p(c in final parse|R,N).
This probability was then linearly combined with
the parser?s constituent probability,
Pp = p(c ? wnm),
according to the equation
P (c) = (1? ?) ? Pp + ? ? Pr
for various values of ?. Evaluation using held-out
data suggested that a value of ? = 0.2 would be
optimal. This style of feedback is an example of
chart subversion, as it is a direct modification of
constituent probabilities by the Mediator, defining
a new probability distribution.
4 Experiments
The Monroe domain (Tetreault et al, 2004; Stent,
2001) is a series of task-oriented dialogues between
human participants set in a simulated rescue op-
eration domain, where participants collaboratively
plan responses to emergency calls. Dialogues were
recorded, broken up into utterances, and then tran-
scribed by hand, removing speech repairs from the
parser input. These transcriptions served as input
for all experiments reported below.
A probabilistic grammar was trained from su-
pervised data, assigning PCFG probabilities for the
rule expansions in the CFG backbone of the hand-
crafted, semantically constrained grammar. The
parser was run using this grammar, but without any
incremental interaction whatsoever, in order to es-
tablish baseline accuracy and efficiency numbers.
The corpus consists of six task-oriented dialogues;
four were used for the PCFG training, one was
held out to establish appropriate parameter values,
and one was selected for testing. The held-out and
test dialogues contain hand-checked gold standard
parses.
Under normal operation of the sequential dia-
logue system, the parser is run in best-first mode,
providing only a single analysis to higher-level
modules, and has a constituent construction limit in
Base All NPs Def-Sing
Precision 94.6 97.2 96.3
Recall 71.1 83.1 78.8
F-statistic 82.9 90.2 87.6
Improvement N/A 7.3 4.7
Error Red. N/A 42.4 27.2
Work Red. N/A 30.3 18.7
Perfect S 224 241 236
Parsed S 270 282 279
Table 1: Results for (a) The baseline parser without
reference feedback, (b) An Oracle Advisor correctly
determining status of all NPs, (c) An Oracle Advi-
sor correctly determining status of definite singular
NPs.
an attempt to simulate the demands of a real-time
system. When the parser reaches the constituent
limit, appropriate partial analyses are collected and
forwarded to higher-level modules. These con-
straints were kept in place during our experiments,
because they would be necessary under normal op-
eration of the system. Thus, the inability to parse a
sentence does not necessarily indicate a lack of cov-
erage of the grammar, but rather a lack of efficiency
in the parsing process.
As can be seen in Table 1, the parser achieves a
94.6% labelled bracket precision, and a 71.1% la-
belled bracket recall. Note that only constituents
of complete parses were checked against the gold
standard, to avoid any bias introduced by the partial
parse evaluation metric. Of the 290 gold standard
utterances in the test data, 270 could be parsed, and
224 were parsed perfectly.
4.1 Oracle Evaluation
We began with a feasibility study to determine
how significant the effects of incremental advice on
noun phrases could be in principle. The feedback
from the reference module is designed to determine
whether particular NPs are good or bad from a refer-
ence standpoint. We constructed a simple feedback
oracle from supervised data which determined, for
each NP, whether or not the final parse of the sen-
tence contained an NP constituent which spanned
the same input. Those NPs marked ?good?, which
did appear in the parse, were added to the chart as
new constituents. NPs marked ?bad? were added to
the chart with a probability of zero1. A second or-
1In some sense, this style of feedback is an example of
heuristic subversion, as it has the effect of keeping ?good? anal-
yses around while removing ?bad? analyses from the search
space. Technically, this is also chart subversion, as each hy-
pothesis has its score multiplied by 1 or 0, depending on
acle evaluation performed this same task, but only
providing feedback on definite singular NPs.
The results of both oracles are shown in Table
1. The first five rows give the precision, recall, f-
statistic, the raw f-statistic improvement, and the f-
statistic error reduction percentage, all determined
in terms of labelled bracket accuracy. There is a
marked increase in both precision and recall, with
an overall error reduction of 42.4% with the full
oracle and 27.2% with the definite singular oracle.
Thus, in this domain over a quarter of all incorrectly
labelled constituents are attributable to syntactically
incorrect definite singular NPs. The number of con-
stituents built during the parse is used as a measure
of efficiency, and the work reduction is reported in
the sixth row of the table, showing an efficiency im-
provement of 30.3% or 18.7%, depending on the or-
acle. The final two lines of the table show that both
the number of sentences which can be parsed and
the number of sentences which are perfectly parsed
increase under both models.
The nature of the oracle experiment ensures some
reduction in error and complexity, but the magni-
tude of the improvement is surprising, and certainly
encouraging for the prospects of incremental refer-
ence. Definite singular NPs typically have a unique
referent, providing a locus for effective feedback,
and we believe that incremental interaction with an
accurate reference module might approach the ora-
cle performance.
4.2 Dialogue Experiments
For these experiments the parser interacted with the
actual reference module, incorporating feedback ac-
cording to the model discussed in Section 3.3. The
first data column of Table 2 repeats the baseline re-
sults of the parser without reference feedback. The
next two columns show statistics for a run of the
parser with incremental feedback from reference,
using a probability model based on a classification
scheme which distinguished only whether or not the
set of referent matches was empty. The second data
column shows the results for the estimated interpo-
lation parameter value of ? = 0.2, while the third
data column shows results for the empirically deter-
mined optimal ? value of 0.1.
The results are encouraging, with an error reduc-
tion of 8.2% or 9.3% on the test dialogue, although
the amount of work the parser performed was re-
duced by only 4.0% and 3.6%. A further encour-
aging sign is that for every exploratory ? value we
whether it is ?good? or ?bad?. In this degenerate case of all-
or-nothing feedback, chart subversion and heuristic subversion
are equivalent.
Base SC SC CC
? = N/A 0.2 0.1 0.2
Precision 94.6 94.5 94.8 93.9
Recall 71.1 74.1 74.2 73.9
F-statistic 82.9 84.3 84.5 83.9
F-stat Imp. N/A 1.4 1.6 1.0
Error Red. N/A 8.2 9.3 5.8
Work Red. N/A 3.6 4.0 4.6
Perfect S 224 225 228 223
Parsed S 270 273 273 273
Table 2: Results for Discourse Experiment with
Simple (SC) and Complex (CC) Classifiers
tried in either the held-out or the test data, both the
accuracy and efficiency improved. Reference infor-
mation also helped increase both the number of sen-
tences that could be parsed and the number of sen-
tences that were parsed perfectly, although the im-
provements were small.
The estimated value of ? = 0.2 produced an error
reduction that was approximately 20% of the orac-
ular, which is a very good start, especially consider-
ing that this experiment used only the information of
whether there was a referent match or not. The effi-
ciency gains were more modest at just above 10% of
the oracular results, although one would expect less
radical efficiency improvements from this experi-
ment, since under the linear interpolation of the ex-
periment, even extremely dispreferred analyses may
be expanded, whereas the oracle simply drops all
dispreferred NPs off the beam immediately.
We performed a second experiment that made
more complete use of the reference data, break-
ing down referent sets according to when and how
often they were mentioned, whether they matched
the focus, and whether they were in the set of
relevant place names. We expected that this in-
formation would provide considerably better re-
sults than the simple match/no-match classification
above. For example, consider a definite singular
NP: if it matches a single referent, one would expect
it to be in the parse with high probability, but multi-
ple matches would indicate that the referent was not
unique, and that the base noun probably requires ad-
ditional discriminating information (e.g. a preposi-
tional phrase or restrictive relative clause).
Unfortunately, as the final column of Table 2
shows, the additional information did not provide
much of an advantage. The amount of work done
was reduced by 4.6%, the largest of any efficiency
improvement, but error reduction was only 5.8%,
and the number of sentences parsed perfectly actu-
ally decreased by one.
We conjecture that co-reference chains may be a
significant source of confusion in the reference data.
Ideally, if several entities in the discourse context
all refer to the same real-world entity, they should
be counted as a single match. The current refer-
ence module does construct co-referential chains,
but a single error in co-reference identification will
cause all future NPs to match both the chain and the
misidentified item, instead of producing the single
match desired.
The reference module has to rely on the parser
to provide the correct context, so there is something
of a bootstrapping problem at work, which indicates
both a drawback and a potential of this type of in-
cremental interaction. The positive feedback loop
bodes well for the potential benefits of the incre-
mental system, because as the incremental reference
information begins to improve the performance of
the parser, the context provided to the reference
resolution module improves, which provides even
more accurate reference information. Of course, in
the early stages of such a system, this works against
us; many of the reference resolution errors could be
a result of the poor quality of the discourse context.
Our current efforts aim to identify and correct
these and other reference resolution issues. Not only
will this improve the performance of the Reference
Advisor from an incremental parsing standpoint, but
it should also further our understanding of reference
resolution itself.
We have shown efficiency improvements in terms
of the overall number of constituents constructed by
the parser; however, one might ask whether this im-
provement in parsing speed comes at a large cost to
the overall efficiency of the system. We suggest that
this is in some sense the wrong question to ask, be-
cause for a real-time interactive system the primary
concern is to keep up with the human interlocutor,
and the incremental approach offers a far greater op-
portunity for parallelism between modules. In terms
of time elapsed from speech to analysis, the system
as a whole should benefit from the incremental ar-
chitecture.
5 Semantic Replacement
When the word ?it? is parsed as a referential NP, it is
given highly underspecified semantics. We have im-
plemented a Mediator which, for each possible ref-
erent for ?it?, adds a new item to the parser?s chart
with the underspecified semantics of ?it? instanti-
ated to the semantics of the referent.
Consider the sentence sequence ?Send the bus to
the hospital?, ?Send it to the mall?. At the point
that the NP ?it? is encountered in the second sen-
tence, it has not yet been connected to the verb,
so the incremental reference resolution determines
that ?the bus? and ?the hospital? are both possi-
ble referents. We add two new constituents to the
chart: ?it?[the hospital] and ?it?[the bus]. They
are given probabilities infinitesimally higher than
the ?it?[underspecified] which already exists on the
chart. Thus, if either of the new versions of ?it?
match the semantic restrictions inherent in the rest
of the parse, they will be featured in parses with a
higher probability than the underspecified version.
?It?[the bus] matches the mobility required of the
object of ?send?, while ?it?[the hospital] does not.
This results in a parse where the semantics of ?it?
are instantiated early and incrementally.
This sort of capability is key for an end-to-end
incremental system, because neither the reference
module nor the parser is capable, by itself, of deter-
mining incrementally that the reference in question
must be ?the bus?. If we want an end-to-end system
which can interact incrementally with the user, this
type of decision-making must be made in an incre-
mental fashion.
This ability is also key in the presence of soft con-
straints or other Advisors which prefer one possi-
ble moveable referent to another; under incremental
parsing, these constraints would have the chance to
be applied during the parsing process, whereas a se-
quential system has no alternatives to the default,
underspecified pronoun, and so cannot apply these
restrictions to discriminate between referents.
Our implementation performs the semantic vet-
ting discussed above, but we have done no large-
scale experiments in this area.
6 Related Work
There are instances in the literature of incremental
parsers that pass forward information to higher-level
modules, but none, to our knowledge, are designed
as continuous understanding systems, where all lev-
els of language analysis occur (virtually) simultane-
ously.
For example, there are a number of robust seman-
tic processing systems (Pinkal et al, 2000; Rose,
2000; Worm, 1998; Zechner, 1998) which contain
incremental parsers that pass on partial results im-
mediately to the robust semantic analysis compo-
nent, which begins to work on combining these
sentence fragments. If the parser cannot find a
parse, then the semantic analysis program has al-
ready done at least part of its work. However, none
of the above systems have a feedback loop between
the semantic analysis component and the incremen-
tal parser. So, while all of these are in some sense
examples of incremental parsing, they are not con-
tinuous understanding models.
Schuler (2002) describes a parser which builds
both a syntactic tree and a denotation-based seman-
tic analysis as it parses. The denotations of con-
stituents in the environment are used to inform pars-
ing decisions, much as we use the static database of
place names. However, the feedback in our system
is richer, based on the context provided by the pre-
ceding discourse. Furthermore, as an instantiation
of the general architecture presented in Section 2,
our system is more easily extensible to other forms
of feedback.
7 Future Work
There is a catch-22 in that the accurate reference in-
formation necessary to improve parsing accuracy is
dependent on an accurate discourse context which
is reliant on accurate parsing. One way to cut this
Gordian Knot is to use supervised data to ensure that
the discourse context in the reference module is up-
dated with the gold standard parse of the sentence
rather than the parse chosen by the parser; a context
oracle, if you will.
A major undertaking necessary to advance this
work is an error analysis of the reference module
and of the parser?s response to feedback; when does
feedback lead to additional work or decreased ac-
curacy on the part of the incremental parser, and is
the feedback that leads to these errors correct from
a reference standpoint?
Currently, the accuracy of the parser is couched
in syntactic terms. The precision of the baseline
PCFG is fairly high at 94.6%, but that could conceal
semantic errors, which could be corrected with ref-
erence information. Assessing semantic accuracy is
one of a number of alternative evaluation metrics
that we are exploring.
We intend to gather timing data and investigate
other efficiency metrics to determine to what extent
the efficiency gains in the parser offset the commu-
nication overhead and the extra work performed by
the reference module.
We also plan to do experiments with different
feedback regimes, experimenting both with the ac-
tual reference results and with the oracle data. Fur-
ther experiments with this oracle data should enable
us to appropriately parameterize the linear interpo-
lation, and indeed, to investigate whether linear in-
terpolation itself is a productive feedback scheme,
or whether an integrated probability distribution
over parser and reference judgments is more effec-
tive. The latter scheme is not only more elegant, but
can also be shown to produce probabilities equiva-
lent to those assigned parses in the parse re-ranking
task (Stoness, 2004).
We?ve shown (Stoness, 2004) that feedback
which punishes constituents that are not in the fi-
nal parse cannot result in reduced accuracy or effi-
ciency; under certain restrictions, the same holds of
rewarding constituents that will be in the final parse.
However, it is not clear how quickly the efficiency
and accuracy gains drop off as errors mount. By in-
troducing random mistakes into the Oracle Advisor,
we can artificially achieve any desired level of accu-
racy, which will enable us to explore the character-
istics of this curve. The accuracy and efficiency re-
sponse under error has drastic consequences on the
types of Advisors that will be suitable under this ar-
chitecture.
Finally, it is clear that finding only the discourse
context referents of a noun phrase is not sufficient;
intuitively, and as shown by Schuler (2002), real-
world referents can also aid in the parsing task. We
intend to enhance the reference resolution compo-
nent of the system to identify both discourse and
real-world referents.
8 Conclusion
These preliminary experiments, using the coars-
est grain of reference information possible, achieve
a significant fraction of the oracular accuracy im-
provements, highlighting the potential benefits of
incremental interaction between the parser and ref-
erence in a continuous understanding system.
The Oracle feedback for NPs shows that it is pos-
sible to simultaneously improve both the accuracy
and efficiency of an incremental parser, providing a
proof-in-principle for the general incremental pro-
cessing architecture we introduced. This architec-
ture holds great promise as a platform for instantiat-
ing the wide range of interactions necessary for true
continuous understanding.
9 Acknowledgements
Partial support for this project was provided by
ONR grant no. N00014-01-1-1015, ?Portable Di-
alog Interfaces? and NSF grant 0328810 ?Continu-
ous Understanding?.
References
J. Allen, B. Miller, E. Ringger, and T. Sikorski.
1996. Robust understanding in a dialogue sys-
tem. In Proc. of ACL-96, pages 62?70.
P. D. Allopenna, J. S. Magnuson, and M. K. Tanen-
haus. 1998. Tracking the time course of spo-
ken word recognition using eye movements: ev-
idence for continuous mapping models. Journal
of Memory and Language, 38:419?439.
G. Altmann and M. Steedman. 1988. Interaction
with context during human sentence processing.
Cognition, 30:191?238.
E. Black, F. Jelinkek, J. Lafferty, D. Magerman,
R. Mercer, and S. Roukos. 1992. Towards
history-based grammars: using richer models
for probabilistic parsing. In Proc. of the Fifth
DARPA Speech and Natural Language Workshop.
E. Brill, R. Florian, J. C. Henderson, and L. Mangu.
1998. Beyond n-grams: Can linguistic sophisti-
cation improve language modeling? In Proc. of
COLING-ACL-98, pages 186?190.
D. K. Byron. 2000. Semantically enhanced pro-
nouns. In Proc. of DAARC2000: 3rd Interna-
tional Conference on Discourse Anaphora and
Anaphor Resolution.
C. G. Chambers, M. K. Tanenhaus, and J. S. Magnu-
son. 1999. Real world knowledge modulates ref-
erential effects on pp-attachment: Evidence from
eye movements in spoken language comprehen-
sion. Conference Abstract. Architechtures and
Mechanisms for Language Processing.
R. M. Cooper. 1974. The control of eye fixation
by the meaning of spoken language. Cognitive
Psychology, 6:84?107.
J. Dowding, J. M. Gawron, D. Appelt, J. Bear,
L. Cherny, R. Moore, and D. Moran. 1993.
Gemini: A natural language system for spoken-
language understanding. In Proc. of ACL-93,
pages 54?61.
J. Dowding, R. Moore, F. Andry, and D. Moran.
1994. Interleaving syntax and semantics in an
efficient bottom-up parser. In Proc. of ACL-94,
pages 110?116.
G. Ferguson and J. Allen. 1998. Trips: An inte-
grated intelligent problem-solving assistant. In
Proc. of AAAI-98, pages 567?572.
G. Ferguson, J. Allen, and B. Miller. 1996. Trains-
95: Towards a mixed-initiative planning assis-
tant. In Proc. of the 3rd International Conference
on Artificial Intelligence Planning Systems, pages
70?77.
Frederick Jelinek and Ciprian Chelba. 1999.
Putting language into language modeling. In
Proc. of Eurospeech-99.
W. Kasper, H.-U. Krieger, J. Spilker, and H. Weber.
1996. From word hypotheses to logical form: An
efficient interleaved approach. In Natural Lan-
guage Processing and Speech Technology: Re-
sults of the 3rd Konvens Conference, pages 77?
88.
Lars Konieczny. 1996. Human Sentence Process-
ing: A Semantics-Oriented Parsing Approach.
Ph.D. thesis, Universitat Freiburg.
R. Moore, D. Appelt, J. Dowding, J. M. Gawron,
and D. Moran. 1995. Combining linguistic and
statistical knowledge sources in natural-language
processing for atis. In Proc. ARPA Spoken Lan-
guage Systems Technology Workshop.
E. Noth, A. Batliner, A. Kiessling, R. Kompe, and
H. Niemann. 2000. Verbmobil: The use of
prosody in the linguistic components of a speech
understanding system. IEEE Transactions on
Speech and Audio Processing, 8(5):519?531.
Colin Phillips. 1996. Order and Structure. Ph.D.
thesis, MIT.
M. Pinkal, C.J. Rupp, and K. Worm. 2000. Ro-
bust semantic processing of spoken language.
In Verbmobil: Foundations of Speech-to-Speech
Translation, pages 321?335.
C. P. Rose. 2000. A framework for robust semantic
interpretation. In Proc. of the Sixth Conference
on Applied Natural Language Processing.
W. Schuler. 2002. Interleaved semantic interpreta-
tion in environment-based parsing. In Proc. of
COLING-02.
J. C. Sedivy, M. K. Tanenhaus, C. G. Chambers,
and G. N. Carlson. 1999. Achieving incremental
semantic interpretation through contextual repre-
sentation. Cognition, 71:109?147.
A. Stent. 2001. Dialogue Systems as Conver-
sational Partners. Ph.D. thesis, University of
Rochester.
S. C. Stoness. 2004. A general architecture for
incremental parsing. Technical report, TR 838,
University of Rochester.
M. K. Tanenhaus and M. Spivey. 1996. Eye-
tracking. Language and Cognition Processes,
11(6):583?588.
J. Tetreault and J. Allen. 2004. Semantics, dia-
logue, and reference resolution. In Catalog-04:
8th Workshop on the Semantics and Pragmatics
of Dialogue.
J. Tetreault, M. Swift, P. Prithviraj, M. Dzikovska,
and J. Allen. 2004. Discourse annotation in the
monroe corpus. In ACL-04 Discourse Annotation
Workshop.
K. Worm. 1998. A model for robust process-
ing of spontaneous speech by integrating viable
fragments. In Proc. of COLING-ACL-98, pages
1403?1407.
K. Zechner. 1998. Automatic construction of frame
representations for spontaneous speech in unre-
stricted domains. In Proc. of COLING-ACL-98,
pages 1448?1452.
Using Reinforcement Learning to Build a Better Model of Dialogue State
Joel R. Tetreault
University of Pittsburgh
Learning Research and Development Center
Pittsburgh PA, 15260, USA
tetreaul@pitt.edu
Diane J. Litman
University of Pittsburgh
Department of Computer Science &
Learning Research and Development Center
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
Given the growing complexity of tasks
that spoken dialogue systems are trying to
handle, Reinforcement Learning (RL) has
been increasingly used as a way of au-
tomatically learning the best policy for a
system to make. While most work has
focused on generating better policies for
a dialogue manager, very little work has
been done in using RL to construct a better
dialogue state. This paper presents a RL
approach for determining what dialogue
features are important to a spoken dia-
logue tutoring system. Our experiments
show that incorporating dialogue factors
such as dialogue acts, emotion, repeated
concepts and performance play a signifi-
cant role in tutoring and should be taken
into account when designing dialogue sys-
tems.
1 Introduction
This paper presents initial research toward the
long-term goal of designing a tutoring system that
can effectively adapt to the student. While most
work in Markov Decision Processes (MDPs) and
spoken dialogue have focused on building better
policies (Walker, 2000; Henderson et al, 2005), to
date very little empirical work has tested the utility
of adding specialized features to construct a better
dialogue state. We wish to show that adding more
complex factors to a representation of student state
is a worthwhile pursuit, since it alters what action
the tutor should make. The five dialogue factors
we explore are dialogue acts, certainty level, frus-
tration level, concept repetition, and student per-
formance. All five are factors that are not just
unique to the tutoring domain but are important
to dialogue systems in general. Our results show
that using these features, combined with the com-
mon baseline of student correctness, leads to a sig-
nificant change in the policies produced, and thus
should be taken into account when designing a
system.
2 Background
We follow past lines of research (such as (Singh
et al, 1999)) for describing a dialogue   as a tra-
jectory within a Markov Decision Process (Sutton
and Barto, 1998). A MDP has four main com-
ponents: states, actions, a policy, which specifies
what is the best action to take in a state, and a re-
ward function which specifies the utility of each
state and the process as a whole. Dialogue man-
agement is easily described using a MDP because
one can consider the actions as actions made by
the system, the state as the dialogue context, and
a reward which for many dialogue systems tends
to be task completion success or dialogue length.
Typically the state is viewed as a vector of features
such as dialogue history, speech recognition con-
fidence, etc.
The goal of using MDPs is to determine the best
policy  for a certain state and action space. That
is, we wish to find the best combination of states
and actions to maximize the reward at the end of
the dialogue. In most dialogues, the exact reward
for each state is not known immediately, in fact,
usually only the final reward is known at the end
of the dialogue. As long as we have a reward func-
tion, Reinforcement Learning allows one to auto-
matically compute the best policy. The following
recursive equation gives us a way of calculating
the expected cumulative value (V-value) of a state
 (-value):
289
 

	

	
		
 

	
		
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 272?279,
New York, June 2006. c?2006 Association for Computational Linguistics
Comparing the Utility of State Features in Spoken Dialogue Using
Reinforcement Learning
Joel R. Tetreault
University of Pittsburgh
Learning Research and Development Center
Pittsburgh PA, 15260, USA
tetreaul@pitt.edu
Diane J. Litman
University of Pittsburgh
Department of Computer Science
Learning Research and Development Center
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
Recent work in designing spoken dialogue
systems has focused on using Reinforce-
ment Learning to automatically learn the
best action for a system to take at any
point in the dialogue to maximize dia-
logue success. While policy development
is very important, choosing the best fea-
tures to model the user state is equally im-
portant since it impacts the actions a sys-
tem should make. In this paper, we com-
pare the relative utility of adding three fea-
tures to a model of user state in the do-
main of a spoken dialogue tutoring sys-
tem. In addition, we also look at the ef-
fects of these features on what type of a
question a tutoring system should ask at
any state and compare it with our previ-
ous work on using feedback as the system
action.
1 Introduction
A host of issues confront spoken dialogue system
designers, such as choosing the best system action to
perform given any user state, and also selecting the
right features to best represent the user state. While
recent work has focused on using Reinforcement
Learning (RL) to address the first issue (such as
(Walker, 2000), (Henderson et al, 2005), (Williams
et al, 2005a)), there has been very little empirical
work on the issue of feature selection in prior RL ap-
proaches to dialogue systems. In this paper, we use
a corpus of dialogues of humans interacting with a
spoken dialogue tutoring system to show the com-
parative utility of adding the three features of con-
cept repetition, frustration level, and student perfor-
mance. These features are not just unique to the tu-
toring domain but are important to dialogue systems
in general. Our empirical results show that these fea-
tures all lead to changes in what action the system
should take, with concept repetition and frustration
having the largest effects.
This paper extends our previous work (Tetreault
and Litman, 2006) which first presented a method-
ology for exploring whether adding more complex
features to a representation of student state will ben-
eficially alter tutor actions with respect to feedback.
Here we present an empirical method of comparing
the effects of each feature while also generalizing
our findings to a different action choice of what type
of follow-up question should a tutor ask the student
(as opposed to what type of feedback should the tu-
tor give). In complex domains such as tutoring, test-
ing different policies with real or simulated students
can be time consuming and costly so it is important
to properly choose the best features before testing,
which this work allows us to do. This in turn aids
our long-term goal of improving a spoken dialogue
system that can effectively adapt to a student to max-
imize their learning.
2 Background
We follow past lines of research (such as (Levin and
Pieraccini, 1997) and (Singh et al, 1999)) for de-
scribing a dialogue
 
as a trajectory within a Markov
Decision Process (MDP) (Sutton and Barto, 1998).
272
A MDP has four main components: 1: states   , 2:
actions

, 3: a policy  , which specifies what is the
best action to take in a state, and 4: a reward func-
tion  which specifies the worth of the entire pro-
cess. Dialogue management is easily described us-
ing a MDP because one can consider the actions as
actions made by the system, the state as the dialogue
context (which can be viewed as a vector of features,
such as ASR confidence or dialogue act), and a re-
ward which for many dialogue systems tends to be
task completion success or dialogue length.
Another advantage of using MDP?s to model a di-
alogue space, besides the fact that the primary MDP
parameters easily map to dialogue parameters, is the
notion of delayed reward. In a MDP, since rewards
are often not given until the final states, dynamic
programming is used to propagate the rewards back
to the internal states to weight the value of each state
(called the V-value), as well as to develop an optimal
policy  for each state of the MDP. This propaga-
tion of reward is done using the policy iteration al-
gorithm (Sutton and Barto, 1998) which iteratively
updates the V-value and best action for each state
based on the values of its neighboring states.
The V-value of each state is important for our pur-
poses not only because it describes the relative worth
of a state within the MDP, but as more data is added
when building the MDP, the V-values should stabi-
lize, and thus the policies stabilize as well. Since,
in this paper, we are comparing policies in a fixed
data set it is important to show that the policies are
indeed reliable, and not fluctuating.
For this study, we used the MDP infrastructure de-
signed in our previous work which allows the user
to easily set state, action, and reward parameters. It
then performs policy iteration to generate a policy
and V-values for each state. In the following sec-
tions, we discuss our corpus, methodology, and re-
sults.
3 Corpus
For our study, we used an annotated corpus of
20 human-computer spoken dialogue tutoring ses-
sions (for our work we use the ITSPOKE system
(Litman and Silliman, 2004) which uses the text-
based Why2-ATLAS dialogue tutoring system as its
?back-end? (VanLehn et al, 2002)). The content
State Feature Values
Certainty Certain (cer)
Uncertain (unc)
Neutral (neu)
Frustration Frustrated (F)
Neutral (N),
Correctness Correct (C)
Partially Correct (PC)
Incorrect (I)
Percent Correct 50-100% (H)igh
0-49% (L)ow
Concept Repetition Concept is new (0)
Concept is repeated (R)
Table 1: Potential Student State Features in MDP
of the system, and all possible dialogue paths, were
authored by physics experts. Each session consists
of an interaction with one student over 5 different
college-level physics problems, for a total of 100 di-
alogues. Before each session, the student is asked to
read physics material for 30 minutes and then take a
pretest based on that material. Each problem begins
with the student writing out a short essay response
to the question posed by the computer tutor. The
fully-automated system assesses the essay for poten-
tial flaws in the reasoning and then starts a dialogue
with the student, asking questions to help the stu-
dent understand the confused concepts. The tutor?s
response and next question is based only on the cor-
rectness of the student?s last answer. Informally, the
dialogue follows a question-answer format. Once
the student has successfully completed the dialogue
section, he is asked to correct the initial essay. Each
of the dialogues takes on average 20 minutes and 60
turns. Finally, the student is given a posttest simi-
lar to the pretest, from which we can calculate their
normalized learning gain: 
	Proceedings of NAACL HLT 2007, pages 276?283,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Estimating the Reliability of MDP Policies: A Confidence Interval Approach
Joel R. Tetreault
University of Pittsburgh
LRDC
Pittsburgh PA, 15260, USA
tetreaul@pitt.edu
Dan Bohus
Carnegie Mellon University
Dept. of Computer Science
Pittsburgh, PA, 15213, USA
dbohus@cs.cmu.edu
Diane J. Litman
University of Pittsburgh
Dept. of Computer Science
LRDC
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
Past approaches for using reinforcement
learning to derive dialog control policies
have assumed that there was enough col-
lected data to derive a reliable policy. In
this paper we present a methodology for
numerically constructing confidence inter-
vals for the expected cumulative reward
for a learned policy. These intervals are
used to (1) better assess the reliability
of the expected cumulative reward, and
(2) perform a refined comparison between
policies derived from different Markov
Decision Processes (MDP) models. We
applied this methodology to a prior ex-
periment where the goal was to select the
best features to include in the MDP state-
space. Our results show that while some
of the policies developed in the prior work
exhibited very large confidence intervals,
the policy developed from the best feature
set had a much smaller confidence interval
and thus showed very high reliability.
1 Introduction
NLP researchers frequently have to deal with issues
of data sparsity. Whether the task is machine transla-
tion or named-entity recognition, the amount of data
one has to train or test with can greatly impact the re-
liability and robustness of one?s models, results and
conclusions.
One research area that is particularly sensitive to
the data sparsity issue is machine learning, specifi-
cally in using Reinforcement Learning (RL) to learn
the optimal action for a dialogue system to make
given any user state. Typically this involves learn-
ing from previously collected data or interacting in
real-time with real users or user simulators. One of
the biggest advantages to this machine learning ap-
proach is that it can be used to generate optimal poli-
cies for every possible state. However, this method
requires a thorough exploration of the state-space to
make reliable conclusions on what the best actions
are. States that are infrequently visited in the train-
ing set could be assigned sub-optimal actions, and
therefore the resulting dialogue manager may not
provide the best interaction for the user.
In this work, we present an approach for esti-
mating the reliability of a policy derived from col-
lected training data. The key idea is to take into ac-
count the uncertainty in the model parameters (MDP
transition probabilities), and use that information to
numerically construct a confidence interval for the
expected cumulative reward for the learned policy.
This confidence interval approach allows us to: (1)
better assess the reliability of the expected cumula-
tive reward for a given policy, and (2) perform a re-
fined comparison between policies derived from dif-
ferent MDP models.
We apply the proposed approach to our previous
work (Tetreault and Litman, 2006) in using RL to
improve a spoken dialogue tutoring system. In that
work, a dataset of 100 dialogues was used to de-
velop a methodology for selecting which user state
features should be included in the MDP state-space.
But are 100 dialogues enough to generate reliable
policies? In this paper we apply our confidence in-
276
terval approach to the same dataset in an effort to in-
vestigate how reliable our previous conclusions are,
given the amount of available training data.
In the following section, we discuss the prior
work and its data sparsity issue. In section 3, we
describe in detail our confidence interval methodol-
ogy. In section 4, we show how this methodology
works by applying it to the prior work. In sections 5
and 6, we present our conclusions and future work.
2 Previous Work
Past research into using RL to improve spoken di-
alogue systems has commonly used Markov Deci-
sion Processes (MDP?s) (Sutton and Barto, 1998)
to model a dialogue (such as (Levin and Pieraccini,
1997) and (Singh et al, 1999)).
A MDP is defined by a set of states {si}i=1..n,a set of actions {ak}k=1..p, and a set of transitionprobabilities which reflect the dynamics of the en-
vironment {p(si|sj, ak)}k=1..pi,j=1..n: if the model is attime t in state sj and takes action ak, then it willtransition to state si with probability p(si|sj , ak).Additionally, an expected reward r(si, sj , ak) is de-fined for each transition. Once these model parame-
ters are known, a simple dynamic programming ap-
proach can be used to learn the optimal control pol-
icy pi?, i.e. the set of actions the model should take
at each state, to maximize its expected cumulative
reward.
The dialog control problem can be naturally cast
in this formalism: the states {si}i=1..n in the MDPcorrespond to the dialog states (or an abstraction
thereof), the actions {ak}k=1..p correspond to theparticular actions the dialog manager might take,
and the rewards r(si, sj , ak) are defined to reflecta particular dialog performance metric. Once the
MDP structure has been defined, the model param-
eters {p(si|sj, ak)}k=1..pi,j=1..n are estimated from a cor-pus of dialogs (either real or simulated), and, based
on them, the policy which maximizes the expected
cumulative reward is computed.
While most work in this area has focused on de-
veloping the best policy (such as (Walker, 2000),
(Henderson et al, 2005)), there has been relatively
little work done with respect to selecting the best
features to include in the MDP state-space. For in-
stance, Singh et al (1999) showed that dialogue
length was a useful state feature and Frampton and
Lemon (2005) showed that the user?s last dialogue
act was also useful. In our previous work, we com-
pare the worth of several features. In addition, Paek
and Chickering?s (2005) work showed how a state-
space can be reduced by only selecting features that
are relevant to maximizing the reward function.
The motivation for this line of research is that if
one can properly select the most informative fea-
tures, one develops better policies, and thus a bet-
ter dialogue system. In the following sections we
summarize our past data, approach, results, and is-
sue with policy reliability.
2.1 MDP Structure
For this study, we used an annotated corpus of
human-computer spoken dialogue tutoring sessions.
The fixed-policy corpus contains data collected from
20 students interacting with the system for five prob-
lems (for a total of 100 dialogues of roughly 50 turns
each). The corpus was annotated with 5 state fea-
tures (Table 1). It should be noted that two of the
features, Certainty and Frustration, were manually
annotated while the other three were done automat-
ically. All features are binary except for Certainty
which has three values.
State Values
Correctness Student is correct or incorrect
in the current turn
Certainty Student is certain, neutral
or uncertain in the current turn
Concept Repetition A particular concept is either new
or repeated
Frustration Student is frustrated or not
in the current turn
Percent Correct Student answers over 66% of
questions correctly in dialogue
so far, or less
Table 1: State Features in Tutoring Corpus
For the action set {ak}k=1..p, we looked at whattype of question the system could ask the student
given the previous state. There are a total of four
possible actions: ask a short answer question (one
that requires a simple one word response), a com-
plex answer question (one that requires a longer,
deeper response), ask both a simple and complex
question in the same turn, or do not ask a question
at all (give a hint). The reward function r was the
277
learning gain of each student based on a pair of tests
before and after the entire session of 5 dialogues.
The 20 students were split into two groups (high
and low learners) based on their learning gain, so
10 students and their respective five dialogues were
given a positive reward of +100, while the remain-
der were assigned a negative reward of -100. The
rewards were assigned in the final dialogue state, a
common approach when applying RL in spoken di-
alogue systems.
2.2 Approach and Results
To investigate the usefulness of different features,
we took the following approach. We started with
two baseline MDPs. The first model (Baseline 1)
used only the Correctness feature in the state-space.
The second model (Baseline 2) included both the
Correctness and Certainty features. Next we con-
structed 3 new models by adding each of the remain-
ing three features (Frustration, Percent Correct and
Concept Repetition) to the Baseline 2 model.
We defined three metrics to compare the policies
derived from these MDPs: (1) Diff?s: the number of
states whose policy differs from the Baseline 2 pol-
icy, (2) Percent Policy change (P.C.): the weighted
amount of change between the two policies (100%
indicates total change), and (3) Expected Cumula-
tive Reward (or ECR) which is the average reward
one would expect in that MDP when in the state-
space.
The intuition is that if a new feature were rele-
vant, the corresponding model would lead to a dif-
ferent policy and a better expected cumulative re-
ward (when compared to the baseline models). Con-
versely, if the features were not useful, one would
expect that the new policies would look similar
(specifically, the Diff?s count and % Policy Change
would be low) or produce similar expected cumula-
tive rewards to the original baseline policy.
The results of this analysis are shown in Table 2 1
The Diff?s and Policy Change metrics are undefined
for the two baselines since we only use these two
metrics to compare the other three features to Base-
1Please note that to due to refinements in code, there is a
slight difference between the ECR?s reported in this work and
the ECR?s reported in the previous work, for the three features
added to Baseline 2. These changes did not alter the rankings
of these models, or the conclusions of the previous work.
line 2. All three metrics show that the best feature
to add to the Baseline 2 model is Concept Repetition
since it results in the most change over the Baseline
2 policy, and also the expected reward is the highest
as well. For the remainder of this paper, when we
refer to Concept Repetition, Frustration, or Percent
Correctness, we are referring to the model that in-
cludes that feature as well as the Baseline 2 features
Correctness and Certainty.
State Feature # Diff?s % P.C. ECR
Baseline 1 N/A N/A 6.15
Baseline 2 N/A N/A 31.92
B2 + Concept Repetition 10 80.2% 42.56
B2 + Frustration 8 66.4% 32.99
B2 + Percent Correctness 4 44.3% 28.50
Table 2: Feature Comparison Results
2.3 Problem with Reliability
However, the approach discussed above assumes
that given the size of the data set, the ECR and poli-
cies are reliable. If the MDP model were very frag-
ile, that is the policy and expected cumulative reward
were very sensitive to the quality of the transition
probability estimates, then the metrics could reveal
quite different rankings. Previously, we used a qual-
itative approach of tracking how the worth of each
state (V-value) changed over time. The V-values
indicate how much reward one would expect from
starting in that state to get to a final state. We hy-
pothesized that if the V-values stabilized as data in-
creased, then the learned policy would be more reli-
able.
So is this V-value methodology adequate for as-
sessing if there is enough data to determine a sta-
ble policy, and also for assessing if one model is
better than another? Since our approach for state-
space selection is based on comparing a new pol-
icy with a baseline policy, having a stable policy is
extremely important since instability could lead to
different conclusions. For example, in one compar-
ison, a new policy could differ with the baseline in
8 out of 10 states. But if the MDP were unstable,
adding just a little more data could result in a differ-
ence of only 4 out of 10 states. Is there an approach
that can categorize whether given a certain data size,
278
that the expected cumulative reward (and thus the
policy) is reliable? In the next section we present a
new methodology for numerically constructing con-
fidence intervals for these value function estimates.
Then, in the following section, we reevaluate our
prior work with this methodology and discuss the
results.
3 Confidence Interval Methodology
3.1 Policy Evaluation with Confidence
Intervals
The starting point for the proposed methodology
is the observation that for each state sj and ac-tion ak in the MDP, the set of transition probabili-ties {p(si|sj, ak)}i=1..n are modeled as multinomialdistributions that are estimated from the transition
counts in the training data:
p?(si|sj, ak) =
c(si, sj, ak)
?n
i=1 c(si, sj , ak)
(1)
where n is the number of states in the model, and
c(si, sj , ak) is the number of times the system wasin state sj , took action ak, and transitioned to state
si in the training data.It is important to note that these parameters are
just estimates. The reliability of these estimates
clearly depends on the amount of training data, more
specifically on the transition counts c(si, sj, ak). Forinstance, consider a model with 3 states and 2 ac-
tions. Say the model was in state s1 and took action
a1 ten times. Out of these, three times the modeltransitioned back to state s1, two times it transi-tioned to state s2, and five times to state s3. Thenwe have:
p?(si|s1, a1) = ?0.3; 0.2; 0.5? = ? 310 ;
2
10 ;
5
10 ? (2)
Additionally, let?s say the same model was in state
s2 and took action a2 1000 times. Following that ac-tion, it transitioned 300 times to state s1, 200 timesto state s2, and 500 times to state s3.
p?(si|s2, a2) = ?0.3; 0.2; 0.5? = ? 3001000 ;
200
1000 ;
500
1000 ? (3)
While both sets of transition parameters have the
same value, the second set of estimates is more reli-
able. The central idea of the proposed approach is to
model this uncertainty in the system parameters, and
use it to numerically construct confidence intervals
for the value of the optimal policy.
Formally, each set of transition probabilities
{p(si|sj , ak)}i=1..n is modeled as a multinomial dis-tribution, estimated from data2. The uncertainty of
multinomial estimates are commonly modeled by
means of a Dirichlet distribution. The Dirichlet dis-
tribution is characterized by a set of parameters ?1,
?2, ..., ?n, which in this case correspond to thecounts {c(si, sj , ak)}i=1..n. For any given j, thelikelihood of the set of multinomial transition pa-
rameters {p(si|sj, ak)}i=1..n is then given by:
P ({p(si|sj , ak)}i=1..n|D) =
= 1Z(D)
?n
i=1 p(si|sj , ak)?i?1 (4)
where Z(D) =
?n
i=1 ?(?i)
?(
?n
i=1 ?i)
and ?i = c(si, sj , ak).
Note that the maximum likelihood estimates for the
formula above correspond to the frequency count
formula we have already described:
p?ML(si|sj, ak) =
?i
?n
i=1 ?i
= c(si, sj, ak)?n
i=1 c(si, sj , ak)(5)
To capture the uncertainty in the model parame-
ters, we therefore simply need to store the counts
of the observed transitions c(si, sj , ak). Based onthis model of uncertainty, we can numerically con-
struct a confidence interval for the value of the opti-
mal policy pi?. Instead of computing the value of the
policy based on the maximum likelihood transition
estimates T?ML = {p?ML(si|sj , ak)}k=1..pi,j=1..n, we gen-
erate a large number of transition matrices T?1, T?1,... T?m by sampling from the Dirichlet distributionscorresponding to the counts observed in the train-
ing data (in the experiments reported in this paper,
we used m = 1000). We then compute the value
of the optimal policy pi? in each of these models
{Vpi?(T?i)}i=1..m. Finally, we numerically constructthe 95% confidence interval for the value function
based on the resulting value estimates: the bounds
for the confidence interval are set at the lowest and
highest 2.5 percentile of the resulting distribution of
the values for the optimal policy {Vpi?(T?i)}i=1..m.The algorithm is outlined below:
2By p we will denote the true model parameters; by p? we
will denote data-driven estimates for these parameters
279
1. compute transition counts from the training set:
C = {c(si, sj, ak)}k=1..pi,j=1..n (6)
2. compute maximum likelihood estimates for
transition probability matrix:
T?ML = {p?ML(si|sj , ak)}k=1..pi,j=1..n (7)
3. use dynamic programming to compute the op-
timal policy pi? for model T?ML
4. sample m transition matrices {T?k}k=1..m, us-ing the Dirichlet distribution for each row:
{p?i(si|sj, ak)}i=1..n =
= Dir({c(si, sj , ak)}i=1..n) (8)
5. evaluate the optimal policy pi? in each of these
m models, and obtain Vpi?(T?i)
6. numerically build the 95% confidence interval
for Vpi? from these estimates.
To summarize, the central idea is to take into ac-
count the reliability of the transition probability esti-
mates and construct a confidence interval for the ex-
pected cumulative reward for the learned policy. In
the standard approach, we would compute an esti-
mate for the expected cumulative reward, by simply
using the transition probabilities derived from the
training set. Note that these transition probabilities
are simply estimates which are more or less accu-
rate, depending on how much data is available. The
proposed methodology does not fully trust these es-
timates, and asks the question: given that the real
world (i.e. real transition probabilities) might actu-
ally be a bit different than we think it is, how well
can we expect the learned policy to perform? Note
that the confidence interval we construct, and there-
fore the conclusions we draw, are with respect to the
policy learned from the current estimates, i.e. from
the current training set. If more data becomes avail-
able, a different optimal policy might emerge, about
which we cannot say much.
3.2 Related Work
Given the stochastic nature of the models, confi-
dence intervals are often used to estimate the reli-
ability of results in machine learning experiments,
e.g. (Rivals and Personnaz, 2002), (Schapire, 2002)
and (Dumais et al, 1998). In this work we use a
confidence interval methodology in the context of
MDPs. The idea of modeling the uncertainty of
the transition probability estimates using Dirichlet
models also appears in (Jaulmes et al, 2005). In
that work, the authors used the uncertainty in model
parameters to develop active learning strategies for
partially observable MDPs, a topic not previously
addressed in the literature. In our work we rely on
the same model of uncertainty for the transition ma-
trix, but use it to derive confidence intervals for the
expected cumulative reward for the learned optimal
policy, in an effort to assess the reliability of this
policy.
4 Results
Our previous results indicated that Concept Repe-
tition was the best feature to add to the Baseline 2
state-space model, but also that Percent Correctness
and Frustration (when added to Baseline 2) offered
an improvement over the Baseline MDP?s. How-
ever, these conclusions were based on a very quali-
tative approach for determining if a policy is reliable
or not. In the following subsection, we apply our ap-
proach of confidence intervals to empirically deter-
mine if given this data set of 100 dialogues, whether
the estimates of the ECR are reliable, and whether
the original rankings and conclusions hold up under
this refined analysis. In subsection 4.2, we provide
a methodology for pinpointing when one model is
better than another.
4.1 Quantitative Analysis of ECR Reliability
For our first investigation, we look at the confidence
intervals of each MDP?s ECR over the entire data set
of 20 students (later in this section we show plots for
the confidence intervals as data increases). Table 3
shows the upper and lower bounds for the ECR orig-
inally reported in Table 2. The first column shows
the original, estimated ECR of the MDP and the last
column is the width of the bound (the difference be-
tween the upper and lower bound).
So what conclusions can we make about the reli-
ability of the ECR, and hence of the learned policies
for the different MDP?s, given this amount of train-
ing data? The confidence interval for the ECR for
280
State Feature ECR Lower Bound Upper Bound Width
Baseline 1 6.15 0.21 23.73 23.52
Baseline 2 (B2) 31.92 -5.31 60.48 65.79
B2 + Concept Repetition 42.56 28.37 59.29 30.92
B2 + Frustration 32.99 -4.12 61.30 65.42
B2 + Percent Correctness 28.50 -5.89 57.82 63.71
Table 3: Confidence Intervals with complete dataset
the Baseline 1 model ranges from 0.21 to 23.73. Re-
call that the final states are capped at +100 and -100,
and are thus the maximum and minimum bounds
that one can see in this experiment. These bounds
tell us that, if we take into account the uncertainty
in the model estimates (given the small training set
size), with probability 0.95 the actual true ECR for
this policy will be greater than 0.21 and smaller than
23.73. The width of this confidence interval is 23.52.
For the Baseline 2 model, the bounds are much
wider: from -5.31 to 60.48, for a total width of
65.79. While the ECR estimate is 31.92 (which
is seemingly larger than 6.15 for the Baseline 1
model), the wide confidence interval tells us that this
estimate is not very reliable. It is possible that the
policy derived from this model with this amount of
data could perform poorly, and even get a negative
reward. From the dialogue system designer?s stand-
point, a model like this is best avoided.
Of the remaining three models ? Concept Repeti-
tion, Frustration, and Percent Correctness, the first
one exhibits a tighter confidence interval, indicat-
ing that the estimated expected cumulative reward
(42.56) is fairly reliable: with 95% probability of
being between 28.37 and 59.29. The ECR for the
other two models (Frustration and Percent Correct-
ness) again shows a wide confidence interval once
we take into account the uncertainty in the model
parameters.
These results shed more light on the shortcom-
ings of the ECR metric used to evaluate the models
in prior work. This estimate does not take into ac-
count the uncertainty of the model parameters. For
example, a model can have an optimal policy with
a very high ECR value, but have very wide confi-
dence bounds reaching even into negative rewards.
On the other hand, another model can have a rela-
tively lower ECR but if its bounds are tighter (and
the lower bound is not negative), one can know that
that policy is less affected by poor parameter esti-
mates stemming from data sparsity issues. Using the
confidence intervals associated with the ECR gives a
much more refined, quantitative estimate of the reli-
ability of the reward, and hence of the policy derived
from that data.
An extension of this result is that confidence in-
tervals can also allow us to make refined judgments
about the comparative utility of different features,
the original motivation of our prior study. Basi-
cally, a model (M1) is better than another (M2) if
M1?s lower bound is greater than the upper bound of
M2. That is, one knows that 95% of the time, the
worst case situation of M1 (the lower bound) will
always yield a higher reward than the best case of
M2. In our data, this happens only once, with Con-
cept Repetition being empirically better than Base-
line 1, since the lower bound of Concept Repetition
is 28.37 and the upper bound of Baseline 1 is 23.73.
Given this situation, Concept Repetition is a useful
feature which, when included in the model, leads to
a better policy than simply using Correctness. We
cannot draw any conclusions about the other fea-
tures, since their bounds are generally quite wide.
Given this amount of training data, we cannot say
whether Percent Correctness and Frustration are bet-
ter features than the Baseline MDP?s. Although their
ECR?s are higher, there is too much uncertainty to
definitely conclude they are better.
4.2 Pinpointing Model Cross-over
The previous analysis focused on a quantitative
method of (1) determining the reliability of the MDP
ECR estimate and policy, as well as (2) assessing
whether one model is better than another. In this
section, we present an extension to the second con-
tribution by answering the question: given that one
model is more reliable than another, is it possible
to determine at which point one model?s estimates
become more reliable than another model?s? In our
281
0 2 4 6 8 10 12 14 16 18 20
?100
?80
?60
?40
?20
0
20
40
60
80
100
Baseline 1
# of students
EC
R
 
 
Confidence Bounds
Calculated ECR
0 2 4 6 8 10 12 14 16 18 20
?100
?80
?60
?40
?20
0
20
40
60
80
100
Baseline 2 +Concept Repetition
# of students
EC
R
 
 
Confidence Bounds
Calculated ECR
Figure 1: Confidence Interval Plots
case, we want to know at what point Concept Rep-
etition becomes more reliable than Baseline 1. To
do this, we investigate how the confidence interval
changes as the amount of training data increases in-
stead of looking at the reliability estimate at only one
particular data size.
We incrementally increase the amount of train-
ing data (adding the data from one new student at a
time), and calculate the corresponding optimal pol-
icy and confidence interval for the expected cumula-
tive reward for that policy. Figure 1 shows the con-
fidence interval plots as data is added to the MDP
for the Baseline 1 and Concept Repetition MDP?s.
For reference, Baseline 2, Percent Correctness and
Frustration plots did not exhibit the same converg-
ing behavior as these two, which is not surprising
given how wide the final bounds are. For each plot,
the bold lines represent the upper and lower bounds,
and the dotted line represents the calculated ECR.
Analyzing the two MDP?s, we find that the confi-
dence intervals for Baseline 1 and Concept Repeti-
tion converge as more data is added, which is an ex-
pected trend. One useful result from observing the
change in confidence intervals is that one can de-
termine the point in one which one model becomes
empirically better than another. Superimposing the
upper and lower bounds (Figure 2) reveals that after
we include the data from the first 13 students, the
lower bound of Concept Repetition crosses over the
upper bound of Baseline 1.
Observing this behavior is especially useful for
performing model switching. In automatic model
switching, a dialogue manager runs in real time and
as it collects data, it can switch from using a sim-
ple dialogue model to a complex model. Confidence
intervals can be used to determine when to switch
from one model to the next by checking if a complex
model?s bounds cross over the bounds of the current
model. Basically, the dialogue manager switches
when it can be sure that the more complex model?s
ECR is not only higher, but statistically significantly
so.
0 2 4 6 8 10 12 14 16 18 20
?50
0
50
100
# of students
EC
R
Baseline 1 and Concept Repetition Superimposed
 
 
Baseline 1
B2 + Concept Repetition
Figure 2: Baseline 1 and Concept Repetition Bounds
5 Conclusions
Past work in using MDP?s to improve spoken dia-
logue systems have usually glossed over the issue of
whether or not there was enough training data to de-
velop reliable policies. In this work, we present a
numerical method for building confidence intervals
for the expected cumulative reward for a learned pol-
icy. The proposed approach allows one to (1) better
282
assess the reliability of the expected cumulative re-
ward for a given policy, and (2) perform a refined
comparison between policies derived from different
MDP models.
We applied this methodology to a prior experi-
ment where the objective was to select the best fea-
tures to include in the MDP state-space. Our results
show that policies constructed from the Baseline 1
and Concept Repetition models are more reliable,
given the amount of data available for training. The
Concept Repetition model (which is composed of
the Concept Repetition, Certainty and Correctness
features) was especially useful, as it led to a policy
that outperformed the Baseline 1 model, even when
we take into account the uncertainty in the model
estimates caused by data sparsity. In contrast, for
the Baseline 2, Percent Correctness, and Frustration
models, the estimates for the expected cumulative
reward are much less reliable, and no conclusion can
be reliably drawn about the usefulness of these fea-
tures. In addition, we showed that our confidence
interval approach has applications in another MDP
problem: model switching.
6 Future Work
As an extension of this work, we are currently inves-
tigating in more detail what makes some MDP?s reli-
able or unreliable for a certain data size (such as the
case where Baseline 2 does not converge but a more
complicated model does, such as Concept Repeti-
tion). Our initial findings indicate that, as more data
becomes available the bounds tighten for most pa-
rameters in the transition matrix. However, for some
of the parameters the bounds can remain wide, and
that is enough to keep the confidence interval for the
expected cumulative reward from converging.
Acknowledgments
We would like to thank Jeff Schneider, Drew Bag-
nell, Pam Jordan, as well as the ITSPOKE and Pitt
NLP groups, and the Dialog on Dialogs group for
their help and comments. Finally, we would like to
thank the four anonymous reviewers for their com-
ments on the initial version of this paper. Support for
this research was provided by NSF grants #0325054
and #0328431.
References
S. Dumais, J. Platt, D. Heckerman, and M. Sahami. 1998.Inductive learning algorithms and representations fortext categorization. In Conference on Information and
Knowledge Management.
M. Frampton and O. Lemon. 2005. Reinforcement learn-ing of dialogue strategies using the user?s last dialogueact. In IJCAI Wkshp. on K&R in Practical Dialogue
Systems.
J. Henderson, O. Lemon, and K. Georgila. 2005. Hybridreinforcement/supervised learning for dialogue poli-cies from communicator data. In IJCAI Wkshp. on
K&R in Practical Dialogue Systems.
R. Jaulmes, J. Pineau, and D. Precup. 2005. Active learn-ing in partially observable markov decision processes.In European Conference on Machine Learning.
E. Levin and R. Pieraccini. 1997. A stochastic model of
computer-human interaction for learning dialogues. In
Proc. of EUROSPEECH ?97.
T. Paek and D. Chickering. 2005. The markov assump-tion in spoken dialogue management. In 6th SIGDial
Workshop on Discourse and Dialogue.
I. Rivals and L. Personnaz. 2002. Construction of con-fidence intervals for neural networks based on leastsquares estimation. In Neural Networks.
R. Schapire. 2002. The boosting approach to machinelearning: An overview. In MSRI Workshop on Nonlin-
ear Estimation and Classification.
S. Singh, M. Kearns, D. Litman, and M. Walker. 1999.
Reinforcement learning for spoken dialogue systems.In Proc. NIPS ?99.
R. Sutton and A. Barto. 1998. Reinforcement Learning.The MIT Press.
J. Tetreault and D. Litman. 2006. Comparing the utility
of state features in spoken dialogue using reinforce-ment learning. In NAACL.
M. Walker. 2000. An application of reinforcement learn-ing to dialogue strategy selection in a spoken dialogue
system for email. JAIR, 12.
283
Proceedings of NAACL HLT 2007, Companion Volume, pages 1?4,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Comparing User Simulation Models For Dialog Strategy Learning
Hua Ai
University of Pittsburgh
Intelligent Systems Program
Pittsburgh PA, 15260, USA
hua@cs.pitt.edu
Joel R. Tetreault
University of Pittsburgh
LRDC
Pittsburgh PA, 15260, USA
tetreaul@pitt.edu
Diane J. Litman
University of Pittsburgh
Dept. of Computer Science
LRDC
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
This paper explores what kind of user sim-
ulation model is suitable for developing
a training corpus for using Markov Deci-
sion Processes (MDPs) to automatically
learn dialog strategies. Our results sug-
gest that with sparse training data, a model
that aims to randomly explore more dialog
state spaces with certain constraints actu-
ally performs at the same or better than a
more complex model that simulates real-
istic user behaviors in a statistical way.
1 Introduction
Recently, user simulation has been used in the de-
velopment of spoken dialog systems. In contrast to
experiments with human subjects, which are usually
expensive and time consuming, user simulation gen-
erates a large corpus of user behaviors in a low-cost
and time-efficient manner. For example, user sim-
ulation has been used in evaluation of spoken dia-
log systems (Lo?pez-Co?zar et al, 2003) and to learn
dialog strategies (Scheffler, 2002). However, these
studies do not systematically evaluate how helpful a
user simulation is. (Schatzmann et al, 2005) pro-
pose a set of evaluation measures to assess the re-
alness of the simulated corpora (i.e. how similar
are the simulated behaviors and human behaviors).
Nevertheless, how realistic a simulated corpus needs
to be for different tasks is still an open question.
We hypothesize that for tasks like system eval-
uation, a more realistic simulated corpus is prefer-
able. Since the system strategies are evaluated and
adapted based on the analysis of these simulated dia-
log behaviors, we would expect that these behaviors
are what we are going to see in the test phase when
the systems interact with human users. However,
for automatically learning dialog strategies, it is not
clear how realistic versus how exploratory (Singh et
al., 2002) the training corpus should be. A train-
ing corpus needs to be exploratory with respect to
the chosen dialog system actions because if a cer-
tain action is never tried at certain states, we will
not know the value of taking that action in that state.
In (Singh et al, 2002), their system is designed to
randomly choose one from the allowed actions with
uniform probability in the training phase in order to
explore possible dialog state spaces. In contrast,we
use user simulation to generate exploratory training
data because in the tutoring system we work with,
reasonable tutor actions are largely restricted by stu-
dent performance. If certain student actions do not
appear, this system would not be able to explore a
state space randomly .
This paper investigates what kind of user simula-
tion is good for using Markov Decision Processes
(MDPs) to learn dialog strategies. In this study,
we compare three simulation models which differ in
their efforts on modeling the dialog behaviors in a
training corpus versus exploring a potentially larger
dialog space. In addition, we look into the impact of
different state space representations and different re-
ward functions on the choice of simulation models.
2 System and Corpus
Our system is a speech-enabled Intelligent Tutor-
ing System that helps students understand qualita-
1
tive physics questions. The dialog policy was deter-
ministic and hand-crafted in a finite state paradigm
(Ai et al, 2006). We collected 130 dialogs (1019
student utterances) with 26 human subjects. Cor-
rectness (correct(c), incorrect(ic)) is automatically
judged by the system1 and kept in the system?s logs.
Percent incorrectness (ic%) is also automatically
calculated and logged. Each student utterance was
manually annotated for certainty (certain, uncer-
tain, neutral, mixed) in a previous study2 based on
both lexical and prosodic information. In this study,
we use a two-way classification (certain(cert), not-
certain(ncert)), where we collapse uncertain, neu-
tral, and mixed to be ncert to balance our data. An
example of coded dialog between the tutor (T) and a
student (S) is given in Table 1.
3 Experimental Setup
3.1 Learning Task
Our current system can only respond to the cor-
rectness of a student?s utterances; the system thus
ignores other underlying information, for exam-
ple, certainty which is believed to provide use-
ful information for the tutor. In our corpus, the
strength of the tutor?s minimal feedback (defined be-
low) is in fact strongly correlated with the percent-
age of student certainty (chi-square test, p<0.01).
Strong Feedback (SF) is when the tutor clearly states
whether the student?s answer is correct or incor-
rect (i.e., ?This is great!?); Weak Feedback (WF)
is when the tutor does not comment on the correct-
ness of a student?s answer or gives slightly negative
feedback such as ?well?. Our goal is to learn how
to manipulate the strength of the tutor minimal feed-
back in order to maximize student?s overall certainty
in the entire dialog. We keep the other parts of the
tutor feedback (e.g. explanations, questions) so the
system?s original design of maximizing the percent-
age of student correct answers is utilized.
3.2 Simulation Models
All three models we describe below are trained from
the real corpus we collected. We simulate on the
word level because generating student?s dialog acts
alone does not provide sufficient information for
1Kappa of 0.79 is gained comparing to human judgements.
2Kappa of 0.68 is gained in a preliminary agreement study.
T1: Which law of motion would you use?
S1: Newton?s second law? [ic, ic%=1, ncert]
T2: Well... The best law to use is Newton?s
third law. Do you recall what it says?
S2: For every action there is an equal and
opposite reaction? [c, ic%=50%, ncert]
Table 1: Sample coded dialog excerpt.
our tutoring system to decide the next system?s ac-
tion. Thus, the output of the three models is a stu-
dent utterance along with the student certainty (cert,
ncert). Since it is hard to generate a natural lan-
guage utterance for each tutor?s question, we use the
student answers in the real corpus as the candidate
answers for the simulated students (Ai et al, 2006).
In addition, we simulate student certainty in a very
simple way: the simulation models output the cer-
tainty originally associated with that utterance.
Probabilistic Model (PM) is meant to capture re-
alistic student behavior in a probabilistic way. Given
a certain tutor question along with a tutor feedback,
it will first compute the probabilities of the four
types of student answers from the training corpus: c
and cert, c and ncert, ic and cert, and ic and ncert.
Then, following this distribution, the model selects
the type of student answers to output, and then it
picks an utterance that satisfies the correctness and
certainty constraints of the chosen answer type from
the candidate answer set and outputs that utterance.
We implement a back-off mechanism to count pos-
sible answers that do not appear in the real corpus.
Total Random Model (TRM) ignores what the
current question is or what feedback is given. It ran-
domly picks one utterance from all the utterances in
the entire candidate answer set. This model tries to
explore all the possible dialog states.
Restricted Random Model (RRM) differs from
the PM in that given a certain tutor question and a
tutor feedback, it chooses to give a c and cert, c and
ncert, ic and cert, or ic and ncert answer with equal
probability. This model is a compromise between
the exploration of the dialog state space and the re-
alness of generated user behaviors.
3.3 MDP Configuration
A MDP has four main components: states, actions,
a policy, and a reward function. In this study, the ac-
tions allowed in each dialog state are SF and WF;
2
the policy we are trying to learn is in every state
whether the tutor should give SF and WF in order
to maximize the percent certainty in the dialog.
Since different state space representations and re-
ward functions have a strong impact on the MDP
policy learning, we investigate different configura-
tions to avoid possible bias introduced by certain
configurations. We use two state space representa-
tions: SSR1 uses the correctness of current student
turn and percent incorrectness so far; and SSR2 adds
in the certainty of the current student turn on top of
SSR1. Two reward functions are investigated: in
RF1, we assign +100 to every dialog that has a per-
cent certainty higher than the median from the train-
ing corpus, and -100 to every dialog that has a per-
cent certainty below the median; in RF2, we assign
different rewards to every different dialog by multi-
plying the percent certainty in that dialog with 100.
Other MDP parameter settings are the same as de-
scribed in (Tetreault et al, 2006).
3.4 Methodology
We first let the three simulation models interact with
the original system to generate different training cor-
pora. Then, we learn three MDP policies in a fixed
configuration from the three training corpora sep-
arately. For each configuration, we run the sim-
ulation models until we get enough training data
such that the learned policies on that corpus do not
change anymore (40,000 dialogs are generated by
each model). After that, the learned new policies are
implemented into the original system respectively 3.
Finally, we use our most realistic model, the PM,
to interact with each new system 500 times to eval-
uate the new systems? performances. We use two
evaluation measures. EM1 is the number of dialogs
that would be assigned +100 using the old median
split. EM2 is the average of percent certainty in ev-
ery single dialog from the newly generated corpus.
A policy is considered better if it can improve the
percentage of certainty more than other policies, or
has more dialogs that will be assigned +100. The
baseline for EM1 is 250, since half of the 500 di-
alogs would be assigned +100 using the old median
3For example, the policy learned from the training corpus
generated by the RRM with SSR1 and RF1 is: give SF when
the current student answer is ic and ic%>50%, otherwise give
WF.
split. The baseline for EM2 is 35.21%, which is
obtained by calculating the percent certainty in the
corpus generated by the 40,000 interactions between
the PM and the original system.
4 Results and Discussion
Table 2 summarizes our results. There are two
columns under each ?state representation+reward
function? configuration, presenting the results using
the two evaluation approaches. EM1 measures ex-
actly what RF1 tries to optimize; while EM2 mea-
sures exactly what RF2 tries to optimize. However,
we show the results evaluated by both EM1 and
EM2 for all configurations since the two evaluation
measures have their own practical values and can
be deployed under different design requirements.
All results that significantly4 outperform the corre-
sponding baselines are marked with ?.
When evaluating using EM1, the RRM signifi-
cantly4 outperforms the other two models in all con-
figurations (in bold in Table 2). Also, the PM per-
forms better (but not statistically significantly) than
the TRM. When evaluating on EM2, the RRM sig-
nificantly4 outperforms the other two when using
SSR1 and RF1 (in bold in Table 2). In all other
configurations, the three models do not differ signif-
icantly. It is not surprising that the RRM outper-
forms the PM in most of the cases even when we
test on the PM. (Schatzmann et al, 2005) also ob-
serve that a good model can still perform well when
tested on a poor model.
We suspect that the performance of the PM is
harmed by the data sparsity issue in the real cor-
pus that we trained the model on. Consider the case
of SSR1: 25.8% of the potentially possible dialog
states do not exist in the real corpus. Although we
implement a back-off mechanism, the PM will still
have much less chance to transition to the states that
are not observed in the real corpus. Thus, when we
learn the MDP policy from the corpus generated by
this model, the actions to take in these less-likely
states are not fully learned. In contrast, the RRM
transitions from one state to each of the next possible
states with equal probability, which compensates for
the data sparsity problem. We further examine the
results obtained using SSR1 and RF1 and evaluated
4Using 2-sided t-test with Bonferroni correction, p<0.05.
3
Model Name SSR1+RF1 SSR2+RF1 SSR1+RF2 SSR2+RF2
EM1 EM2 EM1 EM2 EM1 EM2 EM1 EM2
Probabilistic Model 222 36.30% 217 37.63% 197 40.78%? 197 40.01%?
Total Random Model 192 36.30% 211 38.57% 188 40.21%? 179 40.21%?
Restricted Random Model 390? 46.11%? 368? 37.27% 309 40.21%? 301 40.21%?
Table 2: Evaluation of the new policies trained with the three simulation models
by EM1 to confirm our hypothesis. When looking
into the frequent states5, 70.1% of them are seen fre-
quently in the training corpus generated by the PM,
while 76.3% are seen frequently in the training cor-
pus generated by the RRM. A higher percentage in-
dicates the policy might be better trained with more
training instances. This explains why the RRM out-
performs the PM in this case.
While the TRM also tries to explore dialog state
space, only 65.2% of the frequent states in testing
phase are observed frequently in the training phase.
This is because the Total Random Model answers
90% of the questions incorrectly and often goes
deeply down the error-correction paths. It does ex-
plore some states that are at the end of the paths,
but since these are the infrequent states in the test
phase, exploring these states does not actually im-
prove the model?s performance much. On the other
hand, while the student correctness rate in the real
corpus is 60%, the RRM prevents itself from being
trapped in the less-likely states on incorrect answer
paths by keeping its correctness rate to be 50%.
Our results are preliminary but suggest interest-
ing points in building simulation models: 1. When
trained from a sparse data set, it may be better to
use a RRM than a more realistic PM or a more ex-
ploratory TRM; 2. State space representation may
not impact evaluation results as much as reward
functions and evaluation measures, since when us-
ing RF2 and evaluating with EM2, the differences
we see using RF1 or EM1 become less significant.
In our future work, we are going to further investi-
gate whether the trends shown in this paper general-
ize to on-line MDP policy learning. We also want to
explore other user simulations that are designed for
sparse training data (Henderson et al, 2005). More
5We define frequent states to be those that comprise at least
1% of the entire corpus. These frequent states add up to more
than 80% of the training/testing corpus. However, deciding the
threshold of the frequent states in training/testing is an open
question.
importantly, we are going to test the new policies
with the other simulations and human subjects to
validate the learning process.
Acknowledgements
NSF (0325054, 0328431) supports this research.
The authors wish to thank Tomas Singliar for his
valuable suggestions, Scott Silliman for his support
on building the simulation system, and the anony-
mous reviewers for their insightful comments.
References
H. Ai and D. Litman. 2006. Comparing Real-Real,
Simulated-Simulated, and Simulated-Real Spoken Di-
alogue Corpora. In Proc. AAAI Workshop on Statis-
tical and Empirical Approaches for SDS.
J. Henderson, O.Lemon, and K.Georgila. 2005. Hybrid
reinforcement/supervised learning for dialogue poli-
cies from COMMUNICATOR data. In Proc. IJCAI
workshop on Knowledge and Reasoning in Practical
Dialogue Systems.
R. Lo?pez-Co?zar, A. De la Torre, J. Segura, and A. Ru-
bio. 2003. Assessment of dialog systems by means of
a new simulation technique. Speech Communication
(40): 387-407.
K. Scheffler. 2002. Automatic Design of Spoken Dialog
Systems. Ph.D. diss., Cambridge University.
J. Schatzmann, K. Georgila, and S. Young. 2005. Quan-
titative Evaluation of User Simulation Techniques for
Spoken Dialog Systems. In Proc. of 6th SIGdial.
J. Schatzmann, M. N. Stuttle, K. Weilhammer and
S. Young. 2005. Effects of the User Model on
Simulation-based Learning of Dialogue Strategies. In
Proc. of ASRU05.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing Dialog Managment with Reinforcement
Learning: Experiments with the NJFun System. Jour-
nal of Artificial Intelligence Research, (16):105-133.
J. Tetreault and D. Litman. 2006. Comparing the Utility
of State Features in Spoken Dialogue Using Reinforce-
ment Learning.. In Proc. NAACL06.
4
Proceedings of NAACL HLT 2007, Companion Volume, pages 41?44,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Exploring Affect-Context Dependencies for Adaptive System Development
Kate Forbes-Riley
Learning R&D Ctr.
Univ. Pittsburgh
Pittsburgh, PA 15260
forbesk@pitt.edu
Mihai Rotaru
Computer Science Dpt.
Univ. Pittsburgh
Pittsburgh, PA 15260
mrotaru@cs.pitt.edu
Diane J. Litman
Learning R&D Ctr.
Computer Science Dpt.
Univ. Pittsburgh
Pittsburgh, PA 15260
litman@cs.pitt.edu
Joel Tetreault
Learning R&D Ctr.
Univ. Pittsburgh
Pittsburgh, PA 15260
tetreaul@pitt.edu
Abstract
We use ?2 to investigate the context de-
pendency of student affect in our com-
puter tutoring dialogues, targeting uncer-
tainty in student answers in 3 automati-
cally monitorable contexts. Our results
show significant dependencies between
uncertain answers and specific contexts.
Identification and analysis of these depen-
dencies is our first step in developing an
adaptive version of our dialogue system.
1 Introduction
Detecting and adapting to user affect is being ex-
plored by many researchers to improve dialogue sys-
tem quality. Detection has received much atten-
tion (e.g., (Litman and Forbes-Riley, 2004; Lee and
Narayanan, 2005)), but less work has been done on
adaptation, due to the difficulty of developing re-
sponses and applying them at the right time. Most
work on adaptation takes a context-independent ap-
proach: use the same type of response after all in-
stances of an affective state. For example, Liu and
Picard (2005)?s health assessment system responds
with empathy to all instances of user stress.
Research suggests, however, that it may be more
effective to take a context-dependent approach: de-
velop multiple responses for each affective state,
whose use depends on the state?s context. E.g., in the
tutoring domain, Pon-Barry et al (2006) show that
human tutors use multiple responses to uncertain
student answers, depending on the answer?s correct-
ness and prior context. In the information-seeking
domain, it is commonly believed that while an apol-
ogy is a good default response to user frustration (as
in (Klein et al, 2002)), one context requires a differ-
ent response: after several frustrated user turns, the
call should be forwarded to a human operator.
A context-dependent approach to affect adapta-
tion must address 2 issues: in what contexts to adapt,
and what responses to use there. This paper ad-
dresses the first issue and targets student uncertainty
in our computer tutoring dialogues. Although our di-
alogues have a Question-Answer format, our system
contains 275 tutor questions. Treating each question
as a context is too labor-intensive for adaptation de-
velopment and creates a data sparsity issue. Instead
we treat automatically monitorable question proper-
ties as contexts. Here we examine 3 contexts: the di-
alogue act interpretation, and the discourse structure
depth and transition, of the prior tutor question. We
use ?2 to investigate the context dependency of un-
certain student answers (correct and incorrect). Our
results show that some contexts are significantly as-
sociated with uncertain answers. Our next step will
be to use these significant dependencies to develop
system responses to uncertain answers in these con-
texts. These responses will be based both on our
hypotheses about why uncertainty is associated with
these contexts, and on analyses of human tutor re-
sponses to uncertain answers in these contexts.
2 Student Uncertainty and Prior Context
ITSPOKE is a speech-enabled version of a text-
based tutoring system (VanLehn et al, 2002). The
student first types an essay answering one of 5 qual-
itative physics problems. ITSPOKE parses the es-
say, extracts dialogue topics concerning misconcep-
tions, then engages the student in dialogue. In this
study we used 2 ITSPOKE corpora containing 4590
student turns over 185 dialogues from 37 students.
Figure 1 shows an annotated dialogue excerpt.
41
ITSPOKE4: Good. Does the packet have an acceleration? If yes please specify its direction. [SAQ, Advance, D1]
STUDENT4: horizontally forward yes [non-uncertain, incorrect]
ITSPOKE5: Well... To find the packet?s acceleration we will use Newton?s 2nd law. What is the definition of this law? [LAQ,
Push, D2]
STUDENT5: an object in motion tends to stay in motion until it?s act by an outside force [uncertain, incorrect]
ITSPOKE6: Well... Newton?s 2nd law says that the net force acting on an object equals the mass times the acceleration of theobject. We can define Newton?s 2nd law as F=m*a. Hence what can we determine about the acceleration of the packet given
that we know the net force acting on it is not zero and points straight down? [DAQ, Advance, D2]
STUDENT6: I don?t know [uncertain, incorrect]
ITSPOKE7: Well... < further discussion of Newton?s 2nd law...> So once again on the original question. Does the packet have anacceleration? If yes please specify its direction.[SAQ, PopUp, D1]
Figure 1: ITSPOKE Annotated Dialogue Excerpt
2.1 Uncertainty and Correctness Annotations
ITSPOKE, like most computer tutors, responds only
to student correctness. ITSPOKE labels each an-
swer as correct or incorrect1 . If correct, ITSPOKE
moves on to the next question. If incorrect, then for
questions on simple topics, ITSPOKE gives the cor-
rect answer and moves on, while for questions on
complex topics (ITSPOKE4, Figure 1), ITSPOKEinitiates a sub-dialogue with remediation questions
(ITSPOKE5 - ITSPOKE6), before moving on.
Recent computer tutoring research has shown in-
terest in responding to student affect2 over cor-
rectness. Uncertainty is of particular interest: re-
searchers hypothesize that uncertainty and incorrect-
ness each create an opportunity to learn (VanLehn
et al, 2003). They cannot be equated, however.
First, an uncertain answer may be correct or incor-
rect (Pon-Barry et al, 2006). Second, uncertainty in-
dicates that the student perceives a possible miscon-
ception in their knowledge. Thus, system responses
to uncertain answers can address both the correct-
ness and the perceived misconception.
In our ITSPOKE corpora, each student answer
has been manually annotated as uncertain or non-
uncertain3 : uncertain is used to label answers ex-
pressing uncertainty or confusion about the material;
non-uncertain is used to label all other answers.
1We have also manually labeled correctness in our data;
agreement between ITSPOKE and human is 0.79 Kappa (90%).
2We use ?affect? to cover emotions and attitudes that affect
how students communicate. Although some argue ?emotion?
and ?attitude? should be distinguished, some speech researchers
find the narrow sense of ?emotion? too restrictive because it ex-
cludes states where emotion is present but not full-blown, in-
cluding arousal and attitude (Cowie and Cornelius, 2003).
3A second annotator relabeled our dataset, yielding inter-
annotator agreement of 0.73 Kappa (92%).
2.2 Context Annotations
Here we examine 3 automatically monitorable tutor
question properties as our contexts for uncertainty:
Tutor Question Acts: In prior work one annotator
labeled 4 Tutor Question Acts in one ITSPOKE cor-
pus (Litman and Forbes-Riley, 2006)4: Short (SAQ),
Long (LAQ), and Deep Answer Question (DAQ) dis-
tinguish the question in terms of content and the type
of answer it requires. Repeat (RPT) labels variants
of ?Can you repeat that?? after rejections. From
these annotations we built a hash table associating
each ITSPOKE question with a Question Act label;
with this table we automatically labeled ITSPOKE
questions in our second ITSPOKE corpus.
Discourse Structure Depth/Transition: In prior
work we showed that the discourse structure Depth
and Transition for each ITSPOKE turn can be au-
tomatically annotated (Rotaru and Litman, 2006).
E.g., as shown in Figure 1, ITSPOKE4,7 have depth1 and ITSPOKE5,6 have depth 2. We combine lev-els 3 and above (3+) due to data sparsity. 6 Transi-
tion labels represent the turn?s position relative to the
prior ITSPOKE turn: NewTopLevel labels the first
question after an essay. Advance labels questions at
the same depth as the prior question (ITSPOKE4,6).
Push labels the first question in a sub-dialogue
(after an incorrect answer) (ITSPOKE5). After asub-dialogue, ITSPOKE asks the original question
again, labeled PopUp (ITSPOKE7), or moves on tothe next question, labeled PopUpAdv. SameGoal la-
bels both ITSPOKE RPTS (after rejections) and re-
peated questions after timeouts.
4Our Acts are based on related work (Graesser et al, 1995).
Two annotators labeled the Acts in 8 dialogues in a parallel hu-
man tutoring corpus, with agreement of 0.75 Kappa (90%).
42
3 Uncertainty Context Dependencies
We use the ?2 test to investigate the context depen-
dency of uncertain (unc) or non-uncertain (nonunc)
student answers that are correct (C) or incorrect (I).
First, we compute an overall ?2 value between each
context variable and the student answer variable. For
example, the Question Act variable (QACT) has 4
values: SAQ, LAQ, DAQ, RPT. The answer vari-
able (SANSWER) also has 4 values: uncC, uncI,
nonuncC, nonuncI. Table 1 (last column) shows the
?2 value between these variables is 203.38, which
greatly exceeds the critical value of 16.92 (p? 0.05,
df=9), indicating a highly significant dependency.
Significance increases as the ?2 value increases.
Dependency Obs. Exp. ?2
QACT ? SANSWER 203.38
LAQ ? uncC + 72 22 133.98
LAQ ? uncI + 43 27 11.17
LAQ ? nonuncC - 96 151 50.13
LAQ ? nonuncI = 48 60 3.10
DAQ ? uncC = 22 22 0.01
DAQ ? uncI + 37 27 4.57
DAQ ? nonuncC = 135 149 3.53
DAQ ? nonuncI = 63 59 0.35
SAQ ? uncC - 285 328 41.95
SAQ ? uncI - 377 408 17.10
SAQ ? nonuncC + 2368 2271 66.77
SAQ ? nonuncI - 875 898 5.31
RPT ? uncC - 7 14 4.15
RPT ? uncI = 22 18 1.25
RPT ? nonuncC - 70 98 20.18
RPT ? nonuncI + 70 39 33.59
Table 1: Tutor Question Act Dependencies (p?.05:
critical ?2=16.92 (df=9); critical ?2=3.84 (df=1))
However, this does not tell us which variable val-
ues are significantly dependent. To do this, we create
a binary variable from each value of the context and
answer variables. E.g., the binary variable for LAQ
has 2 values: ?LAQ? and ?Anything Else?, and the
binary variable for uncC has 2 values: ?uncC? and
?Anything Else?. We then compute the ?2 value be-
tween the binary variables. Table 1 shows this value
is 133.98, which greatly exceeds the critical value of
3.84 (p? 0.05, df=1). The table also shows the ob-
served (72) and expected (22) counts. Comparison
determines the sign of the dependency: uncC occurs
significantly more than expected (+) after LAQ. The
?=? sign indicates a non-significant dependency.
Table 1 shows uncertain answers (uncC and uncI)
occur significantly more than expected after LAQs.
In contrast, non-uncertain answers occur signifi-
cantly less (-), or aren?t significantly dependent (=).
Also, uncI occurs significantly more than expected
after DAQs. We hypothesize that LAQs and DAQs
are associated with more uncertainty because they
are harder questions requiring definitions or deep
reasoning. Not surprisingly, uncertain (and incor-
rect) answers occur significantly less than expected
after SAQs (easier fill-in-the-blank questions). Un-
certainty shows very weak dependencies on RPTs.
Table 2 shows that Depth1 is associated with more
correctness and less uncertainty overall. Both types
of correct answer occur significantly more than ex-
pected, but this dependency is stronger for nonuncC.
Both incorrect answers occur significantly less than
expected, but this dependency is stronger for uncI.
Dependency Obs. Exp. ?2
Depth# ? SANSWER 53.85
Depth1 ? uncC + 250 228 5.46
Depth1 ? uncI - 230 283 27.55
Depth1 ? nonuncC + 1661 1579 24.73
Depth1 ? nonuncI - 575 625 12.66
Depth2 ? uncC - 78 101 7.80
Depth2 ? uncI + 156 125 11.26
Depth2 ? nonuncC - 664 699 5.65
Depth2 ? nonuncI + 304 277 4.80
Depth3+ ? uncC = 58 57 0.05
Depth3+ ? uncI + 93 70 9.76
Depth3+ ? nonuncC - 344 391 15.66
Depth3+ ? nonuncI + 177 155 4.94
Table 2: Depth Dependencies (p?.05: critical
?2=12.59 (df=6); critical ?2=3.84 (df=1))
At Depths 2 and 3+, correct answers occur sig-
nificantly less than expected or show no signifi-
cance. Incorrect answers occur significantly more
than expected, and the dependencies are stronger for
uncI. We hypothesize that deeper depths are asso-
ciated with increased uncertainty and incorrectness
because they correspond to deeper knowledge gaps;
uncertainty here may also relate to a perceived lack
of cohesion between sub-topic and larger solution.
Table 3 shows Pushes have the same dependen-
cies as deeper depths (increased uncertainty and in-
correctness); however, here the uncI dependency is
only slightly stronger than nonuncI, which suggests
that increased uncertainty at deeper depths is more
reliably associated with remediation questions after
the Push. Although uncertainty shows only weak
43
dependencies on PopUps, after PopUpAdvs the uncI
dependency is strong, with uncI occurring more than
expected. We hypothesize that this dependency re-
lates to students losing track of the original ques-
tion/larger topic. Uncertainty shows only weak de-
pendencies on Advances. After NewTopLevels, in-
correct answers occur less than expected, but the de-
pendency is stronger for nonuncI. After SameGoals,
incorrect answers occur more than expected, but the
dependency is stronger for nonuncI. Compared with
the RPT results, the SameGoal results suggest stu-
dents feel increased uncertainty after timeouts.
Dependency Obs. Exp. ?2
TRANS ? SANSWER 190.97
Push ? uncC = 68 57 2.89
Push ? uncI + 100 70 16.37
Push ? nonuncC - 313 392 44.51
Push ? nonuncI + 193 155 14.13
PopUp ? uncC - 23 36 5.89
PopUp ? uncI - 32 45 4.68
PopUp ? nonuncC = 260 251 0.81
PopUp ? nonuncI + 117 99 4.47
PopUpAdv ? uncC = 8 13 2.50
PopUpAdv ? uncI + 32 17 16.22
PopUpAdv ? nonuncC - 76 93 7.72
PopUpAdv ? nonuncI = 44 37 1.89
Advance ? uncC = 217 205 1.70
Advance ? uncI - 223 254 9.06
Advance ? nonuncC + 1465 1416 8.66
Advance ? nonuncI - 530 560 4.51
NewTopLevel ? uncC = 53 54 0.04
NewTopLevel ? uncI - 49 67 6.47
NewTopLevel ? nonuncC + 463 375 57.33
NewTopLevel ? nonuncI - 80 148 47.63
SameGoal ? uncC = 17 21 0.70
SameGoal ? uncI + 43 25 14.24
SameGoal ? nonuncC - 92 152 44.25
SameGoal ? nonuncI + 92 56 31.43
Table 3: Transition Dependencies (p?.05: critical
?2=25.00 (df=15); critical ?2=3.84 (df=1))
4 Current Directions
We analyzed dependencies between uncertain stu-
dent answers and 3 automatically monitorable con-
texts. We plan to examine more contexts, such as
a Topic Repetition variable that tracks similar ques-
tions about a topic (e.g. gravity) across dialogues.
Our next step will be to use the significant de-
pendencies to develop system responses to uncer-
tain answers in these contexts. These responses will
be based both on our hypotheses about why uncer-
tainty is significantly associated with these contexts,
as well as on analyses of human tutor responses
in these contexts, using our human tutoring corpus,
which was collected with our first ITSPOKE corpus
using the same experimental procedure.
We also plan to investigate context dependencies
for other affective states, such as student frustration.
Acknowledgments
NSF (#0631930, #0354420 and #0328431) and
ONR (N00014-04-1-0108) support this research.
References
R. Cowie and R. R. Cornelius. 2003. Describing the
emotional states that are expressed in speech. Speech
Communication, 40:5?32.
A. Graesser, N. Person, and J. Magliano. 1995. Collabo-
rative dialog patterns in naturalistic one-on-one tutor-ing. Applied Cognitive Psychology, 9:495?522.
J. Klein, Y. Moon, and R. Picard. 2002. This computer
responds to user frustration: Theory, design, and re-sults. Interacting with Computers, 14:119?140.
C. M. Lee and S. Narayanan. 2005. Towards detect-
ing emotions in spoken dialogs. IEEE Transactions
on Speech and Audio Processing, 13(2), March.
D. Litman and K. Forbes-Riley. 2004. Predicting student
emotions in computer-human tutoring dialogues. In
Proc. ACL, pages 352?359.
D. J. Litman and K. Forbes-Riley. 2006. Correlations
between dialogue acts and learning in spoken tutoringdialogues. Natural Language Engineering, 12(2).
K. Liu and R. W. Picard. 2005. Embedded empathy
in continuous, interactive health assessment. In CHI
Workshop on HCI Challenges in Health Assessment.
H. Pon-Barry, K. Schultz, E. Bratt, B. Clark, and S. Pe-
ters. 2006. Responding to student uncertainty in spo-ken tutorial dialogue systems. International Journal
of Artificial Intelligence in Education, 16:171?194.
M. Rotaru and D. Litman. 2006. Exploiting discoursestructure for spoken dialogue performance analysis. In
Proceedings of EMNLP, Sydney, Australia.
K. VanLehn, P. W. Jordan, and C. P. Rose? et al 2002. Thearchitecture of Why2-Atlas: A coach for qualitativephysics essay writing. In Proceedings of ITS.
K. VanLehn, S. Siler, and C. Murray. 2003. Why doonly some events cause learning during human tutor-ing? Cognition and Instruction, 21(3):209?249.
44
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 865?872
Manchester, August 2008
The Ups and Downs of Preposition Error Detection in ESL Writing
Joel R. Tetreault
Educational Testing Service
660 Rosedale Road
Princeton, NJ, USA
JTetreault@ets.org
Martin Chodorow
Hunter College of CUNY
695 Park Avenue
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
In this paper we describe a methodology
for detecting preposition errors in the writ-
ing of non-native English speakers. Our
system performs at 84% precision and
close to 19% recall on a large set of stu-
dent essays. In addition, we address the
problem of annotation and evaluation in
this domain by showing how current ap-
proaches of using only one rater can skew
system evaluation. We present a sampling
approach to circumvent some of the issues
that complicate evaluation of error detec-
tion systems.
1 Introduction
The long-term goal of our work is to develop a
system which detects errors in grammar and us-
age so that appropriate feedback can be given to
non-native English writers, a large and growing
segment of the world?s population. Estimates are
that in China alone as many as 300 million peo-
ple are currently studying English as a second lan-
guage (ESL). Usage errors involving prepositions
are among the most common types seen in the
writing of non-native English speakers. For ex-
ample, (Izumi et al, 2003) reported error rates for
English prepositions that were as high as 10% in
a Japanese learner corpus. Errors can involve in-
correct selection (?we arrived to the station?), ex-
traneous use (?he went to outside?), and omission
(?we are fond null beer?). What is responsible
for making preposition usage so difficult for non-
native speakers?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
At least part of the difficulty seems to be due to
the great variety of linguistic functions that prepo-
sitions serve. When a preposition marks the ar-
gument of a predicate, such as a verb, an ad-
jective, or a noun, preposition selection is con-
strained by the argument role that it marks, the
noun which fills that role, and the particular predi-
cate. Many English verbs also display alternations
(Levin, 1993) in which an argument is sometimes
marked by a preposition and sometimes not (e.g.,
?They loaded the wagon with hay? / ?They loaded
hay on the wagon?). When prepositions introduce
adjuncts, such as those of time or manner, selec-
tion is constrained by the object of the preposition
(?at length?, ?in time?, ?with haste?). Finally, the
selection of a preposition for a given context also
depends upon the intended meaning of the writer
(?we sat at the beach?, ?on the beach?, ?near the
beach?, ?by the beach?).
With so many sources of variation in English
preposition usage, we wondered if the task of se-
lecting a preposition for a given context might
prove challenging even for native speakers. To
investigate this possibility, we randomly selected
200 sentences from Microsoft?s Encarta Encyclo-
pedia, and, in each sentence, we replaced a ran-
domly selected preposition with a blank line. We
then asked two native English speakers to perform
a cloze task by filling in the blank with the best
preposition, given the context provided by the rest
of the sentence. Our results showed only about
75% agreement between the two raters, and be-
tween each of our raters and Encarta.
The presence of so much variability in prepo-
sition function and usage makes the task of the
learner a daunting one. It also poses special chal-
lenges for developing and evaluating an NLP error
detection system. This paper addresses both the
865
development and evaluation of such a system.
First, we describe a machine learning system
that detects preposition errors in essays of ESL
writers. To date there have been relatively few
attempts to address preposition error detection,
though the sister task of detecting determiner er-
rors has been the focus of more research. Our sys-
tem performs comparably with other leading sys-
tems. We extend our previous work (Chodorow et
al., 2007) by experimenting with combination fea-
tures, as well as features derived from the Google
N-Gram corpus and Comlex (Grishman et al,
1994).
Second, we discuss drawbacks in current meth-
ods of annotating ESL data and evaluating error
detection systems, which are not limited to prepo-
sition errors. While the need for annotation by
multiple raters has been well established in NLP
tasks (Carletta, 1996), most previous work in error
detection has surprisingly relied on only one rater
to either create an annotated corpus of learner er-
rors, or to check the system?s output. Some gram-
matical errors, such as number disagreement be-
tween subject and verb, no doubt show very high
reliability, but others, such as usage errors involv-
ing prepositions or determiners are likely to be
much less reliable. Our results show that relying
on one rater for system evaluation can be problem-
atic, and we provide a sampling approach which
can facilitate using multiple raters for this task.
In the next section, we describe a system that
automatically detects errors involving incorrect
preposition selection (?We arrived to the station?)
and extraneous preposition usage (?He went to
outside?). In sections 3 and 4, we discuss the
problem of relying on only one rater for exhaus-
tive annotation and show how multiple raters can
be used more efficiently with a sampling approach.
Finally, in section 5 we present an analysis of com-
mon preposition errors that non-native speakers
make.
2 System
2.1 Model
We have used a Maximum Entropy (ME) classi-
fier (Ratnaparkhi, 1998) to build a model of correct
preposition usage for 34 common English prepo-
sitions. The classifier was trained on 7 million
preposition contexts extracted from parts of the
MetaMetrics Lexile corpus that contain textbooks
and other materials for high school students. Each
context was represented by 25 features consisting
of the words and part-of-speech (POS) tags found
in a local window of +/- two positions around the
preposition, plus the head verb of the preceding
verb phrase (PV), the head noun of the preceding
noun phrase (PN), and the head noun of the follow-
ing noun phrase (FH), among others. In analyz-
ing the contexts, we used only tagging and heuris-
tic phrase-chunking, rather than parsing, so as to
avoid problems that a parser might encounter with
ill-formed non-native text
1
. In test mode, the clas-
sifier was given the context in which a preposition
occurred, and it returned a probability for each of
the 34 prepositions.
2.2 Other Components
While the ME classifier constitutes the core of the
system, it is only one of several processing com-
ponents that refines or blocks the system?s output.
Since the goal of an error detection system is to
provide diagnostic feedback to a student, typically
a system?s output is heavily constrained so that it
minimizes false positives (i.e., the system tries to
avoid saying a writer?s preposition is used incor-
rectly when it is actually right), and thus does not
mislead the writer.
Pre-Processing Filter: A pre-processing pro-
gram skips over preposition contexts that contain
spelling errors. Classifier performance is poor in
such cases because the classifier was trained on
well-edited text, i.e., without misspelled words. In
the context of a diagnostic feedback and assess-
ment tool for writers, a spell checker would first
highlight the spelling errors and ask the writer to
correct them before the system analyzed the prepo-
sitions.
Post-Processing Filter: After the ME clas-
sifier has output a probability for each of the 34
prepositions but before the system has made its fi-
nal decision, a series of rule-based post-processing
filters block what would otherwise be false posi-
tives that occur in specific contexts. The first filter
prevents the classifier from marking as an error a
case where the classifier?s most probable preposi-
tion is an antonym of what the writer wrote, such
as ?with/without? and ?from/to?. In these cases,
resolution is dependent on the intent of the writer
and thus is outside the scope of information cap-
1
For an example of a common ungrammatical sentence
from our corpus, consider: ?In consion, for some reasons,
museums, particuraly known travel place, get on many peo-
ple.?
866
tured by the current feature set. Another problem
for the classifier involves differentiating between
certain adjuncts and arguments. For example, in
the sentence ?They described a part for a kid?, the
system?s top choices were of and to. The benefac-
tive adjunct introduced by for is difficult for the
classifier to learn, perhaps because it so freely oc-
curs in many locations within a sentence. A post-
processing filter prevents the system from marking
as an error a prepositional phrase that begins with
for and has an object headed by a human noun (a
WordNet hyponym of person or group).
Extraneous Use Filter: To cover extraneous
use errors, we developed two rule-based filters:
1) Plural Quantifier Constructions, to handle cases
such as ?some of people? and 2) Repeated Prepo-
sitions, where the writer accidentally repeated the
same preposition two or more times, such as ?can
find friends with with?. We found that extrane-
ous use errors usually constituted up to 18% of all
preposition errors, and our extraneous use filters
handle a quarter of that 18%.
Thresholding: The final step for the preposi-
tion error detection system is a set of thresholds
that allows the system to skip cases that are likely
to result in false positives. One of these is where
the top-ranked preposition and the writer?s prepo-
sition differ by less than a pre-specified amount.
This was also meant to avoid flagging cases where
the system?s preposition has a score only slightly
higher than the writer?s preposition score, such as:
?My sister usually gets home around 3:00? (writer:
around = 0.49, system: by = 0.51). In these cases,
the system?s and the writer?s prepositions both fit
the context, and it would be inappropriate to claim
the writer?s preposition was used incorrectly. An-
other system threshold requires that the probabil-
ity of the writer?s preposition be lower than a pre-
specified value in order for it to be flagged as an
error. The thresholds were set so as to strongly fa-
vor precision over recall due to the high number of
false positives that may arise if there is no thresh-
olding. This is a tactic also used for determiner
selection in (Nagata et al, 2006) and (Han et al,
2006). Both thresholds were empirically set on a
development corpus.
2.3 Combination Features
ME is an attractive choice of machine learning al-
gorithm for a problem as complex as preposition
error detection, in no small part because of the
availability of ME implementations that can han-
dle many millions of training events and features.
However, one disadvantage of ME is that it does
not automatically model the interactions among
features as some other approaches do, such as sup-
port vector machines (Jurafsky and Martin, 2008).
To overcome this, we have experimented with aug-
menting our original feature set with ?combination
features? which represent richer contextual struc-
ture in the form of syntactic patterns.
Table 1 (first column) illustrates the four com-
bination features used for the example context
?take our place in the line?. The p denotes a
preposition, so N-p-N denotes a syntactic context
where the preposition is preceded and followed
by a noun phrase. We use the preceding noun
phrase (PN) and following head (FH) from the
original feature set for the N-p-N feature. Column
3 shows one instantiation of combination features:
Combo:word. For the N-p-N feature, the cor-
responding Combo:word instantiation is ?place-
line? since ?place? is the PN and ?line? is the
FH. We also experimented with using combina-
tions of POS tags (Combo:tag) and word+tag com-
binations (Combo:word+tag). So for the example,
the Combo:tag N-p-N feature would be ?NN-NN?,
and the Combo:word+tag N-p-N feature would be
place NN+line NN (see the fourth column of Ta-
ble 1). The intuition with the Combo:tag features
is that the Combo:word features have the potential
to be sparse, and these capture more general pat-
terns of usage.
We also experimented with other features such
as augmenting the model with verb-preposition
preferences derived from Comlex (Grishman et al,
1994), and querying the Google Terabyte N-gram
corpus with the same patterns used in the combina-
tion features. The Comlex-based features did not
improve the model, and though the Google N-gram
corpus represents much more information than our
7 million event model, its inclusion improved per-
formance only marginally.
2.4 Evaluation
In our initial evaluation of the system we col-
lected a corpus of 8,269 preposition contexts,
error-annotated by two raters using the scheme de-
scribed in Section 3 to serve as a gold standard. In
this study, we focus on two of the three types of
preposition errors: using the incorrect preposition
and using an extraneous preposition. We compared
867
Class Components Combo:word Features Combo:tag Features
p-N FH line NN
N-p-N PN-FH place-line NN-NN
V-p-N PV-PN take-line VB-NN
V-N-p-N PV-PN-FH take-place-line VB-NN-NN
Table 1: Feature Examples for take our place in the line
different models: the baseline model of 25 features
and baseline with combination features added. The
precision and recall for the top performing mod-
els are shown in Table 2. These results do not in-
clude the extraneous use filter; this filter generally
increased precision by as much as 2% and recall
by as much as 5%.
Evaluation Metrics In the tasks of determiner
and preposition selection in well-formed, native
texts (such as (Knight and Chander, 1994), (Min-
nen et al, 2000), (Turner and Charniak, 2007) and
(Gamon et al, 2008)), the evaluation metric most
commonly used is accuracy. In these tasks, one
compares the system?s output on a determiner or
preposition to the gold standard of what the writer
originally wrote. However, in the tasks of deter-
miner and preposition error detection, precision
and recall are better metrics to use because one
is only concerned with a subset of the preposi-
tions (or determiners), those used incorrectly, as
opposed to all of them in the selection task. In
essence, accuracy has the problem of distorting
system performance.
Results The baseline system (described in
(Chodorow et al, 2007)) performed at 79.8% pre-
cision and 11.7% recall. Next we tested the differ-
ent combination models: word, tag, word+tag, and
all three. Surprisingly, three of the four combina-
tion models: tag, word+tag, all, did not improve
performance of the system when added to the
model, but using just the +Combo:word features
improved recall by 1%. We use the +Combo:word
model to test our sampling approach in section 4.
As a final test, we tuned our training corpus of
7 million events by removing any contexts with
unknown or misspelled words, and then retrained
the model. This ?purge? resulted in a removal
of nearly 200,000 training events. With this new
training corpus, the +Combo:tag feature showed
the biggest improvement over the baseline, with
an improvement in both precision (+2.3%) and re-
call (+2.4%) to 82.1% and 14.1% respectively (last
line of Table 2. While this improvement may seem
small, it is in part due to the difficulty of the prob-
lem, but also the high baseline system score that
was established in our prior work (Chodorow et
al., 2007).
It should be noted that with the inclusion
of the extraneous use filter, performance of the
+Combo:tag rose to 84% precision and close to
19% recall.
Model Precision Recall
Baseline 79.8% 11.7%
+Combo:word 79.8% 12.8%
+Combo:tag (with purge) 82.1% 14.1%
Table 2: Best System Results on Incorrect Selec-
tion Task
2.5 Related Work
Currently there are only a handful of approaches
that tackle the problem of preposition error detec-
tion in English learner texts. (Gamon et al, 2008)
used a language model and decision trees to de-
tect preposition and determiner errors in the CLEC
corpus of learner essays. Their system performs at
79% precision (which is on par with our system),
however recall figures are not presented thus mak-
ing comparison difficult. In addition, their eval-
uation differs from ours in that they also include
errors of omission, and their work focuses on the
top twelve most frequent prepositions, while ours
has greater coverage with the top 34. (Izumi et
al., 2003) and (Izumi et al, 2004) used an ME ap-
proach to classify different grammatical errors in
transcripts of Japanese interviews. They do not
present performance of prepositions specifically,
but overall performance for the 13 error types
they target reached 25% precision and 7% recall.
(Eeg-Olofsson and Knuttson, 2003) created a rule-
based approach to detecting preposition errors in
Swedish language learners (unlike the approaches
presented here, which focus on English language
learners), and their system performed at 25% ac-
curacy. (Lee and Seneff, 2006) used a language
model to tackle the novel problem of preposition
selection in a dialogue corpus. While their perfor-
mance results are quite high, 88% precision and
868
78% recall, it should be noted that their evaluation
was on a small corpus with a highly constrained
domain, and focused on a limited number of prepo-
sitions, thus making direct comparison with our
approach difficult.
Although our recall figures may seem low, es-
pecially when compared to other NLP tasks such
as parsing and anaphora resolution, this is really a
reflection of how difficult the task is. For example,
in the problem of preposition selection in native
text, a baseline using the most frequent preposition
(of) results in precision and recall of 26%. In addi-
tion, the cloze tests presented earlier indicate that
even in well-formed text, agreement between na-
tive speakers on preposition selection is only 75%.
In texts written by non-native speakers, rater dis-
agreement increases, as will be shown in the next
section.
3 Experiments with Multiple Raters
While developing an error detection system for
prepositions is certainly challenging, given the re-
sults from our work and others, evaluation also
poses a major challenge. To date, single human
annotation has typically been the gold standard for
grammatical error detection, such as in the work
of (Izumi et al, 2004), (Han et al, 2006), (Nagata
et al, 2006), (Eeg-Olofsson and Knuttson, 2003)
2
.
Another method for evaluation is verification ((Ga-
mon et al, 2008), where a human rater checks over
a system?s output. The drawbacks of this approach
are: 1. every time the system is changed, a rater
is needed to re-check the output, and 2. it is very
hard to estimate recall. What these two evaluation
methods have in common is that they side-step the
issue of annotator reliability.
In this section, we show how relying on only one
rater can be problematic for difficult error detec-
tion tasks, and in section 4, we propose a method
(?the sampling approach?) for efficiently evaluat-
ing a system that does not require the amount of
effort needed in the standard approach to annota-
tion.
3.1 Annotation
To create a gold-standard corpus of error anno-
tations for system evaluation, and also to deter-
mine whether multiple raters are better than one,
2
(Eeg-Olofsson and Knuttson, 2003) had a small evalua-
tion on 40 preposition contexts and it is unclear whether mul-
tiple annotators were used.
we trained two native English speakers with prior
NLP annotation experience to annotate preposition
errors in ESL text. The training was very exten-
sive: both raters were trained on 2000 preposi-
tion contexts and the annotation manual was it-
eratively refined as necessary. To summarize the
procedure, the two raters were shown sentences
randomly selected from student essays with each
preposition highlighted in the sentence. They
marked each context (?2-word window around the
preposition, plus the commanding verb) for gram-
mar and spelling errors, and then judged whether
the writer used an incorrect preposition, a correct
preposition, or an extraneous preposition. Finally,
the raters suggested prepositions that would best
fit the context, even if there were no error (some
contexts can license multiple prepositions).
3.2 Reliability
Each rater judged approximately 18,000 prepo-
sitions contexts, with 18 sets of 100 contexts
judged by both raters for purposes of comput-
ing kappa. Despite the rigorous training regimen,
kappa ranged from 0.411 to 0.786, with an overall
combined value of 0.630. Of the prepositions that
Rater 1 judged to be errors, Rater 2 judged 30.2%
to be acceptable. Conversely, of the prepositions
Rater 2 judged to be erroneous, Rater 1 found
38.1% acceptable. The kappa of 0.630 shows the
difficulty of this task and also shows how two
highly trained raters can produce very different
judgments. Details on our annotation and human
judgment experiments can be found in (Tetreault
and Chodorow, 2008).
Variability in raters? judgments translates to
variability of system evaluation. For instance, in
our previous work (Chodorow et al, 2007), we
found that when our system?s output was com-
pared to judgments of two different raters, there
was a 10% difference in precision and a 5% differ-
ence in recall. These differences are problematic
when evaluating a system, as they highlight the po-
tential to substantially over- or under-estimate per-
formance.
4 Sampling Approach
The results from the previous section motivate the
need for a more refined evaluation. They sug-
gest that for certain error annotation tasks, such as
preposition usage, it may not be appropriate to use
only one rater and that if one uses multiple raters
869
for error annotation, there is the possibility of cre-
ating an adjudicated set, or at least calculating the
variability of the system?s performance. However,
annotation with multiple raters has its own disad-
vantages as it is much more expensive and time-
consuming. Even using one rater to produce a
sizeable evaluation corpus of preposition errors is
extremely costly. For example, if we assume that
500 prepositions can be annotated in 4 hours us-
ing our annotation scheme, and that the base rate
for preposition errors is 10%, then it would take at
least 80 hours for a rater to find and mark 1000 er-
rors. In this section, we propose a more efficient
annotation approach to circumvent this problem.
4.1 Methodology
Figure 1: Sampling Approach Example
The sampling procedure outlined here is in-
spired by the one described in (Chodorow and Lea-
cock, 2000) for the task of evaluating the usage of
nouns, verbs and adjectives. The central idea is
to skew the annotation corpus so that it contains a
greater proportion of errors.
Here are the steps in the procedure:
1. Process a test corpus of sentences so that each
preposition in the corpus is labeled ?OK? or
?Error? by the system.
2. Divide the processed corpus into two sub-
corpora, one consisting of the system?s ?OK?
prepositions and the other of the system?s
?Error? prepositions. For the hypothetical
data in Figure 1, the ?OK? sub-corpus con-
tains 90% of the prepositions, and the ?Error?
sub-corpus contains the remaining 10%.
3. Randomly sample cases from each sub-
corpus and combine the samples into an an-
notation set that is given to a ?blind? human
rater. We generally use a higher sampling
rate for the ?Error? sub-corpus because we
want to ?enrich? the annotation set with a
larger proportion of errors than is found in the
test corpus as a whole. In Figure 1, 75% of
the ?Error? sub-corpus is sampled while only
16% of the ?OK? sub-corpus is sampled.
4. For each case that the human rater judges to
be an error, check to see which sub-corpus it
came from. If it came from the ?OK? sub-
corpus, then the case is a Miss (an error that
the system failed to detect). If it came from
the ?Error? sub-corpus, then the case is a Hit
(an error that the system detected). If the rater
judges a case to be a correct usage and it came
from the ?Error? sub-corpus, then it is a False
Positive (FP).
5. Calculate the proportions of Hits and FPs in
the sample from the ?Error? sub-corpus. For
the hypothetical data in Figure 1, these val-
ues are 600/750 = 0.80 for Hits, and 150/750
= 0.20 for FPs. Calculate the proportion of
Misses in the sample from the ?OK? sub-
corpus. For the hypothetical data, this is
450/1500 = 0.30 for Misses.
6. The values computed in step 5 are conditional
proportions based on the sub-corpora. To cal-
culate the overall proportions in the test cor-
pus, it is necessary to multiply each value
by the relative size of its sub-corpus. This
is shown in Table 3, where the proportion of
Hits in the ?Error? sub-corpus (0.80) is mul-
tiplied by the relative size of the ?Error? sub-
corpus (0.10) to produce an overall Hit rate
(0.08). Overall rates for FPs and Misses are
calculated in a similar manner.
7. Using the values from step 6, calculate Preci-
sion (Hits/(Hits + FP)) and Recall (Hits/(Hits
+ Misses)). These are shown in the last two
rows of Table 3.
Estimated Overall Rates
Sample Proportion * Sub-Corpus Proportion
Hits 0.80 * 0.10 = 0.08
FP 0.20 * 0.10 = 0.02
Misses 0.30 * 0.90 = 0.27
Precision 0.08/(0.08 + 0.02) = 0.80
Recall 0.08/(0.08 + 0.27) = 0.23
Table 3: Sampling Calculations (Hypothetical)
870
This method is similar in spirit to active learning
((Dagan and Engelson, 1995) and (Engelson and
Dagan, 1996)), which has been used to iteratively
build up an annotated corpus, but it differs from
active learning applications in that there are no it-
erative loops between the system and the human
annotator(s). In addition, while our methodology
is used for evaluating a system, active learning is
commonly used for training a system.
4.2 Application
Next, we tested whether our proposed sam-
pling approach provides good estimates of a sys-
tem?s performance. For this task, we used the
+Combo:word model to separate a large corpus
of student essays into the ?Error? and ?OK? sub-
corpora. The original corpus totaled over 22,000
prepositions which would normally take several
weeks for two raters to double annotate and then
adjudicate. After the two sub-corpora were propor-
tionally sampled, this resulted in an annotation set
of 752 preposition contexts (requiring roughly 6
hours for annotation), which is substantially more
manageable than the full corpus. We had both
raters work together to make judgments for each
preposition.
It is important to note that while these are not
the exact same essays used in the previous evalua-
tion of 8,269 preposition contexts, they come from
the same pool of student essays and were on the
same topics. Given these strong similarities, we
feel that one can compare scores between the two
approaches. The precision and recall scores for
both approaches are shown in Table 4 and are ex-
tremely similar, thus suggesting that the sampling
approach can be used as an alternative to exhaus-
tive annotation.
Precision Recall
Standard Approach 80% 12%
Sampling Approach 79% 14%
Table 4: Sampling Results
It is important with the sampling approach to use
appropriate sample sizes when drawing from the
sub-corpora, because the accuracy of the estimates
of hits and misses will depend upon the propor-
tion of errors in each sub-corpus as well as on the
sample sizes. The OK sub-corpus is expected to
have even fewer errors than the overall base rate,
so it is especially important to have a relatively
large sample from this sub-corpus. The compari-
son study described above used an OK sub-corpus
sample that was twice as large as the Error sub-
corpus sample (about 500 contexts vs. 250 con-
texts).
In short, the sampling approach is intended to
alleviate the burden on annotators when faced with
the task of having to rate several thousand errors
of a particular type in order to produce a sizeable
error corpus. On the other hand, one advantage
that exhaustive annotation has over the sampling
method is that it makes possible the comparison
of multiple systems. With the sampling approach,
one would have to resample and annotate for each
system, thus multiplying the work needed.
5 Analysis of Learner Errors
One aspect of automatic error detection that usu-
ally is under-reported is an analysis of the errors
that learners typically make. The obvious benefit
of this analysis is that it can focus development of
the system.
From our annotated set of preposition errors,
we found that the most common prepositions
that learners used incorrectly were in (21.4%), to
(20.8%) and of (16.6%). The top ten prepositions
accounted for 93.8% of all preposition errors in our
learner corpus.
Next, we ranked the common preposition ?con-
fusions?, the common mistakes made for each
preposition. The top ten most common confusions
are listed in Table 5, where null refers to cases
where no preposition is licensed (the writer used
an extraneous preposition). The most common of-
fenses were actually extraneous errors (see Table
5): using to and of when no preposition was li-
censed accounted for 16.8% of all errors.
It is interesting to note that the most common
usage errors by learners overwhelmingly involved
the ten most frequently occurring prepositions in
native text. This suggests that our effort to handle
the 34 most frequently occurring prepositions may
be overextended and that a system that is specif-
ically trained and refined on the top ten preposi-
tions may provide better diagnostic feedback to a
learner.
6 Conclusions
This paper has two contributions to the field of
error detection in non-native writing. First, we
discussed a system that detects preposition errors
with high precison (up to 84%) and is competitive
871
Writer?s Prep. Rater?s Prep. Frequency
to null 9.5%
of null 7.3%
in at 7.1%
to for 4.6%
in null 3.2%
of for 3.1%
in on 3.1%
of in 2.9%
at in 2.7%
for to 2.5%
Table 5: Common Preposition Confusions
with other leading methods. We used an ME
approach augmented with combination features
and a series of thresholds. This system is currently
incorporated in the Criterion writing evaluation
service. Second, we showed that the standard ap-
proach to evaluating NLP error detection systems
(comparing a system?s output with a gold-standard
annotation) can greatly skew system results when
the annotation is done by only one rater. However,
one reason why a single rater is commonly used
is that building a corpus of learner errors can be
extremely costly and time consuming. To address
this efficiency issue, we presented a sampling
approach that produces results comparable to
exhaustive annotation. This makes using multiple
raters possible since less time is required to
assess the system?s performance. While the work
presented here has focused on prepositions, the
arguments against using only one rater, and for
using a sampling approach generalize to other
error types, such as determiners and collocations.
Acknowledgements We would first like to
thank our two annotators Sarah Ohls and Waverly
VanWinkle for their hours of hard work. We would
also like to acknowledge the three anonymous re-
viewers and Derrick Higgins for their helpful com-
ments and feedback.
References
Carletta, J. 1996. Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational Lin-
guistics, pages 249?254.
Chodorow, M. and C. Leacock. 2000. An unsupervised
method for detecting grammatical errors. In NAACL.
Chodorow, M., J. Tetreault, and N-R. Han. 2007. De-
tection of grammatical errors involving prepositions.
In Proceedings of the Fourth ACL-SIGSEM Work-
shop on Prepositions.
Dagan, I. and S. Engelson. 1995. Committee-based
sampling for training probabilistic classifiers. In
Proceedings of ICML, pages 150?157.
Eeg-Olofsson, J. and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. In Nodalida.
Engelson, S. and I. Dagan. 1996. Minimizing manual
annotation cost in supervised training from corpora.
In Proceedings of ACL, pages 319?326.
Gamon, M., J. Gao, C. Brockett, A. Klementiev, W. B.
Dolan, D. Belenko, and L. Vanderwende. 2008. Us-
ing contextual speller techniques and language mod-
eling for esl error correction. In IJCNLP.
Grishman, R., C. Macleod, and A. Meyers. 1994.
Comlex syntax: Building a computational lexicon.
In COLING.
Han, N-R., M. Chodorow, and C. Leacock. 2006. De-
tecting errors in english article usage by non-native
speakers. Natural Language Engineering, 12:115?
129.
Izumi, E., K. Uchimoto, T. Saiga, T. Supnithi, and
H. Isahara. 2003. Automatic error detection in the
Japanese leaners? English spoken data. In ACL.
Izumi, E., K. Uchimoto, and H. Isahara. 2004. The
overview of the sst speech corpus of Japanese learner
English and evaluation through the experiment on
automatic detection of learners? errors. In LREC.
Jurafsky, D. and J. Martin. 2008. Speech and Language
Processing (2nd Edition). Prentice Hall. To Appear.
Knight, K. and I. Chander. 1994. Automated postedit-
ing of documents. In Conference on Artificial Intel-
ligence.
Lee, J. and S. Seneff. 2006. Automatic grammar cor-
rection for second-language learners. In Interspeech.
Levin, B. 1993. English verb classes and alternations:
a preliminary investigation. Univ. of Chicago Press.
Minnen, G., F. Bond, and A. Copestake. 2000.
Memory-based learning for article generation. In
CoNLL.
Nagata, R., A. Kawai, K. Morihiro, and N. Isu. 2006.
A feedback-augmented method for detecting errors
in the writing of learners of English. In Proceedings
of the ACL/COLING.
Ratnaparkhi, A. 1998. Maximum Entropy Models for
natural language ambiguity resolution. Ph.D. thesis,
University of Pennsylvania.
Tetreault, J. and M. Chodorow. 2008. Native Judg-
ments of non-native usage: Experiments in preposi-
tion error detection. In COLING Workshop on Hu-
man Judgments in Computational Linguistics.
Turner, J. and E. Charniak. 2007. Language modeling
for determiner selection. In HLT/NAACL.
872
Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 25?30,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Detection of Grammatical Errors Involving Prepositions
Martin Chodorow
Hunter College of CUNY
695 Park Avenue
New York, NY, 10021
mchodoro@hunter.cuny.edu
Joel R. Tetreault and Na-Rae Han
Educational Testing Services
Rosedale Road
Princeton, NJ, 08541
jtetreault|nzhan@ets.org
Abstract
This paper presents ongoing work on the de-
tection of preposition errors of non-native
speakers of English. Since prepositions
account for a substantial proportion of all
grammatical errors by ESL (English as a
Second Language) learners, developing an
NLP application that can reliably detect
these types of errors will provide an invalu-
able learning resource to ESL students. To
address this problem, we use a maximum
entropy classifier combined with rule-based
filters to detect preposition errors in a corpus
of student essays. Although our work is pre-
liminary, we achieve a precision of 0.8 with
a recall of 0.3.
1 Introduction
The National Clearinghouse for English Language
Acquisition (2002) estimates that 9.6% of the stu-
dents in the US public school population speak a
language other than English and have limited En-
glish proficiency. Clearly, there is a substantial and
increasing need for tools for instruction in English
as a Second Language (ESL).
In particular, preposition usage is one of the most
difficult aspects of English grammar for non-native
speakers to master. Preposition errors account for
a significant proportion of all ESL grammar errors.
They represented the largest category, about 29%,
of all the errors by 53 intermediate to advanced ESL
students (Bitchener et al, 2005), and 18% of all er-
rors reported in an intensive analysis of one Japanese
writer (Murata and Ishara, 2004). Preposition errors
are not only prominent among error types, they are
also quite frequent in ESL writing. Dalgish (1985)
analyzed the essays of 350 ESL college students
representing 15 different native languages and re-
ported that preposition errors were present in 18%
of sentences in a sample of text produced by writ-
ers from first languages as diverse as Korean, Greek,
and Spanish.
The goal of the research described here is to pro-
vide software for detecting common grammar and
usage errors in the English writing of non-native En-
glish speakers. Our work targets errors involving
prepositions, specifically those of incorrect preposi-
tion selection, such as arrive to the town, and those
of extraneous prepositions, as in most of people.
We present an approach that combines machine
learning with rule-based filters to detect preposition
errors in a corpus of ESL essays. Even though this
is work in progress, we achieve precision of 0.8 with
a recall of 0.3. The paper is structured as follows: in
the next section, we describe the difficulty in learn-
ing English preposition usage; in Section 3, we dis-
cuss related work; in Sections 4-7 we discuss our
methodology and evaluation.
2 Problem of Preposition Usage
Why are prepositions so difficult to master? Perhaps
it is because they perform so many complex roles. In
English, prepositions appear in adjuncts, they mark
the arguments of predicates, and they combine with
other parts of speech to express new meanings.
The choice of preposition in an adjunct is largely
constrained by its object (in the summer, on Friday,
25
at noon) and the intended meaning (at the beach,
on the beach, near the beach, by the beach). Since
adjuncts are optional and tend to be flexible in their
position in a sentence, the task facing the learner is
quite complex.
Prepositions are also used to mark the arguments
of a predicate. Usually, the predicate is expressed
by a verb, but sometimes it takes the form of an ad-
jective (He was fond of beer), a noun (They have
a thirst for knowledge), or a nominalization (The
child?s removal from the classroom). The choice of
the preposition as an argument marker depends on
the type of argument it marks, the word that fills the
argument role, the particular word used as the pred-
icate, and whether the predicate is a nominalization.
Even with these constraints, there are still variations
in the ways in which arguments can be expressed.
Levin (1993) catalogs verb alternations such as They
loaded hay on the wagon vs. They loaded the wagon
with hay, which show that, depending on the verb,
an argument may sometimes be marked by a prepo-
sition and sometimes not.
English has hundreds of phrasal verbs, consist-
ing of a verb and a particle (some of which are
also prepositions). To complicate matters, phrasal
verbs are often used with prepositions (i.e., give up
on someone; give in to their demands). Phrasal
verbs are particularly difficult for non-native speak-
ers to master because of their non-compositionality
of meaning, which forces the learner to commit them
to rote memory.
3 Related Work
If mastering English prepositions is a daunting task
for the second language learner, it is even more
so for a computer. To our knowledge, only three
other groups have attempted to automatically de-
tect errors in preposition usage. Eeg-Olofsson et al
(2003) used 31 handcrafted matching rules to detect
extraneous, omitted, and incorrect prepositions in
Swedish text written by native speakers of English,
Arabic, and Japanese. The rules, which were based
on the kinds of errors that were found in a training
set of text produced by non-native Swedish writers,
targeted spelling errors involving prepositions and
some particularly problematic Swedish verbs. In a
test of the system, 11 of 40 preposition errors were
correctly detected.
Izumi et al (2003) and (2004) used error-
annotated transcripts of Japanese speakers in an
interview-based test of spoken English to train a
maximum entropy classifier (Ratnaparkhi, 1998) to
recognize 13 different types of grammatical and lex-
ical errors, including errors involving prepositions.
The classifier relied on lexical and syntactic features.
Overall performance for the 13 error types reached
25.1% precision with 7.1% recall on an independent
test set of sentences from the same source, but the
researchers do not separately report the results for
preposition error detection. The approach taken by
Izumi and colleagues is most similar to the one we
have used, which is described in the next section.
More recently, (Lee and Seneff, 2006) used a
language model and stochastic grammar to replace
prepositions removed from a dialogue corpus. Even
though they reported a precision of 0.88 and recall
of 0.78, their evaluation was on a very restricted do-
main with only a limited number of prepositions,
nouns and verbs.
4 The Selection Model
A preposition error can be a case of incorrect prepo-
sition selection (They arrived to the town), use of a
preposition in a context where it is prohibited (They
came to inside), or failure to use a preposition in a
context where it is obligatory (e.g., He is fond this
book). To detect the first type of error, incorrect
selection, we have employed a maximum entropy
(ME) model to estimate the probability of each of
34 prepositions, based on the features in their lo-
cal contexts. The ME Principle says that the best
model will satisfy the constraints found in the train-
ing, and for those situations not covered in the train-
ing, the best model will assume a distribution of
maximum entropy. This approach has been shown
to perform well in combining heterogeneous forms
of evidence, as in word sense disambiguation (Rat-
naparkhi, 1998). It also has the desirable property of
handling interactions among features without having
to rely on the assumption of feature independence,
as in a Naive Bayesian model.
Our ME model was trained on 7 million ?events?
consisting of an outcome (the preposition that ap-
peared in the training text) and its associated con-
26
text (the set of feature-value pairs that accompa-
nied it). These 7 million prepositions and their con-
texts were extracted from the MetaMetrics corpus of
1100 and 1200 Lexile text (11th and 12th grade) and
newspaper text from the San Jose Mercury News.
The sentences were then POS-tagged (Ratnaparkhi,
1998) and then chunked into noun phrases and verb
phrases by a heuristic chunker.
The maximum entropy model was trained with
25 contextual features. Some of the features repre-
sented the words and tags found at specific locations
adjacent to the preposition; others represented the
head words and tags of phrases that preceded or fol-
lowed the preposition. Table 1 shows a subset of the
feature list.
Some features had only a few values while oth-
ers had many. PHR pre is the ?preceding phrase?
feature that indicates whether the preposition was
preceded by a noun phrase (NP) or a verb phrase
(VP). In the example in Table 2, the preposition
into is preceded by an NP. In a sentence that be-
gins After the crowd was whipped up into a frenzy
of anticipation, the preposition into is preceded by
a VP. There were only two feature#value pairs for
this feature: PHR pre#NP and PHR pre#VP. Other
features had hundreds or even thousands of differ-
ent values because they represented the occurrence
of specific words that preceded or followed a prepo-
sition. Any feature#value pairs which occurred with
very low frequency in the training (less than 10 times
in the 7 million contexts) were eliminated to avoid
the need for smoothing their probabilities. Lemma
forms of words were used as feature values to fur-
ther reduce the total number and to allow the model
to generalize across inflectional variants. Even after
incorporating these reductions, the number of val-
ues was still very large. As Table 1 indicates, TGR,
the word sequence including the preposition and the
two words to its right, had 54,906 different values.
Summing across all features, the model contained a
total of about 388,000 feature#value pairs. Table 2
shows an example of where some of the features are
derived from.
5 Evaluation on Grammatical Text
The model was tested on 18,157 preposition con-
texts extracted from 12 files randomly selected from
a portion of 1100 Lexile text (11th grade) that had
not been used for training. For each context, the
model predicted the probability of each preposi-
tion given the contextual representation. The high-
est probability preposition was then compared to
the preposition that had actually been used by the
writer. Because the test corpus consisted of pub-
lished, edited text, we assumed that this material
contained few, if any, errors. In this and subsequent
tests, the model was used to classify each context as
one of 34 classes (prepositions).
Results of the comparison between the classifier
and the test set showed that the overall proportion
of agreement between the text and the classifier was
0.69. The value of kappa was 0.64. When we ex-
amined the errors, we discovered that, frequently,
the classifier?s most probable preposition (the one
it assigned) differed from the second most probable
by just a few percentage points. This corresponded
to a situation in which two or more prepositions
were likely to be found in a given context. Con-
sider the context They thanked him for his consider-
ation this matter, where either of or in could fill
the blank. Because the classifier was forced to make
a choice in this and other close cases, it incurred a
high probability of making an error. To avoid this
situation, we re-ran the test allowing the classifier
to skip any preposition if its top ranked and sec-
ond ranked choices differed by less than a specified
amount. In other words, we permitted it to respond
only when it was confident of its decision. When
the difference between the first and second ranked
choices was 0.60 or greater, 50% of the cases re-
ceived no decision, but for the remaining half of the
test cases, the proportion of agreement was 0.90 and
kappa was 0.88. This suggests that a considerable
improvement in performance can be achieved by us-
ing a more conservative approach based on a higher
confidence level for the classifier.
6 Evaluation on ESL Essays
To evaluate the ME model?s suitability for analyzing
ungrammatical text, 2,000 preposition contexts were
extracted from randomly selected essays written on
ESL tests by native speakers of Chinese, Japanese,
and Russian. This set of materials was used to look
for problems that were likely to arise as a conse-
27
Feature Description No. of values with freq ? 10
BGL Bigram to left; includes preceding word and POS 23,620
BGR Bigram to right; includes following word and POS 20,495
FH Headword of the following phrase 19,718
FP Following phrase 40,778
PHR pre Preceding phrase type 2
PN Preceding noun 18,329
PNMod Adjective modifying preceding noun 3,267
PNP Preceding noun phrase 29,334
PPrep Preceding preposition 60
PV Preceding verb 5,221
PVP Preceding verb phrase 23,436
PVtag POS tag of the preceding verb 24
PVword Lemma of the preceding verb 5,221
PW Lemma of the preceding word 2,437
TGL Trigram to left; includes two preceding words and POS 44,446
TGR Trigram to right; includes two following words and POS 54,906
Table 1: Some features used in ME Model
After whipping the crowd up into a frenzy of anticipation...
PVword PN PW FH
BGL BGR
??TGL?? ??TGR??
Table 2: Locations of some features in the local context of a preposition
quence of the mismatch between the training cor-
pus (edited, grammatical text) and the testing corpus
(ESL essays with errors of various kinds). When the
model was used to classify prepositions in the ESL
essays, it became obvious, almost immediately, that
a number of new performance issues would have to
be addressed.
The student essays contained many misspelled
words. Because misspellings were not in the train-
ing, the model was unable to use the features associ-
ated with them (e.g., FHword#frinzy) in its decision
making. The tagger was also affected by spelling
errors, so to avoid these problems, the classifier
was allowed to skip any context that contained mis-
spelled words in positions adjacent to the preposi-
tion or in its adjacent phrasal heads. A second prob-
lem resulted from punctuation errors in the student
writing. This usually took the form of missing com-
mas, as in I disagree because from my point of view
there is no evidence. In the training corpus, commas
generally separated parenthetical expressions, such
as from my point of view, from the rest of the sen-
tence. Without the comma, the model selected of
as the most probable preposition following because,
instead of from. A set of heuristics was used to lo-
cate common sites of comma errors and skip these
contexts.
There were two other common sources of clas-
sification error: antonyms and benefactives. The
model very often confused prepositions with op-
posite meanings (like with/without and from/to), so
when the highest probability preposition was an
antonym of the one produced by the writer, we
blocked the classifier from marking the usage as an
error. Benefactive phrases of the form for + per-
son/organization (for everyone, for my school) were
also difficult for the model to learn, most likely be-
cause, as adjuncts, they are free to appear in many
different places in a sentence and the preposition is
not constrained by its object, resulting in their fre-
quency being divided among many different con-
texts. When a benefactive appeared in an argument
position, the model?s most probable preposition was
generally not the preposition for. In the sentence
They described a part for a kid, the preposition of
has a higher probability. The classifier was pre-
vented from marking for + person/organization as
a usage error in such contexts.
To summarize, the classifier consisted of the ME
model plus a program that blocked its application
28
Rater 1 vs. Classifier vs. Classifier vs.
Rater 2 Rater 1 Rater 2
Agreement 0.926 0.942 0.934
Kappa 0.599 0.365 0.291
Precision N/A 0.778 0.677
Recall N/A 0.259 0.205
Table 3: Classifer vs. Rater Statistics
in cases of misspelling, likely punctuation errors,
antonymous prepositions, and benefactives. An-
other difference between the training corpus and the
testing corpus was that the latter contained grammat-
ical errors. In the sentence, This was my first experi-
ence about choose friends, there is a verb error im-
mediately following the preposition. Arguably, the
preposition is also wrong since the sequence about
choose is ill-formed. When the classifier marked the
preposition as incorrect in an ungrammatical con-
text, it was credited with correctly detecting a prepo-
sition error.
Next, the classifier was tested on the set of 2,000
preposition contexts, with the confidence threshold
set at 0.9. Each preposition in these essays was
judged for correctness of usage by one or two human
raters. The judged rate of occurrence of preposition
errors was 0.109 for Rater 1 and 0.098 for Rater 2,
i.e., about 1 out of every 10 prepositions was judged
to be incorrect. The overall proportion of agreement
between Rater1 and Rater 2 was 0.926, and kappa
was 0.599.
Table 3 (second column) shows the results for the
Classifier vs. Rater 1, using Rater 1 as the gold stan-
dard. Note that this is not a blind test of the clas-
sifier inasmuch as the classifier?s confidence thresh-
old was adjusted to maximize performance on this
set. The overall proportion of agreement was 0.942,
but kappa was only 0.365 due to the high level of
agreement expected by chance, as the Classifier used
the response category of ?correct? more than 97%
of the time. We found similar results when com-
paring the judgements of the Classifier to Rater 2:
agreement was high and kappa was low. In addition,
for both raters, precision was much higher than re-
call. As noted earlier, the table does not include the
cases that the classifier skipped due to misspelling,
antonymous prepositions, and benefactives.
Both precision and recall are low in these com-
parisons to the human raters. We are particularly
concerned about precision because the feedback that
students receive from an automated writing analy-
sis system should, above all, avoid false positives,
i.e., marking correct usage as incorrect. We tried to
improve precision by adding to the system a naive
Bayesian classifier that uses the same features found
in Table 1. As expected, its performance is not as
good as the ME model (e.g., precision = 0.57 and
recall = 0.29 compared to Rater 1 as the gold stan-
dard), but when the Bayesian classifier was given a
veto over the decision of the ME classifier, overall
precision did increase substantially (to 0.88), though
with a reduction in recall (to 0.16). To address the
problem of low recall, we have targeted another type
of ESL preposition error: extraneous prepositions.
7 Prepositions in Prohibited Contexts
Our strategy of training the ME classifier on gram-
matical, edited text precluded detection of extrane-
ous prepositions as these did not appear in the train-
ing corpus. Of the 500-600 errors in the ESL test set,
142 were errors of this type. To identify extraneous
preposition errors we devised two rule-based filters
which were based on analysis of the development
set. Both used POS tags and chunking information.
Plural Quantifier Constructions This filter ad-
dresses the second most common extraneous prepo-
sition error where the writer has added a preposi-
tion in the middle of a plural quantifier construction,
for example: some of people. This filter works by
checking if the target word is preceded by a quanti-
fier (such as ?some?, ?few?, or ?three?), and if the
head noun of the quantifier phrase is plural. Then, if
there is no determiner in the phrase, the target word
is deemed an extraneous preposition error.
Repeated Prepositions These are cases such as
people can find friends with with the same interests
where a preposition occurs twice in a row. Repeated
prepositions were easily screened by checking if the
same lexical item and POS tag were used for both
words.
These filters address two types of extraneous
preposition errors, but there are many other types
(for example, subcategorization errors, or errors
with prepositions inserted incorrectly in the begin-
ning of a sentence initial phrase). Even though these
filters cover just one quarter of the 142 extraneous
29
errors, they did improve precision from 0.778 to
0.796, and recall from 0.259 to 0.304 (comparing
to Rater 1).
8 Conclusions and Future Work
We have presented a combined machine learning
and rule-based approach that detects preposition er-
rors in ESL essays with precision of 0.80 or higher
(0.796 with the ME classifier and Extraneous Prepo-
sition filters; and 0.88 with the combined ME and
Bayesian classifiers). Our work is novel in that we
are the first to report specific performance results for
a preposition error detector trained and evaluated on
general corpora.
While the training for the ME classifier was done
on a separate corpus, and it was this classifier that
contributed the most to the high precision, it should
be noted that some of the filters were tuned on the
evaluation corpus. Currently, we are in the course
of annotating additional ESL essays for preposition
errors in order to obtain a larger-sized test set.
While most NLP systems are a balancing act be-
tween precision and recall, the domain of designing
grammatical error detection systems is distinguished
in its emphasis on high precision over high recall.
Essentially, a false positive, i.e., an instance of an er-
ror detection system informing a student that a usage
is incorrect when in fact it is indeed correct, must be
reduced at the expense of a few genuine errors slip-
ping through the system undetected. Given this, we
chose to set the threshold for the system so that it en-
sures high precision which in turn resulted in a recall
figure (0.3) that leaves us much room for improve-
ment. Our plans for future system development in-
clude:
1. Using more training data. Even a cursory ex-
amination of the training corpus reveals that there
are many gaps in the data. Seven million seems
like a large number of examples, but the selection
of prepositions is highly dependent on the presence
of other specific words in the context. Many fairly
common combinations of Verb+Preposition+Noun
or Noun+Preposition+Noun are simply not attested,
even in a sizable corpus. Consistent with this, there
is a strong correlation between the relative frequency
of a preposition and the classifier?s ability to predict
its occurrence in edited text. That is, prediction is
better for prepositions that have many examples in
the training set and worse for those with fewer ex-
amples. This suggests the need for much more data.
2. Combining classifiers. Our plan is to use the
output of the Bayesian model as an input feature for
the ME classifier. We also intend to use other classi-
fiers and let them vote.
3. Using semantic information. The ME
model in this study contains no semantic informa-
tion. One way to extend and improve its cover-
age might be to include features of verbs and their
noun arguments from sources such as FrameNet
(http://framenet.icsi.berkeley.edu/), which detail the
semantics of the frames in which many English
words appear.
References
J. Bitchener, S. Young, and D. Cameron. 2005. The ef-
fect of different types of corrective feedback on esl stu-
dent writing. Journal of Second Language Writing.
G. Dalgish. 1985. Computer-assisted esl research and
courseware development. Computers and Composi-
tion.
J. Eeg-Olofsson and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. In Nodalida.
National Center for Educational Statistics. 2002. Public
school student counts, staff, and graduate counts by
state: School year 2000-2001.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the japanese
leaners? english spoken data. In ACL.
E. Izumi, K. Uchimoto, and H. Isahara. 2004. The
overview of the sst speech corpus of japanese learner
english and evaluation through the experiment on au-
tomatic detection of learners? errors. In LREC.
J. Lee and S. Seneff. 2006. Automatic grammar correc-
tion for second-language learners. In Interspeech.
B. Levin. 1993. English verb classes and alternations: a
preliminary investigation. Univ. of Chicago Press.
M. Murata and H. Ishara. 2004. Three english learner
assistance systems using automatic paraphrasing tech-
niques. In PACLIC 18.
A. Ratnaparkhi. 1998. Maximum Entropy Models for
natural language ambiguity resolution. Ph.D. thesis,
University of Pennsylvania.
30
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 60?63,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Human Evaluation of Article and Noun Number Usage:
Influences of Context and Construction Variability
John Lee
Spoken Language Systems
MIT CSAIL
Cambridge, MA 02139, USA
jsylee@csail.mit.edu
Joel Tetreault
Educational Testing Service
Princeton, NJ 08541
jtetreault@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY 10021
martin.chodorow@
hunter.cuny.edu
Abstract
Evaluating systems that correct errors in
non-native writing is difficult because of
the possibility of multiple correct answers
and the variability in human agreement.
This paper seeks to improve the best prac-
tice of such evaluation by analyzing the
frequency of multiple correct answers and
identifying factors that influence agree-
ment levels in judging the usage of articles
and noun number.
1 Introduction
In recent years, systems have been developed with
the long-term goal of detecting, in the writing of
non-native speakers, usage errors involving arti-
cles, prepositions and noun number (Knight and
Chander, 1994; Minnen et al, 2000; Lee, 2004;
Han et al, 2005; Peng and Araki, 2005; Brockett
et al, 2006; Turner and Charniak, 2007). These
systems should, ideally, be evaluated on a cor-
pus of learners? writing, annotated with accept-
able corrections. However, since such corpora are
expensive to compile, many researchers have in-
stead resorted to measuring the accuracy of pre-
dicting what a native writer originally wrote in
well-formed text. This type of evaluation effec-
tively makes the assumption that there is one cor-
rect form of native usage per context, which may
not always be the case.
Two studies have already challenged this ?sin-
gle correct construction? assumption by compar-
ing the output of a system to the original text.
In (Tetreault and Chodorow, 2008), two human
judges were presented with 200 sentences and, for
each sentence, they were asked to select which
preposition (either the writer?s preposition, or the
system?s) better fits the context. In 28% of the
cases where the writer and the system differed, the
human raters found the system?s prediction to be
equal to or better than the writer?s original prepo-
sition. (Lee and Seneff, 2006) found similar re-
sults on the sentence level in a task that evaluated
many different parts of speech.
Percentage Article Number Example
42.5% null singular stone
22.7% the singular the stone
17.6% null plural stones
11.4% a/an singular a stone
5.7% the plural the stones
Table 1: Distribution of the five article-number
constructions of head nouns, based on 8 million
examples extracted from the MetaMetrics Lexile
Corpus. The various constructions are illustrated
with the noun ?stone?.
2 Research Questions
It is clear that using what the author wrote as
the gold standard can underestimate the sys-
tem?s performance, and that multiple correct an-
swers should be annotated. Using this annotation
scheme, however, raises two questions that have
not yet been thoroughly researched: (1) what is the
human agreement level on such annotation? (2)
what factors might influence the agreement level?
In this paper, we consider two factors: the context
of a word, and the variability of its usage.
In the two studies cited above, the human judges
were shown only the target sentence and did not
take into account any constraint on the choice of
word that might be imposed by the larger con-
text. For PP attachment, human performance im-
proves when given more context (Ratnaparkhi et
al., 1994). For other linguistic phenomena, such
as article/number selection for nouns, a larger con-
text window of at least several sentences may be
required, even though some automatic methods for
exploiting context have not been shown to boost
performance (Han et al, 2005).
The second factor, variability of usage, may be
60
Three years ago John Small, a sheep farmer in the Mendip
Hills, read an editorial in his local newspaper which claimed
that foxes never killed lambs. He drove down to the pa-
per?s office and presented [?], killed the night before,
to the editor.
NO-CONTEXT IN-CONTEXT
lamb: no no
a lamb: yes yes*
the lamb: yes no
lambs: yes yes
the lambs: yes no
Table 2: An example of a completed annotation
item.
expressed as the entropy of the distribution of the
word?s constructions. Table 1 shows the over-
all distribution of five article/number constructions
for head nouns, i.e. all permissible combinations
of number (singular or plural), and article (?a/an?,
?the?, or the ?null article?). A high entropy noun
such as ?stone? can appear freely in all of these, ei-
ther as a count noun or a non-count noun. This
contrasts with a low entropy noun such as ?pollu-
tion? which is mostly limited to two construction
types (?pollution? and ?the pollution?).
In this paper, we analyze the effects of varying
context and noun entropy on human judgments of
the acceptability of article-number constructions.
As a result of this study, we hope to advance the
best practice in annotation for evaluating error de-
tection systems. ?3 describes our annotation task.
In ?4, we test the ?single correct construction? as-
sumption for article and noun number. In ?5, we
investigate to what extent context and entropy con-
strain the range of acceptable constructions and in-
fluence the level of human agreement.
3 Annotation Design
3.1 Annotation Scheme
Two native speakers of English participated in
an annotation exercise, which took place in two
stages: NO-CONTEXT and IN-CONTEXT. Both
stages used a common set of sentences, each con-
taining one noun to be annotated. That noun was
replaced by the symbol [?], and the five possible
constructions, as listed in Table 1, were displayed
below the sentence to be judged.
In the NO-CONTEXT stage, only the sentence
in question and the five candidate constructions
(i.e., the bolded parts in Table 2) were shown to
the raters. They were asked to consider each of
the five constructions, and to select yes if it would
null a the
anaphoric not anaphoric
singular 2 2 2 2
plural 2 n/a 2 2
Table 3: For each noun, two sentences were se-
lected from each configuration of number, article
and anaphor.
yield a good sentence in some context, and no oth-
erwise1.
The IN-CONTEXT stage began after a few days?
break. The raters were presented with the same
sentences, but including the context, which con-
sisted of the five preceding sentences, some of
which are shown in Table 2. The raters were again
asked to select yes if the choice would yield a
good sentence given the context, and no other-
wise. Among the yes constructions, they were
asked to mark with an asterisk (yes*) the con-
struction(s) most likely to have been used in the
original text.
3.2 Annotation Example
In Table 2, ?lambs? are mentioned in the context,
but only in the generic sense. Therefore, the [?]
in the sentence must be indefinite, resulting in yes
for both ?a lamb? and ?lambs?. Of these two con-
structions, the singular was judged more likely to
have been the writer?s choice.
If the context is removed, then the [?] in the
sentence could be anaphoric, and so ?the lamb?
and ?the lambs? are also possible. Finally, regard-
less of context, the null singular ?lamb? is not ac-
ceptable.
3.3 Item Selection
All items were drawn from the Grade 10 material
in the 2.5M-sentence MetaMetrics Lexile corpus.
To avoid artificially inflating the agreement level,
we excluded noun phrases whose article or num-
ber can be predicted with very high confidence,
such as proper nouns, pronouns and non-count
nouns. Noun phrases with certain words, such as
non-article determiners (e.g., this car), possessive
pronouns (e.g., his car), cardinal numbers (e.g.,
one car) or quantifiers (e.g., some cars), also fall
into this category. Most of these preclude the arti-
cles a and the.
1Originally, a third response category was offered to the
rater to mark constructions that fell in a grey area between
yes and no. This category was merged with yes.
61
Rater NO-CONTEXT IN-CONTEXT
yes no yes no
R1 62.4% 37.6% 29.3% 70.7%
R2 51.8% 48.2% 39.2% 60.8%
Table 4: Breakdown of the annotations by rater
and by stage. See ?4 for a discussion.
Once these easy cases were filtered out, the head
nouns in the corpus were divided into five sets ac-
cording to their dominant construction. Each set
was then ranked according to the entropy of the
distribution of their constructions. Low entropy
typically means that there is one particular con-
struction whose frequency dwarfs the others?, such
as the singular definite for ?sun?. High entropy
means that the five constructions are more evenly
represented in the corpus; these are mostly generic
objects that can be definite or indefinite, singular
or plural, such as ?stone?. For each of the five
constructions, the three nouns with the highest en-
tropies, and three with the lowest, were selected.
This yielded a total of 15 ?high-entropy? and 15
?low-entropy? nouns.
For each noun, 14 sentences were drawn ac-
cording to the breakdown in Table 3, ensuring a
balanced representation of the article and num-
ber used in the original text, and the presence of
anaphoric references2. A total of 368 items3 were
generated.
4 Multiple Correct Constructions
We first establish the reliability of the annotation
by measuring agreement with the original text,
then show how and when multiple correct con-
structions can arise. All results in this section are
from the IN-CONTEXT stage.
Since the items were drawn from well-formed
text, each noun?s original construction should be
marked yes. The two raters assigned yes to the
original construction 80% and 95% of the time,
respectively. These can be viewed as the upper
bound of system performance if we assume there
can be only one correct construction. A stricter
ceiling can be obtained by considering how of-
ten the yes* constructions overlap with the orig-
2For practical reasons, we have restricted the study of con-
text to direct anaphoric references, i.e., where the same head
noun has already occurred in the context.
3In theory, there should be 420 items, but some of the con-
figurations in Table 3 are missing for certain nouns, mostly
the low-entropy ones.
NO-CONTEXT IN-CONTEXT
R1:? R2:? yes no yes no
yes 846 302 462 77
no 108 584 260 1041
Table 5: The confusion tables of the two raters for
the two stages.
inal one4. The yes* items overlapped with the
original 72% and 83% of the time, respectively.
These relatively high figures serve as evidence of
the quality of the annotation.
Both raters frequently found more than one
valid construction ? 18% of the time if only
considering yes*, and 49% if considering both
yes and yes*. The implication for auto-
matic system evaluation is that one could po-
tentially underestimate a system?s performance
by as much as 18%, if not more. For both
raters, the most frequent combinations of yes*
constructions were {null-plural,the-plural}, {a-
singular,the-singular}, {a-singular,null-plural},
and {the-singular,the-plural}. From the stand-
point of designing a grammar-checking system,
a system should be less confident in proposing
change from one construction to another within
the same construction pair.
5 Sources of Variation in Agreement
It is unavoidable for agreement levels to be af-
fected by how accepting or imaginative the in-
dividual raters are. In the NO-CONTEXT stage,
Rater 1 awarded more yes?s than Rater 2, per-
haps attributable to her ability to imagine suitable
contexts for some of the less likely constructions.
In the IN-CONTEXT stage, Rater 1 used yesmore
sparingly than Rater 2. This reflects their different
judgments on where to draw the line among con-
structions in the grey area between acceptable and
unacceptable.
We have identified, however, two other factors
that led to variations in the agreement level: the
amount of context available, and the distribution
of the noun itself in the English language. Careful
consideration of these factors should lead to better
agreement.
Availability of Context As shown in Table 4, for
both raters, the context sharply reduced the num-
ber of correct constructions. The confusion tables
4Both raters assigned yes* to an average of 1.2 construc-
tions per item.
62
for the two raters are shown in Table 5. For the
NO-CONTEXT stage, they agreed 78% of the time
and the kappa statistic was 0.55. When context is
provided, human judgment can be expected to in-
crease. Indeed, for the IN-CONTEXT stage, agree-
ment rose to 82% and kappa to 0.605.
Another kind of context ? previous mention
of the noun ? also increases agreement. Among
nouns originally constructed with ?the?, the kappa
statistics for those with direct anaphora was 0.63,
but only 0.52 for those without6.
Most previous research on article-number pre-
diction has only used features extracted from the
target sentence. These results suggest that using
features from a wider context should improve
performance.
Noun Construction Entropy For the low-entropy
nouns, we found a marked difference in human
agreement among the constructions depending on
their frequencies. For the most frequent construc-
tion in a noun?s distribution, the kappa was 0.78;
for the four remaining constructions, which are
much more rare, the kappa was only 0.527. They
probably constitute ?border-line? cases for which
the line between yes and no was often hard to
draw, leading to the lower kappa.
Entropy can thus serve as an additional factor
when a system decides whether or not to mark a
usage as an error. For low-entropy nouns, the sys-
tem should be more confident of predicting a fre-
quent construction, but more wary of suggesting
the other constructions.
6 Conclusions & Future Work
We conducted a human annotation exercise on ar-
ticle and noun number usage, making two obser-
vations that can help improve the evaluation pro-
cedure for this task. First, although the context
substantially reduces the range of acceptable an-
swers, there are still often multiple acceptable an-
swers given a context; second, the level of human
agreement is influenced by the availability of the
5This kappa value is on the boundary between ?moderate?
and ?substantial? agreement on the scale proposed in (Landis
and Koch, 1977). The difference between the kappa values
for the NO-CONTEXT and IN-CONTEXT stages approaches
statistical significance, z = 1.71, p < 0.10.
6The difference between these kappa values is statistically
significant, z = 2.06, p < 0.05.
7The two kappa values are significantly different, z =
4.35, p < 0.001.
context and the distribution of the noun?s construc-
tions.
These observations should help improve not
only the evaluation procedure but also the design
of error correction systems for articles and noun
number. Entropy, for example, can be incorpo-
rated into the estimation of a system?s confidence
in its prediction. More sophisticated contextual
features, beyond simply noting that a noun has
been previously mentioned (Han et al, 2005; Lee,
2004), can also potentially reduce uncertainty and
improve system performance.
Acknowledgments
We thank the two annotators, Sarah Ohls and Wa-
verely VanWinkle.
References
C. Brockett, W. Dolan, and M. Gamon. 2006. Cor-
recting ESL Errors using Phrasal SMT Techniques.
Proc. ACL.
N.-R. Han, M. Chodorow, and C. Leacock. 2005.
Detecting Errors in English Article Usage by Non-
Native Speakers. Natural Language Engineering,
1(1):1?15.
K. Knight and I. Chander. 1994. Automated Postedit-
ing of Documents. Proc. AAAI.
J. R. Landis and G. G. Koch. 1977. The Measurement
of Observer Agreement for Categorical Data. Bio-
metrics 33:159?174.
J. Lee. 2004. Automatic Article Restoration. Proc.
HLT-NAACL Student Research Workshop.
J. Lee and S. Seneff. 2006. Automatic Grammar Cor-
rection for Second-Language Learners. Proc. Inter-
speech.
G. Minnen, F. Bond, and A. Copestake. 2000.
Memory-based Learning for Article Generation.
Proc. CoNLL/LLL.
J. Peng and K. Araki. 2005. Correction of Arti-
cle Errors in Machine Translation Using Web-based
Model. Proc. IEEE NLP-KE.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A
Maximum Entropy Model for Prepositional Phrase
Attachment. Proc. ARPA Workshop on Human Lan-
guage Technology.
J. Tetreault and M. Chodorow. 2008. Native Judg-
ments of Non-Native Usage. Proc. COLING Work-
shop on Human Judgements in Computational Lin-
guistics.
J. Turner and E. Charniak. 2007. Language Modeling
for Determiner Selection. Proc. HLT-NAACL.
63
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts,
pages 8?10, Dublin, Ireland, August 23-29 2014.
Automated Grammatical Error Correction for Language Learners
Joel Tetreault
Yahoo Labs
111 W. 40th Street
New York, NY, 10018, USA
tetreaul@yahoo-inc.com
Claudia Leacock
McGraw-Hill Education CTB
22 Ryan Ranch Road
Monterey, CA, 93940
claudia.leacock@ctb.com
Tutorial Description
A fast growing area in Natural Language Processing is the use of automated tools for identifying and
correcting grammatical errors made by language learners. This growth, in part, has been fueled by
the needs of a large number of people in the world who are learning and using a second or foreign
language. For example, it is estimated that there are currently over one billion people who are non-native
writers of English. These numbers drive the demand for accurate tools that can help learners to write
and speak proficiently in another language. Such demand also makes this an exciting time for those in
the NLP community who are developing automated methods for grammatical error correction (GEC).
Our motivation for the COLING tutorial is to make others more aware of this field and its particular set
of challenges. For these reasons, we believe that the tutorial will potentially benefit a broad range of
conference attendees.
In general, there has been a surge in interest in using NLP to address educational needs, which in turn,
has spawned the recurring ACL/NAACL workshop ?Innovative Use of Natural Language Processing
for Building Educational Applications? that had its 9th edition at ACL 2014. The last three years, in
particular, have been pivotal for GEC. Papers on the topic have become more commonplace at main
conferences such as ACL, NAACL and EMNLP, as well as two editions of a Morgan Claypool Synthesis
Series book on the topic (Leacock et al., 2010; Leacock et al., 2014). In 2011 and 2012, the first shared
tasks in GEC (Dale and Kilgarriff, 2011; Dale et al., 2012) were created, and dozens of teams from all
over the world participated. This was followed by two successful CoNLL Shared Tasks on the topic in
2013 and 2014 (Ng et al., 2013; Ng et al., 2014).
While there have been many exciting developments in GEC over the last few years, there is still
considerable room for improvement as state-of-the-art performance in detecting and correcting several
important error types is still inadequate for real world applications. We hope to engage researchers from
other NLP fields to develop novel and effective approaches to these problems. Our tutorial is specifically
designed to:
? Introduce an NLP audience to the challenges that language learners face and thus the challenges of
designing NLP tools to assist in language acquisition
? Provide a history of GEC and the state-of-the-art approaches for different error types
? Show the need for multi-lingual error correction approaches and discuss novel methods for achiev-
ing this
? Discuss ways in which error correction techniques can have an impact on other NLP tasks
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
8
Outline
1. Introduction
2. Special Problems of Language Learners
? Errors made by English Language Learners (ELLs)
? Influence of L1
3. Heuristic and Data Driven Approaches to Error Correction
(a) Early heuristic rule-based methods
(b) Methods for detection and correction
(c) Types of training data
(d) Features
(e) Web-based methods
4. Annotation and Evaluation
(a) Annotation schemes
(b) Proposals for efficient annotation
(c) Evaluation Measures
(d) Crowdsourcing for annotation and evaluation
5. Current Trends in Error Correction
(a) Detection of ungrammatical sentences and Other error types
(b) Shared tasks
(c) Going beyond the classification methodology
(d) Error correction in other languages
6. Conclusions
Organizers
Joel Tetreault is a Senior Research Scientist at Yahoo Labs in New York City. His research focus is
Natural Language Processing with specific interests in anaphora, dialogue and discourse processing,
machine learning, and applying these techniques to the analysis of English language learning and au-
tomated essay scoring. Previously he was Principal Manager of the Core Natural Language group at
Nuance Communications, Inc. where he worked on the research and development of NLP tools and
components for the next generation of intelligent dialogue systems. Prior to Nuance, he worked at Ed-
ucational Testing Service for six years as a Managing Senior Research Scientist where he researched
automated methods for detecting grammatical errors by non-native speakers, plagiarism detection, and
content scoring. Tetreault received his B.A. in Computer Science from Harvard University (1998) and
his M.S. and Ph.D. in Computer Science from the University of Rochester (2004). He was also a post-
doctoral research scientist at the University of Pittsburgh?s Learning Research and Development Center
(2004-2007), where he worked on developing spoken dialogue tutoring systems. In addition he has co-
organized the Building Educational Application workshop series for 7 years, the CoNLL 2013 Shared
Task on Grammatical Error Correction, and is currently NAACL Treasurer.
Claudia Leacock is a Research Scientist at McGraw-Hill Education CTB who has been working on
using NLP in educational applications for 20 years focusing on automated scoring and grammatical er-
ror detection. She was previously a consultant for Microsoft Research where she collaborated on the
development of ESL Assistant: a web-based prototype tool for detecting and correcting grammatical er-
rors of English language learners. As a Distinguished Member of Technical Staff at Pearson Knowledge
9
Technologies, and previously as a Principal Development Scientist at Educational Testing Service, she
developed tools for automated assessment of short-response content-based questions and for grammati-
cal error detection. As a member of the WordNet group at Princeton University?s Cognitive Science Lab,
her research focused on word sense disambiguation. Dr. Leacock received a B.A. in English from NYU,
a Ph.D. in Linguistics from the City University of New York, Graduate Center and was a post-doctoral
fellow at IBM, T.J. Watson Research Center.
References
Robert Dale and Adam Kilgarriff. 2011. Helping Our Own: The HOO 2011 pilot shared task. In Proceedings
of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages
242?249, Nancy, France, September. Association for Computational Linguistics.
Robert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A report on the preposition and determiner
error correction shared task. In Proceedings of the Seventh Workshop on Building Educational Applications
Using NLP, pages 54?62, Montr?eal, Canada, June. Association for Computational Linguistics.
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreaukt. 2010. Automated grammatical error
detection for language learners. Morgan & Claypool Publishers.
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreaukt. 2014. Automated grammatical error
detection for language learners, second edition. Morgan & Claypool Publishers.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, and Joel Tetreault. 2013. The conll-2013 shared task on grammatical
error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,
Sofia, Bulgaria, August. Association for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher
Bryant. 2014. The conll-2014 shared task on grammatical error correction. In Proceedings of the Eighteenth
Conference on Computational Natural Language Learning: Shared Task, pages 1?14, Baltimore, Maryland,
June. Association for Computational Linguistics.
10
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1291?1300,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exploiting Syntactic and Distributional Information
for Spelling Correction with Web-Scale N-gram Models
Wei Xuc,?Joel Tetreaulta Martin Chodorowb Ralph Grishmanc Le Zhaod
aEducational Testing Service, Princeton, NJ, USA
jtetreault@ets.org
bHunter College of CUNY, New York, NY, USA
martin.chodorow@hunter.cuny.edu
cNew York University, NY, USA
{xuwei,grishman}@cs.nyu.edu
dCarnegie Mellon University, Pittsburgh, PA, USA
lezhao@cs.cmu.edu
Abstract
We propose a novel way of incorporating de-
pendency parse and word co-occurrence in-
formation into a state-of-the-art web-scale n-
gram model for spelling correction. The syn-
tactic and distributional information provides
extra evidence in addition to that provided by a
web-scale n-gram corpus and especially helps
with data sparsity problems. Experimental
results show that introducing syntactic fea-
tures into n-gram based models significantly
reduces errors by up to 12.4% over the current
state-of-the-art. The word co-occurrence in-
formation shows potential but only improves
overall accuracy slightly.
1 Introduction
The function of context-sensitive text correction is
to identify word-choice errors in text (Bergsma et
al., 2009). It can be viewed as a lexical disambigua-
tion task (Lapata and Keller, 2005), where a system
selects from a predefined confusion word set, such
as {affect, effect} or {complement, compliment},
and provides the most appropriate word choice given
the context. Typically, one determines if a word has
been used correctly based on lexical, syntactic and
semantic information from the context of the word.
One of the top performing models of spelling cor-
rection (Bergsma et al, 2010) is based on web-scale
n-gram counts, which reflect both syntax and mean-
ing. However, even with a large-scale n-gram cor-
pus, data sparsity can hurt performance in two ways.
?This work was done when the first author was an intern
for Educational Testing Service.
First, n-gram based methods require exact word and
order matches. If there is a low frequency word in
the context, such as a person?s name, there will be
little, if any, evidence in the n-gram data to sup-
port the usage. Second, if the target confusable word
is rare, there will not be enough n-gram support or
training data to render a confident decision. Because
of the data sparsity problem, language modeling is
not always sufficient to capture the meaning of the
sentence and the correct usage of the word.
Take a sentence from The New York Times
(NYT) for example: ??This fellow?s won a war,? the
dean of the capital?s press corps, David Broder, an-
nounced on ?Meet the Press? after complimenting
the president on the ?great sense of authority and
command? he exhibited in a flight suit.? Unfortu-
nately, neither the phrase ?complementing the pres-
ident? nor ?complimenting the president? exists in
the web-scale Google N-gram corpus (Brants and
Franz, 2006). The n-gram models decide solely
based on the frequency of the bi-grams ?after com-
ple(i)menting? and ?comple(i)menting the?, which
are common usages for both words. The real ques-
tion is whether we are more likely to ?compliment?
or ?complement? a person, the ?president?. Several
clues could help us answer that question. A de-
pendency parser can identify the word ?president?
as the subject of ?compliment? or ?complement?
which also may be the case in some of the train-
ing data. Lexical co-occurrence (Edmonds, 1997)
and semantic word relatedness measurements, such
as Random Indexing (Sahlgren, 2006), could pro-
vide evidence that ?compliment? is more likely to
co-occur with ?president? than ?complement?. Fur-
1291
thermore, some important clues can be quite distant
from the target word, e.g. outside the 9-word context
window Bergsma et al (2010) and Carlson (2007)
used. Consider another sentence in the NYT corpus,
?GM says the addition of OnStar, which includes a
system that automatically notifies an OnStar opera-
tor if the vehicle is involved in a collision, comple-
ments the Vue?s top five-star safety rating for the
driver and front passenger in both front- and side-
impact crash tests.? The dependency parser finds the
object of ?complement? is ?rating?, which is outside
the 9-word window.
We propose enhancing state-of-the-art web-scale
n-gram models for spelling correction with syntac-
tic structures and distributional information. For our
work, we build on a baseline system that combines
n-gram and lexical features (Bergsma et al, 2010).
Specifically, this paper makes the following contri-
butions:
1. We show that the baseline system can be
improved by augmenting it with dependency
parse features.
2. We show that the impact of parse features can
be further augmented when combined with dis-
tributional information, specifically word co-
occurrence information.
In the following section, we describe related
work and how our approach differs from these ap-
proaches. In Sections 3 and 4, we discuss our meth-
ods for using parse features and word co-occurrence
information. In Section 5, we present experimental
results and analysis.
2 Related Work
A variety of approaches have been proposed for
context-sensitive spelling correction ranging from
semantic methods to machine learning classifiers to
large-scale n-gram models.
Some semantics-based systems have been devel-
oped based on an intuitive assumption that the in-
tended word is more likely to be semantically coher-
ent with the context than is a spelling error. Jones
and Martin (1997) made use of the semantic simi-
larity produced by Latent Semantic Analysis. Bu-
danitsky and Hirst (2001) investigated the effective-
ness of predicting words based on different semantic
similarity/distance measures in WordNet. Both sys-
tems report performance that is lower than systems
developed more recently.
A variety of machine-learning methods have been
proposed in spelling correction and preposition and
article error correction fields, such as Bayesian clas-
sifiers (Golding, 1995; Golding and Roth, 1996),
Winnow-based learning (Golding and Roth, 1999),
decision lists (Golding, 1995), transformation-based
learning (Mangu and Brill, 1997), augmented mix-
ture models (Cucerzan and Yarowsky, 2002) and
maximum entropy classifiers (Izumi et al, 2003;
Han et al, 2006; Chodorow et al, 2007; Tetreault
and Chodorow, 2008; Felice and Pulman, 2008).
Despite their differences, these approaches mainly
use contextual features to capture the lexical, seman-
tic and/or syntactic environment of the target word.
The use of distributional similarity measures for
spelling correction has been previously explored in
(Mohammad and Hist, 2006). In our work, distribu-
tional similarity is not the primary contribution but
we show the impact it can have when used in con-
junction with a large scale n-gram model and with
parse features, which allows the system to select
words outside the local window for distributional
similarity. In the prior work, the words for distri-
butional similarity are constrained to the local win-
dow, and positional information of the words is not
encoded.
Recent work (Carlson and Fette, 2007; Gamon
et al, 2008; Bergsma et al, 2009) has demon-
strated that large-scale language modeling is ex-
tremely helpful for contextual spelling correction
and other lexical disambiguation tasks. These sys-
tems make the word choice depending on how fre-
quently each candidate word has been seen in the
given context in web-scale data. As n-gram data has
become more readily available, such as the Google
N-gram Corpus, the likelihood of a word being used
in a certain context can be better estimated.
Bergsma et al (2009; 2010) presented a series
of simple but powerful models which relied heavily
on web-scale n-gram counts. From the Google Web
N-gram Corpus, they retrieve counts of n-grams of
different sizes (2-5) and positions that span the tar-
get word w0 within a window of 9 words. For
example, for the following sentence: ?The system
tried to decide {among, between} the two confus-
1292
able words.?, the method would extract the five 5-
gram patterns, shown below in Figure 2, where w0
can be either word in the confusion set {among, be-
tween} in this particular example. Similarly, there
are four 4-grams, three 3-grams, and two 2-grams,
in total, 14 n-grams for each of the words in the con-
fusion set.
system tried to decide w0
tried to decide w0 the
to decide w0 the two
decide w0 the two confusable
w0 the two confusable words
We briefly describe three of Bergsma et al?s
(2009; 2010) best systems below, which are reported
to achieve state-of-the-art accuracy (NG = n-gram;
LEX = lexical).
1. sumLM: For each candidate word, (Bergsma
et al, 2009) sum the log-counts of all 14 pat-
terns filled with the candidate, and choose the
candidate with the highest total.
2. NG: Bergsma et al (2009) exploit each can-
didate?s 14 log-counts of n-gram patterns as
features in a Support Vector Machine (SVM)
model.
3. NG+LEX: Bergsma et al (2010) augment the
NG model with lexical features (described in
detail in Section 3.1).
Bergsma et al (2009; 2010) restricted their exper-
iments to only five confusion sets where the reported
performance in (Golding and Roth, 1999) was below
90%: {among, between}, {amount, number}, {cite,
sight, site}, {peace, piece} and {raise, rise}. They
reported that the SVM model with NG features out-
performed its unsupervised version, sumLM. How-
ever, the limited confusion word sets they evaluated
may not comprehensively represent the word usage
errors that writers typically make. In this paper, we
test nine additional commonly confused word pairs
to expand the scope of the evaluation. These words
were selected based on their lower frequencies com-
pared to the five pairs in the above work (as shown
later in Table 2).
3 Enhanced N-gram Models with Parse
Features
To our knowledge, only (Elmi and Evans, 1998)
have used parsing for spell correction. They focus
on using a parser as a filter to discriminate between
possible real-world corrections where the part-of-
speech differs. In our work, we show that parse fea-
tures are effective when used directly in the classifi-
cation mode (as opposed to as a final filter) to select
the best correction regardless of whether or not the
part-of-speech of the choices differ.
Statistical parsers have also seen limited use in
the sister tasks of preposition and article error detec-
tion (Hermet et al, 2008; Lee and Knutsson, 2008;
Felice and Pulman, 2009; Tetreault et al, 2010)
and verb sense disambiguation (Dligach and Palmer,
2008). In those instances where parsers have been
used, they have mainly provided shallow analyses
or relations involving specific target words, such as
a preposition or verb. Unlike preposition errors,
spelling errors can occur in any word.
In this paper, we propose a novel way to incor-
porate the parse into spelling correction, applying
the parser to sentences filled by each candidate word
equivalently and extracting salient features. This
overcomes two problem in the existing methods: 1)
the parse trees of the same sentence filled by differ-
ent confusion words can be different. However, in
the test phase, we do not know which word should
be put in the sentences to create parse features for
test examples. Previous studies (Tetreault et al,
2010) failed to discuss this issue. 2) Some existing
work (Whitelaw et al, 2009; Rozovskaya and Roth,
2010) in the text correction field introduced artificial
errors into training data to adapt the system to bet-
ter handle ill-formed text. But this method will en-
counter serious data sparsity problems when facing
rare words.
3.1 Baseline System
We chose one of the leading spelling correction sys-
tems, (Bergsma et al, 2010), as our primary base-
line. As noted earlier, it is an SVM-based system
combining web-scale n-gram counts (NG) and con-
textual words (LEX) as features. To simplify the ex-
planation, throughout the paper, we will only con-
sider the situation with two confusion words. The
1293
problem with more than two words in pre-defined
confusion sets can be solved similarly by using a
one-vs.-all strategy. As we mentioned in Section 2,
NG features include log-counts of 3-to-5-gram pat-
terns for each candidate word with the given context.
LEX features can be broken down into three sub-
categories: 1) bag-of-words (words at all positions
in a 9-word window around the target word), 2) in-
dicators for the words preceding or following the tar-
get word, and 3) indicators for all n-grams and their
positions. For the sentence ?The system tried to de-
cide {among, between} the two confusable words.?,
examples of bag-of-word features would be ?tried?,
?two?, etc., the two positional bigrams would be
?decide? and ?the?, and examples of the n-gram fea-
tures would be right-trigram = ?among the two? and
left-4-gram = ?tried to decide between?.
3.2 Parse Features
The benefit of introducing dependency parse fea-
tures is that 1) parse features capture contextual in-
formation in a larger context window; 2) parse fea-
tures specify which words in the context are salient
to the usage of the target word while purely lexi-
cally based approaches treat all words in the context
equally. We use the Stanford dependency parser (de
Marneffe et al, 2006) to extract six relevant feature
classes.
Parse Features (PAR):
1. relation names (target word as head)
2. complement of the target word
3. combination of 1 and 2
4. relation names (target word as complement)
5. head of the target word
6. combination of 4 and 5
Each of these six classes of PAR features can
contain zero to many values, since the target word
can be involved in none to multiple grammatical
relations and features of different filler words are
merged together. The PAR features, like the LEX
features, are binary. In Table 1, we present the parse
features for an example sentence. The parse fea-
tures here are listed as string values, but are later
converted into binary numbers in the vectors for the
SVM model.
4 Distributional Word Co-occurrence
Though lexical and parse features are complemen-
tary to n-gram models, they are learned from a nor-
mal training corpus and may not have enough cov-
erage due to data sparsity. Take a sentence from the
NYT for example: ?An economist, he began his ca-
reer as a professor ? he is still called ?the professor,?
by friends as a compliment and by foes as an insult ?
and taught at Harvard and Stanford .? If the most in-
dicative word ?friends? does not appear or does not
appear enough times in the local context or depen-
dencies with ?compliment? as compared to ?com-
plement? in the training corpus, then the classifier
may be unable to make the correct selection.
It is impractical and computationally costly to en-
large the training corpus without limit to include
all possible language phenomena. A good compro-
mise is to use word co-occurrence information from
web-scale data. The other option is to make use of
high-order word co-occurrence, which is included in
many semantic word relatedness measures, such as
Latent Semantic Analysis (LSA) (Landauer et al,
1998; Deerwester et al, 1990) or Random Indexing,
both of which can be estimated from a moderate-size
corpus.
Our intuition is to choose the confusion word
which is most relevant to a given context. We define
the salient words in context as a set M=m1, m2, m3,
..., and the relevance between two words as a func-
tion Relevance(w1, w2), which can either be calcu-
lated fromword co-occurrence or Random Indexing.
The score of each candidate word c in the confusion
set given a context with meaningful words M is cal-
culated by the following formula:
Score(c) =
?
m?M
Relevance(c,m)
In this paper, we experiment with first-order word
co-occurrence and Random Indexing as relevance
measures. And we define salient contextual words
as heads or complements in the dependency rela-
tions with the target word. In this way, we use the
parse information to constrain the two distribution
models. Thus the word co-occurrence information
1294
Feature Name PAR Features (compliment) PAR Features (complement)
1. Head Relation Name ccomp appos
2. Head of Relation says collisions
3. Head Combination ccomp says appos collisions
4. Comp Relation Name nsubj dep
5. Comp of Relation addition rating
6. Comp Combination nsub addition dep rating
Table 1: Parse Feature Example for the sentence: ?GM says the addition of OnStar, which includes a system that
automatically notifies an OnStar operator if the vehicle is involved in a collision, complements the Vue?s top five-star
safety rating for the driver and front passenger in both front- and side-impact crash tests.?
considerably overlaps with some values of the PAR
features, but provides extra evidence from web-scale
data rather than a limited amount of training data.
4.1 First-order Word Co-occurrence
The relevance based on first-order word co-
occurrence is calculated from the Google Web 5-
gram Corpus in a fashion similar to how we dealt
with n-gram counts in the previous section. Given
two words, w1 and w2, we consider all 8 possible
patterns that appear in a local context (5-word win-
dow), where we use wildcard (*) to indicate any to-
ken:
w1 w2
w1 * w2
w1 * * w2
w1 * * * w2
w2 w1
w2 * w1
w2 * * w1
w2 * * * w1
The relevance is then calculated by summing the
logarithm of each of the 8 different counts. Finally,
we compare the score of each candidate word and
output the one with higher score.
4.2 Random Indexing
The relevance scores based on Random Indexing
are provided by a tool FRanI (Higgins, 2004) and
a model trained on the Touchstone Applied Science
Associates (TASA) corpus which contains 750k sen-
tences and covers diverse topics (from a diversity of
textbooks up to the college level). Take the sentence
at the beginning of this section for example, where
only the words ?a? and ?friends? are related to the
target word (either ?complement? or ?compliment?)
by either relevance measure. The relevance based
on Random Indexing for (complement, friends) is
0.08, (compliment, friends) is 0.19 and both (com-
pliment, a) and (complement, a) are 0 because ?a?
is in the stop word list. Meanwhile, the relevance
based on first order word co-occurrence for (com-
pliment, friends) is 7.39, (complement, friends) is
5.38, (compliment, a) is 13.25, and (complement, a)
is 13.42. The system with either kind of relevance
outputs ?compliment?.
4.3 System Combination
Since the numeric measurement of word co-
occurrence is not as specific as the PAR features and
less trustworthy, adding word co-occurrence infor-
mation as features into the classifier along with n-
gram counts, lexical and parse features will hurt the
overall performance. It is more practical to combine
the two approaches in the following fashion:
1. When the SVM classifier (using NG, LEX and
PAR features) has high confidence (over a cer-
tain threshold) in the output label, output that
label;
2. Otherwise, output the results of the word
relatedness/co-occurrence-based system.
5 Evaluation
We evaluate the effectiveness of syntactic and dis-
tributional information on spelling correction. The
performance of the system is measured by accu-
racy: the percentage of sentences in the test data
for which the system chooses the correct word. We
compare our results against two baselines: 1) MA-
JOR chooses the most frequent candidate from the
1295
confusion set in the training corpus, and 2) Bergsma
et al?s (2010) best systems, NG+LEX. We include
inflectional variants (?-ing?, ?-ed?, ?-s?, ?-ly?) of
confusion words in the evaluation, such as comple-
menting, complimenting in addition to complement,
compliment, because this better corresponds to the
range of errors that may be encountered in actual
use and thus increases the scope of the system as a
real world application. Also following Bergsma et
al. (2010), we use a linear SVM, more exactly, the
L2-regularized L2-loss dual SVM in LIBLINEAR
(Fan et al, 2008). Unlike Bergsma et al, who used
development data to optimize parameters, we always
use default parameters, since training data is limited
for many of the words we are dealing with.
5.1 Data
Following Bergsma et al (2009; 2010), the test
examples are extracted from The New York Times
(NYT) portion of Gigaword1, but constrained to a
9-month publication time frame from October 2005
to July 2006. Unlike Bergsma et al who use the
same source as training data for the lexical features,
our training data (for both lexical and parse features)
comes from larger and more diverse news sources.
We use the very large database from Sekine?s n-gram
search engine (Sekine, 2008) as training data, which
consists of 1.9B words of newspaper text spanning
89 years from NYT, BBC, WSJ, Xinhua, etc.
We evaluate our systems on 5 confusion sets from
Bergsma et al (2009; 2010) and 9 commonly con-
fused word pairs with moderate frequency in daily
usage (randomly selected from those listed in En-
glish educational resources2). Shown in Table 2,
these 9 sets of words appear much less frequently
than the words selected by Bergsma et al, even
given the fact that we are using a considerably large
training corpus.
For each confusable word pair, sentences that
contain either of the words are extracted to form
training and test data. The word that appears in the
original sentences of the news article is treated as
the gold standard. For frequently occurring confu-
sion word sets used by Bergsma et al, we extract
up to 10k examples for testing, and up to 100k ex-
1Available from the LDC as LDC2003T05
2Such as an English learning blog post at
http://elisaenglish.pixnet.net/blog/post/1335194
Word Confusion Set # in Training Corpus
adverse / averse 13.5k / 1.8k
advice / advise 62.k / 12.9k
allusion / illusion 1.0k / 5.4k
complement / compliment 6.8k / 3.1k
confidant / confident 2.4k / 63.6k
desert / dessert 24.7k / 3.7k
discreet / discrete 0.7k / 2.4k
elicit / illicit 1.9k / 10.0k
stationary / stationery 2.5k/2.3k
wander / wonder 3.3k / 39.5k
Table 2: Training Data Sizes for Common ESL Confused
Words
amples for training. For the 9 less frequent confu-
sion word sets, we extract all the unique examples
for training and testing from the above sources. The
spelling correction system is evaluated by measur-
ing its accuracy in comparison to the gold standard
in test data. The error rate is the complement of ac-
curacy.
Following Carlson et al (2007) and Bergsma
et al (2009; 2010), we obtain the n-gram counts
from the GoogleWeb 1T 5-gram Corpus (Brants and
Franz, 2006).
5.2 Experimental Results
We present the results for each set separately be-
cause each set may behave very differently, depend-
ing upon its frequency, part-of-speech, number of
senses and other differences between the words in
each confusion set. The overall accuracy across con-
fusion sets is also presented to show the effective-
ness of different approaches. The results are tested
for statistical significance using McNemar?s test of
correlated proportions. The performance differences
are marked as significant when p < 0.05.
5.2.1 Effectiveness of Parse Features
We exploit the n-gram counts (NG), lexical fea-
tures (LEX) of Bergsma et al (2010) and our own
parse features (PAR) in linear SVM models.
The first comparison is between the supervised
learning systems with LEX and LEX+PAR. As
shown in Table 3, by exploiting our unique parse
features, for the total 14 confusion sets, the accuracy
increases on 12 sets and decreases on 2 sets. Over-
all, the spelling correction accuracy improves an ab-
1296
solute 1.35% for our 9 confusion sets and 0.60% for
Bergsma et al?s 5 confusion sets.
The second comparison is to see how parse fea-
tures interact with n-gram count features in a su-
pervised classifier. The best system from (Bergsma
et al, 2010) is listed in the table as ?NG+LEX?.
As shown in Table 3, the parse features proved to
be beneficial when augmenting this baseline, except
for the decrease in accuracy on adverse, averse by
only 2 cases out of 368, and among, between by
2 cases out of 10227. For all other confusion sets,
parse features decrease the error rate by as much as
2.74% (absolute) and as much as 38.5% (relative).
Improvements are statistically significant on all con-
fusion sets together, although for each separate set,
improvements are significant on only 5 sets, in part
due to an insufficient number of test cases.
The reason that parse features are occasionally not
helpful is because they sometimes include an un-
common word in dependencies, which happens to
appear once with the wrong word but not with the
correct word in the training data; or they sometimes
include too common words, which bias the classifier
in favor of the more frequent word in the confusion
set. We also noticed that lexical features are not al-
ways helpful when added to n-gram count features,
even for in-domain applications (i.e., with training
data and test data coming from the same domain or
corpus), as marked by underlines. However, lexical
and parse features together show more significant
and constant improvement over n-gram count-based
models, as marked by ?.
Of the six systems, every system that uses parse
features gets the example correct in Section 1, ?com-
plementing the president?; LEX by itself also gets
the example correct, but NG and NG+LEX fail.
In summary, our system NG+LEX+PAR outper-
forms the state-of-the-art system NG+LEX. It re-
duces the error rate by 12.4% across our 9 confusion
sets and by 8.4% across Bergsma et al?s 5 confusion
sets. Both improvements are significant (p < 0.05)
by the McNemar test. In addition, while NG+LEX
is not always better than NG, NG+LEX+PAR is con-
sistently better than NG.
5.2.2 Impact of Word Co-occurrence
The LIBLINEAR tool does not provide probabil-
ity estimates for SVM models but Logistic Regres-
sion can. In this set of experiments, we train a Logis-
tic Regression model with NG+LEX+PAR features
and empirically set the confidence threshold at 0.6,
as described in Section 4, based on the performance
on two word pairs. In the combined system, when
the Logistic Regression model estimates a probabil-
ity higher than the threshold we output its results,
otherwise we output the result of the system based
on word co-occurrence.
Surprisingly, although Random Indexing takes
into account more information than first-order word
co-occurrence, it lowered overall performance sub-
stantially. Thus in Table 4, we only present results
of using first-order word co-occurrence rather than
Random Indexing. For all 12 confusion sets, distri-
butional word co-occurrence information improves
9 sets and hurts 5 sets. Overall, it reduces the er-
ror rate slightly by 0.2% for our 9 sets and 1.5% for
Bergsma et al?s sets.
We believe there are two reasons why Ran-
dom Indexing fared worse than first-order word
co-occurrence: 1) Random Indexing considers co-
occurrence on a document level, while our first-
order word co-occurrence is limited to a 5-word win-
dow context. The latter is more suitable to context-
sensitive spelling correction. 2) The model for Ran-
dom Indexing is trained on a relatively small size
corpus compared to the web-scale data we used to
get n-gram count features for the classifier and thus
is not able to introduce much new evidence besides
the information carried by NG+LEX+PAR features.
Reason 2) also suggests why first-order co-
occurrence helps on some occasions while not on
other occasions. Its impact is limited because the
word co-occurrence information overlaps with some
of the PAR feature values as mentioned earlier. It
improves some cases because it provides some new
evidence from web-scale data to the system based on
NG+LEX+PAR features. It introduces new errors
because it simply favors the word that co-occurred
more often regardless of other factors. Its impact is
also limited because it is only considered when clas-
sifiers with NG+LEX+PAR features are not confi-
dent.
1297
CONFUSION SET # TEST MAJOR LEX LEX+PAR NG NG+LEX NG+LEX+PAR (&)
9 commonly cited ESL confusion pairs
adverse / averse 368 85.87 97.01 96.74 91.03 97.55 97.01 (+22.2%) ?
allusion / illusion 535 76.64 91.22 91.40 91.40 92.52 93.08 (-7.5%) ?
complement / compliment 860 51.51 83.84 85.12 88.49 88.37 89.53 (-10.0%)
confidant / confident 2416 94.41 97.97 98.30 98.51 99.05 99.09 (-4.3%) ?
desert / dessert 2357 70.81 90.71 91.56 87.31 93.68 94.57 (-14.1%) ?*
discreet / discrete 219 79.45 84.48 85.84 85.84 90.41 91.32 (-9.5%) ?
elicit / illicit 563 53.46 82.77 95.56 97.51 97.51 98.22 (-28.6%)
stationary / stationery 182 62.64 87.36 92.31* 93.96 92.86 95.60 (-38.5%)
wander / wonder 6506 86.37 96.42 97.42* 97.56 98.23 98.48 (-13.9%) ?*
Total 13972 81.08 93.94 95.29* 94.82 96.56 96.99 (-12.4%) ?*
5 Original Bergsma pairs
# among / between 10227 57.46 91.89 91.86 88.34 93.60 93.58 (+3.1%) ?
# amount / number 7398 76.44 92.34 93.16* 93.03 93.42 94.08 (-10.1%) ?*
# cite / site 10185 95.71 99.42 99.53 99.16 99.52 99.63 (-22.4%)?
# peace / piece 7330 56.81 95.01 97.01* 95.55 96.74 97.46 (-22.2%)? *
# raise / rise 9464 55.98 96.12 96.64* 94.45 96.68 97.05 (-11.5%) ?
Total 44604 68.92 95.09 95.69* 94.07 96.09 96.42 (-8.4%) ?
Table 3: Spelling correction precision (%), impact of adding parse features
SVM trained on 1G words of news text, tested on 9-months of NYT data.
*: Improvement of (NG+)LEX+PAR vs. (NG+)LEX is statistically significant.
?: Improvement of NG+LEX+PAR vs. NG is statistically significant.
&: Relative increase or decrease of error rate compared to ?NG+LEX?
#: As in Bergsma et al (2009; 2010) no morphological variants of the words are used in evaluation
CONFUSION SET # TEST MAJOR CLASSIFIER COMBINED SYSTEM (&)
9 commonly cited ESL confusion pairs
adverse / averse 368 85.87 97.55 96.74 (+33.3%)
allusion / illusion 535 76.64 92.34 92.34 (- 0.0%)
complement / compliment 860 51.51 89.88 90.81 (-9.2%)
confidant / confident 2416 94.41 99.13 99.05 (+9.5%)
desert / dessert 2357 70.81 93.98 94.23 (-3.7%)
discreet / discrete 219 79.45 90.41 91.78 (-14.3%)
elicit / illicit 563 53.46 98.40 98.76 (-22.2%)
stationary / stationery 182 62.64 93.41 93.96 (-9.1%)
wander / wonder 6506 86.37 98.49 98.36 (+9.2%)
5 Original Bergsma pairs
# among / between 10227 57.46 92.73 92.73 (-0.1%)
# amount / number 7398 76.44 93.44 93.76 (-4.74%)
# cite / site 10185 95.71 99.49 99.47 (+3.8%)
# peace / piece 7330 56.81 96.19 96.38 (-5.0%)
# raise / rise 9464 55.98 96.66 96.59 (+2.2%)
Table 4: Spelling correction accuracy (%), impact of combining word co-occurrence
CLASSIFIER: Logistic Regression trained on 1G words of news text, tested on 9-months NYT data.
COMBINED SYSTEM: CLASSIFER plus system based on first-order word co-occurrence.
&: Relative increase or decrease in error rate compared to CLASSIFIER
#: As in Bergsma et al (2009; 2010), no morphological variants of the words are used in evaluation
1298
6 Conclusions
We propose a novel approach that uses parse
features and lexical features together to improve
the performance of web-scale n-gram models for
spelling correction. This method is especially adap-
tive when less training data are available, which is
the case for confusable words that are not very fre-
quently used. We also investigate the effectiveness
of incorporating web-scale word co-occurrence and
corpus-based semantic word relatedness (Random
Indexing).
For future work, we will investigate using seman-
tic information (e.g. WordNet) to extend n-gram
models. It will be interesting to see if the usage of
the word ?compliment? in ?complimenting the pres-
ident? can be estimated by considering similar us-
ages in the corpus, such as ?complimenting the stu-
dent? or by creating an n-gram database of synset
patterns. We will investigate extending, to other ap-
plications, this general methodology combining dis-
tributional, semantic and syntactic information with
language models.
Acknowledgments
We wish to thank Michael Flor of Educational
Testing Service for his TrendStream tool, which
provides fast access and easy manipulation of the
Google N-gram Corpus. We also thank Derrick Hig-
gins of Educational Testing Service for his Random
Indexing support. We also thank Satoshi Sekine of
New York University, Matthew Snover of City Uni-
versity of New York, and Jing Jiang of Singapore
Management University for their advice.
References
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-scale n-gram models for lexical disambiguation.
In IJCAI.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In ACL.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Available at http://www.ldc.upenn.edu.
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet: An experimental, application-
oriented evaluation of five measures. In ACL Work-
shop on WordNet and Other Lexical Resources.
Andrew Carlson and Ian Fette. 2007. Memory-based
context sensitive spelling correction at web scale. In
ICMLA.
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 2007.
Detection of grammatical errors involving preposi-
tions. In Proceedings of the Fourth ACL-SIGSEM
Workshop on Prepositions, pages 25?30.
Silviu Cucerzan and David Yarowsky. 2002. Aug-
mented mixture models for lexical disambigua-tion. In
EMNLP.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, Genoa, Italy.
Scott Deerwester, Susan Dumais, George Furmas,
Thomas Landauer, and Richar Harshman. 1990. In-
dexing by latent semantic analysis. The American So-
ciety for Information Science.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
ACL.
Philip Edmonds. 1997. Choosing the word most typical
in context using a lexical co-occurrence network. In
EACL.
Mohammed Ali Elmi and Martha Evans. 1998. Spelling
correction using context. In COLING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. Machine Learning Re-
search, 9(1871-1874).
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceedings
of COLING, Manchester, UK.
Rachele De Felice and Stephen G. Pulman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3).
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using contextual speller
techniques and language modeling for ESL error cor-
rection. In Proceedings of the International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 449?456, Hyderabad, India.
Andrew Golding and Dan Roth. 1996. Applying Win-
now to context-sensitive spelling correction. In Pro-
ceedings of the International Conference on Machine
Learning (ICML), pages 182?190.
Andrew Golding and Dan Roth. 1999. A winnow-based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
Andrew Golding. 1995. A Bayesian hybrid method for
context sensitive spelling correction. In Proceedings
1299
of the Third Workshop on Very Large Corpora (WVLC-
3), pages 39?53.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Matthieu Hermet, Alain De?silets, and Stan Szpakowicz.
2008. Using the web as a linguistic resource to au-
tomatically correct lexico-syntactic errors. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC), pages 390?
396, Marrekech, Morocco.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic er-
ror detection in the Japanese learners? English spoken
data. In Companion Volume to the Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 145?148.
Michael Jones and James Martin. 1997. Contextual
spelling correction using latent semantic analysis. In
ANLC.
Thomas Landauer, Darrell Laham, and Peter Foltz. 1998.
Learning human-like knowledge by singular value de-
composition: A progress report. Advances in Neural
Information Processing Systems, 10:45?51.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 21:1?31.
John Lee and Ola Knutsson. 2008. The role of pp attach-
ment in preposition generation. In CICLING.
Lidia Mangu and Eric Brill. 1997. Automatic rule acqui-
sition for spelling correction. In ICML.
Saif Mohammad and Graeme Hist. 2006. Distributional
measures of concept distance: A task-oriented evalua-
tion. In EMNLP.
Alla Rozovskaya and Dan Roth. 2010. Training
paradigms for correcting errors in grammar and usage.
In ACL.
Magnus Sahlgren. 2006. The Word-Space Model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis.
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of prepostion error detection in esl writing. In
COLING.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In ACL.
Casey Whitelaw, Ben Hutchinson, Grace Y. Chung, and
Gerard Ellis. 2009. Using the web for language inde-
pendent spellchecking and autocorrection. In ACL.
1300
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 124?129,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Parsing and Disfluency Detection in Linear Time
Mohammad Sadegh Rasooli?
Department of Computer Science
Columbia University, New York, NY
rasooli@cs.columbia.edu
Joel Tetreault
Nuance Communications, Inc.
Sunnyvale, CA
joel.tetreault@nuance.com
Abstract
We introduce a novel method to jointly parse
and detect disfluencies in spoken utterances.
Our model can use arbitrary features for pars-
ing sentences and adapt itself with out-of-
domain data. We show that our method, based
on transition-based parsing, performs at a high
level of accuracy for both the parsing and
disfluency detection tasks. Additionally, our
method is the fastest for the joint task, running
in linear time.
1 Introduction
Detecting disfluencies in spontaneous speech has
been widely studied by researchers in different com-
munities including natural language processing (e.g.
Qian and Liu (2013)), speech processing (e.g. Wang
et al (2013)) and psycholinguistics (e.g. Finlayson
and Corley (2012)). While the percentage of spo-
ken words which are disfluent is typically not more
than ten percent (Bortfeld et al, 2001), this addi-
tional ?noise? makes it much harder for spoken lan-
guage systems to predict the correct structure of the
sentence.
Disfluencies can be filled pauses (e.g. ?uh?, ?um?,
?huh?), discourse markers (e.g. ?you know?, ?I
mean?) or edited words which are repeated or cor-
rected by the speaker. For example, in the follow-
ing sentence, an edited phrase or reparandum inter-
val (?to Boston?) occurs with its repair (?to Den-
ver?), a filled pause (?uh?) and discourse marker (?I
? The first author worked on this project while he was a
research intern in CoreNL research group, NLU lab, Nuance
Communications, Sunnyvale, CA.
mean?).1
I want a flight to Boston? ?? ?
Reparandum
Interregnum
? ?? ?
uh????
FP
I mean? ?? ?
DM
to Denver? ?? ?
Repair
Filled pauses and discourse markers are to some
extent a fixed and closed set. The main challenge
in finding disfluencies is the case where the edited
phrase is neither a rough copy of its repair or has any
repair phrase (i.e. discarded edited phrase). Hence,
in previous work, researchers report their method
performance on detecting edited phrases (reparan-
dum) (Johnson and Charniak, 2004).
In contrast to most previous work which focuses
solely on either detection or on parsing, we intro-
duce a novel framework for jointly parsing sentences
with disfluencies. To our knowledge, our work is
the first model that is based on joint dependency and
disfluency detection. We show that our model is ro-
bust enough to detect disfluencies with high accu-
racy, while still maintaining a high level of depen-
dency parsing accuracy that approaches the upper
bound. Additionally, our model outperforms prior
work on joint parsing and disfluency detection on
the disfluency detection task, and improves upon this
prior work by running in linear time complexity.
The remainder of this paper is as follows. In ?2,
we overview some the previous work on disfluency
detection. ?3 describes our model. Experiments are
described in ?4 and Conclusions are made in ?5.
1In the literature, edited words are also known as ?reparan-
dum?, and the fillers are known as ?interregnum?. Filled pauses
are also called ?Interjections?.
124
2 Related Work
Disfluency detection approaches can be divided into
two different groups: text-first and speech first
(Nakatani and Hirschberg, 1993). In the first ap-
proach, all prosodic and acoustic cues are ignored
while in the second approach both grammatical and
acoustic features are considered. For this paper, we
focus on developing a text-first approach but our
model is easily flexible with speech-first features be-
cause there is no restriction on the number and types
of features in our model.
Among text-first approaches, the work is split
between developing systems which focus specifi-
cally on disfluency detection and those which couple
disfluency detection with parsing. For the former,
Charniak and Johnson (2001) employ a linear clas-
sifier to predict the edited phrases in Switchboard
corpus (Godfrey et al, 1992). Johnson and Char-
niak (2004) use a TAG-based noisy channel model
to detect disfluencies while parsing with getting n-
best parses from each sentence and re-ranking with
a language model. The original TAG parser is not
used for parsing itself and it is used just to find
rough copies in the sentence. Their method achieves
promising results on detecting edited words but at
the expense of speed (the parser has a complexity of
O(N5). Kahn et al (2005) use the same TAG model
and add semi-automatically extracted prosodic fea-
tures. Zwarts and Johnson (2011) improve the per-
formance of TAG model by adding external lan-
guage modeling information from data sets such as
Gigaword in addition to using minimal expected F-
loss in n-best re-ranking.
Georgila (2009) uses integer linear programming
combined with CRF for learning disfluencies. That
work shows that ILP can learn local and global con-
straints to improve the performance significantly.
Qian and Liu (2013) achieve the best performance
on the Switchboard corpus (Godfrey et al, 1992)
without any additional data. They use three steps for
detecting disfluencies using weighted Max-Margin
Markov (M3) network: detecting fillers, detecting
edited words, and refining errors in previous steps.
Some text-first approaches treat parsing and dis-
fluency detection jointly, though the models differ
in the type of parse formalism employed. Lease and
Johnson (2006) use a PCFG-based parser to parse
sentences along with finding edited phrases. Miller
and Schuler (2008) use a right-corner transform of
binary branching structures on bracketed sentences
but their results are much worse than (Johnson and
Charniak, 2004). To date, none of the prior joint ap-
proaches have used a dependency formalism.
3 Joint Parsing Model
We model the problem using a deterministic
transition-based parser (Nivre, 2008). These parsers
have the advantage of being very accurate while be-
ing able to parse a sentence in linear time. An ad-
ditional advantage is that they can use as many non-
local and local features as needed.
Arc-Eager Algorithm We use the arc-eager algo-
rithm (Nivre, 2004) which is a bottom-up parsing
strategy that is used in greedy and k-beam transition-
based parsers. One advantage of this strategy is that
the words can get a head from their left side, before
getting right dependents. This is particularly bene-
ficial for our task, since we know that reparanda are
similar to their repairs. Hence, a reparandum may
get its head but whenever the parser faces a repair, it
removes the reparandum from the sentence and con-
tinues its actions.
The actions in an arc-eager parsing algorithm are:
? Left-arc (LA): The first word in the buffer be-
comes the head of the top word in the stack.
The top word is popped after this action.
? Right-arc (RA): The top word in the stack be-
comes the head of the first word in the buffer.
? Reduce (R): The top word in the stack is
popped.
? Shift (SH): The first word in the buffer goes to
the top of the stack.
Joint Parsing and Disfluency Detection We first
extend the arc-eager algorithm by augmenting the
action space with three new actions:
? Reparandum (Rp[i:j]): treat a phrase (words i
to j) outside the look-ahead buffer as a reparan-
dum. Remove them from the sentence and clear
their dependencies.
? Discourse Marker (Prn[i]): treat a phrase in
the look-ahead buffer (first i words) as a dis-
course marker and remove them from the sen-
tence.
125
Stack Buffer Act.
flight to Boston uh I mean ... RA
flight to Boston uh I mean to ... RA
flight to Boston uh I mean to Denver Intj[1]
flight to Boston I mean to Denver Prn[1]
flight to Boston to Denver RP[2:3]
flight to Denver RA
flight to Denver RA
flight to Denver R
flight to R
flight R
Figure 1: A sample transition sequence for the sentence
?flight to Boston uh I mean to Denver?. In the third col-
umn, only the underlined parse actions are learned by the
parser (second classifier). The first classifier uses all in-
stances for training (learns fluent words with ?regular?
label).
? Interjection (Intj[i]): treat a phrase in the
look-ahead buffer (first i words) as a filled
pause and remove them from the sentence.2
Our model has two classifiers. The first classi-
fier decides between four possible actions and pos-
sible candidates in the current configuration of the
sentence. These actions are the three new ones
from above and a new action Regular (Reg): which
means do one of the original arc-eager parser ac-
tions.
At each configuration, there might be several can-
didates for being a prn, intj or reparandum, and
one regular candidate. The candidates for being
a reparandum are a set of words outside the look-
ahead buffer and the candidates for being an intj or
prn are a set of words beginning from the head of
the look-ahead buffer. If the parser decides regular
as the correct action, the second classifier predicts
the best parsing transition, based on arc-eager pars-
ing (Nivre, 2004).
For example, in the 4th state in Figure 1, there are
multiple candidates for the first classifier: regular,
?I? as prn[1] or intj[1], ?I mean? as prn[2] or intj[2],
?I mean to? as prn[3] or intj[3], ?I mean to Denver?
as prn[4] or intj[4], ?Boston? as rp[3:3], ?to Boston?
as rp[2:3], and ?flight to Boston? as rp[1:3].
2In the bracketed version of Switchboard corpus, reparan-
dum is tagged with EDITED and discourse markers and paused
fillers are tagged as PRN and INTJ respectively.
Training A transition-based parser action (our
second-level classifier) is sensitive to the words in
the buffer and stack. The problem is that we do not
have gold dependencies for edited words in our data.
Therefore, we need a parser to remove reparandum
words from the buffer and push them into the stack.
Since our parser cannot be trained on disfluent sen-
tences from scratch, the first step is to train it on
clean treebank data.
In the second step, we adapt parser weights by
training it on disfluent sentences. Our assumption
is that we do not know the correct dependencies be-
tween disfluent words and other words in the sen-
tence. At each configuration, the parser updates it-
self with new instances by traversing all configura-
tions in the sentences. In this case, if at the head of
the buffer there is an intj or prn tag, the parser allows
them to be removed from the buffer. If a reparan-
dum word is not completely outside the buffer (the
first two states in Figure 1), the parser decides be-
tween the four regular arc-eager actions (i.e. left-
arc, right-arc, shift, and reduce). If the last word
pushed into the stack is a reparandum and the first
word in the buffer is a regular word, the parser re-
moves all reparanda at the same level (in the case of
nested edited words), removes their dependencies to
other words and push their dependents into the stack.
Otherwise, the parser performs the oracle action and
adds that action as its new instance.3
With an adapted parser which is our second-level
classifier, we can train our first-level classifier. The
same procedure repeats, except that instances for
disfluency detection are used for updating param-
eter weights for the first classifier for deciding the
actions. In Figure 1, only the oracle actions (under-
lined) are added to the instances for updating parser
weights but all first-level actions are learned by the
first level classifier.
4 Experiments and Evaluation
For our experiments, we use the Switchboard corpus
(Godfrey et al, 1992) with the same train/dev/test
split as Johnson and Charniak (2004). As in that
3The reason that we use a parser instead of expanding all
possible transitions for an edited word is that, the number of reg-
ular actions will increase and the other actions become sparser
than natural.
126
work, incomplete words and punctuations are re-
moved from data (except that we do not remove in-
complete words that are not disfluent4) and all words
are turned into lower-case. The main difference with
previous work is that we use Switchboard mrg files
for training and testing our model (since they con-
tain parse trees) instead of the more commonly used
Swithboard dps text files. Mrg files are a subset of
dps files with about more than half of their size.
Unfortunately, the disfluencies marked in the dps
files are not exactly the same as those marked in
the corresponding mrg files. Hence, our result is not
completely comparable to previous work except for
(Kahn et al, 2005; Lease and Johnson, 2006; Miller
and Schuler, 2008).
We use Tsurgeon (Levy and Andrew, 2006) for
extracting sentences from mrg files and use the
Penn2Malt tool5 to convert them to dependencies.
Afterwards, we provide dependency trees with dis-
fluent words being the dependent of nothing.
Learning For the first classifier, we use averaged
structured Perceptron (AP) (Collins, 2002) with a
minor modification. Since the first classifier data is
heavily biased towards the ?regular label?, we mod-
ify the weight updates in the original algorithm to 2
(original is 1) for the cases where a ?reparandum?
is wrongly recognized as another label. We call
the modified version ?weighted averaged Perceptron
(WAP)?. We see that this simple modification im-
proves the model accuracy.6 For the second classi-
fier (parser), we use the original averaged structured
Perceptron algorithm. We report results on both AP
and WAP versions of the parser.
Features Since for every state in the parser config-
uration, there are many candidates for being disflu-
ent; we use local features as well as global features
for the first classifier. Global features are mostly
useful for discriminating between the four actions
and local features are mostly useful for choosing a
phrase as a candidate for being a disfluent phrase.
The features are described in Figure 2. For the sec-
ond classifier, we use the same features as (Zhang
and Nivre, 2011, Table 1) except that we train our
4E.g. I want t- go to school.
5http://stp.lingfil.uu.se/?nivre/
research/Penn2Malt.html
6This is similar to WM3N in (Qian and Liu, 2013).
Global Features
First n words inside/outside buffer (n=1:4)
First n POS i/o buffer (n=1:6)
Are n words i/o buffer equal? (n=1:4)
Are n POS i/o buffer equal? (n=1:4)
n last FG transitions (n=1:5)
n last transitions (n=1:5)
n last FG transitions + first POS in the buffer (n=1:5)
n last transitions + first POS in the buffer (n=1:5)
(n+m)-gram of m/n POS i/o buffer (n,m=1:4)
Refined (n+m)-gram of m/n POS i/o buffer (n,m=1:4)
Are n first words of i/o buffer equal? (n=1:4)
Are n first POS of i/o buffer equal? (n=1:4)
Number of common words i/o buffer words (n=1:6)
Local Features
First n words of the candidate phrase (n=1:4)
First n POS of the candidate phrase (n=1:6)
Distance between the candidate and first word in the buffer
Figure 2: Features used for learning the first classifier.
Refined n-gram is the n-gram without considering words
that are recognized as disfluent. Fine-grained (FG) tran-
sitions are enriched with parse actions (e.g. ?regular:left-
arc?).
parser in a similar manner as the MaltParser (Nivre
et al, 2007) without k-beam training.
Parser Evaluation We evaluate our parser with
both unlabeled attachment accuracy of correct words
and precision and recall of finding the dependencies
of correct words.7 The second classifier is trained
with 3 iterations in the first step and 3 iterations in
the second step. We use the attachment accuracy
of the parse tree of the correct sentences (without
disfluencies) as the upper-bound attachment score
and parsed tree of the disfluent sentences (without
disfluency detection) as our lower-bound attachment
score. As we can see in Table 1, WAP does a slightly
better job parsing sentences. The upper-bound pars-
ing accuracy shows that we do not lose too much in-
formation while jointly detecting disfluencies. Our
parser is not comparable to (Johnson and Charniak,
2004) and (Miller and Schuler, 2008), since we use
dependency relations for evaluation instead of con-
stituencies.
Disfluency Detection Evaluation We evaluate
our model on detecting edited words in the sentences
7The parser is actually trained to do labeled attachment and
labeled accuracy is about 1-1.5% lower than UAS.
127
UAS LB UB Pr. Rec. F2
AP 88.6 70.7 90.2 86.8 88.0 87.4
WAP 88.1 70.7 90.2 87.2 88.0 87.6
Table 1: Parsing results. UB = upperbound (parsing clean
sentences), LB = lowerbound (parsing disfluent sentences
without disfluency correction). UAS is unlabeled attach-
ment score (accuracy), Pr. is precision, Rec. is recall and
F1 is f-score.
Pr. Rec. F1
AP 92.9 71.6 80.9
WAP 85.1 77.9 81.4
KL (2005) ? ? 78.2
LJ (2006) ? ? 62.4
MS (2008) ? ? 30.6
QL (2013) ? Default ? ? 81.7
QL (2013) ? Optimized ? ? 82.1
Table 2: Disfluency results. Pr. is precision, Rec. is recall
and F1 is f-score. KL = (Kahn et al, 2005), LJ = (Lease
and Johnson, 2006), MS = (Miller and Schuler, 2008) and
QL = (Qian and Liu, 2013).
(words with ?EDITED? tag in mrg files). As we
see in Table 2, WAP works better than the original
method. As mentioned before, the numbers are not
completely comparable to others except for (Kahn
et al, 2005; Lease and Johnson, 2006; Miller and
Schuler, 2008) which we outperform. For the sake
of comparing to the state of the art, the best result
for this task (Qian and Liu, 2013) is replicated from
their available software8 on the portion of dps files
that have corresponding mrg files. For a fairer com-
parison, we also optimized the number of training
iterations of (Qian and Liu, 2013) for the mrg set
based on dev data (10 iterations instead of 30 iter-
ations). As shown in the results, our model accu-
racy is slightly less than the state-of-the-art (which
focuses solely on the disfluency detection task and
does no parsing), but we believe that the perfor-
mance can be improved through better features and
by changing the model. Another characteristic of
our model is that it operates at a very high precision,
though at the expense of some recall.
8We use the second version of the code: http://code.
google.com/p/disfluency-detection/. Results
from the first version are 81.4 and 82.1 for the default and opti-
mized settings.
5 Conclusion
In this paper, we have developed a fast, yet accurate,
joint dependency parsing and disfluency detection
model. Such a parser is useful for spoken dialogue
systems which typically encounter disfluent speech
and require accurate syntactic structures. The model
is completely flexible with adding other features (ei-
ther text or speech features).
There are still many ways of improving this
framework such as using k-beam training and decod-
ing, using prosodic and acoustic features, using out
of domain data for improving the language and pars-
ing models, and merging the two classifiers into one
through better feature engineering. It is worth noting
that we put the dummy root word in the first position
of the sentence. Ballesteros and Nivre (2013) show
that parser accuracy can improve by changing that
position for English.
One of the main challenges in this problem is
that most of the training instances are not disflu-
ent and thus the sample space is very sparse. As
seen in the experiments, we can get further improve-
ments by modifying the weight updates in the Per-
ceptron learner. In future work, we will explore
different learning algorithms which can help us ad-
dress the sparsity problem and improve the model
accuracy. Another challenge is related to the parser
speed, since the number of candidates and features
are much greater than the number used in classical
dependency parsers.
Acknowledgements We would like to thank
anonymous reviewers for their helpful comments
on the paper. Additionally, we were aided by re-
searchers by their prompt responses to our many
questions: Mark Core, Luciana Ferrer, Kallirroi
Georgila, Mark Johnson, Jeremy Kahn, Yang Liu,
Xian Qian, Kenji Sagae, and Wen Wang. Finally,
this work was conducted during the first author?s
summer internship at the Nuance Sunnyvale Re-
search Lab. We would like to thank the researchers
in the group for the helpful discussions and assis-
tance on different aspects of the problem. In particu-
lar, we would like to thank Chris Brew, Ron Kaplan,
Deepak Ramachandran and Adwait Ratnaparkhi.
128
References
Miguel Ballesteros and Joakim Nivre. 2013. Going to
the roots of dependency parsing. Computational Lin-
guistics, 39(1):5?13.
Heather Bortfeld, Silvia D. Leon, Jonathan E. Bloom,
Michael F. Schober, and Susan E. Brennan. 2001.
Disfluency rates in conversation: Effects of age, re-
lationship, topic, role, and gender. Language and
Speech, 44(2):123?147.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In NAACL-
HLT, pages 1?9.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In ACL, pages 1?8.
Ian R. Finlayson and Martin Corley. 2012. Disfluency
in dialogue: an intentional signal from the speaker?
Psychonomic bulletin & review, 19(5):921?928.
Kallirroi Georgila. 2009. Using integer linear program-
ming for detecting speech disfluencies. In NAACL-
HLT, pages 109?112.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech corpus
for research and development. In ICASSP, volume 1,
pages 517?520.
Mark Johnson and Eugene Charniak. 2004. A tag-based
noisy channel model of speech repairs. In ACL, pages
33?39.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
EMNLP, pages 233?240.
Matthew Lease and Mark Johnson. 2006. Early dele-
tion of fillers in processing conversational speech. In
NAACL-HLT, pages 73?76.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In LREC, pages 2231?2234.
Tim Miller and William Schuler. 2008. A unified syn-
tactic model for parsing fluent and disfluent speech. In
ACL-HLT, pages 105?108.
Christine Nakatani and Julia Hirschberg. 1993. A
speech-first model for repair detection and correction.
In ACL, pages 46?53.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In the Workshop on Incremen-
tal Parsing: Bringing Engineering and Cognition To-
gether, pages 50?57.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Xian Qian and Yang Liu. 2013. Disfluency detection
using multi-step stacked learning. In NAACL-HLT,
pages 820?825.
Wen Wang, Andreas Stolcke, Jiahong Yuan, and Mark
Liberman. 2013. A cross-language study on auto-
matic speech disfluency detection. In NAACL-HLT,
pages 703?708.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
ACL (Short Papers), pages 188?193.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disflu-
ency detection. In ACL, pages 703?711.
129
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 48?53,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Non-Monotonic Parsing of Fluent umm I Mean Disfluent Sentences
Mohammad Sadegh Rasooli
Department of Computer Science
Columbia University, New York, NY, USA
rasooli@cs.columbia.edu
Joel Tetreault
Yahoo Labs
New York, NY, USA
tetreaul@yahoo-inc.com
Abstract
Parsing disfluent sentences is a challeng-
ing task which involves detecting disflu-
encies as well as identifying the syntactic
structure of the sentence. While there have
been several studies recently into solely
detecting disfluencies at a high perfor-
mance level, there has been relatively lit-
tle work into joint parsing and disfluency
detection that has reached that state-of-
the-art performance in disfluency detec-
tion. We improve upon recent work in this
joint task through the use of novel features
and learning cascades to produce a model
which performs at 82.6 F-score. It outper-
forms the previous best in disfluency de-
tection on two different evaluations.
1 Introduction
Disfluencies in speech occur for several reasons:
hesitations, unintentional mistakes or problems in
recalling a new object (Arnold et al., 2003; Merlo
and Mansur, 2004). Disfluencies are often de-
composed into three types: filled pauses (IJ) such
as ?uh? or ?huh?, discourse markers (DM) such
as ?you know? and ?I mean? and edited words
(reparandum) which are repeated or corrected by
the speaker (repair). The following sentence illus-
trates the three types:
I want a flight to Boston
? ?? ?
Reparandum
uh
????
IJ
I mean
? ?? ?
DM
to Denver
? ?? ?
Repair
To date, there have been many studies on disflu-
ency detection (Hough and Purver, 2013; Rasooli
and Tetreault, 2013; Qian and Liu, 2013; Wang et
al., 2013) such as those based on TAGs and the
noisy channel model (e.g. Johnson and Charniak
(2004), Zhang et al. (2006), Georgila (2009), and
Zwarts and Johnson (2011)). High performance
disfluency detection methods can greatly enhance
the linguistic processing pipeline of a spoken dia-
logue system by first ?cleaning? the speaker?s ut-
terance, making it easier for a parser to process
correctly. A joint parsing and disfluency detection
model can also speed up processing by merging
the disfluency and parsing steps into one. How-
ever, joint parsing and disfluency detection mod-
els, such as Lease and Johnson (2006), based
on these approaches have only achieved moder-
ate performance in the disfluency detection task.
Our aim in this paper is to show that a high perfor-
mance joint approach is viable.
We build on our previous work (Rasooli and
Tetreault, 2013) (henceforth RT13) to jointly
detect disfluencies while producing dependency
parses. While this model produces parses at a
very high accuracy, it does not perform as well as
the state-of-the-art in disfluency detection (Qian
and Liu, 2013) (henceforth QL13). In this pa-
per, we extend RT13 in two important ways: 1)
we show that by adding a set of novel features se-
lected specifically for disfluency detection we can
outperform the current state of the art in disfluency
detection in two evaluations
1
and 2) we show that
by extending the architecture from two to six clas-
sifiers, we can drastically increase the speed and
reduce the memory usage of the model without a
loss in performance.
2 Non-monotonic Disfluency Parsing
In transition-based dependency parsing, a syntac-
tic tree is constructed by a set of stack and buffer
actions where the parser greedily selects an action
at each step until it reaches the end of the sentence
with an empty buffer and stack (Nivre, 2008). A
state in a transition-based system has a stack of
words, a buffer of unprocessed words and a set of
arcs that have been produced in the parser history.
The parser consists of a state (or a configuration)
1
Honnibal and Johnson (2014) have a forthcoming paper
based on a similar idea but with a higher performance.
48
which is manipulated by a set of actions. When an
action is made, the parser goes to a new state.
The arc-eager algorithm (Nivre, 2004) is a
transition-based algorithm for dependency pars-
ing. In the initial state of the algorithm, the buffer
contains all words in the order in which they ap-
pear in the sentence and the stack contains the arti-
ficial root token. The actions in arc-eager parsing
are left-arc (LA), right-arc (RA), reduce (R) and
shift (SH). LA removes the top word in the stack
by making it the dependent of the first word in the
buffer; RA shifts the first word in the buffer to the
stack by making it the dependent of the top stack
word; R pops the top stack word and SH pushes
the first buffer word into the stack.
The arc-eager algorithm is a monotonic parsing
algorithm, i.e. once an action is performed, subse-
quent actions should be consistent with it (Honni-
bal et al., 2013). In monotonic parsing, if a word
becomes a dependent of another word or acquires
a dependent, other actions shall not change those
dependencies that have been constructed for that
word in the action history. Disfluency removal is
an issue for monotonic parsing in that if an ac-
tion creates a dependency relation, the other ac-
tions cannot repair that dependency relation. The
main idea proposed by RT13 is to change the
original arc-eager algorithm to a non-monotonic
one so it is possible to repair a dependency tree
while detecting disfluencies by incorporating three
new actions (one for each disfluency type) into a
two-tiered classification process. The structure is
shown in Figure 1(a). In short, at each state the
parser first decides between the three new actions
and a parse action (C1). If the latter is selected, an-
other classifier (C2) is used to select the best parse
action as in normal arc eager parsing.
The three additional actions to the arc-eager al-
gorithm to facilitate disfluency detection are as
follows: 1) RP[i:j]: From the words outside the
buffer, remove words i to j from the sentence and
tag them as reparandum, delete all of their depen-
dencies and push all of their dependents onto the
stack. 2) IJ[i]: Remove the first i words from
the buffer (without adding any dependencies to
them) and tag them as interjection. 3) DM[i]:
Remove the first i words from the buffer (with-
out adding any dependencies) and tag them as dis-
course marker.
State
C1
Parse
RP[i:j]
IJ[i]DM[i]
C2
LA RA RSH
(a) A structure with two classifiers.
IJ[i]
C3
DM[i]
Parse
C5
C2
IJDM
C1 Other
C4 RP
C6
RLARA SH
RP[i:j]
State
(b) A structure with six classifiers.
Figure 1: Two kinds of cascades for disfluency
learning. Circles are classifiers and light-colored
blocks show the final decision by the system.
3 Model Improvements
To improve upon RT13, we first tried to learn all
actions jointly. Essentially, we added the three
new actions to the original arc-eager action set.
However, this method (henceforth M1) performed
poorly on the disfluency detection task. We be-
lieve this stems from a feature mismatch, i.e. some
of the features, such as rough copies, are only use-
ful for reparanda while some others are useful for
other actions. Speed is an additional issue. Since
for each state, there are many candidates for each
of the actions, the space of possible candidates
makes the parsing time potentially squared.
Learning Cascades One possible solution for
reducing the complexity of the inference is to for-
mulate and develop learning cascades where each
cascade is in charge of a subset of predictions with
its specific features. For this task, it is not es-
sential to always search for all possible phrases
because only a minority of cases in speech texts
are disfluent (Bortfeld et al., 2001). For address-
ing this problem, we propose M6, a new structure
for learning cascades, shown in Figure 1(b) with
a more complex structure while more efficient in
terms of speed and memory. In the new structure,
we do not always search for all possible phrases
which will lead to an expected linear time com-
plexity. The main processing overhead here is the
number of decisions to make by classifiers but this
is not as time-intensive as finding all candidate
phrases in all states.
Feature Templates RT13 use different feature
sets for the two classifiers: C2 uses the parse fea-
49
tures promoted in Zhang and Nivre (2011, Table
1) and C1 uses features which are shown with
regular font in Figure 2. We show that one can
improve RT13 by adding new features to the C1
classifier which are more appropriate for detecting
reparanda (shown in bold in Figure 2). We call
this new model M2E, ?E? for extended. In Figure
3, the features for each classifier in RT13, M2E,
M6 and M1 are described.
We introduce the following new features: LIC
looks at the number of common words between the
reparandum candidate and words in the buffer; e.g.
if the candidate is ?to Boston? and the words in the
buffer are ?to Denver?, LIC[1] is one and LIC[2]
is also one. In other words, LIC is an indicator
of a rough copy. The GPNG (post n-gram fea-
ture) allows us to model the fluency of the result-
ing sentence after an action is performed, without
explicitly going into it. It is the count of possible
n-grams around the buffer after performing the ac-
tion; e.g. if the candidate is a reparandum action,
this feature introduces the n-grams which will ap-
pear after this action. For example, if the sentence
is ?I want a flight to Boston | to Denver? (where
| is the buffer boundary) and the candidate is ?to
Boston? as reparandum, the sentence will look like
?I want a flight | to Denver? and then we can count
all possible n-grams (both lexicalized and unlexi-
calized) in the range i and j inside and outside the
buffer. GBPF is a collection of baseline parse fea-
tures from (Zhang and Nivre, 2011, Table 1).
The need for classifier specific features be-
comes more apparent in the M6 model. Each of
the classifiers uses a different set of features to op-
timize performance. For example, LIC features
are only useful for the sixth classifier while post
n-gram features are useful for C2, C3 and C6. For
the joint model we use the C1 features from M2B
and the C1 features from M6.
4 Experiments and Evaluation
We evaluate our new models, M2E and M6,
against prior work on two different test conditions.
In the first evaluation (Eval 1), we use the parsed
section of the Switchboard corpus (Godfrey et al.,
1992) with the train/dev/test splits from Johnson
and Charniak (2004) (JC04). All experimental set-
tings are the same as RT13. We compare our new
models against this prior work in terms of disflu-
ency detection performance and parsing accuracy.
In the second evaluation (Eval 2), we compare our
Abbr. Description
GS[i/j] First n Ws/POS outside ? (n=1:i/j)
GB[i/j] First n Ws/POS inside ? (n=1:i/j)
GL[i/j] Are n Ws/POS i/o ? equal? (n=1:i/j)
GT[i] n last FGT; e.g. parse:la (n=1:i)
GTP[i] n last FGT e.g. parse (n=1:i)
GGT[i] n last FGT + POS of ?
0
(n=1:i)
GGTP[i] n last CGT + POS of ?
0
(n=1:i)
GN[i] (n+m)-gram of m/n POS i/o ? (n,m=1:i)
GIC[i] # common Ws i/o ? (n=1:i)
GNR[i] Rf. (n+m)-gram of m/n POS i/o ? (n,m=1:i)
GPNG[i/j] PNGs from n/m Ws/POS i/o ? (m,n:1:i/j)
GBPF Parse features (Zhang and Nivre, 2011)
LN[i,j] First n Ws/POS of the cand. (n=1:i/j)
LD Distance between the cand. and s
0
LL[i,j] first n Ws/POS of rp and ? equal? (n=1:i/j)
LIC[i] # common Ws for rp/repair (n=1:i)
Figure 2: Feature templates used in this paper and
their abbreviations. ?: buffer, ?
0
: first word in
the buffer, s
0
: top stack word, Ws: words, rp:
reparadnum, cand.: candidate phrase, PNGs: post
n-grams, FGT: fine-grained transitions and CGT:
coarse-grained transitions. Rf. n-gram: n-gram
from unremoved words in the state.
Classifier Features
M2 Features
C1 (RT13) GS[4/4], GB[4/4], GL[4/6], GT[5], GTP[5]
GGT[5], GGTP[5], GN[4], GNR[4], GIC[6]
LL[4/6], LD
C1 (M2E) RT13 ? (LIC[6], GBPF, GPNG[4/4]) - LD
C2 GBPF
M6 Features
C1 GBPF, GB[4/4], GL[4/6], GT[5], GTP[5]
GGT[5], GGTP[5], GN[4], GNR[4], GIC[6]
C2 GB[4/4], GT[5], GTP[5], GGT[5], GGTP[5]
GN[4], GNR[4], GPNG[4/4], LD, LN[24/24]
C3 GB[4/4], GT[5], GTP[5], GGT[5], GGTP[5]
GN[4], GNR[4], GPNG[4/4], LD, LN[12/12]
C4 GBPF, GS[4/6], GT[5], GTP[5], GGT[5]
GGTP[5], GN[4], GNR[4], GIC[13]
C5 GBPF
C6 GBPF, LL[4/6], GPNG[4/4]
LN[6/6], LD, LIC[13]
M1 Features: RT13 C1 features ? C2 features
Figure 3: Features for each model. M2E is the
same as RT13 with extended features (bold fea-
tures in Figure 2). M6 is the structure with six
classifiers. Other abbreviations are described in
Figure 2.
work against the current best disfluency detection
method (QL13) on the JC04 split as well as on a
10 fold cross-validation of the parsed section of
the Switchboard. We use gold POS tags for all
evaluations.
For all of the joint parsing models we use the
weighted averaged Perceptron which is the same
as averaged Perceptron (Collins, 2002) but with a
50
loss weight of two for reparandum candidates as
done in prior work. The standard arc-eager parser
is first trained on a ?cleaned? Switchboard corpus
(i.e. after removing disfluent words) with 3 train-
ing iterations. Next, it is updated by training it on
the real corpus with 3 additional iterations. For
the other classifiers, we use the same number of
iterations determined from the development set.
Eval 1 The disfluency detection and parse re-
sults on the test set are shown in Table 1 for the
four systems (M1, RT13, M2E and M6). The joint
model performs poorly on the disfluency detection
task, with an F-score of 41.5, and the prior work
performance which serves as our baseline (RT13)
has a performance of 81.4. The extended version
of this model (M2E) raises performance substan-
tially to 82.2. This shows the utility of training the
C1 classifier with additional features. Finally, the
M6 classifier is the top performing model at 82.6.
Disfluency Parse
Model Pr. Rec. F1 UAS F1
M1 27.4 85.8 41.5 60.2 64.6
RT13 85.1 77.9 81.4 88.1 87.6
M2E 88.1 77.0 82.2 88.1 87.6
M6 87.7 78.1 82.6 88.4 87.7
Table 1: Comparison of joint parsing and disflu-
ency detection methods. UAS is the unlabeled
parse accuracy score.
The upperbound for the parser attachment ac-
curacy (UAS) is 90.2 which basically means that
if we have gold standard disfluencies and remove
disfluent words from the sentence and then parse
the sentence with a regular parser, the UAS will
be 90.2. If we had used the regular parser to parse
the disfluent sentences, the UAS for correct words
would be 70.7. As seen in Table 1, the best parser
UAS is 88.4 (M6) which is very close to the up-
perbound, however RT13, M2E and M6 are nearly
indistinguishable in terms of parser performance.
Eval 2 To compare against QL13, we use the
second version of the publicly provided code and
modify it so it uses gold POS tags and retrain and
optimize it for the parsed section of the Switch-
board corpus (these are known as mrg files, and
are a subset of the section of the Switchboard cor-
pus used in QL13, known as dps files). Since their
system has parameters tuned for the dps Switch-
board corpus we retrained it for a fair comparison.
As in the reimplementation of RT13, we have eval-
uated the QL13 system with optimal number of
training iterations (10 iterations). As seen in Table
2, although the annotation in the mrg files is less
precise than in the dps files, M6 outperforms all
models on the JC04 split thus showing the power
of the new features and new classifier structure.
Model JC04 split xval
RT13 81.4 81.6
QL13 (optimized) 82.5 82.2
M2E 82.2 82.8
M6 82.6 82.7
Table 2: Disfluency detection results (F1 score) on
JC04 split and with cross-validation (xval)
To test for robustness of our model, we per-
form 10-fold cross validation after clustering files
based on their name alphabetic order and creating
10 data splits. As seen in Table 2, the top model
is actually M2E, nudging out M6 by 0.1. More
noticeable is the difference in performance over
QL13 which is now 0.6.
Speed and memory usage Based on our Java
implementation on a 64-bit 3GHz Intel CPU with
68GB of memory, the speed for M6 (36 ms/sent)
is 3.5 times faster than M2E (128 ms/sent) and 5.2
times faster than M1 (184 ms/sent) and it requires
half of the nonzero features overall compared to
M2E and one-ninth compared to M1.
5 Conclusion and Future Directions
In this paper, we build on our prior work by in-
troducing rich and novel features to better handle
the detection of reparandum and by introducing an
improved classifier structure to decrease the uncer-
tainty in decision-making and to improve parser
speed and accuracy. We could use early updating
(Collins and Roark, 2004) for learning the greedy
parser which is shown to be useful in greedy pars-
ing (Huang and Sagae, 2010). K-beam parsing is a
way to improve the model though at the expense of
speed. The main problem with k-beam parsers is
that it is complicated to combine classifier scores
from different classifiers. One possible solution
is to modify the three actions to work on just one
word per action, thus the system will run in com-
pletely linear time with one classifier and k-beam
parsing can be done by choosing better features
for the joint parser. A model similar to this idea is
designed by Honnibal and Johnson (2014).
51
Acknowledgement We would like to thank the
reviewers for their comments and useful insights.
The bulk of this research was conducted while
both authors were working at Nuance Commu-
nication, Inc.?s Laboratory for Natural Language
Understanding in Sunnyvale, CA.
References
Jennifer E. Arnold, Maria Fagnano, and Michael K.
Tanenhaus. 2003. Disfluencies signal theee, um,
new information. Journal of Psycholinguistic Re-
search, 32(1):25?36.
Heather Bortfeld, Silvia D. Leon, Jonathan E. Bloom,
Michael F. Schober, and Susan E. Brennan. 2001.
Disfluency rates in conversation: Effects of age, re-
lationship, topic, role, and gender. Language and
Speech, 44(2):123?147.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain. Association for
Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics.
Kallirroi Georgila. 2009. Using integer linear pro-
gramming for detecting speech disfluencies. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Companion Volume: Short Papers, pages
109?112, Boulder, Colorado. Association for Com-
putational Linguistics.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP-92), volume 1, pages 517?520.
Matthew Honnibal and Mark Johnson. 2014. Joint in-
cremental disuency detection and dependency pars-
ing. Transactions of the Association for Computa-
tional Linguistics (TACL), to appear.
Matthew Honnibal, Yoav Goldberg, and Mark John-
son. 2013. A non-monotonic arc-eager transition
system for dependency parsing. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning, pages 163?172, Sofia, Bul-
garia. Association for Computational Linguistics.
Julian Hough and Matthew Purver. 2013. Modelling
expectation in the self-repair processing of annotat-,
um, listeners. In The 17th Workshop on the Seman-
tics and Pragmatics of Dialogue.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077?
1086, Uppsala, Sweden. Association for Computa-
tional Linguistics.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 33?39, Barcelona, Spain.
Matthew Lease and Mark Johnson. 2006. Early dele-
tion of fillers in processing conversational speech.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 73?76, New York City, USA.
Association for Computational Linguistics.
Sandra Merlo and Let?cia Lessa Mansur. 2004.
Descriptive discourse: topic familiarity and dis-
fluencies. Journal of Communication Disorders,
37(6):489?503.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50?57. Association
for Computational Linguistics.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Xian Qian and Yang Liu. 2013. Disfluency detection
using multi-step stacked learning. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 820?825, At-
lanta, Georgia. Association for Computational Lin-
guistics.
Mohammad Sadegh Rasooli and Joel Tetreault. 2013.
Joint parsing and disfluency detection in linear time.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
124?129, Seattle, Washington, USA. Association
for Computational Linguistics.
Wen Wang, Andreas Stolcke, Jiahong Yuan, and Mark
Liberman. 2013. A cross-language study on au-
tomatic speech disfluency detection. In Proceed-
ings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
703?708, Atlanta, Georgia. Association for Compu-
tational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
52
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA. Association for Computational Linguis-
tics.
Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A pro-
gressive feature selection algorithm for ultra large
feature spaces. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 561?568, Sydney, Aus-
tralia. Association for Computational Linguistics.
Simon Zwarts and Mark Johnson. 2011. The impact
of language models and loss functions on repair dis-
fluency detection. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
703?711, Portland, Oregon, USA. Association for
Computational Linguistics.
53
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 681?684,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Entity-Based Features to Model Coherence in Student Essays  
 
 
Jill Burstein 
Educational Testing Service 
Princeton, NJ 08541 
Joel Tetreault 
Educational Testing Service 
Princeton, NJ 08541 
Slava Andreyev 
Educational Testing Service 
Princeton, NJ 08541 
jburstein@ets.org  jtetreault@ets.org     sandreyev@ets.org 
 
 
Abstract 
 
We show how the Barzilay and Lapata entity-
based coherence algorithm (2008) can be 
applied to a new, noisy data domain ? student 
essays. We demonstrate that by combining 
Barzilay and Lapata?s entity-based features 
with novel features related to grammar errors 
and word usage, one can greatly improve the 
performance of automated coherence prediction 
for student essays for different populations.  
 
1 Introduction 
 
There is a small body of work that has investigated 
using NLP for the problem of identifying 
coherence in student essays.  For example, Foltz, 
Kintsch & Landauer (1998), and Higgins, Burstein, 
Marcu & Gentile (2004) have developed systems 
that examine coherence in student writing.  Foltz, 
et al (1998) systems measure lexical relatedness 
between text segments by using vector-based 
similarity between adjacent sentences; Higgins et 
al?s (2004) system computes similarity across text 
segments.  Foltz et al?s (1998) approach is in line 
with the earlier TextTiling method that identifies 
subtopic structure in text (Hearst, 1997). 
Miltsakaki and Kukich (2000) addressed essay 
coherence using Centering Theory (Grosz, Joshi & 
Weinstein, 1995).  More recently, Barzilay and 
Lapata?s (2008) approach (henceforth, BL08) used 
an entity-based representation to evaluate 
coherence. In BL08, entities (nouns and pronouns) 
are represented by their sentence roles in a text. 
The algorithm keeps track of the distribution of 
entity transitions between adjacent sentences, and 
computes a value for all transition types based on 
their proportion of occurrence in a text.  BL08 
apply their algorithm to three tasks, using well-
formed newspaper corpora: text ordering, summary 
coherence evaluation, and readability assessment.  
For each task, their system outperforms a Latent 
Semantic Analysis baseline.  In addition, best 
performance on each task is achieved using 
different system and feature configurations. Pitler 
& Nenkova (2008) applied BL08 to detect text 
coherence in well-formed texts.   
 Coherence quality is typically present in scoring 
criteria for evaluating a student?s essay. This paper 
focuses on the development of models to predict 
low-and high-coherence ratings for essays.  
Student essay data, unlike newspaper text, is 
typically noisy, especially when students are non-
native English speakers (NNES).  Here, we 
evaluate how BL08 algorithm features can be used 
to model coherence in a new, noisy data domain -- 
student essays.  We found that coherence can be 
best modeled by combining BL08 entity-based 
features with novel writing quality features. 
Further, our use of data sets from three different 
test-taker populations also shows that coherence 
models will differ across populations. Different 
populations might use language differently which 
could affect how coherence is presented. We 
expect to incorporate coherence ratings into e-
rater?, ETS?s automated essay scoring system 
(Attali & Burstein, 2006).   
 
2 Corpus and Annotation 
 
We collected approximately 800 essays (in total) 
across three data sets1: 1) adult, NNES test essays 
(TOEFL); 2) adult, native and NNES test essays; 
(GRE) 3) U.S. middle- and high-school native and 
NNES student essay submissions to Criterion?, 
ETS?s instructional writing application. 
Two annotators were trained to rate coherence 
quality based on how easily they could read an 
essay without stumbling on a coherence barrier 
(i.e., a confusing sentence(s)).  Annotators rated 
                                                 
1 TOEFL? is the Test of English as a Foreign Language, 
and GRE? is the Graduate Record Admissions Test. 
681
essays on a 3-point scale: 1) low coherence, 2) 
somewhat coherent, and 3) high coherence.  They 
were instructed to ignore grammar and spelling 
errors, unless they affected essay comprehension. 
During training, Kappa agreement statistics 
indicated that annotators had difficulty agreeing on 
the middle, somewhat coherent category. The 
annotation scale was therefore collapsed into a 2-
point scale: somewhat coherent and high 
coherence categories were collapsed into the high 
coherence class (H), and low-coherence (L) 
remained unchanged.  Two annotators labeled an 
overlapping set of about 100 essays to calculate 
inter-rater agreement; weighted Kappa was 0.677. 
 
3 System  
 
3.1 BL08 Algorithm 
 
We implemented BL08?s entity-based algorithm to 
build and evaluate coherence models for the essay 
data. In short, the algorithm generates a vector of 
entity transition probabilities for documents 
(essays, here). Vectors are used to build coherence 
models.  The first step in the algorithm is to 
construct an entity grid in which all entities (nouns 
and pronouns) are represented by their roles (i.e., 
Subject (S), Object (O), Other (X)). Entity roles 
are then used to generate entity transitions ? the 
role transitions across adjacent sentences (e.g., 
Subject-to-Object, Object-to-Object). Entity 
transition probabilities are the proportions of 
different entity transition types within a text. The 
probability values are used then used as features to 
build a coherence model.  
Entity roles can be represented in the following 
ways. In this study, consistent with BL08, different 
combinations are applied and reported (see Tables 
2-4).  Entities can be represented in grids with 
specified roles (Syntax+) (S,O,X). Alternatively, 
roles can be reduced to show only the presence and 
absence of an entity (Syntax-) (i.e., Entity Present 
(P) or Not (N). Co-referential entities can be 
resolved (Coreference+) or not (Coreference-).  
Finally, the Salience option reflects the frequency 
with which an entity appears in the discourse: if 
the entity is mentioned two or more times, it is 
salient (Salient+), otherwise, not (Salient-).  
Consistent with BL08, we systematically 
completed runs using various configurations of 
entity representations (see Section 4).  
Given the combination, the entity transition 
probabilities were computed for all labeled essays 
in each data set. We used n-fold cross-validation 
for evaluation. Feature vectors were input to C5.0, 
a decision-tree machine learning application.  
 
3.2 Additional Features 
 
In BL08, augmenting the core coherence features 
with additional features improved the power of the 
algorithm.  We extended the feature set with 
writing quality features (Table 1).  GUMS features 
describe the technical quality of the essay.  The 
motivation for type/token features (*_TT) is to 
measure word variety.  For example, a high 
probability for a ?Subject-to-Subject? transition 
indicates that the writer is repeating an entity in 
Subject position across adjacent sentences. 
However, this does not take into account whether 
the same word is repeated or a variety of words are 
used.  The {S,O,X,SOX}_TT (type/token) features 
uncover the actual words collapsed into the entity 
transition probabilities. Shell nouns (Atkas & 
Cortes, 2008), common in essay writing, might 
also affect coherence. 
NNES essays can contain many spelling errors.  
We evaluated the impact of a context-sensitive 
spell checker (SPCR+), as spelling variation will 
affect the transition probabilities in the entity grid.  
Finally, we experimented with a majority vote 
method that combined the best performing feature 
combinations.  
 
4 Evaluation 
 
For all experiments, we used a series of n-fold 
cross-validation runs with C5.0 to evaluate 
performance for numerous feature configurations.  
In Tables 2, 3 and 4, we report: baselines, results 
on our data with BL08?s best system configuration 
from the summary coherence evaluation task 
(closest to our task), and our best systems. In the 
Tables, ?best systems? combined feature sets and 
outperformed baselines.   Rows in bold indicate 
final independent best systems that contribute to 
best performance in the majority vote method.  
Agreement is reported as Weighted Kappa (WK), 
Precision (P), Recall (R) and F-measure (F).  
Baselines. We implemented three non-trivial 
baseline systems. E-rater indicates use of the full  
682
feature set from e-rater. The GUMS (GUMS+) 
feature baseline, uses the Grammar (G+), Usage 
 
Feature Descriptor Feature Description 
GUMS  Grammar, usage, and 
mechanics errors, and style 
features from an AES system 
S_TT 
O_TT 
X_TT 
SOX_TT2 
P_TT 
Type/token ratios for actual 
words recovered from the 
entity grid, using the entity 
roles.  
S_TT_Shellnouns 
O_TT_Shellnouns 
X_TT_Shellnouns 
Type/token ratio of non-topic  
content, shell nouns (e.g., 
approach, aspect, challenge) 
Table 1: New feature category description 
 
 (U+), Mechanics (M+), and Style (ST+) flags 
(subset of e-rater features) to evaluate a coherence 
model.  The third baseline represents the best run 
using type/token features ({S,O,X,SOX}_TT), and 
{S,O,X}_TT_Shellnouns feature sets (Table 1).  
The baseline majority voting system includes e-
rater, GUMS, and the best performing type/token 
baseline (see Tables 2-4).  
Extended System.   We combined our writing 
quality features with the core BL08 feature set. 
The combination improved performance over the 
three baselines, and over the best performing BL08 
feature.  Type/token features added to BL08 entity 
transitions probabilities improved performance of 
all single systems. This supports the need to 
recover actual word use.  In Table 2, for TOEFL 
data, spell correction improved performance with 
the Mechanics error feature (where Spelling is 
evaluated). This would suggest that annotators 
were trying to ignore spelling errors when labeling 
coherence. In Table 3, for GRE data, spell 
correction improved performance with the 
Grammar error feature. Spell correction did 
change grammar errors detected: annotators may 
have self-corrected for grammar. Finally, the 
majority vote outperformed all systems. In Tables 
3 and 4, Kappa was comparable to human 
agreement (K=0.677). 
 
5 Conclusions and Future Work 
We have evaluated how the BL08 algorithm 
features can be used to model coherence for 
                                                 
2 Indicates an aggregate feature that computes the type/token 
ratio for entities that appear in any of S,O,X role. 
student essays across three different populations.   
We found that the best coherence models for 
essays are built by combining BL08 entity-based 
features with writing quality features. BL08?s 
outcomes showed that optimal performance was 
obtained by using different feature sets for 
different tasks. Our task was most similar to 
BL08?s summary coherence task, but we used 
noisy essay data. The difference in the data types 
might also explain the need for our systems to 
include additional writing quality features. 
Our majority vote method outperformed three 
baselines (and a baseline majority vote). For two of 
the populations, Weighted Kappa between system 
and human agreement was comparable. These 
results show promise toward development of an 
entity-based method that produces reliable 
coherence ratings for noisy essay data. We plan to 
evaluate this method on additional data sets, and in 
the context of automated essay scoring. 
 
References 
 
Aktas, R. N., & Cortes, V. (2008). Shell nouns as 
cohesive devices in published and ESL  student 
writing. Journal of English for Academic Purposes, 
7(1), 3?14. 
Attali, Y., & Burstein, J. (2006). Automated essay 
scoring with e-rater v.2.0 . Journal of Technology, 
Learning, and Assessment, 4(3). 
Barzilay, R. and Lapata, M. (2008). Modeling local 
coherence: An entity-based approach. 
Computational Linguistics, 34(1), 1-34. 
Foltz, P., Kintsch, W., and Landauer, T. K. (1998). The 
measurement of textual coherence with Latent 
Semantic Analysis. Discourse Processes, 
25(2&3):285?307. 
Higgins, D., Burstein, J., Marcu, D., & Gentile, C. 
(2004).  Evaluating multiple aspects of coherence in 
student essays . In Proceedings of HLT-NAACL 
2004, Boston, MA. 
Grosz, B., Joshi, A., and Weinstein, S.  1995, Centering: 
A framework for modeling the local coherence of 
discourse.  Computational Linguistics, 21(2): 203-
226. 
Hearst, M. A. (1997). TextTiling: Segmenting text into 
multi-paragraph subtopic passages. Computational 
Linguistics, 23(1):33?6 
Miltsakaki, E. and Kukich, K. (2000). Automated 
evaluation of coherence in student essays. In 
Proceedings of LREC 2000, Athens, Greece 
Pitler, E.,and Nenkova, A (2008). Revisiting 
Readability: A Unified Framework for Predicting 
683
Text Quality. In Proceedings of EMNLP 2008, 
Honolulu, Hawaii. 
 
 
 
 
  L (n=64) H  (n=196) L+H (n=260) 
BASELINES: NO BL08  FEATURES WK P R F P R F P R F 
(a) E-rater 0.472 56 69 62 89 82 86 79 79 79 
(b) GUMS 0.455 55 66 60 88 83 85 79 79 79 
(c)  SOX_TT3  0.484 66 55 60 86 91 88 82 82 82 
SYSTEMS: Includes BL08  FEATURES  
Coreference-Syntax+Salient+ (B&L08 
summary task configuration) 
0.253 49 34 40 81 88 84 75 75 75 
(d) Coreference-Syntax-Salient-SPCR+M+ 0.472 76 45 57 84 95 90 83 83 83 
(e) Coreference+Syntax+Salient-GUMS+ 0.590 68 70 69 90 89 90 85 85 85 
(f) Coreference+Syntax+Salient-
GUMS+O_TT_Shellnouns+ 
0.595 68 72 70 91 89 90 85 85 85 
Baseline Majority vote: (a),(b), (c) 0.450 55 64 59 88 83 85 79 79 79 
Majority vote:  (d), (e), (f) 0.598 69 70 70 90 90 90 85 85 85 
 
Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement  
 
  L (n=48) H (n=210) L+H (n=258) 
BASELINES: NO BL08  FEATURES WK P R F P R F P R F 
(a) E-rater 0.383 79 31 45 86 98 92 86 86 86 
(b) GUMS 0.316 68 27 39 85 97 91 84 84 84 
(c)  e-rater+SOX_TT4  0.359 78 29 42 86 98 92 85 85 85 
SYSTEMS: INCLUDES BL08  FEATURES  
Coreference-Syntax+Salient+ (BL08 summary 
task configuration) 
0.120 35 17 23 83 93 88 79 79 79 
(d) Coreference+Syntax+Salient-SPCR+G+ 0.547 1.0 43 60 89 1.0 94 90 90 90 
(e) Coreference+Syntax-Salient-P_TT+ 0.462 70 44 54 88 96 92 86 86 86 
(f) Coreference+Syntax+Salient+GUMS+ 
SOX_TT+ 
0.580 71 60 65 91 94 93 88 88 88 
Baseline Majority vote: (a),(b), (c) 0.383 79 31 45 86 98 92 86 86 86 
Majority vote: (d), (e), (f) 0.610 1.0 49 66 90 1.0 95 91 91 91 
 
Table 3: Native and Non-Native English Speaker Test-taker Data (GRE): Annotator/System Agreement  
 
  L (n=37) H  (n=226) L+H (n=263) 
BASELINES: NO BL08  FEATURES WK P R F P R F P R F 
(a) E-rater 0.315 39 46 42 91 88 89 82 82 82 
(b) GUMS 0.350 47 41 43 90 92 91 85 85 85 
(c)  SOX_TT 0.263 78 19 30 88 99 93 88 88 88 
SYSTEMS: INCLUDES BL08  FEATURES  
(d) Coreference-Syntax+Salient+ (BL08 
summary task configuration) 
0.383 79 30 43 90 99 94 89 89 89 
(e) Coreference-Syntax-Salient-SPCR+ 0.424 67 38 48 90 97 94 89 89 89 
(f) Coreference+Syntax+Salient+S_TT+ 0.439 65 41 50 91 96 94 89 89 89 
Baseline Majority vote: (a),(b), (c) 0.324 43 41 42 90 91 91 84 84 84 
Majority vote: (d), (e), (f) 0.471 82 38 52 91 99 94 90 90 90 
Table 4:  Criterion Essay Data: Annotator/System Agreement   
                                                 
3 Type/token ratios from all roles using a Coreference+Syntax+Salient+ grid. 
4 Type/token ratios from all roles using Coreference+Syntax+Salient- grid. 
684
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20?28,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Identifying High-Level Organizational Elements
in Argumentative Discourse
Nitin Madnani Michael Heilman Joel Tetreault
Educational Testing Service
Princeton, NJ, USA
{nmadnani,mheilman,jtetreault}@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
Argumentative discourse contains not only
language expressing claims and evidence, but
also language used to organize these claims
and pieces of evidence. Differentiating be-
tween the two may be useful for many appli-
cations, such as those that focus on the content
(e.g., relation extraction) of arguments and
those that focus on the structure of arguments
(e.g., automated essay scoring). We propose
an automated approach to detecting high-level
organizational elements in argumentative dis-
course that combines a rule-based system and
a probabilistic sequence model in a principled
manner. We present quantitative results on a
dataset of human-annotated persuasive essays,
and qualitative analyses of performance on es-
says and on political debates.
1 Introduction
When presenting an argument, a writer or speaker
usually cannot simply state a list of claims and
pieces of evidence. Instead, the arguer must explic-
itly structure those claims and pieces of evidence, as
well as explain how they relate to an opponent?s ar-
gument. Consider example 1 below, adapted from
an essay rebutting an opponent?s argument that griz-
zly bears lived in a specific region of Canada.
The argument states that based on the
result of the recent research, there proba-
bly were grizzly bears in Labrador. It may
seem reasonable at first glance, but ac-
tually, there are some logical mistakes
in it. . . . There is a possibility that they
were a third kind of bear apart from black
and grizzly bears. Also, the explorer ac-
counts were recorded in the nineteenth
century, which was more than 100 years
ago. . . . In sum, the conclusion of this
argument is not reasonable since the ac-
count and the research are not convinc-
ing enough. . . .
The argument begins by explicitly restating the
opponent?s claim, prefacing the claim with the
phrase ?The argument states that.? Then, the sec-
ond sentence explicitly marks the opponent?s argu-
ment as flawed. Later on, the phrase ?There is a
possibility that? indicates the subsequent clause in-
troduces evidence contrary to the opponent?s claim.
Finally, the sentence ?In sum, . . .? sums up the ar-
guer?s stance in relation to the opponent?s claim.1
As illustrated in the above example, argumenta-
tive discourse can be viewed as consisting of lan-
guage used to express claims and evidence, and
language used to organize them. We believe that
differentiating organizational elements from content
would be useful for analyzing persuasive discourse.
1The word Also signals that additional evidence is about to
be presented and should also be marked as shell. However, it
was not marked in this specific case by our human annotator
(?3.2).
20
We refer to such organizational elements as shell, in-
dicating that they differ from the specific claims and
evidence, or ?meat,? of an argument. In this work,
we develop techniques for detecting shell in texts.
We envision potential applications in political sci-
ence (e.g., to better understand political debates), in-
formation extraction or retrieval (e.g., to help a sys-
tem focus on content rather than organization), and
automated essay scoring (e.g., to analyze the quality
of a test-taker?s argument), though additional work
is needed to determine exactly how to integrate our
approach into such applications.
Detecting organizational elements could also be a
first step in parsing an argument to infer its structure.
We focus on this initial step, leaving the other steps
of categorization of spans (as to whether they evalu-
ate the opponent?s claims, connect one?s own claims,
etc.), and the inference of argumentation structure to
future work.
Before describing our approach to identifying
shell, we begin by defining it. Shell refers to se-
quences of words used to refer to claims and evi-
dence in persuasive writing or speaking, providing
an organizational framework for an argument. It
may be used by the writer or the speaker in the fol-
lowing ways:
? to declare one?s own claims (e.g., ?There is the
possibility that?)
? to restate an opponent?s claims (e.g., ?The argu-
ment states that?)
? to evaluate an opponent?s claims (e.g., ?It may
seem reasonable at first glance, but actually, there
are some logical mistakes in it?)
? to present evidence and relate it to specific claims
(e.g., ?To illustrate my point, I will now give the
example of?)
There are many ways of analyzing discourse. The
most relevant is perhaps rhetorical structure theory
(RST) (Mann and Thompson, 1988). To our knowl-
edge, the RST parser from Marcu (2000) is the only
RST parser readily available for experimentation.
The parser is trained to model the RST corpus (Carl-
son et al, 2001), which treats complete clauses (i.e.,
clauses with their obligatory complements) as the el-
ementary units of analysis. Thus, the parser treats
the first sentence in example 1 as a single unit and
does not differentiate between the main and subordi-
nate clauses. In contrast, our approach distinguishes
the sequence ?The argument states that . . . ? as shell
(which is used here to restate the external claim).
Furthermore, we identify the entire second sentence
as shell (here, used to evaluate the external claim),
whereas the RST parser splits the sentence into two
clauses, ?It may seem . . .? and ?but actually . . .?,
linked by a ?contrast? relationship.2 Finally, our
approach focuses on explicit markers of organiza-
tional structure in arguments, whereas RST covers a
broader range of discourse connections (e.g., elabo-
ration, background information, etc.), including im-
plicit ones. (Note that additional related work is de-
scribed in ?6.)
This work makes the following contributions:
? We describe a principled approach to the task
of detecting high-level organizational elements in
argumentative discourse, combining rules and a
probabilistic sequence model (?2).
? We conduct experiments to validate the approach
on an annotated sample of essays (?3, ?4).
? We qualitatively explore how the approach per-
forms in a new domain: political debate (?5).
2 Detection Methods
In this section, we describe three approaches to the
problem of shell detection: a rule-based system
(?2.1), a supervised probabilistic sequence model
(?2.2), and a simple lexical baseline (?2.3).
2.1 Rule-based system
We begin by describing a knowledge-based ap-
proach to detecting organizational elements in argu-
mentative discourse. This approach uses a set of 25
hand-written regular expression patterns.3
In order to develop these patterns, we created a
sample of 170 annotated essays across 57 distinct
prompts.4 The essays were written by test-takers of
a standardized test for graduate admissions. This
sample of essays was similar in nature to but did
not overlap with those discussed in other sections
2We used the RST parser of Marcu (2000) to analyze the
original essay from which the example was adapted.
3We use the PyParsing toolkit to parse sentences with the
grammar for the rule system.
4Prompts are short texts that present an argument or issue
and ask test takers to respond to it, either by analyzing the given
argument or taking a stance on the given issue.
21
MODAL? do | don?t | can | cannot | will | would | . . .
ADVERB? strongly | totally | fundamentally | vehemently | . . .
AGREEVERB? disagree | agree | concur | . . .
AUTHORNOUN? writer | author | speaker | . . .
SHELL? I [MODAL] [ADVERB] AGREEVERB with the AUTHORNOUN
Figure 1: An example pattern that recognizes shell language describing the author?s position with respect to an oppo-
nent?s, e.g., I totally agree with the author or I will strongly disagree with the speaker.
of the paper (?2.2, ?3.2). The annotations were car-
ried out by individuals experienced in scoring per-
suasive writing. No formal annotation guidelines
were provided. Besides shell language, there were
other annotations relevant to essay scoring. How-
ever, we ignored them for this study because they
are not directly relevant to the task of shell language
detection.
From this sample, we computed lists of n-grams
(n = 1, 2, . . . , 9) that occurred more than once in
essays from at least half of the 57 distinct essay
prompts. We then wrote rules to recognize the shell
language present in the n-gram lists. Additional
rules were added to cover instances of shell that we
observed in the annotated essays but that were not
frequent enough to appear in the n-gram analysis.
We use ?Rules? to refer to this method.
2.2 Supervised Sequence Model
The next approach we describe is a supervised, prob-
abilistic sequence model based on conditional ran-
dom fields (CRFs) (Lafferty et al, 2001), using a
small number of general features based on lexical
frequencies. We assume access to a labeled dataset
of N examples (w,y) indexed by i, containing se-
quences of words w(i) and sequences of labels y(i),
with individual words and labels indexed by j (?3
describes our development and testing sets). y(i) is a
sequence of binary values, indicating whether each
word w(i)j in the sequence is shell (y
(i)
j = 1) or not
(y(i)j = 0). Following Lafferty et al (2001), we find
a parameter vector ? that maximizes the following
log-likelihood objective function:
L(?|w,y) =
N?
i=1
log p
(
y(i) | w(i), ?
)
(1)
=
N?
i=1
(
?>f(w(i), y(i))? logZ(i)
)
The normalization constant Zi is a sum over all
possible label sequences for the ith example, and f
is a feature function that takes pairs of word and la-
bel sequences and returns a vector of feature values,
equal in dimensions to the number of parameters in
?.5
The feature values for the jth word and label pair
are as follows (these are summed over all elements
to compute the values of f for the entire sequence):
? The relative frequency of w(i)j in the British Na-
tional Corpus.
? The relative frequency of w(i)j in a set of 100,000
essays (see below).
? Eight binary features for whether the above fre-
quencies meet or exceed the following thresholds:
10{?6,?5,?4,?3}.
? The proportion of prompts for which w(i)j ap-
peared in at least one essay about that prompt in
the set of 100,000.
? Three binary features for whether the above pro-
portion of prompts meets or exceeds the following
thresholds: {0.25, 0.50, 0.75}.
? A binary feature with value 1 if w(i)j consists only
of letters a-z, and 0 otherwise. This feature dis-
tinguishes punctuation and numbers from other to-
kens.
5We used CRFsuite 0.12 (Okazaki, 2007) to implement the
CRF model.
22
? A binary feature with value 1 if the rule-based sys-
tem predicts that w(i)j is shell, and 0 otherwise.
? A binary feature with value 1 if the rule-based sys-
tem predicts that w(i)j?1 is shell, and 0 otherwise.
? Two binary features for whether or not the current
token was the first or last in the sentence, respec-
tively.
? Four binary features for the possible transitions
between previous and current labels (y(i)j and y
(i)
j?1,
respectively).
To define the features related to essay prompts
and lexical frequencies in essays, we created a set
of 100,000 essays from a larger set of essays written
by test-takers of a standardized test for graduate ad-
missions (the same domain as in ?2.1). The essays
were written in response to 228 different prompts
that asked students to analyze various issues or ar-
guments. We use additional essays sampled from
this source later to acquire annotated training and
test data (?3.2).
We developed the above feature set using cross-
validation on our development set (?3). The intu-
ition behind developing the word frequency features
is that shell language generally consists of chunks of
words that occur frequently in persuasive language
(e.g., ?claims,? ?conclude?) but not necessarily as
frequently in general text (e.g., the BNC). The se-
quence model can also learn to disprefer changes of
state, such that multi-word subsequences are labeled
as shell even though some of the individual words in
the subsequence are stop words, punctuation, etc.
Note there are a relatively small number of pa-
rameters in the model,6 which allows us to estimate
parameters on a relatively small set of labeled data.
We briefly experimented with adding an `2 penalty
on the magnitude of ? in Equation 2, but this did not
seem to improve performance.
When making predictions y?(i) about the label se-
quence for a new sentence, the most common ap-
proach is to find the most likely sequence of labels y
given the words w(i), found with Viterbi decoding:
6There were 42 parameters in our implementation of the full
CRF model. Excluding the four transition features, each of the
19 features had two parameters, one for the positive class and
one for the negative class. Having two parameters for each is
unnecessary, but we are not aware of how to have the crfsuite
toolkit avoid these extra features.
y?(i) = argmax
y
p?(y | w
(i)) (2)
We use ?CRFv? to refer to this approach. We use
the suffix ?+R? to denote models that include the
two rule-based system prediction features, and we
use ?-R? to denote models that exclude these two
features.
In development, we observed that this decoding
approach seemed to very strongly prefer labeling an
entire sentence as shell or not, which is often not
desirable since shell often appears at just the begin-
nings of sentences (e.g., ?The argument states that?).
We therefore test an alternative prediction rule
that works at the word-level, rather than sequence-
level. This approach labels each word as shell if
the sum of the probabilities of all paths in which
the word was labeled as shell?that is, the marginal
probability?exceeds some threshold ?. Words are
labeled as non-shell otherwise. Specifically, an indi-
vidual word w(i)j is labeled as shell (i.e., y?
(i)
j = 1)
according to the following equation, where 1(q) is
an indicator function that returns 1 if its argument q
is true, and 0 otherwise.
y?(i)j = 1
((
?
y
p?(y | w
(i)) yj
)
? ?
)
(3)
We tune ? using the development set, as discussed
in ?3.
We use ?CRFm? to refer to this approach.
2.3 Lexical Baseline
As a simple baseline, we also evaluated a method
that labels words as shell if they appear frequently
in persuasive writing?specifically, in the set of
100,000 unannotated essays described in ?2.2. In
this approach, word tokens are marked as shell
if they belonged to the set of k most frequent
words from the essays. Using the development
set discussed in ?3.2, we tested values of k in
{100, 200, . . . , 1000}. Setting k = 700 led to the
highest F1.
We use ?TopWords? to refer to this method.
23
3 Experiments
In this section, we discuss the design of our exper-
imental evaluation and present results on our devel-
opment set, which we used to select the final meth-
ods to evaluate on the held-out test set.
3.1 Metrics
In our experiments, we evaluated the performance
of the shell detection methods by comparing token-
level system predictions to human labels. Shell lan-
guage typically occurs as fairly long sequences of
words, but identifying the exact span of a sequence
of shell seems less important than in related tag-
ging tasks, such as named entity recognition. There-
fore, rather than evaluating based on spans (either
with exact or a partial credit system), we measured
performance at the word token-level using standard
metrics: precision, recall, and the F1 measure. For
example, for precision, we computed the propor-
tion of tokens predicted as shell by a system that
were also labeled as shell in our human-annotated
datasets.
3.2 Annotated Data
To evaluate the methods described in ?2, we gath-
ered annotations for 200 essays that were not in the
larger, unannotated set discussed in ?2.2. We split
this set of essays into a development set of 150 es-
says (68,601 word tokens) and a held-out test set of
50 essays (21,277 word tokens). An individual with
extensive experience at scoring persuasive writing
and familiarity with shell language annotated all to-
kens in the essays with judgments of whether they
were shell or not (in contrast to ?2.1, this annotation
only involved labeling shell language).
From the first annotator?s judgments on the devel-
opment set, we created a set of annotation guidelines
and trained a second annotator. The second anno-
tator marked the held-out test set so that we could
measure human agreement. Comparing the two an-
notators? test set annotations, we observed agree-
ment of F1 = 0.736 and Cohen?s ? = 0.699 (we
do not use ? in our experiments but report it here
since it is a common measure of human agreement).
Except for measuring agreement, we did not use the
second annotator?s judgments in our experiments.7
7In the version of this paper submitted for review, we mea-
recall
pre
cisi
on
0.0
0.2
0.4
0.6
0.8
1.0
l
0.0 0.2 0.4 0.6 0.8 1.0
linesCRFm?RCRFm+R
points
l CRFm?RCRFm+RCRFv?RCRFv+RRulesTopWords
Figure 2: Precision and recall of the detection methods at
various thresholds, computed through cross-validation on
the development set. Points indicate performance for the
rule-based and baseline system as well as points where
F1 is highest.
3.3 Cross-validation Results
To develop the CRF?s feature set, to tune hyperpa-
rameters, and to select the most promising systems
to evaluate on the test set, we randomly split the sen-
tences from the development set into two halves and
conducted tests with two-fold cross-validation.
We tested thresholds for the CRF at ? =
{0.01, 0.02, . . . , 1.00}.
Figure 2 shows the results on the development set.
For the rule-based system, which did not require la-
beled data, performance is computed on the entire
development set. For the CRF approaches, the pre-
cision and recall were computed after concatenating
predictions on each of the cross-validation folds.
The TopWords baseline performed quite poorly,
with F1 = 0.205. The rule-based system performed
much better, with F1 = 0.382, but still not as well
as the CRF systems. The CRF systems that pre-
dict maximum sequences had F1 = 0.382 without
the rule-based system features (CRFv?R), and F1 =
0.467 with the rule-based features (CRFv+R). The
CRF systems that made predictions from marginal
scores performed best, with F1 = 0.516 without
the rule-based features, and F1 = 0.551 with the
rule-based features. Thus, both the rule-based sys-
sured test set agreement with judgments from a third individ-
ual, who was informally trained by the first, without the formal
guidelines. Agreement was somewhat lower: F1 = 0.668 and
? = 0.613.
24
Method P R F1 Len
TopWords 0.125 0.759 0.214 ? 2.80
Rules 0.561 0.360 0.439 ? 4.99
CRFv?R 0.729 0.268 0.392 ? 15.67
CRFv+R 0.763 0.369 0.498 ? 13.30
CRFm?R 0.586 0.574 0.580 9.00
CRFm+R 0.556 0.670 0.607 9.96
Human 0.685 0.796 0.736 ? 7.91
Table 1: Performance on the held-out test set, in terms of
precision (P), recall (R), F1 measure, and average length
in tokens of sequences of one or more words labeled as
shell (Len). ? indicates F1 scores that are statistically
reliably different from CRFm+R at the p < 0.01 level.
tem features and the marginal prediction approach
led to gains in performance.
From an examination of the predictions from the
CRFm+R and CRFm?R systems, it appears that a
major contribution of the features derived from the
rule-based system is to help the hybrid CRFm+R
system avoid tagging entire sentences as shell when
only parts of them are actually shell. For exam-
ple, consider the sentence ?According to this state-
ment, the speaker asserts that technology can not
only influence but also determine social customs and
ethics? (typographical errors included). CRFm?R
tags everything up to ?determine? as shell, whereas
the rule-based system and CRFm+R correctly stop
after ?asserts that.?
4 Test Set Results
Next, we present results on the held-out test set.
For the CRFm systems, we used the thresholds that
led to the highest F1 scores on the development
set (? = 0.26 for CRFm+R and ? = 0.32 for
CRFm?R). Table 1 presents the results for all sys-
tems, along with results comparing the second anno-
tator?s labels (?Human?) to the gold standard labels
from the first annotator.
The same pattern emerged as on the development
set, with CRFm+R performing the best. The F1
score of 0.607 for the CRFm+R system was rel-
atively close to the F1 score of 0.736 for agree-
ment between human annotators. To test whether
CRFm+R?s relatively high performance was due to
chance, we computed 99% confidence intervals for
the differences in F1 score between CRFm+R and
each of the other methods. We used the bias-
corrected and accelerated (BCa) Bootstrap (Efron
and Tibshirani, 1993) with 10,000 rounds of resam-
pling at the sentence level for each comparison. A
difference is statistically reliable at the ? level (i.e.,
p < ?) if the (1 ? ?)% confidence interval for the
difference does not contain zero, which corresponds
to the null hypothesis. Statistically reliable differ-
ences are indicated in Table 1. The only system that
did not have a reliably lower F1 score than CRFm+R
was CRFm?R, though due to the relatively small
size of our test set, we do not take this as strong ev-
idence against using the rule-based system features
in the CRF.
We note that while the CRFm+R system had lower
precision (0.556) than the CRFv+R system (0.763),
its threshold ? could be tuned to prefer high preci-
sion rather than the best development set F1. Such
tuning could be very important depending on the rel-
ative costs of false positives and false negatives for
a particular application.
We also computed the mean length of sequences
of one or more contiguous words labeled as shell.
Here also, we observed that the CRFm+R approach
provided a close match to human performance. The
mean lengths of shell for the first and second anno-
tators were 8.49 and 7.91 tokens, respectively. For
the CRFm+R approach, the mean length was slightly
higher at 9.96 tokens, but this was much closer to the
means of the human annotators than the mean for
the CRFv+R system, which was 13.30 tokens. For
the rule-based system, the mean length was 4.99 to-
kens, indicating that it captures short sequences such
as ?In addition,? more often than the other systems.
5 Observations about a New Domain
In this section, we apply our system to a corpus of
transcripts of political debates8 in order to under-
stand whether the system can generalize to a new
domain with a somewhat different style of argu-
mentation. Our analyses are primarily qualitative
in nature due to the lack of gold-standard annota-
tions. We chose two historically well-known debates
8The Lincoln?Douglas debates were downloaded from
http://www.bartleby.com/251/. The other debates
were downloaded from http://debates.org/.
25
(Lincoln?Douglas from 1858 and Kennedy?Nixon
from 1960) and two debates that occurred more re-
cently (Gore?Bush from 2000 and Obama?McCain
from 2008). These debates range in length from
38,000 word tokens to 65,000 word tokens.
Political debates are similar to the persuasive es-
says we used above in that debate participants state
their own claims and evidence as well as evaluate
their opponents? claims. They are different from es-
says in that they are spoken rather than written?
meaning that they contain more disfluencies, collo-
quial language, etc.?and that they cover different
social and economic issues. Also, the debates are in
some sense a dialogue between two people.
We tagged all the debates using the CRFm+R sys-
tem, using the same parameters as for the test set
experiments (?4).
First, we observed that a smaller percentage of
tokens were tagged as shell in the debates than in
the essays. For the annotated essay test set (?3.2),
the percentage of tokens tagged as shell was 14.0%
(11.6% were labeled as shell by the first annota-
tor). In contrast, the percentage of tokens tagged
as shell was 4.2% for Lincoln?Douglas, 5.4% for
Kennedy?Nixon, 4.6% for Gore?Bush, and 4.8% for
Obama?McCain. It is not completely clear whether
the smaller percentages tagged as shell are due to a
lack of coverage by the shell detector or more sub-
stantial differences in the domain.
However, it seems that these debates genuinely in-
clude less shell. One potential reason is that many of
the essay prompts asked test-takers to respond to a
particular argument, leading to responses containing
many phrases such as ?The speaker claims that? and
?However, the argument lacks specificity . . . ?.
We analyzed the system?s predictions and ex-
tracted a set of examples, some of which appear in
Table 2, showing true positives, where most of the
tokens appear to be labeled correctly as shell; false
positives, where tokens were incorrectly labeled as
shell; and false negatives, where the system missed
tokens that should have been marked.
Table 2 also provides some examples from our de-
velopment set, for comparison.
We observed many instances of correctly marked
shell, including many that appeared very different
in style than the language used in essays. For ex-
ample, Lincoln demonstrates an aggressive style in
the following: ?Now, I say that there is no charitable
way to look at that statement, except to conclude that
he is actually crazy.? Also, Bush employs a some-
what atypical sentence structure here: ?It?s not what
I think and its not my intentions and not my plan.?
However, the system also incorrectly tagged se-
quences as shell, particularly in short sentences (e.g.,
?Are we as strong as we should be??). It also missed
shell, partially or entirely, such as in the following
example: ?But let?s get back to the core issue here.?
These results suggest that although there is poten-
tial for improvement in adapting to new domains,
our approach to shell detection at least partially gen-
eralizes beyond our initial domain of persuasive es-
say writing.
6 Related Work
There has been much previous work on analyzing
discourse. In this section, we describe similarities
and differences between that work and ours.
Rhetorical structure theory (Mann and Thomp-
son, 1988) is perhaps the most relevant area of work.
See ?1 for a discussion.
In research on intentional structure, Grosz and
Sidner (1986) propose that any discourse is com-
posed of three interacting components: the linguistic
structure defined by the actual utterances, the inten-
tional structure defined by the purposes underlying
the discourse, and an attentional structure defined by
the discourse participants? focus of attention. De-
tecting shell may also be seen as trying to identify
explicit cues of intentional structure in a discourse.
Additionally, the categorization of shell spans as to
whether they evaluate the opponents claims, connect
ones own claims, etc., may be seen as determining
what Grosz and Sidener call ?discourse segment pur-
poses? (i.e., the intentions underlying the segments
containing the shell spans).
We can also view shell detection as the task of
identifying phrases that indicate certain types of
speech acts (Searle, 1975). In particular, we aim to
identify markers of assertive speech acts, which de-
clare that the speaker believes a certain proposition,
and expressive speech acts, which express attitudes
toward propositions.
Shell also overlaps with the concept of discourse
markers (Hutchinson, 2004), such as ?however? or
26
LINCOLN (L) ? DOUGLAS (D) DEBATES
TP L: Now, I say that there is no charitable way to look at that statement, except to conclude that he is
actually crazy.
L: The first thing I see fit to notice is the fact that . . .
FP D: He became noted as the author of the scheme to . . .
D: . . . such amendments were to be made to it as would render it useless and inefficient . . .
FN D: I wish to impress it upon you, that every man who voted for those resolutions . . .
L: That statement he makes, too, in the teeth of the knowledge that I had made the stipulation to
come down here . . .
KENNEDY (K) ? NIXON (N) DEBATES
TP N: I favor that because I believe that?s the best way to aid our schools . . .
N: And in our case, I do believe that our programs will stimulate the creative energies of . . .
FP N: We are for programs, in addition, which will see that our medical care for the aged . . .
K: Are we as strong as we should be?
FN K: I should make it clear that I do not think we?re doing enough . . .
N: Why did Senator Kennedy take that position then? Why do I take it now?
BUSH (B) ? GORE (G) DEBATES
TP B: It?s not what I think and its not my intentions and not my plan.
G: And FEMA has been a major flagship project of our reinventing government efforts. And I agree, it
works extremely well now.
FP B: First of all, most of this is at the state level.
G: And it focuses not only on increasing the supply, which I agree we have to do, but also on . . .
FN B: My opponent thinks the government?the surplus is the government?s money. That?s not what I
think
G: I strongly support local control, so does Governor Bush.
OBAMA (O) ? MCCAIN (M) DEBATES
TP M: But the point is?the point is, we have finally seen Republicans and Democrats sitting down and
negotiating together . . .
O: And one of the things I think we have to do is make sure that college is affordable . . .
FP O: . . . but in the short term there?s an outlay and we may not see that money for a while.
O: We have to do that now, because it will actually make our businesses and our families better off.
FN O: So I think the lesson to be drawn is that we should never hesitate to use military force . . . to keep the
American people safe.
O: But let?s get back to the core issue here.
PERSUASIVE ESSAYS (DEVELOPMENT SET, SPELLING ERRORS INCLUDED)
TP However, the argument lacks specificity and relies on too many questionable assumptions to make a
strong case for adopting an expensive and logistically complicated program.
I believe that both of these claims have been made in hase and other factors need to be considered.
FP Since they are all far from now, the prove is not strong enough to support the conclusion.
As we know that one mind can not think as the other does.
FN History has proven that . . .
The given issue which states that in any field of inquiry . . . is a controversional one.
Table 2: Examples of CRFm+R performance. Underlining marks tokens predicted to be shell, and bold font indicates
shell according to human judgments (our judgments for the debate transcripts, and the annotator?s judgments for the
development set). Examples include true positives (TP), false positives (FP), and false negatives (FN). Note that some
FP and FN examples include partially accurate predictions.
27
?therefore.? Discourse markers, however, are typ-
ically only single words or short phrases that ex-
press a limited number of relationships. On the other
hand, shell can capture longer sequences that ex-
press more complex relationships between the com-
ponents of an argumentative discourse (e.g., ?But
let?s get back to the core issue here? signals that the
following point is more important than the previous
one).
There are also various other approaches to ana-
lyzing arguments. Notably, much recent theoreti-
cal research on argumentation has focused on ar-
gumentation schemes (Walton et al, 2008), which
are high-level strategies for constructing arguments
(e.g., argument from consequences). Recently, Feng
and Hirst (2011) developed automated methods for
classifying texts by argumentation scheme. In sim-
ilar work, Anand et al (2011) use argumentation
schemes to identify tactics in blog posts (e.g., moral
appeal, social generalization, appeals to external au-
thorities etc.). Although shell language can certainly
be found in persuasive writing, it is used to orga-
nize the persuader?s tactics and claims rather than
to express them. For example, consider the follow-
ing sentence: ?It must be the case that this diet
works since it was recommended by someone who
lost 20 pounds on it.? In shell detection, we focus
on the lexico-syntactic level, aiming to identify the
bold words as shell. In contrast, work on argumenta-
tion schemes focuses at a higher level of abstraction,
aiming to classify the sentence as an attempt to per-
suade by appealing to an external authority.
7 Conclusions
In this paper, we described our approach to detect-
ing language used to explicitly structure an arguer?s
claims and pieces of evidence as well as explain
how they relate to an opponent?s argument. We im-
plemented a rule-based system, a supervised proba-
bilistic sequence model, and a principled hybrid ver-
sion of the two. We presented evaluations of these
systems using human-annotated essays, and we ob-
served that the hybrid sequence model system per-
formed the best. We also applied our system to po-
litical debates and found evidence of the potential to
generalize to new domains.
Acknowledgments
We would like to thank the annotators for helping
us create the essay data sets. We would also like
to thank James Carlson, Paul Deane, Yoko Futagi,
Beata Beigman Klebanov, Melissa Lopez, and the
anonymous reviewers for their useful comments on
the paper and annotation scheme.
References
P. Anand, J. King, J. Boyd-Graber, E. Wagner, C. Martell,
D. Oard, and P. Resnik. 2011. Believe me?we can
do this! annotating persuasive acts in blog text. In
Proc. of AAAI Workshop on Computational Models of
Natural Argument.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proc. of the Second
SIGdial Workshop on Discourse and Dialogue.
B. Efron and R. Tibshirani. 1993. An Introduction to the
Bootstrap. Chapman and Hall/CRC.
V. W. Feng and G. Hirst. 2011. Classifying arguments
by scheme. In Proc. of ACL.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse. Com-
put. Linguist., 12(3):175?204.
B. Hutchinson. 2004. Acquiring the meaning of dis-
course markers. In Proc. of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3).
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press.
N. Okazaki. 2007. CRFsuite: a fast implementation of
conditional random fields (CRFs).
J. R. Searle. 1975. A classification of illocutionary acts.
Language in Society, 5(1).
D. Walton, C. Reed, and F. Macagno. 2008. Argumenta-
tion Schemes. Cambridge University Press.
28
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182?190,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Re-examining Machine Translation Metrics for Paraphrase Identification
Nitin Madnani Joel Tetreault
Educational Testing Service
Princeton, NJ, USA
{nmadnani,jtetreault}@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
We propose to re-examine the hypothesis that
automated metrics developed for MT evalu-
ation can prove useful for paraphrase iden-
tification in light of the significant work on
the development of new MT metrics over the
last 4 years. We show that a meta-classifier
trained using nothing but recent MT metrics
outperforms all previous paraphrase identifi-
cation approaches on the Microsoft Research
Paraphrase corpus. In addition, we apply our
system to a second corpus developed for the
task of plagiarism detection and obtain ex-
tremely positive results. Finally, we conduct
extensive error analysis and uncover the top
systematic sources of error for a paraphrase
identification approach relying solely on MT
metrics. We release both the new dataset and
the error analysis annotations for use by the
community.
1 Introduction
One of the most important reasons for the recent
advances made in Statistical Machine Translation
(SMT) has been the development of automated met-
rics for evaluation of translation quality. The goal
of any such metric is to assess whether the trans-
lation hypothesis produced by a system is seman-
tically equivalent to the source sentence that was
translated. However, cross-lingual semantic equiv-
alence is even harder to assess than monolingual,
therefore, most MT metrics instead try to measure
whether the hypothesis is semantically equivalent to
a human-authored reference translation of the same
source sentence. Using such automated metrics as
proxies for human judgments can provide a quick as-
sessment of system performance and allow for short
feature and system development cycles, which are
important for evaluating research ideas.
In the last 5 years, several shared tasks and com-
petitions have led to the development of increasingly
sophisticated metrics that go beyond the computa-
tion of n-gram overlaps (BLEU, NIST) or edit dis-
tances (TER, WER, PER etc.). Note that the task
of an MT metric is essentially one of identifying
whether the translation produced by a system is a
paraphrase of the reference translation. Although
the notion of using MT metrics for the task of para-
phrase identification is not novel (Finch et al, 2005;
Wan et al, 2006), it merits a re-examination in the
light of the development of these novel MT metrics
for which we can ask ?How much better, if at all,
do these newer metrics perform for the task of para-
phrase identification??
This paper describes such a re-examination. We
employ 8 different MT metrics for identifying
paraphrases across two different datasets - the
well-known Microsoft Research paraphrase corpus
(MSRP) (Dolan et al, 2004) and the plagiarism
detection corpus (PAN) from the 2010 Uncovering
Plagiarism, Authorship and Social Software Misuse
shared task (Potthast et al, 2010). We include both
MSRP and PAN in our study because they represent
two very different sources of paraphrased text. The
creation of MSRP relied on the massive redundancy
of news articles on the web and extracted senten-
tial paraphrases from different stories written about
the same topic. In the case of PAN, humans con-
sciously paraphrased existing text to generate new,
182
plagiarized text.
In the next section, we discuss previous work on
paraphrase identification. In ?3, we describe our ap-
proach to paraphrase identification using MT met-
rics as features. Our approach yields impressive re-
sults ? the current state of the art for MSRP and ex-
tremely positive for PAN. In the same section, we
examine whether each metric?s purported strength is
demonstrated in our datasets. Next, in ?4 we con-
duct an analysis of our system?s misclassifications
for both datasets and outline a taxonomy of errors
that our system makes. We also look at annotation
errors in the datasets themselves. We discuss the
findings of the error analysis in ?5 and conclude in
?6.
2 Related Work & Our Contributions
Our goal in this paper is to examine the utility of a
paraphrase identification approach that relies solely
on MT evaluation metrics and no other evidence of
semantic equivalence. Given this setup, the most rel-
evant previous work is by Finch et al (2005) which
uses BLEU, NIST, WER and PER as features for
a supervised classification approach using SVMs.
In addition, they also incorporate part-of-speech in-
formation as well as the Jiang-Conrath WordNet-
based lexical relatedness measure (Jiang and Con-
rath, 1997) into their edit distance calculations. In
the first part of our paper, we present classification
experiments with newer MT metrics not available in
2005, a worthwhile exercise in itself. However, we
go much further in our study:
? We apply our approach to two different para-
phrase datasets (MSRP and PAN) that were cre-
ated via different processes.
? We attempt to find evidence of each metric?s
purported strength in both datasets.
? We conduct an extensive error analysis to find
types of errors that a system based solely on
MT metrics is likely to make. In addition, we
also discover interesting paraphrase pairs in the
datasets.
? We release our sentence-level PAN dataset (see
?3.3.2) which contains more realistic exam-
ples of paraphrase and can prove useful to the
community for future evaluations of paraphrase
identification.
BLEU-based features were also employed by
Wan et al (2006) who use them in combination with
several other features based on dependency relations
and tree edit-distance inside an SVM.
There are several other supervised approaches to
paraphrase identification that do not use any features
based on MT metrics. Mihalcea et al (2006) com-
bine pointwise mutual information, latent semantic
analysis and WordNet-based measures of word se-
mantic similarity into an arbitrary text-to-text sim-
ilarity metric. Qiu et al (2006) build a frame-
work that detects dissimilarities between sentences
and makes its paraphrase judgment based on the
significance of such dissimilarities. Kozareva and
Montoyo (2006) use features based on LCS, skip
n-grams and WordNet with a meta-classifier com-
posed of SVM, k-nearest neighbor and maximum
entropy classifiers. Islam and Inkpen (2007) mea-
sure semantic similarity using a corpus-based mea-
sure and a modified version of the Longest Common
Subsequence (LCS) algorithm. Rus et al (2008)
take a graph-based approach originally developed
for recognizing textual entailment and adapt it for
paraphrase identification. Fernando and Stevenson
(2008) construct a matrix of word similarities be-
tween all pairs of words in both sentences instead
of relying only on the maximal similarities. Das and
Smith (2009) use an explicit model of alignment be-
tween the corresponding parts of two paraphrastic
sentences and combine it with a logistic regression
classifier built from n-gram overlap features. Most
recently, Socher et al (2011) employ a joint model
that incorporates the similarities between both sin-
gle word features as well as multi-word phrases ex-
tracted from the parse trees of the two sentences.
We compare our results to those from all the ap-
proaches described in this section later in ?3.4.
3 Classifying with MT Metrics
In this section, we first describe our overall approach
to paraphrase identification that utilizes only MT
metrics. We then discuss the actual MT metrics we
used. Finally, we describe the datasets on which we
evaluated our approach and present our results.
183
MSRP
They had published an advertisement on the Internet on June 10,
offering the cargo for sale, he added.
On June 10, the ship?s owners had published an advertisement on the
Internet, offering the explosives for sale.
Security lights have also been installed and police have swept
the grounds for booby traps.
Security lights have also been installed on a barn near the front gate.
PAN
Dense fogs wrapped the mountains that shut in the little hamlet,
but overhead the stars were shining in the near heaven.
The hamlet is surrounded by mountains which is wrapped with dense
fogs, though above it, near heaven, the stars were shining.
In still other places, the strong winds carry soil over long
distances to be mixed with other soils.
In other places, where strong winds blow with frequent regularity,
sharp soil grains are picked up by the air and hurled against the
rocks, which, under this action, are carved into fantastic forms.
Table 1: Examples of paraphrases and non-paraphrases (in italics) from the MSRP and PAN corpora.
3.1 Classifier
Our best system utilized a classifier combination ap-
proach. We used a simple meta-classifier that uses
the average of the unweighted probability estimates
from the constituent classifiers to make its final de-
cision. We used three constituent classifiers: Logis-
tic regression, the SMO implementation of a support
vector machine (Platt, 1999; Keerthi et al, 2001)
and a lazy, instance-based classifier that extends the
nearest neighbor algorithm (Aha et al, 1991). We
used the WEKA machine learning toolkit to perform
our experiments (Hall et al, 2009). 1
3.2 MT metrics used
1. BLEU (Papineni et al, 2002) is the most com-
monly used metric for MT evaluation. It is
computed as the amount of n-gram overlap?
for different values of n?between the system
output and the reference translation, tempered
by a penalty for translations that might be too
short. BLEU relies on exact matching and has
no concept of synonymy or paraphrasing. We
use BLEU1 through BLEU4 as 4 different fea-
1These constituent classifiers were chosen since they were
the top 3 performers in 5-fold cross-validation experiments
conducted on both MSRP and PAN training sets. The meta-
classifier was chosen similarly once the constituent classifiers
had been chosen.
tures for our classifier (hereafter BLEU(1-4)).
2. NIST (Doddington, 2002) is a variant of BLEU
that uses the arithmetic mean of n-gram over-
laps, rather than the geometric mean. It also
weights each n-gram according to its informa-
tiveness as indicated by its frequency. We use
NIST1 through NIST5 as 5 different features
for our classifier (hereafter NIST(1-5)).
3. TER (Snover et al, 2006) is defined as the
number of edits needed to ?fix? the translation
output so that it matches the reference. TER
differs from WER in that it includes a heuris-
tic algorithm to deal with shifts in addition to
insertions, deletions and substitutions.
4. TERp (TER-Plus) (Snover et al, 2009) builds
upon the core TER algorithm by providing ad-
ditional edit operations based on stemming,
synonymy and paraphrase.
5. METEOR (Denkowski and Lavie, 2010) uses
a combination of both precision and recall un-
like BLEU which focuses on precision. Fur-
thermore, it incorporates stemming, synonymy
(via WordNet) and paraphrase (via a lookup ta-
ble).
6. SEPIA (Habash and El Kholy, 2008) is a
syntactically-aware metric designed to focus on
184
structural n-grams with long surface spans that
cannot be captured efficiently with surface n-
gram metrics. Like BLEU, it is a precision-
based metric and requires a length penalty to
minimize the effects of length.
7. BADGER (Parker, 2008) is a language inde-
pendent metric based on compression and in-
formation theory. It computes a compression
distance between the two sentences that utilizes
the Burrows Wheeler Transformation (BWT).
The BWT enables taking into account common
sentence contexts with no limit on the size of
these contexts.
8. MAXSIM (Chan and Ng, 2008) treats the
problem as one of bipartite graph matching and
maps each word in one sentence to at most one
word in the other sentence. It allows the use of
arbitrary similarity functions between words.2
Our choice of metrics was based on their popular-
ity in the MT community, their performance in open
competitions such as the NIST MetricsMATR chal-
lenge (NIST, 2008) and the WMT shared evaluation
task (Callison-Burch et al, 2010), their availability,
and their relative complementarity.
3.3 Datasets
In this section, we describe the two datasets that we
used to evaluate our approach.
3.3.1 Microsoft Research Paraphrase Corpus
The MSRP corpus was created by mining news
articles on the web for topically similar articles and
then extracting potential sentential paraphrases us-
ing a set of heuristics. Extracted pairs were then
shown to two human judges with disagreements
handled by a third adjudicator. The kappa was re-
ported as 0.62, which indicates moderate to high
agreement. We used the pre-stipulated train-test
splits (4,076 sentence pairs in training and 1,725 in
test) to train and test our classifier.
2We also experimented with TESLA?a variant of
MAXSIM that performs better for MT evaluation?in our pre-
liminary experiments However, both MAXSIM and TESLA
performed almost identically in our cross-validation experi-
ments. Therefore, we only retained MAXSIM in our final ex-
periment since it was significantly faster to run than the version
of TESLA we had.
3.3.2 Plagiarism Detection Corpus (PAN)
We wanted to evaluate our approach on a set of
paraphrases where the semantic similarity was not
simply an accidental by-product of topical similarity
but rather consciously generated. We used the test
collection from the PAN 2010 plagiarism detection
competition. This dataset consists of 41,233 text
documents from Project Gutenberg in which 94,202
cases of plagiarism have been inserted. The pla-
giarism was created either by using an algorithm or
by explicitly asking Turkers to paraphrase passages
from the original text. We focus only on the human-
created plagiarism instances.
Note also that although the original PAN dataset
has been used in plagiarism detection shared tasks,
those tasks are generally formulated differently in
that the goal is to find all potentially plagiarized pas-
sages in a given set of documents along with the cor-
responding source passages from other documents.
In this paper, we wanted to focus on the task of iden-
tifying whether two given sentences can be consid-
ered paraphrases.
To generate a sentence-level PAN dataset, we
wrote a heuristic alignment algorithm to find cor-
responding pairs of sentences within a passage pair
linked by the plagiarism relationship. The align-
ment algorithm utilized only bag-of-words overlap
and length ratios and no MT metrics. For our nega-
tive evidence, we sampled sentences from the same
document and extracted sentence pairs that have at
least 4 content words in common. We then sampled
randomly from both the positive and negative evi-
dence files to create a training set of 10,000 sentence
pairs and a test set of 3,000 sentence pairs.
Table 1 shows examples of paraphrastic and non-
paraphrastic sentence pairs from both the MSRP and
PAN datasets.
3.4 Results
Before presenting the results of experiments that
used multiple metrics as features, we wanted to de-
termine how well each metric performs on its own
when used for paraphrase identification. Table 2
shows the classification results on both the MSRP
and PAN datasets using each metric as the only fea-
ture. Although previously explored metrics such as
BLEU and NIST perform reasonably well, they are
185
MSRP PAN
Metric Acc. F1 Acc. F1
MAXSIM 67.2 79.4 84.7 83.4
BADGER 67.6 79.9 88.5 87.9
SEPIA 68.1 79.8 87.7 86.8
TER 69.9 80.9 85.7 83.8
BLEU(1-4) 72.3 80.9 87.9 87.1
NIST(1-5) 72.8 81.2 88.2 87.3
METEOR 73.1 81.0 89.5 88.9
TERp 74.3 81.8 91.2 90.9
Table 2: Classification results for MSRP and PAN with
individual metrics as features. Entries are sorted by accu-
racies on MSRP.
clearly outperformed by some of the more robust
metrics such as TERp and METEOR.
Table 3 shows the results of our experiments em-
ploying multiple metrics as features, for both MSRP
and PAN. The final row in the table shows the results
of our best system. The remaining rows of this table
show the top performing metrics for both datasets;
we treat BLEU, NIST and TER as our baseline met-
rics since they are not new and are not the primary
focus of our investigation. In terms of novel met-
rics, we find that the top 3 metrics for both datasets
were TERp, METEOR and BADGER respectively
as shown. Combining all 8 metrics led to the best
performance for MSRP but showed no performance
increase for PAN.
MSRP PAN
Features Acc. F1 Acc. F1
Base Metrics 74.1 81.5 88.6 87.8
+ TERp 75.6 82.5 91.5 91.2
+ METEOR 76.6 83.2 92.0 91.8
+ BADGER 77.0 83.7 92.3 92.1
+ Others 77.4 84.1 92.3 92.1
Table 3: The top 3 performing MT metrics for both
MSRP and PAN datasets as identified by ablation stud-
ies. BLEU(1-4), NIST(1-5) and TER were used as the 10
base features in the classifiers.
Our results for the PAN dataset are much better than
those for MSRP since:
(a) It is likely that our negative evidence is too easy
for most MT metrics.
(b) Many plagiarized pairs are linked simply via
lexical synonymy which can be easily captured
by metrics like METEOR and TERp, e.g., the
sentence ?Young?s main contention is that in lit-
erature genius must make rules for itself, and
that imitation is suicidal? is simply plagiarized
as ?Young?s major argument is that in litera-
ture intellect must make rules for itself, and
that replication is dangerous.? However, the
PAN corpus does contains some very challeng-
ing and interesting examples of paraphrases?
even more so than MSRP?which we describe
in ?4.
Finally, Table 4 shows that the results from our
best system are the best ever reported on the MSRP
test set when compared to all previously published
work. Furthermore, the single best performing met-
ric (TERp)?also shown in the table?outperforms,
by itself, many previous approaches utilizing multi-
ple, complex features.
Model Acc. F1
All Paraphrase Baseline 66.5 79.9
(Mihalcea et al, 2006) 70.3 81.3
(Rus et al, 2008) 70.6 80.5
(Qiu et al, 2006) 72.0 81.6
(Islam and Inkpen, 2007) 72.6 81.3
(Fernando and Stevenson, 2008) 74.1 82.4
TERp 74.3 81.8
(Finch et al, 2005) 75.0 82.7
(Wan et al, 2006) 75.6 83.0
(Das and Smith, 2009) 76.1 82.7
(Kozareva and Montoyo, 2006) 76.6 79.6
(Socher et al, 2011) 76.8 83.6
Best MT Metrics 77.4 84.1
Table 4: Comparing the accuracy and F -score for the sin-
gle best performing MT metric TERp (in gray) as well as
the best metric combination system (in gray and bold)
with previously reported results on the MSRP test set
(N = 1, 752). Entries are sorted by accuracy.
3.5 Metric Contributions
In addition to quantitative results, we also wanted to
highlight specific examples from our datasets that
can demonstrate the strength of the new metrics
over simple n-gram overlap and edit-distance based
metrics. Below we present examples for the 4 best
186
metrics across both datasets:
? TERp uses stemming and phrasal paraphrase
recognition to accurately classify the sentence
pair ?For the weekend, the top 12 movies
grossed $157.1 million, up 52 percent from
the same weekend a year earlier.? and ?The
overall box office soared, with the top 12
movies grossing $157.1 million, up 52 percent
from a year ago.? from MSRP as paraphrases.
? METEOR uses synonymy and stemming
to accurately classify the sentence pair ?Her
letters at this time exhibited the two extremes of
feeling in a marked degree.? and ?Her letters
at this time showed two extremes of feelings.?
from PAN as plagiarized.
? BADGER uses unsupervised contextual
similarity detection to accurately classify the
sentence pair ?Otherwise they were false or
mistaken reactions? and ?Otherwise, were false
or wrong responses? from PAN as plagiarized.
? SEPIA uses structural n-grams via dependency
trees to accurately classify the sentence pair
?At his sentencing, Avants had tubes in his
nose and a portable oxygen tank beside him.?
and ?Avants, wearing a light brown jumpsuit,
had tubes in his nose and a portable oxygen
tank beside him.? from MSRP as paraphrases.
4 Error Analysis
In this section, we conduct an analysis of the
misclassifications that our system makes on both
datasets. Our analyses consisted of finding the sen-
tences pairs from the test set for each dataset which
none of our systems (not just the best one) ever clas-
sified correctly and inspecting a random sample of
100 of these. This inspection yields not only the top
sources of error for an approach that relies solely on
MT metrics but also uncovers sources of annotation
errors in both datasets themselves.
4.1 MSRP
In their paper describing the creation of the MSRP
corpus, Dolan et al (2004) clearly state that ?the de-
gree of mismatch allowed before the pair was judged
non-equivalent was left to the discretion of the indi-
vidual rater? and that ?many of the 33% of sentence
pairs judged to be not equivalent still overlap signif-
icantly in information content and even wording?.
We found evidence that the raters were not always
consistent in applying the annotation guidelines. For
example, in some cases the lack of attribution for a
quotation led the raters to label a pair as paraphrastic
whereas in other cases it did not. For example, the
pair ?These are real crimes that hurt a lot of people.?
and ??These are real crimes that disrupt the lives of
real people,? Smith said.? was not marked as para-
phrastic. Furthermore, even though the guidelines
instruct the raters to ?treat anaphors and their full
forms as equivalent, regardless of how great the dis-
parity in length or lexical content between the two
sentences?, we found pairs of sentences marked as
non-paraphrastic which only differed in anaphora.
However, the primary goal of this analysis is to find
sources of errors in an MT-metric driven approach
and below we present the top 5 such sources:
1. Misleading Lexical Overlap. Non-
paraphrastic pairs where there is large
lexical overlap of secondary material between
the two sentences but the primary semantic
content is different. For example, ?Gyorgy
Heizler, head of the local disaster unit, said the
coach had been carrying 38 passengers.?
and ?The head of the local disaster
unit, Gyorgy Heizler, said the coach
driver had failed to heed red stop lights.?.
2. Lack of World Knowledge. Paraphrastic
pairs that require world knowledge. For ex-
ample, ?Security experts are warning that a
new mass-mailing worm is spreading widely
across the Internet, sometimes posing as e-
mail from the Microsoft founder.? and ?A
new worm has been spreading rapidly across
the Internet, sometimes pretending to be
an e-mail from Microsoft Chairman Bill Gates,
antivirus vendors said Monday.?.
3. Tricky Phrasal Paraphrases. Paraphras-
187
tic pairs that contain domain-dependent se-
mantic alternations. For example, ?The
leading actress nod went to energetic new-
comer Marissa Jaret Winokur as Edna?s
daughter Tracy.? and ?Marissa Jaret Winokur,
as Tracy, won for best actress in a musical.?.
4. Date, Time and Currency Differences. Para-
phrastic pairs that contain different temporal
or currency references. These references were
normalized to generic tokens (e.g., $NUMBER)
before being shown to MSRP raters but are re-
tained in the released dataset. For example,
?Expenses are expected to be approximately
$2.3 billion, at the high end of the previous ex-
pectation of $2.2-to-$2.3 billion.? and ?Spend-
ing on research and development is expected to
be $4.4 billion for the year, compared with the
previous expectation of $4.3 billion.?.
5. Anaphoric References. Paraphrastic pairs
wherein one member of the pair contains
anaphora and the other doesn?t (these are con-
sidered paraphrases according to MSRP guide-
lines). For example, ?They certainly reveal a
very close relationship between Boeing and se-
nior Washington officials.? and ?The e-mails
reveal the close relationship between Boeing
and the Air Force.?.
Note that most misclassified sentence pairs can be
categorized into more than one of the above cate-
gories.
4.2 PAN
For the PAN corpus, the only real source of error in
the dataset itself was the sentence alignment algo-
rithm. There were many sentence pairs that were
erroneously linked as paraphrases. Leaving aside
such pairs, the 3 largest sources of error for our MT-
metric based approach were:
1. Complex Sentential Paraphrases. By far,
most of the misclassified pairs were paraphras-
tic pairs that could be categorized as real world
plagiarism, i.e., where the plagiarizer copies
the idea from the source but makes several
complex transformations, e.g., sentence split-
ting, structural paraphrasing etc. so as to ren-
der an MT-metric based approach powerless.
For example, consider the pair ?The school
bears the honored name of one who, in the long
years of the anti-slavery agitation, was known
as an uncompromising friend of human free-
dom.? and ?The school is named after a man
who defended the right of all men and women
to be free, all through the years when people
campaigned against slavery.? Another inter-
esting example is the pair ?The most unpromis-
ing weakly-looking creatures sometimes live to
ninety while strong robust men are carried off
in their prime.? and ?Sometimes the strong per-
sonalities live shorter than those who are unex-
pected.?.
2. Misleading Lexical Overlap. Similar to
MSRP. For example, ?Here was the second pe-
riod of Hebraic influence, an influence wholly
moral and religious.? and ?This was the sec-
ond period of Hellenic influence, an influence
wholly intellectual and artistic.?.
3. Typographical and Spelling Errors. Para-
phrastic pairs where the Turkers creating the
plagiarism also introduced other typos and
spelling errors. For example, ?The boat then
had on board over 1,000 souls in all? and
?1000 people where on board at that tim?.
5 Discussion
The misses due to ?Date, Time, and Currency Dif-
ferences? are really just the result of an artifact in
the testing. It is possible that an MT metrics based
approach could accurately predict these cases if the
references to dates etc. were replaced with generic
tokens as was done for the human raters. In a
similar vein, some of the misses that are due to a
lack of world knowledge might become hits if a
named entity recognizer could discover that ?Mi-
crosoft founder? is the same as ?Microsoft Chair-
man?. Similarly, some of the cases of anaphoric ref-
erence might be recognized with an anaphora res-
olution system. And the problem of misspelling in
PAN could be remedied with automatic spelling cor-
rection. Therefore, it is possible to improve the MT
metrics based approach further by utilizing certain
NLP systems as pre-processing modules for the text.
The only error category in MSRP and PAN
188
that caused false positives was ?Misleading Lexical
Overlap?. Here, the take-away message is that not
every part of a sentence is equally important for rec-
ognizing semantic equivalence or non-equivalence.
In a sentence that describes what someone commu-
nicated, the content of what was said is crucial. For
example, despite lexical matches everywhere else,
the mismatch of ?the coach had been carrying 38
passengers? and ?the driver had failed to heed the
red stop lights? disqualifies the respective sentences
from being paraphrases. Along the same line, dif-
ferences in proper names and their variants should
receive more weight than other words. A sentence
about ?Hebraic influence? on a period in history is
not the same as a sentence which matches in ev-
ery other way but is instead about ?Hellenic influ-
ence?. These sentences represent a bigger chal-
lenge for an approach based solely on MT metrics.
Given enough pairs of ?near-miss? non-paraphrases,
our system might be able to figure this out, but this
would require a large amount of annotated data.
6 Conclusions
In this paper, we re-examined the idea that automatic
metrics used for evaluating translation quality can
perform well explicitly for the task of paraphrase
recognition. The goal of our paper was to deter-
mine whether approaches developed for the related
but different task of MT evaluation can be as com-
petitive as approaches developed specifically for the
task of paraphrase identification. While we do treat
the metrics as black boxes to an extent, we explic-
itly chose metrics that were high performing but also
complementary in nature.
Specifically, our re-examination focused on the
more sophisticated MT metrics of the last few years
that claim to go beyond simple n-gram overlap and
edit distance. We found that a meta-classifier trained
using only MT metrics outperforms all previous ap-
proaches for the MSRP corpus. Unlike previous
studies, we also applied our approach to a new pla-
giarism dataset and obtained extremely positive re-
sults. We examined both datasets not only to find
pairs that demonstrated the strength of each met-
ric but also to conduct an error analysis to discover
the top sources of errors that an MT metric based
approach is susceptible to. Finally, we discovered
that using the TERp metric by itself provides fairly
good performance and can outperform many other
supervised classification approaches utilizing multi-
ple, complex features.
We also have two specific suggestions that we be-
lieve can benefit the community. First, we believe
that binary indicators of semantic equivalence are
not ideal and a continuous value between 0 and 1
indicating the degree to which two pairs are para-
phrastic is more suitable for most approaches. How-
ever, rather than asking annotators to rate pairs on
a scale, a better idea might be to show the sentence
pairs to a large number of Turkers (? 20) on Ama-
zon Mechanical Turk and ask them to classify it as
either a paraphrase or a non-paraphrase. A simple
estimate of the degree of semantic equivalence of
the pair is simply the proportion of the Turkers who
classified the pair as paraphrastic. An example of
such an approach, as applied to the task of grammat-
ical error detection, can be found in (Madnani et al,
2011).3 Second, we believe that the PAN corpus?
with Turker simulated plagiarism?contains much
more realistic examples of paraphrase and should
be incorporated into future evaluations of paraphrase
identification. In order to encourage this, we are re-
leasing our PAN dataset containing 13,000 sentence
pairs.
We are also releasing our error analysis data (100
pairs for MSRP and 100 pairs for PAN) since they
might prove useful to other researchers as well. Note
that the annotations for this analysis were produced
by the authors themselves and, although, they at-
tempted to accurately identify all error categories for
most sentence pairs, it is possible that the errors in
some sentence pairs were not comprehensively iden-
tified.4
Acknowledgments
We would like to thank Aoife Cahill, Michael Heil-
man and the three anonymous reviewers for their
useful comments and suggestions.
3A good approximation is to use an ordinal scale for the
human judgments as in the Semantic Textual Similarity task
of SemEval 2012. See http://www.cs.york.ac.uk/
semeval-2012/task6/ for more details.
4The data is available at http://bit.ly/mt-para.
189
References
D. W. Aha, D. Kibler, and M. K. Albert. 1991. Instance-
based learning algorithms. Mach. Learn., 6:37?66.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, and
O. Zaidan, editors. 2010. Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR.
Y. S. Chan and H. T. Ng. 2008. MAXSIM: A maxi-
mum similarity metric for machine translation evalua-
tion. In Proceedings of ACL-HLT, pages 55?62.
D. Das and N.A. Smith. 2009. Paraphrase Identifica-
tion as Probabilistic Quasi-synchronous Recognition.
In Proceedings of ACL-IJCNLP, pages 468?476.
M. Denkowski and M. Lavie. 2010. Extending the
METEOR Machine Translation Metric to the Phrase
Level. In Proceedings of NAACL.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality using N-gram Co-occurrence
Statistics. In Proceedings of HLT, pages 138?145.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised Construction of Large Paraphrase Corpora: Ex-
ploiting Massively Parallel News Sources. In Proceed-
ings of COLING, pages 350?356, Geneva, Switzer-
land.
S. Fernando and M. Stevenson. 2008. A Semantic Simi-
larity Approach to Paraphrase Detection. In Proceed-
ings of the Computational Linguistics UK (CLUK)
11th Annual Research Colloquium.
A. Finch, Y.S. Hwang, and E. Sumita. 2005. Using Ma-
chine Translation Evaluation Techniques to Determine
Sentence-level Semantic Equivalence. In Proceedings
of the Third International Workshop on Paraphrasing,
pages 17?24.
N. Habash and A. El Kholy. 2008. SEPIA: Surface
Span Extension to Syntactic Dependency Precision-
based MT Evaluation. In Proceedings of the Workshop
on Metrics for Machine Translation at AMTA.
M. Hall, E. Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11.
A. Islam and D. Inkpen. 2007. Semantic Similarity of
Short Texts. In Proceedings of RANLP, pages 291?
297.
J. J. Jiang and D. W. Conrath. 1997. Semantic Similar-
ity Based on Corpus Statistics and Lexical Taxonomy.
CoRR, cmp-lg/9709008.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 2001. Improvements to Platt?s SMO
Algorithm for SVM Classifier Design. Neural Com-
put., 13(3):637?649.
Z. Kozareva and A. Montoyo. 2006. Paraphrase Identi-
fication on the Basis of Supervised Machine Learning
Techniques. In Proceedings of FinTAL, pages 524?
233.
N. Madnani, J. Tetreault, M. Chodorow, and A. Ro-
zovskaya. 2011. They Can Help: Using Crowdsourc-
ing to Improve the Evaluation of Grammatical Error
Detection Systems. In Proceedings of ACL (Short Pa-
pers).
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and Knowledge-based Measures Of
Text Semantic Similarity. In Proceedings of AAAI,
pages 775?780.
NIST. 2008. NIST MetricsMATR Challenge. Informa-
tion Access Division. http://www.itl.nist.
gov/iad/mig/tests/metricsmatr/.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of ACL.
S. Parker. 2008. BADGER: A New Machine Translation
Metric. In Proceedings of the Workshop on Metrics
for Machine Translation at AMTA.
John C. Platt. 1999. Advances in kernel methods. chap-
ter Fast Training of Support Vector Machines using Se-
quential Minimal Optimization, pages 185?208. MIT
Press.
M. Potthast, B. Stein, A. Barro?n-Ceden?o, and P. Rosso.
2010. An Evaluation Framework for Plagiarism De-
tection. In Proceedings of COLING, pages 997?1005.
L. Qiu, M. Y. Kan, and T. S. Chua. 2006. Paraphrase
Recognition via Dissimilarity Significance Classifica-
tion. In Proceedings of the EMNLP, pages 18?26.
V. Rus, P.M. McCarthy, M.C. Lintean, D.S. McNamara,
and A.C. Graesser. 2008. Paraphrase Identification
with Lexico-Syntactic Graph Subsumption. In Pro-
ceedings of FLAIRS, pages 201?206.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings of
AMTA.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009.
TER-Plus: Paraphrase, Semantic, and Alignment En-
hancements to Translation Edit Rate. Machine Trans-
lation, 23(2?3):117?127.
R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D.
Manning. 2011. Dynamic Pooling and Unfolding
Recursive Autoencoders for Paraphrase Detection. In
Advances in Neural Information Processing Systems
24 (NIPS).
S. Wan, R. Dras, M. Dale, and C. Paris. 2006. Using
Dependency-based Features to Take the ?para-farce?
Out of Paraphrase. In Proceedings of the Australasian
Language Technology Workshop (ALTW), pages 131?
138.
190
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 284?294,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Correcting Comma Errors in Learner Essays, and Restoring Commas in
Newswire Text
Ross Israel
Indiana University
Memorial Hall 322
Bloomington, IN 47405, USA
raisrael@indiana.edu
Joel Tetreault
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
jtetreault@ets.org
Martin Chodorow
Hunter College of CUNY
695 Park Avenue
New York, NY 10065, USA
mchodoro@hunter.cuny.edu
Abstract
While the field of grammatical error detection
has progressed over the past few years, one
area of particular difficulty for both native and
non-native learners of English, comma place-
ment, has been largely ignored. We present a
system for comma error correction in English
that achieves an average of 89% precision and
25% recall on two corpora of unedited student
essays. This system also achieves state-of-the-
art performance in the sister task of restor-
ing commas in well-formed text. For both
tasks, we show that the use of novel features
which encode long-distance information im-
proves upon the more lexically-driven features
used in prior work.
1 Introduction
Automatically detecting and correcting grammati-
cal errors in learner language is a growing sub-field
of Natural Language Processing. As the field has
progressed, we have seen research focusing on a
range of grammatical phenomena including English
articles and prepositions (c.f. Tetreault et al, 2010;
De Felice and Pulman, 2008), particles in Korean
and Japanese (c.f. Dickinson et al, 2011; Oyama,
2010), and broad approaches that aim to find mul-
tiple error types (c.f Rozovskaya et al, 2011; Ga-
mon, 2011). However, to the best of our knowledge,
there has not been any research published specifi-
cally on correcting erroneous comma usage in En-
glish (though there have been efforts such as the MS
Word grammar checker, and products like Gram-
marly and White Smoke that include comma check-
ing).
There are a variety of reasons that motivate our
interest in attempting to correct comma errors. First
of all, a review of error typologies in Leacock et al
(2010) reveals that comma usage errors are the
fourth most common error type among non-native
writers in the Cambridge Learner Corpus (Nicholls,
1999), which is composed of millions of words of
text from essays written by learners of English. The
problem of comma usage is not limited to non-
native writers; six of the top twenty error types for
native writers involve misuse of commas (Connors
and Lunsford, 1988). Given these apparent deficits
among both non-native and native speakers, devel-
oping a sound methodology for automatically iden-
tifying comma errors will prove useful in both learn-
ing and automatic assessment environments.
A quick examination of English learner essays re-
veals a variety of errors, with writers both overusing
and underusing commas in certain contexts. Con-
sider examples (1) and (2):
(1) erroneous: If you want to be a master you
should know your subject well.
corrected: If you want to be a master , you
should know your subject well.
(2) erroneous: I suppose , that it is better to spe-
cialize in one specific subject.
corrected: I suppose that it is better to special-
ize in one specific subject.
In example (1), an introductory conditional phrase
begins the sentence, but the learner has not used the
appropriate comma to separate the dependent clause
from the independent clause. The comma in this
case helps the reader to see where one clause ends
284
and another begins. In example (2), the comma after
suppose is unnecessary in American English, and al-
though this error is related more to style than to read-
ability, most native writers would omit the comma
in this context, so it should be avoided by learners as
well.
Another motivating factor for this work is the fact
that sentence internal punctuation contributes to the
overall readability of a sentence (Hill and Murray,
1998). Proper comma placement can lead to faster
reading times and reduce the need to re-read en-
tire sentences. Commas also help remove or reduce
problems arising from difficult ambiguities; the gar-
den path effect can be greatly reduced if commas are
correctly inserted after introductory phrases and re-
duced relative clauses.
This paper makes the following contributions:
? We present the first published comma error cor-
rection system for English, evaluated on essays
written by both native and non-native speakers
of English.
? The same system also achieves state-of-the-art
performance in the task of restoring commas in
well-edited text.
? We describe a novel annotation scheme that al-
lows for robust mark up of comma errors and
use it to annotate two corpora of student essays.
? We show that distance and combination fea-
tures can improve performance for both the er-
ror correction and restoration tasks.
The rest of this paper is organized as follows.
In section 2, we review prior work. Section 3 de-
tails our typology of comma usage. We discuss our
choice of classifier and selection of features in sec-
tion 4. In section 5, we apply our system to the task
of comma restoration. We describe our annotation
scheme and error correction system and evaluation
in sections 6 and 7. Finally, we summarize and out-
line plans for future research in section 8.
2 Previous Work
The only reported research that we are aware
of which specifically deals with comma errors in
learner writing is reported in Hardt (2001) and Ale-
gria et al (2006), two studies that deal with Dan-
ish and Basque, respectively. Hardt (2001) employs
an error driven approach featuring the Brill tagger
(Brill, 1993). The Brill tagger works as it would
for the part-of-speech tagging task for which it was
designed, i.e. it learns rules based on templates by
iterating over a large corpus. This work is also eval-
uated on native text where all existing commas are
considered correct, and additional ?erroneous? com-
mas are added randomly to a sub-corpus, so that the
tagger can learn from the errors. The system is tested
on a distinct subset for the task of correcting exist-
ing comma errors and achieves 91.4% precision and
76.9% recall.
Alegria et al (2006) compare implementations
of Naive Bayes, decision-tree, and support vector
machine (SVM) classifiers and utilize a feature set
based on word-forms, categories, and syntactic in-
formation about each decision point. While the sys-
tem is designed as a possible means for correcting
errors, it is only evaluated on the task of restor-
ing commas in well-formed text produced by native
writers. The system obtains good precision (96%)
and recall (98.3%) for correctly not inserting com-
mas, but performs less well at actually inserting
commas (69.6% precision, 48.6% recall).
It is important to note that the results in both of the
projects are based on constructed errors in an other-
wise native corpus which is free of any other con-
textual errors that might be present in actual learner
data. Moreover, as we will show in section 6, er-
rors of omission (failing to use needed commas) are
much more common than errors of commission (in-
serting commas inappropriately) in the English as
a Foreign Language (EFL) data that we use. Cru-
cially, our error correction efforts described in sec-
tion 7 must be able to account for noise and be able
to insert new commas as well as remove erroneous
ones, as we do evaluate on a set of English learner
essays.
Although we have not found any work published
specifically on correcting comma errors in English,
for language learners or otherwise, there is a fairly
large amount of work that focuses on the task of
comma restoration. Comma restoration refers to
placing commas in a sentence which is presented
with no sentence internal punctuation. This task is
285
mostly attempted in the larger context of Automatic
Speech Recognition (ASR), since there are no ab-
solute cues of where commas should be placed in a
stream of speech. Many of these systems use feature
sets that include prosodic elements that are clearly
not available for text based work (see e.g., Favre
et al, 2009; Huang and Zweig, 2002; Moniz et al,
2009).
There are, however, a few punctuation restora-
tion projects that have used well-formed text-only
data. Shieber and Tao (2003) explore restoring com-
mas to the Wall Street Journal (WSJ) section of
the Penn Treebank (PTB). The authors augment a
HMM trigram-based system with constituency parse
information at each insertion point. Using fully
correct parses directly from the PTB, the authors
achieve an F-score of 74.8% and sentence accuracy
of 57.9%1. However, a shortcoming of this method-
ology is that it dictates that all commas are missing,
but these parses were generated with comma infor-
mation present in the sentence and moreover hand-
corrected by human annotators. Using parses auto-
matically generated with commas removed from the
data, they achieve an F-score of 70.1% and sentence
accuracy of 54.9%.
More recently, Gravano et al (2009), who work
with newswire text, including WSJ, pursue the task
of inserting all punctuation and correcting capital-
ization in a string of text in a single pass, rather
than just comma restoration, but do provide results
based solely on comma insertion. The authors em-
ploy an n-gram language model and experiment with
n-grams from size n = 3 to n = 6, and with different
training data sizes. The result relevant to our work is
their comma F-score on WSJ test data, which is just
over 60% when using 5-grams and 55 billion train-
ing tokens. Baldwin and Joseph (2009) also restore
punctuation and capitalization to newswire texts, us-
ing machine based learning with retagging. Their
results are difficult to compare with our work be-
cause they use a different data set and do not focus
on commas in their evaluation.
Lu and Ng (2010) take an approach that inserts all
1Sentence accuracy is a measure used by some in the field
that counts sentences with 100% correct comma decisions as
correct, and any sentence where a comma is missing or mis-
takenly placed as incorrect. It is motivated by the idea that all
commas are essential to understanding a sentence.
punctuation symbols into text. They use transcribed
English and Chinese speech data and do not provide
specific evaluation for commas, however one im-
portant contribution of their research to our current
task is the finding that Conditional Random Fields
(CRFs) perform better at this task than Hidden Event
Language Models, another algorithm that has been
used for restoration. One reason for this could be
CRFs? better handling of long range dependencies
because they model the entire sequence, rather than
making a singular decision based on information at
each point in the sequence (Liu et al, 2005). CRFs
also do not suffer from the label bias problem that
affects Maximum Entropy classifiers (Lafferty et al,
2001).
3 Comma Usage
One of the challenges present in this research is the
ambiguity as to what constitutes ?correct? comma
usage in American English. For one thing, not
all commas contribute to grammaticality; some are
more tied to stylistic rules and preferences. While
there are certainly rule-based decision points for
comma insertion (Doran, 1998), particularly in the
case of commas that set off significant chunks or
phrases within sentences, there are also some com-
mas that appear to be more prescriptive, as they have
less of an effect on sentence processing (such as in
example (2) in the introduction), and opposing us-
age rules for the same contexts are attested in differ-
ent style manuals. A common example of opposing
rules is the notorious serial or Oxford comma that
refers to the final comma found in a series, which
is required by the Chicago Manual of Style (Univer-
sity of Chicago, 1993), but is considered incorrect
by the New York Times Manual of Style (Siegal and
Connolly, 1999).
As a starting point, we needed to know what kinds
of commas are taught by English language teachers,
as well as what style manuals recommend and/or re-
quire. However, creating a list of comma uses was
a non-trivial part of the process. After consulting
style manuals (University of Chicago, 1993; Siegal
and Connolly, 1999; Strunk and White, 1999) and
popular ESL websites, we compiled a list of over 30
rules for use of commas in English. We took the
most commonly mentioned rules and created a final
286
Rule Example
Elements in a List Paul put the kettle on, Don fetched the teapot, and I made tea.
Initial Word/Phrase Hopefully, this car will last for a while.
Dependent Clause After I brushed the cat, I lint-rollered my clothes.
Independent Clause I have finished painting, but he is still sanding the doors.
Parentheticals My father, a jaded and bitter man, ate the muffin.
Quotations ?Why,? I asked, ?do you always forget to do it??
Adjectives She is a strong, healthy woman.
Conjunctive Adverbs I would be happy, however, to volunteer for the Red Cross.
Contrasting Elements He was merely ignorant, not stupid.
Numbers 345,280,000
Dates She met her husband on December 5, 2003.
Geographical Names I lived in San Francisco, California, for 20 years.
Titles Al Mooney, M.D., is a good doctor
Introducing Words You may be required to bring many items, e.g., spoons, pans, and flashlights.
Other Catch-all rule for any other comma use
Table 1: Common Comma Uses
list of 15 usage rules (the 14 most common plus one
miscellaneous category) for our annotation scheme,
which is discussed in section 6. These rules are
given in Table 1. The 16 rules that were removed
from the list occurred in only one source or were
similar enough to other rules to be conflated. It is
worth noting here that while many of the comma
uses in this table might be best served by some sta-
tistical methodology like the one we describe in sec-
tion 4, one can envision fairly simple heuristic rules
to insert commas and find errors in numbers, dates,
geographical names, titles, and introducing words.
4 Classifier and Features
We use CRFs2 as the basis for our system and treat
the task of comma insertion as a sequence label-
ing task; each space between words is considered
by the classifier, and a comma is either inserted or
not. The feature set incorporates features that have
proven useful in comma restoration and other error
correction tasks, as well as a handful of new features
devised for this specific task (combination and dis-
tance features). The full set of features used in our
final system is given in Figure 1 along with exam-
ples of each feature for the sentence If the teacher
easily gets mad , then the child will always fear go-
ing to school and class. The target insertion point is
after the word mad.
2http://crfpp.sourceforge.net/
Feature Example(s)
Lexical and Syntactic Features
unigram easily, gets, mad, then, the
bigram easily gets, gets mad, mad then, ...
trigram easily gets mad, gets mad then, ...
pos uni RB, VBZ, JJ, RB, DT
pos bi RB VBZ, VBZ JJ, JJ RB, ...
pos tri RB VBZ JJ, VBZ JJ RB, ...
combo easily+RB, gets+VBZ,mad+JJ, ...
first combo If+RB
Distance Features
bos dist 5
eos dist 10
prevCC dist -
nextCC dist 9
Figure 1: CRF Features with examples for:
If the teacher easily gets mad , then the child will always
fear going to school and class.
4.1 Lexical and Syntactic Features
The first six features in Figure 1 refer to simple uni-
grams, bigrams, and trigrams of the words and POS
tags in a sliding 5 word window (target word, +/- 2
words). The lexical items help to encode any id-
iosyncratic relationships between words and com-
mas that might not be exploited through the exami-
nation of more in-depth linguistic features. For ex-
ample, then is a special case of an adverb (RB) that
is often preceded by a comma, even if other adverbs
are not, so POS tags might not capture this relation-
287
ship. The lexical items also provide an approxima-
tion of a language model or hidden event language
model approach, which has proven to be useful in
comma restoration tasks (see e.g. Lu and Ng, 2010).
The POS features abstract away from the words
and avoid the problem of data sparseness by allow-
ing the classifier to focus on the categories of the
words, rather than the lexical items themselves. The
combination (combo) feature is a unigram of the
word+pos for every word in the sliding window. It
reinforces the relationship between the lexical items
and their POS tags, further strengthening the evi-
dence of entries like then RB. All of these features
have been used in previous grammatical error detec-
tion tasks which target particle, article, and prepo-
sition errors (c.f., Dickinson et al, 2011; Gamon,
2010; Tetreault and Chodorow, 2008).
The first combo feature keeps track of the first
combination feature of the sentence so that it can
be referred to by the classifier throughout process-
ing the entire sentence. This feature is helpful when
an introductory phrase is longer than the classifier?s
five word window. Figure 1 provides a good exam-
ple of the utility of this feature, as If the teacher eas-
ily gets mad is so long that by the time the window
has moved to the target position of the space follow-
ing mad, the first word and POS, If RB, which can
often indicate an introductory phrase, is beyond the
scope of the sliding window.
4.2 Distance Features
Next, we encode four distance features. We keep
track of the following distances: from the beginning
of the sentence (bos dist), to the end of the sentence
(eos dist), from the previous coordinating conjunc-
tion (prevCC dist), and to the next coordinating con-
junction (nextCC dist). All of these distance fea-
tures help the classifier by encoding measures for
components of the sentence that can affect the deci-
sion to insert a comma. These features are especially
helpful over long range dependencies, when the in-
formation encoded by the feature is far outside the
scope of the 5-word window the CRF uses. The dis-
tance to the beginning of the sentence helps to en-
code introductory words and phrases, which make
up the bulk of the commas used in essays by learners
of English. The distance to the end of the sentence
is less obviously useful, but it can let the classifier
know the likelihood of a phrase beginning or ending
at a certain point in the sentence. The distances to
and from the nearest CC are useful because many
commas are collocated with coordinating conjunc-
tions. The distance features, as well as first combo,
were designed specifically for the task of comma er-
ror correction, and have not, as far as we know, been
utilized in previous research.
5 Comma Restoration
Before applying our system to the task of error cor-
rection, we tested its utility in restoring commas in
newswire texts. Specifically, we evaluate on section
23 of the WSJ, training on sections 02-22. Here,
the task is straightforward: we remove all commas
from the test data and performance is measured on
the system?s ability to put the commas back in the
right places. After stripping all commas from our
test data, the text is tokenized and POS tagged using
a maximum entropy tagger (Ratnaparkhi, 1996) and
every token is considered by the classifier as either
requiring a following comma or not. Out of 53,640
tokens, 3062 should be followed by a comma. We
provide accuracy, precision, recall, F1-score, and
sentence accuracy (S Acc.) for these tests, along
with results from Gravano et al (2009) and Shieber
and Tao (2003) in Table 2. The first system (LexSyn)
includes only the lexical and syntactic features from
Figure 1; the second (LexSyn+Dist) includes all of
the features.
System Acc. P R F S Acc.
LexSyn 97.4 85.8 64.9 73.9 60.5
LexSyn+Dist 97.5 85.8 66.3 74.8 61.4
Shieber & Tao 97.0 79.7 62.6 70.1 54.9
Gravano et al N.A. 57 67 ?61 N.A.
Table 2: Comma Restoration System Results (%)
As can be seen in Table 2, the full system
(LexSyn+Dist) performs significantly better than
WSJ LexSyn (p < .02, two-tailed), achieving an
F-score of 74.8 on WSJ. This F-score outperforms
Shieber and Tao?s system, which was also tested on
section 23 of the WSJ, by about 4% and our sentence
accuracy of 61.5% is about 7% higher than theirs.
Our F-score is also about 13% higher than that of
Gravano et al (2009), however, they evaluate on the
288
entire WSJ section of the Penn Treebank, so it is not
totally fair to compare results.
6 Annotation
For the comma restoration task, we needed only to
obtain well-formed text and remove the commas to
produce a test set. However, this is not so in the case
of error correction. In order to test a system that
corrects errors in learner essays, we need an anno-
tated test corpus that tells us where the errors are.
Although there are a handful of corpora that include
punctuation errors in their annotation scheme, such
as NUCLE (Dahlmeier and Ng, 2011) and HOO
(Dale and Kilgarriff, 2010), there are none to our
knowledge that focus specifically on commas. Thus,
we designed and implemented our own annotation
scheme on a set of essays to allow us the freedom to
identify the most important aspects of comma usage
for our work.
Our annotation scheme allows the mark-up of a
number of aspects of comma usage. First, each
comma in a text is marked as rejected or accepted
by the annotator. Additionally, any space between
words can be treated as an insertion point for a miss-
ing comma. The annotators also marked all accepted
and inserted commas as either required or optional.
Finally, the annotation also includes the appropriate
usage rule from the set in Table 1.3 In contrast, the
NUCLE and HOO data sets do not have this gran-
ularity of information (the annotation only indicates
whether a comma should be inserted or removed)
and are not exhaustively annotated.
After a one-hour training session on comma us-
age rules, three native English speakers were given a
set of ten learner essays comprising 3,665 tokens to
annotate for comma errors. To assess the difficulty
of the annotation task, we calculated agreement and
kappa. Agreement is a simple measure of how often
the annotators agree, and kappa provides a more ro-
bust measure of agreement since it takes chance into
account (Cohen, 1960). Table 3 provides the results
of these measurements. As can be seen in the table,
the agreement is quite high at either 97 or 98%, and
kappa is a bit lower, ranging from 72 to 81%. The
3The full annotation manual is available at
http://www.cs.rochester.edu/?tetreaul/
comma-manual.pdf
agreement is likely so high due to the great number
of decision points where it is obvious to any native
writer that no comma is needed. To account for this
imbalance, we also provide an adjusted agreement
in the final column of the table that excludes all de-
cisions where both annotators agree that no comma
is necessary.
Annotators Agreement Kappa Adj. agr.
1 & 2 97 74 61
1 & 3 98 72 61
2 & 3 98 81 76
Table 3: Agreement over Annotation Training Set (%)
After completing the training phase, we assigned
one annotator the task of annotating our develop-
ment and test data from two different corpora: es-
says written by English as a foreign language learn-
ers (EFL) and essays written by native speakers of
English (Native). For both data sets we selected 60
essays for development and 60 essays for test. The
annotation was carried out using an annotation tool
developed in-house that gives the annotator an easy
to use interface and outputs standoff annotations in
xml format. (3) is an example of an annotated sen-
tence from an EFL essay, where ? ?? marks a span
for annotation.
(3) The new millenium , 1 the 21st century 2
has dawned upon us 3 and this new century
has brought many positive advancements in our
daily lives .
1) Accept, required, parenthetical
2) Insert, required, parenthetical
3) Insert, required, independent clause
Table 4 provides the comma usage information for
the essays in both sets used in development and test-
ing. The table shows the total number of sentences,
commas in the original text that were accepted by
the annotator, and errors (rejected and missing com-
mas) for the 60 essays in each set.
As can be seen in Table 4, the majority of exist-
ing commas (columns Accept plus Rej) in the texts
were accepted by the annotator; about 84% in the
EFL development set, 87% in the EFL test set, 85%
in the Native development set, and 88% in the Na-
tive test set. The important fact uncovered by these
numbers is that most of the commas that learners do
289
Data Set Sent
Commas
Accept
Errors
Rej Miss
EFL Dev 717 474 49 233
EFL Test 683 427 65 232
Native Dev 970 506 86 363
Native Test 839 377 50 314
Table 4: Comma Usage Statistics
use are correct. However, there are a great number
of commas that the annotator inserted (over 80% of
all errors are missing commas) meaning that these
learners are more prone to underusing than overus-
ing commas. Another interesting fact that can be
gleaned from our annotation is that the top five
comma uses, those listed in the first five rows of Ta-
ble 1, account for more than 80% of all commas in
these essays.
7 Error Correction
With a competitive comma restoration system in
place, we turn to the primary task of correcting er-
rors in learner essays. While the task remains simi-
lar to comma restoration, error correction in student
writing brings a new set of challenges, especially
when the writers are non-native. Newswire texts are
most often well-formed, so the system should not
experience interference from other contextual errors
around the missing commas. Sentences taken from
learner texts, though, often contain multiple errors
that can make it difficult to focus on a single problem
at a time. Spelling errors, for example, can exacer-
bate error correction efforts that use contextual lex-
ical features because well-formed text that is often
used for training data is usually free of such noise.
In these experiments, we use the annotated es-
says described in section 6 for evaluation and train
on 40,000 sentences taken from essays written by
both native and non-native high level college stu-
dents. All of the essays are run through automatic
spelling correction to reduce the noise in the test set
before being tagged with the same tagger used in the
comma restoration experiments.
Because we approach comma error correction as
essentially a comma restoration task, we can we use
largely the same system for error correction as we
did for comma restoration. We still employ CRFs
and label each space between words as requiring
a comma or not, however, there is one significant
change to our methodology for this task. Namely,
we can leave the commas that were present in the
text as provided by the writer as we pre-process
the data for error correction, whereas they were re-
moved in the comma restoration task. For error cor-
rection, the task is really comparing the system?s an-
swer to the annotator?s and the learner?s, as opposed
to simply inserting commas into raw text. Leaving
the learners? commas in the text does introduce some
errors to the POS tagging phase. However, since
over 85% of the existing commas in the development
set were judged as acceptable by our annotator (cf.
section 6) , the number of erroneous commas is not
so great as to contaminate the system. Removing all
of the commas would introduce unnecessary errors
in the pre-processing phase.
We also augment the system with three post-
processing filters that we tuned on the development
set. One requires that the classifier be completely
confident before a change is made to an existing
comma; crf++ will give 100% confidence to a single
class in some cases. This filter is based on the fact
that 85% of the existing commas can be expected
to be correct. A similar filter requires that the clas-
sifier be at least 90% confident in a decision to in-
sert a new comma. The final filter, which overrides
any other information provided by the system, does
not allow commas to be inserted before the word be-
cause. These ensure high precision even though they
may reduce recall.
Table 5 provides the accuracy, precision, recall,
F-score, and number of errors in each set for tests on
our 60 annotated EFL and Native essays, and the re-
sult for the combined corpus. The system performs
quite well on the EFL test set, with scores of 94%
precision, 31.7% recall, and 47.4% F-score for the
LexSyn+Dist system. The results for the Native set
are a bit lower, with 84.9% precision, 20% recall,
and 32.4% F-score for the LexSyn+Dist system.
For both data sets, when the distance features are
added to the model, precision increases by 1%, and
in the EFL set, recall also increases. In keeping with
practices established within the field of grammatical
error correction, the system has been optimized for
high precision even at the cost of recall, to ensure
that feedback systems avoid confusing learners by
290
Data System Acc. P R F n
EFL
LexSyn 98.2 92.9 30.9 46.5 297
LexSyn+Dist 98.3 94.0 31.7 47.4 297
Native
LexSyn 97.8 83.9 20.0 32.3 365
LexSyn+Dist 97.8 84.9 20.0 32.4 365
Combined
LexSyn 98.1 88.7 24.9 38.9 662
LexSyn+Dist 98.1 89.8 25.2 39.4 662
Table 5: Comma Error Correction Results (%)
marking correct comma usage as erroneous. Con-
sidering performance over all of the test data, the
system achieves over 89% precision and 25% recall,
results which are comparable to those in other er-
ror correction tasks. For example, the preposition
error detection system described in Tetreault and
Chodorow (2008) achieved 84% precision, 19% re-
call for prepositions.
It is worth noting that the results in Table 5 in-
clude commas that the annotator had marked as
optional. For these, whatever decision the system
makes is scored as correct. Since the grammatical-
ity/readability of the sentence will not be affected by
the presence or absence of a comma in these cases,
we feel this is the fairest assessment of the system.
7.1 Error Analysis
In order to get a sense of what kinds of construc-
tions are difficult for our system, we randomly ex-
tracted 50 sentences from the output that exhibited
at least one wrong comma decision made by the sys-
tem. The 50 sentences contained a combined total of
62 system errors. Among these cases, the most com-
mon context where the system makes the wrong de-
cision is in introductory words and phrases, which is
not surprising given the frequency with which com-
mas occur in these environments in our development
set (about 40% of all commas in the essays). In (4),
for example, the first word, Here, should be followed
by a comma. Since Here is not a common introduc-
tory word in this type of sentence structure in the
training data, this is a difficult case for the system to
correct.
(4) Here we can get specific knowledge in the sci-
ence that we like the most .
The next most common misclassification involves
comma splices, i.e. conjoining complete sentences
with a comma rather than separating them with a full
stop. In (5), for example, there should be a full stop
between college and I, rather than a comma. This
result is not surprising because the system is not
yet equipped to deal with comma splices. Comma
splices are a different type of phenomenon because
correcting them requires removing the comma and
inserting a full stop, essentially two separate steps
rather than the single reject/accept step that the sys-
tem currently handles.
(5) I entered college, I could learn it and make an
effort to achieve my goal.
The next most common context for system errors
was between clauses that are conjoined with a co-
ordinating conjunction as in (6), where there should
not be a comma. In (6), the second clause is actually
a dependent clause, so no comma should precede
the coordinating conjunction. There are a number
of system errors dealing with commas between two
independent clauses. For example in (7), our annota-
tor recommended a comma between things and but,
however the system did not make the insertion. The
problem with these examples likely stems from the
fact that the rule for comma usage in these contexts
is not clearly stated, even in well-respected manu-
als, and therefore likely not clearly understood, even
by high-level native writers. For example, the NYT
style manual (Siegal and Connolly, 1999) states that
?Commas should be used in compound sentences
before conjunctions... When the clauses are excep-
tionally short, however, the comma may be omit-
ted.? Adding a feature that measures clause length
might help, but even then the classifier must rely on
training data that may have considerable variation
as to what length of clauses requires an intervening
comma.
291
(6) They wants to see their portfolio, and what kind
of skill do they have for company.
(7) I have many things but the best is my parents.
Another facet of the data that consistently chal-
lenges the system is the existence of errors other
than the commas in the sentences. Consider the sen-
tence in (8), where erroneous is the original text
from the essay and corrected is a well-formed in-
terpretation.
(8) erroneous: In the other hand , having just
one specific subject , which represents a great
downfall for many students
corrected: On the other hand, knowing only
one subject is a downfall for many students.
The comma after subject is unnecessary, but so is
the word which. In fact, which would normally sig-
nify the beginning of a non-restrictive clause in this
context, which should be set off with a comma. It
is no surprise then, that the system has trouble re-
moving commas in these types of contexts. At least
11 of the 62 system mistakes that we examined have
grammatical errors in the immediate context of the
comma in question, which makes the classification
more difficult.
8 Summary and Conclusion
We presented a novel comma error correction sys-
tem for English that achieves an average of 89% pre-
cision and 25% recall on essays written by learn-
ers of different levels and language backgrounds,
including native English speakers. The system
achieves state-of-the-art performance on the task of
comma restoration, beating previous systems? F-
score and sentence accuracy by 4% and 7%, respec-
tively. We discovered that augmenting lexical fea-
tures, which have been commonly used in previous
work, with the combination and distance features
can improve F-score by as much as 1% in both the
error correction and comma restoration tasks. We
also developed and implemented a novel comma er-
ror annotation scheme.
Additionally, we are interested in the effect of
correct comma placement on other NLP processes.
Jones (1994) and Briscoe and Carroll (1995) show
that adding punctuation to grammars that utilize
part-of-speech (POS) tags, rather than lexical items,
adds more structure and reduces ambiguity as well
as the number of parses for each sentence. Simi-
larly, Doran (1998) and White and Rajkumar (2008)
found that adding punctuation improved parsing re-
sults in tree-adjoining grammar (TAG) and combi-
natorial categorial grammar (CCG) parsing, respec-
tively. These studies all highlight the importance of
correctly inserted punctuation, especially commas,
for parsing. Given these results, we believe that by
enhancing the quality of the text, comma error cor-
rection will improve not only tagging and parsing,
but also the ability of systems to correct many other
forms of grammatical errors, such as those involv-
ing incorrect word order, number disagreement, and
misuse of prepositions, articles, and collocations.
Acknowledgments
We would like to thank Melissa Lopez for help with
annotating our corpora and Michael Flor for kindly
developing an annotation tool for our purposes. We
also thank Aoife Cahill, Robbie Kantor, Markus
Dickinson, Michael Heilman, Nitin Madnani, and
our anonymous reviewers for insightful comments
and discussion.
References
In?aki Alegria, Bertol Arrieta, Arantza Diaz de Ilar-
raza, Eli Izagirre, and Montse Maritxalar. 2006.
Using machine learning techniques to build a
comma checker for Basque. In Proceedings of the
COLING/ACL main conference poster sessions.
Timothy Baldwin and Manuel Paul Anil Kumar
Joseph. 2009. Restoring punctuation and casing
in English text. In Australasian Conference on
Artificial Intelligence?09.
E. Brill. 1993. A Corpus-Based Approach to Lan-
guage Learning. Ph.D. thesis, The University of
Pennsylvania, Philadelpha, PA.
Ted Briscoe and John Carroll. 1995. Developing and
evaluating a probabilistic LR parser of part-of-
speech and punctuation labels. In Proceedings of
the ACL/SIGPARSE 4th International Workshop
on Parsing Technologies.
Jacob Cohen. 1960. A coefficient of agreement for
292
nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Robert J. Connors and Andrea A. Lunsford. 1988.
Frequency of formal errors in current college
writing, or Ma and Pa Kettle do research. Col-
lege Composition and Communication, 39(4).
Daniel Dahlmeier and Hwee Tou Ng. 2011. Gram-
matical error correction with alternating structure
optimization. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies - Volume 1. Association for Computational
Linguistics.
Robert Dale and Adam Kilgarriff. 2010. Helping
our own: Text massaging for computational lin-
guistics as a new shared task. In International
Conference on Natural Language Generation.
Rachele De Felice and Stephen Pulman. 2008. A
classifier-based approach to preposition and de-
terminer error correction in L2 English. In Pro-
ceedings of COLING-08. Manchester.
Markus Dickinson, Ross Israel, and Sun-Hee Lee.
2011. Developing methodology for Korean par-
ticle error detection. In Proceedings of the 6th
Workshop on Innovative Use of NLP for Building
Educational Applications. Portland, Oregon.
Christine Doran. 1998. Incorporating Punctuation
into the Sentence Grammar: A Lexicalized Tree-
Adjoining Grammar Perspective. Ph.D. thesis,
University of Pennsylvania.
Benoit Favre, Dilek Hakkani-Tur, and Elizabeth
Shriberg. 2009. Syntactically-informed models
for comma prediction. In Proceedings of the
2009 IEEE International Conference on Acous-
tics, Speech and Signal Processing.
Michael Gamon. 2010. Using mostly native data
to correct errors in learners? writing: A meta-
classifier approach. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics.
Michael Gamon. 2011. High-order sequence model-
ing for language learner detection high-order se-
quence modeling for language learner error de-
tection. In Proceedings of the 6th Workshop on
Innovative Use of NLP for Building Educational
Applications.
Agustin Gravano, Martin Jansche, and Michiel Bac-
chiani. 2009. Restoring punctuation and capi-
talization in transcribed speech. In Proceedings
of the 2009 IEEE International Conference on
Acoustics, Speech and Signal Processing.
Daniel Hardt. 2001. Comma Checking in Danish. In
Corpus Linguistics.
Robin L. Hill and Wayne S. Murray. 1998. Commas
and spaces: The point of punctuation. In 11th An-
nual CUNY Conference on Human Sentence Pro-
cessing.
Jing Huang and Geoffrey Zweig. 2002. Maximum
entropy model for punctuation annotation from
speech. In Proceedings of ICSLP 2002.
Bernard E. M. Jones. 1994. Exploring the role of
punctuation in parsing natural text. In Proceed-
ings of the 15th conference on Computational lin-
guistics - Volume 1.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning.
Claudia Leacock, Martin Chodorow, Michael Ga-
mon, and Joel R. Tetreault. 2010. Auto-
mated Grammatical Error Detection for Lan-
guage Learners. Synthesis Lectures on Hu-
man Language Technologies. Morgan & Claypool
Publishers.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, and
Mary Harper. 2005. Comparing hmm, maximum
entropy, and conditional random fields for disflu-
ency detection. In In Proceeedings of the Euro-
pean Conference on Speech Communication and
Technology.
Wei Lu and Hwee T. Ng. 2010. Better punctua-
tion prediction with dynamic conditional random
fields. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing.
Helena Moniz, Fernando Batista, Hugo Meinedo,
and Alberto Abad. 2009. Prosodically-based au-
tomatic segmentation and punctuation. In Pro-
293
ceedings of the 5th International Conference on
Speech Prosody.
Diane Nicholls. 1999. The cambridge learner corpus
- error coding and analysis for writing dictionaries
and other books for english learners. In Summer
Workshop on Learner Corpora. Showa Woman?s
University.
Hiromi Oyama. 2010. Automatic error detection
method for japanese particles. Polyglossia, 18.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Eric Brill
and Kenneth Church, editors, Proceedings of the
Empirical Methods in Natural Language Process-
ing.
Alla Rozovskaya, Mark Sammons, Joshua Gioja,
and Dan Roth. 2011. University of Illinois sys-
tem in HOO text correction shared task. In Pro-
ceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 263?266. Association
for Computational Linguistics, Nancy, France.
Stuart M. Shieber and Xiaopeng Tao. 2003. Comma
restoration using constituency information. In
Proceedings of the 2003 Human Language Tech-
nology Conference and Conference of the North
American Chapter of the Association for Compu-
tational Linguistics.
Allan M. Siegal and William G. Connolly. 1999. The
New York Times Manual of Style and Usage : The
Official Style Guide Used by the Writers and Edi-
tors of the World?s Most Authoritative Newspaper.
Crown, rev sub edition.
William Strunk and E. B. White. 1999. The Ele-
ments of Style, Fourth Edition. Longman, fourth
edition.
Joel Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL
writing. In Proceedings of COLING-08. Manch-
ester.
Joel Tetreault, Jennifer Foster, and Martin
Chodorow. 2010. Using parse features for
preposition selection and error detection. In
Proceedings of the ACL 2010 Conference Short
Papers.
University of Chicago. 1993. The Chicago Manual
of Style. University Of Chicago Press, Chicago,
fourteenth edition.
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Pro-
ceedings of the Workshop on Grammar Engineer-
ing Across Frameworks.
294
Proceedings of NAACL-HLT 2013, pages 507?517,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Robust Systems for Preposition Error Correction Using Wikipedia Revisions
Aoife Cahill?, Nitin Madnani?, Joel Tetreault? and Diane Napolitano?
? Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA
{acahill, nmadnani, dnapolitano}@ets.org
? Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USA
Joel.Tetreault@nuance.com
Abstract
We show that existing methods for training
preposition error correction systems, whether
using well-edited text or error-annotated cor-
pora, do not generalize across very differ-
ent test sets. We present a new, large error-
annotated corpus and use it to train systems
that generalize across three different test sets,
each from a different domain and with differ-
ent error characteristics. This new corpus is
automatically extracted from Wikipedia revi-
sions and contains over one million instances
of preposition corrections.
1 Introduction
One of the main themes that has defined the field of
automatic grammatical error correction has been the
availability of error-annotated learner data to train
and test a system. Some errors, such as determiner-
noun number agreement, are easily corrected us-
ing rules and regular expressions (Leacock et al,
2010). On the other hand, errors involving the usage
of prepositions and articles are influenced by sev-
eral factors including the local context, the prior dis-
course and semantics. These errors are better han-
dled by statistical models which potentially require
millions of training examples.
Most statistical approaches to grammatical error
correction have used one of the following training
paradigms: 1) training solely on examples of cor-
rect usage (Han et al, 2006); 2) training on exam-
ples of correct usage and artificially generated er-
rors (Rozovskaya and Roth, 2010); and 3) training
on examples of correct usage and real learner er-
rors (Dahlmeier and Ng, 2011; Dale et al, 2012).
The latter two methods require annotated corpora of
errors, and while they have shown great promise,
manually annotating grammatical errors in a large
enough corpus of learner writing is often a costly
and time-consuming endeavor.
In order to efficiently and automatically acquire a
very large corpus of annotated learner errors, we in-
vestigate the use of error corrections extracted from
Wikipedia revision history. While Wikipedia re-
vision history has shown promise for other NLP
tasks including paraphrase generation (Max and
Wisniewski, 2010; Nelken and Yamangil, 2008) and
spelling correction (Zesch, 2012), this resource has
not been used for the task of grammatical error cor-
rection.
To evaluate the usefulness of Wikipedia revision
history for grammatical error correction, we address
the task of correcting errors in preposition selection
(i.e., where the context licenses the use of a prepo-
sition, but the writer selects the wrong one). We
first train a model directly on instances of correct
and incorrect preposition usage extracted from the
Wikipedia revision data. We also generate artificial
errors using the confusion distributions derived from
this data. We compare both of these approaches to
models trained on well-edited text and evaluate each
on three test sets with a range of different character-
istics. Each training paradigm is applied to multiple
data sources for comparison. With these multiple
evaluations, we address the following research ques-
tions:
1. Across multiple test sets, which data source
507
is more useful for correcting preposition er-
rors: a large amount of well-edited text, a large
amount of potentially noisy error-annotated
data (either artificially generated or automati-
cally extracted) or a smaller amount of higher
quality error-annotated data?
2. Given error-annotated data, is it better to train
on the corrections directly or to use the con-
fusion distributions derived from these correc-
tions for generating artificial errors in well-
edited text?
3. What is the impact of having a mismatch in the
error distributions of the training and test sets?
2 Related Work
In this section, we only review work in preposi-
tion error correction in terms of the three training
paradigms and refer the reader to Leacock et al
(2010) for a more comprehensive review of the field.
2.1 Training on Well-Edited Text
Early approaches to error detection and correction
did not have access to large amounts of error-
annotated data to train statistical models and thus,
systems were trained on millions of well-edited ex-
amples from news text instead (Gamon et al, 2008;
Tetreault and Chodorow, 2008; De Felice and Pul-
man, 2009). Feature sets usually consisted of n-
grams around the preposition, POS sequences, syn-
tactic features and semantic information. Since the
model only had knowledge of correct usage, an error
was flagged if the system?s prediction for a particu-
lar preposition context differed from the preposition
the writer used.
2.2 Artificial Errors
The issue with training solely on correct usage was
that the systems had no knowledge of typical learner
errors. Ideally, a system would be trained on ex-
amples of correct and incorrect usage, however, for
many years, such error-annotated corpora were not
available. Instead, several researchers generated ar-
tificial errors based on the error distributions derived
from the error-annotated learner corpora available at
the time. Izumi et al (2003) was the first to evaluate
a model trained on incorrect usage as well as artifi-
cial errors for the task of correcting several different
error types, including prepositions. However, with
limited training data, system performance was quite
poor. Rozovskaya and Roth (2010) evaluated dif-
ferent ways of generating artificial errors and found
that a system trained on artificial errors could outper-
form the more traditional training paradigm of using
only well-edited texts. Most recently, Imamura et al
(2012) showed that performance could be improved
by training a model on artificial errors and address-
ing domain adaptation for the task of Japanese par-
ticle correction.
2.3 Error-Annotated Learner Corpora
Recently, error-annotated learner data has become
more readily and publicly available allowing models
to be trained on both examples of correct usage as
well typical learner errors. Han et al (2010) showed
that a preposition error detection and correction sys-
tem trained on 100,000 annotated preposition errors
from the Chungdahm Corpus of Korean Learner En-
glish (in addition to 1 million examples of correct
usage) outperformed a model trained only on 5 mil-
lion examples of correct usage. Gamon (2010) and
Dahlmeier and Ng (2011) showed that combining
models trained separately on examples of correct
and incorrect usage could also improve the perfor-
mance of a preposition error correction system.
3 Mining Wikipedia Revisions for
Grammatical Error Corrections
3.1 Related Work
Many NLP researchers have taken advantage of the
wealth of information available in Wikipedia revi-
sions. Dutrey et al (2011) define a typology of mod-
ifications found in the French Wikipedia (WiCo-
PaCo). They show that the kinds of edits made range
from specific lexical changes to more general rewrite
edits. Similar types of edits are found in the En-
glish Wikipedia. The data extracted from Wikipedia
revisions has been used for a wide variety of tasks
including spelling correction (Max and Wisniewski,
2010; Zesch, 2012), lexical error detection (Nelken
and Yamangil, 2008), sentence compression (Ya-
mangil and Nelken, 2008), paraphrase generation
(Max and Wisniewski, 2010; Nelken and Yamangil,
2008), lexical simplification (Yatskar et al, 2010)
and entailment (Zanzotto and Pennacchiotti, 2010;
508
(1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at? in) the refrigerator.
(2) [Wiki clean] Also none of the witnesses present (of? on) those dates supports Ranneft?s claims.
(3) [Wiki dirty] . . . cirque has a permanent production (to? at) the Mirage, love.
(4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his
performances for French tourists (in? to) Petersburg.
Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions. The second preposition is
assumed to be the correction.
Cabrio et al, 2012). To our knowledge, no one has
previously extracted data for training a grammatical
error detection system from Wikipedia revisions.
3.2 Extracting Preposition Correction Data
from Wikipedia Revisions
As the source of our Wikipedia revisions, we used an
XML snapshot of Wikipedia generated in July 2011
containing 8,735,890 articles and 288,583,063 revi-
sions.1 We then used the following process to ex-
tract preposition errors and their corresponding cor-
rections from this snapshot:
Step 1: Extract the plain text versions of all revi-
sions of all articles using the Java Wikipedia
Library (Ferschke et al, 2011).
Step 2: For each Wikipedia article, compare each
revision with the revision immediately preced-
ing it using an efficient diff algorithm.2
Step 3: Compute all 1-word edit chains for the arti-
cle, i.e., sequences of related edits derived from
all revisions of the same article. For example,
say revision 10 of an article inserts the preposi-
tion of into a sentence and revision 12 changes
that preposition to on. Assuming that no other
revisions change this sentence, the correspond-
ing edit chain would contain the following 3 el-
ements: ?of?on. The extracted chains con-
tain the full context on either side of the 1-word
edit, up to the automatically detected sentence
boundaries.
Step 4: (a) Ignore any circular chains, i.e., where
the first element in the edit chain is the same as
the last element. (b) Collapse all non-circular
1http://dumps.wikimedia.org/enwiki/
2http://code.google.com/p/google-diff-match-patch/
chains, i.e., only retain the first and the last ele-
ments in a chain. Both these decisions are mo-
tivated by the assumption that the intermediate
links in the chain are unreliable for training an
error correction system since a Wikipedia con-
tributor modified them.
Step 5 : From all remaining 2-element chains, find
those where a preposition is replaced with an-
other preposition. If the preposition edit is the
only edit in the sentence, we convert the chain
into a sentence pair and label it clean. If there
are other 1-word edits but not within 5 words of
the preposition edit on either side, we label the
sentence somewhat clean. Otherwise, we label
it dirty. The motivation is that the presence of
other nearby edits make the preposition correc-
tion less reliable when used in isolation, due to
the possible dependencies between corrections.
All extracted sentences were part-of-speech tagged
using the Stanford Tagger (Toutanova et al, 2003).
Using the above process, we are able to extract ap-
proximately 2 million sentences containing preposi-
tions errors and their corrections. Some examples
of the sentences we extracted are given in Figure 1.
Example (4) shows an example of a bad correction.
4 Corpora
We use several corpora for training and testing our
preposition error correction system. The proper-
ties of each are outlined in Table 1, organized by
paradigm. For each corpus we report the total num-
ber of prepositions used for training, as well as the
number and percentage of preposition corrections.
4.1 Well-edited Text
We train our system on two well-edited corpora.
The first is the same corpus used by Tetreault and
509
Corpus Total # Preps # Corrected Preps
Well-edited Text
Wikipedia Snapshot (10m sents) 26,069,860 0 (0%)
Lexile/SJM 6,719,077 0 (0%)
Artificially Generated
Errors
Wikipedia Snapshot 26,127,464 2,844,227 (10.9%)
Lexile/SJM 6,723,206 792,195 (11.8%)
Naturally Occurring
Errors
Wikipedia Revisions All 7,125,317 1,027,643 (20.6%)
Wikipedia Revisions ?Clean 3,001,900 381,644 (12.7%)
Wikipedia Revisions Clean 1,978,802 266,275 (14.4%)
Lang-8 129,987 53,493 (41.2%)
NUCLE Train 72,741 922 (1.3%)
Test Corpora
NUCLE Test 9,366 125 (1.3%)
FCE 33,243 2,900 (8.7%)
HOO 2011 Test 1,703 81 (4.8%)
Table 1: Corpora characteristics
Chodorow (2008), comprising roughly 1.8 million
sentences from the San Jose Mercury News Corpus3
and roughly 1.8 million sentences from grades 11
and 12 of the MetaMetrics Lexile Corpus. Our sec-
ond corpus is a random sample of 10 million sen-
tences containing at least one preposition from the
June 2012 snapshot of English Wikipedia Articles.4
4.2 Artificially Generated Errors
Similar to Foster and Andersen (2009) and Ro-
zovskaya and Roth (2010), we artificially introduce
preposition errors into well-edited corpora (the two
described above). We do this based on a distribu-
tion of possible confusions and train a model that
is aware of the corrections. The two sets of con-
fusion distributions we used were derived based on
the errors extracted from Wikipedia revisions and
Lang-8 respectively (discussed in Section 4.3). For
each corrected preposition pi in the revision data,
we calculated P (pi|pj), where pj is each of the pos-
sible original prepositions that were confused with
pi. Then, for each sentence in the well-edited text,
all prepositions are extracted. A preposition is ran-
domly selected (without replacement) and changed
based on the distribution of possible confusions
(note that the original preposition is also included
in the distribution, usually with a high probabil-
3The San Jose Mercury News is available from the Linguis-
tic Data Consortium (catalog number LDC93T3A).
4We used a newer version of the Wikipedia text for the well-
edited text, since we assume that more recent versions of the
text will be most grammatical, and therefore closer to well-
edited.
ity, meaning that there is a strong preference not to
change the preposition). If a preposition is changed
to something other than the original preposition, all
remaining prepositions in the sentence are left un-
changed.
4.3 Naturally Occurring Errors
We have a number of corpora that contain annotated
preposition errors. Note that we are only considering
incorrectly selected prepositions, we do not consider
missing or extraneous.
NUCLE The NUS Corpus of Learner English (NU-
CLE)5 contains one million words of learner
essay text, manually annotated with error tags
and corrections. We use the same training, dev
and test splits as Dahlmeier and Ng (2011).
FCE The CLC FCE Dataset6 is a collection of
1,244 exam scripts written by learners of En-
glish as part of the Cambridge ESOL First Cer-
tificate in English (Yannakoudakis et al, 2011).
It includes demographic metadata about the
candidate, a grade for each essay and manually-
annotated error corrections.
Wikipedia We use three versions of the preposi-
tion errors extracted from the Wikipedia revi-
sions as described in Section 3.2. The first in-
cludes corrections where the preposition was
the only word corrected in the entire sentence
5http://bit.ly/nuclecorpus
6http://ilexir.co.uk/applications/clc-fce-dataset/
510
(clean). The second contains all clean cor-
rections, as well as all corrections where there
were no other edits within a five-word span on
either side of the preposition (?clean). The
third contains all corrections regardless of any
other changes in the surrounding context (all).
Lang-8 The Lang-8 website contains journals writ-
ten by language learners, where native speakers
highlight and correct errors on a sentence-by-
sentence basis. As a result, it contains typical
grammatical mistakes made by language learn-
ers, which can be easily downloaded. We auto-
matically extract 75,622 sentences with prepo-
sition errors and corrections from the first mil-
lion journal entries.7
HOO 2011 We take the test set from the HOO 2011
shared task (Dale and Kilgarriff, 2011) and ex-
tract all examples of preposition selection er-
rors. The texts are fragments of ACL papers
that have been manually annotated for gram-
matical errors.8
It is important to note that the three test sets we use
are from entirely different domains: exam scripts
from non-native English speakers (FCE), essays by
highly proficient college students in Singapore (NU-
CLE) and ACL papers (HOO). In addition, they have
a different number of total prepositions as well as er-
roneous prepositions.
5 Preposition Error Correction
Experiments
We use the preposition error correction model de-
scribed in Tetreault and Chodorow (2008)9 to eval-
uate the many ways of using Wikipedia error cor-
rections as described in the Section 4. We use this
system since it has been recreated for other work
(Dahlmeier and Ng, 2011; Tetreault et al, 2010) and
is similar in methodology to Gamon et al (2008)
7Tajiri et al (2012) extract a corpus of English verb phrases
corrected for tense/aspect errors from Lang-8. They kindly pro-
vided us with their scripts to carry out the scraping of Lang-8.
8The results of the HOO 2011 shared task were not reported
at level of preposition selection error, therefore it is not possible
to compare the results presented in this paper with those results.
9Note that in that work, the model was evaluated in terms of
preposition error detection rather than correction, however the
model itself does not change.
and De Felice and Pulman (2009). In short, the
method models the problem of preposition error cor-
rection (for replacement errors) as a 36-way classifi-
cation problem using a multinomial logistic regres-
sion model.10 The system uses 25 lexical, syntac-
tic and n-gram features derived from the contexts of
each preposition training instance.
We modified the training paradigm of Tetreault
and Chodorow (2008) so that a model could be
trained on examples of correct usage as well as ac-
tual errors. We did this by adding a new feature
specifying the writer?s original preposition (as in
Han et al (2010) and Dahlmeier and Ng (2011)).
5.1 Results
We train a preposition correction system using each
of the three data paradigms and test on the FCE,
NUCLE and HOO 2011 test corpora. For each
preposition in the test corpus, we record whether
the system predicted that it should be changed,
and if so, what it should be changed to. We then
compare the prediction to the annotation in the test
corpus. We report results in terms of f-score, where
precision and recall are calculated as follows:11
Precision = Number of correct preposition correctionsTotal number of corrections suggested
Recall = Number of correct preposition correctionsTotal number of corrections in test set
Note that due to the high volume of unchanged
prepositions in the test corpus, we obtain very high
accuracies, which are not indicative of true perfor-
mance, and are not included in our results.
The results of our experiments are presented in
Table 2.12 The first part of the table shows the f-
scores of preposition error correction systems that
10We use liblinear (Fan et al, 2008) with the L1-regularized
logistic regression solver and default parameters.
11As Chodorow et al (2012) note, it is not clear how to han-
dle cases where the system predicts a preposition that is neither
the same as the writer preposition nor the correct preposition.
We count these cases as false positives.
12No thresholds were used in the systems that were trained
on well-edited text. Traditionally, thresholds are applied so as
to only predict a correction when the system is highly confident.
This has the effect of increasing precision at the cost of recall,
and sometimes leads to an overall improved f-score. Here we
take the prediction of the system, regardless of the confidence,
reflecting a lower-bound of this method.
511
Data Source Paradigm CLC-FCE NUCLE HOO2011
N=33,243 N=9,366 N=1,703
Without
Wikipedia
Revisions
(nonWikiRev)
Wikipedia Snapshot Well-edited Text 24.43? 5.02? 12.36?
Lexile/SJM Well-edited Text 24.73? 4.29? 9.73?
Wikipedia Snapshot Artificial Errors (Lang-8) 42.15? 19.91? 28.75
Lexile/SJM Artificial Errors (Lang-8) 45.36 18.00? 25.15
Lang-8 Error-annotated Text 38.22? 8.18? 24.00
NUCLE train Error-annotated Text 5.38? 20.14 4.82?
With
Wikipedia
Revisions
(WikiRev)
Wikipedia Snapshot Artificial Errors (Wiki) 31.17? 24.52 28.30
Lexile/SJM Artificial Errors (Wiki) 34.35? 23.38 32.76
Wikipedia Revisions All Error-annotated Text 33.59? 26.39 36.84
Wikipedia Revisions ?Clean Error-annotated Text 29.68? 22.13 36.04
Wikipedia Revisions Clean Error-annotated Text 28.09? 21.74 28.30
Table 2: Preposition selection error correction results (f-score). The systems with scores in bold are statistically
significantly better than all systems marked with an asterisk (p < 0.01). Confidence intervals were obtained using
bootstrap resampling with 50,000 replicates.
one might be able to train with publicly available
data excluding the Wikipedia revisions that we have
extracted. We refer to these systems as nonWikiRev
systems. The second part of the table shows the f-
scores of systems trained on the Wikipedia revisions
data ? either directly on the annotated errors or on
the artificial errors produced using the confusion dis-
tributions derived from these annotated errors. We
refer to this second set of systems as WikiRev sys-
tems. The nonWikiRev systems perform inconsis-
tently, heavily dependent on the characteristics of
the test set in question. On the other hand, it is
obvious that the WikiRev systems ? while not al-
ways outperforming the best nonWikiRev systems
? generalize much better across the three test sets.
In fact, for the NUCLE test set, the best WikiRev
system performs as well as the nonWikiRev system
trained on data from the same domain and with iden-
tical error characteristics as the test set. The distri-
butions of errors in the three test sets are not sim-
ilar, and therefore, the stability in performance of
the WikiRev systems cannot be attributed to the hy-
pothesis that the WikiRev training data error distri-
butions are more similar to the test data than any of
the other training corpora. Therefore, we claim that
if a preposition error correction system is to be de-
ployed on data for which the error characteristics are
not known in advance, i.e. most real-world scenar-
ios, training the system using Wikipedia revisions is
likely to be the most robust option.
6 Discussion
We examine the results of our experiments in light
of the research questions we posed in Section 1.
6.1 Which Data Source is More Useful?
We wanted to know whether it was better to have
a smaller corpus of carefully annotated corrections,
or a much larger (but automatically generated, and
therefore noisier) error-annotated corpus. We also
wanted to compare this scenario to training on large
amounts of well-edited text. From our experiments,
it is clear that the composition of the test set plays
a major role in answering this question. On a test
set with few corrections (NUCLE), training on well-
edited text (and without using thresholds) performs
particularly poorly. On the other hand, when eval-
uating on the FCE test set which contains far more
errors, training on well-edited text performs reason-
ably well (though statistically significantly worse
than training on all of the Wikipedia errors). Sim-
ilarly, training on the smaller, high-quality NU-
CLE corpus and evaluating on the NUCLE test set
achieves good results, however training on NUCLE
and testing on FCE achieves the lowest f-score of all
our systems on that test set.
Figure 2 shows the learning curves obtained by
increasing the size of the training data for two
of the test sets.13 Although one might assume
13For space reasons, the graph for HOO2011 is omitted. Also
note that the results in Table 2 may not appear in the graph,
512
Wiki (All)Wiki (Clean)Lang-8NUCLELexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
10
20
30
40
50
log(training data size in thousands of instances)1 2 3 4
Wiki (All)Wiki (Clean)Lang-8NUCLELexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
5
10
15
20
25
1 2 3 4
(a) NUCLE
(b) FCE
Figure 2: The effect of varying the size of the training corpus
that Wikipedia-clean would be more reliable than
Wikipedia-all, the cleanness of the Wikipedia data
seems to make very little difference, probably be-
cause the data extracted in the dirty contexts is not
as noisy as we expected. Interestingly, it also seems
that additional data would lead to further improve-
ments for models trained on artificial errors in Lexile
data and for those trained on all of the automatically
extracted Wikipedia errors.
Another interesting aspect of Figure 2 is that
since we were sampling at specific data points which did not
correspond exactly to the total sizes of the training corpora.
training on the Lang-8 data shows a very steep rising
trend. This suggests that automatically-scraped data
that is highly targeted towards language learners is
very useful in correcting preposition errors in texts
where they are reasonably frequent.
6.2 Natural or Artificially Generated Errors?
Table 2 shows that training on artificially generated
errors via Wikipedia revisions performs fairly con-
sistently across test corpora. While using Lang-8
for artificial error generation is also quite promis-
ing for FCE, it does not generalize across test sets.
513
Wiki (All)Wiki (Clean)Lang-8Lexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
10
20
30
40
50
Percentage of Errors in Training Data0 5 10 15 20 25 30 35 40 45 50 55
Wiki (All)Wiki (Clean)Lang-8Lexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
5
10
15
20
25
30
0 5 10 15 20 25 30 35 40 45 50 55
(a) NUCLE
(b) FCE
Figure 3: The effect of varying the percentage of errors in the training corpus
On FCE it achieves the highest results, on NUCLE
it performs statistically significantly worse than the
best system, and on HOO 2011 it achieves a lower
(though not statistically significant) result than the
best system. This highlights that extracting errors
from Wikipedia is useful in two ways: (1) training a
system on the errors alone works well and (2) gener-
ating artificial errors in well-edited corpora of differ-
ent domains and training a system on that also works
well. It also indicates that if the system were to be
applied to a specific domain, applying the confusion
distributions to a domain specific corpus ? if avail-
able ? would likely yield the best results.
6.3 Mismatching Distributions
The proportion of errors in the training and test data
plays an important role in the performance of any
preposition error correction system. This is clearly
evident by comparing system performances across
the three test sets which have fairly different compo-
sitions. FCE contains a much higher proportion of
errors than NUCLE, and HOO falls somewhere in
between. Interestingly, the system trained on Lang-
8 data (which contains the highest proportion of er-
514
rors among all training corpora) performs best on
the FCE data. On the other hand, the same sys-
tem performs poorly on NUCLE test which contains
far fewer errors. In this instance, the system learns
to predict an incorrect preposition too often. We
see a similar pattern with the system trained on the
NUCLE training data. It performs poorly on FCE
which contains many errors, but well on NUCLE
test which contains a similar proportion of errors.
In order to better understand the relationship be-
tween the percentage of errors in the training data
and system performance, we vary the percentage of
errors in each training corpus from 1-50% and test
on the unchanged FCE and NUCLE test corpora.
For each training corpus, we reduce the size to be
twice the size of the total number of errors.14 Keep-
ing this size constant, we then artificially change the
percentage of errors. Note that because the total size
of the corpus has changed, the results in Table 2 may
not appear in the graph. Figure 3 shows the effect on
f-score when the data composition is changed. For
both test sets, there is a peak after which increas-
ing the proportion of errors in the training corpus is
detrimental. For NUCLE test with its low number
of preposition errors, this peak is very pronounced.
For FCE, it is more of a gentle degradation in per-
formance, but the pattern is clear. Also noteworthy
is the fact that the degradation for models trained on
artificial errors is less steep suggesting that they may
be more stable across test sets.
In general, these results indicate that when
building a preposition error detection using error-
annotated data, the characteristics of the data to
which the system will be applied should play a vital
role in how the system is to be trained. Our results
show that the WikiRev systems are robust across
test sets, however if the exact distribution of errors
in the data is known in advance, other models may
perform better.
7 Conclusion
Although previous approaches to preposition er-
ror correction using either well-edited text or small
hand-annotated corrections performed well on some
specific test set, they did not generalize well across
14We omit the NUCLE train corpus from this comparison,
because it contains too few errors to obtain a meaningful result.
very different test sets. In this paper, we present
work that automatically extracts preposition error
corrections from Wikipedia Revisions and uses it
to build robust error correction systems. We show
that this data is useful for two purposes. Firstly, a
model trained directly on the corrections performs
well across test sets. Secondly, models trained on ar-
tificial errors generated from the distribution of con-
fusions in the Wikipedia data perform equally well.
The distribution of confusions can also be applied to
other well-edited corpora in different domains, pro-
viding a very powerful method of automatically gen-
erating error corpora. The results of our experiments
also highlight the importance of the distribution of
expected errors in the test set. Models that perform
well on one kind of distribution may not necessar-
ily work on a completely different one, as evident
in the performances of the systems trained on either
Lang-8 or NUCLE. In general, the WikiRev mod-
els perform well across distributions. We also con-
ducted some preliminary system combination exper-
iments and found that while they yielded promising
results, further investigation is necessary. We have
also made the Wikipedia preposition correction cor-
pus available for download.15
In future work, we will examine whether the
results we obtain for English generalize to other
Wikipedia languages. We also plan to extract multi-
word corrections for other types of errors and to ex-
amine the usefulness of including error contexts in
our confusion distributions (e.g., preposition confu-
sions following verbs versus those following nouns).
Acknowledgments
The authors would like to thank Daniel Dahlmeier,
Torsten Zesch, Mamoru Komachi, Tajiri Toshikazu,
Tomoya Mizumoto and Yuji Matsumoto for provid-
ing scripts and data that enabled us to carry out
this research. We would also like to thank Martin
Chodorow and the anonymous reviewers for their
helpful suggestions and comments.
References
Elena Cabrio, Bernardo Magnini, and Angelina Ivanova.
2012. Extracting Context-Rich Entailment Rules from
15http://bit.ly/etsprepdata
515
Wikipedia Revision History. In Proceedings of the 3rd
Workshop on the People?s Web Meets NLP: Collabora-
tively Constructed Semantic Resources and their Ap-
plications to NLP, pages 34?43, Jeju, Republic of Ko-
rea, July. Association for Computational Linguistics.
Martin Chodorow, Markus Dickinson, Ross Israel, and
Joel Tetreault. 2012. Problems in Evaluating Gram-
matical Error Detection Systems. In Proceedings of
COLING 2012, pages 611?628, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical Error Correction with Alternating Structure Op-
timization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 915?923, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 242?249, Nancy, France, September. As-
sociation for Computational Linguistics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?62,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Rachele De Felice and Stephen G. Pulman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3):512?528.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aure?lien Max. 2011. Local modifications and
paraphrases in Wikipedias revision history. SEPLN
journal(Revista de Procesamiento del Lenguaje Nat-
ural), 46:51?58.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently Access-
ing Wikipedia?s Edit History. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
System Demonstrations.
Jennifer Foster and Oistein Andersen. 2009. Gen-
ERRate: Generating Errors for Use in Grammatical
Error Detection. In Proceedings of the Fourth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 82?90, Boulder, Colorado,
June. Association for Computational Linguistics.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using Contextual Speller
Techniques and Language Modeling for ESL Error
Correction. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 449?456, Hyderabad, India.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171, Los An-
geles, California, June. Association for Computational
Linguistics.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using Error-Annotated ESL Data
to Develop an ESL Error Correction System. In Pro-
ceedings of the Seventh International Conference on
Language Resources and Evaluation (LREC), Malta.
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and
Hitoshi Nishikawa. 2012. Grammar Error Correc-
tion Using Pseudo-Error Sentences and Domain Adap-
tation. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 388?392, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic Error
Detection in the Japanese Learners? English Spoken
Data. In The Companion Volume to the Proceedings
of 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 145?148, Sapporo, Japan,
July. Association for Computational Linguistics.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Aure?lien Max and Guillaume Wisniewski. 2010. Mining
Naturally-occurring Corrections and Paraphrases from
Wikipedia?s Revision History. In Nicoletta Calzo-
lari (Conference Chair), Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh conference on International Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, may. European Language Resources Associa-
tion (ELRA).
Rami Nelken and Elif Yamangil. 2008. Mining
Wikipedias Article Revision History for Training
516
Computational Linguistics Algorithms. In Proceed-
ings of the 1st AAAI Workshop on Wikipedia and Arti-
ficial Intelligence, pages 31?36, Chicago, IL.
Alla Rozovskaya and Dan Roth. 2010. Generating Con-
fusion Sets for Context-Sensitive Error Correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 961?
970, Cambridge, MA, October. Association for Com-
putational Linguistics.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and Aspect Error Correction
for ESL Learners Using Global Context. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), Short Papers, pages
198?202, Jeju Island, Korea.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 865?872, Manchester, UK.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using Parse Features for Preposition Selection
and Error Detection. In Proceedings of the ACL 2010
Conference Short Papers, pages 353?358, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich Part-of-speech
Tagging with a Cyclic Dependency Network. In Pro-
ceedings of NAACL, pages 173?180.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia Revision Histories for Improving Sentence
Compression. In Proceedings of ACL-08: HLT, Short
Papers, pages 137?140, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 180?189, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 365?368, Los Angeles, California, June.
Association for Computational Linguistics.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of the
2nd Workshop on The People?s Web Meets NLP: Col-
laboratively Constructed Semantic Resources, pages
28?36, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Torsten Zesch. 2012. Measuring Contextual Fitness Us-
ing Error Contexts Extracted from the Wikipedia Revi-
sion History. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 529?538, Avignon, France,
April. Association for Computational Linguistics.
517
Proceedings of the ACL 2010 Conference Short Papers, pages 353?358,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Using Parse Features for Preposition Selection and Error Detection
Joel Tetreault
Educational Testing Service
Princeton
NJ, USA
JTetreault@ets.org
Jennifer Foster
NCLT
Dublin City University
Ireland
jfoster@computing.dcu.ie
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow
@hunter.cuny.edu
Abstract
We evaluate the effect of adding parse fea-
tures to a leading model of preposition us-
age. Results show a significant improve-
ment in the preposition selection task on
native speaker text and a modest increment
in precision and recall in an ESL error de-
tection task. Analysis of the parser output
indicates that it is robust enough in the face
of noisy non-native writing to extract use-
ful information.
1 Introduction
The task of preposition error detection has re-
ceived a considerable amount of attention in re-
cent years because selecting an appropriate prepo-
sition poses a particularly difficult challenge to
learners of English as a second language (ESL).
It is not only ESL learners that struggle with En-
glish preposition usage ? automatically detecting
preposition errors made by ESL speakers is a chal-
lenging task for NLP systems. Recent state-of-the-
art systems have precision ranging from 50% to
80% and recall as low as 10% to 20%.
To date, the conventional wisdom in the error
detection community has been to avoid the use
of statistical parsers under the belief that a WSJ-
trained parser?s performance would degrade too
much on noisy learner texts and that the tradi-
tionally hard problem of prepositional phrase at-
tachment would be even harder when parsing ESL
writing. However, there has been little substantial
research to support or challenge this view. In this
paper, we investigate the following research ques-
tion: Are parser output features helpful in mod-
eling preposition usage in well-formed text and
learner text?
We recreate a state-of-the-art preposition usage
system (Tetreault and Chodorow (2008), hence-
forth T&C08) originally trained with lexical fea-
tures and augment it with parser output features.
We employ the Stanford parser in our experiments
because it consists of a competitive phrase struc-
ture parser and a constituent-to-dependency con-
version tool (Klein and Manning, 2003a; Klein
and Manning, 2003b; de Marneffe et al, 2006;
de Marneffe and Manning, 2008). We com-
pare the original model with the parser-augmented
model on the tasks of preposition selection in well-
formed text (fluent writers) and preposition error
detection in learner texts (ESL writers).
This paper makes the following contributions:
? We demonstrate that parse features have a
significant impact on preposition selection in
well-formed text. We also show which fea-
tures have the greatest effect on performance.
? We show that, despite the noisiness of learner
text, parse features can actually make small,
albeit non-significant, improvements to the
performance of a state-of-the-art preposition
error detection system.
? We evaluate the accuracy of parsing and
especially preposition attachment in learner
texts.
2 Related Work
T&C08, De Felice and Pulman (2008) and Ga-
mon et al (2008) describe very similar preposi-
tion error detection systems in which a model of
correct prepositional usage is trained from well-
formed text and a writer?s preposition is com-
pared with the predictions of this model. It is
difficult to directly compare these systems since
they are trained and tested on different data sets
353
but they achieve accuracy in a similar range. Of
these systems, only the DAPPER system (De Fe-
lice and Pulman, 2008; De Felice and Pulman,
2009; De Felice, 2009) uses a parser, the C&C
parser (Clark and Curran, 2007)), to determine
the head and complement of the preposition. De
Felice and Pulman (2009) remark that the parser
tends to be misled more by spelling errors than
by grammatical errors. The parser is fundamental
to their system and they do not carry out a com-
parison of the use of a parser to determine the
preposition?s attachments versus the use of shal-
lower techniques. T&C08, on the other hand, re-
ject the use of a parser because of the difficulties
they foresee in applying one to learner data. Her-
met et al (2008) make only limited use of the
Xerox Incremental Parser in their preposition er-
ror detection system. They split the input sentence
into the chunks before and after the preposition,
and parse both chunks separately. Only very shal-
low analyses are extracted from the parser output
because they do not trust the full analyses.
Lee and Knutsson (2008) show that knowl-
edge of the PP attachment site helps in the task
of preposition selection by comparing a classifier
trained on lexical features (the verb before the
preposition, the noun between the verb and the
preposition, if any, and the noun after the preposi-
tion) to a classifier trained on attachment features
which explicitly state whether the preposition is
attached to the preceding noun or verb. They also
argue that a parser which is capable of distinguish-
ing between arguments and adjuncts is useful for
generating the correct preposition.
3 Augmenting a Preposition Model with
Parse Features
To test the effects of adding parse features to
a model of preposition usage, we replicated the
lexical and combination feature model used in
T&C08, training on 2M events extracted from a
corpus of news and high school level reading ma-
terials. Next, we added the parse features to this
model to create a new model ?+Parse?. In 3.1 we
describe the T&C08 system and features, and in
3.2 we describe the parser output features used to
augment the model. We illustrate our features us-
ing the example phrase many local groups around
the country. Fig. 1 shows the phrase structure tree
and dependency triples returned by the Stanford
parser for this phrase.
3.1 Baseline System
The work of Chodorow et al (2007) and T&C08
treat the tasks of preposition selection and er-
ror detection as a classification problem. That
is, given the context around a preposition and a
model of correct usage, a classifier determines
which of the 34 prepositions covered by the model
is most appropriate for the context. A model of
correct preposition usage is constructed by train-
ing a Maximum Entropy classifier (Ratnaparkhi,
1998) on millions of preposition contexts from
well-formed text.
A context is represented by 25 lexical features
and 4 combination features:
Lexical Token and POS n-grams in a 2 word
window around the preposition, plus the head verb
in the preceding verb phrase (PV), the head noun
in the preceding noun phrase (PN) and the head
noun in the following noun phrase (FN) when
available (Chodorow et al, 2007). Note that these
are determined not through full syntactic parsing
but rather through the use of a heuristic chun-
ker. So, for the phrase many local groups around
the country, examples of lexical features for the
preposition around include: FN = country, PN =
groups, left-2-word-sequence = local-groups, and
left-2-POS-sequence = JJ-NNS.
Combination T&C08 expand on the lexical fea-
ture set by combining the PV, PN and FN fea-
tures, resulting in features such as PN-FN and
PV-PN-FN. POS and token versions of these fea-
tures are employed. The intuition behind creat-
ing combination features is that the Maximum En-
tropy classifier does not automatically model the
interactions between individual features. An ex-
ample of the PN-FN feature is groups-country.
3.2 Parse Features
To augment the above model we experimented
with 14 features divided among five main classes.
Table 1 shows the features and their values for
our around example. The Preposition Head and
Complement feature represents the two basic at-
tachment relations of the preposition, i.e. its head
(what it is attached to) and its complement (what
is attached to it). Relation specifies the relation
between the head and complement. The Preposi-
tion Head and Complement Combined features
are similar to the T&C08 Combination features
except that they are extracted from parser output.
354
NP
NP
DT
many
JJ
local
NNS
groups
PP
IN
around
NP
DT
the
NN
country
amod(groups-3, many-1)
amod(groups-3, local-2)
prep(groups-3, around-4)
det(country-6, the-5)
pobj(around-4, country-6)
Figure 1: Phrase structure tree and dependency
triples produced by the Stanford parser for the
phrase many local groups around the country
Prep. Head & Complement
1. head of the preposition: groups
2. POS of the head: NNS
3. complement of the preposition: country
4. POS of the complement: NN
Prep. Head & Complement Relation
5. Prep-Head relation name: prep
6. Prep-Comp relation name: pobj
Prep. Head & Complement Combined
7. Head-Complement tokens: groups-country
8. Head-Complement tags: NNS-NN
Prep. Head & Complement Mixed
9. Head Tag and Comp Token: NNS-country
10. Head Token and Comp Tag: groups-NN
Phrase Structure
11. Preposition Parent: PP
12. Preposition Grandparent: NP
13. Left context of preposition parent: NP
14. Right context of preposition parent: -
Table 1: Parse Features
Model Accuracy
combination only 35.2
parse only 60.6
combination+parse 61.9
lexical only 64.4
combination+lexical (T&C08) 65.2
lexical+parse 68.1
all features (+Parse) 68.5
Table 2: Accuracy on preposition selection task
for various feature combinations
The Preposition Head and Complement Mixed
features are created by taking the first feature in
the previous set and backing-off either the head
or the complement to its POS tag. This mix of
tags and tokens in a word-word dependency has
proven to be an effective feature in sentiment anal-
ysis (Joshi and Penstein-Rose?, 2009). All the fea-
tures described so far are extracted from the set of
dependency triples output by the Stanford parser.
The final set of features (Phrase Structure), how-
ever, is extracted directly from the phrase structure
trees themselves.
4 Evaluation
In Section 4.1, we compare the T&C08 and +Parse
models on the task of preposition selection on
well-formed texts written by native speakers. For
every preposition in the test set, we compare the
system?s top preposition for that context to the
writer?s preposition, and report accuracy rates. In
Section 4.2, we evaluate the two models on ESL
data. The task here is slightly different - if the
most likely preposition according to the model dif-
fers from the likelihood of the writer?s preposition
by a certain threshold amount, a preposition error
is flagged.
4.1 Native Speaker Test Data
Our test set consists of 259K preposition events
from the same source as the original training data.
The T&C08 model performs at 65.2% and when
the parse features are added, the +Parse model im-
proves performance by more than 3% to 68.5%.1
The improvement is statistically significant.
1Prior research has shown preposition selection perfor-
mance accuracy ranging from 65% to nearly 80%. The dif-
ferences are largely due to different test sets and also training
sizes. Given the time required to train large models, we report
here experiments with a relatively small model.
355
Model Accuracy
T&C08 65.2
+Phrase Structure Only 67.1
+Dependency Only 68.2
+Parse 68.5
+head-tag+comp-tag 66.9
+left 66.8
+grandparent 66.6
+head-token+comp-tag 66.6
+head-tag 66.5
+head-token 66.4
+head-tag+comp-token 66.1
Table 3: Which parse features are important? Fea-
ture Addition Experiment
Table 2 shows the effect of various feature class
combinations on prediction accuracy. The results
are clear: a significant performance improvement
is obtained on the preposition selection task when
features from parser output are added. The two
best models in Table 2 contain parse features. The
table also shows that the non-parser-based feature
classes are not entirely subsumed by the parse fea-
tures but rather provide, to varying degrees, com-
plementary information.
Having established the effectiveness of parse
features, we investigate which parse feature
classes contribute the most. To test each contri-
bution, we perform a feature addition experiment,
separately adding features to the T&C08 model
(see Table 3). We make three observations. First,
while there is overlapping information between
the dependency features and the phrase structure
features, the phrase structure features are mak-
ing a contribution. This is interesting because
it suggests that a pure dependency parser might
be less useful than a parser which explicitly pro-
duces both constituent and dependency informa-
tion. Second, using a parser to identify the prepo-
sition head seems to be more useful than using it to
identify the preposition complement.2 Finally, as
was the case for the T&C08 features, the combina-
tion parse features are also important (particularly
the tag-tag or tag/token pairs).
4.2 ESL Test Data
Our test data consists of 5,183 preposition events
extracted from a set of essays written by non-
2De Felice (2009) observes the same for the DAPPER sys-
tem.
Method Precision Recall
T&C08 0.461 0.215
+Parse 0.486 0.225
Table 4: ESL Error Detection Results
native speakers for the Test of English as a Foreign
Language (TOEFL R?). The prepositions were
judged by two trained annotators and checked
by the authors using the preposition annotation
scheme described in Tetreault and Chodorow
(2008b). 4,881 of the prepositions were judged to
be correct and the remaining 302 were judged to
be incorrect.
The writer?s preposition is flagged as an error by
the system if its likelihood according to the model
satisfied a set of criteria (e.g., the difference be-
tween the probability of the system?s choice and
the writer?s preposition is 0.8 or higher). Un-
like the selection task where we use accuracy as
the metric, we use precision and recall with re-
spect to error detection. To date, performance
figures that have been reported in the literature
have been quite low, reflecting the difficulty of the
task. Table 4 shows the performance figures for
the T&C08 and +Parse models. Both precision
and recall are higher for the +Parse model, how-
ever, given the low number of errors in our an-
notated test set, the difference is not statistically
significant.
5 Parser Accuracy on ESL Data
To evaluate parser performance on ESL data,
we manually inspected the phrase structure trees
and dependency graphs produced by the Stanford
parser for 210 ESL sentences, split into 3 groups:
the sentences in the first group are fluent and con-
tain no obvious grammatical errors, those in the
second contain at least one preposition error and
the sentences in the third are clearly ungrammati-
cal with a variety of error types. For each preposi-
tion we note whether the parser was successful in
determining its head and complement. The results
for the three groups are shown in Table 5. The
figures in the first row are for correct prepositions
and those in the second are for incorrect ones.
The parser tends to do a better job of de-
termining the preposition?s complement than its
head which is not surprising given the well-known
problem of PP attachment ambiguity. Given the
preposition, the preceding noun, the preceding
356
OK
Head Comp
Prep Correct 86.7% (104/120) 95.0% (114/120)
Prep Incorrect - -
Preposition Error
Head Comp
Prep Correct 89.0% (65/73) 97.3% (71/73)
Prep Incorrect 87.1% (54/62) 96.8% (60/62)
Ungrammatical
Head Comp
Prep Correct 87.8% (115/131) 89.3% (117/131)
Prep Incorrect 70.8% (17/24) 87.5% (21/24)
Table 5: Parser Accuracy on Prepositions in a
Sample of ESL Sentences
verb and the following noun, Collins (1999) re-
ports an accuracy rate of 84.5% for a PP attach-
ment classifier. When confronted with the same
information, the accuracy of three trained annota-
tors is 88.2%. Assuming 88.2% as an approximate
PP-attachment upper bound, the Stanford parser
appears to be doing a good job. Comparing the
results over the three sentence groups, its ability
to identify the preposition?s head is quite robust to
grammatical noise.
Preposition errors in isolation do not tend to
mislead the parser: in the second group which con-
tains sentences which are largely fluent apart from
preposition errors, there is little difference be-
tween the parser?s accuracy on the correctly used
prepositions and the incorrectly used ones. Exam-
ples are
(S (NP I)
(VP had
(NP (NP a trip)
(PP for (NP Italy))
)
)
)
in which the erroneous preposition for is correctly
attached to the noun trip, and
(S (NP A scientist)
(VP devotes
(NP (NP his prime part)
(PP of (NP his life))
)
(PP in (NP research))
)
)
in which the erroneous preposition in is correctly
attached to the verb devotes.
6 Conclusion
We have shown that the use of a parser can boost
the accuracy of a preposition selection model
tested on well-formed text. In the error detection
task, the improvement is less marked. Neverthe-
less, examination of parser output shows the parse
features can be extracted reliably from ESL data.
For our immediate future work, we plan to carry
out the ESL evaluation on a larger test set to bet-
ter gauge the usefulness of a parser in this context,
to carry out a detailed error analysis to understand
why certain parse features are effective and to ex-
plore a larger set of features.
In the longer term, we hope to compare different
types of parsers in both the preposition selection
and error detection tasks, i.e. a task-based parser
evaluation in the spirit of that carried out by Miyao
et al (2008) on the task of protein pair interaction
extraction. We would like to further investigate
the role of parsing in error detection by looking at
other error types and other text types, e.g. machine
translation output.
Acknowledgments
We would like to thank Rachele De Felice and the
reviewers for their very helpful comments.
References
Martin Chodorow, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions, Prague, Czech
Republic, June.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Rachele De Felice and Stephen G. Pulman. 2008. A
classifier-based approach to preposition and deter-
miner error correction in L2 english. In Proceedings
of the 22nd COLING, Manchester, United Kingdom.
Rachele De Felice and Stephen Pulman. 2009. Au-
tomatic detection of preposition errors in learning
writing. CALICO Journal, 26(3):512?528.
Rachele De Felice. 2009. Automatic Error Detection
in Non-native English. Ph.D. thesis, Oxford Univer-
sity.
357
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING08 Work-
shop on Cross-framework and Cross-domain Parser
Evaluation, Manchester, United Kingdom.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, Genoa, Italy.
Michael Gamon, Jianfeng Gao, Chris Brockett,
Alexandre Klementiev, William B. Dolan, Dmitriy
Belenko, and Lucy Vanderwende. 2008. Using con-
textual speller techniques and language modelling
for ESL error correction. In Proceedings of the In-
ternational Joint Conference on Natural Language
Processing, Hyderabad, India.
Matthieu Hermet, Alain De?silets, and Stan Szpakow-
icz. 2008. Using the web as a linguistic resource
to automatically correct lexico-syntactic errors. In
Proceedings of LREC, Marrekech, Morocco.
Mahesh Joshi and Carolyn Penstein-Rose?. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 313?316, Singapore.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the ACL, pages 423?430,
Sapporo, Japan.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for exact pars-
ing. In Advances in Neural Information Processing
Systems, pages 3?10. MIT Press, Cambridge, MA.
John Lee and Ola Knutsson. 2008. The role of PP at-
tachment in preposition generation. In Proceedings
of CICling. Springer-Verlag Berlin Heidelberg.
Yusuke Miyao, Rune Saetre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings of the 46th Annual Meeting of
the ACL, pages 46?54, Columbus, Ohio.
Adwait Ratnaparkhi. 1998. Maximum Entropy Mod-
els for natural language ambiguity resolution. Ph.D.
thesis, University of Pennsylvania.
Joel Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL
writing. In Proceedings of the 22nd COLING,
Manchester, United Kingdom.
Joel Tetreault and Martin Chodorow. 2008b. Na-
tive Judgments of non-native usage: Experiments in
preposition error detection. In COLING Workshop
on Human Judgments in Computational Linguistics,
Manchester, United Kingdom.
358
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 508?513,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
They Can Help: Using Crowdsourcing to Improve the Evaluation of
Grammatical Error Detection Systems
Nitin Madnania Joel Tetreaulta Martin Chodorowb Alla Rozovskayac
aEducational Testing Service
Princeton, NJ
{nmadnani,jtetreault}@ets.org
bHunter College of CUNY
martin.chodorow@hunter.cuny.edu
cUniversity of Illinois at Urbana-Champaign
rozovska@illinois.edu
Abstract
Despite the rising interest in developing gram-
matical error detection systems for non-native
speakers of English, progress in the field has
been hampered by a lack of informative met-
rics and an inability to directly compare the
performance of systems developed by differ-
ent researchers. In this paper we address
these problems by presenting two evaluation
methodologies, both based on a novel use of
crowdsourcing.
1 Motivation and Contributions
One of the fastest growing areas in need of NLP
tools is the field of grammatical error detection for
learners of English as a Second Language (ESL).
According to Guo and Beckett (2007), ?over a bil-
lion people speak English as their second or for-
eign language.? This high demand has resulted in
many NLP research papers on the topic, a Synthesis
Series book (Leacock et al, 2010) and a recurring
workshop (Tetreault et al, 2010a), all in the last five
years. In this year?s ACL conference, there are four
long papers devoted to this topic.
Despite the growing interest, two major factors
encumber the growth of this subfield. First, the lack
of consistent and appropriate score reporting is an
issue. Most work reports results in the form of pre-
cision and recall as measured against the judgment
of a single human rater. This is problematic because
most usage errors (such as those in article and prepo-
sition usage) are a matter of degree rather than sim-
ple rule violations such as number agreement. As a
consequence, it is common for two native speakers
to have different judgments of usage. Therefore, an
appropriate evaluation should take this into account
by not only enlisting multiple human judges but also
aggregating these judgments in a graded manner.
Second, systems are hardly ever compared to each
other. In fact, to our knowledge, no two systems
developed by different groups have been compared
directly within the field primarily because there is
no common corpus or shared task?both commonly
found in other NLP areas such as machine transla-
tion.1 For example, Tetreault and Chodorow (2008),
Gamon et al (2008) and Felice and Pulman (2008)
developed preposition error detection systems, but
evaluated on three different corpora using different
evaluation measures.
The goal of this paper is to address the above
issues by using crowdsourcing, which has been
proven effective for collecting multiple, reliable
judgments in other NLP tasks: machine transla-
tion (Callison-Burch, 2009; Zaidan and Callison-
Burch, 2010), speech recognition (Evanini et al,
2010; Novotney and Callison-Burch, 2010), au-
tomated paraphrase generation (Madnani, 2010),
anaphora resolution (Chamberlain et al, 2009),
word sense disambiguation (Akkaya et al, 2010),
lexicon construction for less commonly taught lan-
guages (Irvine and Klementiev, 2010), fact min-
ing (Wang and Callison-Burch, 2010) and named
entity recognition (Finin et al, 2010) among several
others.
In particular, we make a significant contribution
to the field by showing how to leverage crowdsourc-
1There has been a recent proposal for a related shared
task (Dale and Kilgarriff, 2010) that shows promise.
508
ing to both address the lack of appropriate evaluation
metrics and to make system comparison easier. Our
solution is general enough for, in the simplest case,
intrinsically evaluating a single system on a single
dataset and, more realistically, comparing two dif-
ferent systems (from same or different groups).
2 A Case Study: Extraneous Prepositions
We consider the problem of detecting an extraneous
preposition error, i.e., incorrectly using a preposi-
tion where none is licensed. In the sentence ?They
came to outside?, the preposition to is an extrane-
ous error whereas in the sentence ?They arrived
to the town? the preposition to is a confusion er-
ror (cf. arrived in the town). Most work on au-
tomated correction of preposition errors, with the
exception of Gamon (2010), addresses preposition
confusion errors e.g., (Felice and Pulman, 2008;
Tetreault and Chodorow, 2008; Rozovskaya and
Roth, 2010b). One reason is that in addition to the
standard context-based features used to detect con-
fusion errors, identifying extraneous prepositions
also requires actual knowledge of when a preposi-
tion can and cannot be used. Despite this lack of
attention, extraneous prepositions account for a sig-
nificant proportion?as much as 18% in essays by
advanced English learners (Rozovskaya and Roth,
2010a)?of all preposition usage errors.
2.1 Data and Systems
For the experiments in this paper, we chose a propri-
etary corpus of about 500,000 essays written by ESL
students for Test of English as a Foreign Language
(TOEFL R?). Despite being common ESL errors,
preposition errors are still infrequent overall, with
over 90% of prepositions being used correctly (Lea-
cock et al, 2010; Rozovskaya and Roth, 2010a).
Given this fact about error sparsity, we needed an ef-
ficient method to extract a good number of error in-
stances (for statistical reliability) from the large es-
say corpus. We found all trigrams in our essays con-
taining prepositions as the middle word (e.g., marry
with her) and then looked up the counts of each tri-
gram and the corresponding bigram with the prepo-
sition removed (marry her) in the Google Web1T
5-gram Corpus. If the trigram was unattested or had
a count much lower than expected based on the bi-
gram count, then we manually inspected the trigram
to see whether it was actually an error. If it was,
we extracted a sentence from the large essay corpus
containing this erroneous trigram. Once we had ex-
tracted 500 sentences containing extraneous prepo-
sition error instances, we added 500 sentences con-
taining correct instances of preposition usage. This
yielded a corpus of 1000 sentences with a 50% error
rate.
These sentences, with the target preposition high-
lighted, were presented to 3 expert annotators who
are native English speakers. They were asked to
annotate the preposition usage instance as one of
the following: extraneous (Error), not extraneous
(OK) or too hard to decide (Unknown); the last cat-
egory was needed for cases where the context was
too messy to make a decision about the highlighted
preposition. On average, the three experts had an
agreement of 0.87 and a kappa of 0.75. For subse-
quent analysis, we only use the classes Error and
OK since Unknown was used extremely rarely and
never by all 3 experts for the same sentence.
We used two different error detection systems to
illustrate our evaluation methodology:2
? LM: A 4-gram language model trained on
the Google Web1T 5-gram Corpus with
SRILM (Stolcke, 2002).
? PERC: An averaged Perceptron (Freund and
Schapire, 1999) classifier? as implemented in
the Learning by Java toolkit (Rizzolo and Roth,
2007)?trained on 7 million examples and us-
ing the same features employed by Tetreault
and Chodorow (2008).
3 Crowdsourcing
Recently,we showed that Amazon Mechanical Turk
(AMT) is a cheap and effective alternative to expert
raters for annotating preposition errors (Tetreault et
al., 2010b). In other current work, we have extended
this pilot study to show that CrowdFlower, a crowd-
sourcing service that allows for stronger quality con-
trol on untrained human raters (henceforth, Turkers),
is more reliable than AMT on three different error
detection tasks (article errors, confused prepositions
2Any conclusions drawn in this paper pertain only to these
specific instantiations of the two systems.
509
& extraneous prepositions). To impose such quality
control, one has to provide ?gold? instances, i.e., ex-
amples with known correct judgments that are then
used to root out any Turkers with low performance
on these instances. For all three tasks, we obtained
20 Turkers? judgments via CrowdFlower for each in-
stance and found that, on average, only 3 Turkers
were required to match the experts.
More specifically, for the extraneous preposition
error task, we used 75 sentences as gold and ob-
tained judgments for the remaining 923 non-gold
sentences.3 We found that if we used 3 Turker judg-
ments in a majority vote, the agreement with any one
of the three expert raters is, on average, 0.87 with a
kappa of 0.76. This is on par with the inter-expert
agreement and kappa found earlier (0.87 and 0.75
respectively).
The extraneous preposition annotation cost only
$325 (923 judgments ? 20 Turkers) and was com-
pleted in a single day. The only restriction on the
Turkers was that they be physically located in the
USA. For the analysis in subsequent sections, we
use these 923 sentences and the respective 20 judg-
ments obtained via CrowdFlower. The 3 expert
judgments are not used any further in this analysis.
4 Revamping System Evaluation
In this section, we provide details on how crowd-
sourcing can help revamp the evaluation of error de-
tection systems: (a) by providing more informative
measures for the intrinsic evaluation of a single sys-
tem (? 4.1), and (b) by easily enabling system com-
parison (? 4.2).
4.1 Crowd-informed Evaluation Measures
When evaluating the performance of grammatical
error detection systems against human judgments,
the judgments for each instance are generally re-
duced to the single most frequent category: Error
or OK. This reduction is not an accurate reflection
of a complex phenomenon. It discards valuable in-
formation about the acceptability of usage because
it treats all ?bad? uses as equal (and all good ones
as equal), when they are not. Arguably, it would
be fairer to use a continuous scale, such as the pro-
portion of raters who judge an instance as correct or
3We found 2 duplicate sentences and removed them.
incorrect. For example, if 90% of raters agree on a
rating of Error for an instance of preposition usage,
then that is stronger evidence that the usage is an er-
ror than if 56% of Turkers classified it as Error and
44% classified it as OK (the sentence ?In addition
classmates play with some game and enjoy? is an ex-
ample). The regular measures of precision and recall
would be fairer if they reflected this reality. Besides
fairness, another reason to use a continuous scale is
that of stability, particularly with a small number of
instances in the evaluation set (quite common in the
field). By relying on majority judgments, precision
and recall measures tend to be unstable (see below).
We modify the measures of precision and re-
call to incorporate distributions of correctness, ob-
tained via crowdsourcing, in order to make them
fairer and more stable indicators of system perfor-
mance. Given an error detection system that classi-
fies a sentence containing a specific preposition as
Error (class 1) if the preposition is extraneous and
OK (class 0) otherwise, we propose the following
weighted versions of hits (Hw), misses (Mw) and
false positives (FPw):
Hw =
N?
i
(cisys ? p
i
crowd) (1)
Mw =
N?
i
((1? cisys) ? p
i
crowd) (2)
FPw =
N?
i
(cisys ? (1? p
i
crowd)) (3)
In the above equations, N is the total number of
instances, cisys is the class (1 or 0) , and p
i
crowd
indicates the proportion of the crowd that classi-
fied instance i as Error. Note that if we were to
revert to the majority crowd judgment as the sole
judgment for each instance, instead of proportions,
picrowd would always be either 1 or 0 and the above
formulae would simply compute the normal hits,
misses and false positives. Given these definitions,
weighted precision can be defined as Precisionw =
Hw/(Hw + FPw) and weighted recall as Recallw =
Hw/(Hw + Mw).
510
agreement
co
un
t
0
100
200
300
400
500
50 60 70 80 90 100
Figure 1: Histogram of Turker agreements for all 923 in-
stances on whether a preposition is extraneous.
Precision Recall
Unweighted 0.957 0.384
Weighted 0.900 0.371
Table 1: Comparing commonly used (unweighted) and
proposed (weighted) precision/recall measures for LM.
To illustrate the utility of these weighted mea-
sures, we evaluated the LM and PERC systems
on the dataset containing 923 preposition instances,
against all 20 Turker judgments. Figure 1 shows a
histogram of the Turker agreement for the major-
ity rating over the set. Table 1 shows both the un-
weighted (discrete majority judgment) and weighted
(continuous Turker proportion) versions of precision
and recall for this system.
The numbers clearly show that in the unweighted
case, the performance of the system is overesti-
mated simply because the system is getting as much
credit for each contentious case (low agreement)
as for each clear one (high agreement). In the
weighted measure we propose, the contentious cases
are weighted lower and therefore their contribution
to the overall performance is reduced. This is a
fairer representation since the system should not be
expected to perform as well on the less reliable in-
stances as it does on the clear-cut instances. Essen-
tially, if humans cannot consistently decide whether
0.0
0.2
0.4
0.6
0.8
1.0
Pre
cisio
n/Re
call
50?75%[n=93] 75?90%[n=114] 90?100%[n=716]Agreement Bin
LM PrecisionPERC PrecisionLM RecallPERC Recall
Figure 2: Unweighted precision/recall by agreement bins
for LM & PERC.
a case is an error then a system?s output cannot be
considered entirely right or entirely wrong.4
As an added advantage, the weighted measures
are more stable. Consider a contentious instance in
a small dataset where 7 out of 15 Turkers (a minor-
ity) classified it as Error. However, it might easily
have happened that 8 Turkers (a majority) classified
it as Error instead of 7. In that case, the change in
unweighted precision would have been much larger
than is warranted by such a small change in the
data. However, weighted precision is guaranteed to
be more stable. Note that the instability decreases
as the size of the dataset increases but still remains a
problem.
4.2 Enabling System Comparison
In this section, we show how to easily compare dif-
ferent systems both on the same data (in the ideal
case of a shared dataset being available) and, more
realistically, on different datasets. Figure 2 shows
(unweighted) precision and recall of LM and PERC
(computed against the majority Turker judgment)
for three agreement bins, where each bin is defined
as containing only the instances with Turker agree-
ment in a specific range. We chose the bins shown
4The difference between unweighted and weighted mea-
sures can vary depending on the distribution of agreement.
511
since they are sufficiently large and represent a rea-
sonable stratification of the agreement space. Note
that we are not weighting the precision and recall in
this case since we have already used the agreement
proportions to create the bins.
This curve enables us to compare the two sys-
tems easily on different levels of item contentious-
ness and, therefore, conveys much more information
than what is usually reported (a single number for
unweighted precision/recall over the whole corpus).
For example, from this graph, PERC is seen to have
similar performance as LM for the 75-90% agree-
ment bin. In addition, even though LM precision is
perfect (1.0) for the most contentious instances (the
50-75% bin), this turns out to be an artifact of the
LM classifier?s decision process. When it must de-
cide between what it views as two equally likely pos-
sibilities, it defaults to OK. Therefore, even though
LM has higher unweighted precision (0.957) than
PERC (0.813), it is only really better on the most
clear-cut cases (the 90-100% bin). If one were to re-
port unweighted precision and recall without using
any bins?as is the norm?this important qualifica-
tion would have been harder to discover.
While this example uses the same dataset for eval-
uating two systems, the procedure is general enough
to allow two systems to be compared on two dif-
ferent datasets by simply examining the two plots.
However, two potential issues arise in that case. The
first is that the bin sizes will likely vary across the
two plots. However, this should not be a significant
problem as long as the bins are sufficiently large. A
second, more serious, issue is that the error rates (the
proportion of instances that are actually erroneous)
in each bin may be different across the two plots. To
handle this, we recommend that a kappa-agreement
plot be used instead of the precision-agreement plot
shown here.
5 Conclusions
Our goal is to propose best practices to address the
two primary problems in evaluating grammatical er-
ror detection systems and we do so by leveraging
crowdsourcing. For system development, we rec-
ommend that rather than compressing multiple judg-
ments down to the majority, it is better to use agree-
ment proportions to weight precision and recall to
yield fairer and more stable indicators of perfor-
mance.
For system comparison, we argue that the best
solution is to use a shared dataset and present the
precision-agreement plot using a set of agreed-upon
bins (possibly in conjunction with the weighted pre-
cision and recall measures) for a more informative
comparison. However, we recognize that shared
datasets are harder to create in this field (as most of
the data is proprietary). Therefore, we also provide
a way to compare multiple systems across differ-
ent datasets by using kappa-agreement plots. As for
agreement bins, we posit that the agreement values
used to define them depend on the task and, there-
fore, should be determined by the community.
Note that both of these practices can also be im-
plemented by using 20 experts instead of 20 Turkers.
However, we show that crowdsourcing yields judg-
ments that are as good but without the cost. To fa-
cilitate the adoption of these practices, we make all
our evaluation code and data available to the com-
munity.5
Acknowledgments
We would first like to thank our expert annotators
Sarah Ohls and Waverely VanWinkle for their hours
of hard work. We would also like to acknowledge
Lei Chen, Keelan Evanini, Jennifer Foster, Derrick
Higgins and the three anonymous reviewers for their
helpful comments and feedback.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon Mechanical Turk
for Subjectivity Word Sense Disambiguation. In Pro-
ceedings of the NAACL Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk,
pages 195?203.
Chris Callison-Burch. 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazon?s Me-
chanical Turk. In Proceedings of EMNLP, pages 286?
295.
Jon Chamberlain, Massimo Poesio, and Udo Kruschwitz.
2009. A Demonstration of Human Computation Us-
ing the Phrase Detectives Annotation Game. In ACM
SIGKDD Workshop on Human Computation, pages
23?24.
5http://bit.ly/crowdgrammar
512
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text Massaging for Computational Linguistics
as a New Shared Task. In Proceedings of INLG.
Keelan Evanini, Derrick Higgins, and Klaus Zechner.
2010. Using Amazon Mechanical Turk for Transcrip-
tion of Non-Native Speech. In Proceedings of the
NAACL Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 53?56.
Rachele De Felice and Stephen Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceedings
of COLING, pages 169?176.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Annotating Named Entities in Twitter Data with
Crowdsourcing. In Proceedings of the NAACL Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 80?88.
Yoav Freund and Robert E. Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Machine Learning, 37(3):277?296.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using Contextual Speller
Techniques and Language Modeling for ESL Error
Correction. In Proceedings of IJCNLP.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Proceedings
of NAACL, pages 163?171.
Y. Guo and Gulbahar Beckett. 2007. The Hegemony
of English as a Global Language: Reclaiming Local
Knowledge and Culture in China. Convergence: In-
ternational Journal of Adult Education, 1.
Ann Irvine and Alexandre Klementiev. 2010. Using
Mechanical Turk to Annotate Lexicons for Less Com-
monly Used Languages. In Proceedings of the NAACL
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 108?113.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. thesis,
Department of Computer Science, University of Mary-
land College Park.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
Fast and Good Enough: Automatic Speech Recogni-
tion with Non-Expert Transcription. In Proceedings
of NAACL, pages 207?215.
Nicholas Rizzolo and Dan Roth. 2007. Modeling
Discriminative Global Inference. In Proceedings of
the First IEEE International Conference on Semantic
Computing (ICSC), pages 597?604, Irvine, California,
September.
Alla Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
NAACLWorkshop on Innovative Use of NLP for Build-
ing Educational Applications.
Alla Rozovskaya and D. Roth. 2010b. Generating Con-
fusion Sets for Context-Sensitive Error Correction. In
Proceedings of EMNLP.
Andreas Stolcke. 2002. SRILM: An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 257?286.
Joel Tetreault and Martin Chodorow. 2008. The Ups and
Downs of Preposition Error Detection in ESL Writing.
In Proceedings of COLING, pages 865?872.
Joel Tetreault, Jill Burstein, and Claudia Leacock, edi-
tors. 2010a. Proceedings of the NAACL Workshop on
Innovative Use of NLP for Building Educational Ap-
plications.
Joel Tetreault, Elena Filatova, and Martin Chodorow.
2010b. Rethinking Grammatical Error Annotation and
Evaluation with the Amazon Mechanical Turk. In Pro-
ceedings of the NAACL Workshop on Innovative Use
of NLP for Building Educational Applications, pages
45?48.
Rui Wang and Chris Callison-Burch. 2010. Cheap Facts
and Counter-Facts. In Proceedings of the NAACL
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 163?167.
Omar F. Zaidan and Chris Callison-Burch. 2010. Pre-
dicting Human-Targeted Translation Edit Rate via Un-
trained Human Annotators. In Proceedings of NAACL,
pages 369?372.
513
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 174?180,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Predicting Grammaticality on an Ordinal Scale
Michael Heilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew Mulholland
Educational Testing Service
Princeton, NJ, USA
{mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org
Joel Tetreault
Yahoo! Research
New York, NY, USA
tetreaul@yahoo-inc.com
Abstract
Automated methods for identifying
whether sentences are grammatical
have various potential applications (e.g.,
machine translation, automated essay
scoring, computer-assisted language
learning). In this work, we construct a
statistical model of grammaticality using
various linguistic features (e.g., mis-
spelling counts, parser outputs, n-gram
language model scores). We also present
a new publicly available dataset of learner
sentences judged for grammaticality on
an ordinal scale. In evaluations, we
compare our system to the one from Post
(2011) and find that our approach yields
state-of-the-art performance.
1 Introduction
In this paper, we develop a system for the task
of predicting the grammaticality of sentences, and
present a dataset of learner sentences rated for
grammaticality. Such a system could be used, for
example, to check or to rank outputs from systems
for text summarization, natural language genera-
tion, or machine translation. It could also be used
in educational applications such as essay scoring.
Much of the previous research on predicting
grammaticality has focused on identifying (and
possibly correcting) specific types of grammati-
cal errors that are typically made by English lan-
guage learners, such as prepositions (Tetreault and
Chodorow, 2008), articles (Han et al, 2006), and
collocations (Dahlmeier and Ng, 2011). While
some applications (e.g., grammar checking) rely
on such fine-grained predictions, others might be
better addressed by sentence-level grammaticality
judgments (e.g., machine translation evaluation).
Regarding sentence-level grammaticality, there
has been much work on rating the grammatical-
ity of machine translation outputs (Gamon et al,
2005; Parton et al, 2011), such as the MT Quality
Estimation Shared Tasks (Bojar et al, 2013, ?6),
but relatively little on evaluating the grammatical-
ity of naturally occurring text. Also, most other re-
search on evaluating grammaticality involves arti-
ficial tasks or datasets (Sun et al, 2007; Lee et al,
2007; Wong and Dras, 2010; Post, 2011).
Here, we make the following contributions.
? We develop a state-of-the-art approach for
predicting the grammaticality of sentences on
an ordinal scale, adapting various techniques
from the previous work described above.
? We create a dataset of grammatical and un-
grammatical sentences written by English
language learners, labeled on an ordinal scale
for grammaticality. With this unique data set,
which we will release to the research com-
munity, it is now possible to conduct realis-
tic evaluations for predicting sentence-level
grammaticality.
2 Dataset Description
We created a dataset consisting of 3,129 sentences
randomly selected from essays written by non-
native speakers of English as part of a test of
English language proficiency. We oversampled
lower-scoring essays to increase the chances of
finding ungrammatical sentences. Two of the au-
thors of this paper, both native speakers of English
with linguistic training, annotated the data. We
refer to these annotators as expert judges. When
making judgments of the sentences, they saw the
previous sentence from the same essay as context.
These two authors were not directly involved in
development of the system in ?3.
Each sentence was annotated on a scale from
1 to 4 as described below, with 4 being the most
174
grammatical. We use an ordinal rather than bi-
nary scale, following previous work such as that of
Clark et al (2013) and Crocker and Keller (2005)
who argue that the distinction between grammati-
cal and ungrammatical is not simply binary. Also,
for practical applications, we believe that it is use-
ful to distinguish sentences with minor errors from
those with major errors that may disrupt communi-
cation. Our annotation scheme was influenced by
a translation rating scheme by Coughlin (2003).
Every sentence judged on the 1?4 scale must be
a clause. There is an extra category (?Other?) for
sentences that do not fit this criterion. We exclude
instances of ?Other? in our experiments (see ?4).
4. Perfect The sentence is native-sounding. It has
no grammatical errors, but may contain very mi-
nor typographical and/or collocation errors, as in
Example (1).
(1) For instance, i stayed in a dorm when i
went to collge.
3. Comprehensible The sentence may contain
one or more minor grammatical errors, includ-
ing subject-verb agreement, determiner, and mi-
nor preposition errors that do not make the mean-
ing unclear, as in Example (2).
(2) We know during Spring Festival, Chinese
family will have a abundand family banquet
with family memebers.
?Chinese family?, which could be corrected to
?Chinese families?, ?each Chinese family?, etc.,
would be an example of a minor grammatical er-
ror involving determiners.
2. Somewhat Comprehensible The sentence
may contain one or more serious grammatical
errors, including missing subject, verb, object,
etc., verb tense errors, and serious preposition
errors. Due to these errors, the sentence may
have multiple plausible interpretations, as in
Example (3).
(3) I can gain the transportations such as buses
and trains.
1. Incomprehensible The sentence contains so
many errors that it would be difficult to correct,
as in Example (4).
(4) Or you want to say he is only a little boy do
not everything clearly?
The phrase ?do not everything? makes the sen-
tence practically incomprehensible since the sub-
ject of ?do? is not clear.
O. Other/Incomplete This sentence is incom-
plete. These sentences, such as Example (5), ap-
pear in our corpus due to the nature of timed tests.
(5) The police officer handed the
This sentence is cut off and does not at least in-
clude one clause.
We measured interannotator agreement on a
subset of 442 sentences that were independently
annotated by both expert annotators. Exact agree-
ment was 71.3%, unweighted ? = 0.574, and
Pearson?s r = 0.759.
1
For our experiments, one
expert annotator was arbitrarily selected, and for
the doubly-annotated sentences, only the judg-
ments from that annotator were retained.
The labels from the expert annotators are dis-
tributed as follows: 72 sentences are labeled 1;
538 are 2; 1,431 are 3; 978 are 4; and 110 are ?O?.
We also gathered 5 additional judgments using
Crowdflower.
2
For this, we excluded the ?Other?
category and any sentences that had been marked
as such by the expert annotators. We used 100
(3.2%) of the judged sentences as ?gold? data in
Crowdflower to block contributors who were not
following the annotation guidelines. For those
sentences, only disagreements within 1 point of
the expert annotator judgment were accepted. In
preliminary experiments, averaging the six judg-
ments (1 expert, 5 crowdsourced) for each item
led to higher human-machine agreement. For all
experiments reported later, we used this average
of six judgments as our gold standard.
For our experiments (?4), we randomly split the
data into training (50%), development (25%), and
testing (25%) sets. We also excluded all instances
labeled ?Other?. These are relatively uncommon
and less interesting to this study. Also, we believe
that simpler, heuristic approaches could be used to
identify such sentences.
We use ?GUG? (?Grammatical? versus ?Un-
Grammatical?) to refer to this dataset. The dataset
is available for research at https://github.
com/EducationalTestingService/
gug-data.
1
The reported agreement values assume that ?Other?
maps to 0. For the sentences where both labels were in the
1?4 range (n = 424), Pearson?s r = 0.767.
2
http://www.crowdflower.com
175
3 System Description
This section describes the statistical model (?3.1)
and features (?3.2) used by our system.
3.1 Statistical Model
We use `
2
-regularized linear regression (i.e., ridge
regression) to learn a model of sentence grammat-
icality from a variety of linguistic features.
34
To tune the `
2
-regularization hyperparameter ?,
the system performs 5-fold cross-validation on the
data used for training. The system evaluates ? ?
10
{?4,...,4}
and selects the one that achieves the
highest cross-validation correlation r.
3.2 Features
Next, we describe the four types of features.
3.2.1 Spelling Features
Given a sentence with with n word tokens, the
model filters out tokens containing nonalpha-
betic characters and then computes the num-
ber of misspelled words n
miss
(later referred to
as num misspelled), the proportion of mis-
spelled words
n
miss
n
, and log(n
miss
+ 1) as fea-
tures. To identify misspellings, we use a freely
available spelling dictionary for U.S. English.
5
3.2.2 n-gram Count and Language Model
Features
Given each sentence, the model obtains the counts
of n-grams (n = 1 . . . 3) from English Gigaword
and computes the following features:
6
?
?
s?S
n
log(count(s) + 1)
?S
n
?
3
We use ridge regression from the scikit-learn
toolkit (Pedregosa et al, 2011) v0.23.1 and the
SciKit-Learn Laboratory (http://github.com/
EducationalTestingService/skll).
4
Regression models typically produce conservative pre-
dictions with lower variance than the original training data.
So that predictions better match the distribution of labels in
the training data, the system rescales its predictions. It saves
the mean and standard deviation of the training data gold
standard (M
gold
and SD
gold
, respectively) and of its own
predictions on the training data (M
pred
and SD
pred
, respec-
tively). During cross-validation, this is done for each fold.
From an initial prediction y?, it produces the final prediction:
y?
?
=
y??M
pred
SD
pred
? SD
gold
+M
gold
. This transformation does
not affect Pearson?s r correlations or rankings, but it would
affect binarized predictions.
5
http://pythonhosted.org/pyenchant/
6
We use the New York Times (nyt), the Los Ange-
les Times-Washington Post (ltw), and the Washington Post-
Bloomberg News (wpb) sections from the fifth edition of En-
glish Gigaword (LDC2011T07).
? max
s?S
n
log(count(s) + 1)
? min
s?S
n
log(count(s) + 1)
where S
n
represents the n-grams of order n from
the given sentence. The model computes the fol-
lowing features from a 5-gram language model
trained on the same three sections of English Gi-
gaword using the SRILM toolkit (Stolcke, 2002):
? the average log-probability of the
given sentence (referred to as
gigaword avglogprob later)
? the number of out-of-vocabulary words in the
sentence
Finally, the system computes the average
log-probability and number of out-of-vocabulary
words from a language model trained on a col-
lection of essays written by non-native English
speakers
7
(?non-native LM?).
3.2.3 Precision Grammar Features
Following Wagner et al (2007) and Wagner et
al. (2009), we use features extracted from preci-
sion grammar parsers. These grammars have been
hand-crafted and designed to only provide com-
plete syntactic analyses for grammatically cor-
rect sentences. This is in contrast to treebank-
trained grammars, which will generally provide
some analysis regardless of grammaticality. Here,
we use (1) the Link Grammar Parser
8
and (2)
the HPSG English Resource Grammar (Copestake
and Flickinger, 2000) and PET parser.
9
We use a binary feature, complete link,
from the Link grammar that indicates whether at
least one complete linkage can be found for a sen-
tence. We also extract several features from the
HPSG analyses.
10
They mostly reflect information
about unification success or failure and the associ-
ated costs. In each instance, we use the logarithm
of one plus the frequency.
7
This did not overlap with the data described in ?2 and
was a subset of the data released by Blanchard et al (2013).
8
http://www.link.cs.cmu.edu/link/
9
http://moin.delph-in.net/PetTop
10
The complete list of relevant statistics used as features
is: trees, unify cost succ, unify cost fail,
unifications succ, unifications fail,
subsumptions succ, subsumptions fail,
words, words pruned, aedges, pedges,
upedges, raedges, rpedges, medges. During
development, we observed that some of these features vary
for some inputs, probably due to parsing search timeouts. On
10 preliminary runs with the development set, this variance
had minimal effects on correlations with human judgments
(less than 0.00001 in terms of r).
176
rour system 0.668
? non-native LM (?3.2.2) 0.665
? HPSG parse (?3.2.3) 0.664
? PCFG parse (?3.2.4) 0.662
? spelling (?3.2.1) 0.643
? gigaword LM (?3.2.2) 0.638
? link parse (?3.2.3) 0.632
? gigaword count (?3.2.2) 0.630
Table 1: Pearson?s r on the development set, for
our full system and variations excluding each fea-
ture type. ?? X? indicates the full model without
the ?X? features.
3.2.4 PCFG Parsing Features
We find phrase structure trees and basic depen-
dencies with the Stanford Parser?s English PCFG
model (Klein and Manning, 2003; de Marneffe et
al., 2006).
11
We then compute the following:
? the parse score as provided by the Stan-
ford PCFG Parser, normalized for sentence
length, later referred to as parse prob
? a binary feature that captures whether the top
node of the tree is sentential or not (i.e. the
assumption is that if the top node is non-
sentential, then the sentence is a fragment)
? features binning the number of dep rela-
tions returned by the dependency conversion.
These dep relations are underspecified for
function and indicate that the parser was un-
able to find a standard relation such as subj,
possibly indicating a grammatical error.
4 Experiments
Next, we present evaluations on the GUG dataset.
4.1 Feature Ablation
We conducted a feature ablation study to iden-
tify the contributions of the different types of fea-
tures described in ?3.2. We compared the perfor-
mance of the full model with all of the features
to models with all but one type of feature. For
this experiment, all models were estimated from
the training set and evaluated on the development
set. We report performance in terms of Pearson?s
r between the averaged 1?4 human labels and un-
rounded system predictions.
The results are shown in Table 1. From these
results, the most useful features appear to be the
n-gram frequencies from Gigaword and whether
the link parser can fully parse the sentence.
4.2 Test Set Results
In this section, we present results on the held-out
test set for the full model and various baselines,
summarized in Table 2. For test set evaluations,
we trained on the combination of the training and
development sets (?2), to maximize the amount of
training data for the final experiments.
We also trained and evaluated on binarized ver-
sions of the ordinal GUG labels: a sentence was
labeled 1 if the average judgment was at least 3.5
(i.e., would round to 4), and 0 otherwise. Evaluat-
ing on a binary scale allows us to measure how
well the system distinguishes grammatical sen-
tences from ungrammatical ones. For some ap-
plications, this two-way distinction may be more
relevant than the more fine-grained 1?4 scale. To
train our system on binarized data, we replaced the
`
2
-regularized linear regression model with an `
2
-
regularized logistic regression and used Kendall?s
? rank correlation between the predicted probabil-
ities of the positive class and the binary gold stan-
dard labels as the grid search metric (?3.1) instead
of Pearson?s r.
For the ordinal task, we report Pearson?s r be-
tween the averaged human judgments and each
system. For the binary task, we report percentage
accuracy. Since the predictions from the binary
and ordinal systems are on different scales, we in-
clude the nonparametric statistic Kendall?s ? as a
secondary evaluation metric for both tasks.
We also evaluated the binary system for the or-
dinal task by computing correlations between its
estimated probabilities and the averaged human
scores, and we evaluated the ordinal system for the
binary task by binarizing its predictions.
12
We compare our work to a modified version of
the publicly available
13
system from Post (2011),
which performed very well on an artificial dataset.
To our knowledge, it is the only publicly available
system for grammaticality prediction. It is very
11
We use the Nov. 12, 2013 version of the Stanford Parser.
12
We selected a threshold for binarization from a grid of
1001 points from 1 to 4 that maximized the accuracy of bina-
rized predictions from a model trained on the training set and
evaluated on the binarized development set. For evaluating
the three single-feature baselines discussed below, we used
the same approach except with grid ranging from the min-
imum development set feature value to the maximum plus
0.1% of the range.
13
The Post (2011) system is available at https://
github.com/mjpost/post2011judging.
177
Ordinal Task Binary Task
r Sig.
r
? % Acc. Sig.
%Acc.
?
our system 0.644 0.479 79.3 0.419
our system
logistic
0.616 * 0.484 80.7 0.428
Post 0.321 * 0.225 75.5 * 0.195
Post
logistic
0.259 * 0.181 74.4 * 0.181
complete link 0.386 * 0.335 74.8 * 0.302
gigaword avglogprob 0.414 * 0.290 76.7 * 0.280
num misspelled -0.462 * -0.370 74.8 * -0.335
Table 2: Human-machine agreement statistics for our system, the system from Post (2011), and simple
baselines, computed from the averages of human ratings in the testing set (?2). ?*? in a Sig. column
indicates a statistically significant difference from ?our system? (p < .05, see text for details). A majority
baseline for the binary task achieves 74.8% accuracy. The best results for each metric are in bold.
different from our system since it relies on par-
tial tree-substitution grammar derivations as fea-
tures. We use the feature computation components
of that system but replace its statistical model. The
system was designed for use with a dataset consist-
ing of 50% grammatical and 50% ungrammatical
sentences, rather than data with ordinal or continu-
ous labels. Additionally, its classifier implementa-
tion does not output scores or probabilities. There-
fore, we used the same learning algorithms as for
our system (i.e., ridge regression for the ordinal
task and logistic regression for the binary task).
14
To create further baselines for comparison,
we selected the following features that represent
ways one might approximate grammaticality if a
comprehensive model was unavailable: whether
the link parser can fully parse the sentence
(complete link), the Gigaword language
model score (gigaword avglogprob),
and the number of misspelled tokens
(num misspelled). Note that we expect
the number of misspelled tokens to be negatively
correlated with grammaticality. We flipped the
sign of the misspelling feature when computing
accuracy for the binary task.
To identify whether the differences in perfor-
mance for the ordinal task between our system and
each of the baselines are statistically significant,
we used the BC
a
Bootstrap (Efron and Tibshirani,
1993) with 10,000 replications to compute 95%
confidence intervals for the absolute value of r for
our system minus the absolute value of r for each
of the alternative methods. For the binary task, we
14
In preliminary experiments, we observed little difference
in performance between logistic regression and the original
support vector classifier used by the system from Post (2011).
used the sign test to test for significant differences
in accuracy. The results are in Table 2.
5 Discussion and Conclusions
In this paper, we developed a system for predict-
ing grammaticality on an ordinal scale and cre-
ated a labeled dataset that we have released pub-
licly (?2) to enable more realistic evaluations in
future research. Our system outperformed an ex-
isting state-of-the-art system (Post, 2011) in eval-
uations on binary and ordinal scales. This is the
most realistic evaluation of methods for predicting
sentence-level grammaticality to date.
Surprisingly, the system from Post (2011) per-
formed quite poorly on the GUG dataset. We spec-
ulate that this is due to the fact that the Post sys-
tem relies heavily on features extracted from au-
tomatic syntactic parses. While Post found that
such a system can effectively distinguish gram-
matical news text sentences from sentences gen-
erated by a language model, measuring the gram-
maticality of real sentences from language learn-
ers seems to require a wider variety of features,
including n-gram counts, language model scores,
etc. Of course, our findings do not indicate that
syntactic features such as those from Post (2011)
are without value. In future work, it may be pos-
sible to improve grammaticality measurement by
integrating such features into a larger system.
Acknowledgements
We thank Beata Beigman Klebanov, Yoko Futagi,
Su-Youn Yoon, and the anonymous reviewers for
their helpful comments. We also thank Jennifer
Foster for discussions about this work and Matt
Post for making his system publicly available.
178
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Alexander Clark, Gianluca Giorgolo, and Shalom Lap-
pin. 2013. Towards a statistical model of grammat-
icality. In Proceedings of the 35th Annual Confer-
ence of the Cognitive Science Society, pages 2064?
2069.
Ann Copestake and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage English grammar using HPSG.
In Proceedings of the 2nd International Confer-
ence on Language Resources and Evaluation (LREC
2000), Athens, Greece.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of MT Summit IX, pages 63?70.
Matthew W. Crocker and Frank Keller. 2005. Prob-
abilistic grammars as models of gradience in lan-
guage processing. In Gradience in Grammar: Gen-
erative Perspectives. University Press.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Correcting
Semantic Collocation Errors with L1-induced Para-
phrases. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 107?117, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC 2006, pages 449?454.
B. Efron and R. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman and Hall/CRC, Boca Ra-
ton, FL.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. In
Proceedings of EAMT, pages 103?111. Springer-
Verlag.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
John Lee, Ming Zhou, and Xiaohua Liu. 2007. De-
tection of Non-Native Sentences Using Machine-
Translated Training Data. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 93?96, Rochester, New York, April. As-
sociation for Computational Linguistics.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 108?115, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Matt Post. 2011. Judging Grammaticality with Tree
Substitution Grammar Derivations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 217?222, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In 7th International Con-
ference on Spoken Language Processing.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting Erroneous Sentences using Auto-
matically Mined Sequential Patterns. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 81?88, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 865?872, Manchester, UK,
August. Coling 2008 Organizing Committee.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2007. A Comparative Evaluation of Deep
and Shallow Approaches to the Automatic Detec-
tion of Common Grammatical Errors. In Proceed-
ings of the 2007 Joint Conference on Empirical
179
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 112?121, Prague, Czech Republic,
June. Association for Computational Linguistics.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experi-
ments in sentence classification. CALICO Journal,
26(3):474?490.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
Features for Sentence Grammaticality Classifica-
tion. In Proceedings of the Australasian Language
Technology Association Workshop 2010, pages 67?
75, Melbourne, Australia, December.
180
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 24?32
Manchester, August 2008
Native Judgments of Non-Native Usage:
Experiments in Preposition Error Detection
Joel R. Tetreault
Educational Testing Service
660 Rosedale Road
Princeton, NJ, USA
JTetreault@ets.org
Martin Chodorow
Hunter College of CUNY
695 Park Avenue
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
Evaluation and annotation are two of the
greatest challenges in developing NLP in-
structional or diagnostic tools to mark
grammar and usage errors in the writing of
non-native speakers. Past approaches have
commonly used only one rater to annotate
a corpus of learner errors to compare to
system output. In this paper, we show how
using only one rater can skew system eval-
uation and then we present a sampling ap-
proach that makes it possible to evaluate a
system more efficiently.
1 Introduction
In this paper, we present a series of experiments
that explore the reliability of human judgments
in rating preposition usage. While one tends to
think of annotator disagreements about discourse
and semantics as being quite common, our studies
show that judgments of preposition usage, which is
largely lexically driven, can be just as contentious.
As a result, this unreliability poses a serious issue
for the development and evaluation of NLP tools
in the task of automatically detecting preposition
usage errors in the writing of non-native speakers
of English.
To date, single human annotation has typically
been the gold standard for grammatical error de-
tection, such as in the work of (Izumi et al, 2004),
(Han et al, 2006), (Nagata et al, 2006), (Gamon et
al., 2008)
1
. Although there are several learner cor-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
(Eeg-Olofsson and Knuttson, 2003) had a small evalu-
ation of 40 prepositions and it is unclear whether they used
multiple annotators or not.
pora annotated for preposition and determiner er-
rors (such as the Cambridge Learners Corpus
2
and
the Chinese Learner English Corpus
3
), it is unclear
which portions of these, if any, were doubly anno-
tated. This previous work has side-stepped the is-
sue of annotator reliability, which we address here
through the following three contributions:
? Judgments of Native Usage To motivate our
work in non-native usage, we first illustrate
the difficulty of preposition selection with
two experiments: a cloze test and a choice
test, where native speakers judge native texts
(section 4).
? Judgments of Non-Native Usage As stated
earlier, most computational work in the field
of error detection tools for non-native speak-
ers has relied on a single rater to annotate
a gold standard corpus to check a system?s
output. We conduct an extensive double-
annotation evaluation to measure inter-rater
reliability and show that using one rater can
be unreliable and may produce misleading re-
sults in a system test (section 5).
? Sampling ApproachMultiple annotation can
be very costly and time-consuming, which
may explain why previous work employed
only one rater. As an alternative to the
standard exhaustive annotation, we propose
a sampling approach in which estimates of
the rates of hits, false positives, and misses
are derived from random samples of the sys-
tem?s output, and then precision and recall
of the system can be calculated. We show
that estimates of system performance derived
2
http://www.cambridge.org/elt
3
http://langbank.engl.polyu.edu.hk/corpus/clec.html
24
from the sampling approach are comparable
to those derived from an exhaustive annota-
tion, but require only a fraction of the effort
(section 6).
In short, through a battery of experiments we
show how rating preposition usage, in either na-
tive or non-native texts, is a task that has sur-
prisingly low inter-annotator reliability and thus
greatly impacts system evaluation. We then de-
scribe a method for efficiently annotating non-
native texts to make multiple annotation more fea-
sible.
In section 2, we discuss in more depth the mo-
tivation for detecting usage errors in non-native
writing, as well as the complexities of preposition
usage. In section 3, we describe a system that au-
tomatically detects preposition errors involving in-
correct selection and extraneous usage. In sections
4 and 5 respectively, we discuss experiments on the
reliability of judging native and non-native prepo-
sition usage. In section 6, we present results of our
system and results from comparing the sampling
approach with the standard approach of exhaustive
annotation.
2 Motivation
The long-term goal of our work is to develop a
system which detects errors in grammar and us-
age so that appropriate feedback can be given
to non-native English writers, a large and grow-
ing segment of the world?s population. Estimates
are that in China alone as many as 300 million
people are currently studying English as a for-
eign language. Even in predominantly English-
speaking countries, the proportion of non-native
speakers can be very substantial. For example,
the US National Center for Educational Statistics
(2002) reported that nearly 10% of the students in
the US public school population speak a language
other than English and have limited English pro-
ficiency . At the university level in the US, there
are estimated to be more than half a million for-
eign students whose native language is not English
(Burghardt, 2002). Clearly, there is an increasing
demand for tools for instruction in English as a
Second Language (ESL).
Some of the most common types of ESL usage
errors involve prepositions, determiners and col-
locations. In the work discussed here, we target
preposition usage errors, specifically those of in-
correct selection (?we arrived to the station?) and
extraneous use (?he went to outside?)
4
. Preposi-
tion errors account for a substantial proportion of
all ESL usage errors. For example, (Bitchener et
al., 2005) found that preposition errors accounted
for 29% of all the errors made by intermediate to
advanced ESL students. In addition, such errors
are relatively common. In our learner corpora, we
found that 6% of all prepositions were incorrectly
used. Some other estimates are even higher: for
example, (Izumi et al, 2003) reported error rates
that were as high as 10% in a Japanese learner cor-
pus.
At least part of the difficulty in mastering prepo-
sitions seems to be due to the great variety of lin-
guistic functions that they serve. When a prepo-
sition marks the argument of a predicate, such as
a verb, an adjective, or a noun, preposition se-
lection is constrained by the argument role that it
marks, the noun which fills that role, and the par-
ticular predicate. Many English verbs also display
alternations (Levin, 1993) in which an argument
is sometimes marked by a preposition and some-
times not (e.g., ?They loaded the wagon with hay?
/ ?They loaded hay on the wagon?). When prepo-
sitions introduce adjuncts, such as those of time
or manner, selection is constrained by the object
of the preposition (?at length?, ?in time?, ?with
haste?). Finally, the selection of a preposition for
a given context also depends upon the intention of
the writer (?we sat at the beach?, ?on the beach?,
?near the beach?, ?by the beach?).
3 Automatically Detecting Preposition
Usage Errors
In this section, we give a description of our sys-
tem and compare its performance to other sys-
tems. Although the focus of this paper is on hu-
man judgments in the task of error detection, we
describe our system to show that variability in hu-
man judgments can impact the evaluation of a sys-
tem in this task. A full description of our system
and its performance can be found in (Tetreault and
Chodorow, 2008).
3.1 System
Our approach treats preposition error detection as
a classification problem: that is, given a context of
two words before and two words after the writer?s
preposition, what is the best preposition to use?
4
There is a third error type, omission (?we are fond null
beer?), that is a topic for our future research.
25
An error is marked when the system?s sugges-
tion differs from the writer?s by a certain threshold
amount.
We have used a maximum entropy (ME) clas-
sifier (Ratnaparkhi, 1998) to select the most prob-
able preposition for a given context from a set of
34 common English prepositions. One advantage
of using ME is that there are implementations of it
which can handle very large models built frommil-
lions of training events and consisting of hundreds
of thousands of feature-value pairs. To construct
a model, we begin with a training corpus that is
POS-tagged and heuristically chunked into noun
phrases and verb phrases
5
. For each preposition
that occurs in the training corpus, a preprocessing
program extracts a total of 25 features. These con-
sist of words and POS tags in positions adjacent to
the preposition and in the heads of nearby phrases.
In addition, we include combination features that
merge the head features. We also include features
representing only the tags to be able to cover cases
in testing where the words in the context were not
seen in training.
In many NLP tasks (parsing, POS-tagging, pro-
noun resolution), it is easy to acquire training data
that is similar to the testing data. However, in the
case of grammatical error detection, one does not
have that luxury because reliable error-annotated
ESL corpora that are large enough for training a
statistical classifier simply do not exist. To circum-
vent this problem, we have trained our classifier on
examples of prepositions used correctly, as in news
text.
3.2 Evaluation
Before evaluating our system on non-native writ-
ing, we evaluated how well it does on the task of
preposition selection in native text, an area where
there has been relatively little work to date. In this
task, the system predicts the writer?s preposition
based on its context. Its prediction is scored au-
tomatically by comparison to what the writer actu-
ally wrote. Most recently, (Gamon et al, 2008) ad-
dressed preposition selection by developing a sys-
tem that combined a decision tree and a language
model. Besides the difference in algorithms, there
is also a difference in coverage between their sys-
tem, which selects among 13 prepositions plus a
category for Other, and the system presented here,
5
We have avoided parsing because our ultimate test corpus
is non-native writing, text that is difficult to parse due to the
presence of numerous errors in spelling and syntax.
Prep (Gamon et al, 2008) (Tetreault et al, 2008)
in 0.592 0.845
for 0.459 0.698
of 0.759 0.906
on 0.322 0.751
to 0.627 0.775
with 0.361 0.675
at 0.372 0.685
by 0.502 0.747
as 0.699 0.711
from 0.528 0.591
about 0.800 0.654
Table 1: Comparison of F-measures on En-
carta/Reuters Corpus
which selects among 34 prepositions. In their sys-
tem evaluation, they split a corpus of Reuters News
text and Microsoft Encarta into two sets: 70% for
training (3.2M examples), and the remaining 30%
for testing (1.4M examples). For purposes of com-
parison, we used the same corpus and evaluation
method. While (Gamon et al, 2008) do not present
their overall accuracy figures on the Encarta eval-
uation, they do present the precision and recall
scores for each preposition. In Table 3.2, we dis-
play their results in terms of F-measures and show
the performance of our system for each preposi-
tion. Our model outperforms theirs for 9 out of the
10 prepositions that both systems handle. Over-
all accuracy for our system is 77.4% and increases
to 79.0% when 7M more training examples are
added. For comparison purposes, using a major-
ity baseline (always selecting the preposition of) in
this domain results in an accuracy of 27.2%.
(Felice and Pullman, 2007) used perceptron
classifiers for preposition selection in BNC News
Text at 85% accuracy. For each of the five most
frequent prepositions, they used a separate binary
classifier to decide whether that preposition should
be used or not. The classifiers are not combined
into a unified model. When we reconfigured our
system and evaluation to be comparable to (Felice
and Pullman, 2007), our model achieved an accu-
racy of 90% on the same five prepositions when
tested on Wall Street Journal News, which is simi-
lar, though not identical, to BNC News.
While systems can perform at close to 80% ac-
curacy in the task of preposition selection in native
texts, this high performance does not transfer to
the end-task of detecting preposition errors in es-
says by non-native writers. For example, (Izumi et
al., 2003) reported precision and recall as low as
25% and 7% respectively when detecting different
26
grammar errors (one of which was prepositions)
in English essays by non-native writers. (Gamon
et al, 2008) reported precision up to 80% in their
evaluation on the CLEC corpus, but no recall fig-
ure was reported. We have found that our system
(the model which performs at 77.4%), also per-
forms as high as 80% precision, but recall ranged
from 12% to 26% depending on the non-native test
corpus.
While our recall figures may seem low, espe-
cially when compared to other NLP tasks such as
parsing and anaphora resolution, this is really a re-
flection of how difficult the task is. In addition, in
error detection tasks, high precision (and thus low
recall) is favored since one wants to minimize the
number of false positives a student may see. This
is a common practice in grammatical error detec-
tion applications, such as in (Han et al, 2006) and
(Gamon et al, 2008).
4 Human Judgments of Native Usage
4.1 Cloze Test
With so many sources of variation in English
preposition usage, we wondered if the task of se-
lecting a preposition for a given context might
prove challenging even for native speakers. To
investigate this possibility, we randomly selected
200 sentences from Microsoft?s Encarta Encyclo-
pedia, and, in each sentence, we replaced a ran-
domly selected preposition with a blank. We then
asked two native English speakers to perform a
cloze task by filling in the blank with the best
preposition, given the context provided by the rest
of the sentence. In addition, we had our system
predict which preposition should fill each blank as
well. Our results (Table 2) showed only about 76%
agreement between the two raters (bottom row),
and between 74% and 78% when each rater was
compared individually with the original preposi-
tion used in Encarta. Surprisingly, the system
performed just as well as the two native raters,
when compared with Encarta (third row). Al-
though these results seem very promising, it should
be noted that in many cases where the system dis-
agreed with Encarta, its prediction was not a good
fit for the context. But in the cases where the
raters disagreed with Encarta, their prepositions
were also licensed by the context, and thus were
acceptable alternatives to the preposition that was
used in the text.
Our cloze study shows that even with well-
Agreement Kappa
Encarta vs. Rater 1 0.78 0.73
Encarta vs. Rater 2 0.74 0.68
Encarta vs. System 0.75 0.68
Rater 1 vs. Rater 2 0.76 0.70
Table 2: Cloze Experiment on Encarta
formed text, native raters can disagree with each
other by 25% in the task of preposition selec-
tion. We can expect even more disagreement when
the task is preposition error detection in ?noisy?
learner texts.
4.2 Choice Test
The cloze test presented above was scored by au-
tomatically comparing the system?s choice (or the
rater?s choice) with the preposition that was actu-
ally written. But there are many contexts that li-
cense multiple prepositions, and in these cases, re-
quiring an exact match is too stringent a scoring
criterion.
To investigate how the exact match metric might
underestimate system performance, and to further
test the reliability of human judgments in native
text, we conducted a choice test in which two
native English speakers were presented with 200
sentences from Encarta and were asked to select
which of two prepositions better fit the context.
One was the originally written preposition and the
other was the system?s suggestion, displayed in
random order. The human raters were also given
the option of marking both prepositions as equally
good or equally bad. The results indicated that
both Rater 1 and Rater 2 considered the system?s
preposition equal to or better than the writer?s
preposition in 28% of the cases. This suggests
that 28% of the mismatched cases in the automatic
evaluation are not system errors but rather are in-
stances where the context licenses multiple prepo-
sitions. If these mismatches in the automatic eval-
uation are actually cases of correct system perfor-
mance, then the Encarta/Reuters test which per-
forms at 75% accuracy (third row of Table 2), is
more realistically around 82% accuracy (28% of
the 25% mismatch rate is 7%).
5 Annotator Reliability
In this section, we address the central problem of
evaluating NLP error detection tools on learner
data. As stated earlier, most previous work has re-
lied on only one rater to either create an annotated
27
corpus of learner errors, or to check the system?s
output. While some grammatical errors, such as
number disagreement between subject and verb,
no doubt show very high reliability, others, such as
usage errors involving prepositions or determiners
are likely to be much less reliable. In section 5.1,
we describe our efforts in annotating a large cor-
pus of student learner essays for preposition us-
age errors. Unlike previous work such as (Izumi
et al, 2004) which required the rater to check for
almost 40 different error types, we focus on anno-
tating only preposition errors in hopes that having
a single type of target will insure higher reliabil-
ity by reducing the cognitive demands on the rater.
Section 5.2 asks whether, under these conditions,
one rater is acceptable for this task. In section 6,
we describe an approach to efficiently evaluating a
system that does not require the amount of effort
needed in the standard approach to annotation.
5.1 Annotation Scheme
To create a gold-standard corpus of error anno-
tations for system evaluation, and also to deter-
mine whether multiple raters are better than one,
we trained two native English speakers to anno-
tate preposition errors in ESL text. Both annota-
tors had prior experience in NLP annotation and
also in ESL error detection. The training was very
extensive: both raters were trained on 2000 prepo-
sition contexts and the annotation manual was it-
eratively refined as necessary. To our knowledge,
this is the first scheme that specifically targets an-
notating preposition errors
6
.
The two raters were shown sentences randomly
selected from student essays, with each preposi-
tion highlighted in the sentence. The raters were
also shown the sentence which preceded the one
containing the preposition that they rated. The an-
notator was first asked to indicate if there were any
spelling errors within the context of the preposi-
tion (?2-word window and the commanding verb).
Next the annotator noted determiner or plural er-
rors in the context, and then checked if there were
any other grammatical errors (for example, wrong
verb form). The reason for having the annota-
tors check spelling and grammar is that other mod-
ules in a grammatical error detection system would
be responsible for these error types. For an ex-
6
(Gamon et al, 2008) did not have a scheme for annotat-
ing preposition errors to create a gold standard corpus, but did
use a scheme for the similar problem of verifying a system?s
output in preposition error detection.
ample of a sentence with multiple spelling, gram-
matical and collocational errors, consider the fol-
lowing sentence: ?In consion, for some reasons,
museums, particuraly known travel place, get on
many people.? A spelling error follows the prepo-
sition In, and a collocational error surrounds on. If
the contexts are not corrected, it is impossible to
discern if the prepositions are correct. Of course,
there is the chance that by removing these we will
screen out cases where there are multiple interact-
ing errors in the context that involve prepositions.
When comparing human judgments to the perfor-
mance of the preposition module, the latter should
not be penalized for other kinds of errors in the
context.
Finally, the annotator judged the writer?s prepo-
sition with a rating of ?0-extraneous preposition?,
?1-incorrect preposition?, ?2-correct preposition?,
or ?e-equally good prepositions?. If the writer
used an incorrect preposition, the rater supplied the
best preposition(s) given the context. Very often,
when the writer?s preposition was correct, several
other prepositions could also have occurred in the
same context. In these cases, the annotator was in-
structed to use the ?e? category and list the other
equally plausible alternatives. After judging the
use of the preposition and, if applicable, supplying
alternatives, the annotator indicated her confidence
in her judgment on a 2-point scale of ?1-low? and
?2-high?.
5.2 Two Raters vs. One?
Following training, each annotator judged approxi-
mately 18,000 occurrences of preposition use. An-
notation of 500 occurrences took an average of 3 to
4 hours. In order to calculate agreement and kappa
values, we periodically provided identical sets of
100 preposition occurrences for both annotators to
judge (totaling 1800 in all). After removing in-
stances where there were spelling or grammar er-
rors, and after combining categories ?2? and ?e?,
both of which were judgments of correct usage,
we computed the kappa values for the remaining
doubly judged sets. These ranged from 0.411 to
0.786, with an overall combined value of 0.630
7
.
The confusion matrix for the combined set (to-
taling 1336 contexts) is shown in Table 3. The
rows represent Rater 1?s (R1) judgments while the
columns represent Rater 2?s judgments. As one
7
When including spelling and grammar annotations,
kappa ranged from 0.474 to 0.773.
28
would expect given the prior reports of preposition
error rates in non-native writing, the raters? agree-
ment for this task was quite high overall (0.952)
due primarily to the large agreement count where
both annotators rated the usage ?OK? (1213 total
contexts). However there were 42 prepositions that
both raters marked as a ?Wrong Choice? and 17 as
?Extraneous.? It is important to note the disagree-
ments in judging these errors: for example, Rater
1 judged 26 prepositions to be errors that Rater 2
judged to be OK, for a disagreement rate of .302
(26/86). Similarly, Rater 2 judged 37 prepositions
to be errors that Rater 1 judged to be OK, for a
disagreement rate of .381 (37/97).
R1?; R2? Extraneous Wrong-Choice OK
Extraneous 17 0 6
Wrong-Choice 1 42 20
OK 4 33 1213
Table 3: Confusion Matrix
The kappa of 0.630 and the off-diagonal cells
in the confusion matrix both show the difficulty
of this task and also show how two highly trained
raters can produce very different judgments. This
suggests that for certain error annotation tasks,
such as preposition usage, it may not be appropri-
ate to use only one rater and that using two or more
raters to produce an adjudicated gold-standard set
is the more acceptable path.
As a second test, we used a set of 2,000 prepo-
sition contexts from ESL essays (Chodorow et al,
2007) that were doubly annotated by native speak-
ers with a scheme similar to that described above.
We then compared an earlier version of our sys-
tem to both raters? judgments, and found that there
was a 10% difference in precision and a 5% differ-
ence in recall between the two system/rater com-
parisons. That means that if one is using only a
single rater as a gold standard, there is the potential
to over- or under-estimate precision by as much as
10%. Clearly this is problematic when evaluating
a system?s performance. The results are shown in
Table 4.
Precision Recall
System vs. Rater 1 0.78 0.26
System vs. Rater 2 0.68 0.21
Table 4: Rater/System Comparison
6 Sampling Approach
If one uses multiple raters for error annotation,
there is the possibility of creating an adjudicated
set, or at least calculating the variability of sys-
tem evaluation. However, annotation with multiple
raters has its own disadvantages in that it is much
more expensive and time-consuming. Even using
one rater to produce a sizeable evaluation corpus
of preposition errors is extremely costly. For ex-
ample, if we assume that 500 prepositions can be
annotated in 4 hours using our annotation scheme,
and that the error rate for prepositions is 10%, then
it would take at least 80 hours for a rater to find
and mark 1000 errors. In this section, we propose
a more efficient annotation approach to circumvent
this problem.
6.1 Methodology
The sampling procedure outlined here is inspired
by the one described in (Chodorow and Leacock,
2000). The central idea is to skew the annotation
corpus so that it contains a greater proportion of
errors. The result is that an annotator checks more
potential errors since he or she is spending less
time checking prepositions used correctly.
Here are the steps in the procedure. Figure 1 il-
lustrates this procedure with a hypothetical corpus
of 10,000 preposition examples.
1. Process a test corpus of sentences so that each
preposition in the corpus is labeled ?OK? or
?Error? by the system.
2. Divide the processed corpus into two sub-
corpora, one consisting of the system?s ?OK?
prepositions and the other of the system?s
?Error? prepositions. For the hypothetical
data in Figure 1, the ?OK? sub-corpus con-
tains 90% of the prepositions, and the ?Error?
sub-corpus contains the remaining 10%.
3. Randomly sample cases from each sub-
corpus and combine the samples into an an-
notation set that is given to a ?blind? human
rater. We generally use a higher sampling
rate for the ?Error? sub-corpus because we
want to ?enrich? the annotation set with a
larger proportion of errors than is found in the
test corpus as a whole. In Figure 1, 75% of
the ?Error? sub-corpus is sampled while only
16% of the ?OK? sub-corpus is sampled.
29
Figure 1: Sampling Approach (with hypothetical sample calculations)
4. For each case that the human rater judges to
be an error, check to see which sub-corpus it
came from. If it came from the ?OK? sub-
corpus, then the case is a Miss (an error that
the system failed to detect). If it came from
the ?Error? sub-corpus, then the case is a Hit
(an error that the system detected). If the rater
judges a case to be a correct usage and it came
from the ?Error? sub-corpus, then it is a False
Positive (FP).
5. Calculate the proportions of Hits and FPs in
the sample from the ?Error? sub-corpus. For
the hypothetical data in Figure 1, these val-
ues are 600/750 = 0.80 for Hits, and 150/750
= 0.20 for FPs. Calculate the proportion of
Misses in the sample from the ?OK? sub-
corpus. For the hypothetical data, this is
450/1500 = 0.30 for Misses.
6. The values computed in step 5 are conditional
proportions based on the sub-corpora. To cal-
culate the overall proportions in the test cor-
pus, it is necessary to multiply each value
by the relative size of its sub-corpus. This
is shown in Table 5, where the proportion of
Hits in the ?Error? sub-corpus (0.80) is mul-
tiplied by the relative size of the ?Error? sub-
corpus (0.10) to produce an overall Hit rate
(0.08). Overall rates for FPs and Misses are
calculated in a similar manner.
7. Using the values from step 6, calculate Preci-
sion (Hits/(Hits + FP)) and Recall (Hits/(Hits
+ Misses)). These are shown in the last two
rows of Table 5.
Estimated Overall Rates
Sample Proportion * Sub-Corpus Proportion
Hits 0.80 * 0.10 = 0.08
FP 0.20 * 0.10 = 0.02
Misses 0.30 * 0.90 = 0.27
Precision 0.08/(0.08 + 0.02) = 0.80
Recall 0.08/(0.08 + 0.27) = 0.23
Table 5: Sampling Calculations (Hypothetical)
This method is similar in spirit to active learning
((Dagan and Engelson, 1995) and (Engelson and
Dagan, 1996)), which has been used to iteratively
build up an annotated corpus, but it differs from
active learning applications in that there are no it-
erative loops between the system and the human
annotator(s). In addition, while our methodology
is used for evaluating a system, active learning is
commonly used for training a system.
6.2 Application
Next, we tested whether our proposed sampling
approach provides good estimates of a system?s
performance. For this task, we split a large corpus
of ESL essays into two sets: first, a set of 8,269
preposition contexts (standard approach corpus) to
be annotated using the scheme in section 5.1, and
30
second, a set of 22,000 preposition contexts to be
rated using the sampling approach (sampling cor-
pus). We used two non-overlapping sets because
the raters were the same for this test of the two ap-
proaches.
Using the standard approach, the sampling cor-
pus of 22,000 prepositions would normally take
several weeks for two raters to double annotate
and then adjudicate. After this corpus was di-
vided into ?OK? and ?Error? sub-corpora, the two
sub-corpora were proportionally sampled, result-
ing in an annotation set of 750 preposition con-
texts (500 contexts from the ?OK? sub-corpus and
250 contexts from the ?Error? sub-corpus). This
required roughly 6 hours for annotation, which is
substantially more manageable than the standard
approach. We had both raters work together to
make judgments for each preposition context.
The precision and recall scores for both ap-
proaches are shown in Table 6 and are quite simi-
lar, thus suggesting that the sampling approach can
be used as an alternative to exhaustive annotation.
Precision Recall
Standard Approach 0.80 0.12
Sampling Approach 0.79 0.14
Table 6: Sampling Results
6.3 Confidence Intervals
It is important with the sampling approach to use
appropriate sample sizes when drawing from the
sub-corpora, because the accuracy of the estimates
of hits and misses will depend upon the propor-
tion of errors in each sub-corpus as well as on the
sample sizes. The ?OK? sub-corpus is expected
to have even fewer errors than the overall base
rate, so it is especially important to have a rela-
tively large sample from this sub-corpus. The com-
parison study described above used an ?OK? sub-
corpus sample that was twice as large as the Error
sub-corpus sample.
One can compute the 95% confidence interval
(CI) for the estimated rates of hits, misses and false
positives by using the formula:
CI = p? 1.96? ?
p
where p is the proportion and ?
p
is the standard
error of the proportion given by:
?
p
=
?
p(1? p)
N
where N is the sample size.
For the example in Figure 1, the confidence in-
terval for the proportion of Hits from the sample of
the ?Error? sub-corpus is:
CI
hits
= 0.80? 1.96?
?
0.8? (1? 0.80)
750
which yields an interval of 0.077 and 0.083. Using
these values, the confidence interval for precision
is 0.77 to 0.83. The interval for recall can be com-
puted in a similar manner. Of course, a larger sam-
ple size will yield narrower confidence intervals.
6.4 Summary
Table 7 summarizes the advantages and disadvan-
tages of three methods for evaluating error detec-
tion systems. The standard (or exhaustive) ap-
proach refers to the method of annotating the er-
rors in a large corpus. Its advantage is that the an-
notated corpus can be reused to evaluate the same
system or compare multiple systems. However,
it is costly and time-consuming which often pre-
cludes the use of multiple raters. The verification
method (as used in (Gamon et al, 2008)), refers to
the method of simply checking the acceptability of
system output with respect to the writer?s preposi-
tion. Like the sampling method, it has the advan-
tages of efficiency and use of multiple raters (when
compared to the standard method). But the dis-
advantage of verification is that it does not permit
estimation of recall. Both verification and vam-
pling methods require re-annotation for system re-
testing and comparison. In terms of system devel-
opment, sampling (and to a lesser extent, verifica-
tion) allows one to quickly assess system perfor-
mance on a new corpus.
In short, the sampling approach is intended to
alleviate the burden on annotators when faced with
the task of having to rate several thousand errors of
a particular type to produce a sizeable error corpus.
7 Conclusions
In this paper, we showed that the standard ap-
proach to evaluating NLP error detection sys-
tems (comparing the system?s output with a gold-
standard annotation) can greatly skew system re-
sults when the annotation is done by only one rater.
However, one reason why a single rater is com-
monly used is that building a corpus of learner er-
rors can be extremely costly and time-consuming.
To address this efficiency issue, we presented a
31
Approach Advantages Disadvantages
Standard Easy to retest system (no re-annotation required) Costly
Easy to compare systems Time-Consuming
Most reliably estimates precision and recall Difficult to use multiple raters
Sampling Efficient, especially for low-frequency errors Less reliable estimate of recall
Permits estimation of precision and recall Hard to re-test system (re-annotation required)
More easily allows use of multiple raters Hard to compare systems
Verification Efficient, especially for low-frequency errors Does not permit estimation of recall
More easily allows use of multiple raters Hard to re-test system (re-annotation required)
Hard to compare systems
Table 7: Comparison of Evaluation Methods
sampling approach that produces results compa-
rable to exhaustive annotation. This makes using
multiple raters possible since less time is required
to assess the system?s performance. While the
work presented here has focused on prepositions,
the reasons for using multiple raters and a sam-
pling approach apply equally to other error types,
such as determiners and collocations.
It should be noted that the work here uses two
raters. For future work, we plan on annotating
preposition errors with more than two raters to de-
rive a range of judgments. We also plan to look at
the effects of feedback for errors involving prepo-
sitions and determiners, on the quality of ESLwrit-
ing.
The preposition error detection system de-
scribed here was recently integrated into Cri-
terion
SM
Online Writing Evaluation Service
developed by Educational Testing Service.
Acknowledgements We would first like to
thank our two annotators Sarah Ohls and Waverly
VanWinkle for their hours of hard work. We would
also like to acknowledge the three anonymous
reviewers and Derrick Higgins for their helpful
comments and feedback.
References
Bitchener, J., S. Young, and D. Cameron. 2005. The ef-
fect of different types of corrective feedback on ESL
student writing. Journal of Second Language Writ-
ing.
Burghardt, L. 2002. Foreign applications soar at uni-
versities. New York Times, April.
Chodorow, M. and C. Leacock. 2000. An unsupervised
method for detecting grammatical errors. In NAACL.
Chodorow, M., J. Tetreault, and N-R. Han. 2007. De-
tection of grammatical errors involving prepositions.
In Proceedings of the Fourth ACL-SIGSEM Work-
shop on Prepositions.
Dagan, I. and S. Engelson. 1995. Committee-based
sampling for training probabilistic classifiers. In
Proceedings of ICML, pages 150?157.
Eeg-Olofsson, J. and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. In Nodalida.
Engelson, S. and I. Dagan. 1996. Minimizing manual
annotation cost in supervised training from corpora.
In Proceedings of ACL, pages 319?326.
Felice, R. De and S. Pullman. 2007. Automatically ac-
quiring models of preposition use. In Proceedings of
the Fourth ACL-SIGSEM Workshop on Prepositions.
Gamon, M., J. Gao, C. Brockett, A. Klementiev, W. B.
Dolan, D. Belenko, and L. Vanderwende. 2008. Us-
ing contextual speller techniques and language mod-
eling for esl error correction. In IJCNLP.
Han, N-R., M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Natural Language Engineering, 12:115?
129.
Izumi, E., K. Uchimoto, T. Saiga, T. Supnithi, and
H. Isahara. 2003. Automatic error detection in the
Japanese leaners? English spoken data. In ACL.
Izumi, E., K. Uchimoto, and H. Isahara. 2004. The
overview of the sst speech corpus of Japanese learner
English and evaluation through the experiment on
automatic detection of learners? errors. In LREC.
Levin, B. 1993. English verb classes and alternations:
a preliminary investigation. Univ. of Chicago Press.
Nagata, R., A. Kawai, K. Morihiro, and N. Isu. 2006.
A feedback-augmented method for detecting errors
in the writing of learners of English. In Proceedings
of the ACL/COLING.
NCES. 2002. National center for educational statis-
tics: Public school student counts, staff, and graduate
counts by state: School year 2000-2001.
Ratnaparkhi, A. 1998. Maximum Entropy Models for
natural language ambiguity resolution. Ph.D. thesis,
University of Pennsylvania.
Tetreault, J. and M. Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In COLING.
32
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45?48,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Rethinking Grammatical Error Annotation and Evaluation with the
Amazon Mechanical Turk
Joel R. Tetreault
Educational Testing Service
Princeton, NJ, 08540, USA
JTetreault@ets.org
Elena Filatova
Fordham University
Bronx, NY, 10458, USA
filatova@fordham.edu
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@
hunter.cuny.edu
Abstract
In this paper we present results from two pi-
lot studies which show that using the Amazon
Mechanical Turk for preposition error anno-
tation is as effective as using trained raters,
but at a fraction of the time and cost. Based
on these results, we propose a new evaluation
method which makes it feasible to compare
two error detection systems tested on different
learner data sets.
1 Introduction
The last few years have seen an explosion in the de-
velopment of NLP tools to detect and correct errors
made by learners of English as a Second Language
(ESL). While there has been considerable empha-
sis placed on the system development aspect of the
field, with researchers tackling some of the tough-
est ESL errors such as those involving articles (Han
et al, 2006) and prepositions (Gamon et al, 2008),
(Felice and Pullman, 2009), there has been a woeful
lack of attention paid to developing best practices for
annotation and evaluation.
Annotation in the field of ESL error detection has
typically relied on just one trained rater, and that
rater?s judgments then become the gold standard for
evaluating a system. So it is very rare that inter-rater
reliability is reported, although, in other NLP sub-
fields, reporting reliability is the norm. Time and
cost are probably the two most important reasons
why past work has relied on only one rater because
using multiple annotators on the same ESL texts
would obviously increase both considerably. This is
especially problematic for this field of research since
some ESL errors, such as preposition usage, occur at
error rates as low as 10%. This means that to collect
a corpus of 1,000 preposition errors, an annotator
would have to check over 10,000 prepositions.1
(Tetreault and Chodorow, 2008b) challenged the
view that using one rater is adequate by showing
that preposition usage errors actually do not have
high inter-annotator reliability. For example, trained
raters typically annotate preposition errors with a
kappa around 0.60. This low rater reliability has
repercussions for system evaluation: Their experi-
ments showed that system precision could vary as
much as 10% depending on which rater?s judgments
they used as the gold standard. For some grammat-
ical errors such as subject-verb agreement, where
rules are clearly defined, it may be acceptable to
use just one rater. But for usage errors, the rules
are less clearly defined and two native speakers can
have very different judgments of what is acceptable.
One way to address this is by aggregating a multi-
tude of judgments for each preposition and treating
this as the gold standard, however such a tactic has
been impractical due to time and cost limitations.
While annotation is a problem in this field, com-
paring one system to another has also been a major
issue. To date, none of the preposition and article
error detection systems in the literature have been
evaluated on the same corpus. This is mostly due to
the fact that learner corpora are difficult to acquire
(and then annotate), but also to the fact that they are
1(Tetreault and Chodorow, 2008b) report that it would take
80hrs for one of their trained raters to find and mark 1,000
preposition errors.
45
usually proprietary and cannot be shared. Examples
include the Cambridge Learners Corpus2 used in
(Felice and Pullman, 2009), and TOEFL data, used
in (Tetreault and Chodorow, 2008a). This makes it
difficult to compare systems since learner corpora
can be quite different. For example, the ?difficulty?
of a corpus can be affected by the L1 of the writ-
ers, the number of years they have been learning En-
glish, their age, and also where they learn English (in
a native-speaking country or a non-native speaking
country). In essence, learner corpora are not equal,
so a system that performs at 50% precision in one
corpus may actually perform at 80% precision on
a different one. Such an inability to compare sys-
tems makes it difficult for this NLP research area to
progress as quickly as it otherwise might.
In this paper we show that the Amazon Mechani-
cal Turk (AMT), a fast and cheap source of untrained
raters, can be used to alleviate several of the evalua-
tion and annotation issues described above. Specifi-
cally we show:
? In terms of cost and time, AMT is an effec-
tive alternative to trained raters on the tasks of
preposition selection in well-formed text and
preposition error annotation in ESL text.
? With AMT, it is possible to efficiently collect
multiple judgments for a target construction.
Given this, we propose a new method for evalu-
ation that finally allows two systems to be com-
pared to one another even if they are tested on
different corpora.
2 Amazon Mechnical Turk
Amazon provides a service called the Mechani-
cal Turk which allows requesters (companies, re-
searchers, etc.) to post simple tasks (known as Hu-
man Intelligence Tasks, or HITs) to the AMT web-
site for untrained raters to perform for payments as
low as $0.01 in many cases (Sheng et al, 2008).
Recently, AMT has been shown to be an effective
tool for annotation and evalatuation in NLP tasks
ranging from word similarity detection and emotion
detection (Snow et al, 2008) to Machine Transla-
tion quality evaluation (Callison-Burch, 2009). In
these cases, a handful of untrained AMT workers
2http://www.cambridge.org/elt
(or Turkers) were found to be as effective as trained
raters, but with the advantage of being considerably
faster and less expensive. Given the success of us-
ing AMT in other areas of NLP, we test whether we
can leverage it for our work in grammatical error de-
tection, which is the focus of the pilot studies in the
next two sections.
The presence of a gold standard in the above pa-
pers is crucial. In fact, the usability of AMT for text
annotation has been demostrated in those studies by
showing that non-experts? annotation converges to
the gold standard developed by expert annotators.
However, in our work we concentrate on tasks where
there is no single gold standard, either because there
are multiple prepositions that are acceptable in a
given context or because the conventions of preposi-
tion usage simply do not conform to strict rules.
3 Selection Task
0.60
0.65
0.70
0.75
0.80
0.85
0.90
1 2 3 4 5 6 7 8 9 10
Kap
pa
Number of Turkers
Writer vs. AMTRater 1 vs. AMTRater 2 vs. AMT
Figure 1: Error Detection Task: Reliability of AMT as a
function of number of judgments
Typically, an early step in developing a preposi-
tion or article error detection system is to test the
system on well-formed text written by native speak-
ers to see how well the system can predict, or select,
the writer?s preposition given the context around
the preposition. (Tetreault and Chodorow, 2008b)
showed that trained human raters can achieve very
high agreement (78%) on this task. In their work, a
rater was shown a sentence with a target preposition
replaced with a blank, and the rater was asked to se-
lect the preposition that the writer may have used.
We replicate this experiment not with trained raters
but with the AMT to answer two research questions:
1. Can untrained raters be as effective as trained
46
raters? 2. If so, how many raters does it take to
match trained raters?
In the experiment, a Turker was presented with
a sentence from Microsoft?s Encarta encyclopedia,
with one preposition in that sentence replaced with
a blank. There were 194 HITs (sentences) in all, and
we requested 10 Turker judgments per HIT. Some
Turkers did only one HIT, while others completed
more than 100, though none did all 194. The Turk-
ers? performance was analyzed by comparing their
responses to those of two trained annotators and to
the Encarta writer?s preposition, which was consid-
ered the gold standard in this task. Comparing each
trained annotator to the writer yielded a kappa of
0.822 and 0.778, and the two raters had a kappa of
0.742. To determine how many Turker responses
would be required to match or exceed these levels of
reliability, we randomly selected samples of various
sizes from the sets of Turker responses for each sen-
tence. For example, when samples were of size N =
4, four responses were randomly drawn from the set
of ten responses that had been collected. The prepo-
sition that occurred most frequently in the sample
was used as the Turker response for that sentence. In
the case of a tie, a preposition was randomly drawn
from those tied for most frequent. For each sample
size, 100 samples were drawn and the mean values
of agreement and kappa were calculated. The reli-
ability results presented in Table 1 show that, with
just three Turker responses, kappa with the writer
(top line) is comparable to the values obtained from
the trained annotators (around 0.8). Most notable is
that with ten judgments, the reliability measures are
much higher than those of the trained annotators. 3
4 Error Detection Task
While the previous results look quite encouraging,
the task they are based on, preposition selection in
well-formed text, is quite different from, and less
challenging than, the task that a system must per-
form in detecting errors in learner writing. To exam-
ine the reliability of Turker preposition error judg-
ments, we ran another experiment in which Turkers
were presented with a preposition highlighted in a
sentence taken from an ESL corpus, and were in-
3We also experimented with 50 judgments per sentence, but
agreement and kappa improved only negligibly.
structed to judge its usage as either correct, incor-
rect, or the context is too ungrammatical to make
a judgment. The set consisted of 152 prepositions
in total, and we requested 20 judgments per prepo-
sition. Previous work has shown this task to be a
difficult one for trainer raters to attain high reliabil-
ity. For example, (Tetreault and Chodorow, 2008b)
found kappa between two raters averaged 0.630.
Because there is no gold standard for the er-
ror detection task, kappa was used to compare
Turker responses to those of three trained anno-
tators. Among the trained annotators, inter-kappa
agreement ranged from 0.574 to 0.650, for a mean
kappa of 0.606. In Figure 2, kappa is shown for the
comparisons of Turker responses to each annotator
for samples of various sizes ranging from N = 1 to
N = 18. At sample size N = 13, the average kappa is
0.608, virtually identical to the mean found among
the trained annotators.
0.40
0.45
0.50
0.55
0.60
0.65
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
Kap
pa
Number of Turkers
Rater 1 vs. AMTRater 2 vs. AMTRater 3 vs. AMTMean
Figure 2: Error Detection Task: Reliability of AMT as a
function of number of judgments
5 Rethinking Evaluation
We contend that the Amazon Mechanical Turk can
not only be used as an effective alternative annota-
tion source, but can also be used to revamp evalu-
ation since multiple judgments are now easily ac-
quired. Instead of treating the task of error detection
as a ?black or white? distinction, where a preposi-
tion is either correct or incorrect, cases of prepo-
sition use can now be grouped into bins based on
the level of agreement of the Turkers. For example,
if 90% or more judge a preposition to be an error,
47
Task # of HITs Judgments/HIT Total Judgments Cost Total Cost # of Turkers Total Time
Selection 194 10 1,940 $0.02 $48.50 49 0.5 hours
Error Detection 152 20 3,040 $0.02 $76.00 74 6 hours
Table 1: AMT Experiment Statistics
the high agreement is strong evidence that this is a
clear case of an error. Conversely, agreement lev-
els around 50% would indicate that the use of a par-
ticular preposition is highly contentious, and, most
likely, it should not be flagged by an automated er-
ror detection system.
The current standard method treats all cases of
preposition usage equally, however, some are clearly
harder to annotate than others. By breaking an eval-
uation set into agreement bins, it should be possible
to separate the ?easy? cases from the ?hard? cases
and report precision and recall results for the differ-
ent levels of human agreement represented by differ-
ent bins. This method not only gives a clearer pic-
ture of how a system is faring, but it also ameliorates
the problem of cross-system evaluation when two
systems are evaluated on different corpora. If each
evaluation corpus is annotated by the same number
of Turkers and with the same annotation scheme, it
will now be possible to compare systems by sim-
ply comparing their performance on each respective
bin. The assumption here is that prepositions which
show X% agreement in corpus A are of equivalent
difficulty to those that show X% agreement in cor-
pus B.
6 Discussion
In this paper, we showed that the AMT is an ef-
fective tool for annotating grammatical errors. At
a fraction of the time and cost, it is possible to
acquire high quality judgments from multiple un-
trained raters without sacrificing reliability. A sum-
mary of the cost and time of the two experiments
described here can be seen in Table 1. In the task of
preposition selection, only three Turkers are needed
to match the reliability of two trained raters; in the
more complicated task of error detection, up to 13
Turkers are needed. However, it should be noted
that these numbers can be viewed as upper bounds.
The error annotation scheme that was used is a very
simple one. We intend to experiment with different
guidelines and instructions, and to screen (Callison-
Burch, 2009) and weight Turkers? responses (Snow
et al, 2008), in order to lower the number of Turk-
ers required for this task. Finally, we will look at
other errors, such as articles, to determine howmany
Turkers are necessary for optimal annotation.
Acknowledgments
We thank Sarah Ohls and Waverely VanWinkle for
their annotation work, and Jennifer Foster and the
two reviewers for their comments and feedback.
References
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP.
Rachele De Felice and Stephen G. Pullman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3).
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using contextual speller
techniques and language modeling for esl error cor-
rection. In Proceedings of IJCNLP, Hyderabad, India,
January.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12:115?129.
Victor Sheng, Foster Provost, and Panagiotis Ipeirotis.
2008. Get another label? Improving data quality and
data mining using multiple, noisy labelers. In Pro-
ceeding of ACM SIGKDD, Las Vegas, Nevada, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In EMNLP.
Joel R. Tetreault and Martin Chodorow. 2008a. The ups
and downs of preposition error detection in ESL writ-
ing. In COLING.
Joel Tetreault and Martin Chodorow. 2008b. Native
Judgments of non-native usage: Experiments in prepo-
sition error detection. In COLING Workshop on Hu-
man Judgments in Computational Linguistics.
48
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 74?79,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards Using Structural Events To Assess Non-native Speech
Lei Chen, Joel Tetreault, Xiaoming Xi
Educational Testing Service (ETS)
Princeton, NJ 08540, USA
{LChen,JTetreault,XXi}@ets.org
Abstract
We investigated using structural events, e.g.,
clause and disfluency structure, from tran-
scriptions of spontaneous non-native speech,
to compute features for measuring speaking
proficiency. Using a set of transcribed au-
dio files collected from the TOEFL Practice
Test Online (TPO), we conducted a sophisti-
cated annotation of structural events, includ-
ing clause boundaries and types, as well as
disfluencies. Based on words and the anno-
tated structural events, we extracted features
related to syntactic complexity, e.g., the mean
length of clause (MLC) and dependent clause
frequency (DEPC), and a feature related to
disfluencies, the interruption point frequency
per clause (IPC). Among these features, the
IPC shows the highest correlation with holis-
tic scores (r = ?0.344). Furthermore, we in-
creased the correlation with human scores by
normalizing IPC by (1) MLC (r = ?0.386),
(2) DEPC (r = ?0.429), and (3) both (r =
?0.462). In this research, the features derived
from structural events of speech transcriptions
are found to predict holistic scores measuring
speaking proficiency. This suggests that struc-
tural events estimated on speech word strings
provide a potential way for assessing non-
native speech.
1 Introduction
In the last decade, a breakthrough in speech pro-
cessing is the emergence of a lot of active research
work on automatic estimation of structural events,
e.g., sentence structure and disfluencies, on sponta-
neous speech (Shriberg et al, 2000; Liu, 2004; Os-
tendorf et al, 2008). The detected structural events
have been successfully used in many natural lan-
guage processing (NLP) applications (Ostendorf et
al., 2008).
However, the structural events in speech data
haven?t been largely utilized by the research on us-
ing automatic speech recognition (ASR) technology
to assess speech proficiency (Neumeyer et al, 2000;
Zechner et al, 2007), which mainly used cues de-
rived at the word level, such as timing information
of spoken words. The information beyond the word
level, e.g., clause/sentence structure of utterances
and disfluency structure, has not been or is poorly
represented. For example, in Zechner et al (2007),
only special words for filled pauses such as um and
uh were obtained from ASR results to represent dis-
fluencies.
Given the successful usage of structural events
on a wide range of NLP applications and the fact
that the usage of these events is missing in the auto-
matic speech assessment research, a research ques-
tion emerges: Can we use structural events of spon-
taneous speech to assess non-native speech profi-
ciency?
We will address this question in this paper. The
paper is organized as follows: Section 2 reviews
previous research. Section 3 describes our annota-
tion convention. Section 4 reports on the data col-
lection, annotation, and quality control. Section 5
reports on features based on structural event anno-
tations. Section 6 reports on our experiments. Sec-
tion 7 discusses our findings and plans for future re-
search work.
74
2 Previous Work
In the last decade, a large amount of research (Os-
tendorf et al, 2008) has been conducted on detection
of structural events, e.g., sentence structure and dis-
fluency structure, in spontaneous speech. In these
research works, the structural events were detected
with a quite high accuracy. Furthermore, the de-
tected sentence and disfluency structures have been
found to help many of the following NLP tasks,
e.g., speech parsing, information retrieval, machine
translation, and extractive speech summary (Osten-
dorf et al, 2008).
In the second language acquisition (SLA) and
child language development research fields, the lan-
guage development is measured according to flu-
ency, accuracy, and complexity (Iwashita, 2006).
The syntactic complexity of learners? writing data
has been extensively studied in the SLA commu-
nity (Ortega, 2003). Recently, this study has been
extended to the learner?s speaking data (Iwashita,
2006). Typical metrics for examining syntactic com-
plexity include: length of production unit (e.g., T-
unit, which is defined as essentially a main clause
plus any other clauses which are dependent upon
it (Hunt, 1970), clauses, verb phrases, and sen-
tences), amount of embedding, subordination and
coordination, range of structural types, and structure
sophistication.
Iwashita (2006) investigated several measures for
syntactic complexity on the data from learners of
Japanese. The author reported that some measure-
ments, e.g., T-unit length, the number of clauses per
T-unit, and the number of independent clauses per T-
Unit, were good at predicting learners? proficiency
levels.
In addition, some previous studies used measure-
ments related to disfluencies to assess speaking pro-
ficiency. For example, Lennon (1990) used a dozen
features related to speed, pauses, and several dis-
fluency markers, such as filler pauses per T-unit,
to measure four German-speaking women?s English
improvement during a half year study in England.
He found a significant change in filled pauses per
T-unit during the studying process.
The features related to syntactic complexity and
the features related to ?smoothness? (disfluency) of
speech were jointly used in some previous stud-
ies. For example, Mizera (2006) used fluency fac-
tors related to speed, voiced smoothness (frequen-
cies of repetitions or self-corrections), pauses, syn-
tactic complexity (mean length of T-units), and
accuracy, to measure speaking proficiency on 20
non-native English speakers. In this experiment,
disfluency-related factors, such as total voiced dis-
fluencies, had a high correlation with fluency (r =
?0.45). However, the syntactic complexity factor
only showed a moderate correlation (r = 0.310).
Yoon (2009) implemented an automated disfluency
detection method and found that the disfluency-
related features lead to the moderate improvement
in the automated speech proficiency scoring.
There were limitations on using the features re-
ported in these SLA studies on standard language
tests. For example, only a very limited number of
subjects (from 20 to 30 speakers) were used in these
studies. Second, the speaking content was narra-
tions of picture books or cartoon videos rather than
standard test questions. Therefore, we conducted a
study using a much larger data set obtained from real
speech tests to address these limitations.
3 Structural Event Annotation Convention
To annotate structural events of speech content, we
have developed a convention based on previous stud-
ies and our observations on the TOEFL Practice On-
line (TPO) test data. Defining clauses is a relatively
simple task; however, defining clause boundaries
and specifying which elements fall within a particu-
lar clause is a much more challenging task for spo-
ken discourse, due to the presence of grammatical
errors, fragments, repetitions, self corrections, and
conversation fillers.
Foster et al (Foster et al, 2000) review various
units for analyzing spoken language, including syn-
tactic, semantic and intonational units, and propose
a new analysis of speech unit (AS-Unit) that they
claim is appropriate for many different purposes. In
this study, we focused on clauses given the charac-
teristics of spontaneous speech. Also, we defined
clause types based on grammar books such as (Azar,
2003). The following clause types were defined:
? Simple sentence (SS) contains a subject and a
verb, and expresses a complete thought.
75
? Independent clause (I) is the main clause that
can stand along syntactically as a complete sen-
tence. It consists minimally a subject and a fi-
nite verb (a verb that shows tense, person, or
singular/plural, e.g., he goes, I went, and I was).
? Subordinate clause is a clause in a complex
sentence that cannot stand alone as a complete
sentence and that functions within the sentence
as a noun, a verb complement, an adjective or
an adverb. There are three types of subordi-
nate clauses: noun clause (NC), relative clause
that functions as an adjective (ADJ), adverbial
clause that functions as an adverb (ADV).
? Coordinate clause (CC) is a clause in a com-
pound sentence that is grammatically equiva-
lent to the main clause and that performs the
same grammatical function.
? Adverbial phrase (ADVP) is a separate clause
from the main clause that contains a non-finite
verb (a verb that does not show tense, person,
or singular/plural).
The clause boundaries and clause types were an-
notated on the word transcriptions. Round brack-
ets were used to indicate the beginning and end of a
clause. Then, the abbreviations described above for
clause types were added. Also, if a specific bound-
ary serves as the boundaries for both the local and
global clause, the abbreviation of the local clause
was followed by that of the global. Some examples
of clause boundaries and types are reported in Ta-
ble 1.
In our annotation manual, a speech disfluency
contains three parts:
? Reparandum: the speech portion that will be
repeated, corrected, or even abandoned. The
end of the reparandum is called the interruption
point (IP), which indicates the stop of a normal
fluent speech stream.
? Editing phrase: optional inserted words, e.g.,
um.
? Correction: the speech portion that repeats,
corrects, or even starts new content.
In our annotation manual, the reparandum was en-
closed by ?*?, the editing phrase was enclosed by
?%?, and the correction was enclosed by ?$?. For
example, in the following utterance, ?He is a * very
mad * % er % $ very bad $ cop?, ?very mad? was
corrected by ?very bad? and an editing phrase, er,
was inserted.
4 Data Collection and Annotation
4.1 Audio data collection and scoring
About 1300 speech responses from the TPO test
were collected and transcribed. Each item was
scored by two experienced human raters indepen-
dently using a 4-point holistic score based on the
scoring rubrics designed for the test.
In the TPO test, some tasks required test-takers to
provide information or opinions on familiar topics
based on their personal experience or background
knowledge. Others required them to summarize and
synthesize information presented in listening and/or
reading materials. Each test-taker was required to
finish six items in one test session. Each item has a
45 or 60 seconds response time.
4.2 Annotation procedure
Two annotators (who were not the human raters
mentioned above) with a linguistics background and
past linguistics annotation experience were first pre-
sented with a draft of the annotation convention.
After reading through it, the annotators, as well as
the second and third author completed four iterative
loops of rating 4 or 5 responses per meeting. All four
discussed differences in annotations and the conven-
tion was refined as needed. After the final iteration
of comparisons, the raters seemed to have very few
disagreement and thus began annotating sets of re-
sponses. Each set consisted of roughly 50-75 re-
sponses and then a kappa set of 30-50 responses
which both annotators completed. Accordingly, be-
tween the two annotators, a set comprised roughly
130 to 200 responses. Each response takes roughly
3-8 minutes to annotate. The annotators were in-
structed to listen to the corresponding audio file if
they needed the prosodic information to annotate a
particular speech disfluency event.
76
Clause type Example
SS (That?s right |SS)
I (He turned away |I) as soon as he saw me |ADV)
NC ((What he did |NC) shocked me |I)
ADJ (She is the woman (I told you about |ADJ)|I)
ADV (As soon as he saw me |ADV) (he turned away |I)
CC (I will go home |I) (and he will go to work |CC)
ADVP (While walking to class |ADVP) (I ran into a friend |I)
Table 1: Examples of clause boundary and type annotation
4.3 Evaluation of annotation
To evaluate the quality of structural event anno-
tation, we measured the inter-rater agreement on
clause boundary (CB) annotation and interruption
point (IP) of disfluencies1.
We used Cohen?s ? to calculate the annotator
agreement on each kappa set. ? is calculated on the
absence or presence of a boundary marker (either a
clause boundary (CB) or an interruption point (IP)
between consecutive words). For each consecutive
pair of words, we check for the existence of one or
more boundaries, and collapse the set into one term
?boundary? and then compute the agreement on this
reduced annotation.
In Table 2, we list the annotator agreement for
both boundary events over 4 kappa sets. The second
column refers to the number of speech responses in
the kappa set, the next two columns refer to the an-
notator agreement using the Cohen?s ? value on CB
and IP annotation results.
Set N ? CB ? IP
Set1 54 0.886 0.626
Set2 71 0.847 0.687
Set3 35 0.855 0.695
Set4 34 0.899 0.833
Table 2: Between-rater agreement of structural event an-
notation
In general, a ? of 0.8-1.0 represents excellent
agreement, 0.6-0.8 represents good agreement, and
so forth. Over each kappa set, ? for CB annota-
tions ranges between 0.8 and 0.9, which is an ex-
1Measurement on CBs and IPs can provide a rough qual-
ity measurement of annotations. In addition, doing so is more
important to us since automatic detection of these two types of
events will be investigated in future.
cellent agreement; ? for IP annotation ranges be-
tween 0.6 and 0.8, which is a good agreement. Com-
pared to annotating clauses, marking disfluencies is
more challenging. As a result, a lower between-rater
agreement is expected.
5 Features Derived On Structural Events
Based on the structural event annotations, including
clause boundaries and their types, as well as disflu-
encies, some features measuring syntactic complex-
ity and disfluency profile were derived.
Since simple sentence (SS), independent clause
(I), and conjunct clause (CC) represent a complete
idea, we treat them as an approximate to a T-unit (T).
The clauses that have no complete idea, are depen-
dent clauses (DEP), including noun clauses (N), rel-
ative clauses that function as adjective (ADJ), adver-
bial clauses (ADV), and adverbial phrases (ADVP).
The total number of clauses is a summation of the
number of T-units (T), dependent clauses (DEP), and
fragments2 (denoted as F). Therefore,
NT = NSS +NI +NCC
NDEP = NNC +NADJ +NADV +NADV P
NC = NT +NDEP +NF
Assuming Nw is the total number of words in
the speech response (without pruning speech re-
pairs), the following features, including mean length
of clause (MLC), dependent clauses per clause
(DEPC), and interruption points per clause (IPC),
are derived:
MLC = Nw/NC
2It is either a subordinate clause that does not have a cor-
responding independent clause or a string of words without a
subject or a verb that does not express a complete thought.
77
DEPC = NDEP /NC
IPC = NIP /NC
Furthermore, we elaborated the IPC feature. Dis-
fluency is a complex behavior and is influenced by
a variety of factors, such as proficiency level, speak-
ing rate, and familiarity with speaking content. The
complexity of utterances is also an important fac-
tor on the disfluency pattern. For example, Roll
et al (Roll et al, 2007) found that complexity of
expression computed based on the language?s pars-
ing tree structure influenced the frequency of disflu-
encies in their experiment on Swedish. Therefore,
since disfluency frequency was not only influenced
by the test-takers? speaking proficiency but also by
the utterance?s syntactic structure?s difficulty, we re-
duced the impact from the syntactic structure so that
we can focus on speakers? ability. For this purpose,
we normalized IPC by dividing by some features re-
lated to syntactic-structure?s complexity, including
MLC, DEPC, and both. Therefore, the following
elaborated disfluency-related features were derived:
IPCn1 = IPC/MLC
IPCn2 = IPC/DEPC
IPCn3 = IPC/MLC/DEPC
6 Experiment
For each item, two human raters rated it separately
with a score from 1 to 4. If these two scores are
consistent (the difference between two scores is ei-
ther zero or one), we put this item in an item-pool.
Finally, a total of 1, 257 audio items were included
in the pool. Following the score-handling protocol
used in the TPO test, we used the first human rater?s
score as the item score. From the obtained item-
pool, we selected speakers with more than three
items so that the averaged score per speaker can be
estimated on several items to achieve a robust score
computation3. As a result, 175 speakers4 were se-
lected.
3The mean holistic score of these speakers is 2.786, which
is close to the mean holistic score of the selected item-pool
(2.785), indicating that score distribution was kept after focus-
ing on speakers with more than three items.
4If a speaker was assigned in a Kappa set in the annotation
as described in Section 4, this speaker would have as many as 12
annotated items. Therefore, the minimum number of speakers
from the item-pool was about 105 (1257/12).
For each speaker, his or her annotations of words
and structural events were used to extract the fea-
tures described in Section 5. Then, we computed
the Pearson correlation among the obtained features
with the averaged holistic scores per speaker.
Feature r
MLC 0.211
DEPC 0.284
IPC -0.344
IPCn1 -0.386
IPCn2 -0.429
IPCn3 -0.462
Table 3: Correlation coefficients (rs) between the fea-
tures derived from structural events with human scores
averaged on test takers
Table 3 reports on the correlation coefficient
(r) between the proposed features derived from
structural events with holistic scores. Relying
on three simple structural event annotations, i.e.,
clause boundaries, dependent clauses, and interrup-
tion points in speech disfluencies, some promising
correlations between features with holistic scores
were found. Between the two syntactic complex-
ity features, the DEPC has a higher correlation with
holistic scores than the MLC (0.284 > 0.211). It ap-
pears that a measurement about clauses? embedding
profile is more informative about a speaker?s profi-
ciency level. Second, compared to features measur-
ing syntactic complexity, the feature measuring the
disfluency profile is better to predict human holis-
tic scores on this non-native data set. For example,
IPC has a r of ?0.344, which is better than the fea-
tures about clause lengths or embedding. Finally, by
jointly using the structural events related to clauses
and disfluencies, we can further achieve a further
improved r. Compared to IPC, IPCn3 has a relative
34.30% correlation increase. This is consistent with
our idea of reducing utterance-complexity?s impact
on disfluency-related features.
7 Discussion
In most current automatic speech assessment sys-
tems, features derived from recognized words, such
as delivery features about speaking rate, pause infor-
mation, and accuracy related to word identities, have
been widely used to assess non-native speech from
78
fluency and accuracy points of view. However, in-
formation beyond recognized words, e.g., the struc-
ture of clauses and disfluencies, has only received
limited attention. Although several previous SLA
studies used features derived from structural events
to measure speaking proficiency, these studies were
limited and the findings from them were difficult to
directly apply to on large-scale standard tests.
In this paper, using a large-sized data set col-
lected in the TPO speaking test, we conducted an
sophisticated annotation of structural events, includ-
ing boundaries and types of clauses and disfluen-
cies, from transcriptions of spontaneous speech test
responses. A series of features were derived from
these structural event annotations and were eval-
uated according to their correlations with holistic
scores. We found that disfluency-related features
have higher correlations to human holistic scores
than features about syntactic complexity, which con-
firms the result reported in (Mizera, 2006). In spon-
taneous speech utterances, simple syntactic structure
tends to be utilized by speakers. This is in contrast to
sophisticated syntactic structure appearing in writ-
ing. This may cause that complexity-related features
are poor at predicting fluency scores. On the other
hand, disfluencies, a pattern unique to spontaneous
speech, were found to play a more important role in
indicating speaking proficiency levels.
Although syntactic complexity features were not
highly indicative of holistic scores, they were useful
to further improve disfluency-related features? corre-
lation with holistic scores. By normalizing IPC us-
ing measurements representing syntactic complex-
ity, we can highlight contributions from speakers?
proficiency levels. Therefore, in our experiment,
IPCn3 shows a 34.30% relative improvement in its
correlation coefficient with human holistic scores
over the original IPC.
The study reported in this paper suggests promise
that structural events beyond speech recognition re-
sults can be utilized to measure non-native speaker
proficiency levels. Recently, in the NLP research
field, an increasing amount of effort has been
made on structural event detection in spontaneous
speech (Ostendorf et al, 2008). Therefore, such
progress can benefit the study of automatic estima-
tion of structural events on non-native speech data.
For our future research plan, first, we will inves-
tigate automatically detecting these structural events
from speech transcriptions and recognition hypothe-
ses. Second, the features derived from the obtained
structural events will be used to augment the features
in automatic speech assessment research to provide
a wider construct coverage than fluency and pronun-
ciation features do.
References
B. Azar. 2003. Fundamentals of English grammar.
Pearson Longman, White Plains, NY, 3rd edition.
P. Foster, A. Tonkyn, and G. Wigglesworth. 2000. Mea-
suring spoken language: A unit for all reasons. Ap-
plied Linguistics, 21(3):354.
K. W. Hunt. 1970. Syntactic maturity in school chil-
dren and adults. In Monographs of the Society for Re-
search in Child Development. University of Chicago
Press, Chicago, IL.
N. Iwashita. 2006. Syntactic complexity measures and
their relation to oral proficiency in Japanese as a for-
eign language. Language Assessment Quarterly: An
International Journal, 3(2):151?169.
P. Lennon. 1990. Investigating fluency in EFL: A quanti-
tative approach. Language Learning, 40(3):387?417.
Y. Liu. 2004. Structural Event Detection for Rich Tran-
scription of Speech. Ph.D. thesis, Purdue University.
G. J. Mizera. 2006. Working memory and L2 oral flu-
ency. Ph.D. thesis, University of Pittsburgh.
L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.
2000. Automatic Scoring of Pronunciation Quality.
Speech Communication, 30:83?93.
L. Ortega. 2003. Syntactic complexity measures and
their relationship to L2 proficiency: A research syn-
thesis of college-level L2 writing. Applied Linguistics,
24(4):492.
M. Ostendorf et al 2008. Speech segmentation and spo-
ken document processing. Signal Processing Maga-
zine, IEEE, 25(3):59?69, May.
M. Roll, J. Frid, and M. Horne. 2007. Measuring syntac-
tic complexity in spontaneous spoken Swedish. Lan-
guage and Speech, 50(2):227.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech Communi-
cation, 32(1-2):127?154.
S. Yoon. 2009. Automated assessment of speech fluency
for L2 English learners. Ph.D. thesis, University of
Illinois at Urbana-Champaign.
K. Zechner, D. Higgins, and Xiaoming Xi. 2007.
SpeechRater: A Construct-Driven Approach to Scor-
ing Spontaneous Non-Native Speech. In Proc. SLaTE.
79
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108?115,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
E-rating Machine Translation
Kristen Parton1 Joel Tetreault2 Nitin Madnani2 Martin Chodorow3
1Columbia University, NY, USA
kristen@cs.columbia.edu
2Educational Testing Service, Princeton, NJ, USA
{jtetreault, nmadnani}@ets.org
3Hunter College of CUNY, New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
We describe our submissions to the WMT11
shared MT evaluation task: MTeRater and
MTeRater-Plus. Both are machine-learned
metrics that use features from e-rater R?, an au-
tomated essay scoring engine designed to as-
sess writing proficiency. Despite using only
features from e-rater and without comparing
to translations, MTeRater achieves a sentence-
level correlation with human rankings equiva-
lent to BLEU. Since MTeRater only assesses
fluency, we build a meta-metric, MTeRater-
Plus, that incorporates adequacy by combin-
ing MTeRater with other MT evaluation met-
rics and heuristics. This meta-metric has a
higher correlation with human rankings than
either MTeRater or individual MT metrics
alone. However, we also find that e-rater fea-
tures may not have significant impact on cor-
relation in every case.
1 Introduction
The evaluation of machine translation (MT) systems
has received significant interest over the last decade
primarily because of the concurrent rising interest in
statistical machine translation. The majority of re-
search on evaluating translation quality has focused
on metrics that compare translation hypotheses to a
set of human-authored reference translations. How-
ever, there has also been some work on methods that
are not dependent on human-authored translations.
One subset of such methods is task-based in that
the methods determine the quality of a translation in
terms of how well it serves the need of an extrin-
sic task. These tasks can either be downstream NLP
tasks such as information extraction (Parton et al,
2009) and information retrieval (Fujii et al, 2009) or
human tasks such as answering questions on a read-
ing comprehension test (Jones et al, 2007).
Besides extrinsic evaluation, there is another set
of methods that attempt to ?learn? what makes a
good translation and then predict the quality of new
translations without comparing to reference trans-
lations. Corston-Oliver et al (2001) proposed the
idea of building a decision tree classifier to sim-
ply distinguish between machine and human transla-
tions using language model (LM) and syntactic fea-
tures. Kulesza and Shieber (2004) attempt the same
task using an support vector machine (SVM) classi-
fier and features derived from reference-based MT
metrics such as WER, PER, BLEU and NIST. They
also claim that the confidence score for the classi-
fier being used, if available, may be taken as an es-
timate of translation quality. Quirk (2004) took a
different approach and examined whether it is pos-
sible to explicitly compute a confidence measure for
each translated sentence by using features derived
from both the source and target language sides. Al-
brecht and Hwa (2007a) expanded on this idea and
conducted a larger scale study to show the viabil-
ity of regression as a sentence-level metric of MT
quality. They used features derived from several
other reference-driven MT metrics. In other work
(Albrecht and Hwa, 2007b), they showed that one
could substitute translations from other MT systems
for human-authored reference translations and de-
rive the regression features from them.
Gamon et al (2005) build a classifier to distin-
guish machine-generated translations from human
108
ones using fluency-based features and show that by
combining the scores of this classifier with LM per-
plexities, they obtain an MT metric that has good
correlation with human judgments but not better
than the baseline BLEU metric.
The fundamental questions that inspired our pro-
posed metrics are as follows:
? Can an operational English-proficiency mea-
surement system, built with absolutely no fore-
thought of using it for evaluation of translation
quality, actually be used for this purpose?
? Obviously, such a system can only assess the
fluency of a translation hypothesis and not the
adequacy. Can the features derived from this
system then be combined with metrics such
as BLEU, METEOR or TERp?measures of
adequacy?to yield a metric that performs bet-
ter?
The first metric we propose (MTeRater) is an
SVM ranking model that uses features derived from
the ETS e-rater R? system to assess fluency of trans-
lation hypotheses. Our second metric (MTeRater-
Plus) is a meta-metric that combines MTeRater fea-
tures with metrics such as BLEU, METEOR and
TERp as well as features inspired by other MT met-
rics.
Although our work is intimately related to some
of the work cited above in that it is a trained regres-
sion model predicting translation quality at the sen-
tence level, there are two important differences:
1. We do not use any human translations ? ref-
erence or otherwise ? for MTeRater, not even
when training the metric. The classifier is
trained using human judgments of translation
quality provided as part of the shared evalua-
tion task.
2. Most of the previous approaches use feature
sets that are designed to capture both transla-
tion adequacy and fluency. However, MTeRater
uses only fluency-based features.
The next section provides some background on
the e-rater system. Section 3 presents a discussion
of the differences between MT errors and learner er-
rors. Section 4 describes how we use e-rater to build
our metrics. Section 5 outlines our experiments and
Section 5 discusses the results of these experiments.
Finally, we conclude in Section 6.
2 E-rater
E-rater is a proprietary automated essay scoring
system developed by Educational Testing Service
(ETS) to assess writing quality.1 The system has
been used operationally for over 10 years in high-
stakes exams such as the GRE and TOEFL given
its speed, reliability and high agreement with human
raters.
E-rater combines 8 main features using linear re-
gression to produce a numerical score for an es-
say. These features are grammar, usage, mechan-
ics, style, organization, development, lexical com-
plexity and vocabulary usage. The grammar feature
covers errors such as sentence fragments, verb form
errors and pronoun errors (Chodorow and Leacock,
2000). The usage feature detects errors related to
articles (Han et al, 2006), prepositions (Tetreault
and Chodorow, 2008) and collocations (Futagi et al,
2008). The mechanics feature checks for spelling,
punctuation and capitalization errors. The style fea-
ture checks for passive constructions and word rep-
etition, among others. Organization and develop-
ment tabulate the presence or absence of discourse
elements and the length of each element. Finally,
the lexical complexity feature details how complex
the writer?s words are based on frequency indices
and writing scales, and the vocabulary feature eval-
uates how appropriate the words are for the given
topic). Since many of the features are essay-specific,
there is certainly some mismatch between what e-
rater was intended for and the genres we are using it
for in this experiment (translated news articles).
In our work, we separate e-rater features into two
classes: sentence level and document level. The
sentence level features consist of all errors marked
by the various features for each sentence alone. In
contrast, the document level features are an aggre-
gation of the sentence level features for the entire
document.
1A detailed description of e-rater is outside the scope of this
paper and the reader is referred to (Attali and Burstein, 2006).
109
3 Learner Errors vs. MT Errors
Since e-rater is trained on human-written text and
designed to look for errors in usage that are com-
mon to humans, one research question is whether it
is even useful for assessing the fluency of machine
translated text. E-rater is unaware of the transla-
tion context, so it does not look for common MT
errors, such as untranslated words, mistranslations
and deleted content words. However, these may get
flagged as other types of learner errors: spelling mis-
takes, confused words, and sentence fragments.
Machine translations do contain learner-like mis-
takes in verb conjugations and word order. In an
error analysis of SMT output, Vilar et al (2006) re-
port that 9.9% - 11.7% of errors made by a Spanish-
English SMT system were incorrect word forms, in-
cluding incorrect tense, person or number. These
error types are also account for roughly 14% of er-
rors made by ESL (English as a Second Language)
writers in the Cambridge Learner Corpus (Leacock
et al, 2010).
On the other hand, some learner mistakes are un-
likely to be made by MT systems. The Spanish-
English SMT system made almost no mistakes in
idioms (Vilar et al, 2006). Idiomatic expressions
are strongly preferred by language models, but may
be difficult for learners to memorize (?kicked a
bucket?). Preposition usage is a common problem
in non-native English text, accounting for 29% of
errors made by intermediate to advanced ESL stu-
dents (Bitchener et al, 2005) but language models
are less likely to prefer local preposition errors e.g.,
?he went to outside?. On the other hand, a language
model will likely not prevent errors in prepositions
(or in other error types) that rely on long-distance
dependencies.
4 E-rating Machine Translation
The MTeRater metric uses only features from e-rater
to score translations. The features are produced di-
rectly from the MT output, with no comparison to
reference translations, unlike most MT evaluation
metrics (such as BLEU, TERp and METEOR).
An obvious deficit of MTeRater is a measure of
adequacy, or how much meaning in the source sen-
tence is expressed in the translation. E-rater was
not developed for assessing translations, and the
MTeRater metric never compares the translation to
the source sentence. To remedy this, we propose
the MTeRater-Plus meta-metric that uses e-rater fea-
tures plus all of the hybrid features described below.
Both metrics were trained on the same data using
the same machine learning model, and differ only in
their feature sets.
4.1 E-rater Features
Each sentence is associated with an e-rater sentence-
level vector and a document-level vector as previ-
ously described and each column in these vectors
was used a feature.
4.2 Features for Hybrid Models
We used existing automatic MT metrics as baselines
in our evaluation, and also as features in our hybrid
metric. The metrics we used were:
1. BLEU (Papineni et al, 2002): Case-insensitive
and case-sensitive BLEU scores were pro-
duced using mteval-v13a.pl, which calculates
smoothed sentence-level scores.
2. TERp (Snover et al, 2009): Translation Edit
Rate plus (TERp) scores were produced using
terp v1. The scores were case-insensitive and
edit costs from Snover et al (2009) were used
to produce scores tuned for fluency and ade-
quacy.
3. METEOR (Lavie and Denkowski, 2009): Me-
teor scores were produced using Meteor-next
v1.2. All types of matches were allowed (ex-
act, stem, synonym and paraphrase) and scores
tuned specifically to rank, HTER and adequacy
were produced using the ?-t? flag in the tool.
We also implemented features closely related to
or inspired by other MT metrics. The set of these
auxiliary features is referred to as ?Aux?.
1. Character-level statistics: Based on the suc-
cess of the i-letter-BLEU and i-letter-recall
metrics from WMT10 (Callison-Burch et al,
2010), we added the harmonic mean of preci-
sion (or recall) for character n-grams (from 1
to 10) as features.
110
2. Raw n-gram matches: We calculated the pre-
cision and precision for word n-grams (up to
n=6) and added each as a separate feature (for
a total of 12). Although these statistics are also
calculated as part of the MT metrics above,
breaking them into separate features gives the
model more information.
3. Length ratios: The ratio between the lengths
of the MT output and the reference translation
was calculated on a character level and a word
level. These ratios were also calculated be-
tween the MT output and the source sentence.
4. OOV heuristic: The percentage of tokens in
the MT that match the source sentence. This
is a low-precision heuristic for counting out of
vocabulary (OOV) words, since it also counts
named entities and words that happen to be the
same in different languages.
4.3 Ranking Model
Following (Duh, 2008), we represent sentence-level
MT evaluation as a ranking problem. For a partic-
ular source sentence, there are N machine transla-
tions and one reference translation. A feature vector
is extracted from each {source, reference, MT} tu-
ple. The training data consists of sets of translations
that have been annotated with relative ranks. Dur-
ing training, all ranked sets are converted to sets of
feature vectors, where the label for each feature vec-
tor is the rank. The ranking model is a linear SVM
that predicts a relative score for each feature vector,
and is implemented by SVM-rank (Joachims, 2006).
When the trained classifier is applied to a set of N
translations for a new source sentence, the transla-
tions can then be ranked by sorting the SVM scores.
5 Experiments
All experiments were run using data from three
years of previous WMT shared tasks (WMT08,
WMT09 and WMT10). In these evaluations, anno-
tators were asked to rank 3-5 translation hypothe-
ses (with ties allowed), given a source sentence and
a reference translation, although they were only re-
quired to be fluent in the target language.
Since e-rater was developed to rate English sen-
tences only, we only evaluated tasks with English
as the target language. All years included source
languages French, Spanish, German and Czech.
WMT08 and WMT09 also included Hungarian and
multisource English. The number of MT systems
was different for each language pair and year, from
as few as 2 systems (WMT08 Hungarian-English) to
as many as 25 systems (WMT10 German-English).
All years had a newswire testset, which was divided
into stories. WMT08 had testsets in two additional
genres, which were not split into documents.
All translations were pre-processed and run
through e-rater. Each document was treated as an es-
say, although news articles are generally longer than
essays. Testsets that were not already divided into
documents were split into pseudo-documents of 20
contiguous sentences or less. Missing end of sen-
tence markers were added so that e-rater would not
merge neighboring sentences.
6 Results
For assessing our metrics prior to WMT11, we
trained on WMT08 and WMT09 and tested on
WMT10. The metrics we submitted to WMT11
were trained on all three years. One criticism of
machine-learned evaluation metrics is that they may
be too closely tuned to a few MT systems, and thus
not generalize well as MT systems evolve or when
judging new sets of systems. In this experiment,
WMT08 has 59 MT systems, WMT09 has 70 dif-
ferent MT systems, and WMT10 has 75 different
systems. Different systems participate each year,
and those that participate for multiple years often
improve from year to year. By training and test-
ing across years rather than within years, we hope
to avoid overfitting.
To evaluate, we measure correlation between each
metric and the human annotated rankings according
to (Callison-Burch et al, 2010): Kendall?s tau is cal-
culated for each language pair and the results are
averaged across language pairs. This is preferable
to averaging across all judgments because the num-
ber of systems and the number of judgments vary
based on the language pair (e.g., there were 7,911
ranked pairs for 14 Spanish-English systems, and
3,575 ranked pairs for 12 Czech-English systems).
It is difficult to calculate the statistical signifi-
cance of Kendall?s tau on these data. Unlike the
111
Source language cz de es fr avg
Individual Metrics & Baselines
MTeRater .32 .31 .19 .23 .26
bleu-case .26 .27 .28 .22 .26
meteor-rank .33 .36 .33 .27 .32
TERp-fluency .30 .36 .28 .28 .30
Meta-Metric & Baseline
BMT+Aux+MTeRater .38 .42 .37 .38 .39
BMT .35 .40 .35 .34 .36
Additional Meta-Metrics
BMT+LM .36 .41 .36 .36 .37
BMT+MTeRater .38 .42 .36 .38 .38
BMT+Aux .38 .41 .38 .37 .39
BMT+Aux+LM .39 .42 .38 .36 .39
Table 1: Kendall?s tau correlation with human rankings.
BMT includes bleu, meteor and TERp; Aux includes aux-
iliary features. BMT+Aux+MTeRater is MTeRater-Plus.
Metrics MATR annotations (Przybocki et al, 2009),
(Peterson and Przybocki, 2010), the WMT judg-
ments do not give a full ranking over all systems for
all judged sentences. Furthermore, the 95% confi-
dence intervals of Kendall?s tau are known to be very
large (Carterette, 2009) ? in Metrics MATR 2010,
the top 7 metrics in the paired-preference single-
reference into-English track were within the same
confidence interval.
To compare metrics, we use McNemar?s test
of paired proportions (Siegel and Castellan, 1988)
which is more powerful than tests of independent
proportions, such as the chi-square test for indepen-
dent samples.2 As in Kendall?s tau, each metric?s
relative ranking of a translation pair is compared to
that of a human. Two metrics, A and B, are com-
pared by counting the number of times both A and B
agree with the human ranking, the number of times
A disagrees but B agrees, the number of times A
agrees but B disagrees, and the number of times both
A and B disagree. These counts can be arranged in
a 2 x 2 contingency table as shown below.
A agrees A disagrees
B agrees a b
B disagrees c d
McNemar?s test determines if the cases of mis-
match in agreement between the metrics (cells b and
c) are symmetric or if there is a significant difference
2See http://faculty.vassar.edu/lowry/propcorr.html for an ex-
cellent description.
in favor of one of the metrics showing more agree-
ment with the human than the other. The two-tailed
probability for McNemar?s test can be calculated us-
ing the binomial distribution over cells b and c.
6.1 Reference-Free Evaluation with MTeRater
The first group of rows in Table 1 shows the
Kendall?s tau correlation with human rankings of
MTeRater and the best-performing version of the
three standard MT metrics. Even though MTeR-
ater is blind to the MT context and does not use the
source or references at all, MTeRater?s correlation
with human judgments is the same as case-sensitive
bleu (bleu-case). This indicates that a metric trained
to assess English proficiency in non-native speakers
is applicable to machine translated text.
6.2 Meta-Metrics
The second group in Table 1 shows the cor-
relations of our second metric, MTeRater-Plus
(BMT+Aux+MTeRater), and a baseline meta-metric
(BMT) that combined BLEU, METEOR and TERp.
MTeRater-Plus performs significantly better than
BMT, according to McNemar?s test.
We also wanted to determine whether the e-
rater features have any significant impact when used
as part of meta-metrics. To this end, we first
created two variants of MTeRater-Plus: one that
removed the MTeRater features (BMT+Aux) and
another that replaced the MTeRater features with
the LM likelihood and perplexity of the sentence
(BMT+Aux+LM).3 Both models perform as well
as MTeRater-Plus, i.e., adding additional fluency
features (either LM scores or MTeRater) to the
BMT+Aux meta-metric has no significant impact.
To determine whether this was generally the case,
we also created two variants of the BMT baseline
meta-metric that added fluency features to it: one in
the form of LM scores (BMT+LM) and another in
the form of the MTeRater score (BMT+MTeRater).
Based on McNemar?s test, both models are sig-
nificantly better than BMT, indicating that these
reference-free fluency features indeed capture an as-
pect of translation quality that is absent from the
standard MT metrics. However, there is no signfi-
cant difference between the two variants of BMT.
3The LM was trained on English Gigaword 3.0, and was
provided by WMT10 organizers.
112
1) Ref: Gordon Brown has discovered yet another hole to fall into; his way out of it remains the same
MT+: Gordon Brown discovered a new hole in which to sink; even if it resigned, the position would not change.
Errors: None marked
MT-: Gordon Brown has discovered a new hole in which could, Even if it demissionnait, the situation does not change not.
Errors: Double negative, spelling, preposition
2) Ref: Jancura announced this in the Twenty Minutes programme on Radiozurnal.
MT+: Jancura said in twenty minutes Radiozurnal. Errors: Spelling
MT-: He said that in twenty minutes. Errors: none marked
Table 2: Translation pairs ranked correctly by MTeRater but not bleu-case (1) and vice versa (2).
6.3 Discussion
Table 2 shows two pairs of ranked translations (MT+
is better than MT-), along with some of the errors de-
tected by e-rater. In pair 1, the lower-ranked trans-
lation has major problems in fluency as detected by
e-rater, but due to n-gram overlap with the reference,
bleu-case ranks it higher. In pair 2, MT- is more
fluent but missing two named entities and bleu-case
correctly ranks it lower.
One disadvantage of machine-learned metrics is
that it is not always clear which features caused one
translation to be ranked higher than another. We
did a feature ablation study for MTeRater which
showed that document-level collocation features sig-
nificantly improve the metric, as do features for
sentence-level preposition errors. Discourse-level
features were harmful to MT evaluation. This is un-
surprising, since MT sentences are judged one at a
time, so any discourse context is lost.
Overall, a metric with only document-level fea-
tures does better than one with only sentence-level
features due to data sparsity ? many sentences have
no errors, and we conjecture that the document-level
features are a proxy for the quality of the MT sys-
tem. Combining both document-level and sentence-
level e-rater features does significantly better than
either alone. Incorporating document-level features
into sentence-level evaluation had one unforeseen
effect: two identical translations can get different
scores depending on how the rest of the document
is translated. While using features that indicate the
relative quality of MT systems can improve overall
correlation, it fails when the sentence-level signal is
not strong enough to overcome the prior belief.
7 Conclusion
We described our submissions to the WMT11 shared
evaluation task: MTeRater and MTeRater-Plus.
MTeRater is a fluency-based metric that uses fea-
tures from ETS?s operational English-proficiency
measurement system (e-rater) to predict the qual-
ity of any translated sentence. MTeRater-Plus is a
meta-metric that combines MTeRater?s fluency-only
features with standard MT evaluation metrics and
heuristics. Both metrics are machine-learned mod-
els trained to rank new translations based on existing
human judgments of translation.
Our experiments showed that MTeRater, by it-
self, achieves a sentence-level correlation as high as
BLEU, despite not using reference translations. In
addition, the meta-metric MTeRater-Plus achieves
higher correlations than MTeRater, BLEU, ME-
TEOR, TERp as well as a baseline meta-metric com-
bining BLEU, METEOR and TERp (BMT). How-
ever, further analysis showed that the MTeRater
component of MTeRater-Plus does not contribute
significantly to this improved correlation. How-
ever, when added to the BMT baseline meta-metric,
MTeRater does make a significant contribution.
Our results, despite being a mixed bag, clearly
show that a system trained to assess English-
language proficiency can be useful in providing an
indication of translation fluency even outside of the
specific WMT11 evaluation task. We hope that this
work will spur further cross-pollination between the
fields of MT evaluation and grammatical error de-
tection. For example, we would like to explore using
MTeRater for confidence estimation in cases where
reference translations are unavailable, such as task-
oriented MT.
Acknowledgments
The authors wish to thank Slava Andreyev at ETS
for his help in running e-rater. This research was
supported by an NSF Graduate Research Fellowship
for the first author.
113
References
Joshua Albrecht and Rebecca Hwa. 2007a. A Re-
examination of Machine Learning Approaches for
Sentence-Level MT Evaluation. In Proceedings of
ACL.
Joshua Albrecht and Rebecca Hwa. 2007b. Regression
for Sentence-Level MT Evaluation with Pseudo Refer-
ences. In Proceedings of ACL.
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3).
John Bitchener, Stuart Young, and Denise Cameron.
2005. The effect of different types of corrective feed-
back on esl student writing. Journal of Second Lan-
guage Writing.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
WMT ?10, pages 17?53, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Ben Carterette. 2009. On rank correlation and the
distance between rankings. In Proceedings of the
32nd international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?09, pages 436?443, New York, NY, USA. ACM.
Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
Proceedings of the Conference of the North American
Chapter of the Association of Computational Linguis-
tics (NAACL), pages 140?147.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A Machine Learning Approach to
the Automatic Evaluation of Machine Translation. In
Proceedings of the 39th Annual Meeting on Associa-
tion for Computational Linguistics, pages 148?155.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, StatMT
?08, pages 191?194, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2009. Evaluating Effects of Ma-
chine Translation Accuracy on Cross-lingual Patent
Retrieval. In Proceedings of SIGIR, pages 674?675.
Yoko Futagi, Paul Deane, Martin Chodorow, and Joel
Tetreault. 2008. A computational approach to de-
tecting collocation errors in the writing of non-native
speakers of English. Computer Assisted Language
Learning, 21:353?367.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT Evaluation Without Refer-
ence Translations: Beyond Language Modeling. In
Proceedings of the European Association for Machine
Translation (EAMT).
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Thorsten Joachims. 2006. Training linear SVMs in linear
time. In ACM SIGKDD International Conference On
Knowledge Discovery and Data Mining (KDD), pages
217?226.
Douglas Jones, Martha Herzog, Hussny Ibrahim, Arvind
Jairam, Wade Shen, Edward Gibson, and Michael
Emonts. 2007. ILR-Based MT Comprehension Test
with Multi-Level Questions. In HLT-NAACL (Short
Papers), pages 77?80.
Alex Kulesza and Stuart M. Shieber. 2004. A Learn-
ing Approach to Improving Sentence-level MT Evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI).
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23:105?115, September.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan &
Claypool Publishers.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Kristen Parton, Kathleen R. McKeown, Bob Coyne,
Mona T. Diab, Ralph Grishman, Dilek Hakkani-Tu?r,
Mary Harper, Heng Ji, Wei Yun Ma, Adam Meyers,
Sara Stolbach, Ang Sun, Gokhan Tur, Wei Xu, and
Sibel Yaman. 2009. Who, What, When, Where, Why?
Comparing Multiple Approaches to the Cross-Lingual
5W Task. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 423?431.
Kay Peterson and Mark Przybocki. 2010. Nist
2010 metrics for machine translation evalua-
tion (metricsmatr10) official release of results.
http://www.itl.nist.gov/iad/mig/
tests/metricsmatr/2010/results.
114
Mark Przybocki, Kay Peterson, Se?bastien Bronsart, and
Gregory Sanders. 2009. The nist 2008 metrics for ma-
chine translation challenge?overview, methodology,
metrics, and results. Machine Translation, 23:71?103,
September.
Christopher Quirk. 2004. Training a Sentence-level Ma-
chine Translation Confidence Measure. In Proceed-
ings of LREC.
Sidney Siegel and N. John Castellan. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw-
Hill, 2 edition.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
hter?: exploring different human judgments with a tun-
able mt metric. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, StatMT ?09,
pages 259?268, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING), pages 865?
872.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of machine transla-
tion output. In International Conference on Language
Resources and Evaluation, pages 697?702, Genoa,
Italy, May.
115
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 44?53,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Exploring Grammatical Error Correction with
Not-So-Crummy Machine Translation?
Nitin Madnani Joel Tetreault
Educational Testing Service
Princeton, NJ, USA
{nmadnani,jtetreault}@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
To date, most work in grammatical error cor-
rection has focused on targeting specific er-
ror types. We present a probe study into
whether we can use round-trip translations ob-
tained from Google Translate via 8 different
pivot languages for whole-sentence grammat-
ical error correction. We develop a novel
alignment algorithm for combining multiple
round-trip translations into a lattice using the
TERp machine translation metric. We further
implement six different methods for extract-
ing whole-sentence corrections from the lat-
tice. Our preliminary experiments yield fairly
satisfactory results but leave significant room
for improvement. Most importantly, though,
they make it clear the methods we propose
have strong potential and require further study.
1 Introduction
Given the large and growing number of non-native
English speakers around the world, detecting and
correcting grammatical errors in learner text cur-
rently ranks as one of the most popular educational
NLP applications. Previously published work has
explored the effectiveness of using round-trip ma-
chine translation (translating an English sentence
to some foreign language F, called the pivot, and
then translating the F language sentence back to En-
glish) for correcting preposition errors (Hermet and
De?silets, 2009). In this paper, we present a pilot
study that explores the effectiveness of extending
?cf. Good Applications for Crummy Machine Translation.
Ken Church & Ed Hovy. Machine Translation, 8(4). 1993
this approach to whole-sentence grammatical error
correction.
Specifically, we explore whether using the con-
cept of round-trip machine translation via multi-
ple?rather than single?pivot languages has the po-
tential of correcting most, if not all, grammatical
errors present in a sentence. To do so, we de-
velop a round-trip translation framework using the
Google Translate API. Furthermore, we propose a
novel combination algorithm that can combine the
evidence present in multiple round-trip translations
and increase the likelihood of producing a whole-
sentence correction. Details of our methodology are
presented in ?3 and of the dataset we use in ?4. Since
this work is of an exploratory nature, we conduct a
detailed error analysis and present the results in ?5.
Finally, ?6 summarizes the contributions of this pi-
lot study and provides a discussion of possible future
work.
2 Related Work
To date, most work in grammatical error detection
has focused on targeting specific error types (usu-
ally prepositions or article errors) by using rule-
based methods or statistical machine-learning clas-
sification algorithms, or a combination of the two.
Leacock et al (2010) present a survey of the com-
mon approaches. However, targeted errors such as
preposition and determiner errors are just two of the
many types of grammatical errors present in non-
native writing. One of the anonymous reviewers for
this paper makes the point eloquently: ?Given the
frequent complexity of learner errors, less holistic,
error-type specific approaches are often unable to
44
disentangle compounded errors of style and gram-
mar.? Below we discuss related work that uses ma-
chine translation to address targeted errors and some
recent work that also focused on whole-sentence er-
ror correction.
Brockett et al (2006) use information about mass
noun errors from a Chinese learner corpus to engi-
neer a ?parallel? corpus with sentences containing
mass noun errors on one side and their corrected
counterparts on the other. With this parallel corpus,
the authors use standard statistical machine transla-
tion (SMT) framework to learn a translation (correc-
tion) model which can then be applied to unseen sen-
tences containing mass noun errors. This approach
was able to correct almost 62% of the errors found
in a test set of 150 errors. In our approach, we do not
treat correction directly as a translation problem but
instead rely on an MT system to round-trip translate
an English sentence back to English.
Park and Levy (2011) use a noisy channel model
to achieve whole-sentence grammar correction; they
learn a noise model from a dataset of errorful sen-
tences but do not rely on SMT. They show that the
corrections produced by their model generally have
higher n-gram overlap with human-authored refer-
ence corrections than the original errorful sentences.
The previous work that is most directly rele-
vant to our approach is that of Hermet and De?silets
(2009) who focused only on sentences containing
pre-marked preposition errors and generated a sin-
gle round-trip translation for such sentences via a
single pivot language (French). They then simply
posited this round-trip translation as the ?correc-
tion? for the original sentence. In their evaluation
on sentences containing 133 unique preposition er-
rors, their round-trip translation system was able to
correct 66.4% of the cases. However, this was out-
performed by a simple method based on web counts
(68.7%). They also found that combining the round-
trip method with the web counts method into a hy-
brid system yielded higher performance (82.1%).
In contrast, we use multiple pivot languages to
generate several round-trip translations. In addition,
we use a novel alignment algorithm that allows us to
combine different parts of different round-trip trans-
lations and explore a whole new set of corrections
that go beyond the translations themselves. Finally,
we do not restrict our analysis to any single type of
error. In fact, our test sentences contain several dif-
ferent types of grammatical errors.
Outside of the literature on grammatical error de-
tection, our combination approach is directly related
to the research on machine translation system com-
bination wherein translation hypotheses produced
by different SMT systems are combined to allow the
extraction of a better, combined hypothesis (Ban-
galore et al, 2001; Rosti et al, 2007; Feng et al,
2009). However, our combination approach is dif-
ferent in that all the round-trip translations are pro-
duced by a single system but via different pivot lan-
guages.
Finally, the idea of combining multiple surface
renderings with the same meaning has also been ex-
plored in paraphrase generation. Pang et al (2003)
propose an algorithm to align sets of parallel sen-
tences driven entirely by the syntactic representa-
tions of the sentences. The alignment algorithm out-
puts a merged lattice from which lexical, phrasal,
and sentential paraphrases could simply be read off.
Barzilay and Lee (2003) cluster topically related
sentences into slotted word lattices by using mul-
tiple sequence alignment for the purpose of down-
stream paraphrase generation from comparable cor-
pora. More recently, Zhao et al (2010) perform
round-trip translation of English sentences via dif-
ferent pivot languages and different off-the-shelf
SMT systems to generate candidate paraphrases.
However, they do not combine the candidate para-
phrases in any way. A detailed survey of paraphrase
generation techniques can be found in (Androut-
sopoulos and Malakasiotis, 2010) and (Madnani and
Dorr, 2010).
3 Methodology
The basic idea underlying our error correction tech-
nique is quite simple: if we can automatically gen-
erate alternative surface renderings of the meaning
expressed in the original sentence and then pick the
one that is most fluent, we are likely to have picked
a version of the sentence in which the original gram-
matical errors have been fixed.
In this paper, we propose generating such alter-
native formulations using statistical machine trans-
lation. For example, we take the original sentence E
and translate it to Chinese using the Google Trans-
45
Original Both experience and books are very important about living.
Swedish Both experience and books are very important in live.
Italian Both books are very important experience and life.
Russian And the experience, and a very important book about life.
French Both experience and the books are very important in life.
German Both experience and books are very important about life.
Chinese Related to the life experiences and the books are very important.
Spanish Both experience and the books are very important about life.
Arabic Both experience and books are very important for life.
Figure 1: Illustrating the deficiency in using an n-gram language model to select one of the 8 round-trip translations
as the correction for the Original sentence. The grammatical errors in the Original sentence are shown in italics. The
round-trip translation via Russian is chosen by a 5-gram language model trained on the English gigaword corpus even
though it changes the meaning of the original sentence entirely.
late API. We then take the resulting Chinese sen-
tence C and translate it back to English. Since
the translation process is designed to be meaning-
preserving, the resulting round-trip translation E?
can be seen as an alternative formulation of the orig-
inal sentence E. Furthermore, if additional pivot lan-
guages besides Chinese are used, several alterna-
tive formulations of E can be generated. We use 8
different pivot languages: Arabic, Chinese, Span-
ish, French, Italian, German, Swedish, Russian. We
chose these eight languages since they are frequently
used in SMT research and shared translation tasks.
To obtain the eight round-trip translations via each
of these pivot languages, we use the Google Trans-
late research API.1
3.1 Round-Trip Translation Combination
Once the translations are generated, an obvious so-
lution is to pick the most fluent alternative, e.g.,
using an n-gram language model. However, since
the language model has no incentive to preserve the
meaning of the sentence, it is possible that it might
pick a translation that changes the meaning of the
original sentence entirely. For example, consider
the sentence and its round-trip translations shown
in Figure 1. For this sentence, a 5-gram language
model trained on gigaword picks the Russian round-
trip translation simply because it has n-grams that
were seen more frequently in the English gigaword
corpus.
Given the deficiencies in statistical phrase-based
translation, it is also possible that no single round-
1http://research.google.com/university/
translate/
trip translation fixes all of the errors. Again, con-
sider Figure 1. None of the 8 round-trip transla-
tions is error-free itself. Therefore, the task is more
complex than simply selecting the right round-trip
translation. We posit that a better approach will be
to combine the evidence of correction produced by
each independent translation model and increase the
likelihood of producing a final whole-sentence cor-
rection. Additionally, by engineering such a combi-
nation, we increase the likelihood that the final cor-
rection will preserve the meaning of the original sen-
tence.
In order to combine the round-trip translations,
we developed a heuristic alignment algorithm that
uses the TERp machine translation metric (Snover
et al, 2009). The TERp metric takes a pair of sen-
tences and computes the least number of edit opera-
tions that can be employed to turn one sentence into
the other.2 As a by-product of computing the edit
sequence, TERp produces an alignment between the
two sentences where each alignment link is defined
by an edit operation. Figure 2 shows an example of
the alignment produced by TERp between the orig-
inal sentence from Figure 1 and its Russian round-
trip translation. Note that TERp also allows shifting
words and phrases in the second sentence in order
to obtain a smaller edit cost (as indicated by the as-
terisk next to the word book which has shifted from
its original position in the Russian round-trip trans-
lation).
Our algorithm starts by treating the original sen-
tence as the backbone of a lattice. First, it cre-
2Edit operations in TERp include matches, substitutions, in-
sertion, deletions, paraphrase, synonymy and stemming.
46
ates a node for each word in the original sentence
and creates edges between them with a weight of
1. Then, for each of the round-trip translations, it
computes its TERp alignment with the original sen-
tence and aligns it to the backbone based on the edit
operations in the alignment. Specifically, each in-
sertion, substitution, stemming, synonymy and para-
phrase operation lead to creation of new nodes that
essentially provide an alternative formulation for the
aligned substring from the backbone. Any duplicate
nodes are merged. Finally, edges produced by dif-
ferent translations between the same pairs of nodes
are merged and their weights added. Figure 3(a)
shows how our algorithm aligns the Russian round-
trip translation from Figure 1 to the original sentence
using the TERp alignment from Figure 2. Figure
3(b) shows the final lattice produced by our algo-
rithm for the sentence and all the round-trip transla-
tions from Figure 1.
-- and [I]
both -- the [S]
experience -- experience [M]
-- , [I]
and -- and [M]
books -- book [T] [*]
are -- a [S]
very -- very [M]
important -- important [M]
about -- about [M]
living -- life [Y]
. -- . [M]
Figure 2: The alignment produced by TERp between the
original sentence from Figure 1 and its Russian round-
trip translation. The alignment operations are indicated
in square brackets after each alignment link: I=insertion,
M=match, S=substitution, T=stemming and Y=WordNet
synonymy. The asterisk next to the work book denotes
that TERp chose to shift its position before computing an
edit operation for it.
3.2 Correction Generation
For each original sentence, we computed six possi-
ble corrections from the round-trip translations and
the combined lattice:
1. Baseline LM (B). The most fluent round-trip
translation out of the eight as measured by a
5-gram language model trained on the English
gigaword corpus.
2. Greedy (G). A path is extracted from the TERp
lattice using a greedy best-first strategy at each
node, i.e., at each node, the outgoing edge with
the largest weight is followed.
3. 1-Best (1): The shortest path is extracted
from the TERp lattice by using the OpenFST
toolkit.3. This method assumes that, like G, the
combined evidence from the round-trip trans-
lations itself is enough to produce a good final
correction and no external method for measur-
ing fluency is required.4
4. LM Re-ranked (L). An n-best (n=20) list is
extracted from the lattice using the OpenFST
toolkit and re-ranked using the 5-gram giga-
word language model. The 1-best reranked
item is then extracted as the correction. This
method assumes that an external method
of measuring fluency?the 5-gram language
model?can help to bring the most grammati-
cal correction to the top of the n-best list.
5. Product Re-ranked (P). Same as L except the
re-ranking is done based on the product of the
cost of each hypothesis in the n-best list and
the language model score, i.e., both the evi-
dence from the round-trip translations and the
language model is weighted equally.
6. Full LM Composition (C). The edge weights
in the TERp lattice are converted to probabil-
ities. The lattice is then composed with a tri-
gram finite state language model (trained on
a corpus of 100, 000 high-scoring student es-
says).5 The shortest path through the composed
lattice is then extracted as the correction. This
method assumes that using an n-gram language
model during the actual search process is better
than using it as a post-processing tool on an al-
ready extracted n-best list, such as for L and
P.
3http://www.openfst.org/
4Note that the edge weights in the lattice must be converted
into costs for this method (we do so by multiplying the weights
by ?1).
5We adapted the code available at http://www.
ling.ohio-state.edu/?bromberg/ngramcount/
ngramcount.html to perform the LM composition.
47
bo
th
exp
eri
enc
e
1
and
1 ,
1
bo
ok
s
1
bo
ok
1
are
1
ver
y
1
im
po
rta
nt
2
abo
ut
2
liv
ing
1
life
1
.
1
and
the
1
1
1
a
1
1
1
(a
)
bo
th
exp
eri
enc
e
1
and
1,
k
bo
osv
y
bo
os
k
the
m
are
2
uer
l
2
igp
ort
ant
f
abo
.t

in

ie
 or
k
iui
n
k m

k
iue
kk
k1
and
the
k
k
k
a
k
k
m
rea
ted
to
k
the
k
exp
eri
enc
ev
k
k
k
(b
)
O
ri
gi
na
l(
O
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
vi
ng
.
B
as
el
in
e
L
M
(B
)
A
nd
th
e
ex
pe
ri
en
ce
,a
nd
a
ve
ry
im
po
rt
an
tb
oo
k
ab
ou
tl
if
e.
G
re
ed
y
(G
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
fe
.
1-
be
st
(1
)
B
ot
h
ex
pe
ri
en
ce
an
d
th
e
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
fe
.
L
M
R
e-
ra
nk
ed
(L
)
A
nd
th
e
ex
pe
ri
en
ce
an
d
th
e
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ti
n
li
fe
.
P
ro
du
ct
R
e-
ra
nk
ed
(P
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
fe
.
L
M
C
om
po
si
ti
on
(C
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
t
in
lif
e.
(c
)
F
ig
ur
e
3:
(a
)
sh
ow
s
th
e
ou
tp
ut
of
ou
r
al
ig
nm
en
t
al
go
ri
th
m
fo
r
th
e
R
us
si
an
ro
un
d-
tr
ip
tr
an
sl
at
io
n
fr
om
F
ig
ur
e
1.
(b
)
sh
ow
s
th
e
fi
na
l
T
E
R
p
la
tt
ic
e
af
te
r
al
ig
ni
ng
al
l
ei
gh
tr
ou
nd
-t
ri
p
tr
an
sl
at
io
ns
fr
om
F
ig
ur
e
1.
(c
)
sh
ow
s
th
e
co
rr
ec
ti
on
s
fo
r
th
e
or
ig
in
al
se
nt
en
ce
(O
)
pr
od
uc
ed
by
th
e
si
x
te
ch
ni
qu
es
di
sc
us
se
d
in
3.
2.
T
he
co
rr
ec
ti
on
pr
od
uc
ed
by
th
e
F
ul
lL
M
C
om
po
si
ti
on
te
ch
ni
qu
e
(C
)
fi
xe
s
bo
th
th
e
er
ro
rs
in
th
e
or
ig
in
al
se
nt
en
ce
.
48
No. of Errors Sentences Avg. Length
1 61 14.4
2 45 19.9
3 29 24.2
4 14 29.4
> 4 13 38.0
Table 1: The distribution of grammatical errors for the
162 errorful sentences.
Figure 3(c) shows these six corrections as computed
for the sentence from Figure 1.
4 Corpus
To assess our system, we manually selected 200
sentences from a corpus of essays written by non-
native English speakers for a college-level English
proficiency exam. In addition to sentences contain-
ing grammatical errors, we also deliberately sam-
pled sentences that contained no grammatical errors
in order to determine how our techniques perform
in those cases. In total, 162 of the sentences con-
tained at least one error, and the remaining 38 were
perfectly grammatical. For both errorful as well
as grammatical sentences, we sampled sentences of
different lengths (under 10 words, 10-20 words, 20-
30 words, 30-40 words, and over 40 words). The
162 errorful sentences varied in the number and type
of errors present. Table 1 shows the distribution of
the number of errors across these 162 sentences.
Specifically, the error types found in these sen-
tences included prepositions, articles, punctuation,
agreement, collocations, confused words, etc. Some
only contained a handful of straightforward errors,
such as ?In recent day, transportation is one of the
most important thing to support human activity?,
where day and thing should be pluralized. On the
other hand, others were quite garbled to the point
where it was difficult to understand the meaning,
such as ?Sometimes reading a book is give me in-
formation about the knowledge of life so that I can
prevent future happened but who knows that when it
will happen and how fastly can react to that hap-
pen.? During development, we noticed that the
round-trip translation process could not handle mis-
spelled words, so we manually corrected all spelling
mistakes which did not result in a real word.6
6A total of 82 spelling errors were manually corrected.
5 Evaluation
In order to evaluate the six techniques for generating
corrections, we designed an evaluation task where
the annotators would be shown a correction along
with the original sentence for which it was gener-
ated. Since there are 6 corrections for each of the
200 sentences, this yields a total of 1, 200 units for
pairwise preference judgments. We chose two anno-
tators, both native English speakers, each of whom
annotated half of the judgment units.
Given the idiosyncrasies of the statistical machine
translation process underlying our correction tech-
niques, it is quite possible that:
? A correction may fix some, but not all, of the
grammatical errors present in the original sen-
tence, and
? A correction may be more fluent but might
change the meaning of the original sentence.
? A correction may introduce a new disfluency,
even though other errors in the sentence have
been largely corrected. This is especially likely
to be the case for longer sentences.
Therefore, the pairwise preference judgment task
is non-trivial in that it expects the annotators to con-
sider two dimensions: that of grammaticality and of
meaning. To accommodate these considerations, we
designed the evaluation task such that it asked the
annotators to answer the following two questions:
1. Grammaticality. The annotators were asked
to choose between three options: ?Original
sentence sounds better?, ?Correction sounds
better? and ?Both sound about the same?.
2. Meaning. The annotators were asked to choose
between two options: ?Correction preserves
the original meaning? and ?Correction changes
the original meaning?. It should be noted that
determining change in or preservation of mean-
ing was treated as a very strict judgment. Subtle
changes such as the omission of a determiner
were deemed to change the meaning. In some
cases, the original sentences were too garbled
to determine the original meaning itself.
49
C > O C = O C < O
Meaning = 1 S D F
Meaning = 0 F F F
Table 2: A matrix illustrating the Success-Failure-Draw
evaluation criterion for the 162 errorful sentences. The
rows represent the meaning dimension (1 = meaning pre-
served, 0 = meaning changed) and the columns represent
the grammaticality dimension (C > O denotes correc-
tion being more grammatical than the original, C = O
denotes they are about the same and C < O denotes that
the correction is worse). Such a matrix is computed for
each of the six techniques.
5.1 Effectiveness
First, we concentrate our analysis on the original
sentences which contain at least one grammatical er-
ror. We aggregated the results of the pairwise pref-
erence judgments for each of the six specific correc-
tion generation techniques and applied the strictest
evaluation criterion by computing the following, for
each technique:
? Successes. Only those sentences for which
the correction generated by method is not only
more grammatical but also preserves the mean-
ing.
? Failures. All those sentences for which the cor-
rection is either less grammatical or changes
the original meaning.
? Draws. Those sentences for which the correc-
tion preserves the meaning but sounds about
the same as the original.
Table 2 shows a matrix of the six possible com-
binations of grammaticality and meaning for each
method and demonstrates which cells of the matrix
contribute to which of the above three measures:
Successes (S), Failures (F) and Draws (D).
In addition to the six techniques, we also posit an
oracle in order to determine the upper bound on the
performance of our round-trip translation approach.
The oracle picks the most accurate correction gen-
eration method for each individual sentence out of
the 6 that are available. For sentences where none of
the six techniques produce an adequate correction,
the oracle just picks the original sentence. Table 3
shows how the various techniques (including the or-
acle) perform on the 162 errorful sentences as mea-
sured by this criterion. Based on this criterion, the
greedy technique performs the best compared to the
others since it has a higher success rate (36%) and
a lower failure rate (31%). The oracle shows that
60% of the errorful sentences are fixed by at least
one of the six correction generation techniques. We
show examples of success and failure for the greedy
technique in Figure 4.
5.2 Effect of sentence length
From our observations on development data (not
part of the test set), we noticed that Google Trans-
late, like most statistical machine translation sys-
tems, performs significantly better on shorter sen-
tences. Therefore, we wanted to measure whether
the successes for the best method were biased to-
wards shorter sentences and the failures towards
longer ones. To do so, we measured the mean and
standard deviation of lengths of sentences compris-
ing the successes and failures of the greedy tech-
nique. Successful sentences had an average length
of approximately 18 words with a standard devia-
tion of 9.5. Failed sentences had an average length
of 23 words with a standard deviation of 12.31.
These numbers indicate that although the failures
are somewhat correlated with larger sentence length,
there is no evidence of a significant length bias.
5.3 Effect on grammatical sentences
Finally, we also carried out the same Success-
Failure-Draw analysis for the 38 sentences in our
test set that were perfectly grammatical to begin
with. The analysis differs from that of errorful sen-
tences in one key aspect: since the sentences are al-
ready free of any grammatical errors, no correction
can be grammatically better. Therefore, sentences
for which the correction preserves the meaning and
is not grammaticality worse will count as successes
and all other cases will count as failures. There are
no draws. Table 4 illustrates this difference and Ta-
ble 5 presents the success and failure rates for all six
methods. The greedy technique again performs the
best out of all six methods and successfully retains
the meaning and grammaticality for almost 80% of
50
Method Success Draw Failure
Baseline LM (B) 21% (34) 9% (15) 70% (113)
Greedy (G) 36% (59) 33% (52) 31% (51)
1-best (1) 32% (52) 30% (48) 38% (62)
LM Re-ranked (L) 30% (48) 17% (27) 54% (87)
Product Re-ranked (P) 23% (37) 38% (61) 40% (64)
LM Composition (C) 19% (31) 12% (20) 69% (111)
Oracle 60% (97) 40% (65) -
Table 3: The success, draw and failure rates for the six correction generation techniques and the oracle as computed for
the 162 errorful sentences from the test set. The oracle picks the method that produces the most meaning-preserving
and grammatical correction for each sentence. For sentences that have no adequate correction, it picks the original
sentence. Numbers in parentheses represent counts.
Success
That?s why I like to make travel by using my own car.
That?s why I like to travel using my own car.
Having discuss all this I must say that I must rather prefer to be a leader than just a member.
After discussing all this, I must say that I would prefer to be a leader than a member.
Failure
And simply there is fantastic for everyone
All magical and simply there is fantastic for all
I hope that share a room with she can be certainly kindle, because she is likely me
and so will not be problems with she.
I hope that sharing a room with her can be certainly kindle, because it is likely that
I and so there will be no problems with it.
Figure 4: Two examples of success and failure for the Greedy (G) technique. Original sentences are shown first
followed by the corrections in bold. Grammatical errors in the original sentences are in italics.
the grammatical sentences.7
C > O C = O C < O
Meaning = 1 - S F
Meaning = 0 - F F
Table 4: A matrix illustrating the Success-Draw-Failure
evaluation criterion for the 38 grammatical sentences.
There are no draws and sentences for which corrections
preserve meaning and are not grammatically worse count
as successes. The rest are failures.
6 Discussion & Future Work
In this paper, we explored the potential of a novel
technique based on round-trip machine translation
for the more ambitious and realistic task of whole-
sentence grammatical error correction. Although the
idea of round-trip machine translation (via a single
pivot language) has been explored before in the con-
text of just preposition errors, we expanded on it sig-
nificantly by combining multiple round-trip transla-
7An oracle for this setup is uninteresting since it will simply
return the original sentence for every sentence.
Method Success Failure
Baseline LM (B) 26% (10) 74% (28)
Greedy (G) 79% (30) 21% (8)
1-best (1) 61% (23) 39% (15)
LM Re-ranked (L) 34% (13) 66% (25)
Product Re-ranked (P) 42% (16) 58% (22)
LM Composition (C) 29% (11) 71% (25)
Table 5: The success and failure rates for the six correc-
tion generation techniques as computed for the 38 gram-
matical sentences from the test set.
tions and developed several new methods for pro-
ducing whole-sentence error corrections. Our oracle
experiments show that the ideas we explore have the
potential to produce whole-sentence corrections for
a variety of sentences though there is clearly room
for improvement.
An important point needs to be made regard-
ing the motivation for the round-trip translation ap-
proach. We claim that this approach is useful not
just because it can produce alternative renderings of
a given sentence but primarily because each of those
51
renderings is likely to retain at least some of mean-
ing of the original sentence.
Most of the problems with our techniques arise
due to the introduction of new errors by Google
Translate. One could use an error detection sys-
tem (or a human) to explicitly identify spans con-
taining grammatical errors and constrain the SMT
system to translate only these errorful spans while
still retaining the rest of the words in the sentence.
This approach should minimize the introduction of
new errors. Note that Google Translate does not
currently provide a way to perform such selective
translation. However, other open-source SMT sys-
tems such as Moses8 and Joshua9 do. Furthermore,
it might also be useful to exploit n-best translation
outputs instead of just relying on the 1-best as we
currently do.
As an alternative to selective translation, one
could simply extract the identified errorful spans and
round-trip translate each of them individually. For
example, consider the sentence: ?Most of all, luck
is null prep no use without a hard work.? where the
preposition of is omitted and there is an extraneous
article a before ?hard work?. With this approach,
one would simply provide Google Translate with the
two phrasal spans containing the errors, instead of
the entire sentence.
More generally, although we use Google Trans-
late for this pilot study due to its easy availability, it
might be more practical and useful to rely on an in-
house SMT system that trades-off translation quality
for additional features.
We also found that the language-model based
techniques performed quite poorly compared to the
other techniques. We suspect that this is due to the
fact that Google Translate already employs large-
order language models trained on trillions of words.
Using lower-order models trained on much smaller
corpora might simply introduce noise. However, a
detailed analysis is certainly warranted.
In conclusion, we claim that our preliminary ex-
ploration of large-scale round-trip translation based
techniques yielded fairly reasonable results. How-
ever, more importantly, it makes it clear that, with
additional research, these techniques have the poten-
8http://www.statmt.org/moses
9https://github.com/joshua-decoder
tial to be very effective at whole-sentence grammat-
ical error correction.
Acknowledgments
We would like to thank Aoife Cahill, Michael Heil-
man and the three anonymous reviewers for their
useful comments and suggestions. We would also
like to thank Melissa Lopez and Matthew Mulhol-
land for helping with the annotation.
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual Entail-
ment Methods. J. Artif. Int. Res., 38(1):135?187.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing Consensus Translation from
Multiple Machine Translation Systems. In Proceed-
ings of ASRU, pages 351?354.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL
2003, pages 16?23.
Chris Brockett, William B. Dolan, and Michael Gamon.
2006. Correcting ESL Errors Using Phrasal SMT
Techniques. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 249?256.
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Ya-
juan Lu?. 2009. Lattice-based System Combination
for Statistical Machine Translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3 - Volume 3, pages
1105?1113.
Matthieu Hermet and Alain De?silets. 2009. Using First
and Second Language Models to Correct Preposition
Errors in Second Language Authoring. In Proceedings
of the Fourth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 64?72.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
driven Methods. Computational Linguistics, 36(3).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102?109.
52
Y. Albert Park and Roger Levy. 2011. Automated Whole
Sentence Grammar Correction using a Noisy Channel
Model. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, pages 934?
944.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining Outputs from Multiple Machine
Translation Systems. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 228?
235.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with
a Tunable MT Metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation at the
12th Meeting of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2009).
Shiqi Zhao, Haifeng Wang, Xiang Lan, and Ting Liu.
2010. Leveraging Multiple MT Engines for Para-
phrase Generation. In COLING, pages 1326?1334.
53
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 233?241,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Precision Isn?t Everything:
A Hybrid Approach to Grammatical Error Detection
Michael Heilman and Aoife Cahill and Joel Tetreault
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
{mheilman,acahill,jtetreault}@ets.org
Abstract
Some grammatical error detection methods,
including the ones currently used by the Edu-
cational Testing Service?s e-rater system (At-
tali and Burstein, 2006), are tuned for pre-
cision because of the perceived high cost
of false positives (i.e., marking fluent En-
glish as ungrammatical). Precision, however,
is not optimal for all tasks, particularly the
HOO 2012 Shared Task on grammatical er-
rors, which uses F-score for evaluation. In this
paper, we extend e-rater?s preposition and de-
terminer error detection modules with a large-
scale n-gram method (Bergsma et al, 2009)
that complements the existing rule-based and
classifier-based methods. On the HOO 2012
Shared Task, the hybrid method performed
better than its component methods in terms of
F-score, and it was competitive with submis-
sions from other HOO 2012 participants.
1 Introduction
The detection of grammatical errors is a challenging
problem that, arguably, requires the use of both lin-
guistic knowledge (e.g., in the form of rules or com-
plex features) and large corpora for statistical learn-
ing. Additionally, grammatical error detection can
be applied in various scenarios (e.g., automated es-
say scoring, writing assistance, language learning),
many of which may benefit from task-specific adap-
tation or tuning. For example, one might want to
take a different approach when detecting errors for
the purpose of providing feedback than when de-
tecting errors to evaluate the quality of writing in
an essay. Thus, it seems desirable to take a flexible
approach to grammatical error detection that incor-
porates multiple, complementary techniques.
In this paper, we extend the preposition and de-
terminer error detection modules currently used in
the Educational Testing Service?s e-rater automated
essay scoring system (Attali and Burstein, 2006) for
the HOO 2012 Shared Task on grammatical errors
(?2). We refer to this set of modules from e-rater as
our ?base system? (?3). While the base system uses
statistical methods to learn models of grammatical
English, it also leverages substantial amounts of lin-
guistic knowledge in the form of various hand-coded
filters and complex syntactic features. The base sys-
tem is also tuned for high precision at the expense
of recall in order to avoid a high rate of potentially
costly false positives (i.e., frequent marking of cor-
rect English sentences as ungrammatical).
We apply the pre-existing base system without
modifications but complement it with a large-scale
n-gram method (?5) based on work by Bergsma et
al. (2009). The n-gram method employs very little
linguistic knowledge and instead relies almost ex-
clusively upon corpus statistics. We also tune the
resulting hybrid system with labeled training data
in order to maximize the primary evaluation met-
ric used in the HOO 2012 Shared Task: balanced
F-score, or F1 (?6). We find that the tuned hybrid
system improves upon the recall and F-score of the
base system. Also, in the HOO 2012 Shared Task,
the hybrid system achieved results that were com-
petitive with other submitted grammatical error de-
tection systems (?7).
233
2 Task Definition
In this section, we provide a brief overview of the
HOO 2012 Shared Task (Dale et al, 2012). The
task focuses on prepositions and determiners only,
distinguishing the following error types: preposition
selection errors (coded ?RT? in the data), extraneous
prepositions (?UT?), missing prepositions (?MT?),
determiner selection errors (?RD?), extraneous de-
terminers (?UD?), and missing determiners (?MD?).
For training and testing data, the shared task uses
short essays from an examination for speakers of En-
glish as a foreign language. The data includes gold
standard human annotations identifying preposition
and determiner errors. These errors are represented
as edits that transform an ungrammatical text into
a grammatical one. Edits consist of start and end
offsets into the original text and a correction string
that should replace the original text at the speci-
fied offsets. The offsets differ by error type: word
selection errors include just the word, extraneous
word errors include an extra space after the word so
that a blank will result in an appropriate amount of
whitespace, and missing word errors specify spans
of length zero.1
There are three subtasks: detection, recognition,
and correction. Each is evaluated according to pre-
cision, recall, and F-score according to a set of
gold standard edits produced by human annotation.
While the correction subtask requires both correct
character offsets and appropriate corrections, the de-
tection and recognition subtasks only consider the
offsets. Detection and recognition are essentially the
same, except that detection allows for loose match-
ing of offsets, which permits mismatches between
the extraneous use (e.g., UT) and word selection
(e.g., RT) error types. For our submission to the
shared task, we chose to tune for the detection sub-
task, and we also chose to avoid the correction task
entirely since the interface to the pre-existing base
system did not give us access to possible corrections.
1The offsets for extraneous word errors prior to punctuation,
a relatively rare occurrence, include a space before the word
rather than after it. Our script for converting our system?s output
into the HOO 2012 format did not account for this, which may
have decreased recognition performance slightly.
3 Base System
As our base system, we repurpose a complex sys-
tem designed to automatically score student essays
(both native and non-native and across a wide range
of competency levels). The system is also used to
give feedback to essay writers, so precision is fa-
vored over recall. There are three main modules in
the essay-scoring system whose purpose it is to de-
tect preposition and determiner errors (as they are
defined in that system). Many of the details have
been reported previously (Chodorow and Leacock,
2000; Han et al, 2004; Han et al, 2006; Chodorow
et al, 2007; Tetreault and Chodorow, 2008), so here
we will only give brief summaries of these modules.
It is important to note that this system was run
without modification. That is, no training of new
models or tuning was carried out specifically for the
shared task. In addition, for the two statistical mod-
ules, we only had access to the final, boolean deci-
sions about whether an error is present or not at a
particular location in text. That is, we did not have
access to confidence scores, and so task-specific tun-
ing for F-score was not an option.
3.1 Preposition Error Detection
The base system detects incorrect and extraneous
prepositions (Chodorow et al, 2007; Tetreault and
Chodorow, 2008). Tetreault and Chodorow (2008)
reports approximately 84% precision and 19% re-
call on both error types combined when evaluating
the system on manually annotated non-native text.
3.1.1 Incorrect Prepositions
The module to detect incorrectly used preposi-
tions consists of a multiclass logistic regression (i.e.,
?Maximum Entropy?) model of grammatical usage,
along with heuristic pre- and post- filters. The mod-
ule works by extracting a set of features from the
?context? around a preposition, generating a distri-
bution over possible prepositions using the model of
grammatical usage, and then flagging an error if the
difference in probability between the text?s original
preposition and an alternative preposition exceeds a
certain threshold. The probability for any correction
also needs to exceed another minimum threshold.
For this work, we used the pre-existing, manually-
set thresholds.
234
A pre-filter prevents any contexts that contain
spelling errors from being submitted to the logistic
regression model. The motivation for this is that the
NLP components that provide the features for the
model are unreliable on such data, and since the sys-
tems favors precision over recall, no attempt is made
to correct prepositions where the system cannot rely
on the accuracy of those features.
The logistic regression model of correct preposi-
tion usage is trained on approximately 82 million
words from the San Jose Mercury News2 and texts
for 11th to 12th grade reading levels from the Meta-
Metrics Lexile corpus, resulting in 7 million prepo-
sition contexts. The model uses 25 types of features:
words and part-of-speech tags around the existing
preposition, head verb (or noun) in the preceding
VP (or NP), head noun in the following NP, among
others. NPs and VPs were detected using chunking
rather than full parsing, as the performance of statis-
tical parsers on erroneous text was deemed to be too
poor.
A post-filter rules out certain candidates based on
the following heuristics: (1) if the suggested correc-
tion is an antonym of the original preposition (e.g.,
from vs to), it is discarded; (2) any correction of the
benefactive for is discarded when the head noun of
the following NP is human (detected as a WordNet
hyponym of person or group).
3.1.2 Extraneous Prepositions
Heuristics are applied to detect common occur-
rences of extraneous prepositions in two scenar-
ios: (1) accidentally repeated prepositions (e.g., with
with) and (2) insertion of unnecessary prepositions
in plural quantifier constructions (e.g., some of peo-
ple).
3.2 Determiner Error Detection
There are two separate components that detect er-
rors related to determiners. The first is a filter-based
model that detects determiner errors involving num-
ber and person agreement. The second is a statistical
system that supplements the rule-based system and
detects article errors.
2The San Jose Mercury News is available from the Linguis-
tic Data Consortium (catalog number LDC93T3A).
3.2.1 Filter-based system
The filter-based system combines unsupervised
detection of a set of possible errors (Chodorow and
Leacock, 2000) with hand-crafted filters designed
to reduce this set to the largest subset of correctly
flagged errors and the smallest possible number
of false positives. Chodorow and Leacock (2000)
found that low-frequency bigrams (sequences of two
lexical categories with a negative log-likelihood) are
quite reliable predictors of grammatical errors. Text
is tagged and chunked, and filters that detect likely
cases of NP-internal agreement violations are ap-
plied. These filters will mark, for example, a sin-
gular determiner followed by a plural noun head and
vice versa, or a number disagreement between a nu-
meral and the noun it modifies. This system has
the ability to take advantage of linguistic knowledge,
which contributes to its ability to detect errors with
high precision.
3.2.2 Statistical model
In addition to the hand-crafted filters described
above, there is a statistical component that detects
incorrect, missing and extraneous articles (Han et
al., 2004; Han et al, 2006). This component con-
sists of a multiclass logistic regression that selects
an appropriate article for every NP from a, an, the,
or . This model is trained on 31.5 million words
of diverse genres from the MetaMetrics Lexile cor-
pus (from 10th to 12th grade reading levels), or 8
million NP contexts. Again, NPs were determined
by chunking. The model includes various features:
words and POS tags around and within the NP, NP
head information including the countability of the
head noun (estimated automatically from large cor-
pora), etc.
In a cross-validation experiment, the model
achieved approximately 83% accuracy on well-
edited text. In an experiment evaluated on non-
native learner text, the model achieved approxi-
mately 85% agreement with human annotators.
4 Task-Specific Heuristic Filtering
There is not a one-to-one mapping between the def-
initions of determiner and preposition errors as used
in the HOO data set and the definitions used in our
base system. For example, our base system marks
235
errors involving every, many and other quantifiers as
determiner errors, while these are not marked in the
current HOO 2012 Shared Task data.
To ensure that our system was aligned with the
HOO 2012 Shared Task, we automatically extracted
lists of the most frequently occurring determiners
and prepositions in the HOO training data. Any RT,
UT, RD or UD edit predicted for a word not in those
lists is automatically discarded. In the training data,
this resulted in the removal of 4 of the 463 RT errors
and 98 of the 361 RD errors detected by the base
system.
5 Large-scale n-Gram Models
In order to complement the high-precision base sys-
tem and increase recall, we incorporate a large
scale n-gram model into our full system. Specifi-
cally, we adapt the SUMLM method from Bergsma
et al (2009). SUMLM creates confusion sets for
each preposition token in an input text and uses the
Google Web 1T 5-gram Corpus to score each item
in the confusion set.3 We extend SUMLM to sup-
port determiners, extraneous use errors, and missing
word errors.
Consider the case of preposition selection errors.
For a preposition token at position i in an input sen-
tence w, we compute the following score for each
possible alternative v, using Eq. 1.4
s(w, i, v) =
?
n=2...5
?
x?G(w,i,n,v)
log(count(x))
|G(w, i, n, v)|
(1)
The function G(w, i, n, v) returns the set of n-
grams in w that include the word at position i and
3The Google Web 1T 5-gram Corpus is available from the
Linguistic Data Consortium (catalog number LDC2006T13).
We plan to test other corpora for n-gram counts in future work.
4The n-gram approach considers all of the following words
to be prepositions: to, of, in, for, on, with, at, by, as, from, about,
up, over, into, down, between, off, during, under, through,
around, among, until, without, along, within, outside, toward,
inside, upon, except, onto, towards, besides, beside, and under-
neath. It considers all of the following words to be determiners:
a, an, and the. The sets of possible prepositions and determiners
for the base system are not exactly the same. Part of speech tags
are not used in the n-gram system except to identify insertion
points for missing prepositions and determiners.
replace that word, wi, with v. For example, if w =
Mary and John went at the store to buy milk, n = 4,
i = 4, and v = to, then G(w, i, n, v) returns the
following 4-grams:
? and John went to
? John went to the
? went to the store
? to the store to
The expression log(count(x)) is the natural loga-
rithm of the number of times the n-gram x occurred
in the corpus.5 |G(w, i, n, v)| is the number of n-
gram count lookups, used to normalize the scores.
Note that this normalization factor is not included in
the original SumLM. When v is an alternative prepo-
sition not near the beginning or end of a sentence,
|G(w, i, n, v)| = 14 since there are 14 n-gram count
lookups in the numerator. Or, for example, if i = 0,
indicating that the preposition occurs at the begin-
ning of the sentence, |G(w, i, n, v)| = 4.6
Next, we compute the ratio of the score of each
alternative to the score for the original, using Eq. 2.
r(w, i, v) =
s(w, i, v)
s(w, i, wi)
(2)
We then identify the best scoring alternative, re-
quiring that its score be higher than the original (i.e.,
r(w, i, v) > 1). The procedure is the same for deter-
miners, except, of course, that the set of alternatives
includes determiners rather than prepositions.
To extend the method from Bergsma et al (2009)
for extraneous prepositions and determiners, we
simply set v to be a blank and sum over j = 3 . . . 5
instead. |G(w, i, n, v)|will then be 12 instead of 14,
since bigrams from the original sentence, which be-
come unigrams when replacing wi with a blank, are
excluded.
To identify positions at which to flag selection or
extraneous use errors, we simply scan for words that
match an item in our sets of possible prepositions
and determiners. To extend the method for missing
5We use the TrendStream system (Flor, 2012) to retrieve n-
gram counts efficiently.
6Our n-gram counts do not include start- or end-of-sentence
symbols. Also, all n-grams are case-normalized with numbers
replaced by a special symbol.
236
Algorithm 1 tune(W, y, y? ?, ?min):
The hill-climbing algorithm for optimizing the n-
gram method?s penalty parameters q. W consists
of the training set texts. y? is a set of candidate edits.
y is a set of gold standard edits. ? is an initial step
size, and ?min is a minimum step size.
qallbest ? 0
scoreallbest ? eval(qallbest,W,y, y?)
while ? > ?min do
scorebest ? ??
qbest ? qallbest
for qtmp ? perturb(qbest, ?) do
scoretmp ? eval(qtmp,W,y, y?)
if scoretmp > scorebest then
qbest ? qtmp
scorebest ? scoretmp
end if
end for
if scorebest > scoreallbest then
qallbest ? qbest
scoreallbest ? scorebest
else
?? 0.5 ? ?
end if
end while
return qallbest
word errors, however, we apply a set of heuristics to
identify potential insertion points.7
6 Tuning
The n-gram approach in ?5 generates a large num-
ber of possible edits of different types. In this sec-
tion, we describe how we filter edits using their
scores and how we combine them with edits from
the base system (?3).
As described above, for an alternative v to be con-
sidered as a candidate edit, the value of r(w, i, v) in
Eq. 2 must be greater than a threshold of 1, indicat-
ing that the alternative scores higher than the origi-
nal word. However, we observed low precision dur-
ing development when including all candidate ed-
its and decided to penalize the ratios. Bergsma et
al. (2009) discuss raising the threshold, which has
7The heuristics are based on those used in Gamon (2010)
(personal communication).
a similar effect. Preliminary experiments indicated
that different edits (e.g., extraneous preposition edits
and preposition selection edits) should have differ-
ent penalties, and we also want to avoid edits with
overlapping spans. Thus, for each location with one
or more candidate edits, we select the best according
to Equation 3 and filter out the rest.
v? = argmax
v
r(w, i, v)? penalty(wi, v) (3)
penalty(wi, v) is a function that takes the current
word wi and the alternative v and returns one of 6
values: qRT for preposition selection, qUT for extra-
neous prepositions, qMT for missing prepositions,
qRD for determiner selection, qUD for extraneous
determiners, and qMD for missing determiners.
If the value for r(w, i, v?)?penalty(wi, v?) does
not exceed 1, we exclude it from the output.
We tune the vector q of all the penalties to op-
timize our objective function (F-score, see ?7) on
the training set using the hill-climbing approach de-
scribed in Algorithm 1. The algorithm initializes
the parameter vector to all zeros, and then itera-
tively evaluates candidate parameter vectors that re-
sult from taking positive and negative steps of size
? in each direction (steps with negative penalties
are skipped). The best step is taken if it improves
the current score, according to the eval function,
which returns the training set F-score after filtering
based on the current parameters.8 This process pro-
ceeds until there is no improvement. Then, the step
size ? is halved, and the whole process is repeated.
The algorithm proceeds as such until the step size
becomes lower than a specified minimum ?min.
When merging edits from the base system and the
n-gram approach, the hybrid system always prefers
edits from the base system if any edit spans overlap,
equivalent to including them in Eq. 3 and assigning
them a penalty of ??.9 Note that the set of pre-
dicted edits y passed as input to the tune algorithm
8Our implementation of the tuning algorithm uses the HOO
2012 Shared Task?s evalfrag.py module to evaluate the F-
score for the error detection subtask.
9If the base system produces overlapping edits, we keep
them all. If there are overlapping edits from the n-gram sys-
tem that have the same highest value for the penalized score in
Equation 3 and do not overlap with any base system edits, we
keep them all.
237
texts
edits
edits
edits
parameters
parameters
gold edits
base system -gram systemn
filteringtuning 
heuristic filtering
training testing
Figure 1: The architecture of the hybrid system. Different
steps are discussed in different parts of the paper: ?base
system? in ?3, ?n-gram system? in ?5, ?heuristic filter-
ing? in ?4, and ?tuning? and ?filtering? in ?6.
includes edits from both the base and n-gram meth-
ods.
Figure 1 illustrates the processes of training and
of producing test output from the hybrid system.
7 Results
Table 1 presents results for the HOO 2012 detec-
tion subtask, including errors of all types. The re-
sults here, reproduced from Dale et al (2012), are
prior to applying participant-suggested revisions to
the set of gold standard edits.10 We include four
variations of our approach: the base system (?3, la-
beled ?base?); the n-gram system (?5, labeled ?n-
gram?) by itself, tuned without edits from the base
system; the hybrid system, tuned with edits from the
base system (?hybrid?); and a variation of the hy-
10After submitting our predictions for the shared task, we
noticed a few minor implementation mistakes in our code re-
lated to the conversion of edits from the base system (?3) and
the task-specific heuristic filtering (?4). We corrected them and
retrained our system. The detection F-scores for the original
and corrected implementations were as follows: 26.45% (orig-
inal) versus 26.23% (corrected) for the base system, 30.70%
(original) versus 30.45% (corrected) for the n-gram system,
35.65% (original) versus 35.24% (corrected) for the hybrid sys-
tem, and 31.82% (original) versus 31.45% (corrected) for the
hybridindep system. Except for this footnote, all results in this
paper are for the original system.
run P R F
base 0 52.63 17.66 26.45
n-gram ? 25.87 37.75 30.70
hybrid 1 33.59 37.97 35.65
hybridindep 2 24.88 44.15 31.82
UI 8 37.22 43.71 40.20
Table 1: Precision, recall, and F-score for the combined
preposition and determiner error detection subtask for
various methods, before participant-suggested revisions
to the gold standard were applied. All values are percent-
ages. Official run numbers are shown in the ?run? col-
umn. The ?n-gram? run was not part of our official sub-
mission. For comparison, ?UI? is the submission, from
another team, that achieved the highest detection F-score
in the HOO 2012 Shared Task.
brid system (?hybridindep?) with the penalties tuned
independently, rather than jointly, to maximize F-
score for detection of each error type. For compari-
son, we also include the best performing run for the
detection subtask in terms of F-score (labeled ?UI?).
We observe that the base and n-gram systems ap-
pear to complement each other well for this task: the
base system achieved 26.45% F-score, and the n-
gram system achieved 30.70%, while the hybrid sys-
tem, with penalties tuned jointly, achieved 35.65%.
Table 2 shows further evidence that the two systems
have complementary performance. We calculate the
overlap between each system?s edits and the gold
standard. We see that only a small number of edits
are predicted by both systems (38 in total, 18 cor-
rect and 20 incorrect), and that the base system pre-
dicts 62 correct edits that the n-gram method does
not predict, and similarly the n-gram method pre-
dicts 92 correct edits that the base system does not
predict. The table also verifies that the base system
exhibits high precision (only 68 false positives in to-
tal) while the n-gram system is tuned for higher re-
call (286 false positives).
Not surprisingly, when the n-gram method?s
penalties were tuned independently (?hybridindep?)
rather than jointly, the overall score was lower, at
31.82% F-score. However, tuning independently
might be desirable if one were concerned with
performance on specific error types or if macro-
averaged F-score were the objective.
The hybrid system performed quite competitively
238
(1) All models had a UD very strange long shoes made from black skin . . .
(2) I think it is a great idea to organise this sort of festival because most of UT people enjoy it.
Figure 2: Examples of errors detected by the base system and missed by the n-gram models.
(3) We have to buy for UT some thing.
(4) I am  MD good diffender.
Figure 3: Examples of errors detected by the n-gram system and missed by the base model.
? gold /? gold
? base /? base ? base /? base
? n-gram 18 92 20 266
/? n-gram 62 276 48 ?
Table 2: The numbers of edits that overlap in the hybrid
system?s output and the gold standard for the test set. The
hybrid system?s output is broken down by whether edits
came from the base system (?3) or the n-gram method
(?5). The empty cell corresponds to hypothetical edits
that were in neither the gold standard or the system?s out-
put (e.g., edits missed by annotators), which we cannot
count.
compared to the other HOO 2012 submissions,
achieving the 3rd best results out of 14 teams for
the detection and recognition subtasks. The per-
formance of the ?UI? system was somewhat higher,
however, at 40.20% F-score compared to the hybrid
system?s 35.65%. We speculate that our hybrid sys-
tem?s performance could be improved somewhat if
we also tuned the base system for the task.
8 Error Analysis
It is illustrative to look at some examples of edits
that the base system correctly detects but the n-gram
model does not, and vice versa. Figure 2 shows ex-
amples of errors detected by the base system, but
missed by the n-gram system. Example (1) illus-
trates that the n-gram model has no concept of syn-
tactic structure. The base system, on the other hand,
carries out simple processing including POS tagging
and chunking, and is therefore aware of at least some
longer-distance dependencies (e.g., a . . . shoes). Ex-
ample (2) shows the effectiveness of the heuris-
tics based on quantifier constructions mentioned in
?3.1.2. These heuristics were developed by devel-
opers familiar with the kinds of errors that language
learners frequently make, and are therefore more tar-
geted than the general n-gram method.
Figure 3 shows examples of errors detected by the
n-gram system but missed by the base system. Ex-
ample (3) shows an example of where the base sys-
tem does not detect the extraneous preposition be-
cause it only searches for these in certain quantifier
constructions. Example (4) contains a spelling error,
which confuses the determiner error detection sys-
tem. It has not seen the misspelling often enough to
be able to reliably judge whether it needs an article
or not before it, and so errs on the side of caution.
When diffender is correctly spelled as defender, the
base system does detect that there is a missing article
in the sentence.
There were a small number of cases where dialect
caused a mismatch between our system?s error pre-
dictions and the gold standard. For example, an ho-
tel is not marked as an error in the gold standard
since it is correct in many dialects. However, it was
always corrected to a hotel by our system. Our sys-
tem also often corrected determiners before the noun
camp, since in American Standard English it is more
usual to talk about going to Summer Camp rather
than going to a/the Summer Camp.
Although the task was to detect preposition and
determiner errors in isolation, there was sometimes
interference from other errors in the sentence. This
impacted the task in two ways. Firstly, in a sentence
239
with multiple errors, it was sometimes possible to
correct it in multiple ways, not all of which involved
preposition or determiner errors. For example, you
could correct the phrase a women by either chang-
ing the a to the, deleting the a entirely or replacing
women with woman. The last change would not fall
under the category of determiner error, and so there
was sometimes a mismatch between the corrections
predicted by the system and the gold standard cor-
rections. Secondly, the presence of multiple errors
impacted the task when a gold standard correction
depended on another error in the same sentence be-
ing corrected in a particular way. For example, you
could correct I?m really excited to read the book. as
I?m really excited about reading the book., however
if you add the preposition about without correcting
to read this correction results in the sentence becom-
ing even more ungrammatical than the original.11
9 Conclusion
In this paper, we have described a hybrid system
for grammatical error detection that combines a pre-
existing base system, which leverages detailed lin-
guistic knowledge and produces high-precision out-
put, with a large-scale n-gram approach, which re-
lies almost exclusively on simple counting of n-
grams in a massive corpus. Though the base system
was not tuned at all for the HOO 2012 Shared Task,
it performed well in the official evaluation. The two
methods also complemented each other well: many
of the predictions from one did not appear in the out-
put of the other, and the F-score of the hybrid system
was considerably higher than the scores for the indi-
vidual methods.
Acknowledgments
We thank Martin Chodorow for discussions about
the base system, Daniel Blanchard for help with run-
ning the base system, Nitin Madnani for discussions
about the paper and for its title, and Michael Flor for
the TrendStream system.
11Many of these cases were addressed in the revised version
of the gold standard data, however we feel that the issue is a
more general one and deserves consideration in the design of
future tasks.
References
Yigal Attali and Jill Burstein. 2006. Automated Es-
say Scoring with e-rater V.2. Journal of Technol-
ogy, Learning, and Assessment, 4(3). Available from
http://www.jtla.org.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-Scale N-gram Models for Lexical Disambigua-
tion. In Proceedings of the 21st international joint
conference on Artifical intelligence, IJCAI?09, pages
1507?1512, Pasadena, California. Morgan Kaufmann
Publishers Inc.
Martin Chodorow and Claudia Leacock. 2000. An Un-
supervised Method for Detecting Grammatical Errors.
In Proceedings of the First Meeting of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL), pages 140?147, Seattle, Wash-
ington. Association for Computational Linguistics.
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 2007.
Detection of Grammatical Errors Involving Preposi-
tions. In Proceedings of the Fourth ACL-SIGSEM
Workshop on Prepositions, pages 25?30, Prague,
Czech Republic. Association for Computational Lin-
guistics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition and
Determiner Error Correction Shared Task. In Proceed-
ings of the Seventh Workshop on Innovative Use of
NLP for Building Educational Applications, Montreal,
Canada. Association for Computational Linguistics.
Michael Flor. 2012. A fast and flexible archi-
tecture for very large word n-gram datasets.
Natural Language Engineering, pages 1?33.
doi:10.1017/S135132491100034.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171, Los An-
geles, California. Association for Computational Lin-
guistics.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2004. Detecting Errors in English Article Usage with
a Maximum Entropy Classifier Trained on a Large, Di-
verse Corpus. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC 2004), pages 1625?1628, Lisbon, Portugal.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12:115?129. doi:10.1017/S1351324906004190.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
240
ESL Writing. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 865?872, Manchester, UK. Coling
2008 Organizing Committee.
241
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 48?57,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
A Report on the First Native Language Identification Shared Task
Joel Tetreault?, Daniel Blanchard? and Aoife Cahill?
? Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USA
Joel.Tetreault@nuance.com
? Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA
{dblanchard, acahill}@ets.org
Abstract
Native Language Identification, or NLI, is the
task of automatically classifying the L1 of a
writer based solely on his or her essay writ-
ten in another language. This problem area
has seen a spike in interest in recent years
as it can have an impact on educational ap-
plications tailored towards non-native speak-
ers of a language, as well as authorship pro-
filing. While there has been a growing body
of work in NLI, it has been difficult to com-
pare methodologies because of the different
approaches to pre-processing the data, differ-
ent sets of languages identified, and different
splits of the data used. In this shared task, the
first ever for Native Language Identification,
we sought to address the above issues by pro-
viding a large corpus designed specifically for
NLI, in addition to providing an environment
for systems to be directly compared. In this
paper, we report the results of the shared task.
A total of 29 teams from around the world
competed across three different sub-tasks.
1 Introduction
One quickly growing subfield in NLP is the task
of identifying the native language (L1) of a writer
based solely on a sample of their writing in an-
other language. The task is framed as a classifica-
tion problem where the set of L1s is known a priori.
Most work has focused on identifying the native lan-
guage of writers learning English as a second lan-
guage. To date this topic has motivated several pa-
pers and research projects.
Native Language Identification (NLI) can be use-
ful for a number of applications. NLI can be used in
educational settings to provide more targeted feed-
back to language learners about their errors. It
is well known that speakers of different languages
make different kinds of errors when learning a lan-
guage (Swan and Smith, 2001). A writing tutor
system which can detect the native language of the
learner will be able to tailor the feedback about the
error and contrast it with common properties of the
learner?s language. In addition, native language is
often used as a feature that goes into authorship pro-
filing (Estival et al, 2007), which is frequently used
in forensic linguistics.
Despite the growing interest in this field, devel-
opment has been encumbered by two issues. First
is the issue of data. Evaluating an NLI system re-
quires a corpus containing texts in a language other
than the native language of the writer. Because of
a scarcity of such corpora, most work has used the
International Corpus of Learner English (ICLEv2)
(Granger et al, 2009) for training and evaluation
since it contains several hundred essays written by
college-level English language learners. However,
this corpus is quite small for training and testing
statistical systems which makes it difficult to tell
whether the systems that are developed can scale
well to larger data sets or to different domains.
Since the ICLE corpus was not designed with the
task of NLI in mind, the usability of the corpus for
this task is further compromised by idiosyncrasies
in the data such as topic bias (as shown by Brooke
and Hirst (2011)) and the occurrence of characters
which only appear in essays written by speakers of
certain languages (Tetreault et al, 2012). As a result,
it is hard to draw conclusions about which features
48
actually perform best. The second issue is that there
has been little consistency in the field in the use of
cross-validation, the number of L1s, and which L1s
are used. As a result, comparing one approach to
another has been extremely difficult.
The first Shared Task in Native Language Identifi-
cation is intended to better unify this community and
help the field progress. The Shared Task addresses
the two deficiencies above by first using a new cor-
pus (TOEF11, discussed in Section 3) that is larger
than the ICLE and designed specifically for the task
of NLI and second, by providing a common set of
L1s and evaluation standards that everyone will use
for this competition, thus facilitating direct compar-
ison of approaches. In this report we describe the
methods most participants used, the data they eval-
uated their systems on, the three sub-tasks involved,
the results achieved by the different teams, and some
suggestions and ideas about what we can do for the
next iteration of the NLI shared task.
In the following section, we provide a summary
of the prior work in Native Language Identification.
Next, in Section 3 we describe the TOEFL11 cor-
pus used for training, development and testing in this
shared task. Section 4 describes the three sub-tasks
of the NLI Shared Task as well as a review of the
timeline. Section 5 lists the 29 teams that partici-
pated in the shared task, and introduce abbreviations
that will be used throughout this paper. Sections 6
and 7 describe the results of the shared task and a
separate post shared task evaluation where we asked
teams to evaluate their system using cross-validation
on a combination of the training and development
data. In Section 8 we provide a high-level view of
the common features and machine learning methods
teams tended to use. Finally, we offer conclusions
and ideas for future instantiations of the shared task
in Section 9.
2 Related Work
In this section, we provide an overview of some of
the common approaches used for NLI prior to this
shared task. While a comprehensive review is out-
side the scope of this paper, we have compiled a
bibliography of related work in the field. It can be
downloaded from the NLI Shared Task website.1
To date, nearly all approaches have treated the
task of NLI as a supervised classification problem
where statistical models are trained on data from the
different L1s. The work of Koppel et al (2005) was
the first in the field and they explored a multitude
of features, many of which are employed in several
of the systems in the shared tasks. These features
included character and POS n-grams, content and
function words, as well as spelling and grammati-
cal errors (since language learners have tendencies
to make certain errors based on their L1 (Swan and
Smith, 2001)). An SVM model was trained on these
features extracted from a subsection of the ICLE
corpus consisting of 5 L1s.
N-gram features (word, character and POS) have
figured prominently in prior work. Not only are they
easy to compute, but they can be quite predictive.
However, there are many variations on the features.
Past reseach efforts have explored different n-gram
windows (though most tend to focus on unigrams
and bigrams), different thresholds for how many n-
grams to include as well as whether to encode the
feature as binary (presence or absence of the partic-
ular n-gram) or as a normalized count.
The inclusion of syntactic features has been a fo-
cus in recent work. Wong and Dras (2011) explored
the use of production rules from two parsers and
Swanson and Charniak (2012) explored the use of
Tree Substitution Grammars (TSGs). Tetreault et
al. (2012) also investigated the use of TSGs as well
as dependency features extracted from the Stanford
parser.
Other approaches to NLI have included the use of
Latent Dirichlet Analysis to cluster features (Wong
et al, 2011), adaptor grammars (Wong et al, 2012),
and language models (Tetreault et al, 2012). Ad-
ditionally, there has been research into the effects of
training and testing on different corpora (Brooke and
Hirst, 2011).
Much of the aforementioned work takes the per-
spective of optimizing for the task of Native Lan-
guage Identification, that is, what is the best way of
modeling the problem to get the highest system ac-
curacy? The problem of Native Language Identifica-
1http://nlisharedtask2013.org/bibliography-of-related-
work-in-nli
49
tion is also of interest to researchers in Second Lan-
guage Acquisition where they seek to explain syn-
tactic transfer in learner language (Jarvis and Cross-
ley, 2012).
3 Data
The dataset for the task was the new TOEFL11
corpus (Blanchard et al, 2013). TOEFL11 con-
sists of essays written during a high-stakes college-
entrance test, the Test of English as a Foreign Lan-
guage (TOEFL R?). The corpus contains 1,100 es-
says per language sampled as evenly as possible
from 8 prompts (i.e., topics) along with score lev-
els (low/medium/high) for each essay. The 11 na-
tive languages covered by our corpus are: Ara-
bic (ARA), Chinese (CHI), French (FRE), German
(GER), Hindi (HIN), Italian (ITA), Japanese (JAP),
Korean (KOR), Spanish (SPA), Telugu (TEL), and
Turkish (TUR).
The TOEFL11 corpus was designed specifically
to support the task of native language identifica-
tion. Because all of the essays were collected
through ETS?s operational test delivery system for
the TOEFL R? test, the encoding and storage of all
texts in the corpus is consistent. Furthermore, the
sampling of essays was designed to ensure approx-
imately equal representation of native languages
across topics, insofar as this was possible.
For the shared task, the corpus was split into
three sets: training (TOEFL11-TRAIN), development
(TOEFL11-DEV), and test (TOEFL11-TEST). The
train corpus consisted of 900 essays per L1, the de-
velopment set consisted of 100 essays per L1, and
the test set consisted of another 100 essays per L1.
Although the overall TOEFL11 corpus was sampled
as evenly as possible with regard to language and
prompts, the distribution for each language is not ex-
actly the same in the training, development and test
sets (see Tables 1a, 1b, and 1c). In fact, the distri-
bution is much closer between the training and test
sets, as there are several languages for which there
are no essays for a given prompt in the development
set, whereas there are none in the training set, and
only one, Italian, for the test set.
It should be noted that in the first instantiation of
the corpus, presented in Tetreault et al (2012), we
used TOEFL11 to denote the body of data consisting
of TOEFL11-TRAIN and TOEFL11-DEV. However,
in this shared task, we added 1,100 sentences for a
test set and thus use the term TOEFL11 to now de-
note the corpus consisting of the TRAIN, DEV and
TEST sets. We expect the corpus to be released
through the the Linguistic Data Consortium in 2013.
4 NLI Shared Task Description
The shared task consisted of three sub-tasks. For
each task, the test set was TOEFL11-TEST and only
the type of training data varied from task to task.
? Closed-Training: The first and main task
was the 11-way classification task using only
the TOEFL11-TRAIN and optionally TOEFL11-
DEV for training.
? Open-Training-1: The second task allowed
the use of any amount or type of training data
(as is done by Brooke and Hirst (2011)) exclud-
ing any data from the TOEFL11, but still evalu-
ated on TOEFL11-TEST.
? Open-Training-2: The third task allowed the
use of TOEFL11-TRAIN and TOEFL11-DEV
combined with any other additional data. This
most closely reflects a real-world scenario.
Additionally, each team could submit up to 5 dif-
ferent systems per task. This allowed a team to ex-
periment with different variations of their core sys-
tem.
The training data was released on January 14,
with the development data and evaluation script re-
leased almost one month later on February 12. The
train and dev data contained an index file with the L1
for each essay in those sets. The previously unseen
and unlabeled test data was released on March 11
and teams had 8 days to submit their system predic-
tions. The predictions for each system were encoded
in a CSV file, where each line contained the file ID
of a file in TOEFL11-TEST and the corresponding
L1 prediction made by the system. Each CSV file
was emailed to the NLI organizers and then evalu-
ated against the gold standard.
5 Teams
In total, 29 teams competed in the shared task com-
petition, with 24 teams electing to write papers de-
scribing their system(s). The list of participating
50
Lang. P1 P2 P3 P4 P5 P6 P7 P8
ARA 113 113 113 112 112 113 112 112
CHI 113 113 113 112 112 113 112 112
FRE 128 128 76 127 127 60 127 127
GER 125 125 125 125 125 26 125 124
HIN 132 132 132 71 132 38 132 131
ITA 142 70 122 141 141 12 141 131
JAP 108 114 113 113 113 113 113 113
KOR 113 113 113 112 112 113 112 112
SPA 124 120 38 124 123 124 124 123
TEL 139 139 139 41 139 26 139 138
TUR 132 132 72 132 132 37 132 131
Total 1369 1299 1156 1210 1368 775 1369 1354
(a) Training Set
Lang. P1 P2 P3 P4 P5 P6 P7 P8
ARA 12 13 13 13 14 7 14 14
CHI 14 14 0 15 15 14 13 15
FRE 17 18 0 14 19 0 13 19
GER 15 15 16 10 13 0 15 16
HIN 16 17 17 0 17 0 16 17
ITA 18 0 0 30 31 0 21 0
JAP 0 14 15 14 15 14 14 14
KOR 15 8 15 2 13 15 16 16
SPA 7 0 0 21 7 21 21 23
TEL 16 17 17 0 17 0 16 17
TUR 22 4 0 22 7 0 22 23
Total 152 120 93 141 168 71 181 174
(b) Dev Set
Lang. P1 P2 P3 P4 P5 P6 P7 P8
ARA 13 11 12 14 10 13 12 15
CHI 13 14 13 13 7 14 14 12
FRE 13 14 11 15 14 8 11 14
GER 15 14 16 16 12 2 12 13
HIN 13 13 14 15 7 15 10 13
ITA 13 19 16 16 15 0 11 10
JAP 8 14 12 11 10 15 14 16
KOR 12 12 8 14 12 14 13 15
SPA 10 13 16 14 4 12 15 16
TEL 10 10 11 14 13 15 11 16
TUR 15 9 18 16 8 6 13 15
Total 135 143 147 158 112 114 136 155
(c) Test Set
Table 1: Number of essays per language per prompt in each data set
teams, along with their abbreviations, can be found
in Table 2.
6 Shared Task Results
This section summarizes the results of the shared
task. For each sub-task, we have tables listing the
51
Team Name Abbreviation
Bobicev BOB
Chonger CHO
CMU-Haifa HAI
Cologne-Nijmegen CN
CoRAL Lab @ UAB COR
CUNI (Charles University) CUN
cywu CYW
dartmouth DAR
eurac EUR
HAUTCS HAU
ItaliaNLP ITA
Jarvis JAR
kyle, crossley, dai, mcnamara KYL
LIMSI LIM
LTRC IIIT Hyderabad HYD
Michigan MIC
MITRE ?Carnie? CAR
MQ MQ
NAIST NAI
NRC NRC
Oslo NLI OSL
Toronto TOR
Tuebingen TUE
Ualberta UAB
UKP UKP
Unibuc BUC
UNT UNT
UTD UTD
VTEX VTX
Table 2: Participating Teams and Team Abbrevia-
tions
top submission for each team and its performance
by overall accuracy and by L1.2
Table 3 shows results for the Closed sub-task
where teams developed systems that were trained
solely on TOEFL11-TRAIN and TOEFL11-DEV. This
was the most popular sub-task with 29 teams com-
peting and 116 submissions in total for the sub-task.
Most teams opted to submit 4 or 5 runs.
The Open sub-tasks had far fewer submissions.
Table 4 shows results for the Open-1 sub-task where
teams could train systems using any training data ex-
cluding TOEFL11-TRAIN and TOEFL11-DEV. Three
teams competed in this sub-task for a total of 13 sub-
2For those interested in the results of all submissions, please
contact the authors.
missions. Table 5 shows the results for the third sub-
task ?Open-2?. Four teams competed in this task for
a total of 15 submissions.
The challenge for those competing in the Open
tasks was finding enough non-TOEFL11 data for
each L1 to train a classifier. External corpora com-
monly used in the competition included the:
? ICLE: which covered all L1s except for Ara-
bic, Hindi and Telugu;
? FCE: First Certificate in English Corpus
(Yannakoudakis et al, 2011): a collection of
essay written for an English assessment exam,
which covered all L1s except for Arabic, Hindi
and Telugu
? ICNALE: International Corpus Network of
Asian Learners of English (Ishikawa, 2011):
a collection of essays written by Chinese,
Japanese and Korean learners of English along
with 7 other L1s with Asian backgrounds.
? Lang8: http://www.lang8.com: a social net-
working service where users write in the lan-
guage they are learning, and get corrections
from users who are native speakers of that lan-
guage. Shared Task participants such as NAI
and TOR scraped the website for all writng
samples from English language learners. All
of the L1s in the shared task are represented on
the site, though the Asian L1s dominate.
The most challenging L1s to find data for seemed
to be Hindi and Telugu. TUE used essays written
by Pakastani students in the ICNALE corpus to sub-
stitute for Hindi. For Telugu, they scraped mate-
rial from bilingual blogs (English-Telugu) as well
as other material for the web. TOR created cor-
pora for Telugu and Hindi by scraping news articles,
tweets which were geolocated in the Hindi and Tel-
ugu speaking areas, and translations of Hindi and
Telugu blogs using Google Translate.
We caution directly comparing the results of the
Closed sub-task to the Open ones. In the Open-1
sub-task most teams had smaller training sets than
used in the Closed competition which automatically
puts them at a disadvantage, and in some cases there
52
L1 F-Score
Team
Name
Run Overall
Acc.
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
JAR 2 0.836 0.785 0.856 0.860 0.893 0.775 0.905 0.854 0.813 0.798 0.802 0.854
OSL 2 0.834 0.816 0.850 0.874 0.912 0.792 0.873 0.828 0.806 0.783 0.792 0.840
BUC 5 0.827 0.840 0.866 0.853 0.931 0.736 0.873 0.851 0.812 0.779 0.760 0.796
CAR 2 0.826 0.859 0.847 0.810 0.921 0.762 0.877 0.825 0.827 0.768 0.802 0.790
TUE 1 0.822 0.810 0.853 0.806 0.897 0.768 0.883 0.842 0.776 0.772 0.824 0.812
NRC 4 0.818 0.804 0.845 0.848 0.916 0.745 0.903 0.818 0.790 0.788 0.755 0.790
HAI 1 0.815 0.804 0.842 0.835 0.903 0.759 0.845 0.825 0.806 0.776 0.789 0.784
CN 2 0.814 0.778 0.845 0.848 0.882 0.744 0.857 0.812 0.779 0.787 0.784 0.827
NAI 1 0.811 0.814 0.829 0.828 0.876 0.755 0.864 0.806 0.789 0.757 0.793 0.802
UTD 2 0.809 0.778 0.846 0.832 0.892 0.731 0.866 0.846 0.819 0.715 0.784 0.784
UAB 3 0.803 0.820 0.804 0.822 0.905 0.724 0.850 0.811 0.736 0.777 0.792 0.786
TOR 1 0.802 0.754 0.827 0.827 0.878 0.722 0.850 0.820 0.808 0.747 0.784 0.798
MQ 4 0.801 0.800 0.828 0.789 0.885 0.738 0.863 0.826 0.780 0.703 0.782 0.802
CYW 1 0.797 0.769 0.839 0.782 0.833 0.755 0.842 0.815 0.770 0.741 0.828 0.788
DAR 2 0.781 0.761 0.806 0.812 0.870 0.706 0.846 0.788 0.776 0.730 0.723 0.767
ITA 1 0.779 0.738 0.775 0.832 0.873 0.711 0.860 0.788 0.742 0.708 0.762 0.780
CHO 1 0.775 0.764 0.835 0.798 0.888 0.721 0.816 0.783 0.670 0.688 0.786 0.758
HAU 1 0.773 0.731 0.820 0.806 0.897 0.686 0.830 0.832 0.763 0.703 0.702 0.736
LIM 4 0.756 0.737 0.760 0.788 0.886 0.654 0.808 0.775 0.756 0.712 0.701 0.745
COR 5 0.748 0.704 0.806 0.783 0.898 0.670 0.738 0.794 0.739 0.616 0.730 0.741
HYD 1 0.744 0.680 0.778 0.748 0.839 0.693 0.788 0.781 0.735 0.613 0.770 0.754
CUN 1 0.725 0.696 0.743 0.737 0.830 0.714 0.838 0.676 0.670 0.680 0.697 0.684
UNT 3 0.645 0.667 0.682 0.635 0.746 0.558 0.687 0.676 0.620 0.539 0.667 0.609
BOB 4 0.625 0.513 0.684 0.638 0.751 0.612 0.706 0.647 0.549 0.495 0.621 0.608
KYL 1 0.590 0.589 0.603 0.643 0.634 0.554 0.663 0.627 0.569 0.450 0.649 0.507
UKP 2 0.583 0.592 0.560 0.624 0.653 0.558 0.616 0.631 0.565 0.456 0.656 0.489
MIC 3 0.430 0.419 0.386 0.411 0.519 0.407 0.488 0.422 0.384 0.400 0.500 0.396
EUR 1 0.386 0.500 0.390 0.277 0.379 0.487 0.522 0.441 0.352 0.281 0.438 0.261
VTX 5 0.319 0.367 0.298 0.179 0.297 0.159 0.435 0.340 0.370 0.201 0.410 0.230
Table 3: Results for closed task
L1 F-Score
Team
Name
Run Overall
Acc.
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
TOR 5 0.565 0.410 0.776 0.692 0.754 0.277 0.680 0.660 0.650 0.653 0.190 0.468
TUE 2 0.385 0.114 0.502 0.420 0.430 0.167 0.611 0.485 0.348 0.385 0.236 0.314
NAI 2 0.356 0.329 0.450 0.331 0.423 0.066 0.511 0.426 0.481 0.314 0.000 0.207
Table 4: Results for open-1 task
L1 F-Score
Team
Name
Run Overall
Acc.
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
TUE 1 0.835 0.798 0.876 0.844 0.883 0.777 0.883 0.836 0.794 0.846 0.826 0.818
TOR 4 0.816 0.770 0.861 0.840 0.900 0.704 0.860 0.834 0.800 0.816 0.804 0.790
HYD 1 0.741 0.677 0.782 0.755 0.829 0.693 0.784 0.777 0.728 0.613 0.766 0.744
NAI 3 0.703 0.676 0.695 0.708 0.846 0.618 0.830 0.677 0.610 0.663 0.726 0.688
Table 5: Results for open-2 task
53
was a mismatch in the genre of corpora (for exam-
ple, tweets by Telugu speakers are different in com-
position than essays written by Telugu speakers).
TUE and TOR were the only two teams to partic-
ipate in all three sub-tasks, and their Open-2 sys-
tems outperformed their respective best systems in
the Closed and Open-1 sub-tasks. This suggests, un-
surprisingly, that adding more data can benefit NLI,
though quality and genre of data are also important
factors.
7 Cross Validation Results
Upon completion of the competition, we asked the
participants to perform 10-fold cross-validation on a
data set consisting of the union of TOEFL11-TRAIN
and TOEFL11-DEV. This was the same set of data
used in the first work to use any of the TOEFL11
data (Tetreault et al, 2012), and would allow another
point of comparison for future NLI work. For direct
comparison with Tetreault et al (2012), we provided
the exact folds used in that work.
The results of the 10-fold cross-validation are
shown in Table 6. Two teams had systems that per-
formed at 84.5 or better, which is just slightly higher
than the best team performance on the TOEFL11-
TEST data. In general, systems that performed well
in the main competition also performed similarly
(in terms of performance and ranking) in the cross-
validation experiment. Please note that we report
results as they are reported in the respective papers,
rounding to just one decimal place where possible.
8 Discussion of Approaches
With so many teams competing in the shared task
competition, we investigated whether there were any
commonalities in learning methods or features be-
tween the teams. In this section, we provide a coarse
grained summary of the common machine learning
methods teams employed as well as some of the
common features. Our summary is based on the in-
formation provided in the 24 team reports.
While there are many machine learning algo-
rithms to choose from, the overwhelming majority
of teams used Support Vector Machines. This may
not be surprising given that most prior work has also
used SVMs. Tetreault et al (2012) showed that one
could achieve even higher performance on the NLI
Team Accuracy
CN 84.6
JAR 84.5
OSL 83.9
BUC 82.6
MQ 82.5
TUE 82.4
CAR 82.2
NAI 82.1
Tetreault et al (2012) 80.9
HAU 79.9
LIM 75.9
CUN 74.2
UNT 63.8
MIC 63
Table 6: Results for 10-fold cross-validation on
TOEFL11-TRAIN + TOEFL11-DEV
task using ensemble methods for combining classi-
fiers. Four teams also experimented with different
ways of using ensemble methods. Three teams used
Maximum Entropy methods for their modeling. Fi-
nally, there were a few other teams that tried differ-
ent methods such as Discriminant Function Analysis
and K-Nearest Neighbors. Possibly the most distinct
method employed was that of string kernels by the
BUC team (who placed third in the closed compe-
tition). This method only used character level fea-
tures. A summary of the machine learning methods
is shown in Table 7.
A summary of the common features used across
teams is shown in Table 8. It should be noted that
the table does not detail the nuanced differences in
how the features are realized. For example, in the
case of n-grams, some teams used only the top k
most frequently n-grams while others used all of the
n-grams available. If interested in more information
about the particulars of a system and its feature, we
recommend reading the team?s summary report.
The most common features were word, character
and POS n-gram features. Most teams used n-grams
ranging from unigrams to trigrams, in line with prior
literature. However several teams used higher-order
n-grams. In fact, four of the top five teams (JAR,
OSL, CAR, TUE) generally used at least 4-grams,
54
Machine Learning Teams
SVM CN, UNT, MQ, JAR, TOR, ITA, CUN, TUE, COR, NRC, HAU, MIC, CAR
MaxEnt / logistic regression LIM, HAI, CAR
Ensemble MQ, ITA, NRC, CAR
Discriminant Function Analysis KYL
String Kernels / LRD BUC
PPM BOB
k-NN VTX
Table 7: Machine Learning algorithms used in Shared Task
and some, such as OSL and JAR, went as high 7 and
9 respectively in terms of character n-grams.
Syntactic features, which were first evaluated in
Wong and Dras (2011) and Swanson and Char-
niak (2012) were used by six teams in the competi-
tion, with most using dependency parses in different
ways. Interestingly, while Wong and Dras (2011)
showed some of the highest performance scores on
the ICLE corpus using parse features, only two of
the six teams which used them placed in the top ten
in the Closed sub-task.
Spelling features were championed by Koppel et
al. (2005) and in subsequent NLI work, however
only three teams in the competition used them.
There were several novel features that teams tried.
For example, several teams tried skip n-grams, as
well as length of words, sentences and documents;
LIM experimented with machine translation; CUN
had different features based on the relative frequen-
cies of the POS and lemma of a word; HAI tried
several new features based on passives and context
function; and the TUE team tried a battery of syn-
tactic features as well as text complexity measures.
9 Summary
We consider the first edition of the shared task a
success as we had 29 teams competing, which we
consider a large number for any shared task. Also
of note is that the task brought together researchers
not only from the Computational Linguistics com-
munity, but also those from other linguistics fields
such as Second Language Acquisition.
We were also delighted to see many teams build
on prior work but also try novel approaches. It is
our hope that finally having an evaluation on a com-
mon data set will allow researchers to learn from
each other on what works well and what does not,
and thus the field can progress more rapidly. The
evaluation scripts are publicly available and we ex-
pect that the data will become available through the
Linguistic Data Consortium in 2013.
For future editions of the NLI shared task, we
think it would be interesting to expand the scope of
NLI from identifying the L1 of student essays to be
able to identify the L1 of any piece of writing. The
ICLE and TOEFL11 corpora are both collections of
academic writing and thus it may be the case that
certain features or methodologies generalize better
to other writing genres and domains. For those in-
terested in robust NLI approaches, please refer to the
TOR team shared task report as well as Brooke and
Hirst (2012).
In addition, since the TOEFL11 data contains pro-
ficiency level one could include an evaluation by
proficiency level as language learners make differ-
ent types of errors and may even have stylistic differ-
ences in their writing as their proficiency progresses.
Finally, while this may be in the periphery of the
scope of an NLI shared task, one interesting evalua-
tion is to see how well human raters can fare on this
task. This would of course involve knowledgeable
language instructors who have years of experience
in teaching students from different L1s. Our think-
ing is that NLI might be one task where computers
would outperform human annotators.
Acknowledgments
We would like to thank Derrick Higgins and mem-
bers of Educational Testing Service for assisting us
in making the TOEFL11 essays available for this
shared task. We would also like to thank Patrick
Houghton for assisting the shared task organizers.
55
Feature Type Teams
Word N-Grams 1 CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, UAB,
CYW, NAI, NRC, MIC, CAR
2 CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, COR,
UAB, CYW, NAI, NRC, HAU, MIC, CAR
3 UNT, MQ, JAR, KYL, CUN, COR, HAU, MIC, CAR
4 JAR, KYL, CAR
5 CAR
POS N-grams 1 CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, HAI, CAR
2 CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, COR, HAI,
NAI, NRC, MIC, CAR
3 CN, UNT, JAR, TOR, LIM, CUN, TUE, COR, HAI, NAI, NRC,
CAR
4 CN, JAR, TUE, HAI, NRC, CAR
5 TUE, CAR
Character N-Grams 1 CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, HAI, CAR
2 CN, UNT, MQ, JAR, TOR, ITA, LIM, BOB, OSL, COR, HAI, NAI,
HAU, MIC, CAR
3 CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, VTX, COR, HAI,
NAI, NRC, HAU, MIC, CAR
4 CN, JAR, LIM, BOB, OSL, HAI, HAU, MIC, CAR
5 CN, JAR, BOB, OSL, HAU, CAR
6 CN, JAR, OSL,
7 JAR, OSL
8-9 JAR
Function N-Grams MQ, UAB
Syntactic Features Dependencies MQ, TOR, ITA, TUE, NAI, NRC
TSG MQ, TOR, NAI,
CF Productions TOR,
Adaptor Grammars MQ
Spelling Features LIM,CN, HAI
Table 8: Common Features used in Shared Task
In addition, thanks goes to the BEA8 Organizers
(Joel Tetreault, Jill Burstein and Claudia Leacock)
for hosting the shared task with their workshop. Fi-
nally, we would like to thank all the teams for partic-
ipating in this first shared task and making it a suc-
cess. Their feedback, patience and enthusiasm made
organizing this shared task a great experience.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391?408, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 263?272, Melbourne, Australia.
Sylviane Granger, Estelle Dagneaux, and Fanny Meunier.
2009. The International Corpus of Learner English:
Handbook and CD-ROM, version 2. Presses Universi-
taires de Louvain, Louvain-la-Neuve, Belgium.
Shin?ichiro Ishikawa. 2011. A New Horizon in Learner
Corpus Studies: The Aim of the ICNALE Projects. In
G. Weir, S. Ishikawa, and K. Poonpon, editors, Cor-
56
pora and Language Technologies in Teaching, Learn-
ing and Research. University of Strathclyde Publish-
ing.
Scott Jarvis and Scott Crossley, editors. 2012. Approach-
ing Language Transfer Through Text Classification:
Explorations in the Detection-based Approach, vol-
ume 64. Multilingual Matters Limited, Bristol, UK.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Michael Swan and Bernard Smith, editors. 2001.
Learner English: A teacher?s guide to interference and
other problems. Cambridge University Press, 2 edi-
tion.
Benjamin Swanson and Eugene Charniak. 2012. Na-
tive Language Detection with Tree Substitution Gram-
mars. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 193?197, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic Modeling for Native Language Identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124, Canberra, Australia, December.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 180?189, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
57
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1?12,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
The CoNLL-2013 Shared Task on Grammatical Error Correction
Hwee Tou Ng
Department of Computer Science
National University of Singapore
nght@comp.nus.edu.sg
Siew Mei Wu
Centre for English Language Communication
National University of Singapore
elcwusm@nus.edu.sg
Yuanbin Wu and Christian Hadiwinoto
Department of Computer Science
National University of Singapore
{wuyb,chrhad}@comp.nus.edu.sg
Joel Tetreault
Nuance Communications, Inc.
Joel.Tetreault@nuance.com
Abstract
The CoNLL-2013 shared task was devoted
to grammatical error correction. In this
paper, we give the task definition, present
the data sets, and describe the evaluation
metric and scorer used in the shared task.
We also give an overview of the various
approaches adopted by the participating
teams, and present the evaluation results.
1 Introduction
Grammatical error correction is the shared task
of the Seventeenth Conference on Computational
Natural Language Learning in 2013 (CoNLL-
2013). In this task, given an English essay written
by a learner of English as a second language, the
goal is to detect and correct the grammatical errors
present in the essay, and return the corrected essay.
This task has attracted much recent research in-
terest, with two shared tasks Helping Our Own
(HOO) 2011 and 2012 organized in the past two
years (Dale and Kilgarriff, 2011; Dale et al,
2012). In contrast to previous CoNLL shared tasks
which focused on particular subtasks of natural
language processing, such as named entity recog-
nition, semantic role labeling, dependency pars-
ing, or coreference resolution, grammatical error
correction aims at building a complete end-to-end
application. This task is challenging since for
many error types, current grammatical error cor-
rection systems do not achieve high performance
and much research is still needed. Also, tackling
this task has far-reaching impact, since it is esti-
mated that hundreds of millions of people world-
wide are learning English and they benefit directly
from an automated grammar checker.
The CoNLL-2013 shared task provides a forum
for participating teams to work on the same gram-
matical error correction task, with evaluation on
the same blind test set using the same evaluation
metric and scorer. This overview paper contains a
detailed description of the shared task, and is orga-
nized as follows. Section 2 provides the task def-
inition. Section 3 describes the annotated training
data provided and the blind test data. Section 4 de-
scribes the evaluation metric and the scorer. Sec-
tion 5 lists the participating teams and outlines the
approaches to grammatical error correction used
by the teams. Section 6 presents the results of the
shared task. Section 7 concludes the paper.
2 Task Definition
The goal of the CoNLL-2013 shared task is to
evaluate algorithms and systems for automati-
cally detecting and correcting grammatical errors
present in English essays written by second lan-
guage learners of English. Each participating
team is given training data manually annotated
with corrections of grammatical errors. The test
data consists of new, blind test essays. Prepro-
cessed test essays, which have been sentence-
segmented and tokenized, are also made available
to the participating teams. Each team is to submit
its system output consisting of the automatically
corrected essays, in sentence-segmented and tok-
enized form.
Grammatical errors consist of many different
types, including articles or determiners, preposi-
tions, noun form, verb form, subject-verb agree-
ment, pronouns, word choice, sentence structure,
punctuation, capitalization, etc. Of all the er-
ror types, determiners and prepositions are among
1
the most frequent errors made by learners of En-
glish. Not surprisingly, much published research
on grammatical error correction focuses on arti-
cle and preposition errors (Han et al, 2006; Ga-
mon, 2010; Rozovskaya and Roth, 2010; Tetreault
et al, 2010; Dahlmeier and Ng, 2011b), with rel-
atively less work on correcting word choice errors
(Dahlmeier and Ng, 2011a). Article and preposi-
tion errors were also the only error types featured
in the HOO 2012 shared task. Likewise, although
all error types were included in the HOO 2011
shared task, almost all participating teams dealt
with article and preposition errors only (besides
spelling and punctuation errors).
In the CoNLL-2013 shared task, it was felt
that the community is now ready to deal with
more error types, including noun number, verb
form, and subject-verb agreement, besides arti-
cles/determiners and prepositions. Table 1 shows
examples of the five error types in our shared task.
Since there are five error types in our shared task
compared to two in HOO 2012, there is a greater
chance of encountering multiple, interacting errors
in a sentence in our shared task. This increases the
complexity of our shared task relative to that of
HOO 2012. To illustrate, consider the following
sentence:
Although we have to admit some bad
effect which is brought by the new
technology, still the advantages of the
new technologies cannot be simply dis-
carded.
The noun number error effect needs to be corrected
(effect? effects). This necessitates the correction
of a subject-verb agreement error (is ? are). A
pipeline system in which corrections for subject-
verb agreement errors occur strictly before correc-
tions for noun number errors would not be able
to arrive at a fully corrected sentence for this ex-
ample. The ability to correct multiple, interacting
errors is thus necessary in our shared task. The re-
cent work of (Dahlmeier and Ng, 2012a), for ex-
ample, is designed to deal with multiple, interact-
ing errors.
Note that the essays in the training data and the
test essays naturally contain grammatical errors of
all types, beyond the five error types focused in our
shared task. In the automatically corrected essays
returned by a participating system, only correc-
tions necessary to correct errors of the five types
are made. The other errors are to be left uncor-
rected.
3 Data
This section describes the training and test data
released to each participating team in our shared
task.
3.1 Training Data
The training data provided in our shared task is
the NUCLE corpus, the NUS Corpus of Learner
English (Dahlmeier et al, 2013). As noted by
(Leacock et al, 2010), the lack of a manually an-
notated and corrected corpus of English learner
texts has been an impediment to progress in gram-
matical error correction, since it prevents com-
parative evaluations on a common benchmark test
data set. NUCLE was created precisely to fill this
void. It is a collection of 1,414 essays written
by students at the National University of Singa-
pore (NUS) who are non-native speakers of En-
glish. The essays were written in response to some
prompts, and they cover a wide range of topics,
such as environmental pollution, health care, etc.
The grammatical errors in these essays have been
hand-corrected by professional English instructors
at NUS. For each grammatical error instance, the
start and end character offsets of the erroneous text
span are marked, and the error type and the cor-
rection string are provided. Manual annotation is
carried out using a graphical user interface specif-
ically built for this purpose. The error annotations
are saved as stand-off annotations, in SGML for-
mat.
To illustrate, consider the following sentence at
the start of the first paragraph of an essay:
From past to the present, many impor-
tant innovations have surfaced.
There is an article/determiner error (past ? the
past) in this sentence. The error annotation, also
called correction or edit, in SGML format is
shown in Figure 1. start par (end par) de-
notes the paragraph ID of the start (end) of the er-
roneous text span (paragraph ID starts from 0 by
convention). start off (end off) denotes the
character offset of the start (end) of the erroneous
text span (again, character offset starts from 0 by
convention). The error tag is ArtOrDet, and the
correction string is the past.
2
Error tag Error type Example sentence Correction (edit)
ArtOrDet Article or determiner In late nineteenth century, there
was a severe air crash happening
at Miami international airport.
late ? the late
Prep Preposition Also tracking people is very
dangerous if it has been con-
trolled by bad men in a not good
purpose.
in ? for
Nn Noun number I think such powerful device
shall not be made easily avail-
able.
device ? devices
Vform Verb form However, it is an achievement as
it is an indication that our soci-
ety is progressed well and peo-
ple are living in better condi-
tions.
progressed ? progressing
SVA Subject-verb agreement People still prefers to bear the
risk and allow their pets to have
maximum freedom.
prefers ? prefer
Table 1: The five error types in our shared task.
<MISTAKE start par="0" start off="5" end par="0" end off="9">
<TYPE>ArtOrDet</TYPE>
<CORRECTION>the past</CORRECTION>
</MISTAKE>
Figure 1: An example error annotation.
The NUCLE corpus was first used in
(Dahlmeier and Ng, 2011b), and has been
publicly available for research purposes since
June 20111. All instances of grammatical errors
are annotated in NUCLE, and the errors are
classified into 27 error types (Dahlmeier et al,
2013).
To help participating teams in their prepara-
tion for the shared task, we also performed au-
tomatic preprocessing of the NUCLE corpus and
released the preprocessed form of NUCLE. The
preprocessing operations performed on the NU-
CLE essays include sentence segmentation and
word tokenization using the NLTK toolkit (Bird
et al, 2009), and part-of-speech (POS) tagging,
constituency and dependency tree parsing using
the Stanford parser (Klein and Manning, 2003;
de Marneffe et al, 2006). The error annotations,
which are originally at the character level, are
then mapped to error annotations at the word to-
ken level. Error annotations at the word token
1http://www.comp.nus.edu.sg/?nlp/corpora.html
level also facilitate scoring, as we will see in Sec-
tion 4, since our scorer operates by matching to-
kens. Note that although we released our own
preprocessed version of NUCLE, the participating
teams were however free to perform their own pre-
processing if they so preferred.
3.1.1 Revised version of NUCLE
NUCLE release version 2.3 was used in the
CoNLL-2013 shared task. In this version, 17 es-
says were removed from the first release of NU-
CLE since these essays were duplicates with mul-
tiple annotations.
In the original NUCLE corpus, there is not an
explicit preposition error type. Instead, prepo-
sition errors are part of the Wcip (wrong collo-
cation/idiom/preposition) and Rloc (local redun-
dancy) error types. The Wcip error type combines
errors concerning collocations, idioms, and prepo-
sitions together into one error type. The Rloc er-
ror type annotates extraneous words which are re-
dundant and should be removed, and they include
redundant articles, determiners, and prepositions.
3
Training data Test data
(NUCLE)
# essays 1,397 50
# sentences 57,151 1,381
# word tokens 1,161,567 29,207
Table 2: Statistics of training and test data.
In our shared task, in order to facilitate the detec-
tion and correction of article/determiner errors and
preposition errors, we performed automatic map-
ping of error types in the original NUCLE cor-
pus. The mapping relies on POS tags, constituent
parse trees, and error annotations at the word token
level. Specifically, we map the error types Wcip
and Rloc to Prep, Wci, ArtOrDet, and Rloc?.
Prepositions in the error type Wcip or Rloc are
mapped to a new error type Prep, and redundant
articles or determiners in the error type Rloc are
mapped to ArtOrDet. The remaining unaffected
Wcip errors are assigned the new error type Wci
and the remaining unaffected Rloc errors are as-
signed the new error type Rloc?. The code that
performs automatic error type mapping was also
provided to the participating teams.
The statistics of the NUCLE corpus (release 2.3
version) are shown in Table 2. The distribution
of errors among the five error types is shown in
Table 3. The newly added noun number error type
in our shared task accounts for the second highest
number of errors among the five error types. The
five error types in our shared task constitute 35%
of all grammatical errors in the training data, and
47% of all errors in the test data. These figures
support our choice of these five error types to be
the focus of our shared task, since they account
for a large percentage of all grammatical errors in
English learner essays.
While the NUCLE corpus is provided in our
shared task, participating teams are free to not use
NUCLE, or to use additional resources and tools
in building their grammatical error correction sys-
tems, as long as these resources and tools are pub-
licly available and not proprietary. For example,
participating teams are free to use the Cambridge
FCE corpus (Yannakoudakis et al, 2011; Nicholls,
2003) (the training data provided in HOO 2012
(Dale et al, 2012)) as additional training data.
Error tag Training % Test %
data data
(NUCLE)
ArtOrDet 6,658 14.8 690 19.9
Prep 2,404 5.3 312 9.0
Nn 3,779 8.4 396 11.4
Vform 1,453 3.2 122 3.5
SVA 1,527 3.4 124 3.6
5 types 15,821 35.1 1,644 47.4
all types 45,106 100.0 3,470 100.0
Table 3: Error type distribution of the training and
test data.
3.2 Test Data
25 NUS students, who are non-native speakers of
English, were recruited to write new essays to be
used as blind test data in the shared task. Each
student wrote two essays in response to the two
prompts shown in Table 4, one essay per prompt.
Essays written using the first prompt are present
in the NUCLE training data, while the second
prompt is a new prompt not used previously. As
a result, 50 test essays were collected. The statis-
tics of the test essays are shown in Table 2.
Error annotation on the test essays was carried
out by a native speaker of English who is a lecturer
at the NUS Centre for English Language Commu-
nication. The distribution of errors in the test es-
says among the five error types is shown in Ta-
ble 3. The test essays were then preprocessed in
the same manner as the NUCLE corpus. The pre-
processed test essays were released to the partici-
pating teams.
Unlike the test data used in HOO 2012 which
was proprietary and not available after the shared
task, the test essays and their error annotations in
the CoNLL-2013 shared task are freely available
after the shared task.
4 Evaluation Metric and Scorer
A grammatical error correction system is evalu-
ated by how well its proposed corrections or edits
match the gold-standard edits. An essay is first
sentence-segmented and tokenized before evalua-
tion is carried out on the essay. To illustrate, con-
sider the following tokenized sentence S written
by an English learner:
There is no a doubt, tracking system
4
ID Prompt
1 Surveillance technology such as RFID (radio-frequency identification) should not be used to
track people (e.g., human implants and RFID tags on people or products). Do you agree? Sup-
port your argument with concrete examples.
2 Population aging is a global phenomenon. Studies have shown that the current average life span
is over 65. Projections of the United Nations indicate that the population aged 60 or over in
developed and developing countries is increasing at 2% to 3% annually. Explain why rising life
expectancies can be considered both a challenge and an achievement.
Table 4: The two prompts used for the test essays.
has brought many benefits in this infor-
mation age .
The set of gold-standard edits of a human annota-
tor is g = {a doubt ? doubt, system ? systems,
has ? have}. Suppose the tokenized output sen-
tence H of a grammatical error correction system
given the above sentence is:
There is no doubt, tracking system has
brought many benefits in this informa-
tion age .
That is, the set of system edits is e = {a doubt
? doubt}. The performance of the grammatical
error correction system is measured by how well
the two sets g and e match, in the form of recall
R, precision P , and F1 measure: R = 1/3, P =
1/1, F1 = 2RP/(R + P ) = 1/2.
More generally, given a set of n sentences,
where gi is the set of gold-standard edits for sen-
tence i, and ei is the set of system edits for sen-
tence i, recall, precision, and F1 are defined as
follows:
R =
?n
i=1 |gi ? ei|?n
i=1 |gi|
(1)
P =
?n
i=1 |gi ? ei|?n
i=1 |ei|
(2)
F1 =
2?R? P
R + P
(3)
where the intersection between gi and ei for sen-
tence i is defined as
gi ? ei = {e ? ei|?g ? gi,match(g, e)} (4)
Evaluation by the HOO scorer (Dale and Kilgar-
riff, 2011) is based on computing recall, precision,
and F1 measure as defined above.
Note that there are multiple ways to specify a
set of gold-standard edits that denote the same cor-
rections. For example, in the above learner-written
sentence S, alternative but equivalent sets of gold-
standard edits are {a ? , system ? systems, has
? have}, {a ? , system has ? systems have},
etc. Given the same learner-written sentence S
and the same system output sentence H shown
above, one would expect a scorer to give the same
R,P, F1 scores regardless of which of the equiv-
alent sets of gold-standard edits is specified by an
annotator.
However, this is not the case with the HOO
scorer. This is because the HOO scorer uses
GNU wdiff2 to extract the differences between
the learner-written sentence S and the system out-
put sentence H to form a set of system edits.
Since in general there are multiple ways to spec-
ify a set of gold-standard edits that denote the
same corrections, the set of system edits com-
puted by the HOO scorer may not match the set of
gold-standard edits specified, leading to erroneous
scores. In the above example, the set of system
edits computed by the HOO scorer for S and H is
{a ? }. Given that the set of gold-standard edits
g is {a doubt ? doubt, system ? systems, has ?
have}, the scores computed by the HOO scorer are
R = P = F1 = 0, which are erroneous.
The MaxMatch (M2) scorer3 (Dahlmeier and
Ng, 2012b) was designed to overcome this limita-
tion of the HOO scorer. The key idea is that the
set of system edits automatically computed and
used in scoring should be the set that maximally
matches the set of gold-standard edits specified by
the annotator. The M2 scorer uses an efficient al-
gorithm to search for such a set of system edits
using an edit lattice. In the above example, given
S, H , and g, the M2 scorer is able to come up
with the best matching set of system edits e = {a
doubt ? doubt}, thus giving the correct scores
R = 1/3, P = 1/1, F1 = 1/2. We use the M2
2http://www.gnu.org/s/wdiff/
3http://www.comp.nus.edu.sg/?nlp/software.html
5
scorer in the CoNLL-2013 shared task.
The original M2 scorer implemented in
(Dahlmeier and Ng, 2012b) assumes that there
is one set of gold-standard edits gi for each
sentence i. However, it is often the case that
multiple alternative corrections are acceptable for
a sentence. As we allow participating teams to
submit alternative sets of gold-standard edits for
a sentence, we also extend the M2 scorer to deal
with multiple alternative sets of gold-standard
edits.
Based on Equations 1 and 2, Equation 3 can be
re-expressed as:
F1 =
2?
?n
i=1 |gi ? ei|?n
i=1 (|gi|+ |ei|)
(5)
To deal with multiple alternative sets of gold-
standard edits gi for a sentence i, the extended
M2 scorer chooses the gi that maximizes the cu-
mulative F1 score for sentences 1, . . . , i. Ties
are broken based on the following criteria: first
choose the gi that maximizes the numerator?n
i=1 |gi ? ei|, then choose the gi that minimizes
the denominator
?n
i=1 (|gi|+ |ei|), finally choose
the gi that appears first in the list of alternatives.
5 Approaches
54 teams registered to participate in the shared
task, out of which 17 teams submitted the output
of their grammatical error correction systems by
the deadline. These teams are listed in Table 5.
Each team is assigned a 3 to 4-letter team ID. In
the remainder of this paper, we will use the as-
signed team ID to refer to a participating team.
Every team submitted a system description paper
(the only exception is the SJT2 team).
Many different approaches are adopted by par-
ticipating teams in the CoNLL-2013 shared task,
and Table 6 summarizes these approaches. A com-
monly used approach in the shared task and in
grammatical error correction research in general
is to build a classifier for each error type. For ex-
ample, the classifier for noun number returns the
classes {singular, plural}, the classifier for article
returns the classes {a/an, the, }, etc. The classi-
fier for an error type may be learned from train-
ing examples encoding the surrounding context of
an error occurrence, or may be specified by deter-
ministic hand-crafted rules, or may be built using
a hybrid approach combining both machine learn-
ing and hand-crafted rules. These approaches are
denoted by M, R, and H respectively in Table 6.
The machine translation approach (denoted by
T in Table 6) to grammatical error correction
treats the task as ?translation? from bad English
to good English. Both phrase-based translation
and syntax-based translation approaches are used
by teams in the CoNLL-2013 shared task. An-
other related approach is the language modeling
approach (denoted by L in Table 6), in which
the probability of a learner sentence is compared
with the probability of a candidate corrected sen-
tence, based on a language model built from a
background corpus. The candidate correction is
chosen if it results in a corrected sentence with a
higher probability. In general, these approaches
are not mutually exclusive. For example, the
work of (Dahlmeier and Ng, 2012a; Yoshimoto et
al., 2013) includes elements of machine learning-
based classification, machine translation, and lan-
guage modeling approaches.
When different approaches are used to tackle
different error types by a system, we break down
the error types into different rows in Table 6, and
specify the approach used for each group of error
types. For instance, the HIT team uses a machine
learning approach to deal with article/determiner,
noun number, and preposition errors, and a rule-
based approach to deal with subject-verb agree-
ment and verb form errors. As such, the entry for
HIT is sub-divided into two rows, to make it clear
which particular error type is handled by which
approach.
Table 6 also shows the linguistic features used
by the participating teams, which include lexical
features (i.e., words, collocations, n-grams), parts-
of-speech (POS), constituency parses, dependency
parses, and semantic features (including semantic
role labels).
While all teams in the shared task use the NU-
CLE corpus, they are also allowed to use addi-
tional external resources (both corpora and tools)
so long as they are publicly available and not pro-
prietary. The external resources used by the teams
are also listed in Table 6.
6 Results
All submitted system output was evaluated using
the M2 scorer, based on the error annotations pro-
vided by our annotator. The recall (R), precision
(P ), and F1 measure of all teams are shown in Ta-
ble 7. The performance of the teams varies greatly,
6
Team ID Affiliation
CAMB University of Cambridge
HIT Harbin Institute of Technology
IITB Indian Institute of Technology, Bombay
KOR Korea University
NARA Nara Institute of Science and Technology
NTHU National Tsing Hua University
SAAR Saarland University
SJT1 Shanghai Jiao Tong University (Team #1)
SJT2 Shanghai Jiao Tong University (Team #2)
STAN Stanford University
STEL Stellenbosch University
SZEG University of Szeged
TILB Tilburg University
TOR University of Toronto
UAB Universitat Auto`noma de Barcelona
UIUC University of Illinois at Urbana-Champaign
UMC University of Macau
Table 5: The list of 17 participating teams.
Rank Team R P F1
1 UIUC 23.49 46.45 31.20
2 NTHU 26.35 23.80 25.01
3 HIT 16.56 35.65 22.61
4 NARA 18.62 27.39 22.17
5 UMC 17.53 28.49 21.70
6 STEL 13.33 27.00 17.85
7 SJT1 10.96 40.18 17.22
8 CAMB 10.10 39.15 16.06
9 IITB 4.99 28.18 8.48
10 STAN 4.69 25.50 7.92
11 TOR 4.81 17.67 7.56
12 KOR 3.71 43.88 6.85
13 TILB 7.24 6.25 6.71
14 SZEG 3.16 5.52 4.02
15 UAB 1.22 12.42 2.22
16 SAAR 1.10 27.69 2.11
17 SJT2 0.24 13.33 0.48
Table 7: Scores (in %) without alternative an-
swers.
from barely half a per cent to 31.20% for the top
team.
The nature of grammatical error correction is
such that multiple, different corrections are of-
ten acceptable. In order to allow the participating
teams to raise their disagreement with the original
gold-standard annotations provided by the anno-
tator, and not understate the performance of the
teams, we allow the teams to submit their pro-
posed alternative answers. This was also the prac-
tice adopted in HOO 2011 and HOO 2012. Specif-
ically, after the teams submitted their system out-
put and the error annotations on the test essays
were released, we allowed the teams to propose al-
ternative answers (gold-standard edits), to be sub-
mitted within four days after the initial error an-
notations were released. The same annotator who
provided the error annotations on the test essays
also judged the alternative answers proposed by
the teams, to ensure consistency. In all, five teams
(NTHU, STEL, TOR, UIUC, UMC) submitted al-
ternative answers.
The same submitted system output was then
evaluated using the extended M2 scorer, with the
original annotations augmented with the alterna-
tive answers. Table 8 shows the recall (R), preci-
sion (P ), and F1 measure of all teams under this
new evaluation setting.
The F1 measure of every team improves when
7
Te
am
E
rr
or
A
pp
ro
ac
h
D
es
cr
ip
ti
on
of
A
pp
ro
ac
h
L
in
gu
is
ti
c
F
ea
tu
re
s
E
xt
er
na
lR
es
ou
rc
es
C
A
M
B
A
N
P
S
V
T
fa
ct
or
ed
ph
ra
se
-b
as
ed
tr
an
sl
at
io
n
m
od
el
w
it
h
IR
S
T
la
ng
ua
ge
m
od
el
le
xi
ca
l,
P
O
S
C
am
br
id
ge
L
ea
rn
er
C
or
pu
s
H
IT
A
N
P
M
m
ax
im
um
en
tr
op
y
w
it
h
co
nfi
de
nc
e
tu
ni
ng
,
an
d
ge
ne
ti
c
al
go
-
ri
th
m
fo
r
fe
at
ur
e
se
le
ct
io
n
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e,
se
m
an
ti
c
W
or
dN
et
,L
on
gm
an
di
ct
io
na
ry
S
V
R
ru
le
-b
as
ed
P
O
S
,d
ep
en
de
nc
y
pa
rs
e,
se
m
an
ti
c
II
T
B
A
N
M
m
ax
im
um
en
tr
op
y
le
xi
ca
l,
P
O
S
,n
ou
n
pr
op
er
ti
es
W
ik
ti
on
ar
y
S
R
ru
le
-b
as
ed
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
K
O
R
A
N
P
M
m
ax
im
um
en
tr
op
y
le
xi
ca
l,
P
O
S
,h
ea
d-
m
od
ifi
er
,d
ep
en
de
nc
y
pa
rs
e
(n
on
e)
N
A
R
A
A
P
T
ph
ra
se
-b
as
ed
st
at
is
ti
ca
lm
ac
hi
ne
tr
an
sl
at
io
n
le
xi
ca
l
L
an
g-
8
N
M
ad
ap
tiv
e
re
gu
la
ri
za
ti
on
of
w
ei
gh
tv
ec
to
rs
le
xi
ca
l,
le
m
m
a,
co
ns
ti
tu
en
cy
pa
rs
e
G
ig
aw
or
d
S
V
L
tr
ee
le
t(
tr
ee
-b
as
ed
)
la
ng
ua
ge
m
od
el
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e
P
en
n
T
re
eb
an
k,
G
ig
aw
or
d
N
T
H
U
A
N
P
V
L
n-
gr
am
-b
as
ed
an
d
de
pe
nd
en
cy
-b
as
ed
la
ng
ua
ge
m
od
el
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
G
oo
gl
e
W
eb
-1
T
S
A
A
R
A
M
m
ul
ti
-c
la
ss
S
V
M
an
d
na
iv
e
B
ay
es
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e
C
M
U
P
ro
no
un
ci
ng
D
ic
ti
on
ar
y
S
R
ru
le
-b
as
ed
P
O
S
,d
ep
en
de
nc
y
pa
rs
e
S
JT
1
A
N
P
S
V
M
m
ax
im
um
en
tr
op
y
(w
it
h
L
M
po
st
-fi
lt
er
in
g)
le
xi
ca
l,
le
m
m
a,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
E
ur
op
ar
l
S
TA
N
A
N
P
S
V
H
E
ng
li
sh
R
es
ou
rc
e
G
ra
m
m
ar
(E
R
G
),
he
ad
-d
ri
ve
n
ph
ra
se
st
ru
c-
tu
re
,e
xt
en
de
d
w
it
h
ha
nd
-c
od
ed
m
al
-r
ul
es
le
xi
ca
l,
P
O
S
,c
on
st
it
ue
nc
y
pa
rs
e,
se
m
an
ti
c
E
ng
li
sh
R
es
ou
rc
e
G
ra
m
m
ar
S
T
E
L
A
N
P
S
V
T
tr
ee
-t
o-
st
ri
ng
w
it
h
G
H
K
M
tr
an
sd
uc
er
co
ns
ti
tu
en
cy
pa
rs
e
W
ik
ip
ed
ia
,W
or
dN
et
S
Z
E
G
A
N
M
m
ax
im
um
en
tr
op
y
L
F
G
,l
ex
ic
al
,c
on
st
it
ue
nc
y
pa
rs
e,
de
pe
nd
en
cy
pa
rs
e
(n
on
e)
T
IL
B
A
N
P
S
V
M
bi
na
ry
an
d
m
ul
ti
-c
la
ss
IG
T
re
e
le
xi
ca
l,
le
m
m
a,
P
O
S
G
oo
gl
e
W
eb
-1
T,
G
ig
aw
or
d
T
O
R
A
N
P
S
V
T
no
is
y
ch
an
ne
lm
od
el
in
vo
lv
in
g
tr
an
sf
or
m
at
io
n
of
si
ng
le
w
or
ds
le
xi
ca
l,
P
O
S
W
ik
ip
ed
ia
U
A
B
A
N
P
S
V
R
ru
le
-b
as
ed
le
xi
ca
l,
de
pe
nd
en
cy
pa
rs
e
To
p
25
0
un
co
un
ta
bl
e
no
un
s,
F
re
eL
in
g
m
or
ph
ol
og
ic
al
di
ct
io
na
ry
U
IU
C
A
N
P
S
V
M
A
:m
ul
ti
-c
la
ss
av
er
ag
ed
pe
rc
ep
tr
on
;o
th
er
s:
na
iv
e
B
ay
es
le
xi
ca
l,
P
O
S
,s
ha
ll
ow
pa
rs
e
G
oo
gl
e
W
eb
-1
T,
G
ig
aw
or
d
U
M
C
A
N
P
S
V
H
pi
pe
li
ne
:
ru
le
-b
as
ed
fi
lt
er
?
se
m
i-
su
pe
rv
is
ed
m
ul
ti
-c
la
ss
m
ax
im
um
en
tr
op
y
cl
as
si
fi
er
?
L
M
co
nfi
de
nc
e
sc
or
er
le
xi
ca
l,
P
O
S
,d
ep
en
de
nc
y
pa
rs
e
N
ew
s
co
rp
us
,J
M
yS
pe
ll
di
ct
io
na
ry
,G
oo
gl
e
W
eb
-1
T,
P
en
n
T
re
eb
an
k
Ta
bl
e
6:
P
ro
fi
le
of
th
e
pa
rt
ic
ip
at
in
g
te
am
s.
T
he
E
rr
or
co
lu
m
n
sh
ow
s
th
e
er
ro
r
ty
pe
,
w
he
re
ea
ch
le
tt
er
de
no
te
s
th
e
er
ro
r
ty
pe
be
gi
nn
in
g
w
it
h
th
at
in
it
ia
l
le
tt
er
.
T
he
A
pp
ro
ac
h
co
lu
m
n
sh
ow
s
th
e
ap
pr
oa
ch
ad
op
te
d
by
ea
ch
te
am
,s
om
et
im
es
br
ok
en
do
w
n
ac
co
rd
in
g
to
th
e
er
ro
r
ty
pe
:
H
de
no
te
s
a
hy
br
id
cl
as
si
fi
er
ap
pr
oa
ch
,L
de
no
te
s
a
la
ng
ua
ge
m
od
el
in
g-
ba
se
d
ap
pr
oa
ch
,M
de
no
te
s
a
m
ac
hi
ne
le
ar
ni
ng
-b
as
ed
cl
as
si
fi
er
ap
pr
oa
ch
,R
de
no
te
s
a
ru
le
-b
as
ed
cl
as
si
fi
er
(n
on
-m
ac
hi
ne
le
ar
ni
ng
)
ap
pr
oa
ch
,a
nd
T
de
no
te
s
a
m
ac
hi
ne
tr
an
sl
at
io
n
ap
pr
oa
ch
8
evaluated with alternative answers. Not surpris-
ingly, the teams which submitted alternative an-
swers tend to show the greatest improvements in
their F1 measure. Overall, the UIUC team (Ro-
zovskaya et al, 2013) achieves the best F1 mea-
sure, with a clear lead over the other teams in the
shared task, under both evaluation settings (with-
out and with alternative answers).
For future research which uses the test data of
the CoNLL-2013 shared task, we recommend that
evaluation be carried out in the setting that does
not use alternative answers, to ensure a fairer eval-
uation. This is because the scores of the teams
which submitted alternative answers tend to be
higher in a biased way when evaluated with alter-
native answers.
Rank Team R P F1
1 UIUC 31.87 62.19 42.14
2 NTHU 34.62 30.57 32.46
3 UMC 23.66 37.12 28.90
4 NARA 24.05 33.92 28.14
5 HIT 20.29 41.75 27.31
6 STEL 18.91 37.12 25.05
7 CAMB 14.19 52.11 22.30
8 SJT1 13.67 47.77 21.25
9 TOR 8.77 30.67 13.64
10 IITB 6.55 34.93 11.03
11 STAN 5.86 29.93 9.81
12 KOR 4.78 53.24 8.77
13 TILB 9.29 7.60 8.36
14 SZEG 4.07 6.67 5.06
15 UAB 1.81 17.39 3.28
16 SAAR 1.68 40.00 3.23
17 SJT2 0.33 16.67 0.64
Table 8: Scores (in %) with alternative answers.
We are also interested in the analysis of scores
of each of the five error types. To compute the
recall of an error type, we need to know the er-
ror type of each gold-standard edit, which is pro-
vided by the annotator. To compute the precision
of each error type, we need to know the error type
of each system edit, which however is not avail-
able since the submitted system output only con-
tains the corrected sentences with no indication of
the error type of the system edits.
In order to determine the error type of system
edits, we first perform POS tagging on the submit-
ted system output using the Stanford parser (Klein
andManning, 2003). We also make use of the POS
tags assigned in the preprocessed form of the test
essays. We then assign an error type to a system
edit based on the automatically determined POS
tags, as follows:
? ArtOrDet: The system edit involves a change
(insertion, deletion, or substitution) of words
tagged as article/determiner, i.e., DT or PDT.
? Prep: The system edit involves a change of
words tagged as preposition, i.e., IN or TO.
? Nn: The system edit involves a change of
words such that a word in the source string
is a singular noun (tagged as NN or NNP)
and a word in the replacement string is a plu-
ral noun (tagged as NNS or NNPS), or vice
versa. Since a word tagged as JJ (adjective)
can serve as a noun, a system edit that in-
volves a change of POS tags from JJ to one of
{NN, NNP, NNS, NNPS} or vice versa also
qualifies.
? Vform/SVA: The system edit involves a
change of words tagged as one of the verb
POS tags, i.e., VB, VBD, VBG, VBN, VBP,
and VBZ.
The verb form and subject-verb agreement error
types are grouped together into one category, since
it is difficult to automatically distinguish the two in
a reliable way.
The scores when distinguished by error type are
shown in Tables 9 and 10. Based on the F1 mea-
sure of each error type, the noun number error type
gives the highest scores, and preposition errors re-
main the most challenging error type to correct.
7 Conclusions
The CoNLL-2013 shared task saw the participa-
tion of 17 teams worldwide to evaluate their gram-
matical error correction systems on a common test
set, using a common evaluation metric and scorer.
The five error types included in the shared task
account for at least one-third to close to one-half
of all errors in English learners? essays. The best
system in the shared task achieves an F1 score of
42%, when it is scored with multiple acceptable
answers. There is still much room for improve-
ment, both in the accuracy of grammatical error
correction systems, and in the coverage of systems
to deal with a more comprehensive set of error
9
Te
am
A
rt
O
rD
et
P
re
p
N
n
V
fo
rm
/S
V
A
R
P
F
1
R
P
F
1
R
P
F
1
R
P
F
1
C
A
M
B
15
.0
7
38
.6
6
21
.6
9
3.
54
40
.7
4
6.
51
7.
58
55
.5
6
13
.3
3
8.
54
31
.8
2
13
.4
6
H
IT
24
.2
0
42
.8
2
30
.9
3
2.
89
28
.1
2
5.
25
17
.1
7
29
.6
9
21
.7
6
11
.3
8
26
.4
2
15
.9
1
II
T
B
1.
30
21
.4
3
2.
46
(n
ot
do
ne
)
9.
85
28
.6
8
14
.6
6
13
.8
2
30
.0
9
18
.9
4
K
O
R
4.
78
53
.2
3
8.
78
0.
32
4.
76
0.
60
6.
82
49
.0
9
11
.9
7
(n
ot
do
ne
)
N
A
R
A
20
.4
3
34
.0
6
25
.5
4
12
.5
4
29
.1
0
17
.5
3
16
.4
1
48
.8
7
24
.5
7
24
.8
0
14
.8
1
18
.5
4
N
T
H
U
21
.0
1
35
.8
0
26
.4
8
12
.8
6
12
.0
1
12
.4
2
45
.9
6
40
.9
0
43
.2
8
26
.8
3
12
.2
2
16
.7
9
S
A
A
R
0.
72
62
.5
0
1.
43
(n
ot
do
ne
)
(n
ot
do
ne
)
5.
28
23
.2
1
8.
61
S
JT
1
16
.8
1
47
.1
5
24
.7
9
1.
29
12
.5
0
2.
33
13
.6
4
42
.1
9
20
.6
1
2.
44
14
.6
3
4.
18
S
JT
2
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
1.
01
13
.3
3
1.
88
0.
00
0.
00
0.
00
S
TA
N
3.
91
20
.4
5
6.
57
0.
32
20
.0
0
0.
63
6.
06
29
.6
3
10
.0
6
10
.1
6
32
.0
5
15
.4
3
S
T
E
L
12
.6
1
27
.7
1
17
.3
3
9.
32
25
.6
6
13
.6
8
18
.1
8
46
.7
5
26
.1
8
12
.6
0
17
.6
1
14
.6
9
S
Z
E
G
1.
16
1.
70
1.
38
(n
ot
do
ne
)
11
.1
1
13
.6
2
12
.2
4
(n
ot
do
ne
)
T
IL
B
4.
49
4.
49
4.
49
10
.6
1
5.
07
6.
86
7.
07
21
.2
1
10
.6
1
10
.9
8
9.
57
10
.2
3
T
O
R
8.
55
25
.5
4
12
.8
1
2.
25
5.
38
3.
17
1.
77
31
.8
2
3.
35
2.
44
12
.2
4
4.
07
U
A
B
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
8.
13
12
.4
2
9.
83
U
IU
C
25
.6
5
47
.8
4
33
.4
0
4.
18
26
.5
3
7.
22
38
.3
8
52
.2
3
44
.2
5
17
.8
9
38
.9
4
24
.5
1
U
M
C
21
.0
1
30
.2
7
24
.8
1
1.
93
35
.2
9
3.
66
23
.2
3
27
.9
6
25
.3
8
18
.2
9
28
.8
5
22
.3
9
Ta
bl
e
9:
S
co
re
s
(i
n
%
)
w
it
ho
ut
al
te
rn
at
iv
e
an
sw
er
s,
di
st
in
gu
is
he
d
by
er
ro
r
ty
pe
.
If
a
te
am
in
di
ca
te
s
th
at
it
s
sy
st
em
do
es
no
th
an
dl
e
a
pa
rt
ic
ul
ar
er
ro
r
ty
pe
,i
ts
en
tr
y
fo
r
th
at
er
ro
r
ty
pe
is
m
ar
ke
d
as
?(
no
td
on
e)
?.
10
Te
am
A
rt
O
rD
et
P
re
p
N
n
V
fo
rm
/S
V
A
R
P
F
1
R
P
F
1
R
P
F
1
R
P
F
1
C
A
M
B
19
.6
2
49
.8
1
28
.1
5
5.
04
50
.0
0
9.
15
9.
50
69
.0
9
16
.7
0
16
.5
2
54
.4
1
25
.3
4
H
IT
27
.4
1
47
.4
4
34
.7
4
4.
58
37
.5
0
8.
16
19
.9
5
35
.3
7
25
.5
1
17
.9
0
38
.3
2
24
.4
0
II
T
B
1.
79
28
.5
7
3.
37
(n
ot
do
ne
)
11
.9
1
35
.2
9
17
.8
1
18
.6
7
36
.8
4
24
.7
8
K
O
R
5.
95
64
.5
2
10
.9
0
1.
53
19
.0
5
2.
83
7.
52
54
.5
5
13
.2
2
(n
ot
do
ne
)
N
A
R
A
25
.7
9
43
.3
4
32
.3
4
17
.6
0
34
.5
6
23
.3
3
19
.1
5
57
.0
4
28
.6
8
34
.6
2
19
.1
0
24
.6
2
N
T
H
U
25
.3
0
42
.5
4
31
.7
3
20
.4
5
16
.1
7
18
.0
6
52
.3
5
50
.8
7
51
.6
0
43
.0
3
19
.2
2
26
.5
7
S
A
A
R
1.
04
87
.5
0
2.
06
(n
ot
do
ne
)
(n
ot
do
ne
)
8.
60
33
.9
3
13
.7
2
S
JT
1
19
.6
5
54
.0
7
28
.8
2
1.
92
15
.6
2
3.
42
16
.4
6
52
.3
4
25
.0
5
4.
05
21
.9
5
6.
84
S
JT
2
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
1.
26
16
.6
7
2.
35
0.
00
0.
00
0.
00
S
TA
N
4.
91
24
.8
1
8.
20
0.
38
20
.0
0
0.
75
6.
78
33
.3
3
11
.2
7
13
.5
1
37
.9
7
19
.9
3
S
T
E
L
16
.2
3
35
.6
9
22
.3
1
12
.8
3
29
.8
2
17
.9
4
26
.0
5
70
.0
0
37
.9
7
20
.5
2
26
.5
5
23
.1
5
S
Z
E
G
1.
50
2.
13
1.
76
(n
ot
do
ne
)
12
.8
7
15
.9
5
14
.2
5
(n
ot
do
ne
)
T
IL
B
5.
78
5.
64
5.
71
13
.9
1
5.
68
8.
07
8.
25
24
.2
6
12
.3
1
16
.3
6
12
.7
7
14
.3
4
T
O
R
13
.1
0
39
.1
3
19
.6
3
5.
97
12
.3
1
8.
04
3.
52
63
.6
4
6.
67
8.
14
35
.2
9
13
.2
4
U
A
B
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
0.
00
12
.6
1
17
.3
9
14
.6
2
U
IU
C
31
.9
9
59
.8
4
41
.6
9
8.
81
46
.9
4
14
.8
4
46
.8
8
70
.0
0
56
.1
5
28
.5
7
60
.7
1
38
.8
6
U
M
C
25
.8
8
36
.7
4
30
.3
7
3.
47
56
.2
5
6.
55
30
.6
1
38
.6
4
34
.1
6
26
.8
1
40
.1
3
32
.1
4
Ta
bl
e
10
:
S
co
re
s
(i
n
%
)
w
it
h
al
te
rn
at
iv
e
an
sw
er
s,
di
st
in
gu
is
he
d
by
er
ro
r
ty
pe
.
If
a
te
am
in
di
ca
te
s
th
at
it
s
sy
st
em
do
es
no
th
an
dl
e
a
pa
rt
ic
ul
ar
er
ro
r
ty
pe
,i
ts
en
tr
y
fo
r
th
at
er
ro
r
ty
pe
is
m
ar
ke
d
as
?(
no
td
on
e)
?.
11
types. The evaluation data sets and scorer used
in our shared task serve as a benchmark for future
research on grammatical error correction4.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Daniel Dahlmeier and Hwee Tou Ng. 2011a. Cor-
recting semantic collocation errors with L1-induced
paraphrases. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 107?117.
Daniel Dahlmeier and Hwee Tou Ng. 2011b. Gram-
matical error correction with alternating structure
optimization. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics, pages 915?923.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
568?578.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
568?572.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English: The NUS Corpus of Learner English. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 22?31.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th EuropeanWorkshop on Natural Lan-
guage Generation, pages 242?249.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the 7th Workshop on the Innovative Use of
NLP for Building Educational Applications, pages
54?62.
4http://www.comp.nus.edu.sg/?nlp/conll13st.html
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth Conference on Language
Resources and Evaluation, pages 449?454.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing: A meta-classifier
approach. In Proceedings of the Annual Meeting of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan &
Claypool Publishers.
Diane Nicholls. 2003. The Cambridge Learner Cor-
pus: Error coding and analysis for lexicography and
ELT. In Proceedings of the Corpus Linguistics 2003
Conference, pages 572?581.
Alla Rozovskaya and Dan Roth. 2010. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
961?970.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The University of Illinois
system in the CoNLL-2013 shared task. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of the ACL 2010
Conference Short Papers, pages 353?358.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 180?189.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,
Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at 2013 CoNLL grammatical error
correction shared task. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task.
12
