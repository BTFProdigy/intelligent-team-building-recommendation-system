Semantic Parsing Based on FrameNet
Cosmin Adrian Bejan, Alessandro Moschitti, Paul Mora?rescu,
Gabriel Nicolae and Sanda Harabagiu
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
Abstract
This paper describes our method based on Support
Vector Machines for automatically assigning seman-
tic roles to constituents of English sentences. This
method employs four different feature sets, one of
which being first reported herein. The combination
of features as well as the extended training data we
considered have produced in the Senseval-3 experi-
ments an F1-score of 92.5% for the unrestricted case
and of 76.3% for the restricted case.
1 Introduction
The evaluation of the Senseval-3 task for Automatic
Labeling of Semantic Roles is based on the annota-
tions made available by the FrameNet Project (Baker
et al, 1998). The idea of automatically identifying
and labeling frame-specific roles, as defined by the
semantic frames, was first introduced by (Gildea and
Jurasfky, 2002). Each semantic frame is character-
ized by a set of target words which can be nouns,
verbs or adjectives. This helps abstracting the the-
matic roles and adding semantics to the given frame,
highlighting the characteristic semantic features.
Frames are characterized by (1) target words or
lexical predicates whose meaning includes aspects of
the frame; (2) frame elements (FEs) which represent
the semantic roles of the frame and (3) examples of
annotations performed on the British National Cor-
pus (BNC) for instances of each target word. Thus
FrameNet frames are schematic representations of
situations lexicalized by the target words (predicates)
in which various participants and conceptual roles
are related (the frame elements), exemplified by sen-
tences from the BNC in which the target words and
the frame elements are annotated.
In Senseval-3 two different cases of automatic la-
beling of the semantic roles were considered. The
Unrestricted Case requires systems to assign FE la-
bels to the test sentences for which (a) the bound-
aries of each frame element were given and the tar-
get words identified. The Restricted Case requires
systems to (i) recognize the boundaries of the FEs
for each evaluated frame as well as to (ii) assign a
label to it. Both cases can be cast as two different
classifications: (1) a classification of the role when
its boundaries are known and (2) a classification of
the sentence words as either belonging to a role or
not1.
A similar approach was used for automati-
cally identifying predicate-argument structures in
English sentences. The PropBank annotations
(www.cis.upenn.edu/?ace) enable training for two
distinct learning techniques: (1) decision trees (Sur-
deanu et al, 2003) and (2) Support Vector Machines
(SVMs) (Pradhan et al, 2004). The SVMs produced
the best results, therefore we decided to use the same
learning framework for the Senseval-3 task for Auto-
matic Labeling of Semantic Roles. Additionally, we
have performed the following enhancements:
? we created a multi-class classifier for each frame,
thus achieving improved accuracy and efficiency;
? we combined some new features with features from
(Gildea and Jurasfky, 2002; Surdeanu et al, 2003;
Pradhan et al, 2004);
? we resolved the data sparsity problem generated
by limited training data for each frame, when using
the examples associated with any other frame from
FrameNet that had at least one FE shared with each
frame that was evaluated;
? we crafted heuristics that improved mappings from
the syntactic constituents to the semantic roles.
We believe that the combination of these four exten-
sions are responsible for our results in Senseval-3.
The remainder of this paper is organized as fol-
lows. Section 2 describes our methods of classify-
ing semantic roles whereas Section 3 describes our
method of identifying role boundaries. Section 4 de-
tails our heuristics and Section 5 details the exper-
imental results. Section 6 summarizes the conclu-
sions.
1The second classification represents the detection of role
boundaries. The semantic parsing defined as two different clas-
sification tasks was introduced in (Gildea and Jurasfky, 2002).
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2 Semantic role classification
The result of the role classifier on a sentence, as il-
lustrated in Figure 1, is the identification of semantic
roles of the FEs when the boundaries of each FE are
known. To be able to assign the labels of each FE, we
used three sets of features. Feature Set 1, illustrated
in Figure 2 was used in the work reported in (Gildea
and Jurasfky, 2002).
VP
S
VP
NP
People were fastening
TARGETAgent
NP
a rope
PP
to the ring
NP
GoalItem
Figure 1: Sentence with annotated frame elements.
? POSITION (pos) ? Indicates if the constituent appears  
before or after the the predicate in the sentence.
? VOICE (voice) ? This feature distinguishes between
active or passive voice for the predicate phrase.
are preserved.
of the evaluated phrase. Case and morphological information
? HEAD WORD (hw) ? This feature contains the head word
? PARSE TREE PATH (path): This feature contains the path
in the parse tree between the predicate phrase and the 
labels linked by direction symbols (up or down), e.g.
? PHRASE TYPE (pt): This feature indicates the syntactic
noun phrases only, and it indicates if the NP is dominated
by a sentence phrase (typical for subject arguments with
active?voice predicates), or by a verb phrase (typical 
for object arguments).
? GOVERNING CATEGORY (gov) ? This feature applies to
type of the phrase labeled as a frame element, e.g.
target word, expressed as a sequence of nonterminal
? TARGET WORD ? In our implementation this feature
(2) LEMMA which represents the target normalized to lower 
the case and morphological information preserved; and 
consists of two components: (1) WORD: the word itself with
case and infinitive form for the verbs or singular for nouns. 
NP for Agent in Figure 1.
NP    S    VP    VP for Agent in Figure 1.
Figure 2: Feature Set 1 (FS1)
Feature Set 2 was introduced in (Surdeanu et al,
2003) and it is illustrated in Figure 3. The CON-
TENT WORD (cw) feature illustrated in Figure 3
applies to PPs, SBARs and VPs, as it was reported
in (Surdeanu et al, 2003). For example, if the PP is
?in the past month?, instead of using ?in?, the head
of the PP, as a feature, ?month?, the head of the NP
is selected since it is more informative. Similarly, if
the SBAR is ?that occurred yesterday?, instead of us-
ing the head ?that? we select ?occurred?, the head of
the VP. When the VP ?to be declared? is considered,
?declared? is selected over ?to?.
Feature set 3 is a novel set of features introduced
in this paper and illustrated in Figure 4. Some
of the new features characterize the frame, e.g.
the frame name (FRAME-NAME); the frame FEs,
(NUMBER-FEs); or the target word associated with
the frame (TAGET-TYPE). Additional characteriza-
tion of the FEs are provided by by the GRAMMATI-
CAL FUNCTION feature and by the list of grammat-
ical functions of all FEs recognized in each sentence(
LIST Grammatical Function feature).
BOOLEAN NAMED ENTITY FLAGS ? A feature set comprising: 
? neOrganization: set to 1 if an organization is recognized in the phrase
? neLocation: set to 1 a location is recognized in the phrase
? nePerson: set to 1 if a person name is recognized in the phrase
? neMoney: set to 1 if a currency expression is recognized in the phrase
? nePercent: set to 1 if a percentage expression is recognized in the phrase
? neTime: set to 1 if a time of day expression is recognized in the phrase
? neDate: set to 1 if a date temporal expression is recognized in the phrase 
word from the constituent, different from the head word.
? CONTENT WORD (cw) ? Lexicalized feature that selects an informative 
PART OF SPEECH OF HEAD WORD (hPos) ? The part of speech tag of
the head word.
PART OF SPEECH OF CONTENT WORD (cPos) ?The part of speech 
tag of the content word.
NAMED ENTITY CLASS OF CONTENT WORD (cNE) ? The class of 
the named entity that includes the content word
Figure 3: Feature Set 2 (FS2)
In FrameNet, sentences are annotated with the
name of the sub-corpus. There are 12,456 possi-
ble names of sub-corpus. For the 40 frames eval-
uated in Senseval-3, there were 1442 names asso-
ciated with the example sentences in the training
data and 2723 names in the test data. Three of
the most frequent sub-corpus names are: ?V-trans-
other? (frequency=613), ?N-all? (frequency=562)
and ?V-trans-simple?(frequency=560). The name
of the sub-corpus indicates the relations between
the target word and some of its FEs. For ex-
ample, the ?V-trans-other? name indicated that the
target word is a transitive verb, and thus its FEs
are likely to have other roles than object or indi-
rect object. A sentence annotated with this sub-
corpus name is: ?Night?s coming, you can see
the black shadow on [Self?mover the stones] that
[TARGET rush] [Pathpast] and [Pathbetween your
feet.?]. For this sentence both FEs with the role of
Path are neither objects or indirect objects of the
transitive verb.
Feature SUPPORT VERBS considers the usage of
support expressions in FrameNet. We have found
that whenever adjectives are target words, their se-
mantic interpretation depends on their co-occurrence
with verbs like ?take?, ?become? or ?is?. Support
verbs are defined as those verbs that combine with a
state-noun, event-noun or state-adjective to create a
verbal predicate, allowing arguments of the verb to
serve as FEs of the frame evoked by the noun or the
adjective.
The CORENESS feature takes advantage of a
more recent implementation concept of core FEs
(vs. non-core FEs) in FrameNet. More specifi-
cally, the FrameNet developers classify frame ele-
ments in terms of how central they are to a particular
frame, distinguishing three levels: core, peripheral
and extra-thematic.
The features were used to produce two types of
examples: positive and negative examples. For each
FE of a frame, aside from the positive examples ren-
dered by the annotations, we considered as negative
examples all the annotations of the other FEs for the
same frame. The positive and the negative examples
were used for training the multi-class classifiers.
SUPPORT_VERBS that are recognized for adjective or noun target words
target word. The values of this feature are either (1) The POS of the head
of the VP containing the target word or (2) NULL if the target word does
not belong to a VP
or ADJECTIVE
LIST_CONSTITUENT (FEs): This feature represents a list of the syntactic
Grammatical Function: This feature indicates whether the FE is:
? an External Argument (Ext)
? an Object (Obj)
? a Complement (Comp)
? a Modifier (Mod)
? Head noun modified by attributive adjective (Head)
? Genitive determiner (Gen)
? Appositive (Appos)
LIST_Grammatical_Function: This feature represents a list of the 
grammatical functions of the FEs recognized in the sentence.
in each sentence.
FRAME_NAME: This feature indicates the name of the semantic frame 
for which FEs are labeled
COVERAGE: This feature indicates whether there is a syntactic structure
in the parse tree that perfectly covers the FE
a conceptually necessary participant of a frame. For example, in the 
are: (1) core; (2) peripheral and (3) extrathemathic. FEs that mark notions
such as Time, Place, Manner and Degree are peripheral. Extrathematic
FEs situate an event against a backdrop of another event, by evoking 
a larger frame for which the target event fills a role.
SUB_CORPUS: In FrameNet, sentences are annotated with the name
of the subcorpus they belong to. For example, for a verb target word,
to a FE included in a relative clause headed by a wh?word.
(2) a hyponym of sense 1 of PERSON in WordNet
(1) a personal pronoun or
HUMAN: This feature indicates whether the syntactic phrase is either
TARGET?TYPE: the lexical class of the target word, e.g. VERB, NOUN
consituents covering each FE of the frame recognized in a sentence. 
For the example illustrated in Figure 1, the list is: [NP, NP, PP]
NUMBER_FEs: This feature indicates how many FEs were recognized 
have the role of predicate for the FEs. For example, if the target word is
"clever" in the sentence "Smith is very clever, but he?s no Einstein", the 
the FE "Smith" is an argument of the support verb "is"? rather than of the
CORENESS: This feature indicates whether the FE instantiates
REVENGE frame, Punishment is a core element. The values 
V?swh represents a subcorpus in which the target word is a predicate
Figure 4: Feature Set 3 (FS3)
Our multi-class classification allows each FE to be
initially labeled with more than one role when sev-
eral classifiers decide so. For example, for the AT-
TACHING frame, an FE may be labeled both as Goal
and as Item if the classifiers for the Goal and Item
select it as a possible role. To choose the final label,
we select the classification which was assigned the
largest score by the SVMs.
PARSE TREE PATH WITH UNIQUE DELIMITER ?  This feature removes
the direction in the path, e.g. VBN?VP?ADVP
PARTIAL PATH ? This feature uses only the path from the constituent to
the lowest common ancestor of the predicate and the constituent
FIRST WORD ? First word covered by constituent
FIRST POS ? POS of first word covered by constituent
LEFT CONSTITUENT ? Left sibling constituent label
RIGHT HEAD ? Right sibling head word
RIGHT POS HEAD ? Right sibling POS of head word
LAST POS ? POS of last word covered by the constituent
LEFT HEAD ? Left sibling head word
LEFT POS HEAD ? Left sibling POS of head word
RIGHT CONSTITUENT ? Right sibling constituent label
PP PREP ? If constituent is labeled PP get first word in PP
DISTANCE ? Distance in the tree from constituent to the target word
LAST WORD ? Last word covered by the constituent
Figure 5: Feature Set 4 (FS4)
3 Boundary Detection
The boundary detection of each FE was required
in the Restricted Case of the Senseval-3 evalua-
tion. To classify a word as belonging to an FE
or not, we used all the entire Feature Set 1 and
2. From the Feature Set 3 we have used only
four features: the Support- Verbs feature; the
Target-Type feature, the Frame-Name feature
and the Sub Corpus feature. For this task we have
also used Feature Set 4, which were first introduced
in (Pradhan et al, 2004). The Feature Set 4 is il-
lustrated in Figure 5. After the boundary detec-
tion was performed, the semantic roles of each FE
were assigned using the role classifier trained for the
Restricted Case
4 Heuristics
Frequently, syntactic constituents do not cover ex-
actly FEs. For the Unrestricted Case we imple-
mented a very simple heuristic: when there is no
parse-tree node that exactly covers the target role r
but a subset of adjacent nodes perfectly match r,
we merge them in a new NPmerge node. For the
Restricted Case, a heuristic for adjectival and nomi-
nal target words w adjoins consecutive nouns that are
in the same noun phrase as w.
5 Experimental Results
In the Senseval-3 task for Automatic Labeling of
Semantic Roles 24,558 sentences from FrameNet
were assigned for training while 8,002 for testing.
We used 30% of the training set (7367 sentences)
as a validation-set for selecting SVM parameters
that optimize accuracy. The number of FEs for
which labels had to be assigned were: 51,010 for
the training set; 15,924 for the validation set and
16,279 for the test set. We used an additional set
of 66,687 sentences (hereafter extended data) as ex-
tended data produced when using the examples as-
sociated with any other frame from FrameNet that
had at least one FE shared with any of the 40
frames evaluated in Senseval-3. These sentences
were parsed with the Collins? parser (Collins, 1997).
The classifier experiments were carried out using the
SVM-light software (Joachims, 1999) available at
http://svmlight.joachims.org/with a poly-
nomial kernel2 (degree=3).
5.1 Unrestricted Task Experiments
For this task we devised four different experiments
that used four different combination of features: (1)
FS1 indicates using only Feature Set 1; (2) +H in-
dicates that we added the heuristics; (3) +FS2+FS3
indicates that we add the feature Set 2 and 3; and
(4) +E indicates that the extended data has also been
used. For each of the four experiments we trained 40
multi-class classifiers, (one for each frame) for a total
of 385 binary role classifiers. The following Table il-
lustrates the overall performance over the validation-
set. To evaluate the results we measure the F1-score
by combining the precision P with the recall R in the
formula F1 = 2?P?RP+R .
FS1 +H +H+FS2+FS3 +H+FS2+FS3+E
84.4 84.9 91.7 93.1
5.2 Restricted Task Experiments
In order to find the best feature combination for this
task we carried out some preliminary experiments
over five frames. In Table 1, the row labeled B lists
the F1-score of boundary detection over 4 different
feature sets: FS1, +H, +FS4 and +E, the extended
data. The row labeled R lists the same results for the
whole Restricted Case.
Table 1: Restrictive experiments on validation-set.
+FS1 +H +H+FS2+FS3 +H+FS4+E
B 80.29 80.48 84.76 84.88
R 74.9 75.4 78 78.9
Table 1 illustrates the overall performance (bound-
ary detection and role classification) of automatic se-
mantic role labeling. The results listed in Tables 1
and 2 were obtained by comparing the FE bound-
aries identified by our parser with those annotated in
FrameNet. We believe that these results are more
2In all experiments and for any classifier, we used the default
SVM-light regularization parameter (e.g., C = 1 for normalized
kernels) and a cost-factor j = 100 to adjust the rate between
Precision and Recall.
indicative of the performance of our systems than
those obtained when using the scorer provided by
Senseval-3. When using this scorer, our results have
a precision of 89.9%, recall of 77.2% and an F1-
score of 83.07% for the Restricted Case.
Table 2: Results on the test-set.
Precision Recall F1
Unrestricted Case 94.5 90.6 92.5
Boundary Detection 87.3 75.1 80.7
Restricted Case 82.4 71.1 76.3
To generate the final Senseval-3 submissions we
selected the most accurate models (for unrestricted
and restricted tasks) of the validation experiments.
Then we re-trained such models with all training data
(i.e. our training plus validation data) and the set-
ting (parameters, heuristics and extended data) de-
rived over the validation-set. Finally, we run all clas-
sifiers on the test-set of the task. Table 2 illustrates
the final results for both sub-tasks.
6 Conclusions
In this paper we describe a method for automatically
labeling semantic roles based on support vector ma-
chines (SVMs). The training benefits from an ex-
tended data set on which multi-class classifiers were
derived. The polynomial kernel of the SVMs en-
able the combination of four feature sets that pro-
duced very good results both for the Restricted Case
and the Unrestricted Case. The paper also describes
some heuristics for mapping syntactic constituents
onto FEs.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceedings
of the COLING-ACL, Montreal, Canada.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of the
ACL-97, pages 16?23.,
Daniel Gildea and Daniel Jurasfky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):496?530.
T. Joachims. 1999. Making Large-Scale SVM Learning
Practical. In B. Schlkopf, C. Burges, and MIT-Press.
A. Smola (ed.), editors, Advances in Kernel Methods -
Support Vector Learning.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2004.
Support vector learning for semantic argument classifi-
cation. Journal of Machine Learning Research.
Mihai Surdeanu, Sanda M. Harabagiu, John Williams,
and John Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceedings
of (ACL-03).
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 275?283,
Sydney, July 2006. c?2006 Association for Computational Linguistics
BESTCUT: A Graph Algorithm for Coreference Resolution
Cristina Nicolae and Gabriel Nicolae
Human Language Technology Research Institute
Department of Computer Science
University of Texas at Dallas
Richardson, TX 75083-0688
{cristina, gabriel}@hlt.utdallas.edu
Abstract
In this paper we describe a coreference
resolution method that employs a classi-
fication and a clusterization phase. In a
novel way, the clusterization is produced
as a graph cutting algorithm, in which
nodes of the graph correspond to the men-
tions of the text, whereas the edges of the
graph constitute the confidences derived
from the coreference classification. In ex-
periments, the graph cutting algorithm for
coreference resolution, called BESTCUT,
achieves state-of-the-art performance.
1 Introduction
Recent coreference resolution algorithms tackle
the problem of identifying coreferent mentions of
the same entity in text as a two step procedure: (1)
a classification phase that decides whether pairs of
noun phrases corefer or not; and (2) a clusteriza-
tion phase that groups together all mentions that
refer to the same entity. An entity is an object or
a set of objects in the real world, while a men-
tion is a textual reference to an entity1. Most of
the previous coreference resolution methods have
similar classification phases, implemented either
as decision trees (Soon et al, 2001) or as maxi-
mum entropy classifiers (Luo et al, 2004). More-
over, these methods employ similar feature sets.
The clusterization phase is different across current
approaches. For example, there are several linking
decisions for clusterization. (Soon et al, 2001) ad-
vocate the link-first decision, which links a men-
tion to its closest candidate referent, while (Ng and
Cardie, 2002) consider instead the link-best deci-
sion, which links a mention to its most confident
1This definition was introduced in (NIST, 2003).
candidate referent. Both these clustering decisions
are locally optimized. In contrast, globally opti-
mized clustering decisions were reported in (Luo
et al, 2004) and (DaumeIII and Marcu, 2005a),
where all clustering possibilities are considered by
searching on a Bell tree representation or by us-
ing the Learning as Search Optimization (LaSO)
framework (DaumeIII and Marcu, 2005b) respec-
tively, but the first search is partial and driven by
heuristics and the second one only looks back in
text. We argue that a more adequate clusterization
phase for coreference resolution can be obtained
by using a graph representation.
In this paper we describe a novel representa-
tion of the coreference space as an undirected
edge-weighted graph in which the nodes repre-
sent all the mentions from a text, whereas the
edges between nodes constitute the confidence
values derived from the coreference classification
phase. In order to detect the entities referred in
the text, we need to partition the graph such that
all nodes in each subgraph refer to the same entity.
We have devised a graph partitioning method for
coreference resolution, called BESTCUT, which is
inspired from the well-known graph-partitioning
algorithm Min-Cut (Stoer and Wagner, 1994).
BESTCUT has a different way of computing the
cut weight than Min-Cut and a different way of
stopping the cut2. Moreover, we have slightly
modified the Min-Cut procedures. BESTCUT re-
places the bottom-up search in a tree representa-
tion (as it was performed in (Luo et al, 2004))
with the top-down problem of obtaining the best
partitioning of a graph. We start by assuming that
all mentions refer to a single entity; the graph cut
splits the mentions into subgraphs and the split-
2Whenever a graph is split in two subgraphs, as defined in
(Cormen et al, 2001), a cut of the graph is produced.
275
ting continues until each subgraph corresponds to
one of the entities. The cut stopping decision has
been implemented as an SVM-based classification
(Cortes and Vapnik, 1995).
The classification and clusterization phases as-
sume that all mentions are detected. In order to
evaluate our coreference resolution method, we
have (1) implemented a mention detection proce-
dure that has the novelty of employing information
derived from the word senses of common nouns as
well as selected lexico-syntactic information; and
(2) used a maximum entropy model for corefer-
ence classification. The experiments conducted on
MUC and ACE data indicate state-of-the-art results
when compared with the methods reported in (Ng
and Cardie, 2002) and (Luo et al, 2004).
The remainder of the paper is organized as fol-
lows. In Section 2 we describe the coreference
resolution method that uses the BESTCUT cluster-
ization; Section 3 describes the approach we have
implemented for detecting mentions in texts; Sec-
tion 4 reports on the experimental results; Section
5 discusses related work; finally, Section 6 sum-
marizes the conclusions.
2 BESTCUT Coreference Resolution
For each entity type (PERSON, ORGANIZATION,
LOCATION, FACILITY or GPE3) we create a graph
in which the nodes represent all the mentions
of that type in the text, the edges correspond to
all pairwise coreference relations, and the edge
weights are the confidences of the coreference re-
lations. We will divide this graph repeatedly by
cutting the links between subgraphs until a stop
model previously learned tells us that we should
stop the cutting. The end result will be a partition
that approximates the correct division of the text
into entities.
We consider this graph approach to clustering a
more accurate representation of the relations be-
tween mentions than a tree-based approach that
treats only anaphora resolution, trying to connect
mentions with candidate referents that appear in
text before them. We believe that a correct reso-
lution has to tackle cataphora resolution as well,
by taking into account referents that appear in the
text after the anaphors. Furthermore, we believe
that a graph representation of mentions in a text is
more adequate than a tree representation because
the coreference relation is symmetrical in addi-
3Entity types as defined by (NIST, 2003).
tion to being transitive. A greedy bottom-up ap-
proach does not make full use of this property. A
graph-based clusterization starts with a complete
overall view of all the connections between men-
tions, therefore local errors are much less proba-
ble to influence the correctness of the outcome. If
two mentions are strongly connected, and one of
them is strongly connected with the third, all three
of them will most probably be clustered together
even if the third edge is not strong enough, and that
works for any order in which the mentions might
appear in the text.
2.1 Learning Algorithm
The coreference confidence values that become
the weights in the starting graphs are provided by
a maximum entropy model, trained on the train-
ing datasets of the corpora used in our experi-
ments. For maximum entropy classification we
used a maxent4 tool. Based on the data seen, a
maximum entropy model (Berger et al, 1996) of-
fers an expression (1) for the probability that there
exists coreference C between a mention mi and a
mention mj .
P (C|mi,mj) =
e(
?
k ?kgk(mi,mj ,C))
Z(mi,mj)
(1)
where gk(mi,mj , C) is a feature and ?k is its
weight; Z(mi,mj) is a normalizing factor.
We created the training examples in the same
way as (Luo et al, 2004), by pairing all men-
tions of the same type, obtaining their feature
vectors and taking the outcome (coreferent/non-
coreferent) from the key files.
2.2 Feature Representation
We duplicated the statistical model used by (Luo
et al, 2004), with three differences. First, no fea-
ture combination was used, to prevent long run-
ning times on the large amount of ACE data. Sec-
ond, through an analysis of the validation data, we
implemented seven new features, presented in Ta-
ble 1. Third, as opposed to (Luo et al, 2004), who
represented all numerical features quantized, we
translated each numerical feature into a set of bi-
nary features that express whether the value is in
certain intervals. This transformation was neces-
sary because our maximum entropy tool performs
better on binary features. (Luo et al, 2004)?s fea-
tures were not reproduced here from lack of space;
please refer to the relevant paper for details.
4http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
276
Category Feature name Feature description
lexical head-match true if the two heads are identical
type-pair for each mention: name? its type, noun? NOUN , pronoun? its spelling
name-alias true if a mention is an alias of the other one
syntactic same-governing-category true if both mentions are covered by the same type of node, e.g. NP, VP, PP
path the parse tree path from m2 to m1
coll-comm true if either mention collocates with a communication verb
grammatical gn-agree true if the two mentions agree in gender and number
Table 1: The added features for the coreference model.
2.3 Clusterization Method: BESTCUT
We start with five initial graphs, one for each en-
tity type, each containing all the mentions of that
type and their weighted connections. This initial
division is correct because no mentions of differ-
ent entity types will corefer. Furthermore, by do-
ing this division we avoid unnecessary confusion
in the program?s decisions and we decrease its run-
ning time. Each of these initial graphs will be cut
repeatedly until the resulting partition is satisfac-
tory. In each cut, we eliminate from the graph the
edges between subgraphs that have a very weak
connection, and whose mentions are most likely
not part of the same entity.
Formally, the graph model can be defined as fol-
lows. Let M = {mi : 1..n} be n mentions in the
document and E = {ej : 1..m} be m entities. Let
g : M ? E be the map from a mention mi ? M
to an entity ej ? E. Let c : MxM ? [0, 1] be the
confidence the learning algorithm attaches to the
coreference between two mentions mi,mj ? M .
Let T = {tk : 1..p} be the set of entity types
or classes. Then we attach to each entity class tk
an undirected, edge-weighted graph Gk(Vk, Ek),
where Vk = {mi|g(mi).type = tk} and Ek =
{(mi,mj , c(mi,mj))|mi,mj ? Vk}.
The partitioning of the graph is based at each
step on the cut weight. As a starting point, we
used the Min-Cut algorithm, presented and proved
correct in (Stoer and Wagner, 1994). In this simple
and efficient method, the weight of the cut of a
graph into two subgraphs is the sum of the weights
of the edges crossing the cut. The partition that
minimizes the cut weight is the one chosen. The
main procedure of the algorithm computes cuts-
of-the-phase repeatedly and selects the one with
the minimum cut value (cut weight). We adapted
this algorithm to our coreference situation.
To decide the minimum cut (from here on called
the BESTCUT), we use as cut weight the number
of mentions that are correctly placed in their set.
The method for calculating the correctness score is
presented in Figure 1. The BESTCUT at one stage
is the cut-of-the-phase with the highest correctness
score.
cut-weight(Graph G, Cut C = (S,T))
1 corrects-avg ? corrects-max ? 0
2 foreach m ? G.V
3 if m ? S.V then setm ? S
4 else setm ? T
7 if avgn?setm.V,n6=mweight(m,n) >
avgn?G.V \setm.V weight(m,n)
6 then corrects-avg++
7 if maxn?setm.V,n6=mweight(m,n) >
maxn?G.V \setm.V weight(m,n)
8 then corrects-max++
9 return (corrects-avg +
corrects-max) / 2
Figure 1: Computing the cut-weight.
An additional learning model was trained to de-
cide if cutting a set of mentions is better or worse
than keeping the mentions together. The model
was optimized to maximize the ECM-F score5. We
will denote by S the larger part of the cut and T
the smaller one. C.E is the set of edges crossing
the cut, and G is the current graph before the cut.
S.V and T.V are the set of vertexes in S and in
T , respectively. S.E is the set of edges from S,
while T.E is the set of edges from T . The features
for stopping the cut are presented in Table 2. The
model was trained using 10-fold cross-validation
on the training set. In order to learn when to stop
the cut, we generated a list of positive and nega-
tive examples from the training files. Each train-
ing example is associated with a certain cut (S, T ).
Since we want to learn a stop function, the positive
examples must be examples that describe when the
cut must not be done, and the negative examples
are examples that present situations when the cut
must be performed. Let us consider that the list
of entities from a text is E = {ej : 1..m} with
ej = {mi1 ,mi2 , ...mik} the list of mentions that
refer to ej . We generated a negative example for
each pair (S = {ei}, T = {ej}) with i 6= j ?
each entity must be separated from any other en-
5As introduced by (Luo et al, 2004).
277
Feature name Feature description
st-ratio |S.V |/|T.V | ? the ratio between the cut
parts
ce-ratio |C.E|/|G.E| ? the proportion of the cut
from the entire graph
c-min min(C.E) ? the smallest edge crossing
the cut
c-max max(C.E) ? the largest edge crossing
the cut
c-avg avg(C.E) ? the average of the edges
crossing the cut
c-hmean hmean(C.E) ? the harmonic mean of
the edges crossing the cut
c-hmeax hmeax(C.E) ? a variant of the har-
monic mean. hmeax(C.E) = 1 ?
hmean(C.E?) where each edge from
E? has the weight equal to 1 minus the
corresponding edge from E
lt-c-avg-ratio how many edges from the cut are less
than the average of the cut (as a ratio)
lt-c-hmean-
ratio
how many edges from the cut are less
than the harmonic mean of the cut (as a
ratio)
st-avg avg(S.E + T.E) ? the average of the
edges from the graph when the edges
from the cut are not considered
g-avg avg(G.E) ? the average of the edges
from the graph
st-wrong-avg-
ratio
how many vertexes are in the wrong part
of the cut using the average measure for
the ?wrong? (as a ratio)
st-wrong-
max-ratio
how many vertexes are in the wrong part
of the cut using the max measure for the
?wrong? (as a ratio)
lt-c-avg-ratio
< st-lt-c-avg-
ratio
1 if r1 < r2, 0 otherwise; r1 is the ratio
of the edges from C.E that are smaller
than the average of the cut; r2 is the ratio
of the edges from S.E + T.E that are
smaller than the average of the cut
g-avg > st-
avg
1 if the avg(G.E) > avg(S.E + T.E),
and 0 otherwise
Table 2: The features for stopping the cut.
tity. We also generated negative examples for all
pairs (S = {ei}, T = E \ S) ? each entity must
be separated from all the other entities considered
together. To generate positive examples, we simu-
lated the cut on a graph corresponding to a single
entity ej . Every partial cut of the mentions of ej
was considered as a positive example for our stop
model.
We chose not to include pronouns in the BEST-
CUT initial graphs, because, since most features
are oriented towards Named Entities and common
nouns, the learning algorithm (maxent) links pro-
nouns with very high probability to many possi-
ble antecedents, of which not all are in the same
chain. Thus, in the clusterization phase the pro-
nouns would act as a bridge between different en-
tities that should not be linked. To prevent this,
we solved the pronouns separately (at the end of
BESTCUT(Graph Gi)
1 entities.clear()
2 queue.push back(Gi)
3 while not queue.empty()
4 G ? queue.pop front()
5 (S,T) ? ProposeCut(G)
6 if StopTheCut(G,S,T)
7 then
8 entities.add(NewEntity(G))
9 else
10 queue.push back(S)
11 queue.push back(T)
12 return entities
Figure 2: The general algorithm for BESTCUT.
the BESTCUT algorithm) by linking them to their
antecedent with the best coreference confidence.
Figure 2 details the main procedure of the
BESTCUT algorithm. The algorithm receives as
input a weighted graph having a vertex for each
mention considered and outputs the list of entities
created. In each stage, a cut is proposed for all
subgraphs in the queue. In case StopTheCut de-
cides that the cut must be performed on the sub-
graph, the two sides of the cut are added to the
queue (lines 10-11); if the graph is well connected
and breaking the graph in two parts would be a
bad thing, the current graph will be used to cre-
ate a single entity (line 8). The algorithm ends
when the queue becomes empty. ProposeCut (Fig-
ProposeCut(Graph G)
1 while |G.V | > 1
2 (S,T) ? ProposeCutPhase(G)
3 if the cut-of-the-phase (S,T)
is-lighter than the current
best cut (Sb, Tb)
4 then store the cut-of-the-phase
as (Sb, Tb)
5 return (Sb, Tb)
Figure 3: The algorithm for ProposeCut.
ure 3) returns a cut of the graph obtained with
an algorithm similar to the Min-Cut algorithm?s
procedure called MinimumCut. The differences
between our algorithm and the Min-Cut proce-
dure are that the most tightly connected vertex
in each step of the ProposeCutPhase procedure, z,
is found using expression 2:
z = argmaxy 6?Awa(A, y) (2)
where wa(A, y) = 1|A|
?
x?A w(x, y), and the is-
lighter test function uses the correctness score
presented before: the partial cut with the larger
correctness score is better. The ProposeCutPhase
function is presented in Figure 4.
278
ProposeCutPhase(Graph G)
1 A ? {G.V.first}
2 while |A| < |G.V |
3 last ? the most tightly
connected vertex
4 add last to A
5 store the cut-of-the-phase and
shrink G by merging the two
vertexes added last
6 return (G.V \ {last}, last)
Figure 4: The algorithm for ProposeCutPhase.
2.4 An Example
Let us consider an example of how the BESTCUT
algorithm works on two simple sentences (Fig-
ure 5). The entities present in this example are:
{Mary1, the girl5} and {a brother2, John3, The
boy4}. Since they are all PERSONs, the algorithm
Mary1 has a brother2, John3. The boy4 is older than
the girl5.
Figure 5: An example.
will be applied on a single graph, corresponding to
the class PERSON and composed of all these men-
tions.
The initial graph is illustrated in Figure 6, with
the coreference relation marked through a differ-
ent coloring of the nodes. Each node number cor-
responds to the mention with the same index in
Figure 5.
     
     
     
     
     





2
1
     
     
     
     
     





4
5
     
     
     
     
     





3
0.20.1
0.6
0.5
0.1
0.7 0.5
Figure 6: The initial graph
The strongest confidence score is between a
brother2 and John3, because they are connected
through an apposition relation. The graph was
simplified by eliminating the edges that have an
insignificant weight, e.g. the edges between John3
and the girl5 or between Mary1 and a brother2.
Function BESTCUT starts with the whole graph.
The first cut of the phase, obtained by function
ProposeCutPhase, is the one in Figure 7.a. This
     
     
     
     
     





2
1
      
      
      
      
      





4
5
     
     
     
     
     





3
2b) Cut  = ({1, 4, 5}, {2, 3})
    Score(Cut )= 42
0.2
0.7
0.6
0.5
0.1
0.5
0.1
     
     
     
     
     





2
1
     
     
     
     
     





4
5
     
     
     
     
     





3
c) Cut  = ({1, 5}, {2, 3, 4})3
    Score(Cut ) = 5; Cut  = BestCut3 3
0.20.1
0.6
0.5
0.1
0.7 0.5
     
     
     
     
     





2
1
      
      
      
      
      





4
5
     
     
     
     
     





3
4d) Cut  = ({1}, {2, 3, 4, 5})
    Score(Cut ) = 3.54
0.2
0.7
0.6
0.5
0.1
0.5
0.1
     
     
     
     
     





2
1
     
     
     
     
     





4
5
     
     
     
     
     





3
1
    Score(Cut ) = 3
a) Cut  = ({1, 3, 4, 5}, {2})
1
0.20.1
0.6
0.5
0.1
0.5
0.7
Figure 7: Cuts-of-the-phase
cut separates node 2 from the rest of the graph.
In calculating the score of the cut (using the algo-
rithm from Figure 1), we obtain an average num-
ber of three correctly placed mentions. This can
be verified intuitively on the drawing: mentions
1, 2 and 5 are correctly placed, while 3 and 4 are
not. The score of this cut is therefore 3. The sec-
ond, the third and the fourth cuts of the phase, in
Figures 7.b, 7.c and 7.d, have the scores 4, 5 and
3.5 respectively. An interesting thing to note at
the fourth cut is that the score is no longer an in-
teger. This happens because it is calculated as an
average between corrects-avg = 4 and corrects-
max = 3. The methods disagree about the place-
ment of mention 1. The average of the outgo-
ing weights of mention 1 is 0.225, less than 0.5
(the default weight assigned to a single mention)
therefore the first method declares it is correctly
placed. The second considers only the maximum;
0.6 is greater than 0.5, so the mention appears to
be more strongly connected with the outside than
the inside. As we can see, the contradiction is be-
cause of the uneven distribution of the weights of
the outgoing edges.
The first proposed cut is the cut with the great-
279
FACILITY ORGANIZATION PERSON LOCATION GPE
POWER#9
PERSON#1 PEOPLE#1
CHARACTER#1
...
expert#1
Peter_Pan#2
womankind#1
population#1
homeless#2
...
...
...
...
...
...
......
Frankenstein#2 oil_tycoon#1worker#1
Figure 8: Part of the hierarchy containing 42 WordNet equivalent concepts for the five entity types, with
all their synonyms and hyponyms. The hierarchy has 31,512 word-sense pairs in total
est score, which is Cut3 (Figure 7.c). Because this
is also the correct cut, all cuts proposed after this
one will be ignored? the machine learning algo-
rithm that was trained when to stop a cut will al-
ways declare against further cuts. In the end, the
cut returned by function BESTCUT is the correct
one: it divides mentions Mary1 and the girl5 from
mentions a brother2, John3 and The boy4.
3 Mention Detection
Because our BESTCUT algorithm relies heavily
on knowing entity types, we developed a method
for recognizing entity types for nominal mentions.
Our statistical approach uses maximum entropy
classification with a few simple lexical and syn-
tactic features, making extensive use of WordNet
(Fellbaum, 1998) hierarchy information. We used
the ACE corpus, which is annotated with men-
tion and entity information, as data in a super-
vised machine learning method to detect nominal
mentions and their entity types. We assigned six
entity types: PERSON, ORGANIZATION, LOCA-
TION, FACILITY, GPE and UNK (for those who are
in neither of the former categories) and two gener-
icity outcomes: GENERIC and SPECIFIC. We
only considered the intended value of the mentions
from the corpus. This was motivated by the fact
that we need to classify mentions according to the
context in which they appear, and not in a general
way. Only contextual information is useful further
in coreference resolution. We have experimentally
discovered that the use of word sense disambigua-
tion improves the performance tremendously (a
boost in score of 10%), therefore all the features
use the word senses from a previously-applied
word sense disambiguation program, taken from
(Mihalcea and Csomai, 2005).
For creating training instances, we associated
an outcome to each markable (NP) detected in the
training files: the markables that were present in
the key files took their outcome from the key file
annotation, while all the other markables were as-
sociated with outcome UNK. We then created a
training example for each of the markables, with
the feature vector described below and as target
function the outcome. The aforementioned out-
come can be of three different types. The first type
of outcome that we tried was the entity type (one
member of the set PERSON, ORGANIZATION, LO-
CATION, FACILITY, GPE and UNK); the second
type was the genericity information (GENERIC or
SPECIFIC), whereas the third type was a combi-
nation between the two (pairwise combinations
of the entity types set and the genericity set, e.g.
PERSON SPECIFIC).
The feature set consists of WordNet features,
lexical features, syntactic features and intelligent
context features, briefly described in Table 3. With
the WordNet features we introduce the WordNet
equivalent concept. A WordNet equivalent con-
cept for an entity type is a word-sense pair from
WordNet whose gloss is compatible with the def-
inition of that entity type. Figure 8 enumerates a
few WordNet equivalent concepts for entity class
PERSON (e.g. CHARACTER#1), with their hier-
archy of hyponyms (e.g. Frankenstein#2). The
lexical feature is useful because some words are
almost always of a certain type (e.g. ?com-
pany?). The intelligent context set of features
are an improvement on basic context features that
use the stems of the words that are within a win-
dow of a certain size around the word. In addi-
tion to this set of features, we created more fea-
tures by combining them into pairs. Each pair
contains two features from two different classes.
For instance, we will have features like: is-a-
280
Category Feature name Feature description
WordNet is-a-TYPE true if the mention is of entity type TYPE; five features
WN-eq-concept-hyp true if the mention is in hyponym set of WN-eq-concept; 42 features
WN-eq-concept-syn true if the mention is in synonym set of WN-eq-concept; 42 features
lexical stem-sense pair between the stem of the word and the WN sense of the word by the WSD
syntactic pos part of speech of the word by the POS tagger
is-modifier true if the mention is a modifier in another noun phrase
modifier-to-TYPE true if the mention is a modifier to a TYPE mention
in-apposition-with TYPE of the mention our mention is in apposition with
intelligent context all-mods the nominal, adjectival and pronominal modifiers in the mention?s parse tree
preps the prepositions right before and after the mention?s parse tree
Table 3: The features for the mention detection system.
PERSON?in-apposition-with(PERSON).
All these features apply to the ?true head? of
a noun phrase, i.e. if the noun phrase is a parti-
tive construction (?five students?, ?a lot of com-
panies?, ?a part of the country?), we extract the
?true head?, the whole entity that the part was
taken out of (?students?, ?companies?, ?coun-
try?), and apply the features to that ?true head?
instead of the partitive head.
For combining the mention detection module
with the BESTCUT coreference resolver, we also
generated classifications for Named Entities and
pronouns by using the same set of features minus
the WordNet ones (which only apply to nominal
mentions). For the Named Entity classifier, we
added the feature Named-Entity-type as obtained
by the Named Entity Recognizer. We generated
a list of all the markable mentions and their en-
tity types and presented it as input to the BEST-
CUT resolver instead of the list of perfect men-
tions. Note that this mention detection does not
contain complete anaphoricity information. Only
the mentions that are a part of the five consid-
ered classes are treated as anaphoric and clus-
tered, while the UNK mentions are ignored, even
if an outside anaphoricity classifier might catego-
rize some of them as anaphoric.
4 Experimental Results
The clusterization algorithms that we imple-
mented to evaluate in comparison with our method
are (Luo et al, 2004)?s Belltree and Link-Best
(best-first clusterization) from (Ng and Cardie,
2002). The features used were described in section
2.2. We experimented on the ACE Phase 2 (NIST,
2003) and MUC6 (MUC-6, 1995) corpora. Since
we aimed to measure the performance of corefer-
ence, the metrics used for evaluation are the ECM-
F (Luo et al, 2004) and the MUC P, R and F scores
(Vilain et al, 1995).
In our first experiment, we tested the three
coreference clusterization algorithms on the
development-test set of the ACE Phase 2 corpus,
first on true mentions (i.e. the mentions annotated
in the key files), then on detected mentions (i.e.
the mentions output by our mention detection sys-
tem presented in section 3) and finally without any
prior knowledge of the mention types. The results
obtained are tabulated in Table 4. As can be ob-
served, when it has prior knowledge of the men-
tion types BESTCUT performs significantly bet-
ter than the other two systems in the ECM-F score
and slightly better in the MUC metrics. The more
knowledge it has about the mentions, the better it
performs. This is consistent with the fact that the
first stage of the algorithm divides the graph into
subgraphs corresponding to the five entity types. If
BESTCUT has no information about the mentions,
its performance ranks significantly under the Link-
Best and Belltree algorithms in ECM-F and MUC
R. Surprisingly enough, the Belltree algorithm, a
globally optimized algorithm, performs similarly
to Link-Best in most of the scores.
Despite not being as dramatically affected as
BESTCUT, the other two algorithms also decrease
in performance with the decrease of the mention
information available, which empirically proves
that mention detection is a very important module
for coreference resolution. Even with an F-score
of 77.2% for detecting entity types, our mention
detection system boosts the scores of all three al-
gorithms when compared to the case where no in-
formation is available.
It is apparent that the MUC score does not vary
significantly between systems. This only shows
that none of them is particularly poor, but it is not
a relevant way of comparing methods? the MUC
metric has been found too indulgent by researchers
((Luo et al, 2004), (Baldwin et al, 1998)). The
MUC scorer counts the common links between the
281
MUC score
Clusterization algorithm Mentions ECM-F% MUC P% MUC R% MUC F%
BESTCUT key 82.7 91.1 88.2 89.63
detected 73.0 88.3 75.1 81.17
undetected 41.2 52.0 82.4 63.76
Belltree (Luo et al, 2004) key 77.9 88.5 89.3 88.90
detected 70.8 86.0 76.6 81.03
undetected 52.6 40.3 87.1 55.10
Link-Best (Ng and Cardie, 2002) key 77.9 88.0 90.0 88.99
detected 70.7 85.1 77.3 81.01
undetected 51.6 39.6 88.5 54.72
Table 4: Comparison of results between three clusterization algorithms on ACE Phase 2. The learning
algorithms are maxent for coreference and SVM for stopping the cut in BESTCUT. In turn, we obtain
the mentions from the key files, detect them with our mention detection algorithm or do not use any
information about them.
annotation keys and the system output, while the
ECM-F metric aligns the detected entities with the
key entities so that the number of common men-
tions is maximized. The ECM-F scorer overcomes
two shortcomings of the MUC scorer: not consid-
ering single mentions and treating every error as
equally important (Baldwin et al, 1998), which
makes the ECM-F a more adequate measure of
coreference.
Our second experiment evaluates the impact
that the different categories of our added features
have on the performance of the BESTCUT sys-
tem. The experiment was performed with a max-
ent classifier on the MUC6 corpus, which was pri-
orly converted into ACE format, and employed
mention information from the key annotations.
MUC score
Model ECM-F% P% R% F%
baseline 78.3 89.5 91.5 90.49
+grammatical 78.4 89.2 92.5 90.82
+lexical 83.1 92.4 91.6 92.00
+syntactic 85.1 92.7 92.4 92.55
Table 5: Impact of feature categories on BEST-
CUT on MUC6. Baseline system has the (Luo et
al., 2004) features. The system was tested on key
mentions.
From Table 5 we can observe that the lexi-
cal features (head-match, type-pair, name-alias)
have the most influence on the ECM-F and MUC
scores, succeeded by the syntactic features (same-
governing-category, path, coll-comm). Despite
what intuition suggests, the improvement the
grammatical feature gn-agree brings to the system
is very small.
5 Related Work
It is of interest to discuss why our implementa-
tion of the Belltree system (Luo et al, 2004) is
comparable in performance to Link-Best (Ng and
Cardie, 2002). (Luo et al, 2004) do the clus-
terization through a beam-search in the Bell tree
using either a mention-pair or an entity-mention
model, the first one performing better in their ex-
periments. Despite the fact that the Bell tree is a
complete representation of the search space, the
search in it is optimized for size and time, while
potentially losing optimal solutions? similarly to
a Greedy search. Moreover, the fact that the two
implementations are comparable is not inconceiv-
able once we consider that (Luo et al, 2004) never
compared their system to another coreference re-
solver and reported their competitive results on
true mentions only.
(Ng, 2005) treats coreference resolution as a
problem of ranking candidate partitions generated
by a set of coreference systems. The overall per-
formance of the system is limited by the perfor-
mance of its best component. The main differ-
ence between this approach and ours is that (Ng,
2005)?s approach takes coreference resolution one
step further, by comparing the results of multiple
systems, while our system is a single resolver; fur-
thermore, he emphasizes the global optimization
of ranking clusters obtained locally, whereas our
focus is on globally optimizing the clusterization
method inside the resolver.
(DaumeIII and Marcu, 2005a) use the Learning
as Search Optimization framework to take into ac-
count the non-locality behavior of the coreference
features. In addition, the researchers treat men-
tion detection and coreference resolution as a joint
problem, rather than a pipeline approach like we
282
do. By doing so, it may be easier to detect the
entity type of a mention once we have additional
clues (expressed in terms of coreference features)
about its possible antecedents. For example, label-
ing Washington as a PERSON is more probable af-
ter encountering George Washington previously in
the text. However, the coreference problem does
not immediately benefit from the joining.
6 Conclusions
We have proposed a novel coreference clusteri-
zation method that takes advantage of the effi-
ciency and simplicity of graph algorithms. The
approach is top-down and globally optimized, and
takes into account cataphora resolution in addition
to anaphora resolution. Our system compares fa-
vorably to two other implemented coreference sys-
tems and achieves state-of-the-art performance on
the ACE Phase 2 corpus on true and detected men-
tions. We have also briefly described our mention
detection system whose output we used in con-
junction with the BESTCUT coreference system to
achieve better results than when no mention infor-
mation was available.
Acknowledgments
We would like to thank the three anonymous re-
viewers for their very helpful suggestions and
comments on the early draft of our paper.
References
B. Baldwin, T. Morton, A. Bagga, J. Baldridge,
R. Chandraseker, A. Dimitriadis, K. Snyder, and
M. Wolska. 1998. Description of the university of
pennsylvania camp system as used for coreference.
In Proceedings of the 7th Message Understanding
Conference (MUC-7).
A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra. 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 1(22):39?
71.
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and
C. Stein. 2001. Introduction to Algorithms, Second
Edition. MIT.
C. Cortes and V. Vapnik. 1995. Support-vector net-
works. Machine Learning, 20(3):273?297.
H. DaumeIII and D. Marcu. 2005a. A large-scale ex-
ploration of effective global features for a joint en-
tity detection and tracking model. pages 97?104,
Vancouver.
H. DaumeIII and D. Marcu. 2005b. Learning as search
optimization: Approximate large margin methods
for structured prediction. In The International Con-
ference on Machine Learning (ICML).
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database and Some of its Applications. MIT
Press.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics, Barcelona, Spain.
R. Mihalcea and A. Csomai. 2005. Senselearner:
Word sense disambiguation for all words in unre-
stricted text. In Proceedings of the 43rd Meeting of
the Association for Computational Linguistics, Ann
Arbor, MI.
MUC-6. 1995. Coreference task definition.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Pro-
ceedings of the 4Oth Meeting of the Association for
Computational Linguistics, Philadelphia, Pennsyl-
vania.
V. Ng. 2004. Learning noun phrase anaphoricity to
improve conference resolution: Issues in representa-
tion and optimization. In Proceedings of the 42nd
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain.
V. Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In
Proceedings of the 43rd Meeting of the Association
for Computational Linguistics, pages 157?164, Ann
Arbor, MI.
NIST. 2003. Entity detection and tracking - phase
1; edt and metonymy annotation guidelines. version
2.5.1 20030502.
M. Pasca and S. Harabagiu. 2001. High performance
question/answering. In Proceedings of the 24th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 366?374, New Orleans, LA.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A
machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics,
4(27):521?544.
M. Stoer and F. Wagner. 1994. A simple min cut al-
gorithm. In Jan van Leeuwen, editor, Proceedings of
the 1994 European Symposium on Algorithms, pages
141?147, New York. Springer-Verlag.
M. Vilain, J.Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6), pages
45?52.
283
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 119?124,
Prague, June 2007. c?2007 Association for Computational Linguistics
Textual Entailment Through Extended Lexical Overlap and
Lexico-Semantic Matching
Rod Adams, Gabriel Nicolae, Cristina Nicolae and Sanda Harabagiu
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, Texas
{rod, gabriel, cristina, sanda}@hlt.utdallas.edu
Abstract
This paper presents two systems for textual
entailment, both employing decision trees
as a supervised learning algorithm. The
first one is based primarily on the con-
cept of lexical overlap, considering a bag of
words similarity overlap measure to form a
mapping of terms in the hypothesis to the
source text. The second system is a lexico-
semantic matching between the text and the
hypothesis that attempts an alignment be-
tween chunks in the hypothesis and chunks
in the text, and a representation of the text
and hypothesis as two dependency graphs.
Their performances are compared and their
positive and negative aspects are analyzed.
1 Introduction
Textual entailment is the task of taking a pair of pas-
sages, referred to as the text and the hypothesis, and
labeling whether or not the hypothesis (H) can be
fully inferred from the text (T), as is illustrated in
Pair 1. In Pair 1, the knowledge that an attorney rep-
resenting someone?s interests entails that they work
for that person.
Pair 1 (RTE2 IE 58)
T: ?A force majeure is an act of God,? said attorney Phil
Wittmann, who represents the New Orleans Saints and owner
Tom Benson?s local interests.
H: Phil Wittmann works for Tom Benson.
The Third PASCAL Recognizing Textual Entail-
ment Challenge1 follows the experience of the sec-
1http://www.pascal-network.org/Challenges/RTE3/
ond challenge (Bar-Haim et al, 2006), whose main
task was to automatically detect if a hypothesis H
is entailed by a text T. To increase the ?reality? of
the task, the text-hypothesis examples were taken
from outputs of actual systems that solved appli-
cations like Question Answering (QA), Informa-
tion Retrieval (IR), Information Extraction (IE) and
Summarization (SUM).
In the challenge, there are two corpora, each con-
sisting of 800 annotated pairs of texts and hypothe-
ses. Pairs are annotated as to whether there exists
a positive entailment between them and from which
application domain each example came from. In-
stances are distributed evenly among the four tasks
in both corpora, as are the positive and negative ex-
amples. One corpus was designated for development
and training, while the other was reserved for test-
ing.
In the Second PASCAL RTE Challenge (Bar-
Haim et al, 2006), one of the best performing sub-
missions was (Adams, 2006), which focused on
strict lexical methods so that the system could re-
main relatively simple and be easily applied to var-
ious entailment applications. However, this simple
approach did not take into account details like the
syntactic structure, the coreference or the semantic
relations between words, all necessary for a deeper
understanding of natural language text. Thus, a new
system, based on the same decision tree learning al-
gorithm, was designed in an attempt to gain perfor-
mance by adding alignment and dependency rela-
tions information. The two systems will be com-
pared and their advantages and disadvantages dis-
cussed.
119
This paper is organized as follows: The first sys-
tem is discussed in Section 2, followed by the sec-
ond system in Section 3. The experimental results
are presented in Section 4, and the paper concludes
in Section 5.
2 Textual entailment through extended
lexical overlap
The first system (Adams, 2006) follows a four step
framework. The first step is a tokenization process
that applies to the content words of the text and
hypothesis. The second step is building a ?token
map? of how the individual tokens in the hypoth-
esis are tied to those in the text, as explained in
Section 2.1. Thirdly, several features, as described
in Section 2.2, are extracted from the token map.
Finally, the extracted features are fed into Weka?s
(Witten and Frank, 2005) J48 decision tree for train-
ing and evaluation.
2.1 The token map
Central to this system is the concept of the token
map. This map is inspired by (Glickman et al,
2005)?s use of the most probable lexical entailment
for each hypothesis pair, but has been modified in
how each pair is evaluated, and that the mapping
is stored for additional extraction of features. The
complete mapping is a list of (Hi, Tj) mappings,where Hi represents the ith token in the hypothesis,and Tj is similarly the jth token in the text. Eachmapping has an associated similarity score. There is
one mapping per token in the hypothesis. Text to-
kens are allowed to appear in multiple mappings.
The mappings are created by considering each hy-
pothesis token and comparing it to each token in the
text and keeping the one with the highest similarity
score.
Similarity scores A similarity score ranging from
0.0 to 1.0 is computed for any two tokens via a com-
bination of two scores. This score can be thought of
as the probability that the text token implies the hy-
pothesis one, even though the methods used to pro-
duce it were not strictly probabilistic in nature.
The first score is derived from the cost of a Word-
Net (Fellbaum, 1998) path. The WordNet paths
between two tokens are built with the method re-
ported in (Hirst and St-Onge, 1998), and designated
as SimWN (Hi, Tj). Exact word matches are al-ways given a score of 1.0, words that are morpho-
logically related or that share a common sense are
0.9 and other paths give lower scores down to 0.0.
This method of obtaining a path makes use of three
groups of WordNet relations: Up (e.g. hypernym,
member meronym), Down (e.g. hyponym, cause)
and Horizontal (e.g. nominalization, derivation).
The path can only follow certain combinations of
these groupings, and assigns penalties for each link
in the path, as well as for changing from one direc-
tion group to another.
The secondary scoring routine is the lexical en-
tailment probability, lep(u, v), from (Glickman et
al., 2005). This probability is estimated by taking
the page counts returned from the AltaVista2 search
engine for a combined u and v search term, and di-
viding by the count for just the v term. This can be
precisely expressed as:
SimAV (Hi, Tj) =
AVCount(Hi &Tj)
AVCount(Tj)
The two scores are combined such that the sec-
ondary score can take up whatever slack the domi-
nant score leaves available. The exact combination
is:
Sim(Hi, Tj) = SimWN (Hh, Tt)
+ ? ? (1 ? SimWN (Hh, Tt)) ? SimAV (Hh, Tt)
where ? is a tuned constant (? ? [0, 1]). Empirical
analysis found the best results with very low values
of ?3. This particular combination was chosen over
a strict linear combination, so as to more strongly re-
late to SimWN when it?s values are high, but allow
SimAV to play a larger role when SimWN is low.
2.2 Feature extraction
The following three features were constructed from
the token map for use in the training of the decision
tree, and producing entailment predictions.
Baseline score This score is the product of the
similarities of the mapped pairs, and is an extension
of (Glickman et al, 2005)?s notion of P (H|T ). This
2http://www.av.com
3The results reported here used ? = 0.1
120
is the base feature of entailment.
ScoreBASE =
?
(Hi,Tj)?Map
Sim(Hi, TJ )
One notable characteristic of this feature is that
the overall score can be no higher than the lowest
score of any single mapping. The failure to locate a
strong similarity for even one token will produce a
very low base score.
Unmapped negations A token is considered un-
mapped if it does not appear in any pair of the token
map, or if the score associated with that mapping is
zero. A token is considered a negation if it is in a set
list of terms such as no or not. Both the text and
the hypothesis are searched for unmapped negations,
and total count of them is kept, with the objective of
determining whether there is an odd or even num-
ber of them. A (possibly) modified, or flipped, score
feature is generated:
n = # of negations found.
ScoreNEG =
{
ScoreBASE if n is even,
1 ? ScoreBASE if n is odd.
Task The task domain used for evaluating entail-
ment (i.e. IE, IR, QA or SUM) was also used as a
feature to allow different thresholds among the do-
mains.
3 Textual entailment through
lexico-semantic matching
This second system obtains the probability of entail-
ment between a text and a hypothesis from a su-
pervised learning algorithm that incorporates lexi-
cal and semantic information extracted from Word-
Net and PropBank. To generate learning examples,
the system computes features that are based upon
the alignment between chunks from the text and the
hypothesis. In the preliminary stage, each instance
pair of text and hypothesis is processed by a chunker.
The resulting chunks can be simple tokens or com-
pound words that exist in WordNet, e.g., pick up.
They constitute the lexical units in the next stages of
the algorithm.
identity 1.0 coreference 0.8
synonymy 0.8 antonymy -0.8
hypernymy 0.5 hyponymy -0.5
meronymy 0.4 holonymy -0.4
entailment 0.6 entailed by -0.6
cause 0.6 caused by -0.6
Table 2: Alignment relations and their scores.
3.1 Alignment
Once all the chunks have been identified, the sys-
tem searches for alignment candidates between the
chunks of the hypothesis and those of the text. The
search pairs all the chunks of the hypothesis, in turn,
with all the text chunks, and for each pair it ex-
tracts all the relations between the two nodes. Stop
words and auxiliary verbs are discarded, and only
two chunks with the same part of speech are com-
pared (a noun must be transformed into a verb to
compare it with another verb). The alignments ob-
tained in this manner constitute a one-to-many map-
ping between the chunks of the hypothesis and the
chunks of the text.
The following relations are identified: (a) iden-
tity (between the original spellings, lowercase forms
or stems), (b) coreference and (c) WordNet relations
(synonymy, antonymy, hypernymy, meronymy, en-
tailment and causation). Each of these relations is
attached to a score between -1 and 1, which is hand-
crafted by trial and error on the development set (Ta-
ble 2).
The score is positive if the relation from the text
word to the hypothesis word is compatible with an
entailment, e.g., identity, coreference, synonymy,
hypernymy, meronymy, entailment and causation,
and negative in the opposite case, e.g., antonymy,
hyponymy, holonymy, reverse entailment and re-
verse causation. This is a way of quantifying in-
tuitions like: ?The cat ate the cake? entails ?The
animal ate the cake?. To identify these relations,
no word sense disambiguation is performed; instead,
all senses from WordNet are considered. Negations
present in text or hypothesis influence the sign of
the score; for instance, if a negated noun is aligned
with a positive noun through a negative link like
antonymy, the two negations cancel each other and
the score of the relation will be positive. The score
of an alignment is the sum of the scores of all the
121
det
arrested
num
arrested
policeman
BelgianA posing
dealer
Swedes
Three
Swedes
Three
were
Belgiana
det
amod partmod
nsubj
prep?as
dobj
art
nsubjpass
num
auxpass
prep_in
det amod
sting operationpolice
an Brussels
prep_in
Figure 1: The dependency graphs and alignment candidates for Pair 2 (RTE3 SUM 633).
Category Feature name Feature description
alignment (score) totaligscore the total alignment score (sum of all scores)
totminaligscore the total alignment score when considering only the minimum scored relation
for any two chunks aligned
totmaxaligscore the total alignment score when considering only the maximum scored relation
for any two chunks aligned
alignment (count) allaligs the number of chunks aligned considering all alignments
posaligs the number of chunks aligned considering only positive alignments
negaligs the number of chunks aligned considering only negative alignments
minposaligs the number of alignments that have the minimum of their scores positive
maxposaligs the number of alignments that have the maximum of their scores positive
minnegaligs the number of alignments that have the minimum their scores negative
maxnegaligs the number of alignments that have the maximum of their scores negative
dependency edgelabels the pair of labels of non matching edges
match the number of relations that match when comparing the two edges
nonmatch the number of relations that don?t match when comparing the two edges
Table 1: Features for lexico-semantic matching.
relations between the two words, and if the sum is
positive, the alignment is considered positive.
3.2 Dependency graphs
The system then creates two dependency graphs, one
for the text and one for the hypothesis. The de-
pendency graphs are directed graphs with chunks as
nodes, interconnected by edges according to the re-
lations between them, which are represented as edge
labels. The tool used is the dependency parser de-
veloped by (de Marneffe et al, 2006), which as-
signs some of 48 grammatical relations to each pair
of words within a sentence. Further information
is added from the predicate-argument structures in
PropBank, e.g., a node can be the ARG0 of another
node, which is a predicate.
Because the text can have more than one sentence,
the dependency graphs for each of the sentences are
combined into a larger one. This is done by col-
lapsing together nodes (chunks) that are coreferent,
identical or in an nn relation (as given by the parser).
The relations between the original nodes and the rest
of the nodes in the text (dependency links) and nodes
in the hypothesis (alignment links) are all inherited
by the new node. Again, each edge can have multi-
ple relations as labels.
3.3 Features
With the alignment candidates and dependency
graphs obtained in the previous steps, the system
computes the values of the feature set. The features
used are of two kinds (Table 1):
(a) The alignment features are based on the scores
and counts of the candidate alignments. All the
scores are represented as real numbers between -1
and 1, normalized by the number of concepts in the
hypothesis.
(b) The dependency features consider each posi-
tively scored aligned pair with each of the other pos-
itively scored aligned pairs, and compare the set of
122
detBerlinguer
succeeded
nsubj
Natta
dobj
elected drew
1984 party Berlinguer
deathsecretary he/Natta 1969 up report
proposingthe
expulsion party
the groupthe
auxpass
was
prep_in nn
prep_after
nsubjpass
poss
nsubj
prep_in
dobj
det partmod
dobj prep_from
det det prep_of
prep_as prt
Manifesto
the
Figure 2: The dependency graphs and alignment candidates for Pair 3 (RTE3 IE 19).
relations between the two nodes in the text with the
set of relations between the two nodes in the hypoth-
esis. This comparison is performed on the depen-
dency graphs, on the edges that immediately connect
the two text chunks and the two hypothesis chunks,
respectively. They have numerical values between 0
and 1, normalized by the square of the total number
of aligned chunks.
3.4 Examples
Pair 2 (RTE3 SUM 633)
T: A Belgian policeman posing as an art dealer in Brussels ar-
rested three Swedes.
H: Three Swedes were arrested in a Belgian police sting opera-
tion.
Figure 1 illustrates the dependency graphs and align-
ment candidates extracted for the instance in Pair 2.
There is no merging of graphs necessary here, be-
cause the text is made up of a single sentence. The
vertical line in the center divides the graph corre-
sponding to the text from the one corresponding to
the hypothesis. The dependency relations in the two
graphs are represented as labels of full lines, while
the alignment candidate pairs are joined by dotted
lines. As can be observed, the alignment was done
based on identity of spelling, e.g., Swedes-Swedes,
and stem, e.g., policeman-police. For the sake of
simplicity, the predicate-argument relations have not
been included in the drawing. This is a case of a pos-
itive instance, and the dependency and alignment re-
lations strongly support the entailment.
Pair 3 (RTE3 IE 19)
T: In 1969, he drew up the report proposing the expulsion from
the party of the Manifesto group. In 1984, after Berlinguer?s
death, Natta was elected as party secretary.
H: Berlinguer succeeded Natta.
Figure 2 contains an example of a negative in-
stance (Pair 3) that cannot be solved through the
simple analysis of alignment and dependency rela-
tions. The graphs corresponding to the two sen-
tences of the text have been merged into a single
graph because of the coreference between the pro-
noun he in the first sentence and the proper name
Natta in the second one. This merging has enriched
the overall information about relations, but the algo-
rithm does not take advantage of this. To correctly
solve this problem of entailment, one needs addi-
tional information delivered by a temporal relations
system. The chain of edges between Berlinguer and
Natta in the text graph expresses the fact that the
event of Natta?s election happened after Berlinguer?s
death. Since the hypothesis states that Berlinguer
succeeded Natta, the entailment is obviously false.
The system presented in this section will almost cer-
tainly solve this kind of instance incorrectly.
4 Results
The experimental results are summarized in Ta-
bles 3 and 4. The first table presents the accu-
racy scores obtained by running the two systems
through 10-fold crossvalidation on incremental RTE
datasets. The first system, based on extended lexical
overlap (ELO), almost consistently outperforms the
second system, lexico-semantic matching (LSM),
123
Evaluation set ELO LSM ELO+LSM
J48 J48 J48 JRip
RTE3Dev 66.38 63.63 65.50 67.50
+RTE2Dev 64.38 59.19 61.56 62.50
+RTE1Dev 62.11 56.67 60.36 59.62
+RTE2Test 61.04 57.77 61.51 61.20
+RTE1Test 60.07 56.57 59.04 60.42
Table 3: Accuracy for the two systems on various
datasets.
Task IE IR QA SUM All
Accuracy 53.50 73.50 80.00 61.00 67.00
Table 4: Accuracy by task for the Extended Lexical
Overlap system tested on the RTE3Test corpus.
and the combination of the two. The only case
when the combination gives the best score is on the
RTE3 development set, using the rule-based classi-
fier JRip. It can be observed from the table that the
more data is added to the evaluation set, the poorer
the results are. This can be explained by the fact that
each RTE dataset covers a specific kind of instances.
Because of this variety in the data, the results ob-
tained on the whole collection of RTE datasets avail-
able are more representative than the results reported
on each set, because they express the way the sys-
tems would perform in real-life natural language
processing as opposed to an academic setup.
Since the ELO system was clearly the better of
the two, it was the one submitted to the Third PAS-
CAL Challenge evaluation. Table 4 contains the
scores obtained by the system on the RTE3 testing
set. The overall accuracy is 67%, which represents
an increase from the score the system achieved at the
Second PASCAL Challenge (62.8%). The task with
the highest performance was Question Answering,
while the task that ranked the lowest was Informa-
tion Extraction. This is understandable, since IE in-
volves a very deep understanding of the text, which
the ELO system is not designed to do.
5 Conclusions
This paper has presented two different approaches of
solving textual entailment: one based on extended
lexical overlap and the other on lexico-semantic
matching. The experiments have shown that the first
approach, while simpler in concept, yields a greater
performance when applied on the PASCAL RTE3
development set. At first glance, it seems puzzling
that a simple approach has outperformed one that
takes advantage of a deeper analysis of the text.
However, ELO system treats the text naively, as a
bag of words, and does not rely on any preprocess-
ing application. The LSM system, while attempting
an understanding of the text, uses three other sys-
tems that are not perfect: the coreference resolver,
the dependency parser and the semantic parser. The
performance of the LSM system is limited by the
performance of the tools it uses. It will be of interest
to evaluate this system again once they increase in
accuracy.
References
Rod Adams. 2006. Textual entailment through extended
lexical overlap. In The Second PASCAL Recognising
Textual Entailment Challenge (RTE-2).
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-ment challenge. In PASCAL RTE Challenge.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-pendency parses from phrase structure parses. In 5th
International Conference on Language Resources and
Evaluation (LREC 2006).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005.Web based probabilistic textual entailment. In PAS-
CAL RTE Challenge.
Graeme Hirst and David St-Onge. 1998. Lexical chainsas representations of context for the detection and cor-rection of malapropisms. In Christiane Fellbaum, ed-
itor, WordNet: An electronic lexical database, pages305?332. The MIT Press.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, 2nd edition.
124
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 454?459,
Prague, June 2007. c?2007 Association for Computational Linguistics
UTD-HLT-CG: Semantic Architecture for Metonymy Resolution and
Classification of Nominal Relations
Cristina Nicolae, Gabriel Nicolae and Sanda Harabagiu
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, Texas
{cristina, gabriel, sanda}@hlt.utdallas.edu
Abstract
In this paper we present a semantic archi-
tecture that was employed for processing
two different SemEval 2007 tasks: Task
4 (Classification of Semantic Relations be-
tween Nominals) and Task 8 (Metonymy
Resolution). The architecture uses multi-
ple forms of syntactic, lexical, and semantic
information to inform a classification-based
approach that generates a different model for
each machine learning algorithm that imple-
ments the classification. We used decision
trees, decision rules, logistic regression and
lazy classifiers. A voting module selects the
best performing module for each task evalu-
ated in SemEval 2007. The paper details the
results obtained when using the semantic ar-
chitecture.
1 Introduction
Automatic semantic interpretations of natural lan-
guage text rely on (1) semantic theories that cap-
ture the subtleties employed by human communi-
cations; (2) lexico-semantic resources that encode
various forms of semantic knowledge; and (3) com-
putational methods that model the selection of the
optimal interpretation derived from the textual data.
Two of the SemEval 2007 tasks, namely Task 4
(Classification of Semantic Relations between Nom-
inals)and Task 8 (Metonymy Resolution) employed
distinct theories for the interpretation of their cor-
responding semantic phenomena, but, nevertheless,
they also shared several lexico-semantic resources,
and, furthermore, both these tasks could have been
cast as classification problems, in vein with most of
the recent work in computational semantic process-
ing. Based on this observation, we have designed
and implemented a semantic architecture that was
used in both tasks. In Section 2 of this paper we
give a brief description of the semantic theories cor-
responding to each of the two tasks, while in Section
3 we detail the semantic architecture. Section 4 de-
scribes the experimental results and evaluation.
We have used three lexico-semantic resources: (i)
the WordNet lexico-semantic database; (ii) VerbNet;
and (iii) the Lexical Conceptual Structure (LCS)
database. Used only by Task 4, WordNet is a lexico-
semantic database created at Princeton University1
(Fellbaum, 1998), which encodes a vast majority
of the English nouns, verbs, adjectives and adverbs,
and groups synonym words into synsets. VerbNet2
is a broad-coverage, comprehensive verb lexicon
created at University of Pennsylvania, compatible
with WordNet, but with explicitly stated syntactic
and semantic information, using Levin verb classes
(Levin, 1993) to systematically construct lexical en-
tities. Classes are hierarchically organized and each
class in the hierarchy has its corresponding syntac-
tic frames, semantic predicates and a list of typical
verb arguments. The Lexical Conceptual Structure
(Traum and Habash, 2000) is a compositional ab-
straction with language-independent properties. An
LCS is a directed graph with a root. Each node is as-
sociated with certain information, including a type, a
primitive and a field. An LCS captures the semantics
1http://wordnet.princeton.edu
2http://verbs.colorado.edu/verb-index/verbnet-2.1.tar.gz
454
Relation Positive example
1. CAUSE-EFFECT Earplugs relieve the discomfort from traveling with a cold allergy or sinus condition.
2. INSTRUMENT-AGENCY The judge hesitates, gavel poised, shooting them a warning look.
3. PRODUCT-PRODUCER The boy who made the threat was arrested, charged, and had items confiscated from his home.
4. ORIGIN-ENTITY Cinnamon oil is distilled from bark chips and used to alleviate stomach upsets.
5. THEME-TOOL The port scanner is a utility to scan a system to get the status of the TCP.
6. PART-WHOLE The granite benches are former windowsills from the Hearst Memorial Mining Building.
7. CONTENT-CONTAINER The kitchen holds patient drinks and snacks.
Table 1: Examples of semantic relations.
of a lexical item through a combination of semantic
structure and semantic content.
2 Semantic Tasks
The two semantic tasks addressed in this paper
are: Classification of Semantic Relations between
Nominals (Task 4), defined in (Girju et al, 2007)
and Metonymy Resolution (Task 8), defined in
(Markert and Nissim, 2007). Please refer to these
task description papers for more details. Both are
cast as classification tasks: given an unlabeled in-
stance, a system must label it according to one class
of a set specific to each task.
The training and testing datasets for the
metonymy resolution task are annotated in an
XML format. There are 1090 training and 842
testing instances for companies, and 941 training
and 908 testing instances for locations. Each
training instance corresponds to a context in
which a single name is annotated with its read-
ing (metonymic/literal/mixed) and, in case of
metonymy, its type (metotype). The testing dataset
for this task is annotated in a similar manner, only
the reading of the name is left unknown and must
be decided by the system.
For the classification of semantic relations be-
tween nominals, there exist seven training sets of
140 instances each for the seven semantic relations,
and seven corresponding testing sets of around 70
instances each. A training instance is annotated with
information about the boundaries of the two nom-
inals whose relation must be determined, the truth
value of their relation, the WordNet sense of each
nominal, and the query that was employed by the an-
notators to retrieve this example from the Web. The
testing instances are similar, with the only difference
being that the truth value of the relations is unknown
and must be determined.
3 Semantic Architecture
The semantic architecture that we have designed
is illustrated in Figure 1, which contains the basic
modules and resources used in the various phases of
processing the input data towards the final submis-
sion format. The grayed-out modules are all used
only for the semantic relations classification task,
while the part of the figure represented by dotted
lines appears only in the metonymy resolution al-
gorithm. The input to the system, for both tasks,
comprises the annotated instances, either from the
training or the testing dataset. Before any feature is
extracted, the data passes through a pipeline of pre-
processing modules. The text is first split into tokens
in a heuristic manner. The resulting tokenized text is
given as input to Brill?s part of speech tagger3 , which
associates each word with its part of speech (e.g.,
NN, PRP). The data further goes through Collins?
syntactic parser4, which builds the syntactic trees for
all the sentences in the text.
Additionally, for semantic relations classification,
the system creates the dependency structures for
all the sentences, using the dependency parser built
at Stanford5 and described in (de Marneffe et al,
2006). The dependency parser extracts some of 48
grammatical relations for each pair of words in a
sentence. A second module that is specific only to
this task is (Surdeanu and Turmo, 2005)?s seman-
tic role labeler, which extracts the shallow seman-
tic structure for each sentence, that is, the predicates
and their arguments.
In order to extract the features for the machine
learning algorithm, the modules described above
are used, and, in addition, information from Word-
Net, VerbNet and the LCS Database is incorporated,
3http://www.cs.jhu.edu/?brill/
4http://people.csail.mit.edu/mcollins/code.html
5http://nlp.stanford.edu/downloads/lex-parser.shtml
455
Parser
PropBank
WordNet
Generation
Models
Extraction
Feature
Selection
Feature
Voting
Submission
Generation
m
o
de
l 2
m
o
de
l 1
m
o
de
l k
VerbNet LCS Database
Parser
Dependency
POS Tagger
Tokenizer
Parser
Syntactic
Handcrafted ...
INSTANCES
INSTANCES
ANNOTATED
Figure 1: Semantic architecture.
Category Feature name Feature description
syntactic prevpos part of speech of previous word in the sentence
nextpos part of speech of next word in the sentence
determiner if the word has a determiner
prepgoverning if the word is governed by a prepositional phrase (PP), we extract the preposition
insidequotes if the word is inside quotes
lemmapost if the word is postmodifier for a noun, take the lemma of the noun
lemmapre if the word is premodifier for a noun, take the lemma of the noun
possession if the word is a possessor, and what it possesses
semantic role the role(s) of the name in the sentence: subject, object, under PP
rolelemma the combination between the role and the lemma of the verb whose argument the word is
rolevn same as above, but using the VerbNet class instead of the verb?s lemma
rolelevin same as above, but using the Levin class instead of the verb?s lemma
rolelcs same as above, but using LCS primitives from the LCS database instead of the verb?s lemma
Table 2: Features for metonymy resolution.
along with other features, based on the manual an-
notations for both the training and testing datasets
by the task organizers. These other features use the
grammatical annotations for the possibly metonymic
name, in the case of metonymy resolution, and the
query that was used to retrieve that particular in-
stance and the disambiguated WordNet sense for the
two nominals, in the case of semantic relations clas-
sification.
The features implemented for the two tasks are
described in Tables 2 and 3. Their types are: syn-
tactic, semantic, lexical and other. The syntactic
features express the relationships between the tar-
get words and words from the rest of the sentence
(e.g., the part of speech of the previous word in the
sentence, or the dependency relations between two
words). The semantic features make use of the in-
formation given by the resources used by the system
(e.g., the VerbNet class of the verb whose argument
the word is, or the lexicographic category of a word
in WordNet). The lexical feature is the lemma of the
word. The other feature is the query provided by
Task 4.
Using these sets of features, a number of models
were generated by different machine learning tech-
niques included with the Weka data mining software
(Witten and Frank, 2005). The machine learning
classifiers comprise decision trees, decision rules,
logistic regression, and ?lazy? classifiers like k-
nearest-neighbor. Because of too many features gen-
erated for a relatively small training dataset, feature
selection is performed by Weka before creating the
models. Metonymy resolution uses in addition the
entire set of features, since the dataset has seven
times more instances than the other task. For the
classification of semantic relations, the initial total
and the number of features that remain after the se-
lection are printed in Table 4.
For metonymy resolution, there are six sub-
tasks to be resolved, which result from all
combinations between organization/location and
coarse/medium/fine granularity of the label. For the
classification of nominal relations, there are 28 sub-
tasks, resulting from the processing of the seven se-
456
Category Feature name Feature description
syntactic dependency the dependency relations between the two words
modifier if one word is a modifier of the other
prepositions the prepositions immediately before and after both words
determiners the determiners of the two words
pattern the simplified pattern that exists in the sentence between the two words
lexical lemmas the lemmas of the words
semantic predicates the predicates whose arguments the two words are
predtypes the predicate types of the predicates above
samepred if the two words are arguments of the same predicate, which one that is
lexname the lexicographic category of each word in WordNet
hyponym if one word is a hyponym of the other in WordNet
partof if one word is a part of the other in WordNet
shareholonym if the two words share a holonym in WordNet
shareparent if the two words share a parent in WordNet
other query the query that was used by the annotators to retrieve the training example from the Web
Table 3: Features for classification of semantic relations between nominals.
R1 R2 R3 R4 R5 R6 R7
before 682 1200 913 898 861 849 677
after 13 19 10 15 15 8 16
Table 4: The number of features before and af-
ter Weka selection, for each semantic relation
dataset: R1 CAUSE-EFFECT, R2 INSTRUMENT-
AGENCY, R3 PRODUCT-PRODUCER, R4 ORIGIN-
ENTITY, R5 THEME-TOOL, R6 PART-WHOLE, and
R7 CONTENT-CONTAINER.
mantic relations, in which four experiments are con-
ducted, each with an increasing number of train-
ing instances. We treated each subtask as a sepa-
rate classification problem. Its training set and fea-
tures are fed into Weka to create several models.
Each classification algorithm mentioned before is
employed to obtain one model. For each subtask,
the voting module selects the best performing model
on 10-fold crossvalidation, which is used to classify
the test instances. These annotated instances make
up the submission dataset for that particular subtask.
To note is that the coarse metonymic level and the
semantic relations classification are binary classifi-
cations, while the rest of the metonymic subtasks
are multi-class classifications, performed in a single
stage.
4 Experimental Results and Evaluation
Both the metonymy resolution system and the sys-
tem for classification of semantic relations per-
formed well in the SemEval 2007 competition. The
Base type Coarse Medium Fine BA
Locations 84.1 84.0 82.2 79.4
Organizations 73.9 71.1 71.1 61.8
Table 5: Accuracy for the metonymy resolution sys-
tem at three granularity levels.
Base type Reading P R F BA
Locations literal 88.2 92.4 90.2 79.4
non-literal 64.1 52.4 57.6 20.6
Organizations literal 75.8 84.8 80.0 61.8
non-literal 69.6 56.2 62.2 38.2
Table 6: Performance for the metonymy resolution
system for the coarse level.
experiments presented in this paper were done on
the training and testing datasets for each subtask. To
note is that no other training data was collected or
used than the one provided by the organizers.
4.1 Results for Metonymy Resolution
This system was scored by measuring its accuracy at
three granularity levels (coarse, medium, and fine)
and the precision, recall and F score for all com-
binations of locations/organizations and literal/non-
literal. These results are tabulated in Tables 5, 6, 7
and 8.
All results are compared with the baseline accu-
racy values (BA). In Table 5, the baselines are com-
puted by taking all readings to be literal; for the rest,
the baseline is the percentage in the gold test data
of each reading. As can be observed, the readings
457
Base type Reading P R F BA
Locations literal 87.8 93.5 90.5 79.4
mixed 0.0 0.0 0.0 2.2
metonymic 63.6 52.3 58.0 18.4
Organizations literal 74.3 90.0 81.4 61.8
mixed 28.6 13.1 18.0 7.2
metonymic 66.8 47.1 55.3 31.0
Table 7: Performance for the metonymy resolution
system for the medium level.
Base type Reading P R F BA
Loc literal 85.7 94.6 89.9 79.4
mixed 0.0 0.0 0.0 2.2
othermet 0.0 0.0 0.0 1.2
obj-for-name 0.0 0.0 0.0 0.0
obj-for-repr 0.0 0.0 0.0 0.0
place-for-people 57.1 45.4 50.6 15.5
place-for-event 0.0 0.0 0.0 1.1
place-for-prod 0.0 0.0 0.0 0.1
Org literal 74.4 90.4 81.6 61.8
mixed 50.0 3.33 6.25 7.1
othermet 0.0 0.0 0.0 1.0
obj-for-name 80.0 66.7 72.7 0.7
obj-for-repr 0.0 0.0 0.0 0.0
org-for-members 61.3 64.0 62.6 19.1
org-for-event 0.0 0.0 0.0 0.1
org-for-prod 60.6 29.9 40.0 8.0
org-for-fac 0.0 0.0 0.0 1.9
org-for-index 0.0 0.0 0.0 0.4
Table 8: Performance for the metonymy resolution
system for the fine level.
for locations were more reliably identified than the
ones for companies. An explanation for this differ-
ence in performance lies in the fact that locations, in
their literal readings, are inactive entities, whereas in
their non-literal readings they are very often active,
especially in the annotated instances of the training
dataset. This cannot be said for organizations? they
can be active in their literal readings. The active vs.
inactive criterion, therefore, functions better for lo-
cations. Furthermore, since the training set contains
a ratio literals/non-literals of 1.7 for organizations
and 3.9 for locations, the models were skewed, iden-
tifying literal readings more easily than non-literal
ones, as shown in Table 6.
4.2 Results for Classification of Semantic
Relations between Nominals
This task?s performance was measured by accuracy,
precision, recall and F-measure, the latter constitut-
Semantic relation P R F Acc Inst
Cause-Effect 65.5 87.8 75.0 70.0 80
Instrument-Agency 68.3 73.7 70.9 70.5 78
Product-Producer 66.7 96.8 78.9 65.6 93
Origin-Entity 62.9 61.1 62.0 66.7 81
Theme-Tool 70.0 24.1 35.9 64.8 71
Part-Whole 55.6 76.9 64.5 69.4 72
Content-Container 82.4 36.8 50.9 63.5 74
Average 67.3 65.3 62.6 67.2 78.4
Avg baseline 81.3 42.9 56.2 57.0 78.4
Table 9: Performance of the semantic relations clas-
sification system for each semantic relation.
ing the score for ranking the systems in the com-
petition. Table 9 presents these scores by seman-
tic relation. The column entitled ?Inst? contains the
number of instances in the testing sets correspond-
ing to each relation. The average baseline values
were computed by guessing the label to be the ma-
jority in the dataset for each relation. From this table
it can be observed that the PRODUCT-PRODUCER,
INSTRUMENT-AGENCY, and CAUSE-EFFECT rela-
tions were detected with a relatively very high per-
formance score, whereas the THEME-TOOL relation
classification yielded a relatively small score. This
can be explained as the effect of their specifications;
the three best-ranked relations are well-defined by
human standards, while the THEME-TOOL relation
is more ambiguous.
Table 10 contains the scores of the 10-fold cross-
validation experiments that were performed on the
training dataset in order to select the best classifi-
cation algorithm. The classifiers used in these ex-
periments were, in the order of appearance in the
table: JRip, Random Forest, ADTree, Logistic Re-
gression, IBk, and Random Tree. The Logistic Re-
gression classifier was chosen in the vast majority of
cases, because it achieved the highest score for six
out of the seven relations. For R6, PART-WHOLE,
Random Forest was preferred. This ranking between
the scores of classifying relations, done consider-
ing training accuracy only, does not however antic-
ipate the final F score ranking in Table 9. In par-
ticular, the crossvalidation accuracy of R5, THEME-
TOOL, is better than the accuracy for R3, PRODUCT-
PRODUCER, which came first in the final results,
whereas R5 came last and at a large distance from
the others. These lower-than-expected results in the
458
Alg R1 R2 R3 R4 R5 R6 R7
JRip 72.1 76.4 68.6 66.4 68.6 66.4 73.6
RandF 78.6 85.0 72.1 77.1 74.3 70.7 73.6
ADTree 72.9 79.3 70.0 70.7 70.7 68.6 69.3
LogReg 79.3 85.7 72.1 80.0 76.4 70.0 75.7
IBk 78.6 83.6 70.7 75.7 74.3 70.0 72.1
RandT 79.3 85.7 71.4 77.1 75.0 70.0 72.1
Table 10: Results on 10-fold crossvalidation for
each relation and each classifier.
evaluation were caused in part by the drastic feature
selection module that was applied before generating
the models. In experiments performed on the de-
velopment data, the accuracy on 10-fold crossvali-
dation was increased with an average of 7% by fea-
ture selection, but the same feature set on the test-
ing data obtained a final score 4.7% less than the
one obtained by using all the features (F=67.3%).
The results submitted in the evaluation were based
on feature selection because of this misleading per-
formance shift observed on the development set.
The task of classification of semantic relations be-
tween nominals required data to be separated into
four training sets: the first 35 instances (D1), the
first 70 instances (D2), the first 105 instances (D3),
and the entire set, 140 instances (D4). The letter ?D?
stands for systems that use both the WordNet and the
query information provided by the organizers. The
results on the four sets are illustrated in Figure 2.
The results generally increase with the size of train-
ing data, and tend to be the same on D3 and D4,
which means that the D4 set does not bring signifi-
cant new information compared to D3.
0
10
20
30
40
50
60
70
80
90
D1 D2 D3 D4
R1
R2
R3
R4
R5
R6
R7
Figure 2: Results of training on different portions of
the training dataset.
5 Conclusions
This paper has presented a semantic architecture
that participated in the SemEval 2007 competition
to evaluate two tasks, one for metonymy resolution,
and the other for the classification of semantic re-
lations between nominals. Although the tasks were
very different, the architecture produced competitive
results. The experimental results are reported in this
paper in a detailed manner, and some interesting ob-
servations can be drawn from them.
References
Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In 5th
International Conference on Language Resources and
Evaluation (LREC 2006).
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
Roxana Girju, Dan Moldovan, Marta Tatu, and DanielAntohe. 2005. On the semantics of noun compounds.In Computer Speech and Language, volume 19, pages
479?496.
Roxana Girju, Marti Hearst, Preslav Nakov, Vivi Nas-
tase, Stan Szpakowicz, Peter Turney, and Deniz Yuret.2007. Task 04: Classification of semantic relations be-tween nominal at semeval 2007. In SemEval 2007.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press, Chicago andLondon.
Katja Markert and Malvina Nissim. 2002. Metonymyresolution as a classification task. In the 2002 Con-
ference on Empirical Methods in Natural LAnguage
Processing (EMNLP2002).
Katja Markert and Malvina Nissim. 2007. Task 08:Metonymy resolution at semeval 2007. In SemEval
2007.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic rolelabeling using complete syntactic analysis. In CoNLL
2005, Shared Task.
David Traum and Nizar Habash. 2000. Generation from
lexical conceptual structure. In Workshop on Applied
Interlinguas, ANLP-2000.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. MorganKaufmann, 2nd edition.
459
