Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 649?656
Manchester, August 2008
Semantic Classification with Distributional Kernels
Diarmuid
?
O S
?
eaghdha
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
do242@cl.cam.ac.uk
Ann Copestake
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
aac10@cl.cam.ac.uk
Abstract
Distributional measures of lexical similar-
ity and kernel methods for classification
are well-known tools in Natural Language
Processing. We bring these two meth-
ods together by introducing distributional
kernels that compare co-occurrence prob-
ability distributions. We demonstrate the
effectiveness of these kernels by present-
ing state-of-the-art results on datasets for
three semantic classification: compound
noun interpretation, identification of se-
mantic relations between nominals and se-
mantic classification of verbs. Finally, we
consider explanations for the impressive
performance of distributional kernels and
sketch some promising generalisations.
1 Introduction
This paper draws a connection between two well-
known topics in statistical Natural Language Pro-
cessing: distributional measures of lexical simi-
larity and kernel methods for classification. Dis-
tributional similarity measures quantify the sim-
ilarity between pairs of words through their ob-
served co-occurrences with other words in corpus
data. The kernel functions used in support vec-
tor machine classifiers also allow an interpretation
as similarity measures; however, not all similar-
ity measures can be used as kernels. In particu-
lar, kernel functions must satisfy the mathemati-
cal property of positive semi-definiteness. In Sec-
tion 2 we consider kernel functions suitable for
comparing co-occurrence probability distributions
and show that these kernels are closely related to
measures known from the distributional similarity
literature. We apply these distributional kernels
?2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/
by-nc-sa/3.0/). Some rights reserved.
to three semantic classification tasks: compound
noun interpretation, identification of semantic re-
lations between nominals and semantic classifica-
tion of verbs. In all cases, the distributional ker-
nels outperform the linear and Gaussian kernels
standardly used for SVM classification and fur-
thermore achieve state-of-the-art results. In Sec-
tion 4 we provide a concrete explanation for the
superior performance of distributional kernels, and
in Section 5 we outline some promising directions
for future research.
2 Theory
2.1 Distributional Similarity Measures
Distributional approaches to lexical similarity as-
sume that words appearing in similar contexts are
likely to have similar or related meanings. To
measure distributional similarity, we use a repre-
sentation of words based on observation of their
relations with other words. Specifically, a target
word w is represented in terms of a set C of ad-
missible co-occurrence types c = (r, w
?
), where
the word w
?
belongs to a co-occurrence vocab-
ulary V
c
and r is a relation that holds between
w and w
?
. Co-occurrence relations may be syn-
tactic (e.g., verb-argument, conjunct-conjunct) or
may simply be one of proximity in text. Counts
f(w, c) of a target word w?s co-occurrences can
be estimated from language corpora, and these
counts can be weighted in a variety of ways to re-
flect prior knowledge or to reduce statistical noise.
A simple weighting method is to represent each
word w as a vector of co-occurrence probabilities
(P (c
1
|w), . . . , P (c
|C|
|w)). This vector defines the
parameters of a categorical or multinomial proba-
bility distribution, giving a useful probabilistic in-
terpretation of the distributional model. As the
vector for each target word must sum to 1, the
marginal distributions of target words have little
effect on the resulting similarity estimates. Many
649
similarity measures and weighting functions have
been proposed for distributional vectors; compara-
tive studies include Lee (1999), Curran (2003) and
Weeds and Weir (2005).
2.2 Kernel Methods for Computing
Similarity and Distance
In this section we describe two classes of func-
tions, positive semi-definite and negative semi-
definite kernels, and state some relationships be-
tween these classes. The mathematical treatment
follows Berg et al (1984). A good general intro-
duction to kernels and support vector machines is
the book by Cristianini and Shawe-Taylor (2000).
Let X be a set of items and let k : X ? X ?
R be a symmetric real-valued function on pairs of
items in X . Then k is a positive semi-definite (psd)
kernel if for all finite n-element sets X ? X , the
n? n Gram matrix K defined by K
ij
= k(x
i
, x
j
)
satisfies the property
v
?
Kv ? 0, ?v ? R
n
(1)
This is equivalent to requiring that k define an in-
ner product in a Hilbert space F which may be the
same as X or may differ in dimensionality or in
type (F is by definition a vector space, but X need
not be). An intuitive interpretation of psd kernels is
that they provide a similarity measure on members
ofX based on an embedding ? from input spaceX
into feature space F . It can be shown that a func-
tion is psd if and only if all Gram matrices K have
no negative eigenvalues.
Kernel functions have received significant atten-
tion in recent years through their applications in
machine learning, most notably support vector ma-
chines (SVMs, Cortes and Vapnik (1995)). SVM
classifiers learn a decision boundary between two
data classes that maximises the minimum distance
or margin from the training points in each class to
the boundary. The notion of distance used and the
feature space in which the boundary is set are de-
termined by the choice of kernel function. So long
as the kernel satisfies (1), the SVM optimisation
algorithm is guaranteed to converge to a global op-
timum that affords the geometric interpretation of
margin maximisation. Besides these desirable op-
timisation properties, kernel methods have the ad-
vantage that the choice of kernel can be based on
prior knowledge about the problem and on the na-
ture of the data.
A negative semi-definite (nsd) kernel is a sym-
metric function
?
k : X ? X ? R such that for all
finite n-element sets X ? X and for all vectors
v = (v
1
, . . . , v
n
) ? R
n
with
?
i
v
i
= 0
v
?
?
Kv ? 0 (2)
Whereas positive semi-definite kernels correspond
to inner products in a Hilbert space F , negative
semi-definite kernels correspond to squared dis-
tances. In particular, if
?
k(x, y) = 0 only when
x = y then
?
?
k is a metric. If a function k is
psd, then ?k is always nsd, but the converse does
not hold.
1
However, Berg et al (1984) describe
two simple methods for inducing a positive semi-
definite function k from negative semi-definite
?
k:
k(x, y) =
?
k(x, x
0
) +
?
k(y, x
0
)?
?
k(x, y)
?
?
k(x
0
, x
0
), ?x
0
? X (3a)
k(x, y) = exp(??
?
k(x, y)), ?? > 0 (3b)
The point x
0
in (3a) can be viewed as providing
an origin in F that is the image of some point in
the input space X ; the choice of x
0
does not have
an effect on SVM classification. A familiar exam-
ple of these transformations arises if we take
?
k to
be the squared Euclidean L
2
distance ?x ? y?
2
=
?
i
(x
i
? y
i
)
2
. Applying (3a) and setting x
0
to
be the zero vector, we obtain a quantity that is
twice the linear kernel k(x, y) =
?
i
x
i
y
i
. Apply-
ing (3b) we derive the Gaussian kernel k(x, y) =
exp(???x?y?
2
). In the next section we consider
kernels obtained by plugging alternative squared
metrics into equations (3a) and (3b).
2.3 Distributional Kernels
Given the effectiveness of distributional similarity
measures for numerous tasks in NLP and the in-
terpretation of kernels as similarity functions, it
seems natural to consider the use of kernels tai-
lored for co-occurrence distributions when per-
forming semantic classification. As shown in Sec-
tion 2.2 the standardly used linear and Gaussian
kernels derive from theL
2
distance, yet Lee (1999)
has shown that this distance measure is relatively
poor at comparing co-occurrence distributions. In-
formation theory provides a number of alterna-
tive distance functions on probability measures, of
which the L
1
distance (also called variational dis-
tance), Kullback-Leibler divergence and Jensen-
Shannon divergence are well-known in NLP and
1
Negated nsd functions are sometimes called condition-
ally psd; they constitute a superset of the psd functions.
650
Distance Definition Derived linear kernel
(L
2
distance)
2
?
c
(P (c|w
1
)? P (c|w
2
))
2
?
c
P (c|w
1
)P (c|w
2
)
L
1
distance
?
c
|P (c|w
1
)? P (c|w
2
)|
?
c
min(P (c|w
1
), P (c|w
2
))
Jensen-Shannon
?
c
P (c|w
1
) log
2
(
2P (c|w
1
)
P (c|w
1
)+P (c|w
2
)
) + ?
?
c
P (c|w
1
) log
2
(
P (c|w
1
)
P (c|w
1
)+P (c|w
2
)
) +
divergence P (c|w
2
) log
2
(
2P (c|w
2
)
P (c|w
1
)+P (c|w
2
)
) P (c|w
2
) log
2
(
P (c|w
2
)
P (c|w
1
)+P (c|w
2
)
)
Hellinger distance
?
c
(
?
P (c|w
1
)?
?
P (c|w
2
))
2
?
c
?
P (c|w
1
)P (c|w
2
)
Table 1: Squared metric distances on co-occurrence distributions and corresponding linear kernels
were shown by Lee to give better similarity esti-
mates than the L
2
distance.
In Section 2.2 we have seen how to derive psd
kernels (similarities) from nsd kernels (distances).
It seems likely that distance measures that are
known to work well for comparing co-occurrence
distributions will also give us suitable psd similar-
ity measures. Negative semi-definite kernels are
by definition symmetric, which rules the Kullback-
Leibler divergence and Lee?s (1999) ?-skew diver-
gence out of consideration. The nsd condition (2)
is met if the distance function is a squared metric in
a Hilbert space. In this paper we use a parametric
family of squared Hilbertian metrics on probability
distributions that has been discussed by Hein and
Bousquet (2005). This family contains many fa-
miliar distances including the L
1
distance, Jensen-
Shannon divergence (JSD) and the Hellinger dis-
tance used in statistics, though not the squared L
2
distance. Positive semi-definite distributional ker-
nels can be derived from these distances through
equations (3a) and (3b). We interpret the distribu-
tional kernels produced by (3a) and (3b) as ana-
logues of the linear and Gaussian kernels respec-
tively, given by a different norm or concept of dis-
tance in the feature space F . Hence the linear dis-
tributional kernels produced by (3a) correspond to
inner products in the input space X , and the rbf
distributional kernels produced by (3b) are radial
basis functions corresponding to inner products
in a high-dimensional Hilbert space of Gaussian-
like functions. In this paper we use the unmodi-
fied term ?linear kernel? in the standard sense of
the linear kernel derived from the L
2
distance and
make explicit the related distance when referring to
other linear kernels, e.g., the ?JSD linear kernel?.
Likewise, we use the standard term ?Gaussian? to
refer to the L
2
rbf kernel, and denote other rbf ker-
nels as, for example, the ?JSD rbf kernel?.
Table 1 lists relevant squared metric distances
and their derived linear kernels. The linear ker-
nel derived from the L
1
distance is the same as the
difference-weighted token-based similarity mea-
sure of Weeds and Weir (2005). The JSD linear
kernel can be rewritten as (2 - JSD), where JSD
is the value of the Jensen-Shannon divergence.
This formulation is used as a similarity measure
by Lin (1999). Dagan et al (1999) use a similar-
ity measure 10
??JSD
, though they acknowledge
that this transformation is heuristically motivated.
The rbf kernel exp(??JSD) provides a theoret-
ically sound alternative when the psd property is
required. It follows from the above discussion
that these previously known distributional similar-
ity measures are valid kernel functions and can be
used directly for SVM classification.
Finally, we consider the status of other popular
distributional measures. The familiar cosine sim-
ilarity measure is provably a valid psd kernel, as
it is the L
2
linear kernel calculated between L
2
-
normalised vectors. Distributional vectors are by
definition L
1
-normalised (they sum to 1), but there
is evidence that L
2
normalisation is optimal when
using L
2
kernels for tasks such as text categori-
sation (Leopold and Kindermann, 2002). Indeed,
in the experiments described below L
2
-normalised
feature vectors are used with the L
2
kernels, and
the L
2
linear kernel function then becomes identi-
cal to the cosine similarity. Other similarity mea-
sures, such as that of Lin (1998), can be shown to
be non-psd by calculating similarity matrices from
real or artificial data and showing that their non-
zero eigenvalues are not all positive, as is required
by psd functions.
651
3 Practice
3.1 General Methodology
All experiments were performed using the LIB-
SVM Support Vector Machine library (Chang and
Lin, 2001), modified to implement one-against-
all classification. The members of the distribu-
tional kernel family all performed similarly but the
Jensen-Shannon divergence kernels gave the most
consistently impressive results, and we restrict dis-
cussion to these kernels due to considerations of
space and clarity. In each experiment we com-
pare the standard linear and Gaussian kernels with
the linear and JSD rbf kernels. As a preprocess-
ing step for the L
2
kernels, each feature vector
was normalised to have unit L
2
norm. For the
Jensen-Shannon kernels, the feature vectors were
normalised to have unit L
1
norm, i.e., to define
a probability distribution. For all datasets and all
training-test splits the SVM cost parameter C was
optimised in the range (2
?6
, 2
?4
, . . . , 2
12
) through
cross-validation on the training set. In addition, the
width parameter ? was optimised in the same way
for the rbf kernels. The number of optimisation
folds differed according to the size of the dataset
and the number of training-test splits to be eval-
uated: we used 10 folds for the compound task,
leave-one-out cross-validation for SemEval Task 4
and 25 folds for the verb classification task.
3.2 Compound Noun Interpretation
The task of interpreting the semantics of noun
compounds is one which has recently received
considerable attention (Lauer, 1995; Girju et al,
2005; Turney, 2006). For a given noun-noun com-
pound, the problem is to identify the semantic re-
lation between the compound?s constituents ? that
a kitchen knife is a knife used in a kitchen but a
steel knife is a knife made of steel.
2
The difficulty
of the task is due to the fact that the knowledge re-
quired to interpret compounds is not made explicit
in the contexts where they appear, and hence stan-
dard context-based methods for classifying seman-
tic relations in text cannot be applied. Most previ-
ous work making use of lexical similarity has been
based on WordNet measures (Kim and Baldwin,
2005; Girju et al, 2005).
?
O S?eaghdha and Copes-
take (2007) were to our knowledge the first to ap-
ply a distributional model. Here we build on their
2
In the classification scheme considered here, kitchen
knife would have the label IN and steel knife would be la-
belled BE.
methodology by introducing a probabilistic feature
weighting scheme and applying the new distribu-
tional kernels.
For our experiments we used the dataset of
?
O S?eaghdha and Copestake (2007), which con-
sists of 1443 noun compounds annotated with
six semantic relations: BE, HAVE, IN, AGENT,
INSTRUMENT and ABOUT.
3
The classification
baseline associated with always choosing the
most frequent relation (IN) is 21.3%. For
each compound (N
1
, N
2
) in the dataset, we
associate the co-occurrence probability vector
(P (c
1
|N
1
), . . . , P (c
|C|
|N
1
)) with N
1
and the vec-
tor (P (c
1
|N
2
), . . . , P (c
|C|
|N
2
)) with N
2
. The
probability vector for the compound is created
by appending the two constituent vectors, each
scaled by 0.5 to weight both constituents equally
and ensure that the new vector sums to 1.
These probability vectors are used to compute
the Jensen-Shannon kernel values. The pre-
processing step for the L
2
kernels is analogous,
except that the co-occurrence frequency vector
(f(c
1
, N
i
), . . . , f(c
|C|
, N
i
)) for each constituent
N
i
is normalised to have unit L
2
norm (instead
of unit L
1
norm); the combined feature vector for
each data item is also L
2
-normalised.
4
The co-occurrence relation we counted to esti-
mate the probability vectors was the conjunction
relation. This relation gives sparse but high-quality
information, and was shown to be effective by
?
O
S?eaghdha and Copestake. We extracted two fea-
ture sets from two very different corpora. The
first is the 90 million word written component of
the British National Corpus (Burnard, 1995). This
corpus was parsed with the RASP parser (Briscoe
et al, 2006) and all instances of the conj gram-
matical relation were counted. The co-occurrence
vocabulary V
c
was set to the 10,000 words most
frequently entering into a conj relation across
the corpus. The second corpus we used was the
Web 1T 5-Gram Corpus (Brants and Franz, 2006),
which contains frequency counts for n-grams up
to length 5 extracted from Google?s index of ap-
proximately 1 trillion words of Web text. As the
nature of this corpus precludes parsing, we used a
simple pattern-based technique to extract conjunc-
tions. An n-gram was judged to contain a conjunc-
tion co-occurrence between N
i
and N
j
if it con-
3
This dataset is available from http://www.cl.cam.
ac.uk/
?
do242/resources.html.
4
The importance of performing both normalisation steps
was suggested to us by an anonymous reviewer?s comments.
652
BNC 5-Gram
Kernel Acc F Acc F
Linear 57.9 55.8 55.0 52.5
Gaussian 58.0 56.2 53.5 50.8
JSD (linear) 59.9 57.8 60.2 58.1
JSD (rbf) 59.8 57.9 61.0 58.8
Table 2: Results for compound interpretation
tained the patternN
i
and (?N)*N
j
(?N)*. A noun
dictionary automatically constructed from Word-
Net and an electronic version of Webster?s 1913
Unabridged Dictionary determined the sets of ad-
missible nouns {N} and non-nouns {?N}.
5
The
vocabulary V
c
was again set to the 10,000 most
frequent conjuncts, and the probability estimates
P (c|w) were based on the n-gram frequencies for
each n-gram matching the extraction pattern. A
third feature set extracted from the 5-Gram Corpus
by using a larger set of joining terms was also stud-
ied but the results were not significantly different
from the sparser conjunction feature sets and are
not presented here.
Performance was measured by splitting the data
into five folds and performing cross-validation.
Results for the two feature sets and four kernels
are presented in Table 2. The kernels derived from
the Jensen-Shannon divergence clearly outperform
the L
2
distance-based linear and Gaussian kernels
in both accuracy and macro-averaged F-score. The
best performing kernel-feature combination is the
Jensen-Shannon rbf kernel with the 5-Gram fea-
tures, which attains 61.0% accuracy and 58.8%
F-score. This surpasses the best previous result
of 57.1% accuracy, 55.3% F-score that was re-
ported by
?
O S?eaghdha and Copestake (2007) for
this dataset. That result was obtained by combin-
ing a distributional model with a relational simi-
larity model based on string kernels; incorporating
relational similarity into the system described here
improves performance even further (
?
O S?eaghdha,
2008).
3.3 SemEval Task 4
Task 4 at the 2007 SemEval competition (Girju et
al., 2007) focused on the identification of seman-
tic relations among nominals in text. Identifica-
tion of each of seven relations was designed as
a binary classification task with 140 training sen-
5
The electronic version of Webster?s is available from
http://msowww.anu.edu.au/
?
ralph/OPTED/.
BNC 5-Gram
Kernel Acc F Acc F
Linear 67.6 57.1 65.4 63.3
Gaussian 66.8 60.7 65.6 62.9
JSD (linear) 71.4 68.8 69.6 65.8
JSD (rbf) 69.9 66.7 70.7 67.5
Table 3: Results for SemEval Task 4
tences and around 70 test sentences.
6
To ensure
that the task be a challenging one, the negative
test examples were all ?near misses? in that they
were plausible candidates for the relation to hold
but failed to meet one of the criteria for that rela-
tion. This was achieved by selecting both positive
and negative examples from the results of the same
targeted Google queries. The majority-class base-
line for this task gives Accuracy = 57.0%, F-score
= 30.8%, while the all-true baseline (label every
test sentence positive) gives Accuracy = 48.5%, F-
score = 64.8%.
We used the same feature sets and kernels as in
Section 3.2. The results are presented in Table 3.
Again, the JSD kernels outperform the standard
L
2
kernels by a considerable margin. The best
performing feature-kernel combination achieves
71.4% Accuracy and 68.8% F-score, higher than
the best performance attained in the SemEval com-
petition without using WordNet similarity mea-
sures (Accuracy = 67.0%, F-score = 65.1%; Nakov
and Hearst (2007)). This is also higher than the
performance of all but three of the 14 SemEval en-
tries which did use WordNet. Davidov and Rap-
poport (2008) have recently described a WordNet-
free method that attains slightly lower accuracy
(70.1%) and slightly higher F-score (70.6%) than
our method. Taken together, Davidov and Rap-
poport?s results and ours define the current state
of the art on this task.
3.4 Verb Classification
To investigate the effectiveness of distributional
kernels on a different kind of semantic classifi-
cation task, we tested our methods on the verb
class data of Sun et al (2008). This dataset con-
sists of 204 verbs assigned to 17 of Levin?s (1993)
verb classes. Each verb is represented by a set
of features corresponding to the distribution of its
instances across subcategorisation frames (SCFs).
6
The relations are Cause-Effect, Instrument-Agency,
Product-Producer, Origin-Entity, Theme-Tool, Part-Whole
and Content-Container.
653
FS3 FS5
Kernel Acc F Acc F
Linear 67.1 65.5 67.6 65.9
Gaussian 60.8 58.6 62.7 60.2
JSD (linear) 70.6 67.3 69.6 66.4
JSD (rbf) 68.6 65.1 70.1 67.2
Sun et al (SVM) 57.8 58.2 57.3 57.4
Sun et al (GS) 59.3 57.1 64.2 62.5
Table 4: Results for leave-one-out verb classifica-
tion and comparison with Sun et al?s (2008) SVM
and Gaussian fitting methods
These frames include information about syntac-
tic constituents (NP, NP NP, NP SCOMP, . . . )
and some lexical information about subcategorised
prepositions (NP with, out, . . . ). The feature val-
ues are counts of SCFs extracted from a large cor-
pus. As the feature vector for each verb natu-
rally defines a probability distribution over SCFs,
it seems intuitive to apply distributional kernels to
the problem of predicting Levin classes for verbs.
Sun et al use multiple feature sets of varying
sparsity and noisiness. We report results on the
two feature sets for which they reported best per-
formance; for continuity we keep the names FS3
and FS5 for these feature sets. These were de-
rived from the least filtered and hence least sparse
subcategorisation lexicon (which they call VALEX
1) and differ in the granularity of prepositional
SCFs. The SCF representation in FS5 is richer
and hence potentially more discriminative, but it is
also sparser. Using an SVM with a Gaussian ker-
nel, Sun et al achieved their best results on FS3.
Perhaps surprisingly, their best results overall were
attained with FS5 by a simple method based on
fitting multivariate Gaussian distributions to each
class in the training data and assigning the maxi-
mum likelihood class to test points.
Following Sun et al, we use a leave-one-out
measure of verb classification performance. As
the examples are distributed equally across the 17
classes, the random baseline accuracy is 5.9%. Ta-
ble 4 presents our results with L
2
and JSD kernels,
as well as those of Sun et al The best overall
performance is attained by the JSD linear kernel,
which scores higher than the L
2
-derived kernels
on both feature sets. The L
2
linear kernel also per-
forms quite well and with consistency. The JSD
rbf kernel was less consistent over cross-validation
runs, seemingly due to uncertainty in selecting the
optimal ? parameter value; it clearly outperforms
the L
2
linear kernel on one feature set (FS5) but
on the other (FS3) it attains a slightly lower F-
score while maintaining a higher accuracy. The
Gaussian kernel seems particularly ill-suited to this
dataset, performing significantly worse than the
other kernels. The difference between Sun et al?s
results with the Gaussian kernel and ours with the
same kernel may be due to the use of one-against-
all classification here instead of one-against-one,
or it may be due to differences in preprocessing or
parameter optimisation.
4 The effect of marginal distributions
It is natural to ask why distributional kernels per-
form better than the standard linear and Gaussian
kernels. One answer might be that just as infor-
mation theory provides the ?correct? notion of in-
formation for many purposes, it also provides the
?correct? notion of distance between probability
distributions. Hein and Bousquet (2005) show that
their family of distributional kernels are invariant
to bijective transformations of the event space C
and suggest that this property is a valuable one for
image histogram classification where data may be
represented in a range of equivalent colour spaces.
However, it is not clear that this confers an advan-
tage when comparing lexical co-occurrence distri-
butions; when transformations are performed on
the space of co-occurrence types, they are gener-
ally not information-conserving, for example lem-
matisation or stemming.
A more practical explanation is that the distri-
butional kernels and distances are less sensitive
than the (squared) L
2
distance and its derived ker-
nels to the marginal frequencies of co-occurrence
types. When a type c has high frequency we expect
that it will have higher variance, i.e., the differ-
ences |P (c|w
1
)? P (c|w
2
)| will tend to be greater
even if c is not a more important signifier of simi-
larity.
7
These differences contribute quadratically
to the L
2
distance and hence also to the associ-
ated rbf kernel, i.e., the Gaussian kernel. It is also
easy to see that types c for which P (c|w
i
) tends
to be large will dominate the value of the linear
kernel. This explanation is also plausibly a fac-
tor in the relatively poor performance of L
2
dis-
tance as a lexical dissimilarity measure, as demon-
7
Chapelle et al (1999) give a similar explanation for the
performance of a related family of kernels on a histogram
classification task.
654
strated by Lee (1999). In contrast, the differences
|P (c|w
1
)?P (c|w
2
)| are not squared in the L
1
dis-
tance formula, and the minimum function in theL
1
linear kernel dampens the effect of high-variance
co-occurrence types. The Jensen-Shannon formula
is more difficult to interpret, as the difference terms
do not directly appear. While co-occurrence types
with large P (c|w
1
) and P (c|w
2
) do contribute
more to the distance and kernel values, it is the
proportional size of the difference that appears in
the log term rather than its magnitude. Finally, the
Hellinger distance and kernels squash the variance
associated with c through the square root function.
5 Discussion and Future Directions
Kernels on probability measures have been dis-
cussed in the machine learning literature (Kondor
and Jebara, 2003; Cuturi et al, 2005; Hein and
Bousquet, 2005), but they have previously been
applied only to standard image and text classifi-
cation benchmark tasks. We seem to be the first to
use distributional kernels for semantic classifica-
tion and to note their connection with familiar lex-
ical similarity measures. Indeed, the only research
we are aware of on kernels tailored for lexical sim-
ilarity is the small body of work on WordNet ker-
nels, e.g., Basili et al (2006). In contrast, Sup-
port Vector Machines have been widely adopted
for computational semantic tasks, from word sense
disambiguation (Gliozzo et al, 2005) to semantic
role labelling (Pradhan et al, 2004). The standard
feature sets for semantic role labelling and many
other tasks are collections of heterogeneous fea-
tures that do not correspond to probability distri-
butions. So long as the features are restricted to
positive values, distributional kernels can be ap-
plied; it will be interesting (and informative) to see
whether they retain their superiority in this setting.
One advantage of kernel methods is that kernels
can be defined for non-vectorial data structures
such as strings, trees, graphs and sets. A promis-
ing topic of future research is the design of distri-
butional kernels for comparing structured objects,
based on the feature space embedding associated
with convolution kernels (Haussler, 1999). These
kernels map structures in X into a space whose di-
mensions correspond to substructures of the ele-
ments of X . Thus strings are mapped onto vec-
tors of substring counts, and trees are mapped onto
vectors of subtree counts. We adopt the perspec-
tive that this mapping represents structures x
i
? X
as measures over substructures x?
1
, . . . , x?
d
. Prop-
erly normalised, this gives a distributional proba-
bility vector (P (x?
1
), . . . , P (x?
d
)) similar to those
used for computing lexical similarity. This per-
spective motivates the use of distributional inner
products instead of the dot products implicitly used
in standard convolution kernels. Several authors
have suggested applying distributional similarity
measures to sentences and phrases for tasks such as
question answering (Lin and Pantel, 2001; Weeds
et al, 2005). Distributional kernels on strings and
trees should provide a flexible implementation of
these suggestions that is compatible with SVM
classification and does not require manual feature
engineering. Furthermore, there is a ready gener-
alisation to kernels on sets of structures; if a set
is represented as the normalised sum of its mem-
ber embeddings in feature space F , distributional
methods can be applied directly.
6 Conclusion
In this paper we have introduced distributional ker-
nels for classification with co-occurrence proba-
bility distributions. The suitability of distribu-
tional kernels for semantic classification is intu-
itive, given their relation to proven distributional
methods for computing semantic similarity, and in
practice they work very well. As these kernels
give state-of-the-art results on the three datasets we
have tested, we expect that they will prove useful
for a wide range of semantic classification prob-
lems in future.
Acknowledgements
We are grateful to Andreas Vlachos and three
anonymous reviewers for their useful comments,
and to Anna Korhonen for providing the verb clas-
sification dataset. This work was supported in part
by EPSRC Grant EP/C010035/1.
References
Basili, Roberto, Marco Cammisa, and Alessandro Mos-
chitti. 2006. A semantic kernel to classify texts with
very few training examples. Informatica, 30(2):163?
172.
Berg, Christian, Jens P. R. Christensen, and Paul Ressel.
1984. Harmonic Analysis on Semigroups: Theory of
Positive Definite and Related Functions. Springer,
Berlin.
Brants, Thorsten and Alex Franz, 2006. Web 1T 5-gram
Corpus Version 1.1. Linguistic Data Consortium.
655
Briscoe, Ted, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the ACL Interactive Presentation Sessions.
Burnard, Lou, 1995. Users? Guide for the British Na-
tional Corpus. British National Corpus Consortium,
Oxford University Computing Service.
Chang, Chih-Chung and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines. Software
available at http://www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Chapelle, Olivier, Patrick Haffner, and Vladimir N.
Vapnik. 1999. Support vector machines for
histogram-based image classification. IEEE Trans-
actions on Neural Networks, 10(5):1055?1064.
Cortes, Corinna and Vladimir Vapnik. 1995. Support
vector networks. Machine Learning, 20(3):273?297.
Cristianini, Nello and John Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines. Cambridge
University Press, Cambridge.
Curran, James. 2003. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Cuturi, Marco, Kenji Fukumizu, and Jean-Philippe
Vert. 2005. Semigroup kernels on measures. Jour-
nal of Machine Learning Research, 6:1169?1198.
Dagan, Ido, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence prob-
abilities. Machine Learning, 34(1?4):43?69.
Davidov, Dmitry and Ari Rappoport. 2008. Classifica-
tion of semantic relationships between nominals us-
ing pattern clusters. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics.
Girju, Roxana, Dan Moldovan, Marta Tatu, and
Daniel Antohe. 2005. On the semantics of
noun compounds. Computer Speech and Language,
19(4):479?496.
Girju, Roxana, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of seman-
tic relations between nominals. In Proceedings of
the 4th International Workshop on Semantic Evalua-
tions.
Gliozzo, Alfio, Claudio Giuliano, and Carlo Strappar-
ava. 2005. Domain kernels for word sense disam-
biguation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics.
Haussler, David. 1999. Convolution kernels on dis-
crete structures. Technical Report UCSC-CRL-99-
10, Computer Science Department, University of
California at Santa Cruz.
Hein, Matthias and Olivier Bousquet. 2005. Hilbertian
metrics and positive definite kernels on probability
measures. In Proceedings of the 10th International
Workshop on Artificial Intelligence and Statistics.
Kim, Su Nam and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing.
Kondor, Risi and Tony Jebara. 2003. A kernel between
sets of vectors. In Proceedings of the 20th Interna-
tional Conference on Machine Learning.
Lauer, Mark. 1995. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University.
Lee, Lillian. 1999. Measures of distributional similar-
ity. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Leopold, Edda and J?org Kindermann. 2002. Text cat-
egorization with support vector machines. how to
represent texts in input space? Machine Learning,
46(1?3):423?444.
Levin, Beth. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
Lin, Dekang and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Lin, Dekang. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning.
Nakov, Preslav I. and Marti A. Hearst. 2007. UCB:
System description for SemEval Task #4. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations.
?
O S?eaghdha, Diarmuid and Ann Copestake. 2007. Co-
occurrence contexts for noun compound interpreta-
tion. In Proceedings of the ACL Workshop on a
Broader Perspective on Multiword Expressions.
?
O S?eaghdha, Diarmuid. 2008. Learning Compound
Noun Semantics. Ph.D. thesis, Computer Labora-
tory, University of Cambridge. In preparation.
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2004. Support vector learning for semantic ar-
gument classification. Machine Learning, 60(1):11?
39.
Sun, Lin, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the 9th International Conference
on Intelligent Text Processing and Computational
Linguistics.
Turney, Peter D. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Weeds, Julie and David Weir. 2005. Co-occurrence
retrieval: A flexible framework for lexical dis-
tributional similarity. Computational Linguistics,
31(4):439?476.
Weeds, Julie, David Weir, and Bill Keller. 2005. The
distributional similarity of sub-parses. In Proceed-
ings of the ACL Workshop on Empirical Modeling of
Semantic Equivalence and Entailment.
656
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 621?629,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Using lexical and relational similarity to classify semantic relations
Diarmuid O? Se?aghdha
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
do242@cl.cam.ac.uk
Ann Copestake
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
aac10@cl.cam.ac.uk
Abstract
Many methods are available for comput-
ing semantic similarity between individ-
ual words, but certain NLP tasks require
the comparison of word pairs. This pa-
per presents a kernel-based framework for
application to relational reasoning tasks of
this kind. The model presented here com-
bines information about two distinct types
of word pair similarity: lexical similarity
and relational similarity. We present an
efficient and flexible technique for imple-
menting relational similarity and show the
effectiveness of combining lexical and re-
lational models by demonstrating state-of-
the-art results on a compound noun inter-
pretation task.
1 Introduction
The problem of modelling semantic similarity be-
tween words has long attracted the interest of re-
searchers in Natural Language Processing and has
been shown to be important for numerous applica-
tions. For some tasks, however, it is more appro-
priate to consider the problem of modelling sim-
ilarity between pairs of words. This is the case
when dealing with tasks involving relational or
analogical reasoning. In such tasks, the chal-
lenge is to compare pairs of words on the basis of
the semantic relation(s) holding between the mem-
bers of each pair. For example, the noun pairs
(steel,knife) and (paper,cup) are similar because
in both cases the relation N2 is made of N1 fre-
quently holds between their members. Analogi-
cal tasks are distinct from (but not unrelated to)
other kinds of ?relation extraction? tasks where
each data item is tied to a specific sentence con-
text (e.g., Girju et al (2007)).
One such relational reasoning task is the prob-
lem of compound noun interpretation, which
has received a great deal of attention in recent
years (Girju et al, 2005; Turney, 2006; But-
nariu and Veale, 2008). In English (and other
languages), the process of producing new lexical
items through compounding is very frequent and
very productive. Furthermore, the noun-noun re-
lation expressed by a given compound is not ex-
plicit in its surface form: a steel knife may be a
knife made from steel but a kitchen knife is most
likely to be a knife used in a kitchen, not a knife
made from a kitchen. The assumption made by
similarity-based interpretation methods is that the
likely meaning of a novel compound can be pre-
dicted by comparing it to previously seen com-
pounds whose meanings are known. This is a
natural framework for computational techniques;
there is also empirical evidence for similarity-
based interpretation in human compound process-
ing (Ryder, 1994; Devereux and Costello, 2007).
This paper presents an approach to relational
reasoning based on combining information about
two kinds of similarity between word pairs: lex-
ical similarity and relational similarity. The as-
sumptions underlying these two models of similar-
ity are sketched in Section 2. In Section 3 we de-
scribe how these models can be implemented for
statistical machine learning with kernel methods.
We present a new flexible and efficient kernel-
based framework for classification with relational
similarity. In Sections 4 and 5 we apply our
methods to a compound interpretation task and
demonstrate that combining models of lexical and
relational similarity can give state-of-the-art re-
sults on a compound noun interpretation task, sur-
passing the performance attained by either model
taken alone. We then discuss previous research
on relational similarity, and show that some previ-
ously proposed models can be implemented in our
framework as special cases. Given the good per-
formance achieved for compound interpretation, it
seems likely that the methods presented in this pa-
621
per can also be applied successfully to other rela-
tional reasoning tasks; we suggest some directions
for future research in Section 7.
2 Two models of word pair similarity
While there is a long tradition of NLP research
on methods for calculating semantic similarity be-
tween words, calculating similarity between pairs
(or n-tuples) of words is a less well-understood
problem. In fact, the problem has rarely been
stated explicitly, though it is implicitly addressed
by most work on compound noun interpretation
and semantic relation extraction. This section de-
scribes two complementary approaches for using
distributional information extracted from corpora
to calculate noun pair similarity.
The first model of pair similarity is based on
standard methods for computing semantic similar-
ity between individual words. According to this
lexical similarity model, word pairs (w1, w2) and
(w3, w4) are judged similar if w1 is similar to w3
and w2 is similar to w4. Given a measure wsim
of word-word similarity, a measure of pair simi-
larity psim can be derived as a linear combination
of pairwise lexical similarities:
psim((w1, w2), (w3, w4)) = (1)
?[wsim(w1, w3)] + ?[wsim(w2, w4)]
A great number of methods for lexical semantic
similarity have been proposed in the NLP liter-
ature. The most common paradigm for corpus-
based methods, and the one adopted here, is based
on the distributional hypothesis: that two words
are semantically similar if they have similar pat-
terns of co-occurrence with other words in some
set of contexts. Curran (2004) gives a comprehen-
sive overview of distributional methods.
The second model of pair similarity rests on the
assumption that when the members of a word pair
are mentioned in the same context, that context
is likely to yield information about the relations
holding between the words? referents. For exam-
ple, the members of the pair (bear, forest) may
tend to co-occur in contexts containing patterns
such as w1 lives in the w2 and in the w2,. . . a w1,
suggesting that a LOCATED IN or LIVES IN re-
lation frequently holds between bears and forests.
If the contexts in which fish and reef co-occur are
similar to those found for bear and forest, this is
evidence that the same semantic relation tends to
hold between the members of each pair. A re-
lational distributional hypothesis therefore states
that two word pairs are semantically similar if their
members appear together in similar contexts.
The distinction between lexical and relational
similarity for word pair comparison is recognised
by Turney (2006) (he calls the former attributional
similarity), though the methods he presents focus
on relational similarity. O? Se?aghdha and Copes-
take?s (2007) classification of information sources
for noun compound interpretation also includes a
description of lexical and relational similarity. Ap-
proaches to compound noun interpretation have
tended to use either lexical or relational similarity,
though rarely both (see Section 6 below).
3 Kernel methods for pair similarity
3.1 Kernel methods
The kernel framework for machine learning is a
natural choice for similarity-based classification
(Shawe-Taylor and Cristianini, 2004). The cen-
tral concept in this framework is the kernel func-
tion, which can be viewed as a measure of simi-
larity between data items. Valid kernels must sat-
isfy the mathematical condition of positive semi-
definiteness; this is equivalent to requiring that the
kernel function equate to an inner product in some
vector space. The kernel can be expressed in terms
of a mapping function ? from the input space X to
a feature space F :
k(xi,xj) = ??(xi), ?(xj)?F (2)
where ??, ??F is the inner product associated with
F . X and F need not have the same dimension-
ality or be of the same type. F is by definition an
inner product space, but the elements of X need
not even be vectorial, so long as a suitable map-
ping function ? can be found. Furthermore, it is
often possible to calculate kernel values without
explicitly representing the elements of F ; this al-
lows the use of implicit feature spaces with a very
high or even infinite dimensionality.
Kernel functions have received significant at-
tention in recent years, most notably due to the
successful application of Support VectorMachines
(Cortes and Vapnik, 1995) to many problems. The
SVM algorithm learns a decision boundary be-
tween two data classes that maximises the mini-
mum distance or margin from the training points
in each class to the boundary. The geometry of the
space in which this boundary is set depends on the
622
kernel function used to compare data items. By
tailoring the choice of kernel to the task at hand,
the user can use prior knowledge and intuition to
improve classification performance.
One useful property of kernels is that any sum
or linear combination of kernel functions is itself
a valid kernel. Theoretical analyses (Cristianini
et al, 2001; Joachims et al, 2001) and empiri-
cal investigations (e.g., Gliozzo et al (2005)) have
shown that combining kernels in this way can have
a beneficial effect when the component kernels
capture different ?views? of the data while indi-
vidually attaining similar levels of discriminative
performance. In the experiments described below,
we make use of this insight to integrate lexical and
relational information for semantic classification
of compound nouns.
3.2 Lexical kernels
O? Se?aghdha and Copestake (2008) demonstrate
how standard techniques for distributional similar-
ity can be implemented in a kernel framework. In
particular, kernels for comparing probability dis-
tributions can be derived from standard probabilis-
tic distance measures through simple transforma-
tions. These distributional kernels are suited to a
data representation where each word w is identi-
fied with the a vector of conditional probabilities
(P (c1|w), . . . , P (c|C||w)) that defines a distribu-
tion over other terms c co-occurring with w. For
example, the following positive semi-definite ker-
nel between words can be derived from the well-
known Jensen-Shannon divergence:
kjsd(w1, w2) =
?
?
c
[P (c|w1) log2(
P (c|w1)
P (c|w1) + P (c|w2)
)
+ P (c|w2) log2(
P (c|w2)
P (c|w1) + P (c|w2)
)] (3)
A straightforward method of extending this model
to word pairs is to represent each pair (w1, w2) as
the concatenation of the co-occurrence probability
vectors forw1 andw2. Taking kjsd as a measure of
word similarity and introducing parameters ? and
? to scale the contributions of w1 and w2 respec-
tively, we retrieve the lexical model of pair similar-
ity defined above in (1). Without prior knowledge
of the relative importance of each pair constituent,
it is natural to set both scaling parameters to 0.5,
and this is done in the experiments below.
3.3 String embedding functions
The necessary starting point for our implementa-
tion of relational similarity is a means of compar-
ing contexts. Contexts can be represented in a va-
riety of ways, from unordered bags of words to
rich syntactic structures. The context representa-
tion adopted here is based on strings, which pre-
serve useful information about the order of words
in the context yet can be processed and compared
quite efficiently. String kernels are a family of ker-
nels that compare strings s, t by mapping them
into feature vectors ?String(s), ?String(t) whose
non-zero elements index the subsequences con-
tained in each string.
A string is defined as a finite sequence s =
(s1, . . . , sl) of symbols belonging to an alphabet
?. ?l is the set of all strings of length l, and ?? is
set of all strings or the language. A subsequence
u of s is defined by a sequence of indices i =
(i1, . . . , i|u|) such that 1 ? i1 < ? ? ? < i|u| ? |s|,
where |s| is the length of s. len(i) = i|u| ? i1 + 1
is the length of the subsequence in s. An embed-
ding ?String : ?? ? R|?|
l
is a function that maps
a string s onto a vector of positive ?counts? that
correspond to subsequences contained in s.
One example of an embedding function is a
gap-weighted embedding, defined as
?gapl(s) = [
?
i:s[i]=u
?len(i)]u??l (4)
? is a decay parameter between 0 and 1; the
smaller its value, the more the influence of a dis-
continuous subsequence is reduced. When l = 1
this corresponds to a ?bag-of-words? embedding.
Gap-weighted string kernels implicitly compute
the similarity between two strings s, t as an inner
product ??(s), ?(t)?. Lodhi et al (2002) present
an efficient dynamic programming algorithm that
evaluates this kernel in O(l|s||t|) time without ex-
plicitly representing the feature vectors ?(s), ?(t).
An alternative embedding is that used by Turney
(2008) in his PairClass system (see Section 6). For
the PairClass embedding ?PC , an n-word context
[0?1 words]N1|2 [0?3 words]N1|2 [0?1 words]
containing target words N1, N2 is mapped onto
the 2n?2 patterns produced by substituting zero
or more of the context words with a wildcard ?.
Unlike the patterns used by the gap-weighted em-
bedding these are not truly discontinuous, as each
wildcard must match exactly one word.
623
3.4 Kernels on sets
String kernels afford a way of comparing individ-
ual contexts. In order to compute the relational
similarity of two pairs, however, we do not want to
associate each pair with a single context but rather
with the set of contexts in which they appear to-
gether. In this section, we use string embeddings
to define kernels on sets of strings.
One natural way of defining a kernel over sets
is to take the average of the pairwise basic kernel
values between members of the two sets A and B.
Let k0 be a kernel on a set X , and let A,B ? X
be sets of cardinality |A| and |B| respectively. The
averaged kernel is defined as
kave(A,B) =
1
|A||B|
?
a?A
?
b?B
k0(a, b) (5)
This kernel was introduced by Ga?rtner et
al. (2002) in the context of multiple instance learn-
ing. It was first used for computing relational sim-
ilarity by O? Se?aghdha and Copestake (2007). The
efficiency of the kernel computation is dominated
by the |A| ? |B| basic kernel calculations. When
each basic kernel calculation k0(a, b) has signifi-
cant complexity, as is the case with string kernels,
calculating kave can be slow.
A second perspective views each set as corre-
sponding to a probability distribution, and takes
the members of that set as observed samples from
that distribution. In this way a kernel on distribu-
tions can be cast as a kernel on sets. In the case of
sets whose members are strings, a string embed-
ding ?String can be used to estimate a probability
distribution over subsequences for each set by tak-
ing the normalised sum of the feature mappings of
its members:
?Set(A) =
1
Z
?
s?A
?String(s) (6)
where Z is a normalisation factor. Different
choices of ?String yield different relational simi-
larity models. In this paper we primarily use the
gap-weighted embedding ?gapl ; we also discuss
the PairClass embedding ?PC for comparison.
Once the embedding ?Set has been calculated,
any suitable inner product can be applied to the
resulting vectors, e.g. the linear kernel (dot prod-
uct) or the Jensen-Shannon kernel defined in (3).
In the latter case, which we term kjsd below, the
natural choice for normalisation is the sum of the
entries in
?
s?A ?String(s), ensuring that ?Set(A)
has unit L1 norm and defines a probability dis-
tribution. Furthermore, scaling ?Set(A) by 1|A| ,
applying L2 vector normalisation and applying
the linear kernel retrieves the averaged set kernel
kave(A,B) as a special case of the distributional
framework for sets of strings.
Instead of requiring |A||B| basic kernel evalua-
tions for each pair of sets, distributional set kernels
only require the embedding ?Set(A) to be com-
puted once for each set and then a single vector
inner product for each pair of sets. This is gen-
erally far more efficient than the kernel averaging
method. The significant drawback is that repre-
senting the feature vector for each set demands
a large amount of memory; for the gap-weighted
embedding with subsequence length l, each vec-
tor potentially contains up to |A|
(|smax|
l
)
entries,
where smax is the longest string in A. In practice,
however, the vector length will be lower due to
subsequences occurring more than once and many
strings being shorter than smax.
One way to reduce the memory load is to re-
duce the lengths of the strings used, either by re-
taining just the part of each string expected to be
informative or by discarding all strings longer than
an acceptable maximum. The PairClass embed-
ding function implicitly restricts the contexts con-
sidered by only applying to strings where no more
than three words occur between the targets, and by
ignoring all non-intervening words except single
ones adjacent to the targets. A further technique
is to trade off time efficiency for space efficiency
by computing the set kernel matrix in a blockwise
fashion. To do this, the input data is divided into
blocks of roughly equal size ? the size that is rele-
vant here is the sum of the cardinalities of the sets
in a given block. Larger block sizes b therefore
allow faster computation, but they require more
memory. In the experiments described below, b
was set to 5,000 for embeddings of length l = 1
and l = 2, and to 3,000 for l = 3.
4 Experimental setup for compound
noun interpretation
4.1 Dataset
The dataset used in our experiments is O? Se?aghdha
and Copestake?s (2007) set of 1,443 compound
nouns extracted from the British National Corpus
(BNC).1 Each compound is annotated with one of
1The data are available from http://www.cl.cam.
ac.uk/?do242/resources.html.
624
six semantic relations: BE,HAVE, IN, AGENT, IN-
STRUMENT and ABOUT. For example, air disas-
ter is labelled IN (a disaster in the air) and freight
train is labelled INSTRUMENT (a train that car-
ries freight). The best previous classification result
on this dataset was reported by O? Se?aghdha and
Copestake (2008), who achieved 61.0% accuracy
and 58.8% F-score with a purely lexical model of
compound similarity.
4.2 General Methodology
All experiments were run using the LIBSVM Sup-
port Vector Machine library.2 The one-versus-all
method was used to decompose the multiclass task
into six binary classification tasks. Performance
was evaluated using five-fold cross-validation. For
each fold the SVM cost parameter was optimised
in the range (2?6, 2?4, . . . , 212) through cross-
validation on the training set.
All kernel matrices were precomputed on near-
identical machines with 2.4 Ghz 64-bit processors
and 8Gb of memory. The kernel matrix compu-
tation is trivial to parallelise, as each cell is inde-
pendent. Spreading the computational load across
multiple processors is a simple way to reduce the
real time cost of the procedure.
4.3 Lexical features
Our implementation of the lexical similarity
model uses the same feature set as O? Se?aghdha
and Copestake (2008). Two corpora were used
to extract co-occurrence information: the writ-
ten component of the BNC (Burnard, 1995) and
the Google Web 1T 5-Gram Corpus (Brants and
Franz, 2006). For each noun appearing as a com-
pound constituent in the dataset, we estimate a co-
occurrence distribution based on the nouns in co-
ordinative constructions. Conjunctions are identi-
fied in the BNC by first parsing the corpus with
RASP (Briscoe et al, 2006) and extracting in-
stances of the conj grammatical relation. As the
5-Gram corpus does not contain full sentences it
cannot be parsed, so regular expressions were used
to extract coordinations. In each corpus, the set of
co-occurring terms is restricted to the 10,000 most
frequent conjuncts in that corpus so that each con-
stituent distribution is represented with a 10,000-
dimensional vector. The probability vector for the
compound is created by appending the two con-
stituent vectors, each scaled by 0.5 to weight both
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm
constituents equally and ensure that the new vec-
tor sums to 1. To perform classification with these
features we use the Jensen-Shannon kernel (3).3
4.4 Relational features
To extract data for computing relational similarity,
we searched a large corpus for sentences in which
both constituents of a compound co-occur. The
corpora used here are the written BNC, contain-
ing 90 million words of British English balanced
across genre and text type, and the English Giga-
word Corpus, 2nd Edition (Graff et al, 2005), con-
taining 2.3 billion words of newswire text. Extrac-
tion from the Gigaword Corpus was performed at
the paragraph level as the corpus is not annotated
for sentence boundaries, and a dictionary of plural
forms and American English variants was used to
expand the coverage of the corpus trawl.
The extracted contexts were split into sentences,
tagged and lemmatised with RASP. Duplicate sen-
tences were discarded, as were sentences in which
the compound head and modifier were more than
10 words apart. Punctuation and tokens containing
non-alphanumeric characters were removed. The
compound modifier and head were replaced with
placeholder tokens M:n and H:n in each sentence
to ensure that the classifier would learn from re-
lational information only and not from lexical in-
formation about the constituents. Finally, all to-
kens more than five words to the left of the left-
most constituent or more than five words to the
right of the rightmost constituent were discarded;
this has the effect of speeding up the kernel com-
putations and should also focus the classifier on
the most informative parts of the context sen-
tences. Examples of the context strings extracted
for the modifier-head pair (history,book) are the:a
1957:m pulitizer:n prize-winning:j H:n describe:v
event:n in:i american:j M:n when:c elect:v of-
ficial:n take:v principle:v and he:p read:v con-
stantly:r usually:r H:n about:i american:j M:n
or:c biography:n.
This extraction procedure resulted in a corpus
of 1,472,798 strings. There was significant varia-
tion in the number of context strings extracted for
each compound: 288 compounds were associated
with 1,000 or more sentences, while 191 were as-
3O? Se?aghdha and Copestake (2008) achieve their single
best result with a different kernel (the Jensen-Shannon RBF
kernel), but the kernel used here (the Jensen-Shannon lin-
ear kernel) generally achieves equivalent performance and
presents one fewer parameter to optimise.
625
kjsd kave
Length Acc F Acc F
1 47.9 45.8 43.6 40.4
2 51.7 49.5 49.7 48.3
3 50.7 48.4 50.1 48.6
?12 51.5 49.6 48.3 46.8
?23 52.1 49.9 50.9 49.5
?123 51.3 49.0 50.5 49.1
?PC 44.9 43.3 40.9 40.0
Table 1: Results for combinations of embedding
functions and set kernels
sociated with 10 or fewer and no sentences were
found for 45 constituent pairs. The largest context
sets were predominantly associated with political
or economic topics (e.g., government official, oil
price), reflecting the journalistic sources of the Gi-
gaword sentences.
Our implementation of relational similarity ap-
plies the two set kernels kave and kjsd defined in
Section 3.4 to these context sets. For each kernel
we tested gap-weighted embedding functions with
subsequence length values l in the range 1, 2, 3,
as well as summed kernels for all combinations
of values in this range. The decay parameter ?
for the subsequence feature embedding was set to
0.5 throughout, in line with previous recommen-
dations (e.g., Cancedda et al (2003)). To inves-
tigate the effects of varying set sizes, we ran ex-
periments with context sets of maximal cardinality
q ? {50, 250, 1000}. These sets were randomly
sampled for each compound; for compounds asso-
ciated with fewer strings than the maximal cardi-
nality, all associated strings were used. For q = 50
we average results over five runs in order to re-
duce sampling variation. We also report some
results with the PairClass embedding ?PC . The
restricted representative power of this embedding
brings greater efficiency and we were able to use
q = 5, 000; for all but 22 compounds, this allowed
the use of all contexts for which the ?PC embed-
ding was defined.
5 Results
Table 1 presents results for classification with re-
lational set kernels, using q = 1, 000 for the gap-
weighted embedding. In general, there is little dif-
ference between the performance of kjsd and kave
with ?gapl ; the only statistically significant differ-
ences (at p < 0.05, using paired t-tests) are be-
tween the kernels kl=1 with subsequence length
l = 1 and the summed kernels k?12 = kl=1+kl=2.
The best performance of 52.1% accuracy, 49.9%
F-score is obtained with the Jensen-Shannon ker-
nel kjsd computed on the summed feature embed-
dings of length 2 and 3. This is significantly lower
than the performance achieved by O? Se?aghdha
and Copestake (2008) with their lexical similar-
ity model, but it is well above the majority class
baseline (21.3%). Results for the PairClass em-
bedding are much lower than for the gap-weighted
embedding; the superiority of ?gapl is statistically
significant in all cases except l = 1.
Results for combinations of lexical co-
occurrence kernels and (gap-weighted) relational
set kernels are given in Table 2. With the excep-
tion of some combinations of the length-1 set
kernel, these results are clearly better than the
best results obtained with either the lexical or
the relational model taken alone. The best result
is obtained by the combining the lexical kernel
computed on BNC conjunction features with the
summed Jensen-Shannon set kernel k?23 ; this
combination achieves 63.1% accuracy and 61.6%
F-score, a statistically significant improvement (at
the p < 0.01 level) over the lexical kernel alone
and the best result yet reported for this dataset.
Also, the benefit of combining set kernels of
different subsequence lengths l is evident; of the
12 combinations presented Table 2 that include
summed set kernels, nine lead to statistically
significant improvements over the corresponding
lexical kernels taken alone (the remaining three
are also close to significance).
Our experiments also show that the distribu-
tional implementation of set kernels (6) is much
more efficient than the averaging implementation
(5). The time behaviour of the two methods
with increasing set cardinality q and subsequence
length l is illustrated in Figure 1. At the largest
tested values of q and l (1,000 and 3, respectively),
the averaging method takes over 33 days of CPU
time, while the distributional method takes just
over one day. In theory, kave scales quadratically
as q increases; this was not observed because for
many constituent pairs there are not enough con-
text strings available to keep adding as q grows
large, but the dependence is certainly superlinear.
The time taken by kjsd is theoretically linear in q,
but again scales less dramatically in practice. On
the other hand kave is linear in l, while kjsd scales
exponentially. This exponential dependence may
626
kjsd kave
BNC 5-Gram BNC 5-Gram
Length Acc F Acc F Acc F Acc F
1 60.6 58.6 60.3 58.1 59.5 57.6 59.1 56.5
2 61.9* 60.4* 62.6 60.8 62.0 60.5* 61.3 59.1
3 62.5* 60.8* 61.7 59.9 62.8* 61.2** 62.3** 60.8**
?12 62.6* 61.0** 62.3* 60.6* 62.0* 60.3* 61.5 59.2
?23 63.1** 61.6** 62.3* 60.5* 62.2* 60.7* 62.0 60.3
?123 62.9** 61.3** 62.6 60.8* 61.9* 60.4* 62.4* 60.6*
No Set 59.9 57.8 60.2 58.1 59.9 57.8 60.2 58.1
Table 2: Results for set kernel and lexical kernel combination. */** indicate significant improvement at
the 0.05/0.01 level over the corresponding lexical kernel alone, estimated by paired t-tests.
50 250 1000100
102
104
106
108
q
time/s
kave
kjsd
(a) l = 1
50 250 1000100
102
104
106
108
q
time/s
kave
kjsd
(b) l = 2
50 250 1000100
102
104
106
108
q
time/s
kavekjsd
(c) l = 3
Figure 1: Timing results (in seconds, log-scaled) for averaged and Jensen-Shannon set kernels
seem worrying, but in practice only short subse-
quence lengths are used with string kernels. In
situations where set sizes are small but long sub-
sequence features are desired, the averaging ap-
proach may be more appropriate. However, it
seems likely that many applications will be sim-
ilar to the task considered here, where short sub-
sequences are sufficient and it is desirable to use
as much data as possible to represent each set.
We note that calculating the PairClass embedding,
which counts far fewer patterns, took just 1h21m.
For optimal efficiency, it seems best to use a gap-
weighted embedding with small set cardinality;
averaged across five runs kjsd with q = 50 and
l = ?123 took 26m to calculate and still achieved
47.6% Accuracy, 45.1% F-score.
6 Related work
Turney et al (2003) suggest combining various in-
formation sources for solving SAT analogy prob-
lems. However, previous work on compound in-
terpretation has generally used either lexical simi-
larity or relational similarity but not both in com-
bination. Previously proposed lexical models in-
clude the WordNet-based methods of Kim and
Baldwin (2005) and Girju et al (2005), and the
distributional model of O? Se?aghdha and Copes-
take (2008). The idea of using relational similar-
ity to understand compounds goes back at least as
far as Lebowitz? (1988) RESEARCHER system,
which processed patent abstracts in an incremental
fashion and associated an unseen compound with
the relation expressed in a context where the con-
stituents previously occurred.
Turney (2006) describes a method (Latent Rela-
tional Analysis) that extracts subsequence patterns
for noun pairs from a large corpus, using query
expansion to increase the recall of the search and
feature selection and dimensionality reduction to
reduce the complexity of the feature space. LRA
performs well on analogical tasks including com-
pound interpretation, but has very substantial re-
source requirements. Turney (2008) has recently
proposed a simpler SVM-based algorithm for ana-
logical classification called PairClass. While it
does not adopt a set-based or distributional model
of relational similarity, we have noted above that
PairClass implicitly uses a feature representation
similar to the one presented above as (6) by ex-
tracting subsequence patterns from observed co-
occurrences of word pair members. Indeed, Pair-
Class can be viewed as a special case of our frame-
627
work; the differences from the model we have
used consist in the use of a different embedding
function ?PC and a more restricted notion of con-
text, a frequency cutoff to eliminate less common
subsequences and the Gaussian kernel to compare
vectors. While we cannot compare methods di-
rectly as we do not possess the large corpus of
5 ? 1010 words used by Turney, we have tested
the impact of each of these modifications on our
model.4 None improve performance with our set
kernels, but the only statistically significant effect
is that of changing the embedding model as re-
ported in section Section 5. Implementing the full
PairClass algorithm on our corpus yields 46.2%
accuracy, 44.9% F-score, which is again signifi-
cantly worse than all results for the gap-weighted
model with l > 1.
In NLP, there has not been widespread use of
set representations for data items, and hence set
classification techniques have received little at-
tention. Notable exceptions include Rosario and
Hearst (2005) and Bunescu and Mooney (2007),
who tackle relation classification and extraction
tasks by considering the set of contexts in which
the members of a candidate relation argument pair
co-occur. While this gives a set representation for
each pair, both sets of authors apply classifica-
tion methods at the level of individual set mem-
bers rather than directly comparing sets. There
is also a close connection between the multino-
mial probability model we have proposed and the
pervasive bag of words (or bag of n-grams) repre-
sentation. Distributional kernels based on a gap-
weighted feature embedding extend these models
by using bags of discontinuous n-grams and down-
weighting gappy subsequences.
A number of set kernels other than those dis-
cussed here have been proposed in the machine
learning literature, though none of these propos-
als have explicitly addressed the problem of com-
paring sets of strings or other structured objects,
and many are suitable only for comparing sets of
small cardinality. Kondor and Jebara (2003) take a
distributional approach similar to ours, fitting mul-
tivariate normal distributions to the feature space
mappings of setsA andB and comparing the map-
pings with the Bhattacharrya vector inner product.
The model described above in (6) implicitly fits
multinomial distributions in the feature space F ;
4Turney (p.c.) reports that the full PairClass model
achieves 50.0% accuracy, 49.3% F-score.
this seems more intuitive for string kernel embed-
dings that map strings onto vectors of positive-
valued ?counts?. Experiments with Kondor and
Jebara?s Bhattacharrya kernel indicate that it can
in fact come close to the performances reported
in Section 5 but has significantly greater compu-
tational requirements due to the need to perform
costly matrix manipulations.
7 Conclusion and future directions
In this paper we have presented a combined model
of lexical and relational similarity for relational
reasoning tasks. We have developed an efficient
and flexible kernel-based framework for compar-
ing sets of contexts using the feature embedding
associated with a string kernel.5 By choosing a
particular embedding function and a particular in-
ner product on subsequence vectors, the previ-
ously proposed set-averaging and PairClass algo-
rithms for relational similarity can be retrieved as
special cases. Applying our methods to the task
of compound noun interpretation, we have shown
that combining lexical and relational similarity is a
very effective approach that surpasses either simi-
larity model taken individually.
Turney (2008) argues that many NLP tasks can
be formulated in terms of analogical reasoning,
and he applies his PairClass algorithm to a number
of problems including SAT verbal analogy tests,
synonym/antonym classification and distinction
between semantically similar and semantically as-
sociated words. Our future research plans include
investigating the application of our combined sim-
ilarity model to analogical tasks other than com-
pound noun interpretation. A second promising
direction is to investigate relational models for un-
supervised semantic analysis of noun compounds.
The range of semantic relations that can be ex-
pressed by compounds is the subject of some con-
troversy (Ryder, 1994), and unsupervised learning
methods offer a data-driven means of discovering
relational classes.
Acknowledgements
We are grateful to Peter Turney, Andreas Vla-
chos and the anonymous EACL reviewers for their
helpful comments. This work was supported in
part by EPSRC grant EP/C010035/1.
5The treatment presented here has used a string represen-
tation of context, but the method could be extended to other
structural representations for which substructure embeddings
exist, such as syntactic trees (Collins and Duffy, 2001).
628
References
Thorsten Brants and Alex Franz, 2006. Web 1T 5-gram
Corpus Version 1.1. Linguistic Data Consortium.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the ACL-06 Interactive Presentation
Sessions.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the Web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL-07).
Lou Burnard, 1995. Users? Guide for the British Na-
tional Corpus. British National Corpus Consortium.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING-08).
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean-Michel Renders. 2003. Word-sequence ker-
nels. Journal of Machine Learning Research,
3:1059?1082.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of the
15th Conference on Neural Information Processing
Systems (NIPS-01).
Corinna Cortes and Vladimir Vapnik. 1995. Support
vector networks. Machine Learning, 20(3):273?
297.
Nello Cristianini, Jaz Kandola, Andre Elisseeff, and
John Shawe-Taylor. 2001. On kernel target algn-
ment. Technical Report NC-TR-01-087, Neuro-
COLT.
James Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, School of Informatics,
University of Edinburgh.
Barry Devereux and Fintan Costello. 2007. Learning
to interpret novel noun-noun compounds: Evidence
from a category learning experiment. In Proceed-
ings of the ACL-07 Workshop on Cognitive Aspects
of Computational Language Acquisition.
Thomas Ga?rtner, Peter A. Flach, Adam Kowalczyk,
and Alex J. Smola. 2002. Multi-instance kernels.
In Proceedings of the 19th International Conference
on Machine Learning (ICML-02).
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun
compounds. Computer Speech and Language,
19(4):479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of seman-
tic relations between nominals. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations (SemEval-07).
Alfio Gliozzo, Claudio Giuliano, and Carlo Strappar-
ava. 2005. Domain kernels for word sense disam-
biguation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05).
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda, 2005. English Gigaword Corpus, 2nd Edi-
tion. Linguistic Data Consortium.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cate-
gorisation. In Proceedings of the 18th International
Conference on Machine Learning (ICML-01).
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP-05).
Risi Kondor and Tony Jebara. 2003. A kernel between
sets of vectors. In Proceedings of the 20th Interna-
tional Conference on Machine Learning (ICML-03).
Michael Lebowitz. 1988. The use of memory in
text processing. Communications of the ACM,
31(12):1483?1502.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Christopher J. C. H. Watkins.
2002. Text classification using string kernels. Jour-
nal of Machine Learning Research, 2:419?444.
Diarmuid O? Se?aghdha and Ann Copestake. 2007. Co-
occurrence contexts for noun compound interpreta-
tion. In Proceedings of the ACL-07 Workshop on A
Broader Perspective on Multiword Expressions.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proceedings of the 22nd International Conference
on Computational Linguistics (COLING-08).
Barbara Rosario and Marti A. Hearst. 2005. Multi-
way relation classification: Application to protein-
protein interactions. In Proceedings of the 2005
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing (HLT-EMNLP-05).
Mary Ellen Ryder. 1994. Ordered Chaos: The Inter-
pretation of English Noun-Noun Compounds. Uni-
versity of California Press, Berkeley, CA.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press, Cambridge.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining indepen-
dent modules to solve multiple-choice synonym and
analogy problems. In Proceedings of the 2003 Inter-
national Conference on Recent Advances in Natural
Language Processing (RANLP-03).
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (COLING-08).
629
Proceedings of NAACL HLT 2009: Short Papers, pages 237?240,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Semantic classification with WordNet kernels
Diarmuid O? Se?aghdha
Computer Laboratory
University of Cambridge
United Kingdom
do242@cl.cam.ac.uk
Abstract
This paper presents methods for performing
graph-based semantic classification using ker-
nel functions defined on the WordNet lexi-
cal hierarchy. These functions are evaluated
on the SemEval Task 4 relation classification
dataset and their performance is shown to be
competitive with that of more complex sys-
tems. A number of possible future develop-
ments are suggested to illustrate the flexibility
of the approach.
1 Introduction
The estimation of semantic similarity between
words is one of the longest-established tasks in Nat-
ural Language Processing and many approaches to
the problem have been proposed. The two domi-
nant lexical similarity paradigms are distributional
similarity, which compares words on the basis of
their observed co-occurrence behaviour in corpora,
and semantic network similarity, which compares
words based on their position in a graph such as
the WordNet hierarchy. In this paper we consider
measures of network similarity for the purpose of
supervised classification with kernel methods. The
utility of kernel functions related to popular distribu-
tional similarity measures has recently been demon-
strated by O? Se?aghdha and Copestake (2008); we
show here that kernel analogues of WordNet simi-
larity can likewise give good performance on a se-
mantic classification task.
2 Kernels derived from graphs
Kernel-based classifiers such as support vector ma-
chines (SVMs) make use of functions called kernel
functions (or simply kernels) to compute the similar-
ity between data points (Shawe-Taylor and Cristian-
ini, 2004). Valid kernels are restricted to the set of
positive semi-definite (psd) functions, i.e., those that
correspond to an inner product in some vector space.
Kernel methods have been widely adopted in NLP
over the past decade, in part due to the good perfor-
mance of SVMs on many tasks and in part due to the
ability to exploit prior knowledge about a given task
through the choice of an appropriate kernel function.
In this section we consider kernel functions that use
spectral properties of a graph to compute the sim-
ilarity between its nodes. The theoretical founda-
tions and some machine learning applications of the
adopted approach have been developed by Kondor
and Lafferty (2002), Smola and Kondor (2003) and
Herbster et al (2008).
Let G be a graph with vertex set V = v1, . . . , vn
and edge set E ? V ? V . We assume that G is
connected and undirected and that all edges have a
positive weight wij > 0. Let A be the symmetric
n?nmatrix with entriesAij = wij if an edge exists
between vertices vi and vj , and Aij = 0 otherwise.
Let D be the diagonal matrix with entries Dii =?
j?V Aij . The graph Laplacian L is then defined
as
L = D?A (1)
The normalised Laplacian is defined as L? =
D? 12LD? 12 . Both L? and L are positive semi-
definite, but they are typically used as starting points
237
for the derivation of kernels rather than as kernels
themselves.
Let ?1 ? ? ? ? ? ?n be the eigenvalues of L and
u1, . . . , un the corresponding eigenvectors. Note
that un = 0 for all graphs. L is singular and hence
has no well-defined inverse, but its pseudoinverse
L+ is defined as
L+ =
n?1?
i=1
??1i uiuTi (2)
L+ is positive definite, and its entries are related to
the resistance distance between points in an elec-
trical circuit (Herbster et al, 2008) and to the av-
erage commute-time distance, i.e., the average dis-
tance of a random walk from one node to another
and back again (Fouss et al, 2007). The similar-
ity measure defined by L+ hence takes information
about the connectivity of the graph into account as
well as information about adjacency. An analogous
pseudoinverse L?+ can be defined for the normalised
Laplacian.
A second class of graph-based kernel functions
are the diffusion kernels introduced by Kondor and
Lafferty (2002). The kernel Ht is defined as Ht =
e?tL?, or equivalently:
Ht =
n?1?
i=1
exp(?t??i)u?iu?Ti (3)
where t > 0, and ??1 ? ? ? ? ? ??n and u?1, . . . , u?n
are the eigenvalues and eigenvectors of L?+ respec-
tively. Ht can be interpreted in terms of heat diffu-
sion or the distribution of a lazy random walk ema-
nating from a given point at a time point t.
3 Methodology
3.1 Graph construction
WordNet (Fellbaum, 1998) is a semantic network in
which nodes correspond to word senses (or synsets)
and edges correspond to relations between senses.
In this work we restrict ourselves to the noun com-
ponent of WordNet and use only hyponymy and in-
stance hyponymy relations for graph construction.
The version of WordNet used is WordNet 3.0.
To evaluate the utility of the graph-based kernels
described in Section 2 for computing lexical sim-
ilarity, we use the dataset developed for the task
on Classifying Semantic Relations Between Nom-
inals at the 2007 SemEval competition (Girju et
al., 2007). The dataset comprises candidate exam-
ple sentences for seven two-argument semantic rela-
tions, with 140 training sentences and approximately
80 test sentences for each relation. It is a particularly
suitable task for evaluating WordNet kernels, as the
candidate relation arguments for each sentence are
tagged with their WordNet sense and it has been pre-
viously shown that a kernel model based on distribu-
tional lexical similarity can attain very good perfor-
mance (O? Se?aghdha and Copestake, 2008).
3.2 Calculating the WordNet kernels
The noun hierarchy in WordNet 3.0 contains 82,115
senses; computing kernel similarities on a graph of
this size raises significant computational issues. The
calculation of the Laplacian pseudoinverse is com-
plicated by the fact that while L and L? are very
sparse, their pseudoinverses are invariably dense and
require very large amounts of memory. To circum-
vent this problem, we follow Fouss et al (2007)
in computing L+ and L?+ one column at a time
through a Cholesky factorisation procedure. Only
those columns required for the classification task
need be calculated, and the kernel computation for
each relation subtask can be performed in a mat-
ter of minutes. Calculating the diffusion kernel in-
volves an eigendecomposition of L?, meaning that
computing the kernel exactly is infeasible. The so-
lution used here is to approximate Ht by using the
m smallest components of the spectrum of L? when
computing (3); from (2) it can be seen that a similar
approximation can be made to speed up computation
of L+ and L?+.
3.3 Experimental setup
For all kernels and relation datasets, the kernel ma-
trix for each argument position was precomputed
and normalised so that every diagonal entry equalled
1. A small number of candidate arguments are not
annotated with a WordNet sense or are assigned a
non-noun sense; these arguments were assumed to
have self-similarity equal to 1 and zero similarity to
all other arguments. This does not affect the pos-
itive semi-definiteness of the kernel matrices. The
per-argument kernel matrices were summed to give
the kernel matrix for each relation subtask. The ker-
238
Full graph m = 500 m = 1000
Kernel Acc F Acc F Acc F
B 72.1 68.4 - - - -
L+ 73.3 69.4 73.2 70.5 73.6 70.6
L?+ 72.5 70.0 72.7 70.0 74.1 71.0
Ht - - 68.6 64.7 69.8 65.1
Table 1: Results on SemEval Task 4
nels described in Section 2 were compared to a base-
line kernel B. This baseline represents each word as
a binary feature vector describing its synset and all
its hypernym synsets in the WordNet hierarchy, and
calculates the linear kernel between vectors.
All experiments were run using the LIBSVM sup-
port vector machine library (Chang and Lin, 2001).
For each relation the SVM cost parameter was op-
timised in the range (2?6, 2?4, . . . , 212) through
cross-validation on the training set. The diffusion
kernel parameter t was optimised in the same way,
in the range (10?3, 10?2, . . . , 103).
4 Results
Macro-averaged accuracy and F-score for each ker-
nel are reported in Table 1. There is little difference
between the Laplacian and normalised Laplacian
pseudoinverses; both achieve better performance
than the baselineB. The results also suggest that the
reduced-eigenspectrum approximations to L+ and
L?+ may bring benefits in terms of performance as
well as efficiency via a smoothing effect. The best
performance is attained by the approximation to L?+
with m = 1, 000 eigencomponents. The heat ker-
nelHt fares less well; the problem here may be that
the optimal range for the t parameter has not been
identified.
Comparing these results to those of the partici-
pants in the 2007 SemEval task, the WordNet-based
lexical similarity model fares very well. All versions
of L+ and L?+ attain higher accuracy than all but one
of 15 systems in the competition and higher F-score
than all but three. Even the baseline B ranks above
all but the top three systems, suggesting that this too
can be a useful model. This is in spite of the fact that
all systems which made use of the sense annotations
also used a rich variety of other information sources
such as features extracted from the sentence context,
while the models presented here use only the graph
structure of WordNet.1
5 Related work
There is a large body of work on using WordNet
to compute measures of lexical similarity (Budanit-
sky and Hirst, 2006). However, many of these mea-
sures are not amenable for use as kernel functions as
they rely on properties which cannot be expressed
as a vector inner product, such as the lowest com-
mon subsumer of two vertices. Hughes and Ram-
age (2007) present a lexical similarity model based
on random walks on graphs derived from WordNet;
Rao et al (2008) propose the Laplacian pseudoin-
verse on such graphs as a lexical similarity measure.
Both of these works share aspects of the current pa-
per; however, neither address supervised learning or
present an application-oriented evaluation.
Extracting features from WordNet for use in su-
pervised learning is a standard technique (Scott and
Matwin, 1999). Siolas and d?Alche-Buc (2000) and
Basili et al (2006) use a measure of lexical similar-
ity from WordNet as an intermediary to smooth bag-
of-words kernels on documents. Siolas and d?Alche-
Buc use an inverse path-based similarity measure,
while Basili et al use a measure of ?conceptual den-
sity? that is not proven to be positive semi-definite.
6 Conclusion and future work
The main purpose of this paper has been to demon-
strate how kernels that capture spectral aspects of
graph structure can be used to compare nodes in
a lexical hierarchy and thus provide a kernelised
measure of WordNet similarity. As far as we are
aware, these measures have not previously been in-
vestigated in the context of semantic classification.
The resulting WordNet kernels have been evaluated
on the SemEval Task 4 dataset and shown to attain
a higher level of performance than many more com-
plicated systems that participated in that task.
Two obvious shortcomings of the kernels dis-
cussed here are that they are defined on senses
rather than words and that they are computed on a
1Of course, information about lexical similarity is not suf-
ficient to classify all examples. In particular, the models pre-
sented here perform relatively badly on the ORIGIN-ENTITY
and THEME-TOOL relations, while scoring better than all
SemEval entrants on INSTRUMENT-AGENCY and PRODUCT-
PRODUCER.
239
rather impoverished graph structure (the WordNet
hyponym hierarchy is quite tree-like). One of the
significant benefits of spectral graph kernels is that
they can be computed on arbitrary graphs and are
most powerful when graphs have a rich connectiv-
ity structure. Some potential future directions that
would make greater use of this flexibility include the
following:
? A simple extension from sense-kernels to
word-kernels involves adding word nodes to
the WordNet graph, with an edge linking each
word to each of its possible senses. This is sim-
ilar to the graph construction method of Hughes
and Ramage (2007) and Rao et al (2008).
However, preliminary experiments on the Se-
mEval Task 4 dataset indicate that further re-
finement of this approach may be necessary
in order to match the performance of kernels
based on distributional lexical similarity (O?
Se?aghdha and Copestake, 2008).
? Incorporating other WordNet relations such as
meronymy and topicality gives a way of ker-
nelising semantic association or relatedness;
one application of this might be in develop-
ing supervised methods for spelling correction
(Budanitsky and Hirst, 2006).
? A WordNet graph can be augmented with in-
formation from other sources, such as links
based on corpus-derived similarity. Alterna-
tively, the graph-based kernel functions could
be applied to graphs constructed from parsed
corpora (Minkov and Cohen, 2008).
References
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2006. A semantic kernel to classify texts with
very few training examples. Informatica, 30(2):163?
172.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: A library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Francois Fouss, Alain Pirotte, Jean-Michel Renders, and
Marco Saerens. 2007. Random-walk computation of
similarities between nodes of a graph with application
to collaborative recommendation. IEEE Transactions
on Knowledge and Data Engineering, 19(3):355?369.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of semantic
relations between nominals. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-07).
Mark Herbster, Massimiliano Pontil, and Sergio Rojas
Galeano. 2008. Fast prediction on a tree. In Pro-
ceedings of the 22nd Annual Conference on Neural In-
formation Processing Systems (NIPS-08).
Thad Hughes and Daniel Ramage. 2007. Lexical seman-
tic relatedness with random graph walks. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL-
07).
Risi Imre Kondor and John Lafferty. 2002. Diffusion
kernels on graphs and other discrete input spaces. In
Proceedings of the 19th International Conference on
Machine Learning (ICML-02).
Einat Minkov and William W. Cohen. 2008. Learning
graph walk based similarity measures for parsed text.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
08).
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING-08).
Delip Rao, David Yarowsky, and Chris Callison-Burch.
2008. Affinity measures based on the graph Lapla-
cian. In Proceedings of the 3rd TextGraphs Workshop
on Graph-based Algorithms for NLP.
Sam Scott and Stan Matwin. 1999. Feature engineering
for text classification. In Proceedings of the 16th In-
ternational Conference on Machine Learning (ICML-
99).
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, Cambridge.
Georges Siolas and Florence d?Alche-Buc. 2000. Sup-
port vector machines based on a semantic kernel for
text categorization. In Proceedings of the IEEE-INNS-
ENNS International Joint Conference on Neural Net-
works.
Alexander J. Smola and Risi Kondor. 2003. Kernels and
regularization on graphs. In Proceedings of the the
16th Annual Conference on Learning Theory and 7th
Workshop on Kernel Machines (COLT-03).
240
Proceedings of the ACL 2007 Student Research Workshop, pages 73?78,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotating and Learning Compound Noun Semantics
Diarmuid O? Se?aghdha
University of Cambridge Computer Laboratory
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
do242@cl.cam.ac.uk
Abstract
There is little consensus on a standard ex-
perimental design for the compound inter-
pretation task. This paper introduces well-
motivated general desiderata for semantic
annotation schemes, and describes such a
scheme for in-context compound annotation
accompanied by detailed publicly available
guidelines. Classification experiments on an
open-text dataset compare favourably with
previously reported results and provide a
solid baseline for future research.
1 Introduction
There are a number of reasons why the interpreta-
tion of noun-noun compounds has long been a topic
of interest for NLP researchers. Compounds oc-
cur very frequently in English and many other lan-
guages, so they cannot be avoided by a robust se-
mantic processing system. Compounding is a very
productive process with a highly skewed type fre-
quency spectrum, and corpus information may be
very sparse. Compounds are often highly ambigu-
ous and a large degree of ?world knowledge? seems
necessary to understand them. For example, know-
ing that a cheese knife is (probably) a knife for
cutting cheese and (probably) not a knife made of
cheese (cf. plastic knife) does not just require an
ability to identify the senses of cheese and knife but
also knowledge about what one usually does with
cheese and knives. These factors combine to yield
a difficult problem that exhibits many of the chal-
lenges characteristic of lexical semantic process-
ing in general. Recent research has made signifi-
cant progress on solving the problem with statisti-
cal methods and often without the need for manu-
ally created lexical resources (Lauer, 1995; Lapata
and Keller, 2004; Girju, 2006; Turney, 2006). The
work presented here is part of an ongoing project
that treats compound interpretation as a classifica-
tion problem to be solved using machine learning.
2 Selecting an Annotation Scheme
For many classification tasks, such as part-of-speech
tagging or word sense disambiguation, there is gen-
eral agreement on a standard set of categories that
is used by most researchers. For the compound
interpretation task, on the other hand, there is lit-
tle agreement and numerous classification schemes
have been proposed. This hinders meaningful com-
parison of different methods and results. One must
therefore consider how an appropriate annotation
scheme should be chosen.
One of the problems is that it is not immedi-
ately clear what level of granularity is desirable, or
even what kind of units the categories should be.
Lauer (1995) proposes a set of 8 prepositions that
can be used to paraphrase compounds: a cheese
knife is a knife FOR cheese but a kitchen knife is
a knife (used) IN a kitchen. An advantage of this
approach is that preposition-noun co-occurrences
can efficiently be mined from large corpora using
shallow techniques. On the other hand, interpret-
ing a paraphrase requires further disambiguation as
one preposition can map onto many semantic rela-
tions.1 Girju et al (2005) and Nastase and Szpakow-
icz (2003) both present large inventories of seman-
1The interpretation of prepositions is itself the focus of a
Semeval task in 2007.
73
tic relations that describe noun-noun dependencies.
Such relations provide richer semantic information,
but it is harder for both humans and machines to
identify their occurrence in text. Larger invento-
ries can also suffer from class sparsity; for exam-
ple, 14 of Girju et al?s 35 relations do not occur in
their dataset and 7 more occur in less than 1% of
the data. Nastase and Szpakowicz? scheme mitigates
this problem by the presence of 5 supercategories.
Each of these proposals has its own advantages
and drawbacks, and there is a need for principled cri-
teria for choosing one. As the literature on semantic
annotation ?best practice? is rather small,2 I devised
a novel set of design principles based on empirical
and theoretical considerations:
1. The inventory of informative categories should
account for as many compounds as possible
2. The category boundaries should be clear and
categories should describe a coherent concept
3. The class distribution should not be overly
skewed or sparse
4. The concepts underlying the categories should
generalise to other linguistic phenomena
5. The guidelines should make the annotation pro-
cess as simple as possible
6. The categories should provide useful semantic
information
These intuitively appear to be desirable principles
for any semantic annotation scheme. The require-
ment of class distribution balance is motivated by
the classification task. Where one category domi-
nates, the most-frequent-class baseline can be diffi-
cult to exceed and care must be taken in evaluation
to consider macro-averaged performance as well as
raw accuracy. It has been suggested that classifiers
trained on skewed data may perform poorly on mi-
nority classes (Zhang and Oles, 2001). Of course,
this is not a justification for conflating concepts with
little in common, and it may well be that the natural
distribution of data is inherently skewed.
There is clearly a tension between these criteria,
and only a best-fit solution is possible. However, it
was felt that a new scheme might satisfy them more
optimally than existing schemes. Such a proposal
2One relevant work is Wilson and Thomas (1997).
Relation Distribution Example
BE 191 (9.55%) steel knife
HAVE 199 (9.95%) street name
IN 308 (15.40%) forest hut
INST 266 (13.30%) rice cooker
ACTOR 236 (11.80%) honey bee
ABOUT 243 (12.15%) fairy tale
REL 81 (4.05%) camera gear
LEX 35 (1.75%) home secretary
UNKNOWN 9 (0.45%) simularity crystal
MISTAG 220 (11.00%) blazing fire
NONCOMP 212 (10.60%) [real tennis] club
Table 1: Sample class frequencies
necessitates a method of evaluation. Not all the cri-
teria are easily evaluable. It is difficult to prove gen-
eralisability and usefulness conclusively, but it can
be maximised by building on more general work on
semantic representation; for example, the guidelines
introduced here use a conception of events and par-
ticipants compatible with that of FrameNet (Baker
et al, 1998). Good results on agreement and base-
line classification will provide positive evidence for
the coherence and balance of the classes; agreement
measures can confirm ease of annotation.
In choosing an appropriate level of granularity, I
wished to avoid positing a large number of detailed
but rare categories. Levi?s (1978) set of nine se-
mantic relations was used as a starting point. The
development process involved a series of revisions
over six months, aimed at satisfying the six criteria
above and maximising interannotator agreement in
annotation trials. The nature of the decisions which
had to be made is exemplified by the compound car
factory, whose standard referent seems to qualify as
FOR, CAUSE, FROM and IN in Levi?s scheme (and
causes similar problems for the other schemes I am
aware of). Likewise there seems to be no princi-
pled way to choose between a locative or purposive
label for dining room. Such examples led to both
redefinition of category boundaries and changes in
the category set; for example, FOR was replaced by
INST and AGENT, which are independent of purpo-
sivity. This resulted in the class inventory shown in
Table 1 and a detailed set of annotation guidelines.3
3The guidelines are publicly available at http://www.
cl.cam.ac.uk/?do242/guidelines.pdf.
74
The scheme?s development is described at length in
O? Se?aghdha (2007b).
Many of the labels are self-explanatory. AGENT
and INST(rument) apply to sentient and non-
sentient participants in an event respectively, with
ties (e.g., stamp collector) being broken by a hier-
archy of coarse semantic roles. REL is an OTHER-
style category for compounds encoding non-specific
association. LEX(icalised) applies to compounds
which are semantically opaque without prior knowl-
edge of their meanings. MISTAG and NON-
COMP(ound) labels are required to deal with se-
quences that are not valid two-noun compounds but
have been identified as such due to tagging errors
and the simple data extraction heuristic described in
Section 3.1. Coverage is good, as 92% of valid com-
pounds in the dataset described below were assigned
one of the six main semantic relations.
3 Annotation Experiment
3.1 Data
A simple heuristic was used to extract noun se-
quences from the 90 million word written part of the
British National Corpus.4 The corpus was parsed
using the RASP parser5 and all sequences of two
common nouns were extracted except those adjacent
to another noun and those containing non-alphabetic
characters. This yielded almost 1.6 million tokens
with 430,555 types. 2,000 unique tokens were ran-
domly drawn for use in annotation and classification
experiments.
3.2 Method
Two annotators were used: the current author and
an annotator experienced in lexicography but with-
out any special knowledge of compounds or any role
in the development of the annotation scheme. In all
the trials described here, each compound was pre-
sented alongside the sentence in which it was found
in the BNC. The annotators had to assign one of the
labels in Table 1 and the rule that licensed that la-
bel in the annotation guidelines. For example, the
compound forest hut in its usual sense would be an-
notated IN,2,2.1.3.1 to indicate the semantic
4http://www.natcorp.ox.ac.uk/
5http://www.informatics.susx.ac.uk/
research/nlp/rasp/
relation, the direction of the relation (it is a hut in
a forest, not a forest in a hut) and that the label is
licensed by rule 2.1.3.1 in the guidelines (N1/N2 is
an object spatially located in or near N2/N1).6 Two
trial batches of 100 compounds were annotated to
familiarise the second annotator with the guidelines
and to confirm that the guidelines were indeed us-
able for others. The first trial resulted in agreement
of 52% and the second in agreement of 73%. The
result of the second trial, corresponding to a Kappa
beyond-chance agreement estimate (Cohen, 1960)
of ?? = 0.693, was very impressive and it was de-
cided to proceed to a larger-scale task. 500 com-
pounds not used in the trial runs were drawn from
the 2,000-item set and annotated.
3.3 Results and Analysis
Agreement on the test set was 66.2% with ?? = 0.62.
This is less than the score achieved in the second
trial run, but may be a more accurate estimator of the
true population ? due to the larger sample size. On
the other hand, the larger dataset may have caused
annotator fatigue. Pearson standardised residuals
(Haberman, 1973) were calculated to identify the
main sources of disagreement.7 In the context of
inter-annotator agreement one expects these residu-
als to have large positive values on the agreement di-
agonal and negative values in all other cells. Among
the six main relations listed at the top of Table 1,
a small positive association was observed between
INST and ABOUT, indicating that borderline topics
such as assessment task and gas alarm were likely
to be annotated as INST by the first annotator and
ABOUT by the second. It seems that the guidelines
might need to clarify this category boundary.
It is clear from analysis of the data that the REL,
LEX and UNKNOWN categories show very low
agreement. They all have low residuals on the agree-
ment diagonal (that for UNKNOWN is negative) and
numerous positive entries off it. REL and LEX are
also the categories for which it is most difficult to
6The additional information provided by the direction and
rule annotations could be used to give a richer classification
scheme but has not yet been used in this way in my experiments.
7The standardised residual of cell ij is calculated as
eij =
nij ? p?i+p?+j
?
p?i+p?+j(1? p?i+)(1? p?+j)
where nij is the observed value of cell ij and p?i+, p?+j are row
and column marginal probabilities estimated from the data.
75
provide clear guidelines. On the other hand, the
MISTAG and NONCOMP categories showed good
agreement, with slightly higher agreement residu-
als than the other categories. To get a rough idea
of agreement on the six categories used in the clas-
sification experiments described below, agreement
was calculated for all items which neither annota-
tor annotated with any of REL, LEX, UNKNOWN,
MISTAG and NONCOMP. This left 343 items with
agreement of 73.6% and ?? = 0.683.
3.4 Discussion
This is the first work I am aware of where com-
pounds were annotated in their sentential context.
This aspect is significant, as compound meaning is
often context dependent (compare school manage-
ment decided. . . and principles of school manage-
ment) and in-context interpretation is closer to the
dynamic of real-world language use. Context can
both help and hinder agreement, and it is not clear
whether in- or out-of-context annotation is easier.
Previous work has given out-of-context agree-
ment figures for corpus data. Kim and Bald-
win (2005) report an experiment using 2,169 com-
pounds taken from newspaper text and the categories
of Nastase and Szpakowicz (2003). Their annota-
tors could assign multiple labels in case of doubt
and were judged to agree on an item if their anno-
tations had any label in common. This less strin-
gent measure yielded agreement of 52.31%. Girju
et al (2005) report agreement for annotation using
both Lauer?s 8 prepositional labels (?? = 0.8) and
their own 35 semantic relations (?? = 0.58). These
figures are difficult to interpret as annotators were
again allowed assign multiple labels (for the prepo-
sitions this occurred in ?almost all? cases) and the
multiply-labelled items were excluded from the cal-
culation of Kappa. This entails discarding the items
which are hardest to classify and thus most likely to
cause disagreement.
Girju (2006) has recently published impressive
agreement results on a related task. This involved
annotating 2,200 compounds extracted from an on-
line dictionary, each presented in five languages, and
resulted in a Kappa score of 0.67. This task may
have been facilitated by the data source and its mul-
tilingual nature. It seems plausible that dictionary
entries are more likely to refer to familiar concepts
than compounds extracted from a balanced corpus,
which are frequently context-dependent coinages or
rare specialist terms. Furthermore, the translations
of compounds in Romance languages often pro-
vide information that disambiguates the compound
meaning (this aspect was the main motivation for the
work) and translations from a dictionary are likely
to correspond to an item?s most frequent meaning.
A qualitative analysis of the experiment described
above suggests that about 30% of the disagreements
can confidently be attributed to disagreement about
the semantics of a given compound (as opposed to
how a given meaning should be annotated).8
4 SVM Learning with Co-occurrence Data
4.1 Method
The data used for classification was taken from the
2,000 items used for the annotation experiment, an-
notated by a single annotator. Due to time con-
straints, this annotation was done before the second
annotator had been used and was not changed af-
terwards. All compounds annotated as BE, HAVE,
IN, INST, AGENT and ABOUT were used, giving a
dataset of 1,443 items. All experiments were run us-
ing Support Vector Machine classifiers implemented
in LIBSVM.9 Performance was measured via 5-fold
cross-validation. Best performance was achieved
with a linear kernel and one-against-all classifica-
tion. The single SVM parameter C was estimated
for each fold by cross-validating on the training set.
Due to the efficiency of the linear kernel the optimi-
sation, training and testing steps for each fold could
be performed in under an hour.
I investigated what level of performance could
be achieved using only corpus information. Feature
vectors were extracted from the written BNC for
each modifier and head in the dataset under the
following conditions:
w5, w10: Each word within a window of 5 or 10
words on either side of the item is a feature.
Rbasic, Rmod, Rverb, Rconj: These feature sets
8For example, one annotator thought peat boy referred to a
boy who sells peat (AGENT) while the other thought it referred
to a boy buried in peat (IN).
9http://www.csie.ntu.edu.tw/?cjlin/
libsvm
76
use the grammatical relation output of the RASP
parser run over the written BNC. The Rbasic feature
set conflates information about 25 grammatical
relations; Rmod counts only prepositional, nominal
and adjectival noun modification; Rverb counts
only relations among subjects, objects and verbs;
Rconj counts only conjunctions of nouns. In each
case, each word entering into one of the target
relations with the item is a feature and only the
target relations contribute to the feature values.
Each feature vector counts the target word?s co-
occurrences with the 10,000 words that most fre-
quently appear in the context of interest over the en-
tire corpus. Each compound in the dataset is rep-
resented by the concatenation of the feature vectors
for its head and modifier. To model aspects of co-
occurrence association that might be obscured by
raw frequency, the log-likelihood ratio G2 was used
to transform the feature space.10
4.2 Results and Analysis
Results for these feature sets are given in Table 2.
The simple word-counting conditions w5 and w10
perform relatively well, but the highest accuracy is
achieved by Rconj. The general effect of the log-
likelihood transformation cannot be stated categor-
ically, as it causes some conditions to improve and
others to worsen, but the G2-transformed Rconj fea-
tures give the best results of all with 54.95% ac-
curacy (53.42% macro-average). Analysis of per-
formance across categories shows that in all cases
accuracy is lower (usually below 30%) on the BE
and HAVE relations than on the others (often above
50%). These two relations are least common in the
dataset, which is why the macro-averaged figures are
slightly lower than the micro-averaged accuracy.
4.3 Discussion
It is interesting that the conjunction-based features
give the best performance, as these features are also
the most sparse. This may be explained by the fact
that words appearing in conjunctions are often tax-
onomically similar (Roark and Charniak, 1998) and
that taxonomic information is particularly useful for
10This measure is relatively robust where frequency counts
are low and consistently outperformed other association mea-
sures in the empirical evaluation of Evert (2004).
Raw G2
Accuracy Macro Accuracy Macro
w5 52.60% 51.07% 51.35% 49.93%
w10 51.84% 50.32% 50.10% 48.60%
Rbasic 51.28% 49.92% 51.83% 50.26%
Rmod 51.35% 50.06% 48.51% 47.03%
Rverb 48.79% 47.13% 48.58% 47.07%
Rconj 54.12% 52.44% 54.95% 53.42%
Table 2: Performance of BNC co-occurrence data
compound interpretation, as evidenced by the suc-
cess of WordNet-based methods (see Section 5).
In comparing reported classification results, it is
difficult to disentangle the effects of different data,
annotation schemes and classification methods. The
results described here should above all be taken to
demonstrate the feasibility of learning using a well-
motivated annotation scheme and to provide a base-
line for future work on the same data. In terms of
methodology, Turney?s (2006) Vector Space Model
experiments are most similar. Using feature vec-
tors derived from lexical patterns and frequencies re-
turned by a Web search engine, a nearest-neighbour
classifier achieves 45.7% accuracy on compounds
annotated with 5 semantic classes. Turney improves
accuracy to 58% with a combination of query ex-
pansion and linear dimensionality reduction. This
method trades off efficiency for accuracy, requiring
many times more resources in terms of time, stor-
age and corpus size than that described here. Lap-
ata and Keller (2004) obtain accuracy of 55.71% on
Lauer?s (1995) prepositionally annotated data using
simple search engine queries. Their method has the
advantage of not requiring supervision, but it cannot
be used with deep semantic relations.
5 SVM Classification with WordNet
5.1 Method
The experiments reported in this section make a ba-
sic use of the WordNet11 hierarchy. Binary feature
vectors are used whereby a vector entry is 1 if the
item belongs to or is a hyponym of the synset corre-
sponding to that feature, and 0 otherwise. Each com-
pound is represented by the concatenation of two
such vectors, for the head and modifier. The same
11http://wordnet.princeton.edu/
77
classification method is used as in Section 4.
5.2 Results and Discussion
This method achieves accuracy of 56.76% and
macro-averaged accuracy of 54.59%, slightly higher
than that achieved by the co-occurrence features.
Combining WordNet and co-occurrence vectors by
simply concatenating the G2-transformed Rconj
vector and WordNet feature vector for each com-
pound gives a further boost to 58.35% accuracy
(56.70% macro-average).
These results are higher than those reported for
similar approaches on open-text data (Kim and
Baldwin, 2005; Girju et al, 2005), though the same
caveat applies about comparison. The best results
(over 70%) reported so far for compound inter-
pretation use a combination of multiple lexical re-
sources and detailed additional annotation (Girju et
al., 2005; Girju, 2006).
6 Conclusion and Future Directions
The annotation scheme described above has been
tested on a rigorous multiple-annotator task and
achieved superior agreement to comparable results
in the literature. Further refinement should be possi-
ble but would most likely yield diminishing returns.
In the classification experiments, my goal was to
see what level of performance could be gained by
using straightforward techniques so as to provide
a meaningful baseline for future research. Good
results were achieved with methods that rely nei-
ther on massive corpora or broad-coverage lexical
resources, though slightly better performance was
achieved using WordNet. An advantage of resource-
poor methods is that they can be used for the many
languages where compounding is common but such
resources are limited.
The learning approach described here only cap-
tures the lexical semantics of the individual con-
situents. It seems intuitive that other kinds of corpus
information would be useful; in particular, contexts
in which the head and modifier of a compound both
occur may make explicit the relations that typically
hold between their referents. Kernel methods for us-
ing such relational information are investigated in O?
Se?aghdha (2007a) with promising results, and I am
continuing my research in this area.
References
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Proc. ACL-
COLING-98, pages 86?90, Montreal, Canada.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37?46.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
Universita?t Stuttgart.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19(4):479?496.
Roxana Girju. 2006. Out-of-context noun phrase seman-
tic interpretation with cross-linguistic evidence. In
Proc. CIKM-06, pages 268?276, Arlington, VA.
Shelby J. Haberman. 1973. The analysis of residuals in
cross-classified tables. Biometrics, 29(1):205?220.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet sim-
ilarity. In Proc. IJCNLP-05, pages 945?956, Jeju Is-
land, Korea.
Mirella Lapata and Frank Keller. 2004. The Web as a
baseline: Evaluating the performance of unsupervised
Web-based models for a range of NLP tasks. In Proc.
HLT-NAACL-04, pages 121?128, Boston, MA.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University.
Judith N. Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proc. IWCS-5,
Tilburg, Netherlands.
Brian Roark and Eugene Charniak. 1998. Noun-
phrase co-occurrence statistics for semi-automatic se-
mantic lexicon construction. In Proc. ACL-COLING-
98, pages 1110?1106, Montreal, Canada.
Diarmuid O? Se?aghdha. 2007a. Co-occurrence contexts
for corpus-based noun compound interpretation. In
Proc. of the ACL Workshop A Broader Perspective on
Multiword Expressions, Prague, Czech Republic.
Diarmuid O? Se?aghdha. 2007b. Designing and evaluating
a semantic annotation scheme for compound nouns. In
Proc. Corpus Linguistics 2007, Birmingham, UK.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Andrew Wilson and Jenny Thomas. 1997. Semantic an-
notation. In R. Garside, G. Leech, and A. McEnery,
editors, Corpus Annotation. Longman, London.
Tong Zhang and Frank J. Oles. 2001. Text categorization
based on regularized linear classification methods. In-
formation Retrieval, 4(1):5?31.
78
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 57?64,
Prague, June 2007. c?2007 Association for Computational Linguistics
Co-occurrence Contexts for Noun Compound Interpretation
Diarmuid O? Se?aghdha
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
do242@cl.cam.ac.uk
Ann Copestake
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
aac10@cl.cam.ac.uk
Abstract
Contextual information extracted from cor-
pora is frequently used to model seman-
tic similarity. We discuss distinct classes
of context types and compare their effec-
tiveness for compound noun interpretation.
Contexts corresponding to word-word sim-
ilarity perform better than contexts corre-
sponding to relation similarity, even when
relational co-occurrences are extracted from
a much larger corpus. Combining word-
similarity and relation-similarity kernels fur-
ther improves SVM classification perfor-
mance.
1 Introduction
The compound interpretation task is frequently cast
as the problem of classifying an unseen compound
noun with one of a closed set of relation categories.
These categories may consist of lexical paraphrases,
such as the prepositions of Lauer (1995), or deeper
semantic relations, such as the relations of Girju et
al. (2005) and those used here. The challenge lies in
the fact that by their very nature compounds do not
give any surface realisation to the relation that holds
between their constituents. To identify the differ-
ence between bread knife and steel knife it is not suf-
ficient to assign correct word-senses to bread, steel
and knife; it is also necessary to reason about how
the entities referred to interact in the world. A com-
mon assumption in data-driven approaches to the
problem is that compounds with semantically sim-
ilar constituents will encode similar relations. If a
hearer knows that a fish knife is a knife used to eat
fish, he/she might conclude that the novel compound
pigeon fork is a fork used to eat pigeon given that
pigeon is similar to fish and knife is similar to fork.
A second useful intuition is that word pairs which
co-occur in similar contexts are likely to enter into
similar relations.
In this paper, we apply these insights to identify
different kinds of contextual information that cap-
ture different kinds of similarity and compare their
applicability using medium- to large-sized corpora.
In keeping with most other research on the prob-
lem,1 we take a supervised learning approach to
compound interpretation.
2 Defining Contexts for Compound
Interpretation
When extracting corpus information to interpret a
compound such as bread knife, there are a number
of context types that might plausibly be of interest:
1. The contexts in which instances of the com-
pound type appear (type similarity); e.g., all
sentences in the corpus that contain the com-
pound bread knife.
2. The contexts in which instances of each con-
stituent appear (word similarity); e.g., all sen-
tences containing the word bread or the word
knife.
3. The contexts in which both constituents appear
together (relation similarity); e.g., all sentences
containing both bread and knife.
4. The context in which the particular compound
token was found (token similarity).
1Such as Girju et al (2005), Girju (2006), Turney (2006).
Lapata and Keller?s (2004) unsupervised approach is a notable
exception.
57
A simple but effective method for exploiting these
contexts is to count features that co-occur with the
target items in those contexts. Co-occurrence may
be defined in terms of proximity in the text, lexi-
cal patterns, or syntactic patterns in a parse graph.
We can parameterise our notion of context further,
for example by enforcing a constraint that the co-
occurrence correspond to a particular type of gram-
matical relation or that co-occurrence features be-
long to a particular word class.2
Research in NLP frequently makes use of one or
more of these similarity types. For example, Culotta
and Sorensen (2004) combine word similarity and
relation similarity for relation extraction; Gliozzo et
al. (2005) combine word similarity and token simi-
larity for word sense disambiguation. Turney (2006)
discusses word similarity (which he calls ?attribu-
tional similarity?) and relation similarity, but fo-
cusses on the latter and does not perform a compar-
ative study of the kind presented here.
The experiments described here investigate type,
word and relation similarity. However, token simi-
larity clearly has a role to play in the interpretation
task, as a given compound type can have a differ-
ent meaning in different contexts ? for example, a
school book can be a book used in school, a book
belonging to a school or a book about a school. As
our data have been annotated in context, we intend
to model this dynamic in future work.
3 Experimental Setup
3.1 Data
We used the dataset of 1443 compounds whose
development is described in O? Se?aghdha (2007).
These compounds have been annotated in their sen-
tential contexts using the six deep semantic rela-
tions listed in Table 1. On the basis of a dual-
annotator study, O? Se?aghdha reports agreement of
66.2% (?? = 0.62) on a more general task of an-
notating a noisy corpus and estimated agreement of
73.6% (?? = 0.68) on annotating the six relations
used here. These figures are superior to previously
reported results on annotating compounds extracted
from corpora. Always choosing the most frequent
class (IN) would give accuracy of 21.34%, and we
2A flexible framework for this kind of context definition is
presented by Pado? and Lapata (2003).
Relation Distribution Example
BE 191 (13.24%) steel knife, elm tree
HAVE 199 (13.79%) street name, car door
IN 308 (21.34%) forest hut, lunch time
INST 266 (18.43%) rice cooker, bread knife
ACTOR 236 (16.35%) honey bee, bus driver
ABOUT 243 (16.84%) fairy tale, history book
Table 1: The 6 relation classes and their distribution
in the dataset
use this as a baseline for our experiments.
3.2 Corpus
The written section of the British National Corpus,3
consisting of around 90 million words, was used in
all our experiments. This corpus is not large com-
pared to other corpora used in NLP, but it has been
manually compiled with a view to a balance of genre
and should be more representative of the language in
general than corpora containing only newswire text.
Furthermore, the compound dataset was also ex-
tracted from the BNC and information derived from
it will arguably describe the data items more accu-
rately than information from other sources. How-
ever, this information may be very sparse given the
corpus? size. For comparison we also use a 187
million word subset of the English Gigaword Cor-
pus (Graff, 2003) to derive relational information
in Section 6. This subset consists of every para-
graph in the Gigaword Corpus belonging to articles
tagged as ?story? and containing both constituents of
a compound in the dataset, whether or not they are
compounded there. Both corpora were lemmatised,
tagged and parsed with RASP (Briscoe et al, 2006).
3.3 Learning Algorithm
In all our experiments we use a one-against-all im-
plementation of the Support Vector Machine.4 Ex-
cept for the work described in Section 6.2 we used
the linear kernel K(x, y) = x ?y to compute similar-
ity between vector representations of the data items.
The linear kernel consistently achieved superior per-
formance to the more flexible Gaussian kernel in
a range tests, presumably due to the sensitivity of
3http://www.natcorp.ox.ac.uk/
4The software used was LIBSVM (Chang and Lin, 2001).
58
the Gaussian kernel to its parameter settings.5 One-
against-all classification (training one classifier per
class) performed better than one-against-one (train-
ing one classifier for each pair of classes). We es-
timate test accuracy by 5-fold cross-validation and
within each fold we perform further 5-fold cross-
validation on the training set to optimise the single
SVM parameter C. An advantage of the linear kernel
is that learning is very efficient. The optimisation,
training and testing steps for each fold take from less
than a minute on a single processor for the sparsest
feature vectors to a few hours for the most dense, and
the folds can easily be distributed across machines.
4 Word Similarity
O? Se?aghdha (2007) investigates the effectiveness of
word-level co-occurrences for compound interpre-
tation, and the results presented in this section are
taken from that paper. Co-occurrences were identi-
fied in the BNC for each compound constituent in
the dataset, using the following context definitions:
win5, win10: Each word within a window of 5 or
10 words on either side of the item is a feature.
Rbasic, Rmod, Rverb, Rconj: These feature sets
use the grammatical relation output of the
RASP parser run over the written BNC. The
Rbasic feature set conflates information about
25 grammatical relations; Rmod counts only
prepositional, nominal and adjectival noun
modification; Rverb counts only relations
among subjects, objects and verbs; Rconj
counts only conjunctions of nouns.
The feature vector for each target constituent counts
its co-occurrences with the 10,000 words that most
frequently appear in the co-occurrence relations of
interest over the entire corpus. A feature vector for
each compound was created by appending the vec-
tors for its modifier and head, and these compound
vectors were used for SVM learning. To model as-
pects of co-occurrence association that might be ob-
scured by raw frequency, the log-likelihood ratio G2
(Dunning, 1993) was also used to transform the fea-
ture space.
5Keerthi and Lin (2003) prove that the Gaussian kernel will
always do as well as or better than the linear kernel for binary
classification. For multiclass classification we use multiple bi-
Raw G2
Accuracy Macro Accuracy Macro
w5 52.60% 51.07% 51.35% 49.93%
w10 51.84% 50.32% 50.10% 48.60%
Rbasic 51.28% 49.92% 51.83% 50.26%
Rmod 51.35% 50.06% 48.51% 47.03%
Rverb 48.79% 47.13% 48.58% 47.07%
Rconj 54.12% 52.44% 54.95% 53.42%
Table 2: Classification results for word similarity
Micro- and macro-averaged performance figures
are given in Table 2. The micro-averaged figure
is calculated as the overall proportion of items that
were classified correctly, whereas the macro-average
is calculated as the average of the accuracy on each
class and thus balances out any skew in the class
distribution. In all cases macro-accuracy is lower
than micro-accuracy; this is due to much better per-
formance on the relations IN, INST, ACTOR and
ABOUT than on BE and HAVE. This may be be-
cause those two relations are slightly rarer and hence
provide less training data, or it may reflect a dif-
ference in the suitability of co-occurrence data for
their classification. It is interesting that features de-
rived only from conjunctions give the best perfor-
mance; these features are the most sparse but ap-
pear to be of high quality. The information con-
tained in conjunctions is conceptually very close to
the WordNet-derived information frequently used in
word-similarity based approaches to compound se-
mantics, and the performance of these features is not
far off the 56.76% accuracy (54.6% macro-average)
reported for WordNet-based classification for the
same dataset by O? Se?aghdha (2007).
5 Type Similarity
Type similarity is measured by identifying co-
occurrences with each instance of the compound
type in the corpus. In effect, we are treating com-
pounds as single words and calculating their word
similarity with each other. The same feature extrac-
tion methods were used as in the previous section.
Classification results are given in Table 3.
This method performs very poorly. Sparsity is un-
doubtedly a factor: 513 of the 1,443 compounds oc-
nary classifiers with a shared set of parameters which may not
be optimal for any single classifier.
59
Accuracy Macro
win5 28.62% 27.71%
win10 30.01% 28.69%
Rbasic 29.31% 28.22%
Rmod 26.54% 25.30%
Rverb 25.02% 23.96%
Rconj 24.60% 24.48%
Table 3: Classification results for type similarity
cur 5 times or fewer in the BNC and 186 occur just
once. The sparser feature sets (Rmod, Rverb and
Rconj) are all outperformed by the more dense ones.
However, there is also a conceptual problem with
type similarity, in that the context of a compound
may contain information about the referent of the
compound but is less likely to contain information
about the implicit semantic relation. For example,
the following compounds all encode different mean-
ings but are likely to appear in similar contexts:
? John cut the bread with the kitchen knife.
? John cut the bread with the steel knife.
? John cut the bread with the bread knife.
6 Relation Similarity
6.1 Vector Space Kernels
The intuition underlying the use of relation similar-
ity is that while the relation between the constituents
of a compound may not be made explicit in the con-
text of that compound, it may be described in other
contexts where both constituents appear. For ex-
ample, sentences containing both bread and knife
may contain information about the typical interac-
tions between their referents. To extract feature vec-
tors for each constituent pair, we took the maximal
context unit to be each sentence in which both con-
stituents appear, and experimented with a range of
refinements to that context definition. The result-
ing definitions are given below in order of intuitive
richness, from measures based on word-counting to
measures making use of the structure of the sen-
tence?s dependency parse graph.
allwords All words in the sentence are co-
occurrence features. This context may be pa-
rameterised by specifying a limit on the win-
dow size to the left of the leftmost constituent
and to the right of the rightmost constituent i.e.,
the words between the two constituents are al-
ways counted.
midwords All words between the constituents are
counted.
allGRs All words in the sentence entering into a
grammatical relation (with any other word) are
counted. This context may be parameterised by
specifying a limit on the length of the shortest
path in the dependency graph from either of the
target constituents to the feature word.
shortest path All words on the shortest depen-
dency path between the two constituents are
features. If there is no such path, no features
are extracted.
path triples The shortest dependency path is de-
composed into a set of triples and these triples
are used as features. Each triple consists of a
node on the shortest path (the triple?s centre
node) and two edges connecting that node with
other nodes in the parse graph (not necessarily
nodes on the path). To generate further triple
features, one or both of the off-centre nodes is
replaced by part(s) of speech. For example, the
RASP dependency parse of The knife cut the
fresh bread is:
(|ncsubj| |cut:3_VVD| |knife:2_NN1| _)
(|dobj| |cut:3_VVD| |bread:6_NN1|)
(|det| |bread:6_NN1| |the:4_AT|)
(|ncmod| _ |bread:6_NN1| |fresh:5_JJ|)
(|det| |knife:2_NN1| |The:1_AT|)
The derived set of features includes the triples
{the:A:det?knife:N?cut:V:ncsubj,
A:det?knife:N?cut:V:ncsubj,
the:A:det?knife:N?V:ncsubj,
A:det?knife:N?V:ncsubj,
knife:N:ncsubj?cut:V?bread:N:dobj,
N:ncsubj?cut:V?bread:N:dobj,
knife:N:ncsubj?cut:V?N:dobj,
N:ncsubj?cut:V?N:dobj,. . .}
(The? and? arrows indicate the direction of
the head-modifier dependency)
60
0 100 200 300 400 500
30
35
40
45
50
55
60
Threshold
Ac
cu
rac
y
aw5
trip
Figure 1: Effect of BNC frequency on test item ac-
curacy for the allwords5 and triples contexts
Table 4 presents results for these contexts; in
the case of parameterisable contexts the best-
performing parameter setting is presented. We are
currently unable to present results for the path-based
contexts using the Gigaword corpus. It is clear
from the accuracy figures that we have not matched
the performance of the word similarity approach.
The best-performing single context definition is all-
words with a window parameter of 5, which yields
accuracy of 38.74% (36.78% macro-average). We
can combine the contributions of two contexts by
generating a new kernel that is the sum of the lin-
ear kernels for the individual contexts;6 the sum of
allwords5 and triples achieves the best performance
with 42.34% (40.20% macro-average).
It might be expected that the richer context def-
initions provide sparser but more precise informa-
tion, and that their relative performance might im-
prove when only frequently observed word pairs are
to be classified. However, thresholding inclusion
in the test set on corpus frequency belies that ex-
pectation; as the threshold increases and the test-
6The summed kernel function value for a pair of items is
simply the sum of the two kernel functions? values for the pair,
i.e.:
Ksum(x, y) = K1(?1(x), ?1(y)) +K2(?2(x), ?2(y))
where ?1, ?2 are the context representations used by the two
kernels. A detailed study of kernel combination is presented by
Joachims et al (2001).
0 100 200 300 400 500
0
50
0
10
00
15
00
Threshold
Siz
e GW
BNC
Figure 2: Effect of corpus frequency on dataset size
for the BNC and Gigaword-derived corpus
ing data contains only more frequent pairs, all con-
texts show improved performance but the effect is
strongest for the allwords and midwords contexts.
Figure 1 shows threshold-accuracy curves for two
representative contexts (the macro-accuracy curves
are similar).
For all frequency thresholds above 6, the number
of noun pairs with above-threshold corpus frequency
is greater for the Gigaword corpus than for the BNC,
and this effect is amplified with increasing threshold
(see Figure 2). However, this difference in sparsity
does not always induce an improvement in perfor-
mance, but nor does the difference in corpus type
consistently favour the BNC.
BNC Gigaword
Accuracy Macro Accuracy Macro
aw 35.97% 33.39% 34.58% 32.62%
aw5 38.74% 36.78% 37.28% 35.25%
mw 32.29% 30.38% 36.24% 34.25%
agr 35.34% 33.40% 35.34% 33.34%
agr2 36.73% 34.81% 37.28% 35.59%
sp 33.54% 31.51%
trip 35.62% 34.39%
aw5+ 42.34% 40.20%
trip
Table 4: Classification results for relation similarity
61
6.2 String Kernels
The classification techniques described in the pre-
vious subsection represent the relational context for
each word pair as a co-occurrence vector in an in-
ner product space and compute the similarity be-
tween two pairs as a function of their vector repre-
sentations. A different kind of similarity measure is
provided by string kernels, which count the num-
ber of subsequences shared by two strings. This
class of kernel function implicitly calculates an in-
ner product in a feature space indexed by all pos-
sible subsequences (possibly restricted by length or
contiguity), but the feature vectors are not explic-
itly represented. This approach affords our notion of
context an increase in richness (features can be se-
quences of length ? 1) without incurring the com-
putational cost of the exponential growth in the di-
mension of our feature space. A particularly flexible
string kernel is the gap-weighted kernel described by
Lodhi et al (2002), which allows the subsequences
to be non-contiguous but penalises the contribution
of each subsequence to the kernel value according to
the number of items occurring between the start and
end of the subsequence, including those that do not
belong to the subsequence (the ?gaps?).
The kernel is defined as follows. Let s and t
be two strings of words belonging to a vocabulary
?. A subsequence u of s is defined by a sequence
of indices i = (i1, . . . , i|u|) such that 1 ? i1 <
. . . < i|u| ? |s|, where s is the length of s. Let
l(i) = i|u|? i1 +1 be the length of the subsequence
in s. For example, if s is the string ?cut the bread
with the knife? and u is the subsequence ?cut with?
indexed by i then l(i) = 4. ? is a decay parameter
between 0 and 1. The gap-weighted kernel value for
subsequences of length n of strings s and t is given
by
KSn(s, t) =
?
u??n
?
i,j:s[i]=u=t[j]
?l(i)+l(j)
Directly computing this function would be in-
tractable, as the sum is over all |?|n possible sub-
sequences of length n; however, Lodhi et al (2002)
present an efficient dynamic programming algo-
rithm that can evaluate the kernel in O(n|s||t|) time.
Those authors? application of string kernels to text
categorisation counts sequences of characters, but it
is generally more suitable for NLP applications to
use sequences of words (Cancedda et al, 2003).
This kernel calculates a similarity score for a pair
of strings, but for context-based compound classi-
fication we are interested in the similarity between
two sets of strings. We therefore define a context
kernel, which sums the kernel scores for each pair
of strings from the two context sets C1, C2 and nor-
malises them by the number of pairs contributing to
the sum:
KCn(C1, C2) =
1
|C1||C2|
?
s?C1,t?C2
KSn(s, t)
That this is a valid kernel (i.e., defines an inner prod-
uct in some induced vector space) can be proven us-
ing the definition of the derived subsets kernel in
Shawe-Taylor and Cristianini (2004, p. 317). In our
experiments we further normalise the kernel to en-
sure that KCn(C1, C2) = 1 if and only if C1 = C2.
To generate the context set for a given word pair,
we extract a string from every sentence in the BNC
where the pair of words occurs no more than eight
words apart. On the hypothesis that the context
between the target words was most important and
to avoid the computational cost incurred by long
strings, we only use this middle context. To facilitate
generalisations over subsequences, the compound
head is replaced by a marker HEAD and the modifier
is replaced by a marker MOD. Word pairs for which
no context strings were extracted (i.e., pairs which
only occur as compounds in the corpus) are repre-
sented by a dummy string that matches no other. The
value of ? is set to 0.5 as in Cancedda et al (2003).
Table 5 presents results for the context kernels with
subsequence lengths 1,2,3 as well as the kernel sum
of these three kernels. These kernels perform better
than the relational vector space kernels, with the ex-
ception of the summed allwords5 + triples kernel.
7 Combining Contexts
We can use the method of kernel summation to com-
bine information from different context types. If our
intuition is correct that type and relation similarity
provide different ?views? of the same semantic rela-
tion, we would expect their combination to give bet-
ter results than either taken alone. This is also sug-
gested by the observation that the different context
62
Accuracy Macro
n = 1 15.94% 19.88%
n = 2 39.09% 37.23%
n = 3 39.29% 39.29%
?1,2,3 40.61% 38.53%
Table 5: Classification results for gap-weighted
string kernels with subsequence lengths 1,2,3 and
the kernel sum of these kernels
Accuracy Macro
Rconj-G2 + aw5 54.95% 53.50%
Rconj-G2 + triples 56.20% 54.54%
Rconj-G2 + aw5 + triples 55.86% 54.13%
Rconj-G2 + KC2 56.48% 54.89%
Rconj-G2 + KC? 56.55% 54.96%
Table 6: Classification results for context combina-
tions
types favour different relations: the summed string
kernel is the best at identifying IN relations (70.45%
precision, 46.67% recall), but Rconj-G2 is best at
identifying all others. This intuition is confirmed by
our experiments, the results of which appear in Ta-
ble 6. The best performance of 56.55% accuracy
(54.96% macro-average) is attained by the com-
bination of the G2-transformed Rconj word simi-
larity kernel and the summed string kernel KC? .
We note that this result, using only information ex-
tracted from the BNC, compares favourably with the
56.76% accuracy (54.60% macro-average) results
described by O? Se?aghdha (2007) for a WordNet-
based method. The combination of Rconj-G2 and
triples is also competitive, demonstrating that a less
flexible learning algorithm (the linear kernel) can
perform well if it has access to a richer source of
information (dependency paths).
8 Comparison with Prior Work
Previous work on compound semantics has tended
to concentrate on either word or relation similarity.
Approaches based on word similarity generally use
information extracted from WordNet. For example,
Girju et al (2005) train SVM classifiers on hyper-
nymy features for each constituent. Their best re-
ported accuracy with an equivalent level of supervi-
sion to our work is 54.2%; they then improve perfor-
mance by adding a significant amount of manually-
annotated semantic information to the data, as does
Girju (2006) in a multilingual context. It is difficult
to make any conclusive comparison with these re-
sults due to fundamental differences in datasets and
classification schemes.
Approaches based on relational similarity of-
ten use relative frequencies of fixed lexical se-
quences estimated from massive corpora. Lap-
ata and Keller (2004) use Web counts for phrases
Noun P Noun where P belongs to a predefined set
of prepositions. This unsupervised approach gives
state-of-the-art results on the assignment of prepo-
sitional paraphrases, but cannot be applied to deep
semantic relations which cannot be directly identi-
fied in text. Turney and Littman (2005) search for
phrases Noun R Noun where R is one of 64 ?join-
ing words?. Turney (2006) presents a more flexible
framework in which automatically identified n-gram
features replace fixed unigrams and additional word
pairs are generated by considering synonyms, but
this method still requires a Web-magnitude corpus
and a very large amount of computational time and
storage space. The latter paper reports accuracy of
58.0% (55.9% macro-average), which remains the
highest reported figure for corpus-based approaches
and demonstrates that relational similarity can per-
form well given sufficient resources.
We are not aware of previous work that compares
the effectiveness of different classes of context for
compound interpretation, nor of work that investi-
gates the utility of different corpora. We have also
described the first application of string kernels to the
compound task, though gap-weighted kernels have
been used successfully for related tasks such as word
sense disambiguation (Gliozzo et al, 2005) and re-
lation extraction (Bunescu and Mooney, 2005).
9 Conclusion and Future Work
We have defined four kinds of co-occurrence con-
texts for compound interpretation and demonstrated
that word similarity outperforms a range of relation
contexts using information derived from the British
National Corpus. Our experiments with the English
Gigaword Corpus indicate that more data is not al-
ways better, and that large newswire corpora may
not be ideally suited to general relation-based tasks.
63
On the other hand it might be expected to be very
useful for disambiguating relations more typical of
news stories (such as tax cut, rail strike).
Future research directions include developing
more sophisticated context kernels. Cancedda et
al. (2003) present a number of potentially useful re-
finements of the gap-weighted string kernel, includ-
ing ?soft matching? and differential values of ? for
different words or word classes. We intend to com-
bine the benefits of string kernels with the linguis-
tic richness of syntactic parses by computing subse-
quence kernels on dependency paths. We have also
begun to experiment with the tree kernels of Mos-
chitti (2006), but are not yet in a position to report
results. As mentioned in Section 2, we also intend
to investigate the potential contribution of the sen-
tential contexts that contain the compound tokens to
be classified (token similarity).
While the BNC has many desirable properties,
it may also be fruitful to investigate the utility of
a large encyclopaedic corpus such as Wikipedia,
which may be more explicit in its description of re-
lations between real-world entities than typical text
corpora. Wikipedia has shown promise as a re-
source for measuring word similarity (Strube and
Ponzetto, 2006) and relation similarity (Suchanek et
al. (2006)).
References
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the ACL-06 Interactive Presentation Sessions.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In Pro-
ceedings of the 19th Conference on Neural Informa-
tion Processing Systems.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean-
Michel Renders. 2003. Word-sequence kernels. Jour-
nal of Machine Learning Research, 3:1059?1082.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL-04.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19(4):479?496.
Roxana Girju. 2006. Out-of-context noun phrase seman-
tic interpretation with cross-linguistic evidence. In
Proceedings of CIKM-06.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL-05.
David Graff, 2003. English Gigaword. Linguistic Data
Consortium, Philadelphia.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cate-
gorisation. In Proceedings of ICML-01.
S. Sathiya Keerthi and Chih-Jen Lin. 2003. Asymptotic
behaviors of support vector machines with Gaussian
kernel. Neural Computation, 15:1667?1689.
Mirella Lapata and Frank Keller. 2004. The Web as a
baseline: Evaluating the performance of unsupervised
Web-based models for a range of NLP tasks. In Pro-
ceedings of HLT-NAACL-04.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML-06.
Sebastian Pado? and Mirella Lapata. 2003. Constructing
semantic space models from parsed corpora. In Pro-
ceedings of ACL-03.
Diarmuid O? Se?aghdha. 2007. Annotating and learn-
ing compound noun semantics. In Proceedings of the
ACL-07 Student Research Workshop.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, Cambridge.
Michael Strube and Simone Paolo Ponzetto. 2006.
WikiRelate! computing semantic relatedness using
Wikipedia. In Proceedings of AAAI-06.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. LEILA: Learning to extract infor-
mation by linguistic analysis. In Proceedings of the
ACL-06 Workshop on Ontology Learning and Popula-
tion.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1?3):251?278.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
64
Proceedings of the Workshop on BioNLP: Shared Task, pages 37?40,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Biomedical Event Extraction without Training Data
Andreas Vlachos, Paula Buttery, Diarmuid O? Se?aghdha, Ted Briscoe
Computer Laboratory
University of Cambridge
Cambridge, UK
av308,pjb48,do242,ejb@cl.cam.ac.uk
Abstract
We describe our system for the BioNLP 2009
event detection task. It is designed to be as
domain-independent and unsupervised as pos-
sible. Nevertheless, the precisions achieved
for single theme event classes range from 75%
to 92%, while maintaining reasonable recall.
The overall F-scores achieved were 36.44%
and 30.80% on the development and the test
sets respectively.
1 Introduction
In this paper we describe the system built for the
BioNLP 2009 event detection and characterization
task (Task 1). The approach is based on the output
of a syntactic parser and standard linguistic process-
ing, augmented by rules acquired from the develop-
ment data. The key idea is that a trigger connected
with an appropriate argument along a path through
the syntactic dependency graph forms an event.
The goal we set for our approach was to avoid
using training data explicitly annotated for the task
and to preserve domain independence. While we
acknowledge the utility of supervision (in the form
of annotated data) and domain knowledge, we be-
lieve it is valuable to explore an unsupervised ap-
proach. Firstly, manually annotated data is ex-
pensive to create and the annotation process itself
is difficult and unavoidably results in inconsisten-
cies, even in well-explored tasks such as named en-
tity recognition (NER). Secondly, unsupervised ap-
proaches, even if they fail to reach the performance
of supervised ones, are likely to be informative in
identifying useful features for the latter. Thirdly, ex-
ploring the potential of such a system may highlight
what domain knowledge is useful and its potential
contribution to performance. Finally, preserving do-
main independence allows us to develop and evalu-
ate a system that could be used for similar tasks with
minimal adaptation.
The overall architecture of the system is as fol-
lows. Initiallly, event triggers are identified and la-
belled with event types using seed terms. Based on
the dependency output of the parser the triggers are
connected with candidate arguments using patterns
identified in the development data. Anaphoric can-
didate arguments are then resolved. Finally, the trig-
gers connected with appropriate arguments are post-
processed to generate the final set of events. Each
of these stages are described in detail in subsequent
sections, followed by experiments and discussion.
2 Trigger identification
We perform trigger identification using the assump-
tion that events are triggered in text either by verbal
or nominal prdicates (Cohen et al, 2008).
To build a dictionary of verbs and their associ-
ated event classes we use the triggers annotated in
the training data. We lemmatize and stem the trig-
gers with the morphology component of the RASP
toolkit (Briscoe et al, 2006)1 and the Porter stem-
mer2 respectively. We sort the trigger stem - event
class pairs found according to their frequency in
the training data and we keep only those pairs that
appear at least 10 times. The trigger stems are
then mapped to verbs. This excludes some rela-
tively common triggers, which will reduce recall,
but, given that we rely exclusively on the parser for
1http://www.cogs.susx.ac.uk/lab/nlp/rasp/
2http://www.tartarus.org/?martin/PorterStemmer
37
argument extraction, such triggers would be difficult
to handle. For verbs with more than one event class
we keep only the most frequent one.
We consider the assumption that each verb de-
notes a single event class to be a reasonable one
given the restricted task domain. It hinders us from
dealing with triggers denoting multiple event classes
but it simplifies the task so that we do not need anno-
tated data. While we use the training data triggers to
obtain the list of verbs and their corresponding event
types, we believe that such lists could be obtained by
clustering (Korhonen et al, 2008) with editing and
labelling by domain experts. This is the only use of
the training data we make in our system.
During testing, using the tokenized text provided,
we attempt to match each token with one of the
verbs associated with an event type. We perform
this by relaxing the matching successively, using the
token lemma, then stem, and finally allowing a par-
tial match in order to deal with particles (so that e.g.
co-transfect matches transfect). This process returns
single-token candidate triggers which, while they do
not reproduce the trigger annotation, are likely to be
adequate for event extraction. We overgenerate trig-
gers, since not all occurrences denote an event, ei-
ther because they are not connected with appropriate
arguments or because they are found in a non-event
denoting context, but we expect to filter these at the
argument extraction stage.
3 Argument extraction
Given a set of candidate triggers, we attempt to con-
nect them with appropriate arguments using the de-
pendency graph provided by a parser. In our ex-
periments we use the domain-independent unlexi-
calized RASP parser, which generates parses over
the part-of-speech (PoS) tags of the tokens generated
by an HMM-based tagger trained on balanced En-
glish text. While we expect that a parser adapted to
the biomedical domain may perform better, we want
to preserve the domain-independence of the system
and explore its potential.
The only adjustment we make is to change the
PoS tags of tokens that are part of a protein name
to proper names tags. We consider such an adjust-
ment domain-independent given that NER is avail-
able in many domains (Lewin, 2007). Following
Haghighi et al(2005), in order to ameliorate pars-
ing errors, we use the top-10 parses and return a
set of bilexical head-dependent grammatical rela-
tions (GRs) weighted according to the proportion
and probability of the top parses supporting that GR.
The GRs produced by the parser define directed
graphs between tokens in the sentence, and a partial
event is formed when a path that connects a trigger
with an appropriate argument is identified. GR paths
that are likely to generate events are selected using
the development data, which does not contradict the
goals of our approach because we do not require an-
notated training data. Development data is always
needed in order to build and test a system, and such
supervision could be provided by a human expert,
albeit not as easily as for the list of trigger verbs.
The set of GR paths identified follow:
VERB-TRIGGER ?subject? ARG
NOUN-TRIGGER ?iobj? PREP ?dobj? ARG
NOUN-TRIGGER ?modifier? ARG
TRIGGER ?modifier? PREP ?obj? ARG
TRIGGER ?passive subject? ARG
The final system uses three sets of GR paths:
one for Regulation events; one for Binding events;
and one for all other events. The difference be-
tween these sets is in the lexicalization of the link-
ing prepositions. For example, in Binding events
the linking preposition required lexicalization since
binds x to/with y denotes a correct event but not
binds x by y. Binding events also required additional
GR paths to capture constructions such as binding of
x to y. For Regulation events, the path set was fur-
ther augmented to differentiate between theme and
cause. When the lexicalized GR pattern sets yielded
no events we backed-off to the unlexicalized pattern
set, which is identical for all event types. In all GR
path sets, the trigger was unlexicalized and only re-
stricted by PoS tag.
4 Anaphora resolution
The events and arguments identified in the parsed
abstracts are post-processed in context to iden-
tify protein referents for event arguments that are
anaphoric (e.g., these proteins, its phosphorylation)
or too complex to be extracted directly from the
grammatical relations (phosphorylation of cellular
proteins , notably phospholipase C gamma 1). The
38
anaphoric linking is performed by a set of heuris-
tic rules manually designed to capture a number of
common cases observed in the development dataset.
A further phenomenon dealt with by rules is coref-
erence between events, for example in The expres-
sion of LAL-mRNA is induced. This induction is de-
pendent on. . . where the Induction event described
by the first sentence is the same as the theme of the
Regulation event in the second and should be given
the same event index. The development of the post-
processing rules favoured precision over recall, but
the low frequency of each case considered means
that some overfitting to the development data may
have been unavoidable.
5 Event post-processing
At the event post-processing stage, we form com-
plete events considering the trigger-argument pairs
produced at the argument extraction stage whose ar-
guments are resolved (possibly using anaphora res-
olution) either to a protein name or to a candidate
trigger. The latter are considered only for regula-
tion event triggers. Furthermore, regulation event
trigger-argument pairs are tagged either as theme or
cause at the argument extraction stage.
For each non-regulation trigger-argument pair, we
generate a single event with the argument marked as
theme. Given that we are dealing only with Task
1, this approach is expected to deal adequately with
all event types except Binding, which can have mul-
tiple themes. Regulation events are formed in the
following way. Given that the cause argument is
optional, we generate regulation events for trigger-
argument pairs whose argument is a protein name or
a trigger that has a formed event. Since regulation
events can have other regulation events as themes,
we repeat this process until no more events can be
formed. Occasionally, the use of multiple parses re-
sults in cycles between regulation triggers which are
resolved using the weighted GR scores. Then, we at-
tach any cause arguments that share the same trigger
with a formed regulation event.
In the analysis performed for trigger identification
in Section 2, we observed that certain verbs were
consistently annotated with two events (namely
overexpress and transfect), a non-regulation event
and a regulation event with the former event as its
theme. For candidate triggers that were recognized
due to such verbs, we treat them as non-regulation
events until the post-processing stage where we gen-
erate two events.
6 Experiments - Discussion
We expected that our approach would achieve high
precision but relatively low recall. The evaluation
of our final submissions on the development and test
data (Table 1) confirmed this to a large extent. For
the non-regulation event classes excluding Binding,
the precisions achieved range from 75% to 92% in
both development and test data, with the exception
of Transcription in the test data. Our approach ex-
tracts Binding events with a single theme, more suit-
ably evaluated by the Event Decomposition evalua-
tion mode in which a similar high precision/low re-
call trend is observed, albeit with lower scores.
Of particular interest are the event classes for
which a single trigger verb was identified, namely
Transcription, Protein catabolism and Phosphoryla-
tion, which makes it easier to identify the strengths
and weaknesses of our approach. For the Phos-
phorylation class, almost all the triggers that were
annotated in the training data can be captured us-
ing the verb phosporylate and as a result, the per-
formances achieved by our system are 70.59% and
60.63% F-score on the development and test data re-
spectively. The precision was approximately 78% in
both datasets, while recall was lower due to parser
errors and unresolved anaphoric references. For the
Protein catabolism class, degrade was identified as
the only trigger verb, resulting in similar high preci-
sion but relatively lower recall due to the higher lex-
ical variation of the triggers for this class. For the
Transcription class we considered only transcribe
as a trigger verb, but while the performance on the
development data is reasonable (55%), the perfor-
mance on the test data is substantially lower (20%).
Inspecting the event triggers in the training data re-
veals that some very common triggers for this class
either cannot be mapped to a verb (e.g., mrna) or are
commonly used as triggers for other event classes.
A notable case of the latter type is the verb express,
which, while mostly a Gene Expressions trigger, is
also annotated as Transcription more than 100 times
in the training data. Assuming that this is desirable,
39
Development Test
Event Class recall precision fscore recall precision fscore
Localization 45.28 92.31 60.76 25.86 90.00 40.18
Binding 12.50 24.41 16.53 12.68 31.88 18.14
Gene expression 52.25 80.79 63.46 45.57 75.81 56.92
Transcription 42.68 77.78 55.12 12.41 56.67 20.36
Protein catabolism 42.86 81.82 56.25 35.71 83.33 50.00
Phosphorylation 63.83 78.95 70.59 49.63 77.91 60.63
Event Total 39.03 65.97 49.05 33.16 68.15 44.61
Regulation 20.12 50.75 28.81 9.28 36.49 14.79
Positive regulation 16.86 48.83 25.06 11.39 38.49 17.58
Negative regulation 11.22 36.67 17.19 6.86 36.11 11.53
Regulation Total 16.29 47.06 24.21 9.98 37.76 15.79
Total 26.55 58.09 36.44 21.12 56.90 30.80
Binding (decomposed) 26.92 66.14 38.27 18.84 54.35 27.99
Table 1: Performance analysis on development and test data using Approximate Span/Partial Recursive Matching.
a more appropriate solution would need to take con-
text into account.
Our performance on the regulation events is sub-
stantially lower in both recall and precision. This
is expected, as they rely on the extraction of non-
regulation events. The variety of lexical triggers is
not causing the drop in performance though, since
our system performed reasonably well in the Gene
Expression and Localization classes which have
similar lexical variation. Rather it is due to the com-
bination of the lexical variation with the requirement
to make the distinction between the theme and op-
tional cause argument, which cannot be handled ap-
propriately by the small set of GR paths employed.
The contribution of anaphora resolution to our
system is limited as it relies on the argument ex-
traction stage which, apart from introducing noise,
is geared towards maintaining high precision. Over-
all, it contributes 22 additional events on the de-
velopment set, of which 14 out of 16 are correct
non-regulation events. Of the remaining 6 regula-
tion events only 2 were correct. Similar trends were
observed on the test data.
7 Conclusions - Future work
We described an almost unsupervised approach for
the BioNLP09 shared task on biomedical event ex-
traction which requires only a dictionary of verbs
and a set of argument extraction rules. Ignoring trig-
ger spans, the performance of the approach is parser-
dependent and while we used a domain-independent
parser in our experiments we also want to explore
the benefits of using an adapted one.
The main weakness of our approach is the han-
dling of events with multiple arguments and the dis-
tinctions between them, which are difficult to deal
with using simple unlexicalized rules. In our fu-
ture work we intend to explore semi-supervised ap-
proaches that allow us to acquire more complex
rules efficiently.
References
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL Interactive presentation ses-
sions, pages 77?80.
Kevin B. Cohen, Martha Palmer, and Lawrence Hunter.
2008. Nominalization and alternations in biomedical
language. PLoS ONE, 3(9).
Aria Haghighi, Kristina Toutanova, and Chris Manning.
2005. A Joint Model for Semantic Role Labeling. In
Proceedings of CoNLL-2005: Shared Task.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
2008. The choice of features for classification of verbs
in biomedical texts. In Proceedings of Coling.
Ian Lewin. 2007. BaseNPs that contain gene names:
domain specificity and genericity. In Proceedings of
the ACL workshop BioNLP: Biological, translational,
and clinical language processing, pages 163?170.
40
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94?99,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx? , Su Nam Kim? , Zornitsa Kozareva? , Preslav Nakov? ,
Diarmuid O? Se?aghdha?, Sebastian Pado?? , Marco Pennacchiotti??,
Lorenza Romano??, Stan Szpakowicz??
Abstract
We present a brief overview of the main
challenges in the extraction of semantic
relations from English text, and discuss the
shortcomings of previous data sets and shared
tasks. This leads us to introduce a new
task, which will be part of SemEval-2010:
multi-way classification of mutually exclusive
semantic relations between pairs of common
nominals. The task is designed to compare
different approaches to the problem and to
provide a standard testbed for future research,
which can benefit many applications in
Natural Language Processing.
1 Introduction
The computational linguistics community has a con-
siderable interest in robust knowledge extraction,
both as an end in itself and as an intermediate step
in a variety of Natural Language Processing (NLP)
applications. Semantic relations between pairs of
words are an interesting case of such semantic
knowledge. It can guide the recovery of useful facts
about the world, the interpretation of a sentence, or
even discourse processing. For example, pears and
bowl are connected in a CONTENT-CONTAINER re-
lation in the sentence ?The bowl contained apples,
?University of Antwerp, iris.hendrickx@ua.ac.be
?University of Melbourne, snkim@csse.unimelb.edu.au
?University of Alicante, zkozareva@dlsi.ua.es
?National University of Singapore, nakov@comp.nus.edu.sg
?University of Cambridge, do242@cl.cam.ac.uk
?University of Stuttgart, pado@stanford.edu
??Yahoo! Inc., pennacc@yahoo-inc.com
??Fondazione Bruno Kessler, romano@fbk.eu
??University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
pears, and oranges.?, while ginseng and taste are in
an ENTITY-ORIGIN relation in ?The taste is not from
alcohol, but from the ginseng.?.
The automatic recognition of semantic relations
can have many applications, such as information
extraction (IE), document summarization, machine
translation, or construction of thesauri and seman-
tic networks. It can also facilitate auxiliary tasks
such as word sense disambiguation, language mod-
eling, paraphrasing or recognizing textual entail-
ment. For example, semantic network construction
can benefit from detecting a FUNCTION relation be-
tween airplane and transportation in ?the airplane
is used for transportation? or a PART-WHOLE rela-
tion in ?the car has an engine?. Similarly, all do-
mains that require deep understanding of text rela-
tions can benefit from knowing the relations that de-
scribe events like ACQUISITION between named en-
tities in ?Yahoo has made a definitive agreement to
acquire Flickr?.
In this paper, we focus on the recognition of se-
mantic relations between pairs of common nomi-
nals. We present a task which will be part of the
SemEval-2010 evaluation exercise and for which we
are developing a new benchmark data set. This data
set and the associated task address three significant
problems encountered in previous work: (1) the def-
inition of a suitable set of relations; (2) the incorpo-
ration of context; (3) the desire for a realistic exper-
imental design. We outline these issues in Section
2. Section 3 describes the inventory of relations we
adopted for the task. The annotation process, the
design of the task itself and the evaluation method-
ology are presented in Sections 4-6.
94
2 Semantic Relation Classification: Issues
2.1 Defining the Relation Inventory
A wide variety of relation classification schemes ex-
ist in the literature, reflecting the needs and granular-
ities of various applications. Some researchers only
investigate relations between named entities or in-
ternal to noun-noun compounds, while others have a
more general focus. Some schemes are specific to a
domain such as biomedical text.
Rosario and Hearst (2001) classify noun com-
pounds from the domain of medicine into 13 classes
that describe the semantic relation between the head
noun and the modifier. Rosario et al (2002) classify
noun compounds using the MeSH hierarchy and a
multi-level hierarchy of semantic relations, with 15
classes at the top level. Stephens et al (2001) pro-
pose 17 very specific classes targeting relations be-
tween genes. Nastase and Szpakowicz (2003) ad-
dress the problem of classifying noun-modifier rela-
tions in general text. They propose a two-level hier-
archy, with 5 classes at the first level and 30 classes
at the second one; other researchers (Kim and Bald-
win, 2005; Nakov and Hearst, 2008; Nastase et al,
2006; Turney, 2005; Turney and Littman, 2005)
have used their class scheme and data set. Moldovan
et al (2004) propose a 35-class scheme to classify
relations in various phrases; the same scheme has
been applied to noun compounds and other noun
phrases (Girju et al, 2005). Lapata (2002) presents a
binary classification of relations in nominalizations.
Pantel and Pennacchiotti (2006) concentrate on five
relations in an IE-style setting. In short, there is little
agreement on relation inventories.
2.2 The Role of Context
A fundamental question in relation classification is
whether the relations between nominals should be
considered out of context or in context. When one
looks at real data, it becomes clear that context does
indeed play a role. Consider, for example, the noun
compound wood shed : it may refer either to a shed
made of wood, or to a shed of any material used to
store wood. This ambiguity is likely to be resolved
in particular contexts. In fact, most NLP applica-
tions will want to determine not all possible relations
between two words, but rather the relation between
two instances in a particular context. While the in-
tegration of context is common in the field of IE (cf.
work in the context of ACE1), much of the exist-
ing literature on relation extraction considers word
pairs out of context (thus, types rather than tokens).
A notable exception is SemEval-2007 Task 4 Clas-
sification of Semantic Relations between Nominals
(Girju et al, 2007; Girju et al, 2008), the first to of-
fer a standard benchmark data set for seven semantic
relations between common nouns in context.
2.3 Style of Classification
The design of SemEval-2007 Task 4 had an im-
portant limitation. The data set avoided the chal-
lenge of defining a single unified standard classifi-
cation scheme by creating seven separate training
and test sets, one for each semantic relation. That
made the relation recognition task on each data set
a simple binary (positive / negative) classification
task.2 Clearly, this does not easily transfer to prac-
tical NLP settings, where any relation can hold be-
tween a pair of nominals which occur in a sentence
or a discourse.
2.4 Summary
While there is a substantial amount of work on re-
lation extraction, the lack of standardization makes
it difficult to compare different approaches. It is
known from other fields that the availability of stan-
dard benchmark data sets can provide a boost to the
advancement of a field. As a first step, SemEval-
2007 Task 4 offered many useful insights into the
performance of different approaches to semantic re-
lation classification; it has also motivated follow-
up research (Davidov and Rappoport, 2008; Ka-
trenko and Adriaans, 2008; Nakov and Hearst, 2008;
O? Se?aghdha and Copestake, 2008).
Our objective is to build on the achievements of
SemEval-2007 Task 4 while addressing its short-
comings. In particular, we consider a larger set of
semantic relations (9 instead of 7), we assume a
proper multi-class classification setting, we emulate
the effect of an ?open? relation inventory by means
of a tenth class OTHER, and we will release to the
research community a data set with a considerably
1http://www.itl.nist.gov/iad/mig/tests/
ace/
2Although it was not designed for a multi-class set-up, some
subsequent publications tried to use the data sets in that manner.
95
larger number of examples than SemEval-2007 Task
4 or other comparable data sets. The last point is cru-
cial for ensuring the robustness of the performance
estimates for competing systems.
3 Designing an Inventory of Semantic Re-
lations Between Nominals
We begin by considering the first of the problems
listed above: defining of an inventory of semantic
relations. Ideally, it should be exhaustive (should al-
low the description of relations between any pair of
nominals) and mutually exclusive (each pair of nom-
inals in context should map onto only one relation).
The literature, however, suggests no such inventory
that could satisfy all needs. In practice, one always
must decide on a trade-off between these two prop-
erties. For example, the gene-gene relation inven-
tory of Stephens et al (2001), with relations like X
phosphorylates Y, arguably allows no overlaps, but
is too specific for applications to general text.
On the other hand, schemes aimed at exhaus-
tiveness tend to run into overlap issues, due
to such fundamental linguistic phenomena as
metaphor (Lakoff, 1987). For example, in the sen-
tence Dark clouds gather over Nepal., the relation
between dark clouds and Nepal is literally a type of
ENTITY-DESTINATION, but in fact it refers to the
ethnic unrest in Nepal.
We seek a pragmatic compromise between the
two extremes. We have selected nine relations with
sufficiently broad coverage to be of general and
practical interest. We aim at avoiding ?real? overlap
to the extent that this is possible, but we include two
sets of similar relations (ENTITY-ORIGIN/ENTITY-
DESTINATION and CONTENT-CONTAINER/COM-
PONENT-WHOLE/MEMBER-COLLECTION), which
can help assess the models? ability to make such
fine-grained distinctions.3
As in Semeval-2007 Task 4, we give ordered two-
word names to the relations, where each word de-
scribes the role of the corresponding argument. The
full list of our nine relations follows4 (the definitions
we show here are intended to be indicative rather
than complete):
3COMPONENT-WHOLE and MEMBER-COLLECTION are
proper subsets of PART-WHOLE, one of the relations in
SemEval-2007 Task 4.
4We have taken the first five from SemEval-2007 Task 4.
Cause-Effect. An event or object leads to an effect.
Example: Smoking causes cancer.
Instrument-Agency. An agent uses an instrument.
Example: laser printer
Product-Producer. A producer causes a product to
exist. Example: The farmer grows apples.
Content-Container. An object is physically stored
in a delineated area of space, the container. Ex-
ample: Earth is located in the Milky Way.
Entity-Origin. An entity is coming or is derived
from an origin (e.g., position or material). Ex-
ample: letters from foreign countries
Entity-Destination. An entity is moving towards a
destination. Example: The boy went to bed.
Component-Whole. An object is a component of a
larger whole. Example: My apartment has a
large kitchen.
Member-Collection. A member forms a nonfunc-
tional part of a collection. Example: There are
many trees in the forest.
Communication-Topic. An act of communication,
whether written or spoken, is about a topic. Ex-
ample: The lecture was about semantics.
We add a tenth element to this set, the pseudo-
relation OTHER. It stands for any relation which
is not one of the nine explicitly annotated relations.
This is motivated by modelling considerations. Pre-
sumably, the data for OTHER will be very nonho-
mogeneous. By including it, we force any model of
the complete data set to correctly identify the deci-
sion boundaries between the individual relations and
?everything else?. This encourages good generaliza-
tion behaviour to larger, noisier data sets commonly
seen in real-world applications.
3.1 Semantic Relations versus Semantic Roles
There are three main differences between our task
(classification of semantic relations between nomi-
nals) and the related task of automatic labeling of
semantic roles (Gildea and Jurafsky, 2002).
The first difference is to do with the linguistic
phenomena described. Lexical resources for theo-
ries of semantic roles such as FrameNet (Fillmore et
96
al., 2003) and PropBank (Palmer et al, 2005) have
been developed to describe the linguistic realization
patterns of events and states. Thus, they target pri-
marily verbs (or event nominalizations) and their de-
pendents, which are typically nouns. In contrast,
semantic relations may occur between all parts of
speech, although we limit our attention to nominals
in this task. Also, semantic role descriptions typi-
cally relate an event to a set of multiple participants
and props, while semantic relations are in practice
(although not necessarily) binary.
The second major difference is the syntactic con-
text. Theories of semantic roles usually developed
out of syntactic descriptions of verb valencies, and
thus they focus on describing the linking patterns of
verbs and their direct dependents, phenomena like
raising and noninstantiations notwithstanding (Fill-
more, 2002). Semantic relations are not tied to
predicate-argument structures. They can also be es-
tablished within noun phrases, noun compounds, or
sentences more generally (cf. the examples above).
The third difference is that of the level of gen-
eralization. FrameNet currently contains more than
825 different frames (event classes). Since the se-
mantic roles are designed to be interpreted at the
frame level, there is a priori a very large number
of unrelated semantic roles. There is a rudimen-
tary frame hierarchy that defines mappings between
roles of individual frames,5 but it is far from com-
plete. The situation is similar in PropBank. Prop-
Bank does use a small number of semantic roles, but
these are again to be interpreted at the level of in-
dividual predicates, with little cross-predicate gen-
eralization. In contrast, all of the semantic relation
inventories discussed in Section 1 contain fewer than
50 types of semantic relations. More generally, se-
mantic relation inventories attempt to generalize re-
lations across wide groups of verbs (Chklovski and
Pantel, 2004) and include relations that are not verb-
centered (Nastase and Szpakowicz, 2003; Moldovan
et al, 2004). Using the same labels for similar se-
mantic relations facilitates supervised learning. For
example, a model trained with examples of sell re-
lations should be able to transfer what it has learned
to give relations. This has the potential of adding
5For example, it relates the BUYER role of the COM-
MERCE SELL frame (verb sell ) to the RECIPIENT role of the
GIVING frame (verb give).
1. People in Hawaii might be feeling
<e1>aftershocks</e1> from that power-
ful <e2>earthquake</e2> for weeks.
2. My new <e1>apartment</e1> has a
<e2>large kitchen</e2>.
Figure 1: Two example sentences with annotation
crucial robustness and coverage to analysis tools in
NLP applications based on semantic relations.
4 Annotation
The next step in our study will be the actual annota-
tion of relations between nominals. For the purpose
of annotation, we define a nominal as a noun or a
base noun phrase. A base noun phrase is a noun and
its pre-modifiers (e.g., nouns, adjectives, determin-
ers). We do not include complex noun phrases (e.g.,
noun phrases with attached prepositional phrases or
relative clauses). For example, lawn is a noun, lawn
mower is a base noun phrase, and the engine of the
lawn mower is a complex noun phrase.
We focus on heads that are common nouns. This
emphasis distinguishes our task from much work in
IE, which focuses on named entities and on consid-
erably more fine-grained relations than we do. For
example, Patwardhan and Riloff (2007) identify cat-
egories like Terrorist organization as participants in
terror-related semantic relations, which consists pre-
dominantly of named entities. We feel that named
entities are a specific category of nominal expres-
sions best dealt with using techniques which do not
apply to common nouns; for example, they do not
lend themselves well to semantic generalization.
Figure 1 shows two examples of annotated sen-
tences. The XML tags <e1> and <e2> mark the
target nominals. Since all nine proper semantic re-
lations in this task are asymmetric, the ordering of
the two nominals must be taken into account. In
example 1, CAUSE-EFFECT(e1, e2) does not hold,
although CAUSE-EFFECT(e2, e1) would. In exam-
ple 2, COMPONENT-WHOLE(e2, e1) holds.
We are currently developing annotation guide-
lines for each of the relations. They will give a pre-
cise definition for each relation and some prototypi-
cal examples, similarly to SemEval-2007 Task 4.
The annotation will take place in two rounds. In
the first round, we will do a coarse-grained search
97
for positive examples for each relation. We will
collect data from the Web using a semi-automatic,
pattern-based search procedure. In order to ensure
a wide variety of example sentences, we will use
several dozen patterns per relation. We will also
ensure that patterns retrieve both positive and nega-
tive example sentences; the latter will help populate
the OTHER relation with realistic near-miss negative
examples of the other relations. The patterns will
be manually constructed following the approach of
Hearst (1992) and Nakov and Hearst (2008).6
The example collection for each relation R will
be passed to two independent annotators. In order to
maintain exclusivity of relations, only examples that
are negative for all relations but R will be included
as positive and only examples that are negative for
all nine relations will be included as OTHER. Next,
the annotators will compare their decisions and as-
sess inter-annotator agreement. Consensus will be
sought; if the annotators cannot agree on an exam-
ple it will not be included in the data set, but it will
be recorded for future analysis.
Finally, two other task organizers will look for
overlap across all relations. They will discard any
example marked as positive in two or more relations,
as well as examples in OTHER marked as positive in
any of the other classes. The OTHER relation will,
then, consist of examples that are negatives for all
other relations and near-misses for any relation.
Data sets. The annotated data will be divided into
a training set, a development set and a test set. There
will be 1000 annotated examples for each of the
ten relations: 700 for training, 100 for development
and 200 for testing. All data will be released under
the Creative Commons Attribution 3.0 Unported Li-
cense7. The annotation guidelines will be included
in the distribution.
5 The Classification Task
The actual task that we will run at SemEval-2010
will be a multi-way classification task. Not all pairs
of nominals in each sentence will be labeled, so the
gold-standard boundaries of the nominals to be clas-
sified will be provided as part of the test data.
6Note that, unlike in Semeval 2007 Task 4, we will not re-
lease the patterns to the participants.
7http://creativecommons.org/licenses/by/
3.0/
In contrast with Semeval 2007 Task 4, in which
the ordering of the entities was provided with each
example, we aim at a more realistic scenario in
which the ordering of the labels is not given. Par-
ticipants in the task will be asked to discover both
the relation and the order of the arguments. Thus,
the more challenging task is to identify the most
informative ordering and relation between a pair
of nominals. The stipulation ?most informative?
is necessary since with our current set of asym-
metrical relations that includes OTHER, each pair
of nominals that instantiates a relation in one di-
rection (e.g., REL(e1, e2)), instantiates OTHER in
the inverse direction (OTHER (e2, e1)). Thus, the
correct answers for the two examples in Figure 1
are CAUSE-EFFECT (earthquake, aftershocks) and
COMPONENT-WHOLE (large kitchen, apartment).
Note that unlike in SemEval-2007 Task 4, we will
not provide manually annotated WordNet senses,
thus making the task more realistic. WordNet senses
did, however, serve for disambiguation purposes in
SemEval-2007 Task 4. We will therefore have to
assess the effect of this change on inter-annotator
agreement.
6 Evaluation Methodology
The official ranking of the participating systems will
be based on their macro-averaged F-scores for the
nine proper relations. We will also compute and re-
port their accuracy over all ten relations, including
OTHER. We will further analyze the results quan-
titatively and qualitatively to gauge which relations
are most difficult to classify.
Similarly to SemEval-2007 Task 4, in order to
assess the effect of varying quantities of training
data, we will ask the teams to submit several sets of
guesses for the labels for the test data, using varying
fractions of the training data. We may, for example,
request test results when training on the first 50, 100,
200, 400 and all 700 examples from each relation.
We will provide a Perl-based automatic evalua-
tion tool that the participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
7 Conclusion
We have introduced a new task, which will be part of
SemEval-2010: multi-way classification of semantic
98
relations between pairs of common nominals. The
task will compare different approaches to the prob-
lem and provide a standard testbed for future re-
search, which can benefit many NLP applications.
The description we have presented here should
be considered preliminary. We invite the in-
terested reader to visit the official task web-
site http://semeval2.fbk.eu/semeval2.
php?location=tasks\#T11, where up-to-
date information will be published; there is also a
discussion group and a mailing list.
References
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. EMNLP 2004, pages 33?40.
Dmitry Davidov and Ari Rappoport. 2008. Classifica-
tion of semantic relationships between nominals using
pattern clusters. In Proc. ACL-08: HLT, pages 227?
235.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Charles J. Fillmore. 2002. FrameNet and the linking be-
tween semantic and syntactic relations. In Proc. COL-
ING 2002, pages 28?36.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Roxana Girju, Dan Moldovan, Marta Tatu, , and Dan An-
tohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19:479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 task 04: Classification of semantic re-
lations between nominals. In Proc. 4th Semantic Eval-
uation Workshop (SemEval-2007).
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2008.
Classification of semantic relations between nominals.
Language Resources and Evaluation. In print.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING
92, pages 539?545.
Sophia Katrenko and Pieter Adriaans. 2008. Semantic
types of some generic relation arguments: Detection
and evaluation. In Proc. ACL-08: HLT, Short Papers,
pages 185?188.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet sim-
ilarity. In Proc. IJCAI, pages 945?956.
George Lakoff. 1987. Women, fire, and dangerous
things. University of Chicago Press, Chicago, IL.
Maria Lapata. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28:357?388.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. In HLT-NAACL
2004: Workshop on Computational Lexical Semantics,
pages 60?67.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proc. ACL-08: HLT, pages 452?460.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,
and Stan Szpakowicz. 2006. Learning noun-modifier
semantic relations with corpus-based and WordNet-
based features. In Proc. AAAI, pages 781?787.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. COLING 2008, pages 649?656.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?106.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proc. COLING/ACL, pages
113?120.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
information extraction with semantic affinity patterns
and relevant regions. In Proc. EMNLP-CoNLL), pages
717?727.
Barbara Rosario and Marti Hearst. 2001. Classifying the
semantic relations in noun compounds via a domain-
specific lexical hierarchy. In Proc. EMNLP 2001,
pages 82?90.
Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002. The descent of hierarchy, and selection in re-
lational semantics. In Proc. ACL-02, pages 247?254.
Matthew Stephens, Mathew Palakal, Snehasis
Mukhopadhyay, Rajeev Raje, and Javed Mostafa.
2001. Detecting gene relations from Medline ab-
stracts. In Pacific Symposium on Biocomputing, pages
483?495.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1-3):251?278.
Peter D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. IJCAI, pages 1136?
1141.
99
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 100?105,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid O? Se?aghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
We present a brief overview of the main
challenges in understanding the semantics of
noun compounds and consider some known
methods. We introduce a new task to be
part of SemEval-2010: the interpretation of
noun compounds using paraphrasing verbs
and prepositions. The task is meant to provide
a standard testbed for future research on noun
compound semantics. It should also promote
paraphrase-based approaches to the problem,
which can benefit many NLP applications.
1 Introduction
Noun compounds (NCs) ? sequences of two or more
nouns acting as a single noun,1 e.g., colon cancer
tumor suppressor protein ? are abundant in English
and pose a major challenge to the automatic anal-
ysis of written text. Baldwin and Tanaka (2004)
calculated that 3.9% and 2.6% of the tokens in
the Reuters corpus and the British National Corpus
(BNC), respectively, are part of a noun compound.
Compounding is also an extremely productive pro-
cess in English. The frequency spectrum of com-
pound types follows a Zipfian or power-law distribu-
tion (O? Se?aghdha, 2008), so in practice many com-
pound tokens encountered belong to a ?long tail?
of low-frequency types. For example, over half of
the two-noun NC types in the BNC occur just once
(Lapata and Lascarides, 2003). Even for relatively
frequent NCs that occur ten or more times in the
BNC, static English dictionaries give only 27% cov-
erage (Tanaka and Baldwin, 2003). Taken together,
1We follow the definition in (Downing, 1977).
the factors of high frequency and high productiv-
ity mean that achieving robust NC interpretation is
an important goal for broad-coverage semantic pro-
cessing. NCs provide a concise means of evoking a
relationship between two or more nouns, and natu-
ral language processing (NLP) systems that do not
try to recover these implicit relations from NCs are
effectively discarding valuable semantic informa-
tion. Broad coverage should therefore be achieved
by post-hoc interpretation rather than pre-hoc enu-
meration, since it is impossible to build a lexicon of
all NCs likely to be encountered.
The challenges presented by NCs and their se-
mantics have generated significant ongoing interest
in NC interpretation in the NLP community. Repre-
sentative publications include (Butnariu and Veale,
2008; Girju, 2007; Kim and Baldwin, 2006; Nakov,
2008b; Nastase and Szpakowicz, 2003; O? Se?aghdha
and Copestake, 2007). Applications that have been
suggested include Question Answering, Machine
Translation, Information Retrieval and Information
Extraction. For example, a question-answering sys-
tem may need to determine whether headaches in-
duced by caffeine withdrawal is a good paraphrase
for caffeine headaches when answering questions
about the causes of headaches, while an information
extraction system may need to decide whether caf-
feine withdrawal headache and caffeine headache
refer to the same concept when used in the same
document. Similarly, a machine translation system
facing the unknown NC WTO Geneva headquarters
might benefit from the ability to paraphrase it as
Geneva headquarters of the WTO or as WTO head-
quarters located in Geneva. Given a query like can-
100
cer treatment, an information retrieval system could
use suitable paraphrasing verbs like relieve and pre-
vent for page ranking and query refinement.
In this paper, we introduce a new task, which will
be part of the SemEval-2010 competition: NC inter-
pretation using paraphrasing verbs and prepositions.
The task is intended to provide a standard testbed
for future research on noun compound semantics.
We also hope that it will promote paraphrase-based
approaches to the problem, which can benefit many
NLP applications.
The remainder of the paper is organized as fol-
lows: Section 2 presents a brief overview of the
existing approaches to NC semantic interpretation
and introduces the one we will adopt for SemEval-
2010 Task 9; Section 3 provides a general descrip-
tion of the task, the data collection, and the evalua-
tion methodology; Section 4 offers a conclusion.
2 Models of Relational Semantics in NCs
2.1 Inventory-Based Semantics
The prevalent view in theoretical and computational
linguistics holds that the semantic relations that im-
plicitly link the nouns of an NC can be adequately
enumerated via a small inventory of abstract re-
lational categories. In this view, mountain hut,
field mouse and village feast all express ?location
in space?, while the relation implicit in history book
and nativity play can be characterized as ?topicality?
or ?aboutness?. A sample of some of the most influ-
ential relation inventories appears in Table 1.
Levi (1978) proposes that complex nominals ?
a general concept grouping together nominal com-
pounds (e.g., peanut butter), nominalizations (e.g.,
dream analysis) and non-predicative noun phrases
(e.g., electric shock) ? are derived through the com-
plementary processes of recoverable predicate dele-
tion and nominalization; each process is associated
with its own inventory of semantic categories. Table
1 lists the categories for the former.
Warren (1978) posits a hierarchical classifica-
tion scheme derived from a large-scale corpus study
of NCs. The top-level relations in her hierar-
chy are listed in Table 1, while the next level
subdivides CONSTITUTE into SOURCE-RESULT,
RESULT-SOURCE and COPULA; COPULA is then
further subdivided at two additional levels.
In computational linguistics, popular invento-
ries of semantic relations have been proposed by
Nastase and Szpakowicz (2003) and Girju et al
(2005), among others. The former groups 30 fine-
grained relations into five coarse-grained super-
categories, while the latter is a flat list of 21 re-
lations. Both schemes are intended to be suit-
able for broad-coverage analysis of text. For spe-
cialized applications, however, it is often useful
to use domain-specific relations. For example,
Rosario and Hearst (2001) propose 18 abstract rela-
tions for interpreting NCs in biomedical text, e.g.,
DEFECT, MATERIAL, PERSON AFFILIATED,
ATTRIBUTE OF CLINICAL STUDY.
Inventory-based analyses offer significant advan-
tages. Abstract relations such as ?location? and ?pos-
session? capture valuable generalizations about NC
semantics in a parsimonious framework. Unlike
paraphrase-based analyses (Section 2.2), they are
not tied to specific lexical items, which may them-
selves be semantically ambiguous. They also lend
themselves particularly well to automatic interpreta-
tion methods based on multi-class classification.
On the other hand, relation inventories have been
criticized on a number of fronts, most influentially
by Downing (1977). She argues that the great vari-
ety of NC relations makes listing them all impos-
sible; creative NCs like plate length (?what your
hair is when it drags in your food?) are intuitively
compositional, but cannot be assigned to any stan-
dard inventory category. A second criticism is that
restricted inventories are too impoverished a repre-
sentation scheme for NC semantics, e.g., headache
pills and sleeping pills would both be analyzed as
FOR in Levi?s classification, but express very differ-
ent (indeed, contrary) relationships. Downing writes
(p. 826): ?These interpretations are at best reducible
to underlying relationships. . . , but only with the loss
of much of the semantic material considered by sub-
jects to be relevant or essential to the definitions.?
A further drawback associated with sets of abstract
relations is that it is difficult to identify the ?correct?
inventory or to decide whether one proposed classi-
fication scheme should be favored over another.
2.2 Interpretation Using Verbal Paraphrases
An alternative approach to NC interpretation asso-
ciates each compound with an explanatory para-
101
Author(s) Relation Inventory
Levi (1978) CAUSE, HAVE, MAKE, USE, BE, IN, FOR, FROM, ABOUT
Warren (1978) POSSESSION, LOCATION, PURPOSE, ACTIVITY-ACTOR, RESEMBLANCE, CONSTITUTE
Nastase and CAUSALITY (cause, effect, detraction, purpose),
Szpakowicz PARTICIPANT (agent, beneficiary, instrument, object property,
(2003) object, part, possessor, property, product, source, whole, stative),
QUALITY (container, content, equative, material, measure, topic, type),
SPATIAL (direction, location at, location from, location),
TEMPORALITY (frequency, time at, time through)
Girju et al (2005) POSSESSION, ATTRIBUTE-HOLDER, AGENT, TEMPORAL, PART-WHOLE, IS-A, CAUSE,
MAKE/PRODUCE, INSTRUMENT, LOCATION/SPACE, PURPOSE, SOURCE, TOPIC, MANNER,
MEANS, THEME, ACCOMPANIMENT, EXPERIENCER, RECIPIENT, MEASURE, RESULT
Lauer (1995) OF, FOR, IN, AT, ON, FROM, WITH, ABOUT
Table 1: Previously proposed inventories of semantic relations for noun compound interpretation. The first two come
from linguistic theories; the rest have been proposed in computational linguistics.
phrase. Thus, cheese knife and kitchen knife can be
expanded as a knife for cutting cheese and a knife
used in a kitchen, respectively. In the paraphrase-
based paradigm, semantic relations need not come
from a small set; it is possible to have many sub-
tle distinctions afforded by the vocabulary of the
paraphrasing language (in our case, English). This
paradigm avoids the problems of coverage and rep-
resentational poverty, which Downing (1977) ob-
served in inventory-based approaches. It also re-
flects cognitive-linguistic theories of NC semantics,
in which compounds are held to express underlying
event frames and whose constituents are held to de-
note event participants (Ryder, 1994).
Lauer (1995) associates NC semantics with
prepositional paraphrases. As Lauer only consid-
ers a handful of prepositions (about, at, for,
from, in, of, on, with), his model is es-
sentially inventory-based. On the other hand, noun-
preposition co-occurrences can easily be identified
in a corpus, so an automatic interpretation can be
implemented through simple unsupervised methods.
The disadvantage of this approach is the absence of a
one-to-one mapping from prepositions to meanings;
prepositions can be ambiguous (of indicates many
different relations) or synonymous (at, in and on
all express ?location?). This concern arises with all
paraphrasing models, but it is exacerbated by the re-
stricted nature of prepositions. Furthermore, many
NCs cannot be paraphrased adequately with prepo-
sitions, e.g., woman driver, honey bee.
A richer, more flexible paraphrasing model is af-
forded by the use of verbs. In such a model, a honey
bee is a bee that produces honey, a sleeping pill
is a pill that induces sleeping and a headache pill
is a pill that relieves headaches. In some previous
computational work on NC interpretation, manually
constructed dictionaries provided typical activities
or functions associated with nouns (Finin, 1980; Is-
abelle, 1984; Johnston and Busa, 1996). It is, how-
ever, impractical to build large structured lexicons
for broad-coverage systems; these methods can only
be applied to specialized domains. On the other
hand, we expect that the ready availability of large
text corpora should facilitate the automatic mining
of rich paraphrase information.
The SemEval-2010 task we present here builds on
the work of Nakov (Nakov and Hearst, 2006; Nakov,
2007; Nakov, 2008b), where NCs are paraphrased
by combinations of verbs and prepositions. Given
the problem of synonymy, we do not provide a sin-
gle correct paraphrase for a given NC but a prob-
ability distribution over a range of candidates. For
example, highly probable paraphrases for chocolate
bar are bar made of chocolate and bar that tastes
like chocolate, while bar that eats chocolate is very
unlikely. As described in Section 3.3, a set of gold-
standard paraphrase distributions can be constructed
by collating responses from a large number of hu-
man subjects.
In this framework, the task of interpretation be-
comes one of identifying the most likely paraphrases
for an NC. Nakov (2008b) and Butnariu and Veale
(2008) have demonstrated that paraphrasing infor-
mation can be collected from corpora in an un-
supervised fashion; we expect that participants in
102
SemEval-2010 Task 9 will further develop suitable
techniques for this problem. Paraphrases of this kind
have been shown to be useful in applications such as
machine translation (Nakov, 2008a) and as an inter-
mediate step in inventory-based classification of ab-
stract relations (Kim and Baldwin, 2006; Nakov and
Hearst, 2008). Progress in paraphrasing is therefore
likely to have follow-on benefits in many areas.
3 Task Description
The description of the task we present below is pre-
liminary. We invite the interested reader to visit the
official Website of SemEval-2010 Task 9, where up-
to-date information will be published; there is also a
discussion group and a mailing list.2
3.1 Preliminary Study
In a preliminary study, we asked 25-30 human sub-
jects to paraphrase 250 noun-noun compounds us-
ing suitable paraphrasing verbs. This is the Levi-
250 dataset (Levi, 1978); see (Nakov, 2008b) for de-
tails.3 The most popular paraphrases tend to be quite
apt, while some less frequent choices are question-
able. For example, for chocolate bar we obtained
the following paraphrases (the number of subjects
who proposed each one is shown in parentheses):
contain (17); be made of (16); be made
from (10); taste like (7); be composed
of (7); consist of (5); be (3); have (2);
smell of (2); be manufactured from (2);
be formed from (2); melt into (2); serve
(1); sell (1); incorporate (1); be made with
(1); be comprised of (1); be constituted
by (1); be solidified from (1); be flavored
with (1); store (1); be flavored with (1); be
created from (1); taste of (1)
3.2 Objective
We propose a task in which participating systems
must estimate the quality of paraphrases for a test
set of NCs. A list of verb/preposition paraphrases
will be provided for each NC, and for each list a
participating system will be asked to provide aptness
2Please follow the Task #9 link at the SemEval-2010 home-
page http://semeval2.fbk.eu
3This dataset is available from http://sourceforge.
net/projects/multiword/
scores that correlate well (in terms of frequency dis-
tribution) with the human judgments collated from
our test subjects.
3.3 Datasets
Trial/Development Data. As trial/development
data, we will release the previously collected para-
phrase sets for the Levi-250 dataset (after further
review and cleaning). This dataset consists of 250
noun-noun compounds, each paraphrased by 25-30
human subjects (Nakov, 2008b).
Test Data. The test data will consist of approx-
imately 300 NCs, each accompanied by a set of
paraphrasing verbs and prepositions. Following the
methodology of Nakov (2008b), we will use the
Amazon Mechanical Turk Web service4 to recruit
human subjects. This service offers an inexpensive
way to recruit subjects for tasks that require human
intelligence, and provides an API which allows a
computer program to easily run tasks and collate
the responses from human subjects. The Mechanical
Turk is becoming a popular means to elicit and col-
lect linguistic intuitions for NLP research; see Snow
et al (2008) for an overview and a discussion of is-
sues that arise.
We intend to recruit 100 annotators for each NC,
and we will require each annotator to paraphrase
at least five NCs. Annotators will be given clear
instructions and will be asked to produce one or
more paraphrases for a given NC. To help us filter
out subjects with an insufficient grasp of English or
an insufficient interest in the task, annotators will
be asked to complete a short and simple multiple-
choice pretest on NC comprehension before pro-
ceeding to the paraphrasing step.
Post-processing. We will manually check the
trial/development data and the test data. Depending
on the quality of the paraphrases, we may decide to
drop the least frequent verbs.
License. All data will be released under the Cre-
ative Commons Attribution 3.0 Unported license5.
3.4 Evaluation
Single-NC Scores. For each NC, we will compare
human scores (our gold standard) with those pro-
posed by each participating system. We have con-
4http://www.mturk.com
5http://creativecommons.org/licenses/by/3.0/
103
sidered three scores: (1) Pearson?s correlation, (2)
cosine similarity, and (3) Spearman?s rank correla-
tion.
Pearson?s correlation coefficient is a standard
measure of the correlation strength between two dis-
tributions; it can be calculated as follows:
? = E(XY ) ? E(X)E(Y )?
E(X2) ? [E(X)]2?E(Y 2) ? [E(Y )]2
(1)
where X = (x1, . . . , xn) and Y = (y1, . . . , yn) are
vectors of numerical scores for each paraphrase pro-
vided by the humans and the competing systems, re-
spectively, n is the number of paraphrases to score,
and E(X) is the expectation of X .
Cosine correlation coefficient is another popu-
lar alternative and was used by Nakov and Hearst
(2008); it can be seen as an uncentered version of
Pearson?s correlation coefficient:
? = X.Y?X??Y ? (2)
Spearman?s rank correlation coefficient is suit-
able for comparing rankings of sets of items; it is
a special case of Pearson?s correlation, derived by
considering rank indices (1,2,. . . ) as item scores . It
is defined as follows:
? = n
?xiyi ? (?xi)(? yi)?
n?x2i ? (
?xi)2
?
n? y2i ? (
? yi)2
(3)
One problem with using Spearman?s rank coef-
ficient for the current task is the assumption that
swapping any two ranks has the same effect. The
often-skewed nature of paraphrase frequency distri-
butions means that swapping some ranks is intu-
itively less ?wrong? than swapping others. Consider,
for example, the following list of human-proposed
paraphrasing verbs for child actor, which is given in
Nakov (2007):
be (22); look like (4); portray (3); start as
(1); include (1); play (1); have (1); involve
(1); act like (1); star as (1); work as (1);
mimic (1); pass as (1); resemble (1); be
classified as (1); substitute for (1); qualify
as (1); act as (1)
Clearly, a system that swaps the positions for
be (22) and look like (4) for child actor will
have made a significant error, while swapping con-
tain (17) and be made of (16) for chocolate bar (see
Section 3.1) would be less inappropriate. However,
Spearman?s coefficient treats both alterations iden-
tically since it only looks at ranks; thus, we do not
plan to use it for official evaluation, though it may
be useful for post-hoc analysis.
Final Score. A participating system?s final score
will be the average of the scores it achieves over all
test examples.
Scoring Tool. We will provide an automatic eval-
uation tool that participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
4 Conclusion
We have presented a noun compound paraphrasing
task that will run as part of SemEval-2010. The goal
of the task is to promote and explore the feasibility
of paraphrase-based methods for compound inter-
pretation. We believe paraphrasing holds some key
advantages over more traditional inventory-based
approaches, such as the ability of paraphrases to rep-
resent fine-grained and overlapping meanings, and
the utility of the resulting paraphrases for other ap-
plications such as Question Answering, Information
Extraction/Retrieval and Machine Translation.
The proposed paraphrasing task is predicated on
two important assumptions: first, that paraphrasing
via a combination of verbs and prepositions pro-
vides a powerful framework for representing and in-
terpreting the meaning of compositional nonlexical-
ized noun compounds; and second, that humans can
agree amongst themselves about what constitutes a
good paraphrase for any given NC. As researchers in
this area and as proponents of this task, we believe
that both assumptions are valid, but if the analysis
of the task were to raise doubts about either assump-
tion (e.g., by showing poor agreement amongst hu-
man annotators), then this in itself would be a mean-
ingful and successful output of the task. As such,
we anticipate that the task and its associated dataset
will inspire further research, both on the theory and
development of paraphrase-based compound inter-
pretation and on its practical applications.
104
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of compound nominals: Getting it
right. In Proceedings of the ACL 2004 Workshop on
Multiword Expressions: Integrating Processing, pages
24?31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING 2008), pages
81?88.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Timothy Finin. 1980. The Semantic Interpretation of
Compound Nominals. Ph.D. Dissertation, University
of Illinois, Urbana, Illinois.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Journal of Computer Speech and Language - Special
Issue on Multiword Expressions, 4(19):479?496.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2007), pages
568?575.
Pierre Isabelle. 1984. Another look at nominal com-
pounds. In Proceedings of the 10th International Con-
ference on Computational Linguistics, pages 509?516.
Michael Johnston and Frederica Busa. 1996. Qualia
structure and the compositional interpretation of com-
pounds. In Proceedings of the ACL 1996 Workshop on
Breadth and Depth of Semantic Lexicons, pages 77?
88.
Su Nam Kim and Timothy Baldwin. 2006. Interpret-
ing semantic relations in noun compounds via verb
semantics. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(COLING/ACL 2006) Main Conference Poster Ses-
sions, pages 491?498.
Mirella Lapata and Alex Lascarides. 2003. Detecting
novel compounds: the role of distributional evidence.
In Proceedings of the 10th conference of the European
chapter of the Association for Computational Linguis-
tics (EACL 2003), pages 235?242.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Dept. of Computing, Macquarie University,
Australia.
Judith Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York.
Preslav Nakov andMarti A. Hearst. 2006. Using verbs to
characterize noun-noun relations. In LNCS vol. 4183:
Proceedings of the 12th international conference on
Artificial Intelligence: Methodology, Systems and Ap-
plications (AIMSA 2006), pages 233?244. Springer.
Preslav Nakov and Marti A. Hearst. 2008. Solving re-
lational similarity problems using the web as a cor-
pus. In Proceedings of the 46th Annual Meeting of the
Association of Computational Linguistics (ACL 2008),
pages 452?460.
Preslav Nakov. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syntax
and Semantics. Ph.D. thesis, EECS Department, Uni-
versity of California, Berkeley, UCB/EECS-2007-173.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. In Pro-
ceedings of the 18th European Conference on Artificial
Intelligence (ECAI?2008), pages 338?342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. In LNAI
vol. 5253: Proceedings of the 13th international con-
ference on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA 2008), pages 103?117.
Springer.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics, pages 285?301.
Diarmuid O? Se?aghdha and Ann Copestake. 2007. Co-
occurrence contexts for noun compound interpreta-
tion. In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 57?64.
Diarmuid O? Se?aghdha. 2008. Learning Compound Noun
Semantics. Ph.D. thesis, University of Cambridge.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 82?90.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008), pages 254?263.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: a feasibility
study on shallow processing. In Proceedings of the
ACL 2003 workshop on Multiword expressions, pages
17?24.
Beatrice Warren. 1978. Semantic patterns of noun-noun
compounds. In Gothenburg Studies in English 41,
Goteburg, Acta Universtatis Gothoburgensis.
105
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 689?697,
Beijing, August 2010
Exploring variation across biomedical subdomains
Tom Lippincott and Diarmuid O? Se?aghdha and Lin Sun and Anna Korhonen
Computer Laboratory
University of Cambridge
{tl318,do242,ls418,alk23}@cam.ac.uk
Abstract
Previous research has demonstrated the
importance of handling differences be-
tween domains such as ?newswire? and
?biomedicine? when porting NLP systems
from one domain to another. In this paper
we identify the related issue of subdomain
variation, i.e., differences between subsets
of a domain that might be expected to be-
have homogeneously. Using a large corpus
of research articles, we explore how subdo-
mains of biomedicine vary across a variety
of linguistic dimensions and discover that
there is rich variation. We conclude that
an awareness of such variation is necessary
when deploying NLP systems for use in
single or multiple subdomains.
1 Introduction
One of the most noticeable trends in the past
decade of Natural Language Processing (NLP) re-
search has been the deployment of language pro-
cessing technology to meet the information re-
trieval and extraction needs of scientists in other
disciplines. This meeting of fields has proven mu-
tually beneficial: scientists increasingly rely on
automated tools to help them cope with the expo-
nentially expanding body of publications in their
field, while NLP researchers have been spurred to
address new conceptual problems in theirs. Among
the fundamental advances from the NLP perspec-
tive has been the realisation that tools which per-
form well on textual data from one source may fail
to do so on another unless they are tailored to the
new source in some way. This has led to signifi-
cant interest in the idea of contrasting domains and
the concomitant problem of domain adaptation,
as well as the production of manually annotated
domain-specific corpora.1
One definition of domain variation associates
it with differences in the underlying probability
distributions from which different sets of data are
drawn (Daume? III and Marcu, 2006). The concept
also mirrors the notion of variation across thematic
subjects and the corpus-linguistic notions of reg-
ister and genre (Biber, 1988). In addition to the
differences in vocabulary that one would expect
to observe, domains can vary in many linguistic
variables that affect NLP systems. The scientific
domain which has received the most attention (and
is the focus of this paper) is the biomedical domain.
Notable examples of corpus construction projects
for the biomedical domain are PennBioIE (Kulick
et al, 2004) and GENIA (Kim et al, 2003). These
corpora have been used to develop systems for a
range of processing tasks, from entity recognition
(Jin et al, 2006) to parsing (Hara et al, 2005) to
coreference resolution (Nguyen and Kim, 2008).
An implicit assumption in much previous work
on biomedical NLP has been that particular subdo-
mains of biomedical literature ? typically molec-
ular biology ? can be used as a model of biomed-
ical language in general. For example, GENIA
consists of abstracts dealing with a specific set
of subjects in molecular biology, while PennBioIE
covers abstracts in two specialised domains, cancer
genomics and the behaviour of a particular class
of enzymes. This assumption of representative-
ness is understandable because linguistic annota-
tion is labour-intensive and it may not be worth-
while to produce annotated corpora for multiple
subdomains within a single discipline if there is lit-
1A workshop dedicated to domain adaptation is collocated
with ACL 2010.
689
tle task-relevant variation across those subdomains.
However, such conclusions should not be made
before studying the actual degree of difference be-
tween the subdomains of interest.
One of the principal goals of this paper is to map
how the concept of ?biomedical language?, often
construed as a monolithic entity, is composed of
diverse patterns of behaviour at more fine-grained
topical levels. Hence we study linguistic variation
in a broad biomedical corpus of abstracts and full
papers, the PMC Open Access Subset.2 We select
a range of lexical and structural phenomena for
quantitative investigation. The results indicate that
common subdomains for resource development are
not representative of biomedical text in general and
furthermore that different linguistic features often
partition the subdomains in quite different ways.
2 Related Work
A number of researchers have explored the dif-
ferences between non-technical and scientific lan-
guage. Biber and Gray (2010) describe two
distinctive syntactic characteristics of academic
writing which set it apart from general English.
Firstly, in academic writing additional information
is most commonly integrated by pre- and post-
modification of phrases rather than by the addi-
tion of extra clauses. Secondly, academic writing
places greater demands on the reader by omitting
non-essential information, through the frequent
use of passivisation, nominalisation and noun com-
pounding. Biber and Gray also show that these ten-
dencies towards ?less elaborate and less explicit?
language have become more pronounced in recent
history.
We now turn to corpus studies that focus on
biomedical writing. Verspoor et al (2009) use
measurements of lexical and structural variation
to demonstrate that Open Access and subscription-
based journal articles in a specific domain (mouse
genomics) are sufficiently similar that research on
the former can be taken as representative of the lat-
ter. While their primary goal is different from ours
and they do not consider variation across multiple
domains, they do compare their mouse genomics
corpus with small reference corpora drawn from
2http://www.ncbi.nlm.nih.gov/pmc/
about/openftlist.html
newswire and general biomedical sources. This
analysis unsurprisingly finds differences between
the domain and newswire corpora across many
linguistic dimensions; more interestingly for our
purposes, the comparison of domain text to the
broader biomedical superdomain shows a more
complex picture with similarities in some aspects
(e.g., passivisation and negation) and dissimilari-
ties in others (e.g., sentence length, semantic fea-
tures).
Friedman et al (2002) document the ?sublan-
guages? associated with two biomedical domains:
clinical reports and molecular biology articles.
They set out restricted ontologies and frequent co-
occurrence templates for the two domains and dis-
cuss the similarities and differences between them,
but they do not perform any quantitative analysis.
Other researchers have focused on specific phe-
nomena, rather than cataloguing a broad scope
of variation. Cohen et al (2008) carry out a de-
tailed analysis of argument realisation with respect
to verbs and nominalisations, using the GENIA
and PennBioIE corpora. Nguyen and Kim (2008)
compare the behaviour of anaphoric pronouns in
newswire and biomedical corpora; they improve
the performance of a pronoun resolver by incorpo-
rating their observations, thus demonstrating the
importance of capturing domain-specific phenom-
ena. Nguyen and Kim?s findings are discussed in
more detail in Section 5.4 below.
3 Subdomains in the OpenPMC Corpus
The Open Access Subset of PubMed (OpenPMC)
is the largest publicly available corpus of full-text
articles in the biomedical domain. OpenPMC is
comprised of 169,338 articles drawn from 1233
medical journals, totalling approximately 400 mil-
lion words. The NIH maintains a one-to-many
mapping from journals to 122 subject areas (NIH,
2009b). This covers about 400 of the OpenPMC
journals, but these account for over 70% of the
database by byte size and word count. Journals are
assigned up to five subject areas with the majority
assigned one (69%) or two (26%) subjects. In this
paper we adopt the OpenPMC subject areas (e.g.
?Pulmonary Medicine?, ?Genetics?, ?Psychiatry?)
as the basis for subdomain comparison.
690
0 10 20 30 40Word count (millions)
Ethics
Complementary Therapies
Education
Obstetrics
Pharmacology
Geriatrics
Gastroenterology
Pediatrics
Veterinary Medicine
Biomedical Engineering
Psychiatry
Embryology
Genetics, Medical
Ophthalmology
Vascular Diseases
Botany
Virology
Endocrinology
Pulmonary Medicine
Physiology
Tropical Medicine
Critical Care
Rheumatology
Cell Biology
Communicable Diseases
Science
Neurology
Biotechnology
Medicine
Microbiology
Environmental Health
Public Health
Biochemistry
Molecular Biology
Neoplasms
Medical Informatics
Genetics
Figure 1: OpenPMC word count by subdomain,
dark colouring indicates data assigned single sub-
domain, each lighter shade indicates an additional
overlapping subdomain
4 Methodology
4.1 Data selection and preprocessing
An important initial question was how to treat data
with multiple classifications: we only consider
journals assigned a single subdomain, to avoid
the added complexity of interactions in data from
overlapping subdomains. To ensure sufficient data
for comparing a variety of linguistic features, we
discard the subdomains with less than one mil-
lion words meeting the single-subdomain criterion.
After review, we also drop the ?Biology? subdo-
main, which appears to function as a catch-all for
many loosely related areas. Figure 1 shows the
distribution of data across the subjects we use, by
word-count, with lighter-coloured areas represent-
ing data that is assigned multiple subjects. These
subjects provide a convenient starting point for di-
viding the corpus into subdomains (hereafter, ?sub-
domain? will be used rather than ?subject?). We
also add a reference subdomain, ?Newswire?, com-
posed of a 6 million word random sample from the
English Gigaword corpus (Graff et al, 2005). The
final data set has a total of 39 subdomains.
Articles in the OpenPMC corpus are formatted
according to a standard XML tag set (NIH, 2009a).
We first convert each article to plain text, ignoring
?non-content? elements such as tables and formulas,
and split the result into sentences, aggregating the
results by subdomain.
4.2 Feature extraction
We investigate subdomain variation in our cor-
pus across a range of lexical, syntactic, sentential
and discourse features. The corpus is lemmatised,
tagged and parsed using the C&C pipeline (Cur-
ran et al, 2007) with the adapted part-of-speech
and lexical category tagging models produced by
Rimell and Clark (2009) for biomedical parsing.
From this output we count occurrences of noun,
verb, adjective and adverb lemmas, part-of-speech
(POS) tags, grammatical relations (GRs), chunks,
and lexical categories. The lemma features are
Zipfian-distributed items from an open class, so
we have experimented with filtering low-frequency
items at various thresholds to reduce noise and
improve processing speed. The other feature sets
can be viewed as closed classes, where filtering is
unnecessary.
Since verbs are central to the meaning and struc-
ture of sentences, we consider their special behav-
ior by constructing features for each verb?s dis-
tribution over other grammatical properties. Sev-
eral grammatical properties are captured by pairing
each verb with its POS (indicating e.g. tense, such
as present, past, and present participle). Voice is de-
termined from additional annotation output by the
C&C parser. Table 1 shows the POS-distribution
for the verb ?restrict?, in two subdomains from
the corpus. Finally, we record distributions over
verb subcategorization frames (SCFs) taken by
each verb, and over the GRs it participates in.
691
Subdomain VB VBG VBN VBP VBZ
Medical Informatics .35 .29 .06 .09 .21
Cell Biology .14 .43 .05 .10 .29
Table 1: Distribution over POS tags for verb ?re-
strict?, in two subdomains
SCFs were extracted using a system of Preiss et al
(2007).
To facilitate a more robust and interpretable anal-
ysis of vocabulary differences, we estimate a ?topic
model? of the corpus with Latent Dirichlet Analy-
sis (Blei et al, 2003) using the MALLET toolkit.3
As preprocessing we divide the corpus into arti-
cles, removing stopwords and words shorter than
3 characters. The Gibbs sampling procedure is
parameterised to induce 100 topics, each giving a
coherent cluster of related words learned from the
data, and to run for 1000 iterations. We collate the
predicted distribution over topics for each article
in a subdomain, weighted by article wordcount, to
produce a topic distribution for the subdomain.
4.3 Measurements of divergence
Our goal is to illustrate the presence or absence
of differences between the feature sets, and to do
so we calculated the Jensen-Shannon divergence
and the Pearson correlation. Jensen-Shannon diver-
gence is a finite symmetric measurement of the di-
vergence between probability distributions, while
Pearson correlation quantifies the linear relation-
ship between two real-valued samples.
The count-features are weighted, for a given
subdomain, by the feature?s log-likelihood be-
tween the subdomain?s data and the rest of the
corpus. Log-likelihood has been shown to perform
well when comparing counts of potentially low-
frequency features (Rayson and Garside, 2000)
such as found in Zipfian-distributed data. This
serves to place more weight in the comparison on
items that are distinctive of the subdomain with
respect to the entire corpus.
While the count-features are treated as a single
distribution for the purposes of JSD, the verbwise-
features are composed of many distributions, one
for each verb lemma. Our approach is to com-
bine the JSD of the verbs, weighted by the log-
3http://mallet.cs.umass.edu
likelihood of the verb lemma between the two
subdomains in question, and normalize the dis-
tances to the interval [0, 1]. Using the lemma?s log-
likelihood assumes that, when a verb?s distribution
behaves differently in a subdomain, its frequency
changes as well.
We present the results as dendrograms and
heat maps. Dendrograms are tree structures that
illustrate the results of hierarchical clustering.
We perform hierarchical clustering on the inter-
subdomain divergences for each set of features.
The algorithm begins with each instance (in our
case, subdomains) as a singleton cluster, and re-
peatedly joins the two most similar clusters until
all the data is clustered together. The order of these
merges is recorded as a tree structure that can be
visualized as a dendrogram in which the length of
a branch represents the distance between its child
nodes. Similarity between clusters is calculated us-
ing average distance between all members, known
as ?average linking?.
Heat maps show the pairwise calculation of
a metric in a grid of squares, where square
(x, y) is shaded according to the value of
metric(subx, suby). For our measurements of
JSD, black represents 0 (i.e. identical distributions)
and white represents the metric?s theoretical maxi-
mum of 1. We also inscribe the actual value inside
each square. Dendrograms are tree structures that
illustrate the hierarchical clustering procedure de-
scribed above. The dendrograms present all 39
subdomains, while for readability the heatmaps
present 12 subdomains selected for representative-
ness.
5 Results
Different thresholds for filtering low-frequency
terms had little effect on the divergence measures,
and served mainly to improve processing time. We
therefore report results using a cutoff of 150 occur-
rences (over the entire 234 million word data set)
and log-likelihood weights. The results of Pearson
correlation and JSD show similar trends, and due
to its specific design for comparing distributions
we only report the latter.
692
5.1 Vocabulary and lexical features
Differences in vocabulary are what first comes to
mind when describing subdomains. Word features
are fundamental components for systems such as
POS taggers and lexicalised parsers; one therefore
expects that these systems will be affected by vari-
ation in lexical distributions. Figure 2a uses JSD
calculated on each subdomain?s distribution over
100 LDA-induced topics to compare vocabulary
distributions. Subdomains related to molecular
biology (Genetics, Molecular Biology) show the
smallest divergences, an interesting fact since these
are heavily used in building resources for BioNLP.
The dendrogram shows a rough division into ?pub-
lic policy?, ?patient-centric?, ?applied? and ?mi-
croscopic? subdomains, with the distance between
unrelated subdomains such as Biochemistry and
Pediatrics almost as large as their respective differ-
ences from Newswire.
We omit figures for variation over noun, verb
and adjective lemmas due to space restrictions; in
general, these correlate with the variation in LDA
topics though there are some differences. Figure 2b
shows JSD calculated on distributions over adverb
lemmas. Part of the variation is due to character-
istic markers of scientific argument (?therefore?,
?significantly?, ?statistically?). A more interesting
factor is the coining of domain-specific adverbs,
an example of the tendency in scientific text to use
complex lexical items and premodifiers rather than
additional clauses. This also has the effect of mov-
ing subdomain-specific objects and processes from
verbs and nouns to adverbs. This behavior seems
non-continuous, in that subdomains either make
heavy, or almost no, use of it: for example, Pedi-
atrics has no subdomain-specific items among the
its ten top adverbs by log-likelihood, while Neo-
plasms has ?histologically?, ?immunohistochemi-
cally? and ?subcutaneously?. These information-
dense terms could prove useful for tasks like auto-
matic curation of subdomain vocabularies, where
they imply relationships between their components,
the items they modify, etc.
5.2 Verb distributional behavior
Modelling verb behavior is important for both syn-
tactic (Collins, 2003) and semantic (Korhonen et
al., 2008) processing, and subdomains are known
to conscript verbs into specific roles that change the
distributions of their syntactic properties (Roland
and Jurafsky, 1998). The four properties we con-
sidered verbs? distributions over (SCF, POS, GR
and voice) produced similar inter-subdomain JSD
values. Figure 2c demonstrates how verbs differ
between subdomains with respect to SCFs. For
example, while the Pediatrics subdomain uses the
verb ?govern? in a single SCF among its 12 pos-
sibilities, the Genetics subdomain distributes its
usage over 7 of them. Two subdomains may both
use ?restrict? with high frequency (e.g. Molecular
Biology and Ethics), but with different frequency
distributions over SCFs.
5.3 Syntax
It is difficult to measure syntactic complexity accu-
rately without access to a hand-annotated treebank,
but it is well-known that sentence length corre-
lates strongly with processing difficulty (Collins,
1996). The first column of Table 2 gives average
sentence lengths (excluding punctuation and ?sen-
tences? of fewer than three words) for selected
domains. All standard errors are < 0.1. It is clear
that all biomedical subdomains typically use longer
sentences than newswire, though there is also vari-
ation within biomedicine, from an average length
of 27 words in Molecular Biology to 24.5 words
in Pediatrics.
?Packaging? information in complex pre- and/or
post-modified noun phrases is a characteristic fea-
ture of academic writing (Biber and Gray, 2010).
This increases the information density of a sen-
tence but brings with it syntactic and semantic
ambiguities. For example, the difficulty of resolv-
ing the internal structure of noun-noun compounds
and strings of prepositional phrases has been the fo-
cus of ongoing research in NLP; these phenomena
have also been identified as significant challenges
in biomedical language processing (Rosario and
Hearst, 2001; Schuman and Bergler, 2006). The
second and third columns of Table 2 present aver-
age lengths for full noun phrases, defined as every
word dominated by a head noun in the grammat-
ical relation graph for a sentence, and for base
nominals, defined as nouns plus premodifying ad-
jectives and nouns only. All standard errors are
? 0.01. Newswire text uses the simplest noun
693
(a) LDA-induced distribution over topics
(b) Adverb lemma frequencies
(c) Verb distributions over subcategorization frames
Figure 2: Subdomain variation plotted as heat maps and dendrograms
694
Sentence length Full NP length Base nominal length
Mol. Biology 27.0 Biochemistry 4.03 Biochemistry 1.85
Genetics 26.6 Genetics 3.90 Neoplasms 1.85
Cell Biology 26.3 Critical Care 3.86 Mol. Biology 1.84
Ethics 26.2 Neoplasms 3.85 Genetics 1.83
PMC Average 25.9 PMC Average 3.85 PMC Average 1.80
Biochemistry 25.8 Pediatrics 3.84 Cell Biology 1.80
Neoplasms 25.5 Med. Informatics 3.84 Critical Care 1.80
Psychiatry 25.3 Comm. Diseases 3.81 Med. Informatics 1.78
Critical Care 25.0 Therapeutics 3.80 Comm. Diseases 1.78
Therapeutics 24.9 Mol. Biology 3.79 Therapeutics 1.75
Comm. Diseases 24.9 Psychiatry 3.77 Psychiatry 1.75
Med. Informatics 24.6 Ethics 3.69 Pediatrics 1.73
Pediatrics 24.6 Cell Biology 3.55 Ethics 1.65
Newswire 19.1 Newswire 3.18 Newswire 1.60
Table 2: Average sentence, NP and base nominal lengths across domains
phrase structures; there is notable variation across
PMC domains. Full NP and base nominal lengths
do not always correlate; for example, Cell Biol-
ogy uses relatively long base NPs (nominalisations
and multitoken names in particular) but relatively
simple full NP structures.
5.4 Coreference
Resolving coreferential terms is a crucial and chal-
lenging task when extracting information from
texts in any domain. Nguyen and Kim (2008)
compare the use of pronouns in the newswire
and biomedical domains, using the GENIA cor-
pus as representative of the latter. Among the dif-
ferences observed between the domains were the
absence of any personal pronouns other than third-
person neuter pronouns in the GENIA corpus, and
a greater proportion of demonstrative pronouns in
GENIA than in the ACE or MUC newswire cor-
pora. Corroborating the importance of domain
modelling, Nguyen and Kim demonstrate that tai-
loring a pronoun resolution system to specific prop-
erties of the biomedical domain improves perfor-
mance.
As our corpus is not annotated for coreference
we restrict our attention to types that are reliably
coreferential: masculine/feminine personal pro-
nouns (he, she and case variations), neuter personal
pronouns (they, it and variations) and definite NPs
with demonstrative determiners such as this and
that. To filter out pleonastic pronouns we used a
combination of the C+C parser?s pleonasm tag and
heuristics based on Lappin and Leass (1994). To
filter out the most common class of non-anaphoric
demonstrative NPs we simply discarded any match-
ing the pattern this. . . paper|study|article.
Table 3 presents statistics for selected types of
coreferential noun phrases in a number of domains.
The results generally agree with the findings of
Nguyen and Kim (2008): biomedical text is on
average 200 times less likely than news text to
use gendered pronouns and twice as likely to use
anaphoric definite noun phrases. At the domain
level, however, there is clear variation within the
biomedical corpus. In contrast to Nguyen and
Kim?s observations about GENIA some domains
do make non-negligible use of gendered pronouns,
most notably Ethics (usually to refer to other schol-
ars) and domains such as Psychiatry and Pediatrics
where studies of actual patients are common. All
biomedical domains use demonstrative NPs more
frequently than newswire and only one (Ethics)
matches newswire for frequent use of neuter 3rd-
person pronouns.
6 Conclusion
In this paper we have explored the phenomenon
of linguistic variation at a finer-grained level than
previous NLP research, focusing on subdomains
695
Pronouns (neuter, 3rd) Pronouns (non-neuter, 3rd) Demonstrative NPs
Ethics 0.0658 Newswire 0.0591 Genetics 0.0275
Newswire 0.0607 Ethics 0.0037 Med. Informatics 0.0263
Therapeutics 0.0354 Pediatrics 0.0015 Biochemistry 0.0263
Med. Informatics 0.0346 Psychiatry 0.0009 Ethics 0.0260
Psychiatry 0.0342 Comm. Diseases 0.0009 Mol. Biology 0.0251
Pediatrics 0.0308 Therapeutics 0.0005 PMC Average 0.0226
PMC Average 0.0284 PMC Average 0.0005 Cell Biology 0.0210
Genetics 0.0275 Critical Care 0.0004 Comm. Diseases 0.0207
Critical Care 0.0272 Neoplasms 0.0002 Neoplasms 0.0205
Mol. Biology 0.0258 Med. Informatics 0.0002 Psychiatry 0.0201
Biochemistry 0.0251 Genetics 0.0001 Critical Care 0.0201
Neoplasms 0.0227 Mol. Biology 2.5? 10?5 Therapeutics 0.0192
Cell Biology 0.0217 Biochemistry 2.0? 10?5 Pediatrics 0.0191
Comm. Diseases 0.0213 Cell Biology 1.5? 10?5 Newswire 0.0118
Table 3: Frequency of coreferential types (proportion of all NPs) across domains
rather than traditional domains such as ?newswire?
and ?biomedicine?. We have identified patterns of
variation across dimensions of vocabulary, syntax
and discourse that are known to be of importance
for NLP applications. While the magnitude of vari-
ation between subdomains is unsurprisingly less
pronounced than between coarser domains, sub-
domain variation clearly does exist and should be
taken into account when considering the generalis-
ability of systems trained and evaluated on specific
subdomains, for example molecular biology.
Future work includes directly evaluating the ef-
fect of subdomain variation on practical tasks, in-
vestigating further dimensions of variation such
as nominalisation usage and learning alternative
subdomain taxonomies directly from the corpus
text. Ultimately, we expect that a more nuanced
understanding of subdomain effects will have tan-
gible benefits for many applications of scientific
language processing.
Acknowledgements
This work was supported by EPSRC grant
EP/G051070/1, the Royal Society (AK) and a
Dorothy Hodgkin Postgraduate Award (LS).
References
Biber, Douglas and Bethany Gray. 2010. Challeng-
ing stereotypes about academic writing: Complex-
ity, elaboration, explicitness. Journal of English for
Academic Purposes, 9(1):2?20.
Biber, Douglas. 1988. Variation Across Speech and
Writing. Cambridge University Press, Cambridge.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Cohen, K. Bretonnel, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9):e3158.
Collins, Michael John. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of ACL-96, Santa Cruz, CA.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Curran, James, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale NLP with C&C
and Boxer. In Proceedings of the ACL-07 Demo and
Poster Sessions, Prague, Czech Republic.
Daume? III, Hal and Daniel Marcu. 2006. Domain
adaptation for statistical classifiers. Journal of Ar-
tificial Intelligence Research, 26:101?126.
Friedman, Carol, Pauline Kraa, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35(4):222?235.
Graff, David, Junbo Kong, Ke Chen, and Kazuaki
Maeda, 2005. English Gigaword Corpus, 2nd Edi-
tion. Linguistic Data Consortium.
696
Hara, Tadayoshi, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Pro-
ceedings of IJCNLP-05, Jeju Island, South Korea.
Jin, Yang, Ryan T. McDonald, Kevin Lerman, Mark A.
Mandel, Steven Carroll, Mark Y. Liberman, Fer-
nando C. Pereira, Raymond S. Winters, and Peter S.
White. 2006. Automated recognition of malignancy
mentions in biomedical literature. BMC Bioinfor-
matics, 7:492.
Kim, J.-D., T. Ohta, Y. Tateisi, and J. Tsujii. 2003.
GENIA corpus - a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(Suppl. 1):i180?
i182.
Korhonen, Anna, Yuval Krymolowski, and Nigel Col-
lier. 2008. The choice of features for classifica-
tion of verbs in biomedical texts. In Proceedings
of COLING-08, Manchester, UK.
Kulick, Seth, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein,
Lyle Ungar, Scott Winters, and Pete White. 2004.
Integrated annotation for biomedical information ex-
traction. In Proceedings of the HLT-NAACL-04
Workshop on Linking Biological Literature, Ontolo-
gies and Databases, Boston, MA.
Lappin, Shalom and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535?561.
Nguyen, Ngan L.T. and Jin-Dong Kim. 2008. Explor-
ing domain differences for the design of a pronoun
resolution system for biomedical text. In Proceed-
ings of COLING-08, Manchester, UK.
NIH. 2009a. Journal publishing tag set.
http://dtd.nlm.nih.gov/publishing/.
NIH. 2009b. National library of
medicine: Journal subject terms.
http://wwwcf.nlm.nih.gov/serials/journals/index.cfm.
Preiss, Judita, E.J. Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In Proceedings of ACL-07, Prague, Czech
Republic.
Rayson, Paul and Roger Garside. 2000. Comparing
corpora using frequency profiling. In Proceedings
of the ACL-00 Workshop on Comparing Corpora,
Hong Kong.
Rimell, Laura and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852?865.
Roland, Douglas and Daniel Jurafsky. 1998. How
verb subcategorization frequencies are affected by
corpus choice. In Proceedings of COLING-ACL-98,
Montreal, Canada.
Rosario, Barbara and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via
a domain-specific lexical hierarchy. In Proceedings
of EMNLP-01, Pittsburgh, PA.
Schuman, Jonathan and Sabine Bergler. 2006. Post-
nominal prepositional phrase attachment in pro-
teomics. In Proceedings of the HLT-NAACL-06
BioNLP Workshop on Linking Natural Language
and Biology, New York, NY.
Verspoor, Karin, K Bretonnel Cohen, and Lawrence
Hunter. 2009. The textual characteristics of tradi-
tional and Open Access scientific journals are simi-
lar. BMC Bioinformatics, 10:183.
697
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1047?1057,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Probabilistic models of similarity in syntactic context
Diarmuid O? Se?aghdha
Computer Laboratory
University of Cambridge
United Kingdom
do242@cl.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
United Kingdom
Anna.Korhonen@cl.cam.ac.uk
Abstract
This paper investigates novel methods for in-
corporating syntactic information in proba-
bilistic latent variable models of lexical choice
and contextual similarity. The resulting mod-
els capture the effects of context on the inter-
pretation of a word and in particular its effect
on the appropriateness of replacing that word
with a potentially related one. Evaluating our
techniques on two datasets, we report perfor-
mance above the prior state of the art for esti-
mating sentence similarity and ranking lexical
substitutes.
1 Introduction
Distributional models of lexical semantics, which
assume that aspects of a word?s meaning can be re-
lated to the contexts in which that word is typically
used, have a long history in Natural Language Pro-
cessing (Spa?rck Jones, 1964; Harper, 1965). Such
models still constitute one of the most popular ap-
proaches to lexical semantics, with many proven ap-
plications. Much work in distributional semantics
treats words as non-contextualised units; the models
that are constructed can answer questions such as
?how similar are the words body and corpse?? but
do not capture the way the syntactic context in which
a word appears can affect its interpretation. Re-
cent developments (Mitchell and Lapata, 2008; Erk
and Pado?, 2008; Thater et al, 2010; Grefenstette et
al., 2011) have aimed to address compositionality of
meaning in terms of distributional semantics, lead-
ing to new kinds of questions such as ?how similar
are the usages of the words body and corpse in the
phrase the body/corpse deliberated the motion. . . ??
and ?how similar are the phrases the body deliber-
ated the motion and the corpse rotted??. In this pa-
per we focus on answering questions of the former
type and investigate models that describe the effect
of syntactic context on the meaning of a single word.
The work described in this paper uses probabilis-
tic latent variable models to describe patterns of syn-
tactic interaction, building on the selectional prefer-
ence models of O? Se?aghdha (2010) and Ritter et al
(2010) and the lexical substitution models of Dinu
and Lapata (2010). We propose novel methods for
incorporating information about syntactic context in
models of lexical choice, yielding a probabilistic
analogue to dependency-based models of contextual
similarity. Our models attain state-of-the-art per-
formance on two evaluation datasets: a set of sen-
tence similarity judgements collected by Mitchell
and Lapata (2008) and the dataset of the English
Lexical Substitution Task (McCarthy and Navigli,
2009). In view of the well-established effectiveness
of dependency-based distributional semantics and of
probabilistic frameworks for semantic inference, we
expect that our approach will prove to be of value in
a wide range of application settings.
2 Related work
The literature on distributional semantics is vast; in
this section we focus on outlining the research that is
most directly related to capturing effects of context
and compositionality.1 Mitchell and Lapata (2008)
1The interested reader is referred to Pado? and Lapata (2007)
and Turney and Pantel (2010) for a general overview.
1047
follow Kintsch (2001) in observing that most dis-
tributional approaches to meaning at the phrase or
sentence level assume that the contribution of syn-
tactic structure can be ignored and the meaning of a
phrase is simply the commutative sum of the mean-
ings of its constituent words. As Mitchell and Lap-
ata argue, this assumption clearly leads to an impov-
erished model of semantics. Mitchell and Lapata in-
vestigate a number of simple methods for combining
distributional word vectors, concluding that point-
wise multiplication best corresponds to the effects
of syntactic interaction.
Erk and Pado? (2008) introduce the concept of a
structured vector space in which each word is as-
sociated with a set of selectional preference vec-
tors corresponding to different syntactic dependen-
cies. Thater et al (2010) develop this geometric ap-
proach further using a space of second-order distri-
butional vectors that represent the words typically
co-occurring with the contexts in which a word typi-
cally appears. The primary concern of these authors
is to model the effect of context on word meaning;
the work we present in this paper uses similar intu-
itions in a probabilistic modelling framework.
A parallel strand of research seeks to represent
the meaning of larger compositional structures us-
ing matrix and tensor algebra (Smolensky, 1990;
Rudolph and Giesbrecht, 2010; Baroni and Zampar-
elli, 2010; Grefenstette et al, 2011). This nascent
approach holds the promise of providing a much
richer notion of context than is currently exploited
in semantic applications.
Probabilistic latent variable frameworks for gen-
eralising about contextual behaviour (in the form
of verb-noun selectional preferences) were proposed
by Pereira et al (1993) and Rooth et al (1999). La-
tent variable models are also conceptually similar
to non-probabilistic dimensionality reduction tech-
niques such as Latent Semantic Analysis (Landauer
and Dumais, 1997). More recently, O? Se?aghdha
(2010) and Ritter et al (2010) reformulated Rooth et
al.?s approach in a Bayesian framework using mod-
els related to Latent Dirichlet Allocation (Blei et al,
2003), demonstrating that this ?topic modelling? ar-
chitecture is a very good fit for capturing selectional
preferences. Reisinger and Mooney (2010) inves-
tigate nonparametric Bayesian models for teasing
apart the context distributions of polysemous words.
As described in Section 3 below, Dinu and Lapata
(2010) propose an LDA-based model for lexical sub-
stitution; the techniques presented in this paper can
be viewed as a generalisation of theirs. Topic models
have also been applied to other classes of semantic
task, for example word sense disambiguation (Li et
al., 2010), word sense induction (Brody and Lapata,
2009) and modelling human judgements of semantic
association (Griffiths et al, 2007).
3 Models
3.1 Latent variable context models
In this paper we consider generative models of lex-
ical choice that assign a probability to a particular
word appearing in a given linguistic context. In par-
ticular, we follow recent work (Dinu and Lapata,
2010; O? Se?aghdha, 2010; Ritter et al, 2010) in as-
suming a latent variable model that associates con-
texts with distributions over a shared set of variables
and associates each variable with a distribution over
the vocabulary of word types:
P (w|c) =
?
z?Z
P (w|z)P (z|c) (1)
The set of latent variables Z is typically much
smaller than the vocabulary size; this induces a (soft)
clustering of the vocabulary. Latent Dirichlet Allo-
cation (Blei et al, 2003) is a powerful method for
learning such models from a text corpus in an unsu-
pervised way; LDA was originally applied to doc-
ument modelling, but it has recently been shown to
be very effective at inducing models for a variety of
semantic tasks (see Section 2).
Given the latent variable framework in (1) we can
develop a generative model of paraphrasing a word
o with another word n in a particular context c:
PC?T (n|o, c) =
?
z
P (n|z)P (z|o, c) (2)
P (z|o, c) = P (o|z)P (z|c)?
z? P (o|z?)P (z?|c)
(3)
In words, the probability P (n|o, c) is the probability
that n would be generated given the latent variable
distribution associated with seeing o in context c;
this latter distribution P (z|o, c) can be derived using
Bayes? rule and the assumption P (o|z, c) = P (o|z).
1048
Given a set of contexts C in which an instance o ap-
pears (e.g., it may be both the subject of a verb and
modified by an adjective), (2) and (3) become:
PC?T (n|o, C) =
?
z
P (n|z)P (z|o, C) (4)
P (z|o, C) = P (o|z)P (z|C)?
z? P (o|z?)P (z?|C)
(5)
P (z|C) =
?
c?C P (z|c)?
z?
?
c?C P (z?|c)
(6)
Equation (6) can be viewed as defining a ?product
of experts? model (Hinton, 2002). Dinu and Lapata
(2010) also use a similar formulation to (5), except
that P (z|o, C) is factorised over P (z|o, C) rather
than just P (z|C):
PDL10(z|o, C) =
?
c?C
P (o|z)P (z|c)?
z? P (o|z?)P (z?|c)
(7)
In Section 5 below, we find that using (5) rather than
(7) gives better results.
The model described above (henceforth C ? T )
models the dependence of a target word on its con-
text. An alternative perspective is to model the de-
pendence of a set of contexts on a target word, i.e.,
we induce a model
P (c|w) =
?
z
P (c|z)P (z|w) (8)
Making certain assumptions, a formula for P (n|o, c)
can be derived from (8):
PT?C(n|o, c) =
P (c|o, n)P (n|o)
P (c|o) (9)
P (c|o, n) =
?
z
P (c|z)P (z|o, n)
P (z|o, n) = P (z|o)P (z|n)?
z? P (z?|o)P (z?|n)
(10)
P (c|o) =
?
z
P (c|z)P (z|o) (11)
P (n|o) = 1/V (12)
The assumption of a uniform prior P (n|o) on the
choice of a paraphrase n for o is clearly not appro-
priate from a language modelling perspective (one
could imagine an alternative P (n) based on corpus
frequency), but in the context of measuring semantic
similarity it serves well. The T ? C model for a set
of contexts C is:
PT?C(n|o, C) =
P (C|o, n)P (n|o)
P (C|o) (13)
P (C|o, n) =
?
z
P (z|o, n)
?
c?C
P (c|z) (14)
P (C|o) =
?
z
P (z|o)
?
c?C
P (c|z) (15)
P (z|o, C) = P (z|o)P (C|o)?
z? P (z?|o)P (C|o)
(16)
With appropriate priors chosen for the distribu-
tions over words and latent variables, P (n|o, C) is
a fully generative model of lexical substitution. A
non-generative alternative is one that estimates the
similarity of the latent variable distributions associ-
ated with seeing n and o in context C. The princi-
ple that similarity between topic distributions corre-
sponds to semantic similarity is well-known in doc-
ument modelling and was proposed in the context
of lexical substitution by Dinu and Lapata (2010).
In terms of the equations presented above, we could
compare the distributions P (z|o, C) with P (z|n,C)
using equations (5) or (16). However, Thater et
al. (2010) and Dinu and Lapata (2010) both ob-
serve that contextualising both o and n can degrade
performance; in view of this we actually compare
P (z|o, C) with P (z|n) and make the further simpli-
fying assumption that P (z|n) ? P (n|z). The sim-
ilarity measure we adopt is the Bhattacharyya coef-
ficient, which is a natural measure of similarity be-
tween probability distributions and is closely related
to the Hellinger distance used in previous work on
topic modelling (Blei and Lafferty, 2007):
simbhatt(Px(z), Py(z)) =
?
z
?
Px(z)Py(z) (17)
This measure takes values between 0 and 1.
In this paper we train LDA models of P (w|c) and
P (c|w). In the former case, the analogy to document
modelling is that each context type plays the role of
a ?document? consisting of all the words observed
in that context in a corpus; for P (c|w) the roles are
reversed. The models are trained by Gibbs sampling
using the efficient procedure of Yao et al (2009).
The empirical estimates for distributions over words
and latent variables are derived from the assignment
1049
of topics over the training corpus in a single sam-
pling state. For example, to model P (w|c) we cal-
culate:
P (w|z) = fzw + ?fz? +N?
(18)
P (z|c) = fzc + ?zf?c +
?
z? ?z?
(19)
where fzw is the number of words of type w as-
signed topic z, fzc is the number of times z is associ-
ated with context c, fz? and f?c are the marginal topic
and context counts respectively, N is the number of
word types and ? and ? parameterise the Dirichlet
prior distributions over P (z|c) and P (w|z). Follow-
ing the recommendations of Wallach et al (2009)
we use asymmetric ? and symmetric ?; rather than
using fixed values for these hyperparameters we es-
timate them from data in the course of LDA train-
ing using an EM-like method.2 We use standard set-
tings for the number of training iterations (1000), the
length of the burnin period before hyperparameter
estimation begins (200 iterations) and the frequency
of hyperparameter estimation (50 iterations).
3.2 Context types
We have not yet defined what the contexts c look
like. In vector space models of semantics it is
common to distinguish between window-based and
dependency-based models (Pado? and Lapata, 2007);
one can make the same distinction for probabilis-
tic context models. A broad generalisation is that
window-based models capture semantic association
(e.g. referee is associated with football), while
dependency models capture a finer-grained notion
of similarity (referee is similar to umpire but not
to football). Dinu and Lapata (2010) propose a
window-based model of lexical substitution; the set
of contexts in which a word appears is the set of
surrounding words within a prespecified ?window
size?. In this paper we also investigate dependency-
based context sets derived from syntactic structure.
Given a sentence such as
2We use the estimation methods provided by the MAL-
LET toolkit, available from http://mallet.cs.umass.
edu/.
The:d executive:j body:n
n:ncmod:j
OO decided:v
v:ncsubj:n
 . . .
the set C of dependency contexts for the noun body
is {executive:j:ncmod?1:n, decide:v:ncsubj:n},
where ncmod?1 denotes that body stands in an in-
verse non-clausal modifier relation to executive (we
assume that nouns are the heads of their adjectival
modifiers).
4 Experiment 1: Similarity in context
4.1 Data
Mitchell and Lapata (2008) collected human judge-
ments of semantic similarity for pairs of short sen-
tences, where the sentences in a pair share the same
subject but different verbs. For example, the sales
slumped and the sales declined should be judged as
very similar while the shoulders slumped and the
shoulders declined should be judged as less similar.
The resulting dataset (henceforth ML08) consists of
120 such pairs using 15 verbs, balanced across high
and low expected similarity. 60 subjects rated the
data using a scale of 1?7; Mitchell and Lapata cal-
culate average interannotator correlation to be 0.40
(using Spearman?s ?). Both Mitchell and Lapata
and Erk and Pado? (2008) split the data into a devel-
opment portion and a test portion, the development
portion consisting of the judgements of six annota-
tors; in order to compare our results with previous
research we use the same data split. To evaluate per-
formance, the predictions made by a model are com-
pared to the judgements of each annotator in turn
(using ?) and the resulting per-annotator ? values are
averaged.
4.2 Models
All models were trained on the written section of the
British National Corpus (around 90 million words),
parsed with RASP (Briscoe et al, 2006). The BNC
was also used by Mitchell and Lapata (2008) and
Erk and Pado? (2008); as the ML08 dataset was com-
piled using words appearing more than 50 times in
the BNC, there are no coverage problems caused
by data sparsity. We trained LDA models for the
grammatical relations v:ncsubj:n and n:ncsubj?1:v
1050
Model PARA SIM
No optimisation
C ? T 0.24 0.34
T ? C 0.36 0.39
T ? C 0.33 0.39
Optimised on dev
C ? T 0.24 0.35
T ? C 0.41 0.41
T ? C 0.37 0.41
Erk and Pado? (2008) Mult 0.24SVS 0.27
Table 1: Performance (average ?) on the ML08 test
set
and used these to create predictors of type C ? T
and T ? C, respectively. For each predictor, we
trained five runs with 100 topics for 1000 iterations
and averaged the predictions produced from their fi-
nal states. We investigate both the generative para-
phrasing model (PARA) and the method of compar-
ing topic distributions (SIM). For both PARA and
SIM we present results using each predictor type on
its own as well as a combination of both types (T ?
C); for PARA the contributions of the types are mul-
tiplied and for SIM they are averaged.3 One poten-
tial complication is that the PARA model is trained
to predict P (n|c, o), which might not be comparable
across different combinations of subject c and verb
o. Using P (n|c, o) as a proxy for the desired joint
distribution P (n, c, o) is tantamount to assuming a
uniform distribution P (c, o), which can be defended
on the basis that the choice of subject noun and ref-
erence verb is not directly relevant to the task. As
shown by the results below, this assumption seems
to work reasonably well in practice.
As well as reporting correlations for straightfor-
ward averages of each set of five runs, we also inves-
tigate whether the development data can be used to
select an optimal subset of runs. This is done by sim-
ply evaluating every possible subset of 1?5 runs on
the development data and picking the best-scoring
subset.
4.3 Results
Table 1 presents the results of the PARA and SIM
predictors on the ML08 dataset. The best results
3This configuration seems the most intuitive; averaging
PARA predictors and multiplying SIM also give good results.
previously reported for this dataset were given by
Erk and Pado? (2008), who measured average ? val-
ues of 0.24 for a vector multiplication method and
0.27 for their structured vector space (SVS) syn-
tactic disambiguation method. Even without using
the development set to select models, performance is
well above the previous state of the art for all predic-
tors except PARAC?T . Model selection on the de-
velopment data brings average ? up to 0.41, which is
comparable to the human ?ceiling? of 0.40 measured
by Mitchell and Lapata. In all cases the T ? C pre-
dictors outperform C ? T : models that associate
target words with distributions over context clusters
are superior to those that associate contexts with dis-
tributions over target words.
Figure 1 plots the beneficial effect of averaging
over multiple runs; as the number of runs n is in-
creased, the average performance over all combi-
nations of n predictors chosen from the set of five
T ? C and five C ? T runs is observed to in-
crease monotonically. Figure 1 also shows that the
model selection procedure is very effective at se-
lecting the optimal combination of models; develop-
ment set performance is a reliable indicator of test
set performance.
5 Experiment 2: Lexical substitution
5.1 Data
The English Lexical Substitution task, run as part
of the SemEval-1 competition, required participants
to propose good substitutes for a set of target words
in various sentential contexts (McCarthy and Nav-
igli, 2009). Table 2 shows two example sentences
and the substitutes appearing in the gold standard,
ranked by the number of human annotators who pro-
posed each substitute. The dataset contains a total of
2,010 annotated sentences with 205 distinct target
words across four parts of speech (noun, verb, ad-
jective, adverb). In line with previous work on con-
textual disambiguation, we focus here on the subtask
of ranking attested substitutes rather than proposing
them from an unrestricted vocabulary. To this end,
a candidate set is constructed for each target word
from all the substitutes proposed for that word in all
sentences in the dataset.
The data contains a number of multiword para-
phrases such as rush at; as our models (like most
1051
1 2 3 4 5
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(a) PARA: Target ? Context
1 2 3 4 5
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(b) PARA: Context ? Target
2 3 4 5 6 7 8 9 10
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(c) PARA: Target ? Context
1 2 3 4 5
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(d) SIM: Target ? Context
1 2 3 4 5
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(e) SIM: Context ? Target
2 3 4 5 6 7 8 9 10
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(f) SIM: Target ? Context
Figure 1: Performance on the ML08 test set with different predictor types and different numbers of LDA
runs per predictor type; the solid line tracks the average performance, the dashed line shows the performance
of the predictor combination that scores best on the development set.
Realizing immediately that strangers have come, attack (5), rush at (1)the animals charge them and the horses began to fight.
Commission is the amount charged to execute a trade. levy (2), impose (1), take (1), demand (1)
Table 2: Examples for the verb charge from the English Lexical Substitution Task
current models of distributional semantics) do not
represent multiword expressions, we remove such
paraphrases and discard the 17 sentences which have
only multiword substitutes in the gold standard.4
There are also 7 sentences for which the gold stan-
dard contains no substitutes. This leaves a total of
1986 sentences. These sentences were lemmatised
and parsed with RASP.
Previous authors have partitioned the dataset in
various ways. Erk and Pado? (2008) use only a sub-
set of the data where the target is a noun headed
by a verb or a verb heading a noun. Thater et al
4Thater et al (2010) and Dinu and Lapata (2010) similarly
remove multiword paraphrases (Georgiana Dinu, p.c.).
(2010) discard sentences which their parser cannot
parse and paraphrases absent from their training cor-
pus and then optimise the parameters of their model
through four-fold cross-validation. Here we aim for
complete coverage on the dataset and do not perform
any parameter tuning. We use two measures to eval-
uate performance: Generalised Averaged Precision
(Kishida, 2005) and Kendall?s ?b rank correlation
coefficient, which were used for this task by Thater
et al (2010) and Dinu and Lapata (2010), respec-
tively. Generalised Averaged Precision (GAP) is
a precision-like measure for evaluating ranked pre-
dictions against a gold standard. ?b is a variant of
Kendall?s ? that is appropriate for data containing
tied ranks. We do not use the ?precision out of ten?
1052
COORDINATION:
Cats and
c:conj:n
OO
c:conj:n
OOdogs run
v:ncsubj:n

? Cats and dogsOO
n:and:n
OO run

v:ncsubj:n

PREDICATION:
The cat is
v:ncsubj:n
OO
v:xcomp:j
fierce ? The cat
n:ncmod:j
is fierce
PREPOSITIONS:
The cat
n:ncmod:i
in
i:dobj:n
OOthe hat ? The cat
n:prep in:n
in the hat
Table 3: Dependency graph preprocessing
measure that was used in the original Lexical Substi-
tution Task; this measure assigns credit for the pro-
portion of the first 10 proposed paraphrases that are
present in the gold standard and in the context of
ranking attested substitutes it is unclear how to ob-
tain non-trivial results for target words with 10 or
fewer possible substitutes. We calculate statistical
significance of performance differences using strati-
fied shuffling (Yeh, 2000).5
5.2 Models
We apply the models developed in Section 3.1 to the
Lexical Substitution Task dataset using dependency-
and window-based context information. Here we
only use the SIM predictor type. PARA did not give
satisfactory results; in particular, it tended to rank
common words highly in most contexts.6
As before we compiled training data by extracting
target-context cooccurrences from a text corpus. In
addition to the parsed BNC described above we used
a corpus of Wikipedia text consisting of over 45 mil-
lion sentences (almost 1 billion words) parsed using
the fast Combinatory Categorial Grammar (CCG)
parser described by Clark et al (2009). The depen-
5We use the software package available at http://www.
nlpado.de/?sebastian/sigf.html.
6Favouring more general words may indeed make sense in
some paraphrasing tasks (Nulty and Costello, 2010).
dency representation produced by this parser is inter-
operable with the RASP dependency format. In or-
der to focus our models on semantically discrimina-
tive information and make inference more tractable
we ignored all parts of speech other than nouns,
verbs, adjectives, prepositions and adverbs. Stop-
words and words of fewer than three characters were
removed. We also removed the very frequent but se-
mantically weak lemmas be and have.
We compare two classes of context models: mod-
els learned from window-based contexts and models
learned from syntactic dependency contexts. For the
syntactic models we extracted all dependencies and
inverse dependencies between lemmas of the afore-
mentioned POS types; in order to maximise the ex-
traction yield, the dependency graph for each sen-
tence was preprocessed using the transformations
shown in Table 3. For the window-based context
model we follow Dinu and Lapata (2010) in treating
each word within five words of a target as a member
of its context set.
It proved necessary to subsample the corpora in
order to make LDA training tractable, especially for
the window-based model where the training set of
context-target counts is extremely dense (each in-
stance of a word in the corpus contributes up to
10 context instances). For the window-based data,
we divided each context-target count by a factor of
5 and a factor of 70 for the BNC and Wikipedia
corpora respectively, rounding fractional counts to
the closest integer. The choice of 70 for scaling
Wikipedia counts is adopted from Dinu and Lap-
ata (2010), who used the same factor for the com-
parably sized English Gigaword corpus. As the de-
pendency data is an order of magnitude smaller we
downsampled the Wikipedia counts by 5 and left the
BNC counts untouched. Finally, we created a larger
corpus by combining the counts from the BNC and
Wikipedia datasets. Type and token counts for the
BNC and combined corpora are given in Table 4.
We trained three LDA predictors for each corpus:
a window-based predictor (W5), a Context ? Tar-
get predictor (C ? T ) and a Target ? Context
predictor (T ? C). For W5 the sets of types and
contexts should be symmetrical (in practice there
is some discrepancy due to preprocessing artefacts).
ForC ? T , individual models were trained for each
of the four target parts of speech; in each case the set
1053
BNC BNC+Wikipedia
Tokens Types Contexts Tokens Types Contexts
Nouns 18723082 122999 316237 54145216 106448 514257
Verbs 7893462 18494 57528 20082658 16673 82580
Adjectives 4385788 73684 37163 11536424 88488 57531
Adverbs 1976837 7124 14867 3017936 4056 18510
Window5 28329238 88265 102792 42828094 139640 143443
Table 4: Type and token counts for the BNC and downsampled BNC+Wikipedia corpora
BNC BNC + Wikipedia
GAP ?b Coverage GAP ?b Coverage
W5 44.5 0.17 100.0 44.8 0.17 100.0
C ? T 43.2 0.16 86.4 48.7 0.21 86.5
T ? C 47.2 0.21 86.4 49.3 0.22 86.5
T ? C 45.7 0.20 86.4 49.1 0.23 86.5
W5 + C ? T 46.0 0.18 100.0 48.7 0.21 100.0
W5 + T ? C 48.6 0.21 100.0 49.3 0.22 100.0
W5 + T ? C 48.1 0.20 100.0 49.5 0.23 100.0
Table 5: Results on the English Lexical Substitution Task dataset; boldface denotes best performance at full
coverage for each corpus
of types is the vocabulary for that part of speech and
the set of contexts is the set of dependencies taking
those types as dependents. For T ? C we again
train four models; the sets of types and contexts are
reversed. For the both corpora we trained models
with Z = {600, 800, 1000, 1200} topics; for each
setting of Z we ran five estimation runs. Each in-
dividual prediction of similarity between P (z|C, o)
and P (z|n) is made by averaging over the predic-
tions of all runs and over all settings of Z. Choosing
a single setting of Z does not degrade performance
significantly; however, averaging over settings is a
convenient way to avoid having to pick a specific
value.
We also investigate combinations of predictor
types, once again produced by averaging: we com-
bine C ? T with C ? T (T ? C) and combine
each of these three models with W5.
5.3 Results
Table 5 presents the results attained by our mod-
els on the Lexical Substitution Task data. The
dependency-based models have imperfect coverage
(86% of the data); they can make no prediction when
no syntactic context is provided for a target, per-
haps as a result of parsing error. The window-based
models have perfect coverage, but score noticeably
lower. By combining dependency- and window-
based models we can reach high performance with
perfect coverage. All combinations outperform the
corresponding W5 results to a statistically signifi-
cant degree (p < 0.01). Performance at full cov-
erage is already very good (GAP= 48.6, ?b = 0.21)
on the BNC corpus, but the best results are attained
by W5 + T ? C trained on the combined corpus
(GAP= 49.5, ?b = 0.23). The results for the W5
model trained on BNC data is comparable to that
trained on the combined corpus; however the syntac-
tic models show a clear benefit from the less sparse
dependency data in the combined training corpus.
As remarked in Section 3.1, Dinu and Lap-
ata (2010) use a slightly different formulation of
P (z|C, o). Using the window-based context model
our formulation (5) outperforms (7) for both training
corpora; the Dinu and Lapata (2010) version scores
GAP = 41.5, ?b = 0.15 for the BNC corpus and
GAP = 42.0, ?b = 0.15 for the combined corpus.
The advantage of our formulation is statistically sig-
nificant for all evaluation measures.
1054
Nouns Verbs Adjectives Adverbs Overall
GAP ?b GAP ?b GAP ?b GAP ?b GAP ?b
W5 46.0 0.16 38.9 0.14 44.0 0.18 54.0 0.22 44.8 0.17
W5 + T ? C 50.7 0.22 45.1 0.20 48.8 0.24 55.9 0.24 49.5 0.23
Thater et al (2010) (Model 1) 46.4 ? 45.9 ? 39.4 ? 48.2 ? 44.6 ?
Thater et al (2010) (Model 2) 42.5 ? ? ? 43.2 ? 51.4 ? ? ?
Dinu and Lapata (2010) (LDA) ? 0.16 ? 0.14 ? 0.17 ? 0.21 ? 0.16
Dinu and Lapata (2010) (NMF) ? 0.15 ? 0.14 ? 0.16 ? 0.26 ? 0.16
Table 6: Performance by part of speech
Table 6 gives a breakdown of performance by tar-
get part of speech for the BNC+Wikipedia-trained
W5 and W5 + T ? C models, as well as figures
provided by previous researchers.7 W5 + T ? C
outperforms W5 on all parts of speech using both
evaluation metrics. As remarked above, previous re-
searchers have used the corpus in slightly different
ways; we believe that the results of Dinu and Lapata
(2010) are fully comparable, while those of Thater et
al. (2010) were attained on a slightly smaller dataset
with parameters set through cross-validation. The
results for W5 + T ? C outperform all of Dinu
and Lapata?s per-POS and overall results except for
a slightly superior score on adverbs attained by their
NMF model (?b = 0.26 compared to 0.24). Turn-
ing to Thater et al, we report higher scores for ev-
ery POS with the exception of the verbs where their
Model 1 achieves 45.9 GAP compared to 45.1; the
overall average for W5 + T ? C is substantially
higher at 49.5 compared to 44.6. On balance, we
suggest that our models do have an advantage over
the current state of the art for lexical substitution.
6 Conclusion
In this paper we have proposed novel methods for
modelling the effect of context on lexical mean-
ing, demonstrating that information about syntactic
context and textual proximity can fruitfully be inte-
grated to produce state-of-the-art models of lexical
choice. We have demonstrated the effectiveness of
our techniques on two datasets but they are poten-
tially applicable to a range of applications where se-
mantic disambiguation is required. In future work,
7The overall average GAP for Thater et al (2010) does not
appear in their paper but can be calculated from the score and
number of instances listed for each POS.
we intend to adapt our approach for word sense dis-
ambiguation as well as related domain-specific tasks
such as gene name normalisation (Morgan et al,
2008). A further, more speculative direction for fu-
ture research is to investigate more richly structured
models of context, for example capturing correla-
tions between words in a text within a framework
similar to the Correlated Topic Model of Blei and
Lafferty (2007) or more explicitly modelling poly-
semy effects as in Reisinger and Mooney (2010).
Acknowledgements
We are grateful to the EMNLP reviewers for their
helpful comments. This research was supported by
EPSRC grant EP/G051070/1.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
10), Cambridge, MA.
David M. Blei and John D. Lafferty. 2007. A correlated
topic model of science. The Annals of Applied Statis-
tics, 1(1):17?35.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the ACL-06 Interactive Presentation Sessions,
Sydney, Australia.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of EACL-09, Athens,
Greece.
1055
Stephen Clark, Ann Copestake, James R. Curran, Yue
Zhang, Aurelie Herbelot, James Haggerty, Byung-Gyu
Ahn, Curt Van Wyk, Jessika Roesner, Jonathan Kum-
merfeld, and Tim Dawborn. 2009. Large-scale syn-
tactic processing: Parsing the web. Technical report,
Final Report of the 2009 JHU CLSP Workshop.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP-10), Cambridge,MA.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-08),
Honolulu, HI.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011. Con-
crete sentence spaces for compositional distributional
models of meaning. In Proceedings of the 9th In-
ternational Conference on Computational Semantics
(IWCS-11), Oxford, UK.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211?244.
Kenneth E. Harper. 1965. Measurement of similarity be-
tween nouns. In Proceedings of the 1965 International
Conference on Computational Linguistics (COLING-
65), New York, NY.
Geoffrey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14(8):1771?1800.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
Kazuaki Kishida. 2005. Property of average precision
and its generalisation: An examination of evaluation
indicator for information retrieval experiments. Tech-
nical Report NII-2005-014E, National Institute of In-
formatics, Tokyo, Japan.
Thomas K Landauer and Susan T Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10), Uppsala, Sweden.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics (ACL-08), Columbus, OH.
Alexander A. Morgan, Zhiyong Lu, Xinglong Wang,
Aaron M Cohen, Juliane Fluck, Patrick Ruch, Anna
Divoli, Katrin Fundel, Robert Leaman, Jo?rg Haken-
berg, Chengjie Sun, Heng hui Liu, Rafael Torres,
Michael Krauthammer, William W Lau, Hongfang
Liu, Chun-Nan Hsu, Martijn Schuemie, K. Bretonnel
Cohen, and Lynette Hirschman. 2008. Overview of
BioCreative II gene normalization. Genome Biology,
9(Suppl 2).
Paul Nulty and Fintan Costello. 2010. UCD-PN: Select-
ing general paraphrases using conditional probability.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (SemEval-2), Uppsala, Sweden.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL-10), Uppsala, Sweden.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st Annual Meeting of the Association
for Computational Linguistics, Columbus, OH.
Joseph Reisinger and Raymond Mooney. 2010. A mix-
ture model with sharing for lexical semantics. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-10),
Cambridge,MA.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent Dirichlet alocation method for selectional prefer-
ences. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-
10), Uppsala, Sweden.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-99), College
Park, MD.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-10), Uppsala,
Sweden.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence, 46(1?
2):159?216.
Karen Spa?rck Jones. 1964. Synonymy and Semantic
Classification. Ph.D. thesis, University of Cambridge.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
1056
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL-10), Uppsala, Swe-
den.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141?
188.
Hanna Wallach, David Mimno, and Andrew McCallum.
2009. Rethinking LDA: Why priors matter. In Pro-
ceedings of NIPS-09, Vancouver, BC.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. In Proceedings
of the 15th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD-09),
Paris, France.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics
(COLING-00), Saarbru?cken, Germany.
1057
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 435?444,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Latent variable models of selectional preference
Diarmuid O? Se?aghdha
University of Cambridge
Computer Laboratory
United Kingdom
do242@cl.cam.ac.uk
Abstract
This paper describes the application of
so-called topic models to selectional pref-
erence induction. Three models related
to Latent Dirichlet Allocation, a proven
method for modelling document-word co-
occurrences, are presented and evaluated
on datasets of human plausibility judge-
ments. Compared to previously proposed
techniques, these models perform very
competitively, especially for infrequent
predicate-argument combinations where
they exceed the quality of Web-scale pre-
dictions while using relatively little data.
1 Introduction
Language researchers have long been aware that
many words place semantic restrictions on the
words with which they can co-occur in a syntactic
relationship. Violations of these restrictions make
the sense of a sentence odd or implausible:
(1) Colourless green ideas sleep furiously.
(2) The deer shot the hunter.
Recognising whether or not a selectional restriction
is satisfied can be an important trigger for metaphor-
ical interpretations (Wilks, 1978) and also plays a
role in the time course of human sentence process-
ing (Rayner et al, 2004). A more relaxed notion of
selectional preference captures the idea that certain
classes of entities are more likely than others to
fill a given argument slot of a predicate. In Natu-
ral Language Processing, knowledge about proba-
ble, less probable and wholly infelicitous predicate-
argument pairs is of value for numerous applica-
tions, for example semantic role labelling (Gildea
and Jurafsky, 2002; Zapirain et al, 2009). The
notion of selectional preference is not restricted
to surface-level predicates such as verbs and mod-
ifiers, but also extends to semantic frames (Erk,
2007) and inference rules (Pantel et al, 2007).
The fundamental problem that selectional prefer-
ence models must address is data sparsity: in many
cases insufficient corpus data is available to reliably
measure the plausibility of a predicate-argument
pair by counting its observed frequency. A rarely
seen pair may be fundamentally implausible (a
carrot laughed) or plausible but rarely expressed
(a manservant laughed).1 In general, it is benefi-
cial to smooth plausibility estimates by integrating
knowledge about the frequency of other, similar
predicate-argument pairs. The task thus share some
of the nature of language modelling; however, it is
a task less amenable to approaches that require very
large training corpora and one where the semantic
quality of a model is of greater importance.
This paper takes up tools (?topic models?)
that have been proven successful in modelling
document-word co-occurrences and adapts them
to the task of selectional preference learning. Ad-
vantages of these models include a well-defined
generative model that handles sparse data well,
the ability to jointly induce semantic classes and
predicate-specific distributions over those classes,
and the enhanced statistical strength achieved by
sharing knowledge across predicates. Section 2
surveys prior work on selectional preference mod-
elling and on semantic applications of topic models.
Section 3 describes the models used in our exper-
iments. Section 4 provides details of the experi-
mental design. Section 5 presents results for our
models on the task of predicting human plausibility
judgements for predicate-argument combinations;
we show that performance is generally competi-
1At time of writing, Google estimates 855 hits
for ?a|the carrot|carrots laugh|laughs|laughed? and 0
hits for ?a|the manservant|manservants|menservants
laugh|laughs|laughed?; many of the carrot hits are false
positives but a significant number are true subject-verb
observations.
435
tive with or superior to a number of other models,
including models using Web-scale resources, espe-
cially for low-frequency examples. In Section 6 we
wrap up by summarising the paper?s conclusions
and sketching directions for future research.
2 Related work
2.1 Selectional preference learning
The representation (and latterly, learning) of selec-
tional preferences for verbs and other predicates
has long been considered a fundamental problem
in computational semantics (Resnik, 1993). Many
approaches to the problem use lexical taxonomies
such as WordNet to identify the semantic classes
that typically fill a particular argument slot for a
predicate (Resnik, 1993; Clark and Weir, 2002;
Schulte im Walde et al, 2008). In this paper, how-
ever, we focus on methods that do not assume
the availability of a comprehensive taxonomy but
rather induce semantic classes automatically from
a corpus of text. Such methods are more generally
applicable, for example in domains or languages
where handbuilt semantic lexicons have insufficient
coverage or are non-existent.
Rooth et al (1999) introduced a model of se-
lectional preference induction that casts the prob-
lem in a probabilistic latent-variable framework.
In Rooth et al?s model each observed predicate-
argument pair is probabilistically generated from a
latent variable, which is itself generated from an un-
derlying distribution on variables. The use of latent
variables, which correspond to coherent clusters
of predicate-argument interactions, allow proba-
bilities to be assigned to predicate-argument pairs
which have not previously been observed by the
model. The discovery of these predicate-argument
clusters and the estimation of distributions on latent
and observed variables are performed simultane-
ously via an Expectation Maximisation procedure.
The work presented in this paper is inspired by
Rooth et al?s latent variable approach, most di-
rectly in the model described in Section 3.3. Erk
(2007) and Pado? et al (2007) describe a corpus-
driven smoothing model which is not probabilistic
in nature but relies on similarity estimates from
a ?semantic space? model that identifies semantic
similarity with closeness in a vector space of co-
occurrences. Bergsma et al (2008) suggest learn-
ing selectional preferences in a discriminative way,
by training a collection of SVM classifiers to recog-
nise likely and unlikely arguments for predicates
of interest.
Keller and Lapata (2003) suggest a simple al-
ternative to smoothing-based approaches. They
demonstrate that noisy counts from a Web search
engine can yield estimates of plausibility for
predicate-argument pairs that are superior to mod-
els learned from a smaller parsed corpus. The as-
sumption inherent in this approach is that given suf-
ficient text, all plausible predicate-argument pairs
will be observed with frequency roughly correlated
with their degree of plausibility. While the model is
undeniably straightforward and powerful, it has a
number of drawbacks: it presupposes an extremely
large corpus, the like of which will only be avail-
able for a small number of domains and languages,
and it is only suitable for relations that are iden-
tifiable by searching raw text for specific lexical
patterns.
2.2 Topic modelling
The task of inducing coherent semantic clusters is
common to many research areas. In the field of
document modelling, a class of methods known
as ?topic models? have become a de facto stan-
dard for identifying semantic structure in docu-
ments. These include the Latent Dirichlet Al-
location (LDA) model of Blei et al (2003) and
the Hierarchical Dirichlet Process model of Teh
et al (2006). Formally seen, these are hierarchi-
cal Bayesian models which induce a set of latent
variables or topics that are shared across docu-
ments. The combination of a well-defined prob-
abilistic model and Gibbs sampling procedure for
estimation guarantee (eventual) convergence and
the avoidance of degenerate solutions. As a result
of intensive research in recent years, the behaviour
of topic models is well-understood and computa-
tionally efficient implementations have been de-
veloped. The tools provided by this research are
used in this paper as the building blocks of our
selectional preference models.
Hierarchical Bayesian modelling has recently
gained notable popularity in many core areas of
natural language processing, from morphological
segmentation (Goldwater et al, 2009) to opinion
modelling (Lin et al, 2006). Yet so far there have
been relatively few applications to traditional lex-
ical semantic tasks. Boyd-Graber et al (2007) in-
tegrate a model of random walks on the WordNet
graph into an LDA topic model to build an unsuper-
vised word sense disambiguation system. Brody
436
and Lapata (2009) adapt the basic LDA model for
application to unsupervised word sense induction;
in this context, the topics learned by the model are
assumed to correspond to distinct senses of a partic-
ular lemma. Zhang et al (2009) are also concerned
with inducing multiple senses for a particular term;
here the goal is to identify distinct entity types in
the output of a pattern-based entity set discovery
system. Reisinger and Pas?ca (2009) use LDA-like
models to map automatically acquired attribute
sets onto the WordNet hierarchy. Griffiths et al
(2007) demonstrate that topic models learned from
document-word co-occurrences are good predictors
of semantic association judgements by humans.
Simultaneously to this work, Ritter et al (2010)
have also investigated the use of topic models
for selectional preference learning. Their goal is
slightly different to ours in that they wish to model
the probability of a binary predicate taking two
specified arguments, i.e., P (n1, n2|v), whereas we
model the joint and conditional probabilities of a
predicate taking a single specified argument. The
model architecture they propose, LinkLDA, falls
somewhere between our LDA and DUAL-LDA
models. Hence LinkLDA could be adapted to esti-
mate P (n, v|r) as DUAL-LDA does, but a prelimi-
nary investigation indicates that it does not perform
well in this context. The most likely explanation
is that LinkLDA generates its two arguments in-
dependently, which may be suitable for distinct
argument positions of a given predicate but is un-
suitable when one of those ?arguments? is in fact
the predicate.
The models developed in this paper, though in-
tended for semantic modelling, also bear some sim-
ilarity to the internals of generative syntax models
such as the ?infinite tree? (Finkel et al, 2007). In
some ways, our models are less ambitious than
comparable syntactic models as they focus on spe-
cific fragments of grammatical structure rather than
learning a more general representation of sentence
syntax. It would be interesting to evaluate whether
this restricted focus improves the quality of the
learned model or whether general syntax models
can also capture fine-grained knowledge about com-
binatorial semantics.
3 Three selectional preference models
3.1 Notation
In the model descriptions below we assume a predi-
cate vocabulary of V types, an argument vocab-
ulary of N types and a relation vocabulary of
R types. Each predicate type is associated with
a singe relation; for example the predicate type
eat:V:dobj (the direct object of the verb eat) is
treated as distinct from eat:V:subj (the subject of
the verb eat). The training corpus consists of W
observations of argument-predicate pairs. Each
model has at least one vocabulary of Z arbitrar-
ily labelled latent variables. fzn is the number of
observations where the latent variable z has been
associated with the argument type n, fzv is the
number of observations where z has been associ-
ated with the predicate type v and fzr is the number
of observations where z has been associated with
the relation r. Finally, fz? is the total number of
observations associated with z and f?v is the total
number of observations containing the predicate v.
3.2 Latent Dirichlet Allocation
As noted above, LDA was originally introduced to
model sets of documents in terms of topics, or clus-
ters of terms, that they share in varying proportions.
For example, a research paper on bioinformatics
may use some vocabulary that is shared with gen-
eral computer science papers and some vocabulary
that is shared with biomedical papers. The analogi-
cal move from modelling document-term cooccur-
rences to modelling predicate-argument cooccur-
rences is intuitive: we assume that each predicate is
associated with a distribution over semantic classes
(?topics?) and that these classes are shared across
predicates. The high-level ?generative story? for
the LDA selectional preference model is as follows:
(1) For each predicate v, draw a multinomial dis-
tribution ?v over argument classes from a
Dirichlet distribution with parameters ?.
(2) For each argument class z, draw a multinomial
distribution ?z over argument types from a
Dirichlet with parameters ?.
(3) To generate an argument for v, draw an ar-
gument class z from ?v and then draw an
argument type n from ?z
The resulting model can be written as:
P (n|v, r) =
?
z
P (n|z)P (z|v, r) (1)
?
?
z
fzn + ?
fz? +N?
fzv + ?z
f?v +
?
z? ?z?
(2)
437
Due to multinomial-Dirichlet conjugacy, the dis-
tributions ?v and ?z can be integrated out and do
not appear explicitly in the above formula. The
first term in (2) can be seen as a smoothed esti-
mate of the probability that class z produces the
argument n; the second is a smoothed estimate of
the probability that predicate v takes an argument
belonging to class z. One important point is that
the smoothing effects of the Dirichlet priors on ?v
and ?z are greatest for predicates and arguments
that are rarely seen, reflecting an intuitive lack of
certainty. We assume an asymmetric Dirichlet prior
on ?v (the ? parameters can differ for each class)
and a symmetric prior on ?z (all ? parameters are
equal); this follows the recommendations of Wal-
lach et al (2009) for LDA. This model estimates
predicate-argument probabilities conditional on a
given predicate v; it cannot by itself provide joint
probabilities P (n, v|r), which are needed for our
plausibility evaluation.
Given a dataset of predicate-argument combina-
tions and values for the hyperparameters ? and ?,
the probability model is determined by the class
assignment counts fzn and fzv. Following Grif-
fiths and Steyvers (2004), we estimate the model
by Gibbs sampling. This involves resampling the
topic assignment for each observation in turn using
probabilities estimated from all other observations.
One efficiency bottleneck in the basic sampler de-
scribed by Griffiths and Steyvers is that the entire
set of topics must be iterated over for each observa-
tion. Yao et al (2009) propose a reformulation that
removes this bottleneck by separating the probabil-
ity mass p(z|n, v) into a number of buckets, some
of which only require iterating over the topics cur-
rently assigned to instances of type n, typically far
fewer than the total number of topics. It is possible
to apply similar reformulations to the models pre-
sented in Sections 3.3 and 3.4 below; depending on
the model and parameterisation this can reduce the
running time dramatically.
Unlike some topic models such as HDP (Teh et
al., 2006), LDA is parametric: the number of top-
ics Z must be set by the user in advance. However,
Wallach et al (2009) demonstrate that LDA is rela-
tively insensitive to larger-than-necessary choices
ofZ when the Dirichlet parameters ? are optimised
as part of model estimation. In our implementation
we use the optimisation routines provided as part
of the Mallet library, which use an iterative proce-
dure to compute a maximum likelihood estimate of
these hyperparameters.2
3.3 A Rooth et al-inspired model
In Rooth et al?s (1999) selectional preference
model, a latent variable is responsible for generat-
ing both the predicate and argument types of an ob-
servation. The basic LDAmodel can be extended to
capture this kind of predicate-argument interaction;
the generative story for the resulting ROOTH-LDA
model is as follows:
(1) For each relation r, draw a multinomial dis-
tribution ?r over interaction classes from a
Dirichlet distribution with parameters ?.
(2) For each class z, draw a multinomial ?z over
argument types from a Dirichlet distribution
with parameters ? and a multinomial ?z over
predicate types from a Dirichlet distribution
with parameters ?.
(3) To generate an observation for r, draw a class
z from ?r, then draw an argument type n
from ?z and a predicate type v from ?z .
The resulting model can be written as:
P (n, v|r) =
?
z
P (n|z)P (v|z)P (z|r) (3)
?
?
z
fzn + ?
fz? +N?
fzv + ?
fz? + V ?
fzr + ?z
f?r +
?
z? ?z?
(4)
As suggested by the similarity between (4) and (2),
the ROOTH-LDA model can be estimated by an
LDA-like Gibbs sampling procedure.
Unlike LDA, ROOTH-LDA does model the joint
probability P (n, v|r) of a predicate and argument
co-occurring. Further differences are that infor-
mation about predicate-argument co-occurrence is
only shared within a given interaction class rather
than across the whole dataset and that the distribu-
tion ?z is not specific to the predicate v but rather
to the relation r. This could potentially lead to a
loss of model quality, but in practice the ability to
induce ?tighter? clusters seems to counteract any
deterioration this causes.
3.4 A ?dual-topic? model
In our third model, we attempt to combine the ad-
vantages of LDA and ROOTH-LDA by cluster-
ing arguments and predicates according to separate
2http://mallet.cs.umass.edu/
438
class vocabularies. Each observation is generated
by two latent variables rather than one, which po-
tentially allows the model to learn more flexible
interactions between arguments and predicates.:
(1) For each relation r, draw a multinomial distri-
bution ?r over predicate classes from a Dirich-
let with parameters ?.
(2) For each predicate class c, draw a multinomial
?c over predicate types and a multinomial ?c
over argument classes from Dirichlets with
parameters ? and ? respectively.
(3) For each argument class z, draw a multinomial
distribution ?z over argument types from a
Dirichlet with parameters ?.
(4) To generate an observation for r, draw a predi-
cate class c from ?r, a predicate type from?c,
an argument class z from ?c and an argument
type from ?z .
The resulting model can be written as:
P (n, v|r) =
?
c
?
z
P (n|z)P (z|c)P (v|c)P (c|r)
(5)
?
?
c
?
z
fzn + ?
fz? +N?
fzc + ?z
f?c +
?
z? ?z?
?
fcv + ?
fc? + V ?
fcr + ?c
f?r +
?
c? ?c?
(6)
To estimate this model, we first resample the class
assignments for all arguments in the data and
then resample class assignments for all predicates.
Other approaches are possible ? resampling argu-
ment and then predicate class assignments for each
observation in turn, or sampling argument and pred-
icate assignments together by blocked sampling ?
though from our experiments it does not seem that
the choice of scheme makes a significant differ-
ence.
4 Experimental setup
In the document modelling literature, probabilistic
topic models are often evaluated on the likelihood
they assign to unseen documents; however, it has
been shown that higher log likelihood scores do
not necessarily correlate with more semantically
coherent induced topics (Chang et al, 2009). One
popular method for evaluating selectional prefer-
ence models is by testing the correlation between
their predictions and human judgements of plausi-
bility on a dataset of predicate-argument pairs. This
can be viewed as a more semantically relevant mea-
surement of model quality than likelihood-based
methods, and also permits comparison with non-
probabilistic models. In Section 5, we use two
plausibility datasets to evaluate our models and
compare to other previously published results.
We trained our models on the 90-million word
written component of the British National Corpus
(Burnard, 1995), parsed with the RASP toolkit
(Briscoe et al, 2006). Predicates occurring with
just one argument type were removed, as were all
tokens containing non-alphabetic characters; no
other filtering was done. The resulting datasets con-
sisted of 3,587,172 verb-object observations with
7,954 predicate types and 80,107 argument types,
3,732,470 noun-noun observations with 68,303
predicate types and 105,425 argument types, and
3,843,346 adjective-noun observations with 29,975
predicate types and 62,595 argument types.
During development we used the verb-noun plau-
sibility dataset from Pado? et al (2007) to direct
the design of the system. Unless stated other-
wise, all results are based on runs of 1,000 iter-
ations with 100 classes, with a 200-iteration burnin
period after which hyperparameters were reesti-
mated every 50 iterations.3 The probabilities es-
timated by the models (P (n|v, r) for LDA and
P (n, v|r) for ROOTH- and DUAL-LDA) were
sampled every 50 iterations post-burnin and av-
eraged over three runs to smooth out variance.
To compare plausibility scores for different pred-
icates, we require the joint probability P (n, v|r);
as LDA does not provide this, we approximate
PLDA(n, v|r) = PBNC(v|r)PLDA(n|v, r), where
PBNC(v|r) is proportional to the frequency with
which predicate v is observed as an instance of
relation r in the BNC.
For comparison, we reimplemented the methods
of Rooth et al (1999) and Pado? et al (2007). As
mentioned above, Rooth et al use a latent-variable
model similar to (4) but without priors, trained
via EM. Our implementation (henceforth ROOTH-
EM) chooses the number of classes from the range
(20, 25, . . . , 50) through 5-fold cross-validation on
a held-out log-likelihood measure. Settings outside
this range did not give good results. Again, we run
for 1,000 iterations and average predictions over
3These settings were based on the MALLET defaults; we
have not yet investigated whether modifying the simulation
length or burnin period is beneficial.
439
LDA 0 Nouns: agreement, contract, permission, treaty, deal, . . .
1 Nouns information, datum, detail, evidence, material, . . .
2 Nouns skill, knowledge, country, technique, understanding, . . .
ROOTH-LDA 0 Nouns force, team, army, group, troops, . . .
0 Verbs join, arm, lead, beat, send, . . .
1 Nouns door, eye, mouth, window, gate, . . .
1 Verbs open, close, shut, lock, slam, . . .
DUAL-LDA 0N Nouns house, building, site, home, station, . . .
1N Nouns stone, foot, bit, breath, line, . . .
0V Verbs involve, join, lead, represent, concern, . . .
1V Verbs see, break, have, turn, round, . . .
ROOTH-EM 0 Nouns system, method, technique, skill, model, . . .
0 Verbs use, develop, apply, design, introduce, . . .
1 Nouns eye, door, page, face, chapter,. . .
1 Verbs see, open, close, watch, keep,. . .
Table 1: Most probable words for sample semantic classes induced from verb-object observations
three runs. Pado? et al (2007), a refinement of Erk
(2007), is a non-probabilistic method that smooths
predicate-argument counts with counts for other ob-
served arguments of the same predicate, weighted
by the similarity between arguments. Following
their description, we use a 2,000-dimensional space
of syntactic co-occurrence features appropriate to
the relation being predicted, weight features with
the G2 transformation and compute similarity with
the cosine measure.
5 Results
5.1 Induced semantic classes
Table 1 shows sample semantic classes induced by
models trained on the corpus of BNC verb-object
co-occurrences. LDA clusters nouns only, while
ROOTH-LDA and ROOTH-EM learn classes that
generate both nouns and verbs and DUAL-LDA
clusters nouns and verbs separately. The LDA clus-
ters are generally sensible: class 0 is exemplified
by agreement and contract and class 1 by informa-
tion and datum. There are some unintuitive blips,
for example country appears between knowledge
and understanding in class 2. The ROOTH-LDA
classes also feel right: class 0 deals with nouns
such as force, team and army which one might join,
arm or lead and class 1 corresponds to ?things that
can be opened or closed? such as a door, an eye or a
mouth (though the model also makes the question-
able prediction that all these items can plausibly
be locked or slammed). The DUAL-LDA classes
are notably less coherent, especially when it comes
to clustering verbs: DUAL-LDA?s class 0V, like
ROOTH-LDA?s class 0, has verbs that take groups
as objects but its class 1V mixes sensible confla-
tions (turn, round) with very common verbs such as
see and have and the unrelated break. The general
impression given by inspection of the DUAL-LDA
model is that it has problems with mixing and does
not manage to learn a good model; we have tried
a number of solutions (e.g., blocked sampling of
argument and predicate classes), without overcom-
ing this brittleness. Unsurprisingly, ROOTH-EM?s
classes have a similar feel to ROOTH-LDA; our
general impression is that some of ROOTH-EM?s
classes look even more coherent than the LDA-
based models, presumably because it does not use
priors to smooth its per-class distributions.
5.2 Comparison with Keller and Lapata
(2003)
Keller and Lapata (2003) collected a dataset of
human plausibility judgements for three classes
of grammatical relation: verb-object, noun-noun
modification and adjective-noun modification. The
items in this dataset were not chosen to balance
plausibility and implausibility (as in prior psy-
cholinguistic experiments) but according to their
corpus frequency, leading to a more realistic task.
30 predicates were selected for each relation;
each predicate was matched with three arguments
from different co-occurrence bands in the BNC,
e.g., naughty-girl (high frequency), naughty-dog
(medium) and naughty-lunch (low). Each predicate
was also matched with three random arguments
440
Verb-object Noun-noun Adjective-noun
Seen Unseen Seen Unseen Seen Unseen
r ? r ? r ? r ? r ? r ?
AltaVista (KL) .641 ? .551 ? .700 ? .578 ? .650 ? .480 ?
Google (KL) .624 ? .520 ? .692 ? .595 ? .641 ? .473 ?
BNC (RASP) .620 .614 .196 .222 .544 .604 .114 .125 .543 .622 .135 .102
ROOTH-EM .455 .487 .479 .520 .503 .491 .586 .625 .514 .463 .395 .355
Pado? et al .484 .490 .398 .430 .431 .503 .558 .533 .479 .570 .120 .138
LDA .504 .541 .558 .603 .615 .641 .636 .666 .594 .558 .468 .459
ROOTH-LDA .520 .548 .564 .605 .607 .622 .691 .722 .575 .599 .501 .469
DUAL-LDA .453 .494 .446 .516 .496 .494 .553 .573 .460 .400 .334 .278
Table 2: Results (Pearson r and Spearman ? correlations) on Keller and Lapata?s (2003) plausibility data
with which it does not co-occur in the BNC (e.g.,
naughty-regime, naughty-rival, naughty-protocol).
In this way two datasets (Seen and Unseen) of 90
items each were assembled for each predicate.
Table 2 presents results for a variety of predictive
models ? the Web frequencies reported by Keller
and Lapata (2003) for two search engines, frequen-
cies from the RASP-parsed BNC,4 the reimple-
mented methods of Rooth et al (1999) and Pado? et
al. (2007), and the LDA, ROOTH-LDA and DUAL-
LDA topic models. Following Keller and Lapata,
we report Pearson correlation coefficients between
log-transformed predicted frequencies and the gold-
standard plausibility scores (which are already log-
transformed). We also report Spearman rank cor-
relations except where we do not have the origi-
nal predictions (the Web count models), for com-
pleteness and because the predictions of preference
models are may not be log-normally distributed as
corpus counts are. Zero values (found only in the
BNC frequency predictions) were smoothed by 0.1
to facilitate the log transformation; it seems natural
to take a zero prediction as a non-specific predic-
tion of very low plausibility rather than a ?missing
value? as is done in other work (e.g., Pado? et al,
2007).
Despite their structural differences, LDA and
ROOTH-LDA perform similarly - indeed, their
predictions are highly correlated. ROOTH-LDA
scores best overall, outperforming Pado? et al?s
(2007) method and ROOTH-EM on every dataset
and evaluation measure, and outperforming Keller
and Lapata?s (2003) Web predictions on every Un-
4The correlations presented here for BNC counts are no-
tably better than those reported by Keller and Lapata (2003),
presumably reflecting our use of full parsing rather than shal-
low parsing.
seen dataset. LDA also performs consistently well,
surpassing ROOTH-EM and Pado? et al on all but
one occasion. For frequent predicate-argument
pairs (Seen datasets), Web counts are clearly better;
however, the BNC counts are unambiguously supe-
rior to LDA and ROOTH-LDA (whose predictions
are based entirely on the generative model even for
observed items) for the Seen verb-object data only.
As might be suspected from the mixing problems
observed with DUAL-LDA, this model does not
perform as well as LDA and ROOTH-LDA, though
it does hold its own against the other selectional
preference methods.
To identify significant differences between mod-
els, we use the statistical test for correlated corre-
lation coefficients proposed by Meng et al (1992),
which is appropriate for correlations that share
the same gold standard.5 For the seen data there
are few significant differences: ROOTH-LDA and
LDA are significantly better (p < 0.01) than Pado?
et al?s model for Pearson?s r on seen noun-noun
data, and ROOTH-LDA is also significantly better
(p < 0.01) using Spearman?s ?. For the unseen
datasets, the BNC frequency predictions are unsur-
prisingly significantly worse at the p < 0.01 level
than all smoothing models. LDA and ROOTH-
LDA are significantly better (p < 0.01) than Pado?
et al on every unseen dataset; ROOTH-EM is sig-
nificantly better (p < 0.01) than Pado? et al on
Unseen adjectives for both correlations. Meng et
al.?s test does not find significant differences be-
tween ROOTH-EM and the LDA models despite
the latter?s clear advantages (a number of condi-
tions do come close). This is because their pre-
dictions are highly correlated, which is perhaps
5We cannot compare our data to Keller and Lapata?s Web
counts as we do not possess their per-item scores.
441
50 100 150 2000
0.10.2
0.30.4
0.50.6
0.70.8
0.91
No. of classes
?
(a) Verb-object
50 100 150 2000
0.10.2
0.30.4
0.50.6
0.70.8
0.91
No. of classes
?
(b) Noun-noun
50 100 150 2000
0.10.2
0.30.4
0.50.6
0.70.8
0.91
No. of classes
?
(c) Adjective-noun
Figure 1: Effect of number of argument classes on Spearman rank correlation with LDA: the solid and
dotted lines show the Seen and Unseen datasets respectively; bars show locations of individual samples
unsurprising given that they are structurally similar
models trained on the same data. We hypothesise
that the main reason for the superior numerical per-
formance of the LDA models over EM is the prin-
cipled smoothing provided by the use of Dirichlet
priors, which has a small but discriminative effect
on model predictions. Collating the significance
scores, we find that ROOTH-LDA achieves the
most positive outcomes, followed by LDA and then
by ROOTH-EM. DUAL-LDA is found significantly
better than Pado? et al?s model on unseen adjective-
noun combinations, and significantly worse than
the same model on seen adjective-noun data.
Latent variable models that use EM for infer-
ence can be very sensitive to the number of latent
variables chosen. For example, the performance
of ROOTH-EM worsens quickly if the number of
clusters is overestimated; for the Keller and Lap-
ata datasets, settings above 50 classes lead to clear
overfitting and a precipitous drop in Pearson cor-
relation scores. On the other hand, Wallach et al
(2009) demonstrate that LDA is relatively insensi-
tive to the choice of topic vocabulary size Z when
the ? and ? hyperparameters are optimised appro-
priately during estimation. Figure 1 plots the effect
of Z on Spearman correlation for the LDA model.
In general, Wallach et al?s finding for document
modelling transfers to selectional preference mod-
els; within the range Z = 50?200 performance
remains at a roughly similar level. In fact, we do
not find that performance becomes significantly
less robust when hyperparameter reestimation is
deactiviated; correlation scores simply drop by a
small amount (1?2 points), irrespective of the Z
chosen. ROOTH-LDA (not graphed) seems slightly
more sensitive to Z; this may be because the ? pa-
rameters in this model operate on the relation level
rather than the document level and thus fewer ?ob-
servations? of class distributions are available when
reestimating them.
5.3 Comparison with Bergsma et al (2008)
As mentioned in Section 2.1, Bergsma et al (2008)
propose a discriminative approach to preference
learning. As part of their evaluation, they compare
their approach to a number of others, including
that of Erk (2007), on a plausibility dataset col-
lected by Holmes et al (1989). This dataset con-
sists of 16 verbs, each paired with one plausible
object (e.g., write-letter) and one implausible ob-
ject (write-market). Bergsma et al?s model, trained
on the 3GB AQUAINT corpus, is the only model
reported to achieve perfect accuracy on distinguish-
ing plausible from implausible arguments. It would
be interesting to do a full comparison that controls
for size and type of corpus data; in the meantime,
we can report that the LDA and ROOTH-LDA
models trained on verb-object observations in the
BNC (about 4 times smaller than AQUAINT) also
achieve a perfect score on the Holmes et al data.6
6 Conclusions and future work
This paper has demonstrated how Bayesian tech-
niques originally developed for modelling the top-
ical structure of documents can be adapted to
learn probabilistic models of selectional preference.
These models are especially effective for estimat-
ing plausibility of low-frequency items, thus distin-
guishing rarity from clear implausibility.
The models presented here derive their predic-
tions by modelling predicate-argument plausibility
through the intermediary of latent variables. As
observed in Section 5.2 this may be a suboptimal
6Bergsma et al report that all plausible pairs were seen in
their corpus; three were unseen in ours, as well as 12 of the
implausible pairs.
442
strategy for frequent combinations, where corpus
counts are probably reliable and plausibility judge-
ments may be affected by lexical collocation ef-
fects. One principled method for folding corpus
counts into LDA-like models would be to use hi-
erarchical priors, as in the n-gram topic model of
Wallach (2006). Another potential direction for
system improvement would be an integration of
our generative model with Bergsma et al?s (2008)
discriminative model ? this could be done in a num-
ber of ways, including using the induced classes
of a topic model as features for a discriminative
classifier or using the discriminative classifier to
produce additional high-quality training data from
noisy unparsed text.
Comparison to plausibility judgements gives an
intrinsic measure of model quality. As mentioned
in the Introduction, selectional preferences have
many uses in NLP applications, and it will be inter-
esting to evaluate the utility of Bayesian preference
models in contexts such as semantic role labelling
or human sentence processing modelling. The prob-
abilistic nature of topic models, coupled with an
appropriate probabilistic task model, may facilitate
the integration of class induction and task learning
in a tight and principled way. We also anticipate
that latent variable models will prove effective for
learning selectional preferences of semantic predi-
cates (e.g., FrameNet roles) where direct estimation
from a large corpus is not a viable option.
Acknowledgements
This work was supported by EPSRC grant
EP/G051070/1. I am grateful to Frank Keller and
Mirella Lapata for sharing their plausibility data,
and to Andreas Vlachos and the anonymous ACL
and CoNLL reviewers for their helpful comments.
References
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preferences
from unlabeled text. In Proceedings of EMNLP-08,
Honolulu, HI.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Proceedings of EMNLP-CoNLL-07, Prague,
Czech Republic.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the ACL-06 Interactive Presentation Ses-
sions, Sydney, Australia.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of EACL-09,
Athens, Greece.
Lou Burnard, 1995. Users? Guide for the British Na-
tional Corpus. British National Corpus Consortium,
Oxford University Computing Service, Oxford, UK.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Proceedings of NIPS-09, Vancouver, BC.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of ACL-
07, Prague, Czech Republic.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
ACL-07, Prague, Czech Republic.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context. Cog-
nition, 112(1):21?54.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl. 1):5228?5235.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114(2):211?244.
Virginia M. Holmes, Laurie Stowe, and Linda Cupples.
1989. Lexical expectations in parsing complement-
verb sentences. Journal of Memory and Language,
28(6):668?689.
Frank Keller and Mirella Lapata. 2003. Using the Web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459?484.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and
sentence levels. In Proceedings of CoNLL-06, New
York, NY.
Xiao-LiMeng, Robert Rosenthal, and Donald B. Rubin.
1992. Comparing correlated correlation coefficients.
Psychological Bulletin, 111(1):172?175.
443
Sebastian Pado?, Ulrike Pado?, and Katrin Erk. 2007.
Flexible, corpus-based modelling of human plau-
sibility judgements. In Proceedings of EMNLP-
CoNLL-07, Prague, Czech Republic.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of NAACL-HLT-07, Rochester, NY.
Keith Rayner, Tessa Warren, Barbara J. Juhasz, and Si-
mon P. Liversedge. 2004. The effect of plausibility
on eye movements in reading. Journal of Experi-
mental Psychology: Learning Memory and Cogni-
tion, 30(6):1290?1301.
Joseph Reisinger and Marius Pas?ca. 2009. Latent vari-
able models of concept-attribute attachment. In Pro-
ceedings of ACL-IJCNLP-09, Singapore.
Philip S. Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation method for selectional pref-
erences. In Proceedings of ACL-10, Uppsala, Swe-
den.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings of ACL-99, College Park, MD.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. 2008. Combining
EM training and the MDL principle for an automatic
verb classification incorporating selectional prefer-
ences. In Proceedings of ACL-08:HLT, Columbus,
OH.
Yee W. Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
HannaWallach, DavidMimno, and AndrewMcCallum.
2009. Rethinking LDA: Why priors matter. In Pro-
ceedings of NIPS-09, Vancouver, BC.
Hanna Wallach. 2006. Topic modeling: Beyond bag-
of-words. In Proceedings of ICML-06, Pittsburgh,
PA.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11:197?225.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. In Proceedings
of KDD-09, Paris, France.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez.
2009. Generalizing over lexical features: Selec-
tional preferences for semantic role classification. In
Proceedings of ACL-IJCNLP-09, Singapore.
Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-Rong
Wen. 2009. Employing topic models for pattern-
based semantic class discovery. In Proceedings of
ACL-IJCNLP-09, Singapore.
444
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 420?429,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning Syntactic Verb Frames Using Graphical Models
Thomas Lippincott
University of Cambridge
Computer Laboratory
United Kingdom
tl318@cam.ac.uk
Diarmuid O? Se?aghdha
University of Cambridge
Computer Laboratory
United Kingdom
do242@cam.ac.uk
Anna Korhonen
University of Cambridge
Computer Laboratory
United Kingdom
alk23@cam.ac.uk
Abstract
We present a novel approach for building
verb subcategorization lexicons using a simple
graphical model. In contrast to previous meth-
ods, we show how the model can be trained
without parsed input or a predefined subcate-
gorization frame inventory. Our method out-
performs the state-of-the-art on a verb clus-
tering task, and is easily trained on arbitrary
domains. This quantitative evaluation is com-
plemented by a qualitative discussion of verbs
and their frames. We discuss the advantages of
graphical models for this task, in particular the
ease of integrating semantic information about
verbs and arguments in a principled fashion.
We conclude with future work to augment the
approach.
1 Introduction
Subcategorization frames (SCFs) give a compact de-
scription of a verb?s syntactic preferences. These
two sentences have the same sequence of lexical
syntactic categories (VP-NP-SCOMP), but the first
is a simple transitive (?X understood Y?), while the
second is a ditransitive with a sentential complement
(?X persuaded Y that Z?):
1. Kim (VP understood (NP the evidence
(SCOMP that Sandy was present)))
2. Kim (VP persuaded (NP the judge) (SCOMP
that Sandy was present))
An SCF lexicon would indicate that ?persuade?
is likely to take a direct object and sentential com-
plement (NP-SCOMP), while ?understand? is more
likely to take just a direct object (NP). A compre-
hensive lexicon would also include semantic infor-
mation about selectional preferences (or restrictions)
on argument heads of verbs, diathesis alternations
(i.e. semantically-motivated alternations between
pairs of SCFs) and a mapping from surface frames
to the underlying predicate-argument structure. In-
formation about verb subcategorization is useful for
tasks like information extraction (Cohen and Hunter,
2006; Rupp et al, 2010), verb clustering (Korho-
nen et al, 2006b; Merlo and Stevenson, 2001) and
parsing (Carroll et al, 1998). In general, tasks that
depend on predicate-argument structure can benefit
from a high-quality SCF lexicon (Surdeanu et al,
2003).
Large, manually-constructed SCF lexicons
mostly target general language (Boguraev and
Briscoe, 1987; Grishman et al, 1994). However,
in many domains verbs exhibit different syntactic
behavior (Roland and Jurafsky, 1998; Lippincott
et al, 2010). For example, the verb ?develop?
has specific usages in newswire, biomedicine and
engineering that dramatically change its probability
distribution over SCFs. In a few domains like
biomedicine, the need for focused SCF lexicons
has led to manually-built resources (Bodenreider,
2004). Such resources, however, are costly, prone to
human error, and in domains where new lexical and
syntactic constructs are frequently coined, quickly
become obsolete (Cohen and Hunter, 2006). Data-
driven methods for SCF acquisition can alleviate
420
these problems by building lexicons tailored to
new domains with less manual effort, and higher
coverage and scalability.
Unfortunately, high quality SCF lexicons are dif-
ficult to build automatically. The argument-adjunct
distinction is challenging even for humans, many
SCFs have no reliable cues in data, and some SCFs
(e.g. those involving control such as type raising)
rely on semantic distinctions. As SCFs follow a Zip-
fian distribution (Korhonen et al, 2000), many gen-
uine frames are also low in frequency. State-of-the-
art methods for building data-driven SCF lexicons
typically rely on parsed input (see section 2). How-
ever, the treebanks necessary for training a high-
accuracy parsing model are expensive to build for
new domains. Moreover, while parsing may aid the
detection of some frames, many experiments have
also reported SCF errors due to noise from parsing
(Korhonen et al, 2006a; Preiss et al, 2007).
Finally, many SCF acquisition methods operate
with predefined SCF inventories. This subscribes to
a single (often language or domain-specific) inter-
pretation of subcategorization a priori, and ignores
the ongoing debate on how this interpretation should
be tailored to new domains and applications, such as
the more prominent role of adjuncts in information
extraction (Cohen and Hunter, 2006).
In this paper, we describe and evaluate a novel
probabilistic data-driven method for SCF acquisi-
tion aimed at addressing some of the problems with
current approaches. In our model, a Bayesian net-
work describes how verbs choose their arguments
in terms of a small number of frames, which are
represented as distributions over syntactic relation-
ships. First, we show that by allowing the infer-
ence process to automatically define a probabilistic
SCF inventory, we outperform systems with hand-
crafted rules and inventories, using identical syntac-
tic features. Second, by replacing the syntactic fea-
tures with an approximation based on POS tags, we
achieve state-of-the-art performance without relying
on error-prone unlexicalized or domain-specific lex-
icalized parsers. Third, we highlight a key advantage
of our method compared to previous approaches: the
ease of integrating and performing joint inference of
additional syntactic and semantic information. We
describe how we plan to exploit this in our future
research.
2 Previous work
Many state-of-the-art SCF acquisition systems take
grammatical relations (GRs) as input. GRs ex-
press binary dependencies between lexical items,
and many parsers produce them as output, with
some variation in inventory (Briscoe et al, 2006;
De Marneffe et al, 2006). For example, a subject-
relation like ?ncsubj(HEAD, DEPENDENT)? ex-
presses the fact that the lexical item referred to by
HEAD (such as a present-tense verb) has the lexi-
cal item referred to by DEPENDENT as its subject
(such as a singular noun). GR inventories include
direct and indirect objects, complements, conjunc-
tions, among other relations. The dependency rela-
tionships included in GRs correspond closely to the
head-complement structure of SCFs, which is why
they are the natural choice for SCF acquisition.
There are several SCF lexicons for general lan-
guage, such as ANLT (Boguraev and Briscoe, 1987)
and COMLEX (Grishman et al, 1994), that depend
on manual work. VALEX (Preiss et al, 2007) pro-
vides SCF distributions for 6,397 verbs acquired
from a parsed general language corpus via a system
that relies on hand-crafted rules. There are also re-
sources which provide information about both syn-
tactic and semantic properties of verbs: VerbNet
(Kipper et al, 2008) draws on several hand-built
and semi-automatic sources to link the syntax and
semantics of 5,726 verbs. FrameNet (Baker et al,
1998) provides semantic frames and annotated ex-
ample sentences for 4,186 verbs. PropBank (Palmer
et al, 2005) is a corpus where each verb is annotated
for its arguments and their semantic roles, covering
a total of 4,592 verbs.
There are many language-specific SCF acquisi-
tion systems, e.g. for French (Messiant, 2008),
Italian (Lenci et al, 2008), Turkish (Han et al,
2008) and Chinese (Han et al, 2008). These typ-
ically rely on language-specific knowledge, either
directly through heuristics, or indirectly through
parsing models trained on treebanks. Furthermore,
some require labeled training instances for super-
vised (Uzun et al, 2008) or semi-supervised (Han
et al, 2008) learning algorithms.
Two state-of-the-art data-driven systems for En-
glish verbs are those that produced VALEX, Preiss et
al. (2007), and the BioLexicon (Venturi et al, 2009).
421
The Preiss system extracts a verb instance?s GRs us-
ing the Rasp general-language unlexicalized parser
(Briscoe et al, 2006) as input, and based on hand-
crafted rules, maps verb instances to a predefined
inventory of 168 SCFs. Filtering is then performed
to remove noisy frames, with methods ranging from
a simple single threshold to SCF-specific hypothesis
tests based on external verb classes and SCF inven-
tories. The BioLexicon system extracts each verb in-
stance?s GRs using the lexicalized Enju parser tuned
to the biomedical domain (Miyao, 2005). Each
unique GR-set considered a potential SCF, and an
experimentally-determined threshold is used to fil-
ter low-frequency SCFs.
Note that both methods require extensive man-
ual work: the Preiss system involves the a priori
definition of the SCF inventory, careful construc-
tion of matching rules, and an unlexicalized pars-
ing model. The BioLexicon system induces its SCF
inventory automatically, but requires a lexicalized
parsing model, rendering it more sensitive to domain
variation. Both rely on a filtering stage that depends
on external resources and/or gold standards to select
top-performing thresholds. Our method, by contrast,
does not use a predefined SCF inventory, and can
perform well without parsed input.
Graphical models have been increasingly popu-
lar for a variety of tasks such as distributional se-
mantics (Blei et al, 2003) and unsupervised POS
tagging (Finkel et al, 2007), and sampling methods
allow efficient estimation of full joint distributions
(Neal, 1993). The potential for joint inference of
complementary information, such as syntactic verb
and semantic argument classes, has a clear and in-
terpretable way forward, in contrast to the pipelined
methods described above. This was demonstrated in
Andrew et al (2004), where a Bayesian model was
used to jointly induce syntactic and semantic classes
for verbs, although that study relied on manually
annotated data and a predefined SCF inventory and
MLE. More recently, Abend and Rappoport (2010)
trained ensemble classifiers to perform argument-
adjunct disambiguation of PP complements, a task
closely related to SCF acquisition. Their study em-
ployed unsupervised POS tagging and parsing, and
measures of selectional preference and argument
structure as complementary features for the classi-
fier.
Finally, our task-based evaluation, verb clustering
with Levin (1993)?s alternation classes as the gold
standard, was previously conducted by Joanis and
Stevenson (2003), Korhonen et al (2008) and Sun
and Korhonen (2009).
3 Methodology
In this section we describe the basic components of
our study: feature sets, graphical model, inference,
and evaluation.
3.1 Input and feature sets
We tested several feature sets either based on, or
approximating, the concept of grammatical relation
described in section 2. Our method is agnostic re-
garding the exact definition of GR, and for example
could use the Stanford inventory (De Marneffe et al,
2006) or even an entirely different lexico-syntactic
formalism like CCG supertags (Curran et al, 2007).
In this paper, we distinguish ?true GRs? (tGRs), pro-
duced by a parser, and ?pseudo GRs? (pGRs), a
POS-based approximation, and employ subscripts to
further specify the variations described below. Our
input has been parsed into Rasp-style tGRs (Briscoe
et al, 2006), which facilitates comparison with pre-
vious work based on the same data set.
We?ll use a simple example sentence to illustrate
how our feature sets are extracted from CONLL-
formatted data (Nivre et al, 2007). The CONLL
format is a common language for comparing output
from dependency parsers: each lexical item has an
index, lemma, POS tag, tGR in which it is the de-
pendent, and index to the corresponding head. Table
1 shows the relevant fields for the sentence ?We run
training programmes in Romania and other coun-
tries?.
We define the feature set for a verb occurrence as
the counts of each GR the verb participates in. Table
2 shows the three variations we tested: the simple
tGR type, with parameterization for the POS tags
of head and dependent, and with closed-class POS
tags (determiners, pronouns and prepositions) lexi-
calized. In addition, we tested the effect of limiting
the features to subject, object and complement tGRs,
indicated by adding the subscript ?lim?, for a total of
six tGR-based feature sets.
While ideally tGRs would give full informa-
422
Index Lemma POS Head tGR
1 we PPIS2 2 ncsubj
2 run VV0 0
3 training NN1 4 ncmod
4 programme NN2 2 dobj
5 in II 4 ncmod
6 romania NP1 7 conj
7 and CC 5 dobj
8 other JB 9 ncmod
9 country NN2 7 conj
Table 1: Simplified CONLL format for example sen-
tence ?We run training programmes in Romania and
other countries?. Head=0 indicates the token is the
root.
Name Features
tGR ncsubj dobj
tGRparam ncsubj(VV0,PPIS2) dobj(VV0,NN2)
tGRparam,lex ncsubj(VV0,PPIS2-we) dobj(VV0,NN2)
Table 2: True-GR features for example sentence:
note there are also tGR?,lim versions of each that
only consider subjects, objects and complements
and are not shown.
tion about the verb?s syntactic relationship to other
words, in practice parsers make (possibly prema-
ture) decisions, such as deciding that ?in? modifies
?programme?, and not ?run? in our example sen-
tence. An unlexicalized parser cannot distinguish
these based just on POS tags, while a lexicalized
parser requires a large treebank. We therefore define
pseudo-GRs (pGRs), which consider each (distance,
POS) pair within a given window of the verb to be
a potential tGR. Table 3 shows the pGR features for
the test sentence using a window of three. As with
tGRs, the closed-class tags can be lexicalized, but
there are no corresponding feature sets for param
(since they are already built from POS tags) or lim
(since there is no similar rule-based approach).
Name Features
pGR -1(PPIS2) 1(NN1) 2(NN2) 3(II)
pGRlex -1(PPIS2-we) 1(NN1) 2(NN2) 3(II-in)
Table 3: Pseudo-GR features for example sentence
with window=3
Whichever feature set is used, an instance is sim-
ply the count of each GR?s occurrences. We extract
instances for the 385 verbs in the union of our two
gold standards from the VALEX lexicon?s data set,
which was used in previous studies (Sun and Korho-
nen, 2009; Preiss et al, 2007) and facilitates com-
parison with that resource. This data set is drawn
from five general-language corpora parsed by Rasp,
and provides, on average, 7,000 instances per verb.
3.2 SCF extraction
Our graphical modeling approach uses the Bayesian
network shown in Figure 1. Its generative story
is as follows: when a verb is instantiated, an SCF
is chosen according to a verb-specific multinomial.
Then, the number and type of syntactic arguments
(GRs) are chosen from two SCF-specific multino-
mials. These three multinomials are modeled with
uniform Dirichlet priors and corresponding hyper-
parameters ?, ? and ?. The model is trained via
collapsed Gibbs sampling, where the probability of
assigning a particular SCF s to an instance of verb v
with GRs (gr1 . . . grn) is the product
P (s|V erb = v,GRs = gr1 . . . grn) =
P (SCF = s|V erb = v)?
P (N = n|SCF = s)?
?
i=1:n
P (GR = gri|SCF = s)
The three terms, given the hyper-parameters and
conjugate-prior relationship between Dirichlet and
Multinomial distributions, can be expressed in terms
of current assignments of s to verb v ( csv ), s to
GR-count n ( csn ) and s to GR ( csg ), the corre-
sponding totals ( cv, cs ), the dimensionality of the
distributions ( |SCF |, |N | and |G| ) and the hyper-
parameters ?, ? and ?:
P (SCF = s|V erb = v) = (csv+?)/(cv+|SCF |?)
P (N = n|SCF = s) = (csn + ?)/(cs + |N |?)
P (GR = gri|SCF = s) = (csgri +?)/(cs + |G|?)
Note that N , the possible GR-count for an in-
stance, is usually constant for pGRs ( 2 ? window
), unless the verb is close to the start or end of the
sentence.
423
? // V erbxSCF
&&
V erbi

i ? I
SCFi //

Ni
||
SCFxNoo ?oo
GRi SCFxGRoo ?oo
Figure 1: Our simple graphical model reflecting subcategorization. Double-circles indicate an observed
value, arrows indicate conditional dependency. What constitutes a ?GR? depends on the feature set being
used.
We chose our hyper-parameters ? = ? = ? = .02
to reflect the characteristic sparseness of the phe-
nomena (i.e. verbs tend to take a small number of
SCFs, which in turn are limited to a small number
of realizations). For the pGRs we used a window
of 5 tokens: a verb?s arguments will fall within a
small window in the majority of cases, so there is
diminished return in expanding the window at the
cost of increased noise. Finally, we set our SCF
count to 40, about twice the size of the strictly syn-
tactic general-language gold standard we describe in
section 3.3. This overestimation allows some flex-
ibility for the model to define its inventory based
on the data; any supernumerary frames will act as
?junk frames? that are rarely assigned and hence
will have little influence. We run Gibbs sampling
for 1000 iterations, and average the final 100 sam-
ples to estimate the posteriors P (SCF |V erb) and
P (GR|SCF ). Variance between adjacent states?
estimates of P (SCF |V erb) indicates that the sam-
pling typically converges after about 100-200 itera-
tions.1
3.3 Evaluation
Quantitative: cluster gold standard
Evaluating the output of unsupervised methods is
not straightforward: discrete, expert-defined cate-
gories (like many SCF inventories) are unlikely to
line up perfectly with data-driven, probabilistic out-
put. Even if they do, finding a mapping between
them is a problem of its own (Meila, 2003).
1Full source code for this work is available at http://cl.
cam.ac.uk/?tl318/files/subcat.tgz
Our goal is to define a fair quantitative compari-
son between arbitrary SCF lexicons. An SCF lexi-
con makes two claims: first, that it defines a reason-
able SCF inventory. Second, that for each verb, it
has an accurate distribution over that inventory. We
therefore compare the lexicons based on their per-
formance on a task that a good SCF lexicon should
be useful for: clustering verbs into lexical-semantic
classes. Our gold standard is from (Sun and Korho-
nen, 2009), where 200 verbs were assigned to 17
classes based on their alternation patterns (Levin,
1993). Previous work (Schulte im Walde, 2009;
Sun and Korhonen, 2009) has demonstrated that the
quality of an SCF lexicon?s inventory and probabil-
ity estimates corresponds to its predictive power for
membership in such alternation classes.
To compare the performance of our feature sets,
we chose the simple and familiar K-Means cluster-
ing algorithm (Hartigan and Wong, 1979). The in-
stances are the verbs? SCF distributions, and we se-
lect the number of clusters by the Silhouette vali-
dation technique (Rousseeuw, 1987). The clusters
are then compared to the gold standard clusters with
the purity-based F-Score from Sun and Korhonen
(2009) and the more familiar Adjusted Rand Index
(Hubert and Arabie, 1985). Our main point of com-
parison is the VALEX lexicon of SCF distributions,
whose scores we report alongside ours.
Qualitative: manual gold standard
We also want to see how our results line up with
a traditional linguistic view of subcategorization,
but this requires digging into the unsupervised out-
424
put and associating anonymous probabilistic objects
with established categories. We therefore present
sample output in three ways: first, we show the
clustering output from our top-performing method.
Second, we plot the probability mass over GRs for
two anonymous SCFs that correspond to recogniz-
able traditional SCFs, and one that demonstrates un-
expected behavior. Third, we compared the out-
put for several verbs to a coarsened version of the
manually-annotated gold standard used to evaluate
VALEX (Preiss et al, 2007). We collapsed the orig-
inal inventory of 168 SCFs to 18 purely syntactic
SCFs based on their characteristic GRs and removed
frames that depend on semantic distinctions, leav-
ing the detection of finer-grained and semantically-
based frames for future work.
4 Results
4.1 Verb clustering
We evaluated SCF lexicons based on the eight fea-
ture sets described in section 3.1, as well as the
VALEX SCF lexicon described in section 2. Table 4
shows the performance of the lexicons in ascending
order.
Method Pur. F-score Adj. Rand
tGR .24 .02
tGRlim .27 .02
pGRlex .32 .09
tGRlim,param .35 .08
pGR .35 .10
VALEX .36 .10
tGRparam,lex .37 .10
tGRparam .39 .12
tGRlim,param,lex .44 .12
Table 4: Task-based evaluation of lexicons acquired
with each of the eight feature types, and the state-of-
the-art rule-based VALEX lexicon.
These results lead to several conclusions: first,
training our model on tGRs outperforms pGRs and
VALEX. Since the parser that produced them is
known to perform well on general language (Briscoe
et al, 2006), the tGRs are of high quality: it makes
sense that reverting to the pGRs is unnecessary in
this case. The interesting point is the major perfor-
mance gain over VALEX, which uses the same tGR
features along with expert-developed rules and in-
ventory.
Second, we achieve performance comparable to
VALEX using pGRs with a narrow window width.
Since POS tagging is more reliable and robust across
domains than parsing, retraining on new domains
will not suffer the effects of a mismatched parsing
model (Lippincott et al, 2010). It is therefore pos-
sible to use this method to build large-scale lexicons
for any new domain with sufficient data.
Third, lexicalizing the closed-class POS tags in-
troduces semantic information outside the scope
of the alternation-based definition of subcatego-
rization. For example, subdividing the indefinite
pronoun tag ?PN1? into ?PN1-anyone? and ?PN1-
anything? gives information about the animacy of
the verb?s arguments. Our results show this degrades
performance for both pGR and tGR features, unless
the latter are limited to tGRs traditionally thought to
be relevant for the task.
4.2 Qualitative analysis
Table 5 shows clusters produced by our top-scoring
method, GRparam,lex,lim. Some clusters are imme-
diately intelligible at the semantic level and corre-
spond closely to the lexical-semantic classes found
in Levin (1993). For example, clusters 1, 6, and 14
include member verbs of Levin?s SAY, PEER and
AMUSE classes, respectively. Some clusters are
based on broader semantic distinctions (e.g. cluster
2 which groups together verbs related to locations)
while others relate semantic classes purely based
on their syntactic similarity (e.g. the verbs in clus-
ter 17 share strong preference for ?to? preposition).
The syntactic-semantic nature of the clusters reflects
the multimodal nature of verbs and illustrates why a
comprehensive subcategorization lexicon should not
be limited to syntactic frames. This phenomenon is
also encouraging for future work to tease apart and
simultaneously exploit several verbal aspects via ad-
ditional latent structure in the model.
An SCF?s distribution over features can reveal its
place in the traditional definition of subcategoriza-
tion. Figure 2 shows the high-probability (>.02)
tGRs for one SCF: the large mass centered on di-
rect object tGRs indicates this approximates the no-
tion of ?transitive?. Looking at the verbs most likely
to take this SCF (?stimulate?, ?conserve?) confirms
425
1 exclaim, murmur, mutter, reply, retort, say,
sigh, whisper
2 bang, knock, snoop, swim, teeter
3 flicker, multiply, overlap, shine
4 batter, charter, compromise, overwhelm,
regard, sway, treat
5 abolish, broaden, conserve, deepen, eradi-
cate, remove, sharpen, shorten, stimulate,
strengthen, unify
6 gaze, glance, look, peer, sneer, squint, stare
7 coincide, commiserate, concur, flirt, inter-
act
8 grin, smile, wiggle
9 confuse, diagnose, march
10 mate, melt, swirl
11 frown, jog, stutter
12 chuckle, mumble, shout
13 announce, envisage, mention, report, state
14 frighten, intimidate, scare, shock, upset
15 bash, falter, snarl, wail, weaken
16 cooperate, eject, respond, transmit
17 affiliate, compare, contrast, correlate, for-
ward, mail, ship
Table 5: Clusters (of size >2 and <20) produced
using tGRparam,lex,lim
this. Figure 3 shows a complement-taking SCF,
which is far rarer than simple transitive but also
clearly induced by our model.
The induced SCF inventory also has some redun-
dancy, such as additional transitive frames beside
figure 2, and frames with poor probability estimates.
Most of these issues can be traced to our simplifying
assumption that each tGR is drawn independently
w.r.t. an instance?s other tGRs. For example, if an
SCF gives any weight to indirect objects, it gives
non-zero probability to an instance with only indi-
rect objects, an impossible case. This can lead to
skewed probability estimates: since some tGRs can
occur multiple times in a given instance (e.g. in-
direct objects and prepositional phrases) the model
may find it reasonable to create an SCF with all
probability focused on that tGR, ignoring all oth-
ers, such as in figure 4. We conclude that our inde-
pendence assumption was too strong, and the model
would benefit from defining more structure within
Figure 2: The SCF corresponding to transitive has
most probability centered on dobj (e.g. stimulate,
conserve, deepen, eradicate, broaden)
Figure 3: The SCF corresponding to verbs taking
complements has more probability on xcomp and
ccomp (e.g. believe, state, agree, understand, men-
tion)
instances.
The full tables necessary to compare verb SCF
distributions from our output with the manual gold
standard are prohibited by space, but a few exam-
ples reinforce the analysis above. The verbs ?load?
and ?fill? show particularly high usage of ditransi-
tive SCFs in the gold standard. In our inventory, this
is reflected in high usage of an SCF with probabil-
ity centered on indirect objects, but due to the inde-
pendence assumptions the frame has a correspond-
ing low probability on subjects and direct objects,
despite the fact that these necessarily occur along
with any indirect object. The verbs ?acquire? and
?buy? demonstrate both a strength of our approach
and a weakness of using parsed input: both verbs
426
Figure 4: This SCF is dominated by indirect objects
and complements, catering to verbs that may take
several such tGRs, at the expense of subjects
show high probability of simple transitive in our
output and the gold standard. However, the Rasp
parser often conflates indirect objects and preposi-
tional phrases due to its unlexicalized model. While
our system correctly gives high probability to ditran-
sitive for both verbs, it inherits this confusion and
over-estimates ?acquire??s probability mass for the
frame. This is an example of how bad decisions
made by the parser cannot be fixed by the graphi-
cal model, and an area where pGR features have an
advantage.
5 Conclusions and future work
Our study reached two important conclusions: first,
given the same data as input, an unsupervised prob-
abilistic model can outperform a hand-crafted rule-
based SCF extractor with a predefined inventory.
We achieve better results with far less effort than
previous approaches by allowing the data to gov-
ern the definition of frames while estimating the
verb-specific distributions in a fully Bayesian man-
ner. Second, simply treating POS tags within a
small window of the verb as pseudo-GRs produces
state-of-the-art results without the need for a pars-
ing model. This is particularly encouraging when
building resources for new domains, where com-
plex models fail to generalize. In fact, by integrat-
ing results from unsupervised POS tagging (Teichert
and Daume? III, 2009) we could render this approach
fully domain- and language-independent.
We did not dwell on issues related to choosing
our hyper-parameters or latent class count. Both of
these can be accomplished with additional sampling
methods: hyper-parameters of Dirichlet priors can
be estimated via slice sampling (Heinrich, 2009),
and their dimensionality via Dirichlet Process priors
(Heinrich, 2011). This could help address the redun-
dancy we find in the induced SCF inventory, with the
potential SCFs growing to accommodate the data.
Our initial attempt at applying graphical models
to subcategorization also suggested several ways to
extend and improve the method. First, the indepen-
dence assumptions between GRs in a given instance
turned out to be too strong. To address this, we could
give instances internal structure to capture condi-
tional probability between generated GRs. Second,
our results showed the conflation of several verbal
aspects, most notably the syntactic and semantic.
In a sense this is encouraging, as it motivates our
most exciting future work: augmenting this simple
model to explicitly capture complementary infor-
mation such as distributional semantics (Blei et al,
2003), diathesis alternations (McCarthy, 2000) and
selectional preferences (O? Se?aghdha, 2010). This
study targeted high-frequency verbs, but the use of
syntactic and semantic classes would also help with
data sparsity down the road. These extensions would
also call for a more comprehensive evaluation, aver-
aging over several tasks, such as clustering by se-
mantics, syntax, alternations and selectional prefer-
ences.
In concrete terms, we plan to introduce latent vari-
ables corresponding to syntactic, semantic and alter-
nation classes, that will determine a verb?s syntac-
tic arguments, their semantic realization (i.e. selec-
tional preferences), and possible predicate-argument
structures. By combining the syntactic classes with
unsupervised POS tagging (Teichert and Daume? III,
2009) and the selectional preferences with distribu-
tional semantics (O? Se?aghdha, 2010), we hope to
produce more accurate results on these complemen-
tary tasks while avoiding the use of any supervised
learning. Finally, a fundamental advantage of a data-
driven, parse-free method is that it can be easily
trained for new domains. We next plan to test our
method on a new domain, such as biomedical text,
where verbs are known to take on distinct syntactic
behavior (Lippincott et al, 2010).
427
6 Acknowledgements
The work in this paper was funded by the Royal So-
ciety, (UK), EPSRC (UK) grant EP/G051070/1 and
EU grant 7FP-ITC-248064. We are grateful to Lin
Sun and Laura Rimell for the use of their cluster-
ing and subcategorization gold standards, and the
ACL reviewers for their helpful comments and sug-
gestions.
References
Omri Abend and Ari Rappoport. 2010. Fully unsuper-
vised core-adjunct argument classification. In ACL
?10.
Galen Andrew, Trond Grenager, and Christopher Man-
ning. 2004. Verb sense and subcategorization: us-
ing joint inference to improve performance on com-
plementary tasks. EMNLP ?04.
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In COLING ACL ?98.
David Blei, Andrew Ng, Michael Jordan, and John Laf-
ferty. 2003. Latent dirichlet alocation. Journal of
Machine Learning Research.
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical termi-
nology. Nucleic Acids Research, 32.
Bran Boguraev and Ted Briscoe. 1987. Large lexicons
for natural language processing. Computational Lin-
guistics, 13.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL on Interactive presentation
sessions.
John Carroll, Guido Minnen, and Ted Briscoe. 1998.
Can subcategorisation probabilities help a statistical
parser? In The 6th ACL/SIGDAT Workshop on Very
Large Corpora.
K Bretonnel Cohen and Lawrence Hunter. 2006. A
critical review of PASBio?s argument structures for
biomedical verbs. BMC Bioinformatics, 7.
James Curran, Stephen Clark, and Johan Bos. 2007. Lin-
guistically motivated large-Scale NLP with C&C and
Boxer. In ACL ?07.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC ?06.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2007. The infinite tree. In ACL ?07.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
1994. Comlex syntax: building a computational lexi-
con. In COLING ?94.
Xiwu Han, Chengguo Lv, and Tiejun Zhao. 2008.
Weakly supervised SVM for Chinese-English cross-
lingual subcategorization lexicon acquisition. In The
11th Joint Conference on Information Science.
J.A. Hartigan and M.A. Wong. 1979. Algorithm AS 136:
A K-Means clustering algorithm. Journal of the Royal
Statistical Society. Series C (Applied Statistics).
Gregor Heinrich. 2009. Parameter estimation for text
analysis. Technical report, Fraunhofer IGD.
428
Gregor Heinrich. 2011. Infinite LDA implementing the
HDP with minimum code complexity. Technical re-
port, arbylon.net.
Lawrence Hubert and Phipps Arabie. 1985. Comparing
partitions. Journal of Classification, 2.
Eric Joanis and Suzanne Stevenson. 2003. A general fea-
ture space for automatic verb classification. In EACL
?03.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification of
English verbs. In LREC ?08.
Anna Korhonen, Genevieve Gorrell, and Diana Mc-
Carthy. 2000. Statistical filtering and subcategoriza-
tion frame acquisition. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006a. A large subcategorization lexicon for natural
language processing applications. In LREC ?06.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
2006b. Automatic classification of verbs in biomedi-
cal texts. In ACL ?06.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
2008. The choice of features for classification of verbs
in biomedical texts. In COLING ?08.
Ro Lenci, Barbara Mcgillivray, Simonetta Montemagni,
and Vito Pirrelli. 2008. Unsupervised acquisition
of verb subcategorization frames from shallow-parsed
corpora. In LREC ?08.
Beth Levin. 1993. English Verb Classes and Alternation:
A Preliminary Investigation. University of Chicago
Press, Chicago, IL.
Thomas Lippincott, Anna Korhonen, and Diarmuid O?
Se?aghdha. 2010. Exploring subdomain variation in
biomedical language. BMC Bioinformatics.
Diana McCarthy. 2000. Using semantic preferences to
identify verbal participation in role switching alterna-
tions. In NAACL ?00.
Marina Meila. 2003. Comparing clusterings by the Vari-
ation of Information. In COLT.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
verb classification based on statistical distributions of
argument structure. Computational Linguistics.
Ce?dric Messiant. 2008. A subcategorization acquisition
system for French verbs. In ACL HLT ?08 Student Re-
search Workshop.
Yusuke Miyao. 2005. Probabilistic disambiguation mod-
els for wide-coverage HPSG parsing. In ACL ?05.
Radford M. Neal. 1993. Probabilistic inference using
markov chain Monte Carlo methods. Technical report,
University of Toronto.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In The CoNLL Shared Task Session
of EMNLP-CoNLL 2007.
Diarmuid O? Se?aghdha. 2010. Latent variable models of
selectional preference. In ACL ?10.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: an annotated corpus of
semantic roles. Computational Linguistics.
Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007. A
system for large-scale acquisition of verbal, nominal
and adjectival subcategorization frames from corpora.
In ACL ?07.
Douglas Roland and Daniel Jurafsky. 1998. How verb
subcategorization frequencies are affected by corpus
choice. In ACL ?98.
Peter Rousseeuw. 1987. Silhouettes: a graphical aid
to the interpretation and validation of cluster analysis.
Journal of Computational and Applied Mathematics.
C.J. Rupp, Paul Thompson, William Black, and John Mc-
Naught. 2010. A specialised verb lexicon as the ba-
sis of fact extraction in the biomedical domain. In In-
terdisciplinary Workshop on Verbs: The Identification
and Representation of Verb Features.
Sabine Schulte im Walde. 2009. The induction of verb
frames and verb classes from corpora. In Corpus
Linguistics. An International Handbook. Mouton de
Gruyter.
Lin Sun and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired selectional
preferences. In EMNLP?09.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In ACL ?03.
Adam R. Teichert and Hal Daume? III. 2009. Unsuper-
vised part of speech tagging without a lexicon. In
NIPS Workshop on Grammar Induction, Representa-
tion of Language and Language Learning.
E. Uzun, Y. Klaslan, H.V. Agun, and E. Uar. 2008.
Web-based acquisition of subcategorization frames for
Turkish. In The Eighth International Conference on
Artificial Intelligence and Soft Computing.
Giulia Venturi, Simonetta Montemagni, Simone Marchi,
Yutaka Sasaki, Paul Thompson, John McNaught, and
Sophia Ananiadou. 2009. Bootstrapping a verb lex-
icon for biomedical information extraction. In Com-
putational Linguistics and Intelligent Text Processing.
Springer Berlin / Heidelberg.
429
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 33?38,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx
?
, Su Nam Kim
?
, Zornitsa Kozareva
?
, Preslav Nakov
?
,
Diarmuid
?
O S
?
eaghdha
?
, Sebastian Pad
?
o
?
, Marco Pennacchiotti
??
,
Lorenza Romano
??
, Stan Szpakowicz
??
Abstract
SemEval-2 Task 8 focuses on Multi-way
classification of semantic relations between
pairs of nominals. The task was designed
to compare different approaches to seman-
tic relation classification and to provide a
standard testbed for future research. This
paper defines the task, describes the train-
ing and test data and the process of their
creation, lists the participating systems (10
teams, 28 runs), and discusses their results.
1 Introduction
SemEval-2010 Task 8 focused on semantic rela-
tions between pairs of nominals. For example, tea
and ginseng are in an ENTITY-ORIGIN relation in
?The cup contained tea from dried ginseng.?. The
automatic recognition of semantic relations has
many applications, such as information extraction,
document summarization, machine translation, or
construction of thesauri and semantic networks.
It can also facilitate auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing, and recognizing textual entailment.
Our goal was to create a testbed for automatic
classification of semantic relations. In developing
the task we met several challenges: selecting a
suitable set of relations, specifying the annotation
procedure, and deciding on the details of the task
itself. They are discussed briefly in Section 2; see
also Hendrickx et al (2009), which includes a sur-
vey of related work. The direct predecessor of Task
8 was Classification of semantic relations between
nominals, Task 4 at SemEval-1 (Girju et al, 2009),
?
University of Lisbon, iris@clul.ul.pt
?
University of Melbourne, snkim@csse.unimelb.edu.au
?
Information Sciences Institute/University of Southern
California, kozareva@isi.edu
?
National University of Singapore, nakov@comp.nus.edu.sg
?
University of Cambridge, do242@cl.cam.ac.uk
?
University of Stuttgart, pado@ims.uni-stuttgart.de
??
Yahoo! Inc., pennacc@yahoo-inc.com
??
Fondazione Bruno Kessler, romano@fbk.eu
??
University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
which had a separate binary-labeled dataset for
each of seven relations. We have defined SemEval-
2010 Task 8 as a multi-way classification task in
which the label for each example must be chosen
from the complete set of ten relations and the map-
ping from nouns to argument slots is not provided
in advance. We also provide more data: 10,717 an-
notated examples, compared to 1,529 in SemEval-1
Task 4.
2 Dataset Creation
2.1 The Inventory of Semantic Relations
We first decided on an inventory of semantic rela-
tions. Ideally, it should be exhaustive (enable the
description of relations between any pair of nomi-
nals) and mutually exclusive (each pair of nominals
in context should map onto only one relation). The
literature, however, suggests that no relation inven-
tory satisfies both needs, and, in practice, some
trade-off between them must be accepted.
As a pragmatic compromise, we selected nine
relations with coverage sufficiently broad to be of
general and practical interest. We aimed at avoid-
ing semantic overlap as much as possible. We
included, however, two groups of strongly related
relations (ENTITY-ORIGIN / ENTITY-DESTINA-
TION and CONTENT-CONTAINER / COMPONENT-
WHOLE / MEMBER-COLLECTION) to assess mod-
els? ability to make such fine-grained distinctions.
Our inventory is given below. The first four were
also used in SemEval-1 Task 4, but the annotation
guidelines have been revised, and thus no complete
continuity should be assumed.
Cause-Effect (CE). An event or object leads to an
effect. Example: those cancers were caused
by radiation exposures
Instrument-Agency (IA). An agent uses an in-
strument. Example: phone operator
Product-Producer (PP). A producer causes a
product to exist. Example: a factory manu-
factures suits
33
Content-Container (CC). An object is physically
stored in a delineated area of space. Example:
a bottle full of honey was weighed
Entity-Origin (EO). An entity is coming or is de-
rived from an origin (e.g., position or mate-
rial). Example: letters from foreign countries
Entity-Destination (ED). An entity is moving to-
wards a destination. Example: the boy went
to bed
Component-Whole (CW). An object is a com-
ponent of a larger whole. Example: my
apartment has a large kitchen
Member-Collection (MC). A member forms a
nonfunctional part of a collection. Example:
there are many trees in the forest
Message-Topic (MT). A message, written or spo-
ken, is about a topic. Example: the lecture
was about semantics
2.2 Annotation Guidelines
We defined a set of general annotation guidelines
as well as detailed guidelines for each semantic
relation. Here, we describe the general guidelines,
which delineate the scope of the data to be col-
lected and state general principles relevant to the
annotation of all relations.
1
Our objective is to annotate instances of seman-
tic relations which are true in the sense of hold-
ing in the most plausible truth-conditional inter-
pretation of the sentence. This is in the tradition
of the Textual Entailment or Information Valida-
tion paradigm (Dagan et al, 2009), and in con-
trast to ?aboutness? annotation such as semantic
roles (Carreras and M`arquez, 2004) or the BioNLP
2009 task (Kim et al, 2009) where negated rela-
tions are also labelled as positive. Similarly, we
exclude instances of semantic relations which hold
only in speculative or counterfactural scenarios. In
practice, this means disallowing annotations within
the scope of modals or negations, e.g., ?Smoking
may/may not have caused cancer in this case.?
We accept as relation arguments only noun
phrases with common-noun heads. This distin-
guishes our task from much work in Information
Extraction, which tends to focus on specific classes
of named entities and on considerably more fine-
grained relations than we do. Named entities are a
specific category of nominal expressions best dealt
1
The full task guidelines are available at http://docs.
google.com/View?id=dfhkmm46_0f63mfvf7
with using techniques which do not apply to com-
mon nouns. We only mark up the semantic heads of
nominals, which usually span a single word, except
for lexicalized terms such as science fiction.
We also impose a syntactic locality requirement
on example candidates, thus excluding instances
where the relation arguments occur in separate sen-
tential clauses. Permissible syntactic patterns in-
clude simple and relative clauses, compounds, and
pre- and post-nominal modification. In addition,
we did not annotate examples whose interpretation
relied on discourse knowledge, which led to the
exclusion of pronouns as arguments. Please see
the guidelines for details on other issues, includ-
ing noun compounds, aspectual phenomena and
temporal relations.
2.3 The Annotation Process
The annotation took place in three rounds. First,
we manually collected around 1,200 sentences for
each relation through pattern-based Web search. In
order to ensure a wide variety of example sentences,
we used a substantial number of patterns for each
relation, typically between one hundred and several
hundred. Importantly, in the first round, the relation
itself was not annotated: the goal was merely to
collect positive and near-miss candidate instances.
A rough aim was to have 90% of candidates which
instantiate the target relation (?positive instances?).
In the second round, the collected candidates for
each relation went to two independent annotators
for labeling. Since we have a multi-way classifi-
cation task, the annotators used the full inventory
of nine relations plus OTHER. The annotation was
made easier by the fact that the cases of overlap
were largely systematic, arising from general phe-
nomena like metaphorical use and situations where
more than one relation holds. For example, there is
a systematic potential overlap between CONTENT-
CONTAINER and ENTITY-DESTINATION depend-
ing on whether the situation described in the sen-
tence is static or dynamic, e.g., ?When I came,
the <e1>apples</e1> were already put in the
<e2>basket</e2>.? is CC(e1, e2), while ?Then,
the <e1>apples</e1> were quickly put in the
<e2>basket</e2>.? is ED(e1, e2).
In the third round, the remaining disagreements
were resolved, and, if no consensus could be
achieved, the examples were removed. Finally, we
merged all nine datasets to create a set of 10,717
instances. We released 8,000 for training and kept
34
the rest for testing.
2
Table 1 shows some statistics about the dataset.
The first column (Freq) shows the absolute and rel-
ative frequencies of each relation. The second col-
umn (Pos) shows that the average share of positive
instances was closer to 75% than to 90%, indicating
that the patterns catch a substantial amount of ?near-
miss? cases. However, this effect varies a lot across
relations, causing the non-uniform relation distribu-
tion in the dataset (first column).
3
After the second
round, we also computed inter-annotator agreement
(third column, IAA). Inter-annotator agreement
was computed on the sentence level, as the per-
centage of sentences for which the two annotations
were identical. That is, these figures can be inter-
preted as exact-match accuracies. We do not report
Kappa, since chance agreement on preselected can-
didates is difficult to estimate.
4
IAA is between
60% and 95%, again with large relation-dependent
variation. Some of the relations were particularly
easy to annotate, notably CONTENT-CONTAINER,
which can be resolved through relatively clear cri-
teria, despite the systematic ambiguity mentioned
above. ENTITY-ORIGIN was the hardest relation to
annotate. We encountered ontological difficulties
in defining both Entity (e.g., in contrast to Effect)
and Origin (as opposed to Cause). Our numbers
are on average around 10% higher than those re-
ported by Girju et al (2009). This may be a side
effect of our data collection method. To gather
1,200 examples in realistic time, we had to seek
productive search query patterns, which invited
certain homogeneity. For example, many queries
for CONTENT-CONTAINER centered on ?usual sus-
pect? such as box or suitcase. Many instances of
MEMBER-COLLECTION were collected on the ba-
sis of from available lists of collective names.
3 The Task
The participating systems had to solve the follow-
ing task: given a sentence and two tagged nominals,
predict the relation between those nominals and the
direction of the relation.
We released a detailed scorer which outputs (1) a
confusion matrix, (2) accuracy and coverage, (3)
2
This set includes 891 examples from SemEval-1 Task 4.
We re-annotated them and assigned them as the last examples
of our training dataset to ensure that the test set was unseen.
3
To what extent our candidate selection produces a biased
sample is a question that we cannot address within this paper.
4
We do not report Pos or IAA for OTHER, since OTHER is
a pseudo-relation that was not annotated in its own right. The
numbers would therefore not be comparable to other relations.
Relation Freq Pos IAA
Cause-Effect 1331 (12.4%) 91.2% 79.0%
Component-Whole 1253 (11.7%) 84.3% 70.0%
Entity-Destination 1137 (10.6%) 80.1% 75.2%
Entity-Origin 974 (9.1%) 69.2% 58.2%
Product-Producer 948 (8.8%) 66.3% 84.8%
Member-Collection 923 (8.6%) 74.7% 68.2%
Message-Topic 895 (8.4%) 74.4% 72.4%
Content-Container 732 (6.8%) 59.3% 95.8%
Instrument-Agency 660 (6.2%) 60.8% 65.0%
Other 1864 (17.4%) N/A
4
N/A
4
Total 10717 (100%)
Table 1: Annotation Statistics. Freq: Absolute and
relative frequency in the dataset; Pos: percentage
of ?positive? relation instances in the candidate set;
IAA: inter-annotator agreement
precision (P), recall (R), and F
1
-Score for each
relation, (4) micro-averaged P, R, F
1
, (5) macro-
averaged P, R, F
1
. For (4) and (5), the calculations
ignored the OTHER relation. Our official scoring
metric is macro-averaged F
1
-Score for (9+1)-way
classification, taking directionality into account.
The teams were asked to submit test data pre-
dictions for varying fractions of the training data.
Specifically, we requested results for the first 1000,
2000, 4000, and 8000 training instances, called
TD1 through TD4. TD4 was the full training set.
4 Participants and Results
Table 2 lists the participants and provides a rough
overview of the system features. Table 3 shows the
results. Unless noted otherwise, all quoted numbers
are F
1
-Scores.
Overall Ranking and Training Data. We rank
the teams by the performance of their best system
on TD4, since a per-system ranking would favor
teams with many submitted runs. UTD submit-
ted the best system, with a performance of over
82%, more than 4% better than the second-best
system. FBK IRST places second, with 77.62%,
a tiny margin ahead of ISI (77.57%). Notably, the
ISI system outperforms the FBK IRST system for
TD1 to TD3, where it was second-best. The accu-
racy numbers for TD4 (Acc TD4) lead to the same
overall ranking: micro- versus macro-averaging
does not appear to make much difference either.
A random baseline gives an uninteresting score of
6%. Our competitive baseline system is a simple
Naive Bayes classifier which relies on words in the
sentential context only; two systems scored below
this baseline.
35
System Institution Team Description Res. Class.
Baseline Task organizers local context of 2 words only BN
ECNU-SR-1 East China Normal
University
Man Lan, Yuan
Chen, Zhimin
Zhou, Yu Xu
stem, POS, syntactic patterns S SVM
(multi)
ECNU-SR-2,3 features like ECNU-SR-1, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-4 stem, POS, syntactic patterns,
hyponymy and meronymy rela-
tions
WN,
S
SVM
(multi)
ECNU-SR-5,6 features like ECNU-SR-4, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-7 majority vote of ECNU-1,2,4,5
FBK IRST-6C32 Fondazione Bruno
Kessler
Claudio Giu-
liano, Kateryna
Tymoshenko
3-word window context features
(word form, part of speech, or-
thography) + Cyc; parameter
estimation by optimization on
training set
Cyc SVM
FBK IRST-12C32 FBK IRST-6C32 + distance fea-
tures
FBK IRST-12VBC32 FBK IRST-12C32 + verbs
FBK IRST-6CA,
-12CA, -12VBCA
features as above, parameter es-
timation by cross-validation
FBK NK-RES1 Fondazione Bruno
Kessler
Matteo Negri,
Milen Kouylekov
collocations, glosses, semantic
relations of nominals + context
features
WN BN
FBK NK-RES 2,3,4 like FBK NK-RES1 with differ-
ent context windows and collo-
cation cutoffs
ISI Information Sci-
ences Institute,
University of
Southern Califor-
nia
Stephen Tratz features from different re-
sources, a noun compound
relation system, and various
feature related to capitalization,
affixes, closed-class words
WN,
RT, G
ME
ISTI-1,2 Istituto di sci-
enca e tecnologie
dell?informazione
?A. Faedo?
Andrea Esuli,
Diego Marcheg-
giani, Fabrizio
Sebastiani
Boosting-based classification.
Runs differ in their initializa-
tion.
WN 2S
JU Jadavpur Univer-
sity
Santanu Pal, Partha
Pakray, Dipankar
Das, Sivaji Bandy-
opadhyay
Verbs, nouns, and prepositions;
seed lists for semantic relations;
parse features and NEs
WN,
S
CRF
SEKA Hungarian
Academy of
Sciences
Eszter Simon, An-
dras Kornai
Levin and Roget classes, n-
grams; other grammatical and
formal features
RT,
LC
ME
TUD-base Technische Univer-
sit?at Darmstadt
Gy?orgy Szarvas,
Iryna Gurevych
word, POS n-grams, depen-
dency path, distance
S ME
TUD-wp TUD-base + ESA semantic re-
latedness scores
+WP
TUD-comb TUD-base + own semantic relat-
edness scores
+WP,WN
TUD-comb-threshold TUD-comb with higher thresh-
old for OTHER
UNITN University of
Trento
Fabio Celli punctuation, context words,
prepositional patterns, estima-
tion of semantic relation
? DR
UTD University of Texas
at Dallas
Bryan Rink, Sanda
Harabagiu
context wods, hypernyms, POS,
dependencies, distance, seman-
tic roles, Levin classes, para-
phrases
WN,
S, G,
PB/NB,
LC
SVM,
2S
Table 2: Participants of SemEval-2010 Task 8. Res: Resources used (WN: WordNet data; WP:
Wikipedia data; S: syntax; LC: Levin classes; G: Google n-grams, RT: Roget?s Thesaurus, PB/NB:
PropBank/NomBank). Class: Classification style (ME: Maximum Entropy; BN: Bayes Net; DR: Decision
Rules/Trees; CRF: Conditional Random Fields; 2S: two-step classification)
36
System TD1 TD2 TD3 TD4 Acc TD4 Rank Best Cat Worst Cat-9
Baseline 33.04 42.41 50.89 57.52 50.0 - MC (75.1) IA (28.0)
ECNU-SR-1 52.13 56.58 58.16 60.08 57.1
4
CE (79.7) IA (32.2)
ECNU-SR-2 46.24 47.99 69.83 72.59 67.1 CE (84.4) IA (52.2)
ECNU-SR-3 39.89 42.29 65.47 68.50 62.0 CE (83.4) IA (46.5)
ECNU-SR-4 67.95 70.58 72.99 74.82 70.5 CE (84.6) IA (61.4)
ECNU-SR-5 49.32 50.70 72.63 75.43 70.2 CE (85.1) IA (60.7)
ECNU-SR-6 42.88 45.54 68.87 72.19 65.8 CE (85.2) IA (56.7)
ECNU-SR-7 58.67 58.87 72.79 75.21 70.2 CE (86.1) IA (61.8)
FBK IRST-6C32 60.19 67.31 71.78 76.81 72.4
2
ED (82.6) IA (69.4)
FBK IRST-12C32 60.66 67.91 72.04 76.91 72.4 MC (84.2) IA (68.8)
FBK IRST-12VBC32 62.64 69.86 73.19 77.11 72.3 ED (85.9) PP (68.1)
FBK IRST-6CA 60.58 67.14 71.63 76.28 71.4 CE (82.3) IA (67.7)
FBK IRST-12CA 61.33 67.80 71.65 76.39 71.4 ED (81.8) IA (67.5)
FBK IRST-12VBCA 63.61 70.20 73.40 77.62 72.8 ED (86.5) IA (67.3)
FBK NK-RES1 55.71
?
64.06
?
67.80
?
68.02 62.1
7
ED (77.6) IA (52.9)
FBK NK-RES2 54.27
?
63.68
?
67.08
?
67.48 61.4 ED (77.4) PP (55.2)
FBK NK-RES3 54.25
?
62.73
?
66.11
?
66.90 60.5 MC (76.7) IA (56.3)
FBK NK-RES4 44.11
?
58.85
?
63.06
?
65.84 59.4 MC (76.1) IA/PP (58.0)
ISI 66.68 71.01 75.51 77.57 72.7 3 CE (87.6) IA (61.5)
ISTI-1 50.49
?
55.80
?
61.14
?
68.42 63.2
6
ED (80.7) PP (53.8)
ISTI-2 50.69
?
54.29
?
59.77
?
66.65 61.5 ED (80.2) IA (48.9)
JU 41.62
?
44.98
?
47.81
?
52.16 50.2 9 CE (75.6) IA (27.8)
SEKA 51.81 56.34 61.10 66.33 61.9 8 CE (84.0) PP (43.7)
TUD-base 50.81 54.61 56.98 60.50 56.1
5
CE (80.7) IA (31.1)
TUD-wp 55.34 60.90 63.78 68.00 63.5 ED (82.9) IA (44.1)
TUD-comb 57.84 62.52 66.41 68.88 64.6 CE (83.8) IA (46.8)
TUD-comb-? 58.35 62.45 66.86 69.23 65.4 CE (83.4) IA (46.9)
UNITN 16.57
?
18.56
?
22.45
?
26.67 27.4 10 ED (46.4) PP (0)
UTD 73.08 77.02 79.93 82.19 77.9 1 CE (89.6) IA (68.5)
Table 3: F
1
-Score of all submitted systems on the test dataset as a function of training data: TD1=1000,
TD2=2000, TD3=4000, TD4=8000 training examples. Official results are calculated on TD4. The results
marked with
?
were submitted after the deadline. The best-performing run for each participant is italicized.
As for the amount of training data, we see a sub-
stantial improvement for all systems between TD1
and TD4, with diminishing returns for the transi-
tion between TD3 and TD4 for many, but not all,
systems. Overall, the differences between systems
are smaller for TD4 than they are for TD1. The
spread between the top three systems is around 10%
at TD1, but below 5% at TD4. Still, there are clear
differences in the influence of training data size
even among systems with the same overall archi-
tecture. Notably, ECNU-SR-4 is the second-best
system at TD1 (67.95%), but gains only 7% from
the eightfold increase of the size of the training data.
At the same time, ECNU-SR-3 improves from less
than 40% to almost 69%. The difference between
the systems is that ECNU-SR-4 uses a multi-way
classifier including the class OTHER, while ECNU-
SR-3 uses binary classifiers and assigns OTHER
if no other relation was assigned with p>0.5. It
appears that these probability estimates for classes
are only reliable enough for TD3 and TD4.
The Influence of System Architecture. Almost
all systems used either MaxEnt or SVM classifiers,
with no clear advantage for either. Similarly, two
systems, UTD and ISTI (rank 1 and 6) split the task
into two classification steps (relation and direction),
but the 2nd- and 3rd-ranked systems do not. The
use of a sequence model such as a CRF did not
show a benefit either.
The systems use a variety of resources. Gener-
ally, richer feature sets lead to better performance
(although the differences are often small ? compare
the different FBK IRST systems). This improve-
ment can be explained by the need for semantic
generalization from training to test data. This need
can be addressed using WordNet (contrast ECNU-1
to -3 with ECNU-4 to -6), the Google n-gram col-
lection (see ISI and UTD), or a ?deep? semantic
resource (FBK IRST uses Cyc). Yet, most of these
resources are also included in the less successful
systems, so beneficial integration of knowledge
sources into semantic relation classification seems
to be difficult.
System Combination. The differences between
the systems suggest that it might be possible to
achieve improvements by building an ensemble
37
system. When we combine the top three systems
(UTD, FBK IRST-12VBCA, and ISI) by predict-
ing their majority vote, or OTHER if there was none,
we obtain a small improvement over the UTD sys-
tem with an F
1
-Score of 82.79%. A combination of
the top five systems using the same method shows
a worse performance, however (80.42%). This sug-
gests that the best system outperforms the rest by
a margin that cannot be compensated with system
combination, at least not with a crude majority vote.
We see a similar pattern among the ECNU systems,
where the ECNU-SR-7 combination system is out-
performed by ECNU-SR-5, presumably since it
incorporates the inferior ECNU-SR-1 system.
Relation-specific Analysis. We also analyze the
performance on individual relations, especially the
extremes. There are very stable patterns across all
systems. The best relation (presumably the eas-
iest to classify) is CE, far ahead of ED and MC.
Notably, the performance for the best relation is
75% or above for almost all systems, with compar-
atively small differences between the systems. The
hardest relation is generally IA, followed by PP.
5
Here, the spread among the systems is much larger:
the highest-ranking systems outperform others on
the difficult relations. Recall was the main prob-
lem for both IA and PP: many examples of these
two relations are misclassified, most frequently as
OTHER. Even at TD4, these datasets seem to be
less homogeneous than the others. Intriguingly, PP
shows a very high inter-annotator agreement (Ta-
ble 1). Its difficulty may therefore be due not to
questionable annotation, but to genuine variability,
or at least the selection of difficult patterns by the
dataset creator. Conversely, MC, among the easiest
relations to model, shows only a modest IAA.
Difficult Instances. There were 152 examples
that are classified incorrectly by all systems. We
analyze them, looking for sources of errors. In ad-
dition to a handful of annotation errors and some
borderline cases, they are made up of instances
which illustrate the limits of current shallow mod-
eling approaches in that they require more lexical
knowledge and complex reasoning. A case in point:
The bottle carrier converts your <e1>bottle</e1>
into a <e2>canteen</e2>. This instance of
OTHER is misclassified either as CC (due to the
5
The relation OTHER, which we ignore in the overall F
1
-
score, does even worse, often below 40%. This is to be ex-
pected, since the OTHER examples in our datasets are near
misses for other relations, thus making a very incoherent class.
nominals) or as ED (because of the preposition
into). Another example: [...] <e1>Rudders</e1>
are used by <e2>towboats</e2> and other ves-
sels that require a high degree of manoeuvrability.
This is an instance of CW misclassified as IA, prob-
ably on account of the verb use which is a frequent
indicator of an agentive relation.
5 Discussion and Conclusion
There is little doubt that 19-way classification is a
non-trivial challenge. It is even harder when the
domain is lexical semantics, with its idiosyncrasies,
and when the classes are not necessarily disjoint,
despite our best intentions. It speaks to the success
of the exercise that the participating systems? per-
formance was generally high, well over an order
of magnitude above random guessing. This may
be due to the impressive array of tools and lexical-
semantic resources deployed by the participants.
Section 4 suggests a few ways of interpreting
and analyzing the results. Long-term lessons will
undoubtedly emerge from the workshop discussion.
One optimistic-pessimistic conclusion concerns the
size of the training data. The notable gain TD3?
TD4 suggests that even more data would be helpful,
but that is so much easier said than done: it took
the organizers well in excess of 1000 person-hours
to pin down the problem, hone the guidelines and
relation definitions, construct sufficient amounts of
trustworthy training data, and run the task.
References
X. Carreras and L. M`arquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role label-
ing. In Proc. CoNLL-04, Boston, MA.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009.
Recognizing textual entailment: Rational, evalua-
tion and approaches. Natural Language Engineer-
ing, 15(4):i?xvii.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2009. Classification of semantic
relations between nominals. Language Resources
and Evaluation, 43(2):105?121.
I. Hendrickx, S. Kim, Z. Kozareva, P. Nakov, D.
?
O
S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Romano,
and S. Szpakowicz. 2009. SemEval-2010 Task
8: Multi-way classification of semantic relations be-
tween pairs of nominals. In Proc. NAACL Workshop
on Semantic Evaluations, Boulder, CO.
J. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of BioNLP?09 shared task on event
extraction. In Proc. BioNLP-09, Boulder, CO.
38
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 39?44,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid
?
O S
?
eaghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
Previous research has shown that the mean-
ing of many noun-noun compounds N
1
N
2
can be approximated reasonably well by
paraphrasing clauses of the form ?N
2
that
. . . N
1
?, where ?. . . ? stands for a verb
with or without a preposition. For exam-
ple, malaria mosquito is a ?mosquito that
carries malaria?. Evaluating the quality of
such paraphrases is the theme of Task 9 at
SemEval-2010. This paper describes some
background, the task definition, the process
of data collection and the task results. We
also venture a few general conclusions be-
fore the participating teams present their
systems at the SemEval-2010 workshop.
There were 5 teams who submitted 7 sys-
tems.
1 Introduction
Noun compounds (NCs) are sequences of two or
more nouns that act as a single noun,1 e.g., stem
cell, stem cell research, stem cell research organi-
zation, etc. Lapata and Lascarides (2003) observe
that NCs pose syntactic and semantic challenges for
three basic reasons: (1) the compounding process
is extremely productive in English; (2) the seman-
tic relation between the head and the modifier is
implicit; (3) the interpretation can be influenced by
contextual and pragmatic factors. Corpus studies
have shown that while NCs are very common in
English, their frequency distribution follows a Zip-
fian or power-law distribution and the majority of
NCs encountered will be rare types (Tanaka and
Baldwin, 2003; Lapata and Lascarides, 2003; Bald-
win and Tanaka, 2004; ?O S?eaghdha, 2008). As a
consequence, Natural Language Processing (NLP)
1
We follow the definition in (Downing, 1977).
applications cannot afford either to ignore NCs or
to assume that they can be handled by relying on a
dictionary or other static resource.
Trouble with lexical resources for NCs notwith-
standing, NC semantics plays a central role in com-
plex knowledge discovery and applications, includ-
ing but not limited to Question Answering (QA),
Machine Translation (MT), and Information Re-
trieval (IR). For example, knowing the (implicit)
semantic relation between the NC components can
help rank and refine queries in QA and IR, or select
promising translation pairs in MT (Nakov, 2008a).
Thus, robust semantic interpretation of NCs should
be of much help in broad-coverage semantic pro-
cessing.
Proposed approaches to modelling NC seman-
tics have used semantic similarity (Nastase and Sz-
pakowicz, 2003; Moldovan et al, 2004; Kim and
Baldwin, 2005; Nastase and Szpakowicz, 2006;
Girju, 2007; ?O S?eaghdha and Copestake, 2007)
and paraphrasing (Vanderwende, 1994; Kim and
Baldwin, 2006; Butnariu and Veale, 2008; Nakov
and Hearst, 2008). The former body of work seeks
to measure the similarity between known and un-
seen NCs by considering various features, usually
context-related. In contrast, the latter group uses
verb semantics to interpret NCs directly, e.g., olive
oil as ?oil that is extracted from olive(s)?, drug
death as ?death that is caused by drug(s)?, flu shot
as a ?shot that prevents flu?.
The growing popularity ? and expected direct
utility ? of paraphrase-based NC semantics has
encouraged us to propose an evaluation exercise
for the 2010 edition of SemEval. This paper gives
a bird?s-eye view of the task. Section 2 presents
its objective, data, data collection, and evaluation
method. Section 3 lists the participating teams.
Section 4 shows the results and our analysis. In
Section 5, we sum up our experience so far.
39
2 Task Description
2.1 The Objective
For the purpose of the task, we focused on two-
word NCs which are modifier-head pairs of nouns,
such as apple pie or malaria mosquito. There are
several ways to ?attack? the paraphrase-based se-
mantics of such NCs.
We have proposed a rather simple problem: as-
sume that many paraphrases can be found ? perhaps
via clever Web search ? but their relevance is up in
the air. Given sufficient training data, we seek to es-
timate the quality of candidate paraphrases in a test
set. Each NC in the training set comes with a long
list of verbs in the infinitive (often with a prepo-
sition) which may paraphrase the NC adequately.
Examples of apt paraphrasing verbs: olive oil ?
be extracted from, drug death ? be caused by, flu
shot ? prevent. These lists have been constructed
from human-proposed paraphrases. For the train-
ing data, we also provide the participants with a
quality score for each paraphrase, which is a simple
count of the number of human subjects who pro-
posed that paraphrase. At test time, given a noun
compound and a list of paraphrasing verbs, a partic-
ipating system needs to produce aptness scores that
correlate well (in terms of relative ranking) with
the held out human judgments. There may be a
diverse range of paraphrases for a given compound,
some of them in fact might be inappropriate, but
it can be expected that the distribution over para-
phrases estimated from a large number of subjects
will indeed be representative of the compound?s
meaning.
2.2 The Datasets
Following Nakov (2008b), we took advantage of
the Amazon Mechanical Turk2 (MTurk) to acquire
paraphrasing verbs from human annotators. The
service offers inexpensive access to subjects for
tasks which require human intelligence. Its API
allows a computer program to run tasks easily and
collate the subjects? responses. MTurk is becoming
a popular means of eliciting and collecting linguis-
tic intuitions for NLP research; see Snow et al
(2008) for an overview and a further discussion.
Even though we recruited human subjects,
whom we required to take a qualification test,3
2
www.mturk.com
3We soon realized that we also had to offer a version of
our assignments without a qualification test (at a lower pay
rate) since very few people were willing to take a test. Overall,
data collection was time-consuming since many
annotators did not follow the instructions. We had
to monitor their progress and to send them timely
messages, pointing out mistakes. Although the
MTurk service allows task owners to accept or re-
ject individual submissions, rejection was the last
resort since it has the triply unpleasant effect of
(1) denying the worker her fee, (2) negatively af-
fecting her rating, and (3) lowering our rating as
a requester. We thus chose to try and educate our
workers ?on the fly?. Even so, we ended up with
many examples which we had to correct manu-
ally by labor-intensive post-processing. The flaws
were not different from those already described by
Nakov (2008b). Post-editing was also necessary to
lemmatize the paraphrasing verbs systematically.
Trial Data. At the end of August 2009, we
released as trial data the previously collected para-
phrase sets (Nakov, 2008b) for the Levi-250 dataset
(after further review and cleaning). This dataset
consisted of 250 noun-noun compounds form (Levi,
1978), each paraphrased by 25-30 MTurk workers
(without a qualification test).
Training Data. The training dataset was an ex-
tension of the trial dataset. It consisted of the same
250 noun-noun compounds, but the number of an-
notators per compound increased significantly. We
aimed to recruit at least 30 additional MTurk work-
ers per compound; for some compounds we man-
aged to get many more. For example, when we
added the paraphrasing verbs from the trial dataset
to the newly collected verbs, we had 131 different
workers for neighborhood bars, compared to just
50 for tear gas. On the average, we had 72.7 work-
ers per compound. Each worker was instructed
to try to produce at least three paraphrasing verbs,
so we ended up with 191.8 paraphrasing verbs per
compound, 84.6 of them being unique. See Table 1
for more details.
Test Data. The test dataset consisted of 388
noun compounds collected from two data sources:
(1) the Nastase and Szpakowicz (2003) dataset;
and (2) the Lauer (1995) dataset. The former
contains 328 noun-noun compounds (there are
also a number of adjective-noun and adverb-noun
pairs), while the latter contains 266 noun-noun
compounds. Since these datasets overlap between
themselves and with the training dataset, we had
to exclude some examples. In the end, we had 388
we found little difference in the quality of work of subjects
recruited with and without the test.
40
Training: 250 NCs Testing: 388 NCs All: 638 NCs
Total Min/Max/Avg Total Min/Max/Avg Total Min/Max/Avg
MTurk workers 28,199 50/131/72.7 17,067 57/96/68.3 45,266 50/131/71.0
Verb types 32,832 25/173/84.6 17,730 41/133/70.9 50,562 25/173/79.3
Verb tokens 74,407 92/462/191.8 46,247 129/291/185.0 120,654 92/462/189.1
Table 1: Statistics about the the training/test datasets. Shown are the total number of verbs proposed as
well as the minimum, maximum and average number of paraphrasing verb types/tokens per compound.
unique noun-noun compounds for testing, distinct
from those used for training. We aimed for 100
human workers per testing NC, but we could only
get 68.3, with a minimum of 57 and a maximum of
96; there were 185.0 paraphrasing verbs per com-
pound, 70.9 of them being unique, which is close
to what we had for the training data.
Data format. We distribute the training data as
a raw text file. Each line has the following tab-
separated format:
NC paraphrase frequency
where NC is a noun-noun compound (e.g., ap-
ple cake, flu virus), paraphrase is a human-
proposed paraphrasing verb optionally followed
by a preposition, and frequency is the number
of annotators who proposed that paraphrase. Here
is an illustrative extract from the training dataset:
flu virus cause 38
flu virus spread 13
flu virus create 6
flu virus give 5
flu virus produce 5
...
flu virus be made up of 1
flu virus be observed in 1
flu virus exacerbate 1
The test file has a similar format, except that the
frequency is not included and the paraphrases for
each noun compound appear in random order:
...
chest pain originate
chest pain start in
chest pain descend in
chest pain be in
...
License. All datasets are released under the Cre-
ative Commons Attribution 3.0 Unported license.4
4
creativecommons.org/licenses/by/3.0
2.3 Evaluation
All evaluation was performed by computing an ap-
propriate measure of similarity/correlation between
system predictions and the compiled judgements of
the human annotators. We did it on a compound-by-
compound basis and averaged over all compounds
in the test dataset. Section 4 shows results for three
measures: Spearman rank correlation, Pearson cor-
relation, and cosine similarity.
Spearman Rank Correlation (?) was adopted
as the official evaluation measure for the competi-
tion. As a rank correlation statistic, it does not use
the numerical values of the predictions or human
judgements, only their relative ordering encoded
as integer ranks. For a sample of n items ranked
by two methods x and y, the rank correlation ? is
calculated as follows:
? =
n
?
x
i
y
i
? (
?
x
i
)(
?
y
i
)
?
n
?
x
2
i
? (
?
x
i
)
2
?
n
?
y
2
i
? (
?
y
i
)
2
(1)
where x
i
, y
i
are the ranks given by x and y to the
ith item, respectively. The value of ? ranges be-
tween -1.0 (total negative correlation) and 1.0 (total
positive correlation).
Pearson Correlation (r) is a standard measure
of correlation strength between real-valued vari-
ables. The formula is the same as (1), but with
x
i
, y
i
taking real values rather than rank values;
just like ?, r?s values fall between -1.0 and 1.0.
Cosine similarity is frequently used in NLP to
compare numerical vectors:
cos =
?
n
i
x
i
y
i
?
?
n
i
x
2
i
?
n
i
y
2
i
(2)
For non-negative data, the cosine similarity takes
values between 0.0 and 1.0. Pearson?s r can be
viewed as a version of the cosine similarity which
performs centering on x and y.
Baseline: To help interpret these evaluation mea-
sures, we implemented a simple baseline. A dis-
tribution over the paraphrases was estimated by
41
System Institution Team Description
NC-INTERP International Institute of
Information Technology,
Hyderabad
Prashant
Mathur
Unsupervised model using verb-argument frequen-
cies from parsed Web snippets and WordNet
smoothing
UCAM University of Cambridge Clemens Hepp-
ner
Unsupervised model using verb-argument frequen-
cies from the British National Corpus
UCD-GOGGLE-I University College
Dublin
Guofu Li Unsupervised probabilistic model using pattern fre-
quencies estimated from the Google N-Gram corpus
UCD-GOGGLE-II Paraphrase ranking model learned from training
data
UCD-GOGGLE-III Combination of UCD-GOGGLE-I and UCD-
GOGGLE-II
UCD-PN University College
Dublin
Paul Nulty Scoring according to the probability of a paraphrase
appearing in the same set as other paraphrases pro-
vided
UVT-MEPHISTO Tilburg University Sander
Wubben
Supervised memory-based ranker using features
from Google N-Gram Corpus and WordNet
Table 2: Teams participating in SemEval-2010 Task 9
summing the frequencies for all compounds in the
training dataset, and the paraphrases for the test ex-
amples were scored according to this distribution.
Note that this baseline entirely ignores the identity
of the nouns in the compound.
3 Participants
The task attracted five teams, one of which (UCD-
GOGGLE) submitted three runs. The participants
are listed in Table 2 along with brief system de-
scriptions; for more details please see the teams?
own description papers.
4 Results and Discussion
The task results appear in Table 3. In an evaluation
by Spearman?s ? (the official ranking measure),
the winning system was UVT-MEPHISTO, which
scored 0.450. UVT also achieved the top Pear-
son?s r score. UCD-PN is the top-scoring system
according to the cosine measure. One participant
submitted part of his results after the official dead-
line, which is marked by an asterisk.
The participants used a variety of information
sources and estimation methods. UVT-MEPHISTO
is a supervised system that uses frequency informa-
tion from the Google N-Gram Corpus and features
from WordNet (Fellbaum, 1998) to rank candidate
paraphrases. On the other hand, UCD-PN uses
no external resources and no supervised training,
yet came within 0.009 of UVT-MEPHISTO in the
official evaluation. The basic idea of UCD-PN ?
that one can predict the plausibility of a paraphrase
simply by knowing which other paraphrases have
been given for that compound regardless of their
frequency ? is clearly a powerful one. Unlike the
other systems, UCD-PN used information about the
test examples (not their ranks, of course) for model
estimation; this has similarities to ?transductive?
methods for semi-supervised learning. However,
post-hoc analysis shows that UCD-PN would have
preserved its rank if it had estimated its model on
the training data only. On the other hand, if the task
had been designed differently ? by asking systems
to propose paraphrases from the set of all possi-
ble verb/preposition combinations ? then we would
not expect UCD-PN?s approach to work as well as
models that use corpus information.
The other systems are comparable to UVT-
MEPHISTO in that they use corpus frequencies
to evaluate paraphrases and apply some kind of
semantic smoothing to handle sparsity. How-
ever, UCD-GOGGLE-I, UCAM and NC-INTERP
are unsupervised systems. UCAM uses the 100-
million word BNC corpus, while the other systems
use Web-scale resources; this has presumably ex-
acerbated sparsity issues and contributed to a rela-
tively poor performance.
The hybrid approach exemplified by UCD-
GOGGLE-III combines the predictions of a sys-
tem that models paraphrase correlations and one
that learns from corpus frequencies and thus at-
tains better performance. Given that the two top-
scoring systems can also be characterized as using
these two distinct information sources, it is natu-
ral to consider combining these systems. Simply
normalizing (to unit sum) and averaging the two
sets of prediction values for each compound does
42
Rank System Supervised? Hybrid? Spearman ? Pearson r Cosine
1 UVT-MEPHISTO yes no 0.450 0.411 0.635
2 UCD-PN no no 0.441 0.361 0.669
3 UCD-GOGGLE-III yes yes 0.432 0.395 0.652
4 UCD-GOGGLE-II yes no 0.418 0.375 0.660
5 UCD-GOGGLE-I no no 0.380 0.252 0.629
6 UCAM no no 0.267 0.219 0.374
7 NC-INTERP* no no 0.186 0.070 0.466
Baseline yes no 0.425 0.344 0.524
Combining UVT and UCD-PN yes yes 0.472 0.431 0.685
Table 3: Evaluation results for SemEval-2010 Task 9 (* denotes a late submission).
indeed give better scores: Spearman ? = 0.472,
r = 0.431, Cosine = 0.685.
The baseline from Section 2.3 turns out to be
very strong. Evaluating with Spearman?s ?, only
three systems outperform it. It is less competitive
on the other evaluation measures though. This
suggests that global paraphrase frequencies may
be useful for telling sensible paraphrases from bad
ones, but will not do for quantifying the plausibility
of a paraphrase for a given noun compound.
5 Conclusion
Given that it is a newly-proposed task, this initial
experiment in paraphrasing noun compounds has
been a moderate success. The participation rate
has been sufficient for the purposes of comparing
and contrasting different approaches to the role
of paraphrases in the interpretation of noun-noun
compounds. We have seen a variety of approaches
applied to the same dataset, and we have been able
to compare the performance of pure approaches to
hybrid approaches, and of supervised approaches
to unsupervised approaches. The results reported
here are also encouraging, though clearly there is
considerable room for improvement.
This task has established a high baseline for sys-
tems to beat. We can take heart from the fact that
the best performance is apparently obtained from a
combination of corpus-derived usage features and
dictionary-derived linguistic knowledge. Although
clever but simple approaches can do quite well on
such a task, it is encouraging to note that the best
results await those who employ the most robust
and the most informed treatments of NCs and their
paraphrases. Despite a good start, this is a chal-
lenge that remains resolutely open. We expect that
the dataset created for the task will be a valuable
resource for future research.
Acknowledgements
This work is partially supported by grants from
Amazon and from the Bulgarian National Science
Foundation (D002-111/15.12.2008 ? SmartBook).
References
Timothy Baldwin and Takaaki Tanaka. 2004. Trans-
lation by Machine of Compound Nominals: Getting
it Right. In Proceedings of the ACL-04 Workshop
on Multiword Expressions: Integrating Processing,
pages 24?31, Barcelona, Spain.
Cristina Butnariu and Tony Veale. 2008. A Concept-
Centered Approach to Noun-Compound Interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING-
08), pages 81?88, Manchester, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Roxana Girju. 2007. Improving the Interpretation
of Noun Phrases with Cross-linguistic Information.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL-07),
pages 568?575, Prague, Czech Republic.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP-05), pages 945?956, Jeju Island, South Ko-
rea.
Su Nam Kim and Timothy Baldwin. 2006. Inter-
preting Semantic Relations in Noun Compounds via
Verb Semantics. In Proceedings of the COLING-
ACL-06 Main Conference Poster Sessions, pages
491?498, Sydney, Australia.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional evi-
dence. In Proceedings of the 10th Conference of the
43
European Chapter of the Association for Computa-
tional Linguistics (EACL-03), pages 235?242, Bu-
dapest, Hungary.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Macquarie University.
Judith Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York, NY.
DanMoldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT-NAACL-04 Workshop on Computa-
tional Lexical Semantics, pages 60?67, Boston, MA.
Preslav Nakov and Marti A. Hearst. 2008. Solving
Relational Similarity Problems Using the Web as a
Corpus. In Proceedings of the 46th Annual Meet-
ing of the Association of Computational Linguistics
(ACL-08), pages 452?460, Columbus, OH.
Preslav Nakov. 2008a. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In Pro-
ceedings of the 18th European Conference on Artifi-
cial Intelligence (ECAI-08), pages 338?342, Patras,
Greece.
Preslav Nakov. 2008b. Noun Compound Interpreta-
tion Using Paraphrasing Verbs: Feasibility Study.
In Proceedings of the 13th International Confer-
ence on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA-08), pages 103?117,
Varna, Bulgaria.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings
of the 5th International Workshop on Computational
Semantics (IWCS-03), pages 285?301, Tilburg, The
Netherlands.
Vivi Nastase and Stan Szpakowicz. 2006. Matching
syntactic-semantic graphs for semantic relation as-
signment. In Proceedings of the 1st Workshop on
Graph Based Methods for Natural Language Pro-
cessing (TextGraphs-06), pages 81?88, New York,
NY.
Diarmuid
?
O S?eaghdha and Ann Copestake. 2007. Co-
occurrence Contexts for Noun Compound Interpre-
tation. In Proceedings of the ACL-07 Workshop
on A Broader Perspective on Multiword Expressions
(MWE-07), pages 57?64, Prague, Czech Republic.
Diarmuid
?
O S?eaghdha. 2008. Learning Compound
Noun Semantics. Ph.D. thesis, University of Cam-
bridge.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and Fast ? But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-08), pages 254?263, Honolulu, HI.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of the
ACL-03 Workshop on Multiword Expressions (MWE-
03), pages 17?24, Sapporo, Japan.
Lucy Vanderwende. 1994. Algorithm for Automatic
Interpretation of Noun Sequences. In Proceedings
of the 15th International Conference on Compu-
tational Linguistics (COLING-94), pages 782?788,
Kyoto, Japan.
44
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 170?179,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Modelling selectional preferences in a lexical hierarchy
Diarmuid O? Se?aghdha
Computer Laboratory
University of Cambridge
Cambridge, UK
do242@cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
Cambridge, UK
Anna.Korhonen@cl.cam.ac.uk
Abstract
This paper describes Bayesian selectional
preference models that incorporate knowledge
from a lexical hierarchy such as WordNet. In-
spired by previous work on modelling with
WordNet, these approaches are based either on
?cutting? the hierarchy at an appropriate level
of generalisation or on a ?walking? model that
selects a path from the root to a leaf. In
an evaluation comparing against human plau-
sibility judgements, we show that the mod-
els presented here outperform previously pro-
posed comparable WordNet-based models, are
competitive with state-of-the-art selectional
preference models and are particularly well-
suited to estimating plausibility for items that
were not seen in training.
1 Introduction
The concept of selectional preference captures the
intuitive fact that predicates in language have a bet-
ter semantic ?fit? for certain arguments than oth-
ers. For example, the direct object argument slot
of the verb eat is more plausibly filled by a type
of food (I ate a pizza) than by a type of vehicle (I
ate a car), while the subject slot of the verb laugh
is more plausibly filled by a person than by a veg-
etable. Human language users? knowledge about
selectional preferences has been implicated in anal-
yses of metaphor processing (Wilks, 1978) and in
psycholinguistic studies of comprehension (Rayner
et al, 2004). In Natural Language Processing, au-
tomatically acquired preference models have been
shown to aid a number of tasks, including semantic
role labelling (Zapirain et al, 2009), parsing (Zhou
et al, 2011) and lexical disambiguation (Thater et
al., 2010; O? Se?aghdha and Korhonen, 2011).
It is tempting to assume that with a large enough
corpus, preference learning reduces to a simple lan-
guage modelling task that can be solved by counting
predicate-argument co-occurrences. Indeed, Keller
and Lapata (2003) show that relatively good perfor-
mance at plausibility estimation can be attained by
submitting queries to a Web search engine. How-
ever, there are many scenarios where this approach
is insufficient: for languages and language domains
where Web-scale data is unavailable, for predicate
types (e.g., inference rules or semantic roles) that
cannot be retrieved by keyword search and for ap-
plications where accurate models of rarer words are
required. O? Se?aghdha (2010) shows that the Web-
based approach is reliably outperformed by more
complex models trained on smaller corpora for less
frequent predicate-argument combinations. Models
that induce a level of semantic representation, such
as probabilistic latent variable models, have a further
advantage in that they can provide rich structured in-
formation for downstream tasks such as lexical dis-
ambiguation (O? Se?aghdha and Korhonen, 2011) and
semantic relation mining (Yao et al, 2011).
Recent research has investigated the potential
of Bayesian probabilistic models such as Latent
Dirichlet Allocation (LDA) for modelling selec-
tional preferences (O? Se?aghdha, 2010; Ritter et al,
2010; Reisinger and Mooney, 2011). These mod-
els are flexible and robust, yielding superior perfor-
mance compared to previous approaches. In this
paper we present a preliminary study of analogous
170
models that make use of a lexical hierarchy (in our
case the WordNet hierarchy). We describe two broad
classes of probabilistic models over WordNet and
how they can be implemented in a Bayesian frame-
work. The two main potential advantages of in-
corporating WordNet information are: (a) improved
predictions about rare and out-of-vocabulary argu-
ments; (b) the ability to perform syntactic word
sense disambiguation with a principled probabilistic
model and without the need for an additional step
that heuristically maps latent variables onto Word-
Net senses. Focussing here on (a), we demon-
strate that our models attain better performance than
previously-proposed WordNet-based methods on a
plausibility estimation task and are particularly well-
suited to estimating plausibility for arguments that
were not seen in training and for which LDA cannot
make useful predictions.
2 Background and Related Work
The WordNet lexical hierarchy (Fellbaum, 1998)
is one of the most-used resources in NLP, finding
many applications in both the definition of tasks (e.g.
the SENSEVAL/SemEval word sense disambigua-
tion tasks) and in the construction of systems. The
idea of using WordNet to define selectional prefer-
ences was first implemented by Resnik (1993), who
proposed a measure of associational strength be-
tween a semantic class s and a predicate p corre-
sponding to a relation type r:
A(s, p, r) =
1
?
P (s|p, r) log2
P (s|p, r)
P (s|r)
(1)
where ? is a normalisation term. This measure cap-
tures the degree to which the probability of seeing
s given the predicate p differs from the prior proba-
bility of s. Given that we are often interested in the
preference of p for a word w rather than a class and
words generally map onto multiple classes, Resnik
suggests calculating A(s, p, r) for all classes that
could potentially be expressed by w and predicting
the maximal value.
Cut-based models assume that modelling the se-
lectional preference of a predicate involves finding
the right ?level of generalisation? in the WordNet
hierarchy. For example, the direct object slot of
the verb eat can be associated with the subhierarchy
rooted at the synset food#n#1, as all hyponyms of
that synset are assumed to be edible and the imme-
diate hypernym of the synset, substance#n#1, is too
general given that many substances are rarely eaten.1
This leads to the notion of ?cutting? the hierarchy at
one or more positions (Li and Abe, 1998). The mod-
elling task then becomes that of finding the cuts that
are maximally general without overgeneralising. Li
and Abe (1998) propose a model in which the appro-
priate cut c is selected according to the Minimum
Description Length principle; this principle explic-
itly accounts for the trade-off between generalisa-
tion and accuracy by minimising a sum of model de-
scription length and data description length. The
probability of a predicate p taking as its argument
an synset s is modelled as:
Pla(s|p, r) = P (s|cs,p,r)P (c|p) (2)
where cs,p,r is the portion of the cut learned for p
that dominates s. The distribution P (s|cs,p,r) is held
to be uniform over all synsets dominated by cs,p,r,
while P (c|p) is given by a maximum likelihood es-
timate.
Clark and Weir (2002) present a model that, while
not explicitly described as cut-based, likewise seeks
to find the right level of generalisation for an obser-
vation. In this case, the hypernym at which to ?cut?
is chosen by a chi-squared test: if the aggregate pref-
erence of p for classes in the subhierarchy rooted at c
differs significantly from the individual preferences
of p for the immediate children of c, the hierarchy is
cut below c. The probability of p taking a synset s
as its argument is given by:
Pcw(s|p, r) =
P (p|cs,p,r, r)
P (s|r)
P (p|r)
?
s??S P (p|cs?,p,r, r)
P (s?|r)
P (p|r)
(3)
where cs,p,r is the root node of the subhierarchy con-
taining s that was selected for p.
An alternative approach to modelling with Word-
Net uses its hierarchical structure to define a Markov
model with transitions from senses to senses and
from senses to words. The intuition here is that each
observation is generated by a ?walk? from the root
of the hierarchy to a leaf node and emitting the word
1In this paper we use WordNet version 3.0, except where
stated otherwise.
171
corresponding to the leaf. Abney and Light (1999)
proposed such a model for selectional preferences,
trained via EM, but failed to achieve competitive
performance on a pseudodisambiguation task.
The models described above have subsequently
been used in many different studies. For exam-
ple: McCarthy and Carroll (2003) use Li and Abe?s
method in a word sense disambiguation setting;
Schulte im Walde et al (2008) use their MDL ap-
proach as part of a system for syntactic and seman-
tic subcategorisation frame learning; Shutova (2010)
deploys Resnik?s method for metaphor interpreta-
tion. Brockmann and Lapata (2003) report a com-
parative evaluation in which the methods of Resnik
and Clark and Weir outpeform Li and Abe?s method
on a plausibility estimation task.
Much recent work on preference learning has fo-
cused on purely distributional methods that do not
use a predefined hierarchy but learn to make general-
isations about predicates and arguments from corpus
observations alone. These methods can be vector-
based (Erk et al, 2010; Thater et al, 2010), dis-
criminative (Bergsma et al, 2008) or probabilistic
(O? Se?aghdha, 2010; Ritter et al, 2010; Reisinger
and Mooney, 2011). In the probabilistic category,
Bayesian models based on the ?topic modelling?
framework (Blei et al, 2003b) have been shown to
achieve state-of-the-art performance in a number of
evaluation settings; the models considered in this pa-
per are also related to this framework.
In machine learning, researchers have proposed
a variety of topic modelling methods where the la-
tent variables are arranged in a hierarchical structure
(Blei et al, 2003a; Mimno et al, 2007). In con-
trast to the present work, these models use a rel-
atively shallow hierarchy (e.g., 3 levels) and any
hierarchy node can in principle emit any vocabu-
lary item; they thus provide a poor match for our
goal of modelling over WordNet. Boyd-Graber et
al. (2007) describe a topic model that is directly in-
fluenced by Abney and Light?s Markov model ap-
proach; this model (LDAWN) is described further in
Section 3.3 below. Reisinger and Pas?ca (2009) in-
vestigate Bayesian methods for attaching attributes
harvested from the Web at an appropriate level in
the WordNet hierarchy; this task is related in spirit
to the preference learning task.
3 Probabilistic modelling over WordNet
3.1 Notation
We assume that we have a lexical hierarchy in the
form of a directed acyclic graph G = (S,E) where
each node (or synset) s ? S is associated with a
set of words Wn belonging to a large vocabulary V .
Each edge e ? E leads from a node n to its children
(or hyponyms) Chn. As G is a DAG, a node may
have more than one parent but there are no cycles.
The ultimate goal is to learn a distribution over the
argument vocabulary V for each predicate p in a set
P , through observing predicate-argument pairs. A
predicate is understood to correspond to a pairing of
a lexical item v and a relation type r, for example
p = (eat, direct object). The list of observations
for a predicate p is denoted by Observations(p).
3.2 Cut-based models
Model 1 Generative story for WN-CUT
for cut c ? {1 . . . |C|} do
?c ?Multinomial(?c)
end for
for predicate p ? {1 . . . |P |} do
?p ? Dirichlet(?)
for argument instance i ? Observations(p)
do
ci ?Multinomial(?p)
wi ?Multinomial(?ci)
end for
end for
The first model we consider, WN-CUT, is directly
inspired by Li and Abe?s model (2). Each predicate
p is associated with a distribution over ?cuts?, i.e.,
complete subgraphs of G rooted at a single node
and containing all nodes dominated by the root. It
follows that the number of possible cuts is the same
as the number of synsets. Each cut c is associated
with a non-uniform distribution over the set of words
Wc that can be generated by the synsets contained
in c. As well as the use of a non-uniform emis-
sion distribution and the placing of Dirichlet priors
on the multinomial over cuts, a significant differ-
ence from Li and Abe?s model is that overlapping
cuts are permitted (indeed, every cut has non-zero
probability given a predicate). For example, the
172
model may learn that the direct object slot of eat
gives high probability to the cut rooted at food#n#1
but also that the cut rooted at substance#n#1 has
non-negligible probability, higher than that assigned
to phenomenon#n#1. It follows that the estimated
probability of p taking argument w takes into ac-
count all possible cuts, weighted by their probability
given p.
The generative story for WN-CUT is given in Al-
gorithm 1; this describes the assumptions made by
the model about how a corpus of observations is gen-
erated. The probability of predicate p taking argu-
ment w is defined as (4); an empirical posterior esti-
mate of this quantity can be computed from a Gibbs
sampling state via (5):
P (w|p) =
?
c
P (c|p)P (w|c) (4)
?
?
c
fcp + ?
f?p + |C|?
fwc + ?
f?c + |Wc|?
(5)
where fcw is the number of observations contain-
ing argument w that have been assigned cut c, fcp
is the number of observations containing predicate
p that have been assigned cut c and fc?, f?p are the
marginal counts for cut c and predicate p, respec-
tively. The two terms that are multiplied in (4) play
complementary roles analogous to those of the two
description lengths in Li and Abe?s MDL formula-
tion; P (c|p) will prefer to reuse more general cuts,
while P (w|c) will prefer more specific cuts with a
smaller associated argument vocabulary.
As the number of words |Wc| that can be emitted
by a cut |c| varies according to the size of the sub-
hierarchy under the cut, the proportion of probability
mass accorded to the likelihood and the prior in (5)
is not constant. An alternative formulation is to keep
the distribution of mass between likelihood and prior
constant but vary the value of the individual ?c pa-
rameters according to cut size. Experiments suggest
that this alternative does not differ in performance.
The second cut-based model, WN-CUT-TOPICS,
extends WN-CUT by adding two extra layers of la-
tent variables. Firstly, the choice of cut is condi-
tional on a ?topic? variable z rather than directly
conditioned on the predicate; when the topic vocab-
ulary Z is much smaller than the cut vocabulary C,
this has the effect of clustering the cuts. Secondly,
Model 2 Generative story for WN-CUT-TOPICS
for topic z ? {1 . . . |Z|} do
?z ? Dirichlet(?)
end for
for cut c ? {1 . . . |C|} do
?c ? Dirichlet(?c)
end for
for synset s ? {1 . . . |S|} do
?s ? Dirichlet(?s)
end for
for predicate p ? {1 . . . |P |} do
?p ? Dirichlet(?)
for argument instance i ? Observations(p)
do
zi ?Multinomial(?p)
ci ?Multinomial(?z)
si ?Multinomial(?c)
wi ?Multinomial(?s)
end for
end for
instead of immediately drawing a word once a cut
has been chosen, the model first draws a synset s
and then draws a word from the vocabularyWs asso-
ciated with that synset. This has two advantages; it
directly disambiguates each observation to a specific
synset rather than to a region of the hierarchy and it
should also improve plausibility predictions for rare
synonyms of common arguments. The generative
story for WN-CUT-TOPICS is given in Algorithm 2;
the distribution over arguments for p is given in (6)
and the corresponding posterior estimate in (7):
P (w|p) =
?
z
P (z|p)
?
c
P (c|z)
?
s
P (s|c)P (w|s)
(6)
?
?
z
fzp + ?z
f?p +
?
z? ?z?
?
c
fcz + ?
f?z + |C|?
?
?
s
fsc + ?
f?c + |Sc|?
fws + ?
f?s + |Ws|?
(7)
As before, fzp, fcz , fsc and fws are the re-
spective co-occurrence counts of topics/predicates,
cuts/topics, synsets/cuts and words/synsets in the
sampling state and f?p, f?z , f?c and f?s are the cor-
responding marginal counts.
173
Since WN-CUT and WN-CUT-TOPICS are con-
structed from multinomials with Dirichlet priors,
it is relatively straightforward to train them by
collapsed Gibbs sampling (Griffiths and Steyvers,
2004), an iterative method whereby each latent vari-
able in the model is stochastically updated accord-
ing to the distribution given by conditioning on the
current latent variable assignments of all other to-
kens. In the case of WN-CUT, this amounts to up-
dating the cut assignment ci for each token in turn.
For WN-CUT-TOPICS there are three variables to
update; ci and si must be updated simultaneously,
but zi can be updated independently for the bene-
fit of efficiency. Although WordNet contains 82,115
noun synsets, updates for ci and si can be computed
very efficiently, as there are typically few possible
synsets for a given word type and few possible cuts
for a given synset (the maximum synset depth is 19).
The hyperparameters for the various Dirichlet pri-
ors are also reestimated in the course of learning; the
values of these hyperparameters control the degree
of sparsity preferred by the model. The ?top-level?
hyperparameters ? in WN-CUT and ? in WN-CUT-
TOPICS are estimated using a fixed-point iteration
proposed by Wallach (2008); the other hyperparam-
eters are learned by slice sampling (Neal, 2003).
3.3 Walk-based models
Abney and Light (1999) proposed an approach to
selectional preference learning in which arguments
are generated for predicates by following a path
? = (l1, . . . , l|?|) from the root of the hierarchy to a
leaf node and emitting the corresponding word. The
path is chosen according to a Markov model with
transition probabilities specific to each predicate. In
this model, each leaf node is associated with a sin-
gle word; the synsets associated with that word are
the immediate parent nodes of the leaf. Abney and
Light found that their model did not match the per-
formance of Resnik?s (1993) simpler method. We
have had a similar lack of success with a Bayesian
version of this model, which we do not describe fur-
ther here.
Boyd-Graber et al (2007) describe a related topic
model, LDAWN, for word sense disambiguation
that adds an intermediate layer of latent variables
Z on which the Markov model parameters are con-
ditioned. In their application, each document in a
Model 3 Generative story for LDAWN
for topic z ? {1 . . . |Z|} do
for synset s ? {1 . . . |S|} do
Draw transition probabilities ?z,s ?
Dirichlet(??s)
end for
end for
for predicate p ? {1 . . . |P |} do
?p ? Dirichlet(?)
for argument instance i ? Observations(p)
do
zi ?Multinomial(?p)
Create a path starting at the root synset ?0:
while not at a leaf node do
?t+1 ?Multinomial(?zi,?t)
end while
Emit the word at the leaf as wi
end for
end for
corpus is associated with a distribution over topics
and each topic is associated with a distribution over
paths. The clustering effect of the topic layer allows
the documents to ?share? information and hence al-
leviate problems due to sparsity. By analogy to Ab-
ney and Light, it is a short and intuitive step to ap-
ply LDAWN to selectional preference learning. The
generative story for LDAWN is given in Algorithm
3; the probability model for P (w|p) is defined by (8)
and the posterior estimate is (9):
P (w|p) =
?
z
P (z|p)
?
?
1[?? w]P (?|z) (8)
?
?
z
fzp + ?z
f?p +
?
z? ?z?
?
?
1[?? w]?
|?|?1?
i=1
fz,li?li+1 + ??li?li+1
fz,li?? + ?
(9)
where 1[? ? w] = 1 when the path ? leads to leaf
node w and has value 0 otherwise. Following Boyd-
Graber et al the Dirichlet priors on the transition
probabilities are parameterised by the product of a
strength parameter ? and a distribution ?s, the latter
being fixed according to relative corpus frequencies
to ?guide? the model towards more fruitful paths.
Gibbs sampling updates for LDAWN are given in
Boyd-Graber et al (2007). As before, we reestimate
174
SEEN:
staff morale 0.4889
team morale 0.5945
issue morale 0.0595
UNSEEN:
pupil morale 0.4318
minute morale -0.0352
snow morale -0.2748
Table 1: Extract from the noun-noun section of Keller and
Lapata?s (2003) dataset, with human plausibility scores
the hyperparameters during learning; ? is estimated
by Wallach?s fixed-point iteration and ? is estimated
by slice sampling.
4 Experiments
4.1 Experimental procedure
We evaluate our methods by comparing their predic-
tions to human judgements of predicate-argument
plausibility. This is a standard approach to se-
lectional preference evaluation (Keller and Lapata,
2003; Brockmann and Lapata, 2003; O? Se?aghdha,
2010) and arguably yields a better appraisal of a
model?s intrinsic semantic quality than other eval-
uations such as pseudo-disambiguation or held-out
likelihood prediction.2 We use a set of plau-
sibility judgements collected by Keller and Lap-
ata (2003). This dataset comprises 180 predicate-
argument combinations for each of three syntactic
relations: verb-object, noun-noun modification and
adjective-noun modification. The data for each re-
lation is divided into a ?seen? portion containing
90 combinations that were observed in the British
National Corpus and an ?unseen? portion contain-
ing 90 combinations that do not appear (though
the predicates and arguments do appear separately).
Plausibility judgements were elicited from a large
group of human subjects, then normalised and log-
transformed. Table 1 gives a representative illus-
tration of the data. Following the evaluation in O?
Se?aghdha (2010), with which we wish to compare,
we use Pearson r and Spearman ? correlation coef-
ficients as performance measures.
All models were trained on the 90-million word
2For a related argument in the context of topic model evalu-
ation, see Chang et al (2009).
written component of the British National Cor-
pus,3 lemmatised, POS-tagged and parsed with the
RASP toolkit (Briscoe et al, 2006). We removed
predicates occurring with just one argument type
and all tokens containing non-alphabetic characters.
The resulting datasets consist of 3,587,172 verb-
object observations (7,954 predicate types, 80,107
argument types), 3,732,470 noun-noun observations
(68,303 predicate types, 105,425 argument types)
and 3,843,346 adjective-noun observations (29,975
predicate types, 62,595 argument types).
All the Bayesian models were trained by Gibbs
sampling, as outlined above. For each model we run
three sampling chains for 1,000 iterations and aver-
age the plausibility predictions for each to produce a
final prediction P (w|p) for each predicate-argument
item. As the evaluation demands an estimate of the
joint probability P (w, p) we multiply the predicted
P (w|p) by a predicate probability P (p|r) estimated
from relative corpus frequencies. In training we use
a burn-in period of 200 iterations, after which hyper-
parameters are reestimated and P (p|r) predictions
are sampled every 50 iterations. All probability es-
timates are log-transformed to match the gold stan-
dard judgements.
In order to compare against previously proposed
selectional preference approaches based on Word-
Net we also reimplemented the methods that per-
formed best in the evaluation of Brockmann and
Lapata (2003): Resnik (1993) and Clark and Weir
(2002). For Resnik?s model we used WordNet 2.1
rather than WordNet 3.0 as the former has multi-
ple roots, a property that turns out to be necessary
for good performance. Clark and Weir?s method
requires that the user specify a significance thresh-
old ? to be used in deciding where to cut; to give
it the best possible chance we tested with a range
of values (0.05, 0.3, 0.6, 0.9) and report results for
the best-performing setting, which consistently was
? = 0.9. One can also use different statistical hy-
pothesis tests; again we choose the test giving the
best results, which was Pearson?s chi-squared test.
As this method produces a probability estimate con-
ditioned on the predicate p we multiply by a MLE
estimate of P (p|r) and log-transform the result.
3http://www.natcorp.ox.ac.uk/
175
eat food#n#1, aliment#n#1, entity#n#1, solid#n#1, food#n#2
drink fluid#n#1, liquid#n#1, entity#n#1, alcohol#n#1, beverage#n#1
appoint individual#n#1, entity#n#1, chief#n#1, being#n#2, expert#n#1
publish abstract entity#n#1, piece of writing#n#1, communication#n#2, publication#n#1
Table 2: Most probable cuts learned by WN-CUT for the object argument of selected verbs
Verb-object Noun-noun Adjective-noun
Seen Unseen Seen Unseen Seen Unseen
r ? r ? r ? r ? r ? r ?
WN-CUT .593 .582 .514 .571 .550 .584 .564 .590 .561 .618 .453 .439
WN-CUT-100 .500 .529 .575 .630 .619 .639 .662 .706 .537 .510 .464 .431
WN-CUT-200 .538 .546 .557 .608 .595 .632 .639 .669 .585 .587 .435 .431
LDAWN-100 .497 .538 .558 .594 .605 .619 .635 .633 .549 .545 .459 .462
LDAWN-200 .546 .562 .508 .548 .610 .654 .526 .568 .578 .583 .453 .450
Resnik .384 .473 .469 .470 .242 .187 .152 .037 .309 .388 .311 .280
Clark/Weir .489 .546 .312 .365 .441 .521 .543 .576 .440 .476 .271 .242
BNC (MLE) .620 .614 .196 .222 .544 .604 .114 .125 .543 .622 .135 .102
LDA .504 .541 .558 .603 .615 .641 .636 .666 .594 .558 .468 .459
Table 3: Results (Pearson r and Spearman ? correlations) on Keller and Lapata?s (2003) plausibility data; underlining
denotes the best-performing WordNet-based model, boldface denotes the overall best performance
4.2 Results
Table 2 demonstrates the top cuts learned by the
WN-CUT model from the verb-object training data
for a selection of verbs. Table 3 gives quanti-
tative results for the WordNet-based models un-
der consideration, as well as results reported by O?
Se?aghdha (2010) for a purely distributional LDA
model with 100 topics and a Maximum Likelihood
Estimate model learned from the BNC. In general,
the Bayesian WordNet-based models outperform the
models of Resnik and Clark and Weir, and are com-
petitive with the state-of-the-art LDA results. To
test the statistical significance of performance differ-
ences we use the test proposed by Meng et al (1992)
for comparing correlated correlations, i.e., correla-
tion scores with a shared gold standard. The dif-
ferences between Bayesian WordNet models are not
significant (p > 0.05, two-tailed) for any dataset or
evaluation measure. However, all Bayesian mod-
els improve significantly over Resnik?s and Clark
and Weir?s models for multiple conditions. Perhaps
surprisingly, the relatively simple WN-CUT model
scores the greatest number of significant improve-
ments over both Resnik (7 out of 12 conditions)
and Clark and Weir (8 out of 12), though the other
Bayesian models do follow close behind. This may
suggest that the incorporation of WordNet structure
into the model in itself provides much of the cluster-
ing benefit provided by an additional layer of ?topic?
latent variables.4
In order to test the ability of the WordNet-based
models to make predictions about arguments that
are absent from the training vocabulary, we created
an artificial out-of-vocabulary dataset by removing
each of the Keller and Lapata argument words from
the input corpus and retraining. An LDA selectional
preference model will completely fail here, but we
hope that the WordNet models can still make rela-
tively accurate predictions by leveraging the addi-
tional lexical knowledge provided by the hierarchy.
For example, if one knows that a tomatillo is classed
as a vegetable in WordNet, one can predict a rel-
atively high probability that it can be eaten, even
though the word tomatillo does not appear in the
BNC.
As a baseline we use a BNC-trained model that
4An alternative hypothesis is that samplers for the more
complex models take longer to ?mix?. We have run some exper-
iments with 5,000 iterations but did not observe an improvement
in performance.
176
Verb-object Noun-noun Adjective-noun
Seen Unseen Seen Unseen Seen Unseen
r ? r ? r ? r ? r ? r ?
WN-CUT .334 .326 .518 .569 .252 .212 .254 .274 .451 .397 .471 .458
WN-CUT-100 .308 .357 .459 .489 .223 .207 .126 .074 .285 .264 .234 .226
WN-CUT-200 .273 .321 .452 .482 .192 .174 .115 .053 .266 .212 .220 .214
LDAWN-100 .223 .235 .410 .391 .259 .220 .132 .138 .016 .037 .264 .254
LDAWN-200 .291 .285 .392 .379 .240 .163 .118 .131 .041 .078 .209 .212
Resnik .203 .341 .472 .497 .054 -.054 .184 .089 .353 .393 .333 .365
Clark/Weir .222 .287 .201 .235 .225 .162 .279 .304 .313 .202 .190 .148
BNC .206 .224 .276 .240 .256 .240 .223 .225 .088 .103 .220 .231
Table 4: Forced-OOV results (Pearson r and Spearman ? correlations) on Keller and Lapata?s (2003) plausibility data
predicts P (w, p) proportional to the MLE predicate
probability P (p); a distributional LDA model will
make essentially the same prediction. Clark and
Weir?s method does not have full coverage; if no
sense s of an argument appears in the data then
P (s|p) is zero for all senses and the resulting pre-
diction is zero, which cannot be log-transformed.
To sidestep this issue, unseen senses are assigned a
pseudofrequency of 0.1. Results for this ?forced-
OOV? task are presented in Table 4. WN-CUT
proves the most adept at generalising to unseen ar-
guments, attaining the best performance on 7 of 12
dataset/evaluation conditions and a statistically sig-
nificant improvement over the baseline on 6. We ob-
serve that estimating the plausibility of unseen ar-
guments for noun-noun modifiers is particularly dif-
ficult. One obvious explanation is that the training
data for this relation has fewer tokens per predi-
cate, making it more difficult to learn their prefer-
ences. A second, more hypothetical, explanation is
that the ontological structure of WordNet is a rela-
tively poor fit for the preferences of nominal modi-
fiers; it is well-known that almost any pair of nouns
can combine to produce a minimally plausible noun-
noun compound (Downing, 1977) and it may be that
this behaviour is ill-suited by the assumption that
preferences are sparse distributions over regions of
WordNet.
5 Conclusion
In this paper we have presented a range of
Bayesian selectional preference models that incor-
porate knowledge about the structure of a lexical hi-
erarchy. One motivation for this work was to test
the hypothesis that such knowledge can be helpful
in constructing robust models that can handle rare
and unseen arguments. To this end we have re-
ported a plausibility-based evaluation in which our
models outperform previously proposed WordNet-
based preference models and make sensible predic-
tions for out-of-vocabulary items. A second motiva-
tion, which we intend to explore in future work, is
to apply our models in the context of a word sense
disambiguation task. Previous studies have demon-
strated the effectiveness of distributional Bayesian
selectional preference models for predicting lexical
substitutes (O? Se?aghdha and Korhonen, 2011) but
these models lack a principled way to map a word
onto its most likely WordNet sense. The methods
presented in this paper offer a promising solution to
this issue. Another potential research direction is in-
tegration of semantic relation extraction algorithms
with WordNet or other lexical resources, along the
lines of Pennacchiotti and Pantel (2006) and Van
Durme et al (2009).
Acknowledgements
The work in this paper was funded by the EP-
SRC (UK) grant EP/G051070/1, EU grant 7FP-ITC-
248064 and the Royal Society, (UK).
References
Steven Abney and Marc Light. 1999. Hiding a semantic
hierarchy in a Markov model. In Proceedings of the
ACL-99 Workshop on Unsupervised Learning in NLP,
College Park, MD.
177
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preferences
from unlabeled text. In Proceedings of EMNLP-08,
Honolulu, HI.
David M. Blei, Thomas L. Griffiths, Michael I. Jordan,
and Joshua B. Tenenbaum. 2003a. Hierarchical topic
models and the nested Chinese Restaurant Process. In
Proceedings of NIPS-03, Vancouver, BC.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003b. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In Pro-
ceedings of EMNLP-CoNLL-07, Prague, Czech Re-
public.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the ACL-06 Interactive Presentation Sessions,
Sydney, Australia.
Carsten Brockmann and Mirella Lapata. 2003. Evalu-
ating and combining approaches to selectional pref-
erence acquisition. In Proceedings of EACL-03, Bu-
dapest, Hungary.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of NIPS-09, Vancouver, BC.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2), 187?206.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723?763.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl. 1):5228?5235.
Frank Keller and Mirella Lapata. 2003. Using the Web to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically
acquired selectional preferences. Computational Lin-
guistics, 29(4):639?654.
Xiao-Li Meng, Robert Rosenthal, and Donald B. Rubin.
1992. Comparing correlated correlation coefficients.
Psychological Bulletin, 111(1):172?175.
David Mimno, Wei Li, and Andrew McCallum. 2007.
Mixtures of hierarchical topics with Pachinko alloca-
tion. In Proceedings of ICML-07, Corvallis, OR.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31(3):705?767.
Diarmuid O? Se?aghdha and Anna Korhonen. 2011. Prob-
abilistic models of similarity in syntactic context. In
Proceedings of EMNLP-11, Edinburgh, UK.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of ACL-10,
Uppsala, Sweden.
Marco Pennacchiotti and Patrick Pantel. 2006. Ontolo-
gizing semantic relations. In Proceedings of COLING-
ACL-06, Sydney, Australia.
Keith Rayner, Tessa Warren, Barbara J. Juhasz, and Si-
mon P. Liversedge. 2004. The effect of plausibil-
ity on eye movements in reading. Journal of Experi-
mental Psychology: Learning Memory and Cognition,
30(6):1290?1301.
Joseph Reisinger and Raymond Mooney. 2011. Cross-
cutting models of lexical semantics. In Proceedings of
EMNLP-11, Edinburgh, UK.
Joseph Reisinger and Marius Pas?ca. 2009. Latent vari-
able models of concept-attribute attachment. In Pro-
ceedings of ACL-IJCNLP-09, Suntec, Singapore.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent Dirichlet alocation method for selectional prefer-
ences. In Proceedings ACL-10, Uppsala, Sweden.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. 2008. Combining EM
training and the MDL principle for an automatic verb
classification incorporating selectional preferences. In
Proceedings of ACL-08:HLT, Columbus, OH.
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Proceedings of
NAACL-HLT-10, Los Angeles, CA.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of ACL-10, Uppsala, Sweden.
Benjamin Van Durme, Philip Michalak, and Lenhart K.
Schubert. 2009. Deriving generalized knowledge
from corpora using WordNet abstraction. In Proceed-
ings of EACL-09, Athens, Greece.
Hanna Wallach. 2008. Structured Topic Models for Lan-
guage. Ph.D. thesis, University of Cambridge.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11:197?225.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
178
generative models. In Proceedings of EMNLP-11, Ed-
inburgh, UK.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez. 2009.
Generalizing over lexical features: Selectional prefer-
ences for semantic role classification. In Proceedings
of ACL-IJCNLP-09, Singapore.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of ACL-11, Portland, OR.
179
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 138?143, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 4: Free Paraphrases of Noun Compounds
Iris Hendrickx
Radboud University Nijmegen &
Universidade de Lisboa
iris@clul.ul.pt
Preslav Nakov
QCRI, Qatar Foundation
pnakov@qf.org.qa
Stan Szpakowicz
University of Ottawa &
Polish Academy of Sciences
szpak@eecs.uottawa.ca
Zornitsa Kozareva
University of Southern California
kozareva@isi.edu
Diarmuid O? Se?aghdha
University of Cambridge
do242@cam.ac.uk
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
In this paper, we describe SemEval-2013 Task
4: the definition, the data, the evaluation and
the results. The task is to capture some of the
meaning of English noun compounds via para-
phrasing. Given a two-word noun compound,
the participating system is asked to produce
an explicitly ranked list of its free-form para-
phrases. The list is automatically compared
and evaluated against a similarly ranked list
of paraphrases proposed by human annota-
tors, recruited and managed through Ama-
zon?s Mechanical Turk. The comparison of
raw paraphrases is sensitive to syntactic and
morphological variation. The ?gold? ranking
is based on the relative popularity of para-
phrases among annotators. To make the rank-
ing more reliable, highly similar paraphrases
are grouped, so as to downplay superficial dif-
ferences in syntax and morphology. Three
systems participated in the task. They all beat
a simple baseline on one of the two evalua-
tion measures, but not on both measures. This
shows that the task is difficult.
1 Introduction
A noun compound (NC) is a sequence of nouns
which act as a single noun (Downing, 1977), as in
these examples: colon cancer, suppressor protein,
tumor suppressor protein, colon cancer tumor sup-
pressor protein, etc. This type of compounding is
highly productive in English. NCs comprise 3.9%
and 2.6% of all tokens in the Reuters corpus and the
British National Corpus (BNC), respectively (Bald-
win and Tanaka, 2004).
The frequency spectrum of compound types fol-
lows a Zipfian distribution (O? Se?aghdha, 2008), so
many NC tokens belong to a ?long tail? of low-
frequency types. More than half of the two-noun
types in the BNC occur exactly once (Kim and Bald-
win, 2006). Their high frequency and high produc-
tivity make robust NC interpretation an important
goal for broad-coverage semantic processing of En-
glish texts. Systems which ignore NCs may give up
on salient information about the semantic relation-
ships implicit in a text. Compositional interpretation
is also the only way to achieve broad NC coverage,
because it is not feasible to list in a lexicon all com-
pounds which one is likely to encounter. Even for
relatively frequent NCs occurring 10 times or more
in the BNC, static English dictionaries provide only
27% coverage (Tanaka and Baldwin, 2003).
In many natural language processing applications
it is important to understand the syntax and seman-
tics of NCs. NCs often are structurally similar,
but have very different meaning. Consider caffeine
headache and ice-cream headache: a lack of caf-
feine causes the former, an excess of ice-cream ? the
latter. Different interpretations can lead to different
inferences, query expansion, paraphrases, transla-
tions, and so on. A question answering system may
have to determine whether protein acting as a tumor
suppressor is an accurate paraphrase for tumor sup-
pressor protein. An information extraction system
might need to decide whether neck vein thrombosis
and neck thrombosis can co-refer in the same doc-
ument. A machine translation system might para-
phrase the unknown compound WTO Geneva head-
quarters as WTO headquarters located in Geneva.
138
Research on the automatic interpretation of NCs
has focused mainly on common two-word NCs. The
usual task is to classify the semantic relation under-
lying a compound with either one of a small number
of predefined relation labels or a paraphrase from an
open vocabulary. Examples of the former take on
classification include (Moldovan et al, 2004; Girju,
2007; O? Se?aghdha and Copestake, 2008; Tratz and
Hovy, 2010). Examples of the latter include (Nakov,
2008b; Nakov, 2008a; Nakov and Hearst, 2008; But-
nariu and Veale, 2008) and a previous NC paraphras-
ing task at SemEval-2010 (Butnariu et al, 2010),
upon which the task described here builds.
The assumption of a small inventory of prede-
fined relations has some advantages ? parsimony and
generalization ? but at the same time there are lim-
itations on expressivity and coverage. For exam-
ple, the NCs headache pills and fertility pills would
be assigned the same semantic relation (PURPOSE)
in most inventories, but their relational semantics
are quite different (Downing, 1977). Furthermore,
the definitions given by human subjects can involve
rich and specific meanings. For example, Down-
ing (1977) reports that a subject defined the NC
oil bowl as ?the bowl into which the oil in the en-
gine is drained during an oil change?, compared to
which a minimal interpretation bowl for oil seems
very reductive. In view of such arguments, linguists
such as Downing (1977), Ryder (1994) and Coulson
(2001) have argued for a fine-grained, essentially
open-ended space of interpretations.
The idea of working with fine-grained para-
phrases for NC semantics has recently grown in pop-
ularity among NLP researchers (Butnariu and Veale,
2008; Nakov and Hearst, 2008; Nakov, 2008a). Task
9 at SemEval-2010 (Butnariu et al, 2010) was de-
voted to this methodology. In that previous work,
the paraphrases provided by human subjects were
required to fit a restrictive template admitting only
verbs and prepositions occurring between the NC?s
constituent nouns. Annotators recruited through
Amazon Mechanical Turk were asked to provide
paraphrases for the dataset of NCs. The gold stan-
dard for each NC was the ranked list of paraphrases
given by the annotators; this reflects the idea that a
compound?s meaning can be described in different
ways, at different levels of granularity and capturing
different interpretations in the case of ambiguity.
For example, a plastic saw could be a saw made
of plastic or a saw for cutting plastic. Systems par-
ticipating in the task were given the set of attested
paraphrases for each NC, and evaluated according to
how well they could reproduce the humans? ranking.
The design of this task, SemEval-2013 Task 4,
is informed by previous work on compound anno-
tation and interpretation. It is also influenced by
similar initiatives, such as the English Lexical Sub-
stitution task at SemEval-2007 (McCarthy and Nav-
igli, 2007), and by various evaluation exercises in
the fields of paraphrasing and machine translation.
We build on SemEval-2010 Task 9, extending the
task?s flexibility in a number of ways. The restric-
tions on the form of annotators? paraphrases was re-
laxed, giving us a rich dataset of close-to-freeform
paraphrases (Section 3). Rather than ranking a set of
attested paraphrases, systems must now both gener-
ate and rank their paraphrases; the task they perform
is essentially the same as what the annotators were
asked to do. This new setup required us to innovate
in terms of evaluation measures (Section 4).
We anticipate that the dataset and task will be of
broad interest among those who study lexical se-
mantics. We believe that the overall progress in the
field will significantly benefit from a public-domain
set of free-style NC paraphrases. That is why our
primary objective is the challenging endeavour of
preparing and releasing such a dataset to the re-
search community. The common evaluation task
which we establish will also enable researchers to
compare their algorithms and their empirical results.
2 Task description
This is an English NC interpretation task, which ex-
plores the idea of interpreting the semantics of NCs
via free paraphrases. Given a noun-noun compound
such as air filter, the participating systems are asked
to produce an explicitly ranked list of free para-
phrases, as in the following example:
1 filter for air
2 filter of air
3 filter that cleans the air
4 filter which makes air healthier
5 a filter that removes impurities from the air
. . .
139
Such a list is then automatically compared and
evaluated against a similarly ranked list of para-
phrases proposed by human annotators, recruited
and managed via Amazon?s Mechanical Turk. The
comparison of raw paraphrases is sensitive to syn-
tactic and morphological variation. The ranking
of paraphrases is based on their relative popular-
ity among different annotators. To make the rank-
ing more reliable, highly similar paraphrases are
grouped so as to downplay superficial differences in
syntax and morphology.
3 Data collection
We used Amazon?s Mechanical Turk service to
collect diverse paraphrases for a range of ?gold-
standard? NCs.1 We paid the workers a small fee
($0.10) per compound, for which they were asked to
provide five paraphrases. Each paraphrase should
contain the two nouns of the compound (in sin-
gular or plural inflectional forms, but not in an-
other derivational form), an intermediate non-empty
linking phrase and optional preceding or following
terms. The paraphrasing terms could have any part
of speech, so long as the resulting paraphrase was a
well-formed noun phrase headed by the NC?s head.
We gave the workers feedback during data col-
lection if they appeared to have misunderstood the
nature of the task. Once raw paraphrases had been
collected from all workers, we collated them into a
spreadsheet, and we merged identical paraphrases
in order to calculate their overall frequencies. Ill-
formed paraphrases ? those violating the syntactic
restrictions described above ? were manually re-
moved following a consensus decision-making pro-
cedure; every paraphrase was checked by at least
two task organizers. We did not require that the
paraphrases be semantically felicitous, but we per-
formed minor edits on the remaining paraphrases if
they contained obvious typos.
The remaining well-formed paraphrases were
sorted by frequency separately for each NC. The
most frequent paraphrases for a compound are as-
signed the highest rank 0, those with the next-
highest frequency are given a rank of 1, and so on.
1Since the annotation on Mechanical Turk was going slowly,
we also recruited four other annotators to do the same work,
following exactly the same instructions.
Total Min / Max / Avg
Trial/Train (174 NCs)
paraphrases 6,069 1 / 287 / 34.9
unique paraphrases 4,255 1 / 105 / 24.5
Test (181 NCs)
paraphrases 9,706 24 / 99 / 53.6
unique paraphrases 8,216 21 / 80 / 45.4
Table 1: Statistics of the trial and test datasets: the total
number of paraphrases with and without duplicates, and
the minimum / maximum / average per noun compound.
Paraphrases with a frequency of 1 ? proposed for
a given NC by only one annotator ? always occupy
the lowest rank on the list for that compound.
We used 174+181 noun-noun compounds from
the NC dataset of O? Se?aghdha (2007). The trial
dataset, which we initially released to the partici-
pants, consisted of 4,255 human paraphrases for 174
noun-noun pairs; this dataset was also the training
dataset. The test dataset comprised paraphrases for
181 noun-noun pairs. The ?gold standard? contained
9,706 paraphrases of which 8,216 were unique for
those 181 NCs. Further statistics on the datasets are
presented in Table 1.
Compared with the data collected for the
SemEval-2010 Task 9 on the interpretation of noun
compounds, the data collected for this new task have
a far greater range of variety and richness. For ex-
ample, the following (selected) paraphrases for work
area vary from parsimonious to expansive:
? area for work
? area of work
? area where work is done
? area where work is performed
? . . .
? an area cordoned off for persons responsible for
work
? an area where construction work is carried out
? an area where work is accomplished and done
? area where work is conducted
? office area assigned as a work space
? . . .
140
4 Scoring
Noun compounding is a generative aspect of lan-
guage, but so too is the process of NC interpretation:
human speakers typically generate a range of possi-
ble interpretations for a given compound, each em-
phasizing a different aspect of the relationship be-
tween the nouns. Our evaluation framework reflects
the belief that there is rarely a single right answer
for a given noun-noun pairing. Participating systems
are thus expected to demonstrate some generativity
of their own, and are scored not just on the accu-
racy of individual interpretations, but on the overall
breadth of their output.
For evaluation, we provided a scorer imple-
mented, for good portability, as a Java class. For
each noun compound to be evaluated, the scorer
compares a list of system-suggested paraphrases
against a ?gold-standard? reference list, compiled
and rank-ordered from the paraphrases suggested
by our human annotators. The score assigned to
each system is the mean of the system?s performance
across all test compounds. Note that the scorer re-
moves all determiners from both the reference and
the test paraphrases, so a system is neither punished
for not reproducing a determiner or rewarded for
producing the same determiners.
The scorer can match words identically or non-
identically. A match of two identical words Wgold
and Wtest earns a score of 1.0. There is a partial
score of (2 |P | / (|PWgold| + |PWtest|))2 for a
match of two words PWgold and PWtest that are
not identical but share a common prefix P , |P | > 2,
e.g., wmatch(cutting, cuts) = (6/11)2 = 0.297.
Two n-grams Ngold = [GW1, . . . , GWn] and
Ntest = [TW1, . . . , TWn] can be matched if
wmatch(GWi, TWi) > 0 for all i in 1..n. The
score assigned to the match of these two n-grams is
then
?
i wmatch(GWi, TWi). For every n-gram
Ntest = [TW1, . . . , TWn] in a system-generated
paraphrase, the scorer finds a matching n-gram
Ngold = [GW1, . . . , GWn] in the reference para-
phrase Paragold which maximizes this sum.
The overall n-gram overlap score for a reference
paraphrase Paragold and a system-generated para-
phrase Paratest is the sum of the score calculated
for all n-grams in Paratest, where n ranges from 1
to the size of Paratest.
This overall score is then normalized by dividing
by the maximum value among the n-gram overlap
score for Paragold compared with itself and the n-
gram overlap score for Paratest compared with it-
self. This normalization step produces a paraphrase
match score in the range [0.0 ? 1.0]. It punishes a
paraphrase Paratest for both over-generating (con-
taining more words than are found in Paragold)
and under-generating (containing fewer words than
are found in Paragold). In other words, Paratest
should ideally reproduce everything in Paragold,
and nothing more or less.
The reference paraphrases in the ?gold standard?
are ordered by rank; the highest rank is assigned to
the paraphrases which human judges suggested most
often. The rank of a reference paraphrase matters
because a good participating system will aim to re-
produce the top-ranked ?gold-standard? paraphrases
as produced by human judges. The scorer assigns
a multiplier of R/(R + n) to reference paraphrases
at rank n; this multiplier asymptotically approaches
0 for the higher values of n of ever lower-ranked
paraphrases. We choose a default setting of R = 8,
so that a reference paraphrase at rank 0 (the highest
rank) has a multiplier of 1, while a reference para-
phrase at rank 5 has a multiplier of 8/13 = 0.615.
When a system-generated paraphrase Paratest is
matched with a reference paraphrase Paragold, their
normalized n-gram overlap score is scaled by the
rank multiplier attaching to the rank of Paragold rel-
ative to the other reference paraphrases provided by
human judges. The scorer automatically chooses the
reference paraphrase Paragold for a test paraphrase
Paratest so as to maximize this product of normal-
ized n-gram overlap score and rank multiplier.
The overall score assigned to each system for
a specific compound is calculated in two differ-
ent ways: using isomorphic matching of suggested
paraphrases to the ?gold-standard?s? reference para-
phrases (on a one-to-one basis); and using non-
isomorphic matching of system?s paraphrases to the
?gold-standard?s? reference paraphrases (in a poten-
tially many-to-one mapping).
Isomorphic matching rewards both precision and
recall. It rewards a system for accurately reproduc-
ing the paraphrases suggested by human judges, and
for reproducing as many of these as it can, and in
much the same order.
141
In isomorphic mode, system?s paraphrases are
matched 1-to-1 with reference paraphrases on a first-
come first-matched basis, so ordering can be crucial.
Non-isomorphic matching rewards only preci-
sion. It rewards a system for accurately reproducing
the top-ranked human paraphrases in the ?gold stan-
dard?. A system will achieve a higher score in a non-
isomorphic match if it reproduces the top-ranked hu-
man paraphrases as opposed to lower-ranked human
paraphrases. The ordering of system?s paraphrases
is thus not important in non-isomorphic matching.
Each system is evaluated using the scorer in both
modes, isomorphic and non-isomorphic. Systems
which aim only for precision should score highly
on non-isomorphic match mode, but poorly in iso-
morphic match mode. Systems which aim for pre-
cision and recall will face a more substantial chal-
lenge, likely reflected in their scores.
A na??ve baseline
We decided to allow preposition-only paraphrases,
which are abundant in the paraphrases suggested
by human judges in the crowdsourcing Mechanical
Turk collection process. This abundance means that
the top-ranked paraphrase for a given compound is
often a preposition-only phrase, or one of a small
number of very popular paraphrases such as used for
or used in. It is thus straightforward to build a na??ve
baseline generator which we can expect to score
reasonably on this task, at least in non-isomorphic
matching mode. For each test compound M H,
the baseline system generates the following para-
phrases, in this precise order: H of M, H in M, H
for M, H with M, H on M, H about M, H has M, H to
M, H used for M, H used in M.
This na??ve baseline is truly unsophisticated. No
attempt is made to order paraphrases by their corpus
frequencies or by their frequencies in the training
data. The same sequence of paraphrases is generated
for each and every test compound.
5 Results
Three teams participated in the challenge, and all
their systems were supervised. The MELODI sys-
tem relied on semantic vector space model built
from the UKWAC corpus (window-based, 5 words).
It used only the features of the right-hand head noun
to train a maximum entropy classifier.
Team isomorphic non-isomorphic
SFS 23.1 17.9
IIITH 23.1 25.8
MELODI-Primary 13.0 54.8
MELODI-Contrast 13.6 53.6
Naive Baseline 13.8 40.6
Table 2: Results for the participating systems; the base-
line outputs the same paraphrases for all compounds.
The IIITH system used the probabilities of the
preposition co-occurring with a relation to identify
the class of the noun compound. To collect statis-
tics, it used Google n-grams, BNC and ANC.
The SFS system extracted templates and fillers
from the training data, which it then combined with
a four-gram language model and a MaxEnt reranker.
To find similar compounds, they used Lin?s Word-
Net similarity. They further used statistics from the
English Gigaword and the Google n-grams.
Table 2 shows the performance of the partici-
pating systems, SFS, IIITH and MELODI, and the
na??ve baseline. The baseline shows that it is rela-
tively easy to achieve a moderately good score in
non-isomorphic match mode by generating a fixed
set of paraphrases which are both common and
generic: two of the three participating systems,
SFS and IIITH, under-perform the na??ve baseline
in non-isomorphic match mode, but outperform it
in isomorphic mode. The only system to surpass
this baseline in non-isomorphic match mode is the
MELODI system; yet, it under-performs against the
same baseline in isomorphic match mode. No par-
ticipating team submitted a system which would out-
perform the na??ve baseline in both modes.
6 Conclusions
The conclusions we draw from the experience of or-
ganizing the task are mixed. Participation was rea-
sonable but not large, suggesting that NC paraphras-
ing remains a niche interest ? though we believe it
deserves more attention among the broader lexical
semantics community and hope that the availabil-
ity of our freeform paraphrase dataset will attract a
wider audience in the future.
142
We also observed a varied response from our an-
notators in terms of embracing their freedom to gen-
erate complex and rich paraphrases; there are many
possible reasons for this including laziness, time
pressure and the fact that short paraphrases are often
very appropriate paraphrases. The results obtained
by our participants were also modest, demonstrating
that compound paraphrasing is both a difficult task
and a novel one that has not yet been ?solved?.
Acknowledgments
This work has partially supported by a small but ef-
fective grant from Amazon; the credit allowed us
to hire sufficiently many Turkers ? thanks! And a
thank-you to our additional annotators Dave Carter,
Chris Fournier and Colette Joubarne for their com-
plete sets of paraphrases of the noun compounds in
the test data.
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of complex nominals: Getting it right.
Proc. ACL04 Workshop on Multiword Expressions: In-
tegrating Processing, Barcelona, Spain, 24-31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK, 81-
88.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. SemEval-2010 Task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. Proc. 5th International ACL Workshop on Se-
mantic Evaluation, Uppsala, Sweden, 39-44.
Seana Coulson. 2001. Semantic Leaps: Frame-Shifting
and Conceptual Blending in Meaning Construction.
Cambridge University Press, Cambridge, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4): 810-842.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. Proc.
45th Annual Meeting of the Association of Computa-
tional Linguistics, Prague, Czech Republic, 568-575.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting
semantic relations in noun compounds via verb seman-
tics. Proc. ACL-06 Main Conference Poster Session,
Sydney, Australia, 491-498.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. Proc.
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), Prague, Czech Republic, 48-53.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. Dan Moldovan
and Roxana Girju, eds., HLT-NAACL 2004: Workshop
on Computational Lexical Semantics, Boston, MA,
USA, 60-67.
Preslav Nakov and Marti Hearst. 2008. Solving rela-
tional similarity problems using the Web as a corpus.
Proc. 46th Annual Meeting of the Association for Com-
putational Linguistics ACL-08, Columbus, OH, USA,
452-460.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. Proc. 18th
European Conference on Artificial Intelligence ECAI-
08, Patras, Greece, 338-342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. Proc.
13th International Conference on Artificial Intelli-
gence: Methodology, Systems, Applications AIMSA-
08, Varna, Bulgaria, Lecture Notes in Computer Sci-
ence 5253, Springer, 103-117.
Diarmuid O? Se?aghdha. 2007. Designing and Evaluating
a Semantic Annotation Scheme for Compound Nouns.
In Proceedings of the 4th Corpus Linguistics Confer-
ence, Birmingham, UK.
Diarmuid O? Se?aghdha. 2008. Learning compound
noun semantics. Ph.D. thesis, Computer Laboratory,
University of Cambridge. Published as University
of Cambridge Computer Laboratory Technical Report
735.
Diarmuid O? Se?aghdha and Ann Copestake. 2009. Using
lexical and relational similarity to classify semantic re-
lations. Proc. 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
EACL-09, Athens, Greece, 621-629.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA, USA.
Takaaki Tanaka and Tim Baldwin. 2003. Noun-noun
compound machine translation: A feasibility study
on shallow processing. Proc. ACL-2003 Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan, 17-24.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. Proc. 48th Annual Meeting of the As-
sociation for Computational Linguistics ACL-10, Up-
psala, Sweden, 678-687.
143
