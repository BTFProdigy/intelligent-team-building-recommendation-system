Proceedings of the 12th Conference of the European Chapter of the ACL, pages 60?68,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Correcting Automatic Translations through Collaborations between MT
and Monolingual Target-Language Users
Joshua S. Albrecht and Rebecca Hwa and G. Elisabeta Marai
Department of Computer Science
University of Pittsburgh
{jsa8,hwa,marai}@cs.pitt.edu
Abstract
Machine translation (MT) systems have
improved significantly; however, their out-
puts often contain too many errors to com-
municate the intended meaning to their
users. This paper describes a collabora-
tive approach for mediating between an
MT system and users who do not under-
stand the source language and thus cannot
easily detect translation mistakes on their
own. Through a visualization of multi-
ple linguistic resources, this approach en-
ables the users to correct difficult transla-
tion errors and understand translated pas-
sages that were otherwise baffling.
1 Introduction
Recent advances in machine translation (MT) have
given us some very good translation systems.
They can automatically translate between many
languages for a variety of texts; and they are
widely accessible to the public via the web. The
quality of the MT outputs, however, is not reliably
high. People who do not understand the source
language may be especially baffled by the MT out-
puts because they have little means to recover from
translation mistakes.
The goal of this work is to help monolingual
target-language users to obtain better translations
by enabling them to identify and overcome er-
rors produced by the MT system. We argue for a
human-computer collaborative approach because
both the users and the MT system have gaps in
their abilities that the other could compensate. To
facilitate this collaboration, we propose an inter-
face that mediates between the user and the MT
system. It manages additional NLP tools for the
source language and translation resources so that
the user can explore this extra information to gain
enough understanding of the source text to correct
MT errors. The interactions between the users and
the MT system may, in turn, offer researchers in-
sights into the translation process and inspirations
for better translation models.
We have conducted an experiment in which we
asked non-Chinese speakers to correct the outputs
of a Chinese-English MT system for several short
passages of different genres. They performed the
correction task both with the help of the visual-
ization interface and without. Our experiment ad-
dresses the following questions:
? To what extent can the visual interface help
the user to understand the source text?
? In what way do factors such as the user?s
backgrounds, the properties of source text,
and the quality of the MT system and other
NLP resources impact that understanding?
? What resources or strategies are more help-
ful to the users? What research directions
do these observations suggest in terms of im-
proving the translation models?
Through qualitative and quantitative analysis of
the user actions and timing statistics, we have
found that users of the interface achieved a more
accurate understanding of the source texts and
corrected more difficult translation mistakes than
those who were given the MT outputs alone. Fur-
thermore, we observed that some users made bet-
ter use of the interface for certain genres, such
as sports news, suggesting that the translation
model may be improved by a better integration of
document-level contexts.
60
2 Collaborative Translation
The idea of leveraging human-computer collab-
orations to improve MT is not new; computer-
aided translation, for instance, was proposed by
Kay (1980). The focus of these efforts has been on
improving the performance of professional trans-
lators. In contrast, our intended users cannot read
the source text.
These users do, however, have the world knowl-
edge and the language model to put together co-
herent sentences in the target-language. From the
MT research perspective, this raises an interesting
question: given that they are missing a transla-
tion model, what would it take to make these users
into effective ?decoders?? While some transla-
tion mistakes are recoverable from a strong lan-
guage model alone, and some might become read-
ily apparent if one can choose from some possi-
ble phrasal translations; the most difficult mistakes
may require greater contextual knowledge about
the source. Consider the range of translation re-
sources available to an MT decoder?which ones
might the users find informative, handicapped as
they are for not knowing the source language?
Studying the users? interactions with these re-
sources may provide insights into how we might
build a better translation model and a better de-
coder.
In exploring the collaborative approach, the de-
sign considerations for facilitating human com-
puter interaction are crucial. We chose to make
available relatively few resources to prevent the
users from becoming overwhelmed by the options.
We also need to determine how to present the in-
formation from the resources so that the users can
easily interpret them. This is a challenge because
the Chinese processing tools and the translation
resources are imperfect themselves. The informa-
tion should be displayed in such a way that con-
flicting analyses between different resources are
highlighted.
3 Prototype Design
We present an overview of our prototype for a col-
laborative translation interface, named The Chi-
nese Room1. A screen-shot is shown in Figure 1. It
1The inspiration for the name of our system came from
Searle?s thought experiment(Searle, 1980). We realize that
there are major differences between our system and Searle?s
description. Importantly, our users get to insert their knowl-
edge rather than purely operate based on instructions. We felt
Figure 1: A screen-shot of the visual interface. It
consists of two main regions. The left pane is a
workspace for users to explore the sentence; the
right pane provides multiple tabs that offer addi-
tional functionalities.
is a graphical environment that supports five main
sources of information and functionalities. The
space separates into two regions. On the left pane
is a large workspace for the user to explore the
source text one sentence at a time. On the right
pane are tabbed panels that provide the users with
access to a document view of the MT outputs as
well as additional functionalities for interpreting
the source. In our prototype, the MT output is ob-
tained by querying Google?s Translation API2. In
the interest of exploiting user interactions as a di-
agnostic tool for improving MT, we chose infor-
mation sources that are commonly used by mod-
ern MT systems.
First, we display the word alignments between
MT output and segmented Chinese3. Even with-
out knowing the Chinese characters, the users
can visually detect potential misalignments and
poor word reordering. For instance, the automatic
translation shown in Figure 1 begins: Two years
ago this month... It is fluent but incorrect. The
crossed alignments offer users a clue that ?two?
and ?months? should not have been split up. Users
can also explore alternative orderings by dragging
the English tokens around.
Second, we make available the glosses for
words and characters from a bilingual dictionary4.
the name was nonetheless evocative in that the user requires
additional resources to process the input ?squiggles.?
2http://code.google.com/apis/translate/
research
3The Chinese segmentation is obtained as a by-product of
Google?s translation process.
4We used the Chinese-English Translation Lexi-
61
The placement of the word gloss presents a chal-
lenge because there are often alternative Chi-
nese segmentations. We place glosses for multi-
character words in the column closer to the source.
When the user mouses over each definition, the
corresponding characters are highlighted, helping
the user to notice potential mis-segmentation in
the Chinese.
Third, the Chinese sentence is annotated with
its parse structure5. Constituents are displayed
as brackets around the source sentence. They
have been color-coded into four major types (noun
phrase, verb phrases, prepositional phrases, and
other). Users can collapse and expand the brack-
ets to keep the workspace uncluttered as they work
through the Chinese sentence. This also indicates
to us which fragments held the user?s focus.
Fourth, based on previous studies reporting
that automatic translations may improve when
given decomposed source inputs (Mellebeek et al,
2005), we allow the users to select a substring
from the source text for the MT system to trans-
late. We display the N -best alternatives in the
Translation Tab. The list is kept short; its purpose
is less for reranking but more to give the users a
sense of the kinds of hypotheses that the MT sys-
tem is considering.
Fifth, users can select a substring from the
source text and search for source sentences from
a bilingual corpus and a monolingual corpus that
contain phrases similar to the query6. The re-
trieved sentences are displayed in the Example
Tab. For sentences from the bilingual corpus, hu-
man translations for the queried phrase are high-
lighted. For sentences retrieved from the monolin-
gual corpus, their automatic translations are pro-
vided. If the users wished to examine any of the
retrieved translation pairs in detail, they can push
it onto the sentence workspace.
4 Experimental Methodology
We asked eight non-Chinese speakers to correct
the machine translations of four short Chinese pas-
con released by the LDC; for a handful of char-
acters that serve as function words, we added the
functional definitions using an online dictionary
http://www.mandarintools.com/worddict.html.
5It is automatically generated by the Stanford Parser for
Chinese (Klein and Manning, 2003).
6We used Lemur (2006) for the information retrieval
back-end; the parallel corpus is from the Federal Broadcast
Information Service corpus; the monolingual corpus is from
the Chinese Gigaword corpus.
Figure 2: The interface for users who are correct-
ing translations without help; they have access to
the document view, but they do not have access to
any of the other resources.
sages, with an average length of 11.5 sentences.
Two passages are news articles and two are ex-
cerpts of a fictional work. Each participant was
instructed to correct the translations for one news
article and one fictional passage using all the re-
sources made available by The Chinese Room and
the other two passages without. To keep the ex-
perimental conditions as similar as possible, we
provided them with a restricted version of the in-
terface (see Figure 2 for a screen-shot) in which all
additional functionalities except for the Document
View Tab are disabled. We assigned each person
to alternate between working with the full and the
restricted versions of the system; half began with-
out, and the others began with. Thus, every pas-
sage received four sets of corrections made collab-
oratively with the system and four sets of correc-
tions made based solely on the participants? inter-
nal language models. All together, there are 184
participant corrected sentences (11.5 sentences ?
4 passages ? 4 participants) for each condition.
The participants were asked to complete each
passage in one sitting. Within a passage, they
could work on the sentences in any arbitrary order.
They could also elect to ?pass? any part of a sen-
tence if they found it too difficult to correct. Tim-
ing statistics were automatically collected while
they made their corrections. We interviewed each
participant for qualitative feedbacks after all four
passages were corrected.
Next, we asked two bilingual speakers to eval-
uate all the corrected translations. The outcomes
between different groups of users are compared,
62
and the significance of the difference is deter-
mined using the two-sample t-test assuming un-
equal variances. We require 90% confidence (al-
pha=0.1) as the cut-off for a difference to be con-
sidered statistically significant; when the differ-
ence can be established with higher confidence,
we report that value. In the following subsections,
we describe the conditions of this study in more
details.
Participants? Background For this study, we
strove to maintain a relatively heterogeneous pop-
ulation; participants were selected to be varied in
their exposures to NLP, experiences with foreign
languages, as well as their age and gender. A sum-
mary of their backgrounds is shown in Table 1.
Prior to the start of the study, the participants
received a 20 minute long presentational tutorial
about the basic functionalities supported by our
system, but they did not have an opportunity to ex-
plore the system on their own. This helps us to de-
termine whether our interface is intuitive enough
for new users to pick up quickly.
Data The four passages used for this study were
chosen to span a range of difficulties and genre
types. The easiest of the four is a news arti-
cle about a new Tamagotchi-like product from
Bandai. It was taken from a webpage that offers
bilingual news to help Chinese students to learn
English. A harder news article is taken from a
past NIST Chinese-English MT Evaluation; it is
about Michael Jordan?s knee injury. For a dif-
ferent genre, we considered two fictional excerpts
from the first chapter of Martin Eden, a novel by
Jack London that has been professionally trans-
lated into Chinese7. One excerpt featured a short
dialog, while the other one was purely descriptive.
Evaluation of Translations Bilingual human
judges are presented with the source text as well as
the parallel English text for reference. Each judge
is then shown a set of candidate translations (the
original MT output, an alternative translation by
a bilingual speaker, and corrected translations by
the participants) in a randomized order. Since the
human corrected translations are likely to be flu-
ent, we have instructed the judges to concentrate
more on the adequacy of the meaning conveyed.
They are asked to rate each sentence on an abso-
7We chose an American story so as to not rely on a
user?s knowledge about Chinese culture. The participants
confirmed that they were not familiar with the chosen story.
Table 2: The guideline used by bilingual judges
for evaluating the translation quality of the MT
outputs and the participants? corrections.
9-10 The meaning of the Chinese sentence
is fully conveyed in the translation.
7-8 Most of the meaning is conveyed.
5-6 Misunderstands the sentence in a
major way; or has many small mistakes.
3-4 Very little meaning is conveyed.
1-2 The translation makes no sense at all.
lute scale of 1-10 using the guideline in Table 2.
To reduce the biases in the rating scales of differ-
ent judges, we normalized the judges? scores, fol-
lowing standard practices in MT evaluation (Blatz
et al, 2003). Post normalization, the correlation
coefficient between the judges is 0.64. The final
assessment score for each translated sentence is
the average of judges? scores, on a scale of 0-1.
5 Results
The results of human evaluations for the user ex-
periment are summarized in Table 3, and the corre-
sponding timing statistics (average minutes spent
editing a sentence) is shown in Table 4. We ob-
served that typical MT outputs contain a range of
errors. Some are primarily problems in fluency
such that the participants who used the restricted
interface, which provided no additional resources
other than the Document View Tab, were still able
to improve the MT quality from 0.35 to 0.42. On
the other hand, there are also a number of more
serious errors that require the participants to gain
some level of understanding of the source in order
to correct them. The participants who had access
to the full collaborative interface were able to im-
prove the quality from 0.35 to 0.53, closing the
gap between the MT and the bilingual translations
by 36.9%. These differences are all statistically
significant (with >98% confidence).
The higher quality of corrections does require
the participants to put in more time. Overall, the
participants took 2.5 times as long when they have
the interface than when they do not. This may be
partly because the participants have more sources
of information to explore and partly because the
participants tended to ?pass? on fewer sentences.
The average Levenshtein edit distance (with words
as the atomic unit, and with the score normalized
to the interval [0,1]) between the original MT out-
63
Table 1: A summary of participants? background. ?User5 recognizes some simple Kanji characters, but
does not have enough knowledge to gain any additional information beyond what the MT system and the
dictionary already provided.
User1 User2 User3 User4 User5? User6 User7 User8
NLP background intro grad none none intro grad intro none
Native English yes no yes yes yes yes yes yes
Other Languages French multiple none none Japanese none none Greek
(beginner) (fluent) (beginner) (beginner)
Gender M F F M M M F M
Education Ugrad PhD PhD Ugrad Ugrad PhD Ugrad Ugrad
puts and the corrected sentences made by partic-
ipants using The Chinese Room is 0.59; in con-
trast, the edit distance is shorter, at 0.40, when par-
ticipants correct MT outputs directly. The timing
statistics are informative, but they reflect the inter-
actions of many factors (e.g., the difficulty of the
source text, the quality of the machine translation,
the background and motivation of the user). Thus,
in the next few subsections, we examine how these
factors correlate with the quality of the participant
corrections.
5.1 Impact of Document Variation
Since the quality of MT varies depending on the
difficulty and genre of the source text, we inves-
tigate how these factors impact our participants?
performances. Columns 3-6 of Table 3 (and Ta-
ble 4) compare the corrected translations on a per-
document basis.
Of the four documents, the baseline MT sys-
tem performed the best on the product announce-
ment. Because the article is straight-forward, par-
ticipants found it relatively easy to guess the in-
tended translation. The major obstacle is in de-
tecting and translating Chinese transliteration of
Japanese names, which stumped everyone. The
quality difference between the two groups of par-
ticipants on this document was not statistically sig-
nificant. Relatedly, the difference in the amount of
time spent is the smallest for this document; par-
ticipants using The Chinese Room took about 1.5
times longer.
The other news article was much more difficult.
The baseline MT made many mistakes, and both
groups of participants spent longer on sentences
from this article than the others. Although sports
news is fairly formulaic, participants who only
read MT outputs were baffled, whereas those who
had access to additional resources were able to re-
cover from MT errors and produced good quality
translations.
Finally, as expected, the two fictional excerpts
were the most challenging. Since the participants
were not given any information about the story,
they also have little context to go on. In both cases,
participants who collaborated with The Chinese
Room made higher quality corrections than those
who did not. The difference is statistically signif-
icant at 97% confidence for the first excerpt, and
93% confidence for the second. The differences in
time spent between the two groups are greater for
these passages because the participants who had
to make corrections without help tended to give
up more often.
5.2 Impact of Participants? Background
We further analyze the results by separating the
participants into two groups according to four
factors: whether they were familiar with NLP,
whether they studied another language, their gen-
der, and their education level.
Exposure to NLP One of our design objectives
for The Chinese Room is accessibility by a diverse
population of end-users, many of whom may not
be familiar with human language technologies. To
determine how prior knowledge of NLP may im-
pact a user?s experience, we analyze the exper-
imental results with respect to the participants?
background. In columns 2 and 3 of Table 5, we
compare the quality of the corrections made by
the two groups. When making corrections on their
own, participants who had been exposed to NLP
held a significant edge (0.35 vs. 0.47). When both
groups of participants used The Chinese Room, the
difference is reduced (0.51 vs. 0.54) and is not sta-
tistically significant. Because all the participants
were given the same short tutorial prior to the start
of the study, we are optimistic that the interface is
intuitive for many users.
None of the other factors distinguished one
64
Table 3: Averaged human judgments of the translation quality of the four different approaches: automatic
MT, corrections by participants without help, corrections by participants using The Chinese Room, and
translation produced by a bilingual speaker. The second column reports score for all documents; columns
3-6 show the per-document scores.
Overall News (product) News (sports) Story1 Story2
Machine translation 0.35 0.45 0.30 0.25 0.26
Corrections without The Chinese Room 0.42 0.56 0.35 0.33 0.41
Corrections with The Chinese Room 0.53 0.55 0.62 0.42 0.49
Bilingual translation 0.83 0.83 0.73 0.92 0.88
Table 4: The average amount of time (minutes) participants spent on correcting a sentence.
Overall News (product) News (sports) Story1 Story2
Corrections without The Chinese Room 2.5 1.9 3.2 2.9 2.3
Corrections with The Chinese Room 6.3 2.9 8.7 6.5 8.5
Table 6: The quality of the corrections produced
by four participants using The Chinese Room for
the sports news article.
User1 0.57
User2 0.46
User5 0.70
User6 0.73
bilingual translator 0.73
group of participants from the others. The results
are summarized in columns 4-9 of Table 5. In each
case, the two groups had similar levels of perfor-
mance, and the differences between their correc-
tions were not statistically significant. This trend
holds for both when they were collaborating with
the system and when editing on their own.
Prior Knowledge Another factor that may im-
pact the success of the outcome is the user?s
knowledge about the domain of the source text.
An example from our study is the sports news ar-
ticle. Table 6 lists the scores that the four partic-
ipants who used The Chinese Room received for
their corrected translations for that passage (aver-
aged over sentences). User5 and User6 were more
familiar with the basketball domain; with the help
of the system, they produced translations that were
comparable to those from the bilingual translator
(the differences are not statistically significant).
5.3 Impact of Available Resources
Post-experiment, we asked the participants to de-
scribe the strategies they developed for collaborat-
ing with the system. Their responses fall into three
main categories:
Figure 3: This graph shows the average counts of
access per sentence for different resources.
Divide and Conquer Some users found the syn-
tactic trees helpful in identifying phrasal units for
N -best re-translations or example searches. For
longer sentences, they used the constituent col-
lapse feature to help them reduce clutter and focus
on a portion of the sentence.
Example Retrieval Using the search interface,
users examined the highlighted query terms to de-
termine whether the MT system made any seg-
mentation errors. Sometimes, they used the exam-
ples to arbitrate whether they should trust any of
the dictionary glosses or the MT?s lexical choices.
Typically, though, they did not attempt to inspect
the example translations in detail.
Document Coherence and Word Glosses
Users often referred to the document view to
determine the context for the sentence they are
editing. Together with the word glosses and other
65
Table 5: A comparison of translation quality, grouped by four characteristics of participant backgrounds:
their level of exposure to NLP, exposure to another language, their gender, and education level.
No NLP NLP No 2nd Lang. 2nd Lang. Female Male Ugrad PhD
without The Chinese Room 0.35 0.47 0.41 0.43 0.41 0.43 0.41 0.45
with The Chinese Room 0.51 0.54 0.56 0.51 0.50 0.55 0.52 0.54
resources, the discourse level clues helped to
guide users to make better lexical choices than
when they made corrections without the full
system, relying on sentence coherence alone.
Figure 3 compares the average access counts
(per sentence) of different resources (aggregated
over all participants and documents). The option
of inspect retrieved examples in detail (i.e., bring
them up on the sentence workspace) was rarely
used. The inspiration for this feature was from
work on translation memory (Macklovitch et al,
2000); however, it was not as informative for our
participants because they experienced a greater de-
gree of uncertainty than professional translators.
6 Discussion
The results suggest that collaborative translation
is a promising approach. Participant experiences
were generally positive. Because they felt like
they understood the translations better, they did
not mind putting in the time to collaborate with
the system. Table 7 shows some of the partici-
pants? outputs. Although there are some transla-
tion errors that cannot be overcome with our cur-
rent system (e.g., transliterated names), the partic-
ipants taken as a collective performed surprisingly
well. For many mistakes, even when the users can-
not correct them, they recognized a problem; and
often, one or two managed to intuit the intended
meaning with the help of the available resources.
As an upper-bound for the effectiveness of the sys-
tem, we construct a combined ?oracle? user out of
all 4 users that used the interface for each sentence.
The oracle user?s average score is 0.70; in contrast,
an oracle of users who did not use the system is
0.54 (cf. the MT?s overall of 0.35 and the bilin-
gual translator?s overall of 0.83). This suggests
The Chinese Room affords a potential for human-
human collaboration as well.
The experiment also made clear some limita-
tions of the current resources. One is domain de-
pendency. Because NLP technologies are typi-
cally trained on news corpora, their bias toward
the news domain may mislead our users. For ex-
ample, there is a Chinese character (pronounced
mei3) that could mean either ?beautiful? or ?the
United States.? In one of the passages, the in-
tended translation should have been: He was re-
sponsive to beauty... but the corresponding MT
output was He was sensitive to the United States...
Although many participants suspected that it was
wrong, they were unable to recover from this mis-
take because the resources (the searchable exam-
ples, the part-of-speech tags, and the MT system)
did not offer a viable alternative. This suggests
that collaborative translation may serve as a useful
diagnostic tool to help MT researchers verify ideas
about what types of models and data are useful in
translation. It may also provide a means of data
collection for MT training. To be sure, there are
important challenges to be addressed, such as par-
ticipation incentive and quality assurance, but sim-
ilar types of collaborative efforts have been shown
fruitful in other domains (Cosley et al, 2007). Fi-
nally, the statistics of user actions may be useful
for translation evaluation. They may be informa-
tive features for developing automatic metrics for
sentence-level evaluations (Kulesza and Shieber,
2004).
7 Related Work
While there have been many successful computer-
aided translation systems both for research and as
commercial products (Bowker, 2002; Langlais et
al., 2000), collaborative translation has not been
as widely explored. Previous efforts such as
DerivTool (DeNeefe et al, 2005) and Linear B
(Callison-Burch, 2005) placed stronger emphasis
on improving MT. They elicited more in-depth in-
teractions between the users and the MT system?s
phrase tables. These approaches may be more ap-
propriate for users who are MT researchers them-
selves. In contrast, our approach focuses on pro-
viding intuitive visualization of a variety of in-
formation sources for users who may not be MT-
savvy. By tracking the types of information they
consulted, the portions of translations they se-
lected to modify, and the portions of the source
66
Table 7: Some examples of translations corrected by the participants and their scores.
Score Translation
MT 0.34 He is being discovered almost hit an arm in the pile of books on the desktop, just
like frightened horse as a Lieju Wangbangbian almost Pengfan the piano stool.
without The Chinese Room 0.26 Startled, he almost knocked over a pile of book on his desk, just like a frightened
horse as a Lieju Wangbangbian almost Pengfan the piano stool.
with The Chinese Room 0.78 He was nervous, and when one of his arms nearly hit a stack of books on the
desktop, he startled like a horse, falling back and almost knocking over the piano
stool.
Bilingual Translator 0.93 Feeling nervous, he discovered that one of his arms almost hit the pile of books
on the table. Like a frightened horse, he stumbled aside, almost turning over a
piano stool.
MT 0.50 Bandai Group, a spokeswoman for the U.S. to be SIN-West said: ?We want to
bring women of all ages that ?the flavor of life?.?
without The Chinese Room 0.67 SIN-West, a spokeswoman for the U.S. Bandai Group declared: ?We want to
bring to women of all ages that ?flavor of life?.?
with The Chinese Room 0.68 West, a spokeswoman for the U.S. Toy Manufacturing Group, and soon to be
Vice President-said: ?We want to bring women of all ages that ?flavor of life?.?
Bilingual Translator 0.75 ?We wanted to let women of all ages taste the ?flavor of life?,? said Bandai?s
spokeswoman Kasumi Nakanishi.
text they attempted to understand, we may alter
the design of our translation model. Our objective
is also related to that of cross-language informa-
tion retrieval (Resnik et al, 2001). This work can
be seen as providing the next step in helping users
to gain some understanding of the information in
the documents once they are retrieved.
By facilitating better collaborations between
MT and target-language readers, we can naturally
increase human annotated data for exploring al-
ternative MT models. This form of symbiosis is
akin to the paradigm proposed by von Ahn and
Dabbish (2004). They designed interactive games
in which the player generated data could be used
to improve image tagging and other classification
tasks (von Ahn, 2006). While our interface does
not have the entertainment value of a game, its
application serves a purpose. Because users are
motivated to understand the documents, they may
willingly spend time to collaborate and make de-
tailed corrections to MT outputs.
8 Conclusion
We have presented a collaborative approach for
mediating between an MT system and monolin-
gual target-language users. The approach encour-
ages users to combine evidences from comple-
mentary information sources to infer alternative
hypotheses based on their world knowledge. Ex-
perimental evidences suggest that the collabora-
tive effort results in better translations than ei-
ther the original MT or uninformed human ed-
its. Moreover, users who are knowledgeable in the
document domain were enabled to correct transla-
tions with a quality approaching that of a bilin-
gual speaker. From the participants? feedbacks,
we learned that the factors that contributed to their
understanding include: document coherence, syn-
tactic constraints, and re-translation at the phrasal
level. We believe that the collaborative translation
approach can provide insights about the transla-
tion process and help to gather training examples
for future MT development.
Acknowledgments
This work has been supported by NSF Grants IIS-
0710695 and IIS-0745914. We would like to thank
Jarrett Billingsley, Ric Crabbe, Joanna Drum-
mund, Nick Farnan, Matt Kaniaris Brian Mad-
den, Karen Thickman, Julia Hockenmaier, Pauline
Hwa, and Dorothea Wei for their help with the ex-
periment. We are also grateful to Chris Callison-
Burch for discussions about collaborative trans-
lations and to Adam Lopez and the anonymous
reviewers for their comments and suggestions on
this paper.
67
References
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence es-
timation for machine translation. Technical Report
Natural Language Engineering Workshop Final Re-
port, Johns Hopkins University.
Lynne Bowker. 2002. Computer-Aided Translation
Technology. University of Ottawa Press, Ottawa,
Canada.
Chris Callison-Burch. 2005. Linear B System descrip-
tion for the 2005 NIST MT Evaluation. In The Pro-
ceedings of Machine Translation Evaluation Work-
shop.
Dan Cosley, Dan Frankowski, Loren Terveen, and John
Riedl. 2007. Suggestbot: using intelligent task rout-
ing to help people find work in wikipedia. In IUI
?07: Proceedings of the 12th international confer-
ence on Intelligent user interfaces, pages 32?41.
Steve DeNeefe, Kevin Knight, and Hayward H. Chan.
2005. Interactively exploring a machine transla-
tion model. In Proceedings of the ACL Interactive
Poster and Demonstration Sessions, pages 97?100,
Ann Arbor, Michigan, June.
Martin Kay. 1980. The proper place of men and
machines in language translation. Technical Re-
port CSL-80-11, Xerox. Later reprinted in Machine
Translation, vol. 12 no.(1-2), 1997.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. Advances in Neural Information
Processing Systems, 15.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-
ing approach to improving sentence-level MT evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI), Baltimore, MD, Octo-
ber.
Philippe Langlais, George Foster, and Guy Lapalme.
2000. Transtype: a computer-aided translation typ-
ing system. In Workshop on Embedded Machine
Translation Systems, pages 46?51, May.
Lemur. 2006. Lemur toolkit for language modeling
and information retrieval. The Lemur Project is a
collaborative project between CMU and UMASS.
Elliott Macklovitch, Michel Simard, and Philippe
Langlais. 2000. Transsearch: A free translation
memory on the world wide web. In Proceedings of
the Second International Conference on Language
Resources & Evaluation (LREC).
Bart Mellebeek, Anna Khasin, Josef van Genabith, and
Andy Way. 2005. Transbooster: Boosting the per-
formance of wide-coverage machine translation sys-
tems. In Proceedings of the 10th Annual Conference
of the European Association for Machine Transla-
tion (EAMT), pages 189?197.
Philip S. Resnik, Douglas W. Oard, and Gina-Anne
Levow. 2001. Improved cross-language retrieval us-
ing backoff translation. In Human Language Tech-
nology Conference (HLT-2001), San Diego, CA,
March.
John R. Searle. 1980. Minds, brains, and programs.
Behavioral and Brain Sciences, 3:417?457.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In CHI ?04: Proceed-
ings of the SIGCHI conference on Human factors in
computing systems, pages 319?326, New York, NY,
USA. ACM.
Luis von Ahn. 2006. Games with a purpose. Com-
puter, 39(6):92?94.
68
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 296?303,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Regression for Sentence-Level MT Evaluation with Pseudo References
Joshua S. Albrecht and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
{jsa8,hwa}@cs.pitt.edu
Abstract
Many automatic evaluation metrics for ma-
chine translation (MT) rely on making com-
parisons to human translations, a resource
that may not always be available. We present
a method for developing sentence-level MT
evaluation metrics that do not directly rely
on human reference translations. Our met-
rics are developed using regression learn-
ing and are based on a set of weaker indi-
cators of fluency and adequacy (pseudo ref-
erences). Experimental results suggest that
they rival standard reference-based metrics
in terms of correlations with human judg-
ments on new test instances.
1 Introduction
Automatic assessment of translation quality is a
challenging problem because the evaluation task, at
its core, is based on subjective human judgments.
Reference-based metrics such as BLEU (Papineni
et al, 2002) have rephrased this subjective task as
a somewhat more objective question: how closely
does the translation resemble sentences that are
known to be good translations for the same source?
This approach requires the participation of human
translators, who provide the ?gold standard? refer-
ence sentences. However, keeping humans in the
evaluation loop represents a significant expenditure
both in terms of time and resources; therefore it is
worthwhile to explore ways of reducing the degree
of human involvement.
To this end, Gamon et al (2005) proposed a
learning-based evaluation metric that does not com-
pare against reference translations. Under a learn-
ing framework, the input (i.e., the sentence to be
evaluated) is represented as a set of features. These
are measurements that can be extracted from the in-
put sentence (and may be individual metrics them-
selves). The learning algorithm combines the fea-
tures to form a model (a composite evaluation met-
ric) that produces the final score for the input. With-
out human references, the features in the model pro-
posed by Gamon et al were primarily language
model features and linguistic indicators that could be
directly derived from the input sentence alone. Al-
though their initial results were not competitive with
standard reference-based metrics, their studies sug-
gested that a referenceless metric may still provide
useful information about translation fluency. How-
ever, a potential pitfall is that systems might ?game
the metric? by producing fluent outputs that are not
adequate translations of the source.
This paper proposes an alternative approach to
evaluate MT outputs without comparing against hu-
man references. While our metrics are also trained,
our model consists of different features and is
trained under a different learning regime. Crucially,
our model includes features that capture some no-
tions of adequacy by comparing the input against
pseudo references: sentences from other MT sys-
tems (such as commercial off-the-shelf systems or
open sourced research systems). To improve flu-
ency judgments, the model also includes features
that compare the input against target-language ?ref-
erences? such as large text corpora and treebanks.
Unlike human translations used by standard
reference-based metrics, pseudo references are not
296
?gold standards? and can be worse than the sen-
tences being evaluated; therefore, these ?references?
in-and-of themselves are not necessarily informative
enough for MT evaluation. The main insight of our
approach is that through regression, the trained met-
rics can make more nuanced comparisons between
the input and pseudo references. More specifically,
our regression objective is to infer a function that
maps a feature vector (which measures an input?s
similarity to the pseudo references) to a score that
indicates the quality of the input. This is achieved by
optimizing the model?s output to correlate against a
set of training examples, which are translation sen-
tences labeled with quantitative assessments of their
quality by human judges. Although this approach
does incur some human effort, it is primarily for the
development of training data, which, ideally, can be
amortized over a long period of time.
To determine the feasibility of the proposed ap-
proach, we conducted empirical studies that com-
pare our trained metrics against standard reference-
based metrics. We report three main findings.
First, pseudo references are informative compar-
ison points. Experimental results suggest that
a regression-trained metric that compares against
pseudo references can have higher correlations with
human judgments than applying standard metrics
with multiple human references. Second, the learn-
ing model that uses both adequacy and fluency fea-
tures performed the best, with adequacy being the
more important factor. Third, when the pseudo ref-
erences are multiple MT systems, the regression-
trained metric is predictive even when the input is
from a better MT system than those providing the
references. We conjecture that comparing MT out-
puts against other imperfect translations allows for a
more nuanced discrimination of quality.
2 Background and Related Work
For a formally organized event, such as the annual
MT Evaluation sponsored by National Institute of
Standard and Technology (NIST MT Eval), it may
be worthwhile to recruit multiple human translators
to translate a few hundred sentences for evaluation
references. However, there are situations in which
multiple human references are not practically avail-
able (e.g., the source may be of a large quantity, and
no human translation exists). One such instance is
translation quality assurance, in which one wishes
to identify poor outputs in a large body of machine
translated text automatically for human to post-edit.
Another instance is in day-to-day MT research and
development, where new test set with multiple ref-
erences are also hard to come by. One could work
with previous datasets from events such as the NIST
MT Evals, but there is a danger of over-fitting. One
also could extract a single reference from parallel
corpora, although it is known that automatic metrics
are more reliable when comparing against multiple
references.
The aim of this work is to develop a trainable au-
tomatic metric for evaluation without human refer-
ences. This can be seen as a form of confidence esti-
mation on MT outputs (Blatz et al, 2003; Ueffing et
al., 2003; Quirk, 2004). The main distinction is that
confidence estimation is typically performed with a
particular system in mind, and may rely on system-
internal information in estimation. In this study, we
draw on only system-independent indicators so that
the resulting metric may be more generally applied.
This allows us to have a clearer picture of the con-
tributing factors as they interact with different types
of MT systems.
Also relevant is previous work that applied ma-
chine learning approaches to MT evaluation, both
with human references (Corston-Oliver et al, 2001;
Kulesza and Shieber, 2004; Albrecht and Hwa,
2007; Liu and Gildea, 2007) and without (Gamon et
al., 2005). One motivation for the learning approach
is the ease of combining multiple criteria. Literature
in translation evaluation reports a myriad of criteria
that people use in their judgments, but it is not clear
how these factors should be combined mathemati-
cally. Machine learning offers a principled and uni-
fied framework to induce a computational model of
human?s decision process. Disparate indicators can
be encoded as one or more input features, and the
learning algorithm tries to find a mapping from input
features to a score that quantifies the input?s quality
by optimizing the model to match human judgments
on training examples. The framework is attractive
because its objective directly captures the goal of
MT evaluation: how would a user rate the quality
of these translations?
This work differs from previous approaches in
297
two aspects. One is the representation of the model;
our model treats the metric as a distance measure
even though there are no human references. An-
other is the training of the model. More so than
when human references are available, regression is
central to the success of the approach, as it deter-
mines how much we can trust the distance measures
against each pseudo reference system.
While our model does not use human references
directly, its features are adapted from the following
distance-based metrics. The well-known BLEU (Pa-
pineni et al, 2002) is based on the number of com-
mon n-grams between the translation hypothesis and
human reference translations of the same sentence.
Metrics such as ROUGE, Head Word Chain (HWC),
METEOR, and other recently proposed methods all
offer different ways of comparing machine and hu-
man translations. ROUGE utilizes ?skip n-grams?,
which allow for matches of sequences of words that
are not necessarily adjacent (Lin and Och, 2004a).
METEOR uses the Porter stemmer and synonym-
matching via WordNet to calculate recall and pre-
cision more accurately (Banerjee and Lavie, 2005).
The HWC metrics compare dependency and con-
stituency trees for both reference and machine trans-
lations (Liu and Gildea, 2005).
3 MT Evaluation with Pseudo References
using Regression
Reference-based metrics are typically thought of as
measurements of ?similarity to good translations?
because human translations are used as references,
but in more general terms, they are distance mea-
surements between two sentences. The distance be-
tween a translation hypothesis and an imperfect ref-
erence is still somewhat informative. As a toy ex-
ample, consider a one-dimensional line segment. A
distance from the end-point uniquely determines the
position of a point. When the reference location is
anywhere else on the line segment, a relative dis-
tance to the reference does not uniquely specify a
location on the line segment. However, the position
of a point can be uniquely determined if we are given
its relative distances to two reference locations.
The problem space for MT evaluation, though
more complex, is not dissimilar to the toy scenario.
There are two main differences. First, we do not
know the actual distance function ? this is what we
are trying to learn. The distance functions we have
at our disposal are all heuristic approximations to the
true translational distance function. Second, unlike
human references, whose quality value is assumed to
be maximum, the quality of a pseudo reference sen-
tence is not known. In fact, prior to training, we do
not even know the quality of the reference systems.
Although the direct way to calibrate a reference sys-
tem is to evaluate its outputs, this is not practically
ideal, since human judgments would be needed each
time we wish to incorporate a new reference system.
Our proposed alternative is to calibrate the reference
systems against an existing set of human judgments
for a range of outputs from different MT systems.
That is, if many of the reference system?s outputs
are similar to those MT outputs that received low
assessments, we conclude this reference system may
not be of high quality. Thus, if a new translation is
found to be similar with this reference system?s out-
put, it is more likely for the new translation to also
be bad.
Both issues of combining evidences from heuris-
tic distances and calibrating the quality of pseudo
reference systems can be addressed by a probabilis-
tic learning model. In particular, we use regression
because its problem formulation fits naturally with
the objective of MT evaluations. In regression learn-
ing, we are interested in approximating a function f
that maps a multi-dimensional input vector, x, to a
continuous real value, y, such that the error over a set
of m training examples, {(x1, y1), . . . , (xm, ym)},
is minimized according to a loss function.
In the context of MT evaluation, y is the ?true?
quantitative measure of translation quality for an in-
put sentence1. The function f represents a mathe-
matical model of human judgments of translations;
an input sentence is represented as a feature vector,
x, which contains the information that can be ex-
tracted from the input sentence (possibly including
comparisons against some reference sentences) that
are relevant to computing y. Determining the set of
relevant features for this modeling is on-going re-
1Perhaps even more so than grammaticality judgments, there
is variability in people?s judgments of translation quality. How-
ever, like grammaticality judgments, people do share some sim-
ilarities in their judgments at a coarse-grained level. Ideally,
what we refer to as the true value of translational quality should
reflect the consensus judgments of all people.
298
search. In this work, we consider some of the more
widely used metrics as features. Our full feature
vector consists of r ? 18 adequacy features, where
r is the number of reference systems used, and 26
fluency features:
Adequacy features: These include features de-
rived from BLEU (e.g., n-gram precision, where
1 ? n ? 5, length ratios), PER, WER, fea-
tures derived from METEOR (precision, recall,
fragmentation), and ROUGE-related features (non-
consecutive bigrams with a gap size of g, where
1 ? g ? 5 and longest common subsequence).
Fluency features: We consider both string-level
features such as computing n-gram precision against
a target-language corpus as well as several syntax-
based features. We parse each input sentence into a
dependency tree and compared aspects of it against a
large target-language dependency treebank. In addi-
tion to adapting the idea of Head Word Chains (Liu
and Gildea, 2005), we also compared the input sen-
tence?s argument structures against the treebank for
certain syntactic categories.
Due to the large feature space to explore, we
chose to work with support vector regression as the
learning algorithm. As its loss function, support vec-
tor regression uses an ?-insensitive error function,
which allows for errors within a margin of a small
positive value, ?, to be considered as having zero er-
ror (cf. Bishop (2006), pp.339-344). Like its classi-
fication counterpart, this is a kernel-based algorithm
that finds sparse solutions so that scores for new test
instances are efficiently computed based on a subset
of the most informative training examples. In this
work, Gaussian kernels are used.
The cost of regression learning is that it requires
training examples that are manually assessed by hu-
man judges. However, compared to the cost of cre-
ating new references whenever new (test) sentences
are evaluated, the effort of creating human assess-
ment training data is a limited (ideally, one-time)
cost. Moreover, there is already a sizable collection
of human assessed data for a range of MT systems
through multiple years of the NIST MT Eval efforts.
Our experiments suggest that there is enough as-
sessed data to train the proposed regression model.
Aside from reducing the cost of developing hu-
man reference translations, the proposed metric also
provides an alternative perspective on automatic MT
evaluation that may be informative in its own right.
We conjecture that a metric that compares inputs
against a diverse population of differently imperfect
sentences may be more discriminative in judging
translation systems than solely comparing against
gold standards. That is, two sentences may be
considered equally bad from the perspective of a
gold standard, but subtle differences between them
may become more prominent if they are compared
against sentences in their peer group.
4 Experiments
We conducted experiments to determine the feasibil-
ity of the proposed approach and to address the fol-
lowing questions: (1) How informative are pseudo
references in-and-of themselves? Does varying the
number and/or the quality of the references have an
impact on the metrics? (2) What are the contribu-
tions of the adequacy features versus the fluency fea-
tures to the learning-based metric? (3) How do the
quality and distribution of the training examples, to-
gether with the quality of the pseudo references, im-
pact the metric training? (4) Do these factors impact
the metric?s ability in assessing sentences produced
within a single MT system? How does that system?s
quality affect metric performance?
4.1 Data preparation and Experimental Setup
The implementation of support vector regression
used for these experiments is SVM-Light (Joachims,
1999). We performed all experiments using the 2004
NIST Chinese MT Eval dataset. It consists of 447
source sentences that were translated by four hu-
man translators as well as ten MT systems. Each
machine translated sentence was evaluated by two
human judges for their fluency and adequacy on a
5-point scale2. To remove the bias in the distribu-
tions of scores between different judges, we follow
the normalization procedure described by Blatz et
al. (2003). The two judge?s total scores (i.e., sum
of the normalized fluency and adequacy scores) are
then averaged.
2The NIST human judges use human reference translations
when making assessments; however, our approach is generally
applicable when the judges are bilingual speakers who compare
source sentences with translation outputs.
299
We chose to work with this NIST dataset because
it contains numerous systems that span over a range
of performance levels (see Table 1 for a ranking of
the systems and their averaged human assessment
scores). This allows us to have control over the vari-
ability of the experiments while answering the ques-
tions we posed above (such as the quality of the sys-
tems providing the pseudo references, the quality of
MT systems being evaluated, and the diversity over
the distribution of training examples).
Specifically, we reserved four systems (MT2,
MT5, MT6, and MT9) for the role of pseudo ref-
erences. Sentences produced by the remaining six
systems are used as evaluative data. This set in-
cludes the best and worst systems so that we can see
how well the metrics performs on sentences that are
better (or worse) than the pseudo references. Met-
rics that require no learning are directly applied onto
all sentences of the evaluative set. For the learning-
based metrics, we perform six-fold cross validation
on the evaluative dataset. Each fold consists of sen-
tences from one MT system. In a round robin fash-
ion, each fold serves as the test set while the other
five are used for training and heldout. Thus, the
trained models have seen neither the test instances
nor other instances from the MT system that pro-
duced them.
A metric is evaluated based on its Spearman rank
correlation coefficient between the scores it gave to
the evaluative dataset and human assessments for
the same data. The correlation coefficient is a real
number between -1, indicating perfect negative cor-
relations, and +1, indicating perfect positive cor-
relations. To compare the relative quality of dif-
ferent metrics, we apply bootstrapping re-sampling
on the data, and then use paired t-test to deter-
mine the statistical significance of the correlation
differences (Koehn, 2004). For the results we re-
port, unless explicitly mentioned, all stated compar-
isons are statistically significant with 99.8% con-
fidence. We include two standard reference-based
metrics, BLEU and METEOR, as baseline compar-
isons. BLEU is smoothed (Lin and Och, 2004b), and
it considers only matching up to bigrams because
this has higher correlations with human judgments
than when higher-ordered n-grams are included.
SysID Human-assessment score
MT1 0.661
MT2 0.626
MT3 0.586
MT4 0.578
MT5 0.537
MT6 0.530
MT7 0.530
MT8 0.375
MT9 0.332
MT10 0.243
Table 1: The human-judged quality of ten partici-
pating systems in the NIST 2004 Chinese MT Eval-
uation. We used four systems as references (high-
lighted in boldface) and the data from the remaining
six for training and evaluation.
4.2 Pseudo Reference Variations vs. Metrics
We first compare different metrics? performance
on the six-system evaluative dataset under different
configurations of human and/or pseudo references.
For the case when only one human reference is used,
the reference was chosen at random from the 2004
NIST Eval dataset3. The correlation results on the
evaluative dataset are summarized in Table 2.
Some trends are as expected: comparing within a
metric, having four references is better than having
just one; having human references is better than an
equal number of system references; having a high
quality system as reference is better than one with
low quality. Perhaps more surprising is the consis-
tent trend that metrics do significantly better with
four MT references than with one human reference,
and they do almost as well as using four human ref-
erences. The results show that pseudo references are
informative, as standard metrics were able to make
use of the pseudo references and achieve higher cor-
relations than judging from fluency alone. How-
ever, higher correlations are achieved when learning
with regression, suggesting that the trained metrics
are better at interpreting comparisons against pseudo
references.
Comparing within each reference configuration,
the regression-trained metric that includes both ad-
3One reviewer asked about the quality this human?s trans-
lations. Although we were not given official rankings of the
human references, we compared each person against the other
three using MT evaluation metrics and found this particular
translator to rank third, though the quality of all four are sig-
nificantly higher than even the best MT systems.
300
equacy and fluency features always has the highest
correlations. If the metric consists of only adequacy
features, its performance degrades with the decreas-
ing quality of the references. At another extreme, a
metric based only on fluency features has an over-
all correlation rate of 0.459, which is lower than
most correlations reported in Table 2. This confirms
the importance of modeling adequacy; even a sin-
gle mid-quality MT system may be an informative
pseudo reference. Finally, we note that a regression-
trained metric with the full features set that com-
pares against 4 pseudo references has a higher cor-
relation than BLEU with four human references.
These results suggest that the feedback from the hu-
man assessed training examples was able to help the
learning algorithm to combine different features to
form a better composite metric.
4.3 Sentence-Level Evaluation on Single
Systems
To explore the interaction between the quality of
the reference MT systems and that of the test MT
systems, we further study the following pseudo ref-
erence configurations: all four systems, a high-
quality system with a medium quality system, two
systems of medium-quality, one medium with one
poor system, and only the high-quality system. For
each pseudo reference configuration, we consider
three metrics: BLEU, METEOR, and the regression-
trained metric (using the full feature set). Each
metric evaluates sentences from four test systems
of varying quality: the best system in the dataset
(MT1), the worst in the set (MT10), and two mid-
ranged systems (MT4 and MT7). The correlation
coefficients are summarized in Table 3. Each row
specifies a metric/reference-type combination; each
column specifies an MT system being evaluated (us-
ing sentences from all other systems as training ex-
amples). The fluency-only metric and standard met-
rics using four human references are baselines.
The overall trends at the dataset level generally
also hold for the per-system comparisons. With the
exception of the evaluation of MT10, regression-
based metrics always has higher correlations than
standard metrics that use the same reference con-
figuration (comparing correlation coefficients within
each cell). When the best MT reference system
(MT2) is included as pseudo references, regression-
based metrics are typically better than or not statisti-
cally different from standard applications of BLEU
and METEOR with 4 human references. Using the
two mid-quality MT systems as references (MT5
and MT6), regression metrics yield correlations that
are only slightly lower than standard metrics with
human references. These results support our con-
jecture that comparing against multiple systems is
informative.
The poorer performances of the regression-based
metrics on MT10 point out an asymmetry in the
learning approach. The regression model aims to
learn a function that approximates human judgments
of translated sentences through training examples.
In the space of all possible MT outputs, the neigh-
borhood of good translations is much smaller than
that of bad translations. Thus, as long as the regres-
sion models sees some examples of sentences with
high assessment scores during training, it should
have a much better estimation of the characteristics
of good translations. This idea is supported by the
experimental data. Consider the scenario of eval-
uating MT1 while using two mid-quality MT sys-
tems as references. Although the reference systems
are not as high quality as the system under evalu-
ation, and although the training examples shown to
the regression model were also generated by systems
whose overall quality was rated lower, the trained
metric was reasonably good at ranking sentences
produced by MT1. In contrast, the task of evaluating
sentences from MT10 is more difficult for the learn-
ing approach, perhaps because it is sufficiently dif-
ferent from all training and reference systems. Cor-
relations might be improved with additional refer-
ence systems.
4.4 Discussions
The design of these experiments aims to simulate
practical situations to use our proposed metrics. For
the more frequently encountered language pairs, it
should be possible to find at least two mid-quality
(or better) MT systems to serve as pseudo refer-
ences. For example, one might use commercial off-
the-shelf systems, some of which are free over the
web. For less commonly used languages, one might
use open source research systems (Al-Onaizan et al,
1999; Burbank et al, 2005).
Datasets from formal evaluation events such as
301
Ref type and # Ref Sys. BLEU-S(2) METEOR Regr (adq. only) Regr (full)
4 Humans all humans 0.628 0.591 0.588 0.644
1 Human HRef #3 0.536 0.512 0.487 0.597
4 Systems all MTRefs 0.614 0.583 0.584 0.632
2 Systems Best 2 MTRefs 0.603 0.577 0.573 0.620
Mid 2 MTRefs 0.579 0.555 0.528 0.608
Worst 2 MTRefs 0.541 0.508 0.467 0.581
1 System Best MTRef 0.576 0.559 0.534 0.596
Mid MTRef (MT5) 0.538 0.528 0.474 0.577
Worst MTRef 0.371 0.329 0.151 0.495
Table 2: Comparisons of metrics (columns) using different types of references (rows). The full regression-
trained metric has the highest correlation (shown in boldface) when four human references are used; it has
the second highest correlation rate (shown in italic) when four MT system references are used instead. A
regression-trained metric with only fluency features has a correlation coefficient of 0.459.
Ref Type Metric MT-1 MT-4 MT-7 MT-10
No ref Regr. 0.367 0.316 0.301 -0.045
4 human refs Regr. 0.538* 0.473* 0.459* 0.247
BLEU-S(2) 0.466 0.419 0.397 0.321*
METEOR 0.464 0.418 0.410 0.312
4 MTRefs Regr. 0.498 0.429 0.421 0.243
BLEU-S(2) 0.386 0.349 0.404 0.240
METEOR 0.445 0.354 0.333 0.243
Best 2 MTRefs Regr. 0.492 0.418 0.403 0.201
BLEU-S(2) 0.391 0.330 0.394 0.268
METEOR 0.430 0.333 0.327 0.267
Mid 2 MTRefs Regr. 0.450 0.413 0.388 0.219
BLEU-S(2) 0.362 0.314 0.310 0.282
METEOR 0.391 0.315 0.284 0.274
Worst 2 MTRefs Regr. 0.430 0.386 0.365 0.158
BLEU-S(2) 0.320 0.298 0.316 0.223
METEOR 0.351 0.306 0.302 0.228
Best MTRef Regr. 0.461 0.401 0.414 0.122
BLEU-S(2) 0.371 0.330 0.380 0.242
METEOR 0.375 0.318 0.392 0.283
Table 3: Correlation comparisons of metrics by test systems. For each test system (columns) the overall
highest correlations is distinguished by an asterisk (*); correlations higher than standard metrics using
human-references are highlighted in boldface; those that are statistically comparable to them are italicized.
NIST MT Evals, which contains human assessed
MT outputs for a variety of systems, can be used
for training examples. Alternatively, one might di-
rectly recruit human judges to assess sample sen-
tences from the system(s) to be evaluated. This
should result in better correlations than what we re-
ported here, since the human assessed training ex-
amples will be more similar to the test instances than
the setup in our experiments.
In developing new MT systems, pseudo refer-
ences may supplement the single human reference
translations that could be extracted from a parallel
text. Using the same setup as Exp. 1 (see Table 2),
adding pseudo references does improve correlations.
Adding four pseudo references to the single human
reference raises the correlation coefficient to 0.650
(from 0.597) for the regression metric. Adding them
to four human references results in a correlation co-
efficient of 0.660 (from 0.644)4.
5 Conclusion
In this paper, we have presented a method for de-
veloping sentence-level MT evaluation metrics with-
out using human references. We showed that by
learning from human assessed training examples,
4BLEU with four human references has a correlation of
0.628. Adding four pseudo references increases BLEU to 0.650.
302
the regression-trained metric can evaluate an input
sentence by comparing it against multiple machine-
generated pseudo references and other target lan-
guage resources. Our experimental results suggest
that the resulting metrics are robust even when the
sentences under evaluation are from a system of
higher quality than the systems serving as refer-
ences. We observe that regression metrics that use
multiple pseudo references often have comparable
or higher correlation rates with human judgments
than standard reference-based metrics. Our study
suggests that in conjunction with regression training,
multiple imperfect references may be as informative
as gold-standard references.
Acknowledgments
This work has been supported by NSF Grants IIS-0612791 and
IIS-0710695. We would like to thank Ric Crabbe, Dan Gildea,
Alon Lavie, Stuart Shieber, and Noah Smith and the anonymous
reviewers for their suggestions. We are also grateful to NIST for
making their assessment data available to us.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight,
John Lafferty, I. Dan Melamed, Franz-Josef Och, David
Purdy, Noah A. Smith, and David Yarowsky. 1999.
Statistical machine translation. Technical report, JHU.
citeseer.nj.nec.com/al-onaizan99statistical.html.
Joshua S. Albrecht and Rebecca Hwa. 2007. A re-examination
of machine learning approaches for sentence-level MT eval-
uation. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics (ACL-2007).
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An auto-
matic metric for MT evaluation with improved correlation
with human judgments. In ACL 2005 Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, June.
Christopher M. Bishop. 2006. Pattern Recognition and Ma-
chine Learning. Springer Verlag.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,
Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola
Ueffing. 2003. Confidence estimation for machine trans-
lation. Technical Report Natural Language Engineering
Workshop Final Report, Johns Hopkins University.
Andrea Burbank, Marine Carpuat, Stephen Clark, Markus
Dreyer, Declan Groves Pamela. Fox, Keith Hall, Mary
Hearne, I. Dan Melamed, Yihai Shen, Andy Way, Ben
Wellington, and Dekai Wu. 2005. Final report of the 2005
language engineering workshop on statistical machine trans-
lation by parsing. Technical Report Natural Language Engi-
neering Workshop Final Report, ?JHU?.
Simon Corston-Oliver, Michael Gamon, and Chris Brockett.
2001. A machine learning approach to the automatic eval-
uation of machine translation. In Proceedings of the 39th
Annual Meeting of the Association for Computational Lin-
guistics, July.
Michael Gamon, Anthony Aue, and Martine Smets. 2005.
Sentence-level MT evaluation without reference translations:
Beyond language modeling. In European Association for
Machine Translation (EAMT), May.
Thorsten Joachims. 1999. Making large-scale SVM learning
practical. In Bernhard Scho?elkopf, Christopher Burges, and
Alexander Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Philipp Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP-04).
Alex Kulesza and Stuart M. Shieber. 2004. A learning ap-
proach to improving sentence-level MT evaluation. In Pro-
ceedings of the 10th International Conference on Theoretical
and Methodological Issues in Machine Translation (TMI),
Baltimore, MD, October.
Chin-Yew Lin and Franz Josef Och. 2004a. Automatic evalu-
ation of machine translation quality using longest common
subsequence and skip-bigram statistics. In Proceedings of
the 42nd Annual Meeting of the Association for Computa-
tional Linguistics, July.
Chin-Yew Lin and Franz Josef Och. 2004b. Orange: a
method for evaluating automatic evaluation metrics for ma-
chine translation. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING 2004),
August.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In ACL 2005 Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, June.
Ding Liu and Daniel Gildea. 2007. Source-language features
and maximum correlation training for machine translation
evaluation. In Proceedings of the HLT/NAACL-2007, April.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics, Philadel-
phia, PA.
Christopher Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of LREC
2004.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003.
Confidence measures for statistical machine translation. In
Machine Translation Summit IX, pages 394?401, September.
303
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 880?887,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Re-examination of Machine Learning Approaches
for Sentence-Level MT Evaluation
Joshua S. Albrecht and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
{jsa8,hwa}@cs.pitt.edu
Abstract
Recent studies suggest that machine learn-
ing can be applied to develop good auto-
matic evaluation metrics for machine trans-
lated sentences. This paper further ana-
lyzes aspects of learning that impact per-
formance. We argue that previously pro-
posed approaches of training a Human-
Likeness classifier is not as well correlated
with human judgments of translation qual-
ity, but that regression-based learning pro-
duces more reliable metrics. We demon-
strate the feasibility of regression-based
metrics through empirical analysis of learn-
ing curves and generalization studies and
show that they can achieve higher correla-
tions with human judgments than standard
automatic metrics.
1 Introduction
As machine translation (MT) research advances, the
importance of its evaluation also grows. Efficient
evaluation methodologies are needed both for facili-
tating the system development cycle and for provid-
ing an unbiased comparison between systems. To
this end, a number of automatic evaluation metrics
have been proposed to approximate human judg-
ments of MT output quality. Although studies have
shown them to correlate with human judgments at
the document level, they are not sensitive enough
to provide reliable evaluations at the sentence level
(Blatz et al, 2003). This suggests that current met-
rics do not fully reflect the set of criteria that people
use in judging sentential translation quality.
A recent direction in the development of met-
rics for sentence-level evaluation is to apply ma-
chine learning to create an improved composite met-
ric out of less indicative ones (Corston-Oliver et al,
2001; Kulesza and Shieber, 2004). Under the as-
sumption that good machine translation will pro-
duce ?human-like? sentences, classifiers are trained
to predict whether a sentence is authored by a human
or by a machine based on features of that sentence,
which may be the sentence?s scores from individ-
ual automatic evaluation metrics. The confidence of
the classifier?s prediction can then be interpreted as a
judgment on the translation quality of the sentence.
Thus, the composite metric is encoded in the confi-
dence scores of the classification labels.
While the learning approach to metric design of-
fers the promise of ease of combining multiple met-
rics and the potential for improved performance,
several salient questions should be addressed more
fully. First, is learning a ?Human Likeness? classi-
fier the most suitable approach for framing the MT-
evaluation question? An alternative is regression, in
which the composite metric is explicitly learned as
a function that approximates humans? quantitative
judgments, based on a set of human evaluated train-
ing sentences. Although regression has been con-
sidered on a small scale for a single system as con-
fidence estimation (Quirk, 2004), this approach has
not been studied as extensively due to scalability and
generalization concerns. Second, how does the di-
versity of the model features impact the learned met-
ric? Third, how well do learning-based metrics gen-
eralize beyond their training examples? In particu-
lar, how well can a metric that was developed based
880
on one group of MT systems evaluate the translation
qualities of new systems?
In this paper, we argue for the viability of a
regression-based framework for sentence-level MT-
evaluation. Through empirical studies, we first
show that having an accurate Human-Likeness clas-
sifier does not necessarily imply having a good MT-
evaluation metric. Second, we analyze the resource
requirement for regression models for different sizes
of feature sets through learning curves. Finally, we
show that SVM-regression metrics generalize better
than SVM-classification metrics in their evaluation
of systems that are different from those in the train-
ing set (by languages and by years), and their corre-
lations with human assessment are higher than stan-
dard automatic evaluation metrics.
2 MT Evaluation
Recent automatic evaluation metrics typically frame
the evaluation problem as a comparison task: how
similar is the machine-produced output to a set of
human-produced reference translations for the same
source text? However, as the notion of similar-
ity is itself underspecified, several different fami-
lies of metrics have been developed. First, simi-
larity can be expressed in terms of string edit dis-
tances. In addition to the well-known word error
rate (WER), more sophisticated modifications have
been proposed (Tillmann et al, 1997; Snover et
al., 2006; Leusch et al, 2006). Second, similar-
ity can be expressed in terms of common word se-
quences. Since the introduction of BLEU (Papineni
et al, 2002) the basic n-gram precision idea has
been augmented in a number of ways. Metrics in the
Rouge family allow for skip n-grams (Lin and Och,
2004a); Kauchak and Barzilay (2006) take para-
phrasing into account; metrics such as METEOR
(Banerjee and Lavie, 2005) and GTM (Melamed et
al., 2003) calculate both recall and precision; ME-
TEOR is also similar to SIA (Liu and Gildea, 2006)
in that word class information is used. Finally, re-
searchers have begun to look for similarities at a
deeper structural level. For example, Liu and Gildea
(2005) developed the Sub-Tree Metric (STM) over
constituent parse trees and the Head-Word Chain
Metric (HWCM) over dependency parse trees.
With this wide array of metrics to choose from,
MT developers need a way to evaluate them. One
possibility is to examine whether the automatic met-
ric ranks the human reference translations highly
with respect to machine translations (Lin and Och,
2004b; Amigo? et al, 2006). The reliability of a
metric can also be more directly assessed by de-
termining how well it correlates with human judg-
ments of the same data. For instance, as a part of the
recent NIST sponsored MT Evaluation, each trans-
lated sentence by participating systems is evaluated
by two (non-reference) human judges on a five point
scale for its adequacy (does the translation retain the
meaning of the original source text?) and fluency
(does the translation sound natural in the target lan-
guage?). These human assessment data are an in-
valuable resource for measuring the reliability of au-
tomatic evaluation metrics. In this paper, we show
that they are also informative in developing better
metrics.
3 MT Evaluation with Machine Learning
A good automatic evaluation metric can be seen as
a computational model that captures a human?s de-
cision process in making judgments about the ade-
quacy and fluency of translation outputs. Inferring a
cognitive model of human judgments is a challeng-
ing problem because the ultimate judgment encom-
passes a multitude of fine-grained decisions, and the
decision process may differ slightly from person to
person. The metrics cited in the previous section
aim to capture certain aspects of human judgments.
One way to combine these metrics in a uniform and
principled manner is through a learning framework.
The individual metrics participate as input features,
from which the learning algorithm infers a compos-
ite metric that is optimized on training examples.
Reframing sentence-level translation evaluation
as a classification task was first proposed by
Corston-Oliver et al (2001). Interestingly, instead
of recasting the classification problem as a ?Hu-
man Acceptability? test (distinguishing good trans-
lations outputs from bad one), they chose to develop
a Human-Likeness classifier (distinguishing out-
puts seem human-produced from machine-produced
ones) to avoid the necessity of obtaining manu-
ally labeled training examples. Later, Kulesza and
Shieber (2004) noted that if a classifier provides a
881
confidence score for its output, that value can be
interpreted as a quantitative estimate of the input
instance?s translation quality. In particular, they
trained an SVM classifier that makes its decisions
based on a set of input features computed from the
sentence to be evaluated; the distance between input
feature vector and the separating hyperplane then
serves as the evaluation score. The underlying as-
sumption for both is that improving the accuracy of
the classifier on the Human-Likeness test will also
improve the implicit MT evaluation metric.
A more direct alternative to the classification ap-
proach is to learn via regression and explicitly op-
timize for a function (i.e. MT evaluation metric)
that approximates human judgments in training ex-
amples. Kulesza and Shieber (2004) raised two
main objections against regression for MT evalua-
tions. One is that regression requires a large set of
labeled training examples. Another is that regression
may not generalize well over time, and re-training
may become necessary, which would require col-
lecting additional human assessment data. While
these are legitimate concerns, we show through em-
pirical studies (in Section 4.2) that the additional re-
source requirement is not impractically high, and
that a regression-based metric has higher correla-
tions with human judgments and generalizes better
than a metric derived from a Human-Likeness clas-
sifier.
3.1 Relationship between Classification and
Regression
Classification and regression are both processes of
function approximation; they use training examples
as sample instances to learn the mapping from in-
puts to the desired outputs. The major difference be-
tween classification and regression is that the func-
tion learned by a classifier is a set of decision bound-
aries by which to classify its inputs; thus its outputs
are discrete. In contrast, a regression model learns
a continuous function that directly maps an input
to a continuous value. An MT evaluation metric is
inherently a continuous function. Casting the task
as a 2-way classification may be too coarse-grained.
The Human-Likeness formulation of the problem in-
troduces another layer of approximation by assum-
ing equivalence between ?Like Human-Produced?
and ?Well-formed? sentences. In Section 4.1, we
show empirically that high accuracy in the Human-
Likeness test does not necessarily entail good MT
evaluation judgments.
3.2 Feature Representation
To ascertain the resource requirements for different
model sizes, we considered two feature models. The
smaller one uses the same nine features as Kulesza
and Shieber, which were derived from BLEU and
WER. The full model consists of 53 features: some
are adapted from recently developed metrics; others
are new features of our own. They fall into the fol-
lowing major categories1:
String-based metrics over references These in-
clude the nine Kulesza and Shieber features as well
as precision, recall, and fragmentation, as calcu-
lated in METEOR; ROUGE-inspired features that
are non-consecutive bigrams with a gap size of m,
where 1 ? m ? 5 (skip-m-bigram), and ROUGE-L
(longest common subsequence).
Syntax-based metrics over references We un-
rolled HWCM into their individual chains of length
c (where 2 ? c ? 4); we modified STM so that it is
computed over unlexicalized constituent parse trees
as well as over dependency parse trees.
String-based metrics over corpus Features in
this category are similar to those in String-based
metric over reference except that a large English cor-
pus is used as ?reference? instead.
Syntax-based metrics over corpus A large de-
pendency treebank is used as the ?reference? instead
of parsed human translations. In addition to adap-
tations of the Syntax-based metrics over references,
we have also created features to verify the argument
structures for certain syntactic categories.
4 Empirical Studies
In these studies, the learning models used for both
classification and regression are support vector ma-
chines (SVM) with Gaussian kernels. All models
are trained with SVM-Light (Joachims, 1999). Our
primary experimental dataset is from NIST?s 2003
1As feature engineering is not the primary focus of this pa-
per, the features are briefly described here, but implementa-
tional details will be made available in a technical report.
882
Chinese MT Evaluations, in which the fluency and
adequacy of 919 sentences produced by six MT sys-
tems are scored by two human judges on a 5-point
scale2. Because the judges evaluate sentences ac-
cording to their individual standards, the resulting
scores may exhibit a biased distribution. We normal-
ize human judges? scores following the process de-
scribed by Blatz et al (2003). The overall human as-
sessment score for a translation output is the average
of the sum of two judges? normalized fluency and
adequacy scores. The full dataset (6 ? 919 = 5514
instances) is split into sets of training, heldout and
test data. Heldout data is used for parameter tuning
(i.e., the slack variable and the width of the Gaus-
sian). When training classifiers, assessment scores
are not used, and the training set is augmented with
all available human reference translation sentences
(4 ? 919 = 3676 instances) to serve as positive ex-
amples.
To judge the quality of a metric, we compute
Spearman rank-correlation coefficient, which is a
real number ranging from -1 (indicating perfect neg-
ative correlations) to +1 (indicating perfect posi-
tive correlations), between the metric?s scores and
the averaged human assessments on test sentences.
We use Spearman instead of Pearson because it
is a distribution-free test. To evaluate the rela-
tive reliability of different metrics, we use boot-
strapping re-sampling and paired t-test to determine
whether the difference between the metrics? correla-
tion scores has statistical significance (at 99.8% con-
fidence level)(Koehn, 2004). Each reported correla-
tion rate is the average of 1000 trials; each trial con-
sists of n sampled points, where n is the size of the
test set. Unless explicitly noted, the qualitative dif-
ferences between metrics we report are statistically
significant. As a baseline comparison, we report the
correlation rates of three standard automatic metrics:
BLEU, METEOR, which incorporates recall and
stemming, and HWCM, which uses syntax. BLEU
is smoothed to be more appropriate for sentence-
level evaluation (Lin and Och, 2004b), and the bi-
gram versions of BLEU and HWCM are reported
because they have higher correlations than when
longer n-grams are included. This phenomenon has
2This corpus is available from the Linguistic Data Consor-
tium as Multiple Translation Chinese Part 4.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 45  50  55  60  65  70  75  80  85
Co
rrel
atio
n C
oef
ficie
nt w
ith 
Hu
ma
n J
udg
em
ent
 (R)
Human-Likeness Classifier Accuracy (%)
Figure 1: This scatter plot compares classifiers? ac-
curacy with their corresponding metrics? correla-
tions with human assessments
been previously observed by Liu and Gildea (2005).
4.1 Relationship between Classification
Accuracy and Quality of Evaluation Metric
A concern in using a metric derived from a Human-
Likeness classifier is whether it would be predic-
tive for MT evaluation. Kulesza and Shieber (2004)
tried to demonstrate a positive correlation between
the Human-Likeness classification task and the MT
evaluation task empirically. They plotted the clas-
sification accuracy and evaluation reliability for a
number of classifiers, which were generated as a
part of a greedy search for kernel parameters and
found some linear correlation between the two. This
proof of concept is a little misleading, however, be-
cause the population of the sampled classifiers was
biased toward those from the same neighborhood as
the local optimal classifier (so accuracy and corre-
lation may only exhibit linear relationship locally).
Here, we perform a similar study except that we
sampled the kernel parameter more uniformly (on
a log scale). As Figure 1 confirms, having an ac-
curate Human-Likeness classifier does not necessar-
ily entail having a good MT evaluation metric. Al-
though the two tasks do seem to be positively re-
lated, and in the limit there may be a system that is
good at both tasks, one may improve classification
without improving MT evaluation. For this set of
heldout data, at the near 80% accuracy range, a de-
rived metric might have an MT evaluation correla-
tion coefficient anywhere between 0.25 (on par with
883
unsmoothed BLEU, which is known to be unsuitable
for sentence-level evaluation) and 0.35 (competitive
with standard metrics).
4.2 Learning Curves
To investigate the feasibility of training regression
models from assessment data that are currently
available, we consider both a small and a large
regression model. The smaller model consists of
nine features (same as the set used by Kulesza and
Shieber); the other uses the full set of 53 features
as described in Section 3.2. The reliability of the
trained metrics are compared with those developed
from Human-Likeness classifiers. We follow a sim-
ilar training and testing methodology as previous
studies: we held out 1/6 of the assessment dataset for
SVM parameter tuning; five-fold cross validation is
performed with the remaining sentences. Although
the metrics are evaluated on unseen test sentences,
the sentences are produced by the same MT systems
that produced the training sentences. In later exper-
iments, we investigate generalizing to more distant
MT systems.
Figure 2(a) shows the learning curves for the two
regression models. As the graph indicates, even
with a limited amount of human assessment data,
regression models can be trained to be comparable
to standard metrics (represented by METEOR in the
graph). The small feature model is close to conver-
gence after 1000 training examples3. The model
with a more complex feature set does require more
training data, but its correlation began to overtake
METEOR after 2000 training examples. This study
suggests that the start-up cost of building even a
moderately complex regression model is not impos-
sibly high.
Although we cannot directly compare the learning
curves of the Human-Likeness classifiers to those of
the regression models (since the classifier?s training
examples are automatically labeled), training exam-
ples for classifiers are not entirely free: human ref-
erence translations still must be developed for the
source sentences. Figure 2(c) shows the learning
curves for training Human-Likeness classifiers (in
terms of improving a classifier?s accuracy) using the
same two feature sets, and Figure 2(b) shows the
3The total number of labeled examples required is closer to
2000, since the heldout set uses 919 labeled examples.
correlations of the metrics derived from the corre-
sponding classifiers. The pair of graphs show, es-
pecially in the case of the larger feature set, that a
large improvement in classification accuracy does
not bring proportional improvement in its corre-
sponding metrics?s correlation; with an accuracy of
near 90%, its correlation coefficient is 0.362, well
below METEOR.
This experiment further confirms that judging
Human-Likeness and judging Human-Acceptability
are not tightly coupled. Earlier, we have shown in
Figure 1 that different SVM parameterizations may
result in classifiers with the same accuracy rate but
different correlations rates. As a way to incorpo-
rate some assessment information into classification
training, we modify the parameter tuning process so
that SVM parameters are chosen to optimize for as-
sessment correlations in the heldout data. By incur-
ring this small amount of human assessed data, this
parameter search improves the classifier?s correla-
tions: the metric using the smaller feature set in-
creased from 0.423 to 0.431, and that of the larger
set increased from 0.361 to 0.422.
4.3 Generalization
We conducted two generalization studies. The first
investigates how well the trained metrics evaluate
systems from other years and systems developed
for a different source language. The second study
delves more deeply into how variations in the train-
ing examples affect a learned metric?s ability to gen-
eralize to distant systems. The learning models for
both experiments use the full feature set.
Cross-Year Generalization To test how well the
learning-based metrics generalize to systems from
different years, we trained both a regression-based
metric (R03) and a classifier-based metric (C03)
with the entire NIST 2003 Chinese dataset (using
20% of the data as heldout4). All metrics are then
applied to three new datasets: NIST 2002 Chinese
MT Evaluation (3 systems, 2634 sentences total),
NIST 2003 Arabic MT Evaluation (2 systems, 1326
sentences total), and NIST 2004 Chinese MT Evalu-
ation (10 systems, 4470 sentences total). The results
4Here, too, we allowed the classifier?s parameters to be
tuned for correlation with human assessment on the heldout data
rather than accuracy.
884
(a) (b) (c)
Figure 2: Learning curves: (a) correlations with human assessment using regression models; (b) correlations
with human assessment using classifiers; (c) classifier accuracy on determining Human-Likeness.
Dataset R03 C03 BLEU MET. HWCM
2002 Ara 0.466 0.384 0.423 0.431 0.424
2002 Chn 0.309 0.250 0.269 0.290 0.260
2004 Chn 0.602 0.566 0.588 0.563 0.546
Table 1: Correlations for cross-year generalization.
Learning-based metrics are developed from NIST
2003 Chinese data. All metrics are tested on datasets
from 2003 Arabic, 2002 Chinese and 2004 Chinese.
are summarized in Table 1. We see that R03 con-
sistently has a better correlation rate than the other
metrics.
At first, it may seem as if the difference between
R03 and BLEU is not as pronounced for the 2004
dataset, calling to question whether a learned met-
ric might become quickly out-dated, we argue that
this is not the case. The 2004 dataset has many
more participating systems, and they span a wider
range of qualities. Thus, it is easier to achieve a
high rank correlation on this dataset than previous
years because most metrics can qualitatively discern
that sentences from one MT system are better than
those from another. In the next experiment, we ex-
amine the performance of R03 with respect to each
MT system in the 2004 dataset and show that its cor-
relation rate is higher for better MT systems.
Relationship between Training Examples and
Generalization Table 2 shows the result of a gen-
eralization study similar to before, except that cor-
relations are performed on each system. The rows
order the test systems by their translation quali-
ties from the best performing system (2004-Chn1,
whose average human assessment score is 0.655 out
of 1.0) to the worst (2004-Chn10, whose score is
0.255). In addition to the regression metric from
the previous experiment (R03-all), we consider two
more regression metrics trained from subsets of the
2003 dataset: R03-Bottom5 is trained from the sub-
set that excludes the best 2003 MT system, and R03-
Top5 is trained from the subset that excludes the
worst 2003 MT system.
We first observe that on a per test-system basis,
the regression-based metrics generally have better
correlation rates than BLEU, and that the gap is as
wide as what we have observed in the earlier cross-
years studies. The one exception is when evaluating
2004-Chn8. None of the metrics seems to correlate
very well with human judges on this system. Be-
cause the regression-based metric uses these individ-
ual metrics as features, its correlation also suffers.
During regression training, the metric is opti-
mized to minimize the difference between its pre-
diction and the human assessments of the training
data. If the input feature vector of a test instance
is in a very distant space from training examples,
the chance for error is higher. As seen from the
results, the learned metrics typically perform better
when the training examples include sentences from
higher-quality systems. Consider, for example, the
differences between R03-all and R03-Top5 versus
the differences between R03-all and R03-Bottom5.
Both R03-Top5 and R03-Bottom5 differ from R03-
all by one subset of training examples. Since R03-
all?s correlation rates are generally closer to R03-
Top5 than to R03-Bottom5, we see that having seen
extra training examples from a bad system is not as
harmful as having not seen training examples from a
good system. This is expected, since there are many
ways to create bad translations, so seeing a partic-
885
R03-all R03-Bottom5 R03-Top5 BLEU METEOR HWCM
2004-Chn1 0.495 0.460 0.518 0.456 0.457 0.444
2004-Chn2 0.398 0.330 0.440 0.352 0.347 0.344
2004-Chn3 0.425 0.389 0.459 0.369 0.402 0.369
2004-Chn4 0.432 0.392 0.434 0.400 0.400 0.362
2004-Chn5 0.452 0.441 0.443 0.370 0.426 0.326
2004-Chn6 0.405 0.392 0.406 0.390 0.357 0.380
2004-Chn7 0.443 0.432 0.448 0.390 0.408 0.392
2004-Chn8 0.237 0.256 0.256 0.265 0.259 0.179
2004-Chn9 0.581 0.569 0.591 0.527 0.537 0.535
2004-Chn10 0.314 0.313 0.354 0.321 0.303 0.358
2004-all 0.602 0.567 0.617 0.588 0.563 0.546
Table 2: Metric correlations within each system. The columns specify which metric is used. The rows
specify which MT system is under evaluation; they are ordered by human-judged system quality, from best
to worst. For each evaluated MT system (row), the highest coefficient in bold font, and those that are
statistically comparable to the highest are shown in italics.
ular type of bad translations from one system may
not be very informative. In contrast, the neighbor-
hood of good translations is much smaller, and is
where all the systems are aiming for; thus, assess-
ments of sentences from a good system can be much
more informative.
4.4 Discussion
Experimental results confirm that learning from
training examples that have been doubly approx-
imated (class labels instead of ordinals, human-
likeness instead of human-acceptability) does nega-
tively impact the performance of the derived metrics.
In particular, we showed that they do not generalize
as well to new data as metrics trained from direct
regression.
We see two lingering potential objections toward
developing metrics with regression-learning. One
is the concern that a system under evaluation might
try to explicitly ?game the metric5.? This is a con-
cern shared by all automatic evaluation metrics, and
potential problems in stand-alone metrics have been
analyzed (Callison-Burch et al, 2006). In a learning
framework, potential pitfalls for individual metrics
are ameliorated through a combination of evidences.
That said, it is still prudent to defend against the po-
tential of a system gaming a subset of the features.
For example, our fluency-predictor features are not
strong indicators of translation qualities by them-
selves. We want to avoid training a metric that as-
5Or, in a less adversarial setting, a system may be perform-
ing minimum error-rate training (Och, 2003)
signs a higher than deserving score to a sentence that
just happens to have many n-gram matches against
the target-language reference corpus. This can be
achieved by supplementing the current set of hu-
man assessed training examples with automatically
assessed training examples, similar to the labeling
process used in the Human-Likeness classification
framework. For instance, as negative training ex-
amples, we can incorporate fluent sentences that are
not adequate translations and assign them low over-
all assessment scores.
A second, related concern is that because the met-
ric is trained on examples from current systems us-
ing currently relevant features, even though it gener-
alizes well in the near term, it may not continue to
be a good predictor in the distant future. While pe-
riodic retraining may be necessary, we see value in
the flexibility of the learning framework, which al-
lows for new features to be added. Moreover, adap-
tive learning methods may be applicable if a small
sample of outputs of some representative translation
systems is manually assessed periodically.
5 Conclusion
Human judgment of sentence-level translation qual-
ity depends on many criteria. Machine learning af-
fords a unified framework to compose these crite-
ria into a single metric. In this paper, we have
demonstrated the viability of a regression approach
to learning the composite metric. Our experimental
results show that by training from some human as-
886
sessments, regression methods result in metrics that
have better correlations with human judgments even
as the distribution of the tested population changes.
Acknowledgments
This work has been supported by NSF Grants IIS-0612791 and
IIS-0710695. We would like to thank Regina Barzilay, Ric
Crabbe, Dan Gildea, Alex Kulesza, Alon Lavie, and Matthew
Stone as well as the anonymous reviewers for helpful comments
and suggestions. We are also grateful to NIST for making their
assessment data available to us.
References
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT evaluation: Human-like vs. human ac-
ceptable. In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, Sydney, Australia, July.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An auto-
matic metric for MT evaluation with improved correlation
with human judgments. In ACL 2005 Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, June.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,
Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola
Ueffing. 2003. Confidence estimation for machine trans-
lation. Technical Report Natural Language Engineering
Workshop Final Report, Johns Hopkins University.
Christopher Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in machine
translation research. In The Proceedings of the Thirteenth
Conference of the European Chapter of the Association for
Computational Linguistics.
Simon Corston-Oliver, Michael Gamon, and Chris Brockett.
2001. A machine learning approach to the automatic eval-
uation of machine translation. In Proceedings of the 39th
Annual Meeting of the Association for Computational Lin-
guistics, July.
Thorsten Joachims. 1999. Making large-scale SVM learning
practical. In Bernhard Scho?elkopf, Christopher Burges, and
Alexander Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
David Kauchak and Regina Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main Confer-
ence, New York City, USA, June.
Philipp Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP-04).
Alex Kulesza and Stuart M. Shieber. 2004. A learning ap-
proach to improving sentence-level MT evaluation. In Pro-
ceedings of the 10th International Conference on Theoretical
and Methodological Issues in Machine Translation (TMI),
Baltimore, MD, October.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT evaluation using block movements. In
The Proceedings of the Thirteenth Conference of the Euro-
pean Chapter of the Association for Computational Linguis-
tics.
Chin-Yew Lin and Franz Josef Och. 2004a. Automatic evalu-
ation of machine translation quality using longest common
subsequence and skip-bigram statistics. In Proceedings of
the 42nd Annual Meeting of the Association for Computa-
tional Linguistics, July.
Chin-Yew Lin and Franz Josef Och. 2004b. Orange: a
method for evaluating automatic evaluation metrics for ma-
chine translation. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING 2004),
August.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In ACL 2005 Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, June.
Ding Liu and Daniel Gildea. 2006. Stochastic iterative align-
ment for machine translation evaluation. In Proceedings
of the Joint Conference of the International Conference on
Computational Linguistics and the Association for Com-
putational Linguistics (COLING-ACL?2006) Poster Session,
July.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003. Preci-
sion and recall of machine translation. In In Proceedings of
the HLT-NAACL 2003: Short Papers, pages 61?63, Edmon-
ton, Alberta.
Franz Josef Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics, Philadel-
phia, PA.
Christopher Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of LREC
2004.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Mic-
ciulla, and John Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In Proceedings of the
8th Conference of the Association for Machine Translation
in the Americas (AMTA-2006).
Christoph Tillmann, Stephan Vogel, Hermann Ney, Hassan
Sawaf, and Alex Zubiaga. 1997. Accelerated DP-based
search for statistical translation. In Proceedings of the 5th
European Conference on Speech Communication and Tech-
nology (EuroSpeech ?97).
887
Proceedings of the Third Workshop on Statistical Machine Translation, pages 187?190,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The Role of Pseudo References in MT Evaluation
Joshua S. Albrecht and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
{jsa8,hwa}@cs.pitt.edu
Abstract
Previous studies have shown automatic evalu-
ation metrics to be more reliable when com-
pared against many human translations. How-
ever, multiple human references may not al-
ways be available. It is more common to have
only a single human reference (extracted from
parallel texts) or no reference at all. Our ear-
lier work suggested that one way to address
this problem is to train a metric to evaluate a
sentence by comparing it against pseudo refer-
ences, or imperfect ?references? produced by
off-the-shelf MT systems. In this paper, we
further examine the approach both in terms of
the training methodology and in terms of the
role of the human and pseudo references. Our
expanded experiments show that the approach
generalizes well across multiple years and dif-
ferent source languages.
1 Introduction
Standard automatic metrics are reference-based;
that is, they compare system-produced translations
against human-translated references produced for
the same source. Since there is usually no single
best way to translate a sentence, each MT output
should be compared against many references. On
the other hand, creating multiple human references
is itself a costly process. For many naturally occur-
ring datasets (e.g., parallel corpora) only a single ref-
erence is readily available.
The focus of this work is on developing auto-
matic metrics for sentence-level evaluation with at
most one human reference. One way to supple-
ment the single human reference is to use pseudo
references, or sentences produced by off-the-shelf
MT systems, as stand-ins for human references.
However, since pseudo references may be imperfect
translations themselves, the comparisons cannot be
fully trusted. Previously, we have taken a learning-
based approach to develop a composite metric that
combines measurements taken from multiple pseudo
references (Albrecht and Hwa, 2007). Experimental
results suggested the approach to be promising; but
those studies did not consider how well the metric
might generalize across multiple years and different
languages. In this paper, we investigate the appli-
cability of the pseudo-reference metrics under these
more general conditions.
Using the WMT06 Workshop shared-task re-
sults (Koehn and Monz, 2006) as training exam-
ples, we train a metric that evaluates new sentences
by comparing them against pseudo references pro-
duced by three off-the-shelf MT systems. We ap-
ply the learned metric to sentences from the WMT07
shared-task (Callison-Burch et al, 2007b) and com-
pare the metric?s predictions against human judg-
ments. We find that additional pseudo references
improve correlations for automatic metrics.
2 Background
The ideal evaluation metric reports an accurate dis-
tance between an input instance and its gold stan-
dard, but even when comparing against imperfect
standards, the measured distances may still convey
some useful information ? they may help to trian-
gulate the input?s position relative to the true gold
standard.
In the context of sentence-level MT evaluations,
187
the challenges are two-fold. First, the ideal quantita-
tive distance function between a translation hypoth-
esis and the proper translations is not known; cur-
rent automatic evaluation metrics produce approxi-
mations to the true translational distance. Second,
although we may know the qualitative goodness of
the MT systems that generate the pseudo references,
we do not know how imperfect the pseudo refer-
ences are. These uncertainties make it harder to es-
tablish the true distance between the input hypoth-
esis and the (unobserved) acceptable gold standard
translations.
In order to combine evidence from these uncertain
observations, we take a learning-based approach.
Each hypothesis sentence is compared with multi-
ple pseudo references using multiple metrics. Rep-
resenting the measurements as a set of input features
and using human-assessed MT sentences as training
examples, we train a function that is optimized to
correlate the features with the human assessments in
the training examples. Specifically, for each input
sentence, we compute a set of 18 kinds of reference-
based measurements for each pseudo reference as
well as 26 monolingual fluency measurements. The
full set of measurements then serves as the input fea-
ture vector into the function, which is trained via
support vector regression. The learned function can
then be used as an evaluation metric itself: it takes
the measurements of a new sentence as input and re-
turns a composite score for that sentence.
The approach is considered successful if the met-
ric?s predictions on new test sentences correlate well
with quantitative human assessments. Like other
learned models, the metric is expected to perform
better on data that are more similar to the training
instances. Therefore, a natural question that arises
with a metric developed in this manner is: how well
does it generalize?
3 Research Questions
To better understand the capability of metrics that
compare against pseudo-references, we consider the
following aspects:
The role of learning Standard reference-based
metrics can also use pseudo references; however,
they would treat the imperfect references as gold
standard. In contrast, the learning process aims
to determine how much each comparison with a
pseudo reference might be trusted. To observe the
role of learning, we compare trained metrics against
standard reference-based metrics, all using pseudo
references.
The amount vs. types of training data The suc-
cess of any learned model depends on its training ex-
periences. We study the trade-off between the size
of the training set and the specificity of the train-
ing data. We perform experiments comparing a met-
ric trained from a large pool of heterogeneous train-
ing examples that include translated sentences from
multiple languages and individual metrics trained
from particular source languages.
The role of a single human reference Previous
studies have shown the importance of comparing
against multiple references. The approach in this
paper attempts to approximate multiple human ref-
erences with machine-produced sentences. Is a sin-
gle trust-worthy translation more useful than multi-
ple imperfect translations? To answer this question,
we compare three different reference settings: using
just a single human reference, using just the three
pseudo references, and using all four references.
4 Experimental Setup
For the experiments reported in this paper, we used
human-evaluated MT sentences from past shared-
tasks of the WMT 2006 and WMT 2007. The data
consists of outputs from German-English, Spanish-
English, and French-English MT systems. The out-
puts are translations from two corpora: Europarl and
news commentary. System outputs have been evalu-
ated by human judges on a 5-point scale (Callison-
Burch et al, 2007a). We have normalized scores
to reduce biases from different judges (Blatz et al,
2003).
We experimented with using four different sub-
sets of the WMT2006 data as training examples:
only German-English, only Spanish-English, only
French-English, all 06 data. The metrics are trained
using support vector regression with a Gaussian
kernel as implemented in the SVM-Light package
(Joachims, 1999). The SVM parameters are tuned
via grid-search on development data, 20% of the full
training set that has been reserved for this purpose.
188
We used three MT systems to generate pseudo ref-
erences: Systran1, GoogleMT 2, and Moses (Koehn
et al, 2007). We chose these three systems because
they are widely accessible and because they take
relatively different approaches. Moreover, although
they have not all been human-evaluated in the past
WMT shared tasks, they are well-known for produc-
ing good translations.
A metric is evaluated based on its Spearman rank
correlation coefficient between the scores it gave to
the evaluative dataset and human assessments for
the same data. The correlation coefficient is a real
number between -1, indicating perfect negative cor-
relations, and +1, indicating perfect positive correla-
tions.
Two standard reference-based metrics, BLEU
(Papineni et al, 2002) and METEOR (Banerjee and
Lavie, 2005), are used for comparisons. BLEU is
smoothed (Lin and Och, 2004), and it considers only
matching up to bigrams because this has higher cor-
relations with human judgments than when higher-
ordered n-grams are included.
5 Results
The full experimental comparisons are summarized
in Table 1. Each cell shows the correlation coef-
ficient between the human judgments and a metric
(column) that uses a particular kind of references
(row) for some evaluation data set (block row).
The role of learning With the exception of the
German-English data, the learned metrics had higher
correlations with human judges than the baselines,
which used standard metrics with a single human
reference. On the other hand, results suggest that
pseudo references often also improve correlations
for standard metrics. This may seem counter-
intuitive because we can easily think of cases in
which pseudo references hurt standard metrics (e.g.,
use poor outputs as pseudo references). We hypoth-
1Available from http://www.systransoft.com/.
We note that Systran is also a participating system under eval-
uation. Although Sys-Test will be deemed to be identical to
Sys-Ref, it will not automatically receive a high score because
the measurement is weighted by whether Sys-Ref was reliable
during training. Furthermore, measurements between Sys-Test
and other pseudo-references will provide alternative evidences
for the metric to consider.
2http://www.google.com/language tools/
esize that because the pseudo references came from
high-quality MT systems and because standard met-
rics are based on simple word matches, the chances
for bad judgments (input words matched against
pseudo reference, but both are wrong) are relatively
small compared to chances for good judgments. We
further hypothesize that the learned metrics would
be robust against the qualities of the pseudo refer-
ence MT systems.
The amount vs. types of training data Com-
paring the three metrics trained from single lan-
guage datasets against the metric trained from all
of WMT06 dataset, we see that the learning process
benefitted from the larger quantity of training exam-
ples. It may be the case that the MT systems for the
three language pairs are at a similar stage of maturity
such that the training instances are mutually helpful.
The role of a single human reference Our results
reinforce previous findings that metrics are more re-
liable when they have access to more than a sin-
gle human reference. Our experimental data sug-
gests that a single human reference often may not be
as reliable as using three pseudo references alone.
Finally, the best correlations are achieved by using
both human and pseudo references.
6 Conclusion
We have presented an empirical study on automatic
metrics for sentence-level MT evaluation with at
most one human reference. We show that pseudo
references from off-the-shelf MT systems can be
used to augment the single human reference. Be-
cause they are imperfect, it is important to weigh the
trustworthiness of these references through a train-
ing phase. The metric seems robust even when the
applied to sentences from different systems of a later
year. These results suggest that multiple imperfect
translations make informative comparison points in
supplement to human references.
Acknowledgments
This work has been supported by NSF Grants IIS-
0612791.
189
Eval. Data Ref Type METEOR BLEU SVM(de06) SVM(es06) SVM(fr06) SVM(wmt06)
de 1HR 0.458 0.471
europarl 3PR 0.521* 0.527* 0.422 0.403 0.480* 0.467
07 1HR+3PR 0.535* 0.547* 0.471 0.480* 0.477* 0.523*
de 1HR 0.290 0.333
news 3PR 0.400* 0.400* 0.262 0.279 0.261 0.261
07 1HR+3PR 0.432* 0.417* 0.298 0.321 0.269 0.330
es 1HR 0.377 0.412
europarl 3PR 0.453* 0.483* 0.336 0.453* 0.432* 0.456*
07 1HR+3PR 0.491* 0.503* 0.405 0.513* 0.483* 0.510*
es 1HR 0.317 0.332
news 3PR 0.320 0.317 0.393* 0.381* 0.426* 0.426*
07 1HR+3PR 0.353* 0.325 0.429* 0.427* 0.380* 0.486*
fr 1HR 0.265 0.246
europarl 3PR 0.196 0.285* 0.270* 0.284* 0.355* 0.366*
07 1HR+3PR 0.221 0.290* 0.277* 0.324* 0.304* 0.381*
fr 1HR 0.226 0.280
news 3PR 0.356* 0.383* 0.237 0.252 0.355* 0.373*
07 1HR+3PR 0.374* 0.394* 0.272 0.339* 0.319* 0.388*
Table 1: Correlation comparisons of metrics (columns) using different references (row): a single human reference
(1HR), 3 pseudo references (3PR), or all (1HR+3PR). The type of training used for the regression-trained metrics
are specified in parentheses. For each evaluated corpus, correlations higher than standard metric using one human
reference are marked by an asterisk(*).
References
Joshua S. Albrecht and Rebecca Hwa. 2007. Regression
for sentence-level MT evaluation with pseudo refer-
ences. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
2007).
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for Machine Translation and/or Summarization,
June.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation
for machine translation. Technical Report Natural
Language Engineering Workshop Final Report, Johns
Hopkins University.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007a. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Cameron Shaw
Fordyce, and Christof Monz, editors. 2007b. Proceed-
ings of the Second Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics, Prague, Czech Republic, June.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?elkopf, Christo-
pher Burges, and Alexander Smola, editors, Advances
in Kernel Methods - Support Vector Learning. MIT
Press.
Philipp Koehn and Christof Monz, editors. 2006. Pro-
ceedings on the Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics, New York City, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. Proceedings
of ACL, Demonstration Session.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING 2004), August.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, Philadelphia, PA.
190
