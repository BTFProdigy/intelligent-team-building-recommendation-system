Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 829?838,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
N -gram Weighting: Reducing Training Data Mismatch
in Cross-Domain Language Model Estimation
Bo-June (Paul) Hsu, James Glass
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar Street, Cambridge, MA, 02139 USA
{bohsu,glass}@csail.mit.edu
Abstract
In domains with insufficient matched training
data, language models are often constructed
by interpolating component models trained
from partially matched corpora. Since the n-
grams from such corpora may not be of equal
relevance to the target domain, we propose
an n-gram weighting technique to adjust the
component n-gram probabilities based on fea-
tures derived from readily available segmen-
tation and metadata information for each cor-
pus. Using a log-linear combination of such
features, the resulting model achieves up to a
1.2% absolute word error rate reduction over a
linearly interpolated baseline language model
on a lecture transcription task.
1 Introduction
Many application domains in machine learning suf-
fer from a dearth of matched training data. However,
partially matched data sets are often available in
abundance. Past attempts to utilize the mismatched
data for training often result in models that exhibit
biases not observed in the target domain. In this
work, we will investigate the use of the often readily
available data segmentation and metadata attributes
associated with each corpus to reduce the effect of
such bias. We will examine this approach in the con-
text of language modeling for lecture transcription.
Compared with other types of audio data, lecture
speech often exhibits a high degree of spontaneity
and focuses on narrow topics with special termi-
nologies (Glass et al, 2004). While we may have
existing transcripts from general lectures or written
text on the precise topic, training data that matches
both the style and topic of the target lecture is often
scarce. Thus, past research has investigated various
adaptation and interpolation techniques to make use
of partially matched corpora (Bellegarda, 2004).
Training corpora are often segmented into docu-
ments with associated metadata, such as title, date,
and speaker. For lectures, if the data contains even
a few lectures on linear algebra, conventional lan-
guage modeling methods that lump the documents
together will tend to assign disproportionately high
probability to frequent terms like vector and matrix.
Can we utilize the segmentation and metadata infor-
mation to reduce the biases resulting from training
data mismatch?
In this work, we present such a technique where
we weight each n-gram count in a standard n-gram
language model (LM) estimation procedure by a rel-
evance factor computed via a log-linear combina-
tion of n-gram features. Utilizing features that cor-
relate with the specificity of n-grams to subsets of
the training documents, we effectively de-emphasize
out-of-domain n-grams. By interpolating models,
such as general lectures and course textbook, that
match the target domain in complementary ways,
and optimizing the weighting and interpolation pa-
rameters jointly, we allow each n-gram probabil-
ity to be modeled by the most relevant interpolation
component. Using a combination of features derived
from multiple partitions of the training documents,
the resulting weighted n-gram model achieves up
to a 1.2% absolute word error rate (WER) reduc-
tion over a linearly interpolated baseline on a lecture
transcription task.
829
2 Related Work
To reduce topic mismatch in LM estimation, we
(2006) have previously assigned topic labels to each
word by applying HMM-LDA (Griffiths et al, 2005)
to the training documents. Using an ad hoc method
to reduce the effective counts of n-grams ending
on topic words, we achieved better perplexity and
WER than standard trigram LMs. Intuitively, de-
emphasizing such n-grams will lower the transition
probability to out-of-domain topic words from the
training data. In this work, we further explore this
intuition with a principled feature-based model, in-
tegrated with LM smoothing and estimation to allow
simultaneous optimization of all model parameters.
As Gao and Lee (2000) observed, even purported
matched training data may exhibit topic, style, or
temporal biases not present in the test set. To ad-
dress the mismatch, they partition the training doc-
uments by their metadata attributes and compute a
measure of the likelihood that an n-gram will appear
in a new partitioned segment. By pruning n-grams
with generality probability below a given threshold,
the resulting model achieves lower perplexity than a
count-cutoff model of equal size. Complementary to
our work, this technique also utilizes segmentation
and metadata information. However, our model en-
ables the simultaneous use of all metadata attributes
by combining features derived from different parti-
tions of the training documents.
3 N -gram Weighting
Given a limited amount of training data, an n-gram
appearing frequently in a single document may be
assigned a disproportionately high probability. For
example, an LM trained from lecture transcripts
tends to assign excessive probability to words from
observed lecture topics due to insufficient coverage
of the underlying document topics. On the other
hand, excessive probabilities may also be assigned
to n-grams appearing consistently across documents
with mismatched style, such as the course textbook
in the written style. Traditional n-gram smoothing
techniques do not address such issues of insufficient
topic coverage and style mismatch.
One approach to addressing the above issues is
to weight the counts of the n-grams according to
the concentration of their document distributions.
Assigning higher weights to n-grams with evenly
spread distributions captures the style of a data set,
as reflected across all documents. On the other hand,
emphasizing the n-grams concentrated within a few
documents focuses the model on the topics of the
individual documents.
In theory, n-gram weighting can be applied to any
smoothing algorithm based on counts. However,
because many of these algorithms assume integer
counts, we will apply the weighting factors to the
smoothed counts, instead. For modified Kneser-Ney
smoothing (Chen and Goodman, 1998), applying n-
gram weighting yields:
p(w|h) = ?(hw)c?
?(hw)?
w ?(hw)c?(hw)
+ ?(h)p(w|h?)
where p(w|h) is the probability of word w given his-
tory h, c? is the adjusted Kneser-Ney count, c?? is the
discounted count, ? is the n-gram weighting factor,
? is the normalizing backoff weight, and h? is the
backoff history.
Although the weighting factor ? can in general be
any function of the n-gram, in this work, we will
consider a log-linear combination of n-gram fea-
tures, or ?(hw) = exp(?(hw) ? ?), where ?(hw)
is the feature vector for n-gram hw and ? specifies
the parameter vector to be learned. To better fit the
data, we allow independent parameter vectors ?o for
each n-gram order o. Note that with ?(hw) = 1, the
model degenerates to the original modified Kneser-
Ney formulation. Furthermore, ? only specifies the
relative weighting among n-grams with a common
history h. Thus, scaling ?(hw) by an arbitrary func-
tion g(h) has no effect on the model.
In isolation, n-gram weighting shifts probability
mass from out-of-domain n-grams via backoff to
the uniform distribution to improve the generality
of the resulting model. However, in combination
with LM interpolation, it can also distribute prob-
abilities to LM components that better model spe-
cific n-grams. For example, n-gram weighting can
de-emphasize off-topic and off-style n-grams from
general lectures and course textbook, respectively.
Tuning the weighting and interpolation parameters
jointly further allows the estimation of the n-gram
probabilities to utilize the best matching LM com-
ponents.
830
3.1 Features
To address the issue of sparsity in the document
topic distribution, we can apply n-gram weight-
ing with features that measure the concentration of
the n-gram distribution across documents. Simi-
lar features can also be computed from documents
partitioned by their categorical metadata attributes,
such as course and speaker for lecture transcripts.
Whereas the features derived from the corpus docu-
ments should correlate with the topic specificity of
the n-grams, the same features computed from the
speaker partitions might correspond to the speaker
specificity. By combining features from multiple
partitions of the training data to compute the weight-
ing factors, n-gram weighting allows the resulting
model to better generalize across categories.
To guide the presentation of the n-gram features
below, we will consider the following example parti-
tion of the training corpus. Words tagged by HMM-
LDA as topic words appear in bold.
A B A A C C A B A B
B A A C C B A A B A
A C B A A C A B B A
One way to estimate the specificity of an n-gram
across partitions is to measure the n-gram frequency
f , or the fraction of partitions containing an n-gram.
For instance, f(A) = 3/3, f(C) = 2/3. However,
as the size of each partition increases, this ratio in-
creases to 1, since most n-grams have a non-zero
probability of appearing in each partition. Thus,
an alternative is to compute the normalized entropy
of the n-gram distribution across the S partitions,
or h = ?1logS
?S
s=1 p(s) log p(s), where p(s) is the
fraction of an n-gram appearing in partition s. For
example, the normalized entropy of the unigram C is
h(C) = ?1log 3 [26 log 26 + 46 log 46 +0] = .58. N -grams
clustered in fewer partitions have lower entropy than
ones that are more evenly spread out.
Following (Hsu and Glass, 2006), we also con-
sider features derived from the HMM-LDA word
topic labels.1 Specifically, we compute the empir-
ical probability t that the target word of the n-gram
1HMM-LDA is performed using 20 states and 50 topics with
a 3rd-order HMM. Hyperparameters are sampled with a log-
normal Metropolis proposal. The model with the highest likeli-
hood from among 10,000 iterations of Gibbs sampling is used.
Feature of
the
ith
ink
km
ea
ns
the
su
n
thi
si
sa
al
ot
of
big
oo
f
em
f
Random 0.03 0.32 0.33 0.19 0.53 0.24 0.37 0.80
log(c) 9.29 8.09 3.47 5.86 6.82 7.16 3.09 4.92
fdoc 1.00 0.93 0.00 0.18 0.92 0.76 0.00 0.04
fcourse 1.00 1.00 0.06 0.56 0.94 0.94 0.06 0.06
f speaker 0.83 0.70 0.00 0.06 0.41 0.55 0.01 0.00
hdoc 0.96 0.84 0.00 0.56 0.93 0.85 0.00 0.34
hcourse 0.75 0.61 0.00 0.55 0.78 0.65 0.00 0.00
hspeaker 0.76 0.81 0.00 0.09 0.65 0.80 0.12 0.00
tdoc 0.00 0.00 0.91 1.00 0.01 0.00 0.00 0.04
tcourse 0.00 0.00 0.88 0.28 0.01 0.00 0.00 1.00
tspeaker 0.00 0.00 0.94 0.92 0.01 0.00 0.09 0.99
Table 1: A list of n-gram weighting features. f : n-gram
frequency, h: normalized entropy, t: topic probability.
is labeled as a topic word. In the example corpus,
t(C) = 3/6, t(A C) = 2/4.
All of the above features can be computed for any
partitioning of the training data. To better illustrate
the differences, we compute the features on a set of
lecture transcripts (see Section 4.1) partitioned by
lecture (doc), course, and speaker. Furthermore, we
include the log of the n-gram counts c and random
values between 0 and 1 as baseline features. Table 1
lists all the features examined in this work and their
values on a select subset of n-grams.
3.2 Training
To tune the n-gram weighting parameters ?, we ap-
ply Powell?s method (Press et al, 2007) to numeri-
cally minimize the development set perplexity (Hsu
and Glass, 2008). Although there is no guarantee
against converging to a local minimum when jointly
tuning both the n-gram weighting and interpolation
parameters, we have found that initializing the pa-
rameters to zero generally yields good performance.
4 Experiments
4.1 Setup
In this work, we evaluate the perplexity and WER of
various trigram LMs trained with n-gram weighting
on a lecture transcription task (Glass et al, 2007).
The target data consists of 20 lectures from an in-
troductory computer science course, from which we
withhold the first 10 lectures for the development
831
Dataset # Words # Sents # Docs
Textbook 131,280 6,762 271
Lectures 1,994,225 128,895 230
Switchboard 3,162,544 262,744 4,876
CS Dev 93,353 4,126 10
CS Test 87,527 3,611 10
Table 2: Summary of evaluation corpora.
Perplexity WER
Model Dev Test Dev Test
FixKN(1) 174.7 196.7 34.9% 36.8%
+ W(hdoc) 172.9 194.8 34.7% 36.7%
FixKN(3) 168.6 189.3 34.9% 36.9%
+ W(hdoc) 166.8 187.8 34.6% 36.6%
FixKN(10) 167.5 187.6 35.0% 37.2%
+ W(hdoc) 165.3 185.8 34.7% 36.8%
KN(1) 169.7 190.4 35.0% 37.0%
+ W(hdoc) 167.3 188.2 34.8% 36.7%
KN(3) 163.4 183.1 35.0% 37.1%
+ W(hdoc) 161.1 181.2 34.7% 36.8%
KN(10) 162.3 181.8 35.1% 37.1%
+ W(hdoc) 160.1 180.0 34.8% 36.8%
Table 3: Performance of n-gram weighting with a variety
of Kneser-Ney settings. FixKN(d): Kneser-Ney with d
fixed discount parameters. KN(d): FixKN(d) with tuned
values. W(feat): n-gram weighting with feat feature.
set (CS Dev) and use the last 10 for the test set
(CS Test). For training, we will consider the course
textbook with topic-specific vocabulary (Textbook),
numerous high-fidelity transcripts from a variety of
general seminars and lectures (Lectures), and the
out-of-domain LDC Switchboard corpus of spon-
taneous conversational speech (Switchboard) (God-
frey and Holliman, 1993). Table 2 summarizes all
the evaluation data.
To compute the word error rate, we use a speaker-
independent speech recognizer (Glass, 2003) with a
large-margin discriminative acoustic model (Chang,
2008). The lectures are pre-segmented into utter-
ances via forced alignment against the reference
transcripts (Hazen, 2006). Since all the models con-
sidered in this work can be encoded as n-gram back-
off models, they are applied directly during the first
recognition pass instead of through a subsequent n-
best rescoring step.
Model Perplexity WER
Lectures 189.3 36.9%
+ W(hdoc) 187.8 (-0.8%) 36.6%
Textbook 326.1 43.1%
+ W(hdoc) 317.5 (-2.6%) 43.1%
LI(Lectures + Textbook) 141.6 33.7%
+ W(hdoc) 136.6 (-3.5%) 32.7%
Table 4: N -gram weighting with linear interpolation.
4.2 Smoothing
In Table 3, we compare the performance of n-gram
weighting with the hdoc document entropy feature
for various modified Kneser-Ney smoothing config-
urations (Chen and Goodman, 1998) on the Lec-
tures dataset. Specifically, we considered varying
the number of discount parameters per n-gram order
from 1 to 10. The original and modified Kneser-Ney
smoothing algorithms correspond to a setting of 1
and 3, respectively. Furthermore, we explored using
both fixed parameter values estimated from n-gram
count statistics and tuned values that minimize the
development set perplexity.
In this task, while the test set perplexity tracks
the development set perplexity well, the WER corre-
lates surprisingly poorly with the perplexity on both
the development and test sets. Nevertheless, n-gram
weighting consistently reduces the absolute test set
WER by a statistically significant average of 0.3%,
according to the Matched Pairs Sentence Segment
Word Error test (Pallet et al, 1990). Given that we
obtained the lowest development set WER with the
fixed 3-parameter modified Kneser-Ney smoothing,
all subsequent experiments are conducted using this
smoothing configuration.
4.3 Linear Interpolation
Applied to the Lectures dataset in isolation, n-gram
weighting with the hdoc feature reduces the test set
WER by 0.3% by de-emphasizing the probability
contributions from off-topic n-grams and shifting
their weights to the backoff distributions. Ideally
though, such weights should be distributed to on-
topic n-grams, perhaps from other LM components.
In Table 4, we present the performance of apply-
ing n-gram weighting to the Lectures and Textbook
models individually versus in combination via linear
interpolation (LI), where we optimize the n-gram
832
Model Perplexity WER
LI(Lectures + Textbook) 141.6 33.7%
+ W(Random) 141.5 (-0.0%) 33.7%
+ W(log(c)) 137.5 (-2.9%) 32.8%
+ W(fdoc) 136.3 (-3.7%) 32.8%
+ W(fcourse) 136.5 (-3.6%) 32.7%
+ W(f speaker) 138.1 (-2.5%) 33.0%
+ W(hdoc) 136.6 (-3.5%) 32.7%
+ W(hcourse) F 136.1 (-3.9%) 32.7%
+ W(hspeaker) 138.6 (-2.1%) 33.1%
+ W(tdoc) 134.8 (-4.8%) 33.2%
+ W(tcourse) 136.4 (-3.6%) 33.1%
+ W(tspeaker) 136.4 (-3.7%) 33.2%
Table 5: N -gram weighting with various features.
weighting and interpolation parameters jointly. The
interpolated model with n-gram weighting achieves
perplexity improvements roughly additive of the re-
ductions obtained with the individual models. How-
ever, the 1.0% WER drop for the interpolated model
significantly exceeds the sum of the individual re-
ductions. Thus, as we will examine in more detail
in Section 5.1, n-gram weighting allows probabili-
ties to be shifted from less relevant n-grams in one
component to more specific n-grams in another.
4.4 Features
With n-gram weighting, we can model the weight-
ing function ?(hw) as a log-linear combination of
any n-gram features. In Table 5, we show the effect
various features have on the performance of linearly
interpolating Lectures and Textbook. As the docu-
ments from the Lectures dataset is annotated with
course and speaker metadata attributes, we include
the n-gram frequency f , entropy h, and topic proba-
bility t features computed from the lectures grouped
by the 16 unique courses and 299 unique speakers.2
In terms of perplexity, the use of the Random
feature has negligible impact on the test set per-
formance, as expected. On the other hand, the
log(c) count feature reduces the perplexity by nearly
3%, as it correlates with the generality of the n-
grams. By using features that leverage the infor-
mation from document segmentation and associated
2Features that are not applicable to a particular corpus (e.g.
hcourse for Textbook) are removed from the n-gram weighting
computation for that component. Thus, models with course and
speaker features have fewer tunable parameters than the others.
metadata, we are generally able to achieve further
perplexity reductions. Overall, the frequency and
entropy features perform roughly equally. However,
by considering information from the more sophisti-
cated HMM-LDA topic model, the topic probability
feature tdoc achieves significantly lower perplexity
than any other feature in isolation.
In terms of WER, the Random feature again
shows no effect on the baseline WER of 33.7%.
However, to our surprise, the use of the simple
log(c) feature achieves nearly the same WER im-
provement as the best segmentation-based feature,
whereas the more sophisticated features computed
from HMM-LDA labels only obtain half of the re-
duction even though they have the best perplexities.
When comparing the performance of different n-
gram weighting features on this data set, the per-
plexity correlates poorly with the WER, on both the
development and test sets. Fortunately, the features
that yield the lowest perplexity and WER on the de-
velopment set alo yield one of the lowest perplex-
ities and WERs, respectively, on the test set. Thus,
during feature selection for speech recognition ap-
plications, we should consider the development set
WER. Specifically, since the differences in WER
are often statistically insignificant, we will select the
feature that minimizes the sum of the development
set WER and log perplexity, or cross-entropy.3
In Tables 5 and 6, we have underlined the per-
plexities and WERs of the features with the lowest
corresponding development set values (not shown)
and bolded the lowest test set values. The features
that achieve the lowest combined cross-entropy and
WER on the development set are starred.
4.5 Feature Combination
Unlike most previous work, n-gram weighting en-
ables a systematic integration of features computed
from multiple document partitions. In Table 6, we
compare the performance of various feature combi-
nations. We experiment with incrementally adding
features that yield the lowest combined development
set cross-entropy and WER. Overall, this metric ap-
pears to better predict the test set WER than either
the development set perplexity or WER alone.
3The choice of cross-entropy instead of perplexity is par-
tially motivated by the linear correlation reported by (Chen and
Goodman, 1998) between cross-entropy and WER.
833
Features Perplexity WER
hcourse 136.1 32.7%
+ log(c) 135.4 (-0.5%) 32.6%
+ fdoc 135.1 (-0.7%) 32.6%
+ hdoc 135.6 (-0.5%) 32.6%
+ tdoc F 133.2 (-2.1%) 32.6%
+ fcourse 136.0 (-0.1%) 32.6%
+ tcourse 134.8 (-1.0%) 32.9%
+ f speaker 136.0 (-0.1%) 32.6%
+ hspeaker 136.1 (-0.0%) 32.8%
+ tspeaker 134.7 (-1.0%) 32.7%
hcourse + tdoc 133.2 32.6%
+ log(c) 132.8 (-0.3%) 32.5%
+ fdoc F 132.8 (-0.4%) 32.5%
+ hdoc 133.0 (-0.2%) 32.5%
+ fcourse 133.1 (-0.1%) 32.5%
+ tcourse 133.0 (-0.1%) 32.6%
+ f speaker 133.1 (-0.1%) 32.5%
+ hspeaker 133.2 (-0.0%) 32.6%
+ tspeaker 133.1 (-0.1%) 32.7%
Table 6: N -gram weighting with feature combinations.
Using the combined feature selection technique,
we notice that the greedily selected features tend to
differ in the choice of document segmentation and
feature type, suggesting that n-gram weighting can
effectively integrate the information provided by the
document metadata. By combining features, we are
able to further reduce the test set WER by a statis-
tically significant (p < 0.001) 0.2% over the best
single feature model.
4.6 Advanced Interpolation
While n-gram weighting with all three features is
able to reduce the test set WER by 1.2% over the
linear interpolation baseline, linear interpolation is
not a particularly effective interpolation technique.
In Table 7, we compare the effectiveness of n-gram
weighting in combination with better interpolation
techniques, such as count merging (CM) (Bacchi-
ani et al, 2006) and generalized linear interpolation
(GLI) (Hsu, 2007). As expected, the use of more
sophisticated interpolation techniques decreases the
perplexity and WER reductions achieved by n-gram
weighting by roughly half for a variety of feature
combinations. However, all improvements remain
statistically significant.
Model Perplexity WER
Linear(L + T) 141.6 33.7%
+ W(hcourse) 136.1 (-3.9%) 32.7%
+ W(tdoc) 133.2 (-5.9%) 32.6%
+ W(fdoc) 132.8 (-6.2%) 32.5%
CM(L + T) 137.9 33.0%
+ W(hcourse) 135.5 (-1.8%) 32.4%
+ W(tdoc) 133.4 (-3.3%) 32.4%
+ W(fdoc) 133.2 (-3.5%) 32.4%
GLIlog(1+c?)(L + T) 135.9 33.0%
+ W(hcourse) 133.0 (-2.2%) 32.4%
+ W(tdoc) 130.6 (-3.9%) 32.4%
+ W(fdoc) 130.5 (-4.2%) 32.4%
Table 7: Effect of interpolation technique. L: Lectures, T:
Textbook.
Feature Parameter Values
hdoc ?L = [3.42, 1.46, 0.12]
?T = [?0.45,?0.35,?0.73]
[?L, ?T] = [0.67, 0.33]
tdoc ?L = [?2.33,?1.63,?1.19]
?T = [1.05, 0.46, 0.12]
[?L, ?T] = [0.68, 0.32]
Table 8: N -gram weighting parameter values. ?L, ?T:
parameters for each order of the Lectures and Textbook
trigram models, ?L,?T: linear interpolation weights.
Although the WER reductions from better inter-
polation techniques are initially statistically signif-
icant, as we add features to n-gram weighting, the
differences among the interpolation methods shrink
significantly. With all three features combined, the
test set WER difference between linear interpolation
and generalized linear interpolation loses its statisti-
cal significance. In fact, we can obtain statistically
the same WER of 32.4% using the simpler model of
count merging and n-gram weighting with hcourse.
5 Analysis
5.1 Weighting Parameters
To obtain further insight into how n-gram weighting
improves the resulting n-gram model, we present in
Table 8 the optimized parameter values for the linear
interpolation model between Lectures and Textbook
using n-gram weighting with hdoc and tdoc features.
Using ?(hw) = exp(?(hw) ? ?) to model the n-
gram weights, a positive value of ?i corresponds to
834
100 300 1000 3000 10000
Development Set Size (Words)
132
134
136
138
140
142
144
P
e
r
p
l
e
x
i
t
y
LI
CM
GLI
LI+W
CM+W
GLI+W
Figure 1: Test set perplexity vs. development set size.
increasing the weights of the ith order n-grams with
positive feature values.
For the hdoc normalized entropy feature, values
close to 1 correspond to n-grams that are evenly dis-
tributed across the documents. When interpolating
Lectures and Textbook, we obtain consistently pos-
itive values for the Lectures component, indicating
a de-emphasis on document-specific terms that are
unlikely to be found in the target computer science
domain. On the other hand, the values correspond-
ing to the Textbook component are consistently neg-
ative, suggesting a reduced weight for mismatched
style terms that appear uniformly across textbook
sections.
For tdoc, values close to 1 correspond to n-grams
ending frequently on topic words with uneven dis-
tribution across documents. Thus, as expected, the
signs of the optimized parameter values are flipped.
By de-emphasizing topic n-grams from off-topic
components and style n-grams from off-style com-
ponents, n-gram weighting effectively improves the
performance of the resulting language model.
5.2 Development Set Size
So far, we have assumed the availability of a large
development set for parameter tuning. To obtain
a sense of how n-gram weighting performs with
smaller development sets, we randomly select utter-
ances from the full development set and plot the test
set perplexity in Figure 1 as a function of the devel-
opment set size for various modeling techniques.
As expected, GLI outperforms both LI and CM.
However, whereas LI and CM essentially converge
in test set perplexity with only 100 words of devel-
100 300 1000 3000 10000
Development Set Size (Words)
32.0
32.5
33.0
33.5
34.0
34.5
35.0
W
E
R
LI
CM
GLI
LI+W
CM+W
GLI+W
Figure 2: Test set WER vs. development set size.
opment data, it takes about 500 words before GLI
converges due to the increased number of parame-
ters. By adding n-gram weighting with the hcourse
feature, we see a significant drop in perplexity for
all models at all development set sizes. However,
the performance does not fully converge until 3,000
words of development set data.
As shown in Figure 2, the test set WER behaves
more erratically, as the parameters are tuned to min-
imize the development set perplexity. Overall, n-
gram weighting decreases the WER significantly,
except when applied to GLI with less than 1000
words of development data when the perplexity of
GLI has not itself converged. In that range, CM with
n-gram weighting performs the best. However, with
more development data, GLI with n-gram weight-
ing generally performs slightly better. From these
results, we conclude that although n-gram weight-
ing increases the number of tuning parameters, they
are effective in improving the test set performance
even with only 100 words of development set data.
5.3 Training Set Size
To characterize the effectiveness of n-gram weight-
ing as a function of the training set size, we evalu-
ate the performance of various interpolated models
with increasing subsets of the Lectures corpus and
the full Textbook corpus. Overall, every doubling of
the number of training set documents decreases both
the test set perplexity and WER by approximately 7
points and 0.8%, respectively. To better compare re-
sults, we plot the performance difference between
various models and linear interpolation in Figures 3
and 4.
835
2 4 8 16 32 64 128 230
Training Set Size (Documents)
-12
-10
-8
-6
-4
-2
0
2
4
P
e
r
p
l
e
x
i
t
y
 
D
i
f
f
e
r
e
n
c
e
LI
CM
GLI
LI+W
CM+W
GLI+W
Figure 3: Test set perplexity vs. training set size.
Interestingly, the peak gain obtained from n-gram
weighting with the hdoc feature appears at around
16 documents for all interpolation techniques. We
suspect that as the number of documents initially
increases, the estimation of the hdoc features im-
proves, resulting in larger perplexity reduction from
n-gram weighting. However, as the diversity of the
training set documents increases beyond a certain
threshold, we experience less document-level spar-
sity. Thus, we see decreasing gain from n-gram
weighting beyond 16 documents.
For all interpolation techniques, even though the
perplexity improvements from n-gram weighting
decrease with more documents, the WER reductions
actually increase. N -gram weighting showed sta-
tistically significant reductions for all configurations
except generalized linear interpolation with less than
8 documents. Although count merging with n-gram
weighting has the lowest WER for most training set
sizes, GLI ultimately achieves the best test set WER
with the full training set.
5.4 Training Corpora
In Table 9, we compare the performance of n-gram
weighting with different combination of training
corpora and interpolation techniques to determine
its effectiveness across different training conditions.
With the exception of interpolating Lectures and
Switchboard using count merging, all other model
combinations yield statistically significant improve-
ments with n-gram weighting using hcourse, tdoc,
and fdoc features.
The results suggest that n-gram weighting with
these features is most effective when interpolating
2 4 8 16 32 64 128 230
Training Set Size (Documents)
-1.4
-1.2
-1.0
-0.8
-0.6
-0.4
-0.2
0.0
0.2
W
E
R
 
D
i
f
f
e
r
e
n
c
e
LI
CM
GLI
LI+W
CM+W
GLI+W
Figure 4: Test set WER vs. training set size.
Model L + T L + S T + S L + T + S
LI 33.7% 36.7% 36.4% 33.6%
LI + W 32.5% 36.4% 35.7% 32.5%
CM 33.0% 36.6% 35.5% 32.9%
CM + W 32.4% 36.5% 35.4% 32.3%
GLI 33.0% 36.6% 35.7% 32.8%
GLI + W 32.4% 36.4% 35.3% 32.2%
Table 9: Test set WER with various training corpus com-
binations. L: Lectures, T: Textbook, S: Switchboard, W:
n-gram weighting.
corpora that differ in how they match the target do-
main. Whereas the Textbook corpus is the only cor-
pus with matching topic, both Lectures and Switch-
board have a similar matching spoken conversa-
tional style. Thus, we see the least benefit from
n-gram weighting when interpolating Lectures and
Switchboard. By combining Lectures, Textbook,
and Switchboard using generalized linear interpola-
tion with n-gram weighting using hcourse, tdoc, and
fdoc features, we achieve our best test set WER of
32.2% on the lecture transcription task, a full 1.5%
over the initial linear interpolation baseline.
6 Conclusion & Future Work
In this work, we presented the n-gram weighting
technique for adjusting the probabilities of n-grams
according to a set of features. By utilizing features
derived from the document segmentation and asso-
ciated metadata inherent in many training corpora,
we achieved up to a 1.2% and 0.6% WER reduc-
tion over the linear interpolation and count merging
baselines, respectively, using n-gram weighting on
a lecture transcription task.
836
We examined the performance of various n-gram
weighting features and generally found entropy-
based features to offer the best predictive perfor-
mance. Although the topic probability features
derived from HMM-LDA labels yield additional
improvements when applied in combination with
the normalized entropy features, the computational
cost of performing HMM-LDA may not justify the
marginal benefit in all scenarios.
In situations where the document boundaries are
unavailable or when finer segmentation is desired,
automatic techniques for document segmentation
may be applied (Malioutov and Barzilay, 2006).
Synthetic metadata information may also be ob-
tained via clustering techniques (Steinbach et al,
2000). Although we have primarily focused on n-
gram weighting features derived from segmentation
information, it is also possible to consider other fea-
tures that correlate with n-gram relevance.
N -gram weighting and other approaches to cross-
domain language modeling require a matched devel-
opment set for model parameter tuning. Thus, for
future work, we plan to investigate the use of the ini-
tial recognition hypotheses as the development set,
as well as manually transcribing a subset of the test
set utterances.
As speech and natural language applications shift
towards novel domains with limited matched train-
ing data, better techniques are needed to maximally
utilize the often abundant partially matched data. In
this work, we examined the effectiveness of the n-
gram weighting technique for estimating language
models in these situations. With similar investments
in acoustic modeling and other areas of natural lan-
guage processing, we look forward to an ever in-
creasing diversity of practical speech and natural
language applications.
Availability An implementation of the n-gram
weighting algorithm is available in the MIT Lan-
guage Modeling (MITLM) toolkit (Hsu and Glass,
2008): http://www.sls.csail.mit.edu/mitlm/.
Acknowledgments
We would like to thank the anonymous reviewers for
their constructive feedback. This research is sup-
ported in part by the T-Party Project, a joint research
program between MIT and Quanta Computer Inc.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochastic
grammars. Computer Speech & Language, 20(1):41?
68.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: Review and perspectives. Speech Commu-
nication, 42(1):93?108.
Hung-An Chang. 2008. Large margin Gaussian mix-
ture modeling for automatic speech recognition. Mas-
sachusetts Institute of Technology. Masters Thesis.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. In Technical Report TR-10-98. Computer Science
Group, Harvard University.
Jianfeng Gao and Kai-Fu Lee. 2000. Distribution-based
pruning of backoff language models. In Proc. Asso-
ciation of Computational Linguistics, pages 579?588,
Hong Kong, China.
James Glass, Timothy J. Hazen, Lee Hetherington, and
Chao Wang. 2004. Analysis and processing of lecture
audio data: Preliminary investigations. In Proc. HLT-
NAACL Workshop on Interdisciplinary Approaches to
Speech Indexing and Retrieval, pages 9?12, Boston,
MA, USA.
James Glass, Timothy J. Hazen, Scott Cyphers, Igor
Malioutov, David Huynh, and Regina Barzilay. 2007.
Recent progress in the MIT spoken lecture process-
ing project. In Proc. Interspeech, pages 2553?2556,
Antwerp, Belgium.
James Glass. 2003. A probabilistic framework for
segment-based speech recognition. Computer Speech
& Language, 17(2-3):137?152.
John J. Godfrey and Ed Holliman. 1993. Switchboard-1
transcripts. Linguistic Data Consortium, Philadelphia,
PA, USA.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in Neural Information Processing
Systems 17, pages 537?544. MIT Press, Cambridge,
MA, USA.
T.J. Hazen. 2006. Automatic alignment and error cor-
rection of human generated transcripts for long speech
recordings. In Proc. Interspeech, Pittsburgh, PA,
USA.
Bo-June (Paul) Hsu and James Glass. 2006. Style &
topic language model adaptation using HMM-LDA.
In Proc. Empirical Methods in Natural Language Pro-
cessing, pages 373?381, Sydney, Australia.
Bo-June (Paul) Hsu and James Glass. 2008. Iterative
language model estimation: Efficient data structure &
algorithms. In Proc. Interspeech, Brisbane, Australia.
837
Bo-June (Paul) Hsu. 2007. Generalized linear interpola-
tion of language models. In Proc. Automatic Speech
Recognition and Understanding, pages 136?140, Ky-
oto, Japan.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
Association for Computational Linguistics, pages 25?
32, Sydney, Australia.
D. Pallet, W. Fisher, and Fiscus. 1990. Tools for the anal-
ysis of benchmark speech recognition tests. In Proc.
ICASSP, Albuquerque, NM, USA.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical Recipes.
Cambridge University Press, 3rd edition.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A comparison of document clustering tech-
niques. Technical Report #00-034, University of Min-
nesota.
838
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 86?93,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Syntactic Phrase Reordering for English-to-Arabic Statistical Machine
Translation
Ibrahim Badr Rabih Zbib
Computer Science and Artificial Intelligence Lab
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{iab02, rabih, glass}@csail.mit.edu
James Glass
Abstract
Syntactic Reordering of the source lan-
guage to better match the phrase struc-
ture of the target language has been
shown to improve the performance of
phrase-based Statistical Machine Transla-
tion. This paper applies syntactic reorder-
ing to English-to-Arabic translation. It in-
troduces reordering rules, and motivates
them linguistically. It also studies the ef-
fect of combining reordering with Ara-
bic morphological segmentation, a pre-
processing technique that has been shown
to improve Arabic-English and English-
Arabic translation. We report on results in
the news text domain, the UN text domain
and in the spoken travel domain.
1 Introduction
Phrase-based Statistical Machine Translation has
proven to be a robust and effective approach to
machine translation, providing good performance
without the need for explicit linguistic informa-
tion. Phrase-based SMT systems, however, have
limited capabilities in dealing with long distance
phenomena, since they rely on local alignments.
Automatically learned reordering models, which
can be conditioned on lexical items from both the
source and the target, provide some limited re-
ordering capability when added to SMT systems.
One approach that explicitly deals with long
distance reordering is to reorder the source side
to better match the target side, using predefined
rules. The reordered source is then used as input
to the phrase-based SMT system. This approach
indirectly incorporates structure information since
the reordering rules are applied on the parse trees
of the source sentence. Obviously, the same re-
ordering has to be applied to both training data and
test data. Despite the added complexity of parsing
the data, this technique has shown improvements,
especially when good parses of the source side ex-
ist. It has been successfully applied to German-to-
English and Chinese-to-English SMT (Collins et
al., 2005; Wang et al, 2007).
In this paper, we propose the use of a similar
approach for English-to-Arabic SMT. Unlike most
other work on Arabic translation, our work is in
the direction of the more morphologically com-
plex language, which poses unique challenges. We
propose a set of syntactic reordering rules on the
English source to align it better to the Arabic tar-
get. The reordering rules exploit systematic differ-
ences between the syntax of Arabic and the syntax
of English; they specifically address two syntac-
tic constructs. The first is the Subject-Verb order
in independent sentences, where the preferred or-
der in written Arabic is Verb-Subject. The sec-
ond is the noun phrase structure, where many dif-
ferences exist between the two languages, among
them the order of adjectives, compound nouns
and genitive constructs, as well as the way defi-
niteness is marked. The implementation of these
rules is fairly straightforward since they are ap-
plied to the parse tree. It has been noted in previ-
ous work (Habash, 2007) that syntactic reordering
does not improve translation if the parse quality is
not good enough. Since in this paper our source
language is English, the parses are more reliable,
and result in more correct reorderings. We show
that using the reordering rules results in gains in
the translation scores and study the effect of the
training data size on those gains.
This paper also investigates the effect of using
morphological segmentation of the Arabic target
86
in combination with the reordering rules. Mor-
phological segmentation has been shown to benefit
Arabic-to-English (Habash and Sadat, 2006) and
English-to-Arabic (Badr et al, 2008) translation,
although the gains tend to decrease with increas-
ing training data size.
Section 2 provides linguistic motivation for the
paper. It describes the rich morphology of Arabic,
and its implications on SMT. It also describes the
syntax of the verb phrase and noun phrase in Ara-
bic, and how they differ from their English coun-
terparts. In Section 3, we describe some of the rel-
evant previous work. In Section 4, we present the
preprocessing techniques used in the experiments.
Section 5 describes the translation system, the data
used, and then presents and discusses the experi-
mental results from three domains: news text, UN
data and spoken dialogue from the travel domain.
The final section provides a brief summary and
conclusion.
2 Arabic Linguistic Issues
2.1 Arabic Morphology
Arabic has a complex morphology compared to
English. The Arabic noun and adjective are in-
flected for gender and number; the verb is inflected
in addition for tense, voice, mood and person.
Various clitics can attach to words as well: Con-
junctions, prepositions and possessive pronouns
attach to nouns, and object pronouns attach to
verbs. The example below shows the decompo-
sition into stems and clitics of the Arabic verb
phrase wsyqAblhm1 and noun phrase wbydh, both
of which are written as one word:
(1) a. w+
and
s+
will
yqAbl
meet-3SM
+hm
them
and he will meet them
b. w+
and
b+
with
yd
hand
+h
his
and with his hand
An Arabic corpus will, therefore, have more
surface forms than an equivalent English corpus,
and will also be sparser. In the LDC news corpora
used in this paper (see Section 5.2), the average
English sentence length is 33 words compared to
the Arabic 25 words.
1All examples in this paper are writ-
ten in the Buckwalter Transliteration System
(http://www.qamus.org/transliteration.htm)
Although the Arabic language family consists
of many dialects, none of them has a standard
orthography. This affects the consistency of the
orthography of Modern Standard Arabic (MSA),
the only written variety of Arabic. Certain char-
acters are written inconsistently in different data
sources: Final ?y? is sometimes written as ?Y? (Alif
mqSwrp), and initial Alif hamza (The Buckwal-
ter characters ?<? and ?{?) are written as bare alif
(A). Arabic is usually written without the diacritics
that denote short vowels. This creates an ambigu-
ity at the word level, since a word can have more
than one reading. These factors adversely affect
the performance of Arabic-to-English SMT, espe-
cially in the English-to-Arabic direction.
Simple pattern matching is not enough to per-
form morphological analysis and decomposition,
since a certain string of characters can, in princi-
ple, be either an affixed morpheme or part of the
base word itself. Word-level linguistic information
as well as context analysis are needed. For exam-
ple the written form wly can mean either ruler or
and for me, depending on the context. Only in the
latter case should it be decomposed.
2.2 Arabic Syntax
In this section, we describe a number of syntactic
facts about Arabic which are relevant to the
reordering rules described in Section 4.2.
Clause Structure
In Arabic, the main sentence usually has
the order Verb-Subject-Object (VSO). The order
Subject-Verb-Object (SVO) also occurs, but is less
frequent than VSO. The verb agrees with the sub-
ject in gender and number in the SVO order, but
only in gender in the VSO order (Examples 2c and
2d).
(2) a. Akl
ate-3SM
Alwld
the-boy
AltfAHp
the-apple
the boy ate the apple
b. Alwld
the-boy
Akl
ate-3SM
AltfAHp
the-apple
the boy ate the apple
c. Akl
ate-3SM
AlAwlAd
the-boys
AltfAHAt
the-apples
the boys ate the apples
d. AlAwlAd
the-boys
AklwA
ate-3PM
AltfAHAt
the-apples
the boys ate the apples
87
In a dependent clause, the order must be SVO,
as illustrated by the ungrammaticality of Exam-
ple 3b below. As we discuss in more detail later,
this distinction between dependent and indepen-
dent clauses has to be taken into account when the
syntactic reordering rules are applied.
(3) a. qAl
said-3SM
An
that
Alwld
the-boy
Akl
ate
AltfAHp
the-apple
he said that the boy ate the apple
b. *qAl
said-3SM
An
that
Akl
ate
Alwld
the-boy
AltfAHp
the-apple
he said that the boy ate the apple
Another pertinent fact is that the negation parti-
cle has to always preceed the verb:
(4) lm
not
yAkl
eat-3SM
Alwld
the-boy
AltfAHp
the-apple
the boy did not eat the apple
Noun Phrase
The Arabic noun phrase can have constructs
that are quite different from English. The adjective
in Arabic follows the noun that it modifies, and it
is marked with the definite article, if the head noun
is definite:
(5) AlbAb
the-door
Alkbyr
the-big
the big door
The Arabic equivalent of the English posses-
sive, compound nouns and the of -relationship is
the Arabic idafa construct, which compounds two
or more nouns. Therefore, N1?s N2 and N2 of N1
are both translated as N2 N1 in Arabic. As Exam-
ple 6b shows, this construct can also be chained
recursively.
(6) a. bAb
door
Albyt
the-house
the house?s door
b. mftAH
key
bAb
door
Albyt
the-house
The key to the door of the house
Example 6 also shows that an idafa construct is
made definite by adding the definite article Al- to
the last noun in the noun phrase. Adjectives follow
the idafa noun phrase, regardless of which noun in
the chain they modify. Thus, Example 7 is am-
biguous in that the adjective kbyr (big) can modify
any of the preceding three nouns. The same is true
for relative clauses that modify a noun.
(7) mftAH
key
bAb
door
Albyt
the-house
Alkbyr
the-big
These and other differences between the Arabic
and English syntax are likely to affect the qual-
ity of automatic alignments, since corresponding
words will occupy positions in the sentence that
are far apart, especially when the relevant words
(e.g. the verb and its subject) are separated by sub-
ordinate clauses. In such cases, the lexicalized dis-
tortion models used in phrase-based SMT do not
have the capability of performing reorderings cor-
rectly. This limitation adversely affects the trans-
lation quality.
3 Previous Work
Most of the work in Arabic machine translation
is done in the Arabic-to-English direction. The
other direction, however, is also important, since
it opens the wealth of information in different do-
mains that is available in English to the Arabic
speaking world. Also, since Arabic is a morpho-
logically richer language, translating into Arabic
poses unique issues that are not present in the
opposite direction. The only works on English-
to-Arabic SMT that we are aware of are Badr et
al. (2008), and Sarikaya and Deng (2007). Badr
et al show that using segmentation and recom-
bination as pre- and post- processing steps leads
to significant gains especially for smaller train-
ing data corpora. Sarikaya and Deng use Joint
Morphological-Lexical Language Models to re-
rank the output of an English-to-Arabic MT sys-
tem. They use regular expression-based segmen-
tation of the Arabic so as not to run into recombi-
nation issues on the output side.
Similarly, for Arabic-to-English, Lee (2004),
and Habash and Sadat (2006) show that vari-
ous segmentation schemes lead to improvements
that decrease with increasing parallel corpus size.
They use a trigram language model and the Ara-
bic morphological analyzer MADA (Habash and
Rambow, 2005) respectively, to segment the Ara-
bic side of their corpora. Other work on Arabic-
to-English SMT tries to address the word reorder-
ing problem. Habash (2007) automatically learns
syntactic reordering rules that are then applied to
the Arabic side of the parallel corpora. The words
are aligned in a sentence pair, then the Arabic sen-
tence is parsed to extract reordering rules based on
how the constituents in the parse tree are reordered
on the English side. No significant improvement is
88
shown with reordering when compared to a base-
line that uses a non-lexicalized distance reordering
model. This is attributed in the paper to the poor
quality of parsing.
Syntax-based reordering as a preprocessing step
has been applied to many language pairs other
than English-Arabic. Most relevant to the ap-
proach in this paper are Collins et al (2005)
and Wang et al (2007). Both parse the source
side and then reorder the sentence based on pre-
defined, linguistically motivated rules. Signifi-
cant gain is reported for German-to-English and
Chinese-to-English translation. Both suggest that
reordering as a preprocessing step results in bet-
ter alignment, and reduces the reliance on the dis-
tortion model. Popovic and Ney (2006) use sim-
ilar methods to reorder German by looking at the
POS tags for German-to-English and German-to-
Spanish. They show significant improvements on
test set sentences that do get reordered as well
as those that don?t, which is attributed to the im-
provement of the extracted phrases. (Xia and
McCord, 2004) present a similar approach, with
a notable difference: the re-ordering rules are au-
tomatically learned from aligning parse trees for
both the source and target sentences. They report
a 10% relative gain for English-to-French trans-
lation. Although target-side parsing is optional
in this approach, it is needed to take full advan-
tage of the approach. This is a bigger issue when
no reliable parses are available for the target lan-
guage, as is the case in this paper. More generally,
the use of automatically-learned rules has the ad-
vantage of readily applicable to different language
pairs. The use of deterministic, pre-defined rules,
however, has the advantage of being linguistically
motivated, since differences between the two lan-
guages are addressed explicitly. Moreover, the im-
plementation of pre-defined transfer rules based
on target-side parses is relatively easy and cheap
to implement in different language pairs.
Generic approaches for translating from En-
glish to more morphologically complex languages
have been proposed. Koehn and Hoang (2007)
propose Factored Translation Models, which ex-
tend phrase-based statistical machine translation
by allowing the integration of additional morpho-
logical features at the word level. They demon-
strate improvements for English-to-German and
English-to-Czech. Tighter integration of fea-
tures is claimed to allow for better modeling of
the morphology and hence is better than using
pre-processing and post-processing techniques.
Avramidis and Koehn (2008) enrich the English
side by adding a feature to the Factored Model that
models noun case agreement and verb person con-
jugation, and show that it leads to a more gram-
matically correct output for English-to-Greek and
English-to-Czech translation. Although Factored
Models are well equipped for handling languages
that differ in terms of morphology, they still use
the same distortion reordering model as a phrase-
based MT system.
4 Preprocessing Techniques
4.1 Arabic Segmentation and Recombination
It has been shown previously work (Badr et al,
2008; Habash and Sadat, 2006) that morphologi-
cal segmentation of Arabic improves the transla-
tion performance for both Arabic-to-English and
English-to-Arabic by addressing the problem of
sparsity of the Arabic side. In this paper, we use
segmented and non-segmented Arabic on the tar-
get side, and study the effect of the combination of
segmentation with reordering.
As mentioned in Section 2.1, simple pattern
matching is not enough to decompose Arabic
words into stems and affixes. Lexical information
and context are needed to perform the decompo-
sition correctly. We use the Morphological Ana-
lyzer MADA (Habash and Rambow, 2005) to de-
compose the Arabic source. MADA uses SVM-
based classifiers of features (such as POS, num-
ber, gender, etc.) to score the different analyses
of a given word in context. We apply morpho-
logical decomposition before aligning the training
data. We split the conjunction and preposition pre-
fixes, as well as possessive and object pronoun suf-
fixes. We then glue the split morphemes into one
prefix and one suffix, such that any given word is
split into at most three parts: prefix+ stem +suffix.
Note that plural markers and subject pronouns are
not split. For example, the word wlAwlAdh (?and
for his children?) is segmented into wl+ AwlAd
+P:3MS.
Since training is done on segmented Arabic, the
output of the decoder must be recombined into its
original surface form. We follow the approach of
Badr et. al (2008) in combining the Arabic out-
put, which is a non-trivial task for several reasons.
First, the ending of a stem sometimes changes
when a suffix is attached to it. Second, word end-
89
ings are normalized to remove orthographic incon-
sistency between different sources (Section 2.1).
Finally, some words can recombine into more than
one grammatically correct form. To address these
issues, a lookup table is derived from the training
data that maps the segmented form of the word to
its original form. The table is also useful in re-
combining words that are erroneously segmented.
If a certain word does not occur in the table, we
back off to a set of manually defined recombina-
tion rules. Word ambiguity is resolved by picking
the more frequent surface form.
4.2 Arabic Reordering Rules
This section presents the syntax-based rules used
for re-ordering the English source to better match
the syntax of the Arabic target. These rules are
motivated by the Arabic syntactic facts described
in Section 2.2.
Much like Wang et al (2007), we parse the En-
glish side of our corpora and reorder using prede-
fined rules. Reordering the English can be done
more reliably than other source languages, such
as Arabic, Chinese and German, since the state-
of-the-art English parsers are considerably better
than parsers of other languages. The following
rules for reordering at the sentence level and the
noun phrase level are applied to the English parse
tree:
1. NP: All nouns, adjectives and adverbs in the
noun phrase are inverted. This rule is moti-
vated by the order of the adjective with re-
spect to its head noun, as well as the idafa
construct (see Examples 6 and 7 in Section
2.2. As a result of applying this rule, the
phrase the blank computer screen becomes
the screen computer blank .
2. PP: All prepositional phrases of the form
N1ofN2 ...ofNn are transformed to
N1N2 ...Nn . All N i are also made indefi-
nite, and the definite article is added to Nn ,
the last noun in the chain. For example, the
phrase the general chief of staff of the armed
forces becomes general chief staff the armed
forces. We also move all adjectives in the
top noun phrase to the end of the construct.
So the real value of the Egyptian pound
becomes value the Egyptian pound real. This
rule is motivated by the idafa construct and
its properties (see Example 6).
3. the: The definite article the is replicated be-
fore adjectives (see Example 5 above). So the
blank computer screen becomes the blank the
computer the screen. This rule is applied af-
ter NP rule abote. Note that we do not repli-
cate the before proper names.
4. VP: This rule transforms SVO sentences to
VSO. All verbs are reordered on the condi-
tion that they have their own subject noun
phrase and are not in the participle form,
since in these cases the Arabic subject occurs
before the verb participle. We also check that
the verb is not in a relative clause with a that
complementizer (Example 3 above). The fol-
lowing example illustrates all these cases: the
health minister stated that 11 police officers
were wounded in clashes with the demonstra-
tors? stated the health minister that 11 po-
lice officers were wounded in clashes with the
demonstrators. If the verb is negated, the
negative particle is moved with the verb (Ex-
ample 4. Finally, if the object of the reordered
verb is a pronoun, it is reordered with the
verb. Example: the authorities gave us all
the necessary help becomes gave us the au-
thorities all the necessary help.
The transformation rules 1, 2 and 3 are applied
in this order, since they interact although they do
not conflict. So, the real value of the Egyptian
pound ? value the Egyptian the pound the real
The VP reordering rule is independent.
5 Experiments
5.1 System description
For the English source, we first tokenize us-
ing the Stanford Log-linear Part-of-Speech Tag-
ger (Toutanova et al, 2003). We then proceed
to split the data into smaller sentences and tag
them using Ratnaparkhi?s Maximum Entropy Tag-
ger (Ratnaparkhi, 1996). We parse the data us-
ing the Collins Parser (Collins, 1997), and then
tag person, location and organization names us-
ing the Stanford Named Entity Recognizer (Finkel
et al, 2005). On the Arabic side, we normalize
the data by changing final ?Y? to ?y?, and chang-
ing the various forms of Alif hamza to bare Alif,
since these characters are written inconsistently in
some Arabic sources. We then segment the data
using MADA according to the scheme explained
in Section 4.1.
90
The English source is aligned to the seg-
mented Arabic target using the standard
MOSES (MOSES, 2007) configuration of
GIZA++ (Och and Ney, 2000), which is IBM
Model 4, and decoding is done using the phrase-
based SMT system MOSES. We use a maximum
phrase length of 15 to account for the increase
in length of the segmented Arabic. We also
use a lexicalized bidirectional reordering model
conditioned on both the source and target sides,
with a distortion limit set to 6. We tune using
Och?s algorithm (Och, 2003) to optimize weights
for the distortion model, language model, phrase
translation model and word penalty over the
BLEU metric (Papineni et al, 2001). For the
segmented Arabic experiments, we experiment
with tuning using non-segmented Arabic as a
reference. This is done by recombining the output
before each tuning iteration is scored and has been
shown by Badr et. al (2008) to perform better than
using segmented Arabic as reference.
5.2 Data Used
We report results on three domains: newswire text,
UN data and spoken dialogue from the travel do-
main. It is important to note that the sentences
in the travel domain are much shorter than in the
news domain, which simplifies the alignment as
well as reordering during decoding. Also, since
the travel domain contains spoken Arabic, it is
more biased towards the Subject-Verb-Object sen-
tence order than the Verb-Subject-Object order
more common in the news domain. Also note
that since most of our data was originally intended
for Arabic-to-English translation, our test and tun-
ing sets have only one reference, and therefore,
the BLEU scores we report are lower than typi-
cal scores reported in the literature on Arabic-to-
English.
The news training data consists of several LDC
corpora2. We construct a test set by randomly
picking 2000 sentences. We pick another 2000
sentences randomly for tuning. Our final training
set consists of 3 million English words. We also
test on the NIST MT 05 ?test set while tuning on
both the NIST MT 03 and 04 test sets. We use the
first English reference of the NIST test sets as the
source, and the Arabic source as our reference. For
2LDC2003E05 LDC2003E09 LDC2003T18
LDC2004E07 LDC2004E08 LDC2004E11 LDC2004E72
LDC2004T18 LDC2004T17 LDC2005E46 LDC2005T05
LDC2007T24
Scheme
RandT MT 05
S NoS S NoS
Baseline 21.6 21.3 23.88 23.44
VP 21.9 21.5 23.98 23.58
NP 21.9 21.8
NP+PP 21.8 21.5 23.72 23.68
NP+PP+VP 22.2 21.8 23.74 23.16
NP+PP+VP+The 21.3 21.0
Table 1: Translation Results for the News Domain
in terms of the BLEU Metric.
the language model, we use 35 million words from
the LDC Arabic Gigaword corpus, plus the Arabic
side of the 3 million word training corpus. Exper-
imentation with different language model orders
shows that the optimal model orders are 4-grams
for the baseline system and 6-grams for the seg-
mented Arabic. The average sentence length is 33
for English, 25 for non-segmented Arabic and 36
for segmented Arabic.
To study the effect of syntactic reordering on
larger training data sizes, we use the UN English-
Arabic parallel text (LDC2003T05). We experi-
ment with two training data sizes: 30 million and
3 million words. The test and tuning sets are
comprised of 1500 and 500 sentences respectively,
chosen at random.
For the spoken domain, we use the BTEC 2007
Arabic-English corpus. The training set consists
of 200K words, the test set has 500 sentences and
the tuning set has 500 sentences. The language
model consists of the Arabic side of the training
data. Because of the significantly smaller data
size, we use a trigram LM for the baseline, and
a 4-gram for segmented Arabic. In this case, the
average sentence length is 9 for English, 8 for Ara-
bic, and 10 for segmented Arabic.
5.3 Translation Results
The translation scores for the News domain are
shown in Table 1. The notation used in the table is
as follows:
? S: Segmented Arabic
? NoS: Non-Segmented Arabic
? RandT: Scores for test set where sentences
were picked at random from NEWS data
? MT 05: Scores for the NIST MT 05 test set
The reordering notation is explained in Section
4.2. All results are in terms of the BLEU met-
91
S NoS
Short Long Short Long
Baseline 22.57 25.22 22.40 24.33
VP 22.95 25.05 22.95 24.02
NP+PP 22.71 24.76 23.16 24.067
NP+PP+VP 22.84 24.62 22.53 24.56
Table 2: Translation Results depending on sen-
tence length for NIST test set.
Scheme Score % Oracle reord
VP 25.76 59%
NP+PP 26.07 58%
NP+PP+VP 26.17 53%
Table 3: Oracle scores for combining baseline sys-
tem with other reordered systems.
ric. It is important to note that the gain that we
report in terms of BLEU are more significant that
comparable gains on test sets that have multiple
references, since our test sets have only one refer-
ence. Any amount of gain is a result of additional
n-gram precision with one reference. We note that
the gain achieved from the reordering of the non-
segmented and segmented systems are compara-
ble. Replicating the before adjectives hurts the
scores, possibly because it increases the sentence
length noticeably, and thus deteriorates the align-
ments? quality. We note that the gains achieved by
reordering on the NIST test set are smaller than
the improvements on the random test set. This is
due to the fact that the sentences in the NIST test
set are longer, which adversely affects the parsing
quality. The average English sentence length is 33
words in the NIST test set, while the random test
set has an average sentence length of 29 words.
Table 2 shows the reordering gains of the non-
segmented Arabic by sentence length. Short sen-
tences are sentences that have less that 40 words of
English, while long sentences have more than 40
words. Out of the 1055 sentence in the NIST test
set 719 are short and 336 are long. We also report
oracle scores in Table 3 for combining the base-
line system with the reordering systems, as well
as the percentage of oracle sentences produced by
the reordered system. The oracle score is com-
puted by starting with the reordered system?s can-
didate translations and iterating over all the sen-
tences one by one: we replace each sentence with
its corresponding baseline system translation then
Scheme 30M 3M
Baseline 32.17 28.42
VP 32.46 28.60
NP+PP 31.73 28.80
Table 4: Translation Results on segmentd UN data
in terms of the BLEU Metric.
compute the total BLEU score of the entire set. If
the score improves, then the sentence in question
is replaced with the baseline system?s translation,
otherwise it remains unchanged and we move on
to the next one.
In Table 4, we report results on the UN corpus
for different training data sizes. It is important to
note that although gains from VP reordering stay
constant when scaled to larger training sets, gains
from NP+PP reordering diminish. This is due to
the fact that NP reordering tend to be more local-
ized then VP reorderings. Hence with more train-
ing data the lexicalized reordering model becomes
more effective in reordering NPs.
In Table 5, we report results on the BTEC
corpus for different segmentation and reordering
scheme combinations. We should first point out
that all sentences in the BTEC corpus are short,
simple and easy to align. Hence, the gain intro-
duced by reordering might not be enough to offset
the errors introduced by the parsing. We also note
that spoken Arabic usually prefers the Subject-
Verb-Object sentence order, rather than the Verb-
Subject-Object sentence order of written Arabic.
This explains the fact that no gain is observed
when the verb phrase is reordered. Noun phrase
reordering produces a significant gain with non-
segmented Arabic. Replicating the definite arti-
cle the in the noun phrase does not create align-
ment problems as is the case with the newswire
data, since the sentences are considerably shorter,
and hence the 0.74 point gain observed on the seg-
mented Arabic system. That gain does not trans-
late to the non-segmented Arabic system since in
that case the definite article Al remains attached to
its head word.
6 Conclusion
This paper presented linguistically motivated rules
that reorder English to look like Arabic. We
showed that these rules produce significant gains.
We also studied the effect of the interaction be-
tween Arabic morphological segmentation and
92
Scheme S NoS
Baseline 29.06 25.4
VP 26.92 23.49
NP 27.94 26.83
NP+PP 28.59 26.42
The 29.8 25.1
Table 5: Translation Results for the Spoken Lan-
guage Domain in the BLEU Metric.
syntactic reordering on translation results, as well
as how they scale to bigger training data sizes.
Acknowledgments
We would like to thank Michael Collins, Ali Mo-
hammad and Stephanie Seneff for their valuable
comments.
References
Eleftherios Avramidis, and Philipp Koehn 2008. En-
riching Morphologically Poor Languages for Statis-
tical Machine Translation. In Proc. of ACL/HLT.
Ibrahim Badr, Rabih Zbib, and James Glass 2008. Seg-
mentation for English-to-Arabic Statistical Machine
Translation. In Proc. of ACL/HLT.
Michael Collins 1997. Three Generative, Lexicalized
Models for Statistical Parsing. In Proc. of ACL.
Michael Collins, Philipp Koehn, and Ivona Kucerova
2005. Clause Restructuring for Statistical Machine
Translation. In Proc. of ACL.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proc. of ACL.
Nizar Habash, 2007. Syntactic Preprocessing for Sta-
tistical Machine Translation. In Proc. of the Ma-
chine Translation Summit (MT-Summit).
Nizar Habash and Owen Rambow, 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proc. of
ACL.
Nizar Habash and Fatiha Sadat, 2006. Arabic Pre-
processing Schemes for Statistical Machine Trans-
lation. In Proc. of HLT.
Philipp Koehn and Hieu Hoang, 2007. Factored
Translation Models. In Proc. of EMNLP/CNLL.
Young-Suk Lee, 2004. Morphological Analysis
for Statistical Machine Translation. In Proc. of
EMNLP.
MOSES, 2007. A Factored Phrase-based Beam-
search Decoder for Machine Translation. URL:
http://www.statmt.org/moses/.
Franz Och 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of ACL.
Franz Och and Hermann Ney 2000. Improved Statisti-
cal Alignment Models. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu 2001. BLUE: a Method for Automatic
Evaluation of Machine Translation. In Proc. of
ACL.
Maja Popovic and Hermann Ney 2006. POS-based
Word Reordering for Statistical Machine Transla-
tion. In Proc. of NAACL LREC.
Adwait Ratnaparkhi 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Proc. of
EMNLP.
Ruhi Sarikaya and Yonggang Deng 2007. Joint
Morphological-Lexical Language Modeling for Ma-
chine Translation. In Proc. of NAACL HLT.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proc. of NAACL HLT.
Chao Wang, Michael Collins, and Philipp Koehn 2007.
Chinese Syntactic Reordering for Statistical Ma-
chine Translation. In Proc. of EMNLP.
Fei Xia and Michael McCord 2004. Improving a
Statistical MT System with Automatically Learned
Rewrite Patterns. In COLING.
93
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 28?29,
Vancouver, October 2005.
THE MIT SPOKEN LECTURE PROCESSING PROJECT 
 
James R. Glass, Timothy J. Hazen, D. Scott Cyphers, Ken Schutte and Alex Park 
The MIT Computer Science and Artificial Intelligence Laboratory 
32 Vassar Street, Cambridge, Massachusetts, 02476, USA 
{hazen,jrg,cyphers}@csail.mit.edu 
 
Abstract 
We will demonstrate the MIT Spoken Lecture 
Processing Server and an accompanying lecture 
browser that students can use to quickly locate and 
browse lecture segments that apply to their query. 
We will show how lecturers can upload recorded 
lectures and companion text material to our server 
for automatic processing. The server automatically 
generates a time-aligned word transcript of the lec-
ture which can be downloaded for use within a 
browser. We will also demonstrate a browser we 
have created which allows students to quickly lo-
cate and browse audio segments that are relevant to 
their query. These tools can provide students with 
easier access to audio (or audio/visual) lectures, 
hopefully improving their educational experience. 
 
1 Introduction 
Over the past decade there has been increasing 
amounts of educational material being made avail-
able on-line. Projects such as MIT OpenCourse-
Ware provide continuous worldwide access to 
educational materials to help satisfy our collective 
thirst for knowledge. While the majority of such 
material is currently text-based, we are beginning 
to see dramatic increases in the amount of audio 
and visual recordings of lecture material. Unlike 
text materials, untranscribed audio data can be te-
dious to browse, making it difficult to utilize the 
information fully without time-consuming data 
preparation. Moreover, unlike some other forms of 
spoken communication such as telephone conver-
sations or television and radio broadcasts, lecture 
processing has until recently received little atten-
tion or benefit from the development of human 
language technology. The single biggest effort, to 
date, is on-going work in Japan using the Corpus 
of Spontaneous Japanese [1,3,4]. 
Lectures are particularly challenging for auto-
matic speech recognizers because the vocabulary 
used within a lecture can be very technical and 
specialized, yet the speaking style can be very 
spontaneous. As a result, even if parallel text mate-
rials are available in the form of textbooks or re-
lated papers, there are significant linguistic 
differences between written and oral communica-
tion styles. Thus, it is a challenge to predict how a 
written passage might be spoken, and vice versa. 
By helping to focus a research spotlight on spoken 
lecture material, we hope to begin to overcome 
these and many other fundamental issues. 
While audio-visual lecture processing will per-
haps be ultimately most useful, we have initially 
focused our attention on the problem of spoken 
lecture processing. Within this realm there are 
many challenging research issues pertaining to the 
development of effective automatic transcription, 
indexing, and summarization. For this project, our 
goals have been to a) help create a corpus of spo-
ken lecture material for the research community, b) 
analyze this corpus to better understand the lin-
guistic characteristics of spoken lectures, c) per-
form speech recognition and information retrieval 
experiments on these data to benchmark perform-
ance on these data, d) develop a prototype spoken 
lecture processing server that will allow educators 
to automatically annotate their recorded lecture 
data, and e) develop prototype software that will 
allow students to browse the resulting annotated 
lectures. 
2 Project Details 
As mentioned earlier, we have developed a web-
based Spoken Lecture Processing Server 
(http://groups.csail.mit.edu/sls/lectures) in which 
users can upload audio files for automatic tran-
scription and indexing. In our work, we have ex-
28
perimented with collecting audio data using a 
small personal digital audio recorder (an iRiver 
N10). To help the speech recognizer, users can 
provide their own supplemental text files, such as 
journal articles, book chapters, etc., which can be 
used to adapt the language model and vocabulary 
of the system.  Currently, the key steps of the tran-
scription process are as follows: a) adapt a topic-
independent vocabulary and language model using 
any supplemental text materials, b) automatically 
segment the audio file into short chunks of pause-
delineated speech, and c) automatically annotate 
these chunks using a speech recognition system. 
Language model adaptation is performed is two 
steps. First the vocabulary of any supplemental text 
material is extracted and added to an existing 
topic-independent vocabulary of nearly 17K 
words. Next, the recognizer merges topic-
independent word sequence statistics from an 
existing corpus of lecture material with the topic-
dependent statistics of the supplemental material to 
create a topic-adapted language model. 
The segmentation algorithm is performed in two 
steps. First the audio file is arbitrarily broken into 
10-second chunks for speech detection processing 
using an efficient speaker-independent phonetic 
recognizer. To help improve its speech detection 
accuracy, this recognizer contains models for non-
lexical artifacts such as laughs and coughs as well 
as a variety of other noises. Contiguous regions of 
speech are identified from the phonetic recognition 
output (typically 6 to 8 second segments of speech) 
and passed alone to our speech recognizer for 
automatic transcription. The speech segmentation 
and transcription steps are currently performed in a 
distributed fashion over a bank of computation 
servers. Once recognition is completed, the audio 
data is indexed (based on the recognition output) in 
preparation for browsing by the user. 
The lecture browser provides a graphical user in-
terface to one or more automatically transcribed 
lectures. A user can type a text query to the 
browser and receive a list of hits within the in-
dexed lectures. When a hit is selected, it is shown 
in the context of the lecture transcription. The user 
can adjust the duration of context preceding and 
following the hit, navigate to and from the preced-
ing and following parts of the lecture, and listen to 
the displayed segment. Orthographic segments are 
highlighted as they are played. 
 
3 Experimental Results 
To date we have collected and analyzed a corpus 
of approximately 300 hours of audio lectures in-
cluding 6 full MIT courses and 80 hours of semi-
nars from the MIT World web site [2]. We are 
currently in the process of expanding this corpus. 
From manual transcriptions we have generated and 
verified time-aligned transcriptions for 169 hours 
of our corpus, and we are in the process of time-
aligning transcriptions for the remainder of our 
corpus.  
We have performed initial speech recognition 
experiments using 10 computer science lectures. In 
these experiments we have discovered that, despite 
high word error rates (in the area of 40%), retrieval 
of short audio segments containing important key-
words and phrases can be performed with a high-
degree of reliability (over 90% F-measure when 
examining precision and recall results) [5]. These 
results are similar in nature to the findings in the 
SpeechBot project (which performs a similar ser-
vice for online broadcast news archives) [6]. 
References  
[1] S. Furui, ?Recent advances in spontaneous speech 
recognition and understanding,? in Proc. ISCA & IEEE 
Workshop on Spontaneous Speech Processing and Rec-
ognition (SSPR), pp. 1-6, Tokyo, April 2003. 
[2] J. Glass, T. Hazen, L. Hetherington, and C. Wang, 
?Analysis and Processing of Lecture Audio Data: Pre-
liminary Investigations,? in Proc. HLT/NAACL Speech 
Indexing Workshop, 9-12, Boston, May 2004. 
[3] T. Kawahara, H. Nanjo. And S. Furui,  ?Automatic 
transcription of spontaneous lecture speech,? in IEEE 
Workshop on Automatic Speech Recognition and Un-
derstanding, pp. 186-189, Trento, Italy, December 
2001. 
[4] H. Nanjo and T. Kawahara, ?Language model and 
speaking rate adaptation for spontaneous speech recog-
nition,? IEEE Transactions of Speech and Audio Proc-
essing, vol. 12, no. 4, pp. 391-400, July 2004. 
[5] A. Park, T. Hazen, and J. Glass, "Automatic 
Processing of Audio Lectures for Information Retrieval: 
Vocabulary Selection and Language Modeling," Proc. 
ICASSP, Philadelphia, PA, March 2005. 
[6] J.-M. Van Thong, et al ?SpeechBot: An experimen-
tal speech-based search engine for multimedia content 
on the web. IEEE Transactions of Multimedia, vol. 4, 
no. 1, pp. 88-96, March 2002.  
29
Feature-based Pronunciation Modeling for Speech Recognition
Karen Livescu and James Glass
MIT Computer Science and Artificial Intelligence Laboratory
Cambridge, MA 02139, USA
{klivescu, glass}@csail.mit.edu
Abstract
We present an approach to pronunciation mod-
eling in which the evolution of multiple lin-
guistic feature streams is explicitly represented.
This differs from phone-based models in that
pronunciation variation is viewed as the result
of feature asynchrony and changes in feature
values, rather than phone substitutions, inser-
tions, and deletions. We have implemented a
flexible feature-based pronunciation model us-
ing dynamic Bayesian networks. In this paper,
we describe our approach and report on a pilot
experiment using phonetic transcriptions of ut-
terances from the Switchboard corpus. The ex-
perimental results, as well as the model?s quali-
tative behavior, suggest that this is a promising
way of accounting for the types of pronuncia-
tion variation often seen in spontaneous speech.
1 Introduction
Pronunciation variation in spontaneous speech has been
cited as a serious obstacle for automatic speech recog-
nition (McAllester et al, 1998). Typical pronunciation
models approach this problem by augmenting a phone-
mic dictionary with additional pronunciations, often re-
sulting from the application of phone substitution, inser-
tion, and deletion rules. By carefully constructing a rule
set (Hazen et al, 2002), or by deriving rules or variants
from data (Riley and Ljolje, 1996), many phenomena can
be accounted for. However, the recognition improvement
over a phonemic dictionary is typically modest, and some
types of variation remain awkward to represent.
These observations have motivated approaches to
speech recognition based on multiple streams of linguis-
tic features rather than a single stream of phones (e.g.,
King et al (1998); Metze and Waibel (2002); Livescu et
al. (2003)). Most of this work, however, has focused on
acoustic modeling, i.e. the mapping between the features
and acoustic observations. The pronunciation model is
typically still phone-based, limiting the feature values to
the target configurations of phones and forcing them to
behave as a synchronous ?bundle?. Some approaches
have begun to relax these constraints. For example, Deng
et al (1997) and Richardson et al (2000) model asyn-
chronous feature trajectories using hidden Markov mod-
els (HMMs), with each state corresponding to a vector
of feature values. This approach is powerful, but it can-
not represent independencies between features. Kirch-
hoff (1996), in contrast, models the feature streams as
independent, except for a requirement that they synchro-
nize at syllable boundaries. As pointed out by Osten-
dorf (2000), such independence assumptions may allow
for too much variability.
In this paper, we propose a more general feature-
based pronunciation model implemented using dynamic
Bayesian networks (Dean and Kanazawa, 1989), which
allow us to take advantage of inter-feature independen-
cies while avoiding overly strong independence assump-
tions. In the following sections, we describe the model
and present proof-of-concept experiments using phonetic
transcriptions of utterances from the Switchboard conver-
sational speech corpus (Greenberg et al, 1996).
2 Serval [sic] examples
To help ground the discussion, we first present several
examples of pronunciation variation. One common phe-
nomenon is the nasalization of vowels preceding nasal
consonants. This is a result of asynchrony: The velum is
lowered before the oral closure is made. In more extreme
cases, the nasal consonant is entirely absent, leaving only
a nasalized vowel, as in can?t ? [ k ae n t ] 1. All of the
feature values are still correct, although phonetically, this
would be described as a deletion.
Another example, taken from the Switchboard corpus,
is several ? [s eh r v ax l]. In this case, the tongue
and lips have desynchronized to the point that the tongue
1Here and throughout, we use the ARPAbet phonetic symbol
set with additional diacritics, such as ? n? for nasalization.
retroflexion for [r] starts and ends before the lip narrow-
ing gesture for [v]. Again, all of the feature streams
are produced correctly, but there is an apparent exchange
of two phones, which cannot be represented via single-
phone confusions conditioned on phonemic context.
A final example from Switchboard is everybody ? [eh
r uw ay]. It is difficult to imagine a set of phonetic trans-
formations that would predict this pronunciation without
allowing a host of other impossible pronunciations. How-
ever, when viewed in terms of features, the transforma-
tion from [eh v r iy bcl b ah dx iy] to [eh r uw ay] is
fairly simple. The tongue and lips desynchronize, caus-
ing the lips to start to close for the [bcl] during the previ-
ous vowel. In addition, the lip constrictions for [bcl] and
[v], and the tongue tip gesture for [dx], are reduced. We
will return to this example in the sections below.
3 Approach
A feature-based pronunciation model is one that explic-
itly models the evolution of multiple underlying linguis-
tic feature streams to predict the allowed realizations of
a word and their probabilities. Our approach begins with
the usual assumption that each word has one or more tar-
get phonemic pronunciations, or baseforms. Each base-
form is converted to a table of underlying feature values.
Table 1 shows what part of this table might look like for
the word everybody. The table may include ?unspecified?
values (?*? in the table). More generally, each table en-
try can be a distribution over the range of feature values.
For now, we assume that all of the features go through the
same sequence of indices (and therefore the same number
of targets) in a given word; e.g., in Table 1, LIP-OPEN
goes through the same indices as TT-LOC, although it
has the same target value for indices 2 and 3. In the first
time frame of speech, all of the features begin in index
0; in subsequent frames, each feature can either stay in
the same index or transition to the next one with some
probability.
The surface feature values?i.e., the ones that are ac-
tually produced by the speaker?can stray from the un-
derlying pronunciation in two ways, typically because of
articulatory inertia: substitution, in which a feature fails
to reach its target underlying value; and asynchrony, in
which different features proceed through their sequences
of indices at different rates. We define the degree of asyn-
chrony between two sets of features as the difference be-
tween the average index of one set relative to the average
index of the second. The degree of asynchrony is con-
strained: More ?synchronous? configurations are more
probable (soft constraints), and we make the further sim-
plifying assumption that there is an upper bound on the
degree of asynchrony (hard constraints).
A natural framework for such a model is provided by
dynamic Bayesian networks (DBNs), because of their
index 0 1 2 3 ...
phoneme eh v r iy ...
LIP-OPEN wide critical wide wide ...
TT-LOC alv. * ret. alv. ...
... ... ... ... ... ...
Table 1: Part of a target pronunciation for everybody.
In this feature set, LIP-OPEN is the lip opening degree;
TT-LOC is the location along the palate to which the
tongue tip is closest (alv. = alveolar; ret. = retroflex).
2 3
U UU
2 3S1 S S
sync =11;2
ind ind ind
lexEntry
sync =11,2;3
1 2 3
t
t t
t
tt
t t
t
1
t
t
twdTr
t
Figure 1: One frame of a DBN for recognition with
a feature-based pronunciation model. Nodes represent
variables; shaded nodes are observed. Edges repre-
sent dependencies between variables. Edges without par-
ents/children point from/to variables in adjacent frames
(see text).
ability to efficiently implement factored state representa-
tions. Figure 1 shows one frame of the type of DBN used
in our model (simplified somewhat for clarity of presenta-
tion). This example DBN assumes a feature set with three
features. The variables at time frame t are as follows:
lexEntryt ? entry in the lexicon corresponding to the cur-
rent word and baseform. Words with multiple base-
forms have one entry per baseform. lexEntryt?s par-
ents are lexEntryt?1 and wdTrt?1
indjt ? index of feature j into the underlying pronun-
ciation, as in Table 1. indj0 = 0; in subsequent
frames indjt is conditioned on lexEntryt?1, ind
j
t?1,
and wdTrt?1 (defined below).
Ujt ? underlying value of feature j. Its distribution
p(U jt |lexEntryt, indjt ) is determined by the target
feature table of lexEntryt.
Sjt ? observed surface value of feature j. p(Sjt |U jt ) en-
codes allowed feature substitutions.
wdTrt ? binary variable indicating whether this is the last
frame of the current word.
syncA;Bt ? binary variable that enforces a synchrony con-
straint between subsets A and B of the feature set.
It is observed with value 1; its distribution is con-
structed in such a way as to force its parent ind vari-
ables to obey the desired constraint. For example,
to enforce a constraint between the average index of
features 1 and 2 and the index of feature 3, we would
have P (sync1,2;3t = 1|ind1t , ind2t , ind3t ) = 0 when-
ever ind1t , ind2t , ind3t violate the constraint.
In an end-to-end recognizer, the acoustic observations
would depend on the Sjt , which would be unobserved.
However, to facilitate quick experimentation and isolate
the pronunciation model, we begin by testing how well
we can do when given observed surface feature values.
4 Experiments
We have performed a pilot experiment using the follow-
ing feature set, based on the vocal tract variables of artic-
ulatory phonology (Browman and Goldstein, 1992): de-
gree of lip opening; tongue tip location and opening de-
gree; tongue body location and opening degree; velum
state; and glottal (voicing) state. We imposed the follow-
ing synchrony constraints: (1) All four tongue features
are completely synchronized; (2) the lips can desynchro-
nize from the tongue by up to one index; and (3) the glot-
tis and velum are synchronized, and their index must be
within 2 of the mean index of the tongue and lips.
We used the Graphical Models Toolkit (Bilmes and
Zweig, 2002) to implement the model. The distri-
butions p(Sjt |U jt ) were constructed by hand based on
linguistic considerations, e.g. that features tend to go
from more ?constricted? values to less constricted ones,
but not vice versa. p(U jt |lexEntryt, indjt ) was de-
rived from manually-constructed phoneme-to-feature-
probability mappings. For these experiments, no param-
eter learning has been done.
The task was to recognize an isolated word, given a set
of observed surface feature sequences Sjt . To create the
observations, we used the detailed phonetic transcriptions
created at ICSI for the Switchboard corpus (Greenberg et
al., 1996). For each word, we converted its transcrip-
tion to a sequence of feature vectors, one vector per 10
ms frame. For this purpose, we divided diphthongs and
stops into pairs of feature configurations. Given the input
feature sequences, we computed a Viterbi score for each
lexical entry in a 3000+-word (5500+-lexEntry) vocabu-
lary, by ?observing? the lexEntry variable and finding
the most likely settings of all remaining variables. The
most likely variable settings can be thought of as a mul-
tistream alignment between the surface and underlying
feature streams. Finally, we output the word correspond-
ing to the highest-scoring lexical entry.
We performed this procedure on a development set of
165 word transcriptions, which was used to tune settings
such as synchronization constraints, and a test set of 236
transcriptions 2. We compared the performance of sev-
eral models, measured in terms of word error rate (WER)
and failure rate (FR), the percentage of inputs that had no
Viterbi alignment with the correct word. To get a sense of
the effect of feature asynchrony, we compared our asyn-
chronous model with a version in which all features are
forced to be synchronized, so that only feature substitu-
tion is allowed. This uses the same DBN, but with de-
generate distributions for the synchronization variables.
Also, since the Sj values are derived from phonetic tran-
scriptions, and are therefore constant over several frames
at a time, we also built a variant of the DBN in which Sj
is allowed to change value with non-zero probability only
when indj changes (by adding parents indjt , indjt?1, Sjt?1
to Sjt ); we refer to this DBN as ?segment-based?, and to
the original as ?frame-based?. We compared four vari-
ants, differing along the ?synchronous vs. asynchronous?
and ?frame-based vs. segment-based? dimensions. The
variant which is both synchronous and segment-based is
similar to a phone-based pronunciation model with only
context-independent phone substitutions.
dev set test set
model WER FR WER FR
baseforms only 63.6 61.2 69.5 66.9
phonological rules 50.3 47.9 59.7 55.5
sync. seg.-based 38.2 24.8 43.2 35.2
sync. fr.-based 35.2 23.0 46.2 31.4
async. seg.-based 32.7 19.4 41.1 31.4
async. fr.-based 29.7 16.4 42.7 26.3
Table 2: Results of Switchboard ranking experiment.
Table 2 shows the performance of these four models,
as well as of two ?baseline? models: one allowing only
the baseform pronunciations (on average 1.7 per word),
and another including all pronunciations produced by
an extensive set of context-dependent phonological rules
(about 4 per word), with no feature substitutions or asyn-
chrony in either case. The phonological rules are the ?full
rule set? described in Hazen et al (2002). We note that
they were not designed with Switchboard in mind.
The models that allow asynchrony outperform the ones
that do not, in terms of both WER and FR. Looking more
closely at the performance on the development set, the
inputs on which the synchronous models failed but the
asynchronous models succeeded were in fact the kinds of
pronunciations that we expect to arise from feature asyn-
chrony, including: nasals replaced by nasalization on a
preceding vowel; a /t r/ sequence realized as /ch/; and
everybody ? [eh r uw ay]. The relative merits of the
frame-based and segment-based models is less clear, as
2We required that words in the development and test sets
have phonemic pronunciations with at least 4 phonemes, so as
to limit context effects from adjacent words.
they have opposite relative performance on the develop-
ment and test sets. For 27 (16.4%) development utter-
ances, none of the models was able to find an alignment
with the correct word. Most of these were due to apparent
gesture deletions and context-dependent feature changes,
which are not yet included in the model.
Figure 2 shows a part of the Viterbi alignment of ev-
erybody with [eh r uw ay], produced by the segment-
based, asynchronous model. Using this model, everybody
was the top-ranked word. As expected, the asynchrony is
manifested in the [uw] region, and the lips do not close
but reach only a narrow (glide-like) configuration.
Figure 2: Spectrogram, transcription, and partial Viterbi
alignment, including the lip opening and tongue tip loca-
tion variables. Indices are relative to the underlying pro-
nunciation /eh v r iy bcl b ah dx iy/. Adjacent frames with
equal values have been merged for easier viewing. WI =
wide; NA = narrow; CR = critical; CL = closed; ALV
= alveolar; P-A = palato-alveolar; RET = retroflex.
5 Discussion
We have motivated our pronunciation model as part of
an overall strategy of feature-based speech recognition.
One way in which this model could fit into a complete
recognizer is, as mentioned above, by adding a variable
A representing the acoustic observations, with the Sj as
its parents. The modeling of p(A|S1, . . . , SM ) (where M
is the number of features) is a significant problem in its
own right. Alternatively, as this study suggests, there may
be some benefit to this type of model even if the acoustic
model is phone-based. One possible setup would be to
use a phonetic recognizer to produce a phone lattice, then
convert the phones into features and proceed as in our
Switchboard experiments.
Thus far we have not trained the variable distribu-
tions. With the exception of the sync variables, these
can be trained from feature transcriptions (i.e. Sj obser-
vations) using the Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977). In the absence of actual
feature transcriptions, they can be approximated by con-
verting detailed phonetic transcriptions, as we have done
in our decoding experiments above. The sync distribu-
tions cannot be trained via EM, since they are always
observed with value 1. They can either be treated as ex-
perimental parameters or trained discriminatively. We are
currently working on a new formulation in which the syn-
chronization constraints can be trained via EM.
In addition, we are currently investigating extensions
to the model, including context-dependent feature substi-
tutions. We also plan to extend this study to a larger data
set and to multi-word utterances.
References
J. Bilmes and G. Zweig, ?The Graphical Models Toolkit: An
open source software system for speech and time-series pro-
cessing,? ICASSP, Orlando, 2002.
C. P. Browman and L. Goldstein, ?Articulatory phonology: An
overview,? Phonetica, 49:155?180, 1992.
T. Dean and K. Kanazawa, ?A model for reasoning about per-
sistence and causation,? Computational Intelligence, 5:142?
150, 1989.
A. P. Dempster, N. M. Laird, and D. B. Rubin, ?Maximum Like-
lihood from Incomplete Data via the EM Algorithm,? Jour-
nal of the Royal Statistical Society, 39:1?38,1977.
L. Deng, G. Ramsay, and D. Sun, ?Production models as a struc-
tural basis for automatic speech recognition,? Speech Com-
munication, 33:93?111, 1997.
S. Greenberg, J. Hollenback, and D. Ellis, ?Insights into spoken
language gleaned from phonetic transcription of the Switch-
board corpus,? ICSLP, Philadelphia, 1996.
T. J. Hazen, I. L. Hetherington, H. Shu, and K. Livescu, ?Pro-
nunciation modeling using a finite-state transducer represen-
tation,? ITRW PMLA, Estes Park, CO, 2002.
S. King, T. Stephenson, S. Isard, P. Taylor, and A. Strachan,
?Speech recognition via phonetically featured syllables,? IC-
SLP, Sydney, 1998.
K. Kirchhoff, ?Syllable-level desynchronisation of phonetic
features for speech recognition,? ICSLP, Philadelphia, 1996.
K. Livescu, J. Glass, and J. Bilmes, ?Hidden feature models
for speech recognition using dynamic Bayesian networks,?
Eurospeech, Geneva, 2003.
D. McAllester, L. Gillick, F. Scattone, and M. Newman, ?Fab-
ricating conversational speech data with acoustic models: A
program to examine model-data mismatch,? ICSLP, Sydney,
1998.
F. Metze and A. Waibel, ?A flexible stream architecture for ASR
using articulatory features,? ICSLP, Denver, 2002.
M. Ostendorf, ?Incorporating linguistic theories of pronuncia-
tion variation into speech-recognition models,? Phil. Trans.
R. Soc. Lond. A, 358:1325-1338, 2000.
M. Richardson, J. Bilmes, and C. Diorio, ?Hidden-articulator
Markov models for speech recognition,? ITRW ASR2000,
Paris, 2000.
M. D. Riley and A. Ljolje, ?Automatic generation of detailed
pronunciation lexicons,? in C.-H. Lee, F. K. Soong, and K.
K. Paliwal (eds.), Automatic Speech and Speaker Recogni-
tion, Kluwer Academic Publishers, Boston, 1996.
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 504?511,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Making Sense of Sound:
Unsupervised Topic Segmentation over Acoustic Input
Igor Malioutov, Alex Park, Regina Barzilay, and James Glass
Massachusetts Institute of Technology
{igorm,malex,regina,glass}@csail.mit.edu
Abstract
We address the task of unsupervised topic
segmentation of speech data operating over
raw acoustic information. In contrast to ex-
isting algorithms for topic segmentation of
speech, our approach does not require in-
put transcripts. Our method predicts topic
changes by analyzing the distribution of re-
occurring acoustic patterns in the speech sig-
nal corresponding to a single speaker. The
algorithm robustly handles noise inherent in
acoustic matching by intelligently aggregat-
ing information about the similarity profile
from multiple local comparisons. Our ex-
periments show that audio-based segmen-
tation compares favorably with transcript-
based segmentation computed over noisy
transcripts. These results demonstrate the
desirability of our method for applications
where a speech recognizer is not available,
or its output has a high word error rate.
1 Introduction
An important practical application of topic segmen-
tation is the analysis of spoken data. Paragraph
breaks, section markers and other structural cues
common in written documents are entirely missing
in spoken data. Insertion of these structural markers
can benefit multiple speech processing applications,
including audio browsing, retrieval, and summariza-
tion.
Not surprisingly, a variety of methods for
topic segmentation have been developed in the
past (Beeferman et al, 1999; Galley et al, 2003;
Dielmann and Renals, 2005). These methods typi-
cally assume that a segmentation algorithm has ac-
cess not only to acoustic input, but also to its tran-
script. This assumption is natural for applications
where the transcript has to be computed as part of the
system output, or it is readily available from other
system components. However, for some domains
and languages, the transcripts may not be available,
or the recognition performance may not be adequate
to achieve reliable segmentation. In order to process
such data, we need a method for topic segmentation
that does not require transcribed input.
In this paper, we explore a method for topic seg-
mentation that operates directly on a raw acoustic
speech signal, without using any input transcripts.
This method predicts topic changes by analyzing the
distribution of reoccurring acoustic patterns in the
speech signal corresponding to a single speaker. In
the same way that unsupervised segmentation algo-
rithms predict boundaries based on changes in lexi-
cal distribution, our algorithm is driven by changes
in the distribution of acoustic patterns. The central
hypothesis here is that similar sounding acoustic se-
quences produced by the same speaker correspond
to similar lexicographic sequences. Thus, by ana-
lyzing the distribution of acoustic patterns we could
approximate a traditional content analysis based on
the lexical distribution of words in a transcript.
Analyzing high-level content structure based on
low-level acoustic features poses interesting compu-
tational and linguistic challenges. For instance, we
need to handle the noise inherent in matching based
on acoustic similarity, because of possible varia-
504
tions in speaking rate or pronunciation. Moreover,
in the absence of higher-level knowledge, informa-
tion about word boundaries is not always discernible
from the raw acoustic input. This causes problems
because we have no obvious unit of comparison. Fi-
nally, noise inherent in the acoustic matching pro-
cedure complicates the detection of distributional
changes in the comparison matrix.
The algorithm presented in this paper demon-
strates the feasibility of topic segmentation over raw
acoustic input corresponding to a single speaker. We
first apply a variant of the dynamic time warping al-
gorithm to find similar fragments in the speech input
through alignment. Next, we construct a compari-
son matrix that aggregates the output of the align-
ment stage. Since aligned utterances are separated
by gaps and differ in duration, this representation
gives rise to sparse and irregular input. To obtain ro-
bust similarity change detection, we invoke a series
of transformations to smooth and refine the compar-
ison matrix. Finally, we apply the minimum-cut seg-
mentation algorithm to the transformed comparison
matrix to detect topic boundaries.
We compare the performance of our method
against traditional transcript-based segmentation al-
gorithms. As expected, the performance of the lat-
ter depends on the accuracy of the input transcript.
When a manual transcription is available, the gap
between audio-based segmentation and transcript-
based segmentation is substantial. However, in
a more realistic scenario when the transcripts are
fraught with recognition errors, the two approaches
exhibit similar performance. These results demon-
strate that audio-based algorithms are an effective
and efficient solution for applications where tran-
scripts are unavailable or highly errorful.
2 Related Work
Speech-based Topic Segmentation A variety of
supervised and unsupervised methods have been
employed to segment speech input. Some of these
algorithms have been originally developed for pro-
cessing written text (Beeferman et al, 1999). Others
are specifically adapted for processing speech input
by adding relevant acoustic features such as pause
length and speaker change (Galley et al, 2003; Diel-
mann and Renals, 2005). In parallel, researchers ex-
tensively study the relationship between discourse
structure and intonational variation (Hirschberg and
Nakatani, 1996; Shriberg et al, 2000). However,
all of the existing segmentation methods require as
input a speech transcript of reasonable quality. In
contrast, the method presented in this paper does
not assume the availability of transcripts, which pre-
vents us from using segmentation algorithms devel-
oped for written text.
At the same time, our work is closely related to
unsupervised approaches for text segmentation. The
central assumption here is that sharp changes in lex-
ical distribution signal the presence of topic bound-
aries (Hearst, 1994; Choi et al, 2001). These ap-
proaches determine segment boundaries by identi-
fying homogeneous regions within a similarity ma-
trix that encodes pairwise similarity between textual
units, such as sentences. Our segmentation algo-
rithm operates over a distortion matrix, but the unit
of comparison is the speech signal over a time in-
terval. This change in representation gives rise to
multiple challenges related to the inherent noise of
acoustic matching, and requires the development of
new methods for signal discretization, interval com-
parison and matrix analysis.
Pattern Induction in Acoustic Data Our work
is related to research on unsupervised lexical acqui-
sition from continuous speech. These methods aim
to infer vocabulary from unsegmented audio streams
by analyzing regularities in pattern distribution (de
Marcken, 1996; Brent, 1999; Venkataraman, 2001).
Traditionally, the speech signal is first converted into
a string-like representation such as phonemes and
syllables using a phonetic recognizer.
Park and Glass (2006) have recently shown the
feasibility of an audio-based approach for word dis-
covery. They induce the vocabulary from the au-
dio stream directly, avoiding the need for phonetic
transcription. Their method can accurately discover
words which appear with high frequency in the au-
dio stream. While the results obtained by Park and
Glass inspire our approach, we cannot directly use
their output as proxies for words in topic segmen-
tation. Many of the content words occurring only
a few times in the text are pruned away by this
method. Our results show that this data that is too
sparse and noisy for robustly discerning changes in
505
lexical distribution.
3 Algorithm
The audio-based segmentation algorithm identifies
topic boundaries by analyzing changes in the dis-
tribution of acoustic patterns. The analysis is per-
formed in three steps. First, we identify recurring
patterns in the audio stream and compute distortion
between them (Section 3.1). These acoustic patterns
correspond to high-frequency words and phrases,
but they only cover a fraction of the words that ap-
pear in the input. As a result, the distributional pro-
file obtained during this process is too sparse to de-
liver robust topic analysis. Second, we generate an
acoustic comparison matrix that aggregates infor-
mation from multiple pattern matches (Section 3.2).
Additional matrix transformations during this step
reduce the noise and irregularities inherent in acous-
tic matching. Third, we partition the matrix to iden-
tify segments with a homogeneous distribution of
acoustic patterns (Section 3.3).
3.1 Comparing Acoustic Patterns
Given a raw acoustic waveform, we extract a set of
acoustic patterns that occur frequently in the speech
document. Continuous speech includes many word
sequences that lack clear low-level acoustic cues to
denote word boundaries. Therefore, we cannot per-
form this task through simple counting of speech
segments separated by silence. Instead, we use a lo-
cal alignment algorithm to search for similar speech
segments and quantify the amount of distortion be-
tween them. In what follows, we first present a vec-
tor representation used in this computation, and then
specify the alignment algorithm that finds similar
segments.
MFCC Representation We start by transforming
the acoustic signal into a vector representation that
facilitates the comparison of acoustic sequences.
First, we perform silence detection on the original
waveform by registering a pause if the energy falls
below a certain threshold for a duration of 2s. This
enables us to break up the acoustic stream into con-
tinuous spoken utterances.
This step is necessary as it eliminates spurious
alignments between silent regions of the acoustic
waveform. Note that silence detection is not equiv-
alent to word boundary detection, as segmentation
by silence detection alone only accounts for 20% of
word boundaries in our corpus.
Next, we convert each utterance into a time se-
ries of vectors consisting of Mel-scale cepstral co-
efficients (MFCCs). This compact low-dimensional
representation is commonly used in speech process-
ing applications because it approximates human au-
ditory models.
The process of extracting MFCCs from the speech
signal can be summarized as follows. First, the 16
kHz digitized audio waveform is normalized by re-
moving the mean and scaling the peak amplitude.
Next, the short-time Fourier transform is taken at
a frame interval of 10 ms using a 25.6 ms Ham-
ming window. The spectral energy from the Fourier
transform is then weighted by Mel-frequency fil-
ters (Huang et al, 2001). Finally, the discrete cosine
transform of the log of these Mel-frequency spec-
tral coefficients is computed, yielding a series of 14-
dimensional MFCC vectors. We take the additional
step of whitening the feature vectors, which normal-
izes the variance and decorrelates the dimensions of
the feature vectors (Bishop, 1995). This whitened
spectral representation enables us to use the stan-
dard unweighted Euclidean distance metric. After
this transformation, the distances in each dimension
will be uncorrelated and have equal variance.
Alignment Now, our goal is to identify acoustic
patterns that occur multiple times in the audio wave-
form. The patterns may not be repeated exactly, but
will most likely reoccur in varied forms. We capture
this information by extracting pairs of patterns with
an associated distortion score. The computation is
performed using a sequence alignment algorithm.
Table 1 shows examples of alignments automati-
cally computed by our algorithm. The correspond-
ing phonetic transcriptions1 demonstrate that the
matching procedure can robustly handle variations
in pronunciations. For example, two instances of the
word ?direction? are matched to one another despite
different pronunciations, (?d ay? vs. ?d ax? in the
first syllable). At the same time, some aligned pairs
form erroneous matches, such as ?my prediction?
matching ?y direction? due to their high acoustic
1Phonetic transcriptions are not used by our algorithm and
are provided for illustrative purposes only.
506
Aligned Word(s) Phonetic Transcription
the x direction dh iy eh kcl k s dcl d ax r eh kcl sh ax n
D iy Ek^k s d^d @r Ek^S@n
the y direction dh ax w ay dcl d ay r eh kcl sh epi en
D @w ay d^ay r Ek^k S@n
of my prediction ax v m ay kcl k r iy l iy kcl k sh ax n
@v m ay k^k r iy l iy k^k S@n
acceleration eh kcl k s eh l ax r ey sh epi en
Ek^k s El @r Ey S- n
"
acceleration ax kcl k s ah n ax r eh n epi sh epi en
@k^k s 2n @r En - S- n
"
the derivation dcl d ih dx ih z dcl dh ey sh epi en
d^d IRIz d^D Ey S- n
"
a demonstration uh dcl d eh m ax n epi s tcl t r ey sh en
Ud^d Em @n - s t^t r Ey Sn
"
Table 1: Aligned Word Paths. Each group of rows
represents audio segments that were aligned to one
another, along with their corresponding phonetic
transcriptions using TIMIT conventions (Garofolo et
al., 1993) and their IPA equivalents.
similarity.
The alignment algorithm operates on the audio
waveform represented by a list of silence-free utter-
ances (u1, u2, . . . , un). Each utterance u? is a time
series of MFCC vectors ( ~x?1, ~x?2, . . . , ~x?m). Given
two input utterances u? and u??, the algorithm out-
puts a set of alignments between the corresponding
MFCC vectors. The alignment distortion score is
computed by summing the Euclidean distances of
matching vectors.
To compute the optimal alignment we use a vari-
ant of the dynamic time warping algorithm (Huang
et al, 2001). For every possible starting alignment
point, we optimize the following dynamic program-
ming objective:
D(ik, jk) = d(ik, jk) + min
?
?
?
?
?
D(ik ? 1, jk)
D(ik, jk ? 1)
D(ik ? 1, jk ? 1)
In the equation above, ik and jk are alignment end-
points in the k-th subproblem of dynamic program-
ming.
This objective corresponds to a descent through
a dynamic programming trellis by choosing right,
down, or diagonal steps at each stage.
During the search process, we consider not only
the alignment distortion score, but also the shape of
the alignment path. To limit the amount of temporal
warping, we enforce the following constraint:
?
?
(
ik ? i1
)
?
(
jk ? j1
)
?
? ? R,?k, (1)
ik ? Nx and jk ? Ny,
where Nx and Ny are the number of MFCC samples
in each utterance. The value 2R + 1 is the width of
the diagonal band that controls the extent of tempo-
ral warping. The parameter R is tuned on a develop-
ment set.
This alignment procedure may produce paths with
high distortion subpaths. Therefore, we trim each
path to retain the subpath with lowest average dis-
tortion and length at least L. More formally, given
an alignment of length N , we seek to find m and n
such that:
arg min
1?m?n?N
1
n ? m + 1
n
?
k=m
d(ik, jk) n?m ? L
We accomplish this by computing the length con-
strained minimum average distortion subsequence
of the path sequence using an O(N log(L)) algo-
rithm proposed by Lin et al(2002). The length
parameter, L, allows us to avoid overtrimming and
control the length of alignments that are found. Af-
ter trimming, the distortion of each alignment path
is normalized by the path length.
Alignments with a distortion exceeding a prespec-
ified threshold are pruned away to ensure that the
aligned phrasal units are close acoustic matches.
This parameter is tuned on a development set.
In the next section, we describe how to aggregate
information from multiple noisy matches into a rep-
resentation that facilitates boundary detection.
3.2 Construction of Acoustic Comparison
Matrix
The goal of this step is to construct an acoustic com-
parison matrix that will guide topic segmentation.
This matrix encodes variations in the distribution of
acoustic patterns for a given speech document. We
construct this matrix by first discretizing the acoustic
signal into constant-length blocks and then comput-
ing the distortion between pairs of blocks.
507
Figure 1: a) Similarity matrix for a Physics lecture constructed using a manual transcript. b) Similarity
matrix for the same lecture constructed from acoustic data. The intensity of a pixel indicates the degree of
block similarity. c) Acoustic comparison matrix after 2000 iterations of anisotropic diffusion. Vertical lines
correspond to the reference segmentation.
Unfortunately, the paths and distortions generated
during the alignment step (Section 3.1) cannot be
mapped directly to an acoustic comparison matrix.
Since we compare only commonly repeated acous-
tic patterns, some portions of the signal correspond
to gaps between alignment paths. In fact, in our cor-
pus only 67% of the data is covered by alignment
paths found during the alignment stage. Moreover,
many of these paths are not disjoint. For instance,
our experiments show that 74% of them overlap with
at least one additional alignment path. Finally, these
alignments vary significantly in duration, ranging
from 0.350 ms to 2.7 ms in our corpus.
Discretization and Distortion Computation To
compensate for the irregular distribution of align-
ment paths, we quantize the data by splitting the in-
put signal into uniform contiguous time blocks. A
time block does not necessarily correspond to any
one discovered alignment path. It may contain sev-
eral complete paths and also portions of other paths.
We compute the aggregate distortion score D(x, y)
of two blocks x and y by summing the distortions of
all alignment paths that fall within x and y.
Matrix Smoothing Equipped with a block dis-
tortion measure, we can now construct an acoustic
comparison matrix. In principle, this matrix can be
processed employing standard methods developed
for text segmentation. However, as Figure 1 illus-
trates, the structure of the acoustic matrix is quite
different from the one obtained from text. In a tran-
script similarity matrix shown in Figure 1 a), refer-
ence boundaries delimit homogeneous regions with
high internal similarity. On the other hand, looking
at the acoustic similarity matrix2 shown in Figure 1
b), it is difficult to observe any block structure cor-
responding to the reference segmentation.
This deficiency can be attributed to the sparsity of
acoustic alignments. Consider, for example, the case
when a segment is interspersed with blocks that con-
tain very few or no complete paths. Even though the
rest of the blocks in the segment could be closely
related, these path-free blocks dilute segment homo-
geneity. This is problematic because it is not always
possible to tell whether a sudden shift in scores sig-
nifies a transition or if it is just an artifact of irreg-
ularities in acoustic matching. Without additional
matrix processing, these irregularities will lead the
system astray.
We further refine the acoustic comparison matrix
using anisotropic diffusion. This technique has been
developed for enhancing edge detection accuracy in
image processing (Perona and Malik, 1990), and has
been shown to be an effective smoothing method in
text segmentation (Ji and Zha, 2003). When ap-
plied to a comparison matrix, anisotropic diffusion
reduces score variability within homogeneous re-
2We converted the original comparison distortion matrix to
the similarity matrix by subtracting the component distortions
from the maximum alignment distortion score.
508
gions of the matrix and makes edges between these
regions more pronounced. Consequently, this trans-
formation facilitates boundary detection, potentially
increasing segmentation accuracy. In Figure 1 c), we
can observe that the boundary structure in the dif-
fused comparison matrix becomes more salient and
corresponds more closely to the reference segmen-
tation.
3.3 Matrix Partitioning
Given a target number of segments k, the goal of
the partitioning step is to divide a matrix into k
square submatrices along the diagonal. This pro-
cess is guided by an optimization function that max-
imizes the homogeneity within a segment or mini-
mizes the homogeneity across segments. This opti-
mization problem can be solved using one of many
unsupervised segmentation approaches (Choi et al,
2001; Ji and Zha, 2003; Malioutov and Barzilay,
2006).
In our implementation, we employ the minimum-
cut segmentation algorithm (Shi and Malik, 2000;
Malioutov and Barzilay, 2006). In this graph-
theoretic framework, segmentation is cast as a prob-
lem of partitioning a weighted undirected graph
that minimizes the normalized-cut criterion. The
minimum-cut method achieves robust analysis by
jointly considering all possible partitionings of a
document, moving beyond localized decisions. This
allows us to aggregate comparisons from multiple
locations, thereby compensating for the noise of in-
dividual matches.
4 Evaluation Set-Up
Data We use a publicly available3 corpus of intro-
ductory Physics lectures described in our previous
work (Malioutov and Barzilay, 2006). This mate-
rial is a particularly appealing application area for an
audio-based segmentation algorithm ? many aca-
demic subjects lack transcribed data for training,
while a high ratio of in-domain technical terms lim-
its the use of out-of-domain transcripts. This corpus
is also challenging from the segmentation perspec-
tive because the lectures are long and transitions be-
tween topics are subtle.
3See http://www.csail.mit.edu/?igorm/
acl06.html
The corpus consists of 33 lectures, with an aver-
age length of 8500 words and an average duration
of 50 minutes. On average, a lecture was anno-
tated with six segments, and a typical segment cor-
responds to two pages of a transcript. Three lectures
from this set were used for development, and 30 lec-
tures were used for testing. The lectures were deliv-
ered by the same speaker.
To evaluate the performance of traditional
transcript-based segmentation algorithms on this
corpus, we also use several types of transcripts at
different levels of recognition accuracy. In addi-
tion to manual transcripts, our corpus contains two
types of automatic transcripts, one obtained using
speaker-dependent (SD) models and the other ob-
tained using speaker-independent (SI) models. The
speaker-independent model was trained on 85 hours
of out-of-domain general lecture material and con-
tained no speech from the speaker in the test set.
The speaker-dependent model was trained by us-
ing 38 hours of audio data from other lectures given
by the speaker. Both recognizers incorporated word
statistics from the accompanying class textbook into
the language model. The word error rates for the
speaker-independent and speaker-dependent models
are 44.9% and 19.4%, respectively.
Evaluation Metrics We use the Pk and WindowD-
iff measures to evaluate our system (Beeferman et
al., 1999; Pevzner and Hearst, 2002). The Pk mea-
sure estimates the probability that a randomly cho-
sen pair of words within a window of length k words
is inconsistently classified. The WindowDiff met-
ric is a variant of the Pk measure, which penalizes
false positives and near misses equally. For both of
these metrics, lower scores indicate better segmen-
tation accuracy.
Baseline We use the state-of-the-art mincut seg-
mentation system by Malioutov and Barzilay (2006)
as our point of comparison. This model is an appro-
priate baseline, because it has been shown to com-
pare favorably with other top-performing segmenta-
tion systems (Choi et al, 2001; Utiyama and Isa-
hara, 2001). We use the publicly available imple-
mentation of the system.
As additional points of comparison, we test the
uniform and random baselines. These correspond
to segmentations obtained by uniformly placing
509
Pk WindowDiff
MAN 0.298 0.311
SD 0.340 0.351
AUDIO 0.358 0.370
SI 0.378 0.390
RAND 0.472 0.497
UNI 0.476 0.484
Table 2: Segmentation accuracy for audio-based
segmentor (AUDIO), random (RAND), uniform
(UNI) and three transcript-based segmentation algo-
rithms that use manual (MAN), speaker-dependent
(SD) and speaker-independent (SI) transcripts. For
all of the algorithms, the target number of segments
is set to the reference number of segments.
boundaries along the span of the lecture and select-
ing random boundaries, respectively.
To control for segmentation granularity, we spec-
ify the number of segments in the reference segmen-
tation for both our system and the baselines.
Parameter Tuning We tuned the number of quan-
tized blocks, the edge cutoff parameter of the min-
imum cut algorithm, and the anisotropic diffusion
parameters on a heldout set of three development
lectures. We used the same development set for the
baseline segmentation systems.
5 Results
The goal of our evaluation experiments is two-fold.
First, we are interested in understanding the condi-
tions in which an audio-based segmentation is ad-
vantageous over a transcript-based one. Second, we
aim to analyze the impact of various design deci-
sions on the performance of our algorithm.
Comparison with Transcript-Based Segmenta-
tion Table 2 shows the segmentation accuracy
of the audio-based segmentation algorithm and three
transcript-based segmentors on the set of 30 Physics
lectures. Our algorithm yields an average Pk mea-
sure of 0.358 and an average WindowDiff mea-
sure of 0.370. This result is markedly better than
the scores attained by uniform and random seg-
mentations. As expected, the best segmentation re-
sults are obtained using manual transcripts. How-
ever, the gap between audio-based segmentation
and transcript-based segmentation narrows when the
recognition accuracy decreases. In fact, perfor-
mance of the audio-based segmentation beats the
transcript-based segmentation baseline obtained us-
ing speaker-independent (SI) models (0.358 for AU-
DIO versus Pk measurements of 0.378 for SI).
Analysis of Audio-based Segmentation A cen-
tral challenge in audio-based segmentation is how to
overcome the noise inherent in acoustic matching.
We addressed this issue by using anisotropic diffu-
sion to refine the comparison matrix. We can quan-
tify the effects of this smoothing technique by gener-
ating segmentations directly from the similarity ma-
trix. We obtain similarities from the distortions in
the comparison matrix by subtracting the distortion
scores from the maximum distortion:
S(x, y) = max
si,sj
[D(si, sj)] ? D(x, y)
Using this matrix with the min-cut algorithm, seg-
mentation accuracy drops to a Pk measure of 0.418
(0.450 WindowDiff). This difference in perfor-
mance shows that anisotropic diffusion compensates
for noise introduced during acoustic matching.
An alternative solution to the problem of irregu-
larities in audio-based matching is to compute clus-
ters of acoustically similar utterances. Each of the
derived clusters can be thought of as a unique word
type.4 We compute these clusters, employing a
method for unsupervised vocabulary induction de-
veloped by Park and Glass (2006). Using the out-
put of their algorithm, the continuous audio stream
is transformed into a sequence of word-like units,
which in turn can be segmented using any stan-
dard transcript-based segmentation algorithm, such
as the minimum-cut segmentor. On our corpus, this
method achieves disappointing results ? a Pk mea-
sure of 0.423 (0.424 WindowDiff). The result can
be attributed to the sparsity of clusters5 generated by
this method, which focuses primarily on discovering
the frequently occurring content words.
6 Conclusion and Future Work
We presented an unsupervised algorithm for audio-
based topic segmentation. In contrast to existing
4In practice, a cluster can correspond to a phrase, word, or
word fragment (See Table 1 for examples).
5We tuned the number of clusters on the development set.
510
algorithms for speech segmentation, our approach
does not require an input transcript. Thus, it can
be used in domains where a speech recognizer is
not available or its output is too noisy. Our ap-
proach approximates the distribution of cohesion
ties by considering the distribution of acoustic pat-
terns. Our experimental results demonstrate the util-
ity of this approach: audio-based segmentation com-
pares favorably with transcript-based segmentation
computed over noisy transcripts.
The segmentation algorithm presented in this pa-
per focuses on one source of linguistic information
for discourse analysis ? lexical cohesion. Multiple
studies of discourse structure, however, have shown
that prosodic cues are highly predictive of changes
in topic structure (Hirschberg and Nakatani, 1996;
Shriberg et al, 2000). In a supervised framework,
we can further enhance audio-based segmentation
by combining features derived from pattern analy-
sis with prosodic information. We can also explore
an unsupervised fusion of these two sources of in-
formation; for instance, we can induce informative
prosodic cues by using distributional evidence.
Another interesting direction for future research
lies in combining the results of noisy recogni-
tion with information obtained from distribution of
acoustic patterns. We hypothesize that these two
sources provide complementary information about
the audio stream, and therefore can compensate for
each other?s mistakes. This combination can be par-
ticularly fruitful when processing speech documents
with multiple speakers or background noise.
7 Acknowledgements
The authors acknowledge the support of the Microsoft Faculty
Fellowship and the National Science Foundation (CAREER
grant IIS-0448168, grant IIS-0415865, and the NSF Graduate
Fellowship). Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of the au-
thor(s) and do not necessarily reflect the views of the National
Science Foundation. We would like to thank T.J. Hazen for
his assistance with the speech recognizer and to acknowledge
Tara Sainath, Natasha Singh, Ben Snyder, Chao Wang, Luke
Zettlemoyer and the three anonymous reviewers for their valu-
able comments and suggestions.
References
D. Beeferman, A. Berger, J. D. Lafferty. 1999. Statistical mod-
els for text segmentation. Machine Learning, 34(1-3):177?
210.
C. Bishop, 1995. Neural Networks for Pattern Recognition,
pg. 38. Oxford University Press, New York, 1995.
M. R. Brent. 1999. An efficient, probabilistically sound algo-
rithm for segmentation and word discovery. Machine Learn-
ing, 34(1-3):71?105.
F. Choi, P. Wiemer-Hastings, J. Moore. 2001. Latent semantic
analysis for text segmentation. In Proceedings of EMNLP,
109?117.
C. G. de Marcken. 1996. Unsupervised Language Acquisition.
Ph.D. thesis, Massachusetts Institute of Technology.
A. Dielmann, S. Renals. 2005. Multistream dynamic Bayesian
network for meeting segmentation. In Proceedings Mul-
timodal Interaction and Related Machine Learning Algo-
rithms Workshop (MLMI?04), 76?86.
M. Galley, K. McKeown, E. Fosler-Lussier, H. Jing. 2003.
Discourse segmentation of multi-party conversation. In Pro-
ceedings of the ACL, 562?569.
J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallet,
N. Dahlgren, V. Zue. 1993. TIMIT Acoustic-Phonetic Con-
tinuous Speech Corpus. Linguistic Data Consortium, 1993.
M. Hearst. 1994. Multi-paragraph segmentation of expository
text. In Proceedings of the ACL, 9?16.
J. Hirschberg, C. H. Nakatani. 1996. A prosodic analysis of
discourse segments in direction-giving monologues. In Pro-
ceedings of the ACL, 286?293.
X. Huang, A. Acero, H.-W. Hon. 2001. Spoken Language Pro-
cessing. Prentice Hall.
X. Ji, H. Zha. 2003. Domain-independent text segmentation
using anisotropic diffusion and dynamic programming. In
Proceedings of SIGIR, 322?329.
Y.-L. Lin, T. Jiang, K.-M. Chao. 2002. Efficient algorithms
for locating the length-constrained heaviest segments with
applications to biomolecular sequence analysis. J. Computer
and System Sciences, 65(3):570?586.
I. Malioutov, R. Barzilay. 2006. Minimum cut model for
spoken lecture segmentation. In Proceedings of the COL-
ING/ACL, 25?32.
A. Park, J. R. Glass. 2006. Unsupervised word acquisition from
speech using pattern discovery. In Proceedings of ICASSP.
P. Perona, J. Malik. 1990. Scale-space and edge detection using
anisotropic diffusion. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 12(7):629?639.
L. Pevzner, M. Hearst. 2002. A critique and improvement of
an evaluation metric for text segmentation. Computational
Linguistics, 28(1):19?36.
J. Shi, J. Malik. 2000. Normalized cuts and image segmenta-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 22(8):888?905.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, G. Tur. 2000.
Prosody-based automatic segmentation of speech into sen-
tences and topics. Speech Communication, 32(1-2):127?
154.
M. Utiyama, H. Isahara. 2001. A statistical model for domain-
independent text segmentation. In Proceedings of the ACL,
499?506.
A. Venkataraman. 2001. A statistical model for word dis-
covery in transcribed speech. Computational Linguistics,
27(3):353?372.
511
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 153?156,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Segmentation for English-to-Arabic Statistical Machine Translation
Ibrahim Badr Rabih Zbib
Computer Science and Artificial Intelligence Lab
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{iab02, rabih, glass}@csail.mit.edu
James Glass
Abstract
In this paper, we report on a set of ini-
tial results for English-to-Arabic Statistical
Machine Translation (SMT). We show that
morphological decomposition of the Arabic
source is beneficial, especially for smaller-size
corpora, and investigate different recombina-
tion techniques. We also report on the use
of Factored Translation Models for English-
to-Arabic translation.
1 Introduction
Arabic has a complex morphology compared to
English. Words are inflected for gender, number,
and sometimes grammatical case, and various cli-
tics can attach to word stems. An Arabic corpus
will therefore have more surface forms than an En-
glish corpus of the same size, and will also be more
sparsely populated. These factors adversely affect
the performance of Arabic?English Statistical Ma-
chine Translation (SMT). In prior work (Lee, 2004;
Habash and Sadat, 2006), it has been shown that
morphological segmentation of the Arabic source
benefits the performance of Arabic-to-English SMT.
The use of similar techniques for English-to-Arabic
SMT requires recombination of the target side into
valid surface forms, which is not a trivial task.
In this paper, we present an initial set of experi-
ments on English-to-Arabic SMT. We report results
from two domains: text news, trained on a large cor-
pus, and spoken travel conversation, trained on a sig-
nificantly smaller corpus. We show that segmenting
the Arabic target in training and decoding improves
performance. We propose various schemes for re-
combining the segmented Arabic, and compare their
effect on translation. We also report on applying
Factored Translation Models (Koehn and Hoang,
2007) for English-to-Arabic translation.
2 Previous Work
The only previous work on English-to-Arabic SMT
that we are aware of is by Sarikaya and Deng (2007).
It uses shallow segmentation, and does not make
use of contextual information. The emphasis of that
work is on using Joint Morphological-Lexical Lan-
guage Models to rerank the output.
Most of the related work, though, is on Arabic-to-
English SMT. Lee (2004) uses a trigram language
model to segment Arabic words. She then pro-
ceeds to deleting or merging some of the segmented
morphemes in order to make the segmented Arabic
source align better with the English target. Habash
and Sadat (2006) use the Arabic morphological an-
alyzer MADA (Habash and Rambow, 2005) to seg-
ment the Arabic source; they propose various seg-
mentation schemes. Both works show that the im-
provements obtained from segmentation decrease as
the corpus size increases. As will be shown later, we
observe the same trend, which is due to the fact that
the model becomes less sparse with more training
data.
There has been work on translating from En-
glish to other morphologically complex languages.
Koehn and Hoang (2007) present Factored Transla-
tion Models as an extension to phrase-based statisti-
cal machine translation models. Factored models al-
low the integration of additional morphological fea-
153
tures, such as POS, gender, number, etc. at the word
level on both source and target sides. The tighter in-
tegration of such features was claimed to allow more
explicit modeling of the morphology, and is better
than using pre-processing and post-processing tech-
niques. FactoredModels demonstrate improvements
when used to translate English to German or Czech.
3 Arabic Segmentation and
Recombination
As mentioned in Section 1, Arabic has a relatively
rich morphology. In addition to being inflected for
gender, number, voice and case, words attach to var-
ious clitics for conjunction (w+ ?and?)1, the definite
article (Al+ ?the?), prepositions (e.g. b+ ?by/with?,
l+ ?for?, k+ ?as?), possessive pronouns and object
pronouns (e.g. +ny ?me/my?, +hm ?their/them?). For
example, the verbal form wsnsAEdhm and the nomi-
nal formwbsyAratnA can be decomposed as follows:
(1) a. w+
and+
s+
will+
n+
we+
sAEd
help
+hm
+them
b. w+
and+
b+
with+
syAr
car
+At
+PL
+nA
+our
Also, Arabic is usually written without the diacritics
that denote the short vowels, and different sources
write a few characters inconsistently. These issues
create word-level ambiguity.
3.1 Arabic Pre-processing
Due to the word-level ambiguity mentioned above,
but more generally, because a certain string of char-
acters can, in principle, be either an affixed mor-
pheme or part of the base word, morphological
decomposition requires both word-level linguistic
information and context analysis; simple pattern
matching is not sufficient to detect affixed mor-
phemes. To perform pre-translation morphologi-
cal decomposition of the Arabic source, we use the
morphological analyzer MADA. MADA uses SVM-
based classifiers for features (such as POS, number
and gender, etc.) to choose among the different anal-
yses of a given word in context.
We first normalize the Arabic by changing final
?Y? to ?y? and the various forms of Alif hamza to bare
1In this paper, Arabic text is written using Buckwalter
transliteration
Alif. We also remove diacritics wherever they occur.
We then apply one of two morphological decompo-
sition schemes before aligning the training data:
1. S1: Decliticization by splitting off each con-
junction clitic, particle, definite article and
pronominal clitic separately. Note that plural
and subject pronoun morphemes are not split.
2. S2: Same as S1, except that the split clitics are
glued into one prefix and one suffix, such that
any given word is split into at most three parts:
prefix+ stem +suffix.
For example the word wlAwlAdh (?and for his kids?)
is segmented to w+ l+ AwlAd +P:3MS according to
S1, and to wl+ AwlAd +P:3MS according to S2.
3.2 Arabic Post-processing
As mentioned above, both training and decoding use
segmented Arabic. The final output of the decoder
must therefore be recombined into a surface form.
This proves to be a non-trivial challenge for a num-
ber of reasons:
1. Morpho-phonological Rules: For example, the
feminine marker ?p? at the end of a word
changes to ?t? when a suffix is attached to the
word. So syArp +P:1S recombines to syArty
(?my car?)
2. Letter Ambiguity: The character ?Y? (Alf
mqSwrp) is normalized to ?y?. In the recom-
bination step we need to be able to decide
whether a final ?y? was originally a ?Y?. For
example, mdy +P:3MS recombines to mdAh
?its extent?, since the ?y? is actually a Y; but fy
+P:3MS recombines to fyh ?in it?.
3. Word Ambiguity: In some cases, a word can
recombine into 2 grammatically correct forms.
One example is the optional insertion of nwn
AlwqAyp (protective ?n?), so the segmented
word lkn +O:1S can recombine to either lknny
or lkny, both grammatically correct.
To address these issues, we propose two recombina-
tion techniques:
1. R: Recombination rules defined manually. To
resolve word ambiguity we pick the grammat-
ical form that appears more frequently in the
154
training data. To resolve letter ambiguity we
use a unigram language model trained on data
where the character ?Y? had not been normal-
ized. We decide on the non-normalized from of
the ?y? by comparing the unigram probability of
the word with ?y? to its probability with ?Y?.
2. T: Uses a table derived from the training set
that maps the segmented form of the word to its
original form. If a segmented word has more
than one original form, one of them is picked
at random. The table is useful in recombin-
ing words that are split erroneously. For ex-
ample, qrDAy, a proper noun, gets incorrectly
segmented to qrDAn +P:1S which makes its re-
combination without the table difficult.
3.3 Factored Models
For the Factored Translation Models experiment, the
factors on the English side are the POS tags and the
surface word. On the Arabic side, we use the sur-
face word, the stem and the POS tag concatenated
to the segmented clitics. For example, for the word
wlAwlAdh (?and for his kids?), the factored words are
AwlAd and w+l+N+P:3MS. We use two language
models: a trigram for surface words and a 7-gram
for the POS+clitic factor. We also use a genera-
tion model to generate the surface form from the
stem and POS+clitic, a translation table from POS
to POS+clitics and from the English surface word to
the Arabic stem. If the Arabic surface word cannot
be generated from the stem and POS+clitic, we back
off to translating it from the English surface word.
4 Experiments
The English source is aligned to the segmented Ara-
bic target using GIZA++ (Och and Ney, 2000), and
the decoding is done using the phrase-based SMT
system MOSES (MOSES, 2007). We use a max-
imum phrase length of 15 to account for the in-
crease in length of the segmented Arabic. Tuning
is done using Och?s algorithm (Och, 2003) to op-
timize weights for the distortion model, language
model, phrase translation model and word penalty
over the BLEU metric (Papineni et al, 2001). For
our baseline system the tuning reference was non-
segmented Arabic. For the segmented Arabic exper-
iments we experiment with 2 tuning schemes: T1
Scheme Training Set Tuning Set
Baseline 34.6% 36.8%
R 4.04% 4.65%
T N/A 22.1%
T + R N/A 1.9%
Table 1: Recombination Results. Percentage of sentences
with mis-combined words.
uses segmented Arabic for reference, and T2 tunes
on non-segmented Arabic. The Factored Translation
Models experiments uses the MOSES system.
4.1 Data Used
We experiment with two domains: text news and
spoken dialogue from the travel domain. For the
news training data we used corpora from LDC2. Af-
ter filtering out sentences that were too long to be
processed by GIZA (> 85 words) and duplicate sen-
tences, we randomly picked 2000 development sen-
tences for tuning and 2000 sentences for testing. In
addition to training on the full set of 3 million words,
we also experimented with subsets of 1.6 million
and 600K words. For the language model, we used
20 million words from the LDC Arabic Gigaword
corpus plus 3 million words from the training data.
After experimenting with different language model
orders, we used 4-grams for the baseline system and
6-grams for the segmented Arabic. The English
source is downcased and the punctuations are sepa-
rated. The average sentence length is 33 for English,
25 for non-segmented Arabic and 36 for segmented
Arabic.
For the spoken language domain, we use the
IWSLT 2007 Arabic-English (Fordyce, 2007) cor-
pus which consists of a 200,000 word training set, a
500 sentence tuning set and a 500 sentence test set.
We use the Arabic side of the training data to train
the language model and use trigrams for the baseline
system and a 4-grams for segmented Arabic. The av-
erage sentence length is 9 for English, 8 for Arabic,
and 10 for segmented Arabic.
2Since most of the data was originally intended for Arabic-
to-English translation our test and tuning sets have only one
reference
155
4.2 Recombination Results
To test the different recombination schemes de-
scribed in Section 3.2, we run these schemes on
the training and development sets of the news data,
and calculate the percentage of sentences with re-
combination errors (Note that, on average, there
is one mis-combined word per mis-combined sen-
tence). The scores are presented in Table 1. The
baseline approach consists of gluing the prefix and
suffix without processing the stem. T +Rmeans that
the words seen in the training set were recombined
using scheme T and the remainder were recombined
using scheme R. In the remaining experiments we
use the scheme T + R.
4.3 Translation Results
The 1-reference BLEU score results for the news
corpus are presented in Table 2; those for IWSLT are
in Table 3. We first note that the scores are generally
lower than those of comparable Arabic-to-English
systems. This is expected, since only one refer-
ence was used to evaluate translation quality and
since translating to a more morphologically com-
plex language is a more difficult task, where there
is a higher chance of translating word inflections in-
correctly. For the news corpus, the segmentation of
Arabic helps but the gain diminishes as the training
data size increases, since the model becomes less
sparse. This is consistent with the larger gain ob-
tained from segmentation for IWSLT. The segmen-
tation scheme S2 performs slightly better than S1.
The tuning scheme T2 performs better for the news
corpus, while T1 is better for the IWSLT corpus.
It is worth noting that tuning without segmentation
hurts the score for IWSLT, possibly because of the
small size of the training data. Factored models per-
form better than our approach with the large train-
ing corpus, although at a significantly higher cost in
terms of time and required resources.
5 Conclusion
In this paper, we showed that making the Arabic
match better to the English through segmentation,
or by using additional translation model factors that
model grammatical information is beneficial, espe-
cially for smaller domains. We also presented sev-
eral methods for recombining the segmented Arabic
Large Medium Small
Training Size 3M 1.6M 0.6M
Baseline 26.44 20.51 17.93
S1 + T1 tuning 26.46 21.94 20.59
S1 + T2 tuning 26.81 21.93 20.87
S2 + T1 tuning 26.86 21.99 20.44
S2 + T2 tuning 27.02 22.21 20.98
Factored Models + tuning 27.30 21.55 19.80
Table 2: BLEU (1-reference) scores for the News data.
No Tuning T1 T2
Baseline 26.39 24.67
S1 29.07 29.82
S2 29.11 30.10 28.94
Table 3: BLEU (1-reference) scores for the IWSLT data.
target. Our results suggest that more sophisticated
techniques, such as syntactic reordering, should be
attempted.
Acknowledgments
We would like to thank Ali Mohammad, Michael Collins and
Stephanie Seneff for their valuable comments.
References
Cameron S. Fordyce 2007. Overview of the 2007 IWSLT Eval-
uation Campaign . In Proc. of IWSLT 2007.
Nizar Habash and Owen Rambow, 2005. Arabic Tokenization,
Part-of-Speech Tagging and Morphological Disambiguation
in One Fell Swoop. In Proc. of ACL.
Nizar Habash and Fatiha Sadat, 2006. Arabic Preprocessing
Schemes for Statistical Machine Translation. In Proc. of
HLT.
Philipp Koehn and Hieu Hoang, 2007. Factored Translation
Models. In Proc. of EMNLP/CNLL.
Young-Suk Lee, 2004. Morphological Analysis for Statistical
Machine Translation. In Proc. of EMNLP.
MOSES, 2007. A Factored Phrase-based Beam-
search Decoder for Machine Translation. URL:
http://www.statmt.org/moses/.
Franz Och, 2003. Minimum Error Rate Training in Statistical
Machine Translation. In Proc. of ACL.
Franz Och and Hermann Ney, 2000. Improved Statistical
Alignment Models. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu, 2001. Bleu: a Method for Automatic Evaluation of
Machine Translation. In Proc. of ACL.
Ruhi Sarikaya and Yonggang Deng 2007. Joint
Morphological-Lexical Language Modeling for Machine
Translation. In Proc. of NAACL HLT.
156
Flexible and Personalizable Mixed-Initiative Dialogue Systems
James Glass and Stephanie Seneff
Spoken Language Systems Group
Laboratory for Computer Science, MIT
Cambridge, MA, USA
{jrg,seneff}@sls.lcs.mit.edu
Abstract
This paper describes our vision for a future
time when end users of mixed-initiative spoken
dialogue systems will be able to dynamically
configure the system to suit their personalized
goals. We argue that spoken dialogue systems
will only become a common utility in society
once they can be reconfigured, essentially in-
stantaneously, to support a new working vocab-
ulary within a new domain or subdomain. For
example, if a user is interested in restaurants in
Seattle, the system would go off-line to gather
information from resources such as the Web,
and would infer from that knowledge an ap-
propriate working vocabulary, language mod-
els, and dialogue control mechanism for a sub-
sequent spoken conversation on this topic. In
addition to painting this vision, the paper also
discusses our recent research efforts directed
towards the technology development necessary
to realize this larger goal.
1 Introduction
Spoken dialogue systems are emerging as an effective
means for humans to access information spaces through
natural spoken interaction with computers. These sys-
tems are usually implemented with a static knowledge
space, or one that is only augmented through manual in-
tervention from the system developers. A significant en-
hancement to the usability of such systems would be the
ability to automatically acquire new knowledge through
interaction with its end users and its available knowledge
resources. We believe, in fact, that the main barrier to
wide acceptance of spoken dialogue systems is their cur-
rent lack of flexibility and personalization.
Over the past decade, researchers in the Spoken Lan-
guage Systems Group at MIT have been developing hu-
man language technologies for mixed initiative conversa-
tional systems, which are distinguished from the emerg-
ing deployed commercial systems in that the interaction
is natural and flexible, modelled after the style of human-
human dialogue (Zue and Glass, 2000). The development
of the Galaxy Communicator architecture (Seneff et al,
1998) has greatly accelerated the pace at which we as ex-
perts can configure complex dialogue systems in a wide
range of different domains. As the underlying technol-
ogy components have matured, our research focus has
evolved to include issues related to portability, modular-
ity, and dynamic configurability of system components.
We believe that the ability for naive system developers,
and even end users, to reconfigure existing systems to
manage their personal needs, will be crucial for the suc-
cessful use of these technologies.
We see several different ways in which such flexible
reconfiguration will become feasible in the near future.
Perhaps most critical is the initial preparation of a new
domain, where available on-line databases will be the cat-
alyst for defining the vocabulary and language models of
the domain, as well as the nature of the dialogue inter-
action needed to guide the user through the information
space (Polifroni et al, 2003). However, the ability to dy-
namically reconfigure based on new information will also
be extremely valuable. For instance, a hotel domain for
the entire U.S. might initially restrict name recognition
to the major chains such as Sheraton and Hyatt, since it
would not be feasible to support all hotel names in the
U.S. Once the user defines a specific geographic region,
the system can enhance the level of detail of its work-
ing vocabulary, but particular to the user?s narrower re-
quest, now supporting explicit mention of perhaps all ho-
tels by name within that designated region. Users would
be able to select hotels based on a number of attributes as
guided by the hotel database, and to specify constraints in
general world knowledge areas such as dates and prices.
Users should be able to further personalize the system
User: I?m interested in a restaurant in Boston?s
North End
Sys: I know of 53 restaurants in the North End.
Of these, 46 are Italian restaurants. 28 of them
are located on either Hanover or Salem Street.
User: Tell me about the ones on Hanover Street.
Sys: There is one seafood restaurant on Hanover
Street. The others are Italian.
User: I?m interested in the Seafood restaurant.
Sys: The Daily Catch is located on 323 Hanover
Street. The phone number is 617-523-8567.
The price range is between $12 and $18.
Figure 1: Illustration of a possible dialogue between a
user and a system in a restaurant domain.
by adding new words instantaneously to the working vo-
cabulary via spoken dialogue. This might also include
specifying the word?s semantic class: ?I want to add the
name John Doe to my rolodex.? When feasible, a user-
specified named entity, such as a restaurant, would be ver-
ified against Web sources to improve the system?s ability
to understand their request.
In order for this vision to become a reality, a num-
ber of specific technology goals must be met. First and
foremost, it is essential to develop tools that will enable
rapid configuration of dialogue systems in new domains
of knowledge, guided mainly from domain-dependent in-
formation sources. Our efforts in generic dialogue devel-
opment represent a strong initiative toward that goal (Po-
lifroni and Chung, 2002). Secondly, we need to be able
to support incremental update of vocabularies and lan-
guage models for speech recognition and understanding,
in essentially instantaneous time (Schalkwyk et al, 2003;
Seneff et al, 1998; Chung et al, 2003). This would allow
great flexibility within a single dialogue where the user
might ask about a named entity that is not yet known to
the system. Third, while we can make use of a large lex-
ical resource for pronunciation modeling, we must have
available as well a high-performance letter-to-sound ca-
pability, integrating multiple knowledge sources such as a
Web page, a spoken name, a spoken spelling of the name,
and/or a key-padded name (Chung and Seneff, 2002).
Fourth, we need to have intelligent knowledge acquisi-
tion systems, capable of populating a database from Web
sources, and extracting and organizing key elements from
the database (Polifroni et al, 2003).
These ideas can best be illustrated through a couple
of example scenarios. In Figure 1, the user begins with
a request for a restaurant in a neighborhood of Boston.
The system then rapidly configures itself to support the
appropriate sub-language, and is able to summarize lists
of restaurants meeting the constraints of the user?s subse-
quent queries, eventually leading to a unique selection.
For the scenario in Figure 2, the user has asked about
User: Can you tell me the phone number of the
Thaiku restaurant in Seattle?
Sys: I may not know the name of the restaurant.
Can you spell it for me?
User: t h a i k u
Sys: The phone number of Thaiku is 206-706-7807.
Figure 2: A sub-dialogue to enroll a new restaurant name.
the phone number for a restaurant they already know
about. The system parses the name within a complete
parse, but with a generic ?unknown word? as a stand-in
for the restaurant name. It can at this point go to the
Web and download a set of candidate restaurant names
for Seattle, to form additional constraints on a solicited
spelling. The integration of the spelling, the spoken pro-
nunciation, and the Web listing, we argue, potentially
provide enough constraint to solve the specific problem
with high accuracy. The system can now retrieve the re-
quested information from the Web.
2 Underlying Technologies
Over the past several years, we have been making ad-
vances on several fronts, directed toward the larger goal
of the vision outlined above. In this section, we will high-
light some of these, with pointers to the literature for an
in-depth description.
SpeechBuilder: Over the past few years, we have been
developing a set of utilities that would enable research
results to be migrated directly into application develop-
ment (Glass and Weinstein, 2001). Our goal is to enable
natural, mixed-initiative interfaces similar to those now
created manually by a relatively small group of expert de-
velopers. We make no distinction between the technology
components of SpeechBuilder and those of our most so-
phisticated dialogue systems, such as the Mercury flight
reservation domain (Seneff and Polifroni, 2000). Speech-
Builder employs a Web-based interface where developers
type in the specifics of their domain, guided by forms and
pull-down menus. Components such as recognition vo-
cabulary, parse rules, and semantic mappings are created
automatically from example sentences entered by the de-
veloper. In several recent short courses, naive developers
have been able to implement a new domain and converse
with it on the telephone in a matter of hours.
Language Modelling: Patchwork Grammars A seri-
ous limitation in today?s technology to immediate deploy-
ment of a new system is the chicken-and-egg problem of
the language model. System performance is critically tied
to the quality of the statistical language model, which typ-
ically depends on large domain-dependent corpora that
don?t exist until the domain is actually deployed and
widely used. We have initiated an effort to automatically
induce a grammar for a new domain from related content
of existing speech corpora for other domains combined
with knowledge derived from the content provider for the
new domain. For instance, our hotel domain can leverage
from an existing auto classified domain to extract patterns
for referring to prices, can induce a grammar for dates
from a flight domain, and can make use of statistics of
hotel counts to determine city probabilities. Parse rules
for general sub-domains such as dates, times, and prices
are organized into sub-grammars that are easily embed-
ded into any application, along with libraries for convert-
ing the resulting meaning representations into a canonical
format, such as ?27SEP2003.?
Flexible Vocabulary: We have recently realized our goal
of enabling users to automatically add a new word to an
existing system through natural interaction with the sys-
tem itself (Schalkwyk et al, 2003; Seneff et al, 1998;
Chung et al, 2003; Chung and Seneff, 2002; Seneff et
al., 2003). We have thus far applied this only to the en-
rollment of the user?s name as part of a personalization
phase (Seneff et al, 1998; Chung et al, 2003), through a
?speak and spell? mode. After confirmation, the system
reconfigures itself to fully support the word such that it
can now be understood in subsequent dialogue. A high
quality sound-to-letter framework (Chung et al, 2003)
and a new ability to automatically derive a class n-gram
from an NL grammar have facilitated this process (Sen-
eff et al, 2003). The recognizer update is currently im-
plemented via full recompilation, which can take up to a
minute of elapsed time, but efforts to support incremental
recognizer updates (Schalkwyk et al, 2003) hold promise
for essentially instantaneous new word addition.
Managing the Dialogue: One of the most time con-
suming aspect of dialogue system development today
is the implementation of the dialogue manager. To re-
duce this development phase, we have been creating a
set of domain-independent functions that can be special-
ized to a particular domain through passed parameters.
These functions perform such tasks as checking a query
for completeness, filtering the database results on user-
specified constraints, or making decisions on fuzzy at-
tributes such as ?near? (Polifroni and Chung, 2002).
One common but important subgoal in dialogue plan-
ning is to generate a succinct description of a set of re-
trieved entries. Our recent research in this area has fo-
cused on organizing database retrievals into a summary
meaning representation, by automatically clustering sets
into natural groupings. In parallel, we are developing
generation tools that will translate these summaries into
fluent English. For instance, in the hotel domain, the re-
sult set is automatically partitioned into ?cheap? or ?ex-
pensive? differently depending upon the city. By basing
such subjective categories on a content provider, we al-
leviate the burden of the system developer, while at the
same time producing a more intelligent system.
3 Summary and Conclusions
While there is inadequate space here to properly cover
such a large topic as flexible and rapidly reconfigurable
mixed-initiative dialogue systems, we hope that we have
managed to convey our long-term research goals ade-
quately and to provide the excitement that we ourselves
feel in our current efforts to turn this vision into a reality.
In fact, important subgoals that we have had for many
years, such as incremental vocabulary update, grammar
development and training through recycled resources,
and tools to enable rapid development of effective dia-
logue interaction, are now finally bearing fruit. We be-
lieve that this is a critical moment in the life of dia-
logue system research, and we anticipate exciting break-
throughs in the near future, leading to systems that are
not only useful but also easy to use and accommodating,
such that users will prefer them over alternative means of
acquiring their information needs.
References
G. Chung and S. Seneff, ?Integrating speech with keypad in-
put for automatic entry of spelling and pronunciation of new
words,? Proc. ICSLP, 2061?2064, Denver, CO, 2002.
G. Chung, S. Seneff, and C. Wang, ?Automatic acquisition of
names using speak and spell mode in spoken dialogue sys-
tems,? Proc. HLT-NAACL ?03, Edmonton, Canada, 2003.
J. Glass and E. Weinstein, ?SPEECHBUILDER: Facilitating
spoken dialogue system development,? Proc. Eurospeech,
1335?1338, Aalborg, Denmark, 2001.
J. Polifroni and G. Chung, ?Promoting portability in dialogue
management,? Proc. ICSLP, 2721?2724, Denver, CO, 2002.
J. Polifroni, G. Chung, and S. Seneff, ?Towards automatic gen-
eration of mixed-initiative dialogue systems from web con-
tent,? submitted to EUROSPEECH, 2003.
J. Schalkwyk, L. Hetherington, and E. Story, ?Speech recogni-
tion with dynamic grammars,? submitted to EUROSPEECH,
2003.
S. Seneff, G. Chung and C. Wang, ?Empowering end users
to personalize dialogue systems through spoken interaction,?
submitted to EUROSPEECH, 2003.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and V. Zue,
?Galaxy-II: A reference architecture for conversational sys-
tem development,? Proc. ICSLP, 931?934, Sydney, Aus-
tralia, 1998.
S. Seneff and J. Polifroni, ?Dialogue management in the MER-
CURY flight reservation system,? Proc. ANLP-NAACL Satel-
lite Workshop, 1?6, Seattle, WA, 2000.
S. Seneff, C. Wang and T. J. Hazen, ?Automatic induction of
N -gram language models from a natural language grammar,?
submitted to EUROSPEECH, 2003.
V. Zue and J. Glass, ?Conversational interfaces: Advances and
challenges,? Proc. IEEE, 88(8), 1166?1180, 2000.
Analysis and Processing of Lecture Audio Data: Preliminary Investigations
James Glass, Timothy J. Hazen, Lee Hetherington, and Chao Wang
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar Street, Cambridge, MA 02139, USA
(glass,hazen,hetherington,wang)@csail.mit.edu
Abstract
In this paper we report on our recent efforts to
collect a corpus of spoken lecture material that
will enable research directed towards fast, ac-
curate, and easy access to lecture content. Thus
far, we have collected a corpus of 270 hours of
speech from a variety of undergraduate courses
and seminars. We report on an initial analysis
of the spontaneous speech phenomena present
in these data and the vocabulary usage patterns
across three courses. Finally, we examine lan-
guage model perplexities trained from written
and spoken materials, and describe an initial
recognition experiment on one course.
1 Introduction
In the past decade, we have seen a dramatic increase
in the availability of on-line academic lecture material.
These educational resources can potentially change the
way people learn ? students with disabilities can en-
hance their educational experience, professionals can
keep up with recent advancements in their field, and peo-
ple of all ages can satisfy their thirst for knowledge. In
contrast to many other communicative activities however,
lecture processing has until recently enjoyed little bene-
fit from the development of human language technology.
Although there has been significant research directed to-
ward audio indexing and retrieval (Bacchiani et al, 2001;
Foote, 1999; Jourlin et al, 2000; Makhoul et al, 2000;
Franz et al, 2003; Renals et al, 2000), lecture tran-
scription and analysis is a relatively unexplored area in
speech and natural language research. The most substan-
tial research on lectures has been performed as part of
the Spontaneous Speech Project in Japan (Furui, 2003),
where researchers are processing a variety of Japanese
monologues such as academic and simulated presenta-
tions, news commentaries, etc. There has also been some
work reported on German lectures (Hurst et al, 2002).
One of the reasons for the minimal research in this
area is due to the limited availability of relevant data.
The only publicly available corpus of academic presen-
tations in English is TED, which includes 48 hours of au-
dio recordings of 188 presentations given at Eurospeech
?93 (Lamel et al, 1994). Only 6 of the presenters were
native English speakers however, and only 39 of the lec-
tures have been transcribed. The Corpus of Spontaneous
Japanese currently contains over 2,500 transcribed pre-
sentations (Kawahara et al, 2003). Both of these corpora
focus on conference presentations, which are shorter and
have a lower degree of spontaneity than a one hour or 90
minute classroom lecture.
We have recently initiated a research effort with the
goal of enabling fast, accurate, and easy access to lec-
ture materials. As part of the first phase of this research,
we have begun to create a large corpus of spoken lecture
material. In this paper, we document our ongoing data
collection activities, and describe the results of our pre-
liminary analyses of these data.
2 Corpus Creation and Annotation
In our efforts to date, we have created an initial corpus
of approximately 270 hours containing lectures from six
different courses, and from over 80 seminars given on a
variety of topics. On average, each course contained over
30 lecture sessions. These data were recorded with an
omni-directional microphone (as part of a video record-
ing), and generally occurred in a classroom environment.
To provide data for acoustic and language model train-
ing, we are in the process of generating transcriptions for
the lecture material we have collected to date. An ini-
tial set of transcriptions have been generated by an au-
dio transcription service. The transcription service was
instructed to pay careful attention to generating a correct
literal transcription of what was spoken (and not a ?clean?
transcript with disfluencies such as filled pauses and false
starts removed). In additional to the spoken words, the
transcription service also provided the following anno-
tations: (1) occasional time markers, usually at obvious
pauses or sentence boundaries, (2) locations of speaker
changes (labeled with speaker identities when known),
and (3) punctuation based on the transcribers subjective
assessment of the structure of the spoken utterances.
In addition to the audio data, we have obtained elec-
tronic versions of texts associated with three of these
courses, and over 100 summaries of lecture content for
one of them. We have also obtained electronic notes and
presentations for another course. These resources will be
used for our research involving written and spoken data.
3 Analysis of Lecture Characteristics
3.1 Qualitative Analysis
As illustrated in Figure 1, lecture data has much in com-
mon with casual, or spontaneous speech data, including
false starts, extraneous filler words ( such as ?okay? and
?well?), and non-lexical filled pauses (such as ?uh? or
?um?). One can also easily observe that the colloquial
nature of the data is dramatically different in style from
the same presentation of this material in a text book. For
example, one linear algebra text book covers this material
using a section header that reads, ?8 Rules of Matrix Mul-
tiplication,? followed by text that reads, ?The method for
multiplying two matrices A and B to get C = AB can be
summarized as follows...? The section header and intro-
ductory sentence express the same information as the ten
utterances spoken in Figure 1. In other words, the textual
format is typically more concise and better organized.
Apart from poor planning at the sentence level, lecture
speech often exhibits poor planning at higher structural
levels as well. For example, tangential threads digressing
from the current primary theme are common in sponta-
neous speech. This is exemplified by the brief diversion
into matrix inversion in utterances (4), (5) and (6). This
off-theme digression occurs only three utterances after
the primary theme of ?the rules for matrix multiplication?
is introduced in (1).
3.2 Quantitative Analysis
In order to better quantify the characteristics of lecture
data, we have recently examined a set of 80 lectures taken
from three undergraduate courses in math, physics, and
computer science. The total number of words in each
approximately one hour lecture ranged between 5K and
12K words, with an average of nearly 7K words, and
standard deviation of 1.5K words. The number of unique
words used per lecture ranged from 500 to 1,100 words,
with an average of 800 words, and standard deviation
of 170 words. A preliminary assessment of spontaneous
speech phenomena showed that there tended to be fewer
(1) I?ve been talking ? I?ve been multiplying matrices
already, but certainly time for me to discuss the rules
for matrix multiplication.
(2) And the interesting part is the many ways you can
do it, and they all give the same answer.
(3) So it?s ? and they?re all important.
(4) So matrix multiplication, and then, uh, come in-
verses.
(5) So we?re ? uh, we ? mentioned the inverse of a
matrix, but there?s ? that?s a big deal.
(6) Lots to do about inverses and how to find them.
(7) Okay, so I?ll begin with how to multiply two ma-
trices.
(8) First way, okay, so suppose I have a matrix A mul-
tiplying a matrix B and ? giving me a result ? well, I
could call it C.
(9) A times B. Okay.
(10) Uh, so, l- let me just review the rule for w- for this
entry.
Figure 1: Transcript from a linear algebra lecture.
filled pauses than in Switchboard (1% vs. 3%), although
there were similar amounts of partial words (1%) and
contractions (3-4% vs. 5%) in the data we observed. It
is also clear that the behavior will very much depend on
the lecturer. However, on the basis of these results, we
hypothesize that in terms of spontaneous speech phenom-
ena, the lecture data is closer to Switchboard quality than
it is to a more carefully spoken corpus such as Broadcast
News.
As a preliminary examination of vocabulary usage, we
measured the out-of-vocabulary (OOV) rate of the lec-
ture material as a function of vocabulary size, where the
words in the vocabulary were the most frequently oc-
curring words for a given set of training data. Figure 2
displays the OOV rate vs. vocabulary size for a variety
of speech and text training sources on the latter half of
the computer science lectures (? 10hrs of speech). Each
curve plots the OOV rate as a function of the most fre-
quent words from a particular set of training material.
Curves (A), and (B) show the results using the 64K-word
Broadcast News, and 27K Switchboard lexicons, respec-
tively. Curve (C) was computed from the combined lec-
tures from a math and physics course. The remaining
curves were all computed from subject-specific material.
Curve (D) was computed from a companion textbook,
while curve (E) was computed from the first half of the
computer science lectures. Curve (F) was computed from
a combination of the text and lecture transcripts from the
course (i.e., (D)+(E)).
If one considers the best vocabulary to be one that has
a small OOV rate and a small size, the best matching data
102 103 104 105
10?2
10?1
100
A
B
C
D
E
F
Vocabulary Size
O
O
V 
Ra
te
A: Broadcast News (64k)
B: Switchboard (27k)
C: math + physics lectures (7k)
D: CS companion textbook (5k)
E: CS lectures (3k)
F: CS textbook D + lectures E (6k)
Figure 2: Out-of-vocabulary (OOV) rate vs. vocabulary
size as a function of training material. Each curve plots
the OOV rate in lectures from the latter half of a computer
science (CS) course as a function of the most frequent
words from a particular set of training material. The vo-
cabularies for curves D?F utilize subject-specific material
from a textbook, and/or the first half of the CS lectures.
was obtained, not surprisingly, from subject-specific ma-
terial. Even material from non-subject-related lectures
match the test data better than data from general human-
human conversations or broadcast news. However, we
have also observed (not plotted) that a combination of
general lecture and conversational material, combined
with related text material, can produce behavior similar
to subject-specific speech material.
In order to examine the impact of language model
training data on predicting word usage in lecture material,
we created a 3.3K-word vocabulary exactly covering the
latter half of the computer science lectures. We then cre-
ated trigram language models from a variety of sources
(ignoring OOV words) using the SRILM Toolkit (Stol-
cke, 2002), and measured their perplexity on this data.
The results, as shown in Table 1, show again, not surpris-
ingly, that spoken material provides the most constraints.
Text material from Broadcast News or even the course
textbook are poor predictors of language usage. Models
of general human conversations do significantly better,
although data from general lectures is better than arbi-
trary conversations. It was interesting to observe that a
mixture of subject-specific textbook material and exam-
ple lectures provided the most constraints for new lec-
ture material, although there is still a considerable gap
between this and the case of training the language model
on the test set.
Finally, to investigate the nature of the OOV words for
a general vocabulary, we created a vocabulary of 1,568
words that were common to all three courses. Table 2
Training corpus Perplexity
Broadcast News (A) 380
Switchboard (B) 271
Other Lectures (C) 243
Course Textbook (D) 400
Subject-specific Lectures (E) 161
Textbook & Subject-specific Lectures (F) 137
Test-set Lectures 40
Table 1: Perplexities on CS lectures using trigrams cre-
ated from different training data. Trigram perplexities of
a 3.3K-word vocabulary trained with different text mate-
rials, and tested on 10hrs of CS lectures. Letter designa-
tions correspond to OOV measures plotted in Figure 2.
lists the ten most frequent subject-specific words for each
of the three courses (i.e., OOV words that were not in
the common vocabulary), along with the rank of each
of these words in the Broadcast News and Switchboard
corpora. Not surprisingly, these OOV words tend to be
subject-specific content words, and are likely to be im-
portant words for any kind of summarization or retrieval
task.
4 Preliminary Transcript Generation
The speech recognition processing that has been used to
generate transcripts of spoken lectures has largely been
based on large-vocabulary continuous speech recognition
technology (Hurst et al, 2002; Leeuwis et al, 2003;
Kawahara et al, 2003; Yokoyama et al, 2003). Lan-
guage modeling research has focused on mixing topic-
dependent textual source material (e.g., conference pa-
pers) with unrelated or topic-independent spoken mate-
rial (e.g., Switchboard data, or transcripts of other spoken
material) (Kato et al, 2000).
In our initial speech recognition experiments, we have
developed a recognizer that has been used to align the
transcripts with the speech signal for three courses (ap-
proximately 80 lectures) (Glass, 2003). Based on man-
ual examination, we believe that the alignments of the
16KHz wide-band speech are of good quality, and are
on par with previous alignments we have performed on
Broadcast News, Switchboard, as well as our own in-
ternal spontaneous speech corpora. Using these data as
training material, we have performed a baseline speech
recognition experiment on one course. Using a 5000
word vocabulary and trigram language model (perplex-
ity 120) derived from a portion of lecture transcriptions
and text book, we obtained a 33% word error rate on un-
seen lectures. This result is in line with other lecture word
error rates of 30-40% that have been reported in the liter-
ature (Leeuwis et al, 2003; Kawahara et al, 2003).
Computer Science Physics Linear Algebra
word BN SB word BN SB word BN SB
procedure 2683 5486 field 1029 890 matrix 23752 12918
expression 4211 6935 charge 1004 750 transpose 51305 25829
environment 1268 1055 magnetic 10599 15961 determinant 29023 ?
stream 5409 3210 electric 3520 1733 null 29431 ?
cons 14173 5385 force 434 922 eigenvalues ? ?
program 370 410 volts 33928 ? rows 12440 8272
procedures 3162 5487 energy 1386 1620 matrices ? ?
machine 2201 906 theta ? ? eigen ? ?
arguments 2279 3738 omega 24266 16279 orthogonal ? ?
cdr ? ? maximum 4107 3775 diagonal 34008 14916
Table 2: Top ten most frequent subject-specific words for three courses. Subject-specific words not contained in a
common 1.5K-word vocabulary. Frequency rank for 64K-word Broadcast News (BN) and 27K-word Switchboard
(SB) corpora also shown (? means never occurred).
5 Ongoing and Future Activities
The technical language of academic lectures and lack of
in-domain spoken data for training makes lecture tran-
scription a significant challenge, that will require new
methods for deriving a vocabulary and language model.
To enable effective use of comparable textual material as
a surrogate for in-domain spoken data, we plan to inves-
tigate techniques to transform written text into a conver-
sation style that can be used for language modelling. We
are also exploring a lecture-independent recognizer struc-
ture that uses a small number of words common to lec-
ture discourse along with a sub-word model to represent
subject-specific words.
Finally, we plan to continue to collect and compile
lecture material into a comprehensive annotated corpus.
It is our plan to make this resource available to the
research community, in the hope that it will facilitate
speech and language processing research in this area.
Acknowledgements Support for this research was
provided in part by the MIT/Microsoft iCampus Alliance
for Educational Technology.
References
M. Bacchiani, J. Hirschberg, A. Rosenberg, S. Whittaker,
D. Hindle, P. Isenhour, M. Jones, L. Stark, and G. Zam-
chick. 2001. SCANMail: Audio navigation in the
voicemail domain. In HLT2001, San Diego.
J. Foote. 1999. An overview of audio information re-
trieval. J. ACM Multimedia Systems, 7(1):2?10.
M. Franz, B. Ramabhadran, T. Ward, and M. Picheny.
2003. Automatic transcription and topic segmentation
of large spoken archives. In Proc. Eurospeech, pages
953?956, Geneva.
S. Furui. 2003. Recent advances in spontaneous speech
recognition and understanding. In Proc. IEEE Work-
shop on Spont. Speech Proc. and Rec., pages 1?6,
Tokyo.
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer, Speech, and Lan-
guage, 17(2-3):137?152.
W. Hurst, T. Kreuzer, and M. Wiesenhutter. 2002. A
qualitative study towards using large vocabulary auto-
matic speech recognition to index recorded presenta-
tions for search and access over the web. In Proceed-
ings of IADIS WWW/Internet 2002 Conference, Lis-
boa, Portugal.
P. Jourlin, S. E. Johnson, K. S. Jones, and P. C. Wood-
land. 2000. Spoken document representations for
probabilistic retrieval. Speech Communication, 32(1-
2):21?36.
K. Kato, H. Nanjo, and T. Kawahara. 2000. Automatic
transcription of lecture speech using topic-independent
language modeling. In Proc. ICSLP, pages 162?165,
Beijing.
T. Kawahara, K. Shitaoka, T. Kitade, and H. Nanjo.
2003. Automatic indexing of key sentences for lecture
archives. In Proc. ASRU, pages 141?144, St. Thomas.
L. Lamel, F. Schiel, A. Fourcin, J. Mariani, and H. Till-
man. 1994. The translingual English database (TED).
In Proc. ICSLP, pages 1795?1798, Yokohama.
E. Leeuwis, M. Federico, and M. Cettolo. 2003. Lan-
guage modeling and transcription of the TED corpus
lectures. In Proc. ICASSP, Hong Kong.
J. Makhoul, F. Kubala, T. Leek, D. Liu, L. Nguyen,
R. Schwartz, and A. Srivastava. 2000. Speech and
language technologies for audio indexing and retrieval.
Proc. IEEE, 88(8):1338?1353.
S. Renals, D. Abberley, D. Kirby, and T. Robinson. 2000.
Indexing and retrieval of broadcast news. SpeechCom-
munication, 32(1-2):5?20.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. ICSLP, pages 901?904, Denver.
T. Yokoyama, T. Shinozaki, K. Iwano, and S. Furui.
2003. Unsupervised class-based language model adap-
tation for spontaneous speech recognition. In Proc.
ICASSP, pages 236?239, Hong Kong.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 373?381,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Style & Topic Language Model Adaptation Using HMM-LDA 
 
 
Bo-June (Paul) Hsu, James Glass 
MIT Computer Science and Artificial Intelligence Laboratory 
32 Vassar Street, Cambridge, MA 02139, USA 
{bohsu,glass}@mit.edu 
 
 
  
 
Abstract 
Adapting language models across styles 
and topics, such as for lecture transcrip-
tion, involves combining generic style 
models with topic-specific content rele-
vant to the target document.  In this 
work, we investigate the use of the Hid-
den Markov Model with Latent Dirichlet 
Allocation (HMM-LDA) to obtain syn-
tactic state and semantic topic assign-
ments to word instances in the training 
corpus.  From these context-dependent 
labels, we construct style and topic mod-
els that better model the target document, 
and extend the traditional bag-of-words 
topic models to n-grams.  Experiments 
with static model interpolation yielded a 
perplexity and relative word error rate 
(WER) reduction of 7.1% and 2.1%, re-
spectively, over an adapted trigram base-
line.  Adaptive interpolation of mixture 
components further reduced perplexity 
by 9.5% and WER by a modest 0.3%. 
1 Introduction 
With the rapid growth of audio-visual materials 
available over the web, effective language mod-
eling of the diverse content, both in style and 
topic, becomes essential for efficient access and 
management of this information.  As a prime 
example, successful language modeling for aca-
demic lectures not only enables the initial tran-
scription via automatic speech recognition, but 
also assists educators and students in the creation 
and navigation of these materials through annota-
tion, retrieval, summarization, and even transla-
tion of the embedded content. 
Compared with other types of audio content, 
lecture speech often exhibits a high degree of 
spontaneity and focuses on narrow topics with 
specific terminology (Furui, 2003; Glass et al 
2004).  Unfortunately, training corpora available 
for language modeling rarely match the target 
lecture in both style and topic.  While transcripts 
from other lectures better match the style of the 
target lecture than written text, it is often difficult 
to find transcripts on the target topic.  On the 
other hand, although topic-specific vocabulary 
can be gleaned from related text materials, such 
as the textbook and lecture slides, written lan-
guage is a poor predictor of how words are actu-
ally spoken.  Furthermore, given that the precise 
topic of a target lecture is often unknown a priori 
and may even shift over time, it is generally dif-
ficult to identify topically related documents.  
Thus, an effective language model (LM) need to 
not only account for the casual speaking style of 
lectures, but also accommodate the topic-specific 
vocabulary of the subject matter. Moreover, the 
ability of the language model to dynamically 
adapt over the course of the lecture could prove 
extremely useful for both increasing transcription 
accuracy, as well as providing evidence for lec-
ture segmentation and information retrieval. 
In this paper, we investigate the application of 
the syntactic state and semantic topic assign-
ments from the Hidden Markov Model with La-
tent Dirichlet Allocation model to the problem of 
language modeling.  We explore the use of these 
context-dependent labels to identify style and 
learn topics from both a large number of spoken 
lectures as well as written text.  By dynamically 
interpolating lecture style models with topic-
specific models, we obtain language models that 
better describe the subtopic structure within a 
lecture.  Initial experiments demonstrate a 16.1% 
perplexity reduction and a 2.4% WER reduction 
over an adapted trigram baseline. 
373
In the following sections, we first summarize 
related research on adaptive and topic-mixture 
language models, and describe previous work on 
the HMM-LDA model.  We then examine the 
ability of the model to learn syntactic classes as 
well as topics from textbook materials and lec-
ture transcripts.  Next, we describe a variety of 
language model experiments we performed to 
combine style and topic models constructed from 
the state and topic labels with conventional tri-
gram models trained from both spoken and writ-
ten materials.  We also demonstrate the use of 
the combined model in an on-line adaptive mode.  
Finally, we summarize the results of this research 
and suggest future opportunities for related mod-
eling techniques in spoken lecture and other con-
tent processing research. 
2 Adaptive and Topic-Mixture LMs 
The concept of adaptive and topic-mixture lan-
guage models has been previously explored by 
many researchers.  Adaptive language modeling 
exploits the property that words appearing earlier 
in a document are likely to appear again. Cache 
language models (Kuhn and De Mori, 1990; 
Clarkson and Robinson, 1997) leverage this ob-
servation and increase the probability of previ-
ously observed words in a document when pre-
dicting the next word. By interpolating with a 
conditional trigram cache model, Goodman 
(2001) demonstrated up to 34% decrease in per-
plexity over a trigram baseline for small training 
sets. 
The cache intuition has been extended by at-
tempting to increase the probability of unob-
served but topically related words.  Specifically, 
given a mixture model with topic-specific com-
ponents, we can increase the mixture weights of 
the topics corresponding to previously observed 
words to better predict the next word.  Some of 
the early work in this area used a maximum en-
tropy language model framework to trigger in-
creases in likelihood of related words (Lau et al, 
1993; Rosenfeld, 1996). 
A variety of methods has been used to explore 
topic-mixture models.  To model a mixture of 
topics within a document, the sentence mixture 
model (Iyer and Ostendorf, 1999) builds multiple 
topic models from clusters of training sentences 
and defines the probability of a target sentence as 
a weighted combination of its probability under 
each topic model.  Latent Semantic Analysis 
(LSA) has been used to cluster topically related 
words and has demonstrated significant reduc-
tion in perplexity and word error rate (Belle-
garda, 2000).  Probabilistic LSA (PLSA) has 
been used to decompose documents into compo-
nent word distributions and create unigram topic 
models from these distributions.  Gildea and 
Hofmann (1999) demonstrated noticeable per-
plexity reduction via dynamic combination of 
these unigram topic models with a generic tri-
gram model. 
To identify topics from an unlabeled corpus, 
(Blei et al, 2003) extends PLSA with the Latent 
Dirichlet Allocation (LDA) model that describes 
each document in a corpus as generated from a 
mixture of topics, each characterized by a word 
unigram distribution. Hidden Markov Model 
with LDA (HMM-LDA) (Griffiths et al, 2004) 
further extends this topic mixture model to sepa-
rate syntactic words from content words whose 
distributions depend primarily on local context 
and document topic, respectively. 
In the specific area of lecture processing, pre-
vious work in language model adaptation has 
primarily focused on customizing a fixed n-gram 
language model for each lecture by combining n-
gram statistics from general conversational 
speech, other lectures, textbooks, and other re-
sources related to the target lecture (Nanjo and 
Kawahara, 2002, 2004; Leeuwis et al, 2003; 
Park et al, 2005). 
Most of the previous work on topic-mixture 
models focuses on in-domain adaptation using 
large amounts of matched training data.  How-
ever, most, if not all, of the data available to train 
a lecture language model are either cross-domain 
or cross-style.  Furthermore, although adaptive 
models have been shown to yield significant per-
plexity reduction on clean transcripts, the im-
provements tend to diminish when working with 
speech recognizer hypotheses with high WER. 
In this work, we apply the concept of dynamic 
topic adaptation to the lecture transcription task.  
Unlike previous work, we first construct a style 
model and a topic-domain model using the clas-
sification of word instances into syntactic states 
and topics provided by HMM-LDA.  Further-
more, we leverage the context-dependent labels 
to extend topic models from unigrams to n-
grams, allowing for better prediction of transi-
tions involving topic words.  Note that although 
this work focuses on the use of HMM-LDA to 
generate the state and topic labels, any method 
that yields such labels suffices for the purpose of 
the language modeling experiments.  The follow-
ing section describes the HMM-LDA framework 
in more detail. 
374
3 HMM-LDA 
3.1 Latent Dirichlet Allocation 
Discrete Principal Component Analysis describes 
a family of models that decompose a set of fea-
ture vectors into its principal components (Bun-
tine and Jakulin, 2005).  Describing feature vec-
tors via their components reduces the number of 
parameters required to model the data, hence im-
proving the quality of the estimated parameters 
when given limited training data.  LSA, PLSA, 
and LDA are all examples from this family. 
Given a predefined number of desired compo-
nents, LSA models feature vectors by finding a 
set of orthonormal components that maximize 
the variance using singular value decomposition 
(Deerwester et al, 1990).  Unfortunately, the 
component vectors may contain non-interpret-
able negative values when working with word 
occurrence counts as feature vectors.  PLSA 
eliminates this problem by using non-negative 
matrix factorization to model each document as a 
weighted combination of a set of non-negative 
feature vectors (Hofmann, 1999).  However, be-
cause the number of parameters grows linearly 
with the number of documents, the model is 
prone to overfitting.  Furthermore, because each 
training document has its own set of topic weight 
parameters, PLSA does not provide a generative 
framework for describing the probability of an 
unseen document (Blei et al, 2003). 
To address the shortcomings of PLSA, Blei et 
al. (2003) introduced the LDA model, which fur-
ther imposes a Dirichlet distribution on the topic 
mixture weights corresponding to the documents 
in the corpus.  With the number of model pa-
rameters dependent only on the number of topic 
mixtures and vocabulary size, LDA is less prone 
to overfitting and is capable of estimating the 
probability of unobserved test documents. 
Empirically, LDA has been shown to outper-
form PLSA in corpus perplexity, collaborative 
filtering, and text classification experiments (Blei 
et al, 2003).  Various extensions to the basic 
LDA model have since been proposed.  The Au-
thor Topic model adds an additional dependency 
on the author(s) to the topic mixture weights of 
each document (Rosen-Zvi et al, 2005).  The 
Hierarchical Dirichlet Process is a nonparametric 
model that generalizes distribution parameter 
modeling to multiple levels.  Without having to 
estimate the number of mixture components, this 
model has been shown to match the best result 
from LDA on a document modeling task (Teh et 
al., 2004). 
3.2 Hidden Markov Model with LDA 
HMM-LDA model proposed by Griffiths et al 
(2004) combines the HMM and LDA models to 
separate syntactic words with local dependencies 
from topic-dependent content words without re-
quiring any labeled data.  Similar to HMM-based 
part-of-speech taggers, HMM-LDA maps each 
word in the document to a hidden syntactic state.  
Each state generates words according to a uni-
gram distribution except the special topic state, 
where words are modeled by document-specific 
mixtures of topic distributions, as in LDA.  
Figure 1 describes this generative process in 
more detail. 
Figure 1: Generative framework and graphical 
model representation of HMM-LDA.  The num-
ber of states and topics are pre-specified.  The 
topic mixture for each document is modeled with 
a Dirichlet distribution.  Each word wi in the n-
word document is generated from its hidden state 
si or hidden topic zi if si is the special topic state. 
 
Unlike vocabulary selection techniques that 
separate domain-independent words from topic-
specific keywords using word collocation statis-
tics, HMM-LDA classifies each word instance 
according to its context.  Thus, an instance of the 
word ?return? may be assigned to a syntactic 
state in ?to return a?, but classified as a topic 
keyword in ?expected return for?.  By labeling 
each word in the training set with its syntactic 
state and mixture topic, HMM-LDA not only 
separates stylistic words from content words in a 
context-dependent manner, but also decomposes 
the corpus into a set of topic word distributions.  
This form of soft, context-dependent classifica-
For each document d in the corpus: 
1. Draw topic weights d?  from )(Dirichlet ?  
2. For each word wi in document d: 
a. Draw topic zi from )l(Multinomia d?  
b. Draw state si from )(ultinomialM 1?ispi  
c. Draw word wi from: 



=
otherwise
s
i
i
s
i
z
)(lMultinomia
)(lMultinomia
?
? topics
 
 
 
D
  d
 
z1 w1 
z2 w2 
zn wn 
s1 
s2 
sn 
? ? ? 
 
375
tion has many potential uses for language model-
ing, topic segmentation, and indexing. 
3.3 Training 
To train an HMM-LDA model, we employ the 
MATLAB Topic Modeling Toolbox 1.3 (Grif-
fiths and Steyvers, 2004; Griffiths et al, 2004).  
This particular implementation performs Gibbs 
sampling, a form of Markov chain Monte Carlo 
(MCMC), to estimate the optimal model parame-
ters fitted to the training data.  Specifically, the 
algorithm creates a Markov chain whose station-
ary distribution matches the expected distribution 
of the state and topic labels for each word in the 
training corpus.  Starting from random labels, 
Gibbs sampling sequentially samples the label 
for each hidden variable conditioned on the cur-
rent value of all other variables.  After a suffi-
cient number of iterations, the Markov chain 
converges to the stationary distribution.  We can 
easily compute the posterior word distribution 
for each state and topic from a single sample by 
averaging over the label counts and prior pa-
rameters.  With a sufficiently large training set, 
we will have enough words assigned to each 
state and topic to yield a reasonable approxima-
tion to the underlying distribution. 
In the following sections, we examine the ap-
plication of models derived from the HMM-LDA 
labels to the task of spoken lecture transcription 
and explore techniques on adaptive topic model-
ing to construct a better lecture language model. 
4 HMM-LDA Analysis 
Our language modeling experiments have been 
conducted on high-fidelity transcripts of ap-
proximately 168 hours of lectures from three un-
dergraduate subjects in math, physics, and com-
puter science (CS), as well as 79 seminars cover-
ing a wide range of topics (Glass et al, 2004).  
For evaluation, we withheld the set of 20 CS lec-
tures and used the first 10 lectures as a develop-
ment set and the last 10 lectures for the test set.  
The remainder of these data was used for training 
and will be referred to as the Lectures dataset. 
To supplement the out-of-domain lecture tran-
scripts with topic-specific textual resources, we 
added the CS course textbook (Textbook) as ad-
ditional training data for learning the target top-
ics.  To create topic-cohesive documents, the 
textbook is divided at every section heading to 
form 271 documents.  Next, the text is heuristi-
cally segmented at sentence-like boundaries and 
normalized into the words corresponding to the 
spoken form of the text.  Table 1 summarizes the 
data used in this evaluation. 
 
Dataset Documents Sentences Vocabulary Words 
Lectures 150 58,626 25,654 1,390,039 
Textbook 271 6,762 4,686 131,280 
CS Dev 10 4,102 3,285 93,348 
CS Test 10 3,595 3,357 87,518 
Table 1: Summary of evaluation datasets. 
 
In the following analysis, we ran the Gibbs 
sampler against the Lectures dataset for a total of 
2800 iterations, computing a model every 10 it-
erations, and took the model with the lowest per-
plexity as the final model.  We built the model 
with 20 states and 100 topics based on prelimi-
nary experiments.  We also trained an HMM-
LDA model on the Textbook dataset using the 
same model parameters.  We ran the sampler for 
a total of 2000 iterations, computing the perplex-
ity every 100 iterations.  Again, we selected the 
lowest perplexity model as the final model. 
4.1 Semantic Topics 
HMM-LDA extracts words whose distributions 
vary across documents and clusters them into a 
set of components.  In Figure 2, we list the top 
10 words from a random selection of 10 topics 
computed from the Lectures dataset.  As shown, 
the words assigned to the LDA topic state are 
representative of content words and are grouped 
into broad semantic topics.  For example, topic 4, 
8, and 9 correspond to machine learning, linear 
algebra, and magnetism, respectively. 
Since the Lectures dataset consists of speech 
transcripts with disfluencies, it is interesting to 
1 2 3 4 5 6 7 8 9 10 
center 
world 
and 
ideas 
new 
technology 
innovation 
community 
place 
building 
work 
research 
right 
people 
computing 
network 
system 
information 
software 
computers 
rights 
human 
U. 
S. 
government 
international 
countries 
president 
world 
support 
system 
things 
robot 
systems 
work 
example 
person 
robots 
learning 
machine 
<laugh> 
her 
children 
book 
Cambridge 
books 
street 
city 
library 
brother 
<partial> 
memory 
ah 
brain 
animal 
okay 
eye 
synaptic 
receptors 
mouse 
class 
people 
tax 
wealth 
social 
American 
power 
world 
<unintelligible> 
society 
basis 
v 
<eh> 
vector 
matrix 
transformation 
linear 
eight 
output 
t 
magnetic 
current 
field 
loop 
surface 
direction 
e 
law 
flux 
m 
light 
red 
water 
colors 
white 
angle 
blue 
here 
rainbow 
sun 
 
Figure 2: The top 10 words from 10 randomly selected topics computed from the Lectures dataset. 
376
observe that ?<laugh>? is the top word in a 
topic corresponding to childhood memories.  
Cursory examination of the data suggests that the 
speakers talking about children tend to laugh 
more during the lecture.  Although it may not be 
desirable to capture speaker idiosyncrasies in the 
topic mixtures, HMM-LDA has clearly demon-
strated its ability to capture distinctive semantic 
topics in a corpus.  By leveraging all documents 
in the corpus, the model yields smoother topic 
word distributions that are less vulnerable to 
overfitting. 
Since HMM-LDA labels the state and topic of 
each word in the training corpus, we can also 
visualize the results by color-coding the words 
by their topic assignments.  Figure 3 shows a 
color-coded excerpt from a topically coherent 
paragraph in the Textbook dataset.  Notice how 
most of the content words (uppercase) are as-
signed to the same topic/color.  Furthermore, of 
the 7 instances of the words ?and? and ?or? 
(underlined), 6 are correctly classified as syntac-
tic or topic words, demonstrating the context-
dependent labeling capabilities of the HMM-
LDA model.  Moreover, from these labels, we 
can identify multi-word topic key phrases (e.g. 
output signals, input signal, ?and? gate) in addi-
tion to standalone keywords, an observation we 
will leverage later on with n-gram topic models. 
 
 
Figure 3: Color-coded excerpt from the Textbook 
dataset showing the context-dependent topic la-
bels.  Syntactic words appear black in lowercase.  
Topic words are shown in uppercase with their 
respective topic colors.  All instances of the 
words ?and? and ?or? are underlined. 
4.2 Syntactic States 
Since the syntactic states are shared across all 
documents, we expect words associated with the 
syntactic states when applying HMM-LDA to the 
Lectures dataset to reflect the lecture style vo-
cabulary.   
In Figure 4, we list the top 10 words from each 
of the 19 syntactic states (state 20 is the topic 
state).  Note that each state plays a clear syntactic 
role.  For example, state 2 contains prepositions 
while state 7 contains verbs.  Since the model is 
trained on transcriptions of spontaneous speech, 
hesitation disfluencies (<uh>, <um>, <partial>) 
are all grouped in state 3 along with other words 
(so, if, okay) that frequently indicate hesitation.  
While many of these hesitation words are con-
junctions, the words in state 6 show that most 
conjunctions are actually assigned to a different 
state representing different syntactic behavior 
from hesitations.  As demonstrated with sponta-
neous speech, HMM-LDA yields syntactic states 
that have a good correspondence to part-of-
speech labels, without requiring any labeled 
training data. 
4.3 Discussions 
Although MCMC techniques converge to the 
global stationary distribution, we cannot guaran-
tee convergence from observation of the perplex-
ity alone.  Unlike EM algorithms, random sam-
pling may actually temporarily decrease the 
model likelihood.  Thus, in the above analysis, 
the number of iterations was chosen to be at least 
double the point at which the perplexity first ap-
peared to converge. 
In addition to the number of iterations, the 
choice of the number of states and topics, as well 
as the values of the hyper-parameters on the 
Dirichlet prior, also impact the quality and effec-
tiveness of the resulting model.  Ideally, we run 
the algorithm with different combinations of the 
parameter values and perform model selection to 
choose the model with the best complexity-
penalized likelihood.  However, given finite 
computing resources, this approach is often im-
We draw an INVERTER SYMBOLICALLY as in Figure 3.24.  
An AND GATE, also shown in Figure 3.24, is a PRIMITIVE 
FUNCTION box with two INPUTS and ONE OUTPUT.  It 
drives its OUTPUT SIGNAL to a value that is the LOGICAL 
AND of the INPUTS.  That is, if both of its INPUT SIGNALS 
BECOME 1.  Then ONE and GATE DELAY time later the AND 
GATE will force its OUTPUT SIGNAL TO be 1; otherwise the 
OUTPUT will be 0.  An OR GATE is a SIMILAR two INPUT 
PRIMITIVE FUNCTION box that drives its OUTPUT SIGNAL 
to a value that is the LOGICAL OR of the INPUTS.  That is, the 
OUTPUT will BECOME 1 if at least ONE of the INPUT 
SIGNALS is 1; otherwise the OUTPUT will BECOME 0. 
 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 
the 
this 
a 
that 
these 
my 
our 
your 
those 
their 
of 
in 
for 
on 
with 
at 
from 
by 
about 
as 
so 
<uh> 
if 
<um> 
<partial> 
now 
then 
okay 
well 
but 
know 
see 
do 
think 
go 
get 
say 
make 
look 
take 
I 
you 
we 
they 
let 
let's 
he 
I'll 
people 
I'd 
and 
but 
or 
because 
as 
that 
where 
thank 
which 
is 
is 
are 
was 
has 
were 
goes 
had 
comes 
means 
says 
it's 
not 
that's 
I'm 
just 
there's 
<uh> 
we're 
also 
you're 
it 
you 
out 
up 
them 
that 
me 
about 
here 
all 
a 
an 
some 
one 
no 
in 
two 
any 
this 
another 
way 
time 
thing 
lot 
question 
kind 
point 
case 
idea 
problem 
it 
this 
that 
there 
which 
he 
here 
course 
who 
they 
two 
one 
three 
hundred 
m 
t 
five 
d 
years 
four 
going 
doing 
one 
looking 
sort 
done 
able 
coming 
talking 
trying 
that 
what 
how 
where 
when 
if 
why 
which 
as 
because 
can 
will 
would 
don't 
could 
do 
just 
me 
should 
may 
very 
more 
little 
much 
good 
different 
than 
important 
long 
as 
to 
just 
longer 
doesn't 
never 
go 
physically 
that'll 
anybody's 
with 
have 
be 
want 
had 
get 
like 
got 
need 
try 
take 
Figure 4: The top 10 words from the 19 syntactic states computed from the Lectures dataset. 
377
practical.  As an alternative for future work, we 
would like to perform Gibbs sampling on the 
hyper-parameters (Griffiths et al, 2004) and ap-
ply the Dirichlet process to estimate the number 
of states and topics (Teh et al, 2004). 
Despite the suboptimal choice of parameters 
and potential lack of convergence, the labels de-
rived from HMM-LDA are still effective for lan-
guage modeling applications, as described next. 
5 Language Modeling Experiments 
To evaluate the effectiveness of models derived 
from the separation of syntax from content, we 
performed experiments that compare the per-
plexities and WERs of various model combina-
tions.  For a baseline, we used an adapted model 
(L+T) that linearly interpolates trigram models 
trained on the Lectures (L) and Textbook (T) 
datasets.  In all models, all interpolation weights 
and additional parameters are tuned on a devel-
opment set consisting of the first half of the CS 
lectures and tested on the second half.  Unless 
otherwise noted, modified Kneser-Ney discount-
ing (Chen and Goodman, 1998) is applied with 
the respective training set vocabulary using the 
SRILM Toolkit (Stolcke, 2002). 
To compute the word error rates associated 
with a specific language model, we used a 
speaker-independent speech recognizer (Glass, 
2003).  The lectures were pre-segmented into 
utterances by forced alignment of the reference 
transcription. 
5.1 Lecture Style 
In general, an n-gram model trained on a limited 
set of topic-specific documents tends to overem-
phasize words from the observed topics instead 
of evenly distributing weights over all potential 
topics.  Specifically, given the list of words fol-
lowing an n-gram context, we would like to 
deemphasize the observed occurrences of topic 
words and ideally redistribute these counts to all 
potential topic words.  As an approximation, we 
can build such a topic-deemphasized style tri-
gram model (S) by using counts of only n-gram 
sequences that do not end on a topic word, 
smoothed over the Lectures vocabulary.  Figure 
5 shows the n-grams corresponding to an utter-
ance used to build the style trigram model.  Note 
that the counts of topic to style word transitions 
are not altered as these probabilities are mostly 
independent of the observed topic distribution. 
By interpolating the style model (S) from 
above with the smoothed trigram model based on 
the Lectures dataset (L), the combined model 
(L+S) achieves a 3.6% perplexity reduction and 
1.0% WER reduction over (L), as shown in Table 
2.  Without introducing topic-specific training 
data, we can already improve the generic lecture 
LM performance using the HMM-LDA labels. 
 
<s> for the SPATIAL MEMORY </s> 
unigrams: for, the, spatial, memory, </s> 
bigrams: <s> for, for the, the spatial, spatial memory, memory </s> 
trigrams: <s> <s> for, <s> for the, for the spatial, 
 the spatial memory, spatial memory </s> 
Figure 5: Style model n-grams.  Topic words in 
the utterance are in uppercase.   
5.2 Topic Domain 
Unlike Lectures, the Textbook dataset contains 
content words relevant to the target lectures, but 
in a mismatched style.  Commonly, the Textbook 
trigram model is interpolated with the generic 
model to improve the probability estimates of the 
transitions involving topic words.  The interpola-
tion weight is chosen to best fit the probabilities 
of these n-gram sequences while minimizing the 
mismatch in style.  However, with only one pa-
rameter, all n-gram contexts must share the same 
mixture weight.  Because transitions from con-
texts containing topic words are rarely observed 
in the off-topic Lectures, the Textbook model (T) 
should ideally have higher weight in these con-
texts than contexts that are more equally ob-
served in both datasets. 
One heuristic approach for adjusting the 
weight in these contexts is to build a topic-
domain trigram model (D) from the Textbook n-
gram counts with Witten-Bell smoothing (Chen 
and Goodman, 1998) where we emphasize the 
sequences containing a topic word in the context 
by doubling their counts.  In effect, this reduces 
the smoothing on words following topic contexts 
with respect to lower-order models without sig-
nificantly affecting the transitions from non-topic 
words.  Figure 6 shows the adjusted counts for an 
utterance used to build the domain trigram 
model.   
 
<s> HUFFMAN CODE can be represented as a BINARY TREE ? 
unigrams: huffman, code, can, be, represented, as, binary, tree, ? 
bigrams: <s> huffman, huffman code (2?), code can (2?),  
 can be, be represented, represented as, a binary,  
 binary tree (2?), ? 
trigrams: <s> <s> hufmann, <s> hufmann code (2?),  
 hufmann code can (2?), code can be (2?),  
 can be represented, be represented as,  
 represented as a, as a binary, a binary tree (2?), ... 
Figure 6: Domain model n-grams.  Topic words 
in the utterance are in uppercase. 
378
Empirically, interpolating the lectures, text-
book, and style models with the domain model 
(L+T+S+D) further decreases the perplexity by 
1.4% and WER by 0.3% over (L+T+S), validat-
ing our intuition.  Overall, the addition of the 
style and domain models reduces perplexity and 
WER by a noticeable 7.1% and 2.1%, respec-
tively, as shown in Table 2. 
 
 Perplexity 
Model Development Test 
L: Lectures Trigram 180.2 (0.0%) 199.6 (0.0%) 
T: Textbook Trigram 291.7 (+61.8%) 331.7 (+66.2%) 
S: Style Trigram 207.0 (+14.9%) 224.6 (+12.5%) 
D: Domain Trigram 354.1 (+96.5%) 411.6 (+106.3%) 
L+S 174.2 (?3.3%) 192.4 (?3.6%) 
L+T: Baseline 138.3 (0.0%) 154.4 (0.0%) 
L+T+S 131.0 (?5.3%) 145.6 (?5.7%) 
L+T+S+D 128.8 (?6.9%) 143.6 (?7.1%) 
L+T+S+D+Topic100 
? Static Mixture (cheat) 
? Dynamic Mixture 
 
118.1 (?14.6%) 
115.7 (?16.4%) 
 
131.3 (?15.0%) 
129.5 (?16.1%) 
 
 Word Error Rate 
Model Development Test 
L: Lectures Trigram 49.5% (0.0%) 50.2% (0.0%) 
L+S 49.2% (?0.7%) 49.7% (?1.0%) 
L+T: Baseline 46.6% (0.0%) 46.7% (0.0%) 
L+T+S 46.0% (?1.2%) 45.8% (?1.8%) 
L+T+S+D 45.8% (?1.8%) 45.7% (?2.1%) 
L+T+S+D+Topic100 
? Static Mixture (cheat) 
? Dynamic Mixture 
 
45.5% (?2.4%) 
45.4% (?2.6%) 
 
45.4% (?2.8%) 
45.6% (?2.4%) 
 
Table 2: Perplexity (top) and WER (bottom) per-
formance of various model combinations.  Rela-
tive reduction is shown in parentheses. 
5.3 Textbook Topics 
In addition to identifying content words, HMM-
LDA also assigns words to a topic based on their 
distribution across documents.  Thus, we can 
apply HMM-LDA with 100 topics to the Text-
book dataset to identify representative words and 
their associated contexts for each topic.  From 
these labels, we can build unsmoothed trigram 
language models (Topic100) for each topic from 
the counts of observed n-gram sequences that 
end in a word assigned to the respective topic. 
Figure 7 shows a sample of the word n-grams 
identified via this approach for a few topics.  
Note that some of the n-grams are key phrases 
for the topic while others contain a mixture of 
syntactic and topic words.  Unlike bag-of-words 
models that only identify the unigram distribu-
tion for each topic, the use of context-dependent 
labels enables the construction of n-gram topic 
models that not only characterize the frequencies 
of topic words, but also describe the transition 
contexts leading up to these words. 
 
Huffman tree 
relative frequency 
relative frequencies 
the tree 
one hundred 
Monte Carlo 
rand update 
random numbers 
trials remaining 
trials passed 
time segment 
the agenda 
segment time 
current time 
first agenda 
assoc key 
the table 
local table 
a table 
of records 
Figure 7: Sample of n-grams from select topics. 
5.4 Topic Mixtures 
Since each target lecture generally only covers a 
subset of the available topics, it will be ideal to 
identify the specific topics corresponding to a 
target lecture and assign those topic models more 
weight in a linearly interpolated mixture model.  
As an ideal case, we performed a cheating ex-
periment to measure the best performance of a 
statically interpolated topic mixture model 
(L+T+S+D+Topic100) where we tuned the 
mixture weights of all mixture components, in-
cluding the lectures, textbook, style, domain, and 
the 100 individual topic trigram models on indi-
vidual target lectures.   
Table 2 shows that by weighting the compo-
nent models appropriately, we can reduce the 
perplexity and WER by an additional 7.9% and 
0.7%, respectively, over the (L+T+S+D) model 
even with simple linear interpolation for model 
combination. 
To gain further insight into the topic mixture 
model, we examine the breakdown of the nor-
malized topic weights for a specific lecture.  As 
shown in Figure 8, of the 100 topic models, 15 of 
them account for over 90% of the total weight.  
Thus, lectures tend to show a significant topic 
skew which topic adaptation approaches can 
model effectively. 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
 
Figure 8: Topic mixture weight breakdown. 
5.5 Topic Adaptation 
Unfortunately, since different lectures cover dif-
ferent topics, we generally cannot tune the topic 
mixture weights ahead of time.  One approach, 
without any a priori knowledge of the target lec-
ture, is to adaptively estimate the optimal mix-
ture weights as we process the lecture (Gildea 
and Hofmann, 1999).  However, since the topic 
distribution shifts over a long lecture, modeling a 
lecture as an interpolation of components with 
fixed weights may not be the most optimal.  In-
stead, we employ an exponential decay strategy 
where we update the current mixture distribution 
by linearly interpolating it with the posterior 
topic distribution given the current word.  Spe-
cifically, applying Bayes? rule, the probability of 
topic t generating the current word w is given by: 
379
( ) ( ) ( )( ) ( ) ???= t tPtwP
tPtwP
wtP |
||  
To achieve the exponential decay, we update the 
topic distribution after each word according to 
Pi+1(t) = (1 ?   )Pi(t) +   P(t | wi), where    is the 
adaptation rate. 
We evaluated this approach of dynamic mix-
ture weight adaptation on the (L+T+S+D+Topic 
100) model, with the same set of components as 
the cheating experiment with static weights.  As 
shown in Table 2, the dynamic model actually 
outperforms the static model by more than 1% in 
perplexity, by better modeling the dynamic topic 
substructure within the lecture. 
To run the recognizer with a dynamic LM, we 
rescored the top 100 hypotheses generated with 
the (L+T+S+D) model using the dynamic LM.  
The WER obtained through such n-best rescoring 
yielded noticeable improvements over the 
(L+T+S+D) model without a priori knowledge 
of the topic distribution, but did not beat the op-
timal static model on the test set.   
To further gain an intuition for mixture weight 
adaptation, we plotted the normalized adapted 
weights of the topic models across the first lec-
ture of the test set in Figure 9.  Note that the 
topic mixture varies greatly across the lecture.  In 
this particular lecture, the lecturer starts out with 
a review of the previous lecture.  Subsequently, 
he shows an example of computation using ac-
cumulators.  Finally, he focuses the lecture on 
stream as a data structure, with an intervening 
example that finds pairs of i and j that sum up to 
a prime.  By comparing the topic labels in Figure 
9 with the top words from the corresponding top-
ics in Figure 10, we observe that the topic 
weights obtained via dynamic adaptation match 
the subject matter of the lecture fairly closely. 
Finally, to assess the effect that word error rate 
has on adaptation performance, we applied the 
adaptation algorithm to the corresponding tran-
script from the automatic speech recognizer 
(ASR).  Traditional cache language models tend 
to be vulnerable to recognition errors since incor-
rect words in the history negatively bias the pre-
diction of the current word.  However, by adapt-
ing at a topic level, which reduces the number of 
dynamic parameters, the dynamic topic model is 
less sensitive to recognition errors.  As seen in 
Figure 9, even with a word error rate around 
40%, the normalized topic mixture weights from 
the ASR transcript still show a strong resem-
blance to the original weights from the manual 
reference transcript.  
 
Figure 9: Adaptation of topic model weights on 
manual and ASR transcription of a single lecture. 
 
T12 T35 T98 T99 
stream 
s 
streams 
integers 
series 
prime 
filter 
delayed 
interleave 
infinite 
pairs 
i 
j 
k 
pair 
s 
integers 
sum 
queens 
t 
sequence 
enumerate 
accumulate 
map 
interval 
filter 
sequences 
operations 
odd 
nil 
of 
see 
and 
in 
for 
vs 
register 
data 
as 
make 
Figure 10: Top 10 words from select Textbook 
topics appearing in Figure 9. 
6 Summary and Conclusions 
In this paper, we have shown how to leverage 
context-dependent state and topic labels, such as 
the ones generated by the HMM-LDA model, to 
construct better language models for lecture tran-
scription and extend topic models beyond tradi-
tional unigrams.  Although the WER of the top 
recognizer hypotheses exceeds 45%, by dynami-
cally updating the mixture weights to model the 
topic substructure within individual lectures, we 
are able to reduce the test set perplexity and 
WER by over 16% and 2.4%, respectively, rela-
tive to the combined Lectures and Textbook 
(L+T) baseline. 
Although we primarily focused on lecture 
transcription in this work, the techniques extend 
to language modeling scenarios where exactly 
matched training data are often limited or non-
existent.  Instead, we have to rely on appropriate 
combination of models derived from partially 
matched data.  HMM-LDA and related tech-
niques show great promise for finding structure 
in unlabeled data, from which we can build more 
sophisticated models. 
The experiments in this paper combine models 
primarily through simple linear interpolation.  As 
motivated in section 5.2, allowing for context-
dependent interpolation weights based on topic 
380
labels may yield significant improvement for 
both perplexity and WER.  Thus, in future work, 
we would like to study algorithms for automati-
cally learning appropriate context-dependent in-
terpolation weights.  Furthermore, we hope to 
improve the convergence properties of the dy-
namic adaptation scheme at the start of lectures 
and across topic transitions.  Lastly, we would 
like to extend the LDA framework to support 
speaker-specific adaptation and apply the result-
ing topic distributions to lecture segmentation. 
Acknowledgements 
We would like to thank the anonymous review-
ers for their useful comments and feedback.  
Support for this research was provided in part by 
the National Science Foundation under grant 
#IIS-0415865.   Any opinions, findings, and con-
clusions, or recommendations expressed in this 
material are those of the authors and do not nec-
essarily reflect the views of the NSF. 
Reference 
Y. Akita and T. Kawahara.  2004.  Language Model 
Adaptation Based on PLSA of Topics and Speak-
ers.  In Proc. ICSLP. 
J. Bellegarda.  2000.  Exploiting Latent Semantic In-
formation in Statistical Language Modeling.  In 
Proc. IEEE, 88(8):1279-1296. 
D. Blei, A. Ng, and M. Jordan.  1993.  Latent 
Dirichlet Allocation.  Journal of Machine Learning 
Research, 3:993-1022. 
W. Buntine and A. Jakulin.  2005.  Discrete Principal 
Component Analysis.  Technical Report, Helsinki 
Institute for Information Technology. 
S. Chen and J. Goodman.  1996.  An Empirical Study 
of Smoothing Techniques for Language Modeling.  
In Proc. ACL, 310-318. 
P. Clarkson and A. Robinson.  1997.  Language 
Model Adaptation Using Mixtures and an Expo-
nentially Decaying Cache.  In Proc. ICASSP.   
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, R. 
Harshman.  1990.  Indexing by Latent Semantic 
Analysis.  Journal of the American Society for In-
formation Science, 41(6):391-407. 
S. Furui.  2003.  Recent Advances in Spontaneous 
Speech Recognition and Understanding.  In Proc. 
IEEE Workshop on Spontaneous Speech Proc. and 
Rec, 1-6. 
D. Gildea and T. Hofmann.  1999.  Topic-Based Lan-
guage Models Using EM.  In Proc. Eurospeech. 
J. Glass.  2003.  A Probabilistic Framework for Seg-
ment-based Speech Recognition.  Computer, Speech 
and Language, 17:137-152. 
J. Glass, T.J. Hazen, L. Hetherington, and C. Wang.  
2004.  Analysis and Processing of Lecture Audio 
Data: Preliminary Investigations.  In Proc. HLT-
NAACL Workshop on Interdisciplinary Approaches 
to Speech Indexing and Retrieval, 9-12. 
J. Goodman.  2001.  A Bit of Progress in Language 
Modeling (Extended Version).  Technical Report, 
Microsoft Research. 
T. Griffiths and M. Steyvers.  2004.  Finding Scien-
tific Topics.  In Proc. National Academy of Sci-
ence, 101(Suppl. 1):5228-5235. 
T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum.  
2004.  Integrating Topics and Syntax.  Adv. in Neu-
ral Information Processing Systems, 17:537-544. 
R. Iyer and M. Ostendorf.  1999.  Modeling Long 
Distance Dependence in Language: Topic Mixtures 
Versus Dynamic Cache.  In IEEE Transactions on 
Speech and Audio Processing, 7:30-39. 
R. Kuhn and R. De Mori.  1990.  A Cache-Based 
Natural Language Model for Speech Recognition.  
In IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 12:570-583. 
R. Lau, R. Rosenfeld, S. Roukos.  1993.  Trigger-
Based Language Models: a Maximum Entropy 
Approach.  In Proc. ICASSP. 
E. Leeuwis, M. Federico, and M. Cettolo.  2003.  Lan-
guage Modeling and Transcription of the TED 
Corpus Lectures.  In Proc. ICASSP. 
H. Nanjo and T. Kawahara.  2002.  Unsupervised 
Language Model Adaptation for Lecture Speech 
Recognition.  In Proc. ICSLP. 
H. Nanjo and T. Kawahara.  2004.  Language Model 
and Speaking Rate Adaptation for Spontaneous 
Presentation Speech Recognition.  In IEEE Trans. 
SAP, 12(4):391-400. 
A. Park, T. Hazen, and J. Glass.  2005.  Automatic 
Processing of Audio Lectures for Information Re-
trieval: Vocabulary Selection and Language Mod-
eling.  In Proc. ICASSP. 
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. 
Smyth.  2004.  The Author-Topic Model for Au-
thors and Documents.  20th Conference on Uncer-
tainty in Artificial Intelligence. 
R. Rosenfeld.  1996.  A Maximum Entropy Approach 
to Adaptive Statistical Language Modeling.  Com-
puter, Speech and Language, 10:187-228. 
A. Stolcke.  2002.  SRILM ? An Extensible Language 
Modeling Toolkit.  In Proc. ICSLP. 
Y. Teh, M. Jordan, M. Beal, and D.  Blei.  2006.  Hi-
erarchical Dirichlet Processes.  To appear in Jour-
nal of the American Statistical Association. 
381
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Multimodal Home Entertainment Interface via a Mobile Device
Alexander Gruenstein Bo-June (Paul) Hsu James Glass Stephanie Seneff
Lee Hetherington Scott Cyphers Ibrahim Badr Chao Wang Sean Liu
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar St, Cambridge, MA 02139 USA
http://www.sls.csail.mit.edu/
Abstract
We describe a multimodal dialogue system for
interacting with a home entertainment center
via a mobile device. In our working proto-
type, users may utilize both a graphical and
speech user interface to search TV listings,
record and play television programs, and listen
to music. The developed framework is quite
generic, potentially supporting a wide variety
of applications, as we demonstrate by integrat-
ing a weather forecast application. In the pro-
totype, the mobile device serves as the locus
of interaction, providing both a small touch-
screen display, and speech input and output;
while the TV screen features a larger, richer
GUI. The system architecture is agnostic to
the location of the natural language process-
ing components: a consistent user experience
is maintained regardless of whether they run
on a remote server or on the device itself.
1 Introduction
People have access to large libraries of digital con-
tent both in their living rooms and on their mobile
devices. Digital video recorders (DVRs) allow peo-
ple to record TV programs from hundreds of chan-
nels for subsequent viewing at home?or, increas-
ingly, on their mobile devices. Similarly, having
accumulated vast libraries of digital music, people
yearn for an easy way to sift through them from the
comfort of their couches, in their cars, and on the go.
Mobile devices are already central to accessing
digital media libraries while users are away from
home: people listen to music or watch video record-
ings. Mobile devices also play an increasingly im-
portant role in managing digital media libraries. For
instance, a web-enabled mobile phone can be used to
remotely schedule TV recordings through a web site
or via a custom application. Such management tasks
often prove cumbersome, however, as it is challeng-
ing to browse through listings for hundreds of TV
channels on a small display. Indeed, even on a large
screen in the living room, browsing alphabetically,
or by time and channel, for a particular show using
the remote control quickly becomes unwieldy.
Speech and multimodal interfaces provide a nat-
ural means of addressing many of these challenges.
It is effortless for people to say the name of a pro-
gram, for instance, in order to search for existing
recordings. Moreover, such a speech browsing ca-
pability is useful both in the living room and away
from home. Thus, a natural way to provide speech-
based control of a media library is through the user?s
mobile device itself.
In this paper we describe just such a prototype
system. A mobile phone plays a central role in pro-
viding a multimodal, natural language interface to
both a digital video recorder and a music library.
Users can interact with the system?presented as a
dynamic web page on the mobile browser?using
the navigation keys, the stylus, or spoken natural
language. In front of the TV, a much richer GUI is
also available, along with support for playing video
recordings and music.
In the prototype described herein, the mobile de-
vice serves as the locus of natural language in-
teraction, whether a user is in the living room or
walking down the street. Since these environments
may be very different in terms of computational re-
1
sources and network bandwidth, it is important that
the architecture allows for multiple configurations in
terms of the location of the natural language pro-
cessing components. For instance, when a device
is connected to a Wi-Fi network at home, recogni-
tion latency may be reduced by performing speech
and natural language processing on the home me-
dia server. Moreover, a powerful server may enable
more sophisticated processing techniques, such as
multipass speech recognition (Hetherington, 2005;
Chung et al, 2004), for improved accuracy. In sit-
uations with reduced network connectivity, latency
may be improved by performing speech recognition
and natural language processing tasks on the mobile
device itself. Given resource constraints, however,
less detailed acoustic and language models may be
required. We have developed just such a flexible ar-
chitecture, with many of the natural language pro-
cessing components able to run on either a server or
the mobile device itself. Regardless of the configu-
ration, a consistent user experience is maintained.
2 Related Work
Various academic researchers and commercial busi-
nesses have demonstrated speech-enabled interfaces
to entertainment centers. A good deal of the work
focuses on adding a microphone to a remote con-
trol, so that speech input may be used in addition
to a traditional remote control. Much commercial
work, for example (Fujita et al, 2003), tends to fo-
cus on constrained grammar systems, where speech
input is limited to a small set of templates corre-
sponding to menu choices. (Berglund and Johans-
son, 2004) present a remote-control based speech
interface for navigating an existing interactive tele-
vision on-screen menu, though experimenters man-
ually transcribed user utterances as they spoke in-
stead of using a speech recognizer. (Oh et al, 2007)
present a dialogue system for TV control that makes
use of concept spotting and statistical dialogue man-
agement to understand queries. A version of their
system can run independently on low-resource de-
vices such as PDAs; however, it has a smaller vo-
cabulary and supports a limited set of user utterance
templates. Finally, (Wittenburg et al, 2006) look
mainly at the problem of searching for television
programs using speech, an on-screen display, and a
remote control. They explore a Speech-In List-Out
interface to searching for episodes of television pro-
grams.
(Portele et al, 2003) depart from the model of
adding a speech interface component to an exist-
ing on-screen menu. Instead, they a create a tablet
PC interface to an electronic program guide, though
they do not use the television display as well. Users
may search an electronic program guide using con-
straints such as date, time, and genre; however, they
can?t search by title. Users can also perform typi-
cal remote-control tasks like turning the television
on and off, and changing the channel. (Johnston et
al., 2007) also use a tablet PC to provide an inter-
face to television content?in this case a database of
movies. The search can be constrained by attributes
such as title, director, or starring actors. The tablet
PC pen can be used to handwrite queries and to point
at items (such as actor names) while the user speaks.
We were also inspired by previous prototypes in
which mobile devices have been used in conjunc-
tion with larger, shared displays. For instance, (Paek
et al, 2004) demonstrate a framework for building
such applications. The prototype we demonstrate
here fits into their ?Jukebox? model of interaction.
Interactive workspaces, such as the one described in
(Johanson et al, 2002), also demonstrate the utility
of integrating mobile and large screen displays. Our
prototype is a departure from these systems, how-
ever, in that it provides for spoken interactions.
Finally, there is related work in the use of mobile
devices for various kinds of search. For instance, of-
ferings from Microsoft (Acero et al, 2008), Vlingo,1
and Promptu2 allow users to search for items like
businesses and songs using their mobile phones.
These applications differ from ours in that speech
is used only for search, without any accompanying
command and control capabilities. Also, these ser-
vices do not allow interaction with your own de-
vices at home. Efforts have been made to use mo-
bile devices for control of devices in the home, such
as in (Nichols and Myers, 2006), however these ef-
forts have not involved the use of speech as an input
modality.
1http://www.vlingo.com
2http://www.promptu.com
2
Na
vig
ati
on
 pa
d o
r s
tyl
us 
can
 
be
 us
ed
 to
 na
vig
ate
 G
UI
Sta
tus
 ic
on
 in
dic
ate
s c
on
ne
cti
on
 to
 se
rve
r, 
mi
cro
ph
on
e i
co
n w
he
n l
ist
en
ing
Ho
ld-
to-
tal
k 
bu
tto
n
Op
era we
b 
bro
ws
er
Sy
nc
hro
niz
ed
 se
lec
tio
n o
n m
ob
ile
 an
d 
tel
ev
isi
on
 di
spl
ay
s
Fir
efo
x
we
b b
row
ser
Ta
bs 
all
ow
 ea
sy 
sw
itc
hin
g 
am
on
g d
om
ain
s
Me
dia
 pl
ay
ba
ck
 
co
ntr
ols
Figure 1: User interface overview.
(a) Program Guide
(b) Scheduled Recordings
(c) Music Library
U: What?s on tonight at seven o?clock?
S: I found seventy entries. [Displays program guide.]
U: Are there any action movies on HBO tomorrow?
S: I found two entries. [Displays results.]
U: When is American Idol on?
S: I found three entries. [Displays results.]
U: Record the first one.
S: I have added American Idol to the list of programs to be
recorded. [Displays recording settings.]
U: Please record Nova on Wednesday night.
S: I have added Nova to the list of programs to be recorded.
[Displays recording settings.]
U: Show all my scheduled recordings.
S: There are seven scheduled recordings. [Shows list.]
U: Delete BBC World News.
S: I have deleted the scheduled recording: BBC World
News. [Updates List.]
U: Show my videos.
S: [Displays list of videos.]
U: Delete Frasier.
S: I have deleted the recording Frasier. [Updates list.]
U: Play Grey?s Anatomy.
S: Playing Grey?s Anatomy. [Plays recording.]
U: Stop Playing.
S: Stopping. [Stops playback.]
U: [Clicks on Music tab.]
S: [Displays artist list.]
U: Show albums by Billie Holiday.
S: I found three albums by Billie Holiday. [Shows albums.]
U: Please play A Hard Rain?s A-Gonna Fall by Bob Dylan.
S: Playing A Hard Rain?s A-Gonna Fall. [Plays song.]
Figure 2: Screenshots and an example interaction. Utterances are labeled with U for user and S for system.
3
3 User Experience
Our current prototype system implements the basic
functionalities that one expects from a home enter-
tainment center. Users can navigate through and
record programs from the television?s electronic pro-
gram guide, manage recording settings, and play
recorded videos. They can also browse and listen to
selections from their music libraries. However, un-
like existing prototypes, ours employs a smartphone
with a navigation pad, touch-sensitive screen, and
built-in microphone as the remote control. Figure 1
provides an overview of the graphical user interface
on both the TV and mobile device.
Mirroring the TV?s on-screen display, the proto-
type system presents a reduced view on the mobile
device with synchronized cursors. Users can navi-
gate the hierarchical menu structure using the arrow
keys or directly click on the target item with the sty-
lus. While away from the living room, or when a
recording is playing full screen, users can browse
and manage their media libraries using only the mo-
bile device.
While the navigation pad and stylus are great for
basic navigation and control, searching for media
with specific attributes, such as title, remains cum-
bersome. To facilitate such interactions, the cur-
rent system supports spoken natural language inter-
actions. For example, the user can press the hold-to-
talk button located on the side of the mobile device
and ask ?What?s on the National Geographic Chan-
nel this afternoon?? to retrieve a list of shows with
the specified channel and time. The system responds
with a short verbal summary ?I found six entries on
January seventh? and presents the resulting list on
both the TV and mobile displays. The user can then
browse the list using the navigation pad or press the
hold-to-talk button to barge in with another com-
mand, e.g. ?Please record the second one.? Depress-
ing the hold-to-talk button not only terminates any
current spoken response, but also mutes the TV to
minimize interference with speech recognition. As
the previous example demonstrates, contextual in-
formation is used to resolve list position references
and disambiguate commands.
The speech interface to the user?s music library
works in a similar fashion. Users can search by
artist, album, and song name, and then play the
songs found. To demonstrate the extensibility of
the architecture, we have also integrated an exist-
ing weather information system (Zue et al, 2000),
which has been previously deployed as a telephony
application. Users simply click on the Weather tab
to switch to this domain, allowing them to ask a wide
range of weather queries. The system responds ver-
bally and with a simple graphical forecast.
To create a natural user experience, we designed
the multimodal interface to allow users to seam-
lessly switch among the different input modalities
available on the mobile device. Figure 2 demon-
strates an example interaction with the prototype, as
well as several screenshots of the user interface.
4 System Architecture
The system architecture is quite flexible with re-
gards to the placement of the natural language pro-
cessing components. Figure 3 presents two possible
configurations of the system components distributed
across the mobile device, home media server, and
TV display. In 3(a), all speech recognition and nat-
ural language processing components reside on the
server, with the mobile device acting as the micro-
phone, speaker, display, and remote control. In 3(b),
the speech recognizer, language understanding com-
ponent, language generation component, and text-
to-speech (TTS) synthesizer run on the mobile de-
vice. Depending on the capabilities of the mobile
device and network connection, different configu-
rations may be optimal. For instance, on a power-
ful device with slow network connection, recogni-
tion latency may be reduced by performing speech
recognition and natural language processing on the
device. On the other hand, streaming audio via a fast
wireless network to the server for processing may
result in improved accuracy.
In the prototype system, flexible and reusable
speech recognition and natural language processing
capabilities are provided via generic components de-
veloped and deployed in numerous spoken dialogue
systems by our group, with the exception of an off-
the-shelf speech synthesizer. Speech input from the
mobile device is recognized using the landmark-
based SUMMIT system (Glass, 2003). The result-
ing N-best hypotheses are processed by the TINA
language understanding component (Seneff, 1992).
4
Gala
xy
Spee
ch?R
ecog
nize
r
Lang
uage
?Und
ersta
ndin
g
Dialo
gue?
Man
ager
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Web
?
Serv
er
Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Gala
xy
Dialo
gue?
Man
ager
Web
?
Serv
er Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Spee
ch?R
ecog
nize
r
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Lang
uage
?Und
ersta
ndin
g
(a) (b)
Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server,
while in (b) processing is primarily performed on the device.
Based on the resulting meaning representation, the
dialogue manager (Polifroni et al, 2003) incorpo-
rates contextual information (Filisko and Seneff,
2003), and then determines an appropriate response.
The response consists of an update to the graph-
ical display, and a spoken system response which
is realized via the GENESIS (Baptist and Seneff,
2000) language generation module. To support on-
device processing, all the components are linked via
the GALAXY framework (Seneff et al, 1998) with
an additional Mobile Manager component responsi-
ble for coordinating the communication between the
mobile device and the home media server.
In the currently deployed system, we use a mo-
bile phone with a 624 MHz ARM processor run-
ning the Windows Mobile operating system and
Opera Mobile web browser. The TV program and
music databases reside on the home media server
running GNU/Linux. The TV program guide data
and recording capabilities are provided via MythTV,
a full-featured, open-source digital video recorder
software package.3 Daily updates to the program
guide information typically contain hundreds of
unique channel names and thousands of unique pro-
gram names. The music library is comprised of
5,000 songs from over 80 artists and 13 major gen-
res, indexed using the open-source text search en-
gine Lucene.4 Lastly, the TV display can be driven
by a web browser on either the home media server or
a separate computer connected to the server via a fast
Ethernet connection, for high quality video stream-
ing.
3http://www.mythtv.org/
4http://lucene.apache.org/
While the focus of this paper is on the natural lan-
guage processing and user interface aspects of the
system, our work is actually situated within a larger
collaborative project at MIT that also includes sim-
plified device configuration (Mazzola Paluska et al,
2008; Mazzola Paluska et al, 2006), transparent ac-
cess to remote servers (Ford et al, 2006), and im-
proved security.
5 Mobile Natural Language Components
Porting the implementation of the various speech
recognizer and natural language processing com-
ponents to mobile devices with limited computa-
tion and memory presents both a research and en-
gineering challenge. Instead of creating a small vo-
cabulary, fixed phrase dialogue system, we aim to
support?on the mobile device?the same flexible
and natural language interactions currently available
on our desktop, tablet, and telephony systems; see
e.g., (Gruenstein et al, 2006; Seneff, 2002; Zue et
al., 2000). In this section, we summarize our ef-
forts thus far in implementing the SUMMIT speech
recognizer and TINA natural language parser. Ports
of the GENESIS language generation system and of
our dialogue manager are well underway, and we ex-
pect to have these components working on the mo-
bile device in the near future.
5.1 PocketSUMMIT
To significantly reduce the memory footprint and
overall computation, we chose to reimplement our
segment-based speech recognizer from scratch, uti-
lizing fixed-point arithmetic, parameter quantiza-
tion, and bit-packing in the binary model files.
The resulting PocketSUMMIT recognizer (Hether-
5
ington, 2007) utilizes only the landmark features,
initially forgoing segment features such as phonetic
duration, as they introduce algorithmic complexities
for relatively small word error rate (WER) improve-
ments.
In the current system, we quantize the mean and
variance of each Gaussian mixture model dimension
to 5 and 3 bits, respectively. Such quantization not
only results in an 8-fold reduction in model size, but
also yields about a 50% speedup by enabling table
lookups for Gaussian evaluations. Likewise, in the
finite-state transducers (FSTs) used to represent the
language model, lexical, phonological, and class di-
phone constraints, quantizing the FST weights and
bit-packing not only compress the resulting binary
model files, but also reduce the processing time with
improved processor cache locality.
In the aforementioned TV, music, and weather do-
mains with a moderate vocabulary of a few thou-
sand words, the resulting PocketSUMMIT recog-
nizer performs in approximately real-time on 400-
600 MHz ARM processors, using a total of 2-4
MB of memory, including 1-2 MB for memory-
mapped model files. Compared with equivalent non-
quantized models, PocketSUMMIT achieves dra-
matic improvements in speed and memory while
maintaining comparable WER performance.
5.2 PocketTINA
Porting the TINA natural language parser to mobile
devices involved significant software engineering to
reduce the memory and computational requirements
of the core data structures and algorithms. TINA
utilizes a best-first search that explores thousands of
partial parses when processing an input utterance.
To efficiently manage memory allocation given the
unpredictability of pruning invalid parses (e.g. due
to subject-verb agreement), we implemented a mark
and sweep garbage collection mechanism. Com-
bined with a more efficient implementation of the
priority queue and the use of aggressive ?beam?
pruning, the resulting PocketTINA system provides
identical output as server-side TINA, but can parse
a 10-best recognition hypothesis list into the corre-
sponding meaning representation in under 0.1 sec-
onds, using about 2 MB of memory.
6 Rapid Dialogue System Development
Over the course of developing dialogue systems for
many domains, we have built generic natural lan-
guage understanding components that enable the
rapid development of flexible and natural spoken di-
alogue systems for novel domains. Creating such
prototype systems typically involves customizing
the following to the target domain: recognizer lan-
guage model, language understanding parser gram-
mar, context resolution rules, dialogue management
control script, and language generation rules.
Recognizer Language Model Given a new do-
main, we first identify a set of semantic classes
which correspond to the back-end application?s
database, such as artist, album, and genre. Ideally,
we would have a corpus of tagged utterances col-
lected from real users. However, when building pro-
totypes such as the one described here, little or no
training data is usually available. Thus, we create
a domain-specific context-free grammar to generate
a supplemental corpus of synthetic utterances. The
corpus is used to train probabilities for the natural
language parsing grammar (described immediately
below), which in turn is used to derive a class n-
gram language model (Seneff et al, 2003).
Classes in the language model which corre-
spond to contents of the database are marked as
dynamic, and are populated at runtime from the
database (Chung et al, 2004; Hetherington, 2005).
Database entries are heuristically normalized into
spoken forms. Pronunciations not in our 150,000
word lexicon are automatically generated (Seneff,
2007).
Parser Grammar The TINA parser uses a prob-
abilistic context-free grammar enhanced with sup-
port for wh-movement and grammatical agreement
constraints. We have developed a generic syntac-
tic grammar by examining hundreds of thousands
of utterances collected from real user interactions
with various existing dialogue systems. In addition,
we have developed libraries which parse and inter-
pret common semantic classes like dates, times, and
numbers. The grammar and semantic libraries pro-
vide good coverage for spoken dialogue systems in
database-query domains.
6
To build a grammar for a new domain, a devel-
oper extends the generic syntactic grammar by aug-
menting it with domain-specific semantic categories
and their lexical entries. A probability model which
conditions each node category on its left sibling and
parent is then estimated from a training corpus of
utterances (Seneff et al, 2003).
At runtime, the recognizer tags the hypothesized
dynamic class expansions with their class names,
allowing the parser grammar to be independent of
the database contents. Furthermore, each semantic
class is designated either as a semantic entity, or as
an attribute associated with a particular entity. This
enables the generation of a semantic representation
from the parse tree.
Dialogue Management & Language Generation
Once an utterance is recognized and parsed, the
meaning representation is passed to the context res-
olution and dialogue manager component. The con-
text resolution module (Filisko and Seneff, 2003)
applies generic and domain-specific rules to re-
solve anaphora and deixis, and to interpret frag-
ments and ellipsis in context. The dialogue man-
ager then interacts with the application back-end
and database, controlled by a script customized for
the domain (Polifroni et al, 2003). Finally, the
GENESIS module (Baptist and Seneff, 2000) ap-
plies domain-specific rules to generate a natural lan-
guage representation of the dialogue manager?s re-
sponse, which is sent to a speech synthesizer. The
dialogue manager also sends an update to the GUI,
so that, for example, the appropriate database search
results are displayed.
7 Mobile Design Challenges
Dialogue systems for mobile devices present a
unique set of design challenges not found in tele-
phony and desktop applications. Here we describe
some of the design choices made while developing
this prototype, and discuss their tradeoffs.
7.1 Client/Server Tradeoffs
Towards supporting network-less scenarios, we have
begun porting various natural language processing
components to mobile platforms, as discussed in
Section 5. Having efficient mobile implementations
further allows the natural language processing tasks
to be performed on either the mobile device or the
server. While building the prototype, we observed
that the Wi-Fi network performance can often be un-
predictable, resulting in erratic recognition latency
that occasionally exceeds on-device recognition la-
tency. However, utilizing the mobile processor for
computationally intensive tasks rapidly drains the
battery. Currently, the component architecture in the
prototype system is pre-configured. A more robust
implementation would dynamically adjust the con-
figuration to optimize the tradeoffs among network
use, CPU utilization, power consumption, and user-
perceived latency/accuracy.
7.2 Speech User Interface
As neither open-mic nor push-to-talk with automatic
endpoint detection is practical on mobile devices
with limited battery life, our prototype system em-
ploys a hold-to-talk hardware button for microphone
control. To guide users to speak commands only
while the button is depressed, a short beep is played
as an earcon both when the button is pushed and
released. Since users are less likely to talk over
short audio clips, the use of earcons mitigates the
tendency for users to start speaking before pushing
down the microphone button.
In the current system, media audio is played over
the TV speakers, whereas TTS output is sent to
the mobile device speakers. To reduce background
noise captured from the mobile device?s far-field mi-
crophone, the TV is muted while the microphone
button is depressed. Unlike telephony spoken di-
alogue systems where the recognizer has to con-
stantly monitor for barge-in, the use of a hold-to-
talk button significantly simplifies barge-in support,
while reducing power consumption.
7.3 Graphical User Interface
In addition to supporting interactive natural lan-
guage dialogues via the spoken user interface, the
prototype system implements a graphical user in-
terface (GUI) on the mobile device to supplement
the TV?s on-screen interface. To faciliate rapid pro-
totyping, we chose to implement both the mobile
and TV GUI using web pages with AJAX (Asyn-
chronous Javascript and XML) techniques, an ap-
proach we have leveraged in several existing mul-
timodal dialogue systems, e.g. (Gruenstein et al,
7
2006; McGraw and Seneff, 2007). The resulting in-
terface is largely platform-independent and allows
display updates to be ?pushed? to the client browser.
As many users are already familiar with the TV?s
on-screen interface, we chose to mirror the same in-
terface on the mobile device and synchronize the
selection cursor. However, unlike desktop GUIs,
mobile devices are constrained by a small display,
limited computational power, and reduced network
bandwidth. Thus, both the page layout and infor-
mation detail were adjusted for the mobile browser.
Although AJAX is more responsive than traditional
web technology, rendering large formatted pages?
such as the program guide grid?is often still un-
acceptably slow. In the current implementation, we
addressed this problem by displaying only the first
section of the content and providing a ?Show More?
button that downloads and renders the full content.
While browser-based GUIs expedite rapid prototyp-
ing, deployed systems may want to take advantage
of native interfaces specific to the device for more
responsive user interactions. Instead of limiting the
mobile interface to reflect the TV GUI, improved us-
ability may be obtained by designing the interface
for the mobile device first and then expanding the
visual content to the TV display.
7.4 Client/Server Communication
In the current prototype, communication between
the mobile device and the media server consists of
AJAX HTTP and XML-RPC requests. To enable
server-side ?push? updates, the client periodically
pings the server for messages. While such an im-
plementation provides a responsive user interface, it
quickly drains the battery and is not robust to net-
work outages resulting from the device being moved
or switching to power-saving mode. Reestablish-
ing connection with the server further introduces la-
tency. In future implementations, we would like to
examine the use of Bluetooth for lower power con-
sumption, and infrared for immediate response to
common controls and basic navigation.
8 Conclusions & Future Work
We have presented a prototype system that demon-
strates the feasibility of deploying a multimodal,
natural language interface on a mobile device for
browsing and managing one?s home media library.
In developing the prototype, we have experimented
with a novel role for a mobile device?that of a
speech-enabled remote control. We have demon-
strated a flexible natural language understanding ar-
chitecture, in which various processing stages may
be performed on either the server or mobile device,
as networking and processing power considerations
require.
While the mobile platform presents many chal-
lenges, it also provides unique opportunities.
Whereas desktop computers and TV remote controls
tend to be shared by multiple users, a mobile device
is typically used by a single individual. By collect-
ing and adapting to the usage data, the system can
personalize the recognition and understanding mod-
els to improve the system accuracy. In future sys-
tems, we hope to not only explore such adaptation
possibilities, but also study how real users interact
with the system to further improve the user interface.
Acknowledgments
This research is sponsored by the TParty Project,
a joint research program between MIT and Quanta
Computer, Inc.; and by Nokia, as part of a joint MIT-
Nokia collaboration. We are also thankful to three
anonymous reviewers for their constructive feed-
back.
References
A. Acero, N. Bernstein, R. Chambers, Y. C. Jui, X. Li,
J. Odell, P. Nguyen, O. Scholz, and G. Zweig. 2008.
Live search for mobile: Web services by voice on the
cellphone. In Proc. of ICASSP.
L. Baptist and S. Seneff. 2000. Genesis-II: A versatile
system for language generation in conversational sys-
tem applications. In Proc. of ICSLP.
A. Berglund and P. Johansson. 2004. Using speech and
dialogue for interactive TV navigation. Universal Ac-
cess in the Information Society, 3(3-4):224?238.
G. Chung, S. Seneff, C. Wang, and L. Hetherington.
2004. A dynamic vocabulary spoken dialogue inter-
face. In Proc. of INTERSPEECH, pages 327?330.
E. Filisko and S. Seneff. 2003. A context resolution
server for the GALAXY conversational systems. In
Proc. of EUROSPEECH.
B. Ford, J. Strauss, C. Lesniewski-Laas, S. Rhea,
F. Kaashoek, and R. Morris. 2006. Persistent personal
names for globally connected mobile devices. In Pro-
ceedings of the 7th USENIX Symposium on Operating
Systems Design and Implementation (OSDI ?06).
8
K. Fujita, H. Kuwano, T. Tsuzuki, and Y. Ono. 2003.
A new digital TV interface employing speech recog-
nition. IEEE Transactions on Consumer Electronics,
49(3):765?769.
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17:137?152.
A. Gruenstein, S. Seneff, and C. Wang. 2006. Scalable
and portable web-based multimodal dialogue interac-
tion with geographical databases. In Proc. of INTER-
SPEECH.
I. L. Hetherington. 2005. A multi-pass, dynamic-
vocabulary approach to real-time, large-vocabulary
speech recognition. In Proc. of INTERSPEECH.
I. L. Hetherington. 2007. PocketSUMMIT: Small-
footprint continuous speech recognition. In Proc. of
INTERSPEECH, pages 1465?1468.
B. Johanson, A. Fox, and T. Winograd. 2002. The in-
teractive workspaces project: Experiences with ubiq-
uitous computing rooms. IEEE Pervasive Computing,
1(2):67?74.
M. Johnston, L. F. D?Haro, M. Levine, and B. Renger.
2007. A multimodal interface for access to content in
the home. In Proc. of ACL, pages 376?383.
J. Mazzola Paluska, H. Pham, U. Saif, C. Terman, and
S. Ward. 2006. Reducing configuration overhead with
goal-oriented programming. In PerCom Workshops,
pages 596?599. IEEE Computer Society.
J. Mazzola Paluska, H. Pham, U. Saif, G. Chau, C. Ter-
man, and S. Ward. 2008. Structured decomposition of
adapative applications. In Proc. of 6th IEEE Confer-
ence on Pervasive Computing and Communications.
I. McGraw and S. Seneff. 2007. Immersive second lan-
guage acquisition in narrow domains: A prototype IS-
LAND dialogue system. In Proc. of the Speech and
Language Technology in Education Workshop.
J. Nichols and B. A. Myers. 2006. Controlling home and
office appliances with smartphones. IEEE Pervasive
Computing, special issue on SmartPhones, 5(3):60?
67, July-Sept.
H.-J. Oh, C.-H. Lee, M.-G. Jang, and Y. K. Lee. 2007.
An intelligent TV interface based on statistical dia-
logue management. IEEE Transactions on Consumer
Electronics, 53(4).
T. Paek, M. Agrawala, S. Basu, S. Drucker, T. Kristjans-
son, R. Logan, K. Toyama, and A. Wilson. 2004. To-
ward universal mobile interaction for shared displays.
In Proc. of Computer Supported Cooperative Work.
J. Polifroni, G. Chung, and S. Seneff. 2003. Towards
the automatic generation of mixed-initiative dialogue
systems from web content. In Proc. EUROSPEECH,
pages 193?196.
T. Portele, S. Goronzy, M. Emele, A. Kellner, S. Torge,
and J. te Vrugt. 2003. SmartKom-Home - an ad-
vanced multi-modal interface to home entertainment.
In Proc. of INTERSPEECH.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. GALAXY-II: A reference architecture
for conversational system development. In Proc. IC-
SLP.
S. Seneff, C. Wang, and T. J. Hazen. 2003. Automatic in-
duction of n-gram language models from a natural lan-
guage grammar. In Proceedings of EUROSPEECH.
S. Seneff. 1992. TINA: A natural language system
for spoken language applications. Computational Lin-
guistics, 18(1):61?86.
S. Seneff. 2002. Response planning and generation in
the MERCURY flight reservation system. Computer
Speech and Language, 16:283?312.
S. Seneff. 2007. Reversible sound-to-letter/letter-to-
sound modeling based on syllable structure. In Proc.
of HLT-NAACL.
K. Wittenburg, T. Lanning, D. Schwenke, H. Shubin, and
A. Vetro. 2006. The prospects for unrestricted speech
input for TV content search. In Proc. of AVI?06.
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. J.
Hazen, and L. Hetherington. 2000. JUPITER: A
telephone-based conversational interface for weather
information. IEEE Transactions on Speech and Audio
Processing, 8(1), January.
9
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 182?192,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Learning of Phonetic Units and Word Pronunciations for ASR
Chia-ying Lee, Yu Zhang, James Glass
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{chiaying,yzhang87,jrg}@csail.mit.edu
Abstract
The creation of a pronunciation lexicon re-
mains the most inefficient process in develop-
ing an Automatic Speech Recognizer (ASR).
In this paper, we propose an unsupervised
alternative ? requiring no language-specific
knowledge ? to the conventional manual ap-
proach for creating pronunciation dictionar-
ies. We present a hierarchical Bayesian model,
which jointly discovers the phonetic inven-
tory and the Letter-to-Sound (L2S) mapping
rules in a language using only transcribed
data. When tested on a corpus of spontaneous
queries, the results demonstrate the superior-
ity of the proposed joint learning scheme over
its sequential counterpart, in which the la-
tent phonetic inventory and L2S mappings are
learned separately. Furthermore, the recogniz-
ers built with the automatically induced lexi-
con consistently outperform grapheme-based
recognizers and even approach the perfor-
mance of recognition systems trained using
conventional supervised procedures.
1 Introduction
Modern automatic speech recognizers require a few
essential ingredients such as a signal representation
of the speech signal, a search component, and typ-
ically a set of stochastic models that capture 1) the
acoustic realizations of the basic sounds of a lan-
guage, for example, phonemes, 2) the realization of
words in terms of these sounds, and 3) how words
are combined in spoken language. When creating
a speech recognizer for a new language the usual
requirements are: first, a large speech corpus with
word-level annotations; second, a pronunciation dic-
tionary that essentially defines a phonetic inventory
for the language as well as word-level pronuncia-
tions, and third, optional additional text data that
can be used to train the language model. Given
these data and some decision about the signal rep-
resentation, e.g., centi-second Mel-Frequency Cep-
stral Coefficients (MFCCs) (Davis and Mermelstein,
1980) with various derivatives, as well as the nature
of the acoustic and language model such as 3-state
HMMs and n-grams, iterative training methods can
be used to effectively learn the model parameters for
the acoustic and language models. Although the de-
tails of the components have changed through the
years, this basic ASR formulation was well estab-
lished by the late 1980?s, and has not really changed
much since then.
One of the interesting aspects of this formulation
is the inherent dependence on the dictionary, which
defines both the phonetic inventory of a language,
and the pronunciations of all the words in the vo-
cabulary. The dictionary is arguably the cornerstone
of a speech recognizer as it provides the essential
transduction from sounds to words. Unfortunately,
the dependency on this resource is a significant im-
pediment to the creation of speech recognizers for
new languages, since they are typically created by
experts, whereas annotated corpora can be relatively
more easily created by native speakers of a language.
The existence of an expert-derived dictionary in
the midst of stochastic speech recognition models is
somewhat ironic, and it is natural to ask why it con-
tinues to receive special status after all these years.
Why can we not learn the inventory of sounds of a
language and associated word pronunciations auto-
matically, much as we learn our acoustic model pa-
rameters? If successful, we would move one step
forward towards breaking the language barrier that
182
limits us from having speech recognizers for all lan-
guages of the world, instead of the less than 2% that
currently exist.
In this paper, we investigate the problem of infer-
ring a pronunciation lexicon from an annotated cor-
pus without exploiting any language-specific knowl-
edge. We formulate our approach as a hierarchi-
cal Bayesian model, which jointly discovers the
acoustic inventory and the latent encoding scheme
between the letters and the sounds of a language.
We evaluate the quality of the induced lexicon and
acoustic model through a series of speech recogni-
tion experiments on a conversational weather query
corpus (Zue et al, 2000). The results demonstrate
that our model consistently generates close perfor-
mance to recognizers that are trained with expert-
defined phonetic inventory and lexicon. Compared
to grapheme-based recognizers, our model is capa-
ble of improving the Word Error Rates (WERs) by
at least 15.3%. Finally, the joint learning framework
proposed in this paper is proven to be much more
effective than modeling the acoustic units and the
letter-to-sound mappings separately, as shown in a
45% WER deduction our model achieves compared
to a sequential approach.
2 Related Work
Various algorithms for learning sub-word based pro-
nunciations were proposed in (Lee et al, 1988;
Fukada et al, 1996; Bacchiani and Ostendorf, 1999;
Paliwal, 1990). In these previous approaches, spo-
ken samples of a word are gathered, and usually
only one single pronunciation for the word is de-
rived based on the acoustic evidence observed in the
spoken samples. The major difference between our
work and these previous works is that our model
learns word pronunciations in the context of letter
sequences. More specifically, our model learns letter
pronunciations first and then concatenates the pro-
nunciation of each letter in a word to form the word
pronunciation. The advantage of our approach is
that pronunciation knowledge learned for a particu-
lar letter in some arbitrary word can subsequently be
used to help learn the letter?s pronunciation in other
words. This property allows our model to potentially
learn better pronunciations for less frequent words.
The more recent work by Garcia and Gish (2006)
and Siu et al (2013) has made extensive use
of self-organizing units for keyword spotting and
other tasks for languages with limited linguistic
resources. Others who have more recently ex-
plored the unsupervised space include (Varadarajan
et al, 2008; Jansen and Church, 2011; Lee and
Glass, 2012). The latter work introduced a non-
parametric Bayesian inference procedure for auto-
matically learning acoustic units that is most similar
to our current work except that our model also infers
word pronunciations simultaneously.
The concept of creating a speech recognizer for
a language with only orthographically annotated
speech data has also been explored previously by
means of graphemes. This approach has been shown
to be effective for alphabetic languages with rela-
tively straightforward grapheme to phoneme trans-
formations and does not require any unsupervised
learning of units or pronunciations (Killer et al,
2003; Stu?ker and Schultz, 2004). As we explain in
later sections, grapheme-based systems can actually
be regarded as a special case of our model; therefore,
we expect our model to have greater flexibilities for
capturing pronunciation rules of graphemes.
3 Model
The goal of our model is to induce a word pronunci-
ation lexicon from spoken utterances and their cor-
responding word transcriptions. No other language-
specific knowledge is assumed to be available, in-
cluding the phonetic inventory of the language. To
achieve the goal, our model needs to solve the fol-
lowing two tasks:
? Discover the phonetic inventory.
? Reveal the latent mapping between the letters
and the discovered phonetic units.
We propose a hierarchical Bayesian model for
jointly discovering the two latent structures from
an annotated speech corpus. Before presenting our
model, we first describe the key latent and observed
variables of the problem.
Letter (lmi ) We use l
m
i to denote the i
th let-
ter observed in the word transcription of the
mth training sample. To be sure, a train-
ing sample involves a speech utterance and its
183
corresponding text transcription. The letter se-
quence composed of lmi and its context, namely
lmi??, ? ? ? , l
m
i?1, l
m
i , l
m
i+1, ? ? ? , l
m
i+?, is denoted as ~l
m
i,?.
Although lmi is referred to as a letter in this paper,
it can represent any character observed in the text
data, including space and symbols indicating sen-
tence boundaries. The set of unique characters ob-
served in the data set is denoted as G. For notation
simplicity, we use L? to denote the set of letter se-
quences of length 2? + 1 that appear in the dataset
and use ~l? to denote the elements in L?. Finally,
P(~l?) is used to represent the parent of ~l?, which is
a substring of ~l? with the first and the last characters
truncated.
Number of Mapped Acoustic Units (nmi ) Each
letter lmi in the transcriptions is assumed to be
mapped to a certain number of phonetic units. For
example, the letter x in the word fox is mapped to
2 phonetic units /k/ and /s/, while the letter e in the
word lake is mapped to 0 phonetic units. We denote
this number as nmi and limit its value to be 0, 1 or 2
in our model. The value of nmi is always unobserved
and needs to be inferred by the our model.
Identity of the Acoustic Unit (cmi,p) For each pho-
netic unit that lmi maps to, we use c
m
i,p, for 1 ? p ?
nmi , to denote the identity of the phonetic unit. Note
that the phonetic inventory that describes the data
set is unknown to our model, and the identities of
the phonetic units are associated with the acoustic
units discovered automatically by our model.
Speech Feature xmt The observed speech data in
our problem are converted to a series of 25 ms 13-
dimensional MFCCs (Davis and Mermelstein, 1980)
and their first- and second-order time derivatives at
a 10 ms analysis rate. We use xmt ? R
39 to denote
the tth feature frame of the mth utterance.
3.1 Generative Process
We present the generative process for a single train-
ing sample (i.e., a speech utterance and its corre-
sponding text transcription); to keep notation sim-
ple, we discard the index variable m in this section.
For each li in the transcription, the model gener-
ates ni, given ~li,?, from the 3-dimensional categori-
cal distribution ?~li,?(ni). Note that for every unique
~li,? letter sequence, there is an associated ?~li,?(ni)
lj 
  1?  p ? ni 
?0 
ci, p 
?0 
K
?c 
di,p  
? 
1 ? i ? Lm 
ni 
xt 
1 ? m ? M 
?l2,n,p 
? ? 
?l,n,p 
G ?{(n,p) | 0 ? n ? 2, 1 ? p ? n} 
?l1,n,p 
G ?G 
G ?G 
?1 
?2 
i-2 ? j ? i+2 
Figure 1: The graphical representation of the pro-
posed hierarchical Bayesian model. The shaded cir-
cle denotes the observed text and speech data, and
the squares denote the hyperparameters of the priors
in our model. See Sec. 3 for a detailed explanation
of the generative process of our model.
distribution, which captures the fact that the number
of phonetic units a letter maps to may depend on its
context. In our model, we impose a Dirichlet distri-
bution prior Dir(?) on ?~li,?(ni).
If ni = 0, li is not mapped to any acoustic units
and the generative process stops for li; otherwise,
for 1 ? p ? ni, the model generates ci,p from:
ci,p ? pi~li,?,ni,p (1)
where pi~li,?,ni,p is a K-dimensional categorical dis-
tribution, whose outcomes correspond to the pho-
netic units discovered by the model from the given
speech data. Eq. 1 shows that for each combination
of~li,?, ni and p, there is an unique categorical distri-
bution. An important property of these categorical
distributions is that they are coupled together such
that their outcomes point to a consistent set of pho-
netic units. In order to enforce the coupling, we con-
struct pi~li,?,ni,p through a hierarchical process.
? ? Dir(?) (2)
pi~li,?,ni,p ? Dir(???) for ? = 0 (3)
pi~li,?,ni,p ? Dir(??pi~li,??1,ni,p) for ? ? 1 (4)
184
To interpret Eq. 2 to Eq. 4, we envision that
the observed speech data are generated by a K-
component mixture model, of which the components
correspond to the phonetic units in the language. As
a result, ? in Eq. 2 can be viewed as the mixture
weight over the components, which indicates how
likely we are to observe each acoustic unit in the
data overall. By adopting this point of view, we
can also regard the mapping between li and the pho-
netic units as a mixture model, and pili,ni,p
1 repre-
sents how probable li is mapped to each phonetic
unit given ni and p. We apply a Dirichlet distribu-
tion prior parametrized by ?0? to pili,ni,p as shown
in Eq. 3. With this parameterization, the mean of
pili,ni,p is the global mixture weight ?, and ?0 con-
trols how similar pili,ni,p is to the mean. More specif-
ically, for large ?0  K, the Dirichlet distribution
is highly peaked around the mean; on the contrary,
for ?0  K, the mean lies in a valley. The parame-
ters of a Dirichlet distribution can also be viewed as
pseudo-counts for each category. Eq. 4 shows that
the prior for pi~li,?,ni,p is seeded by pseudo-counts
that are proportional to the mapping weights over
the phonetic units of li in a shorter context. In other
words, the mapping distribution of li in a shorter
context can be thought of as a back-off distribution
of li?s mapping weights in a longer context.
Each component of the K-dimensional mixture
model is linked to a 3-state Hidden Markov Model
(HMM). These K HMMs are used to model the
phonetic units in the language (Jelinek, 1976). The
emission probability of each HMM state is modeled
by a diagonal Gaussian Mixture Model (GMM). We
use ?c to represent the set of parameters that define
the cth HMM, which includes the state transition
probability and the GMM parameters of each state
emission distribution. The conjugate prior of ?c is
denoted as H(?0)2.
Finally, to finish the generative process, for each
ci,p we use the corresponding HMM ?ci,p to gen-
erate the observed speech data xt, and the genera-
tive process of the HMM determines the duration,
1An abbreviation of pi~li,0,ni,p
2H(?0) includes a Dirichlet prior for the transition probabil-
ity of each state, and a Dirichlet prior for each mixture weight
of the three GMMs, and a normal-Gamma distribution for the
mean and precision of each Gaussian mixture in the 3-state
HMM.
di,p, of the speech segment. The complete genera-
tive model, with ? set to 2, is depicted in Fig. 1; M
is the total number of transcribed utterances in the
corpus, and Lm is the number of letters in utterance
m. The shaded circles denote the observed data, and
the squares denote the hyperparameters of the priors
used in our model. Lastly, the unshaded circles de-
note the latent variables of our model, for which we
derive inference algorithms in the next section.
4 Inference
We employ Gibbs sampling (Gelman et al, 2004) to
approximate the posterior distribution of the latent
variables in our model. In the following sections, we
first present a message-passing algorithm for block-
sampling ni and ci,p, and then describe how we
leverage acoustic cues to accelerate the computa-
tion of the message-passing algorithm. Note that the
block-sampling algorithm for ni and ci,p can be par-
allelized across utterances. Finally, we briefly dis-
cuss the inference procedures for ?~l? , pi~l?,n,p, ?, ?c.
4.1 Block-sampling ni and ci,p
To understand the message-passing algorithm in this
study, it is helpful to think of our model as a sim-
plified Hidden Semi-Markov Model (HSMM), in
which the letters represent the states and the speech
features are the observations. However, unlike in
a regular HSMM, where the state sequence is hid-
den, in our case, the state sequence is fixed to be the
given letter sequence. With this point of view, we
can modify the message-passing algorithms of Mur-
phy (2002) and Johnson and Willsky (2013) to com-
pute the posterior information required for block-
sampling ni and ci,p.
Let L(xt) be a function that returns the index
of the letter from which xt is generated; also, let
Ft = 1 be a tag indicating that a new phone segment
starts at t+ 1. Given the constraint that 0 ? ni ? 2,
for 0 ? i ? Lm and 0 ? t ? Tm, the backwards
messages Bt(i) and B?t (i) for the m
th training sam-
ple can be defined and computed as in Eq. 5 and
Eq. 7. Note that for clarity we discard the index vari-
able m in the derivation of the algorithm.
185
Bt(i) , p(xt+1:T |L(xt) = i, Ft = 1)
=
min{L,i+1+U}?
j=i+1
B?t (j)
j?1?
k=i+1
p(nk = 0|~li,?)
=
min{L,i+1+U}?
j=i+1
B?t (j)
j?1?
k=i+1
?~li,?(0) (5)
B?t (i) , p(xt+1:T |L(xt+1) = i, Ft = 1)
=
T?t?
d=1
p(xt+1:t+d|~li,?)Bt+d(i) (6)
=
T?t?
d=1
{
K?
ci,1=1
?~li,?(1)pi~li,?,1,1(ci,1)p(xt+1:t+d|?ci,1)
+
d?1?
v=1
K?
ci,1
K?
ci,2
?~li,?(2)pi~li,?,2,1(ci,1)pi~li,?,2,2(ci,2)
? p(xt+1:t+v|?ci,1)p(xt+v+1:t+d|?ci,2)}Bt+d(i)
(7)
We use xt1:t2 to denote the segment consisting of
xt1 , ? ? ? , xt2 . Our inference algorithm only allows
up to U letters to emit 0 acoustic units in a row. The
value of U is set to 2 for our experiments. Bt(i)
represents the total probability of all possible align-
ments between xt+1:T and li+1:L. B?t (i) contains
the probability of all the alignments between xt+1:T
and li+1:L that map xt+1 to li particularly. This
alignment constraint between xt+1 and li is explic-
itly shown in the first term of Eq. 6, which represents
how likely the speech segment xt+1:t+d is generated
by li given li?s context. This likelihood is simply
the marginal probability of p(xt+1:t+d, ni, ci,p|~li,?)
with ni and ci,p integrated out, which can be ex-
panded and computed as shown in the last three rows
of Eq. 7. The index v specifies where the phone
boundary is between the two acoustic units that li
is aligned with when ni = 2. Eq. 8 to Eq. 10 are
the boundary conditions of the message passing al-
gorithm. B0(0) carries the total probably of all pos-
sible alignments between l1:L and x1:T . Eq. 9 spec-
ifies that at most U letters at the end of an sentence
can be left unaligned with any speech features, while
Eq. 10 indicates that all of the speech features in an
utterance must be assigned to a letter.
Algorithm 1 Block-sample ni and ci,p from Bt(i)
and B?t (i)
1: i? 0
2: t? 0
3: while i < L ? t < T do
4: nexti ? SampleFromBt(i)
5: if nexti > i+ 1 then
6: for k = i+ 1 to k = nexti ? 1 do
7: nk ? 0
8: end for
9: end if
10: d, ni, ?ci,p?, v ? SampleFromB?t (nexti)
11: t? t+ d
12: i? nexti
13: end while
B0(0) =
min{L,U+1}?
j=1
B?0(j)
j?1?
k=1
?~li,?(0) (8)
BT (i) ,
?
??
??
1 if i = L
?L
j=i+1 ?~li,?(0) if L? U ? i < L
0 if i < L? U
(9)
Bt(L) ,
{
1 if t = T
0 otherwise
(10)
Given Bt(i) and B?t (i), ni and ci,p for each letter
in the utterance can be sampled using Alg. 1. The
SampleFromBt(i) function in line 4 returns a ran-
dom sample from the relative probability distribu-
tion composed by entries of the summation in Eq. 5.
Line 5 to line 9 check whether li (and maybe li+1)
is mapped to zero phonetic units. nexti points to
the letter that needs to be aligned with 1 or 2 phone
segments starting from xt. The number of phonetic
units that lnexti maps to and the identities of the
units are sampled in SampleFromB?t (i). This sub-
routine generates a tuple of d, ni, ?ci,p? as well as
v (if ni = 2) from all the entries of the summation
shown in Eq. 73.
3We use ?ci,p? to denote that ?ci,p?may consist of two num-
bers, ci,1 and ci,2, when ni = 2.
186
4.2 Heuristic Phone Boundary Elimination
The variables d and v in Eq. 7 enumerate through
every frame index in a sentence, treating each fea-
ture frame as a potential boundary between acous-
tic units. However, it is possible to exploit acoustic
cues to avoid checking feature frames that are un-
likely to be phonetic boundaries. We follow the pre-
segmentation method described in Glass (2003) to
skip roughly 80% of the feature frames and greatly
speed up the computation of B?t (i).
Another heuristic applied to our algorithm to re-
duce the search space for d and v is based on the
observation that the average duration of phonetic
units is usually no longer than 300 ms. Therefore,
when computing B?t (i), we only consider speech
segments that are shorter than 300 ms to avoid align-
ing letters to speech segments that are too long to be
phonetic units.
4.3 Sampling ?~l? , pi~l?,ni,p, ? and ?c
Sampling ?~l? To compute the posterior distribu-
tion of ?~l? , we count how many times
~l? is mapped
to 0, 1 and 2 phonetic units from nmi . More specifi-
cally, we define N~l?(j) for 0 ? j ? 2 as follows:
N~l?(j) =
M?
m=1
Lm?
i=1
?(nmi , j)?(~l
m
i,?,~l?)
where we use ?(?) to denote the discrete Kronecker
delta. With N~l? , we can simply sample a new value
for ?~l? from the following distribution:
?~l? ? Dir(? +N~l?)
Sampling pi~l?,n,p and ? The posterior distribu-
tions of pi~l?,n,p and ? are constructed recursively due
to the hierarchical structure imposed on pi~l?,n,p and
?. We start with gathering counts for updating the
pi variables at the lowest level, i.e., pi~l2,n,p given that
? is set to 2 in our model implementation, and then
sample pseudo-counts for the pi variables at higher
hierarchies as well as ?. With the pseudo-counts, a
new ? can be generated, which allows pi~l?,n,p to be
re-sampled sequentially.
More specifically, we define C~l2,n,p(k) to be the
number of times that ~l2 is mapped to n units and
the unit in position p is the kth phonetic unit. This
value can be counted from the current values of cmi,p
as follows.
C~l2,n,p(k) =
M?
m=1
Lm?
i=1
?(~li,2,~l2)?(n
m
i , n)?(c
m
i,p, k)
To derive the posterior distribution of pi~l1,n,p an-
alytically, we need to sample pseudo-counts C~l1,n,p,
which is defined as follows.
C~l1,n,p(k) =
?
~l2?U~l1
C~l2,n,p
(k)
?
i=1
I[?i <
?2pi~l1,n,p(k)
i+ ?2pi~l1,n,p(k)
]
(11)
We use U~l1 = {
~l2|P(~l2) = ~l1} to denote the set of
~l2 whose parent is~l1 and ?i to represent random vari-
ables sampled from a uniform distribution between
0 and 1. Eq. 11 can be applied recursively to com-
pute C~l0,n,p(k) and C ,n,p(k), the pseudo-counts that
are applied to the conjugate priors of pi~l0,n,p and ?.
With the pseudo-count variables computed, new val-
ues for ? and pi~l?,n,p can be sampled sequentially as
shown in Eq. 12 to Eq. 14.
? ? Dir(? + C ,n,p) (12)
pi~l?,n,p ? Dir(??? + C~l?,n,p) for ? = 0 (13)
pi~l?,n,p ? Dir(??pi~l??1,n,p + C~l?,n,p) for ? ? 1
(14)
5 Experimental Setup
To test the effectiveness of our model for joint learn-
ing phonetic units and word pronunciations from an
annotated speech corpus, we construct speech rec-
ognizers out of the training results of our model.
The performance of the recognizers is evaluated and
compared against three baselines: first, a grapheme-
based speech recognizer; second, a recognizer built
by using an expert-crafted lexicon, which is referred
to as an expert lexicon in the rest of the paper for
simplicity; and third, a recognizer built by discover-
ing the phonetic units and L2S pronunciation rules
sequentially without using a lexicon. In this section,
we provide a detailed description of the experimen-
tal setup.
187
? ? ?0 ?1 ?2 ?0 ? K
?0.1?3 ?10?100 1 0.1 0.2 * 2 100
Table 1: The values of the hyperparameters of our
model. We use ?a?D to denote aD-dimensional vec-
tor with all entries being a. *We follow the proce-
dure reported in (Lee and Glass, 2012) to set up the
HMM prior ?0.
5.1 Dataset
All the speech recognition experiments reported in
this paper are performed on a weather query dataset,
which consists of narrow-band, conversational tele-
phone speech (Zue et al, 2000). We follow the ex-
perimental setup of McGraw et al (2013) and split
the corpus into a training set of 87,351 utterances, a
dev set of 1,179 utterances and a test set of 3,497 ut-
terances. A subset of 10,000 utterances is randomly
selected from the training set. We use this subset of
data for training our model to demonstrate that our
model is able to discover the phonetic composition
and the pronunciation rules of a language even from
just a few hours of data.
5.2 Building a Recognizer from Our Model
The values of the hyperparameters of our model are
listed in Table 1. We run the inference procedure de-
scribed in Sec. 4 for 10,000 times on the randomly
selected 10,000 utterances. The samples of ?~l? and
pi~l?n,p from the last iteration are used to decode n
m
i
and cmi,p for each sentence in the entire training set by
following the block-sampling algorithm described
in Sec. 4.1. Since cmi,p is the phonetic mapping of
lmi , by concatenating the phonetic mapping of ev-
ery letter in a word, we can obtain a pronunciation
of the word represented in the labels of discovered
phonetic units. For example, assume that word w
appears in sentence m and consists of l3l4l5 (the
sentence index m is ignored for simplicity). Also,
assume that after decoding, n3 = 1, n4 = 2 and
n5 = 1. A pronunciation ofw is then encoded by the
sequence of phonetic labels c3,1c4,1c4,2c5,1. By re-
peating this process for each word in every sentence
for the training set, a list of word pronunciations can
be compiled and used as a stochastic lexicon to build
a speech recognizer.
In theory, the HMMs inferred by our model can be
directly used as the acoustic model of a monophone
speech recognizer. However, if we regard the ci,p
labels of each utterance as the phone transcription
of the sentence, then a new acoustic model can be
easily re-trained on the entire data set. More conve-
niently, the phone boundaries corresponding to the
ci,p labels are the by-products of the block-sampling
algorithm, which are indicated by the values of d and
v in line 10 of Alg. 1 and can be easily saved during
the sampling procedure. Since these data are readily
available, we re-build a context-independent model
on the entire data set. In this new acoustic model,
a 3-state HMM is used to model each phonetic unit,
and the emission probability of each state is modeled
by a 32-mixture GMM.
Finally, a trigram language model is built by using
the word transcriptions in the full training set. This
language model is utilized in all speech recogni-
tion experiments reported in this paper. Finite State
Transducers (FSTs) are used to build all the recog-
nizers used in this study. With the language model,
the lexicon and the context-independent acoustic
model constructed by the methods described in this
section, we can build a speech recognizer from
the learning output of the proposed model without
the need of a pre-defined phone inventory and any
expert-crafted lexicons.
5.2.1 Pronunciation Mixture Model Retraining
McGraw et al (2013) presented the Pronuncia-
tion Mixture Model (PMM) for composing stochas-
tic lexicons that outperform pronunciation dictionar-
ies created by experts. Although the PMM frame-
work was designed to incorporate and augment ex-
pert lexicons, we found that it can be adapted to pol-
ish the pronunciation list generated by our model.
In particular, the training procedure for PMMs in-
cludes three steps. First, train a L2S model from
a manually specified expert-pronunciation lexicon;
second, generate a list of pronunciations for each
word in the dataset using the L2S model; and finally,
use an acoustic model to re-weight the pronuncia-
tions based on the acoustic scores of the spoken ex-
amples of each word.
To adapt this procedure for our purposes, we sim-
ply plug in the word pronunciations and the acous-
tic model generated by our model. Once we ob-
tain the re-weighted lexicon, we re-generate forced
188
phone alignments and retrain the acoustic model,
which can be utilized to repeat the PMM lexicon re-
weighting procedure. For our experiments, we it-
erate through this model refining process until the
recognition performance converges.
5.2.2 Triphone Model
Conventionally, to train a context-dependent
acoustic model, a list of questions based on the
linguistic properties of phonetic units is required
for growing decision tree classifiers (Young et al,
1994). However, such language-specific knowledge
is not available for our training framework; there-
fore, our strategy is to compile a question list that
treats each phonetic unit as a unique linguistic class.
In other words, our approach to training a context-
dependent acoustic model for the automatically dis-
covered units is to let the decision trees grow fully
based on acoustic evidence.
5.3 Baselines
We compare the recognizers trained by following
the procedures described in Sec. 5.2 against three
baselines. The first baseline is a grapheme-based
speech recognizer. We follow the procedure de-
scribed in Killer et al (2003) and train a 3-state
HMM for each grapheme, which we refer to as the
monophone grapheme model. Furthermore, we cre-
ate a singleton question set (Killer et al, 2003), in
which each grapheme is listed as a question, to train
a triphone grapheme model. Note that to enforce
better initial alignments between the graphemes and
the speech data, we use a pre-trained acoustic model
to identify the non-speech segments at the beginning
and the end of each utterance before starting training
the monophone grapheme model.
Our model jointly discovers the phonetic inven-
tory and the L2S mapping rules from a set of tran-
scribed data. An alternative of our approach is to
learn the two latent structures sequentially. We fol-
low the training procedure of Lee and Glass (2012)
to learn a set of acoustic models from the speech
data and use these acoustic models to generate a
phone transcription for each utterance. The phone
transcriptions along with the corresponding word
transcriptions are fed as inputs to the L2S model
proposed in Bisani and Ney (2008). A stochastic
lexicon can be learned by applying the L2S model
unit(%) Monophone
Our model 17.0
Oracle 13.8
Grapheme 32.7
Sequential model 31.4
Table 2: Word error rates generated by the four
monophone recognizers described in Sec. 5.2 and
Sec. 5.3 on the weather query corpus.
and the discovered acoustic models to PMM. This
two-stage approach for training a speech recognizer
without an expert lexicon is referred to as the se-
quential model in this paper.
Finally, we compare our system against a rec-
ognizer trained from an oracle recognition system.
We build the oracle recognizer on the same weather
query corpus by following the procedure presented
in McGraw et al (2013). This oracle recognizer is
then applied to generate forced-aligned phone tran-
scriptions for the training utterances, from which
we can build both monophone and triphone acous-
tic models. The expert-crafted lexicon used in the
oracle recognizer is also used in this baseline. Note
that for training the triphone model, we compose a
singleton question list (Killer et al, 2003) that has
every expert-defined phonetic unit as a question. We
use this singleton question list instead of a more so-
phisticated one to ensure that this baseline and our
system differ only in the acoustic model and the lex-
icon used to generate the initial phone transcriptions.
We call this baseline the oracle baseline.
6 Results and Analysis
6.1 Monophone Systems
Table 2 shows the WERs produced by the four
monophone recognizers described in Sec. 5.2 and
Sec. 5.3. It can be seen that our model outper-
forms the grapheme and the sequential model base-
lines significantly while approaching the perfor-
mance of the supervised oracle baseline. The im-
provement over the sequential baseline demonstrates
the strength of the proposed joint learning frame-
work. More specifically, unlike the sequential base-
line, in which the acoustic units are discovered in-
dependently from the text data, our model is able to
exploit the L2S mapping constraints provided by the
word transcriptions to cluster speech segments.
189
By comparing our model to the grapheme base-
line, we can see the advantage of modeling the
pronunciations of a letter using a mixture model,
especially for a language like English which has
many pronunciation irregularities. However, even
for languages with straightforward pronunciation
rules, the concept of modeling letter pronunciations
using mixture models still applies. The main dif-
ference is that the mixture weights for letters of
languages with simple pronunciation rules will be
sparser and spikier. In other words, in theory, our
model should always perform comparable to, if not
better than, grapheme recognizers.
Last but not least, the recognizer trained with the
automatically induced lexicon performs similarly to
the recognizer initialized by an oracle recognition
system, which demonstrates the effectiveness of the
proposed model for discovering the phonetic inven-
tory and a pronunciation lexicon from an annotated
corpus. In the next section, we provide some in-
sights into the quality of the learned lexicon and
into what could have caused the performance gap
between our model and the conventionally trained
recognizer.
6.2 Pronunciation Entropy
The major difference between the recognizer that is
trained by using our model and the recognizer that
is seeded by an oracle recognition system is that
the former uses an automatically discovered lexicon,
while the latter exploits an expert-defined pronun-
ciation dictionary. In order to quantify, as well as
to gain insights into, the difference between these
two lexicons, we define the average pronunciation
entropy, H? , of a lexicon as follows.
H? ?
?1
|V |
?
w?V
?
b?B(w)
p(b) log p(b) (15)
where V denotes the vocabulary of a lexicon, B(w)
represents the set of pronunciations of a word w
and p(b) stands for the weight of a certain pronun-
ciation b. Intuitively, we can regard H? as an in-
dicator of how much pronunciation variation that
each word in a lexicon has on average. Table 3
shows that the H? values of the lexicon induced by
our model and the expert-defined lexicon as well as
Our model PMM iterations
(Discovered lexicon) 0 1 2
H? (bit) 4.58 3.47 3.03
WER (%) 17.0 16.6 15.9
Oracle PMM iterations
(Expert lexicon) 0 1 2
H? (bit) 0.69 0.90 0.92
WER (%) 13.8 12.8 12.4
Table 3: The upper-half of the table shows the aver-
age pronunciation entropies, H? , of the lexicons in-
duced by our model and refined by PMM as well
as the WERs of the monophone recognizers built
with the corresponding lexicons for the weather
query corpus. The definition of H? can be found in
Sec. 6.2. The first row of the lower-half of the ta-
ble lists the average pronunciation entropies, H? , of
the expert-defined lexicon and the lexicons gener-
ated and weighted by the L2P-PMM framework de-
scribed in McGraw et al (2013). The second row of
the lower-half of the table shows the WERs of the
recognizers that are trained with the expert-lexicon
and its PMM-refined versions.
their respective PMM-refined versions4. In Table 3,
we can see that the automatically-discovered lexi-
con and its PMM-reweighted versions have much
higher H? values than their expert-defined counter-
parts. These higher H? values imply that the lexicon
induced by our model contains more pronunciation
variation than the expert-defined lexicon. Therefore,
the lattices constructed during the decoding process
for our recognizer tend to be larger than those con-
structed for the oracle baseline, which explains the
performance gap between the two systems in Table 2
and Table 3.
As shown in Table 3, even though the lexicon
induced by our model is noisier than the expert-
defined dictionary, the PMM retraining framework
consistently refines the induced lexicon and im-
proves the performance of the recognizers5. To the
best of our knowledge, we are the first to apply
PMM to lexicons that are created by a fully unsu-
4We build the PMM-refined version of the expert-defined
lexicon by following the L2P-PMM framework described
in McGraw et al (2013).
5The recognition results all converge in 2 ? 3 PMM retrain-
ing iterations.
190
pronunciations
pronunciation probabilities
Our model 1 PMM 2 PMM
93 56 87 39 19 0.125 - -
93 56 61 87 73 99 0.125 - -
11 56 61 87 73 99 0.125 0.400 0.419
93 20 75 87 17 27 52 0.125 0.125 0.124
55 93 56 61 87 73 84 19 0.125 0.220 0.210
93 26 61 87 49 0.125 0.128 0.140
63 83 86 87 73 53 19 0.125 - -
93 26 61 87 61 0.125 0.127 0.107
Table 4: Pronunciation lists of the word Burma pro-
duced by our model and refined by PMM after 1 and
2 iterations.
pervised method. Therefore, in this paper, we pro-
vide further analysis on how PMM helps enhance
the performance of our model.
We compare the pronunciation lists for the word
Burma generated by our model and refined itera-
tively by PMM in Table 4. The first column of Ta-
ble 4 shows all the pronunciations of Burma dis-
covered by our model, to which our model assigns
equal probabilities to create a stochastic list6. As
demonstrated in the third and the fourth columns of
Table 4, the PMM framework is able to iteratively
re-distribute the pronunciation weights and filter out
less-likely pronunciations, which effectively reduces
both the size and the entropy of the stochastic lexi-
con generated by our model. The benefits of using
the PMM to refine the induced lexicon are twofold.
First, the search space constructed during the recog-
nition decoding process with the refined lexicon is
more constrained, which is the main reason why the
PMM is capable of improving the performance of
the monophone recognizer that is trained with the
output of our model. Secondly, and more impor-
tantly, the refined lexicon can greatly reduce the size
of the FST built for the triphone recognizer of our
model. These two observations illustrate why the
PMM framework can be an useful tool for enhancing
the lexicon discovered automatically by our model.
6.3 Triphone Systems
The best monophone systems of the grapheme base-
line, the oracle baseline and our model are used to
6It is also possible to assign probabilities proportional to the
decoding scores of the word tokens.
Unit(%) Triphone
Our model 13.4
Oracle 10.0
Grapheme 15.7
Table 5: Word error rates of the triphone recogniz-
ers. The triphone recognizers are all built by us-
ing the phone transcriptions generated by their best
monohpone system. For the oracle initialized base-
line and for our model, the PMM-refined lexicons
are used to build the triphone recognizers.
generate forced-aligned phone transcriptions, which
are used to train the triphone models described in
Sec. 5.2.2 and Sec. 5.3. Table 5 shows the WERs
of the triphone recognition systems. Note that if a
more conventional question list, for example, a list
that contains rules to classify phones into different
broad classes, is used to build the oracle triphone
system, the WER can be reduced to 6.5%. However,
as mentioned earlier, in order to gain insights into
the quality of the induced lexicon and the discovered
phonetic set, we compare our model against an ora-
cle triphone system that is built by using a singleton
question set.
By comparing Table 2 and Table 5, we can see
that the grapheme triphone improves by a large mar-
gin compared to its monophone counterpart, which
is consistent with the results reported in (Killer et
al., 2003). However, even though the grapheme
baseline achieves a great performance gain with
context-dependent acoustic models, the recognizer
trained using the lexicon learned by our model and
subsequently refined by PMM still outperforms the
grapheme baseline. The consistently better perfor-
mance our model achieves over the grapheme base-
line demonstrates the strength of modeling the pro-
nunciation of each letter with a mixture model that
is presented in this paper.
Last but not least, by comparing Table 2 and
Table 5, it can be seen that the relative perfor-
mance gain achieved by our model is similar to
that obtained by the oracle baseline. Both Table 2
and Table 5 show that even without exploiting any
language-specific knowledge during training, our
recognizer is able to perform comparably with the
recognizer trained using an expert lexicon. The abil-
ity of our model to obtain such similar performance
191
further supports the effectiveness of the joint learn-
ing framework proposed in this paper for discover-
ing the phonetic inventory and the word pronuncia-
tions from simply an annotated speech corpus.
7 Conclusion
We present a hierarchical Bayesian model for si-
multaneously discovering acoustic units and learn-
ing word pronunciations from transcribed spoken ut-
terances. Both monophone and triphone recogniz-
ers can be built on the discovered acoustic units and
the inferred lexicon. The recognizers trained with
the proposed unsupervised method consistently out-
performs grapheme-based recognizers and approach
the performance of recognizers trained with expert-
defined lexicons. In the future, we plan to apply this
technology to develop ASRs for more languages.
Acknowledgements
The authors would like to thank Ian McGraw and
Ekapol Chuangsuwanich for their advice on the
PMM and recognition experiments presented in this
paper. Thanks to the anonymous reviewers for help-
ful comments. Finally, the authors would like to
thank Stephen Shum for proofreading and editing
the early drafts of this paper.
References
Michiel Bacchiani and Mari Ostendorf. 1999. Joint lexi-
con, acoustic unit inventory and model design. Speech
Communication, 29:99 ? 114.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451, May.
Steven B. Davis and Paul Mermelstein. 1980. Com-
parison of parametric representations for monosyllabic
word recognition in continuously spoken sentences.
IEEE Trans. on Acoustics, Speech, and Signal Pro-
cessing, 28(4):357?366.
Toshiaki Fukada, Michiel Bacchiani, Kuldip Paliwal, and
Yoshinori Sagisaka. 1996. Speech recognition based
on acoustically derived segment units. In Proceedings
of ICSLP, pages 1077 ? 1080.
Alvin Garcia and Herbert Gish. 2006. Keyword spotting
of arbitrary words using minimal speech resources. In
Proceedings of ICASSP, pages 949?952.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-
ald B. Rubin. 2004. Bayesian Data Analysis. Texts
in Statistical Science. Chapman & Hall/CRC, second
edition.
James Glass. 2003. A probabilistic framework for
segment-based speech recognition. Computer Speech
and Language, 17:137 ? 152.
Aren Jansen and Kenneth Church. 2011. Towards un-
supervised training of speaker independent acoustic
models. In Proceedings of INTERSPEECH, pages
1693 ? 1696.
Frederick Jelinek. 1976. Continuous speech recogni-
tion by statistical methods. Proceedings of the IEEE,
64:532 ? 556.
Matthew J. Johnson and Alan S. Willsky. 2013. Bayesian
nonparametric hidden semi-markov models. Journal
of Machine Learning Research, 14:673?701, February.
Mirjam Killer, Sebastian Stu?ker, and Tanja Schultz.
2003. Grapheme based speech recognition. In Pro-
ceeding of the Eurospeech, pages 3141?3144.
Chia-ying Lee and James Glass. 2012. A nonparamet-
ric Bayesian approach to acoustic model discovery. In
Proceedings of ACL, pages 40?49.
Chin-Hui Lee, Frank Soong, and Biing-Hwang Juang.
1988. A segment model based approach to speech
recognition. In Proceedings of ICASSP, pages 501?
504.
Ian McGraw, Ibrahim Badr, and James Glass. 2013.
Learning lexicons from speech using a pronunciation
mixture model. IEEE Trans. on Speech and Audio
Processing, 21(2):357?366.
Kevin P. Murphy. 2002. Hidden semi-Markov mod-
els (hsmms). Technical report, University of British
Columbia.
Kuldip Paliwal. 1990. Lexicon-building methods for an
acoustic sub-word based speech recognizer. In Pro-
ceedings of ICASSP, pages 729?732.
Man-hung Siu, Herbert Gish, Arthur Chan, William
Belfield, and Steve Lowe. 2013. Unsupervised train-
ing of an HMM-based self-organizing unit recgonizer
with applications to topic classification and keyword
discovery. Computer, Speech, and Language.
Sebastian Stu?ker and Tanja Schultz. 2004. A grapheme
based speech recognition system for Russian. In Pro-
ceedings of the 9th Conference Speech and Computer.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-
manuel Dupoux. 2008. Unsupervised learning of
acoustic sub-word units. In Proceedings of ACL-08:
HLT, Short Papers, pages 165?168.
Steve J. Young, J.J. Odell, and Philip C. Woodland. 1994.
Tree-based state tying for high accuracy acoustic mod-
elling. In Proceedings of HLT, pages 307?312.
Victor Zue, Stephanie Seneff, James Glass, Joseph Po-
lifroni, Christine Pao, Timothy J. Hazen, and Lee Het-
herington. 2000. Jupiter: A telephone-based con-
versational interface for weather information. IEEE
Trans. on Speech and Audio Processing, 8:85?96.
192
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 40?49,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Nonparametric Bayesian Approach to Acoustic Model Discovery
Chia-ying Lee and James Glass
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{chiaying,jrg}@csail.mit.edu
Abstract
We investigate the problem of acoustic mod-
eling in which prior language-specific knowl-
edge and transcribed data are unavailable. We
present an unsupervised model that simultane-
ously segments the speech, discovers a proper
set of sub-word units (e.g., phones) and learns
a Hidden Markov Model (HMM) for each in-
duced acoustic unit. Our approach is formu-
lated as a Dirichlet process mixture model in
which each mixture is an HMM that repre-
sents a sub-word unit. We apply our model
to the TIMIT corpus, and the results demon-
strate that our model discovers sub-word units
that are highly correlated with English phones
and also produces better segmentation than the
state-of-the-art unsupervised baseline. We test
the quality of the learned acoustic models on a
spoken term detection task. Compared to the
baselines, our model improves the relative pre-
cision of top hits by at least 22.1% and outper-
forms a language-mismatched acoustic model.
1 Introduction
Acoustic models are an indispensable component
of speech recognizers. However, the standard pro-
cess of training acoustic models is expensive, and
requires not only language-specific knowledge, e.g.,
the phone set of the language, a pronunciation dic-
tionary, but also a large amount of transcribed data.
Unfortunately, these necessary data are only avail-
able for a very small number of languages in the
world. Therefore, a procedure for training acous-
tic models without annotated data would not only
be a breakthrough from the traditional approach, but
would also allow us to build speech recognizers for
any language efficiently.
In this paper, we investigate the problem of unsu-
pervised acoustic modeling with only spoken utter-
ances as training data. As suggested in Garcia and
Gish (2006), unsupervised acoustic modeling can
be broken down to three sub-tasks: segmentation,
clustering segments, and modeling the sound pattern
of each cluster. In previous work, the three sub-
problems were often approached sequentially and
independently in which initial steps are not related to
later ones (Lee et al, 1988; Garcia and Gish, 2006;
Chan and Lee, 2011). For example, the speech data
was usually segmented regardless of the clustering
results and the learned acoustic models.
In contrast to the previous methods, we approach
the problem by modeling the three sub-problems as
well as the unknown set of sub-word units as la-
tent variables in one nonparametric Bayesian model.
More specifically, we formulate a Dirichlet pro-
cess mixture model where each mixture is a Hid-
den Markov Model (HMM) used to model a sub-
word unit and to generate observed segments of that
unit. Our model seeks the set of sub-word units,
segmentation, clustering and HMMs that best repre-
sent the observed data through an iterative inference
process. We implement the inference process using
Gibbs sampling.
We test the effectiveness of our model on the
TIMIT database (Garofolo et al, 1993). Our model
shows its ability to discover sub-word units that are
highly correlated with standard English phones and
to capture acoustic context information. For the seg-
mentation task, our model outperforms the state-of-
40
the-art unsupervised method and improves the rel-
ative F-score by 18.8 points (Dusan and Rabiner,
2006). Finally, we test the quality of the learned
acoustic models through a keyword spotting task.
Compared to the state-of-the-art unsupervised meth-
ods (Zhang and Glass, 2009; Zhang et al, 2012),
our model yields a relative improvement in precision
of top hits by at least 22.1% with only some degra-
dation in equal error rate (EER), and outperforms
a language-mismatched acoustic model trained with
supervised data.
2 Related Work
Unsupervised Sub-word Modeling We follow
the general guideline used in (Lee et al, 1988; Gar-
cia and Gish, 2006; Chan and Lee, 2011) and ap-
proach the problem of unsupervised acoustic mod-
eling by solving three sub-problems of the task:
segmentation, clustering and modeling each cluster.
The key difference, however, is that our model does
not assume independence among the three aspects of
the problem, which allows our model to refine its so-
lution to one sub-problem by exploiting what it has
learned about other parts of the problem. Second,
unlike (Lee et al, 1988; Garcia and Gish, 2006) in
which the number of sub-word units to be learned is
assumed to be known, our model learns the proper
size from the training data directly.
Instead of segmenting utterances, the authors
of (Varadarajan et al, 2008) trained a single state
HMM using all data at first, and then iteratively
split the HMM states based on objective functions.
This method achieved high performance in a phone
recognition task using a label-to-phone transducer
trained from some transcriptions. However, the per-
formance seemed to rely on the quality of the trans-
ducer. For our work, we assume no transcriptions
are available and measure the quality of the learned
acoustic units via a spoken query detection task as
in Jansen and Church (2011).
Jansen and Church (2011) approached the task of
unsupervised acoustic modeling by first discovering
repetitive patterns in the data, and then learned a
whole-word HMM for each found pattern, where the
state number of each HMM depends on the average
length of the pattern. The states of the whole-word
HMMs were then collapsed and used to represent
acoustic units. Instead of discovering repetitive pat-
terns first, our model is able to learn from any given
data.
Unsupervised Speech Segmentation One goal
of our model is to segment speech data into
small sub-word (e.g., phone) segments. Most un-
supervised speech segmentation methods rely on
acoustic change for hypothesizing phone bound-
aries (Scharenborg et al, 2010; Qiao et al, 2008;
Dusan and Rabiner, 2006; Estevan et al, 2007).
Even though the overall approaches differ, these al-
gorithms are all one-stage and bottom-up segmenta-
tion methods (Scharenborg et al, 2010). Our model
does not make a single one-stage decision; instead, it
infers the segmentation through an iterative process
and exploits the learned sub-word models to guide
its hypotheses on phone boundaries.
Bayesian Model for Segmentation Our model is
inspired by previous applications of nonparametric
Bayesian models to segmentation problems in NLP
and speaker diarization (Goldwater, 2009; Fox et al,
2011); particularly, we adapt the inference method
used in (Goldwater, 2009) to our segmentation task.
Our problem is, in principle, similar to the word seg-
mentation problem discussed in (Goldwater, 2009).
The main difference, however, is that our model
is under the continuous real value domain, and the
problem of (Goldwater, 2009) is under the discrete
symbolic domain. For the domain our problem is ap-
plied to, our model has to include more latent vari-
ables and is more complex.
3 Problem Formulation
The goal of our model, given a set of spoken utter-
ances, is to jointly learn the following:
? Segmentation: To find the phonetic boundaries
within each utterance.
? Nonparametric clustering: To find a proper set
of clusters and group acoustically similar seg-
ments into the same cluster.
? Sub-word modeling: To learn a HMM to model
each sub-word acoustic unit.
We model the three sub-tasks as latent variables
in our approach. In this section, we describe the ob-
served data, latent variables, and auxiliary variables
41
? 
x
2
i
? 
x
3
i
? 
x
4
i
? 
x
5
i
? 
x
6
i
? 
x
7
i
? 
x
8
i
? 
x
9
i
? 
x
10
i
? 
x
11
i
? 
x
1
i
b a n a n a 
? 
(x
t
i
)
? 
(t) 1 2 3 4 5 6 7 8 9 10 11 
? 
(b
t
i
)
? 
(g
q
i
)
? 
g
0
i
? 
g
1
i
? 
g
2
i
? 
g
3
i
? 
g
4
i
? 
g
5
i
? 
g
6
i
? 
(p
j ,k
i
)
? 
p
1,1
i
? 
p
2,4
i
? 
p
5,6
i
? 
p
7,8
i
? 
p
9,9
i
? 
p
10,11
i
? 
(c
j ,k
i
)
? 
c
1,1
i
? 
c
2,4
i
? 
c
5,6
i
? 
c
7,8
i
? 
c
9,9
i
? 
c
10,11
i
? 
(?
c
)
? 
?
1
? 
?
2
? 
?
3
? 
?
4
? 
?
3
? 
?
2
? 
(s
t
i
) 1 1 2 3 1 3 1 3 1 1 3 
Frame index Speech feature Boundary variable Boundary index Segment 
Cluster label 
HMM 
Hidden state 
[b] [ax] [n] [ae] [n] [ax] Pronunciation 
1 0 0 1 0 1 0 1 1  0 1 
Duration 
? 
(d
j,k
i
) 1 3 2 2 1 2 
1 1 6 8 3 7 5 2 8 2 8 Mixture ID 
Figure 1: An example of the observed data and hidden
variables of the problem for the word banana. See Sec-
tion 3 for a detailed explanation.
of the problem and show an example in Fig. 1. In
the next section, we show the generative process our
model uses to generate the observed data.
Speech Feature (xit) The only observed data for
our problem are a set of spoken utterances, which are
converted to a series of 25 ms 13-dimensional Mel-
Frequency Cepstral Coefficients (MFCCs) (Davis
and Mermelstein, 1980) and their first- and second-
order time derivatives at a 10 ms analysis rate. We
use xit ? R
39 to denote the tth feature frame of the
ith utterance. Fig. 1 illustrates how the speech signal
of a single word utterance banana is converted to a
sequence of feature vectors xi1 to x
i
11.
Boundary (bit) We use a binary variable b
i
t to in-
dicate whether a phone boundary exists between xit
and xit+1. If our model hypothesizes x
i
t to be the last
frame of a sub-word unit, which is called a boundary
frame in this paper, bit is assigned with value 1; or 0
otherwise. Fig. 1 shows an example of the boundary
variables where the values correspond to the true an-
swers. We use an auxiliary variable giq to denote the
index of the qth boundary frame in utterance i. To
make the derivation of posterior distributions easier
in Section 5, we define gi0 to be the beginning of
an utterance, and Li to be the number of boundary
frames in an utterance. For the example shown in
Fig. 1, Li is equal to 6.
Segment (pij,k) We define a segment to be com-
posed of feature vectors between two boundary
frames. We use pij,k to denote a segment that con-
sists of xij , x
i
j+1 ? ? ?x
i
k and d
i
j,k to denote the length
of pij,k. See Fig. 1 for more examples.
Cluster Label (cij,k) We use c
i
j,k to specify the
cluster label of pij,k. We assume segment p
i
j,k is gen-
erated by the sub-word HMM with label cij,k.
HMM (?c) In our model, each HMM has three
emission states, which correspond to the beginning,
middle and end of a sub-word unit (Jelinek, 1976).
A traversal of each HMM must start from the first
state, and only left-to-right transitions are allowed
even though we allow skipping of the middle and
the last state for segments shorter than three frames.
The emission probability of each state is modeled by
a diagonal Gaussian Mixture Model (GMM) with 8
mixtures. We use ?c to represent the set of param-
eters that define the cth HMM, which includes state
transition probability aj,kc , and the GMM parameters
of each state emission probability. We use wmc,s ? R,
?mc,s ? R
39 and ?mc,s ? R
39 to denote the weight,
mean vector and the diagonal of the inverse covari-
ance matrix of the mth mixture in the GMM for the
sth state in the cth HMM.
Hidden State (sit) Since we assume the observed
data are generated by HMMs, each feature vector,
xit, has an associated hidden state index. We denote
the hidden state of xit as s
i
t.
Mixture ID (mit) Similarly, each feature vector is
assumed to be emitted by the state GMM it belongs
to. We use mit to identify the Gaussian mixture that
generates xit.
4 Model
We aim to discover and model a set of sub-word
units that represent the spoken data. If we think of
utterances as sequences of repeated sub-word units,
then in order to find the sub-words, we need a model
that concentrates probability on highly frequent pat-
terns while still preserving probability for previously
unseen ones. Dirichlet processes are particulary
suitable for our goal. Therefore, we construct our
model as a Dirichlet Process (DP) mixture model,
of which the components are HMMs that are used
42
parameter of Bernoulli distribution 
? 
?
b
? 
?
? 
?
0
concentration parameter of DP base distribution of DP 
? 
? prior distribution for cluster labels 
? 
b
t
boundary variable 
? 
d
j ,k duration of a segment 
? 
c
j,k
cluster label 
? 
?
c
HMM parameters 
? 
s
t
hidden state 
? 
m
t
Gaussian mixture id 
? 
x
t
observed feature vector deterministic relation 
? 
?
? 
T
? 
?
? 
d
j ,k
? 
?
? 
?
b
? 
?
0
? 
c
j,k
? 
s
t
? 
j,k = g
q
+1,g
q+1
? 
x
t
? 
d
j ,k
? 
m
t
? 
b
t
? 
?
c
? 
0 ? q < L
? 
T
total number of  observed features frames 
? 
L
total number of  segments determined by  
? 
b
t
? 
g
q
the index of the       boundary variable with value 1 
? 
q
th
Figure 2: The graphical model for our approach. The shaded circle denotes the observed feature vectors, and the
squares denote the hyperparameters of the priors used in our model. The dotted arrows indicate deterministic relations.
Note that the Markov chain structure over the st variables is not shown here due to limited space.
to model sub-word units. We assume each spoken
segment is generated by one of the clusters in this
DP mixture model. Here, we describe the genera-
tive process our model uses to generate the observed
utterances and present the corresponding graphical
model. For clarity, we assume that the values of
the boundary variables bit are given in the genera-
tive process. In the next section, we explain how to
infer their values.
Let pi
giq+1,g
i
q+1
for 0 ? q ? Li ? 1 be the seg-
ments of the ith utterance. Our model assumes each
segment is generated as follows:
1. Choose a cluster label ci
giq+1,g
i
q+1
for pi
giq+1,g
i
q+1
.
This cluster label can be either an existing la-
bel or a new one. Note that the cluster label
determines which HMM is used to generate the
segment.
2. Given the cluster label, choose a hidden state
for each feature vector xit in the segment.
3. For each xit, based on its hidden state, choose a
mixture from the GMM of the chosen state.
4. Use the chosen Gaussian mixture to generate
the observed feature vector xit.
The generative process indicates that our model
ignores utterance boundaries and views the entire
data as concatenated spoken segments. Given this
viewpoint, we discard the utterance index, i, of all
variables in the rest of the paper.
The graphical model representing this generative
process is shown in Fig. 2, where the shaded circle
denotes the observed feature vectors, and the squares
denote the hyperparameters of the priors used in our
model. Specifically, we use a Bernoulli distribution
as the prior of the boundary variables and impose
a Dirichlet process prior on the cluster labels and
the HMM parameters. The dotted arrows represent
deterministic relations. For example, the boundary
variables deterministically construct the duration of
each segment, d, which in turn sets the number of
feature vectors that should be generated for a seg-
ment. In the next section, we show how to infer the
value of each of the latent variables in Fig. 21.
5 Inference
We employ Gibbs sampling (Gelman et al, 2004)
to approximate the posterior distribution of the hid-
den variables in our model. To apply Gibbs sam-
pling to our problem, we need to derive the condi-
tional posterior distributions of each hidden variable
of the model. In the following sections, we first de-
rive the sampling equations for each hidden variable
and then describe how we incorporate acoustic cues
to reduce the sampling load at the end.
1Note that the value of pi is irrelevant to our problem; there-
fore, it is integrated out in the inference process
43
5.1 Sampling Equations
Here we present the sampling equations for each
hidden variable defined in Section 3. We use
P (?| ? ? ? ) to denote a conditional posterior probabil-
ity given observed data, all the other variables, and
hyperparameters for the model.
Cluster Label (cj,k) Let C be the set of distinctive
label values in c?j,k, which represents all the cluster
labels except cj,k. The conditional posterior proba-
bility of cj,k for c ? C is:
P (cj,k = c| ? ? ? ) ? P (cj,k = c|c?j,k; ?)P (pj,k|?c)
=
n(c)
N ? 1 + ?
P (pj,k|?c) (1)
where ? is a parameter of the DP prior. The first line
of Eq. 1 follows Bayes? rule. The first term is the
conditional prior, which is a result of the DP prior
imposed on the cluster labels 2. The second term is
the conditional likelihood, which reflects how likely
the segment pj,k is generated by HMMc. We use n(c)
to represent the number of cluster labels in c?j,k tak-
ing the value c and N to represent the total number
of segments in current segmentation.
In addition to existing cluster labels, cj,k can also
take a new cluster label, which corresponds to a new
sub-word unit. The corresponding conditional pos-
terior probability is:
P (cj,k 6= c, c ? C| ? ? ? ) ?
?
N ? 1 + ?
?
?
P (pj,k|?) d?
(2)
To deal with the integral in Eq. 2, we follow the
suggestions in (Rasmussen, 2000; Neal, 2000). We
sample an HMM from the prior and compute the
likelihood of the segment given the new HMM to
approximate the integral.
Finally, by normalizing Eq. 1 and Eq. 2, the Gibbs
sampler can draw a new value for cj,k by sampling
from the normalized distribution.
Hidden State (st) To enforce the assumption that
a traversal of an HMM must start from the first state
and end at the last state3, we do not sample hidden
state indices for the first and the last frame of a seg-
ment. For each of the remaining feature vectors in
2See (Neal, 2000) for an overview on Dirichlet process mix-
ture models and the inference methods.
3If a segment has only 1 frame, we assign the first state to it.
a segment pj,k, we sample a hidden state index ac-
cording to the conditional posterior probability:
P (st = s| ? ? ? ) ?
P (st = s|st?1)P (xt|?cj,k , st = s)P (st+1|st = s)
= ast?1,scj,k P (xt|?cj,k , st = s)a
s,st+1
cj,k (3)
where the first term and the third term are the condi-
tional prior ? the transition probability of the HMM
that pj,k belongs to. The second term is the like-
lihood of xt being emitted by state s of HMMcj,k .
Note for initialization, st is sampled from the first
prior term in Eq. 3.
Mixture ID (mt) For each feature vector in a seg-
ment, given the cluster label cj,k and the hidden state
index st, the derivation of the conditional posterior
probability of its mixture ID is straightforward:
P (mt = m| ? ? ? )
? P (mt = m|?cj,k , st)P (xt|?cj,k , st,mt = m)
= wmcj,k,stP (xt|?
m
cj,k,st , ?
m
cj,k,st) (4)
where 1 ? m ? 8. The conditional posterior con-
sists of two terms: 1) the mixing weight of the mth
Gaussian in the state GMM indexed by cj,k and st
and 2) the likelihood of xt given the Gaussian mix-
ture. The sampler draws a value for mt from the
normalized distribution of Eq. 4.
HMM Parameters (?c) Each ?c consists of two
sets of variables that define an HMM: the state emis-
sion probabilities wmc,s, ?
m
c,s, ?
m
c,s and the state transi-
tion probabilities aj,kc . In the following, we derive
the conditional posteriors of these variables.
Mixture Weight wmc,s: We use wc,s = {w
m
c,s|1 ?
m ? 8} to denote the mixing weights of the Gaus-
sian mixtures of state s of HMM c. We choose a
symmetric Dirichlet distribution with a positive hy-
perparameter ? as its prior. The conditional poste-
rior probability of wc,s is:
P (wc,s| ? ? ? ) ? P (wc,s;?)P (mc,s|wc,s)
? Dir(wc,s;?)Mul(mc,s;wc,s)
? Dir(wc,s;?
?) (5)
where mc,s is the set of mixture IDs of feature vec-
tors that belong to state s of HMM c. The mth entry
of ?? is ? +
?
mt?mc,s ?(mt,m), where we use ?(?)
44
P (pl,t, pt+1,r|c?,?) = P (pl,t|c?,?)P (pt+1,r|c?, cl,t,?)
=
[
?
c?C
n(c)
N? + ?
P (pl,t|?c) +
?
N? + ?
?
?
P (pl,t|?) d?
]
?
[
?
c?C
n(c) + ?(cl,t, c)
N? + 1 + ?
P (pt+1,r|?c) +
?
N? + 1 + ?
?
?
P (pt+1,r|?) d?
]
P (pl,r|c?,?) =
?
c?C
n(c)
N? + ?
P (pl,r|?c) +
?
N? + ?
?
?
P (pl,r|?) d?
Figure 3: The full derivation of the relative conditional posterior probabilities of a boundary variable.
to denote the discrete Kronecker delta. The last line
of Eq. 5 comes from the fact that Dirichlet distribu-
tions are a conjugate prior for multinomial distribu-
tions. This property allows us to derive the update
rule analytically.
Gaussian Mixture ?mc,s, ?
m
c,s: We assume the di-
mensions in the feature space are independent. This
assumption allows us to derive the conditional pos-
terior probability for a single-dimensional Gaussian
and generalize the results to other dimensions.
Let the dth entry of ?mc,s and ?
m
c,s be ?
m,d
c,s and
?m,dc,s . The conjugate prior we use for the two vari-
ables is a normal-Gamma distribution with hyperpa-
rameters ?0, ?0, ?0 and ?0 (Murphy, 2007).
P (?m,dc,s , ?
m,d
c,s |?0, ?0, ?0, ?0)
= N(?m,dc,s |?0, (?0?
m,d
c,s )
?1)Ga(?m,dc,s |?0, ?0)
By tracking the dth dimension of feature vectors
x ? {xt|mt = m, st = s, cj,k = c, xt ? pj,k}, we
can derive the conditional posterior distribution of
?m,dc,s and ?
m,d
c,s analytically following the procedures
shown in (Murphy, 2007). Due to limited space,
we encourage interested readers to find more details
in (Murphy, 2007).
Transition Probabilities aj,kc : We represent the
transition probabilities at state j in HMM c using ajc.
If we view ajc as mixing weights for states reachable
from state j, we can simply apply the update rule
derived for the mixing weights of Gaussian mixtures
shown in Eq. 5 to ajc. Assume we use a symmetric
Dirichlet distribution with a positive hyperparameter
? as the prior, the conditional posterior for ajc is:
P (ajc| ? ? ? ) ? Dir(a
j
c; ?
?)
where the kth entry of ?? is ? + nj,kc , the number
of occurrences of the state transition pair (j, k) in
segments that belong to HMM c.
Boundary Variable (bt) To derive the conditional
posterior probability for bt, we introduce two vari-
ables:
l = (argmax
gq
gq < t) + 1
r = argmin
gq
t < gq
where l is the index of the closest turned-on bound-
ary variable that precedes bt plus 1, while r is the in-
dex of the closest turned-on boundary variable that
follows bt. Note that because g0 and gL are defined,
l and r always exist for any bt.
Note that the value of bt only affects segmentation
between xl and xr. If bt is turned on, the sampler hy-
pothesizes two segments pl,t and pt+1,r between xl
and xr. Otherwise, only one segment pl,r is hypoth-
esized. Since the segmentation on the rest of the data
remains the same no matter what value bt takes, the
conditional posterior probability of bt is:
P (bt = 1| ? ? ? ) ? P (pl,t, pt+1,r|c?,?) (6)
P (bt = 0| ? ? ? ) ? P (pl,r|c?,?) (7)
where we assume that the prior probabilities for
bt = 1 and bt = 0 are equal; c? is the set of cluster
labels of all segments except those between xl and
xr ; and ? indicates the set of HMMs that have as-
sociated segments. Our Gibbs sampler hypothesizes
bt?s value by sampling from the normalized distribu-
tion of Eq. 6 and Eq. 7. The full derivations of Eq. 6
and Eq. 7 are shown in Fig. 3.
Note that in Fig. 3, N? is the total number of seg-
ments in the data except those between xl and xr.
45
For bt = 1, to account the fact that when the model
generates pt+1,r, pl,t is already generated and owns
a cluster label, we sample a cluster label for pl,t that
is reflected in the Kronecker delta function. To han-
dle the integral in Fig. 3, we sample one HMM from
the prior and compute the likelihood using the new
HMM to approximate the integral as suggested in
(Rasmussen, 2000; Neal, 2000).
5.2 Heuristic Boundary Elimination
To reduce the inference load on the boundary vari-
ables bt, we exploit acoustic cues in the feature space
to eliminate bt?s that are unlikely to be phonetic
boundaries. We follow the pre-segmentation method
described in Glass (2003) to achieve the goal. For
the rest of the boundary variables that are proposed
by the heuristic algorithm, we randomly initialize
their values and proceed with the sampling process
described above.
6 Experimental Setup
To the best of our knowledge, there are no stan-
dard corpora for evaluating unsupervised methods
for acoustic modeling. However, numerous related
studies have reported performance on the TIMIT
corpus (Dusan and Rabiner, 2006; Estevan et al,
2007; Qiao et al, 2008; Zhang and Glass, 2009;
Zhang et al, 2012), which creates a set of strong
baselines for us to compare against. Therefore, the
TIMIT corpus is chosen as the evaluation set for
our model. In this section, we describe the methods
used to measure the performance of our model on
the following three tasks: sub-word acoustic model-
ing, segmentation and nonparametric clustering.
Unsupervised Segmentation We compare the
phonetic boundaries proposed by our model to the
manual labels provided in the TIMIT dataset. We
follow the suggestion of (Scharenborg et al, 2010)
and use a 20-ms tolerance window to compute re-
call, precision rates and F-score of the segmentation
our model proposed for TIMIT?s training set. We
compare our model against the state-of-the-art un-
supervised and semi-supervised segmentation meth-
ods that were also evaluated on the TIMIT training
set (Dusan and Rabiner, 2006; Qiao et al, 2008).
Nonparametric Clustering Our model automat-
ically groups speech segments into different clus-
ters. One question we are interested in answering
is whether these learned clusters correlate to En-
glish phones. To answer the question, we develop
a method to map cluster labels to the phone set in
a dataset. We align each cluster label in an utter-
ance to the phone(s) it overlaps with in time by
using the boundaries proposed by our model and
the manually-labeled ones. When a cluster label
overlaps with more than one phone, we align it
to the phone with the largest overlap.4 We com-
pile the alignment results for 3696 training utter-
ances5 and present a confusion matrix between the
learned cluster labels and the 48 phonetic units used
in TIMIT (Lee and Hon, 1989).
Sub-word Acoustic Modeling Finally, and most
importantly, we need to gauge the quality of the
learned sub-word acoustic models. In previous
work, Varadarajan et al (2008) and Garcia and
Gish (2006) tested their models on a phone recog-
nition task and a term detection task respectively.
These two tasks are fair measuring methods, but per-
formance on these tasks depends not only on the
learned acoustic models, but also other components
such as the label-to-phone transducer in (Varadara-
jan et al, 2008) and the graphone model in (Garcia
and Gish, 2006). To reduce performance dependen-
cies on components other than the acoustic model,
we turn to the task of spoken term detection, which
is also the measuring method used in (Jansen and
Church, 2011).
We compare our unsupervised acoustic model
with three supervised ones: 1) an English triphone
model, 2) an English monophone model and 3) a
Thai monophone model. The first two were trained
on TIMIT, while the Thai monophone model was
trained with 32 hour clean read Thai speech from
the LOTUS corpus (Kasuriya et al, 2003). All
of the three models, as well as ours, used three-
state HMMs to model phonetic units. To conduct
spoken term detection experiments on the TIMIT
dataset, we computed a posteriorgram representa-
tion for both training and test feature frames over the
4Except when a cluster label is mapped to /vcl/ /b/, /vcl/ /g/
and /vcl/ /d/, where the duration of the release /b/, /g/, /d/ is
almost always shorter than the closure /vcl/. In this case, we
align the cluster label to both the closure and the release.
5The TIMIT training set excluding the sa-type subset.
46
? ?b ? ? ?0 ?0 ?0 ?0
1 0.5 3 3 ?d 5 3 3/?d
Table 1: The values of the hyperparameters of our model,
where ?d and ?d are the dth entry of the mean and the
diagonal of the inverse covariance matrix of training data.
HMM states for each of the four models. Ten key-
words were randomly selected for the task. For ev-
ery keyword, spoken examples were extracted from
the training set and were searched for in the test set
using segmental dynamic time warping (Zhang and
Glass, 2009).
In addition to the supervised acoustic models,
we also compare our model against the state-of-
the-art unsupervised methods for this task (Zhang
and Glass, 2009; Zhang et al, 2012). Zhang and
Glass (2009) trained a GMM with 50 components
to decode posteriorgrams for the feature frames, and
Zhang et al (2012) used a deep Boltzmann machine
(DBM) trained with pseudo phone labels generated
from an unsupervised GMM to produce a posteri-
orgram representation. The evaluation metrics they
used were: 1) P@N, the average precision of the top
N hits, where N is the number of occurrences of each
keyword in the test set; 2) EER: the average equal er-
ror rate at which the false acceptance rate is equal to
the false rejection rate. We also report experimental
results using the P@N and EER metrics.
Hyperparameters and Training Iterations The
values of the hyperparameters of our model are
shown in Table 1, where ?d and ?d are the dth en-
try of the mean and the diagonal of the inverse co-
variance matrix computed from training data. We
pick these values to impose weak priors on our
model.6 We run our sampler for 20,000 iterations,
after which the evaluation metrics for our model all
converged. In Section 7, we report the performance
of our model using the sample from the last iteration.
7 Results
Fig. 4 shows a confusion matrix of the 48 phones
used in TIMIT and the sub-word units learned from
3696 TIMIT utterances. Each circle represents a
mapping pair for a cluster label and an English
phone. The confusion matrix demonstrates a strong
6In the future, we plan to extend the model and infer the
values of these hyperparameters from data directly.
0510
152025
303540
455055
606570
758085
9095100
105110115
120
iy ix ih ey eh y ae ay aw aa ao ah ax uh uw ow oy w l el er r m n en ng z s zh sh ch jh hh v f dh th d b dx g vcl t p k cl epi sil 
 
Figure 4: A confusion matrix of the learned cluster labels
from the TIMIT training set excluding the sa type utter-
ances and the 48 phones used in TIMIT. Note that for
clarity, we show only pairs that occurred more than 200
times in the alignment results. The average co-occurrence
frequency of the mapping pairs in this figure is 431.
correlation between the cluster labels and individ-
ual English phones. For example, clusters 19, 20
and 21 are mapped exclusively to the vowel /ae/. A
more careful examination on the alignment results
shows that the three clusters are mapped to the same
vowel in a different acoustic context. For example,
cluster 19 is mapped to /ae/ followed by stop conso-
nants, while cluster 20 corresponds to /ae/ followed
by nasal consonants. This context-dependent rela-
tionship is also observed in other English phones
and their corresponding sets of clusters. Fig. 4 also
shows that a cluster may be mapped to multiple En-
glish phones. For instance, clusters 85 and 89 are
mapped to more than one phone; nevertheless, a
closer look reveals that these clusters are mapped to
/n/, /d/ and /b/, which are sounds with a similar place
of articulation (i.e. labial and dental). These corre-
lations indicate that our model is able to discover the
phonetic composition of a set of speech data without
any language-specific knowledge.
The performance of the four acoustic models on
the spoken term detection task is presented in Ta-
ble 2. The English triphone model achieves the best
P@N and EER results and performs slightly bet-
ter than the English monophone model, which indi-
cates a correlation between the quality of an acous-
tic model and its performance on the spoken term
detection task. Although our unsupervised model
does not perform as well as the supervised English
47
unit(%) P@N EER
English triphone 75.9 11.7
English monophone 74.0 11.8
Thai monophone 56.6 14.9
Our model 63.0 16.9
Table 2: The performance of our model and three super-
vised acoustic models on the spoken term detection task.
acoustic models, it generates a comparable EER and
a more accurate detection performance for top hits
than the Thai monophone model. This indicates that
even without supervision, our model captures and
learns the acoustic characteristics of a language au-
tomatically and is able to produce an acoustic model
that outperforms a language-mismatched acoustic
model trained with high supervision.
Table 3 shows that our model improves P@N by
a large margin and generates only a slightly worse
EER than the GMM baseline on the spoken term
detection task. At the end of the training process,
our model induced 169 HMMs, which were used to
compute posteriorgrams. This seems unfair at first
glance because Zhang and Glass (2009) only used
50 Gaussians for decoding, and the better result of
our model could be a natural outcome of the higher
complexity of our model. However, Zhang and
Glass (2009) pointed out that using more Gaussian
mixtures for their model did not improve their model
performance. This indicates that the key reason for
the improvement is our joint modeling method in-
stead of simply the higher complexity of our model.
Compared to the DBM baseline, our model pro-
duces a higher EER; however, it improves the rel-
ative detection precision of top hits by 24.3%. As
indicated in (Zhang et al, 2012), the hierarchical
structure of DBM allows the model to provide a
descent posterior representation of phonetic units.
Even though our model only contains simple HMMs
and Gaussians, it still achieves a comparable, if not
better, performance as the DBM baseline. This
demonstrates that even with just a simple model
structure, the proposed learning algorithm is able
to acquire rich phonetic knowledge from data and
generate a fine posterior representation for phonetic
units.
Table 4 summarizes the segmentation perfor-
mance of the baselines, our model and the heuristic
unit(%) P@N EER
GMM (Zhang and Glass, 2009) 52.5 16.4
DBM (Zhang et al, 2012) 51.1 14.7
Our model 63.0 16.9
Table 3: The performance of our model and the GMM
and DBM baselines on the spoken term detection task.
unit(%) Recall Precision F-score
Dusan (2006) 75.2 66.8 70.8
Qiao et al (2008)* 77.5 76.3 76.9
Our model 76.2 76.4 76.3
Pre-seg 87.0 50.6 64.0
Table 4: The segmentation performance of the baselines,
our model and the heuristic pre-segmentation on TIMIT
training set. *The number of phone boundaries in each
utterance was assumed to be known in this model.
pre-segmentation (pre-seg) method. The language-
independent pre-seg method is suitable for seeding
our model. It eliminates most unlikely boundaries
while retaining about 87% true boundaries. Even
though this indicates that at best our model only
recalls 87% of the true boundaries, the pre-seg re-
duces the search space significantly. In addition,
it also allows the model to capture proper phone
durations, which compensates the fact that we do
not include any explicit duration modeling mecha-
nisms in our approach. In the best semi-supervised
baseline model (Qiao et al, 2008), the number of
phone boundaries in an utterance was assumed to
be known. Although our model does not incorpo-
rate this information, it still achieves a very close
F-score. When compared to the baseline in which
the number of phone boundaries in each utterance
was also unknown (Dusan and Rabiner, 2006), our
model outperforms in both recall and precision, im-
proving the relative F-score by 18.8%. The key dif-
ference between the two baselines and our method
is that our model does not treat segmentation as a
stand-alone problem; instead, it jointly learns seg-
mentation, clustering and acoustic units from data.
The improvement on the segmentation task shown
by our model further supports the strength of the
joint learning scheme proposed in this paper.
8 Conclusion
We present a Bayesian unsupervised approach to the
problem of acoustic modeling. Without any prior
48
knowledge, this method is able to discover phonetic
units that are closely related to English phones, im-
prove upon state-of-the-art unsupervised segmenta-
tion method and generate more precise spoken term
detection performance on the TIMIT dataset. In the
future, we plan to explore phonological context and
use more flexible topological structures to model
acoustic units within our framework.
Acknowledgements
The authors would like to thank Hung-an Chang and
Ekapol Chuangsuwanich for training the English
and Thai acoustic models. Thanks to Matthew John-
son, Ramesh Sridharan, Finale Doshi, S.R.K. Brana-
van, the MIT Spoken Language Systems group and
the anonymous reviewers for helpful comments.
References
Chun-An Chan and Lin-Shan Lee. 2011. Unsupervised
hidden Markov modeling of spoken queries for spo-
ken term detection without speech recognition. In Pro-
ceedings of INTERSPEECH, pages 2141 ? 2144.
Steven B. Davis and Paul Mermelstein. 1980. Com-
parison of parametric representations for monosyllabic
word recognition in continuously spoken sentences.
IEEE Trans. on Acoustics, Speech, and Signal Pro-
cessing, 28(4):357?366.
Sorin Dusan and Lawrence Rabiner. 2006. On the re-
lation between maximum spectral transition positions
and phone boundaries. In Proceedings of INTER-
SPEECH, pages 1317 ? 1320.
Yago Pereiro Estevan, Vincent Wan, and Odette Scharen-
borg. 2007. Finding maximum margin segments in
speech. In Proceedings of ICASSP, pages 937 ? 940.
Emily Fox, Erik B. Sudderth, Michael I. Jordan, and
Alan S. Willsky. 2011. A sticky HDP-HMM with
application to speaker diarization. Annals of Applied
Statistics.
Alvin Garcia and Herbert Gish. 2006. Keyword spotting
of arbitrary words using minimal speech resources. In
Proceedings of ICASSP, pages 949?952.
John S. Garofolo, Lori F. Lamel, William M. Fisher,
Jonathan G. Fiscus, David S. Pallet, Nancy L.
Dahlgren, and Victor Zue. 1993. Timit acoustic-
phonetic continuous speech corpus.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-
ald B. Rubin. 2004. Bayesian Data Analysis. Texts
in Statistical Science. Chapman & Hall/CRC, second
edition.
James Glass. 2003. A probabilistic framework for
segment-based speech recognition. Computer Speech
and Language, 17:137 ? 152.
Sharon Goldwater. 2009. A Bayesian framework for
word segmentation: exploring the effects of context.
Cognition, 112:21?54.
Aren Jansen and Kenneth Church. 2011. Towards un-
supervised training of speaker independent acoustic
models. In Proceedings of INTERSPEECH, pages
1693 ? 1696.
Frederick Jelinek. 1976. Continuous speech recogni-
tion by statistical methods. Proceedings of the IEEE,
64:532 ? 556.
Sawit Kasuriya, Virach Sornlertlamvanich, Patcharika
Cotsomrong, Supphanat Kanokphara, and Nattanun
Thatphithakkul. 2003. Thai speech corpus for Thai
speech recognition. In Proceedings of Oriental CO-
COSDA, pages 54?61.
Kai-Fu Lee and Hsiao-Wuen Hon. 1989. Speaker-
independent phone recognition using hidden Markov
models. IEEE Trans. on Acoustics, Speech, and Sig-
nal Processing, 37:1641 ? 1648.
Chin-Hui Lee, Frank Soong, and Biing-Hwang Juang.
1988. A segment model based approach to speech
recognition. In Proceedings of ICASSP, pages 501?
504.
Kevin P. Murphy. 2007. Conjugate Bayesian analysis of
the Gaussian distribution. Technical report, University
of British Columbia.
Radford M. Neal. 2000. Markov chain sampling meth-
ods for Dirichlet process mixture models. Journal
of Computational and Graphical Statistics, 9(2):249?
265.
Yu Qiao, Naoya Shimomura, and Nobuaki Minematsu.
2008. Unsupervised optimal phoeme segmentation:
Objectives, algorithms and comparisons. In Proceed-
ings of ICASSP, pages 3989 ? 3992.
Carl Edward Rasmussen. 2000. The infinite Gaussian
mixture model. In Advances in Neural Information
Processing Systems, 12:554?560.
Odette Scharenborg, Vincent Wan, and Mirjam Ernestus.
2010. Unsupervised speech segmentation: An analy-
sis of the hypothesized phone boundaries. Journal of
the Acoustical Society of America, 127:1084?1095.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-
manuel Dupoux. 2008. Unsupervised learning of
acoustic sub-word units. In Proceedings of ACL-08:
HLT, Short Papers, pages 165?168.
Yaodong Zhang and James Glass. 2009. Unsuper-
vised spoken keyword spotting via segmental DTW
on Gaussian posteriorgrams. In Proceedings of ASRU,
pages 398 ? 403.
Yaodong Zhang, Ruslan Salakhutdinov, Hung-An Chang,
and James Glass. 2012. Resource configurable spoken
query detection using deep Boltzmann machines. In
Proceedings of ICASSP, pages 5161?5164.
49
