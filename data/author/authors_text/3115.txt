BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 106?107,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Temporal Annotation of Clinical Text  Danielle L. Mowery MS, Henk Harkema PhD, Wendy W. Chapman PhD Department of Biomedical Informatics University of Pittsburgh, Pittsburgh, PA 15260, USA dlm31@pitt.edu, heh23@pitt.edu, wec6@pitt.edu    Abstract 
We developed a temporal annotation schema that provides a structured method to capture contextual and temporal features of clinical conditions found in clinical reports. In this poster we describe the elements of the annota-tion schema and provide results of an initial annotation study on a document set compris-ing six different types of clinical reports.  
1 Introduction Distinguishing between historical and recent con-ditions is important for most tasks involving re-trieval of patients or extraction of information from textual clinical records. Various approaches can be used to determine whether a condition is historical or recent. Chapman et al (2007) developed an al-gorithm called ConText that uses trigger terms like ?history? to predict whether a condition is histori-cal. Studies of ConText show that this approach is inadequate for determining whether a condition is historical, achieving recall of 67% and precision 74% on emergency department reports. Temporal modeling methods commonly reason about the temporality of an event with respect to absolute time and other temporally related events (Zhou et al, 2006; Chambers et al, 2007). Knowing the relative or absolute time the condition occurred can be useful in determining whether the condition is historical. However, we hypothesize that many clinical conditions in clinical reports are not modi-fied by explicit temporal references. To test this hypothesis and explore other types of information that may be useful in automatically distinguishing historical from recent clinical condi-tions in dictated clinical records, we developed a temporal annotation schema that accounts for ex-plicit temporal expressions, temporal trigger terms, 
and clinical reporting acts described in reports. Three annotators applied the schema to six types of reports. We measured inter-annotator agreement scores and obtained prevalence and distribution figures for the three annotation types. 
2 Methods 2.1 Dataset Our dataset is comprised of 24 clinical reports of six types dictated at the University of Pittsburgh Medical Center during 2007: discharge summaries, surgical pathology, radiology, echocardiograms, operative gastrointestinal, and emergency depart-ment reports. A physician pre-annotated the 518 clinical conditions in the reports and marked each one as recent or historical. We developed our annotation schema using one of each report type (six reports). Annotators (authors HH, DM and WC) annotated the remain-ing 18 reports as described below.  2.2 Annotation Schema  For our temporal annotation study, each pre-annotated clinical condition was annotated with three types of information: temporal expression, trigger term, and clinical reporting act. The set of temporal expressions (TEs) is taken from Zhou et al (2006) and includes categories such as DATE AND TIME for explicit TEs and KEY EVENTS for TEs relative to significant clinical events. A given clinical condition is annotated with the category of the TE it is modified by. For exam-ple, in the sentence ?The stroke occurred on 1/5/2000?, the condition ?stroke? is annotated with category DATE AND TIME. There is also a category NO TEMPORAL EXPRESSION for annotating condi-tions that are not linked to a TE. Trigger terms (TTs) are explicit signals (words and phrases) in text other than TEs that indicate 
106
whether a condition is recent or historical (Chap-man et al, 2007). If a condition co-occurs with a TT, it is annotated with TRIGGER: YES. For exam-ple, ?pneumonia? in the sentence ?Films indicate pneumonia, which is new for this patient? is anno-tated as TRIGGER: YES because ?new? is a TT.  Error analyses of our previous studies indicate that the context in which a condition is mentioned in a report is potentially useful for prediction of a condition as recent or historical. Clinical reports consist of statements that group into segments ac-cording to the clinical reporting act (CRA) they describe, such as noting a past history and consid-ering a diagnosis. CRAs are tightly correlated with report sections; however, sections are not consis-tent, and different CRAs can occur within a single section. We distinguish 16 CRAs. Each clinical condition is annotated with one CRA. For exam-ple, the condition ?smoker? in the sentence ?She was a smoker? is annotated SOCIAL HISTORY.  2.3 Analysis  To establish the level of inter-annotator agreement, we iteratively annotated groups of six reports (one of each type). After each iteration, we refined our annotation schema and guidelines. We analyzed annotations, overall and by report type, in the fol-lowing way: 1) calculate inter-annotator kappa score, 2) measure prevalence of TT and TE catego-ries, and 3) observe distribution of CRAs. 3 Results and Discussion As shown in figure 1, average inter-annotator scores as measured by Cohen's kappa for TE, TT, and CRA (.68, .82 and .72 respectively) reached acceptable levels after three iterations and are ex-pected to rise further with increased annotation experience and understanding of the guidelines. Table 1 shows the prevalence of TEs and TTs across six report types, where prevalence is defined as the frequency of TE or TT found in a given re-port. Use of TEs across report types ranged from 0% to 52% whereas TTs were found less often at 0% to 34% by report genre. Table 2 plots the cor-relation between the CRA assigned to a clinical condition and the condition's classification as re-cent or historical. We found that there is a strong correlation for the most commonly occurring clini-cal reporting acts (PH, PR, and PO). We are there-fore optimistic that CRAs can serve as an 
informative feature for a statistical recent/historical classifier. 
kappa 
0
1
1 2 3
i t e r a t i o n
TE
TT
CRA
 Figure 1. Average Cohen?s kappa agreement for 3 iterations.   DS E ED GI RAD SP O TE 48(52) 0(0) 51(20) 2(10) 1(5) 8(36) 110(21) TT 32(34) 0(0) 54(21) 1(5) 0(0) 6(27) 93(17)  Table 1. Prevalence, count (%), of TE and TT across report types, overall. DS: discharge summary, E: echocardiogram, ED: emergency department, GI: operative gastrointestinal, RAD: radiology, SP: surgical pathology and O: overall.   
0%
100%
 
P
H
P
R
H
P
I
P
O
A
l
l
 
C
C
S
H
P
F
P
M
x
D
x
P
T
x
M
d
x
R
P
R
M
D
C
D
x
C R A
Recent
Historical
 Table 2. Historical/recent distribution of CRAs. PH: Past his-tory, PR, Patient reporting, HPI: History of present illness, PO: Physician observing, All: Allergies, CC: Chief complaint, SH: Social history, FH: Family history, PF: Past Finding, PMx, Past medication, Dx: Diagnosis, PTx: Plan treatment, Mdx: Prescribing medication, RP: Referring problem, RMD: Refer to MD, CDx: Considering diagnosis.  The finding that many conditions are associated with neither a TE nor a TT and study of ConText?s limitations with such categories at the scope of the sentence suggests that additional features are nec-essary to discern a condition as recent or historical. Whereas temporality in discourse may follow a sequential chronology as narrative unfolds, refer-ences to past instances within clinical text are not easily resolved. We are optimistic that CRAs may help this issue and will focus our study to evaluate whether these three features are sufficient together. References  L. Zhou, G. B. Melton, S. Parsons, G. Hripcsak. 2006. A temporal constraint structure for extracting tempo-ral information from clinical narrative. Journal of Biomedical Informatics, 39(4):424-439 N. Chambers, S. Wang, D. Jurafsky. 2007. Classifying Temporal Relations Between Events. In: ACL-07.  W. Chapman, D. Chu, J. N. Dowling. 2007. ConText: An Algorithm for Identifying Contextual Features from Clinical Text. In: ACL-07. 
107
Proceedings of the Workshop on BioNLP, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Distinguishing Historical from Current Problems in Clinical      
Reports?Which Textual Features Help?  
 
Danielle L. Mowery MS, Henk Harkema PhD, John N. Dowling MS MD,  
Jonathan L. Lustgarten PhD, Wendy W. Chapman PhD 
Department of Biomedical Informatics 
University of Pittsburgh, Pittsburgh, Pa 15260, USA 
dlm31@pitt.edu, heh23@pitt.edu, dowling@pitt.edu, jll47@pitt.edu, wec6@pitt.edu 
 
 
Abstract 
Determining whether a condition is historical 
or recent is important for accurate results in 
biomedicine. In this paper, we investigate four 
types of information found in clinical text that 
might be used to make this distinction. We 
conducted a descriptive, exploratory study us-
ing annotation on clinical reports to determine 
whether this temporal information is useful 
for classifying conditions as historical or re-
cent. Our initial results suggest that few of 
these feature values can be used to predict 
temporal classification. 
1 Introduction 
Clinical applications for decision support, biosur-
veillance and quality of care assessment depend on 
patient data described in unstructured, free-text 
reports.  For instance, patient data in emergency 
department reports contain valuable indicators for 
biosurveillance applications that may provide early 
signs and symptoms suggestive of an outbreak. 
Quality assurance departments can use free-text 
medical record data to assess adherence to quality 
care guidelines, such as determining whether an 
MI patient was given an aspirin within twenty-four 
hours of arrival. In either application, one must 
consider how to address the question of time, but 
each of the applications requires a different level of 
temporal granularity: the biosurveillance system 
needs a coarse-grained temporal model that dis-
cerns whether the signs and symptoms are histori-
cal or recent. In contrast, the quality assurance 
system needs a fine-grained temporal model to 
identify the admission event, when (or if) aspirin 
was given, and the order and duration of time be-
tween these events. One important problem in nat-
ural language processing is extracting the appro-
priate temporal granularity for a given task. 
Many solutions exist for extracting temporal in-
formation, and each is designed to address ques-
tions of various degrees of temporal granularity, 
including determining whether a condition is his-
torical or recent, identifying explicit temporal ex-
pressions, and identifying temporal relations 
among events in text. (Chapman et al, 2007; Zhou 
et al, 2008; Irvine et al, 2008;  Verhagen and Pus-
tejovsky, 2008; Bramsen et al, 2006). We pre-
viously extended the NegEx algorithm in ConText, 
a simple algorithm that relies on lexical cues to 
determine whether a condition is historical or re-
cent (Chapman et al, 2007). However, ConText 
performs with moderate recall (76%) and precision 
(75%) across different report types implying that 
trigger terms and simple temporal expressions are 
not sufficient for the task of identifying historical 
conditions.  
In order to extend work in identifying historical 
conditions, we conducted a detailed annotation 
study of potentially useful temporal classification 
features for conditions found in six genres of clini-
cal text. Our three main objectives were: (1) cha-
racterize the temporal similarity and differences 
found in different genres of clinical text; (2) de-
termine which features successfully predict wheth-
er a condition is historical, and (3) compare 
ConText to machine learning classifiers that ac-
count for this broader set of temporal features. 
2 Temporality in Clinical Text 
For several decades, researchers have been study-
ing temporality in clinical records (Zhou and 
Hripcsak, 2007). Readers use a variety of clues to 
distinguish temporality from the clinical narrative, 
and we wanted to identify features from other tem-
10
poral models that may be useful for determining 
whether a condition is historical or recent.  
There are a number of automated systems for 
extracting, representing, and reasoning time in a 
variety of text. One system that emerged from the 
AQUAINT workshops for temporal modeling of 
newspaper articles is TARSQI. TARSQI processes 
events annotated in text by anchoring and ordering 
them with respect to nearby temporal expressions 
(Verhagen and Pustejovsky, 2008). A few recent 
applications, such as TimeText and TN-TIES 
(Zhou et al, 2008; Irvine et al, 2008), identify 
medically relevant events from clinical texts and 
use temporal expressions to order the events. One 
method attempts to order temporal segments of 
clinical narratives (Bramsen et al, 2006). One key 
difference between these previous efforts and our 
work is that these systems identify all temporal 
expressions from the text and attempt to order all 
events. In contrast, our goal is to determine wheth-
er a clinical condition is historical or recent, so we 
focus only on temporal information related to the 
signs, symptoms, and diseases described in the 
text. Therefore, we ignore explicit temporal ex-
pressions that do not modify clinical conditions. If 
a condition does not have explicit temporal mod-
ifiers, we still attempt to determine the historical 
status for that condition (e.g., ?Denies cough?). In 
order to improve the ability to determine whether a 
condition is historical, we carried out this annota-
tion study to identify any useful temporal informa-
tion related to the clinical conditions in six clinical 
genres. Building on work in this area, we explored 
temporal features used in other temporal annota-
tion studies. 
TimeML is a well-known standard for complex, 
temporal annotation. TimeML supports the annota-
tion of events defined as ?situations that happen or 
occur? and temporal expressions such as dates and 
durations in order to answer temporal questions 
about these events and other entities in news text 
(Saur??, et al, 2006). One notable feature of the 
TimeML schema is its ability to capture verb tense 
such as past or present and verb aspect such as 
perfective or progressing. We annotated verb tense 
and aspect in medical text according to the Time-
ML standard. 
Within the medical domain, Zhou et al (2006) 
developed an annotation schema used to identify 
temporal expressions and clinical events. They 
measured the prevalence of explicit temporal ex-
pressions and key medical events like admission or 
transfer found in discharge summaries. We used 
the Zhou categorization scheme to explore tempor-
al expressions and clinical events across genres of 
reports. 
A few NLP systems rely on lexical cues to ad-
dress time. MediClass is a knowledge-based sys-
tem that classifies the content of an encounter 
using both free-text and encoded information from 
electronic medical records (Hazelhurst et al, 
2005). For example, MediClass classifies smoking 
cessation care delivery events by identifying the 
status of a smoker as continued, former or history 
using words like continues. ConText, an extension 
of the NegEx algorithm, temporally classifies con-
ditions as historical, recent, or hypothetical using 
lexical cues such as history, new, and if, respec-
tively (Chapman et al, 2007). Drawing from these 
applications, we used state and temporal trigger 
terms like active, unchanged, and history to cap-
ture coarse, temporal information about a condi-
tion.  
Temporal information may also be implied in 
the document structure, particularly with regards to 
the section in which the condition appears. SecTag 
marks explicit and implicit sections found 
throughout patient H&P notes (Denny et al, 2008). 
We adopted some section headers from the SecTag 
terminology to annotate sections found in reports.  
Our long-term goal is to build a robust temporal 
classifier for information found in clinical text 
where the output is classification of whether a con-
dition is historical or recent (historical categoriza-
tion). An important first step in classifying 
temporality in clinical text is to identify and cha-
racterize temporal features found in clinical re-
ports. Specifically, we aim to determine which 
expressions or features are predictive of historical 
categorization of clinical conditions in dictated 
reports. 
3 Historical Assignment and Temporal 
Features 
We conducted a descriptive, exploratory study of 
temporal features found across six genres of clini-
cal reports. We had three goals related to our task 
of determining whether a clinical condition was 
historical or recent. First, to develop a temporal 
classifier that is generalizable across report types, 
we compared temporality among different genres 
11
of clinical text. Second, to determine which fea-
tures predict whether a condition is historical or 
recent, we observed common rules generated by 
three different rule learners based on manually an-
notated temporal features we describe in the fol-
lowing section. Finally, we compared the 
performance of ConText and automated rule learn-
ers and assessed which features may improve the 
ConText algorithm.  
Next, we describe the temporal features we as-
sessed for identification of historical signs, symp-
toms, or diseases, including temporal expressions, 
lexical cues, verb tense and aspect, and sections.  
(1) Temporal Expressions: Temporal expres-
sions are time operators like dates (May 5th 2005) 
and durations (for past two days), as well as clini-
cal processes related to the encounter (discharge, 
transfer). For each clinical condition, we annotated 
whether a temporal expression modified it and, if 
so, the category of temporal expression. We used 
six major categories from Zhou et al (2006) in-
cluding: Date and Time, Relative Date and Time, 
Durations, Key Events, Fuzzy Time, and No Tem-
poral Expression. These categories also have 
types. For instance, Relative Date and Time has a 
type Yesterday, Today or Tomorrow.  For the con-
dition in the sentence ?The patient had a stroke in 
May 2006?, the temporal expression category is 
Date and Time with type Date. Statements without 
a temporal expression were annotated No Tempor-
al Expression with type N/A. 
(2) Tense and Aspect: Tense and aspect define 
how a verb is situated and related to a particular 
time. We used TimeML Specification 1.2.1 for 
standardization of tense and aspect where exam-
ples of tense include Past or Present and aspect 
may be Perfective, Progressive, Both or None as 
found in Saur??, et al (2006). We annotated the 
verb that scoped a condition and annotated its tense 
and aspect. The primary verb may be a predicate 
adjective integral to interpretation of the condition 
(Left ventricle is enlarged), a verb preceding the 
condition (has hypertension), or a verb following a 
condition (Chest pain has resolved). In ?her chest 
pain has resolved,? we would mark ?has resolved? 
with tense Present and aspect Perfective. State-
ments without verbs (e.g., No murmurs) would be 
annotated Null for both.  
(3) Trigger Terms: We annotated lexical cues 
that provide temporal information about a condi-
tion. For example, in the statement, ?Patient has 
past history of diabetes,? we would annotate ?his-
tory? as Trigger Term: Yes and would note the ex-
act trigger term. 
     (4) Sections: Sections are ?clinically meaning-
ful segments which act independently of the 
unique narrative? for a patient (Denny et al 2008). 
Examples of report sections include Review of Sys-
tems (Emergency Department), Findings (Opera-
tive Gastrointestinal and Radiology) and 
Discharge Diagnosis (Emergency Department and 
Discharge Summary).  
We extended Denny?s section schema with ex-
plicit, report-specific section headers not included 
in the original terminology. Similar to Denny, we 
assigned implied sections in which there was an 
obvious change of topic and paragraph marker. For 
instance, if the sentence ?the patient is allergic to 
penicillin? followed the Social History section, we 
annotated the section as Allergies, even if there 
was not a section heading for allergies. 
4 Methods 
4.1 Dataset Generation 
We randomly selected seven reports from each of 
six genres of clinical reports dictated at the Univer-
sity of Pittsburgh Medical Center during 2007 
These included Discharge Summaries, Surgical 
Pathology, Radiology, Echocardiograms, Opera-
tive Gastrointestinal, and Emergency Department 
reports. The dataset ultimately contained 42 clini-
cal reports and 854 conditions. Figure 1 show our 
annotation process, which was completed in 
GATE, an open-source framework for building 
NLP systems (http://gate.ac.uk/). A physician 
board-certified in internal medicine and infectious 
diseases annotated all clinical conditions in the set 
and annotated each condition as either historical or 
recent. He used a general guideline for annotating 
a condition as historical if the condition began 
more than 14 days before the current encounter and 
as recent if it began or occurred within 14 days or 
during the current visit. However, the physician 
was not bound to this definition and ultimately 
used his own judgment to determine whether a 
condition was historical. 
Provided with pre-annotated clinical conditions 
and blinded to the historical category, three of the 
authors annotated the features iteratively in groups 
of six (one of each report type) using guidelines we 
12
developed for the first two types of temporal fea-
tures (temporal expressions and trigger terms.) 
Between iterations, we resolved disagreements 
through discussion and updated our guidelines. 
Cohen?s kappa for temporal expressions and trig-
ger terms by the final iteration was at 0.66 and 0.69 
respectively. Finally, one author annotated sec-
tions, verb tense, and aspect.  Cases in which as-
signing the appropriate feature value was unclear 
were resolved after consultation with one other 
author-annotator.  
4.2 Data Analysis 
 
We represented each condition as a vector with  
temporal features and their manually-assigned val-
ues as input features for predicting the binary out-
come value of historical or recent. We trained three 
rule learning algorithms to classify each condition 
as historical or recent: J48 Decision Tree, Ripper, 
and Rule Learner (RL) (Witten and Frank, 2005; 
Clearwater and Provost, 1990). Rule learners per-
form well at classification tasks and provide expli-
cit rules that can be viewed, understood, and 
potentially implemented in existing rule-based ap-
plications. We used Weka 3.5.8, an openly-
available machine learning application for predic-
tion modeling, to implement the Decision Tree 
(J48) and Ripper (JRip) algorithms, and we applied 
an in house version of RL retrieved from 
www.dbmi.pitt.edu\probe. For all rule learners, we 
used the default settings and ran ten-fold cross-
validation. The J48 algorithm produces mutually 
exclusive rules for predicting the outcome value. 
Thus, two rules cannot cover or apply to any one 
case. In contrast, both JRip and RL generate non-
mutually-exclusive rules for predicting the out-
come value. Although J48 and JRip are sensitive to 
bias in outcome values, RL accounts for skewed 
distribution of the data.  
We also applied ConText to the test cases to 
classify them as historical or recent. ConText looks 
for trigger terms and a limited set of temporal ex-
pressions within a sentence. Clinical conditions 
within the scope of the trigger terms are assigned 
the value indicated by the trigger terms (e.g., his-
torical for the term history). Scope extends from 
the trigger term to the end of the sentence or until 
the presence of a termination term, such as pre-
senting. For instance, in the sentence ?History of 
CHF, presenting with chest pain,? CHF would be 
annotated as historical.  
5 Evaluation 
To characterize the different reports types, we es-
tablished the overall prevalence and proportion of 
conditions annotated as historical for each clinical 
report genre.  We assessed the prevalence of each 
feature (temporal expressions, trigger terms, tense 
and aspect, and sections) by report genre to deter-
mine the level of similarity or difference between 
genres. To determine which features values are 
predictive of whether a condition is historical or 
recent, we observed common rules found by more 
than one rule learning algorithm. Amongst com-
mon rules, we identified new rules that could im-
prove the ConText algorithm.  
We also measured predictive performance with 
95% confidence intervals of the rule learners and 
ConText by calculating overall accuracy, as well as 
recall and precision for historical classifications 
and recall and precision for recent classifications.  
Table 1 describes equations for the evaluation me-
trics. 
 
Table 1. Description of evaluation metrics. RLP = rule 
learner prediction. RS = Reference Standard 
 
 
Figure 1. Annotation process for dataset and objectives 
for evaluation. 
13
Recall:                 number of TP              
(number of TP + number of FN) 
 
Precision:           number of TP              
(number of TP + number of FP) 
 
Accuracy:   number of instances correctly classified 
                      total number of possible instances  
6 Results 
Overall, we found 854 conditions of interest across 
all six report genre. Table 2 illustrates the preva-
lence of conditions across report genres. Emergen-
cy Department reports contained the highest 
concentration of conditions. Across report genres, 
87% of conditions were recent (741 conditions). 
All conditions were recent in Echocardiograms, in 
contrast to Surgical Pathology reports in which 
68% were recent.  
 
Table 2. Prevalence and count of conditions by temporal 
category and report genre. DS = Discharge Summary, 
Echo = Echocardiogram, ED = Emergency Department, 
GI = Operative Gastrointestinal, RAD = Radiology and 
SP = Surgical Pathology. (%) = percent; Ct = count.  
 
6.1 Prevalence of Temporal Features 
Table 3 shows that most conditions were not mod-
ified by a temporal expression or a trigger term. 
Conditions were modified by a temporal expres-
sion in Discharge Summaries more often than in 
other report genres. Similarly, Surgical Pathology 
had the highest prevalence of conditions modified 
by a trigger term. Operative Gastrointestinal and 
Radiology reports showed the lowest prevalence of 
both temporal expressions and trigger terms. Nei-
ther temporal expressions nor trigger terms oc-
curred in Echocardiograms. Overall, the 
prevalence of conditions scoped by a verb varied 
across report types ranging from 46% (Surgical 
Pathology) to 81% (Echocardiogram). 
Table 3. Prevalence of conditions modified by temporal 
features. All conditions were assigned a section and are 
thereby excluded. TE = temporal expression; TT = trig-
ger term; V = scoped by verb.  
 
6.2 Common Rules 
Rule learners generated a variety of rules. The J48 
Decision Tree algorithm learned 27 rules, six for 
predicting conditions as historical and the remain-
ing for classifying the condition as recent. The 
rules predominantly incorporated the trigger term 
and verb tense and aspect feature values. JRip 
learned nine rules, eight for classifying the histori-
cal temporal category and one ?otherwise? rule for 
the majority class. The JRip rules most heavily 
incorporated the section feature. The RL algorithm 
found 79 rules, 18 of which predict the historical 
category. Figure 2 illustrates historical rules 
learned by each rule learner. JRip and RL pre-
dicted the following sections alone can be used to 
predict a condition as historical: Past Medical His-
tory, Allergies and Social History. Both J48 and 
RL learned that trigger terms like previous, known 
and history predict historical. There was only one 
common, simple rule for the historical category 
found amongst all three learners: the trigger term 
no change predicts the historical category. All al-
gorithms learned a number of rules that include 
two features values; however, none of the com-
pound rules were common amongst all three algo-
rithms.    
 
Figure 2. Historical rules learned by each rule learner 
algorithm. Black dots represent simple rules whereas 
triangles represent compound rules. Common rules 
shared by each algorithm occur in the overlapping areas 
of each circle. 
14
6.3 Predictive Performance 
Table 4 shows predictive performance for each 
rule learner and for ConText. The RL algorithm 
outperformed all other algorithms in almost all 
evaluation measures. The RL scores were com-
puted based on classifying the 42 cases (eight his-
torical) for which the algorithm did not make a 
prediction as recent. ConText and J48, which ex-
clusively relied on trigger terms, had lower recall 
for the historical category.  
All of the rule learners out-performed ConText. 
JRip and RL showed substantially higher recall for 
assigning the historical category, which is the most 
important measure in a comparison with ConText, 
because ConText assigns the default value of re-
cent unless there is textual evidence to indicate a 
historical classification. Although the majority 
class baseline shows high accuracy due to high 
prevalence of the recent category, all other classifi-
ers show even higher accuracy, achieving fairly 
high recall and precision for the historical cases 
while maintaining high performance on the recent 
category. 
 
Table 4. Performance results with 95% confidence in-
tervals for three rule learners trained on manually anno-
tated features and ConText, which uses automatically 
generated features. Bolded values do not have overlap-
ping confidence intervals with ConText. MCB = Ma-
jority Class Baseline (recent class)   
 
7 Discussion 
Our study provides a descriptive investigation of 
temporal features found in clinical text. Our first 
objective was to characterize the temporal similari-
ties and differences amongst report types. We 
found that the majority of conditions in all report 
genres were recent conditions, indicating that a 
majority class classifier would produce an accura-
cy of about 87% over our data set.  According to 
the distributions of temporal category by report 
genre (Table 2), Echocardiograms exclusively de-
scribe recent conditions. Operative Gastrointestinal 
and Radiology reports contain similar proportions 
of historical conditions (9% and 6%). Echocardio-
grams appear to be most similar to Radiology re-
ports and Operative Gastrointestinal reports, which 
may be supported by the fact that these reports are 
used to document findings from tests conducted 
during the current visit. Emergency Department 
reports and Discharge Summaries contain similar 
proportions of historical conditions (17% and 19% 
respectively), which might be explained by the fact 
that both reports describe a patient?s temporal pro-
gression throughout the stay in the Emergency De-
partment or the hospital.  
Surgical Pathology reports may be the most 
temporally distinct report in our study, showing the 
highest proportion of historical conditions. This 
may seem counter-intuitive given that Surgical 
Pathology reports also facilitate the reporting of 
findings described from a recent physical speci-
men. However, we had a small sample size (28 
conditions in seven reports), and most of the his-
torical conditions were described in a single ad-
dendum report. Removing this report decreased the 
prevalence of historical conditions to 23% (3/13).  
Discharge Summaries and Emergency Depart-
ment reports displayed more variety in the ob-
served types of temporal expressions (9 to 14 
subtypes) and trigger terms (10 to 12 terms) than 
other report genres. This is not surprising consider-
ing the range of events described in these reports. 
Other reports tend to have between zero and three 
subtypes of temporal expressions and zero and 
seven different trigger terms. In all report types, 
temporal expressions were mainly subtype past, 
and the most frequent trigger term was history. 
Our second objective was to identify which fea-
tures predict whether a condition is historical or 
recent. Due to high prevalence of the recent cate-
gory, we were especially interested in discovering 
temporal features that predict whether a condition 
is historical. With one exception (date greater than 
four weeks prior to the current visit), temporal ex-
pression features always occurred in compound 
rules in which the temporal expression value had to 
co-occur with another feature value. For instance, 
any temporal expression in the category key event 
had to also occur in the secondary diagnosis sec-
tion to classify the condition as historical. For ex-
15
ample, in ?SECONDARY DIAGNOSIS: Status 
post Coronary artery bypass graft with complica-
tion of mediastinitis? the key event is the coronary 
artery bypass graft, the section is secondary diag-
nosis, and the correct classification is historical.  
Similarly, verb tense and aspect were only use-
ful in conjunction with other feature values. One 
rule predicted a condition as historical if the condi-
tion was modified by the trigger term history and 
fell within the scope of a present tense verb with 
no aspect. An example of this is ?The patient is a 
50 year old male with history of hypertension.? 
Intuitively, one would think that a past tense verb 
would always predict historical; however, we 
found the presence of a past tense verb with no 
aspect was a feature only when the condition was 
in the Patient History section.  Sometimes the ab-
sence of a verb in conjunction with another feature 
value predicted a condition as historical. For ex-
ample, in the sentences ?PAST MEDICAL 
HISTORY: History of COPD. Also diabetes?? 
also functioned as a trigger term that extended the 
scope of a previous trigger term, history, in the 
antecedent sentence.  
A few historical trigger terms were discovered 
as simple rules by the rule learners: no change, 
previous, known, status post, and history. A few 
rules incorporated both a trigger term and a partic-
ular section header value. One rule predicted his-
torical if the trigger term was status post and the 
condition occurred in the History of Present Illness 
section. This rule would classify the condition 
CABG as historical in ?HISTORY OF PRESENT 
ILLNESS: The patient is...status post CABG.? 
One important detail to note is that a number of the 
temporal expressions categorized as Fuzzy Time 
also act as trigger terms, such as history and status 
post?both of which were learned by J48. A histor-
ical trigger term did not always predict the catego-
ry historical. In the sentence ?No focal sensory or 
motor deficits on history,? history may suggest that 
the condition was not previously documented, but 
was interpreted as not presently identified during 
the current physical exam.   
Finally, sections appeared in the majority of 
JRip and RL historical rules: 4/8 simple rules and 
13/18 compound rules. A few sections were con-
sistently classified as historical: Past Medical His-
tory, Allergies, and Social History.  One important 
point to address is that these sections were manual-
ly annotated.  
Our results revealed a few unexpected observa-
tions. We found at least two trigger terms indicated 
in the J48 rules, also and status post, which did not 
have the same predictive ability across report ge-
nres.  For instance, in the statement ?TRANSFER 
DIAGNOSIS: status post coiling for left posterior 
internal carotid artery aneurysm,? status post indi-
cates the reason for the transfer as an inpatient 
from the Emergency Department and the condition 
is recent. In contrast, status post in a Surgical Pa-
thology report was interpreted to mean historical 
(e.g., PATIENT HISTORY: Status post double 
lung transplant for COPD.) In these instances, 
document knowledge of the meaning of the section 
may be useful to resolve these cases.  
One other unexpected finding was that the trig-
ger term chronic was predictive of recent rather 
than historical. This may seem counterintuitive; 
however, in the statement ?We are treating this as 
chronic musculoskeletal pain with oxycodone?, the 
condition is being referenced in the context of the 
reason for the current visit. Contextual information 
surrounding the condition, in this case treating or 
administering medication for the condition, may 
help discriminate several of these cases.  
Our third objective was to assess ConText in re-
lation to the rules learned from manually annotated 
temporal features. J48 and ConText emphasized 
the use of trigger terms as predictors of whether a 
condition was historical or recent and performed 
with roughly the same overall accuracy. JRip and 
RL learned rules that incorporated other feature 
values including sections and temporal expres-
sions, resulting in a 12% increase in historical re-
call over ConText and a 31% increase in historical 
recall over J48. 
Many of the rules we learned can be easily ex-
tracted and incorporated into ConText (e.g., trigger 
terms previous and no change). The ConText algo-
rithm largely relies on the use of trigger terms like 
history and one section header, Past Medical His-
tory. By incorporating additional section headers 
that may strongly predict historical, ConText could 
potentially predict a condition as historical when a 
trigger term is absent and the header title is the 
only predictor as in the case of ?ALLERGIES: 
peanut allergy?. Although these sections header 
may only be applied to Emergency Department 
and Discharge Summaries, trigger terms and tem-
poral expressions may be generalizable across ge-
nre of reports.  Some rules do not lend themselves 
16
to ConText?s trigger-term-based approach, particu-
larly those that require sophisticated representation 
and reasoning. For example, ConText only reasons 
some simple durations like several day history. 
ConText cannot compute dates from the current 
visit to reason that a condition occurred in the past 
(e.g., stroke in March 2000).  The algorithm per-
formance would gain from such a function; how-
ever, such a task would greatly add to its 
complexity.   
8 Limitations 
The small sample size of reports and few condi-
tions found in three report genres (Operative Ga-
strointestinal, Radiology, and Surgical Pathology) 
is a limitation in this study. Also, annotation of 
conditions, temporal category, sections, verb tense 
and aspect were conducted by a single author, 
which may have introduced bias to the study. Most 
studies on temporality in text focus on the temporal 
features themselves. For instance, the prevalence 
of temporal expressions reported by Zhou et al 
(2006) include all temporal expressions found 
throughout a discharge summary, whereas we an-
notated only those expressions that modified the 
condition. This difference makes comparing our 
results to other published literature challenging.  
9 Future Work  
Although our results are preliminary, we be-
lieve our study has provided a few new insights 
that may help improve the state of the art for his-
torical categorization of a condition. The next step 
to building on this work includes automatically 
extracting the predictive features identified by the 
rule learners. Some features may be easier to ex-
tract than others. Since sections appear to be strong 
indicators for historical categorization we may start 
by implementing the SecTag tagger. Often a sec-
tion header does not exist between text describing 
the past medical history and a description of the 
current problem, so relying merely on the section 
heading is not sufficient. The SecTag tagger identi-
fies both implicit and explicit sections and may 
prove useful for this task. To our knowledge, Sec-
Tag was only tested on Emergency Department 
reports, so adapting it to other report genres will be 
necessary. Both JRip and RL produced high per-
formance, suggesting a broader set of features may 
improve historical classification; however, because 
these features do not result in perfect performance, 
there are surely other features necessary for im-
proving historical classification. For instance, hu-
mans use medical knowledge about conditions that 
are inherently chronic or usually experienced over 
the course of a patient?s life (i.e., HIV, social ha-
bits like smoking, allergies etc). Moreover, physi-
cians are able to integrate knowledge about chronic 
conditions with understanding of the patient?s rea-
son for visit to determine whether a chronic condi-
tion is also a recent problem. An application that 
imitated experts would need to integrate this type 
of information. We also need to explore adding 
features captured at the discourse level, such as 
nominal and temporal coreference. We have begun 
work in these areas and are optimistic that they 
will improve historical categorization.  
10 Conclusion 
Although most conditions in six clinical report ge-
nres are recent problems, identifying those that are 
historical is important in understanding a patient?s 
clinical state. A simple algorithm that relies on lex-
ical cues and simple temporal expressions can 
classify the majority of historical conditions, but 
our results indicate that the ability to reason with 
temporal expressions, to recognize tense and as-
pect, and to place conditions in the context of their 
report sections will improve historical classifica-
tion. We will continue to explore other features to 
predict historical categorization. 
 
Acknowledgments 
 
This work was funded by NLM grant 1 
R01LM009427-01, ?NLP Foundational Studies 
and Ontologies for Syndromic Surveillance from 
ED Reports?.  
References 
 
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee, 
and Regina Barzilay. 2006. Finding Temporal Order 
in Discharge Summaries. AMIA Annu Symp Proc. 
2006; 81?85 
Wendy W Chapman, David Chu, and John N. Dowling. 
2007. ConText: An Algorithm for Identifying Contex-
tual Features from Clinical Text. Association for 
Computational Linguistics, Prague, Czech Republic 
17
Scott H. Clearwater and Foster J. Provost. 1990. RL4: A 
Tool for Knowledge-Based Induction. Tools for Ar-
tificial Intelligence, 1990. Proc of the 2nd Intern 
IEEE Conf: 24-30. 
Joshua C. Denny, Randolph A. Miller, Kevin B. John-
son, and Anderson Spickard III. 2008. Development 
and Evaluation of a Clinical Note Section Header 
Terminology. SNOMED. AMIA 2008 Symp. Pro-
ceedings: 156-160. 
Brian Hazlehurst, H. Robert Frost, Dean F. Sittig, and 
Victor J. Stevens. 2005. MediClass: A system for de-
tecting and classifying encounter-based clinical 
events in any electronic medical record. J Am Med 
Inform Assoc 12(5): 517-29 
Ann K. Irvine, Stephanie W. Haas, and Tessa Sullivan. 
2008. TN-TIES: A System for Extracting Temporal 
Information from Emergency Department Triage 
Notes. AMIA 2008 Symp Proc: 328-332. 
Roser Saur??, Jessica Littman, Bob Knippen, Robert 
Gaizauskas, Andrea Setzer, and James Pustejovsky. 
2006. TimeML Annotation Guidelines Version 1.2.1. 
at: 
http://www.timeml.org/site/publications/timeMLdocs
/annguide_1.2.1.pdf 
Marc Verhagen and James Pustejovsky. 2008. Temporal 
Processing with TARSQI Toolkit. Coling 2008: Com-
panion volume ? Posters and Demonstrations, Man-
chester, 189?192 
Ian H. Witten and Eibe Frank. 2005. Data Mining: 
Practical machine learning tools and techniques, 2nd 
Edition, Morgan Kaufmann, San Francisco, 2005. 
Li Zhou, Genevieve B. Melton, Simon Parsons and 
George Hripcsak. 2006. A temporal constraint struc-
ture for extracting temporal information from clini-
cal narrative. J Biomed Inform 39(4): 424-439. 
Li Zhou and George Hripcsak. 2007. Temporal reason-
ing with medical data--a review with emphasis on 
medical natural language processing. J Biomed In-
form Apr; 40(2):183-202. 
Li Zhou, Simon Parson, and George Hripcsak. 2008. 
The Evaluation of a Temporal Reasoning System in 
Processing Discharge Summaries. J Am Med Inform 
Assoc 15(1): 99?106.  
 
18
Proceedings of the Workshop on BioNLP, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
ONYX: A System for the Semantic Analysis of Clinical Text  Lee M. Christensen, Henk Harkema, Peter J. Haug,  Jeannie Y. Irwin, Wendy W. Chapman Department of Biomedical Informatics University of Pittsburgh University of Utah Pittsburgh, PA 15214, USA Salt Lake City, Utah, 84143, USA lmc61 heh23 rey3 wec6 @pitt.edu Peter.Haug@intermountainmail.org     Abstract 
This paper introduces ONYX, a sentence-level text analyzer that implements a number of innovative ideas in syntactic and semantic analysis. ONYX is being developed as part of a project that seeks to translate spoken dental examinations directly into chartable findings. ONYX integrates syntax and semantics to a high degree. It interprets sentences using a combination of probabilistic classifiers, graphical unification, and semantically anno-tated grammar rules. In this preliminary evaluation, ONYX shows inter-annotator agreement scores with humans of 86% for as-signing semantic types to relevant words, 80% for inferring relevant concepts from words, and 76% for identifying relations between concepts. 
1 Introduction This paper describes ONYX, a sentence-level medical language analyzer currently under devel-opment at the University of Pittsburgh. Since ONYX contains a number of innovative ideas at an early stage of development, the objective of this paper is to paint a broad picture of ONYX and to present preliminary evaluation results rather than analyzing any single aspect in detail.  ONYX is being developed as part of a project aimed at extracting information from spoken dental examinations. Currently, dental findings must be charted after an exam is completed or may be charted by an assistant who acts as a transcription-ist during the exam. Our goal is to design a system capable of automatically extracting chartable find-
ings directly from spoken exams, potentially also supporting automated decision support and quality control. We are also developing tools to enable the system to be ported to other clinical domains and settings.   Extracting information from unedited speech tran-scriptions presents a number of challenges. Sen-tences may be fragmented or telegraphic, and much of the speech may be irrelevant for our pur-poses. The following example illustrates some of these difficulties:  "Okay. Okay. Open. Okay. No. 1 is missing. Two oc-clusal distal amalgam. Actually, make that occlusal. Also, one palatal amalgam. Can you close just slightly? And perfect. Okay, now open again."  The relevant findings in this example are that tooth number one is missing and tooth number two has amalgam fillings on the occlusal and palatal sur-faces. Our ultimate challenge is to create a system that can recognize relevant sentences and perform competently in the face of the inherent ambiguity and noise commonly found in conversational speech. ONYX does not yet address all of these challenges, although we have clear directions we are pursuing as described in the Future Work sec-tion of this paper. Our goal in this paper is to de-scribe the current state of ONYX and the innovations we feel will enable it to be adapted to complex NLP tasks in the future. 2 Overview of ONYX  ONYX is the middle component of a pipelined architecture as illustrated in figure 1. The entry point to this architecture is a speech-to-text ana-lyzer, which takes input from a microphone worn 
19
by the dentist and produces a transcription that ONYX analyzes for semantic content. ONYX's output is then passed to a discourse analyzer that applies dental knowledge to assemble ONYX's sentence-level semantic representations into chartable exam findings.  
 Figure 1. Speech-to-chart pipeline.  ONYX looks for dental conditions such as caries, fractures and translucencies; restorations such as fillings and crowns; tooth locations; and modifiers such as tooth part, tooth surface, and condition ex-tent. It produces templates of words and concepts. Table 1 shows a summary of four templates (Den-tal Condition, Tooth Location, Surface and State) representing the meaning of "eight mesio might have a slight translucency."   
 ONYX?s interpretations are represented as binary predicates that take the templates as arguments (for convenience, only the summary concepts from the templates are shown): 
ConditionAt(*translucency, *numberEight) & LocationHasSurface(*numberEight, *mesial) &  StateOf(*translucency, *possible)   ONYX builds on ideas from MPLUS (Christensen et al 2002), which was used primarily to interpret radiology reports. MPLUS uses Bayesian networks (BNs) to produce filled templates. Through a train-ing process, words from the corpus of training documents are manually associated with states of terminal nodes in a BN, and concepts are associ-ated with states of nonterminal nodes. When MPLUS interprets a sentence, it instantiates the BNs with words from the sentence and infers the most probable concepts consistent with those words. It then generates templates filled with those words and concepts.   BNs have proven useful in semantic analysis (e.g. Ranum 1989, Koehler 1998, Christensen 2002): their performance degrades gracefully in the face of various types of lexical and syntactic noise. The main disadvantage with using BNs is their inherent computational complexity. ONYX employs a se-mantics-intensive form of parsing, interpreting each phrase as it is constructed rather than waiting until the syntactic analysis is completed to do the interpretation. For this reason we have developed an experimental probabilistic classifier for ONYX called a Concept Model (CM). CMs support a tree-structured representation of related words and con-cepts (figure 5), structurally similar to the BNs used by MPLUS, but using a more efficient model of computation. In essence CMs are trees of Na?ve Bayes classifiers, although they contain enhance-ments, not described in this study, which in general make them more accurate than strict Na?ve Bayes. Each node together with its children constitutes a single classifier. When a CM is applied to words in a sentence, word-level CM states are assigned a probability based on training data. Probabilities are propagated upwards through the CM, calculating probabilities for all concepts that depend directly or indirectly on the words of the sentence.  3 ONYX Syntactic Analyzer For this project we desired a parser that was fast, flexible and robust. We designed a variation on a bottom-up chart parser (Kay, 1980) and hand-crafted an initial set of 52 context-free grammar 
Dental Condition   Condition Concept *translucency   Condition Term "translucency"   Severity Concept *superficial   Severity Term "slight" Tooth Location   Location Concept *numberEight   Tooth Number "eight" Surface   Surface Concept *mesial   Front/Back Term "mesio" State   State Concept *possible   State Term "might" Table 1: ONYX templates for "eight mesio might have a slight translucency." Terms with an * are in-ferred concepts. 
20
rules. Chart parsers based on Kay?s algorithm maintain an agenda of ?edges,? which correspond to partially or completely instantiated grammar rules. In the original algorithm, for each new phrase added to the chart an edge is created for each rule that can begin with that phrase. In addi-tion, each existing edge that abuts and can be ex-tended with that phrase is duplicated with a pointer to the new phrase. When an edge has no more un-matched components, it is regarded as a new phrase that can begin or extend other edges. Since edges are used to anticipate all possible continua-tions of phrases vis-?-vis the grammar, the number of edges grows quickly relative to the number of words in the sentence. Charniak et al (1998) noted that exhaustively parsing maximum-40-word sen-tences from the Penn II treebank requires an aver-age of 1.2 million edges per sentence.   ONYX?s parse algorithm replaces edges with bi-nary links. We briefly describe this new algorithm. A set of binary link templates is defined for each grammar rule. For instance, the rule S->NP AUX VP (labeled S1) would produce the templates [s1:np,aux] and [s1:aux,vp]. When a phrase is added to the chart, binary links for all applicable rules are added from that phrase to juxtaposed phrases to the left and right on the chart. When a right or left-terminating link is added (all links for rules with two or three components are right or left terminating), a quick search is done in the other direction for links belonging to the same rule. Each complete set of links defines a new phrase of the target type, as shown in figure 2.  
 Figure 2. Binary links for the rule S1:S->NP AUX VP used to generate new phrases of type S1 from juxta-posed NP, AUX, and VP phrases on the chart.  Although we have not analyzed the time and space complexity of this algorithm, it has proven to be 
more efficient than the edge-based parser used by MPLUS. Time and space complexity for chart parsers is calculated based on the number of edges produced, which has been shown to be O(n3), with n words in a sentence. Since binary links, unlike edges, are only used to record grammatical rela-tions between juxtaposed phrases on the chart (rather than anticipating possible continuations), are not duplicated, and can participate in the crea-tion of multiple new phrases, the number of binary links grows more slowly than the number of edges.  On the other hand, the need to search for com-pleted link sets increases processing time. We plan to formally analyze the time and space require-ments of this algorithm in a future study. 4  ONYX Semantic Analyzer In ONYX syntax and semantics are highly inte-grated. Rather than waiting for a completed parse tree to begin the interpretation process, ONYX semantically interprets each phrase as it is created and before it is placed on the chart. Each phrase is assigned a ?goodness? score based in part on the goodness of its semantic interpretation, and this score is used in determining the order in which phrases are expanded, resulting in a semantically guided best-first search.  To represent semantic relations between templates, ONYX uses a custom-built first-order predicate language with a syntax based roughly on the Knowledge Interchange Format (Genesareth & Fikes, 1992). ONYX interpretations are conjuncts of binary predicates formulated in this language, with templates as arguments. This language is for internal use only; ONYX will use standard lan-guage protocols for communicating with external systems. We decided to implement our own lan-guage rather than using an existing implementation in order to have access to the underlying data structures, which we use in three ways not tradi-tionally applied to symbolic languages: 1- We have extended our language to include Java objects as constants and Java methods as functions and rela-tions. In particular, CM templates are treated as constants in the language, and CMs are semanti-cally typed functions that map words to templates. 2- As described next, ONYX's default mode of semantic interpretation is based on a form of graph unification. Binary predicates are treated as unifi-
21
able links in a graph as shown in figure 3. 3- ONYX uses the predicate structure of an interpre-tation to pass information between CMs. For in-stance, if an interpretation contains the relation ConditionAt(Condition, Location), ONYX inserts the summary concept from the Location CM into the Condition CM. This allows the Condition CM to factor tooth location into its determination of the most probable Condition concept.  Figure 3 illustrates ONYX?s unification-based in-terpretation process. ONYX relies on a semantic network that defines types and relations in the den-tal domain (figure 4). As dental concepts are brought together in a phrase, links connecting those concepts are extracted from the semantic network and formulated into binary predicates in an interpretation. As phrases are joined together in larger phrases, their relations and templates are merged, resulting in an interpretation tree denoting a dental object (e.g. dental condition, tooth loca-tion, tooth surface) with possibly multiple levels of modifiers. For instance, the interpretation for "eight mesio might have a slight translucency" can be generated from the partial interpretations of the phrases "eight mesio", "might" and "slight translu-cency" as shown in figure 3.  
 Figure 3. Interpreting ?eight mesio might have a slight trans-lucency? using graph unification.   There are two primary justifications for using uni-fication in this way. First, conjoined phrases, par-ticularly noun phrases, often contain unifiable partial descriptions of a single object. Second, if concepts appear together in a phrase, there is a good chance that relations connecting those con-cepts in the semantic network are captured, explic-itly or implicitly, in the meaning of the phrase.  
The dental semantic network is shown in figure 4. Terminal (white) nodes define concrete semantic types associated with dental CMs. For instance, the DentalCondition type is associated with the con-cept model shown in figure 5. 
 Figure 4. Semantic network for dental exams. Nonterminal (gray) nodes represent abstract types with no associated CMs. A concrete type may have more than one abstract parent type. For instance, a Restoration, such as a crown, is both a Condition and a Location. As such, it can exist at a tooth lo-cation, e.g., "the crown on tooth 5," and it can be the location of condition, e.g., "the crack on the crown on tooth 5." Since a concrete type can have multiple parent types, ONYX often produces mul-tiple alternative interpretations over words of a sentence. For instance, ONYX may produce two interpretations for "mesial amalgam"?one refer-ring to the mesial surface of an amalgam filling, and one referring to an amalgam filling on the me-sial surface of some unspecified tooth. ONYX uses probabilities derived from training cases to prefer the latter interpretation, which is the more likely of the two.  
 Figure 5. Dental Condition Concept Model. Each concept model has a tree structure as illus-trated in figure 5, which shows the structure of the Dental Condition CM. Nonterminal nodes repre-
22
sent concepts, and terminal nodes represent words, with the exception of stub nodes. The value of a stub node is the summary concept (i.e., root node) from the CM of the same name.   One problem with ONYX?s graph-based model of interpretation is that the semantic network does not capture all relations that might be expressed in a dental exam. The network was deliberately kept simple by including mostly relations that are cate-gorically true (e.g., all teeth have surfaces) or that are frequently talked about (e.g., restorations are frequently mentioned as being locations of other conditions). This restriction helps keep the unifica-tion process tractable and minimizes ambiguity, but interpretations may miss important points. For instance, the ONYX interpretation of "15 occlusal amalgam" is ConditionAt(*filling, *toothFifteen) & Loca-tionHasSurface(*toothFifteen, *occlusal) which can be paraphrased as "a filling at tooth 15 and tooth 15 has an occlusal surface". This interpretation misses the important fact that the filling is on the occlusal surface of tooth 15, which we would normally in-fer from the fact that ?occlusal? adjectivally modi-fies ?amalgam.? Another limitation is that although the semantic network as it stands can describe sin-gle objects with their modifiers, it cannot be used to build up complex descriptions involving multi-ple objects of the same type.  To address these limitations we have added a sec-ond, more specialized mode of interpretation that is contingent on lexical and syntactic information from the parse and that can introduce into an inter-pretation predicates that do not exist in the seman-tic network. This mode of interpretation uses semantic types and patterns attached to grammar rules. As an example, the rule NP -> AP NP can be semantically annotated thus:     NP<Restoration> -> AP<Surface> NP<Restoration>  => OnSurface(Restoration, Surface)  This rule captures the idea that if a Surface-type adjectival phrase modifies a Restoration-type noun phrase, the restoration exists on that surface. Ap-plied to ?occlusal amalgam? this rule would pro-duce an interpretation OnSurface(*filling, *occlusal), which is the relation missing from the previous example. Semantically annotated grammar rules 
can also connect objects of the same semantic type. For instance, we might define a rule      NP<Condition> -> NP<Condition1> "caused by"     NP<Condition2>      => CausedBy(condition1, condition2)  This rule can match phrases such as "leakage caused by a crack along the lingual surface", and link the two conditions (leakage and crack) with a CausedBy relation. This mechanism enables ONYX to construct complex descriptions with multiple objects.  We have added a mechanism to the ONYX train-ing tool that allows semantically annotated gram-mar rules to be generated semi-automatically during training. A human annotator with sufficient linguistic background can view the parse trees generated by ONYX for corpus sentences, repair those parse trees and/or add new semantic relations if necessary, then apply a function that creates cop-ies of the rules embodied in those trees with se-mantic types and predicates attached. 5  Integrating Syntax and Semantics Although most NLP systems apply semantic analy-sis to completed parse trees, in humans the two processes are more integrated. Syntactic expecta-tions are greatly influenced by word meanings, as illustrated by ?garden path? sentences such as ?The man whistling tunes pianos.? In ONYX, syntax and semantics are highly interleaved. This is ac-complished in several ways:  1- ONYX?s parse algorithm permits words to be processed in any order, rather than strictly left-to-right, since binary grammar links can be added to the phrase chart in any order. This allows ONYX to be instructed to focus on semantically interest-ing words first, which can be used, among other things, to gather useful information from ungram-matical speech or run-on sentences where attempt-ing to look for complete sentences in strict left-to-right fashion would be unsuccessful.  2- ONYX implements a variation on a probabilistic context free grammar (PCFG) (Charniak, 1997) that associates grammar rules with semantic types. Based on training, a conditional probability is cal-culated for each <rule, type> pair given specific 
23
<rule, type> assignments to the rule?s components. The probability of a phrase is then calculated as the product of the probabilities of the phrase rule and its semantic type, given the rule and type of each of its child phrases. ONYX is then able to prefer phrases that best accommodate the semantic types of their constituents. Specifically,  prob(phrase) =  ?(prob(rule(phrase) + semtype(phrase) |         rule(childPhrase) + semtype(childPhrase)))  3- One hard problem in parsing is determining the correct structure of conjunctive noun phrases. ONYX applies semantic guidance to solve this problem. For instance, in a chest radiology report the words "right and left lower lobe opacity" can be grouped in several different ways, and different groupings can produce different interpretations. The correct grouping should be something like: [[[right and left] [lower lobe]] opacity], rather than [[right and [left lower]] [lobe opacity]]. ONYX currently employs a simplistic representation of the meaning of a conjunctive phrase as a list of inter-pretations. The correct interpretations for "right and left lower lobe opacity" would be two predi-cate expressions covering the words (right, lower, lobe, opacity) and (left, lower, lobe, opacity). ONYX generates a measure of the similarity of these expressions based on the cosine similarity of the lists of non-null nodes in their CM templates. This measure is factored into the phrase's goodness score under the heuristic that semantically bal-anced conjunctive phrases are more likely to be correct than imbalanced ones.  4- As mentioned earlier, ONYX can utilize gram-mar rules annotated with semantic types and pat-terns. Semantically annotated rules constrain phrases to match particular semantic types, and can contribute predicates to the interpretation of those phrases. This gives ONYX's grammar the character of a semantic grammar.  5- Phrases are weighted and preferred by ONYX according to their goodness score, which is based on three measures: the probability of the phrase as determined by the PCFG formula, the conjunct cosine similarity score, if applicable, and the goodness score of the phrase's semantic interpreta-tion. The PCFG and conjunct similarity formulas 
are based on semantic criteria, as mentioned ear-lier. Interpretation goodness scores are calculated as a simple product of the probabilities of the se-mantic relation predicates they contain. Relation probabilities are in turn derived from training data, and are conditioned on the concepts they contain. The probability of a relation is calculated as the number of times a pair of concepts appears to-gether in the target relation divided by the number of times they appear together in any set of rela-tions. The goodness score of a phrase is thus highly semantically determined.  goodness(phrase) = F(prob(phrase, PCFG),     conjunctSimilarity(phrase),     goodness(interp(phrase)) goodness(interp(phrase)) =    ?prob(relations(interp(phrase))) prob(relation) =   count(relation + concepts(relation)) /  count(anyConnection(concepts(relation))) 6  Evaluation We performed a preliminary evaluation of ONYX for the extraction of relevant dental concepts and relations on a set of twelve documents in our cur-rent training corpus.   Reference Standard. Each document was inde-pendently annotated by three human annotators (authors LC, JI and HH), who used the ONYX training tool to fill in templates representing dental conditions, tooth locations and other relevant con-cepts, as well as to select the semantic relations linking those templates. The annotators then re-viewed disagreements and by consensus created a reference standard set of templates and relations. Where the annotators did not have sufficient dental knowledge to reach an agreement they consulted dental clinicians.  Outcome Metrics. To evaluate ONYX on the rela-tively small corpus of documents, we applied a leave-one-out approach: for each sentence in the reference standard, ONYX was trained using the templates from the remaining reference standard sentences. ONYX was then applied to the target sentence, and the resulting templates and relations were compared to the reference standard. We measured inter-annotator agreement (IAA) be-tween ONYX and the reference standard using the formula described in Roberts et al(2007): 
24
 IAA = (2 * correct) / (spurious + missing + correct)  We calculated IAA separately for CM words, con-cepts, and semantic relations. A correct match is a word, concept or relation generated by both the reference standard and ONYX; a spurious item is one ONYX generated that did not exist in the ref-erence standard; and a missing item is one that ex-isted in the reference standard but was not generated by ONYX. In addition to IAA we identi-fied the concepts and relations most commonly in error and calculated percentages for those errors.   We compared ONYX?s performance on the target documents with that of a simple baseline parser we created for this purpose. The baseline parser proc-esses the words of a sentence from left to right, creating phrases for sets of juxtaposed words that can be interpreted together using the semantic net-work. No grammar rules are employed, there is no analysis of conjunctive phrases, and goodness scores are not calculated. Our goal was to get a feel for how much these factors contribute to generat-ing correct interpretations. There is no precedence for this particular approach as far as we are aware, so we regard this comparison as informative but not definitive. 7 Results IAA results for ONYX and the baseline parser are shown in table 2. ONYX performs best at inserting words into appropriate nodes in the CMs, with IAA of 86%, and less well for inferring the best concept (80%) and identifying relations among concepts (76%). ONYX consistently out-performs the baseline parser.  Table 2: IAA for assignment of words, concepts, and relations.  IAA ONYX  86% Words (n = 904) Baseline  57% ONYX  80% Concepts (n = 1186) Baseline  53% ONYX  76% Relations (n = 297) Baseline  41%  Although this study does not examine all the rea-sons for the differences in performance between ONYX and the baseline parser, some reasons can 
be illustrated with an example. Conjunctive phrases are common in dental discourse, and a failure to handle conjuncts can result in both con-cept and relation errors. For instance, given the sentence "4, 5, 6, 7 fine" ONYX generates separate interpretations covering the word groupings (4, fine), (5, fine), (6, fine), and (7, fine), which would yield four ConditionAt relations, four Location concepts (*numberFour, *numberFive, *numberSix, *numberSeven) and one Condition concept (*normalTooth) appearing in each relation. The baseline parser in contrast does not discover this distribution of terms and so omits all but the ConditionAt relation over (7, fine). Trying to merge juxtaposed tooth numbers, the baseline parser also infers that at least some of these denote tooth ranges instead of individual teeth (e.g. inter-preting ?4, 5? as ?4 to 5? instead of ?4 and 5?), which causes it to misclassify Location concepts. The ability to generate correct parse trees and to use the structure of those parse trees in the inter-pretation process is important in generating correct interpretations.  Tables 3 and 4 show breakdowns by percentage of the concepts and relations most commonly in error in ONYX?s interpretations (errors accounting for more than 15%).  Table 3: Per-concept error percentages  Dental Condition Summary Concept 18% Tooth Location Summary Concept 17% Dental Condition Intermediate Concept 16% Surface Summary Concept 15% Total  66%  Table 4: Per-relation error percentages. Surface of Part 47% Location of Condition 23% Total 70%  8  Related Work ONYX is a new application inspired by SPRUS (Ranum, 1989), Symtext (Koehler, 1998), and MPLUS (Christensen, 2002), which all used Baye-sian Networks to infer relevant findings from text. Other medical language processing systems im-plement different approaches to encode clinical concepts and their modifiers, along with relations between concepts, including MedLEE (Friedman, 
25
1994), a largely statistical system by Taira and col-leagues (Taira, 2007), and MedSyndikate (Hahn, 2002).   Many of ONYX?s components leverage research in the general and clinical NLP domains, including the use of chart parsing (Kay, 1980) and probabil-istic context free grammars (Charniak, 1997). ONYX's use of semantically annotated grammar rules was inspired in part by MedLEE (Friedman et al 1994), which uses a semantic grammar.   Although incorporating ideas and approaches from others, we feel that ONYX is unique in several ways, including its high level of syntactic/semantic integration and the ways in which it blends sym-bolic and probabilistic representations of domain knowledge. We plan to make ONYX available through open source when the system is more complete.  9 Limitations There are several limitations to this study. Al-though ONYX introduces several innovations, these are not described in detail in this study and are not individually evaluated for their effect on ONYX?s performance. Instead, this study presents a broad overview of ONYX and evaluates ONYX's overall performance against a reference standard on a small test sample. Another limitation of our study is the baseline system?because similar sys-tems generate different output than ONYX and do not model the same domain, finding a competitive baseline application is difficult. In spite of its im-perfection, we believe the baseline we imple-mented to be reasonable. 10 Future Work One limitation of a system like ONYX is the over-head of manually creating complex training cases. To address this shortcoming, the ONYX training tool invokes ONYX to automatically create tem-plates and relations for corpus sentences, and hu-man trainers correct any mistakes. A semi-automated approach greatly speeds up the training process and facilitates agreement among human trainers. We plan to further automate this process using an approach derived from Thelen & Riloff (2002), which uses a classifier with features based 
on extraction patterns derived from Autoslog (Riloff, 1996). We plan to adapt this approach to automatically classify CM word assignments, and also to automatically classify semantic relations between CM templates. We will add this function-ality to the training tool to enable it to find and an-notate relevant sentences automatically where possible. We will also apply this functionality to enable ONYX to recognize relevant sentences in new documents based on their similarity to training sentences, and we will use semantic patterns stored with training sentences to aid in interpreting noisy segments of text that ONYX cannot parse. We plan to compare the performance of grammar-based and feature-based semantic analysis in future studies. With more fully automated training, we also hope to make ONYX more easily portable to new do-mains and clinical settings in the future.   Conclusions  This paper describes ONYX, which is being devel-oped as part of a system for extracting chartable findings from spoken dental examinations. ONYX contains a number of innovative ideas including a novel adaptation of Kay's (1980) parse algorithm; a symbolic language extended to include probabilis-tic and procedural elements; an integration of syn-tax and semantics that includes a semantically weighted probabilistic context free grammar and interpretation based both on a semantic network and a semantic grammar. Considering ONYX?s early stage of development it performed reasonably well in this limited evaluation but must be ex-tended to address challenges in extracting findings from spoken dental exams. Acknowledgments  This work was funded by NIDCR 1 R21DE018158-01A1 ?Feasibility of a Natural Language Processing-based Dental Charting Application. References  E. Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial In-telligence, pp. 598-603. E. Charniak, S. Goldwater and M. Johnson. 1998. Edge-Based Best-First Chart Parsing. In Proceedings of 
26
the Sixth Workshop on Very Large Corpora, pp. 127-133. Lee M. Christensen, Peter J. Haug, and Marcelo Fisz-man. 2002. MPLUS: A Probabilistic Medical Lan-guage Understanding System. Proceedings of the Workshop on Natural Language Processing in the Biomedical Domain, Philadelphia, pp. 29 ? 36. Carol Friedman, Phil Alderson, John Austin, James Ci-mino, & Stephen Johnson. 1994. A general natural language text processor for clinical radiology. Jour-nal of American Medical Informatics Association 1(2), pp. 161?174.  M. R. Genesereth and R. E. Fikes. Knowledge Inter-change Format, Version 3.0 Reference Manual. Technical Report Logic-92-1, Stanford, CA, USA, 1992.  Hahn U, Romacker M, Schulz S. 2002. Medsyndikate-a natural language system for the extraction of medical information from findings reports. Int J Med Inf. 67(1-3), pp. 63-74.  M. Kay. 1980. Algorithm schemata and data structures in syntactic parsing. In Readings in Natural Lan-guage Processing, pp. 35 ? 70. Morgan Kaufmann Publishers Inc.  Koehler, S. B. 1998. SymText: A natural language un-derstanding system for encoding free text medical data. Ph.D. Dissertation, University of Utah.  Ranum D.L. 1989. Knowledge-based understanding of radiology text. Comput Methods ProBiomed. Oct-Nov;30(2-3) pp. 209-215. Ellen Riloff, 1996. Automatically Generating Extraction Patterns from Untagged Text. Proceedings of the Thirteenth National Conference on Artiticial Intelli-gence, pp. 1044 ? 1049. The AAAI Press/MIT Press. Angus Roberts, Robert Gaizauskas, Mark Hepple, Neil Davis, George Demetriou, Yikun Guo, Jay Kola, Ian Roberts, Andrea Setzer, Archana Tapuria, Bill Wheeldin. 2007. The CLEF Corpus: Semantic Anno-tation of Clinical Text. AMIA 2007, pp. 625 ? 629.  Taira R, Bashyam V, Kangarloo H. 2007. A field theory approach to medical natural language processing. IEEE Transactions in Inform Techn in Biomedicine 11(2). Michael Thelen and Ellen Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons using Ex-traction Pattern Contexts. Proceedings of the ACL-02 conference on Empirical methods in natural lan-guage processing, pp. 214 ? 221. 
27
A Large Scale Terminology Resource for Biomedical Text Processing
Henk Harkema, Robert Gaizauskas, Mark Hepple, Angus Roberts,
Ian Roberts, Neil Davis, Yikun Guo
Department of Computer Science, University of Sheffield, UK
biomed@dcs.shef.ac.uk
Abstract
In this paper we discuss the design, implemen-
tation, and use of Termino, a large scale termi-
nological resource for text processing. Dealing
with terminology is a difficult but unavoidable
task for language processing applications, such
as Information Extraction in technical domains.
Complex, heterogeneous information must be
stored about large numbers of terms. At the
same time term recognition must be performed
in realistic times. Termino attempts to recon-
cile this tension by maintaining a flexible, ex-
tensible relational database for storing termino-
logical information and compiling finite state
machines from this database to do term look-
up. While Termino has been developed for
biomedical applications, its general design al-
lows it to be used for term processing in any
domain.
1 Introduction
It has been widely recognized that the biomedical litera-
ture is now so large, and growing so quickly, that it is be-
coming increasingly difficult for researchers to access the
published results that are relevant to their research. Con-
sequently, any technology that can facilitate this access
should help to increase research productivity. This has
led to an increased interest in the application of natural
language processing techniques for the automatic capture
of biomedical content from journal abstracts, complete
papers, and other textual documents (Gaizauskas et al,
2003; Hahn et al, 2002; Pustejovsky et al, 2002; Rind-
flesch et al, 2000).
An essential processing step in these applications is
the identification and semantic classification of techni-
cal terms in text, since these terms often point to enti-
ties about which information should be extracted. Proper
semantic classification of terms also helps in resolving
anaphora and extracting relations whose arguments are
restricted semantically.
1.1 Challenge
Any technical domain generates very large numbers of
terms ? single or multiword expressions that have some
specialised use or meaning in that domain. For exam-
ple, the UMLS Metathesaurus (Humphreys et al, 1998),
which provides a semantic classification of terms from a
wide range of vocabularies in the clinical and biomedical
domain, currently contains well over 2 million distinct
English terms.
For a variety of reasons, recognizing these terms in
text is not a trivial task. First of all, terms are often
long multi-token sequences, e.g. 3-methyladenine-DNA
glycosylase I. Moreover, since terms are referred to re-
peatedly in discourses there is a benefit in their being
short and unambiguous, so they are frequently abbre-
viated and acronymized, e.g. CvL for chromobacterium
viscosum lipase. However, abbreviations may not al-
ways occur together with their full forms in a text, the
method of abbreviation is not predictable in all cases, and
many three letter abbreviations are highly overloaded.
Terms are also subject to a high degree of orthographic
variation as a result of the representation of non-Latin
characters, e.g. a-helix vs. alpha-helix, capitalization,
e.g. DNA vs. dna, hyphenation, e.g. anti-histamine vs. an-
tihistamine, and British and American spelling variants,
e.g. tumour vs. tumor. Furthermore, biomedical science
is a dynamic field: new terms are constantly being in-
troduced while old ones fall into disuse. Finally, certain
classes of biomedical terms exhibit metonomy, e.g. when
a protein is referred to by the gene that expresses it.
To begin to address these issues in term recognition, we
are building a large-scale resource for storing and recog-
nizing technical terminology, called Termino. This re-
source must store complex, heterogeneous information
about large numbers of terms. At the same time term
recognition must be performed in realistic times. Ter-
mino attempts to reconcile this tension by maintaining a
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 53-60.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
flexible, extensible relational database for storing termi-
nological information and compiling finite state machines
from this database to do term look-up.
1.2 Context
Termino is being developed in the context of two ongoing
projects: CLEF, for Clinical E-Science Framework (Rec-
tor et al, 2003) and myGrid (Goble et al, 2003). Both
these projects involve an Information Extraction compo-
nent. Information Extraction is the activity of identifying
pre-defined classes of entities and relationships in natural
language texts and storing this information in a structured
format enabling rapid and effective access to the informa-
tion, e.g. Gaizauskas and Wilks (1998), Grishman (1997).
The goal of the CLEF project is to extract information
from patient records regarding the treatment of cancer.
The treatment of cancer patients may extend over several
years and the resulting clinical record may include many
documents, such as clinic letters, case notes, lab reports,
discharge summaries, etc. These documents are gener-
ally full of medical terms naming entities such as body
parts, drugs, problems (i.e. symptoms and diseases), in-
vestigations and interventions. Some of these terms are
particular to the hospital from which the document origi-
nates. We aim to identify these classes of entities, as well
as relationships between such entities, e.g. that an investi-
gation has indicated a particular problem, which, in turn,
has been treated with a particular intervention. The infor-
mation extracted from the patient records is potentially of
value for immediate patient care, but can also be used to
support longitudinal and epidemiological medical stud-
ies, and to assist policy makers and health care managers
in regard to planning and clinical governance.
The myGrid project aims to present research biolo-
gists with a unified workbench through which component
bioinformatic services can be accessed using a workflow
model. These services may be remotely located from the
user and will be exploited via grid or web-service chan-
nels. A text extraction service will form one of these ser-
vices and will facilitate access to information in the sci-
entific literature. This text service comprises an off-line
and an on-line component. The off-line component in-
volves pre-processing a large biological sciences corpus,
in this case the contents of Medline, in order to identify
various biological entities such as genes, enzymes, and
proteins, and relationships between them such as struc-
tural and locative relations. These entities and relation-
ships are referred to in Medline abstracts by a very large
number of technical terms and expressions, which con-
tributes to the complexity of processing these texts. The
on-line component supports access to the extracted infor-
mation, as well as to the raw texts, via a SOAP interface
to an SQL database.
Despite the different objectives for text extraction
within the CLEF and myGrid projects, many of the tech-
nical challenges they face are the same, such as the
need for extensive capabilities to recognize and classify
biomedical entities as described using complex techni-
cal terminology in text. As a consequence we are con-
structing a general framework for the extraction of infor-
mation from biomedical text: AMBIT, a system for ac-
quiring medical and biological information from text. An
overview of the AMBIT logical architecture is shown in
figure 1.
The AMBIT system contains several engines, of which
Termino is one. The Information Extraction Engine pulls
selected information out of natural language text and
pushes this information into a set of pre-defined tem-
plates. These are structured objects which consists of one
or more slots for holding the extracted entities and rela-
tions. The Query Engine allows users to access informa-
tion through traditional free text search and search based
on the structured information produced by the Informa-
tion Extraction Engine, so that queries may refer to spe-
cific entities and classes of entities, and specific kinds of
relations that are recognised to hold between them. The
Text Indexing Engine is used to index text and extracted,
structured information for the purposes of information re-
trieval. The AMBIT system contains two further compo-
nents: an interface layer, which provides a web or grid
channel to allow user and program access to the system;
and a database which holds free text and structured infor-
mation that can be searched through the Query Engine.
Termino interacts with the Query Engine and the Text
Indexing Engine to provide terminological support for
query formulation and text indexation. It also provides
knowledge for the Information Extraction Engine to use
in identifying and classifying biomedical entities in text.
The Terminology Engine can furthermore be called by
users and remote programs to access information from
the various lexical resources that are integrated in the ter-
minological database.
2 Related Work
Since identification and classification of technical terms
in biomedical text is an essential step in information
extraction and other natural language processing tasks,
most natural language processing systems contain a
terminological resource of some sort. Some systems
make use of existing terminological resources, notably
the UMLS Metathesaurus, e.g. Rindflesch et al (2000),
Pustejovski et al (2002); other systems rely on re-
sources that have been specifically built for the applica-
tion, e.g. Humphreys et al (2000), Thomas et al (2000).
The UMLS Metathesaurus provides a semantic classi-
fication of terms drawn from a wide range of vocabularies
in the clinical and biomedical domain (Humphreys et al,
1998). It does so by grouping strings from the source vo-
from Hospital 1
Clinical Records
Journals
On?line
Abstracts
Medline
Literature
Biomedical
Engine
Indexing
Text
Engine
Extraction
...
(Termino)
Engine
Terminology
Ambit
from Hospital 2
Clinical Records
Web GRID
Interface layer
Raw text
(entities / relations)
Structured InfoFree text
  search
Engine
Query
Information
SOAP /
     HTTP
& Annotations
Structured Info
Figure 1: AMBIT Architecture
cabularies that are judged to have the same meaning into
concepts, and mapping these concepts onto nodes or se-
mantic types in a semantic network. Although the UMLS
Metathesaurus is used in a number of biomedical natural
language processing applications, we have decided not to
adopt the UMLS Metathesaurus as the primary terminol-
ogy resource in AMBIT for a variety of reasons.
One of the reasons for this decision is that the Metathe-
saurus is a closed system: strings are classified in terms
of the concepts and the semantic types that are present
in the Metathesaurus and the semantic network, whereas
we would like to be able to link our terms into multi-
ple ontologies, including in-house ontologies that do not
figure in any of the Metathesaurus? source vocabularies
and hence are not available through the Metathesaurus.
Moreover, we would also like to be able to have access to
additional terminological information that is not present
in the Metathesaurus, such as, for example, the annota-
tions in the Gene Ontology (The Gene Ontology Con-
sortium, 2001) assigned to a given human protein term.
While the terms making up the the tripartite Gene On-
tology are present in the UMLS Metathesaurus, assign-
ments of these terms to gene products are not recorded
in the Metathesaurus. Furthermore, as new terms appear
constantly in the biomedical field we would like to be
able to instantly add these to our terminological resource
and not have to wait until they have been included in the
UMLS Metathesaurus. Additionally, some medical terms
appearing in patient notes are hospital-specific and are
unlikely to be included in the Metathesaurus at all.
With regard to systems that do not use the UMLS
Metathesaurus, but rather depend on terminological re-
sources that have been specifically built for an applica-
tion, we note that these terminological resources tend to
be limited in the following two respects. First, the struc-
ture of these resources is often fixed and in some cases
amounts to simple gazetteer lists. Secondly, because of
their fixed structure, these resources are usually popu-
lated with content from just a few sources, leaving out
many other potentially interesting sources of terminolog-
ical information.
Instead, we intend for Termino to be an exten-
sible resource that can hold diverse kinds of termi-
nological information. The information in Termino
is either imported from existing, outside knowledge
sources, e.g. the Enzyme Nomenclature (http://www.
chem.qmw.ac.uk/iubmb/enzyme/), the Structural Classi-
fication of Proteins database (Murzin et al, 1995), and
the UMLS Metathesaurus, or it is induced from on-line
raw text resources, e.g. Medline abstracts. Termino thus
provides uniform access to terminological information
aggregated across many sources. Using Termino re-
moves the need for multiple, source-specific terminolog-
ical components within text processing systems that em-
ploy multiple terminological resources.
3 Architecture
Termino consists of two components: a database holding
terminological information and a compiler for generating
term recognizers from the contents of the database. These
two components will be discussed in the following two
sections.
STRINGS
string str id
. . . . . .
neurofibromin str728
abdomen str056
mammectomy str176
mastectomy str183
. . . . . .
TERMOID STRINGS
trm id str id
. . . . . .
trm023 str056
trm656 str056
trm924 str728
trm369 str728
trm278 str176
trm627 str183
. . . . . .
PART OF SPEECH
trm id pos
. . . . . .
trm023 N
. . . . . .
SYNONYMY
syn id trm id scl id
. . . . . . . . .
syn866 trm278 syn006
syn435 trm627 syn006
. . . . . . . . .
GO ANNOTATIONS
trm id annotation version
. . . . . . . . .
trm924 GO:0004857 9/2003
trm369 GO:0008285 9/2003
. . . . . . . . .
UMLS
trm id cui lui sui version
. . . . . . . . . . . . . . .
trm278 C0024881 L0024669 S0059711 2003AC
trm656 C0000726 L0000726 S0414154 2003AC
. . . . . . . . . . . . . . .
Figure 2: Structure of the terminological database
3.1 Terminological Database
The terminological database is designed to meet three re-
quirements. First of all, it must be capable of storing large
numbers of terms. As we have seen, the UMLS Metathe-
saurus contains over 2 million distinct terms. However,
as UMLS is just one of many resources whose terms may
need to be stored, many millions of terms may need to
be stored in total. Secondly, Termino?s database must
also be flexible enough to hold a variety of information
about terms, including information of a morpho-syntactic
nature, such as part of speech and morphological class;
information of a semantic nature, such as quasi-logical
form and links to concepts in ontologies; and provenance
information, such as the sources of the information in the
database. The database will also contain links to connect
synonyms and morphological and orthographic variants
to one another and to connect abbreviations and acronyms
to their full forms. Finally, the database must be orga-
nized in such a way that it allows for fast and efficient
recognition of terms in text.
As mentioned above, the information in Termino?s
database is either imported from existing, outside knowl-
edge sources or induced from text corpora. Since these
sources are heterogeneous in both information content
and format, Termino?s database is ?extensional?: it stores
strings and information about strings. Higher-order con-
cepts such as ?term? emerge as the result of interconnec-
tions between strings and information in the database.
The database is organized as a set of relational tables,
each storing one of the types of information mentioned
above. In this way, new information can easily be in-
cluded in the database without any global changes to the
structure of the database.
Terminological information about any given string is
usually gathered from multiple sources. As information
about a string accumulates in the database, we must make
sure that co-dependencies between various pieces of in-
formation about the string are preserved. This considera-
tion leads to the fundamental element of the terminologi-
cal database, a termoid. A termoid consists of a string to-
gether with associated information of various kinds about
the string. Information in one termoid holds conjunc-
tively for the termoid?s string, while multiple termoids
for the same string express disjunctive alternatives.
For instance, taking an example from UMLS, we may
learn from one source that the string cold as an adjective
refers to a temperature, whereas another source may tell
us that cold as a noun refers to a disease. This informa-
tion is stored in the database as two termoids: abstractly,
?cold, adjective, temperature? and ?cold, noun, disease?.
A single termoid ?cold, adjective, noun, temperature, dis-
ease? would not capture the co-dependency between the
part of speech and the ?meaning? of cold.1 This example
illustrates that a string can be in more than one termoid.
1Note that the UMLS Metathesaurus has no mechanism for
storing this co-dependency between grammatical and semantic
information.
Each termoid, however, has one and only one string.
Figure 2 provides a detailed example of part of the
structure of the terminological database. In the table
STRINGS every unique string is assigned a string iden-
tifier (str id). In the table TERMOID STRINGS each string
identifier is associated with one or more termoid iden-
tifiers (trm id). These termoid identifiers then serve as
keys into the tables holding terminological information.
Thus, in this particular example, the database includes
the information that in the Gene Ontology the string
neurofibromin has been assigned the terms with identi-
fiers GO:0004857 and GO:0008285. Furthermore, in the
UMLS Metathesaurus version 2003AC, the string mam-
mectomy has been assigned the concept-unique identifier
C0024881 (CUI), the lemma-unique identifier L0024669
(LUI), and the string-unique identifier S0059711 (SUI).
Connections between termoids such as those arising
from synonymy and orthographic variation are recorded
in another set of tables. For example, the table SYN-
ONYMY in figure 2 indicates that termoids 278 and
627 are synonymous, since they have the same syn-
onymy class identifier (scl id).2 The synonymy identifier
(syn id) identifies the assignment of a termoid to a partic-
ular synonymy class. This identifier is used to record the
source on which the assignment is based. This can be a
reference to a knowledge source from which synonymy
information has been imported into Termino, or a refer-
ence to both an algorithm by which and a corpus from
which synonyms have been extracted. Similarly there are
tables containing provenance information for strings, in-
dexed by str id, and termoids, indexed by trm id. These
tables are not shown in he example.
With regard to the first requirement for the design of
the terminological database mentioned at the beginning
of this section ? scalability ?, an implementation of Ter-
mino in MySQL has been loaded with 427,000 termoids
for 363,000 strings (see section 4 for more details). In it
the largest table, STRINGS, measures just 16MB, which is
nowhere near the default limit of 4GB that MySQL im-
poses on the size of tables. Hence, storing a large num-
ber of terms in Termino is not a problem size-wise. The
second requirement, flexibility of the database, is met by
distributing terminological information over a set of rela-
tively small tables and linking the contents of these tables
to strings via termoid identifiers. In this way we avoid the
strictures of any one fixed representational scheme, thus
making it possible for the database to hold information
from disparate sources. The third requirement on the de-
sign of the database, efficient recognition of terms, will
2The function of synonymy class identifiers in Termino is
similar to the function of CUIs in the UMLS Metathesaurus.
However, since we are not bound to a classification into UMLS
CUIs, we can assert synonymy between terms coming from ar-
bitrary sources.
be addressed in the next section.
3.2 Term Recognition
To ensure fast term recognition with Termino?s vast ter-
minological database, the system comes equipped with
a compiler for generating finite state machines from the
strings in the terminological database discussed in the
previous section. Direct look-up of strings in the database
is not an option, because it is unknown in advance at
which positions in a text terms will start and end. In order
to be complete, one would have to look up all sequences
of words or tokens in the text, which is very inefficient.
Compilation of a finite state recognizer proceeds in
the following way. First, each string in the database is
broken into tokens, where a token is either a contigu-
ous sequence of alpha-numeric characters or a punctu-
ation symbol. Next, starting from a single initial state, a
path through the machine is constructed, using the tokens
of the string to label transitions. For example, for the
string Graves? disease the machine will include a path
with transitions on Graves, ?, and disease. New states are
only created when necessary. The state reached on the fi-
nal token of a string will be labeled final and is associated
with the identifiers of the termoids for that string.
To recognize terms in text, the text is tokenized and the
finite state machine is run over the text, starting from the
initial state at each token in the text. For each sequence
of tokens leading to a final state, the termoid identifiers
associated with that state are returned. These identifiers
are then used to access the terminological database and
retrieve the information contained in the termoids. Where
appropriate the machine will produce multiple termoid
identifiers for strings. It will also recognize overlapping
and embedded strings.
Figure 3 shows a small terminological database and a
finite state recognizer derived from it. Running this rec-
ognizer over the phrase . . . thyroid dysfunction, such as
Graves? disease . . . produces four annotations: thyroid
is assigned the termoid identifiers trm1 and trm2; thyroid
dysfunction, trm3; and Graves? disease, trm4.
It should be emphasised at this point that term recog-
nition as performed by Termino is in fact term look-up
and not the end point of term processing. Term look-up
might return multiple possible terms for a given string,
or for overlapping strings, and subsequent processes may
apply to filter these alternatives down to the single option
that seems most likely to be correct in the given context.
Furthermore, more flexible processes of term recognition
might apply over the results of look-up. For example, a
term grammar might be provided for a given domain, al-
lowing longer terms to be built from shorter terms that
have been identified by term look-up.
The compiler can be parameterized to produce finite
state machines that match exact strings only, or that ab-
STRINGS
string str id
thyroid str12
thyroid disfunction str15
Graves? disease str25
TERMOID STRINGS
trm id str id
trm1 str12
trm2 str12
trm3 str15
trm4 str25
? trm4disease
thyroid
Graves
trm3
trm2
trm1
disfunction
Figure 3: Sample terminological database and finite state term recognizer
stract away from morphological and orthographical vari-
ation. At the moment, morphological information about
strings is supplied by a component outside Termino. In
our current term recognition system, this component ap-
plies to a text before the recognition process and asso-
ciates all verbs and nouns with their base form. Similarly,
the morphological component applies to the strings in the
terminological database before the compilation process.
The set-up in which term recognizers are compiled
from the contents of the terminological database turns
Termino into a general terminological resource which is
not restricted to any single domain or application. The
database can be loaded with terms from multiple domains
and compilation can be restricted to particular subsets of
strings by selecting termoids from the database based on
their source, for example. In this way one can produce
term recognizers that are tailored towards specific do-
mains or specific applications within domains.
4 Implementation & Performance
A first version of Termino has been implemented. It uses
a database implemented in MySQL and currently con-
tains over 427,000 termoids for around 363,000 strings.
Content has been imported from various sources by
means of source-specific scripts for extracting relevant
information from sources and a general script for load-
ing this extracted information into Termino. More specif-
ically, to support information extraction from patient
records, we have included in Termino strings from the
UMLS Metathesaurus falling under the following seman-
tic types: pharmacologic substances, anatomical struc-
tures, therapeutic procedure, diagnostic procedure, and
several others. We have also loaded a list of hu-
man proteins and their assignments to the Gene Ontol-
ogy as produced by the European Bioinformatics Insti-
tute (http://www.ebi.ac.uk/GOA/) into Termino. Further-
more, we have included several gazetteer lists containing
terms in the fields of molecular biology and pharmacol-
ogy that were assembled for previous information extrac-
tion projects in our NLP group. A web services (SOAP)
API to the database is under development. We plan to
make the resource available to researchers as a web ser-
vice or in downloadable form.3
The compiler to construct finite state recognizers from
the database is fully implemented, tested, and integrated
into AMBIT. The compiled recognizer for the 363,000
strings of Termino has 1.2 million states and an on-disk
size of around 80MB. Loading the matcher from disk
into memory requires about 70 seconds (on an UltraSparc
900MHz), but once loaded recognition is a very fast pro-
cess. We have been able to annotate a corpus of 114,200
documents, drawn from electronic patient records from
the Royal Marsden NHS Trust in London and each ap-
proximately 1kB of text, in approximately 44 hours ? an
average rate of 1.4 seconds per document, or 42 docu-
ments per minute. On average, about 30 terms falling un-
der the UMLS ?clinical? semantic types mentioned above
were recognized in each document. We are currently an-
notating a bench-mark corpus in order to obtain precision
and recall figures. We are also planning to compile rec-
ognizers for differently sized subsets of the terminologi-
cal database and measure their recognition speed over a
given collection of texts. This will provide some indica-
tion as to the scalability of the system.
Since Termino currently contains many terms imported
from the UMLS Metathesaurus, it is interesting to com-
pare its term recognition performance against the per-
formance of MetaMap. MetaMap is a program avail-
able from at the National Library of Medicine ? the de-
velopers of UMLS ? specifically designed to discover
UMLS Metathesaurus concepts referred to in text (Aron-
son, 2001). An impressionistic comparison of the per-
formance of Termino and MetaMap on the CLEF patient
records shows that the results differ in two ways. First,
MetaMap recognizes more terms than Termino. This
is simply because MetaMap draws on a comprehensive
version of UMLS, whereas Termino just contains a se-
lected subset of the strings in the Metathesaurus. Sec-
ondly, MetaMap is able to recognize variants of terms,
e.g. it will map the verb to treat and its inflectional forms
onto the term treatment, whereas Termino currently does
not do this. To recognize term variants MetaMap re-
lies on UMLS?s SPECIALIST lexicon, which provides
3Users may have to sign license agreements with third par-
ties in order to be able to use restricted resources that have been
integrated into Termino.
syntactic, morphological, and orthographic information
for many of the terms occurring in the Metathesaurus.
While the performance of both systems differs in favor
of MetaMap, it is important to note that the source of
these differences is unrelated to the actual design of Ter-
mino?s terminological database or Termino?s use of fi-
nite state machines to do term recognition. Rather, the
divergence in performance follows from a difference in
breadth of content of both systems at the moment. With
regard to practical matters, the comparison showed that
term recognition with Termino is much faster than with
MetaMap. Also, compiling a finite state recognizer from
the terminological database in Termino is a matter of min-
utes, whereas setting up MetaMap can take several hours.
However, since MetaMap?s processing is more involved
than Termino?s, e.g. MetaMap parses the input first, and
hence requires more resources, these remarks should be
backed up with a more rigorous comparison between Ter-
mino and MetaMap, which is currently underway.
The advantage of term recognition with Termino over
MetaMap and UMLS or any other recognizer with a sin-
gle source, is that it provides immediate entry points
into a variety of outside ontologies and other knowledge
sources, making the information in these sources avail-
able to processing steps subsequent to term recognition.
For example, for a gene or protein name recognized in a
text, Termino will return the database identifiers of this
term in the HUGO Nomenclature database (Wain et al,
2002) and the OMIM database (Online Mendelian Inher-
itance in Man, OMIM (TM), 2000). These identifiers
give access to the information stored in these databases
about the gene or protein, including alternative names,
gene map locus, related disorders, and references to rele-
vant papers.
5 Conclusions & Future Work
Dealing with terminology is an essential step in natural
language processing in technical domains. In this paper
we have described the design, implementation, and use of
Termino, a large scale terminology resource for biomedi-
cal language processing.
Termino includes a relational database which is de-
signed to store a large number of terms together with
complex, heterogeneous information about these terms,
such as morpho-syntactic information, links to concepts
in ontologies, and other kinds of annotations. The
database is also designed to be extensible: it is easy to
include terms and information about terms found in out-
side biological databases and ontologies. Term look-up
in text is done via finite state machines that are compiled
from the contents of the database. This approach allows
the database to be very rich without sacrificing speed at
look-up time. These three features make Termino a flexi-
ble tool for inclusion in a biomedical text processing sys-
tem.
As noted in section 3.2, Termino has not been designed
to be used as a stand-alone term recognition system but
rather as the first component, the lexical look-up com-
ponent, in a multi-component term processing system.
Since Termino may return multiple terms for a given
string, or for overlapping strings, some post-filtering of
these alternatives is necessary. Secondly, it is likely that
better term recognition performance will be obtained by
supplementing Termino look-up with a term parser which
uses a grammar to give a term recognizer the generative
capacity to recognize previously unseen terms. For ex-
ample, many terms for chemical compounds conform to
grammars that allow complex terms to be built out of sim-
pler terms prefixed or suffixed with numerals separated
from the simpler term with hyphens. It does not make
sense to attempt to store in Termino all of these variants.
Termino provides a firm basis on which to build large-
scale biomedical text processing applications. However,
there are a number of directions where further work can
be done. First, as noted in 3.2, morphological informa-
tion is currently not held in Termino, but rather resides
in an external morphological analyzer. We are working
to extend the Termino data model to enable information
about morphological variation to be stored in Termino,
so that Termino serves as a single source of information
for the terms it contains. Secondly, we are working to
build term induction modules to allow Termino content
to be automatically acquired from corpora, in addition
to deriving it from manually created resources such as
UMLS. Finally, while we have already incorporated Ter-
mino into the AMBIT system where it collaborates with
a term parser to perform more complete term recogni-
tion, more work can be done to with respect to such an
integration. For example, probabilities could be incorpo-
rated into Termino to assist with probabilistic parsing of
terms; or, issues of trade-off between what should be in
the term lexicon versus the term grammar could be fur-
ther explored by looking to see which compound terms
in the lexicon contain other terms as substrings and at-
tempt to abstract away from these to grammar rules. For
example, in the example thyroid disfunction above, both
thyroid and disfunction are terms, the first of class ?body
part?, the second of class ?problem?. Their combination
thyroid disfunction is a term of class ?problem?, suggest-
ing a rule of the form ?problem?   ?body part? ?problem?.
References
A.R. Aronson. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. In Proceedings of the American Medical Infor-
matics Association Symposium, pages 17?21.
R. Gaizauskas and Y. Wilks. 1998. Information extrac-
tion: Beyond document retrieval. Journal of Docu-
mentation, 54(1):70?105.
R. Gaizauskas, G. Demetriou, P. Artymiuk, and P. Wil-
lett. 2003. Protein structures and information extrac-
tion from biological texts: The PASTA system. Jour-
nal of Bioinformatics, 19(1):135?143.
C.A. Goble, C.J. Wroe, R. Stevens, and the my-
Grid consortium. 2003. The myGrid project:
Services, architecture and demonstrator. In
S. Cox, editor, Proceedings of UK e-Science
All Hands Meeting 2003, Nottingham, UK.
http://www.nesc.ac.uk/events/ahm2003/AHMCD/.
R. Grishman. 1997. Information extraction: Techniques
and challenges. In Maria Teresa Pazienza, editor, In-
formation Extraction, pages 10?27. Springer Verlag.
U. Hahn, M. Romacker, and S. Schulz. 2002. Creating
knowledge repositories from biomedical reports: the
medSynDiKATe text mining system. In Proceedings
of the Pacific Symposium on Biocomputing, pages 338?
349.
L. Humphreys, D.A.B. Lindberg, H.M. Schoolman, and
G.O. Barnett. 1998. The Unified Medical Language
System: An informatics research collaboration. Jour-
nal of the American Medical Informatics Association,
1(5):1?13.
K. Humphreys, G. Demetriou, and R. Gaizauskas. 2000.
Two applications of information extraction to biolog-
ical science journal articles: Enzyme interactions and
protein structures. In Proceedings of the Pacific Sym-
posium on Biocomputing, pages 505?516.
A.G. Murzin, S.E. Brenner, T. Hubbard, and C. Chothia.
1995. SCOP: A structural classification of proteins
database for the investigation of sequences and struc-
tures. Journal of Molecular Biology, (247):536?540.
(http://scop.mrc-lmb.cam.ac.uk/scop/).
Online Mendelian Inheritance in Man, OMIM (TM).
2000. McKusick-Nathans Institute for Genetic
Medicine, Johns Hopkins University (Baltimore, MD)
and National Center for Biotechnology Informa-
tion, National Library of Medicine (Bethesda, MD).
http://www.ncbi.nlm.nih.gov/omim/.
J. Pustejovsky, J. Castan?o, R. Saur??, A. Rumshisky,
J. Zhang, and W. Luo. 2002. Medstract: Creat-
ing large-scale information servers for biomedical li-
braries. In Proceedings of the Workshop on Natural
Language Processing in the Biomedical Domain, As-
sociation for Computational Linguistics 40th Anniver-
sary Meeting (ACL-02), pages 85?92.
A. Rector, J. Rogers, A. Taweel, D. Ingram, D. Kalra,
J. Milan, R. Gaizauskas, M. Hepple, D. Scott,
and R. Power. 2003. Joining up health care
with clinical and post-genomic research. In
S. Cox, editor, Proceedings of UK e-Science
All Hands Meeting 2003, Nottingham, UK.
http://www.nesc.ac.uk/events/ahm2003/AHMCD/.
C.T. Rindflesch, J.V. Rajan, and L. Hunter. 2000. Ex-
tracting molecular binding relationships from biomed-
ical text. In Proceedings of the 6th Applied Natu-
ral Language Processing conference / North American
chapter of the Association for Computational Linguis-
tics annual meeting, pages 188?915.
The Gene Ontology Consortium. 2001. Creating the
gene ontology resource: design and implementation.
Genome Research, 11(8):1425?1433.
J. Thomas, D. Milward, C. Ouzounis, and S. Pulman.
2000. Automatic extraction of protein interactions
from scientific abstracts. In Proceedings of the Pacific
Symposium on Biocomputing, pages 538?549.
H.M. Wain, M. Lush, F. Ducluzeau, and S. Povey.
2002. Genew: The human nomenclature
database. Nucleic Acids Research, 30(1):169?171.
(http://www.gene.ucl.ac.uk/nomenclature/).
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 206?213,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Using Natural Language Processing to Identify Pharmacokinetic Drug-
Drug Interactions Described in Drug Package Inserts  
Richard Boyce, PhD 
University of Pittsburgh 
5607 Baum Avenue 
 Pittsburgh, PA 15206, USA 
rdb20@pitt.edu 
Gregory Gardner, MS 
University of Pittsburgh 
5607 Baum Avenue 
 Pittsburgh, PA 15206, USA
 gag30@pitt.edu  
Henk Harkema, PhD 
University of Pittsburgh 
6425 Penn Ave. 
Pittsburgh, PA 15206 
 hendrik.harkema@nuance.com 
 
 
Abstract 
The package insert (aka drug product label) is 
the only publicly-available source of infor-
mation on drug-drug interactions (DDIs) for 
some drugs, especially newer ones. Thus, an 
automated method for identifying DDIs in 
drug package inserts would be a potentially 
important complement to methods for identi-
fying DDIs from other sources such as the 
scientific literature. To develop such an algo-
rithm, we created a corpus of Federal Drug 
Administration approved drug package insert 
statements that have been manually annotated 
for pharmacokinetic DDIs by a pharmacist 
and a drug information expert. We then evalu-
ated three different machine learning algo-
rithms for their ability to 1) identify 
pharmacokinetic DDIs in the package insert 
corpus and 2) classify pharmacokinetic DDI 
statements by their modality (i.e., whether 
they report a DDI or no interaction between 
drug pairs). Experiments found that a support 
vector machine algorithm performed best on 
both tasks with an F-measure of 0.859 for 
pharmacokinetic DDI identification and 0.949 
for modality assignment. We also found that 
the use of syntactic information is very helpful 
for addressing the problem of sentences con-
taining both interacting and non-interacting 
pairs of drugs. 
1 Introduction 
Package inserts (PIs, aka drug product label) are 
the primary source of information for newly ap-
proved drugs and a potentially authoritative source 
of drug information from a medical-legal stand-
point (Marroum & Gobburu 2002). Among the 
information provided by PIs are drug-drug interac-
tions (DDIs): known and predicted drug combina-
tions that could lead to a clinically meaningful 
alteration in the effect of one of the drugs. The 
United States Federal Drug Administration (FDA) 
mandates that PIs for FDA-approved drugs include 
both observed and predicted clinically significant 
DDIs, as well as the results of pharmacokinetic 
studies that establish the absence of effect (FDA. 
2010). Moreover, the PI is the only publically-
available source of information on DDIs for some 
drugs, especially newer ones (Dal-R? et al 2010). 
Hence, an automated method for identifying DDIs 
from drug PIs would be an important complement 
to methods for identifying DDIs from other 
sources such as the scientific literature. In this pa-
per we describe the creation of a new corpus of 
FDA-approved drug package insert statements that 
have been manually annotated for pharmacokinetic 
DDIs. We then discuss how three different ma-
chine learning algorithms were evaluated for their 
ability to 1) identify pharmacokinetic DDIs in drug 
package inserts and 2) classify pharmacokinetic 
DDI statements by their modality (i.e., whether 
they report a DDI or that a drug pair does not in-
teract).  
2 Materials and Methods 
2.1 The DDI Corpus and Schema 
A corpus of annotated statements derived from 
FDA-approved drug PIs was created for use as 
training and test data while developing automated 
DDI extraction algorithms. The statements were 
derived from PIs using a strategy that ensured there 
206
would be a representative sample of statements 
that 1) unambiguously identified interacting drug 
pairs, 2) unambiguously identified non-interacting 
drug pairs, and 3) included no mention of interact-
ing drug pairs. Previous experience by our research 
group suggested that the manner in which DDI 
statements are described in PIs has changed over 
time in response to changing FDA regulations. 
Most notably, an FDA guidance document issued 
in 1999 was (to our knowledge) the first to explic-
itly suggest the inclusion of brief descriptions of 
pharmacokinetic DDI studies within specific sec-
tions of drug PIs (FDA. 1999). To account for this, 
investigators selected 64 PIs using a strategy that 
ensured the corpus would have a balanced sample 
of statements from drugs marketed before and after 
2000. For the purpose of this study we designated 
all PIs for drugs marketed prior to 2000 as ?older? 
and those for drugs marketed in or after 2000 as 
?newer.? PIs were downloaded from the DailyMed 
website,1 and the entire ?Drug Interactions? and 
?Clinical Pharmacology? sections were selected as 
text sources from ?newer? PIs. For ?older? PIs, 
which often lacked these two sections, investiga-
tors chose a section containing an apparent interac-
tion statement and one randomly-selected section. 
DDIs are typically classified as occurring by 
either pharmacodynamic or pharmacokinetic 
mechanisms. A pharmacodynamic DDI involves 
the additive or synergistic amplification of a drug?s 
effect. In a pharmacokinetic (PK) DDI, one drug, 
called a precipitant, affects (inhibits or induces) 
the absorption, distribution, metabolism, or excre-
tion of another drug, called the object. To simplify 
our task, we decided to focus specifically on PK 
DDIs. Prior to annotating the PI statements, a 
schema was created for the entities that the investi-
gators considered important components of a PK 
DDI.  The schema modeled drugs as having two 
characteristics, type and role. The type of drug 
could be active ingredient (e.g., simvastatin), 
drug product (e.g., Zocor), or metabolite 
(e.g., beta-OH-simvastatin). Drugs annotated as 
metabolite also referred to the active ingre-
dient parent compound. The role of a drug could 
be either an object or a precipitant. Two oth-
er properties were provided to model each PK 
DDI: 1) whether the statement from which the DDI 
was identified suggested an observed effect or a 
                                                          
1 http://dailymed.nlm.nih.gov/   
lack of an observed effect between two coadminis-
tered drugs (i.e., positive vs negative modali-
ty statements), and 2) whether the statement 
included quantitative or qualitative data in describ-
ing an interaction or non-interaction between a 
drug pair (i.e., quantitative vs qualitative 
statements). Finally, the segment of text in which 
the interaction claim was made was annotated as 
an interaction phrase. With the corpus and 
schema in place, drugs and PK DDIs present in the 
PI statements were then annotated by two inde-
pendent reviewers using Knowtator, an annotation 
tool integrated with the Prot?g? ontology editor 
(Ogren 2006).   
One annotator was a pharmacist and DDI 
expert, and the other a librarian specializing in 
drug information retrieval. To help the annotators, 
co-investigator RB ran the NCBO Annotator (Jon-
quet, Shah & Musen 2009) over the corpus using 
the RxNorm drug terminology (Nelson et al 2011) 
to pre-annotate as many active ingredients and 
drug products as possible. The annotators reviewed 
these ?pre-annotations? while identifying entities 
that missed during the pre-annotation process. Co-
investigator HH used Knowtator to calculate inter-
annotator agreement statistics from the annotators? 
initial annotation sets. RB then worked with the 
two annotators to achieve consensus on the final 
corpus of annotated DDI statements. 
2.2 Setting up the DDI statement extraction 
experiment 
Once the set of DDI annotations was compiled, we 
devised two machine learning tasks.  The first task 
was to determine whether two drugs mentioned in 
a statement taken from a PI are noted as either in-
teracting or not interacting with each other by 
pharmacokinetic mechanisms (i.e., does the state-
ment report a PK DDI with the drug pair of either 
a positive or negative modality?). The second task 
was to determine the modality of a given PK DDI. 
The first task did not include determining the roles 
of the drugs if an interaction is found, i.e., which 
member of the pair of drug mentions is the precipi-
tant and which one is the object. To enable the ex-
ploration of the performance of multiple machine 
learning methods, we divided two-thirds of the 
annotated PI statements into a development set and 
one-third into a blind test set. PI statements anno-
tated as reporting DDIs were stratified within the 
207
two sets using a random selection method that en-
sured a representative balance of sentence distance 
between drug mentions, DDI modality, DDI type, 
and drug PI age designation (see above).  State-
ments not containing an interaction were stratified 
by sentence distance between drug mentions, and 
PI age designation. Stratification was done on the 
level of statements. Thus, statements taken from 
the same package insert may have been distributed 
over the development and test set. 
We observed that 99% of corpus statements 
annotated as a PK DDI mentioned an interacting 
drug pair within a three sentence region. Thus, we 
created a baseline dataset by iterating through PI 
statements in the development set and identifying 
all drug pair mentions that occurred within a three-
sentence span. Throughout the remainder of this 
paper we refer to the statements identified by this 
process as instances.  
Instances containing drug pairs that were 
manually annotated as participating in an interac-
tion (either with positive or negative modality) 
were labeled as positive instances for the extraction 
task; all other pairs were labeled as negative in-
stances. Prior to generating features for machine 
learning, each instance was pre-processed. Num-
bers (e.g. ?1?, ?34?, ?5.2?, etc.) were replaced by 
the string ?num? to make them more meaningful to 
a learning algorithm across instances. This allowed 
the algorithm to associate numerical references 
with each other using a general pattern, instead of 
learning phrases with specific numbers (e.g. the 
phrase ?num mg? may be significant, whereas ?10 
mg? may be less significant).  Similarly, to abstract 
away from specific names, the names of drug 
products, active ingredients, and metabolites in 
each statement were replaced by the string 
?drugname?. This forces the learning algorithm to 
generalize over the participants of interactions, 
preventing it from identifying interactions based on 
the identity of the participants. 
In the baseline dataset, each instance?s pre-
processed sentence text was translated to bigrams 
using TagHelper, a text analysis program written 
on top of the Weka machine learning software 
(Hall et al 2009; Ros? et al 2008).  Bigrams are a 
comprehensive set of consecutive word pairs that 
appear in a sentence.  Words in bigrams were 
stemmed by TagHelper to facilitate learning more 
general concepts conveyed by phrases. For exam-
ple, the commonly occurring phrases ?increases 
auc? and ?increased auc? are stemmed to ?increase 
auc? and then merged to the bigram. The baseline 
set of instances was loaded into Weka and three 
models were built using three different machine 
learning algorithms.  The three algorithms were a 
rule learner (?JRip?), a decision tree (?J48?), and 
an SVM algorithm (?SMO?).  Algorithm parame-
ters were left at Weka defaults and 10-fold cross-
validation was used to develop each model. 
Exploration of Weka predictions from the 
baseline dataset showed that a major source of con-
fusion for the machine learning algorithms was an 
inability to distinguish between pairs of drugs that 
do and do not interact within the same sentence. A 
frequent source of this kind of occurrence in the 
package insert text was coordinate structures such 
as ?Drug A interacts with Drugs B and C?, where 
?B and C? is a coordinate structure.  For such sen-
tences, the baseline dataset contains the interacting 
pairs (A,B) and (A,C), along with the non-
interacting pair (B,C).  However, because all three 
pairs are represented by the same set of bigrams, it 
is obvious that information from bigrams alone is 
insufficient to distinguish which pairs interact and 
which simply co-occur within the sentence.  
Another problem was that of multiple men-
tions of the same drug within an instance?s sen-
tence span, as, for example, in the sentence ?Co-
administration of A and B leads to increased AUC 
levels for B.? Because the annotators had identified 
only one drug mention per annotated interaction, 
the algorithms incorrectly considered other men-
tions of the same drug as part of a non-interacting 
pair. Two solutions were implemented to help alle-
viate these problems.  First, the dataset was con-
densed to a set of instances with unique drug pairs 
and sentence spans.  If any of the baseline instanc-
es contributing to the condensed instance contained 
interactions, the condensed instance was said to 
contain an interaction. In this way, multiple drug 
mentions within a sentence span containing an in-
teraction would translate to a single instance repre-
senting an interaction between the two drugs. 
Second, two natural language dependency 
parsers were used to extract extra features from the 
sentence text for each instance: the Stanford NLP 
Parser (Klein & Manning 2003) and ClearParser 
(Choi 2011).  Following approaches to relation 
extraction proposed in other domains e.g., (Bunes-
cu & Mooney 2005), the dependency structure 
produced by each parser was searched for the 
208
shortest path between the pair of drug mentions of 
the instance.  The words on this path were 
stemmed using the Stanford NLP Tools stemmer 
(Stanford NLP 2011), and added to the dataset as 
the instance?s ?syntactic path?. 
Once a statement is classified as describing a 
PK DDI between two drugs, it is important to 
know if there is an observed effect or a lack of ef-
fect between two coadministered drugs (i.e., posi-
tive vs negative modality statements). To present 
the learning algorithms with the most relevant 
training data, modality prediction was treated as a 
separate task from interaction prediction.  Devel-
opment and test sets were created in the same 
manner as for interaction prediction, however in-
stances that did not represent interactions were ex-
cluded. Only bigram features were used for 
modality prediction.  Model training and testing 
proceeded in the same manner as for interaction 
prediction. 
3 Results 
A total of 208 multi-sentence sections were ex-
tracted from 64 PIs. Prior to consensus, inter-
annotator agreement between the two annotators 
on PK DDI, active ingredient, drug product, me-
tabolite mentions and was found to be 60%, 
96.3%, 99.5%, and 60.8% respectively. The major-
ity of disagreements about DDIs were due to a ten-
dency of one annotator to incorrectly annotate 
some pharmacodynamic DDIs as PK DDIs. Also, 
one annotator incorrectly assumed that all metabo-
lites had been pre-annotated and so did not actively 
attempt to annotate metabolite entities. These and 
other minor issues were corrected and full consen-
sus was reached by both annotators. The final drug 
package insert PK DDI corpus contains 592 PK 
DDIs, 3,351 active ingredient mentions, 234 drug 
product mentions, and 201 metabolite mentions.2 
Tables 1 and 2 provide more details on the mo-
dality and drug types present in the 592 consensus 
PK DDI statements. Table 1 shows that 388 state-
                                                          
2 http://purl.org/NET/nlprepository/PI-PK-DDI-Corpus  
ments indicated that a PK DDI would occur be-
tween a drug pair, while 204 statements indicated 
that an interaction would not occur. The table also 
shows that 204 statements reported quantitative 
measures while 388 did not. Table 2 shows that the 
majority (86%) of PK DDI statements reported 
interactions by stating the two active ingredients 
involved in the DDI, with a much smaller propor-
tion using a drug product in the description. Also, 
35 DDI statements reported an effect on a drug 
metabolite. 
A total of 11,048 PI instances were generated 
for the baseline dataset. This was reduced to 5,015 
instances after condensing the instances down to 
unique drug pairs and sentence spans. In the final 
dataset, about a third of instances were drug pairs 
within the same sentence (1,583). The rest were 
split between drug pairs in adjacent sentences 
(1,717), and drug pairs with two sentences of sepa-
ration (1,715).  The dataset included 542 interac-
tions of which 493 included the drug pair within a 
single sentence.  355 interactions were positive 
modality and 187 negative; 360 were qualitative, 
182 quantitative. 1,636 instances were categorized 
as ?new? based on drug release data while 3,379 
were classified as ?old?. 
Results for interaction and modality prediction 
are shown in Table 3. For both the interaction and 
modality prediction tasks, the SVM algorithm 
(SMO) outperformed the rule learner (Jrip) and 
decision tree (J48). On the test set which was not 
used in training, the SVM classifier identified PK 
DDIs with an F-measure of 0.859 vs 0.762 for the 
rule learner and 0.802 for the decision tree algo-
rithm. All algorithms performed quite well on the 
modality classification task but the SVM algorithm 
performed best with an F-measure of 0.949 vs 
0.929 (rule learner) and 0.917 (decision tree). 
4 Discussion 
The automatic identification of DDIs in unstruc-
tured text is a topic that is gaining much interest. 
This work makes an important contribution to the 
field by being the first to demonstrate that machine 
learning can be applied quite effectively to the task 
of extracting PK DDIs from FDA-approved PIs.  
Interaction Type   
Modality Qualitative Quantitative Total 
Negative 202 2 204 
Positive 186 202 388 
Total 388 204 592 
Table 1. PK DDI statement modality shown by in-
teraction type. 
209
Object Type   
Precipitant Type Active ingredient Drug product Metabolite Total 
Active ingredient 506 14 34 554 
Drug product 37 - 1 38 
Total 543 14 35 592 
Table 2. A summary of consensus annotated PK DDIs by precipitant and object type. 
As our work focuses on extracting PK DDIs, it is 
most similar to that of Karnik et al (Karnik et al 
2011) who explored the performance of an ?all 
paths? graph kernel (Airola et al 2008) on a corpo-
ra of PK DDIs derived from 219 MEDLINE ab-
stracts. The best performing algorithm in their 
experiments had an F-measure of 0.658 which is 
considerably less than the F-measure of 0.859 that 
our SVM achieved. However, the two results are 
not directly comparable because of unknown dif-
ferences between the corpora. For example, it may 
be that PIs use more standard language patterns to 
report PK DDIs than what is found in MEDLINE 
abstracts. In future work we will explore how well 
the SVM algorithm performs over MEDLINE ab-
stracts and contrast any differences between the 
two DDI sources that might affect NLP. 
The only other project we are aware of that fo-
cused explicitly on extracting PK DDIs from un-
structured text is that of Tari et al (Tari et al 
2010), who evaluated a rule-based algorithm for 
extracting PK DDIs from papers and abstracts in 
the scientific literature. In this study the authors 
distinguished between explicit DDIs (statements 
indicating a direct observation of a PK effect from 
a give drug combination) and implicit DDIs (DDIs 
that can be inferred based on claims about drug 
metabolic properties extracted from scientific 
texts). The algorithm was ran over ~17 million 
MEDLINE abstracts and the output DDIs were 
compared with a reference standard set of 494 
DDIs identified manually from 265 DrugBank 
drug pages. The algorithm?s recall of DrugBank 
interactions was only 12%. However, a manual 
inspection of the results found that 78% of the 
DDIs extracted by the algorithm were valid based 
on the source texts, even though they were not pre-
sent in their reference standard. These results are 
important because they suggest that the set of DDIs 
present in DrugBank are incomplete and highlight 
the need for corpora derived from other text 
sources such as the one we developed from drug 
PIs for this study. 
A larger body of research exists for the task of 
extracting DDIs of any type (i.e., PK or pharmaco-
dynamic DDIs). Ten research papers were present-
ed at the recent ?Challenge Task on Drug-Drug 
Interaction Extraction? held at the 2011 SemEval 
Conference (Segura-Bedmar, Martinez & Sanchez-
Cisneros 2011).  All systems in this challenge were 
tested against the ?DrugDDI corpus?; a set of 579 
documents from the DrugBank database with 
3,160 manually-annotated DDIs (Segura-Bedmar, 
Martinez & Pablo-Sanchez 2010). The best per-
forming system in this challenge utilized an en-
semble learning approach (Thomas et al 2011) and 
produced an F-measure of 0.657. The  second best 
performing method utilized composite kernels, a 
method that combines feature-based and kernel-
based methods, and was found to perform with an 
F-measure of 0.64 (Chowdhurry et al 2011). Airo-
la et als ?all paths? graph kernel (mentioned 
above) performed much more poorly on the Drug-
DDI corpora than on the Karnik?s PK-DDI corpus 
(F-measure 0.16 vs 0.658). The authors note that 
there were significant differences between in the 
two corpora with regards to the length and com-
plexity of the sentences reporting DDIs . 
To the best of our knowledge, only one other 
NLP study that has focused specifically on drug 
interactions reported in drug product labeling (Ru-
brichi & Quaglini 2012). The investigators com-
pared the ability of an SVM classifier and a 
conditional random fields (CRF) classifier for as-
signing 13 semantic labels to Italian language text 
present in the interaction section of  ?Summary of 
Product Characteristics? documents (the Italian 
equivalent of PIs). The investigators explored the 
influence of a range of features on classifier per-
formance, including orthographical, neighboring 
word, syntactic, parts of speech, and dictionary 
features. When all features were employed, the 
SVM had slightly better performance than the CRF 
classifier (micro-averaged F-measure: 91.41 vs 
91.13, macro-averaged F-measure: 84.99 vs 
80.83).  
 
210
Jrip J48 SMO 
Model (dataset) Prec Recall F Prec Recall F Prec Recall F 
Baseline (development) 0.588 0.656 0.62 0.584 0.573 0.578 0.639 0.677 0.658 
Stanford Parser (develop-
ment) 0.762 0.68 0.719 0.809 0.804 0.807 0.851 0.815 0.833 
ClearParser (development) 0.787 0.793 0.79 0.822 0.791 0.806 0.828 0.887 0.856 
Stanford Parser (test) 0.778 0.665 0.717 0.828 0.832 0.83 0.843 0.838 0.84 
ClearParser (test) 0.764 0.76 0.762 0.85 0.76 0.802 0.836 0.883 0.859 
Modality (test) 0.963 0.897 0.929 0.887 0.948 0.917 0.941 0.957 0.949 
Table 3. Results for interaction prediction on the baseline, development, and blind test set. Also shown are re-
sults for modality prediction for the blind test set (results over the development set are similar but not shown). 
One key difference between the Rubrichi study 
and ours is that the task of tagging unstructured 
text with semantic elements that describe a DDI is 
not the same as classifying whether or not a state-
ment containing a drug pair is reporting a DDI be-
tween the drugs. The difference is especially 
apparent when considering coordinate structures 
such as ?Drug A interacts with Drugs B and C?, 
Semantic tagging would be useful for identifying 
the drug entities but is not useful (on its own) for 
identifying which of the three drug pairs interact 
with each other.  
It is interesting to note that most recent work on 
DDI extraction had not made the distinction be-
tween PK and pharmacodynamic DDIs that is 
standard in the fields of pharmacology and phar-
macy. This distinction might be relevant to DDI 
extraction because the two types of interactions are 
discovered in distinct ways that might lead to sig-
nificant differences in how they are described in 
scientific documents. For example, there is a fairly 
standard set of in vitro experiments and clinical 
trials that have been a routine part of drug devel-
opment for more than a decade (FDA. 1999). The 
same is not true for pharmacodynamic DDIs, 
which are more challenging to study because they 
involve additive and synergistic effects that are not 
necessarily related to a drug?s dose or clearance. 
Since it is reasonable that the methods used to in-
vestigate a DDI strongly influences its description, 
we think future work should examine if PK and 
pharmacodynamic DDI descriptions are different 
enough to warrant distinct DDI extraction efforts. 
An error analysis of the final dataset suggested 
some reasons for cases where the machine learning 
algorithms misclassified instances. Instances that 
were not interactions, but were classified as such, 
contained a large number of sentences with de-
scriptions of studies or biochemical processes and 
measurements.  These types of statements may 
share a number of features with actual interactions 
(e.g. numerical data, changing levels of drug, etc.) 
without containing an interaction.  There also re-
main cases where several drug names occur and 
the classifiers were unable to differentiate between 
the interacting pair and non-interacting pairs. Un-
fortunately, no such clear pattern was apparent for 
instances that descrived interactions, but were clas-
sified as containing no interaction statement. A 
number of large sentences were observed in these 
instances, suggesting sentence complexity may 
play a role, increasing the difficulty of natural lan-
guage parsing. 
Analysis of the attribute weights assigned by 
the SVM  algorithm (SMO) after training for inter-
action prediction shows some commonality regard-
less of whether the data was processed by the 
Stanford Parser or the ClearParser. For example, 
19 out of the 20 most significant features identified 
by the algorithm from the dataset when processed 
by the Stanford Parser were words on the syntactic 
path; one less than when the dataset was processed 
by the ClearParser. Common significant features 
include words such as ?coadminister?, ?auc?, 
?pharmacokinetic?, and ?absorption?.  The algo-
rithm placed greater importance on the words ?in-
crease? and ?decrease? when the dataset was 
processed by the Stanford Parser, while the words 
?reduce? and ?enhance? received greater attribute 
weights when the data was processed by the 
ClearParser. A similar analysis of the SVM algo-
rithm developed for PK DDI modality prediction 
shows that bigrams with the words ?no? or ?not? 
are clearly the features of most importance to the 
model. 
211
We also note that the algorithm?s performance 
on the test set of PI statements is very similar to 
the algorithm?s performance over the development 
set (see Table 3). We think that this finding is 
largely due to the careful stratification approach 
we used when creating the development and test 
sets. It might also be possible that the features in 
the unstructured PI text do not vary greatly be-
tween PIs regardless of their age. However, Table 
2 shows that our PK DDI corpora had considerable 
variation in terms of quantitative vs qualitative and 
positive vs negative DDI statements. Thus, we an-
ticipate that the SVM algorithm?s performance will 
be maintained when ran against a much larger PI 
corpus and future work will test how well the algo-
rithm generalizes to other sets of PIs.  
5 Conclusion 
We created a new, publically available, corpus of 
FDA-approved drug PI statements that have been 
manually annotated for PK DDIs by a pharmacist 
and a drug information expert. Also, we evaluated 
three different machine learning algorithms for 
their ability to 1) identify PK DDIs in drug PIs and 
2) classify PK DDI statements by their modality 
(i.e., whether they report a DDI or no interaction 
between drug pairs). Experiments found that an 
SVM algorithm performed best on both tasks with 
an F-measure of 0.859 for PK DDI identification 
and 0.949 for modality assignment. We found that 
the use of syntactic information is very helpful for 
addressing the problem of sentences containing 
both interacting and non-interacting pairs of drugs. 
The strong performance of our algorithm for PK 
DDIs suggests that approaching pharmacokinetic 
and pharmacodynamic interactions as different 
NLP tasks is a potentially promising approach for 
advancing automated DDI extraction. Given the 
marked difference in performance between our 
extraction methods and previous work, we are 
planning further experiments to establish whether 
this difference reflects the comparative simplicity 
of the extraction task represented by our corpus, 
some specific strength of the applied extraction 
methods, or some other factor. 
Acknowledgement 
This project was funded by grant K12-HS019461 
from the Agency for Healthcare Research and 
Quality (AHRQ). The content is solely the respon-
sibility of the authors and does not represent the 
official views of AHRQ. We also thank John Horn, 
PharmD (University of Washington) and Mr. Rob 
Guzman (University of Pittsburgh) for their work 
annotating the corpus and identifying related re-
search. 
References 
Airola, Antti, Sampo Pyysalo, Jari Bj?rne, Tapio 
Pahikkala, Filip Ginter & Tapio Salakoski. 
2008. All-paths graph kernel for protein-
protein interaction extraction with evaluation 
of cross-corpus learning. BMC Bioinformatics 
9(Suppl 11). S2. doi:10.1186/1471-2105-9-
S11-S2 (3 May, 2012). 
Bunescu, Razvan C. & Raymond J. Mooney. 2005. A 
shortest path dependency kernel for relation 
extraction. Proceedings of the conference on 
Human Language Technology and Empirical 
Methods in Natural Language Processing, 
724?731. (HLT  ?05). Stroudsburg, PA, USA: 
Association for Computational Linguistics. 
doi:10.3115/1220575.1220666. 
http://dx.doi.org/10.3115/1220575.1220666 (2 
May, 2012). 
Choi, Jinho. 2011. ClearParser GoogleCode page. 
clearparser. 
http://code.google.com/p/clearparser/ (10 De-
cember, 2011). 
Chowdhurry, Md. Faisal Mahbub, Asma Ben Abacha, 
Alberto Lavelli & Pierre Zweigenbaum. 2011. 
Two Different Machine Learning Techniques 
for Drug-Drug Interaction Extraction. 1st Chal-
lenge task on Drug-Drug Interaction Extrac-
tion (DDIExtraction 2011), 19?26. Huelva, 
Spain. 
Dal-R?, R., A. Pedromingo, M. Garc?a-Losa, J. Lahuer-
ta & R. Ortega. 2010. Are results from phar-
maceutical-company-sponsored studies 
available to the public? European Journal of 
Clinical Pharmacology 66(11). 1081?1089. 
doi:10.1007/s00228-010-0898-y (5 August, 
2011). 
FDA. 1999. FDA Guideline: In Vivo Drug Metabo-
lism/Drug Interaction Studies ? Study Design, 
Data Analysis, and Implications for Dosing 
and Labeling. Rockville, MD: Food and Drug 
Administration. 
http://www.fda.gov/downloads/Drugs/Guidanc
eComplianceRegulatoryInfor-
mation/Guidances/ucm072119.pdf. 
FDA. 2010. CFR - Code of Federal Regulations Title 
21. 
212
http://www.accessdata.fda.gov/scripts/cdrh/cfd
ocs/cfcfr/CFRSearch.cfm?fr=201.57 (7 June, 
2011). 
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann & Ian H Witten. 
2009. The WEKA data mining software: an 
update. SIGKDD Explorations 11(1). 10?18. 
Jonquet, Clement, Nigam H Shah & Mark A Musen. 
2009. The open biomedical annotator. Summit 
on Translational Bioinformatics 2009. 56?60. 
(10 December, 2011). 
Karnik, Shreyas, Abhinita Subhadarshini, Zhiping 
Wang, Luis M Rocha & Lang Li. 2011. Extrac-
tion Of Drug-Drug Interactions Using All 
Paths Graph Kernel. 1st Challenge task on 
Drug-Drug Interaction Extraction (DDIExtrac-
tion 2011). Huelva, Spain. 
Klein, Dan & Christopher D Manning. 2003. Fast Exact 
Inference with a Factored Model for Natural 
Language Parsing. (Ed.) S Thrun S Becker & 
Keditors Obermayer. Science 15. 3?10. 
Marroum, P.J. & J. Gobburu. 2002. The product label: 
how pharmacokinetics and pharmacodynamics 
reach the prescriber. Clinical Pharmacokinetics 
41(3). 161?169. (7 June, 2011). 
Nelson, Stuart J, Kelly Zeng, John Kilbourne, Tammy 
Powell & Robin Moore. 2011. Normalized 
names for clinical drugs: RxNorm at 6 years. 
Journal of the American Medical Informatics 
Association: JAMIA 18(4). 441?448. 
doi:10.1136/amiajnl-2011-000116 (10 Decem-
ber, 2011). 
Ogren, Philip V. 2006. Knowtator: a Prot?g? plug-in for 
annotated corpus construction. Proceedings of 
the 2006 Conference of the North American 
Chapter of the Association for Computational 
Linguistics on Human Language Technology, 
273?275. Morristown, NJ, USA: Association 
for Computational Linguistics. 
doi:http://dx.doi.org/10.3115/1225785.122579
1. 
Ros?, Carolyn, Yi-Chia Wang, Yue Cui, Jaime Arguel-
lo, Karsten Stegmann, Armin Weinberger & 
Frank Fischer. 2008. Analyzing collaborative 
learning processes automatically: Exploiting 
the advances of computational linguistics in 
computer-supported collaborative learning. In-
ternational Journal of Computer-Supported 
Collaborative Learning 3(3). 237?271. 
doi:10.1007/s11412-007-9034-0 (10 Decem-
ber, 2011). 
Rubrichi, S & S Quaglini. 2012. Summary of Product 
Characteristics content extraction for a safe 
drugs usage. Journal of Biomedical Informatics 
45(2). 231?239. doi:10.1016/j.jbi.2011.10.012 
(3 May, 2012). 
Segura-Bedmar, Isabel, Paloma Martinez & Cesar 
Pablo-Sanchez. 2010. Extracting drug-drug in-
teractions from biomedical texts. Workshop on 
Advances in Bio Text Mining, vol. 11 Suppl 5, 
9. Madrid, Spaim: BMC Bioinformatics. 
http://www.biomedcentral.com/1471-
2105/11/S5/P9. 
Segura-Bedmar, Isabel, Paloma Martinez & Daniel 
Sanchez-Cisneros (eds.). 2011. Proceedings of 
the First Challenge Task: Drug-Drug Interac-
tion Extraction 2011. Huelva, Spain. 
http://sunsite.informatik.rwth-
aachen.de/Publications/CEUR-WS/Vol-761/ (9 
December, 2011). 
Stanford NLP. 2011. The Stanford NLP (Natural Lan-
guage Processing) Group. 
http://nlp.stanford.edu/software/ (10 December, 
2011). 
Tari, Luis, Saadat Anwar, Shanshan Liang, James Cai & 
Chitta Baral. 2010. Discovering drug-drug in-
teractions: a text-mining and reasoning ap-
proach based on properties of drug metabolism. 
Bioinformatics (Oxford, England) 26(18). 
i547?553. doi:10.1093/bioinformatics/btq382 
(9 December, 2011). 
Thomas, Philippe, Mariana Neves, Illes Solt, Domonkos 
Tikk & Ulf Leser. 2011. Relation Extraction 
for Drug-Drug Interactions using Ensemble 
Learning. 1st Challenge task on Drug-Drug In-
teraction Extraction (DDIExtraction 2011), 
11?18. Huelva, Spain. 
 
213
