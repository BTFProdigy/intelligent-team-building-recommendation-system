Detecting Errors in Corpora Using Support Vector Machines
Tetsuji Nakagawa? and Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916?5 Takayama, Ikoma, Nara 630?0101, Japan
nakagawa378@oki.com, matsu@is.aist-nara.ac.jp
Abstract
While the corpus-based research relies on hu-
man annotated corpora, it is often said that a
non-negligible amount of errors remain even in
frequently used corpora such as Penn Treebank.
Detection of errors in annotated corpora is im-
portant for corpus-based natural language pro-
cessing. In this paper, we propose a method
to detect errors in corpora using support vec-
tor machines (SVMs). This method is based
on the idea of extracting exceptional elements
that violate consistency. We propose a method
of using SVMs to assign a weight to each ele-
ment and to find errors in a POS tagged corpus.
We apply the method to English and Japanese
POS-tagged corpora and achieve high precision
in detecting errors.
1 Introduction
Corpora are widely used in natural language
processing today. For example, many statistical
part-of-speech (POS) taggers have been devel-
oped and they use corpora as the training data
to obtain statistical information or rules (Brill,
1995; Ratnaparkhi, 1996). For natural language
processing systems based on a corpus, the quan-
tity and quality of the corpus affect their per-
formance. In general, corpora are annotated
by hand, and therefore are error-prone. These
errors are problematic for corpus-based sys-
tems. The errors become false training exam-
ples and deteriorate the performance of the sys-
tems. Furthermore, incorrect instances may be
used as testing examples and prevent the accu-
rate measurement of performance. Many stud-
ies and improvements have been conducted for
? Presently with Service Media Laboratory, Corporate
Research and Development Center, Oki Electric Industry
Co.,Ltd.
POS tagging, and major methods of POS tag-
ging achieve an accuracy of 96?97% on the Penn
Treebank WSJ corpus, but obtaining higher ac-
curacies is difficult (Ratnaparkhi, 1996). It is
mentioned that the limitation is largely caused
by inconsistencies in the corpus (Ratnaparkhi,
1996; Padro? and Ma`rquez, 1998; van Halteren
et al, 2001). Therefore, correcting the errors in
a corpus and improving its quality is important.
However, to find and correct errors in corpora
by hand is costly, since the size of corpora is
usually very large. Hence, automatic detection
of errors in corpora is necessary.
One of the approaches for corpus error de-
tection is use of machine learning techniques
(Abney et al, 1999; Matsumoto and Yamashita,
2000; Ma et al, 2001). These methods regard
difficult elements for a learning model (boosting
or neural networks) to learn as corpus errors.
Abney et al (1999) studied corpus error detec-
tion using boosting. Boosting assigns weights to
training examples, and the weights are large for
the examples that are difficult to classify. Misla-
beled examples caused by annotators tend to be
difficult examples to classify and these authors
conducted error detection of POS tags and PP
attachment information in a corpus by extract-
ing examples with a large weight.
Some probabilistic approaches for corpus er-
ror detection have also been studied (Eskin,
2000; Murata et al, 2000). Eskin (2000) con-
ducted corpus error detection using anomaly de-
tection. He supposed that all the elements in a
corpus are generated by a mixture model con-
sisting of two distributions, a majority distri-
bution (typically a structured distribution) and
an anomalous distribution (a uniform random
distribution), and erroneous elements are gen-
erated by the anomalous distribution. For each
element in a corpus, the likelihood of the mixed
model is calculated in both cases when the el-
ement is generated from the majority distribu-
tion and from the anomalous one. The element
is detected as an error if the likelihood in the
latter case is large enough.
In this paper, we focus on detection of er-
rors in corpora annotated with POS tags, and
propose a method for corpus error detection us-
ing support vector machines (SVMs). SVMs
are one of machine learning models and applied
to many natural language processing tasks with
success recently. In the next section, we explain
a method to use SVMs for corpus error detec-
tion.
2 Corpus Error Detection Using
Support Vector Machines
Training data for corpus error detection is usu-
ally not available, so we have to solve it as an
unsupervised learning problem. We consider in
the following way: in general, a corpus is built
according to a set of guidelines, thus it should
be consistent. If there is an exceptional element
in the corpus that jeopardizes consistency, it is
likely to be an error. Therefore, corpus error
detection can be conducted by detecting excep-
tional elements that causes inconsistency.
While this is a simple and straightforward
approach and any machine learning method is
applicable to this task, we will use SVMs as
the learning algorithm in the settings described
in Section 2.2. The advantage of using SVMs
in this setting is the following: In our setting,
each position in the annotated corpus receives
a weight according to the SVM algorithm and
these weights can be used as the confidence
level of erroneous examples. By effectively us-
ing those weights the inspection of the erroneous
parts can be undertaken in the order of the con-
fidence level, so that an efficient browsing of
corpus becomes possible. We believe this is a
particular advantage of our method compared
with the methods that use other machine learn-
ing methods.
2.1 Support Vector Machines
Support Vector Machines (SVMs) are a su-
pervised machine learning algorithm for binary
classification (Vapnik, 1998). Given l training
examples of feature vector xi ? RL with label
yi ? {+1,?1}, SVMs map them into a high di-
mensional space by a nonlinear function ?(x)
and linearly separate them. The optimal hy-
perplane to separate them is found by solving
the following quadratic programming problem:
minimize?1,...,?l
1
2
l?
i,j=1
?i?jyiyjK(xi,xj)?
l?
i=1
?i,
subject to 0 ? ?i ? C (1 ? i ? l),
l?
i=1
?iyi = 0,
where the function K(xi,xj) is the inner prod-
uct of the nonlinear function (K(xi,xj) =
?(xi) ??(xj)) called a kernel function, and the
constant C controls the training errors and be-
comes the upper bound of ?i. Given a test
example x, its label y is decided by summing
the inner products of the test example and the
training examples weighted by ?i:
y = sgn
( l?
i=1
?iyiK(xi,x) + b
)
,
where b is a threshold value. Thus, SVMs as-
sign a weight ?i to each training example. The
weights are large for examples that are hard for
SVMs to classify, that is, exceptional examples
in training data have a large weight. We con-
duct corpus error detection using the weights.
To detect exceptional examples in a corpus
annotated with POS tags, we first construct an
SVM model for POS tagging using all the el-
ements in a corpus as the training examples.
Note that each example corresponds to a word
in the corpus. Then SVMs assign weights to
the examples, and large weights are assigned to
difficult examples. Finally, we extract examples
with a large weight greater than or equal to a
threshold value ??. In the next subsection, we
describe how to construct an SVM model for
POS tagging.
2.2 Revision Learning for POS tagging
We use a revision learning method (Nakagawa
et al, 2002) for POS tagging with SVMs1. This
method creates training examples of SVMs with
1The well known one-versus-rest method (Allwein et
al., 2000) can be also used for POS tagging with SVMs,
but it has large computational cost and cannot han-
dle segmentation of words directly that is necessary for
Japanese morphological analysis.
binary labels for each POS tag class using a
stochastic model (e.g. n-gram) as follows: each
word in a corpus becomes a positive example
of its POS tag class. We then build a simple
stochastic POS tagger based on n-gram (POS
bigram or trigram) model, and words in the cor-
pus that the stochastic model failed to tag with
a correct part-of-speech are collected as nega-
tive examples of the incorrect POS tag class.
In such way, revision learning makes a model of
SVMs to revise outputs of the stochastic model.
For example, assume that for a sentence:
"11/CD million/CD yen/NNS are/VBP paid/VBN",
a stochastic model tags incorrectly:
"11/CD million/CD yen/NN are/VBP paid/VBN".
In this case, the following training examples
are created for SVMs (each line corresponds to
an example):
<Class (Label)> <Feature Vector>
CD (+1) (word:11, word-1:BOS, ...)
CD (+1) (word:million, word-1:11, ...)
NN (-1) (word:yen, word-1:million, ...)
NNS (+1) (word:yen, word-1:million, ...)
VBP (+1) (word:are, word-1:yen, ...)
VBN (+1) (word:paid, word-1:are, ...)
Thus, the positive and negative examples are
created for each class (POS tag), and a model
of SVMs is trained for each class using the
training examples.
In English POS tagging, for each word w in
the tagged corpus, we use the following features
for SVMs:
1. the POS tags and the lexical forms of the
two words preceding w;
2. the POS tags and the lexical forms of the
two words succeeding w;
3. the lexical form of w and the prefixes and
suffixes of up to four characters, the exis-
tence of numerals, capital letters and hy-
phens in w.
Japanese morphological analysis can be con-
ducted with revision learning almost in the same
way as English POS tagging, and we use the fol-
lowing features for a morpheme ?:
1. the POS tags, the lexical forms and the in-
flection forms of the two morphemes pre-
ceding ?;
2. the POS tags and the lexical forms of the
two morphemes succeeding ?;
3. the lexical form and the inflection form of
?.
2.3 Extraction of Inconsistencies
So far, we discussed how to detect exceptional
elements in a corpus. However, it is insuffi-
cient and inconvenient for corpus error detec-
tion, because an exceptional element is not al-
ways an error, that is, an exceptional element
may be a correct or an incorrect exceptional el-
ement. Furthermore, it is often difficult to judge
whether it is a true error or not when only the
exceptional element is shown. To solve these
problems, we extract not only an exceptional
example but also another similar example that
is inconsistent with the exceptional example. If
the exceptional example is correct, the second
example is likely to be an error, and vice versa.
We assume that an inconsistency occurs when
two examples have similar features but have op-
posite labels. The similarity between two ex-
amples xi and xj on SVMs is measured by the
following distance:
d(xi,xj) =
?
??(xi)??(xj)?2,
=
?
K(xi,xi) +K(xj ,xj)? 2K(xi,xj).
We can extract inconsistencies from a corpus
as follows: given an example x which was de-
tected as an exceptional example (following the
proposal in the previous subsection), we extract
an example z with the smallest values of the
distance d(x, z) from the examples whose label
is different from x. Intuitively, z is a closest
opposite example to x in the SVMs? higher di-
mensional space and may be a cause for x to be
attached a large weight.
3 Experiments
We perform experiments of corpus error detec-
tion using the Penn Treebank WSJ corpus (in
English), the RWCP corpus (in Japanese) and
the Kyoto University Corpus (in Japanese). In
the following experiments, we use SVMs with
second order polynomial kernel, and the upper
bound value C is set to 1.
Table 1: Examples of Correctly Detected Errors and Incorrectly Detected Errors in the WSJ Corpus
Correctly Detected Errors
pay about 11 million yen/NNS ( $ 77,000 budgeted about 11 million yen/NN ( $ 77,500
, president and chief/JJ executive officer of named president and chief/NN executive officer
for its fiscal first quarter ended/VBN Sept. 30 its first quarter ended/VBD Sept. 30 was
Incorrectly Detected Errors
EOS 3/LS . EOS Send your child to Nov. 1-Dec . EOS 3/CD . EOS
3.1 Experiments on the Penn Treebank
WSJ Corpus (English)
Experiments are performed on the Penn Tree-
bank WSJ corpus, which consists of 53,113 sen-
tences (1,284,792 tokens).
We create models of SVMs for POS tag-
ging using the corpus with revision learning.
The distribution of the obtained weights ?i are
shown in Figure 1. The values of ?i concentrate
near the lower bound zero and the upper bound
C. The examples with ?i near the upper bound
seem to be exceptional. Therefore, we regarded
the examples with ?i ? 0.5 as exceptional ex-
amples (i.e. ?? = 0.5). As a result, 1,740 ele-
ments were detected as errors. We implemented
a browsing tool for corpus error detection with
HTML (see Figure 2). A detected inconsistency
pair is displayed in the lower part of the screen.
We examined by hand whether the detected er-
rors are true errors or not for the first 200 el-
ements in the corpus from the detected 1,740
elements, and 199 were actual errors and 1 was
not. The precision (the ratio of correctly de-
tected errors for all of the detected errors) was
99.5%. Examples of correctly detected errors
and incorrectly detected errors from the corpus
are shown in Table 1. The underlined words
were detected as errors. To judge whether they
are true errors or not is easy by comparing the
pair of examples that contradict each other.
To examine the recall (the ratio of correctly
detected errors for all of the existing actual er-
rors in corpora), we conduct another experi-
ments on an artificial data. We made the arti-
ficial data by randomly changing the POS tags
of randomly selected ambiguous tokens in the
WSJ corpus. The tags of 12,848 tokens (1% for
the whole corpus) are changed, and the results
1
10
100
1000
10000
100000
1000000
10000000
0 0.2 0.4 0.6 0.8 1
Num
ber
Positive ExamplesNegative Examples
?
Figure 1: Distribution of the Value ? on the
WSJ Corpus
Figure 2: A Tool for Corpus Error Detection
are shown in Table 2 for various values of ??2.
For the smaller threshold ??, the larger recall
were obtained, but the value is not high.
2Precisions cannot be calculated automatically be-
cause actual errors as well as the mixed errors are also
detected.
Table 2: Recall for the Artificial Data
?? # of Correctly Detected Errors Recall
1.0 607 4.7%
0.5 1520 11.8%
0.2 1555 12.1%
0.1 1749 13.6%
0.05 2381 18.5%
1
10
100
1000
10000
100000
1000000
0 0.2 0.4 0.6 0.8 1
Num
ber
?
Positive ExamplesNegative Examples
Figure 3: Distribution of the Value ? on the
RWCP Corpus
3.2 Experiments on the RWCP Corpus
(Japanese)
We use the RWCP corpus, which consists of
35,743 sentences (921,946 morphemes).
The distribution of the weights ?i are shown
in Figure 3. The distribution of ?i shows the
same tendency as in the case of the WSJ corpus.
We conducted corpus error detection for vari-
ous values of ??, and examined by hand whether
the detected errors are true errors or not. The
results are shown in Table 3, where the correctly
detected errors are distinguished into two types,
one type is errors of word segmentation and the
other is errors of POS tagging, since Japanese
has two kinds of ambiguities, word segmenta-
tion and POS tagging. Precision of more than
80% are obtained, and the number of POS tag
errors is larger than that of segmentation errors.
Examples of correctly detected errors and in-
correctly detected errors from the corpus are
shown in Table 4. The underlined morphemes
were detected as errors. In the examples of
correctly detected errors, both segmentation er-
rors (upper) and POS tag errors (lower) are de-
tected. On the other hand, the examples of in-
correctly detected errors show the limitations
of our method. We use the two morphemes on
either side of the current morpheme as features
for SVMs. In the examples, the two morphemes
on either side are the same and only the POS
tag of the current morpheme is different, so that
SVMs cannot distinguish them and regard them
as errors (inconsistency).
3.3 Experiments on the Kyoto
University Corpus (Japanese)
Experiments are performed on a portion of the
Kyoto University corpus version 2.0, consisting
of the articles of January 1, and from January 3
to January 9 (total of 9,204 sentences, 229,816
morphemes). We set the value of ?? to 0.5.
By repeating corpus error detection and cor-
rection of the detected errors by hand, new er-
rors that are not detected previously may be
detected. To examine this, we repeated corpus
error detection and correction by hand. Table 5
shows the result. All the detected errors in all
rounds were true errors, that is, the precision
was 100%. Applying the corpus error detection
repeatedly, the number of detected errors de-
crease rapidly, and no errors are detected in the
fourth round. In short, even if we repeat corpus
error detection with feedback, few new errors
were detected in this experiment.
4 Discussion
Compared to conventional probabilistic ap-
proaches for corpus error detection, although
precise comparison is difficult, our approach
achieved relatively high precision. Using a prob-
abilistic approach, Murata et al (2000) de-
tected errors of morphemes in a corpus with a
precision of 70?80%, and Eskin (2000) detected
errors with a precision of 69%, but our approach
achieved more than 80%. The probabilistic
methods cannot handle infrequent events or
compare events with similar probabilities, since
the probabilities cannot be calculated or com-
pared with enough confidence, but our method
can handle such infrequent events.
SVMs are similar to boosting, and our ap-
proach uses the weights attached by SVMs
in a similar manner to what Abney et al
(1999) studied. However, we introduced a post-
processing step to extract inconsistent similar
Table 3: Number of Detected Errors on the RWCP Corpus
?? Correct Detection (Segmentation Error/POS Tag Error) Incorrect Detection Precision
1.0 110 ( 30 / 80 ) 8 93.2%
0.5 165 ( 43 / 122 ) 11 93.8%
0.2 171 ( 45 / 126 ) 12 93.4%
0.1 188 ( 51 / 137 ) 31 85.8%
0.05 300 ( 73 / 227 ) 73 80.4%
Table 4: Examples of Correctly Detected Errors and Incorrectly Detected Errors in the RWCP
Corpus
Table 5: Number of Detected Errors on the Kyoto University Corpus for Repeated Experiment
Round 1 2 3 4
Correct Detection 85 11 2 0
(Segmentation Error) (21) (2) (0) (0)
(POS Tag Error) (64) (9) (2) (0)
Incorrect Detection 0 0 0 0
Total 85 11 2 0
examples, and this improved the precision of de-
tection and usability. Ma et al (2001) studied
corpus error detection by finding conflicting ele-
ments using min-max modular neural networks.
Compared to their method, our method is use-
ful in the point that the detected errors can be
sorted by the attached weights, because human
can check more likely elements first.
In the experiment, our method had a high
precision but a low recall. The value will be
controlled by tuning the features for SVMs as
well as the threshold value ??, and detecting
more errors in a corpus remains as future work.
5 Conclusion
In this paper, we proposed a method for corpus
error detection using SVMs. This method can
extract inconsistencies in corpora. We achieved
precision of 80?100% and showed that many
annotation errors exist in widely used corpora.
The performance seems to be high enough for
practical use in corpus refinement.
References
Steven Abney, Robert E. Schapire, and Yoram
Singer. 1999. Boosting Applied to Tag-
ging and PP Attachment. In Proceedings of
the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and
Very Large Corpora, pages 38?45.
Erin L. Allwein, Robert E. Schapire, and Yoram
Singer. 2000. Reducing Multiclass to Binary:
A Unifying Approach for Margin Classifiers.
In Proceedings of 17th International Confer-
ence on Machine Learning, pages 9?16.
Eric Brill. 1995. Transformation-Based Error-
Driven Learning and Natural Language Pro-
cessing: A Case Study in Part-of-Speech Tag-
ging. Computational Linguistics, 21(4):543?
565.
Eleazar Eskin. 2000. Detecting Errors within
a Corpus using Anomaly Detection. In Pro-
ceedings of the 6th Applied Natural Language
Processing Conference and the 1st Meeting of
the North American Chapter of the Associa-
tion of Computational Linguistics, pages 148?
153.
Qing Ma, Bao-Liang Lu, Masaki Murata, Michi-
nori Ichikawa, and Hitoshi Isahara. 2001.
On-Line Error Detection of Annotated Cor-
pus Using Modular Neural Networks. In Pro-
ceedings of International Conference on Arti-
ficial Neural Networks (ICANN 2001), pages
1185?1192.
Yuji Matsumoto and Tatsuo Yamashita. 2000.
Using Machine Learning Methods to Improve
Quality of Tagged Corpora and Learning
Models. In Proceedings of the Second Interna-
tional Conference on Language Resource and
Evaluation, pages 11?16.
Masaki Murata, Masao Utiyama, Kiyotaka
Uchimoto, Qing Ma, and Hitoshi Isahara.
2000. Corpus Error Detection and Correc-
tion Using the Decision-List and Example-
Based Methods. In Information Processing
Society of Japan SIG Notes, Natural Lan-
guage No.136, pages 49?56. (in Japanese).
Tetsuji Nakagawa, Taku Kudo, and Yuji Mat-
sumoto. 2002. Revision Learning and its
Application to Part-of-Speech Tagging. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics. (to appear).
Llu??s Padro? and Llu??s Ma`rquez. 1998. On the
Evaluation and Comparison of Taggers: the
Effect of Noise in Testing Corpora. In Pro-
ceedings of the joint 17th International Con-
ference on Computational Linguistics and
36th Annual Meeting of the Association for
Computational Linguistics, pages 997?1002.
Adwait Ratnaparkhi. 1996. A Maximum En-
tropy Model for Part-of-Speech Tagging. In
Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pages 133?142.
Hans van Halteren, Jakub Zavrel, and Wal-
ter Daelemans. 2001. Improving Accuracy
in Wordclass Tagging through Combination
of Machine Learning Systems. Computational
Linguistics, 27(2):199?230.
Vladimir Vapnik. 1998. Statistical Learning
Theory. Springer.
Revision Learning and its Application to Part-of-Speech Tagging
Tetsuji Nakagawa? and Taku Kudo and Yuji Matsumoto
tetsu-na@plum.freemail.ne.jp,{taku-ku,matsu}@is.aist-nara.ac.jp
Graduate School of Information Science
Nara Institute of Science and Technology
8916?5 Takayama, Ikoma, Nara 630?0101, Japan
Abstract
This paper presents a revision learn-
ing method that achieves high per-
formance with small computational
cost by combining a model with high
generalization capacity and a model
with small computational cost. This
method uses a high capacity model to
revise the output of a small cost model.
We apply this method to English part-
of-speech tagging and Japanese mor-
phological analysis, and show that the
method performs well.
1 Introduction
Recently, corpus-based approaches have been
widely studied in many natural language pro-
cessing tasks, such as part-of-speech (POS) tag-
ging, syntactic analysis, text categorization and
word sense disambiguation. In corpus-based
natural language processing, one important is-
sue is to decide which learning model to use.
Various learning models have been studied such
as Hidden Markov models (HMMs) (Rabiner
and Juang, 1993), decision trees (Breiman et
al., 1984) and maximum entropy models (Berger
et al, 1996). Recently, Support Vector Ma-
chines (SVMs) (Vapnik, 1998; Cortes and Vap-
nik, 1995) are getting to be used, which are
supervised machine learning algorithm for bi-
nary classification. SVMs have good generaliza-
tion performance and can handle a large num-
ber of features, and are applied to some tasks
? Presently with Oki Electric Industry
successfully (Joachims, 1998; Kudoh and Mat-
sumoto, 2000). However, their computational
cost is large and is a weakness of SVMs. In
general, a trade-off between capacity and com-
putational cost of learning models exists. For
example, SVMs have relatively high generaliza-
tion capacity, but have high computational cost.
On the other hand, HMMs have lower compu-
tational cost, but have lower capacity and dif-
ficulty in handling data with a large number of
features. Learning models with higher capac-
ity may not be of practical use because of their
prohibitive computational cost. This problem
becomes more serious when a large amount of
data is used.
To solve this problem, we propose a revision
learning method which combines a model with
high generalization capacity and a model with
small computational cost to achieve high per-
formance with small computational cost. This
method is based on the idea that processing the
entire target task using a model with higher ca-
pacity is wasteful and costly, that is, if a large
portion of the task can be processed easily using
a model with small computational cost, it should
be processed by such a model, and only difficult
portion should be processed by the model with
higher capacity.
Revision learning can handle a general multi-
class classification problem, which includes POS
tagging, text categorization and many other
tasks in natural language processing. We ap-
ply this method to English POS tagging and
Japanese morphological analysis.
This paper is organized as follows: Section
2 describes the general multi-class classification
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 497-504.
                         Proceedings of the 40th Annual Meeting of the Association for
problem and the one-versus-rest method which
is known as one of the solutions for the prob-
lem. Section 3 introduces revision learning, and
discusses how to combine learning models. Sec-
tion 4 describes one way to conduct Japanese
morphological analysis with revision learning.
Section 5 shows experimental results of English
POS tagging and Japanese morphological anal-
ysis with revision learning. Section 6 discusses
related works, and Section 7 gives conclusion.
2 Multi-Class Classification
Problems and the One-versus-Rest
Method
Let us consider the problem to decide the class
of an example x among multiple classes. Such a
problem is called multi-class classification prob-
lem. Many tasks in natural language processing
such as POS tagging are regarded as a multi-
class classification problem. When we only have
binary (positive or negative) classification algo-
rithm at hand, we have to reformulate a multi-
class classification problem into a binary classi-
fication problem. We assume a binary classifier
f(x) that returns positive or negative real value
for the class of x, where the absolute value |f(x)|
reflects the confidence of the classification.
The one-versus-rest method is known as one
of such methods (Allwein et al, 2000). For one
training example of a multi-class problem, this
method creates a positive training example for
the true class and negative training examples
for the other classes. As a result, positive and
negative examples for each class are generated.
Suppose we have five candidate classes A, B, C,
D and E , and the true class of x is B. Fig-
ure 1 (left) shows the created training examples.
Note that there are only two labels (positive and
negative) in contrast with the original problem.
Then a binary classifier for each class is trained
using the examples, and five classifiers are cre-
ated for this problem. Given a test example x?,
all the classifiers classify the example whether
it belongs to a specific class or not. Its class
is decided by the classifier that gives the largest
value of f(x?). The algorithm is shown in Figure
2 in a pseudo-code.
x A : B : C : D : E : 
Training Data
OX
X
X
X
A
E
B
C
D
A : B : 
Training Data
OX
123
Rank A
E
B
C
D45
xxxxx
x xx-X-O
-X
-X
-X
-X
-O
OX
Label : Positive: Negative
Class Class
Figure 1: One-versus-Rest Method (left) and
Revision Learning (right)
# Training Procedure of One-versus-Rest
# This procedure is given training examples
# {(xi, yi)}, and creates classifiers.
# C = {c0, . . . , ck?1}: the set of classes,
# xi: the ith training example,
# yi ? C: the class of xi,
# k: the number of classes,
# l: the number of training examples,
# fc(?): the binary classifier for the class c
# (see the text).
procedure TrainOVR({(x0, y0), . . . , (xl?1, yl?1)})
begin
# Create the training data with binary label
for i := 0 to l ? 1
begin
for j := 0 to k ? 1
begin
if cj 6= yi then
Add xi to the training data for the class cj as a
negative example.
else
Add xi to the training data for the class cj as a
positive example.
end
end
# Train the binary classifiers
for j := 0 to k ? 1
Train the classifier fcj (?) using the training data.
end
# Test Function of One-versus-Rest
# This function is given a test example and
# returns the predicted class of it.
# C = {c0, . . . , ck?1}: the set of classes,
# x: the test example,
# k: the number of classes,
# fc(?): binary classifier trained with the
# algorithm above.
function TestOVR(x)
begin
for j := 0 to k ? 1
confidencej := fcj (x)
return cargmaxj confidencej
end
Figure 2: Algorithm of One-versus-Rest
However, this method has the problem of be-
ing computationally costly in training, because
the negative examples are created for all the
classes other than the true class, and the to-
tal number of the training examples becomes
large (which is equal to the number of original
training examples multiplied by the number of
classes). The computational cost in testing is
also large, because all the classifiers have to work
on each test example.
3 Revision Learning
As discussed in the previous section, the one-
versus-rest method has the problem of compu-
tational cost. This problem become more se-
rious when costly binary classifiers are used or
when a large amount of data is used. To cope
with this problem, let us consider the task of
POS tagging. Most portions of POS tagging is
not so difficult and a simple POS-based HMMs
learning 1 achieves more than 95% accuracy sim-
ply using the POS context (Brants, 2000). This
means that the low capacity model is enough
to do most portions of the task, and we need
not use a high accuracy but costly algorithm in
every portion of the task. This is the base mo-
tivation of the revision model we are proposing
here.
Revision learning uses a binary classifier with
higher capacity to revise the errors made by
the stochastic model with lower capacity as fol-
lows: During the training phase, a ranking is
assigned to each class by the stochastic model
for a training example, that is, the candidate
classes are sorted in descending order of its con-
ditional probability given the example. Then,
the classes are checked in their ranking order to
create binary classifiers as follows. If the class
is incorrect (i.e. it is not equal to the true class
for the example), the example is added to the
training data for that class as a negative exam-
ple, and the next ranked class is checked. If
the class is correct, the example is added to the
training data for that class as a positive exam-
1HMMs can be applied to either of unsupervised or
supervised learning. In this paper, we use the latter case,
i.e., visible Markov Models, where POS-tagged data is
used for training.
ple, and the remaining ranked classes are not
taken into consideration (Figure 1, right). Us-
ing these training data, binary classifiers are cre-
ated. Note that each classifier is a pure binary
classifier regardless with the number of classes
in the original problem. The binary classifier is
trained just for answering whether the output
from the stochastic model is correct or not.
During the test phase, first the ranking of
the candidate classes for a given example is as-
signed by the stochastic model as in the training.
Then the binary classifier classifies the example
according to the ranking. If the classifier an-
swers the example as incorrect, the next high-
est ranked class becomes the next candidate for
checking. But if the example is classified as cor-
rect, the class of the classifier is returned as the
answer for the example. The algorithm is shown
in Figure 3.
The amount of training data generated in the
revision learning can be much smaller than that
in one-versus-rest. Since, in revision learning,
negative examples are created only when the
stochastic model fails to assign the highest prob-
ability to the correct POS tag, whereas negative
examples are created for all but one class in the
one-versus-rest method. Moreover, testing time
of the revision learning is shorter, because only
one classifier is called as far as it answers as cor-
rect, but all the classifiers are called in the one-
versus-rest method.
4 Morphological Analysis with
Revision Learning
We introduced revision learning for multi-class
classification in the previous section. How-
ever, Japanese morphological analysis cannot be
regarded as a simple multi-class classification
problem, because words in a sentence are not
separated by spaces in Japanese and the mor-
phological analyzer has to segment the sentence
into words as well as to decide the POS tag of
the words. So in this section, we describe how
to apply revision learning to Japanese morpho-
logical analysis.
For a given sentence, a lattice consisting of all
possible morphemes can be built using a mor-
# Training Procedure of Revision Learning
# This procedure is given training examples
# {(xi, yi)}, and creates classifiers.
# C = {c0, . . . , ck?1}: the set of classes,
# xi: the ith training example,
# yi ? C: the class of xi,
# k: the number of classes,
# l: the number of training examples,
# ni: the ordered indexes of C
# (see the following code),
# fc(?): the binary classifier for the class c
# (see the text).
procedure TrainRL({(x0, y0), . . . , (xl?1, yl?1)})
begin
# Create the training data with binary label
for i := 0 to l ? 1
begin
Call the stochastic model to obtain the
ordered indexes {n0, . . . , nk?1}
such that P (cn0 |xi) ? ? ? ? ? P (cnk?1 |xi).for j := 0 to k ? 1
begin
if cnj 6= yi then
Add xi to the training data for the class cnj as a
negative example.
else
begin
Add xi to the training data for the class cnj as a
positive example.
break
end
end
end
# Train the binary classifiers
for j := 0 to k ? 1
Train the classifier fcj (?) using the training data.
end
# Test Function of Revision Learning
# This function is given a test example and
# returns the predicted class of it.
# C = {c0, . . . , ck?1}: the set of classes,
# x: the test example,
# k: the number of classes,
# ni: the ordered indexes of C
# (see the following code),
# fc(?): binary classifier trained with the
# algorithm above.
function TestRL(x)
begin
Call the stochastic model to obtain the
ordered indexes {n0, . . . , nk?1}
such that P (cn0 |x) ? ? ? ? ? P (cnk?1 |x).for j := 0 to k ? 1
if fcnj (x) > 0 then
return cnj
return undecidable
end
Figure 3: Algorithm of Revision Learning
pheme dictionary as in Figure 4. Morphological
analysis is conducted by choosing the most likely
path on it. We adopt HMMs as the stochastic
model and SVMs as the binary classifier. For
any sub-paths from the beginning of the sen-
tence (BOS) in the lattice, its generative prob-
ability can be calculated using HMMs (Nagata,
1999). We first pick up the end node of the
sentence as the current state node, and repeat
the following revision learning process backward
until the beginning of the sentence. Rankings
are calculated by HMMs to all the nodes con-
nected to the current state node, and the best
of these nodes is identified based on the SVMs
classifiers. The selected node then becomes the
current state node in the next round. This can
be seen as SVMs deciding whether two adjoining
nodes in the lattice are connected or not.
In Japanese morphological analysis, for any
given morpheme ?, we use the following features
for the SVMs:
1. the POS tags, the lexical forms and the in-
flection forms of the two morphemes pre-
ceding ?;
2. the POS tags and the lexical forms of the
two morphemes following ?;
3. the lexical form and the inflection form of
?.
The preceding morphemes are unknown because
the processing is conducted from the end of the
sentence, but HMMs can predict the most likely
preceding morphemes, and we use them as the
features for the SVMs.
English POS tagging is regarded as a special
case of morphological analysis where the seg-
mentation is done in advance, and can be con-
ducted in the same way. In English POS tag-
ging, given a word w, we use the following fea-
tures for the SVMs:
1. the POS tags and the lexical forms of the
two words preceding w, which are given by
HMMs;
2. the POS tags and the lexical forms of the
two words following w;
3. the lexical form of w and the prefixes and
suffixes of up to four characters, the exis-
BOS EOS
kinou (yesterday)
[noun]
ki (tree)
[noun]
nou (brain)
[noun]
ki (come)
[verb]
no 
[particle]
u
[auxiliary]
gakkou (school)
[noun]
sentence: 
ni (to)
[particle]
ni (resemble)
[verb]
it (went)
[verb]
ta
[auxiliary]
kinou
gakkou
it
ki
ki
noun
verb
noun
verb
noun... ...
Dictionary:
Lattice:
"kinougakkouniitta (I went to school yesterday)"
Figure 4: Example of Lattice for Japanese Morphological Analysis
tence of numerals, capital letters and hy-
phens in w.
5 Experiments
This section gives experimental results of En-
glish POS tagging and Japanese morphological
analysis with revision learning.
5.1 Experiments of English
Part-of-Speech Tagging
Experiments of English POS tagging with revi-
sion learning (RL) are performed on the Penn
Treebank WSJ corpus. The corpus is randomly
separated into training data of 41,342 sentences
and test data of 11,771 sentences. The dictio-
nary for HMMs is constructed from all the words
in the training data.
T3 of ICOPOST release 0.9.0 (Schro?der,
2001) is used as the stochastic model for ranking
stage. This is equivalent to POS-based second
order HMMs. SVMs with second order polyno-
mial kernel are used as the binary classifier.
The results are compared with TnT (Brants,
2000) based on second order HMMs, and with
POS tagger using SVMs with one-versus-rest (1-
v-r) (Nakagawa et al, 2001).
The accuracies of those systems for known
words, unknown words and all the words are
shown in Table 1. The accuracies for both
known words and unknown words are improved
through revision learning. However, revision
learning could not surpass the one-versus-rest.
The main difference in the accuracies stems from
those for unknown words. The reason for that
seems to be that the dictionary of HMMs for
POS tagging is obtained from the training data,
as a result, virtually no unknown words exist in
the training data, and the HMMs never make
mistakes for unknown words during the train-
ing. So no example of unknown words is avail-
able in the training data for the SVM reviser.
This is problematic: Though the HMMs handles
unknown words with an exceptional method,
SVMs cannot learn about errors made by the
unknown word processing in the HMMs. To
cope with this problem, we force the HMMs
to make mistakes by eliminating low frequent
words from the dictionary. We eliminated the
words appearing only once in the training data
so as to make SVMs to learn about unknown
words. The results are shown in Table 1 (row
?cutoff-1?). Such procedure improves the accu-
racies for unknown words.
One advantage of revision learning is its small
computational cost. We compare the computa-
tion time with the HMMs and the one-versus-
rest. We also use SVMs with linear kernel func-
tion that has lower capacity but lower computa-
tional cost compared to the second order poly-
nomial kernel SVMs. The experiments are per-
formed on an Alpha 21164A 500MHz processor.
Table 2 shows the total number of training ex-
amples, training time, testing time and accu-
racy for each of the five systems. The training
time and the testing time of revision learning
are considerably smaller than those of the one-
versus-rest. Using linear kernel, the accuracy
decreases a little, but the computational cost is
much lower than the second order polynomial
kernel.
Accuracy (Known Words / Unknown Words) Number of Errors
T3 Original 96.59% (96.90% / 82.74%) 9720
with RL 96.93% (97.23% / 83.55%) 8734
with RL (cutoff-1) 96.98% (97.25% / 85.11%) 8588
TnT 96.62% (96.90% / 84.19%) 9626
SVMs 1-v-r 97.11% (97.34% / 86.80%) 8245
Table 1: Result of English POS Tagging
Total Number of Training Time Testing Time Accuracy
Examples for SVMs (hour) (second)
T3 Original ? 0.004 89 96.59%
with RL (polynomial kernel, cutoff-1) 1027840 16 2089 96.98%
with RL (linear kernel, cutoff-1) 1027840 2 129 96.94%
TnT ? 0.002 4 96.62%
SVMs 1-v-r 999984?50 625 55239 97.11%
Table 2: Computational Cost of English POS Tagging
5.2 Experiments of Japanese
Morphological Analysis
We use the RWCP corpus and some additional
spoken language data for the experiments of
Japanese morphological analysis. The corpus is
randomly separated into training data of 33,831
sentences and test data of 3,758 sentences. As
the dictionary for HMMs, we use IPADIC ver-
sion 2.4.4 with 366,878 morphemes (Matsumoto
and Asahara, 2001) which is originally con-
structed for the Japanese morphological ana-
lyzer ChaSen (Matsumoto et al, 2001).
A POS bigram model and ChaSen version
2.2.8 based on variable length HMMs are used as
the stochastic models for the ranking stage, and
SVMs with the second order polynomial kernel
are used as the binary classifier.
We use the following values to evaluate
Japanese morphological analysis:
recall = ?# of correct morphemes in system?s output??# of morphemes in test data? ,
precision = ?# of correct morphemes in system?s output??# of morphemes in system?s output? ,
F-measure = 2? recall? precisionrecall + precision .
The results of the original systems and those
with revision learning are shown in Table 3,
which provides the recalls, precisions and F-
measures for two cases, namely segmentation
(i.e. segmentation of the sentences into mor-
phemes) and tagging (i.e. segmentation and
POS tagging). The one-versus-rest method is
not used because it is not applicable to mor-
phological analysis of non-segmented languages
directly.
When revision learning is used, all the mea-
sures are improved for both POS bigram and
ChaSen. Improvement is particularly clear for
the tagging task.
The numbers of correct morphemes for each
POS category tag in the output of ChaSen with
and without revision learning are shown in Ta-
ble 4. Many particles are correctly revised by
revision learning. The reason is that the POS
tags for particles are often affected by the fol-
lowing words in Japanese, and SVMs can revise
such particles because it uses the lexical forms of
the following words as the features. This is the
advantage of our method compared to simple
HMMs, because HMMs have difficulty in han-
dling a lot of features such as the lexical forms
of words.
6 Related Works
Our proposal is to revise the outputs of a
stochastic model using binary classifiers. Brill
studied transformation-based error-driven learn-
ing (TBL) (Brill, 1995), which conducts POS
tagging by applying the transformation rules to
the POS tags of a given sentence, and has a
resemblance to revision learning in that the sec-
ond model revises the output of the first model.
Word Segmentation Tagging Training Testing
Time Time
Recall Precision F-measure Recall Precision F-measure (hour) (second)
POS Original 98.06% 98.77% 98.42% 95.61% 96.30% 95.96% 0.02 8
bigram with RL 99.06% 99.27% 99.16% 98.13% 98.33% 98.23% 11 184
ChaSen Original 99.06% 99.20% 99.13% 97.67% 97.81% 97.74% 0.05 15
with RL 99.22% 99.34% 99.28% 98.26% 98.37% 98.32% 6 573
Table 3: Result of Morphological Analysis
Part-of-Speech # in Test Data Original with RL Difference
Noun 41512 40355 40556 +201
Prefix 817 781 784 +3
Verb 8205 8076 8115 +39
Adjective 678 632 655 +23
Adverb 779 735 750 +15
Adnominal 378 373 373 0
Conjunction 258 243 243 0
Particle 20298 19686 19942 +256
Auxiliary 4419 4333 4336 +3
Interjection 94 90 91 +1
Symbol 15665 15647 15651 +4
Others 1 1 1 0
Filler 43 36 36 0
Table 4: The Number of Correctly Tagged Morphemes for Each POS Category Tag
However, our method differs from TBL in two
ways. First, our revision learner simply answers
whether a given pattern is correct or not, and
any types of binary classifiers are applicable.
Second, in our model, the second learner is ap-
plied to the output of the first learner only once.
In contrast, rewriting rules are applied repeat-
edly in the TBL.
Recently, combinations of multiple learners
have been studied to achieve high performance
(Alpaydm, 1998). Such methodologies to com-
bine multiple learners can be distinguished into
two approaches: one is the multi-expert method
and the other is the multi-stage method. In the
former, each learner is trained and answers inde-
pendently, and the final decision is made based
on those answers. In the latter, the multiple
learners are ordered in series, and each learner is
trained and answers only if the previous learner
rejects the examples. Revision learning belongs
to the latter approach. In POS tagging, some
studies using the multi-expert method were con-
ducted (van Halteren et al, 2001; Ma`rquez et
al., 1999), and Brill and Wu (1998) combined
maximum entropy models, TBL, unigram and
trigram, and achieved higher accuracy than any
of the four learners (97.2% for WSJ corpus).
Regarding the multi-stage methods, cascading
(Alpaydin and Kaynak, 1998) is well known,
and Even-Zohar and Roth (2001) proposed the
sequential learning model and applied it to POS
tagging. Their methods differ from revision
learning in that each learner behaves in the same
way and more than one learner is used in their
methods, but in revision learning the stochastic
model assigns rankings to candidates and the bi-
nary classifier selects the output. Furthermore,
mistakes made by a former learner are fatal in
their methods, but is not so in revision learn-
ing because the binary classifier works to revise
them.
The advantage of the multi-expert method is
that each learner can help each other even if
it has some weakness, and generalization er-
rors can be decreased. On the other hand,
the computational cost becomes large because
each learner is trained using every training data
and answers for every test data. In contrast,
multi-stage methods can decrease the computa-
tional cost, and seem to be effective when a large
amount of data is used or when a learner with
high computational cost such as SVMs is used.
7 Conclusion
In this paper, we proposed the revision learning
method which combines a stochastic model and
a binary classifier to achieve higher performance
with lower computational cost. We applied it to
English POS tagging and Japanese morpholog-
ical analysis, and showed improvement of accu-
racy with small computational cost.
Compared to the conventional one-versus-rest
method, revision learning has much lower com-
putational cost with almost comparable accu-
racy. Furthermore, it can be applied not only to
a simple multi-class classification task but also
to a wider variety of problems such as Japanese
morphological analysis.
Acknowledgments
We would like to thank Ingo Schro?der for making
ICOPOST publicly available.
References
Erin L. Allwein, Robert E. Schapire, and Yoram
Singer. 2000. Reducing Multiclass to Binary: A
Unifying Approach for Margin Classifiers. In Pro-
ceedings of 17th International Conference on Ma-
chine Learning, pages 9?16.
Ethem Alpaydin and Cenk Kaynak. 1998. Cascad-
ing Classifiers. Kybernetika, 34(4):369?374.
Ethem Alpaydm. 1998. Techniques for Combining
Multiple Learners. In Proceedings of Engineering
of Intelligent Systems ?98 Conference.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A Maximum Entropy
Approach to Natural Language Processing. Com-
putational Linguistics, 22(1):39?71.
Thorsten Brants. 2000. TnT ? A Statistical
Part-of-Speech Tagger. In Proceedings of ANLP-
NAACL 2000, pages 224?231.
Leo Breiman, Jerome H. Friedman, Richard A. Ol-
shen, and Charles J. Stone. 1984. Classification
and Regression Trees. Wadsworth and Brooks.
Eric Brill and Jun Wu. 1998. Classifier Combi-
nation for Improved Lexical Disambiguation. In
Proceedings of the Thirty-Sixth Annual Meeting of
the Association for Computational Linguistics and
Seventeenth International Conference on Compu-
tational Linguistics, pages 191?195.
Eric Brill. 1995. Transformation-Based Error-
Driven Learning and Natural Language Process-
ing: A Case Study in Part-of-Speech Tagging.
Computational Linguistics, 21(4):543?565.
Corinna Cortes and Vladimir Vapnik. 1995. Support
Vector Networks. Machine Learning, 20:273?297.
Yair Even-Zohar and Dan Roth. 2001. A Sequential
Model for Multi-Class Classification. In Proceed-
ings of the 2001 Conference on Empirical Methods
in Natural Language Processing, pages 10?19.
Thorsten Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with Many
Relevant Features. In Proceedings of the 10th Eu-
ropean Conference on Machine Learning, pages
137?142.
Taku Kudoh and Yuji Matsumoto. 2000. Use of Sup-
port Vector Learning for Chunk Identification. In
Proceedings of the Fourth Conference on Compu-
tational Natural Language Learning, pages 142?
144.
Llui??s Ma`rquez, Horacio Rodr??guez, Josep Carmona,
and Josep Montolio. 1999. Improving POS Tag-
ging Using Machine-Learning Techniques. In Pro-
ceedings of 1999 Joint SIGDAT Conference on
Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 53?62.
Yuji Matsumoto and Masayuki Asahara. 2001.
IPADIC User?s Manual version 2.2.4. Nara In-
stitute of Science and Technology. (in Japanese).
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, Kazuma
Takaoka, and Masayuki Asahara. 2001. Mor-
phological Analysis System ChaSen version 2.2.8
Manual. Nara Institute of Science and Technol-
ogy.
Masaaki Nagata. 1999. Japanese Language Process-
ing Based on Stochastic Models. Kyoto University,
Doctoral Thesis. (in Japanese).
Tetsuji Nakagawa, Taku Kudoh, and Yuji Mat-
sumoto. 2001. Unknown Word Guessing and
Part-of-Speech Tagging Using Support Vector Ma-
chines. In Proceedings of 6th Natural Language
Processing Pacific Rim Symposium, pages 325?
331.
Lawrence R. Rabiner and Biing-Hwang Juang.
1993. Fundamentals of Speech Recognition. PTR
Prentice-Hall.
Ingo Schro?der. 2001. ICOPOST ? Ingo?s Collection
Of POS Taggers.
http://nats-www.informatik.uni-hamburg.de
/~ingo/icopost/.
Hans van Halteren, Jakub Zavrel, and Walter Daele-
mans. 2001. Improving Accuracy in Word-
class Tagging through Combination of Machine
Learning Systems. Computational Linguistics,
27(2):199?230.
Vladimir Vapnik. 1998. Statistical Learning Theory.
Springer.
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 1?4,
Suntec, Singapore, 3 August 2009. c?2009 ACL and AFNLP
WISDOM: A Web Information Credibility Analysis System 
Susumu Akamine?  Daisuke Kawahara?  Yoshikiyo Kato? 
Tetsuji Nakagawa?  Kentaro Inui?  Sadao Kurohashi??  Yutaka Kidawara? 
?National Institute of Information and Communications Technology 
? Graduate School of Informatics, Kyoto University 
{akamine, dk, ykato, tnaka, inui, kidawara}@nict.go.jp, kuro@i.kyoto-u.ac.jp 
 
 
 
 
Abstract 
We demonstrate an information credibility 
analysis system called WISDOM. The purpose 
of WISDOM is to evaluate the credibility of in-
formation available on the Web from multiple 
viewpoints. WISDOM considers the following 
to be the source of information credibility: in-
formation contents, information senders, and 
information appearances. We aim at analyzing 
and organizing these measures on the basis of 
semantics-oriented natural language processing 
(NLP) techniques. 
1. Introduction 
As computers and computer networks become 
increasingly sophisticated, a vast amount of in-
formation and knowledge has been accumulated 
and circulated on the Web. They provide people 
with options regarding their daily lives and are 
starting to have a strong influence on govern-
mental policies and business management. How-
ever, a crucial problem is that the information 
available on the Web is not necessarily credible. 
It is actually very difficult for human beings to 
judge the credibility of the information and even 
more difficult for computers. However, comput-
ers can be used to develop a system that collects, 
organizes, and relativises information and helps 
human beings view information from several 
viewpoints and judge the credibility of the in-
formation. 
Information organization is a promising en-
deavor in the area of next-generation Web search. 
The search engine Clusty provides a search result 
clustering1, and Cuil classifies a search result on 
the basis of query-related terms2. The persuasive 
technology research project at Stanford Universi-
ty discussed how websites can be designed to 
influence people?s perceptions (B. J. Fogg, 2003). 
However, as per our knowledge, no research has 
been carried out for supporting the human judg-
ment on information credibility and information 
organization systems for this purpose. 
In order to support the judgment of informa-
tion credibility, it is necessary to extract the 
background, facts, and various opinions and their 
                                                 
1 http://clusty.com/, http://clusty.jp/  
distribution for a given topic. For this purpose, 
syntactic and discourse structures must be ana-
lyzed, their types and relations must be extracted, 
and synonymous and ambiguous expressions 
should be handled properly.  
Furthermore, it is important to determine the 
identity of the information sender and his/her 
specialty as criteria for credibility, which require 
named entity recognition and total analysis of 
documents. 
In this paper, we describe an information cre-
dibility analysis system called WISDOM, which 
automatically analyzes and organizes the above 
aspects on the basis of semantically oriented 
NLP techniques. WISDOM currently operates 
over 100 million Japanese Web pages. 
2. Overview of WISDOM 
We consider the following three criteria for the 
judgment of information credibility.  
(1) Credibility of information contents,  
(2) Credibility of the information sender, and  
(3) Credibility estimated from the document 
style and superficial characteristics. 
In order to help people judge the credibility of 
information from these viewpoints, we have been 
developing an information analysis system called 
WISDOM. Figure 1 shows the analysis result of 
WISDOM on the analysis topic ?Is bio-ethanol 
good for the environment?? Figure 2 shows the 
system architecture of WISDOM. 
Given an analysis topic (query), WISDOM 
sends the query to the search engine TSUBAKI 
(Shinzato et al, 2008), and TSUBAKI returns a 
list of the top N relevant Web pages (N is usually 
set to 1000). 
Then, those pages are automatically analyzed, 
and major and contradictory expressions and eva-
luative expressions are extracted. Furthermore, 
the information senders of the Web pages, which 
were analyzed beforehand, are collected and the 
distribution is calculated. 
The WISDOM analysis results can be viewed 
from several viewpoints by changing the tabs 
using a Web browser. The leftmost tab, ?Sum-
mary,? shows the summary of the analysis, with 
major phrases and major/contradictory state-
ments first.  
1
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Query: ?Is bio-ethanol good for the environment?? Summary 
Figure 1. An analysis example of the information credibility analysis system WISDOM. 
 
 
Figure 2. System architecture of WISDOM. 
 
By referring to these phrases and statements, 
a user can grasp the important issues related to 
the topic at a glance. The pie diagram indicates 
the distribution of the information sender class 
spread over 1000 pages, such as company, indus-
try group, and government. The names of the 
information senders of the class can be viewed 
by placing the cursor over a class region. The last 
bar chart shows the distribution of positive and 
negative opinions related to the topic spread over 
1000 pages, for all and for each sender class. For 
example, with regard to ?Bio-ethanol,? we can 
see that the number of positive opinions is more 
than that of negative opinions, but it is the oppo-
site in the case of some sender classes. Several 
display units in the Summary tab are cursor sen-
sitive, providing links to more detailed informa-
tion (e.g., the page list including a major state-
Sender 
Opinion 
Search Result Major/Contradictory Expressions
2
ment, the page list of a sender class, and the page 
list containing negative opinions). 
The ?Search Result? tab shows the search re-
sult by TSUBAKI, i.e., ranking the relevant pag-
es according to the TSUBAKI criteria. The ?Ma-
jor/Contradictory Expressions? tab shows the list 
of major phrases and major/contradictory state-
ments about the given topic and the list of pages 
containing the specified phrase or statement. The 
?Opinion? tab shows the analysis result of the 
evaluative expressions, classified according to 
for/against, like/dislike, merit/demerit, and others, 
and it also shows the list of pages containing the 
specified type of evaluative expressions. The 
?Sender? tab classifies the pages according to the 
class of the information sender, for example, a 
user can view the pages created only by the gov-
ernment.  
Furthermore, the superficial characteristics of 
pages called as information appearance are ana-
lyzed beforehand and can be viewed in WIS-
DOM, such as whether or not the contact address 
is shown in the page and the privacy policy is on 
the page, the volume of advertisements on the 
page, the number of images, and the number of 
in/out links. 
As shown thus far, given an analysis topic, 
WISDOM collects and organizes the relevant 
information available on the Web and provides 
users with multi-faceted views. We believe that 
such a system can considerably support the hu-
man judgment of information credibility. 
3. Data Infrastructure  
We usually utilize 100 million Japanese Web 
pages as the analysis target. The Web pages have 
been converted into the standard formatted Web 
data, an XML format. The format includes sever-
al metadata such as URLs, crawl dates, titles, and 
in/out links. A text in a page is automatically 
segmented into sentences (note that the sentence 
boundary is not clear in the original HTML file), 
and the analysis results obtained by a morpholog-
ical analyzer, parser, and synonym analyzer are 
also stored in the standard format. Furthermore, 
the site operator, the page author, and informa-
tion appearance (e.g., contact address, privacy 
policy, volume of advertisements, and images) 
are automatically analyzed and stored in the 
standard format. 
4. Extraction of Major Expressions and 
Their Contradictions 
For the organization of information contents, 
WISDOM extracts and presents the major ex-
pressions and their contradictions on a given 
analysis topic (Kawahara et al, 2008). Major 
expressions are defined as expressions occurring 
at a high frequency in the set of Web pages on 
the analysis topic. They are classified into two: 
noun phrases and predicate-argument structures 
(statements). Contradictions are the predicate-
argument structures that contradict the major ex-
pressions. For the Japanese phrase yutori kyouiku 
(cram-free education), for example, tsumekomi 
kyouiku (cramming education) and ikiru chikara 
(life skills) are extracted as the major noun 
phrases; yutori kyouiku-wo minaosu (reexamine 
cram-free education) and gakuryokuga teika-suru 
(scholastic ability deteriorates), as the major pre-
dicate-argument structures; and gakuryoku-ga 
koujousuru (scholastic ability ameliorates), as its 
contradiction. This kind of summarized informa-
tion enables a user to grasp the facts and argu-
ments on the analysis topic available on the Web. 
We use 1000 Web pages for a topic retrieved 
from the search engine TSUBAKI. Our method 
of extracting major expressions and their contra-
dictions consists of the following steps: 
1. Extracting candidates of major expressions: 
The candidates of major expressions are ex-
tracted from each Web page in the search result. 
From the relevant sentences to the analysis topic 
that consist of approximately 15 sentences se-
lected from each Web page, compound nouns, 
parenthetical expressions, and predicate-
argument structures are extracted as the candi-
dates of the major expressions. 
2. Distilling major expressions: 
Simply presenting expressions at a high fre-
quency is not always information of high quality. 
This is because scattering synonymous expres-
sions such as karikyuramu (curriculum) and 
kyouiku katei (course of study) and entailing ex-
pressions such as IWC and IWC soukai (IWC 
plenary session), all of which occur frequently, 
hamper the understanding process of users. Fur-
ther, synonymous predicate-argument structures 
such as gakuryoku-ga teika-suru (scholastic 
ability deteriorates) and gakuryoku-ga sagaru 
(scholastic ability lowers) have the same problem. 
To overcome this problem, we distill major ex-
pressions by merging spelling variations with 
morphological analysis, merging synonymous 
expressions automatically acquired from an ordi-
nary dictionary and the Web, and merging ex-
pressions that can be entailed by another expres-
sion. 
3. Extracting contradictory expressions: 
Predicate-argument structures that negate the 
predicate of major ones and that replace the pre-
dicate of major ones with its antonym are ex-
tracted as contradictions. For example, gakuryo-
ku-ga teika-shi-nai (scholastic ability does not 
deteriorate) and gakuryokuga koujou-suru (scho-
lastic ability ameliorates) are extracted as the 
contradictions to gakuryoku-ga teikasuru (scho-
lastic ability deteriorates). This process is per-
formed using an antonym lexicon, which consists 
of approximately 2000 pairs; these pairs are ex-
tracted from an ordinary dictionary. 
5. Extraction of Evaluative Information 
The extraction and classification of evaluative 
information from texts are important tasks with 
3
many applications and they have been actively 
studied recently (Pang and Lee, 2008). Most pre-
vious studies on opinion extraction or sentiment 
analysis deal with only subjective and explicit 
expressions. For example, Japanese sentences 
such as watashi-wa apple-ga sukida (I like ap-
ples) and kono seido-ni hantaida (I oppose the 
system) contain evaluative expressions that are 
directly expressed with subjective expressions. 
However, sentences such as kono shokuhin-wa 
kou-gan-kouka-ga aru (this food has an anti-
cancer effect) and kono camera-wa katte 3-ka-de 
kowareta (this camera was broken 3 days after I 
bought it) do not contain subjective expressions 
but contain negative evaluative expressions. 
From the viewpoint of information credibility, it 
appears important to deal with a wide variety of 
evaluative information including such implicit 
evaluative expressions (Nakagawa et al, 2008). 
A corpus annotated with evaluative informa-
tion was developed for evaluative information 
analysis studies. Fifty topics such as ?Bio-
ethanol? and ?Pension plan? were chosen. For 
each topic, 200 sentences containing the topic 
word were collected from the Web to construct 
the corpus totaling 10,000 sentences. For each 
sentence, annotators judged whether or not the 
sentence contained evaluative expressions. When 
evaluative expressions were identified, the evalu-
ative expressions, their holders, their sentiment 
polarities (positive or negative), and their relev-
ance to the topic were annotated. 
We developed an automatic analyzer of evalu-
ative information using the corpus. We per-
formed experiments of sentiment polarity classi-
fication using Support Vector Machines. Word 
forms, POS tags, and sentiment polarities from 
an evaluative word dictionary of all the words in 
evaluative expressions were used as features, and 
an accuracy of 83% was obtained. From the error 
analysis, we found that it was difficult to classify 
domain-specific evaluative expressions; we are 
now planning the automatic acquisition of evalu-
ative word dictionaries. 
6. Information Sender Analysis 
The source of information (or information sender) 
is one of the important elements when judging the 
credibility of information. It is rather easy for human 
beings to identify the information sender of a Web 
page. When reading a Web page, whether it is deli-
berate or not, we attribute some characteristics to the 
information sender and accordingly form our atti-
tudes toward the information. However, the state-of-
the-art search engines do not provide facilities to 
organize a vast amount of information on the basis 
of the information sender. If we can organize the 
information on a topic on the basis of who or what 
type the information sender is, it would enable the 
user to grasp an overview of the topic or to judge the 
credibility of relevant information. 
WISDOM automatically identifies the site op-
erators of Web pages and classifies them into 
predefined categories of information sender 
called information sender class. A site operator 
of a Web page is the governing body of a website 
on which the page is published. The information 
sender class categorizes the information sender 
on the basis of axes such as individuals vs. or-
ganizations and profit vs. nonprofit organizations. 
The list below shows the categories of informa-
tion sender class. 
 
 
 
1. Organization (cont?d) 
  (c) Press 
    i. Broadcasting Station 
    ii. Newspaper 
    iii. Publisher 
2. Individual 
  (a) Real Name 
  (b) Anonymous,  
Screen Name 
 
1. Organization 
  (a) Profit Organization 
    i. Company 
    ii. Industry Group 
  (b) Nonprofit Organization 
    i. Academic Society 
    ii. Government 
    iii. Political Organization 
    iv. Public Service Corp., 
         Nonprofit Organization 
    v. University 
    vi. Voluntary Association 
   vii. Education Institution
WISDOM allows the user to organize the in-
formation on the basis of the information sender 
class assigned to each Web page. Technical de-
tails of the information sender analysis employed 
in WISDOM can be found in (Kato et al, 2008). 
7. Conclusions 
This paper has described an information analy-
sis system called WISDOM. As shown in this pa-
per, WISDOM already provides a reasonably nice 
organized view for a given topic and can serve as a 
useful tool for handling informational queries and 
for supporting human judgment of information 
credibility. WISDOM is freely available at 
http://wisdom-nict.jp/.  
References 
B. J. Fogg. 2003. Persuasive Technology: Using Com-
puters to Change What We Think and Do (The Mor-
gan Kaufmann Series in Interactive Technologies). 
Morgan Kaufmann. 
K. Shinzato, T. Shibata, D. Kawahara, C. Hashimoto, 
and S. Kurohashi 2008. TSUBAKI: An open search 
engine infrastructure for developing new information 
access methodology. In Proceedings of IJCNLP2008. 
D. Kawahara, S. Kurohashi, and K. Inui 2008. Grasping 
major statements and their contradictions toward in-
formation credibility analysis of web contents. In 
Proceedings of  WI?08. 
B. Pang and L. Lee 2008. Opinion mining and senti-
ment analysis, Foundations and Trends in Informa-
tion Retrieval, Volume 2, Issue 1-2, 2008. 
T. Nakagawa, T. Kawada, K. Inui, and S. Kurohashi 
2008. Extracting subjective and objective evaluative 
expressions from the web. In Proceedings of 
ISUC2008. 
Y. Kato, D. Kawahara, K. Inui, S. Kurohashi, and T. 
Shibata 2008. Extracting the author of web pages. In 
Proceedings of WICOW2008. 
4
Chinese and Japanese Word Segmentation Using Word-Level and
Character-Level Information
Tetsuji Nakagawa
Corporate Research and Development Center
Oki Electric Industry Co., Ltd.
2?5?7 Honmachi, Chuo-ku, Osaka 541-0053, Japan
nakagawa378@oki.com
Abstract
In this paper, we present a hybrid method
for Chinese and Japanese word segmentation.
Word-level information is useful for analysis
of known words, while character-level informa-
tion is useful for analysis of unknown words,
and the method utilizes both these two types
of information in order to effectively handle
known and unknown words. Experimental re-
sults show that this method achieves high over-
all accuracy in Chinese and Japanese word seg-
mentation.
1 Introduction
Word segmentation in Chinese and Japanese is
an important and difficult task. In these lan-
guages, words are not separated by explicit delim-
iters, and word segmentation must be conducted
first in most natural language processing applica-
tions. One of the problems which makes word seg-
mentation more difficult is existence of unknown
(out-of-vocabulary) words. Unknown words are de-
fined as words that do not exist in a system?s dictio-
nary. The word segmentation system has no knowl-
edge about these unknown words, and determining
word boundaries for such words is difficult. Accu-
racy of word segmentation for unknown words is
usually much lower than that for known words.
In this paper, we propose a hybrid method for
Chinese and Japanese word segmentation, which
utilizes both word-level and character-level infor-
mation. Word-level information is useful for anal-
ysis of known words, and character-level informa-
tion is useful for analysis of unknown words. We
use these two types of information at the same time
to obtain high overall performance.
This paper is organized as follows: Section 2
describes previous work on Chinese and Japanese
word segmentation on which our method is based.
Section 3 introduces the hybrid method which com-
bines word-level and character-level processing.
Section 4 shows experimental results of Chinese and
Japanese word segmentation. Section 5 discusses
related work, and Section 6 gives the conclusion.
2 Previous Work on Word Segmentation
Our method is based on two existing methods for
Chinese or Japanese word segmentation, and we ex-
plain them in this section.
2.1 The Markov Model-Based Method
Word-based Markov models are used in English
part-of-speech (POS) tagging (Charniak et al,
1993; Brants, 2000). This method identifies POS-
tags T = t1, . . . , tn, given a sentence as a word se-
quence W = w1, . . . , wn, where n is the number
of words in the sentence. The method assumes that
each word has a state which is the same as the POS
of the word and the sequence of states is a Markov
chain. A state t transits to another state s with prob-
ability P (s|t), and outputs a word w with probabil-
ity P (w|t). From such assumptions, the probability
that the word sequence W with parts-of-speech T is
generated is
P (W,T ) =
n?
i=1
P (witi|w0t0 . . . wi?1ti?1),
'
n?
i=1
P (wi|ti)P (ti|ti?1), (1)
where w0(t0) is a special word(part-of-speech) rep-
resenting the beginning of the sentence. Given a
word sequence W , its most likely POS sequence T?
can be found as follows:
T? = argmax
T
P (T |W ),
= argmax
T
P (W,T )
P (W ) ,
= argmax
T
P (W,T ),
' argmax
T
n?
i=1
P (wi|ti)P (ti|ti?1). (2)
The equation above can be solved efficiently by the
Viterbi algorithm (Rabiner and Juang, 1993).
In Chinese and Japanese, the method is used
with some modifications. Because each word in a
Figure 1: Example of Lattice Used in the Markov Model-Based Method
sentence is not separated explicitly in Chinese and
Japanese, both segmentation of words and identifi-
cation of the parts-of-speech tags of the words must
be done simultaneously. Given a sentence S, its
most likely word sequence W? and POS sequence
T? can be found as follows where W ranges over the
possible segments of S (w1 ? ? ?wn = S):
(W? , T? ) = argmax
W,T
P (W,T |S),
= argmax
W,T
P (W,T, S)
P (S) ,
= argmax
W,T
P (W,T, S),
= argmax
W,T
P (W,T ),
' argmax
W,T
n?
i=1
P (wi|ti)P (ti|ti?1). (3)
The equation above can be solved using the Viterbi
algorithm as well.
The possible segments of a given sentence are
represented by a lattice, and Figure 1 shows an ex-
ample. Given a sentence, this method first con-
structs such a lattice using a word dictionary, then
chooses the best path which maximizes Equation
(3).
This Markov model-based method achieves high
accuracy with low computational cost, and many
Japanese word segmentation systems adopt it
(Kurohashi and Nagao, 1998; Matsumoto et al,
2001). However, the Markov model-based method
has a difficulty in handling unknown words. In the
constructing process of a lattice, only known words
are dealt with and unknown words must be handled
with other methods. Many practical word segmen-
tation systems add candidates of unknown words to
Tag Description
B The character is in the beginning of a word.
I The character is in the middle of a word.
E The character is in the end of a word.
S The character is itself a word.
Table 1: The ?B, I, E, S? Tag Set
the lattice. The candidates of unknown words can be
generated by heuristic rules(Matsumoto et al, 2001)
or statistical word models which predict the proba-
bilities for any strings to be unknown words (Sproat
et al, 1996; Nagata, 1999). However, such heuris-
tic rules or word models must be carefully designed
for a specific language, and it is difficult to properly
process a wide variety of unknown words.
2.2 The Character Tagging Method
This method carries out word segmentation by tag-
ging each character in a given sentence, and in
this method, the tags indicate word-internal posi-
tions of the characters. We call such tags position-
of-character (POC) tags (Xue, 2003) in this paper.
Several POC-tag sets have been studied (Sang and
Veenstra, 1999; Sekine et al, 1998), and we use the
?B, I, E, S? tag set shown in Table 1 1.
Figure 2 shows an example of POC-tagging. The
POC-tags can represent word boundaries for any
sentences, and the word segmentation task can be
reformulated as the POC-tagging task. The tagging
task can be solved by using general machine learn-
ing techniques such as maximum entropy (ME)
models (Xue, 2003) and support vector machines
(Yoshida et al, 2003; Asahara et al, 2003).
1The ?B, I, E, S? tags are also called ?OP-CN, CN-CN, CN-
CL, OP-CL? tags (Sekine et al, 1998) or ?LL, MM, RR, LR?
tags (Xue, 2003).
Figure 2: Example of the Character Tagging Method: Word boundaries are indicated by vertical lines (?|?).
This character tagging method can easily han-
dle unknown words, because known words and un-
known words are treated equally and no other ex-
ceptional processing is necessary. This approach is
also used in base-NP chunking (Ramshaw and Mar-
cus, 1995) and named entity recognition (Sekine et
al., 1998) as well as word segmentation.
3 Word Segmentation Using Word-Level
and Character-Level Information
We saw the two methods for word segmentation
in the previous section. It is observed that the
Markov model-based method has high overall ac-
curacy, however, the accuracy drops for unknown
words, and the character tagging method has high
accuracy for unknown words but lower accuracy
for known words (Yoshida et al, 2003; Xue, 2003;
Sproat and Emerson, 2003). This seems natural be-
cause words are used as a processing unit in the
Markov model-based method, and therefore much
information about known words (e.g., POS or word
bigram probability) can be used. However, un-
known words cannot be handled directly by this
method itself. On the other hand, characters are
used as a unit in the character tagging method. In
general, the number of characters is finite and far
fewer than that of words which continuously in-
creases. Thus the character tagging method may be
robust for unknown words, but cannot use more de-
tailed information than character-level information.
Then, we propose a hybrid method which com-
bines the Markov model-based method and the char-
acter tagging method to make the most of word-
level and character-level information, in order to
achieve high overall accuracy.
3.1 A Hybrid Method
The hybrid method is mainly based on word-level
Markov models, but both POC-tags and POS-tags
are used in the same time and word segmentation
for known words and unknown words are conducted
simultaneously.
Figure 3 shows an example of the method given
a Japanese sentence ? ?,
where the word ? ?(person?s name) is an un-
known word. First, given a sentence, nodes of
lattice for known words are made as in the usual
Markov model-based method. Next, for each char-
acter in the sentence, nodes of POC-tags (four nodes
for each character) are made. Then, the most likely
path is searched (the thick line indicates the correct
path in the example). Unknown words are identified
by the nodes with POC-tags. Note that some transi-
tions of states are not allowed (e.g. from I to B, or
from any POS-tags to E), and such transitions are
ignored.
Because the basic Markov models in Equation
(1) are not expressive enough, we use the following
equation instead to estimate probability of a path in
a lattice more precisely:
P (W,T ) =
n?
i=1
P (witi|w0t0 . . . wi?1ti?1),
'
n?
i=1
{?1P (wi|ti)P (ti)
+?2P (wi|ti)P (ti|ti?1)
+?3P (wi|ti)P (ti|ti?2ti?1)
+?4P (witi|wi?1ti?1)},
(?1 + ?2 + ?3 + ?4 = 1). (4)
The probabilities in the equation above are esti-
mated from a word segmented and POS-tagged cor-
pus using the maximum-likelihood method, for ex-
ample,
P (wi|ti) =
?
?
?
f(wi,ti)?
w f(w,ti)
(f(wi, ti) > 0),
0.5?
w f(w,ti)
(f(wi, ti) = 0),
(5)
where f(w, t) is a frequency that the word w with
the tag t occurred in training data. Unseen events
in the training data are handled as they occurred 0.5
times for smoothing. ?1, ?2, ?3, ?4 are calculated
by deleted interpolation as described in (Brants,
2000). A word dictionary for a Markov model-
based system is often constructed from a training
corpus, and no unknown words exist in the training
corpus in such a case. Therefore, when the param-
eters of the above probabilities are trained from a
training corpus, words that appear only once in the
training corpus are regarded as unknown words and
decomposed to characters with POC-tags so that
statistics about unknown words are obtained2.
2As described in Equation (5), we used the additive smooth-
ing method which is simple and easy to implement. Although
there are other more sophisticated methods such as Good-
Turing smoothing, they may not necessarily perform well be-
cause the distribution of words is changed by this operation.
Figure 3: Example of the Hybrid Method
In order to handle various character-level fea-
tures, we calculate word emission probabilities for
POC-tags by Bayes? theorem:
P (wi|ti)
= P (ti|wi, ti ? TPOC)P (wi, ti ? TPOC)P (ti) ,
= P (ti|wi, ti ? TPOC)
?
t?TPOC P (wi, t)
P (ti) , (6)
where TPOC = {B, I,E,S}, wi is a character and
ti is a POC-tag. In the above equation, P (ti) and
P (wi, t) are estimated by the maximum-likelihood
method, and the probability of a POC tag ti, given
a character wi (P (ti|wi, ti ? TPOC)) is estimated
using ME models (Berger et al, 1996). We use the
following features for ME models, where cx is the
xth character in a sentence, wi = ci? and yx is the
character type of cx (Table 2 shows the definition of
character types we used):
(1) Characters (ci??2, ci??1, ci? , ci?+1, ci?+2)
(2) Pairs of characters (ci??2ci??1, ci??1ci? ,
ci??1ci?+1, ci?ci?+1, ci?+1ci?+2)
(3) Character types (yi??2, yi??1, yi? , yi?+1, yi?+2)
(4) Pairs of character types (yi??2yi??1, yi??1yi? ,
yi??1yi?+1, yi?yi?+1, yi?+1yi?+2)
Parameters of ME are trained using all the words in
training data. We use the Generalized Iterative Scal-
ing algorithm (Darroch and Ratcliff, 1972) for pa-
rameter estimation, and features that appeared less
than or equal to 10 times in training data are ignored
in order to avoid overfitting.
What our method is doing for unknown words
can be interpreted as follows: The method exam-
ines all possible unknown words in a sentence, and
probability for an unknown word of length k, wi =
Character Type Description
Alphabet Alphabets
Numeral Arabic and Chinese numerals
Symbol Symbols
Kanji Chinese Characters
Hiragana Hiragana (Japanese scripts)
Katakana Katakana (Japanese scripts)
Table 2: Character Types
cj ? ? ? cj+k?1 is calculated as:
P (witi|h) (7)
=
?
??
??
P (cjS|h) (k = 1),
P (cjB|h)
?j+k?2
l=j+1 P (clI|h)P (cj+k?1E|h)
(k > 1),
where h is a history of the sequence. In other words,
the probability of the unknown word is approxi-
mated by the product of the probabilities of the com-
posing characters, and this calculation is done in the
framework of the word-level Markov model-based
method.
4 Experiments
This section gives experimental results of Chinese
and Japanese word segmentation with the hybrid
method. The following values are used to evaluate
the performance of word segmentation:
R : Recall (The number of correctly segmented
words in system?s output divided by the num-
ber of words in test data)
P : Precision (The number of correctly segmented
words in system?s output divided by the num-
ber of words in system?s output)
F : F-measure (F = 2?R? P/(R+ P ))
Rknown : Recall for known words
Runknown : Recall for unknown words
Corpus # of Training Words # of Testing Words # of Words Rate of
(known/unknown) in Dictionary Unknown Words
AS 5,806,611 11,985 (11,727/ 258) 146,212 0.0215
HK 239,852 34,955 (32,463/2,492) 23,747 0.0713
PK 1,121,017 17,194 (16,005/1,189) 55,226 0.0692
RWCP 840,879 93,155 (93,085/ 70) 315,602 0.0008
Table 3: Statistical Information of Corpora
4.1 Experiments of Chinese Word
Segmentation
We use three Chinese word-segmented corpora, the
Academia Sinica corpus (AS), the Hong Kong City
University corpus (HK) and the Beijing University
corpus (PK), all of which were used in the First
International Chinese Word Segmentation Bake-
off (Sproat and Emerson, 2003) at ACL-SIGHAN
2003.
The three corpora are word-segmented corpora,
but POS-tags are not attached, therefore we need to
attach a POS-tag (state) which is necessary for the
Markov model-based method to each word. We at-
tached a state for each word using the Baum-Welch
algorithm (Rabiner and Juang, 1993) which is used
for Hidden Markov Models. The algorithm finds
a locally optimal tag sequence which maximizes
Equation (1) in an unsupervised way. The initial
states are randomly assigned, and the number of
states is set to 64.
We use the following systems for comparison:
Bakeoff-1, 2, 3 The top three systems participated
in the SIGHAN Bakeoff (Sproat and Emerson,
2003).
Maximum Matching A word segmentation sys-
tem using the well-known maximum matching
method.
Character Tagging A word segmentation system
using the character tagging method. This sys-
tem is almost the same as the one studied by
Xue (2003). Features described in Section 3.1
(1)?(4) and the following (5) are used to esti-
mate a POC tag of a character ci? , where tx is
a POC-tag of the xth character in a sentence:
(5) Unigram and bigram of previous POC-
tags (ti??1, ti??2ti??1)
All these systems including ours do not use any
other knowledge or resources than the training data.
In this experiments, word dictionaries used by the
hybrid method and Maximum Matching are con-
structed from all the words in each training corpus.
Statistical information of these data is shown in Ta-
ble 3. The calculated values of ?i in Equation (4)
are shown in Table 4.
Corpus ?1 ?2 ?3 ?4
AS 0.037 0.178 0.257 0.528
HK 0.048 0.251 0.313 0.388
PK 0.055 0.207 0.242 0.495
RWCP 0.073 0.105 0.252 0.571
Table 4: Calculated Values of ?i
The results are shown in Table 5. Our system
achieved the best F-measure values for the three
corpora. Although the hybrid system?s recall val-
ues for known words are not high compared to the
participants of SIGHAN Bakeoff, the recall values
for known words and unknown words are relatively
well-balanced. The results of Maximum Matching
and Character Tagging show the trade-off between
the word-based approach and the character-based
approach which was discussed in Section 3. Max-
imum Matching is word-based and has the higher
recall values for known words than Character Tag-
ging on the HK and PK corpus. Character Tagging
is character-based and has the highest recall values
for unknown words on the AS, HK and PK corpus.
4.2 Experiments of Japanese Word
Segmentation
We use the RWCP corpus, which is a Japanese
word-segmented and POS-tagged corpus.
We use the following systems for comparison:
ChaSen The word segmentation and POS-tagging
system based on extended Markov models
(Asahara and Matsumoto, 2000; Matsumoto et
al., 2001). This system carries out unknown
word processing using heuristic rules.
Maximum Matching The same system used in the
Chinese experiments.
Character Tagging The same system used in the
Chinese experiments.
As a dictionary for ChaSen, Maximum Matching
and the hybrid method, we use IPADIC (Matsumoto
and Asahara, 2001) which is attached to ChaSen.
Statistical information of these data is shown in Ta-
ble 3. The calculated values of ?i in Equation (4)
are shown in Table 4.
Corpus System R P F Rknown Runknown
Hybrid method 0.973 0.971 0.972 0.979 0.717
Bakeoff-1 0.966 0.956 0.961 0.980 0.364
AS Bakeoff-2 0.961 0.958 0.959 0.966 0.729
Bakeoff-3 0.944 0.945 0.945 0.952 0.574
Maximum Matching 0.917 0.912 0.915 0.938 0.000
Character Tagging 0.962 0.959 0.960 0.966 0.744
Hybrid method 0.951 0.948 0.950 0.969 0.715
Bakeoff-1 0.947 0.934 0.940 0.972 0.625
HK Bakeoff-2 0.940 0.908 0.924 0.980 0.415
Bakeoff-3 0.917 0.915 0.916 0.936 0.670
Maximum Matching 0.908 0.830 0.867 0.975 0.037
Character Tagging 0.917 0.917 0.917 0.932 0.728
Hybrid method 0.957 0.952 0.954 0.970 0.774
Bakeoff-1 0.962 0.940 0.951 0.979 0.724
PK Bakeoff-2 0.955 0.938 0.947 0.976 0.680
Bakeoff-3 0.955 0.938 0.946 0.977 0.647
Maximum Matching 0.930 0.883 0.906 0.974 0.020
Character Tagging 0.932 0.931 0.931 0.943 0.786
Table 5: Performance of Chinese Word Segmentation
Corpus System R P F Rknown Runknown
Hybrid method 0.993 0.994 0.993 0.993 0.586
RWCP ChaSen 0.991 0.992 0.991 0.991 0.243
Maximum Matching 0.880 0.918 0.898 0.880 0.100
Character Tagging 0.972 0.968 0.970 0.972 0.629
Table 6: Performance of Japanese Word Segmentation
The results are shown in Table 63. Compared to
ChaSen, the hybrid method has the comparable F-
measure value and the higher recall value for un-
known words (the difference is statistically signif-
icant at 95% confidence level). Character Tagging
has the highest recall value for unknown words as
in the Chinese experiments.
5 Discussion
Several studies have been conducted on word seg-
mentation and unknown word processing. Xue
(2003) studied Chinese word segmentation using
the character tagging method. As seen in the pre-
vious section, this method handles known and un-
known words in the same way basing on character-
level information. Our experiments showed that the
method has quite high accuracy for unknown words,
but accuracy for known words tends to be lower than
other methods.
3In this evaluation, Rknown and Runknown are calculated
considering words in the dictionary as known words. Words
which are in the training corpus but not in the dictionary are
handled as unknown words in the calculations. The number of
known/unknown words of the RWCP corpus shown in Table 3
is also calculated in the same way.
Uchimoto et al (2001) studied Japanese word
segmentation using ME models. Although their
method is word-based, no word dictionaries are
used directly and known and unknown words are
handled in a same way. The method estimates how
likely a string is to be a word using ME. Given a
sentence, the method estimates the probabilities for
every substrings in the sentence. Word segmenta-
tion is conducted by finding a division of the sen-
tence which maximizes the product of probabilities
that each divided substring is a word. Compared
to our method, their method can handle some types
of features for unknown words such as ?the word
starts with an alphabet and ends with a numeral? or
?the word consists of four characters?. Our method
cannot handle such word-level features because un-
known words are handled by using a character as
a unit. On the other hand, their method seems to
have a computational cost problem. In their method,
unknown words are processed by using a word as
a unit, and the number of candidates for unknown
words in a sentence which consists of n characters
is equal to n(n + 1)/2. Actually, they did not con-
sider every substrings in a sentence, and limited the
length of substrings to be less than or equal to five
characters. In our method, the number of POC-
tagged characters which is necessary for unknown
word processing is equal to 4n, and there is no lim-
itation for the length of unknown words.
Asahara et al (2003) studied Chinese word seg-
mentation based on a character tagging method
with support vector machines. They preprocessed
a given sentence using a word segmenter based on
Markov models, and the output is used as features
for character tagging. Their method is a character-
based method incorporating word-level information
and that is reverse to our approach. They did not use
some of the features we used like character types,
and our method achieved higher accuracies com-
pared to theirs on the AS, HK and PK corpora (Asa-
hara et al, 2003).
6 Conclusion
In this paper, we presented a hybrid method for
word segmentation, which utilizes both word-level
and character-level information to obtain high ac-
curacy for known and unknown words. The method
combines two existing methods, the Markov model-
based method and character tagging method. Ex-
perimental results showed that the method achieves
high accuracy compared to the other state-of-the-art
methods in both Chinese and Japanese word seg-
mentation. The method can conduct POS tagging
for known words as well as word segmentation, but
tagging identified unknown words is left as future
work.
Acknowledgements
This work was supported by a grant from the Na-
tional Institute of Information and Communications
Technology of Japan.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Ex-
tended Models and Tools for High-performance Part-
of-Speech Tagger. In Proceedings of the 18th Inter-
national Conference on Computational Linguistics,
pages 21?27.
Masayuki Asahara, Chooi Ling Goh, Xiaojie Wang, and
Yuji Matsumoto. 2003. Combining Segmenter and
Chunker for Chinese Word Segmentation. In Pro-
ceedings of the 2nd SIGHAN Workshop on Chinese
Language Processing, pages 144?147.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistics, 22(1):39?71.
Thorsten Brants. 2000. TnT ? A Statistical Part-
of-Speech Tagger. In Proceedings of ANLP-NAACL
2000, pages 224?231.
Eugene Charniak, Curtis Hendrickson, Neil Jacobson,
and Mike Perkowitz. 1993. Equations for Part-of-
Speech Tagging. In Proceedings of the Eleventh Na-
tional Conference on Artificial Intelligence, pages
784?789.
J. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The annuals of Mathe-
matical Statistics, 43(5):1470?1480.
Sadao Kurohashi and Makoto Nagao. 1998. Japanese
Morphological Analysis System JUMAN version 3.61.
Department of Informatics, Kyoto University. (in
Japanese).
Yuji Matsumoto and Masayuki Asahara. 2001. IPADIC
User?s Manual version 2.2.4. Nara Institute of Sci-
ence and Technology. (in Japanese).
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, Kazuma
Takaoka, and Masayuki Asahara. 2001. Morpholog-
ical Analysis System ChaSen version 2.2.8 Manual.
Nara Institute of Science and Technology.
Masaki Nagata. 1999. A Part of Speech Estimation
Method for Japanese Unknown Words using a Statis-
tical Model of Morphology and Context. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics, pages 227?284.
Lawrence R. Rabiner and Biing-Hwang Juang. 1993.
Fundamentals of Speech Recognition. PTR Prentice-
Hall.
Lance Ramshaw and Mitch Marcus. 1995. Text Chunk-
ing using Transformation-Based Learning. In Pro-
ceedings of the 3rd Workwhop on Very Large Corpora,
pages 88?94.
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting Text Chunks. In Proceedings of 9th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 173?179.
Satoshi Sekine, Ralph Grishman, and Hiroyuki Shinnou.
1998. A Decision Tree Method for Finding and Clas-
sifying Names in Japanese Texts. In Proceedings of
the 6th Workshop on Very Large Corpora, pages 171?
177.
Richard Sproat and Thomas Emerson. 2003. The First
International Chinese Word Segmentation Bakeoff. In
Proceedings of the Second SIGHAN Workshop on Chi-
nese Language Processing, pages 133?143.
Richard Sproat, Chilin Shih, William Gale, and Nancy
Chang. 1996. A Stochastic Finite-State Word-
Segmentation Algorithm for Chinese. Computational
Linguistics, 22(3):377?404.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
2001. The Unknown Word Problem: a Morphological
Analysis of Japanese Using Maximum Entropy Aided
by a Dictionary. In Proceedings of the 2001 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 91?99.
Nianwen Xue. 2003. Chinese Word Segmentation as
Character Tagging. International Journal of Compu-
tational Linguistics and Chinese, 8(1):29?48.
Tatsumi Yoshida, Kiyonori Ohtake, and Kazuhide Ya-
mamoto. 2003. Performance Evaluation of Chinese
Analyzers with Support Vector Machines. Journal
of Natural Language Processing, 10(1):109?131. (in
Japanese).
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 952?956,
Prague, June 2007. c?2007 Association for Computational Linguistics
Multilingual Dependency Parsing using Global Features
Tetsuji Nakagawa
Oki Electric Industry Co., Ltd.
2?5?7 Honmachi, Chuo-ku, Osaka 541?0053, Japan
nakagawa378@oki.com
Abstract
In this paper, we describe a two-stage multi-
lingual dependency parser used for the mul-
tilingual track of the CoNLL 2007 shared
task. The system consists of two compo-
nents: an unlabeled dependency parser us-
ing Gibbs sampling which can incorporate
sentence-level (global) features as well as
token-level (local) features, and a depen-
dency relation labeling module based on
Support Vector Machines. Experimental re-
sults show that the global features are useful
in all the languages.
1 Introduction
Making use of as many informative features as pos-
sible is crucial to obtain high performance in ma-
chine learning based NLP. Recently, several meth-
ods for incorporating non-local features have been
investigated, though such features often make mod-
els complex and thus complicate inference. Collins
and Koo (2005) proposed a reranking method for
phrase structure parsing with which any type of
global features in a parse tree can be used. For
dependency parsing, McDonald and Pereira (2006)
proposed a method which can incorporate some
types of global features, and Riedel and Clarke
(2006) studied a method using integer linear pro-
gramming which can incorporate global linguistic
constraints. In this paper, we study dependency
parsing using Gibbs sampling which can incorpo-
rate any type of global feature in a sentence. The
parser determines unlabeled dependency structures
only, and we attach dependency relation labels us-
ing Support Vector Machines afterwards.
We participated in the multilingual track of the
CoNLL 2007 shared task (Nivre et al, 2007), and
evaluated the system on data sets of 10 languages
(Hajic? et al, 2004; Aduriz et al, 2003; Mart?? et
al., 2007; Chen et al, 2003; Bo?hmova? et al, 2003;
Marcus et al, 1993; Johansson and Nugues, 2007;
Prokopidis et al, 2005; Csendes et al, 2005; Mon-
temagni et al, 2003; Oflazer et al, 2003).
The rest of the paper describes the specification of
the system and the evaluation results.
2 Unlabeled Dependency Parsing using
Global Features
2.1 Probabilistic Model
Rosenfeld et al (2001) proposed whole-sentence ex-
ponential language models which can incorporate
arbitrary features in a sentence, and we consider here
a similar probabilistic model for dependency pars-
ing which can incorporate any sentence-level fea-
ture. Let w = w1 ? ? ?w|w| denote an input sentence
consisting of |w| tokens, and h = h1 ? ? ?h|w| denote
the sequence of the indices of each token?s head.
Root nodes of a sentence do not have heads, and we
regard the index of a root node?s head as zero, i.e.,
hi ? {0, 1, ? ? ? , |w|} \ {i}. We define the probabil-
ity distribution of the dependency structure h given
a sentence w using exponential models as follows:
P?,M(h|w)= 1Z?,M(w)QM(h|w)exp
{ K?
k=1
?kfk(w,h)
}
,(1)
Z?,M(w)=
?
h??H(w)
QM(h?|w) exp
{ K?
k=1
?kfk(w,h?)
}
, (2)
where QM(h|w) is an initial distribution, fk(w,h)
is the k-th feature function, K is the number of fea-
ture functions, and ?k is the weight of the k-th fea-
ture. H(w) is the set of possible configurations of
heads for a given sentence w. Although it is ap-
propriate that H(w) is the set of projective trees for
projective languages, and is the set of non-projective
trees (which is a superset of the set of projective
trees) for non-projective languages, in this study, we
define H(w) to be the set of all the possible graphs,
which contains |w||w| elements. P?,M(h|w) and
QM(h|w) are defined over H(w)1. The probabil-
ity distribution P?,M(h|w) is a joint distribution of
all the heads conditioned by a sentence, therefore
we call this model sentence-level model. The fea-
ture function fk(w,h) is defined on a sentence w
with heads h, and we can use any information in the
sentence without the independence assumption for
the heads of the tokens, therefore we call fk(w,h)
1H(w) is a superset of the set of non-projective trees, and
is an unnecessarily large set which contains ill-formed depen-
dency trees such as trees with cycles. This issue may cause
reduction of parsing performance, but we adopt this approach
for computational efficiency.
952
sentence-level (global) feature. We define initial
distribution QM(h|w) as the product of qM(h|w, t)
which is the probability distribution of the head h of
each t-th token calculated with maximum entropy
models:
QM(h|w)=
|w|?
t=1
qM(ht|w, t), (3)
qM(h|w, t)= 1YM(w, t) exp
{ L?
l=1
?lgl(w, t, h)
}
, (4)
YM(w, t)=
|w|?
h?=0
h? 6=t
exp
{ L?
l=1
?lgl(w, t, h?)
}
, (5)
where gl(w, t, h) is the l-th feature function, L is the
number of feature functions, and ?l is the weight of
the l-th feature. qM(h|w, t) is a model of the head
of a single token, calculated independently from
other tokens, therefore we call qM(h|w, t) token-
level model, and gl(w, t, h) token-level (local) fea-
ture.
2.2 Decoding and Parameter Estimation
Let us consider how to find the optimal solution
h?, given a sentence w, parameters of the sentence-
level model ? = {?1, ? ? ? , ?K}, and parameters of
the token-level model M = {?1, ? ? ? , ?L}. Since
the probabilistic model contains global features and
efficient algorithms such as dynamic programming
cannot be used, we use Gibbs sampling to obtain
an approximated solution. Gibbs sampling can ef-
ficiently generate samples from high-dimensional
probability distributions with complex dependencies
among variables (Andrieu et al, 2003), and we as-
sume that R samples {h(1), ? ? ? ,h(R)} are generated
from P?,M(h|w) using Gibbs sampling. Then, the
marginal distribution of the head of the t-th token
given w, Pt(h|w), is approximately calculated as
follows:
Pt(h|w) =
?
h1,???,ht?1,ht+1,???,h|w|
ht=h
P?,M(h|w),
=
?
h
P?,M(h|w)?(h, ht) ' 1R
R?
r=1
?(h, h(r)t ), (6)
where ?(i, j) is the Kronecker delta. In order to
find a solution using the marginal distribution, we
adopt the maximum spanning tree (MST) frame-
work proposed by McDonald et al (2005a). In this
framework, scores for possible edges in dependency
graphs are defined, and the optimal dependency tree
is found as the MST in which the summation of the
edge scores is maximized. Let s(i, j) denote the
score of the edge from a parent node (head) i to a
child node (dependent) j. We define s(i, j) as fol-
lows:
s(i, j)=logPj(i|w). (7)
We use the logarithm of the marginal distribution be-
cause the summation of edge scores is maximized
by the MST search algorithms but the product of the
marginal distributions should be maximized. The
best projective parse tree is obtained using the Eis-
ner algorithm (Eisner, 1996) with the scores, and the
best non-projective one is obtained using the Chu-
Liu-Edmonds (CLE) algorithm (McDonald et al,
2005b).
Although in this method, the factored score s(i, j)
is used to measure likelihood of dependency trees,
the score is calculated taking a whole sentence into
consideration using Gibbs sampling.
Next, we explain how to estimate the parame-
ters of our models, given training data consisting of
N examples {?w1,h1?, ? ? ? , ?wN ,hN ?}. In order
to estimate the parameters of the token-level model
M = {?1, ? ? ? , ?L}, we use maximum a posteriori
estimation with Gaussian priors. We define the fol-
lowing objective function M:
M=log
N?
n=1
QM(hn|wn)? 12?2
L?
l=1
?2l , (8)
where ? is a hyper parameter of Gaussian priors.
The optimal parameters M which maximize M can
be obtained by quasi-Newton methods such as the
L-BFGS algorithm with above M and its partial
derivatives. The parameters of the sentence-level
model ? = {?1, ? ? ? , ?K} can also be estimated in
a similar way with the following objective function
L after the parameters of the token-level model are
estimated.
L=log
N?
n=1
P?,M(hn|wn)? 12??2
K?
k=1
?2k. (9)
This objective function and its partial derivative con-
tain summations over all the possible configura-
tions which are difficult to calculate. We approx-
imately calculate these values using static Monte
Carlo (not MCMC) methods with fixed S samples
{hn(1), ? ? ? ,hn(S)} generated from QM(h|wn)2:
logZ?,M(wn)'log 1S
S?
s=1
exp
{ K?
k=1
?kfk(wn,hn(s))
}
,(10)
?
h??H(wn)
P?,M(h?|wn)fk(wn,h?)
' 1S
S?
s=1
fk(wn,hn(s))
Z?,M(wn) exp
{ K?
k?=1
?k?fk?(wn,hn(s))
}
. (11)
2Static Monte Carlo methods become inefficient when the
dimension of the probabilistic distribution is high, and more so-
phisticated methods would be used for accurate parameter esti-
mation.
953
2.3 Local Features
The token-level features used in the system are the
same as those used in MSTParser version 0.4.23.
The features include lexical forms and (coarse and
fine) POS tags of parent tokens, child tokens, their
surrounding tokens, and tokens between the child
and the parent. The direction and the distance from a
parent to its child, and the FEATS fields of the parent
and the child which are split into elements and then
combined are also included. Features that appeared
less than 5 times in training data are ignored.
2.4 Global Features
Global features can capture any information in de-
pendency trees, and the following nine types of
global features are used (In the following, parent
node means a head token, and child node means a
dependent token):
Child Unigram+Parent+Grandparent This fea-
ture template is a 4-tuple consisting of (1) a
child node, (2) its parent node, (3) the direc-
tion from the parent node to the child node, and
(4) the grandparent node.
Each node in the feature template is expanded
to its lexical form and coarse POS tag in or-
der to obtain actual features. Features that ap-
peared in four or less sentences are ignored.
The same procedure is applied to the following
other features.
Child Bigram+Parent This feature template is a 4-
tuple consisting of (1) a child node, (2) its par-
ent node, (3) the direction from the parent node
to the child node, and (4) the nearest outer sib-
ling node (the nearest sibling node which exists
on the opposite side of the parent node) of the
child node. This feature template is almost the
same as the one used by McDonald and Pereira
(2006).
Child Bigram+Parent+Grandparent This feature
template is a 5-tuple. The first four ele-
ments (1)?(4) are the same as the Child Bi-
gram+Parent feature template, and the addi-
tional element (5) is the grandparent node.
Child Trigram+Parent This feature template is a
5-tuple. The first four elements (1)?(4) are the
same as the Child Bigram+Parent feature tem-
plate, and the additional element (5) is the next
nearest outer sibling node of the child node.
3http://sourceforge.net/projects/mstparser
Parent+All Children This feature template is a tu-
ple with more than one element. The first ele-
ment is a parent node, and the other elements
are all of its child nodes.
Parent+All Children+Grandparent This feature
template is a tuple with more than two ele-
ments. The elements other than the last one
are the same as the Parent+All Children feature
template, and the last element is the grandpar-
ent node.
Child+Ancestor This feature template is a 2-tuple
consisting of (1) a child node, and (2) one of its
ancestor nodes.
Acyclic This feature type has one of two values,
true if the dependency tree is acyclic, or false
otherwise.
Projective This feature type has one of two val-
ues, true if the dependency tree is projective,
or false otherwise.
3 Dependency Relation Labeling
3.1 Model
Dependency relation labeling can be handled as a
multi-class classification problem, and we use Sup-
port Vector Machines (SVMs) which have been suc-
cessfully applied to many NLP tasks. Solving large-
scale multi-class classification problem with SVMs
requires substantial computational resources, so we
use the revision learning method (Nakagawa et al,
2002). The revision learning method combines
a probabilistic model which has smaller computa-
tional cost with a binary classifier which has higher
generalization capacity. In the method, the latter
classifier revises the output of the former model to
conduct multi-class classification with higher ac-
curacy and reasonable computational cost. In this
study, we use maximum entropy (ME) models as
the probabilistic model and SVMs with the second
order polynomial kernel as the binary classifier. The
dependency label of each node is determined inde-
pendently of the labeling of other nodes.
3.2 Features
As the features for SVMs to predict the dependency
relation label of the i-th token, we use the lexical
forms, coarse and fine POS tags, and FEATS fields
of the i-th and the hi-th tokens. We also use lex-
ical forms and POS tags of the tokens surround-
ing and in between them (i.e. the j-th token where
j ? {j|min{i, hi} ? 1 ? j ? max{i, hi} + 1}),
the grandparent (hhi-th) token, the sibling tokens ofi (the j?-th token where j? ? {j?|hj? = hi, j? 6= i}),
954
Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish Average
LAS 75.08 72.56 87.90 83.84 80.19 88.41 76.31 76.74 83.61 78.22 80.29
UAS 86.09 81.04 92.86 88.88 86.28 90.13 84.08 82.49 87.91 85.77 86.55
Table 1: Results of Multilingual Dependency Parsing
Algorithm Features Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Eisner local 85.15 80.20 91.75 86.75 84.19 88.65 83.31 80.27 86.72 84.82
(proj.) +global 86.09 81.00 92.86 88.88 85.99 90.13 84.08 81.55 87.91 84.82
CLE local 84.80 80.39 91.23 86.71 84.21 88.07 83.03 81.15 86.85 85.35
(non-proj.) +global 85.83 81.04 92.64 88.84 86.28 90.05 83.87 82.49 87.97 85.77
Table 2: Unlabeled Attachment Scores in Different Settings (underlined values indicate submitted results,
and bold values indicate the highest scores)
and the child tokens of i (the j??-th token where
j?? ? {j??|hj?? = i})4. As the features for ME mod-
els, a subset of them is used since ME models are
used just for reducing the search space, and do not
need so many features.
4 Results and Analysis
In order to tune the system, we split each training
data set into two parts, and used the first half for
training and the remaining half for testing in devel-
opment. The CLE algorithm was used for Basque,
Czech, Hungarian and Turkish, and the Eisner algo-
rithm was used for the others. We used lemmas for
Catalan, Czech, Greek and Italian, and word forms
for all others. The values of the parameters to be
fixed were chosen as R = 500, S = 200, ? = 0.25,
and ?? = 0.25. With these parameter settings, train-
ing took 247 hours, and testing took 343 minutes on
an Opteron 250 processor.
Table 1 shows the evaluation results on the test
sets. Accuracy was measured with the labeled at-
tachment score (LAS) and the unlabeled attachment
score (UAS). Among the participating systems in the
shared task, we obtained the second best average
accuracy in the labeled attachment score, and the
best average accuracy in the unlabeled attachment
score. Compared with other systems, the gap be-
tween our labeled and unlabeled scores is relatively
big. In this study, labeling of dependency relations
was performed in a separate post-processing step,
and each label was predicted independently. The la-
beled scores may be improved if the parsing process
and the labeling process are performed at the same
time, and dependencies among labels are taken into
account.
We conducted experiments with different settings.
Table 2 shows the results measured with the unla-
beled attachment score. In the table, Eisner and
4Although polynomial kernels of SVMs can implicitly han-
dle combined features, some of combined features were also in-
cluded explicitly because using unnecessarily high order poly-
nomial kernels decreases performance.
CLE indicate that the Eisner algorithm and the
CLE algorithm are used in decoding, and local and
+global indicate that local features alone, and local
and global features together are used. The CLE al-
gorithm performed better than the Eisner algorithm
for Basque, Czech, Hungarian, Italian and Turkish.
All of these data sets except Italian contain relatively
a large number of non-projective sentences (the per-
centage of sentences with at least one non-projective
relation in the training data is over 20% (Nivre et al,
2007)), though the Greek data set, on which the Eis-
ner algorithm performed better, also contains many
non-projective sentences (20.3%).
By using the global features, the accuracy was
improved in all the cases except for Turkish with
the Eisner algorithm (Table 2). The increase was
rather large in Chinese and Czech. When the global
features were used in these languages, the depen-
dency accuracy for tokens whose heads had con-
junctions as parts-of-speech was notably improved;
from 80.5% to 86.0% in Chinese (Eisner), and from
73.2% to 77.6% in Czech (CLE). We investigated
the trained global models, and found that Parent+All
Children features, whose parents were conjunctions
and whose children had compatible classes, had
large positive weights, and those whose children had
incompatible classes had large negative weights. A
feature with a larger weight is generally more influ-
ential. Riedel and Clarke (2006) suggested to use
linguistic constraints such as ?arguments of a coor-
dination must have compatible word classes,? and
such constraint seemed to be represented by the fea-
tures in our models.
5 Conclusion
In this study, we applied a dependency parser us-
ing global features to multilingual dependency pars-
ing. Evaluation results showed that the use of global
features was effective to obtain higher accuracy in
multilingual dependency parsing. Improving depen-
dency relation labeling is left for future work.
955
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque Dependency Tree-
bank. In Proc. of the 2nd Workshop on Treebanks and
Linguistic Theories (TLT), pages 201?204.
C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan.
2003. An Introduction to MCMC for Machine Learn-
ing. Machine Learning, 50:5?43.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica Treebank: Design Crite-
ria, Representational Issues and Implementation. In
Abeille? (Abeille?, 2003), chapter 13, pages 231?248.
M. Collins and T. Koo. 2005. Discriminative Rerank-
ing for Natural Language Parsing. Computational Lin-
guistics, 31(1):25?69.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Eisner. 1996. Three New Probabilistic Models for De-
pendency Parsing: An Exploration. In Proc. of COL-
ING ?96, pages 340?345.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka.
2004. Prague Arabic Dependency Treebank: Develop-
ment in Data and Tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
Constituent-to-Dependency Conversion for English.
In Proc. of the 16th Nordic Conference on Computa-
tional Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English:
the Penn Treebank. Computational Linguistics,
19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A Multilingual and Multilevel
Annotated Corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proc. of EACL 2006, pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line Large-Margin Training of Dependency Parsers. In
Proc. of ACL 2005, pages 91?98.
R. McDonald, F. Pereira, K. Ribarow, and J. Hajic.
2005b. Non-projective dependency parsing using
Spanning Tree Algorithms. In Proc. of HLT/EMNLP
2005, pages 523?530.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
T. Nakagawa, T. Kudo, and Y. Matsumoto. 2002. Re-
vision Learning and its Application to Part-of-speech
Tagging. In Proc. of ACL 2002, pages 497?504.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. In Proc. of
the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish Treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
Practical Issues in the Construction of a Greek Depen-
dency Treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
S. Riedel and J. Clarke. 2006. Incremental Integer Linear
Programming for Non-projective Dependency Parsing.
In Proc. of EMNLP 2006, pages 129?137.
R. Rosenfeld, S. F. Chen, and X. Zhu. 2001. Whole-
Sentence Exponential Language Models: A Vehi-
cle For Linguistic-Statistical Integration. Computers
Speech and Language, 15(1):55?73.
956
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 705?712,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Guessing Parts-of-Speech of Unknown Words Using Global Information
Tetsuji Nakagawa
Corporate R&D Center
Oki Electric Industry Co., Ltd.
2?5?7 Honmachi, Chuo-ku
Osaka 541?0053, Japan
nakagawa378@oki.com
Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916?5 Takayama, Ikoma
Nara 630?0101, Japan
matsu@is.naist.jp
Abstract
In this paper, we present a method for
guessing POS tags of unknown words us-
ing local and global information. Al-
though many existing methods use only
local information (i.e. limited window
size or intra-sentential features), global in-
formation (extra-sentential features) pro-
vides valuable clues for predicting POS
tags of unknown words. We propose a
probabilistic model for POS guessing of
unknown words using global information
as well as local information, and estimate
its parameters using Gibbs sampling. We
also attempt to apply the model to semi-
supervised learning, and conduct experi-
ments on multiple corpora.
1 Introduction
Part-of-speech (POS) tagging is a fundamental
language analysis task. In POS tagging, we fre-
quently encounter words that do not exist in train-
ing data. Such words are called unknown words.
They are usually handled by an exceptional pro-
cess in POS tagging, because the tagging sys-
tem does not have information about the words.
Guessing the POS tags of such unknown words is
a difficult task. But it is an important issue both
for conducting POS tagging accurately and for
creating word dictionaries automatically or semi-
automatically. There have been many studies on
POS guessing of unknown words (Mori and Na-
gao, 1996; Mikheev, 1997; Chen et al, 1997; Na-
gata, 1999; Orphanos and Christodoulakis, 1999).
In most of these previous works, POS tags of un-
known words were predicted using only local in-
formation, such as lexical forms and POS tags
of surrounding words or word-internal features
(e.g. suffixes and character types) of the unknown
words. However, this approach has limitations
in available information. For example, common
nouns and proper nouns are sometimes difficult
to distinguish with only the information of a sin-
gle occurrence because their syntactic functions
are almost identical. In English, proper nouns
are capitalized and there is generally little ambi-
guity between common nouns and proper nouns.
In Chinese and Japanese, no such convention ex-
ists and the problem of the ambiguity is serious.
However, if an unknown word with the same lex-
ical form appears in another part with informa-
tive local features (e.g. titles of persons), this will
give useful clues for guessing the part-of-speech
of the ambiguous one, because unknown words
with the same lexical form usually have the same
part-of-speech. For another example, there is a
part-of-speech named sahen-noun (verbal noun) in
Japanese. Verbal nouns behave as common nouns,
except that they are used as verbs when they are
followed by a verb ?suru?; e.g., a verbal noun
?dokusho? means ?reading? and ?dokusho-suru?
is a verb meaning to ?read books?. It is diffi-
cult to distinguish a verbal noun from a common
noun if it is used as a noun. However, it will
be easy if we know that the word is followed by
?suru? in another part in the document. This issue
was mentioned by Asahara (2003) as a problem
of possibility-based POS tags. A possibility-based
POS tag is a POS tag that represents all the possi-
ble properties of the word (e.g., a verbal noun is
used as a noun or a verb), rather than a property of
each instance of the word. For example, a sahen-
noun is actually a noun that can be used as a verb
when it is followed by ?suru?. This property can-
not be confirmed without observing real usage of
the word appearing with ?suru?. Such POS tags
may not be identified with only local information
of one instance, because the property that each in-
stance has is only one among all the possible prop-
erties.
To cope with these issues, we propose a method
that uses global information as well as local in-
formation for guessing the parts-of-speech of un-
known words. With this method, all the occur-
rences of the unknown words in a document1 are
taken into consideration at once, rather than that
each occurrence of the words is processed sepa-
rately. Thus, the method models the whole doc-
ument and finds a set of parts-of-speech by max-
imizing its conditional joint probability given the
document, rather than independently maximizing
the probability of each part-of-speech given each
sentence. Global information is known to be use-
ful in other NLP tasks, especially in the named en-
tity recognition task, and several studies success-
fully used global features (Chieu and Ng, 2002;
Finkel et al, 2005).
One potential advantage of our method is its
1In this paper, we use the word document to denote the
whole data consisting of multiple sentences (training corpus
or test corpus).
705
ability to incorporate unlabeled data. Global fea-
tures can be increased by simply adding unlabeled
data into the test data.
Models in which the whole document is taken
into consideration need a lot of computation com-
pared to models with only local features. They
also cannot process input data one-by-one. In-
stead, the entire document has to be read before
processing. We adopt Gibbs sampling in order to
compute the models efficiently, and these models
are suitable for offline use such as creating dictio-
naries from raw text where real-time processing is
not necessary but high-accuracy is needed to re-
duce human labor required for revising automati-
cally analyzed data.
The rest of this paper is organized as follows:
Section 2 describes a method for POS guessing of
unknown words which utilizes global information.
Section 3 shows experimental results on multiple
corpora. Section 4 discusses related work, and
Section 5 gives conclusions.
2 POS Guessing of Unknown Words with
Global Information
We handle POS guessing of unknown words as a
sub-task of POS tagging, in this paper. We assume
that POS tags of known words are already deter-
mined beforehand, and positions in the document
where unknown words appear are also identified.
Thus, we focus only on prediction of the POS tags
of unknown words.
In the rest of this section, we first present a
model for POS guessing of unknown words with
global information. Next, we show how the test
data is analyzed and how the parameters of the
model are estimated. A method for incorporating
unlabeled data with the model is also discussed.
2.1 Probabilistic Model Using Global
Information
We attempt to model the probability distribution
of the parts-of-speech of all occurrences of the
unknown words in a document which have the
same lexical form. We suppose that such parts-
of-speech have correlation, and the part-of-speech
of each occurrence is also affected by its local
context. Similar situations to this are handled in
physics. For example, let us consider a case where
a number of electrons with spins exist in a system.
The spins interact with each other, and each spin is
also affected by the external magnetic field. In the
physical model, if the state of the system is s and
the energy of the system is E(s), the probability
distribution of s is known to be represented by the
following Boltzmann distribution:
P (s)= 1Z exp{??E(s)}, (1)
where ? is inverse temperature and Z is a normal-
izing constant defined as follows:
Z=
?
s
exp{??E(s)}. (2)
Takamura et al (2005) applied this model to an
NLP task, semantic orientation extraction, and we
apply it to POS guessing of unknown words here.
Suppose that unknown words with the same lex-
ical form appear K times in a document. Assume
that the number of possible POS tags for unknown
words is N , and they are represented by integers
from 1 to N . Let tk denote the POS tag of the kth
occurrence of the unknown words, let wk denote
the local context (e.g. the lexical forms and the
POS tags of the surrounding words) of the kth oc-
currence of the unknown words, and let w and t
denote the sets of wk and tk respectively:
w={w1, ? ? ? , wK}, t={t1, ? ? ? , tK}, tk?{1, ? ? ? , N}.
?i,j is a weight which denotes strength of the in-
teraction between parts-of-speech i and j, and is
symmetric (?i,j = ?j,i). We define the energy
where POS tags of unknown words given w are
t as follows:
E(t|w)=?
{
1
2
K?
k=1
K?
k?=1
k? 6=k
?tk,tk? +
K?
k=1
log p0(tk|wk)
}
,
(3)
where p0(t|w) is an initial distribution (local
model) of the part-of-speech t which is calculated
with only the local context w, using arbitrary sta-
tistical models such as maximum entropy models.
The right hand side of the above equation consists
of two components; one represents global interac-
tions between each pair of parts-of-speech, and the
other represents the effects of local information.
In this study, we fix the inverse temperature
? = 1. The distribution of t is then obtained from
Equation (1), (2) and (3) as follows:
P (t|w)= 1Z(w)p0(t|w) exp
{
1
2
K?
k=1
K?
k?=1
k? 6=k
?tk,tk?
}
, (4)
Z(w)=
?
t?T (w)
p0(t|w) exp
{
1
2
K?
k=1
K?
k?=1
k? 6=k
?tk,tk?
}
, (5)
p0(t|w)?
K?
k=1
p0(tk|wk), (6)
where T (w) is the set of possible configurations
of POS tags given w. The size of T (w) is NK ,
because there are K occurrences of the unknown
words and each unknown word can have one of N
POS tags. The above equations can be rewritten as
follows by defining a function fi,j(t):
fi,j(t)?12
K?
k=1
K?
k?=1
k? 6=k
?(tk, i)?(tk? , j), (7)
P (t|w)= 1Z(w)p0(t|w) exp
{ N?
i=1
N?
j=1
?i,jfi,j(t)
}
, (8)
Z(w)=
?
t?T (w)
p0(t|w) exp
{ N?
i=1
N?
j=1
?i,jfi,j(t)
}
, (9)
706
where ?(i, j) is the Kronecker delta:
?(i, j)=
{ 1 (i = j),
0 (i 6= j). (10)
fi,j(t) represents the number of occurrences of the
POS tag pair i and j in the whole document (di-
vided by 2), and the model in Equation (8) is es-
sentially a maximum entropy model with the doc-
ument level features.
As shown above, we consider the conditional
joint probability of all the occurrences of the un-
known words with the same lexical form in the
document given their local contexts, P (t|w), in
contrast to conventional approaches which assume
independence of the sentences in the document
and use the probabilities of all the words only in
a sentence. Note that we assume independence
between the unknown words with different lexical
forms, and each set of the unknown words with the
same lexical form is processed separately from the
sets of other unknown words.
2.2 Decoding
Let us consider how to find the optimal POS tags t
basing on the model, given K local contexts of the
unknown words with the same lexical form (test
data) w, an initial distribution p0(t|w) and a set
of model parameters ? = {?1,1, ? ? ? , ?N,N}. One
way to do this is to find a set of POS tags which
maximizes P (t|w) among all possible candidates
of t. However, the number of all possible candi-
dates of the POS tags is NK and the calculation is
generally intractable. Although HMMs, MEMMs,
and CRFs use dynamic programming and some
studies with probabilistic models which have spe-
cific structures use efficient algorithms (Wang et
al., 2005), such methods cannot be applied here
because we are considering interactions (depen-
dencies) between all POS tags, and their joint dis-
tribution cannot be decomposed. Therefore, we
use a sampling technique and approximate the so-
lution using samples obtained from the probability
distribution.
We can obtain a solution t? = {t?1, ? ? ? , t?K} as
follows:
t?k=argmax
t
Pk(t|w), (11)
where Pk(t|w) is the marginal distribution of the
part-of-speech of the kth occurrence of the un-
known words given a set of local contexts w, and
is calculated as an expected value over the distri-
bution of the unknown words as follows:
Pk(t|w)=
?
t1,???,tk?1,tk+1,???,tK
tk=t
P (t|w),
=
?
t?T (w)
?(tk, t)P (t|w). (12)
Expected values can be approximately calculated
using enough number of samples generated from
the distribution (MacKay, 2003). Suppose that
A(x) is a function of a random variable x, P (x)
initialize t(1)
for m := 2 to M
for k := 1 to K
t(m)k ? P (tk|w, t(m)1 , ? ? ? , t(m)k?1, t(m?1)k+1 , ? ? ? , t(m?1)K )
Figure 1: Gibbs Sampling
is a distribution of x, and {x(1), ? ? ? ,x(M)} are M
samples generated from P (x). Then, the expec-
tation of A(x) over P (x) is approximated by the
samples: ?
x
A(x)P (x)' 1M
M?
m=1
A(x(m)). (13)
Thus, if we have M samples {t(1), ? ? ? , t(M)}
generated from the conditional joint distribution
P (t|w), the marginal distribution of each POS tag
is approximated as follows:
Pk(t|w)' 1M
M?
m=1
?(t(m)k , t). (14)
Next, we describe how to generate samples
from the distribution. We use Gibbs sampling
for this purpose. Gibbs sampling is one of the
Markov chain Monte Carlo (MCMC) methods,
which can generate samples efficiently from high-
dimensional probability distributions (Andrieu et
al., 2003). The algorithm is shown in Figure 1.
The algorithm firstly set the initial state t(1), then
one new random variable is sampled at a time
from the conditional distribution in which all other
variables are fixed, and new samples are cre-
ated by repeating the process. Gibbs sampling is
easy to implement and is guaranteed to converge
to the true distribution. The conditional distri-
bution P (tk|w, t1, ? ? ? , tk?1, tk+1, ? ? ? , tK) in Fig-
ure 1 can be calculated simply as follows:
P (tk|w, t1, ? ? ? , tk?1, tk+1, ? ? ? , tK)
= P (t|w)P (t1, ? ? ? , tk?1, tk+1, ? ? ? , tK |w) ,
=
1
Z(w)p0(t|w) exp{ 12
?K
k?=1
?K
k??=1
k?? 6=k?
?tk? ,tk?? }
?N
t?k=1
P (t1, ? ? ? , tk?1, t?k, tk+1, ? ? ? , tK |w)
,
=
p0(tk|wk) exp{
?K
k?=1
k? 6=k
?tk? ,tk}
?N
t?k=1
p0(t?k|wk) exp{
?K
k?=1
k? 6=k
?tk? ,t?k}
, (15)
where the last equation is obtained using the fol-
lowing relation:
1
2
K?
k?=1
K?
k??=1
k?? 6=k?
?tk? ,tk??=
1
2
K?
k?=1
k? 6=k
K?
k??=1
k?? 6=k,k?? 6=k?
?tk? ,tk?? +
K?
k?=1
k? 6=k
?tk? ,tk .
In later experiments, the number of samples M is
set to 100, and the initial state t(1) is set to the POS
tags which maximize p0(t|w).
The optimal solution obtained by Equation (11)
maximizes the probability of each POS tag given
w, and this kind of approach is known as the maxi-
mum posterior marginal (MPM) estimate (Marro-
quin, 1985). Finkel et al (2005) used simulated
annealing with Gibbs sampling to find a solution
in a similar situation. Unlike simulated annealing,
this approach does not need to define a cooling
707
schedule. Furthermore, this approach can obtain
not only the best solution but also the second best
or the other solutions according to Pk(t|w), which
are useful when this method is applied to semi-
automatic construction of dictionaries because hu-
man annotators can check the ranked lists of can-
didates.
2.3 Parameter Estimation
Let us consider how to estimate the param-
eter ? = {?1,1, ? ? ? , ?N,N} in Equation (8)
from training data consisting of L examples;
{?w1, t1?, ? ? ? , ?wL, tL?} (i.e., the training data
contains L different lexical forms of unknown
words). We define the following objective func-
tion L?, and find ? which maximizes L? (the sub-
script ? denotes being parameterized by ?):
L? = log
L?
l=1
P?(tl|wl) + logP (?),
= log
L?
l=1
1
Z?(wl)p0(t
l|wl) exp
{ N?
i=1
N?
j=1
?i,jfi,j(tl)
}
+ logP (?),
=
L?
l=1
[
?logZ?(wl)+log p0(tl|wl)+
N?
i=1
N?
j=1
?i,jfi,j(tl)
]
+ logP (?). (16)
The partial derivatives of the objective function
are:
?L?
??i,j =
L?
l=1
[
fi,j(tl)? ???i,j logZ?(w
l)
]
+ ???i,j logP (?),
=
L?
l=1
[
fi,j(tl)?
?
t?T (wl)
fi,j(t)P?(t|wl)
]
+ ???i,j logP (?).(17)
We use Gaussian priors (Chen and Rosenfeld,
1999) for P (?):
logP (?)=?
N?
i=1
N?
j=1
?2i,j
2?2 + C,
?
??i,j logP (?) = ?
?i,j
?2 .
where C is a constant and ? is set to 1 in later
experiments. The optimal ? can be obtained by
quasi-Newton methods using the above L? and
?L?
??i,j , and we use L-BFGS (Liu and Nocedal,
1989) for this purpose2. However, the calculation
is intractable because Z?(wl) (see Equation (9))
in Equation (16) and a term in Equation (17) con-
tain summations over all the possible POS tags. To
cope with the problem, we use the sampling tech-
nique again for the calculation, as suggested by
Rosenfeld et al (2001). Z?(wl) can be approx-
imated using M samples {t(1), ? ? ? , t(M)} gener-
ated from p0(t|wl):
Z?(wl)=
?
t?T (wl)
p0(t|wl) exp
{ N?
i=1
N?
j=1
?i,jfi,j(t)
}
,
2In later experiments, L-BFGS often did not converge
completely because we used approximation with Gibbs sam-
pling, and we stopped iteration of L-BFGS in such cases.
' 1M
M?
m=1
exp
{ N?
i=1
N?
j=1
?i,jfi,j(t(m))
}
. (18)
The term in Equation (17) can also be approxi-
mated using M samples {t(1), ? ? ? , t(M)} gener-
ated from P?(t|wl) with Gibbs sampling:
?
t?T (wl)
fi,j(t)P?(t|wl)' 1M
M?
m=1
fi,j(t(m)). (19)
In later experiments, the initial state t(1) in Gibbs
sampling is set to the gold standard tags in the
training data.
2.4 Use of Unlabeled Data
In our model, unlabeled data can be easily used
by simply concatenating the test data and the unla-
beled data, and decoding them in the testing phase.
Intuitively, if we increase the amount of the test
data, test examples with informative local features
may increase. The POS tags of such examples can
be easily predicted, and they are used as global
features in prediction of other examples. Thus,
this method uses unlabeled data in only the test-
ing phase, and the training phase is the same as
the case with no unlabeled data.
3 Experiments
3.1 Data and Procedure
We use eight corpora for our experiments; the
Penn Chinese Treebank corpus 2.0 (CTB), a part
of the PFR corpus (PFR), the EDR corpus (EDR),
the Kyoto University corpus version 2 (KUC), the
RWCP corpus (RWC), the GENIA corpus 3.02p
(GEN), the SUSANNE corpus (SUS) and the Penn
Treebank WSJ corpus (WSJ), (cf. Table 1). All
the corpora are POS tagged corpora in Chinese(C),
English(E) or Japanese(J), and they are split into
three portions; training data, test data and unla-
beled data. The unlabeled data is used in ex-
periments of semi-supervised learning, and POS
tags of unknown words in the unlabeled data are
eliminated. Table 1 summarizes detailed informa-
tion about the corpora we used: the language, the
number of POS tags, the number of open class
tags (POS tags that unknown words can have, de-
scribed later), the sizes of training, test and un-
labeled data, and the splitting method of them.
For the test data and the unlabeled data, unknown
words are defined as words that do not appear in
the training data. The number of unknown words
in the test data of each corpus is shown in Ta-
ble 1, parentheses. Accuracy of POS guessing of
unknown words is calculated based on how many
words among them are correctly POS-guessed.
Figure 2 shows the procedure of the experi-
ments. We split the training data into two parts;
the first half as sub-training data 1 and the latter
half as sub-training data 2 (Figure 2, *1). Then,
we check the words that appear in the sub-training
708
Corpus # of POS # of Tokens (# of Unknown Words) [partition in the corpus]
(Lang.) (Open Class) Training Test Unlabeled
CTB 34 84,937 7,980 (749) 6,801
(C) (28) [sec. 1?270] [sec. 271?300] [sec. 301?325]
PFR 42 304,125 370,627 (27,774) 445,969
(C) (39) [Jan. 1?Jan. 9] [Jan. 10?Jan. 19] [Jan. 20?Jan. 31]
EDR 15 2,550,532 1,280,057 (24,178) 1,274,458
(J) (15) [id = 4n+ 0, id = 4n+ 1] [id = 4n+ 2] [id = 4n+ 3]
KUC 40 198,514 31,302 (2,477) 41,227
(J) (36) [Jan. 1?Jan. 8] [Jan. 9] [Jan. 10]
RWC 66 487,333 190,571 (11,177) 210,096
(J) (55) [1?10,000th sentences] [10,001?14,000th sentences] [14,001?18,672th sentences]
GEN 47 243,180 123,386 (7,775) 134,380
(E) (36) [1?10,000th sentences] [10,001?15,000th sentences] [15,001?20,546th sentences]
SUS 125 74,902 37,931 (5,760) 37,593
(E) (90) [sec. A01?08, G01?08, [sec. A09?12, G09?12, [sec. A13?20, G13?22,
J01?08, N01?08] J09?17, N09?12] J21?24, N13?18]
WSJ 45 912,344 129,654 (4,253) 131,768
(E) (33) [sec. 0?18] [sec. 22?24] [sec. 19?21]
Table 1: Statistical Information of Corpora
Corpus TrainingData
Test
Data
Unlabeled
Data
Sub-
Training
data 1(*1)
Sub-
Training
data 2(*1)
Sub-Local Model 1(*3)
Sub-Local Model 2(*3)
Global Model
Local Model(*2)
(optional)
Test
Result
Data flow for training
Data flow for testing
Figure 2: Experimental Procedure
data 1 but not in the sub-training data 2, or vice
versa. We handle these words as (pseudo) un-
known words in the training data. Such (two-fold)
cross-validation is necessary to make training ex-
amples that contain unknown words3. POS tags
that these pseudo unknown words have are defined
as open class tags, and only the open class tags
are considered as candidate POS tags for unknown
words in the test data (i.e., N is equal to the num-
ber of the open class tags). In the training phase,
we need to estimate two types of parameters; local
model (parameters), which is necessary to calcu-
late p0(t|w), and global model (parameters), i.e.,
?i,j . The local model parameters are estimated
using all the training data (Figure 2, *2). Local
3A major method for generating such pseudo unknown
words is to collect the words that appear only once in a cor-
pus (Nagata, 1999). These words are called hapax legom-
ena and known to have similar characteristics to real un-
known words (Baayen and Sproat, 1996). These words are
interpreted as being collected by the leave-one-out technique
(which is a special case of cross-validation) as follows: One
word is picked from the corpus and the rest of the corpus
is considered as training data. The picked word is regarded
as an unknown word if it does not exist in the training data.
This procedure is iterated for all the words in the corpus.
However, this approach is not applicable to our experiments
because those words that appear only once in the corpus do
not have global information and are useless for learning the
global model, so we use the two-fold cross validation method.
model parameters and training data are necessary
to estimate the global model parameters, but the
global model parameters cannot be estimated from
the same training data from which the local model
parameters are estimated. In order to estimate the
global model parameters, we firstly train sub-local
models 1 and 2 from the sub-training data 1 and
2 respectively (Figure 2, *3). The sub-local mod-
els 1 and 2 are used for calculating p0(t|w) of un-
known words in the sub-training data 2 and 1 re-
spectively, when the global model parameters are
estimated from the entire training data. In the test-
ing phase, p0(t|w) of unknown words in the test
data are calculated using the local model param-
eters which are estimated from the entire training
data, and test results are obtained using the global
model with the local model.
Global information cannot be used for unknown
words whose lexical forms appear only once in
the training or test data, so we process only non-
unique unknown words (unknown words whose
lexical forms appear more than once) using the
proposed model. In the testing phase, POS tags of
unique unknown words are determined using only
the local information, by choosing POS tags which
maximize p0(t|w).
Unlabeled data can be optionally used for semi-
supervised learning. In that case, the test data and
the unlabeled data are concatenated, and the best
POS tags which maximize the probability of the
mixed data are searched.
3.2 Initial Distribution
In our method, the initial distribution p0(t|w) is
used for calculating the probability of t given lo-
cal context w (Equation (8)). We use maximum
entropy (ME) models for the initial distribution.
p0(t|w) is calculated by ME models as follows
(Berger et al, 1996):
p0(t|w)= 1Y (w) exp
{ H?
h=1
?hgh(w, t)
}
, (20)
709
Language Features
English Prefixes of ?0 up to four characters,
suffixes of ?0 up to four characters,
?0 contains Arabic numerals,
?0 contains uppercase characters,
?0 contains hyphens.
Chinese Prefixes of ?0 up to two characters,
Japanese suffixes of ?0 up to two characters,
?1, ?|?0|, ?1 & ?|?0|,?|?0|
i=1 {?i} (set of character types).
(common) |?0| (length of ?0),
??1, ?+1, ??2 & ??1, ?+1 & ?+2,
??1 & ?+1, ??1 & ??1, ?+1 & ?+1,
??2 & ??2 & ??1 & ??1,
?+1 & ?+1 & ?+2 & ?+2,
??1 & ??1 & ?+1 & ?+1.
Table 2: Features Used for Initial Distribution
Y (w)=
N?
t=1
exp
{ H?
h=1
?hgh(w, t)
}
, (21)
where gh(w, t) is a binary feature function. We
assume that each local context w contains the fol-
lowing information about the unknown word:
? The POS tags of the two words on each side
of the unknown word: ??2, ??1, ?+1, ?+2.4
? The lexical forms of the unknown word itself
and the two words on each side of the un-
known word: ??2, ??1, ?0, ?+1, ?+2.
? The character types of all the characters com-
posing the unknown word: ?1, ? ? ? , ?|?0|.
We use six character types: alphabet, nu-
meral (Arabic and Chinese numerals), sym-
bol, Kanji (Chinese character), Hiragana
(Japanese script) and Katakana (Japanese
script).
A feature function gh(w, t) returns 1 if w and t
satisfy certain conditions, and otherwise 0; for ex-
ample:
g123(w, t)=
{ 1 (??1 =?President? and ??1 =?NNP? and t = 5),
0 (otherwise).
The features we use are shown in Table 2, which
are based on the features used by Ratnaparkhi
(1996) and Uchimoto et al (2001).
The parameters ?h in Equation (20) are esti-
mated using all the words in the training data
whose POS tags are the open class tags.
3.3 Experimental Results
The results are shown in Table 3. In the table, lo-
cal, local+global and local+global w/ unlabeled
indicate that the results were obtained using only
local information, local and global information,
and local and global information with the extra un-
labeled data, respectively. The results using only
local information were obtained by choosing POS
4In both the training and the testing phases, POS tags of
known words are given from the corpora. When these sur-
rounding words contain unknown words, their POS tags are
represented by a special tag Unk.
PFR (Chinese)
+162 vn (verbal noun)
+150 ns (place name)
+86 nz (other proper noun)
+85 j (abbreviation)
+61 nr (personal name)
? ? ? ? ? ?
?26 m (numeral)
?100 v (verb)
RWC (Japanese)
+33 noun-proper noun-person name-family name
+32 noun-proper noun-place name
+28 noun-proper noun-organization name
+17 noun-proper noun-person name-first name
+6 noun-proper noun
+4 noun-sahen noun
? ? ? ? ? ?
?2 noun-proper noun-place name-country name
?29 noun
SUS (English)
+13 NP (proper noun)
+6 JJ (adjective)
+2 VVD (past tense form of lexical verb)
+2 NNL (locative noun)
+2 NNJ (organization noun)
? ? ? ? ? ?
?3 NN (common noun)
?6 NNU (unit-of-measurement noun)
Table 4: Ordered List of Increased/Decreased
Number of Correctly Tagged Words
tags t? = {t?1, ? ? ? , t?K} which maximize the proba-
bilities of the local model:
t?k=argmax
t
p0(t|wk). (22)
The table shows the accuracies, the numbers of er-
rors, the p-values of McNemar?s test against the
results using only local information, and the num-
bers of non-unique unknown words in the test
data. On an Opteron 250 processor with 8GB of
RAM, model parameter estimation and decoding
without unlabeled data for the eight corpora took
117 minutes and 39 seconds in total, respectively.
In the CTB, PFR, KUC, RWC and WSJ cor-
pora, the accuracies were improved using global
information (statistically significant at p < 0.05),
compared to the accuracies obtained using only lo-
cal information. The increases of the accuracies on
the English corpora (the GEN and SUS corpora)
were small. Table 4 shows the increased/decreased
number of correctly tagged words using global in-
formation in the PFR, RWC and SUS corpora.
In the PFR (Chinese) and RWC (Japanese) cor-
pora, many proper nouns were correctly tagged us-
ing global information. In Chinese and Japanese,
proper nouns are not capitalized, therefore proper
nouns are difficult to distinguish from common
nouns with only local information. One reason
that only the small increases were obtained with
global information in the English corpora seems to
be the low ambiguities of proper nouns. Many ver-
bal nouns in PFR and a few sahen-nouns (Japanese
verbal nouns) in RWC, which suffer from the
problem of possibility-based POS tags, were also
correctly tagged using global information. When
the unlabeled data was used, the number of non-
unique words in the test data increased. Compared
with the case without the unlabeled data, the accu-
710
Corpus Accuracy for Unknown Words (# of Errors)
(Lang.) [p-value] ?# of Non-unique Unknown Words?
local local+global local+global w/ unlabeled
CTB 0.7423 (193) 0.7717 (171) 0.7704 (172)
(C) [0.0000] ?344? [0.0001] ?361?
PFR 0.6499 (9723) 0.6690 (9193) 0.6785 (8930)
(C) [0.0000] ?16019? [0.0000] ?18861?
EDR 0.9639 (874) 0.9643 (863) 0.9651 (844)
(J) [0.1775] ?4903? [0.0034] ?7770?
KUC 0.7501 (619) 0.7634 (586) 0.7562 (604)
(J) [0.0000] ?788? [0.0872] ?936?
RWC 0.7699 (2572) 0.7785 (2476) 0.7787 (2474)
(J) [0.0000] ?5044? [0.0000] ?5878?
GEN 0.8836 (905) 0.8837 (904) 0.8863 (884)
(E) [1.0000] ?4094? [0.0244] ?4515?
SUS 0.7934 (1190) 0.7957 (1177) 0.7979 (1164)
(E) [0.1878] ?3210? [0.0116] ?3583?
WSJ 0.8345 (704) 0.8368 (694) 0.8352 (701)
(E) [0.0162] ?1412? [0.7103] ?1627?
Table 3: Results of POS Guessing of Unknown Words
Corpus Mean?Standard Deviation
(Lang.) Marginal S.A.
CTB (C) 0.7696?0.0021 0.7682?0.0028
PFR (C) 0.6707?0.0010 0.6712?0.0014
EDR (J) 0.9644?0.0001 0.9645?0.0001
KUC (J) 0.7595?0.0031 0.7612?0.0018
RWC (J) 0.7777?0.0017 0.7772?0.0020
GEN (E) 0.8841?0.0009 0.8840?0.0007
SUS (E) 0.7997?0.0038 0.7995?0.0034
WSJ (E) 0.8366?0.0013 0.8360?0.0021
Table 5: Results of Multiple Trials and Compari-
son to Simulated Annealing
racies increased in several corpora but decreased
in the CTB, KUC and WSJ corpora.
Since our method uses Gibbs sampling in the
training and the testing phases, the results are af-
fected by the sequences of random numbers used
in the sampling. In order to investigate the influ-
ence, we conduct 10 trials with different sequences
of pseudo random numbers. We also conduct ex-
periments using simulated annealing in decoding,
as conducted by Finkel et al (2005) for informa-
tion extraction. We increase inverse temperature ?
in Equation (1) from ? = 1 to ? ? ? with the
linear cooling schedule. The results are shown in
Table 5. The table shows the mean values and the
standard deviations of the accuracies for the 10 tri-
als, and Marginal and S.A. mean that decoding is
conducted using Equation (11) and simulated an-
nealing respectively. The variances caused by ran-
dom numbers and the differences of the accuracies
between Marginal and S.A. are relatively small.
4 Related Work
Several studies concerning the use of global infor-
mation have been conducted, especially in named
entity recognition, which is a similar task to POS
guessing of unknown words. Chieu and Ng (2002)
conducted named entity recognition using global
features as well as local features. In their ME
model-based method, some global features were
used such as ?when the word appeared first in a
position other than the beginning of sentences, the
word was capitalized or not?. These global fea-
tures are static and can be handled in the same
manner as local features, therefore Viterbi decod-
ing was used. The method is efficient but does not
handle interactions between labels.
Finkel et al (2005) proposed a method incorpo-
rating non-local structure for information extrac-
tion. They attempted to use label consistency of
named entities, which is the property that named
entities with the same lexical form tend to have
the same label. They defined two probabilis-
tic models; a local model based on conditional
random fields and a global model based on log-
linear models. Then the final model was con-
structed by multiplying these two models, which
can be seen as unnormalized log-linear interpola-
tion (Klakow, 1998) of the two models which are
weighted equally. In their method, interactions be-
tween labels in the whole document were consid-
ered, and they used Gibbs sampling and simulated
annealing for decoding. Our model is largely sim-
ilar to their model. However, in their method, pa-
rameters of the global model were estimated using
relative frequencies of labels or were selected by
hand, while in our method, global model parame-
ters are estimated from training data so as to fit to
the data according to the objective function.
One approach for incorporating global infor-
mation in natural language processing is to uti-
lize consistency of labels, and such an approach
have been used in other tasks. Takamura et al
(2005) proposed a method based on the spin mod-
els in physics for extracting semantic orientations
of words. In the spin models, each electron has
one of two states, up or down, and the models give
probability distribution of the states. The states
of electrons interact with each other and neighbor-
ing electrons tend to have the same spin. In their
711
method, semantic orientations (positive or nega-
tive) of words are regarded as states of spins, in
order to model the property that the semantic ori-
entation of a word tends to have the same orienta-
tion as words in its gloss. The mean field approxi-
mation was used for inference in their method.
Yarowsky (1995) studied a method for word
sense disambiguation using unlabeled data. Al-
though no probabilistic models were considered
explicitly in the method, they used the property of
label consistency named ?one sense per discourse?
for unsupervised learning together with local in-
formation named ?one sense per collocation?.
There exist other approaches using global in-
formation which do not necessarily aim to use
label consistency. Rosenfeld et al (2001) pro-
posed whole-sentence exponential language mod-
els. The method calculates the probability of a
sentence s as follows:
P (s)= 1Z p0(s) exp
{?
i
?ifi(s)
}
,
where p0(s) is an initial distribution of s and any
language models such as trigram models can be
used for this. fi(s) is a feature function and can
handle sentence-wide features. Note that if we re-
gard fi,j(t) in our model (Equation (7)) as a fea-
ture function, Equation (8) is essentially the same
form as the above model. Their models can incor-
porate any sentence-wide features including syn-
tactic features obtained by shallow parsers. They
attempted to use Gibbs sampling and other sam-
pling methods for inference, and model parame-
ters were estimated from training data using the
generalized iterative scaling algorithm with the
sampling methods. Although they addressed mod-
eling of whole sentences, the method can be di-
rectly applied to modeling of whole documents
which allows us to incorporate unlabeled data eas-
ily as we have discussed. This approach, modeling
whole wide-scope contexts with log-linear models
and using sampling methods for inference, gives
us an expressive framework and will be applied to
other tasks.
5 Conclusion
In this paper, we presented a method for guessing
parts-of-speech of unknown words using global
information as well as local information. The
method models a whole document by consider-
ing interactions between POS tags of unknown
words with the same lexical form. Parameters of
the model are estimated from training data using
Gibbs sampling. Experimental results showed that
the method improves accuracies of POS guess-
ing of unknown words especially for Chinese and
Japanese. We also applied the method to semi-
supervised learning, but the results were not con-
sistent and there is some room for improvement.
Acknowledgements
This work was supported by a grant from the Na-
tional Institute of Information and Communica-
tions Technology of Japan.
References
Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and
Michael I. Jordan. 2003. An introduction to MCMC for Machine
Learning. Machine Learning, 50:5?43.
Masayuki Asahara. 2003. Corpus-based Japanese morphological
analysis. Nara Institute of Science and Technology, Doctor?s
Thesis.
Harald Baayen and Richard Sproat. 1996. Estimating Lexical Priors
for Low-Frequency Morphologically Ambiguous Forms. Com-
putational Linguistics, 22(2):155?166.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra.
1996. A Maximum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):39?71.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian Prior
for Smoothing Maximum Entropy Models. Technical Report
CMUCS-99-108, Carnegie Mellon University.
Chao-jan Chen, Ming-hong Bai, and Keh-Jiann Chen. 1997. Cate-
gory Guessing for Chinese Unknown Words. In Proceedings of
NLPRS ?97, pages 35?40.
Hai Leong Chieu and Hwee Tou Ng. 2002. Named Entity Recogni-
tion: A Maximum Entropy Approach Using Global Information.
In Proceedings of COLING 2002, pages 190?196.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceedings of ACL
2005, pages 363?370.
D. Klakow. 1998. Log-linear interpolation of language models. In
Proceedings of ICSLP ?98, pages 1695?1699.
Dong C. Liu and Jorge Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathematical Pro-
gramming, 45(3):503?528.
David J. C. MacKay. 2003. Information Theory, Inference, and
Learning Algorithms. Cambridge University Press.
Jose. L. Marroquin. 1985. Optimal Bayesian Estimators for Image
Segmentation and Surface Reconstruction. A.I. Memo 839, MIT.
Andrei Mikheev. 1997. Automatic Rule Induction for Unknown-
Word Guessing. Computational Linguistics, 23(3):405?423.
Shinsuke Mori and Makoto Nagao. 1996. Word Extraction from
Corpora and Its Part-of-Speech Estimation Using Distributional
Analysis. In Proceedings of COLING ?96, pages 1119?1122.
Masaki Nagata. 1999. A Part of Speech Estimation Method for
Japanese Unknown Words using a Statistical Model of Morphol-
ogy and Context. In Proceedings of ACL ?99, pages 277?284.
Giorgos S. Orphanos and Dimitris N. Christodoulakis. 1999. POS
Disambiguation and Unknown Word Guessing with Decision
Trees. In Proceedings of EACL ?99, pages 134?141.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-
Speech Tagging. In Proceedings of EMNLP ?96, pages 133?142.
Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001.
Whole-Sentence Exponential Language Models: A Vehicle For
Linguistic-Statistical Integration. Computers Speech and Lan-
guage, 15(1):55?73.
Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Ex-
tracting Semantic Orientations of Words using Spin Model. In
Proceedings of ACL 2005, pages 133?140.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 2001. The
Unknown Word Problem: a Morphological Analysis of Japanese
Using Maximum Entropy Aided by a Dictionary. In Proceedings
of EMNLP 2001, pages 91?99.
Shaojun Wang, Shaomin Wang, Russel Greiner, Dale Schuurmans,
and Li Cheng. 2005. Exploiting Syntactic, Semantic and Lexical
Regularities in Language Modeling via Directed Markov Random
Fields. In Proceedings of ICML 2005, pages 948?955.
David Yarowsky. 1995. Unsupervised Word Sense Disambiguation
Rivaling Supervised Methods. In Proceedings of ACL ?95, pages
189?196.
712
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 217?220,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Hybrid Approach to Word Segmentation and POS Tagging
Tetsuji Nakagawa
Oki Electric Industry Co., Ltd.
2?5?7 Honmachi, Chuo-ku
Osaka 541?0053, Japan
nakagawa378@oki.com
Kiyotaka Uchimoto
National Institute of Information and
Communications Technology
3?5 Hikaridai, Seika-cho, Soraku-gun
Kyoto 619?0289, Japan
uchimoto@nict.go.jp
Abstract
In this paper, we present a hybrid method for
word segmentation and POS tagging. The
target languages are those in which word
boundaries are ambiguous, such as Chinese
and Japanese. In the method, word-based
and character-based processing is combined,
and word segmentation and POS tagging are
conducted simultaneously. Experimental re-
sults on multiple corpora show that the inte-
grated method has high accuracy.
1 Introduction
Part-of-speech (POS) tagging is an important task
in natural language processing, and is often neces-
sary for other processing such as syntactic parsing.
English POS tagging can be handled as a sequential
labeling problem, and has been extensively studied.
However, in Chinese and Japanese, words are not
separated by spaces, and word boundaries must be
identified before or during POS tagging. Therefore,
POS tagging cannot be conducted without word seg-
mentation, and how to combine these two processing
is an important issue. A large problem in word seg-
mentation and POS tagging is the existence of un-
known words. Unknown words are defined as words
that are not in the system?s word dictionary. It is dif-
ficult to determine the word boundaries and the POS
tags of unknown words, and unknown words often
cause errors in these processing.
In this paper, we study a hybrid method for Chi-
nese and Japanese word segmentation and POS tag-
ging, in which word-based and character-based pro-
cessing is combined, and word segmentation and
POS tagging are conducted simultaneously. In the
method, word-based processing is used to handle
known words, and character-based processing is
used to handle unknown words. Furthermore, infor-
mation of word boundaries and POS tags are used
at the same time with this method. The following
sections describe the hybrid method and results of
experiments on Chinese and Japanese corpora.
2 Hybrid Method for Word Segmentation
and POS Tagging
Many methods have been studied for Chinese and
Japanese word segmentation, which include word-
based methods and character-based methods. Nak-
agawa (2004) studied a method which combines a
word-based method and a character-based method.
Given an input sentence in the method, a lattice is
constructed first using a word dictionary, which con-
sists of word-level nodes for all the known words in
the sentence. These nodes have POS tags. Then,
character-level nodes for all the characters in the
sentence are added into the lattice (Figure 1). These
nodes have position-of-character (POC) tags which
indicate word-internal positions of the characters
(Xue, 2003). There are four POC tags, B, I , E
and S, each of which respectively indicates the be-
ginning of a word, the middle of a word, the end
of a word, and a single character word. In the
method, the word-level nodes are used to identify
known words, and the character-level nodes are used
to identify unknown words, because generally word-
level information is precise and appropriate for pro-
cessing known words, and character-level informa-
tion is robust and appropriate for processing un-
known words. Extended hidden Markov models are
used to choose the best path among all the possible
candidates in the lattice, and the correct path is indi-
cated by the thick lines in Figure 1. The POS tags
and the POC tags are treated equally in the method.
Thus, the word-level nodes and the character-level
nodes are processed uniformly, and known words
and unknown words are identified simultaneously.
In the method, POS tags of known words as well as
word boundaries are identified, but POS tags of un-
known words are not identified. Therefore, we ex-
tend the method in order to conduct unknown word
POS tagging too:
Hybrid Method
The method uses subdivided POC-tags in or-
der to identify not only the positions of charac-
ters but also the parts-of-speech of the compos-
ing words (Figure 2, A). In the method, POS
tagging of unknown words is conducted at the
same time as word segmentation and POS tag-
217
Figure 1: Word Segmentation and Known Word POS Tagging using Word and Character-based Processing
ging of known words, and information of parts-
of-speech of unknown words can be used for
word segmentation.
There are also two other methods capable of con-
ducting unknown word POS tagging (Ng and Low,
2004):
Word-based Post-Processing Method
This method receives results of word segmen-
tation and known word POS tagging, and pre-
dicts POS tags of unknown words using words
as units (Figure 2, B). This approach is the
same as the approach widely used in English
POS tagging. In the method, the process of
unknown word POS tagging is separated from
word segmentation and known word POS tag-
ging, and information of parts-of-speech of un-
known words cannot be used for word segmen-
tation. In later experiments, maximum entropy
models were used deterministically to predict
POS tags of unknown words. As features for
predicting the POS tag of an unknown word w,
we used the preceding and the succeeding two
words of w and their POS tags, the prefixes and
the suffixes of up to two characters of w, the
character types contained in w, and the length
of w.
Character-based Post-Processing Method
This method is similar to the word-based post-
processing method, but in this method, POS
tags of unknown words are predicted using
characters as units (Figure 2, C). In the method,
POS tags of unknown words are predicted us-
ing exactly the same probabilistic models as
the hybrid method, but word boundaries and
POS tags of known words are fixed in the post-
processing step.
Ng and Low (2004) studied Chinese word seg-
mentation and POS tagging. They compared sev-
eral approaches, and showed that character-based
approaches had higher accuracy than word-based
approaches, and that conducting word segmentation
and POS tagging all at once performed better than
conducting these processing separately. Our hy-
brid method is similar to their character-based all-at-
once approach. However, in their experiments, only
word-based and character-based methods were ex-
amined. In our experiments, the combined method
of word-based and character-based processing was
examined. Furthermore, although their experiments
were conducted with only Chinese data, we con-
ducted experiments with Chinese and Japanese data,
and confirmed that the hybrid method performed
well on the Japanese data as well as the Chinese
data.
3 Experiments
We used five word-segmented and POS-tagged cor-
pora; the Penn Chinese Treebank corpus 2.0 (CTB),
a part of the PFR corpus (PFR), the EDR cor-
pus (EDR), the Kyoto University corpus version
2 (KUC) and the RWCP corpus (RWC). The first
two were Chinese (C) corpora, and the rest were
Japanese (J) corpora, and they were split into train-
ing and test data. The dictionary distributed with
JUMAN version 3.61 (Kurohashi and Nagao, 1998)
was used as a word dictionary in the experiments
with the KUC corpus, and word dictionaries were
constructed from all the words in the training data in
the experiments with other corpora. Table 1 summa-
rizes statistical information of the corpora: the lan-
guage, the number of POS tags, the sizes of training
and test data, and the splitting methods of them1. We
used the following scoring measures to evaluate per-
formance of word segmentation and POS tagging:
R : Recall (The ratio of the number of correctly
segmented/POS-tagged words in system?s out-
put to the number of words in test data),
P : Precision (The ratio of the number of correctly
segmented/POS-tagged words in system?s out-
put to the number of words in system?s output),
1The unknown word rate for word segmentation is not equal
to the unknown word rate for POS tagging in general, since
the word forms of some words in the test data may exist in the
word dictionary but the POS tags of them may not exist. Such
words are regarded as known words in word segmentation, but
as unknown words in POS tagging.
218
Figure 2: Three Methods for Word Segmentation and POS Tagging
F : F-measure (F = 2?R? P/(R+ P )),
Runknown : Recall for unknown words,
Rknown : Recall for known words.
Table 2 shows the results2. In the table, Word-
based Post-Proc., Char.-based Post-Proc. and Hy-
brid Method respectively indicate results obtained
with the word-based post-processing method, the
character-based post-processing method, and the hy-
brid method. Two types of performance were mea-
sured: performance of word segmentation alone,
and performance of both word segmentation and
POS tagging. We first compare performance of
both word segmentation and POS tagging. The
F-measures of the hybrid method were highest on
all the corpora. This result agrees with the ob-
servation by Ng and Low (2004) that higher accu-
racy was obtained by conducting word segmenta-
tion and POS tagging at the same time than by con-
ducting these processing separately. Comparing the
word-based and the character-based post-processing
methods, the F-measures of the latter were higher
on the Chinese corpora as reported by Ng and
Low (2004), but the F-measures of the former were
slightly higher on the Japanese corpora. The same
tendency existed in the recalls for known words;
the recalls of the character-based post-processing
method were highest on the Chinese corpora, but
2The recalls for known words of the word-based and the
character-based post-processing methods differ, though the
POS tags of known words are identified in the first common
step. This is because known words are sometimes identified as
unknown words in the first step and their POS tags are predicted
in the post-processing step.
those of the word-based method were highest on
the Japanese corpora, except on the EDR corpus.
Thus, the character-based method was not always
better than the word-based method as reported by Ng
and Low (2004) when the methods were used with
the word and character-based combined approach on
Japanese corpora. We next compare performance of
word segmentation alone. The F-measures of the hy-
brid method were again highest in all the corpora,
and the performance of word segmentation was im-
proved by the integrated processing of word seg-
mentation and POS tagging. The precisions of the
hybrid method were highest with statistical signifi-
cance on four of the five corpora. In all the corpora,
the recalls for unknown words of the hybrid method
were highest, but the recalls for known words were
lowest.
Comparing our results with previous work is not
easy since experimental settings are not the same.
It was reported that the original combined method
of word-based and character-based processing had
high overall accuracy (F-measures) in Chinese word
segmentation, compared with the state-of-the-art
methods (Nakagawa, 2004). Kudo et al (2004) stud-
ied Japanese word segmentation and POS tagging
using conditional random fields (CRFs) and rule-
based unknown word processing. They conducted
experiments with the KUC corpus, and achieved F-
measure of 0.9896 in word segmentation, which is
better than ours (0.9847). Some features we did
not used, such as base forms and conjugated forms
of words, and hierarchical POS tags, were used in
219
Corpus Number Number of Words (Unknown Word Rate for Segmentation/Tagging)
(Lang.) of POS [partition in the corpus]
Tags Training Test
CTB 34 84,937 7,980 (0.0764 / 0.0939)
(C) [sec. 1?270] [sec. 271?300]
PFR 41 304,125 370,627 (0.0667 / 0.0749)
(C) [Jan. 1?Jan. 9] [Jan. 10?Jan. 19]
EDR 15 2,550,532 1,280,057 (0.0176 / 0.0189)
(J) [id = 4n+ 0, id = 4n+ 1] [id = 4n+ 2]
KUC 40 198,514 31,302 (0.0440 / 0.0517)
(J) [Jan. 1?Jan. 8] [Jan. 9]
RWC 66 487,333 190,571 (0.0513 / 0.0587)
(J) [1?10,000th sentences] [10,001?14,000th sentences]
Table 1: Statistical Information of Corpora
Corpus Scoring Word Segmentation Word Segmentation & POS Tagging
(Lang.) Measure Word-based Char.-based Hybrid Word-based Char.-based Hybrid
Post-Proc. Post-Proc. Method Post-Proc. Post-Proc. Method
R 0.9625 0.9625 0.9639 0.8922 0.8935 0.8944
CTB P 0.9408 0.9408 0.9519* 0.8721 0.8733 0.8832
(C) F 0.9516 0.9516 0.9578 0.8821 0.8833 0.8887
Runknown 0.6492 0.6492 0.7148 0.4219 0.4312 0.4713
Rknown 0.9885 0.9885 0.9845 0.9409 0.9414 0.9382
R 0.9503 0.9503 0.9516 0.8967 0.8997 0.9024*
PFR P 0.9419 0.9419 0.9485* 0.8888 0.8917 0.8996*
(C) F 0.9461 0.9461 0.9500 0.8928 0.8957 0.9010
Runknown 0.6063 0.6063 0.6674 0.3845 0.3980 0.4487
Rknown 0.9749 0.9749 0.9719 0.9382 0.9403 0.9392
R 0.9525 0.9525 0.9525 0.9358 0.9356 0.9357
EDR P 0.9505 0.9505 0.9513* 0.9337 0.9335 0.9346
(J) F 0.9515 0.9515 0.9519 0.9347 0.9345 0.9351
Runknown 0.4454 0.4454 0.4630 0.4186 0.4103 0.4296
Rknown 0.9616 0.9616 0.9612 0.9457 0.9457 0.9454
R 0.9857 0.9857 0.9850 0.9572 0.9567 0.9574
KUC P 0.9835 0.9835 0.9843 0.9551 0.9546 0.9566
(J) F 0.9846 0.9846 0.9847 0.9562 0.9557 0.9570
Runknown 0.9237 0.9237 0.9302 0.6724 0.6774 0.6879
Rknown 0.9885 0.9885 0.9876 0.9727 0.9719 0.9721
R 0.9574 0.9574 0.9592 0.9225 0.9220 0.9255*
RWC P 0.9533 0.9533 0.9577* 0.9186 0.9181 0.9241*
(J) F 0.9553 0.9553 0.9585 0.9205 0.9201 0.9248
Runknown 0.6650 0.6650 0.7214 0.4941 0.4875 0.5467
Rknown 0.9732 0.9732 0.9720 0.9492 0.9491 0.9491
(Statistical significance tests were performed for R and P , and * indicates significance at p < 0.05)
Table 2: Performance of Word Segmentation and POS Tagging
their study, and it may be a reason of the differ-
ence. Although, in our experiments, extended hid-
den Markov models were used to find the best so-
lution, the performance will be further improved by
using CRFs instead, which can easily incorporate a
wide variety of features.
4 Conclusion
In this paper, we studied a hybrid method in which
word-based and character-based processing is com-
bined, and word segmentation and POS tagging are
conducted simultaneously. We compared its perfor-
mance of word segmentation and POS tagging with
other methods in which POS tagging is conducted
as a separated post-processing. Experimental results
on multiple corpora showed that the hybrid method
had high accuracy in Chinese and Japanese.
References
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Proceedings of
EMNLP 2004, pages 230?237.
Sadao Kurohashi and Makoto Nagao. 1998. Japanese
Morphological Analysis System JUMAN version 3.61.
Department of Informatics, Kyoto University. (in
Japanese).
Tetsuji Nakagawa. 2004. Chinese and Japanese Word
Segmentation Using Word-Level and Character-Level
Information. In Proceedings of COLING 2004, pages
466?472.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once?
Word-Based or Character-Based? In Proceedings of
EMNLP 2004, pages 277?284.
Nianwen Xue. 2003. Chinese Word Segmentation as
Character Tagging. International Journal of Compu-
tational Linguistics and Chinese, 8(1):29?48.
220
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 786?794,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Dependency Tree-based Sentiment Classification using CRFs with Hidden
Variables
Tetsuji Nakagawa?, Kentaro Inui?? and Sadao Kurohashi??
?National Institute of Information and Communications Technology
?Tohoku University
?Kyoto University
tnaka@nict.go.jp, inui@ecei.tohoku.ac.jp, kuro@i.kyoto-u.ac.jp
Abstract
In this paper, we present a dependency tree-
based method for sentiment classification of
Japanese and English subjective sentences us-
ing conditional random fields with hidden
variables. Subjective sentences often con-
tain words which reverse the sentiment po-
larities of other words. Therefore, interac-
tions between words need to be considered
in sentiment classification, which is difficult
to be handled with simple bag-of-words ap-
proaches, and the syntactic dependency struc-
tures of subjective sentences are exploited in
our method. In the method, the sentiment po-
larity of each dependency subtree in a sen-
tence, which is not observable in training data,
is represented by a hidden variable. The po-
larity of the whole sentence is calculated in
consideration of interactions between the hid-
den variables. Sum-product belief propaga-
tion is used for inference. Experimental re-
sults of sentiment classification for Japanese
and English subjective sentences showed that
the method performs better than other meth-
ods based on bag-of-features.
1 Introduction
Sentiment classification is a useful technique for an-
alyzing subjective information in a large number of
texts, and many studies have been conducted (Pang
and Lee, 2008). A typical approach for sentiment
classification is to use supervised machine learning
algorithms with bag-of-words as features (Pang et
al., 2002), which is widely used in topic-based text
classification. In the approach, a subjective sen-
tence is represented as a set of words in the sen-
tence, ignoring word order and head-modifier rela-
tion between words. However, sentiment classifi-
cation is different from traditional topic-based text
classification. Topic-based text classification is gen-
erally a linearly separable problem ((Chakrabarti,
2002), p.168). For example, when a document con-
tains some domain-specific words, the document
will probably belong to the domain. However, in
sentiment classification, sentiment polarities can be
reversed. For example, let us consider the sentence
?The medicine kills cancer cells.? While the phrase
cancer cells has negative polarity, the word kills re-
verses the polarity, and the whole sentence has pos-
itive polarity. Thus, in sentiment classification, a
sentence which contains positive (or negative) polar-
ity words does not necessarily have the same polar-
ity as a whole, and we need to consider interactions
between words instead of handling words indepen-
dently.
Recently, several methods have been proposed to
cope with the problem (Zaenen, 2004; Ikeda et al,
2008). However, these methods are based on flat
bag-of-features representation, and do not consider
syntactic structures which seem essential to infer
the polarity of a whole sentence. Other methods
have been proposed which utilize composition of
sentences (Moilanen and Pulman, 2007; Choi and
Cardie, 2008; Jia et al, 2009), but these methods
use rules to handle polarity reversal, and whether po-
larity reversal occurs or not cannot be learned from
labeled data. Statistical machine learning can learn
useful information from training data and generally
robust for noisy data, and using it instead of rigid
rules seems useful. Wilson et al (2005) proposed
a method for sentiment classification which utilizes
head-modifier relation and machine learning. How-
ever, the method is based on bag-of-features and po-
larity reversal occurred by content words is not han-
dled. One issue of the approach to use sentence
composition and machine learning is that only the
whole sentence is labeled with its polarity in gen-
eral corpora for sentiment classification, and each
component of the sentence is not labeled, though
such information is necessary for supervised ma-
786
Whole Dependency Tree
Polarities of Dependency Subtrees
It cancer and heart disease.
prevents
cancer and heart disease.
prevents
cancer and heart disease.
+? ?
Figure 1: Polarities of Dependency Subtrees
chine learning to infer the sentence polarity from its
components.
In this paper, we propose a dependency tree-based
method for Japanese and English sentiment classifi-
cation using conditional random fields (CRFs) with
hidden variables. In the method, the sentiment po-
larity of each dependency subtree, which is not ob-
servable in training data, is represented by a hidden
variable. The polarity of the whole sentence is cal-
culated in consideration of interactions between the
hidden variables.
The rest of this paper is organized as follows: Sec-
tion 2 describes a dependency tree-based method
for sentiment classification using CRFs with hid-
den variables, and Section 3 shows experimental re-
sults on Japanese and English corpora. Section 4
discusses related work, and Section 5 gives conclu-
sions.
2 Dependency Tree-based Sentiment
Classification using CRFs with Hidden
Variables
In this study, we handle a task to classify the polar-
ities (positive or negative) of given subjective sen-
tences. In the rest of this section, we describe a prob-
abilistic model for sentiment classification based on
dependency trees, methods for inference and param-
eter estimation, and features we use.
2.1 A Probabilistic Model based on
Dependency Trees
Let us consider the subjective sentence ?It prevents
cancer and heart disease.? In the sentence, cancer
and heart disease have themselves negative polari-
It cancer and heart disease.prevents
s0+
<root>
s10 s2+ s3? s4?
Figure 2: Probabilistic Model based on Dependency Tree
s0 s1 s2 s3 s4
g1 g2 g3 g4
g5g6 g7 g8
Figure 3: Factor Graph
ties. However, the polarities are reversed by modi-
fying the word prevents, and the dependency subtree
?prevents cancer and heart disease? has positive po-
larity. As a result, the whole dependency tree ?It
prevents cancer and heart disease.? has positive po-
larity (Figure 1). In such a way, we can consider
the sentiment polarity for each dependency subtree
of a subjective sentence. Note that we use phrases as
a basic unit instead of words in this study, because
phrases are useful as a meaningful unit for sentiment
classification1. In this paper, a dependency subtree
means the subtree of a dependency tree whose root
node is one of the phrases in the sentence.
We use a probabilistic model as shown in Fig-
ure 2. We consider that each phrase in the subjective
sentence has a random variable (indicated by a cir-
cle in Figure 2). The random variable represents the
polarity of the dependency subtree whose root node
is the corresponding phrase. Two random variables
are dependent (indicated by an edge in Figure 2) if
their corresponding phrases have head-modifier re-
lation in the dependency tree. The node denoted as
<root> in Figure 2 indicates a virtual phrase which
represents the root node of the sentence, and we re-
gard that the random variable of the root node is the
polarity of the whole sentence. In usual annotated
corpora for sentiment classification, only each sen-
tence is labeled with its polarity, and each phrase
(dependency subtree) is not labeled, so all the ran-
dom variables except the one for the root node are
1From an empirical view, in our preliminary experiments
with the proposed method, phrase-based processing performed
better than word-based processing in accuracy and in computa-
tional efficiency.
787
hidden variables that cannot be observed in labeled
data (indicated by gray circles in Figure 2). With
such a probabilistic model, it is possible to utilize
properties such that phrases which contain positive
(or negative) words tend to have positive (negative)
polarities, and two phrases with head-modifier rela-
tion tend to have opposite polarities if the head con-
tains a word which reverses sentiment polarity.
Next, we define the probabilistic model as shown
in Figure 2 in detail. Let n denote the number of
phrases in a subjective sentence, wi the i-th phrase,
and hi the head index of the i-th phrase. Let si de-
note the random variable which represents the po-
larity of the dependency subtree whose root is the
i-th phrase (si ? {+1,?1}), and let p denote the
polarity of the whole sentence (p ? {+1,?1}). We
regard the 0-th phrase as a virtual phrase which rep-
resents the root of the sentence. w,h, s respectively
denote the sequence of wi, hi, si.
w = w1 ? ? ?wn, h = h1 ? ? ?hn, s = s0 ? ? ? sn,
p = s0.
For the example sentence in Figure 1, w1 =It,
w2 =prevents, w3 =cancer, w4 =and heart dis-
ease., h1 = 2, h2 = 0, h3 = 2, h4 = 2. We define
the joint probability distribution of the sentiment po-
larities of dependency subtrees s, given a subjective
sentence w and its dependency tree h, using log-
linear models:
P?(s|w,h)=
1
Z?(w,h)
exp
{ K
?
k=1
?kFk(w,h, s)
}
,
(1)
Z?(w,h)=
?
s
exp
{ K
?
k=1
?kFk(w,h, s)
}
, (2)
Fk(w,h, s)=
n
?
i=1
fk(i,w,h, s), (3)
where ? = {?1, ? ? ? , ?K} is the set of parameters
of the model. fk(i,w,h, s) is the feature function
of the i-th phrase, and is classified to node feature
which considers only the corresponding node, or
edge feature which considers both the correspond-
ing node and its head, as follows:
fk(i,w,h, s)=
{ fnk (wi, si) (k ? Kn),
f ek(wi, si, whi , shi) (k ? Ke),
(4)
where Kn and Ke respectively represent the sets of
indices of node features and edge features.
2.2 Classification of Sentiment Polarity
Let us consider how to infer the sentiment polarity
p ? {+1,?1}, given a subjective sentence w and
its dependency tree h. The polarity of the root node
(s0) is regarded as the polarity of the whole sentence,
and p can be calculated as follows:
p=argmax
p?
P?(p?|w,h), (5)
P?(p|w,h)=
?
s:s0=p
P?(s|w,h). (6)
That is, the polarity of the subjective sentence is ob-
tained as the marginal probability of the root node
polarity, by summing the probabilities for all the
possible configurations of hidden variables. How-
ever, enumerating all the possible configurations of
hidden variables is computationally hard, and we use
sum-product belief propagation (MacKay, 2003) for
the calculation.
Belief propagation enables us to efficiently calcu-
late marginal probabilities. In this study, the graph-
ical model to be solved has a tree structure (identi-
cal to the syntactic dependency tree) which has no
loops, and an exact solution can be obtained us-
ing belief propagation. Dependencies among ran-
dom variables in Figure 2 are represented by a factor
graph in Figure 3. The factor graph consists of vari-
able nodes si indicated by circles, and factor (fea-
ture) nodes gi indicated by squares. In the exam-
ple in Figure 3, gi(1 ? i ? 4) correspond to the
node features in Equation (4), and gi(5 ? i ? 8)
correspond to the edge features. In belief propa-
gation, marginal distribution is calculated by pass-
ing messages (beliefs) among the variables and fac-
tors connected by edges in the factor graph (Refer
to (MacKay, 2003) for detailed description of belief
propagation).
2.3 Parameter Estimation
Let us consider how to estimate model parameters?,
given L training examples D = {?wl,hl, pl?}Ll=1.
In this study, we use the maximum a posteriori es-
timation with Gaussian priors for parameter estima-
tion. We define the following objective function L?,
788
and calculate the parameters ?? which maximize the
value:
L?=
L
?
l=1
logP?(pl|wl,hl) ?
1
2?2
K
?
k=1
?2k, (7)
??=argmax
?
L?, (8)
where ? is a parameter of Gaussian priors and is set
to 1.0 in later experiments. The partial derivatives of
L? are as follows:
?L?
??k
=
L
?
l=1
[
?
s
P?(s|wl,hl, pl)Fk(wl,hl, s)
?
?
s
P?(s|wl,hl)Fk(wl,hl, s)
]
? 1
?2
?k.
(9)
The model parameters can be calculated with the
L-BFGS quasi-Newton method (Liu and Nocedal,
1989) using the objective function and its partial
derivatives. While the partial derivatives contain
summation over all the possible configurations of
hidden variables, it can be calculated efficiently us-
ing belief propagation as explained in Section 2.2.
This parameter estimation method is same to one
used for Latent-Dynamic Conditional Random Field
(Morency et al, 2007). Note that the objective func-
tion L? is not convex, and there is no guarantee for
global optimality. The estimated model parameters
depend on the initial values of the parameters, and
the setting of the initial values of model parameters
will be explained in Section 2.4.
2.4 Features
Table 1 shows the features used in this study. Fea-
tures (a)?(h) in Table 1 are used as the node fea-
tures (Equation (4)) for the i-th phrase, and fea-
tures (A)?(E) are used as the edge features for the
i-th and j-th phrases (j=hi). In Table 1, si denotes
the hidden variable which represents the polarity of
the dependency subtree whose root node is the i-
th phrase, qi denotes the prior polarity of the i-th
phrase (explained later), ri denotes the polarity re-
versal of the i-th phrase (explained later), mi de-
notes the number of words in the i-th phrase, ui,k,
bi,k, ci,k, fi,k respectively denote the surface form,
base form, coarse-grained part-of-speech (POS) tag,
Node Features
a si
b si&qi
c si&qi&ri
d si&ui,1, ? ? ? , si&ui,mi
e si&ci,1, ? ? ? , si&ci,mi
f si&fi,1, ? ? ? , si&fi,mi
g si&ui,1&ui,2, ? ? ? , si&ui,mi?1&ui,mi
h si&bi,1&bi,2, ? ? ? , si&bi,mi?1&bi,mi
Edge Features
A si&sj
B si&sj&rj
C si&sj&rj&qj
D si&sj&bi,1, ? ? ? , si&sj&bi,mi
E si&sj&bj,1, ? ? ? , si&sj&bj,mj
Table 1: Features Used in This Study
fine-grained POS tag of the k-th word in the i-th
phrase.
We used the morphological analysis system JU-
MAN and the dependency parser KNP2 for pro-
cessing Japanese data, and the POS tagger MX-
POST (Ratnaparkhi, 1996) and the dependency
parser MaltParser3 for English data. KNP outputs
phrase-based dependency trees, but MaltParser out-
puts word-based dependency trees, and we con-
verted the word-based ones to phrase-based ones us-
ing simple heuristic rules explained in Appendix A.
The prior polarity of a phrase qi ? {+1, 0,?1} is
the innate sentiment polarity of a word contained in
the phrase, which can be obtained from sentiment
polarity dictionaries. We used sentiment polarity
dictionaries made by Kobayashi et al (2007) and Hi-
gashiyama et al (2008)4 for Japanese experiments
(The resulting dictionary contains 6,974 positive ex-
pressions and 8,428 negative expressions), and a dic-
tionary made by Wilson et al (2005)5 for English
experiments (The dictionary contains 2,289 positive
expressions and 4,143 negative expressions). When
a phrase contains the words registered in the dictio-
naries, its prior polarity is set to the registered po-
larity, otherwise the prior polarity is set to 0. When
a phrase contains multiple words in the dictionaries,
the registered polarity of the last (nearest to the end
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/
3http://maltparser.org/
4http://cl.naist.jp/?inui/research/EM/sentiment-lexicon.html
5http://www.cs.pitt.edu/mpqa/
789
of the sentence) word is used.
The polarity reversal of a phrase ri ? {0, 1} rep-
resents whether it reverses the polarities of other
phrases (1) of not (0). We prepared polarity revers-
ing word dictionaries, and the polarity reversal of
a phrase is set to 1 if the phrase contains a word
in the dictionaries, otherwise set to 0. We con-
structed polarity reversing word dictionaries which
contain such words as decrease and vanish that re-
verse sentiment polarity. A Japanese polarity revers-
ing word dictionary was constructed from an auto-
matically constructed corpus, and the construction
procedure is described in Appendix B (The dictio-
nary contains 219 polarity reversing words). An
English polarity reversing word dictionary was con-
structed from the General Inquirer dictionary6 in the
same way as Choi and Cardie (2008), by collecting
words which belong to either NOTLW or DECREAS
categories (The dictionary contains 121 polarity re-
versing words).
Choi and Cardie (2008) categorized polarity re-
versing words into two categories: function-word
negators such as not and content-word negators such
as eliminate. The polarity reversal of a phrase ri ex-
plained above handles only the content-word nega-
tors, and function-word negators are handled in an-
other way, since the scope of a function-word nega-
tor is generally limited to the phrase containing it in
Japanese, and the number of function-word negators
is small. The prior polarity qi and the polarity rever-
sal ri of a phrase are changed to the following q?i and
r?i, if the phrase contains a function-word negator (in
Japanese) or if the phrase is modified by a function-
word negator (in English):
q?i=?qi, (10)
r?i=1 ? ri. (11)
In this paper, unless otherwise noted, the word po-
larity reversal is used to indicate polarity reversing
caused by content-word negators, and function-word
negators are assumed to be applied to qi and ri in the
above way beforehand.
As described in Section 2.3, there is no guaran-
tee of global optimality for estimated parameters,
since the objective function is not convex. In our
6http://www.wjh.harvard.edu/ inquirer/
preliminary experiments, L-BFGS often did not con-
verge and classification accuracy was unstable when
the initial values of parameters were randomly set.
Therefore, in later experiments, we set the initial
values in the following way. For the feature (A) in
Table 1 in which si and sj are equal, we set the ini-
tial parameter ?i of the feature to a random number
in [0.9, 1.1], otherwise we set to a random number in
[?0.1, 0.1]7. By setting such initial values, the initial
model parameters have a property that two phrases
with head-modifier relation tend to have the same
polarity, which is intuitively reasonable.
3 Experiments
We conducted experiments of sentiment classifica-
tion on four Japanese corpora and four English cor-
pora.
3.1 Data
We used four corpora for experiments of Japanese
sentiment classification: the Automatically Con-
structed Polarity-tagged corpus (ACP) (Kaji and
Kitsuregawa, 2006), the Kyoto University and NTT
Blog corpus (KNB) 8, the NTCIR Japanese opinion
corpus (NTC-J) (Seki et al, 2007; Seki et al, 2008),
the 50 Topics Evaluative Information corpus (50
Topics) (Nakagawa et al, 2008). The ACP corpus
is an automatically constructed corpus from HTML
documents on the Web using lexico-syntactic pat-
terns and layout structures. The size of the corpus
is large (it consists of 650,951 instances), and we
used 1/100 of the whole corpus. The KNB corpus
consists of Japanese blogs, and is manually anno-
tated. The NTC-J corpus consists of Japanese news-
paper articles. There are two NTCIR Japanese opin-
ion corpora available, the NTCIR-6 corpus and the
NTCIR-7 corpus; and we combined the two cor-
pora. The 50 Topics corpus is collected from various
pages on the Web, and is manually annotated.
We used four corpora for experiments of English
sentiment classification: the Customer Review data
7The values of most learned parameters distributed between
-1.0 and 1.0 in our preliminary experiments. Therefore, we de-
cided to give values around the upper bound (1.0) and the mean
(0.0) to the features in order to incorporate minimal prior knowl-
edge into the model.
8http://nlp.kuee.kyoto-u.ac.jp/kuntt/
790
(CR)9, the MPQA Opinion corpus (MPQA)10, the
Movie Review Data (MR) 11, and the NTCIR En-
glish opinion corpus (NTC-E) (Seki et al, 2007;
Seki et al, 2008). The CR corpus consists of re-
view articles about products such as digital cameras
and cellular phones. There are two customer review
datasets, the 5 products dataset and the 9 products
dataset, and we combined the two datasets. In the
MPQA corpus, sentiment polarities are attached not
to sentences but expressions (sub-sentences), and we
regarded the expressions as sentences and classified
the polarities. There are two NTCIR English cor-
pora available, the NTCIR-6 corpus and the NTCIR-
7 corpus, and we combined the two corpora.
The statistical information of the corpora we used
is shown in Table 2. We randomly split each corpus
into 10 portions, and conducted 10-fold cross valida-
tion. Accuracy of sentiment classification was cal-
culated as the number of correctly predicted labels
(polarities) divided by the number of test examples.
3.2 Compared Methods
We compared our method to 6 baseline methods,
and this section describes them. In the following,
p0 ? {+1,?1} denotes the major polarity in train-
ing data, Hi denotes the set consisting of all the an-
cestor nodes of the i-th phrase in the dependency
tree, and sgn(x) is defined as below:
sgn(x)=
?
?
?
?
?
+1 (x > 0),
0 (x = 0),
?1 (x < 0).
Voting without Polarity Reversal The polarity of
a subjective sentence is decided by voting of
each phrase?s prior polarity. In the case of a
tie, the major polarity in the training data is
adopted.
p=sgn
( n
?
i=1
qi + 0.5p0
)
. (12)
Voting with Polarity Reversal Same to Voting
without Polarity Reversal, except that the po-
larities of phrases which have odd numbers of
9http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html
10http://www.cs.pitt.edu/mpqa/
11http://www.cs.cornell.edu/People/pabo/movie-review-
data/
reversal phrases in their ancestors are reversed
before voting.
p=sgn
( n
?
i=1
qi
?
j?Hi
(?1)rj + 0.5p0
)
. (13)
Rule The polarity of a subjective sentence is deter-
ministically decided basing on rules, by con-
sidering the sentiment polarities of dependency
subtrees. The polarity of the dependency sub-
tree whose root is the i-th phrase is decided by
voting the prior polarity of the i-th phrase and
the polarities of the dependency subtrees whose
root nodes are the modifiers of the i-th phrase.
The polarities of the modifiers are reversed if
their head phrase has a reversal word. The de-
cision rule is applied from leaf nodes in the de-
pendency tree, and the polarity of the root node
is decided at the last.
si=sgn
(
qi +
?
j:hj=i
sj(?1)ri
)
, (14)
p=sgn(s0 + 0.5p0). (15)
Bag-of-Features with No Dictionaries The polar-
ity of a subjective sentence is classified us-
ing Support Vector Machines. Surface forms,
base forms, coarse-grained POS tags and fine-
grained POS tags of word unigrams and bi-
grams in the subjective sentence are used as
features12. The second order polynomial ker-
nel is used and the cost parameter C is set to
1.0. No prior polarity information (dictionary)
is used.
Bag-of-Features without Polarity Reversal Same
to Bag-of-Features with No Dictionaries, ex-
cept that the voting result of prior polarities
(one of positive, negative or tie) is also used
as a feature.
Bag-of-Features with Polarity Reversal Same to
Bag-of-Features without Polarity Reversal, ex-
cept that the polarities of phrases which have
12In experiments on English corpora, only the features of un-
igrams are used and those of bigrams are not used, since the
bigram features decreased accuracies in our preliminary experi-
ments as reported in previous work (Andreevskaia and Bergler,
2008).
791
Language Corpus Number of Instances (Positive / Negative)
ACP 6,510 (2,738 / 3,772)
Japanese KNB 2,288 (1,423 / 865)
NTC-J 3,485 (1,083 / 2,402)
50 Topics 5,366 (3,175 / 2,191)
CR 3,772 (2,406 / 1,366)
English MPQA 10,624 (3,316 / 7,308)
MR 10,662 (5,331 / 5,331)
NTC-E 3,812 (1,226 / 2,586)
Table 2: Statistical Information of Corpora
Method Japanese English
ACP KNB NTC-J 50 Topics CR MPQA MR NTC-E
Voting-w/o Rev. 0.686 0.764 0.665 0.727 0.714 0.804 0.629 0.730
Voting-w/ Rev. 0.732 0.792 0.714 0.765 0.742 0.817 0.631 0.740
Rule 0.734 0.792 0.742 0.764 0.743 0.818 0.629 0.750
BoF-no Dic. 0.798 0.758 0.754 0.761 0.793 0.818 0.757 0.768
BoF-w/o Rev. 0.812 0.823 0.794 0.805 0.802 0.840 0.761 0.793
BoF-w/ Rev. 0.822 0.830 0.804 0.819 0.814 0.841 0.764 0.797
Tree-CRF 0.846* 0.847* 0.826* 0.841* 0.814 0.861* 0.773* 0.804
(* indicates statistical significance at p < 0.05)
Table 3: Accuracy of Sentiment Classification
odd numbers of reversal phrases in their ances-
tors are reversed before voting.
Tree-CRF The proposed method based on depen-
dency trees using CRFs, described in Section 2.
3.3 Experimental Results
The experimental results are shown in Table 3. The
proposed method Tree-CRF obtained the best ac-
curacies for all the four Japanese corpora and the
four English corpora, and the differences against
the second best methods were statistically signifi-
cant (p < 0.05) with the paired t-test for the six
of the eight corpora. Tree-CRF performed better
for the Japanese corpora than for the English cor-
pora. For both the Voting methods and the Bag-of-
Features methods, the methods with polarity rever-
sal performed better than those without it13.
Both BoF-w/ Rev. and Tree-CRF use supervised
machine learning and the same dictionaries (the
13The Japanese polarity reversing word dictionary was con-
structed from the ACP corpus as described in Appendix B, and
it is not reasonable to compare the methods with and without
polarity reversal on the ACP corpus. However, the tendency
can be seen on the other 7 corpora.
prior polarity dictionaries and the polarity revers-
ing word dictionaries), but the latter performed bet-
ter than the former. Our error analysis showed that
BoF-w/ Rev. was not robust for erroneous words in
the prior polarity dictionaries. BoF-w/ Rev. uses the
voting result of the prior polarities as a feature, and
the feature is sensitive to the errors in the dictionary,
while Tree-CRF uses several information as well as
the prior polarities to decide the polarities of depen-
dency subtrees, and was robust to the dictionary er-
rors. We investigated the trained model parameters
of Tree-CRF, and found that the features (E) in Ta-
ble 1, in which the head and the modifier have op-
posite polarities and the head word is such as pro-
tect and withdraw, have large positive weights. Al-
though these words were not included in the polar-
ity reversing word dictionary, the property that these
words reverse polarities of other words seems to be
learned with the model.
4 Related Work
Various studies on sentiment classification have
been conducted, and there are several methods pro-
792
posed for handling reversal of polarities. In this pa-
per, our method was not directly compared with the
other methods, since it is difficult to completely im-
plement them or conduct experiments with exactly
the same settings.
Choi and Cardie (2008) proposed a method to
classify the sentiment polarity of a sentence bas-
ing on compositional semantics. In their method,
the polarity of the whole sentence is determined
from the prior polarities of the composing words by
pre-defined rules, and the method differs from ours
which uses the probabilistic model to handle interac-
tions between hidden variables. Syntactic structures
were used in the studies of Moilanen and Pulman
(2007) and, Jia et al (2009), but their methods are
based on rules and supervised learning was not used
to handle polarity reversal. As discussed in Sec-
tion 1, Wilson et al (2005) studied a bag-of-features
based statistical sentiment classification method in-
corporating head-modifier relation.
Ikeda et al (2008) proposed a machine learning
approach to handle sentiment polarity reversal. For
each word with prior polarity, whether the polarity is
reversed or not is learned with a statistical learning
algorithm using its surrounding words as features.
The method can handle only words with prior polar-
ities, and does not use syntactic dependency struc-
tures.
Conditional random fields with hidden variables
have been studied so far for other tasks. Latent-
Dynamic Conditional Random Fields (LDCRF)
(Morency et al, 2007; Sun et al, 2008) are prob-
abilistic models with hidden variables for sequen-
tial labeling, and belief propagation is used for in-
ference. Out method is similar to the models, but
there are several differences. In our method, only
one variable which represents the polarity of the
whole sentence is observable, and dependency re-
lation among random variables is not a linear chain
but a tree structure which is identical to the syntactic
dependency.
5 Conclusion
In this paper, we presented a dependency tree-based
method for sentiment classification using condi-
tional random fields with hidden variables. In this
method, the polarity of each dependency subtree
of a subjective sentence is represented by a hid-
den variable. The values of the hidden variables
are calculated in consideration of interactions be-
tween variables whose nodes have head-modifier re-
lation in the dependency tree. The value of the
hidden variable of the root node is identified with
the polarity of the whole sentence. Experimental
results showed that the proposed method performs
better for Japanese and English data than the base-
line methods which represents subjective sentences
as bag-of-features.
Appendix
A Rules for Converting Word Sequence to
Phrase Sequence
Let v1, ? ? ? , vN denote an English word sequence, yi
the part-of-speech of the i-th word, and zi the head
index of the i-th word. The word sequence was con-
verted to a phrase sequence as follows, by applying
rules which combine two adjacent words:
LT ? {?,(,-LRB-,-LSB-,-LCB-,CC}
RT ? {?,),,,--,.,:,POS,-RRB-,-RSB-,-RCB-}
PP ? {IN,RP,TO,DT,PDT,PRP,WDT,WP,WP$,WRB}
NN ? {CD,FW,NN,NNP,NNPS,NNS,SYM,JJ}
do
for i := 1 to N ? 1
if xi and xi+1 are not yet combined ?
(xi ? LT ?
xi+1 ? RT ?
((yi = yi+1 ? yi = i+ 1 ? yi+1 = i) ?
(xi ? PP ?
(xi ? NN ? xi+1 ? NN )))) then
Combine the words vi and vi+1
until No rules are applied
B Construction of Japanese Polarity
Reversing Word Dictionary
We constructed a Japanese polarity reversing word
dictionary from the Automatically Constructed
Polarity-tagged corpus (Kaji and Kitsuregawa,
2006). First, we collected sentences, each of which
contains just one phrase having prior polarity, and
the phrase modifies a phrase which modifies the root
node. Among them, we selected sentences in which
the prior polarity is not equal to the polarity of the
whole sentence. We extracted all the words in the
head phrase, and manually checked them whether
they should be put into the dictionary or not. The ra-
tionale behind the procedure is that the prior polarity
can be considered to be reversed by a certain word
in the head phrase.
793
References
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Overcom-
ing Domain Dependence in Sentiment Tagging. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 290?298.
Soumen Chakrabarti. 2002. Mining the Web: Dis-
covering Knowledge from Hypertext Data. Morgan-
Kauffman.
Yejin Choi and Claire Cardie. 2008. Learning with
Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 793?801.
Masahiko Higashiyama, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Acquiring Noun Polarity Knowledge
Using Selectional Preferences. In Proceedings of the
14th Annual Meeting of the Association for Natural
Language Processing, pages 584?587. (in Japanese).
Daisuke Ikeda, Hiroya Takamura, Lev-Arie Ratinov, and
Manabu Okumura. 2008. Learning to Shift the Po-
larity of Words for Sentiment Classification. In Pro-
ceedings of the 3rd International Joint Conference on
Natural Language Processing, pages 296?303.
Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The Ef-
fect of Negation on Sentiment Analysis and Retrieval
Effectiveness. In Proceeding of the 18th ACM Con-
ference on Information and Knowledge Management,
pages 1827?1830.
Nobuhiro Kaji and Masaru Kitsuregawa. 2006. Auto-
matic Construction of Polarity-Tagged Corpus from
HTML Documents. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 452?459.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Opinion Mining from Web Documents: Extrac-
tion and Structurization. Journal of the Japanese So-
ciety for Artificial Intelligence, 22(2):227?238.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503?528.
David J. C. MacKay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of the Recent Advances
in Natural Language Processing International Confer-
ence, pages 378?382.
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-Dynamic Discriminative Mod-
els for Continuous Gesture Recognition. In Proceed-
ings of the 2007 IEEE Conference on Computer Vision
and Pattern Recognition, pages 1?8.
Tetsuji Nakagawa, Takuya Kawada, Kentaro Inui, and
Sadao Kurohashi. 2008. Extracting Subjective and
Objective Evaluative Expressions from the Web. In
Proceedings of the 2nd International Symposium on
Universal Communication.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, pages 79?86.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-of-Speech Tagging. In Proceedings of the
1996 Conference on Empirical Methods in Natural
Language Processing Conference, pages 133?142.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-His
Chen, Noriko Kando, and Chin-Yew Lin. 2007.
Overview of Opinion Analysis Pilot Task at NTCIR-
6. In Proceedings of the 6th NTCIR Workshop, pages
265?278.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, and Noriko Kando. 2008. Overview
ofMultilingual Opinion Analysis Task at NTCIR-7. In
Proceedings of the 7th NTCIR Workshop.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun?ichi Tsujii. 2008. Modeling Latent-Dynamic
in Shallow Parsing: A Latent Conditional Model with
Improved Inference. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 841?848.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of the 2005
Joint Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 347?354.
Livia Polanyi Annie Zaenen. 2004. Contextual Lexical
Valence Shifters. In Proceedings of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text.
794
