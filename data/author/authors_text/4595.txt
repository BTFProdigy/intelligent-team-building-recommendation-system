Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 233?240, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Effective Use of Prosody in Parsing Conversational Speech
Jeremy G. Kahn? Matthew Lease?
Eugene Charniak? Mark Johnson? Mari Ostendorf?
University of Washington, SSLI? Brown University?
{jgk,mo}@ssli.ee.washington.edu {mlease,ec,mj}@cs.brown.edu
Abstract
We identify a set of prosodic cues for parsing con-
versational speech and show how such features can
be effectively incorporated into a statistical parsing
model. On the Switchboard corpus of conversa-
tional speech, the system achieves improved parse
accuracy over a state-of-the-art system which uses
only lexical and syntactic features. Since removal
of edit regions is known to improve downstream
parse accuracy, we explore alternatives for edit de-
tection and show that PCFGs are not competitive
with more specialized techniques.
1 Introduction
For more than a decade, the Penn Treebank?s Wall
Street Journal corpus has served as a benchmark for
developing and evaluating statistical parsing tech-
niques (Collins, 2000; Charniak and Johnson, 2005).
While this common benchmark has served as a valu-
able shared task for focusing community effort, it
has unfortunately led to the relative neglect of other
genres, particularly speech. Parsed speech stands to
benefit from practically every application envisioned
for parsed text, including machine translation, infor-
mation extraction, and language modeling. In con-
trast to text, however, speech (in particular, conver-
sational speech) presents a distinct set of opportu-
nities and challenges. While new obstacles arise
from the presence of speech repairs, the possibility
of word errors, and the absence of punctuation and
sentence boundaries, speech also presents a tremen-
dous opportunity to leverage multi-modal input, in
the form of acoustic or even visual cues.
As a step in this direction, this paper identifies a
set of useful prosodic features and describes how
they can be effectively incorporated into a statisti-
cal parsing model, ignoring for now the problem
of word errors. Evaluated on the Switchboard cor-
pus of conversational telephone speech (Graff and
Bird, 2000), our prosody-aware parser out-performs
a state-of-the-art system that uses lexical and syntac-
tic features only. While we are not the first to employ
prosodic cues in a statistical parsing model, previous
efforts (Gregory et al, 2004; Kahn et al, 2004) in-
corporated these features as word tokens and thereby
suffered from the side-effect of displacing words in
the n-gram models by the parser. To avoid this prob-
lem, we generate a set of candidate parses using an
off-the-shelf, k-best parser, and use prosodic (and
other) features to rescore the candidate parses.
Our system architecture combines earlier models
proposed for parse reranking (Collins, 2000) and
filtering out edit regions (Charniak and Johnson,
2001). Detecting and removing edits prior to parsing
is motivated by the claim that probabilistic context-
free grammars (PCFGs) perform poorly at detect-
ing edit regions. We validate this claim empirically:
two state-of-the-art PCFGs (Bikel, 2004; Charniak
and Johnson, 2005) are both shown to perform sig-
nificantly below a state-of-the-art edit detection sys-
tem (Johnson et al, 2004).
2 Previous Work
As mentioned earlier, conversational speech
presents a different set of challenges and opportu-
nities than encountered in parsing text. This paper
focuses on the challenges associated with disfluen-
cies (Sec. 2.1) and the opportunity of leveraging
acoustic-prosodic cues at the sub-sentence level
(Sec. 2.2). Here, sentence segmentation is assumed
to be known (though punctuation is not available);
233
. . . while I think,
? ?? ?
Reparandum
+ uh, I mean,
? ?? ?
Editing phrase
I know
? ?? ?
Repair
that. . .
Figure 1: The structure of a typical repair, with ?+? indicating the interruption point.
the impact of automatic segmentation is addressed
in other work (Kahn et al, 2004).
2.1 Speech Repairs and Parsing
Spontaneous speech abounds with disfluencies such
as partial words, filled pauses (e.g., ?uh?, ?um?),
conversational fillers (e.g., ?you know?), and par-
enthetical asides. One type of disfluency that has
proven particularly problematic for parsing is speech
repairs: when a speaker amends what he is saying
mid-sentence (see Figure 1). Following the analy-
sis of (Shriberg, 1994), a speech repair can be un-
derstood as consisting of three parts: the reparan-
dum (the material repaired), the editing phrase (that
is typically either empty or consists of a filler), and
the repair. The point between the reparandum and
the editing phrase is referred to as the interruption
point (IP), and it is the point that may be acousti-
cally marked. We refer to the reparandum and edit-
ing phrase together as an edit or edit region. Speech
repairs are difficult to model with HMM or PCFG
models, because these models can induce only linear
or tree-structured dependencies between words. The
relationship between reparandum and repair is quite
different: the repair is often a ?rough copy? of the
reparandum, using the same or very similar words
in roughly the same order. A language model char-
acterizing this dependency with hidden stack opera-
tions is proposed in (Heeman and Allen, 1999).
Several parsing models have been proposed which
accord special treatment to speech repairs. Most
prior work has focused on handling disfluencies
and continued to rely on hand-annotated transcripts
that include punctuation, case, and known sentence
boundaries (Hindle, 1983; Core and Schubert, 1999;
Charniak and Johnson, 2001; Engel et al, 2002).
Of particular mention is the analysis of the rela-
tionship between speech repairs and parsing accu-
racy presented by Charniak and Johnson (2001), as
this directly influenced our work. They presented
evidence that improved edit detection (i.e. detect-
ing the reparandum and edit phrase) leads to better
parsing accuracy, showing a relative reduction in F -
score error of 14% (2% absolute) between oracle and
automatic edit removal. Thus, this work adopts their
edit detection preprocessing approach. They have
subsequently presented an improved model for de-
tecting edits (Johnson et al, 2004), and our results
here complement their analysis of the edit detection
and parsing relationship, particularly with respect to
the limitations of PCFGs in edit detection.
2.2 Prosody and parsing
While spontaneous speech poses problems for pars-
ing due to the presence of disfluencies and lack of
punctuation, there is information in speech associ-
ated with prosodic cues that can be taken advantage
of in parsing. Certainly, prosodic cues are useful
for sentence segmentation (Liu et al, 2004), and
the quality of automatic segmentation can have a
significant impact on parser performance (Kahn et
al., 2004). There is also perceptual evidence that
prosody provides cues to human listeners that aid
in syntactic disambiguation (Price et al, 1991), and
the most important of these cues seems to be the
prosodic phrases (perceived groupings of words) or
the boundary events marking them. However, the
utility of sentence-internal prosody in parsing con-
versational speech is not well established.
Most early work on integrating prosody in parsing
was in the context of human-computer dialog sys-
tems, where parsers typically operated on isolated
utterances. The primary use of prosody was to rule
out candidate parses (Bear and Price, 1990; Batliner
et al, 1996). Since then, parsing has advanced con-
siderably, and the use of statistical parsers makes the
candidate pruning benefits of prosody less impor-
tant. This raises the question of whether prosody
is useful for improving parsing accuracy for con-
versational speech, apart from its use in sentence
234
Figure 2: System architecture
boundary detection. Extensions of Charniak and
Johnson (2001) look at using quantized combina-
tions of prosodic features as additional ?words?,
similar to the use of punctuation in parsing written
text (Gregory et al, 2004), but do not find that the
prosodic features are useful. It may be that with the
short ?sentences? in spontaneous speech, sentence-
internal prosody is rarely of use in parsing. How-
ever, in edit detection using a parsing model (John-
son et al, 2004), posterior probabilities of automati-
cally detected IPs based on prosodic cues (Liu et al,
2004) are found to be useful. The seeming discrep-
ancy between results could be explained if prosodic
cues to IPs are useful but not other sub-sentence
prosodic constituents. Alternatively, it could be that
including a representation of prosodic features as
terminals in (Gregory et al, 2004) displaces words
in the parser n-gram model history. Here, prosodic
event posteriors are used, with the goal of providing
a more effective way of incorporating prosody than
a word-like representation.
3 Approach
3.1 Overall architecture
Our architecture, shown in Figure 2, combines the
parse reranking framework of (Collins, 2000) with
the edit detection and parsing approach of (Charniak
and Johnson, 2001). The system operates as follows:
1. Edit words are identified and removed.
2. Each resulting string is parsed to produce a set
of k candidate parses.
3. Edit words reinserted into the candidates with
a new part-of-speech tag EW. Consecutive se-
quences of edit words are inserted as single, flat
EDITED constituents.
4. Features (syntactic and/or prosodic) are ex-
tracted for each candidate, i.e. candidates are
converted to feature vector representation.
5. The candidates are rescored by the reranker to
identify the best parse.
Use of Collins? parse reranking model has several
advantages for our work. In addition to allowing us
to incorporate prosody without blocking lexical de-
pendencies, the discriminative model makes it rela-
tively easy to experiment with a variety of prosodic
features, something which is considerably more dif-
ficult to do directly with a typical PCFG parser.
Our use of the Charniak-Johnson approach of sep-
arately detecting disfluencies is motivated by their
result that edit detection error degrades parser accu-
racy, but we also include experiments that omit this
step (forcing the PCFG to model the edits) and con-
firm the practical benefit of separating responsibili-
ties between the edit detection and parsing tasks.
3.2 Baseline system
We adopt an existing parser-reranker as our base-
line (Charniak and Johnson, 2005). The parser
component supports k-best parse generation, and
the reranker component is used to rescore candi-
date parses proposed by the parser. In detail, the
reranker selects from the set of k candidates T =
{t1, . . . tk} the parse t? ? T with the highest bracket
F -score (in comparison with a hand-annotated ref-
erence). To accomplish this, a feature-extractor con-
verts each candidate parse t ? T into a vector of
real-valued features f(t) = (f1(t), . . . , fm(t)) (e.g.,
the value fj(t) of the feature fj might be the num-
ber of times a certain syntactic structure appears in
t). The reranker training procedure associates each
feature fj with a real-valued weight ?j , and ??f(t)
(the dot product of the feature vector and the weight
vector ?) is a single scalar weight for each parse can-
didate. The reranker employs a maximum-entropy
estimator that selects the ? that minimizes the log
loss of the highest bracket F -score parse t? condi-
tioned on T (together with a Gaussian regularizer
to prevent overtraining). Informally, ? is chosen to
235
make high F -score parses as likely as possible un-
der the (conditional) distribution defined by f and ?.
As in (Collins, 2000), we generate training data for
the reranker by reparsing the training corpus, using
n ? 1 folds as training data to parse the n-th fold.
The existing system also includes a feature extrac-
tor that identifies interesting syntactic relationships
not included in the PCFG parsing model (but used
in the reranker). These features are primarily related
to non-local dependencies, including parallelism of
conjunctions, the number of terminals dominated by
coordinated structures, right-branching root-to-leaf
length, lexical/functional head pairs, n-gram style
sibling relationships, etc.
3.3 Prosodic Features
Most theories of prosody have a symbolic represen-
tation for prosodic phrasing, where different combi-
nations of acoustic cues (fundamental frequency, en-
ergy, timing) combine to give categorical perceptual
differences. Our approach to integrating prosody in
parsing is to use such symbolic boundary events, in-
cluding prosodic break labels that build on linguistic
notions of intonational phrases and hesitation phe-
nomena. These events are predicted from a com-
bination of continuous acoustic correlates, rather
than using the acoustic features directly, because
the intermediate representation simplifies training
with high-level (sparse) structures. Just as phone-
based acoustic models are useful in speech recogni-
tion systems as an intermediate level between words
and acoustic features (especially for characterizing
unseen words), the small set of prosodic boundary
events are used here to simplify modeling the inter-
dependent set of continuous-valued acoustic cues re-
lated to prosody. However, also as in speech recog-
nition, we use posterior probabilities of these events
as features rather than making hard decisions about
presence vs. absence of a constituent boundary.
In the past, the idea of using perceptual categories
has been dismissed as impractical due to the high
cost of hand annotation. However, with advances
in weakly supervised learning, it is possible to train
prosodic event classifiers with only a small amount
of hand-labeled data by leveraging information in
syntactic parses of unlabeled data. Our strategy is
similar to that proposed in (No?th et al, 2000), which
uses categorical labels defined in terms of syntactic
structure and pause duration. However, their sys-
tem?s category definitions are without reference to
human perception, while we leverage learned re-
lations between perceptual events and syntax with
other acoustic cues, without predetermining the re-
lation or requiring a direct coupling to syntax.
More specifically, we represent three classes of
prosodic boundaries (or, breaks): major intonational
phrase, hesitation, and all other word boundaries.1
A small set of hand-labeled data from the treebanked
portion of the Switchboard corpus (Ostendorf et al,
2001) was used to train initial break prediction mod-
els based on both parse and acoustic cues. Next, the
full set of treebanked Switchboard data is used with
an EM algorithm that iterates between: i) finding
probabilities of prosodic breaks in unlabeled data
based on the current model, again using parse and
acoustic features, and ii) re-estimating the model us-
ing the probabilities as weighted counts. Finally, a
new acoustic-only break prediction model was de-
signed from this larger data set for use in the parsing
experiments.
In each stage, we use decision trees for models, in
part because of an interest in analyzing the prosody-
syntax relationships learned. The baseline system
trained on hand-labeled data has error rates of 9.6%
when all available cues are used (both syntax and
prosody) and 16.7% when just acoustic and part-of-
speech cues are provided (our target environment).
Using weakly supervised (EM) training to incorpo-
rate unannotated data led to a 15% reduction in error
rate to 14.2% for the target trees. The final decision
tree was used to generate posteriors for each of the
three classes, one for each word in a sentence.
?From perceptual studies and decision tree analy-
ses, we know that major prosodic breaks tend to co-
occur with major clauses, and that hesitations often
occur in edit regions or at high perplexity points in
the word sequence. To represent the co-occurrence
as a feature for use in parse reranking, we treat
the prosodic break posteriors as weighted counts in
accumulating the number of constituents in parse
t of type i with break type j at their right edge,
which (with some normalization and binning) be-
comes feature fij . Note that the unweighted count
1The intonational phrase corresponds to a break of ?4? in the
ToBI labeling system (Pitrelli et al, 1994), and a hesitation is
marked with the ?p? diacritic.
236
for constituent i corresponds directly to a feature
in the baseline set, but the baseline set of features
also includes semantic information via association
with specific words. Here, we simply use syntactic
constituents. It is also known that major prosodic
breaks tend to be associated with longer syntactic
constituents, so we used the weighted count strategy
with length-related features as well. In all, the vari-
ous attributes associated with prosodic break counts
were the constituent label of the subtree, its length
(in words), its height (maximal distance from the
constituent root to any leaf), and the depth of the
rightmost word (distance from the right word to the
subtree root). For each type in each of these cate-
gories, there are three prosodic features, correspond-
ing to the three break types.
3.4 Edit detection
To provide a competitive baseline for our parsing
experiments, we used an off-the-shelf, state-of-the-
art TAG-based model as our primary edit detec-
tor (Johnson et al, 2004).2 This also provided us a
competitive benchmark for contrasting the accuracy
of PCFGs on the edit detection task (Section 4.2).
Whereas the crossing-dependencies inherent in
speech repairs makes them difficult to model us-
ing HMM or PCFG approaches (Section 2.1), Tree
Adjoining Grammars (TAGs) are capable of cap-
turing these dependencies. To model both the
crossed-dependencies of speech repairs and the lin-
ear or tree-structured dependencies of non-repaired
speech, Johnson et al?s system applies the noisy
channel paradigm: a PCFG language model defines
a probability distribution over non-repaired speech,
and a TAG is used to model the optional insertion of
edits. The output of this noisy channel model is a
set of candidate edits which are then reranked using
a max-ent model (similar to what is done here for
parse reranking). This reranking step enables incor-
poration of features based on an earlier word-based
classifier (Charniak and Johnson, 2001) in addition
to output features of the TAG model. Acoustic fea-
tures are not yet incorporated.
2We also evaluated another state-of-the-art edit detection
system (Liu et al, 2004) but found that it suffered from a mis-
match between the current LDC specification of edits (LDC,
2004) and that used in the treebank.
4 Experimental design
4.1 Corpus
Experiments were carried out on conversational
speech using the hand-annotated transcripts associ-
ated with the Switchboard treebank (Graff and Bird,
2000). As was done in (Kahn et al, 2004), we
resegmented the treebank?s sentences into V5-style
sentence-like units (SUs) (LDC, 2004), since our ul-
timate goal was to be able to parse speech given au-
tomatically detected boundaries. Unfortunately, the
original transcription effort did not provide punctu-
ation guidelines, and the Switchboard treebanking
was performed on the transcript unchanged, with-
out reference to the audio. As a result, the sentence
boundaries sometimes do not match human listener
decisions using SU annotation guidelines, with dif-
ferences mainly corresponding to treatment of dis-
course markers and backchannels. In the years since
the original Switchboard annotation was performed,
LDC has iteratively refined guidelines for annotating
SUs, and significant progress has been made in au-
tomatically recovering SU boundaries annotated ac-
cording to this standard (Liu et al, 2004). To even-
tually leverage this work, we have taken the Meteer-
annotated SUs (Meteer et al, 1995), for which there
exists treebanked training data, and automatically
adjusted them to be more like the V5 LDC stan-
dard, and resegmented the Switchboard treebank ac-
cordingly. In cases where the original syntactic con-
stituents span multiple SUs, we discard any con-
stituents violating the SU boundary, and in the event
that an SU spans a treebank sentence boundary, a
new top-level constituent SUGROUP is inserted to
produce a proper tree (and evaluated like any other
constituent in the gold tree).3 While this SU reseg-
mentation makes it difficult to compare our experi-
mental results to past work, we believe this is a nec-
essary step towards developing a more realistic base-
line for fully automated parsing of speech.
In addition to resegmention, we removed all punc-
tuation and case from the corpus to more closely
reflect the form of output available from a speech
recognizer. We retained partial words for consis-
3SU and treebank segments disagree at about 5% in each di-
rection, due mostly to the analysis of discourse markers as con-
junctions (sentences of >1 SU) and the separation of backchan-
nels into separate treebank sentences (SUs of >1 sentence).
237
Table 1: Statistics on the Switchboard division used.
Section Sides SUs Words
Train 1,031 87,599 659,437
Tune 126 13,147 103,500
Test 128 8,726 61,313
Total 1,285 109,472 824,250
tency with other work (Liu et al, 2004; Johnson et
al., 2004), although word fragments would not typ-
ically be available from ASR. Finally, of the 1300
total conversation sides, we discarded 15 for which
we did not have prosodic data. Our corpus division
statistics are given in Table 1. During development,
experiments were carried out on the tune section; the
test section was reserved for a final test run.
4.2 Experimental Variables
Our primary goal is to evaluate the extent to which
prosodic cues could augment and/or stand-in for lex-
ical and syntactic features. Correspondingly, we
report on using three flavors of feature extraction:
syntactic and lexical features (Section 3.2), prosodic
features (Section 3.3), and both sets of features com-
bined. For all three conditions, the first-stage score
for each parse (generated by the off-the-shelf k-best
parser) was always included as a feature.
A second parameter varied in the experiments was
the method of upstream edit detection employed
prior to parsing: PCFG, TAG-based, and oracle
knowledge of treebank edit annotations. While it
had been claimed that PCFGs perform poorly as edit
detectors (Charniak and Johnson, 2001), we could
not find empirical evidence in the literature quan-
tifying the severity of the problem. Therefore, we
evaluated two PCFGs (Bikel, 2004; Charniak and
Johnson, 2005) on edit detection and compared their
performance to a state-of-the-art TAG-based edit de-
tection system (Johnson et al, 2004). For this ex-
periment, we evaluated edit detection accuracy on a
per-word basis, where any tree terminal is consid-
ered an edit word if and only if it is dominated by
an EDITED constituent in the gold tree. The PCFGs
were trained on the train section of the treebank data
with the flattened edit regions included4 and then
4Training on flattened EDITED nodes improved detection ac-
curacy for both PCFGs: as much as 15% for Bikel-Collins.
Table 2: Edit word detection performance for two
word-based PCFGs and the TAG-based edit detec-
tor. F -score and error are word-based measures.
Edit Detector Edit F -score Edit Error
Bikel-Collins PCFG 65.3 62.1
Charniak PCFG 65.8 59.9
TAG-based 78.2 42.2
Table 3: Parsing F -score of various feature and edit-
detector combinations.
PCFG TAG Oracle
Edit F (Table 2) 65.8 78.2 100.0
Parser 1-best 84.4 85.0 86.9
Prosodic feats 85.0 85.6 87.6
Syntactic feats 85.9 86.4 88.4
Combined feats 86.0 86.6 88.6
Oracle-rate 92.6 93.2 95.2
used to parse the test data.5 The TAG-based de-
tector was trained on the same conversation sides,
with its channel model trained on the Penn Treebank
disfluency-annotated files and its language model
trained on trees with the EDITED nodes excised. As
shown in Table 2, we did find that both PCFGs per-
formed significantly below the TAG-based detector.
5 Results
In evaluating parse accuracy, we adopt the relaxed
edited revision (Charniak and Johnson, 2001) to the
standard PARSEVAL metric, which penalizes sys-
tems that get EDITED spans wrong, but does not pe-
nalize disagreements in the attachment or internal
structure of edit regions. This metric is based on the
assumption that there is little reason to recover syn-
tactic structure in regions of speech that have been
repaired or restarted by the speaker.
Table 3 shows the F -scores for the top-ranked
parses after reranking, where the first-stage PCFG
parser was run with a beam-size of 50. The first
and last rows show lower and upper bounds, respec-
tively, for reranked parsing accuracy on each edit
condition. As the oracle rate6 shows, there is con-
5For the Charniak parser, edits were detected using only its
PCFG component in 1-best mode, not its 2nd stage reranker.
6Oracle F uses the best parse in the 50-best list.
238
siderable room for improvement. Statistical signif-
icance was computed using a non-parametric shuf-
fle test similar to that in (Bikel, 2004). For TAG
and oracle edit detection conditions, the improve-
ment from using the combined features over either
prosodic or syntactic features in isolation was sig-
nificant (p < 0.005). (For PCFG edit detection,
p < 0.04.) Similarly, for all three feature extraction
conditions, the improvement from using the TAG-
based edit detector instead of the PCFG edit detector
was also significant (p < 0.001). Interestingly, the
TAG?s 34% reduction in edit detection error relative
to the PCFG yielded only about 23% of the parse
accuracy differential between the PCFG and oracle
conditions. Nevertheless, there remains a promising
2.0% difference in parse F -score between the TAG
and oracle detection conditions to be realized by fur-
ther improvements in edit detection. Training for
the syntactic feature condition resulted in a learned
weight ? with approximately 50K features, while
the prosodic features used only about 1300 features.
Despite this difference in the length of the ? vectors,
the prosodic feature condition achieved 40?50% of
the improvement of the syntactic features.
In some situations, e.g. for language modeling,
improving the ordering and weights of the entire
parse set (an not just the top ranked parse) is im-
portant. To illustrate the overall improvement of the
reranked order, in Table 4 we report the reranked-
oracle rate over the top s parses, varying the beam s.
The error for each feature condition, relative to using
the PCFG parser in isolation, is shown in Figure 3.
Both the table and figure show that the reranked
beam achieves a consistent trend in parse accuracy
improvement relative to the PCFG beam, similar to
what is demonstrated by the 1-best scores (Table 3).
Table 4: Reranked-oracle rate parse F -score for the
top s parses with reference edit detection.
s 1 2 3 5 10 25
PCFG 86.9 89.8 91.0 92.2 93.4 94.6
Pros. 87.6 90.3 91.5 92.7 93.9 94.8
Syn. 88.4 91.3 92.4 93.4 94.3 95.0
Comb. 88.6 91.5 92.5 93.5 94.4 95.0
Figure 3: Reduction in error (Error = 1?F ) for the
s-best reranked-oracle relative to the parser-only or-
acle, for different feature rerankings (reference edit
detection).
6 Conclusion
This study shows that incorporating prosodic infor-
mation into the parse selection process, along with
non-local syntactic information, leads to improved
parsing accuracy on accurate transcripts of conver-
sational speech. Gains are shown to be robust to dif-
ficulties introduced by automatic edit detection and,
in addition to improving the one-best performance,
the overall ordering of the parse candidates is im-
proved. While the gains from combining prosodic
and syntactic features are not additive, since the
prosodic features incorporates some constituent-
structure information, the combined gains are sig-
nificant. These results are consistent with related ex-
periments with a different type of prosodically cued
event, which showed that automatically detected IPs
based on prosodic cues (Liu et al, 2004) are useful
in the reranking stage of a TAG-based speech repair
detection system (Johnson et al, 2004).
The experiments described here used automat-
ically extracted prosodic features in combination
with human-produced transcripts. It is an open ques-
tion as to whether the conclusions will hold for er-
rorful ASR transcripts and automatically detected
SU boundaries. However, there is reason to believe
that relative gains from using prosody may be larger
than those observed here for reference transcripts
239
(though overall performance will degrade), based on
prior work combining prosody and lexical cues to
detect other language structures (Shriberg and Stol-
cke, 2004). While the prosody feature extraction de-
pends on timing of the hypothesized word sequence,
the acoustic cues are relatively robust to word errors
and the break model can be retrained on recognizer
output to automatically learn to discount the lexical
evidence. Furthermore, if parse reranking operates
on the top N ASR hypotheses, the reranking pro-
cedure can improve recognition outputs, as demon-
strated in (Kahn, 2005) for syntactic features alone.
Allowing for alternative SU hypotheses in reranking
may also lead to improved SU segmentation.
In addition to assessing the impact of prosody
in a fully automatic system, other avenues for fu-
ture work include improving feature extraction. One
could combine IP and prosodic break features (so
far explored separately), find new combinations of
prosody and syntactic structure, and/or incorporate
other prosodic events. Finally, it may also be use-
ful to integrate the prosodic events directly into the
PCFG, in addition to their use in reranking.
This work was supported by the NSF under grants DMS-
0074276, IIS-0085940, IIS-0112432, IIS-0326276, and LIS-
9721276. Conclusions are those of the authors and do not nec-
essarily reflect the views of the NSF.
References
A. Batliner et al 1996. Prosody, empty categories and
parsing - a success story. Proc. ICSLP, pp. 1169-1172.
J. Bear and P. Price. 1990. Prosody, syntax and parsing.
Proc. ACL, pp. 17-22.
D. Bikel. 2004. On the Parameter Space of Lexicalized
Statistical Parsing Models. Ph.D. thesis, U. Penn.
E. Charniak and M. Johnson. 2001. Edit detection and
parsing for transcribed speech. NAACL, pp. 118-126.
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking.
Proc. ACL.
M. Collins. 2000. Discriminative reranking for natural
language parsing. Proc. ICML, pp. 175-182.
M. Core and L. Schubert. 1999. A syntactic framework
for speech repairs and other disruptions. Proc. ACL,
pp. 413-420.
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing
and disfluency placement. Proc. EMNLP, pp. 49-54.
D. Graff and S. Bird. 2000. Many uses, many annota-
tions for large speech corpora: Switchboard and TDT
as case studies. Proc. LREC, pp. 427-433.
M. Gregory, M. Johnson, and E. Charniak. 2004.
Sentence-internal prosody does not help parsing the
way punctuation does. Proc. NAACL, pp. 81-88.
P. A. Heeman and J. F. Allen. 1999. Speech repairs,
intonational phrases, and discourse markers: Model-
ing speaker?s utterances in spoken dialogue. Compu-
tational Linguistics, 25(4):527-571.
D. Hindle. 1983. Deterministic parsing of syntactic non-
fluencies. Proc. ACL, pp. 123-128.
M. Johnson, E. Charniak, and M. Lease. 2004. An im-
proved model for recognizing disfluencies in conver-
sational speech. Proc. Rich Transcription Workshop.
J. G. Kahn, M. Ostendorf, and C. Chelba. 2004. Pars-
ing conversational speech using enhanced segmenta-
tion. Proc. HLT-NAACL 2004, pp. 125-128.
J. G. Kahn. 2005. Moving beyond the lexical layer in
parsing conversational speech. M.A. thesis, U. Wash.
LDC. 2004. Simple metadata annotation specification.
Tech. report, Linguistic Data Consortium. Available
at http://www.ldc.upenn.edu/Projects/MDE.
Y. Liu et al 2004. The ICSI-SRI-UW metadata extrac-
tion system. Proc. ICSLP, pp. 577-580.
M. Meteer, A. Taylor, R. MacIntyre, and R. Iyer. 1995.
Dysfluency annotation stylebook for the switchboard
corpus. Tech. report, Linguistic Data Consortium.
E. No?th et al 2000. Verbmobil: The use of prosody in
the linguistic components of a speech understanding
system. IEEE Trans. SAP, 8(5):519-532.
M. Ostendorf et al 2001. A prosodically labeled
database of spontaneous speech. ISCA Workshop on
Prosody in Speech Recognition and Understanding,
pp. 119-121, 10.
J. Pitrelli, M. Beckman, and J. Hirschberg. 1994. Eval-
uation of prosodic transcription labeling reliability in
the ToBI framework. Proc. ICSLP, pp. 123-126.
P. J. Price et al 1991. The use of prosody in syntactic
disambiguation. JASA, 90(6):2956-2970, 12.
E. Shriberg. 1994. Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, U.C. Berkeley.
E. Shriberg and A. Stolcke. 2004. Prosody modeling
for automatic speech recognition and understanding.
Mathematical Foundations of Speech and Language
Processing. Springer-Verlag, pp. 105-114.
240
Parsing Biomedical Literature
Matthew Lease and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP),
Brown University, Providence, RI USA
{mlease, ec}@cs.brown.edu
Abstract. We present a preliminary study of several parser adaptation
techniques evaluated on the GENIA corpus of MEDLINE abstracts [1,2].
We begin by observing that the Penn Treebank (PTB) is lexically im-
poverished when measured on various genres of scientific and techni-
cal writing, and that this significantly impacts parse accuracy. To re-
solve this without requiring in-domain treebank data, we show how ex-
isting domain-specific lexical resources may be leveraged to augment
PTB-training: part-of-speech tags, dictionary collocations, and named-
entities. Using a state-of-the-art statistical parser [3] as our baseline, our
lexically-adapted parser achieves a 14.2% reduction in error. With oracle-
knowledge of named-entities, this error reduction improves to 21.2%.
1 Introduction
Since the advent of the Penn Treebank (PTB) [4], statistical approaches to nat-
ural language parsing have quickly matured [3,5]. By providing a very large
corpus of manually labeled parsing examples, PTB has played an invaluable
role in enabling the broad analysis, automatic training, and quantitative evalu-
ation of parsing techniques. However, while PTB?s Wall Street Journal (WSJ)
corpus has historically served as the canonical benchmark for evaluating statis-
tical parsing, the need for broader evaluation has been increasingly recognized
in recent years. Furthermore, since it is impractical to create a large treebank
like PTB for every genre of interest, significant attention has been directed to-
wards maximally reusing existing training data in order to mitigate the need
for domain-specific training examples. These issues have been most notably ex-
plored in parser adaptation studies conducted between PTB?s WSJ and Brown
corpora [6,7,8,9].
As part of our own exploration of these issues, we have been investigating
statistical parser adaptation to a novel domain: biomedical literature. This lit-
erature presents a stark contrast to WSJ and Brown: it is suffused with domain-
specific vocabulary, has markedly different stylistic constraints, and is often writ-
ten by non-native speakers. Moreover, broader consideration of technical litera-
ture shows this challenge and opportunity is not confined to biomedical literature
 We would like to thank the National Science Foundation for their support of this work
(IIS-0112432, LIS-9721276, and DMS-0074276), as well as thank Sharon Goldwater
and our anonymous reviewers for their valuable feeback.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 58?69, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Parsing Biomedical Literature 59
alone, but is also demonstrated by patent literature, engineering manuals, and
field-specific scientific discourse. Through our work with biomedical literature,
we hope to gain insights into effective techniques for adapting statistical parsing
to technical literature in general.
Our interest in biomedical literature is also motivated by a real need to im-
prove information extraction in this domain. With over 15 million citations in
PubMed today, biomedical literature is the largest and fastest growing knowl-
edge domain of any science. As such, simply managing the sheer volume of its
accumulated information has become a significant problem. In response to this,
a large research community has formed around the challenge of enabling auto-
mated mining of the literature [10,11]. While the potential value of parsing has
often been discussed by this community, attempts to employ it thus far appear
to have been limited by the parsing technologies employed. Reported difficul-
ties include poor coverage, inability to resolve syntactic ambiguity, unacceptable
memory and speed, and difficulty in hand-crafting rules of grammar [12,13].
Perhaps the most telling indicator of community perspective came in a recent
survey?s bleak observation that efficient and accurate parsing of unrestricted text
appears to be out of reach of current techniques [14].
In this paper, we show that broad, accurate parsing of biomedical literature
is indeed possible. Using an off-the-shelf WSJ-trained statistical parser [3] as our
baseline, we provide the first full-coverage parse accuracy results for biomedi-
cal literature, as measured on the GENIA corpus of MEDLINE abstracts [1,2].
Furthermore, after showing that PTB is lexically impoverished when measured
on various genres of scientific and technical writing, we describe three methods
for improving parse accuracy by leveraging lexical resources from the domain:
part-of-speech (POS) tags, dictionary collocations, and named-entities. Our gen-
eral hope is that lexically-based techniques such as these can provide alternative
and complementary value to treebank-based adaptation methods such as co-
training [9] and sample selection [15]. Our lexically-adapted parser achieves a
14.2% reduction in error over the baseline, and in the case of oracle-knowledge
of named-entities, this reduction improves to 21.2%.
Section 2 describes the GENIA corpus in detail. In Section 3, we present
unknown word rate experiments which measure the coverage of PTB?s gram-
mar on various genres of scientific and technical writing. Section 4 describes
our methods for lexical adaptation and their corresponding effects on parse ac-
curacy. Section 5 concludes with a discussion challenges and opportunities for
future work.
2 The GENIA Corpus
The GENIA corpus [1,2] consists of MEDLINE abstracts related to transcription
factors in human blood cells. Version 3.02p of the corpus includes 19991 ab-
stracts (18,545 sentences, 436,947 words) annotated with part-of-speech (POS)
1 The reported total of 2000 abstracts includes repetition of article ID 97218353.
60 M. Lease and E. Charniak
tags and named-entities. Named-entities were labelled according to a corpus-
defined ontology, and the POS-tagging scheme employed is very similar to that
used in PTB (see Section 4.1).
Using these POS annotations and PTB guidelines [16], we hand-parsed 21
of these abstracts (215 sentences) to create a pilot treebank for measuring parse
accuracy. We performed the treebanking using the GRAPH2 tool developed for
the Prague Dependency Treebank. Initial bracketing was performed without any
form of automation. Following this, our baseline parser [3] was used to propose
alternative parses. In cases where hand-generated parses conflicted with those
proposed by the parser, hand-parses were manually corrected, or not corrected,
according to PTB bracketing guidelines. Our pilot treebank is publicly available3.
Subsequent to this, the Tsujii lab released its own beta version treebank,
which includes 200 abstracts (1761 sentences) from the original corpus. This
treebanking was performed largely in accordance with PTB guidelines (perhaps
the most significant difference being constituent labels NAC and NX were excluded
in favor of NP). Because there is no redundancy in the coverage of the Tsuijii lab?s
treebank and our own pilot treebank (and by chance, NAC and NX do not occur
in our pilot treebank either), we have combined the two treebanks to maximize
our evaluation treebank (see Table 3).
An additional note is required regarding our use of named-entities (Sec-
tion 4.3). Entity annotations (not available in the treebank) were obtained from
the earlier 3.02p version of the corpus. Any sentences that did not match be-
tween the two versions of the corpus (due to differences in tokenization or other
variations) were discarded. The practical impact of this was negligible, as only
25 sentences had to be discarded4.
3 Unknown Words
Casual reading of technical literature quickly reveals a rich, field-specific vocab-
ulary. For example, consider the following sentence taken from GENIA:
The study of NF-kappaB showed that oxLDLs led to a decrease of
activation-induced p65/p50 NF-kappaB heterodimer binding to DNA,
whereas the presence of the constitutive nuclear form of p50 dimer was
unchanged.
To quantitatively measure the size and field-specificity of domain vocabulary, we
extracted the lexicon contained in WSJ sections 2-21 and evaluated the unknown
word rate (by token) for various genres of technical literature. Results are given
in Table 1.
2 http://quest.ms.mff.cuni.cz/pdt/Tools/Tree Editors/Graph
3 http://www.cog.brown.edu/Research/nlp
4 Because our preliminary use of named-entities assumes oracle-knowledge, this exper-
iment was carried out on the development section only, thus only the development
section was reduced in this way.
Parsing Biomedical Literature 61
Table 1. Unknown word rate on various technical corpora given WSJ 2-21 lexion
Corpus Unknown Word Rate
WSJ sect. 24 2.7
Brown-DEV 5.8
Brown sect. J 7.3
CRAN 10.0
CACM 10.7
DOE 16.7
GENIA 25.5
Brown-DEV corresponds to a balanced sampling of the Brown corpus (see Ta-
ble 4). Section J of Brown contains ?Learned? writing samples and demonstrated
the highest rate of any single Brown section. CRAN contains 1400 abstracts in
the field of aerodynamics, and CACM includes 3200 abstracts from Communica-
tions of the ACM [17]. DOE contains abstracts from the Department of Energy,
released as part of PTB. GENIA here refers to 333 abstracts (IDs 97449161-
99101008) not overlapping our treebank. As this table shows, unknown word
rate clearly increases as we move to increasingly technical domains. Annecdotal
evaluation on patent literature suggests its unknown rate lies somewhere between
that of DOE and GENIA.
While these results appear to indicate WSJ is lexically impoverished with
respect to increasingly technical domains, it was also necessary to consider the
possibility that the results were simply symptomatic of technical domains having
very large lexicons. If such were the case, we would expect to see these domains
demonstrate high unknown word rates even in the presence of a domain-specific
lexicon. To test this hypothesis, we contrasted unknown word rates on GENIA
using lexicons extracted from WSJ sections 2-21, Brown (training section from
Table 4), and from GENIA itself (1,333 abstracts: IDs 90110496-97445684)5.
Results are presented in Table 2.
Table 2. Unknown word rate on GENIA using lexicons extracted from WSJ, Brown,
and GENIA
Lexicon Size Unknown Word Rate
Brown 25K 28.2
WSJ 40K 25.5
Brown+WSJ 50K 22.4
GENIA 15K 5.3
Brown+WSJ+GENIA 60K 4.6
5 While this set of abstracts does overlap the Tsujii treebank, this experiment was run
prior to the treebank?s release.
62 M. Lease and E. Charniak
Although the unknown word rate in the presence of in-domain training for
GENIA (5.3%, Table 2) is nearly twice that of out-of-domain training (2.7%,
Table 1), suggesting a larger lexicon does indeed exist, it is also strikingly clear
that WSJ and Brown provide almost no lexical value to the domain: expanding
GENIAs lexicon by 45,000 new terms found in WSJ and Brown produced only a
meager 0.7% reduction in unknown word rate. Contrast this with the enormous
reduction achieved through using GENIA?s lexicon instead of the WSJ or Brown
lexicons (Table 2).
4 Parser Adaptation
In this section, we present three methods for parser adaptation motivated by
the results of our unknown word rate experiments (Section 3). The goal of
these adaptations is to help an off-the-shelf PTB-trained parser compensate
for the large amount of domain-specific vocabulary found in technical liter-
ature, specifically biomedical text. To accomplish this without depending on
in-domain treebank data, we consider three alternative (and less expensive)
domain-specific knowledge sources: part-of-speech tags, dictionary collocations,
and named-entities. We report on the results of each technique both in isolation
and in combination.
We adopt as our baseline for these experiments the publicly available Charniak
parser [3] trained on WSJ sections 2-21 of the Penn Treebank. Our division of the
GENIA corpus into development and test sets is shown in Table 3. Analysis was
carried out on the development section, and the test section was reserved for final
evaluation. Parse accuracy was measured using the standard PARSEVAL met-
ric of bracket-bracket scoring, assuming the usual conventions regarding punctua-
tion [18]. Statistical significance for eachexperimentwas assessedusing a two-tailed
paired t-test on sentence-averaged f-measure scores. Since our evaluation treebank
excludes NX and NAC constituent labels in favor ofNP (Section 2), for all experiments
Table 3. Division of the GENIA combined treebank into development and test sections
Source Section Abstract IDs Sentences
Pilot Development 99101510-99120900 215
Tsujii Development 91079577-92060325 732
Tsujii Test 92062170-94051535 1004
Table 4. Brown corpus division. Training and evaluation sections were obtained from
Gildea [7]. The development (and final training) section was created by extracting
every tenth sentence from Gildea?s training corpus.
POS-Train Development Test
Sentences 19637 2181 2425
Parsing Biomedical Literature 63
Table 5. PARSEVAL f-measure scores on the GENIA development section using the
adaptation methods described in Section 4. Statistical significance of individual adap-
tations are compared against no adaptation, and combined adaptations are compared
against the best prior adaptation. As the p values indicate, all of the adaptions listed
here produced a significant improvement in parse accuracy.
Adaptation F-measure Error reduction Significance
none 78.3 ? ?
lexicon 78.6 1.4 p = 0.002
no NNP 79.1 3.7 p = 0.002
train POS 80.8 11.5 p < 0.001
entities 80.9 12.0 p < 0.001
no NNP, train POS 81.5 14.7 p = 0.043
no NNP, train POS, entities 82.9 21.2 p < 0.001
Table 6. Final PARSEVAL f-measure results on GENIA compared with scores on
Brown and WSJ sect. 23. In all cases, the parser was trained on WSJ sect. 2-21 with
the over-parsing parameter set to 21x over-parsing. Adapted GENIA results includes
POS adaptations only (oracle-type entity adaptation was not used). Adapted Brown
results use POS re-training on Brown train section.
Corpus F-measure Error reduction Significance
GENIA-unadapted 76.3 ? ?
GENIA-adapted 79.6 14.2 p < 0.001
Brown-unadapted 83.4 ? ?
Brown-adapted 84.1 4.1 p = 0.002
WSJ 89.5 ? ?
(including baseline)wepost-processedparser output to collapse these label distinc-
tions6. Results from our various experiments are summarized in Table 5.
Final results of our adapted parser are given in Table 6. For comparison with
standard benchmarks, parser performance was also evaluated on WSJ section
23 and on Brown. Table 4 shows our division of the Brown corpus.
4.1 Using POS Tags
Part-of-speech tags provide an important data feature to statistical parsers [3,5].
Since technical and scientific texts introduce a significant amount of domain-
specific vocabulary (Section 3), a POS-tagger trained only on everyday
6 While PTB examples could be similarly pre-processed prior to training, thereby reduc-
ing the search spacewhile parsing, the reductionwould beminor andwouldmean giving
up a potentially useful distinction in syntactic contexts.
64 M. Lease and E. Charniak
English is immediately at a disadvantage for tagging such text. Indeed, our
off-the-shelf PTB-trained parser achieves only 84.6% tagging accuracy on GE-
NIA. Consequently, our simple first adaptation step was to retrain the parser?s
POS-tagger on the 1,778 GENIA abstracts not present in the combined tree-
bank (in addition to WSJ sections 2-21). This simple fix raised tagging accuracy
to 95.9%. Correspondingly, parsing accuracy improved from 78.3% to 80.8%
(Table 5).
While such POS-retraining is a direct remedy to learning appropriate tags
for new vocabulary, it is only a partial fix to a larger problem. In particular, the
trees found in PTB codify a relationship between PTB POS tags and constituent
structure, and any mismatch between the tagging schemata used in PTB and
that used by our new corpus could result in misapplication or underutilization of
the bracketing rules acquired by the parser during training. To overcome this, it
is necessary to introduce an additional mapping step which converts between the
two POS tagging schemata. For closely related schemata, this mapping may be
trivial, but this cannot be assumed without a carefully analysis of tag distribution
and usage across the two corpora.
In the case of GENIA, the tagging guidelines used were based on PTB and
only subsequently revised (to improve inter-annotator agreement), so while dif-
ferences do exist, the problem is much less significant than the general case
of arbitrarily different schemata. Reported differences include treatment of hy-
phenated, partial, and foreign terms, and most notably, the distinction between
proper (NNP) and common (NN) nouns [2]. In order to quantitatively assess
the degree to which these and other revisions were made to the tagging scheme,
we extracted the POS distribution for 333 GENIA abstracts (as used in our
unknown word rate experiments from Section 3). From this distribution, we
learned that NNP almost never occurs in GENIA. This meant that our PTB-
trained parser would be unable to leverage PTB?s constituent structure examples
examples that involved proper nouns.
As a preliminary remedy, we simply relabeled all proper nouns as common in
PTBand re-trained the parser.This improved tagging accuracy to 96.4%and pars-
ing accuracy to 81.5% (Table 5). We should note, however, that this solution is
not ideal. While it does allow use of PTB?s NNP-examples, it does so at the cost
of confusing legitimate differences in the syntactic distribution of common and
proper nouns in English (as reflected by a 0.7% loss in accuracy on WSJ evalua-
tion when using this NN-NNP conflated training data). Clearly it would be better
if GENIA?s nouns could be re-tagged to preserve this distinction while preserving
inter-annotator agreement. A first step in this direction would be to perform this
re-tagging automatically based on determiner usage and GENIA?s entity annota-
tions, with success measured by the corresponding impact on parse accuracy. This,
along with a more careful analysis of tagging differences, remains for future work.
Wehavealso evaluatedparser performanceunder the oracle conditionofperfect
tags. This was implemented as a soft constraint so that the parser?s joint probabil-
ity model could overrule the oracle tag for cases in which no parse could be found
using it (cases of annotator error or data sparsity). Using the oracle tag 99.8% of
Parsing Biomedical Literature 65
the time (in addition to other POS adaptations) had almost no impact on parse ac-
curacy, suggesting that further POS-related improvements in parse accuracy will
only come from the sort of careful analysis of the tagging schemata discussed above.
4.2 Using a Domain-Specific Lexicon
Another strategy we employed for lexical adaptation was the use of a domain-
specific dictionary. For biomedicine, such a dictionary is available from the Na-
tional Library of Medicine: the Unified Medical Language System (UMLS) SPE-
CIALIST lexicon [19]. Covering both general English as well as biomedical vo-
cabulary, the SPECIALIST lexicon contains over 415,000 entries (including or-
thographic and morphological variants). Entries are also assigned one of eleven
POS categories specified as part of the lexicon.
Given our finding from Section 4.1 that even oracle POS tags would do little
to improve upon our re-trained POS tagger, we did not make use of lexicon POS
tags. Instead, we restricted our use of the lexicon to extracting collocations. We
then added a hard-constraint to the parser that these collocations could not be
cross-bracketed and that each collocation must represent a flat phrase with no
internal sub-constituents. This approach was motivated by a couple of observa-
tions. On one hand, we observed cases where the parser would be confused by
long compound nouns; in desperation to find the start of a verb phrase, it would
sometimes use part of the compound to head a new verb phrase. Unfortunately,
WSJ sections 2-21 contain approximately 500 verb phrases headed by present-
participle verbs mistagged as nouns, thus making this bizarre bracketing rule
statistically viable. A second observation was the frequency with which we saw
the terms ?in vivo? and ?in vitro? (treebanked as foreign adjverbial or adjecti-
val collocations) mis-analyzed. Even in biomedical texts, ?in? appears far more
often as a preposition than as part of such collocations, and as such, is almost
always mis-parsed in these collocational contexts to head a prepositional phrase.
Our hope was that by preventing such collocations from being cross-bracketted,
we could prevent this class of parsing mistakes.
We found use of lexical collocations did yield a small (0.3%) but statistically
significant improvement in performance over the unmodified parser (Table 5).
However, when combined with either POS or entity adaptations, the lexicon?s
impact on parsing accuracy was statistically insignificant. Our interpretation of
this latter result is that the primary limitation of the lexicon is coverage, despite
its size. That is, when either of the other adaptations were used, the lexicon
did not offer much beyond them. It is not surprising that oracle-knowledge of
entities (Section 4.3) provided greater coverage than the generic dictionary, and
the improvement in tagging from POS adaptation (sharper tag probabilities)
helped somewhat in preventing the verb-ification of some of the long compound
nouns. While the lexicon was the only adaptation to correctly fix ?in vivo? type
mistakes, these phrases alone were not sufficiently frequent to provide a statisti-
cally significant improvement in parse accuracy on top of other adaptations. As
such, the primary value of this method would be in cases where such a lexicon
is available but POS tags and labelled entities are not.
66 M. Lease and E. Charniak
4.3 Using Named-Entities
The primary focus of the GENIA corpus is to support training and evaluation of
automatic named-entity recognition. As such, a variety of biologically meaningful
terms have been annotated in the corpus according to a corpus-defined ontology.
Given the availability of these annotations, we were interested in considering
the extent to which they could be used as a source of lexical information for
parser adaptation.
Given the problems described earlier with regard to lexical collocations being
cross-bracketted by our off-the-shelf PTB-trained parser (Section 4.2), our hope
was that named-entities could be used similarly to lexical collocations in helping
to prevent this class of mistakes. To put it another way, we hoped to exploit
the correlation between named-entities and noun phrase (NP) boundaries. A
common preprocessing step in detecting named-entities is to use a chunker to
find NPs. Our approach was to do the reverse: to use named-entities as a feature
for finding NP boundaries.
Our initial plan was to use the same strategy we had used with dictionary
collocations: to add a hard-constraint to the parser that a named-entity could
not be cross-bracketed and had to represent a flat phrase with no internal sub-
constituents. However, we found upon closer inspection that the entities often
did contain substructure (primarily parenthetical acronyms), and so we relaxed
the flat-constituent constraint and enforced only the cross-bracketing constraint.
As a preliminary step, we evaluated the utility of this method using oracle-
knowledge of named-entities. By itself, this method was roughly equivalent to POS
re-training in improving parsing accuracy from78.3%to 80.9%(Table 5).Butwhen
combined with POS adaptations, use of named-entities provided another signifi-
cant improvement in performance, from81.5%to 82.9%.Clearly this is a promising
avenue for further work, and it will be interesting to see how much of this benefit
from the oracle case can be realized when using automatically detected entities.
5 Discussion
We have found only limited use of parsing reported to date for biomedical liter-
ature, thus it is difficult to compare our parsing results against previous work in
parsing this domain. To the best of our knowledge, only one other wide-coverage
parser has been applied to biomedical literature: Grover et al report 99% cov-
erage using a hand-written grammar with a statistical ranking component [20].
We do not know of any quantitative accuracy figures reported for this domain
other than those described here.
For those interested in mining the biomedical literature, the next important
step will be assessing the utility of PTB-style parsing compared to other pars-
ing models that have been employed for information extraction. There has been
promising work in using PTB-style parses for information extraction by inducing
predicate-argument structures from the output parses [21]. It will be interesting to
see for the biomedical domain how these predicate-argument structures compare
to those induced by other grammar formalisms currently in use, such as HPSG [22].
Parsing Biomedical Literature 67
The next immediate extension of our work is to evaluate use of detected
named-entities in place of the oracle case described in Section 4.3, replacing the
current hard-constraint with a soft-constraint confidence term to be incorporated
into the parser?s generative model. Performance of named-entity recognition on
GENIA was recently studied as part of a shared task at BioNLP/NLPBA 2004.
The best system achieved 72.6% f-measure [23], though note that this task re-
quired both detection and classification of named-entities. As our usage of en-
tities does not require classification, this number should be considered a lower-
bound in the context of our usage model. We expect this level of accuracy should
be sufficient to improve parse scores, though how much of the oracle benefit we
can realize remains to be seen.
There are also interesting POS issues meriting further investigation. As dis-
cussed in Section 4.1, we would like to find a better solution to the lack of
proper noun annotations in GENIA, perhaps by detecting proper nouns using
determiners and labelled entities. More careful analysis of the differences be-
tween the PTB and GENIA tagging schemata is also needed. Additionally, there
are interesting issues regarding how POS tags are used by the parsing model.
Whereas the Collins? model [5] treats POS tagging as an external preprocessing
step (a single best tag is input to the parsing model), the Charniak model [3]
generates tag hypotheses as part of its combined generative model, and thus
considers multiple hypotheses in searching for the best parse. The significance
of this is that other components of the generative model can influence tag se-
lection, and Charniak has reported adding this feature to his simulated version
of the Collins model improved its accuracy by 0.6% [24]. However, this result
was for in-domain evaluation; the picture becomes more complicated when we
begin parsing out-of-domain. If we have an in-domain trained POS-tagger, we
might not want a combined model trained on out-of-domain data overruling our
tagger?s predictions. One option may be introducing a weighting factor into the
generative model to indicate the degree of confidence assigned to our tagger
relative to the other components of the combined model.
Another issue for further work is the parsing of paper titles. In the GENIA
development section, only 28% of the titles are sentences whereas 71% are noun
phrases. This distribution is radically different than the rest of the corpus, which
is heavily dominated by sentence-type utterances. As headlines are even more
rare in our WSJ training data than titles are in GENIA (since WSJ contains
full article text), our parser performs miserably at utterance-type detection (i.e.
correctly labelling the top-most node in the parse tree): 58.6%. Correspondingly,
parse accuracy on titles is only 69.1%, which represents a statistically significant
decrease in accuracy in comparison to the entire development section (p = 0.038).
In investigating this, we noticed an oddity in GENIA in that most titles were
encoded in the corpus with an ending period that did not exist in the original
papers the corpus was derived from. By removing these periods, we improved
utterance-type detection to 77.9%. While parse accuracy rose to 72.0%, this
was statistically insignificant (p = 0.082). The solution we would like to move
towards is to respect the legitimate distributional differences between title and
68 M. Lease and E. Charniak
non-title utterances and parameterize the parser differently for the two cases.
Generally speaking, such ?contextual parsing? might allow us to improve parsing
accuracy more widely by parameterizing our parser differently based on where
the current utterance fits in the larger discourse. This example of period usage
in titles also highlights a broader issue that seemingly innocuous issues in corpus
preparation can have significant impact when parsing. As a further example of
this, the choice to (at times) separately tokenize term-embedded parentheses in
GENIA creates unnecessary attachment ambiguity in the resulting parenthetical
phrases. For example, in the phrase ?C3a and C3a(desArg)?, ?C3a(desArg)? is
tokenized as ?C3a ( desArg )?, which produces ambiguity as to whether the
parenthetical should attach low (to the latter ?C3a?) or high (to the compound
?C3a and C3a?). Issues such as these remind us to be mindful of the relationship
between corpus preparation and parsing, as well as downstream processing, and
that some issues which appear difficult to resolve while parsing might be handled
more easily at another stage in the processing pipeline.
We view biomedical and other technical texts as providing an interesting set
of challenges and questions for future parsing research. An interesting introduc-
tion to some of these challenges, supported by examples drawn from the domain,
can be found in [25]. A significant question for consideration is the degree to
which these challenges are related to domain knowledge vs. stylistic norms of
the genre. For example, [2] reports that whereas POS determination required
domain expertise, prepositional phrase (PP)-attachment could be largely deter-
mined even by non-biologists. Our own treebanking experience left us with the
opposite impression. For example, in the phrase ?gene expression and protein
secretion of IL-6?, should the PP attach high (IL-6 gene expression and protein
secretion) or low (gene expression and IL-6 protein secretion)? Domain knowl-
edge appears to be necessary here for correct resolution. In contrast to this, POS
tags appear to be a distributional rather than a semantic concern. Issues like
this highlight how little we really understand currently about the parameters
of corpus variation. How do the frequencies of different syntactic constructions
vary by genre, and are there key structural variations at work? How do we ef-
fectively adapt parsers in response? These issues remain important topics for
future investigation.
References
1. Kim, J.d., Ohta, T., Tateisi, Y., Tsujii, J.: Genia corpus - a semantically annotated
corpus for bio-textmining. Bioinformatics (Supplement: Eleventh International
Conference on Intelligent Systems for Molecular Biology) 19 (2003) i180?i182
2. Tateisi, Y., Ohta, T., dong Kim, J., Hong, H., Jian, S., Tsujii, J.: The genia corpus:
Medline abstracts annotated with linguistic information. In: Third meeting of SIG
on Text Mining, Intelligent Systems for Molecular Biology (ISMB). (2003)
3. Charniak, E.: A maximum-entropy-inspired parser. In: Proc. NAACL. (2000)
132?139
4. Marcus, M., Santorini, B., Marcinkiewicz, M.A.: Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics 19 (1993) 313?330
Parsing Biomedical Literature 69
5. Collins, M.: Discriminative reranking for natural language parsing. In: Proc. ICML.
(2000) 175?182
6. Ratnaparkhi, A.: Learning to parse natural language with maximum entropy mod-
els. Machine Learning 34 (1999) 151?175
7. Gildea, D.: Corpus variation and parser performance. In: Proceedings of the 2001
Conference on Empirical Methods in Natural Language Processing. (2001) 167?202
8. Roark, B., Bacchiani, M.: Supervised and unsupervised pcfg adaptation to novel
domains. In: Proceedings of HLT-NAACL. (2003) 205?212
9. Steedman, M., Hwa, R., Clark, S., Osborne, M., Sarkar, A., Hockenmaier, J.,
Ruhlen, P., Baker, S., Crim, J.: Example selection for bootstrapping statistical
parsers. In: Proceedings of HLT-NAACL. (2003) 331?338
10. de Bruijn, B., Martin, J.: Literature mining in molecular biology. In: Proceedings
of the European Federation for Medical Informatics (EFMI) Workshop on Natural
Language Processing in Biomedical Applications. (2002)
11. Hirschman, L., Park, J., Tsujii, J., Wong, L., Wu, C.: Accomplishments and chal-
lenges in literature data mining for biology. Bioinformatics 18 (2002) 1553?1561
12. Yakushiji, A., Tateisi, Y., Miyao, Y., Tsujii, J.: Event extraction from biomedical
papers using a full parser. In: Pacific Symposium on Biocomputing. (2001) 408?419
13. Daraselia, N., Yuryev, A., Egorov, S., Novichkova, S., Nikitin, A., Mazo, I.: Extract-
ing human protein interactions from medline using a full-sentence parser. Bioin-
formatics 20 (2004) 604?611
14. Shatkay, H., Feldman, R.: Mining the biomedical literature in the genomic era: An
overview. Journal of Computational Biology 10 (2003) 821?855
15. Hwa, R.: Learning Probabilistic Lexicalized Grammars for Natural Language Pro-
cessing. PhD thesis, Harvard University (2001)
16. Bies, A., Ferguson, M., Katz, K., MacIntyre, R.: Bracketting Guideliness for Tree-
bank II style Penn Treebank Project. Linguistic Data Consortium. (1995)
17. Buckley, C.: Implementation of the smart information retrieval system. Technical
Report 85-686, Cornell University (1985)
18. Goodman, J.: Parsing inside-out. PhD thesis, Harvard University (1998)
19. McCray, A.T., Srinivasan, S., Browne, A.C.: Lexical methods for managing varia-
tion in biomedical terminologies. In: Proceedings of the 18th Annual Symposium
on Computer Applications in Medical Care (SCAMC). (1994) 235?239
20. Grover, C., Lapata, M., Lascarides, A.: A comparison of parsing technologies for
the biomedical domain. Journal of Natural Language Engineering (2002)
21. Surdeanu, M., Harabagiu, S., Williams, J., Aarseth, P.: Using predicate-argument
structures for information extraction. In: Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL-03). (2003) 8?15
22. Miyao, Y., Ninomiya, T., Tsujii, J.: Corpus-oriented grammar development for
acquiring a head-driven phrase structure grammar from the penn treebank. In:
Proc. of IJCNLP-04. (2004) 684?693
23. Zhou, G., Su, J.: Exploring deep knowledge resources in biomedical name recog-
nition. In: Proceedings of the Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (JNLPBA-04). (2004)
24. Charniak, E.: Statistical parsing with a context-free grammar and word statistics.
In: Proceedings of the Fourteenth National Conference on Artificial Intelligence,
Menlo Park, AAAI Press/MIT Press (1997)
25. Park, J.C.: Using combinatory categorical grammar to extract biomedical infor-
mation. IEEE Intelligent Systems 16 (2001) 62?67
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 73?76,
New York, June 2006. c?2006 Association for Computational Linguistics
Early Deletion of Fillers In Processing Conversational Speech
Matthew Lease and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{mlease,mj}@cs.brown.edu
Abstract
This paper evaluates the benefit of deleting fillers
(e.g. you know, like) early in parsing conver-
sational speech. Readability studies have shown
that disfluencies (fillers and speech repairs) may
be deleted from transcripts without compromising
meaning (Jones et al, 2003), and deleting repairs
prior to parsing has been shown to improve its
accuracy (Charniak and Johnson, 2001). We ex-
plore whether this strategy of early deletion is also
beneficial with regard to fillers. Reported exper-
iments measure the effect of early deletion under
in-domain and out-of-domain parser training con-
ditions using a state-of-the-art parser (Charniak,
2000). While early deletion is found to yield only
modest benefit for in-domain parsing, significant
improvement is achieved for out-of-domain adap-
tation. This suggests a potentially broader role for
disfluency modeling in adapting text-based tools
for processing conversational speech.
1 Introduction
This paper evaluates the benefit of deleting fillers
early in parsing conversational speech. We follow
LDC (2004) conventions in using the term filler to
encompass a broad set of vocalized space-fillers that
can introduce syntactic (and semantic) ambiguity.
For example, in the questions
Did you know I do that?
Is it like that one?
colloquial use of fillers, indicated below through use
of commas, can yield alternative readings
Did, you know, I do that?
Is it, like, that one?
Readings of the first example differ in querying lis-
tener knowledge versus speaker action, while read-
ings of the second differ in querying similarity ver-
sus exact match. Though an engaged listener rarely
has difficulty distinguishing between such alterna-
tives, studies show that deleting disfluencies from
transcripts improves readability with no reduction in
reading comprehension (Jones et al, 2003).
The fact that disfluencies can be completely re-
moved without compromising meaning is important.
Earlier work had already made this claim regard-
ing speech repairs1 and argued that there was con-
sequently little value in syntactically analyzing re-
pairs or evaluating our ability to do so (Charniak
and Johnson, 2001). Moreover, this work showed
that collateral damage to parse accuracy caused by
repairs could be averted by deleting them prior to
parsing, and this finding has been confirmed in sub-
sequent studies (Kahn et al, 2005; Harper et al,
2005). But whereas speech repairs have received
significant attention in the parsing literature, fillers
have been relatively neglected. While one study
has shown that the presence of interjection and par-
enthetical constituents in conversational speech re-
duces parse accuracy (Engel et al, 2002), these con-
stituent types are defined to cover both fluent and
disfluent speech phenomena (Taylor, 1996), leaving
the impact of fillers alone unclear.
In our study, disfluency annotations (Taylor,
1995) are leveraged to identify fillers precisely, and
these annotations are merged with treebank syn-
tax. Extending the arguments of Charniak and John-
son with regard to repairs (2001), we argue there
is little value in recovering the syntactic structure
1See (Core and Schubert, 1999) for a prototypical counter-
example that rarely occurs in practice.
73
of fillers, and we relax evaluation metrics accord-
ingly (?3.2). Experiments performed (?3.3) use a
state-of-the-art parser (Charniak, 2000) to study the
impact of early filler deletion under in-domain and
out-of-domain (i.e. adaptation) training conditions.
In terms of adaptation, there is tremendous poten-
tial in applying textual tools and training data to
processing transcribed speech (e.g. machine trans-
lation, information extraction, etc.), and bleaching
speech data to more closely resemble text has been
shown to improve accuracy with some text-based
processing tasks (Rosenfeld et al, 1995). For our
study, a state-of-the-art filler detector (Johnson et al,
2004) is employed to delete fillers prior to parsing.
Results show parse accuracy improves significantly,
suggesting disfluency filtering may have a broad role
in enabling text-based processing of speech data.
2 Disfluency in Brief
In this section we give a brief introduction to disflu-
ency, providing an excerpt from Switchboard (Graff
and Bird, 2000) that demonstrates typical production
of repairs and fillers in conversational speech.
We follow previous work (Shriberg, 1994) in de-
scribing a repair in terms of three parts: the reparan-
dum (the material repaired), the corrected alteration,
and between these an optional interregnum (or edit-
ing term) consisting of one or more fillers. Our no-
tion of fillers encompasses filled pauses (e.g. uh,
um, ah) as well as other vocalized space-fillers
annotated by LDC (Taylor, 1995), such as you
know, i mean, like, so, well, etc. An-
notations shown here are typeset with the following
conventions: fillers are bold, [reparanda] are square-
bracketed, and alterations are underlined.
S1: Uh first um i need to know uh how
do you feel [about] uh about sending uh
an elderly uh family member to a nursing
home
S2: Well of course [it?s] you know it?s one
of the last few things in the world you?d
ever want to do you know unless it?s just
you know really you know uh [for their]
uh you know for their own good
Though disfluencies rarely complicate understand-
ing for an engaged listener, deleting them from tran-
scripts improves readability with no reduction in
reading comprehension (Jones et al, 2003). For au-
tomated analysis of speech data, this means we may
freely explore processing alternatives which delete
disfluencies without compromising meaning.
3 Experiments
This section reports parsing experiments studying
the effect of early deletion under in-domain and out-
of-domain parser training conditions using the Au-
gust 2005 release of the Charniak parser (2000). We
describe data and evaluation metrics used, then pro-
ceed to describe the experiments.
3.1 Data
Conversational speech data was drawn from the
Switchboard corpus (Graff and Bird, 2000), which
annotates disfluency (Taylor, 1995) as well as syn-
tax. Our division of the corpus follows that used
in (Charniak and Johnson, 2001). Speech recognizer
(ASR) output is approximated by removing punctua-
tion, partial words, and capitalization, but we do use
reference words, representing an upperbound condi-
tion of perfect ASR. Likewise, annotated sentence
boundaries are taken to represent oracle boundary
detection. Because fillers are annotated only in
disfluency markup, we perform an automatic tree
transform to merge these two levels of annotation:
each span of contiguous filler words were pruned
from their corresponding tree and then reinserted at
the same position under a flat FILLER constituent,
attached as highly as possible. Transforms were
achieved using TSurgeon2 and Lingua::Treebank3.
For our out-of-domain training condition, the
parser was trained on sections 2-21 of the Wall Street
Journal (WSJ) corpus (Marcus et al, 1993). Punctu-
ation and capitalization were removed to bleach our
our textual training data to more closely resemble
speech (Rosenfeld et al, 1995). We also tried auto-
matically changing numbers, symbols, and abbrevi-
ations in the training text to match how they would
be read (Roark, 2002), but this did not improve ac-
curacy and so is not discussed further.
3.2 Evaluation Metrics
As discussed earlier (?1), Charniak and John-
son (2001) have argued that speech repairs do not
2http://nlp.stanford.edu/software/tsurgeon.shtml
3http://www.cpan.org
74
contribute to meaning and so there is little value
in syntactically analyzing repairs or evaluating our
ability to do so. Consequently, they relaxed stan-
dard PARSEVAL (Black et al, 1991) to treat EDITED
constituents like punctuation: adjacent EDITED con-
stituents are merged, and the internal structure and
attachment of EDITED constituents is not evaluated.
We propose generalizing this approach to disfluency
at large, i.e. fillers as well as repairs. Note that the
details of appropriate evaluation metrics for parsed
speech data is orthogonal to the parsing methods
proposed here: however parsing is performed, we
should avoid wasting metric attention evaluating
syntax of words that do not contribute toward mean-
ing and instead evaluate only how well such words
can be identified.
Relaxed metric treatment of disfluency was
achieved via simple parameterization of the SPar-
seval tool (Harper et al, 2005). SParseval also
has the added benefit of calculating a dependency-
based evaluation alongside PARSEVAL?s bracket-
based measure. The dependency metric performs
syntactic head-matching for each word using a set
of given head percolation rules (derived from Char-
niak?s parser (2000)), and its relaxed formulation
ignores terminals spanned by FILLER and EDITED
constituents. We found this metric offered additional
insights in analyzing some of our results.
3.3 Results
In the first set of experiments, we train the parser on
Switchboard and contrast early deletion of disfluen-
cies (identified by an oracle) versus parsing in the
more usual fashion. Our method for early deletion
generalizes the approach used with repairs in (Char-
niak and Johnson, 2001): contiguous filler and edit
words are deleted from the input strings, the strings
are parsed, and the removed words are reinserted
into the output trees under the appropriate flat con-
stituent, FILLER or EDITED.
Results in Table 1 give F-scores for PARSEVAL
and dependency-based parse accuracy (?3.2), as well
as per-word edit and filler detection accuracy (i.e.
how well the parser does in identifying which termi-
nals should be spanned by EDITED and FILLER con-
stituents when early deletion is not performed). We
see that the parser correctly identifies filler words
with 93.1% f-score, and that early deletion of fillers
Table 1: F-scores on Switchboard when trained in-
domain. LB and Dep refer to relaxed labelled-
bracket and dependency parse metrics (?3.2). Edit
and filler word detection f-scores are also shown.
Edits Fillers Edit F Filler F LB Dep
oracle oracle 100.0 100.0 88.9 88.5
oracle parser 100.0 93.1 87.8 87.9
parser oracle 64.3 100.0 85.0 85.6
parser parser 62.4 94.1 83.9 85.0
(via oracle knowledge) yields only a modest im-
provement in parsing accuracy (87.8% to 88.9%
bracket-based, 87.9% to 88.5% dependency-based).
We conclude from this that for in-domain training,
early deletion of fillers has limited potential to im-
prove parsing accuracy relative to what has been
seen with repairs. It is still worth noting, however,
that the parser does perform better when fillers are
absent, consistent with Engel et al?s findings (2002).
While fillers have been reported to often occur at
major clause boundaries (Shriberg, 1994), suggest-
ing their presence may benefit parsing, we do not
find this to be the case. Results shown for repair de-
tection accuracy and its impact on parsing are con-
sistent with previous work (Charniak and Johnson,
2001; Kahn et al, 2005; Harper et al, 2005).
Our second set of experiments reports the effect
of deleting fillers early when the parser is trained on
text only (WSJ, ?3.1). Our motivation here is to see
if disfluency modeling, particularly filler detection,
can help bleach speech data to more closely resem-
ble text, thereby improving our ability to process it
using text-based methods and training data (Rosen-
feld et al, 1995). Again we contrast standard
parsing with deleting disfluencies early (via oracle
knowledge). Given our particular interest in fillers,
we also report the effect of detecting them via a
state-of-the-art system (Johnson et al, 2004).
Results appear in Table 2. It is worth noting that
since our text-trained parser never produces FILLER
or EDITED constituents, the bracket-based metric
penalizes it for each such constituent appearing in
the gold trees. Similarly, since the dependency
metric ignores terminals occurring under these con-
stituents in the gold trees, the metric penalizes the
parser for producing dependencies for these termi-
75
Table 2: F-scores parsing Switchboard when trained
on WSJ. Edit word detection varies between parser
and oracle, and filler word detection varies between
none, system (Johnson et al, 2004), and oracle.
Filler F, LB, and Dep are defined as in Table 1.
Edits Fillers Filler F LB Dep
oracle oracle 100.0 83.6 81.4
oracle detect 89.3 81.6 80.5
oracle none - 71.8 75.4
none oracle 100.0 76.3 76.7
none detect 74.6 75.9 91.3
none none - 66.8 71.5
nals. Taken together, the two metrics provide a com-
plementary perspective in interpreting results.
The trend observed across metrics and edit detec-
tion conditions shows that early deletion of system-
detected fillers improves parsing accuracy 5-10%.
As seen with in-domain training, early deletion of
repairs is again seen to have a significant effect.
Given that state-of-the-art edit detection performs at
about 80% f-measure (Johnson and Charniak, 2004),
much of the benefit derived here from oracle re-
pair detection should be realizable in practice. The
broader conclusion we draw from these results is
that disfluency modeling has significant potential to
improve text-based processing of speech data.
4 Conclusion
While early deletion of fillers has limited benefit for
in-domain parsing of speech data, it can play an im-
portant role in bleaching speech data for more accu-
rate text-based processing. Alternative methods of
integrating detected filler information, such as parse
reranking (Kahn et al, 2005), also merit investiga-
tion. It will also be important to evaluate the inter-
action with ASR error and sentence boundary de-
tection error. In terms of bleaching, we saw that
even with oracle detection of disfluency, our text-
trained model still significantly under-performed the
in-domain model, indicating additional methods for
bleaching are still needed. We also plan to evaluat-
ing the benefit of disfluency modeling in bleaching
speech data for text-based machine translation.
Acknowledgments
This work was supported by NSF grants 0121285, LIS9720368,
and IIS0095940, and DARPA GALE contract HR0011-06-2-
0001. We would like to thank Brian Roark, Mary Harper, and
the rest of the JHU PASSED team for its support of this work.
References
E. Charniak and M. Johnson. 2001. Edit detection and parsing
for transcribed speech. In Proc. NAACL, pages 118?126.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proc. NAACL, pages 132?139.
M.G. Core and L.K. Schubert. 1999. A syntactic framework
for speech repairs and other disruptions. In Proc. ACL, pages
413?420.
E. Black et al 1991. Procedure for quantitatively comparing
the syntactic coverage of English grammars. In Proc. Work-
shop on Speech and Natural Language, pages 306?311.
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing and
disfluency placement. In Proc. EMNLP, pages 49?54.
D. Graff and S. Bird. 2000. Many uses, many annotations for
large speech corpora: Switchboard and TDT as case studies.
In Proc. LREC, pages 427?433.
M. Harper et al 2005 Johns Hopkins Summer Workshop Final
Report on Parsing and Spoken Structural Event Detection.
J.G. Kahn et al 2005. Effective use of prosody in parsing
conversational speech. In Proc. HLT/EMNLP, 233?240.
M. Johnson and E. Charniak. 2004. A TAG-based noisy chan-
nel model of speech repairs. In Proc. ACL, pages 33?39.
M. Johnson, E. Charniak, and M. Lease. 2004. An improved
model for recognizing disfluencies in conversational speech.
In Proc. Rich Text 2004 Fall Workshop (RT-04F).
D. Jones et al 2003. Measuring the readability of automatic
speech-to-text transcripts. In Proc. Eurospeech, 1585?1588.
Linguistic Data Consortium (LDC). 2004. Simple metadata
annotation specification version 6.2.
M. Marcus et al 1993. Building a large annotated corpus of
English: The Penn Treebank. Computational Linguistics,
19(2): 313?330.
B. Roark. 2002. Markov parsing: Lattice rescoring with a sta-
tistical parser. In Proc. ACL, pages 287?294.
R. Rosenfeld et al 1995. Error analysis and disfluency mod-
eling in the Swichboard domain: 1995 JHU Summer Work-
shop project team report.
E. Shriberg. 1994. Preliminaries to a Theory of Speech Disflu-
encies. Ph.D. thesis, UC Berkeley.
A. Taylor, 1995. Revision of Meteer et al?s Dysfluency Annota-
tion Stylebook for the Switchboard Corpus. LDC.
A. Taylor, 1996. Bracketing Switchboard: An addendum to the
Treebank II Bracketing Guidelines. LDC.
76
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 161?168,
Sydney, July 2006. c?2006 Association for Computational Linguistics
PCFGs with Syntactic and Prosodic Indicators of Speech Repairs
John Halea Izhak Shafranb Lisa Yungc
Bonnie Dorrd Mary Harperde Anna Krasnyanskayaf Matthew Leaseg
Yang Liuh Brian Roarki Matthew Snoverd Robin Stewartj
a Michigan State University; b,c Johns Hopkins University; d University of Maryland, College Park; e Purdue University
f UCLA; g Brown University; h University of Texas at Dallas; i Oregon Health & Sciences University; j Williams College
Abstract
A grammatical method of combining two
kinds of speech repair cues is presented.
One cue, prosodic disjuncture, is detected
by a decision tree-based ensemble clas-
sifier that uses acoustic cues to identify
where normal prosody seems to be inter-
rupted (Lickley, 1996). The other cue,
syntactic parallelism, codifies the expec-
tation that repairs continue a syntactic
category that was left unfinished in the
reparandum (Levelt, 1983). The two cues
are combined in a Treebank PCFG whose
states are split using a few simple tree
transformations. Parsing performance on
the Switchboard and Fisher corpora sug-
gests that these two cues help to locate
speech repairs in a synergistic way.
1 Introduction
Speech repairs, as in example (1), are one kind
of disfluent element that complicates any sort
of syntax-sensitive processing of conversational
speech.
(1) and [ the first kind of invasion of ] the first
type of privacy seemed invaded to me
The problem is that the bracketed reparan-
dum region (following the terminology of Shriberg
(1994)) is approximately repeated as the speaker
The authors are very grateful for Eugene Charniak?s help
adapting his parser. We also thank the Center for Language
and Speech processing at Johns Hopkins for hosting the sum-
mer workshop where much of this work was done. This
material is based upon work supported by the National Sci-
ence Foundation (NSF) under Grant No. 0121285. Any opin-
ions, findings and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the views of the NSF.
?repairs? what he or she has already uttered.
This extra material renders the entire utterance
ungrammatical?the string would not be gener-
ated by a correct grammar of fluent English. In
particular, attractive tools for natural language
understanding systems, such as Treebank gram-
mars for written corpora, naturally lack appropri-
ate rules for analyzing these constructions.
One possible response to this mismatch be-
tween grammatical resources and the brute facts
of disfluent speech is to make one look more
like the other, for the purpose of parsing. In
this separate-processing approach, reparanda are
located through a variety of acoustic, lexical or
string-based techniques, then excised before sub-
mission to a parser (Stolcke and Shriberg, 1996;
Heeman and Allen, 1999; Spilker et al, 2000;
Johnson and Charniak, 2004). The resulting
parse tree then has the reparandum re-attached in
a standardized way (Charniak and Johnson, 2001).
An alternative strategy, adopted in this paper, is
to use the same grammar to model fluent speech,
disfluent speech, and their interleaving.
Such an integrated approach can use syntac-
tic properties of the reparandum itself. For in-
stance, in example (1) the reparandum is an
unfinished noun phrase, the repair a finished
noun phrase. This sort of phrasal correspon-
dence, while not absolute, is strong in conver-
sational speech, and cannot be exploited on the
separate-processing approach. Section 3 applies
metarules (Weischedel and Sondheimer, 1983;
McKelvie, 1998a; Core and Schubert, 1999) in
recognizing these correspondences using standard
context-free grammars.
At the same time as it defies parsing, con-
versational speech offers the possibility of lever-
aging prosodic cues to speech repairs. Sec-
161
Figure 1: The pause between two or s and the glottalization at the end of the first makes it easy for a
listener to identify the repair.
tion 2 describes a classifier that learns to label
prosodic breaks suggesting upcoming disfluency.
These marks can be propagated up into parse
trees and used in a probabilistic context-free gram-
mar (PCFG) whose states are systematically split
to encode the additional information.
Section 4 reports results on Switchboard (God-
frey et al, 1992) and Fisher EARS RT04F data,
suggesting these two features can bring about in-
dependent improvements in speech repair detec-
tion. Section 5 suggests underlying linguistic and
statistical reasons for these improvements. Sec-
tion 6 compares the proposed grammatical method
to other related work, including state of the art
separate-processing approaches. Section 7 con-
cludes by indicating a way that string- and tree-
based approaches to reparandum identification
could be combined.
2 Prosodic disjuncture
Everyday experience as well as acoustic anal-
ysis suggests that the syntactic interruption in
speech repairs is typically accompanied by a
change in prosody (Nakatani and Hirschberg,
1994; Shriberg, 1994). For instance, the spectro-
gram corresponding to example (2), shown in Fig-
ure 1,
(2) the jehovah?s witness or [ or ] mormons or
someone
reveals a noticeable pause between the occurrence
of the two ors, and an unexpected glottalization at
the end of the first one. Both kinds of cues have
been advanced as explanations for human listen-
ers? ability to identify the reparandum even before
the repair occurs.
Retaining only the second explanation, Lickley
(1996) proposes that there is no ?edit signal? per se
but that repair is cued by the absence of smooth
formant transitions and lack of normal juncture
phenomena.
One way to capture this notion in the syntax
is to enhance the input with a special disjunc-
ture symbol. This symbol can then be propa-
gated in the grammar, as illustrated in Figure 2.
This work uses a suffix ?+ to encode the percep-
tion of abnormal prosody after a word, along with
phrasal -BRK tags to decorate the path upwards to
reparandum constituents labeled EDITED. Such
NP
NP EDITED CC NP
NP NNP CC?BRK or NNPS
DT NNP POS witness
the jehovah ?s
or~+ mormons
Figure 2: Propagating BRK, the evidence of dis-
fluent juncture, from acoustics to syntax.
disjuncture symbols are identified in the ToBI la-
beling scheme as break indices (Price et al, 1991;
Silverman et al, 1992).
The availability of a corpus annotated with
ToBI labels makes it possible to design a break
index classifier via supervised training. The cor-
pus is a subset of the Switchboard corpus, con-
sisting of sixty-four telephone conversations man-
ually annotated by an experienced linguist accord-
ing to a simplified ToBI labeling scheme (Osten-
dorf et al, 2001). In ToBI, degree of disjuncture
is indicated by integer values from 0 to 4, where
a value of 0 corresponds to clitic and 4 to a major
phrase break. In addition, a suffix p denotes per-
ceptually disfluent events reflecting, for example,
162
hesitation or planning. In conversational speech
the intermediate levels occur infrequently and the
break indices can be broadly categorized into three
groups, namely, 1, 4 and p as in Wong et al
(2005).
A classifier was developed to predict three
break indices at each word boundary based on
variations in pitch, duration and energy asso-
ciated with word, syllable or sub-syllabic con-
stituents (Shriberg et al, 2005; Sonmez et al,
1998). To compute these features, phone-level
time-alignments were obtained from an automatic
speech recognition system. The duration of these
phonological constituents were derived from the
ASR alignment, while energy and pitch were com-
puted every 10ms with snack, a public-domain
sound toolkit (Sjlander, 2001). The duration, en-
ergy, and pitch were post-processed according to
stylization procedures outlined in Sonmez et al
(1998) and normalized to account for variability
across speakers.
Since the input vector can have missing val-
ues such as the absence of pitch during unvoiced
sound, only decision tree based classifiers were
investigated. Decision trees can handle missing
features gracefully. By choosing different com-
binations of splitting and stopping criteria, an
ensemble of decision trees was built using the
publicly-available IND package (Buntine, 1992).
These individual classifiers were then combined
into ensemble-based classifiers.
Several classifiers were investigated for detect-
ing break indices. On ten-fold cross-validation,
a bagging-based classifier (Breiman, 1996) pre-
dicted prosodic breaks with an accuracy of 83.12%
while chance was 67.66%. This compares favor-
ably with the performance of the supervised classi-
fiers on a similar task in Wong et al (2005). Ran-
dom forests and hidden Markov models provide
marginal improvements at considerable computa-
tional cost (Harper et al, 2005).
For speech repair, the focus is on detecting dis-
fluent breaks. The precision and recall trade-off
on its detection can be adjusted using a thresh-
old on the posterior probability of predicting ?p?,
as shown in Figure 3.
In essence, the large number of acoustic and
prosodic features related to disfluency are encoded
via the ToBI label ?p?, and provided as additional
observations to the PCFG. This is unlike previous
work on incorporating prosodic information (Gre-
00.10.20.30.40.50.6 0
0.1
0.2
0.3
0.4
0.5
0.6
Probability of Miss
Probab
ility of 
False 
Alarm
Figure 3: DET curve for detecting disfluent breaks
from acoustics.
gory et al, 2004; Lease et al, 2005; Kahn et al,
2005) as described further in Section 6.
3 Syntactic parallelism
The other striking property of speech repairs is
their parallel character: subsequent repair regions
?line up? with preceding reparandum regions. This
property can be harnessed to better estimate the
length of the reparandum by considering paral-
lelism from the perspective of syntax. For in-
stance, in Figure 4(a) the unfinished reparandum
noun phrase is repaired by another noun phrase ?
the syntactic categories are parallel.
3.1 Levelt?s WFR and Conjunction
The idea that the reparandum is syntactically par-
allel to the repair can be traced back to Levelt
(1983). Examining a corpus of Dutch picture de-
scriptions, Levelt proposes a bi-conditional well-
formedness rule for repairs (WFR) that relates the
structure of repairs to the structure of conjunc-
tions. The WFR conceptualizes repairs as the con-
junction of an unfinished reparandum string (?)
with a properly finished repair (?). Its original
formulation, repeated here, ignores optional inter-
regna like ?er? or ?I mean.?
Well-formedness rule for repairs (WFR) A re-
pair ???? is well-formed if and only if there
is a string ? such that the string ??? and? ??
is well-formed, where ? is a completion of
the constituent directly dominating the last
element of ?. (and is to be deleted if that
last element is itself a sentence connective)
In other words, the string ? is a prefix of a phrase
whose completion, ??if it were present?would
163
render the whole phrase ?? grammatically con-
joinable with the repair ?. In example (1) ? is the
string ?the first kind of invasion of?, ? is ?the first
type of privacy? and ? is probably the single word
?privacy.?
This kind of conjoinability typically requires
the syntactic categories of the conjuncts to be the
same (Chomsky, 1957, 36). That is, a rule schema
such as (2) where X is a syntactic category, is pre-
ferred over one where X is not constrained to be
the same on either side of the conjunction.
X ? X Conj X (2)
If, as schema (2) suggests, conjunction does fa-
vor like-categories, and, as Levelt suggests, well-
formed repairs are conjoinable with finished ver-
sions of their reparanda, then the syntactic cate-
gories of repairs ought to match the syntactic cat-
egories of (finished versions of) reparanda.
3.2 A WFR for grammars
Levelt?s WFR imposes two requirements on a
grammar
? distinguishing a separate category of ?unfin-
ished? phrases
? identifying a syntactic category for reparanda
Both requirements can be met by adapting Tree-
bank grammars to mirror the analysis of McK-
elvie1 (1998a; 1998b). McKelvie derives phrase
structure rules for speech repairs from fluent rules
by adding a new feature called abort that can
take values true and false. For a given gram-
mar rule of the form
A ? B C
a metarule creates other rules of the form
A [abort = Q] ?
B [abort = false] C [abort = Q]
where Q is a propositional variable. These rules
say, in effect, that the constituent A is aborted just
in case the last daughter C is aborted. Rules that
don?t involve a constant value for Q ensure that the
same value appears on parents and children. The
1McKelvie?s metarule approach declaratively expresses
Hindle?s (1983) Stack Editor and Category Copy Editor rules.
This classic work effectively states the WFR as a program for
the Fidditch deterministic parser.
WFR is then implemented by rule schemas such
as (3)
X ? X [abort = true] (AFF) X (3)
that permit the optional interregnum AFF to con-
join an unfinished X-phrase (the reparandum) with
a finished X-phrase (the repair) that comes after it.
3.3 A WFR for Treebanks
McKelvie?s formulation of Levelt?s WFR can be
applied to Treebanks by systematically recoding
the annotations to indicate which phrases are un-
finished and to distinguish matching from non-
matching repairs.
3.3.1 Unfinished phrases
Some Treebanks already mark unfinished
phrases. For instance, the Penn Treebank pol-
icy (Marcus et al, 1993; Marcus et al, 1994) is
to annotate the lowest node that is unfinished with
an -UNF tag as in Figure 4(a).
It is straightforward to propagate this mark up-
wards in the tree from wherever it is annotated to
the nearest enclosing EDITED node, just as -BRK
is propagated upwards from disjuncture marks on
individual words. This percolation simulates the
action of McKelvie?s [abort = true]. The re-
sulting PCFG is one in which distributions on
phrase structure rules with ?missing? daughters are
segregated from distributions on ?complete? rules.
3.4 Reparanda categories
The other key element of Levelt?s WFR is the
idea of conjunction of elements that are in some
sense the same. In the Penn Treebank annota-
tion scheme, reparanda always receive the label
EDITED. This means that the syntactic category
of the reparandum is hidden from any rule which
could favor matching it with that of the repair.
Adding an additional mark on this EDITED node
(a kind of daughter annotation) rectifies the situ-
ation, as depicted in Figure 4(b), which adds the
notation -childNP to a tree in which the unfin-
ished tags have been propagated upwards. This
allows a Treebank PCFG to represent the general-
ization that speech repairs tend to respect syntactic
category.
4 Results
Three kinds of experiments examined the effec-
tiveness of syntactic and prosodic indicators of
164
SCC EDITED NP
and NP NP
NP PP
DT JJ NN IN NP
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(a) The lowest unfinished node is given.
S
CC EDITED?childNP NP
and NP?UNF NP
NP PP?UNF
DT JJ NN IN NP?UNF
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(b) -UNF propagated, daughter-annotated Switchboard tree
Figure 4: Input (a) and output (b) of tree transformations.
speech repairs. The first two use the CYK algo-
rithm to find the most likely parse tree on a gram-
mar read-off from example trees annotated as in
Figures 2 and 4. The third experiment measures
the benefit from syntactic indicators alone in Char-
niak?s lexicalized parser (Charniak, 2000). The ta-
bles in subsections 4.1, 4.2, and 4.3 summarize
the accuracy of output parse trees on two mea-
sures. One is the standard Parseval F-measure,
which tracks the precision and recall for all labeled
constituents as compared to a gold-standard parse.
The other measure, EDIT-finding F, restricts con-
sideration to just constituents that are reparanda. It
measures the per-word performance identifying a
word as dominated by EDITED or not. As in pre-
vious studies, reference transcripts were used in all
cases. A check (
?
) indicates an experiment where
prosodic breaks where automatically inferred by
the classifier described in section 2, whereas in the
(?) rows no prosodic information was used.
4.1 CYK on Fisher
Table 1 summarizes the accuracy of a stan-
dard CYK parser on the newly-treebanked
Fisher corpus (LDC2005E15) of phone conver-
sations, collected as part of the DARPA EARS
program. The parser was trained on the entire
Switchboard corpus (ca. 107K utterances) then
tested on the 5368-utterance ?dev2? subset of the
Fisher data. This test set was tagged using MX-
POST (Ratnaparkhi, 1996) which was itself trained
on Switchboard. Finally, as described in section 2
these tags were augmented with a special prosodic
break symbol if the decision tree rated the proba-
bility a ToBI ?p? symbol higher than the threshold
value of 0.75.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 66.54 22.9?
66.08 26.1
daughter annotation ? 66.41 29.4? 65.81 31.6
-UNF propagation ? 67.06 31.5? 66.45 34.8
both ? 69.21 40.2? 67.02 40.6
Table 1: Improvement on Fisher, MXPOSTed tags.
The Fisher results in Table 1 show that syntac-
tic and prosodic indicators provide different kinds
of benefits that combine in an additive way. Pre-
sumably because of state-splitting, improvement
in EDIT-finding comes at the cost of a small decre-
ment in overall parsing performance.
4.2 CYK on Switchboard
Table 2 presents the results of similar experi-
ments on the Switchboard corpus following the
165
train/dev/test partition of Charniak and Johnson
(2001). In these experiments, the parser was given
correct part-of-speech tags as input.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 70.92 18.2?
69.98 22.5
daughter annotation ? 71.13 25.0? 70.06 25.5
-UNF propagation ? 71.71 31.1? 70.36 30.0
both ? 71.16 41.7? 71.05 36.2
Table 2: Improvement on Switchboard, gold tags.
The Switchboard results demonstrate independent
improvement from the syntactic annotations. The
prosodic annotation helps on its own and in com-
bination with the daughter annotation that imple-
ments Levelt?s WFR.
4.3 Lexicalized parser
Finally, Table 3 reports the performance of Char-
niak?s non-reranking, lexicalized parser on the
Switchboard corpus, using the same test/dev/train
partition.
Annotation Parseval F EDIT F
baseline 83.86 57.6
daughter annotation 80.85 67.2
-UNF propagation 81.68 64.7
both 80.16 70.0
flattened EDITED 82.13 64.4
Table 3: Charniak as an improved EDIT-finder.
Since Charniak?s parser does its own tagging,
this experiment did not examine the utility of
prosodic disjuncture marks. However, the com-
bination of daughter annotation and -UNF prop-
agation does lead to a better grammar-based
reparandum-finder than parsers trained on flat-
tened EDITED regions. More broadly, the re-
sults suggest that Levelt?s WFR is synergistic with
the kind of head-to-head lexical dependencies that
Charniak?s parser uses.
5 Discussion
The pattern of improvement in tables 1, 2, and
3 from none or baseline rows where no syntac-
tic parallelism or break index information is used,
to subsequent rows where it is used, suggest why
these techniques work. Unfinished-category an-
notation improves performance by preventing the
grammar of unfinished constituents from being
polluted by the grammar of finished constituents.
Such purification is independent of the fact that
rules with daughters labeled EDITED-childXP
tend to also mention categories labeled XP fur-
ther to the right (or NP and VP, when XP starts
with S). This preference for syntactic parallelism
can be triggered either by externally-suggested
ToBI break indices or grammar rules annotated
with -UNF. The prediction of a disfluent break
could be further improved by POS features and N-
gram language model scores (Spilker et al, 2001;
Liu, 2004).
6 Related Work
There have been relatively few attempts to harness
prosodic cues in parsing. In a spoken language
system for VERBMOBIL task, Batliner and col-
leagues (2001) utilize prosodic cues to dramati-
cally reduce lexical analyses of disfluencies in a
end-to-end real-time system. They tackle speech
repair by a cascade of two stages ? identification of
potential interruption points using prosodic cues
with 90% recall and many false alarms, and the
lexical analyses of their neighborhood. Their ap-
proach, however, does not exploit the synergy be-
tween prosodic and syntactic features in speech re-
pair. In Gregory et al (2004), over 100 real-valued
acoustic and prosodic features were quantized into
a heuristically selected set of discrete symbols,
which were then treated as pseudo-punctuation in
a PCFG, assuming that prosodic cues function like
punctuation. The resulting grammar suffered from
data sparsity and failed to provide any benefits.
Maximum entropy based models have been more
successful in utilizing prosodic cues. For instance,
in Lease et al (2005), interruption point probabil-
ities, predicted by prosodic classifiers, were quan-
tized and introduced as features into a speech re-
pair model along with a variety of TAG and PCFG
features. Towards a clearer picture of the inter-
action with syntax and prosody, this work uses
ToBI to capture prosodic cues. Such a method is
analogous to Kahn et al (2005) but in a genera-
tive framework.
The TAG-based model of Johnson and Charniak
(2004) is a separate-processing approach that rep-
166
resents the state of the art in reparandum-finding.
Johnson and Charniak explicitly model the
crossed dependencies between individual words
in the reparandum and repair regions, intersect-
ing this sequence model with a parser-derived lan-
guage model for fluent speech. This second step
improves on Stolcke and Shriberg (1996) and Hee-
man and Allen (1999) and outperforms the specific
grammar-based reparandum-finders tested in sec-
tion 4. However, because of separate-processing
the TAG channel model?s analyses do not reflect
the syntactic structure of the sentence being ana-
lyzed, and thus that particular TAG-based model
cannot make use of properties that depend on the
phrase structure of the reparandum region. This
includes the syntactic category parallelism dis-
cussed in section 3 but also predicate-argument
structure. If edit hypotheses were augmented to
mention particular tree nodes where the reparan-
dum should be attached, such syntactic paral-
lelism constraints could be exploited in the rerank-
ing framework of Johnson et al (2004).
The approach in section 3 is more closely re-
lated to that of Core and Schubert (1999) who
also use metarules to allow a parser to switch from
speaker to speaker as users interrupt one another.
They describe their metarule facility as a modi-
fication of chart parsing that involves copying of
specific arcs just in case specific conditions arise.
That approach uses a combination of longest-first
heuristics and thresholds rather than a complete
probabilistic model such as a PCFG.
Section 3?s PCFG approach can also be viewed
as a declarative generalization of Roark?s (2004)
EDIT-CHILD function. This function helps an
incremental parser decide upon particular tree-
drawing actions in syntactically-parallel contexts
like speech repairs. Whereas Roark conditions the
expansion of the first constituent of the repair upon
the corresponding first constituent of the reparan-
dum, in the PCFG approach there exists a separate
rule (and thus a separate probability) for each al-
ternative sequence of reparandum constituents.
7 Conclusion
Conventional PCFGs can improve their detection
of speech repairs by incorporating Lickley?s hy-
pothesis about interrupted prosody and by im-
plementing Levelt?s well-formedness rule. These
benefits are additive.
The strengths of these simple tree-based tech-
niques should be combinable with sophisticated
string-based (Johnson and Charniak, 2004; Liu,
2004; Zhang and Weng, 2005) approaches by
applying the methods of Wieling et al (2005)
for constraining parses by externally-suggested
brackets.
References
L. Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
W. Buntine. 1992. Tree classication software. In Tech-
nology 2002: The Third National Technology Trans-
fer Conference and Exposition, Baltimore.
E. Charniak and M. Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings
of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-00, pages 132?
139.
N. Chomsky. 1957. Syntactic Structures. Anua Lin-
guarum Series Minor 4, Series Volume 4. Mouton
de Gruyter, The Hague.
M. G. Core and L. K. Schubert. 1999. A syntactic
framework for speech repairs and other disruptions.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 413?
420.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proceedings of ICASSP,
volume I, pages 517?520, San Francisco.
M. Gregory, M. Johnson, and E. Charniak. 2004.
Sentence-internal prosody does not help parsing the
way punctuation does. In Proceedings of North
American Association for Computational Linguis-
tics.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, and L. Yung. 2005.
Parsing and spoken structural event detection. In
2005 Johns Hopkins Summer Workshop Final Re-
port.
P. A. Heeman and J. F. Allen. 1999. Speech repairs,
intonational phrases and discourse markers: model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?571.
D. Hindle. 1983. Deterministic parsing of syntactic
non-fluencies. In Proceedings of the ACL.
M. Johnson and E. Charniak. 2004. A TAG-based
noisy channel model of speech repairs. In Proceed-
ings of ACL, pages 33?39.
167
M. Johnson, E. Charniak, and M. Lease. 2004. An im-
proved model for recognizing disfluencies in conver-
sational speech. In Proceedings of Rich Transcrip-
tion Workshop.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in
parsing conversational speech. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 233?240.
M. Lease, E. Charniak, and M. Johnson. 2005. Pars-
ing and its applications for conversational speech. In
Proceedings of ICASSP.
W. J. M. Levelt. 1983. Monitoring and self-repair in
speech. Cognitive Science, 14:41?104.
R. J. Lickley. 1996. Juncture cues to disfluency. In
Proceedings the International Conference on Speech
and Language Processing.
Y. Liu. 2004. Structural Event Detection for Rich
Transcription of Speech. Ph.D. thesis, Purdue Uni-
versity.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger. 1994. The Penn Treebank: Annotating
Predicate Argument Structure. In Proceedings of
the 1994 ARPA Human Language Technology Work-
shop.
D. McKelvie. 1998a. SDP ? Spoken Dialog Parser.
ESRC project on Robust Parsing and Part-of-speech
Tagging of Transcribed Speech Corpora, May.
D. McKelvie. 1998b. The syntax of disfluency in spon-
taneous spoken language. ESRC project on Robust
Parsing and Part-of-speech Tagging of Transcribed
Speech Corpora, May.
C. Nakatani and J. Hirschberg. 1994. A corpus-based
study of repair cues in spontaneous speech. Journal
of the Acoustical Society of America, 95(3):1603?
1616, March.
M. Ostendorf, I. Shafran, S. Shattuck-Hufnagel,
L. Carmichael, and W. Byrne. 2001. A prosodically
labelled database of spontaneous speech. In Proc.
ISCA Tutorial and Research Workshop on Prosody
in Speech Recognition and Understanding, pages
119?121.
P. Price, M. Ostendorf, S. Shattuck-Hufnagel, and
C. Fong. 1991. The use of prosody in syntactic
disambiguation. Journal of the Acoustic Society of
America, 90:2956?2970.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of Empirical Methods
in Natural Language Processing Conference, pages
133?141.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
E. Shriberg, L. Ferrer, S. Kajarekar, A. Venkataraman,
and A. Stolcke. 2005. Modeling prosodic feature
sequences for speaker recognition. Speech Commu-
nication, 46(3-4):455?472.
E. Shriberg. 1994. Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, UC Berkeley.
H. F. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,
C. Wightman, P. Price, J. Pierrehumbert, and J. Hir-
shberg. 1992. ToBI: A standard for labeling English
prosody. In Proceedings of ICSLP, volume 2, pages
867?870.
K. Sjlander, 2001. The Snack sound visualization mod-
ule. Royal Institute of Technology in Stockholm.
http://www.speech.kth.se/SNACK.
K. Sonmez, E. Shriberg, L. Heck, and M. Weintraub.
1998. Modeling dynamic prosodic variation for
speaker verification. In Proceedings of ICSLP, vol-
ume 7, pages 3189?3192.
Jo?rg Spilker, Martin Klarner, and Gu?nther Go?rz. 2000.
Processing self-corrections in a speech-to-speech
system. In Wolfgang Wahlster, editor, Verbmobil:
Foundations of speech-to-speech translation, pages
131?140. Springer-Verlag, Berlin.
J. Spilker, A. Batliner, and E. No?th. 2001. How to
repair speech repairs in an end-to-end system. In
R. Lickley and L. Shriberg, editors, Proc. of ISCA
Workshop on Disfluency in Spontaneous Speech,
pages 73?76.
A. Stolcke and E. Shriberg. 1996. Statistical language
modeling for speech disfluencies. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 405?408, At-
lanta, GA.
R. M. Weischedel and N. K. Sondheimer. 1983.
Meta-rules as a basis for processing ill-formed in-
put. American Journal of Computational Linguis-
tics, 9(3-4):161?177.
M. Wieling, M-J. Nederhof, and G. van Noord. 2005.
Parsing partially bracketed input. Talk presented at
Computational Linguistics in the Netherlands.
D. Wong, M. Ostendorf, and J. G. Kahn. 2005. Us-
ing weakly supervised learning to improve prosody
labeling. Technical Report UWEETR-2005-0003,
University of Washington Electrical Engineering
Dept.
Q. Zhang and F. Weng. 2005. Exploring features for
identifying edited regions in disfluent sentences. In
Proceedings of the Nineth International Workshop
on Parsing Technologies, pages 179?185.
168
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 172?179,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Crowdsourcing Document Relevance Assessment with Mechanical Turk
Catherine Grady and Matthew Lease
School of Information
University of Texas at Austin
{cgrady,ml}@ischool.utexas.edu
Abstract
We investigate human factors involved in de-
signing effective Human Intelligence Tasks
(HITs) for Amazon?s Mechanical Turk1. In
particular, we assess document relevance to
search queries via MTurk in order to evaluate
search engine accuracy. Our study varies four
human factors and measures resulting experi-
mental outcomes of cost, time, and accuracy
of the assessments. While results are largely
inconclusive, we identify important obstacles
encountered, lessons learned, related work,
and interesting ideas for future investigation.
Experimental data is also made publicly avail-
able for further study by the community2.
1 Introduction
Evaluating accuracy of new search algorithms on
ever-growing information repositories has become
increasingly challenging in terms of the time and
expense required by traditional evaluation tech-
niques. In particular, while the Cranfield evalua-
tion paradigm has proven remarkably effective for
decades (Voorhees, 2002), enormous manual effort
is involved in assessing topic relevance of many dif-
ferent documents to many different queries. Conse-
quently, there has been significant recent interest in
developing more scalable evaluation methodology.
This has included developing robust accuracy met-
rics using few assessments (Buckley and Voorhees,
2004), inferring implicit relevance assessments from
1http://aws.amazon.com/mturk
2http://www.ischool.utexas.edu/?ml/data
user behavior (Joachims, 2002), more carefully se-
lecting documents for assessment (Aslam and Pavlu,
2008; Carterette et al, 2006), and leveraging crowd-
sourcing (Alonso et al, 2008).
We build on this line of work to investigat-
ing crowdsourcing-based relevance assessment via
MTurk. While MTurk has quickly become popular
as a means of obtaining data annotations quickly and
inexpensively (Snow et al, 2008), relatively little at-
tention has been given to addressing human-factors
involved in crowdsourcing and their impact on re-
sultant cost, time, and accuracy of the annotations
obtained (Mason and Watts, 2009). The advent of
crowdsourcing has led to many researchers, whose
work might otherwise fall outside the realm of
human-computer interaction (HCI), suddenly find-
ing themselves creating HITs for MTurk and thereby
directly confronting important issues of interface de-
sign and usability which could significantly impact
the quality or quantity of annotations they obtain. A
similar observation has been made recently regard-
ing the importance of effective HCI for obtaining
quality answers from users in a social search set-
ting (Horowitz and Kamvar, 2010).
Our overarching hypothesis is that better address-
ing human factors in HIT design can yield signifi-
cantly reduce cost, reduce time, and/or increase ac-
curacy of the annotations obtained via crowdsourc-
ing. Such improvement could come through a va-
riety of complimentary effects, such as attracting
more or better workers, incentivizing them to do bet-
ter work, better explaining the task to be performed
and reducing confusion, etc. While the results of
this study are largely inconclusivewith regard to our
172
experimental hypothesis, other contributions of the
work are identified in the abstract above.
2 Background
To evaluate search accuracy in the Cranfield
paradigm (Voorhees, 2002), a predefined set of doc-
uments (e.g., web pages) are typically manually as-
sessed for relevance with respect to some fixed set
of topics. Each topic corresponds to some static
information need of a hypothetical user. Because
language allows meaning to be conveyed in vari-
ous ways and degrees of brevity, each topic can be
expressed via a myriad of different queries. Ta-
ble 1 shows the four topics used in our study which
were generated by NIST for TREC3. We do use
the paragraph-length ?narrative? queries under an
(untested) assumption that they are overly complex
and technical for a layman assessor. Instead, we
use (1) the short keyword ?title? queries and (2)
more verbose and informative ?description? queries,
which are typically expressed as a one-sentence
question or statement.
NIST has typically invested significant time train-
ing annotators, something far less feasible in a
crowdsourced setting. NIST has also typically em-
ployed a single human assessor per topic to en-
sure consistent topic interpretation and relevance as-
sessment. One downside of this practice is limited
scalability of annotation, particularly in a crowd-
sourced setting. When multiple annotators have
been used, previous studies have also found rela-
tively low inner-annotator agreement for relevance
assessment due to the highly subjective nature of
relevance (Voorhees, 2002). Thus in addition to re-
ducing time and cost of assessment, crowdsourcing
may also enable us to improve assessment accuracy
by integrating assessment decisions by a commit-
tee of annotators. This is particularly important for
generating reusable test collections for benchmark-
ing. Practical costs involved in relevance assess-
ment based on standard pooling methods is signif-
icant and becoming increasingly prohibitive as col-
lection sizes grow (Carterette et al, 2009).
MTurk allows ?requesters? to crowdsource large
numbers of HITs online which workers can search,
browse, preview, accept, and complete or abandon.
3http://trec.nist.gov
3. Joint Ventures. Document will announce a new joint
venture involving a Japanese company.
13. Mitsubishi Heavy Industries Ltd. Document refers to
Mitusbishi Heavy Industries Ltd.
68. Health Hazards from Fine-Diameter Fibers. Docu-
ment will report actual studies, or even unsubstantiated con-
cerns about the safety to manufacturing employees and in-
stallation workers of fine-diameter fibers used in insulation
and other products.
78. Greenpeace. Document will report activity by Green-
peace to carry out their environmental protection goals.
Table 1: The four TREC topics used in our study. Topic
number and <title> field are shown in bold. Remain-
ing text constitutes the description (<desc>) field.
With regard to measuring the impact of different
design alternatives on resulting HIT effectiveness,
MTurk provides requesters with many useful statis-
tics regarding completion of their HITs. Some ef-
fects cannot be measured, however, such as when
HITs are skipped, when HITs are viewed in search
results but not selected, and other outcomes which
could usefully inform effective HIT design.
3 Methodology
Our study investigated how varying certain aspects
of HIT design affected annotation accuracy and
time, as well as the relationship between expense
and these outcomes. In particular, workers were
asked to make binary assessments regarding the rel-
evance of various documents to different queries.
3.1 Experimental Variables
We varied four simple aspects of HIT design:
? Query: <title> vs. <desc>
? Terminology: HIT title of ?binary relevance judg-
ment? (technical) vs. ?yes/no decision? (layman)
? Pay: $0.01 vs. $0.02
? Bonus: no bonus offered vs. $0.02
The Query is clearly central to relevance assess-
ment since it provides the annotator?s primary ba-
sis for judging relevance. Since altering a query
can have enormous impact on the assessment, and
because we were testing the ability of Mechanical
Turk workers to replicate assessments made previ-
ously by TREC assessors, we preserved wording of
173
the queries as they appeared in the original TREC
topics (see ?2). We hypothesized that the greater de-
tail found in the topic description vs. the title would
improve accuracy with some corresponding increase
in HIT completion time (longer query to read, at
times with more stilted language, and more specific
relevance criteria requiring more careful reading of
documents). An alternative hypothesis would be
that a very conscientious worker might take longer
wrestling with a vague title query.
Terminology: the HIT title is arguably one of a
HIT?s more prominent features since it is one of the
first (and often the only) description of a HIT a po-
tential worker sees. An attractive title could conceiv-
ably draw workers to a task while an unattractive one
could repel them. Besides the simple variation stud-
ied here, future experiments could test other aspects
of title formulation. For example, greater specificity
as to the content of documents or topics within the
HIT could attract workers that are knowledgeable or
interested in a particular subject. Additionally, a title
that indicates a task is for research purposes might
attract workers motivated to contribute to society.
Pay: the base pay rate has obvious implications
for attracting workers and incentivizing them to do
quality work. While anecdotal knowledge suggested
the ?going rate? for simple HITs was about $0.02,
we started at the lowest possible rate and increased
from there. Although higher pay rates are certainly
more attractive to legitimate workers, they also tend
to attract more spammers, so determining appropri-
ate pay is something of a careful balancing act.
Bonus: Two important questions are 1) How does
knowing that one could receive a bonus affect per-
formance on the current HIT?, and 2) How does ac-
tually receiving a bonus affect performance on fu-
ture HITs? We focused on the first question. When
bonuses were offered, we both advertised this fact
in the HIT title (see Title 4 above) and appended the
following statement to the instructions: ?[b]onuses
will be given for good work with good explana-
tions of the reasoning behind your relevance assess-
ment.? If a worker?s explanation made clear why
she made the relevance judgment she did, bonuses
were awarded regardless of the assessment?s correct-
ness with regard to ground truth. Decisions to award
bonus pay were made manually (see ?5).
3.2 Experimental Constants
Various factors kept constant in our study could also
be interesting to investigate in future work:
? Description: the worker may optionally view a
brief description of the task before accepting the
HIT. For all HITs, our description was simply: ?(1)
Decide whether a document is relevant to a topic, 2)
Click ?relevant? or ?not relevant?, and 3) Submit?.
? Keywords: HITs were advertised for search via
keywords ?judgment, document, relevance, search?
? Duration: once accepted, all HITs had to be com-
pleted within one hour
? Approval Rate: workers had to have a 95% ap-
proval rate to accept our HITs
? HIT approval: all HITs were accepted, but ap-
proval was not immediate to suggest that HITs were
being carefully reviewed before pay was awarded
? Feedback to workers: none given
More careful selection of high-interest Keywords
(e.g., ?easy? or ?fun?) may be a surprisingly effec-
tive way to attract more workers. It would be very
interesting to analyze the query logs for keywords
used by Workers in searching for HITs of interest.
Omar Alonso suggests workers should always
be paid (personal communication). Given the low
cost involved, keeping Workers individually happy
avoids the effort of having to justify rejections to an-
gry Workers, maintains one?s reputation for attract-
ing Workers, and still allows problematic workers to
be filtered out in future batches.
3.3 Experimental Outcomes
With regard to outcomes, we were principally in-
terested in measuring accuracy, time, and expense.
Base statistics, such as the completion time of a par-
ticular HIT, allowed us to compute derived statis-
tics like averages per topic, per Worker, per Batch,
per experimental variable, etc. We could then also
look for correlations between outcomes as well as
between experimental variables and outcomes.
Accuracy was measured by simply computing the
annotator mean accuracy with regard to ?ground
truth? binary relevance labels from NIST. A vari-
ety of other possibilities exist, such as deciding bi-
nary annotations by majority vote and comparing
these to ground truth. Recent work has explored en-
semble methods for weighting and combining anno-
174
Topic Relevant Non-Relevant
3 48, 55, 84, 120 85
13 28, 30 *193*, 84, 117
68 157, 163, 170, 182, 186
78 *9978* 134, 166, 167,*0062*
Table 2: Documents assessed per topic, along with ?true?
binary relevance judgments according to official TREC
NIST annotation. Document prefixes used in table: (3
and 13) WSJ920324-, except *WSJ920323-0193*,
(68 and 78) AP901231- except *FBIS4-9978* and
*WSJ920324-0062*. Only one document, 84, was
shared across queries (3 and 13).
# Name Query Term. Pay Bonus
1 Baseline title BRJ $0.01 -
2 P=0.02 title BRJ $0.02 -
3 T=yes/no title yes/no $0.01 -
4 Q=desc. desc. yes/no $0.01 -
5 B=0.02 title yes/no $0.01 $0.02
Table 3: Experimental matrix. Batches 2 and 3 changed
one variable with respect to Batch 1. Batches 4 and 5
changed one variable with respect to Batch 3. Terminol-
ogy varied as specified in ?3. For batch 5, 23 bonuses
were awarded at total cost of $0.46.
tations (Snow et al, 2008; Whitehill et al, 2009)
which also could have been used like majority vote.
As for time, we measured HIT completion time
(from acceptance to completion) and Batch com-
pletion time (from publishing the Batch to all its
HITs being completed). We only anecdotally mea-
sured our own time required to generate HIT de-
signs, shepherd the Batches, assess outcomes, etc.
Cost was measured solely with respect to what
was paid to Workers and does not include overhead
costs charged by Amazon (?2). We also did not ac-
count for the cost of our own salaries, equipment, or
other indirect expenses associated with the work.
3.4 Additional Details
Assessment was performed on XML documents
taken from the TREC TIPSTER collection of news
articles. Documents were simply presented as text
after simple pre-processing; a better alternative for
the future would be to associate an attractive style
sheet with the XML to enhance readability and at-
tractiveness of HITs. Relatively little pre-processing
HITs per Worker
0
10
20
30
40
50
60
0
10
20
30
40
50
60
Figure 1: Number of HITs completed by each worker
was performed: (1) XML tags were replaced with
HTML, (2) document ID, number, and TREC-
related info was commented out, and (3) paragraph
tags were added to break up text.
Our basic HIT layout was based on a pre-existing
template for assessing binary relevance provided by
Omar Alonso (personal communication). This tem-
plate reflected several useful design decisions like
having HITs be self-contained rather than referring
to content at an external URL, a design previously
found to be effective (Alonso et al, 2008).
4 Evaluation
We performed five batch evaluations, shown in Ta-
ble 3. For each of the four topics shown in Ta-
ble 1, five documents were assessed (Table 2), and
ten assessments (one per HIT) were collected for
each document. Each batch therefore consisted of
4 ? 5 ? 10 = 200 HITs, for an overall total of 1000
HITs. Document length varied from 162 words to
2129 words per document (including HTML tags
and single-character tokens). Each HIT required the
worker to make a single binary relevance judgment
(i.e. relevant or non-relevant) for a given query-
document pair. In all cases, ?ground truth? was
available to us in the form of prior relevance assess-
ments created by NIST. 149 unique Workers com-
175
0 10 20 30 40 50 60
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
#HITs
Ac
cu
ra
cy
Figure 2: HITs completed vs. accuracy achieved shows
negligible direct correlation: Pearson |?| < 0.01.
0 200 400 600
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Seconds
Ac
cu
ra
cy
Figure 3: HIT completion time vs. accuracy achieved
shows negligible direct correlation: Pearson |?| ? 0.06.
pleted the 1000 HITs, with some Workers complet-
ing far more HITs than others (Figure 1).
We did not restrict Workers from accepting HITs
from different batches, and some Workers even par-
ticipated in all 5 Batches. Since in some cases a
single Worker assessed the same query-document
pair multiple times, our results likely reflect unan-
ticipated effects of training or fatigue (see ?5).
Statistical significance was measured via a two-
tailed unpaired t-test. The only significant outcomes
observed were increase in comment length and num-
ber of comments for higher-payingor bonus batches.
We note p-values < 0.05 where they occur.
Maximum accuracy of 70.5% was achieved with
Batch 3, which featured use of Title query and
yes/no response. Similar accuracy of 69.5% was
also achieved in both Batch 1 and 2. Accuracy fell
in Batch 4 (using the Description query) to 66.5%,
and fell further to 64% in Batch 5, which featured
bonuses. With regard to varying use of Title vs. De-
scription query (Batches 1-3,5 vs. 4), accuracy for
the Title query HITs was 68.4% vs. the 66.5% re-
ported above for Batch 4. Thus use of Description
queries was not observed to lead to more accurate
assessments. HIT completion time was also highest
for Batch 4, with workers taking an average of 72s
to complete a HIT, vs. mean HIT completion time of
63s over the four Title query batches.
The number of unique workers (UW) per Batch
gives some sense of how attractive a Batch was,
where a high number could alternatively suggest
many workers were attracted (positive) or incentives
were too weak to encourage a few Workers to do
many HITs (negative). UW in batches 1-4 ranged
from 64-72. This fell to 38 UW in Batch 5 (bonus
batch), perhaps indicating that workers were incen-
tivized to do more HITs to earn bonuses. At the
same time that the number of workers went down,
the accuracy per worker went up, with the average
worker judging 3.37 documents correctly, compared
to a range of 2.10 - 2.20 correct answers per aver-
age worker for Batches 1-3 and 1.85 correct answers
per average worker for Batch 4 (which, interestingly,
had slightly more UWs than the other batches).
176
Subset Cost Batch Completion Time HIT Completion Time
#B HITs noB withB Total MeanH MeanB sdB Total MeanH MeanB sdH
Query 3 5 250 $3.00 $3.14 N/A N/A N/A N/A 16127 64.50 3225.4 92.48
Query 13 5 250 $3.00 $3.14 N/A N/A N/A N/A 17148 68.59 3429.6 139.08
Query 68 5 250 $3.00 $3.06 N/A N/A N/A N/A 14880 59.52 2976 111.23
Query 78 5 250 $3.00 $3.12 N/A N/A N/A N/A 17117 68.46 3423.4 122.89
Pay=$0.01 4 800 $8.00 $8.46 1078821 1348.52 269705.25 47486.7 54379 67.97 13594.75 123.57
Pay=$0.02 1 200 $4.00 $4.00 386324 1931.62 386324 N/A 10893 54.465 10893 88.87
Title 4 800 $10.00 $10.46 1227585 1534.48 306896.25 67820.58 50968 63.71 12742 117.14
Desc. 1 200 $4.00 $4.00 237560 1187.8 237560 N/A 14304 71.52 14304 119.20
No Bonus 4 800 $10.00 $10.00 1124799 1405.99 281199.75 70347.43 51966 64.95 12991.5 111.32
Bonus 1 200 $2.00 $2.46 340346 1701.73 340346 N/A 13306 66.53 13306 139.97
Batch 1 1 200 $2.00 $2.00 249921 1249.60 249921 N/A 13935 69.67 13935 130.66
Batch 2 1 200 $4.00 $4.00 386324 1931.62 386324 N/A 10893 54.46 10893 88.87
Batch 3 1 200 $2.00 $2.00 250994 1254.97 250994 N/A 12834 64.17 12834 102.01
Batch 4 1 200 $2.00 $2.00 237560 1187.8 237560 N/A 14304 71.52 14304 119.20
Batch 5 1 200 $2.00 $2.46 340346 1701.73 340346 N/A 13306 66.53 13306 139.97
All 5 1000 $12.00 $12.46 1465145 1465.14 293029 66417.07 65272 65.272 13054.4 117.54
Table 4: Preliminary analysis 1. Column labels: #B: Number of Batches, # HITs, noB: Cost without bonuses, withB:
Cost with bonuses, Total, MeanH/B: Mean per-HIT/Batch, sdB/H: std-deviation across Batches/HITs.
Recall that bonuses were awarded whenever
Workers provided clear justification of their judg-
ments (whether or not those judgments matched
ground truth). In 74% of these cases (17 of the 23
HITs awarded bonuses), relevance assessments were
correct. Thus there may be a useful correlation to ex-
ploit provided practical heuristics exist for automat-
ically distinguishing quality feedback from spam.
Feedback length might serve as a more practi-
cal alternative to measuring quality while still cor-
relating with accuracy. Mean comment length for
Batches 2 and 5 was 38.6 and 28.1 characters per
comment, whereas Batches 1, 3, and 4 had mean
comment lengths of 13.9, 12.7, and 19.3 charac-
ters per comment. The mean difference in comment
length between Batch 2 and Batch 1 was 24.7 char-
acters (p<0.01), 25.9 characters between Batches
2 and 3 (p<0.01), and 19.3 characters between
Batches 2 and 4 (p<0.01). Batch 5 and Batch 1 had
a mean comment-length difference of 14.2 charac-
ters (p<0.01), and Batches 5 and 3 differed by 15.4
characters (p<0.01). Thus higher-paying HITs or
HITs with bonus opportunities may correlate with
greater Worker effort. Batches 2 (pay=$0.02) and 5
(bonus batch) garnered the highest number of com-
ments, with each averaging 0.37 comments per HIT.
In contrast, Batches 1, 3, and 4 averaged only 0.21,
0.18, and 0.23 comments per HIT, or a difference of
0.16 (p<0.01 ), 0.19 (p<0.01), and 0.14 (p<0.01)
comments, respectively.
5 Discussion
How to control for the same worker participating
in multiple experiments. We found many of the
same workers completed HITs in multiple batches,
compromising our experimental control and likely
introducing effects of training or fatigue. It does
not appear that MTurk provides an easy way to pre-
venting this; one can block a worker from doing
jobs, but blocking is more of a tool to prevent poor
performance. It is also construed as a punishment:
workers? ratings can be negatively affected by block-
ing. Because of this, blocking is not a substitute
for a mechanism that simply allows requesters to
hideHITs or otherwise disallow repeat workers from
completing HITs. It would be nice to develop a sim-
ple mechanism for automatically ensuring each ex-
periment involves a different set of workers.
Automatic HIT validation. MTurk does not ap-
pear to automatically ensure a submitted HIT was
actually completed, i.e. a worker can submit a HIT
without having actually done anything. While the
submitted HIT can be rejected and re-requested,
building some trivial validation of HITs to catch
such cases automatically appears worthwhile.
Automatic bonus pay. For Batch 5 (which in-
cluded bonus pay), one of the authors spent an hour
manually processing/evaluating worker annotations
and feedback, distributing bonus pay for 23 of the
200 HITs. While some time is certainly well spent
in manually analyzing annotations and feedback, the
177
Subset Accuracy Unique Workers HPW Feedback Given Feedback Length
#Correct MeanH MeanB sdH Total MeanH MeanB Acc Mean Total MeanH MeanH sd
Query 3 144 0.58 28.8 0.50 84 0.34 16.8 1.71 2.98 60 0.24 17.91 42.44
Query 13 191 0.76 38.2 0.43 88 0.35 17.6 2.17 2.84 71 0.28 25.04 55.82
Query 68 183 0.73 36.6 0.44 83 0.33 16.6 2.20 3.01 69 0.28 21.08 44.69
Query 78 162 0.65 32.4 0.48 83 0.33 16.6 1.95 3.01 76 0.30 26.02 56.52
Pay=$0.01 541 0.68 135.25 0.47 137 0.17 34.25 3.95 5.84 201 0.25 18.49 42.52
Pay=$0.02 139 0.70 139 0.46 64 0.32 64 2.17 3.13 75 0.38 38.60 71.53
Title 547 0.68 136.75 0.47 132 0.17 33 4.14 6.06 229 0.29 23.32 51.23
Desc. 133 0.67 133 0.47 72 0.36 72 1.85 2.78 47 0.24 19.29 46.40
No Bonus 552 0.69 138 0.46 121 0.15 30.25 4.56 6.61 201 0.25 21.12 50.26
Bonus 128 0.64 128 0.48 38 0.19 38 3.37 5.26 75 0.38 28.09 50.21
Batch 1 139 0.70 139 0.46 66 0.33 66 2.11 3.03 42 0.21 13.90 37.62
Batch 2 139 0.70 139 0.46 64 0.32 64 2.17 3.13 75 0.38 38.60 71.53
Batch 3 141 0.71 141 0.46 64 0.32 64 2.20 3.13 37 0.19 12.67 31.96
Batch 4 133 0.67 133 0.47 72 0.36 72 1.85 2.78 47 0.24 19.29 46.40
Batch 5 128 0.64 128 0.48 38 0.19 38 3.37 5.26 75 0.38 28.09 50.21
All 680 0.68 136 0.47 149 0.15 29.8 4.56 6.71 276 0.28 22.51 50.30
Table 5: Preliminary analysis 2. Column labels: HPW: HITs per worker, MeanH/B: Mean per-HIT/Batch, sd(H):
std-deviation (across HITs), Acc: mean worker accuracy. Feedback length is in characters.
disparity in cost of our own salaries vs. bonus ex-
penses suggests decisions on bonus pay should be
automated if possible (and it likely pays to err on the
side of being generous). Of course, automated bonus
distribution may negatively affect quality of work
if, for example, any string of characters in the feed-
back box yields bonus pay and workers catch on to
this. Similarly, automation may fail to reward truly
valuable qualitative feedback from workers which is
harder to automatically assess than simply evaluat-
ing worker accuracy on known examples.
6 Future Work
Assessing relevance of Web pages. In the near-
term, we will be using MTurk to evaluate search ac-
curacy of systems participating in the TREC 2010
Relevance Feedback Track. This will involve ad-
dressing several significant challenges: (1) achiev-
ing scalable evaluation, (2) protecting workers from
malicious attack pages while maintaining assess-
ment accuracy, (3) addressing issues of Web spam,
and (4) handling issues of unknown mature content
workers may encounter during assessment.
With regard to (1), we will be scaling up
Cranfield-based relevance assessment to support
search evaluation on the massive ClueWeb09 Web
crawl4. As for (2), many Web pages containing
attack code designed to compromise the viewer?s
computer, and in a crowdsourced environment we
4http://boston.lti.cs.cmu.edu/Data/clueweb09
cannot ensure all workers have installed the latest
security patches for their Web browsers. Various
tradeoffs may be involved between security and us-
ability in pre-renderingWeb pages to assess as static
images, creating a ?safe-viewer? applet, etc. Web
spam (3) can be annoying to workers and thereby
impact the quality of their work, wastes time and
money since spam is never relevant to any query by
definition, and spam detection is conceptually a dis-
tinct task and ought to be handled as such. In the
short term, we may simply ask workers to not only
decide relevance vs. non-relevance, but to simulta-
neously differentiate non-relevant content from non-
relevant spam, but a better solution would be prefer-
able. Mature content (4) is similar to spam but can
be far worse than annoying to workers, touches on
legal issues, and inability to filter it could signifi-
cantly reduce the number of workers willing to ac-
cept HITs which may contain it. Our short-term so-
lution will likely be to perform some simple pre-
filtering and simply warn workers they may en-
counter such content, but this solution is not ideal.
Varying number of annotations in proportion
to annotator agreement. While we collected a
fixed number of relevance assessments for each
query-document pair, it may be both more efficient
and more effective to collect few assessments when
inner-annotator agreement is high and proportion-
ally more assessments when greater disagreement
exists between annotators (Von Ahn et al, 2008).
178
Graded vs. binary relevance. We want asses-
sors to be both maximally informative and max-
imally consistent, and there is an inherent trade-
off here. Allowing assessors to make graded rel-
evance judgments corresponds to the intuitive no-
tion that relevance is typically not a binary propo-
sition. Evaluation of commercial search engines to-
day often reports use of a five-point graded scale,
and such graded feedback allows us to better distin-
guish relative effectiveness of different search algo-
rithms at a finer scale. However, the right number of
relevance levels to assess is unclear, and too many
would likely involve making overly nuanced judg-
ments that could overwhelm assessors and lead to
low inner-annotator agreement. We may similarly
ask assessors to further differentiate relevance judg-
ment from cases of ?I don?t know? and ?this HIT
seems broken?. There is also the possibility of in-
ducing graded relevance levels from binary judg-
ments, such as by averaging and rescaling. The util-
ity could be measured by comparing benchmark al-
gorithms using the explicit or induced assessments.
Evaluating annotation accuracy with regard
to ground-truth labels vs. task accuracy. While
much research with MTurk has measured accuracy
in terms of reproducing a ground-truth label, ulti-
mately we are not interested in the labels themselves
but rather in what we can do with them. Rele-
vance assessment in particular suffers from notori-
ously low inner-annotator agreement. Consequently,
one alternative to comparing against ?ground-truth?
labels would be to evaluate the ability of crowd-
sourced labels for effectively distinguish between
different benchmark algorithms.
Crowd demographics. While it is typically sug-
gested that experts produce superior annotations,
there are important questions of effects from who
is judging the annotations. For example, if you want
to know if the general public will think a particu-
lar web page is relevant to a particular query, more
useful assessments might be obtained from a layman
than from someone who builds search engines for a
living. This also suggests another reason why it may
even be preferable in some circumstances for crowd-
source annotations to disagree with ?ground-truth?
expert labels. It also raises questions about gener-
ality of system comparisons based on expert labels
when systems are to be used by the general public.
References
Omar Alonso, Daniel E. Rose, and Benjamin Stewart.
2008. Crowdsourcing for relevance evaluation. SIGIR
Forum, 42(2):9?15.
J. Aslam and V. Pavlu. 2008. A practical sampling strat-
egy for efficient retrieval evaluation. Technical report,
Northeastern University.
C. Buckley and E.M. Voorhees. 2004. Retrieval evalu-
ation with incomplete information. In Proceedings of
the 27th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 25?32. ACM New York, NY, USA.
B. Carterette, J. Allan, and R. Sitaraman. 2006. Minimal
test collections for retrieval evaluation. In Proceed-
ings of the 29th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 268?275.
B. Carterette, V. Pavlu, E. Kanoulas, J.A. Aslam, and
J. Allan. 2009. If I Had a Million Queries. In Pro-
ceedings of the 31st European Conference on Infor-
mation Retrieval, pages 288?300.
D. Horowitz and S.D. Kamvar. 2010. The Anatomy of a
Large-Scale Social Search Engine. In Proc. of the 19th
international conference on World wide web (WWW).
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD ?02: Proceedings of
the 8th SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133?142.
W. Mason and D.J. Watts. 2009. Financial incentives
and the performance of crowds. In Proceedings of
the ACM SIGKDDWorkshop on Human Computation,
pages 77?85. ACM.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263. Association for
Computational Linguistics.
L. Von Ahn, B. Maurer, C. McMillen, D. Abraham, and
M. Blum. 2008. recaptcha: Human-based charac-
ter recognition via web security measures. Science,
321(5895):1465.
E.M. Voorhees. 2002. The philosophy of information re-
trieval evaluation. Lecture Notes in Computer Science,
pages 355?370.
J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movel-
lan. 2009. Whose Vote Should Count More: Optimal
Integration of Labels from Labelers of Unknown Ex-
pertise. Proceedings of the 2009 Neural Information
Processing Systems (NIPS) Conference.
179
