Proceedings of the 12th Conference of the European Chapter of the ACL, pages 799?807,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
MINT: A Method for Effective and Scalable Mining of  
Named Entity Transliterations from Large Comparable Corpora 
Raghavendra Udupa         K Saravanan         A Kumaran        Jagadeesh Jagarlamudi*          
Microsoft Research India 
Bangalore 560080 INDIA 
 [raghavu,v-sarak,kumarana,jags}@microsoft.com 
 
Abstract 
In this paper, we address the problem of min-
ing transliterations of Named Entities (NEs) 
from large comparable corpora. We leverage 
the empirical fact that multilingual news ar-
ticles with similar news content are rich in 
Named Entity Transliteration Equivalents 
(NETEs). Our mining algorithm, MINT, uses 
a cross-language document similarity model to 
align multilingual news articles and then 
mines NETEs from the aligned articles using a 
transliteration similarity model. We show that 
our approach is highly effective on 6 different 
comparable corpora between English and 4 
languages from 3 different language families. 
Furthermore, it performs substantially better 
than a state-of-the-art competitor.   
1 Introduction 
Named Entities (NEs) play a critical role in many 
Natural Language Processing and Information 
Retrieval (IR) tasks.  In Cross-Language Infor-
mation Retrieval (CLIR) systems, they play an 
even more important role as the accuracy of their 
transliterations is shown to correlate highly with 
the performance of the CLIR systems (Mandl 
and Womser-Hacker, 2005, Xu and Weischedel, 
2005).  Traditional methods for transliterations 
have not proven to be very effective in CLIR. 
Machine Transliteration systems (AbdulJaleel 
and Larkey, 2003; Al-Onaizan and Knight, 2002; 
Virga and Khudanpur, 2003) usually produce 
incorrect transliterations and translation lexcions 
such as hand-crafted or statistical dictionaries are 
too static to have good coverage of NEs1 occur-
ring in the current news events. Hence, there is a 
critical need for creating and continually updat-
                                                 
* Currently with University of Utah. 
1 New NEs are introduced to the vocabulary of a lan-
guage every day. On an average, 260 and 452 new 
NEs appeared daily in the XIE and AFE segments of 
the LDC English Gigaword corpora respectively. 
ing multilingual Named Entity transliteration 
lexicons. 
The ubiquitous availability of comparable 
news corpora in multiple languages suggests a 
promising alternative to Machine Transliteration, 
namely, the mining of Named Entity Translitera-
tion Equivalents (NETEs) from such corpora. 
News stories are typically rich in NEs and there-
fore, comparable news corpora can be expected 
to contain NETEs (Klementiev and Roth, 2006; 
Tao et al, 2006). The large quantity and the per-
petual availability of news corpora in many of 
the world?s languages, make mining of NETEs a 
viable alternative to traditional approaches. It is 
this opportunity that we address in our work. 
    In this paper, we detail an effective and scala-
ble mining method, called MINT (MIning 
Named-entity Transliteration equivalents), for 
mining of NETEs from large comparable corpo-
ra. MINT addresses several challenges in mining 
NETEs from large comparable corpora: exhaus-
tiveness (in mining sparse NETEs), computa-
tional efficiency (in scaling on corpora size), 
language independence (in being applicable to 
many language pairs) and linguistic frugality (in 
requiring minimal external linguistic resources).   
Our contributions are as follows: 
? We give empirical evidence for the hypo-
thesis that news articles in different languages 
with reasonably similar content are rich sources 
of NETEs (Udupa, et al, 2008).  
? We demonstrate that the above insight can 
be translated into an effective approach for min-
ing NETEs from large comparable corpora even 
when similar articles are not known a priori. 
? We demonstrate MINT?s effectiveness on 
4 language pairs involving 5 languages (English, 
Hindi, Kannada, Russian, and Tamil) from 3 dif-
ferent language families, and its scalability on 
corpora of vastly different sizes (2,000 to 
200,000 articles).  
? We show that MINT?s performance is sig-
nificantly better than a state of the art method 
(Klementiev and Roth, 2006). 
 
799
We discuss the motivation behind our ap-
proach in Section 2 and present the details in 
Section 3.  In Section 4, we describe the evalua-
tion process and in Section 5, we present the re-
sults and analysis.  We discuss related work in 
Section 6.  
2 Motivation 
MINT is based on the hypothesis that news ar-
ticles in different languages with similar content 
contain highly overlapping set of NEs. News 
articles are typically rich in NEs as news is about 
events involving people, locations, organizations, 
etc2. It is reasonable to expect that multilingual 
news articles reporting the same news event 
mention the same NEs in the respective languag-
es. For instance, consider the English and Hindi 
news reports from the New York Times and the 
BBC on the second oath taking of President Ba-
rack Obama (Figure 1). The articles are not pa-
rallel but discuss the same event. Naturally, they 
mention the same NEs (such as Barack Obama, 
John Roberts, White House) in the respective 
languages, and hence, are rich sources of NETEs.    
Our empirical investigation of comparable 
corpora confirmed the above insight. A study of 
                                                 
2 News articles from the BBC corpus had, on an 
average, 12.9 NEs and new articles from the The 
New Indian Express, about 11.8 NEs. 
 
200 pairs of similar news articles published by 
The New Indian Express in 2007 in English and 
Tamil showed that 87% of the single word NEs 
in the English articles had at least one translitera-
tion equivalent in the conjugate Tamil articles.  
The MINT method leverages this empirically 
backed insight to mine NETEs from such compa-
rable corpora.   
However, there are several challenges to the 
mining process: firstly, vast majority of the NEs 
in comparable corpora are very sparse; our anal-
ysis showed that 80% of the NEs in The New 
Indian Express news corpora appear less than 5 
times in the entire corpora.  Hence, any mining 
method that depends mainly on repeated occur-
rences of the NEs in the corpora is likely to miss 
vast majority of the NETEs.  Secondly, the min-
ing method must restrict the candidate NETEs 
that need to be examined for match to a reasona-
bly small number, not only to minimize false 
positives but also to be computationally efficient.  
Thirdly, the use of linguistic tools and resources 
must be kept to a minimum as resources are 
available only in a handful of languages.  Finally, 
it is important to use as little language-specific 
knowledge as possible in order to make the min-
ing method applicable across a vast majority of 
languages of the world.  The MINT method pro-
posed in this paper addresses all the above is-
sues. 
 
800
3 The MINT Mining Method 
MINT has two stages. In the first stage, for 
every document in the source language side, the 
set of documents in the target language side with 
similar news content are found using a cross-
language document similarity model. In the 
second stage, the NEs in the source language 
side are extracted using a Named Entity Recog-
nizer (NER) and, subsequently, for each NE in a 
source language document, its transliterations are 
mined from the corresponding target language 
documents. We present the details of the two 
stages of MINT in the remainder of this section. 
3.1 Finding Similar Document Pairs  
The first stage of MINT method (Figure 2) works 
on the documents from the comparable corpora 
(CS, CT) in languages S and T and produces a col-
lection AS,T  of similar article pairs (DS, DT).  Each 
article pair (DS, DT) in AS,T consists of an article 
(DS) in language S and an article (DT) in language 
T, that have similar content. The cross-language 
similarity between DS and DT, as measured by the 
cross-language similarity model MD, is at least ? 
> 0. 
 
Cross-language Document Similarity Model: 
The cross-language document similarity model 
measures the degree of similarity between a pair 
of documents in source and target languages.  
We use the negative KL-divergence between 
source and target document probability distribu-
tions as the similarity measure. 
  Given two documents DS, DT in source and tar-
get languages respectively, with 
TS VV , denoting 
the vocabulary of source and target languages, 
the similarity between the two documents is giv-
en by the KL-divergence measure, -KL(DS || DT), 
as: 
?
? TTw ST
TT
ST
V Dwp
DwpDwp )|(
)|(log)|(
  
where p(w | D) is the likelihood of word w in D. 
As we are interested in target documents which 
are similar to a given source document, we can 
ignore the numerator as it is independent of the 
target document.  Finally, expanding p(wT | Ds) 
as 
)|()|( SVw TSS wwpDwpSS??
we specify the 
cross-language similarity score as follows: 
 
Cross-language similarity =       
)|(log)|()|( TTSTw w SS DwpwwpDwpTVT SVS? ?? ?
 
 
3.2 Mining NETEs from Document Pairs  
The second stage of the MINT method works on 
each pair of articles (DS, DT) in the collection AS,T  
and produces a set PS,T of NETEs. Each pair (?S, 
?T) in PS,T  consists of an NE ?S in language S, and 
a token ?T in language T, that are transliteration 
equivalents of each other.  Furthermore, the 
transliteration similarity between ?S and ?T, as 
measured by the transliteration similarity model 
MT, is at least ? > 0. Figure 3 outlines this algo-
rithm.  
 
Discriminative Transliteration Similarity 
Model:  
The transliteration similarity model MT measures 
the degree of transliteration equivalence between 
a source language and a target language term.  
Input: Comparable news corpora (CS, CT) in languages (S,T)  
           Crosslanguage Document Similarity Model MD for (S, T) 
           Threshold score ?. 
Output: Set AS,T of pairs of similar articles (DS, DT) from (CS, CT). 
1 AS,T  ? ? ;         // Set of Similar articles (DS, DT) 
2 for each article DS in CS do 
3     XS   ? ? ;       // Set of candidates for DS. 
4      for each article dT  in CT  do 
5         score = CrossLanguageDocumentSimilarity(DS,dT,MD); 
6         if (score ? ?) then XS  ? XS  ? (dT , score) ; 
7      end 
8     DT  = BestScoringCandidate(XS); 
9    if (DT  ? ?) then AS,T  ? AS,T  ? (DS, DT) ; 
10 end 
CrossLanguageSimilarDocumentPairs 
Figure 2. Stage 1 of MINT 
Input:  
      Set AS,T  of similar documents (DS, DT)  in languages  
(S,T),   
      Transliteration Similarity Model MT for (S, T),  
      Threshold score ?. 
Output: Set PS,T  of NETEs (?S, ?T) from  AS,T ; 
1   PS,T  ? ? ;  
2   for each pair of articles (DS, DT) in AS,T  do 
3        for each named entity ?S in DS do  
4            YS ? ? ; // Set of candidates for ?S. 
5            for each candidate eT  in DT  do 
6                 score = TransliterationSimilarity(?S, eT, MT) ; 
7                 if (score ? ?)  then   YS  ?  YS ? (eT , score) ; 
8            end 
9            ?T  = BestScoringCandidate(YS) ;  
10          if (?T  ? null) then PS,T  ?  PS,T  ? (?S, ?T) ; 
11      end 
12 end 
TransliterationEquivalents 
Figure 3. Stage 2 of MINT 
801
We employ a logistic function as our translitera-
tion similarity model MT, as follows: 
 
 TransliterationSimilarity (?S,eT,MT) = 
),( TS1
1
ewte ?????
 
where ? (?S, eT) is the feature vector for the pair 
(?S, eT) and w is the weights vector.  Note that the 
transliteration similarity takes a value in the 
range [0..1]. The weights vector w is learnt dis-
criminatively over a training corpus of known 
transliteration equivalents in the given pair of 
languages. 
 
Features: The features employed by the model 
capture interesting cross-language associations 
observed in (?S, eT): 
 
? All unigrams and bigrams from the 
source and target language strings. 
? Pairs of source string n-grams and target 
string n-grams such that difference in the 
start positions of the source and target n-
grams is at most 2. Here n ? ?2,1? . 
? Difference in the lengths of the two 
strings.  
 
Generative Transliteration Similarity Model: 
We also experimented with an extension of He?s 
W-HMM model (He, 2007). The transition prob-
ability depends on both the jump width and the 
previous source character as in the W-HMM 
model. The emission probability depends on the 
current source character and the previous target 
character unlike the W-HMM model (Udupa et 
al., 2009). Instead of using any single alignment 
of characters in the pair (wS, wT), we marginalize 
over all possible alignments: 
? ? ? ? ? ?11
1
11 ,|,|| 1 ??
?
???? jajajj
A
m
j
nm tstpsaapstP
jj
 
 
Here, 
jt
(and resp. 
is ) denotes the j
th (and resp. 
ith) character in wT (and resp. wS) and maA 1? is 
the hidden alignment between wT and wS where 
jt
is aligned to 
jas
, ,m,j ?1? . We estimate 
the parameters of the model using the EM algo-
rithm. The transliteration similarity score of a 
pair (wS, wT) is log P(wT  | wS) appropriately trans-
formed. 
 
 
4 Experimental Setup 
Our empirical investigation consists of experi-
ments in three data environments, with each en-
vironment providing answer to specific set of 
questions, as listed below: 
 
1. Ideal Environment (IDEAL): Given a collec-
tion AS,T of oracle-aligned article pairs (DS, DT) 
in S and T, how effective is Stage 2 of MINT in 
mining NETE from AS,T? 
2. Near Ideal Environment (NEAR-IDEAL): 
Let AS,T  be a collection of similar article pairs 
(DS, DT) in S and T. Given comparable corpora 
(CS, CT) consisting of only articles from AS,T, but 
without the knowledge of pairings between the 
articles,  
a. How effective is Stage 1 of MINT in re-
covering AS,T  from (CS, CT) ? 
b. What is the effect of Stage 1 on the 
overall effectiveness of MINT? 
3. Real Environment (REAL): Given large 
comparable corpora (CS, CT), how effective is 
MINT, end-to-end? 
 
The IDEAL environment is indeed ideal for 
MINT since every article in the comparable cor-
pora is paired with exactly one similar article in 
the other language and the pairing of articles in 
the comparable corpora is known in advance.  
We want to emphasize here that such corpora are 
indeed available in many domains such as tech-
nical documents and interlinked multilingual 
Wikipedia articles. In the IDEAL environment, 
only Stage 2 of MINT is put to test, as article 
alignments are given.  
In the NEAR-IDEAL data environment, every 
article in the comparable corpora is known to 
have exactly one conjugate article in the other 
language though the pairing itself is not known 
in advance.  In such a setting, MINT needs to 
discover the article pairing before mining NETEs 
and therefore, both stages of MINT are put to 
test.  The best performance possible in this envi-
ronment should ideally be the same as that of 
IDEAL, and any degradation points to the short-
coming of the Stage 1 of MINT.  These two en-
vironments quantify the stage-wise performance 
of the MINT method.    
Finally, in the data environment REAL, we 
test MINT on large comparable corpora, where 
even the existence of a conjugate article in the 
target side for a given article in the source side of 
the comparable corpora is not guaranteed, as in 
802
any normal large multilingual news corpora. In 
this scenario both the stages of MINT are put to 
test.  This is the toughest, and perhaps the typical 
setting in which MINT would be used.  
4.1 Comparable Corpora 
In our experiments, the source language is Eng-
lish whereas the 4 target languages are from 
three different language families (Hindi from the 
Indo-Aryan family, Russian from the Slavic fam-
ily, Kannada and Tamil from the Dravidian fami-
ly). Note that none of the five languages use a 
common script and hence identification of cog-
nates, spelling variations, suffix transformations, 
and other techniques commonly used for closely 
related languages that have a common script are 
not applicable for mining NETEs.  Table 1 sum-
marizes the 6 different comparable corpora that 
were used for the empirical investigation; 4 for 
the IDEAL and NEAR-IDEAL environments (in 
4 language pairs), and 2 for the REAL environ-
ment (in 2 language pairs). 
 
Cor-
pus 
Source -
Target 
Data 
Environ-
ment 
Articles (in 
Thousands) 
Words (in 
Millions) 
Src Tgt Src Tgt 
EK-S 
English- 
Kannada 
IDEAL& 
NEAR-IDEAL 
2.90 2.90 0.42 0.34 
ET-S 
English- 
Tamil 
IDEAL& 
NEAR-IDEAL 
2.90 2.90 0.42 0.32 
ER-S 
English- 
Russian 
IDEAL& 
NEAR-IDEAL 
2.30 2.30 1.03 0.40 
EH-S 
English- 
Hindi 
IDEAL& 
NEAR-IDEAL 
11.9 11.9 3.77 3.57 
EK-L 
English- 
Kannada 
REAL 103.8 111.0 27.5 18.2 
ET-L 
English- 
Tamil 
REAL 103.8 144.3 27.5 19.4 
Table 1: Comparable Corpora 
 
The corpora can be categorized into two sepa-
rate groups, group S (for Small) consisting of 
EK-S, ET-S, ER-S, and EH-S and group L (for 
Large) consisting of EK-L and ET-L. Corpora in 
group S are relatively small in size, and contain 
pairs of articles that have been judged by human 
annotators as similar. Corpora in group L are two 
orders of magnitude larger in size than those in 
group S and contain a large number of articles 
that may not have conjugates in the target side. 
In addition the pairings are unknown even for the 
articles that have conjugates. All comparable 
corpora had publication dates, except EH-S, 
which is known to have been published over the 
same year. 
The EK-S, ET-S, EK-L and ET-L corpora are 
from The New Indian Express news paper, whe-
reas the EH-S corpora are from Web Dunia and 
the ER-S corpora are from BBC/Lenta News 
Agency respectively. 
4.2 Cross-language Similarity Model  
The cross-language document similarity model 
requires a bilingual dictionary in the appropriate 
language pair. Therefore, we generated statistical 
dictionaries for 3 language pairs (from parallel 
corpora of the following sizes: 11K sentence 
pairs in English-Kannada, 54K in English-Hindi, 
and 14K in English-Tamil) using the GIZA++ 
statistical alignment tool (Och et al, 2003), with 
5 iterations each of IBM Model 1 and HMM.  
We did not have access to an English-Russian 
parallel corpus and hence could not generate a 
dictionary for this language pair. Hence, the 
NEAR-IDEAL experiments were not run for the 
English-Russian language pair.   
Although the coverage of the dictionaries was 
low, this turned out to be not a serious issue for 
our cross-language document similarity model as 
it might have for topic based CLIR (Ballesteros 
and Croft, 1998). Unlike CLIR, where the query 
is typically smaller in length compared to the 
documents, in our case we are dealing with news 
articles of comparable size in both source and 
target languages.  
When many translations were available for a 
source word, we considered only the top-4 trans-
lations.  Further, we smoothed the document 
probability distributions with collection frequen-
cy as described in (Ponte and Croft, 1998). 
4.3 Transliteration Similarity Model  
The transliteration similarity models for each of 
the 4 language pairs were produced by learning 
over a training corpus consisting of about 16,000 
single word NETEs, in each pair of languages.  
The training corpus in English-Hindi, English-
Kannada and English-Tamil were hand-crafted 
by professionals, the English-Russian name pairs 
were culled from Wikipedia interwiki links and 
were cleaned heuristically.  Equal number of 
negative samples was used for training the mod-
els. To produce the negative samples, we paired 
each source language NE with a random non-
matching target language NE.  No language spe-
cific features were used and the same feature set 
was used in each of the 4 language pairs making 
MINT language neutral.   
In all the experiments, our source side lan-
guage is English, and the Stanford Named Entity 
Recognizer (Finkel et al 2005) was used to ex-
tract NEs from the source side article.  It should 
be noted here that while the precision of the NER 
803
used was consistently high, its recall was low, 
(~40%) especially in the New Indian Express 
corpus, perhaps due to the differences in the data 
used for training the NER and the data on which 
we used it.   
4.4 Performance Measures  
Our intention is to measure the effectiveness of 
MINT by comparing its performance with the 
oracular (human annotator) performance.  As 
transliteration equivalents must exist in the 
paired articles to be found by MINT, we focus 
only on those NEs that actually have at least one 
transliteration equivalent in the conjugate article. 
Three performance measures are of interest to 
us: the fraction of distinct NEs from source lan-
guage for which we found at least one translitera-
tion in the target side (Recall on distinct NEs), 
the fraction of distinct NETEs (Recall on distinct 
NETEs) and the Mean Reciprocal Rank (MRR) 
of the NETEs mined.  Since we are interested in 
mining not only the highly frequent but also the 
infrequent NETEs, recall metrics measure how 
effective our method is in mining NETEs ex-
haustively. The MRR score indicates how effec-
tive our method is in preferring the correct ones 
among candidates. 
To measure the performance of MINT, we 
created a test bed for each of the language pairs. 
The test beds are summarized in Table 2.  
The test beds consist of pairs of similar ar-
ticles in each of the language pairs. It should be 
noted here that as transliteration equivalents must 
exist in the paired articles to be found by MINT, 
we focus only on those NEs that actually have at 
least one transliteration equivalent in the conju-
gate article. 
5 Results & Analysis 
In this section, we present qualitative and quan-
titative performance of the MINT algorithm, in 
mining NETEs from comparable news corpora. 
All the results in Sections 5.1 to 5.3 were ob-
tained using the discriminative transliteration 
similarity model described in Section 3.2. The 
results using the generative transliteration simi-
larity model are discussed in Section 5.4. 
5.1 IDEAL Environment 
Our first set of experiments investigated the ef-
fectiveness of Stage 2 of MINT, namely the min-
ing of NETEs in an IDEAL environment. As 
MINT is provided with paired articles in this ex-
periment, all experiments for this environment 
were run on test beds created from group S cor-
pora (Table 2).  
 
 
Results in the IDEAL Environment:  
The recall measures for distinct NEs and distinct 
NETEs for the IDEAL environment are reported 
in Table 3.  
 
Test 
Bed 
Recall (%) 
Distinct NEs Distinct NETEs 
EK-ST 97.30 95.07 
ET-ST 99.11 98.06 
EH-ST 98.55 98.66 
ER-ST 93.33 85.88 
 Table 3: Recall of MINT in IDEAL 
 
Note that in the first 3 language pairs MINT was 
able to mine a transliteration equivalent for al-
most all the distinct NEs. The performance in 
English-Russian pair was relatively worse, per-
haps due to the noisy training data.   
In order to compare the effectiveness of 
MINT with a state-of-the-art NETE mining ap-
proach, we implemented the time series based 
Co-Ranking algorithm based on (Klementiev and 
Roth, 2006).  
 
Table 4 shows the MRR results in the IDEAL 
environment ? both for MINT and the Co-
Ranking baseline: MINT outperformed Co-
Ranking on all the language pairs, despite not 
using time series similarity in the mining 
process.  The high MRRs (@1 and @5) indicate 
that in almost all the cases, the top-ranked candi-
date is a correct NETE.  Note that Co-Ranking 
could not be run on the EH-ST test bed as the 
articles did not have a date stamp. Co-Ranking is 
crucially dependent on time series and hence re-
quires date stamps for the articles. 
 
Test Bed 
Comparable 
Corpora 
Article 
Pairs 
Distinct 
NEs 
Distinct 
NETEs 
EK-ST EK-S 200 481 710 
ET-ST ET-S 200 449 672 
EH-ST EH-S 200 347 373 
ER-ST ER-S 100 195 347 
Table 2: Test Beds for IDEAL & NEAR-IDEAL 
Test 
Bed 
MRR@1 MRR@5 
MINT CoRanking MINT CoRanking 
EK-ST 0.94 0.26 0.95 0.29 
ET-ST 0.91 0.26 0.94 0.29 
EH-ST 0.93 - 0.95 - 
ER-ST 0.80 0.38 0.85 0.43 
Table 4: MINT & Co-Ranking in IDEAL 
804
5.2 NEAR-IDEAL Environment 
The second set of experiments investigated the 
effectiveness of Stage 1 of MINT on comparable 
corpora that are constituted by pairs of similar 
articles, where the pairing information between 
the articles is with-held.  MINT reconstructed the 
pairings using the cross-language document si-
milarity model and subsequently mined NETEs. 
As in previous experiments, we ran our experi-
ments on test beds described in Section 4.4. 
 
Results in the NEAR-IDEAL Environment: 
There are two parts to this set of experiments. In 
the first part, we investigated the effectiveness of 
the cross-language document similarity model 
described in Section 3.1. Since we know the 
identity of the conjugate article for every article 
in the test bed, and articles can be ranked accord-
ing to the cross-language document similarity 
score, we simply computed the MRR for the 
documents identified in each of the test beds, 
considering only the top-2 results. Further, where 
available, we made use of the publication date of 
articles to restrict the number of target articles 
that are considered in lines 4 and 5 of the MINT 
algorithm in Figure 2.  Table 5 shows the results 
for two date windows ? 3 days and 1 year. 
 
 Test 
Bed 
MRR@1 MRR@2 
3 days 1 year 3 days 1 year 
EK-ST 0.99 0.91 0.99 0.93 
ET-ST 0.96 0.83 0.97 0.87 
EH-ST - 0.81 - 0.82 
Table 5: MRR of Stage 1 in NEAR-IDEAL 
 
Subsequently, the output of the Stage 1 was giv-
en as the input to the Stage 2 of the MINT me-
thod. In Table 6 we report the MRR @1 and @5 
for the second stage, for both time windows (3 
days & 1 year). 
 
It is interesting to compare the results of MINT 
in NEAR-IDEAL data environment (Table 6) 
with MINT?s results in IDEAL environment 
(Table 4). The drop in MRR@1 is small: ~2% 
for EK-ST and ~3% for ET-ST. For EH-ST the 
drop is relatively more (~12%) as may be ex-
pected since the time window (3 days) could not 
be applied for this test bed.  
5.3 REAL Environment 
The third set of experiments investigated the ef-
fectiveness of MINT on large comparable corpo-
ra. We ran the experiments on test beds created 
from group L corpora.   
 
 Test-beds for the REAL Environment: The 
test beds for the REAL environment (Table 7) 
consisted of only English articles since we do not 
know in advance whether these articles have any 
similar articles in the target languages. 
 
 Results in the REAL Environment: In real 
environment, we examined the top 2 articles of 
returned by Stage 1 of MINT, and mined NETEs 
from them. We used a date window of 3 in Stage 
1. Table 8 summarizes the results for the REAL 
environment. 
 
We observe that the performance of MINT is 
impressive, considering the fact that the compa-
rable corpora used in the REAL environment is 
two orders of magnitude larger than those used in 
IDEAL and NEAR-IDEAL environments. This 
implies that MINT is able to effectively mine 
NETEs whenever the Stage 1 algorithm was able 
to find a good conjugate for each of the source 
language articles.  
5.4 Generative Transliteration Similarity 
Model 
We employed the extended W-HMM translitera-
tion similarity model in MINT and used it in the 
IDEAL data environment.  Table 9 shows the 
results. 
Test 
Bed 
MRR@1 MRR@5 
3 days 1 year 3 days 1 year 
EK-ST 0.92 0.87 0.94 0.90 
ET-ST 0.88 0.74 0.91 0.78 
EH-ST - 0.82 - 0.87 
Table 6: MRR of Stage 2 in NEAR-IDEAL 
Test 
Bed 
Comparable 
Corpora 
Articles 
Distinct  
NEs 
EK-LT EK-L 100 306 
ET-LT ET-L 100 228 
Table 7: Test Beds for REAL 
 
Test Bed 
MRR 
@1 @5 
EK-LT 0.86 0.88 
ET-LT 0.82 0.85 
Table 8: MRR of Stage 2 in REAL 
Test Bed 
MRR 
@1 @5 
EK-S 0.85 0.86 
ET-S 0.81 0.82 
EH-S 0.91 0.93 
Table 9:  MRR of Stage 2 in IDEAL using genera-
tive transliteration similarity model 
805
We see that the results for the generative transli-
teration similarity model are good but not as 
good as those for the discriminative translitera-
tion similarity model. As we did not stem either 
the English NEs or the target language words, 
the generative model made more mistakes on 
inflected words compared to the discriminative 
model.   
5.5  Examples of Mined NETEs 
Table 10 gives some examples of the NETEs 
mined from the comparable news corpora.  
 
6  Related Work 
CLIR systems have been studied in several 
works (Ballesteros and Croft, 1998; Kraiij et al 
2003). The limited coverage of dictionaries has 
been recognized as a problem in CLIR and MT 
(Demner-Fushman & Oard, 2002; Mandl & 
Womser-hacker, 2005; Xu &Weischedel, 2005).  
In order to address this problem, different 
kinds of approaches have been taken, from learn-
ing transformation rules from dictionaries and 
applying the rules to find cross-lingual spelling 
variants (Pirkola et al, 2003), to  learning trans-
lation lexicon from monolingual and/or compa-
rable corpora (Fung, 1995; Al-Onaizan and 
Knight, 2002; Koehn and Knight, 2002; Rapp, 
1996). While these works have focused on find-
ing translation equivalents of all class of words, 
we focus specifically on transliteration equiva-
lents of NEs.  (Munteanu and Marcu, 2006; 
Quirk et al, 2007) addresses mining of parallel 
sentences and fragments from nearly parallel 
sentences. In contrast, our approach mines 
NETEs from article pairs that may not even have 
any parallel or nearly parallel sentences.   
NETE discovery from comparable corpora 
using time series and transliteration model was 
proposed in (Klementiev and Roth, 2006), and 
extended for NETE mining for several languages 
in (Saravanan and Kumaran, 2007).  However, 
such methods miss vast majority of the NETEs 
due to their dependency on frequency signatures.   
In addition, (Klementiev and Roth, 2006) may 
not scale for large corpora, as they examine 
every word in the target side as a potential trans-
literation equivalent. NETE mining from compa-
rable corpora using phonetic mappings was pro-
posed in (Tao et al, 2006), but the need for lan-
guage specific knowledge restricts its applicabili-
ty across languages.  We proposed the idea of 
mining NETEs from multilingual articles with 
similar content in (Udupa, et al, 2008). In this 
work, we extend the approach and provide a de-
tailed description of the empirical studies. 
7  Conclusion 
In this paper, we showed that MINT, a simple 
and intuitive technique employing cross-
language document similarity and transliteration 
similarity models, is capable of mining NETEs 
effectively from large comparable news corpora. 
Our three stage empirical investigation showed 
that MINT performed close to optimal on com-
parable corpora consisting of pairs of similar ar-
ticles when the pairings are known in advance. 
MINT induced fairly good pairings and performs 
exceedingly well even when the pairings are not 
known in advance. Further, MINT outperformed 
a state-of-the-art baseline and scaled to large 
comparable corpora.  Finally, we demonstrated 
the language neutrality of MINT, by mining 
NETEs from 4 language pairs (between English 
and one of Russian, Hindi, Kannada or Tamil) 
from 3 vastly different linguistic families. 
As a future work, we plan to use the ex-
tended W-HMM model to get features for the 
discriminative transliteration similarity model. 
We also want to use a combination of the cross-
language document similarity score and the 
transliteration similarity score for scoring the 
NETEs. Finally, we would like to use the mined 
NETEs to improve the performance of the first 
stage of MINT. 
Acknowledgments 
We thank Abhijit Bhole for his help and Chris 
Quirk for valuable comments. 
 
Language 
Pair 
Source NE Transliteration 
English-
Kannada 
Woolmer ??????? 
Kafeel ????? 
Baghdad ???????? 
English-Tamil Lloyd ??????  
Mumbai ?????????? 
Manchester ??????????? 
English-Hindi Vanhanen ??????? 
Trinidad ???????????  
Ibuprofen ?????????? 
English-
Russian 
Kreuzberg ?????????? 
Gaddafi ??????? 
Karadzic ???????? 
Table 10: Examples of Mined NETEs 
806
References 
AbdulJaleel, N. and Larkey, L.S. 2003. Statistical translite-
ration for English-Arabic cross language information re-
trieval. Proceedings of CIKM 2003.  
Al-Onaizan, Y. and Knight, K. 2002. Translating named 
entities using monolingual and bilingual resources. Pro-
ceedings of the 40th Annual Meeting of ACL. 
Ballesteros, L. and Croft, B. 1998. Dictionary Methods for 
Cross-Lingual Information Retrieval. Proceedings of 
DEXA?96.  
Chen, H., et al 1998. Proper Name Translation in Cross-
Language Information Retrieval. Proceedings of the 36th 
Annual Meeting of the ACL. 
Demner-Fushman, D., and Oard, D. W. 2002. The effect of 
bilingual term list size on dictionary-based cross-
language information retrieval. Proceedings of the 36th 
Hawaii International Conference on System Sciences.  
Finkel, J. Trond Grenager, and Christopher Manning. 2005. 
Incorporating Non-local Information into Information 
Extraction Systems by Gibbs Sampling. Proceedings of 
the 43nd Annual Meeting of the ACL. 
Fung, P. 1995. Compiling bilingual lexicon entries from a 
non-parallel English-Chinese corpus. Proceedings of the 
3rd Workshop on Very Large Corpora. 
Fung, P. 1995. A pattern matching method for finding noun 
and proper noun translations from noisy parallel corpora.  
Proceedings of ACL 1995.  
He. X. 2007: Using word dependent transition models in 
HMM based word alignment for statistical machine 
translation. In Proceedings of 2nd ACL Workshop on Sta-
tistical Machine Translation . 
Hermjakob, U., Knight, K., and Daume, H. 2008. Name 
translation in statistical machine translation: knowing 
when to transliterate. Proceedings ACL 2008. 
Klementiev, A. and Roth, D. 2006. Weakly supervised 
named entity transliteration and discovery from multilin-
gual comparable corpora. Proceedings of the 44th Annual 
Meeting of the ACL.  
Knight, K. and Graehl, J. 1998. Machine Transliteration. 
Computational Linguistics.  
Koehn, P. and Knight, K. 2002. Learning a translation lex-
icon from monolingual corpora. Proceedings of Unsu-
pervised Lexical Acquisition. 
Kraiij, W., Nie, J-Y. and  Simard, M. 2003. Emebdding 
Web-based Statistical Translation Models in Cross-
Language Information Retrieval. Computational Linguis-
tics., 29(3):381-419. 
Mandl, T., and Womser-Hacker, C.  2004. How do named 
entities contribute to retrieval effectiveness? Proceedings 
of the 2004 Cross Language Evaluation Forum Cam-
paign 2004. 
Mandl, T., and Womser-Hacker, C.  2005. The Effect of 
named entities on effectiveness in cross-language infor-
mation retrieval evaluation. ACM Symposium on Applied 
Computing.  
Munteanu, D. and Marcu D. 2006. Extracting parallel sub-
sentential fragments from non-parallel corpora. Proceed-
ings of the ACL 2006. 
Och, F. and Ney, H. 2003. A systematic comparison of var-
ious statistical alignment models. Computational Lin-
guistics. 
Pirkola, A., Toivonen, J., Keskustalo, H., Visala, K. and 
Jarvelin, K. 2003. Fuzzy translation of cross-lingual 
spelling variants. Proceedings of SIGIR 2003.  
Ponte, J. M. and Croft, B. 1998. A Language Modeling 
Approach to Information Retrieval. Proceedings of ACM 
SIGIR 1998.  
Quirk, C., Udupa, R. and Menezes, A. 2007. Generative 
models of noisy translations with applications to parallel 
fragments extraction. Proceedings of the 11th MT Sum-
mit. 
Rapp, R. 1996. Automatic identification of word transla-
tions from unrelated English and German corpora. Pro-
ceedings of ACL?99 
Saravanan, K. and Kumaran, A. 2007. Some experiments in 
mining named entity transliteration pairs from compara-
ble corpora. Proceedings of the 2nd International Work-
shop on Cross Lingual Information Access. 
Tao, T., Yoon, S., Fister, A., Sproat, R. and Zhai, C. 2006. 
Unsupervised named entity transliteration using temporal 
and phonetic correlation. Proceedings of EMNLP 2006.  
Udupa, R., Saravanan, K., Kumaran, A. and Jagarlamudi, J.  
2008.  Mining Named Entity Transliteration Equivalents 
from Comparable Corpora. Proceedings of the CIKM 
2008. 
Udupa, R., Saravanan, K., Bakalov, A. and Bhole, A.  2009.  
?They are out there if you know where to look?: Mining 
transliterations of OOV terms in cross-language informa-
tion retrieval. Proceedings of the ECIR 2009. 
Virga, P. and Khudanpur, S. 2003. Transliteration of proper 
names in cross-lingual information retrieval. Proceedings 
of the ACL Workshop on Multilingual and Mixed Lan-
guage Named Entity Recognition.  
Xu, J. and Weischedel, R. 2005. Empirical studies on the 
impact of lexical resources on CLIR performance. In-
formation Processing and Management. 
807
Some Experiments in Mining Named Entity Transliteration Pairs from 
Comparable Corpora 
K Saravanan 
Microsoft Research India 
Bangalore, India 
v-sarak@microsoft.com 
A Kumaran 
Microsoft Research India 
Bangalore, India 
kumarana@microsoft.com 
 
  
Abstract 
Parallel Named Entity pairs are important 
resources in several NLP tasks, such as, 
CLIR and MT systems.  Further, such pairs 
may also be used for training transliteration 
systems, if they are transliterations of each 
other.  In this paper, we profile the perfor-
mance of a mining methodology in mining 
parallel named entity transliteration pairs in  
English and an Indian language, Tamil,   
leveraging linguistic tools in English, and 
article-aligned comparable corpora in  the 
two languages.  We adopt a methodology 
parallel to that of [Klementiev and Roth, 
2006], but we focus instead on mining    
parallel named entity transliteration pairs,   
using a well-trained linear classifier to 
identify transliteration pairs.  We profile 
the performance at several operating para-
meters of our algorithm and present the   
results that show the potential of the       
approach in mining transliterations pairs; in 
addition, we uncover a host of issues that 
need to be resolved, for effective mining of  
parallel named entity transliteration pairs. 
1 Introduction & Motivation 
Parallel Named Entity (NE) pairs are important 
resources in several NLP tasks, from supporting 
Cross-Lingual Information Retrieval (CLIR)     
systems, to improving Machine Translation (MT) 
systems.  In addition, such pairs may also be used 
for developing transliteration systems, if they are 
transliterations of each other.  Transliteration of a 
name, for the purpose of this work, is defined as its 
transcription in a different language, preserving the 
phonetics, perhaps in a different orthography 
[Knight and Graehl, 1997] 1 .  While traditional 
transliteration systems have relied on hand-crafted 
linguistic rules, more recently, statistical machine 
learning techniques have been shown to be effec-
tive in transliteration tasks [Jung et al, 2000] [Ab-
dulJaleel and Larkey, 2003] [Virga  and Kudhan-
pur , 2003] [Haizhou et al, 2004].  However, such 
data-driven approaches require significant amounts 
of training data, namely pairs of names in two dif-
ferent languages, possibly in different orthography, 
referred to as transliteration pairs, which are not 
readily available in many resource-poor languages.  
It is important to note at this point, that NEs are 
found typically in news corpora in any given     
language.  In addition, news articles covering the 
same event in two different languages may reason-
ably be expected to contain the same NEs in the 
respective languages.  The perpetual availability of 
news corpora in the world?s languages, points to 
the promise of  mining transliteration pairs        
endlessly, provided an effective identification of 
such NEs in specific languages and pairing them 
appropriately, could be devised.  
 
Recently, [Klementiev and Roth, 2006] outlined an 
approach by leveraging the availability of article-
aligned news corpora between English and Rus-
sian, and tools in English, for discovering translite-
ration pairs between the two languages, and pro-
gressively refining the discovery process.  In this 
paper, we adopt their basic methodology, but we 
focus on 3 different issues:  
                                                 
1 London rewritten as ?????? in Tamil, or ???? in Arabic (both 
pronounced as London), are considered as transliterations, but 
not the rewriting of New Delhi as ???? ?????? (puthu thilli) in 
Tamil.   
1. mining comparable corpora for NE pairs, leve-
raging a well trained classifier, 
2. calibrating the performance of this mining 
framework, systematically under different pa-
rameters for mining, and,  
3. uncovering further research issues in mining NE 
pairs between English and an Indian language, 
Tamil. 
While our analysis points to a promising approach 
for mining transliteration pairs, it also uncovers 
several issues that may need to be resolved, to 
make this process highly effective. As in [Klemen-
tiev and Roth, 2006] no language specific know-
ledge was used to refine our mining process, mak-
ing the approach broadly applicable. 
2 Transliteration Pairs Discovery 
In this section, we outline briefly the methodology 
presented in [Klementiev and Roth, 2006], and 
refer interested readers to the source for details. 
 
They present a methodology to automatically 
discover parallel NE transliteration pairs between 
English and Russian, leveraging the availability of 
a good-quality Named Entity Recognizer (NER) in 
English, and article-aligned bilingual comparable 
corpora, in English and Russian.  The key idea of 
their approach is to extract all NEs in English, and 
identify a set of potential transliteration pairs in 
Russian for these NEs using a simple classifier 
trained on a small seed corpus, and re-ranking the 
identified pairs using the similarity between the 
frequency distributions of the NEs in the 
comparable corpora.  Once re-ranked, the 
candidate pairs, whose scores are above a threshold 
are used to re-train the classifier, and the process is 
repeated to make the discovery process more 
effective. 
 
To discriminate transliteration pairs from other 
content words, a simple perceptron-based linear 
classifier, which is trained on n-gram features 
extracted from a small seed list of NE pairs, is 
employed leveraging the fact that transliteration 
relies on approximately monotonic alignment 
between the names in two languages.  The 
potential transliteration pairs identified by this 
classifier are subsequently re-ranked using a 
Discrete Fourier Transform based similarity 
metric, computed based on the frequency of words 
of the candidate pair, found in the article-aligned 
comparable corpora.  For the frequency analysis, 
equivalence classes of the words are formed, using 
a common prefix of 5 characters, to account for the 
rich morphology of Russian language.  The 
representative prefix of each of the classes are used 
for classification. 
 
Finally, the high scoring pairs of words are used to 
re-train the perceptron-based linear classifier, to 
improve the quality of the subsequent rounds.  The 
quality of the extracted NE pairs is shown to 
improve, demonstrating viability of such an 
approach for successful discovery of NE pairs 
between English and Russian. 
3 Adoption for Transliteration Pairs 
Mining  
We adopt the basic methodology presented in 
[Klementiev and Roth, 2006], but we focus on 
three specific issues described in the introduction.   
3.1 Mining of Transliteration Pairs  
We start with comparable corpora in English and 
Tamil, similar in size to that used in [Klementiev 
and Roth, 2006], and using the English side of this 
corpora, first, we extract all the NEs that occur 
more than a given threshold parameter, FE, using a 
standard NER tool.  The higher the threshold is, 
the more will be the evidence for legitimate transli-
teration pairs, in the comparable corpora, which 
may be captured by the mining methodology. The 
extracted list of NEs provides the set of NEs in 
English, for which we mine for transliteration pairs 
from the Tamil side of the comparable corpora.   
 
We need to identify all NEs in the Tamil side of 
the corpora, in order to appropriately pair-up with 
English NEs.  However, given that there is no pub-
licly available NER tool in Tamil (as the case may 
be in many resource-poor languages) we start with 
an assumption that all words found in the Tamil 
corpus are potentially NEs.  However, since Tamil 
is a highly morphologically inflected language, the 
same NE may occur in its various inflected forms 
in the Tamil side of the corpora; hence, we collect 
those words with the same prefix (of fixed size) 
into a single bucket, called equivalence class, and 
consider a representative prefix, referred to as sig-
nature of the collection for comparison.  The     
assumption here is that the common prefix would 
stand for a Tamil NE, and all the members of the 
equivalence class are the various inflected forms of 
the NE. We use such a signature to classify a Ta-
mil word as potential transliteration of an English 
word. Again, we consider only those signatures 
that have occurred more than a threshold parame-
ter, FT, in the Tamil side of the comparable corpora, 
in order to strengthen support for a meaningful  
similarity in their frequency of occurrence. 
 
We used a linear Support Vector Machine classifi-
er (details given in a later section) trained on a   
sizable seed corpus of transliterations between 
English and Tamil, and use it to identify potential   
Tamil signatures with any of the NEs extracted 
from the English side.  We try to match each of the 
NEs extracted from the English side, to every sig-
nature from the Tamil side, and produce an ordered 
list of Tamil signatures that may be potential trans-
literations for a given English NE.  Every Tamil 
signature, thus, would get a score, which is used to 
rank the signatures in the decreasing order of simi-
larity.  Subsequently, we consider only those above 
a certain threshold for analysis, and in addition, 
consider only the top-n candidates. 
3.2 Quality Refinement 
Since a number of such transliteration candidates 
are culled from the Tamil corpus for a given NE in 
English, we further cull out unlikely candidates, by 
re-ranking them using frequency cues from the 
aligned comparable corpora.  For this, we start 
with the hypothesis, that the NEs will have similar 
normalized frequency distributions with respect to 
time, in the two corpora.  Given that the news cor-
pora are expected to contain same names in similar 
time periods in the two different languages, the 
frequency distribution of words in the two         
languages provides a strong clue about possible 
transliteration pairs; however, such potential pairs 
might also include other content words, such as, 
? ???????? (soshaliSt), ?????? (kavanamaa-
ka), ??????? (keetpathu), etc., which are common 
nouns, adjectives or even adverbs and verbs.  On 
the other hand, function words are expected to be 
uniformly distributed in the corpus, and hence may 
not have high variability like content words.   Note 
that the NEs in English are not usually inflected. 
Since Tamil NEs usually have inflections, the   
frequency of occurrence of a NE in Tamil must be 
normalized across all forms, to make it reasonably 
comparable to the frequency of the corresponding 
English NE. This was taken care of by considering 
the signature and its equivalence class. Hence the 
frequency of occurrence of a NE (i.e., its signature) 
in Tamil is the sum of frequencies of all members 
in its equivalence class.    
 
For identifying the names between the languages, 
we first create a frequency distribution of every 
word in English and Tamil, by creating temporal 
bins of specific duration, covering the entire time-
line of the corpus.  The frequency is calculated as 
the number of occurrences of each signature in the 
bin interval.  Once the frequency distributions are 
formed, they are normalized for every signature.  
Given the normalized frequencies, two words are 
considered to have same (or, similar) pattern of 
occurrence in the corpus, if the normalized        
frequency vectors of the two words are the same 
(or, close within a threshold).  Figure 1 shows the 
frequency of the word Abishek, and its Tamil ver-
sion, ??????? (apishek) as a frequency plot, 
where a high correlation between the frequencies 
can be observed. 
 
 
 
Figure 1: Names Frequency Plot in Comparable Corpora 
 
Hence, to refine the quality of the classifier output, 
we re-rank the list of candidates, using the distance 
between the frequency vectors of the English NE, 
and the Tamil candidate signature.   This step 
moves up those signatures that have similar pat-
terns of occurrence, and moves down those that do 
not.  It is likely that such frequency cues from the 
comparable corpora will make the quality of 
matched transliteration pairs better, yielding better 
mined data. 
4 Experimental Setup & Results 
In this section, we present the experimental setup 
and the data that we used for mining transliteration 
pairs from comparable corpora in two languages: 
English and the Indian language, Tamil.  We eva-
luate and present the effectiveness of the metho-
dology in extracting NE pairs, between these lan-
guages, under various parameters. 
4.1 Comparable Corpora 
We used a set of news articles from the New     
Indian Express (in English) and Dinamani (in   
Tamil) roughly covering similar events in English 
and Tamil respective, and covering a period of 
about 8 months, between January and August of 
2007.  The articles were verified to contain similar 
set of NEs, though only a fraction of them are   
expected to be legitimate transliteration pairs.  
Others related NEs could be translations,  for    
example, chief minister in English vs ???????? 
(muthalvar) in Tamil, abbreviation which are not 
usually transliterated but spelled out , for example, 
ICC in English, and ? ? ? (aicici) in Tamil, or      
co-references , for example, New Delhi in English, 
and ?????????? (puthu thilli) in Tamil.  While the 
number of      articles used were roughly the same 
(~2,400), the number of words in Tamil were only 
about 70% of that in English.  This is partially due 
to the fact Tamil is a highly agglutinative lan-
guage, where various affixes (prefixes and suffixes 
of other content words) stand for function words 
and prepositions in English, thus do not contribute 
to the word count.  Further, since our focus is on 
mining names, we expect the same NEs to be cov-
ered in both the corpora, and hence we do not   
expect a severe impact on mining. 
 
Corpus Time  
Period 
Size  
Articles Words 
New Indian  
Express  
(English) 
2007.01.01 to 
2007.08.31 
2,359 347,050 
Dinamani 
(Tamil) 
2007.01.01 to 
2007.08.31 
2,359 256,456 
Table 1: Statistics on Comparable Corpora 
 
From the above corpora, we first extracted all the 
NEs from the English side, using the Stanford 
NER tool [Finkel et al 2005].  No multiword    
expressions were considered for this experiment.  
Also, only those NEs that have a frequency count 
of more than a threshold value of FE were consi-
dered, in order to avoid unusual names that are 
hard to identify in the comparable corpora.  Thus, 
we extracted from the above corpora, only a subset 
of NEs found in the English side to be matched 
with their potential transliteration pairs; for exam-
ple, for a parameter setting of FE to 10, we extract 
only 274 legitimate NEs.   
 
From the Tamil side of the corpora, we extracted 
all words, and grouped them in to equivalence 
classes, by considering a prefix of 5 characters.  
That is, all words that share the same 5 characters 
were considered to be morphological variations of 
the same root word or NE in Tamil.  After they 
were grouped, the longest common prefix of the 
group is extracted, and is used as the signature of 
the equivalence class.  It should be noted here that 
though the number of unique words in the corpus 
is about 46,503, the number of equivalence classes 
to be considered changes depending on the filter-
ing threshold that we use in the Tamil side.  For 
example, at a threshold (FT) value of 1, the number 
of equivalence classes is 14,101.  It changes to 
4,612 at a threshold (FT) value of 5, to 2,888 at a 
threshold (FT) value of 10 and to 1779 at a thre-
shold (FT) value of 20.  However, their signature 
(i.e., longest common prefix) sizes ranged from 5 
to 13 characters.  Thus, we had about 14,101 equi-
valence classes, covering all the words from the 
Tamil corpus.  The equivalence classes thus 
formed were as shown in Figure 2: 
 
Tamil  
Signature 
Tamil  
Equiv. Class 
???????? 
(aiSvaryaa) 
???????? (aiSvaryaa),  
???????????? (aiSvaryaavin),  
?????????????? (aiSvaryaavukku), 
?????????? (aiSvaryaavai),  
???????????????? (aiSvaryaaviRkum),                           
????????????? (aiSvaryaavutan) 
?????    
(piram) 
?????????????? (pirammapuththiraa), 
????????????? (pirammaaNdamaana),         
??????? (pirampu), ??????? (pirammaa) 
?????? 
(kaaveeri) 
?????? (kaaveeri) 
? ? ?  
(aicici) 
? ? ? (aicici), ? ? ????? (aicicyin),                
? ? ???? (aicici kku), ? ? ????? (aicicithaan),         
? ? ?????? (aiciciyidam) 
Figure 2: Signatures and Equivalence Classes 
 
As can be seen in the table, all elements of an 
equivalence class share the same signature (by  
definition). However, some signatures, such as 
???????? (aiSvaryaa), correspond to an equiva-
lence class in which every element is a morpholog-
ical variation of the signature.  Such equivalence 
classes, we name them pure.  Some signatures 
represent only a subset of the members, as this set 
includes some members unrelated to this stem; for 
example, the signature ????? (piram), correctly   
corresponds to ??????? (pirammaa), and incorrect-
ly to the noun ??????? (pirambu), as well as incor-
rectly to the adjective ????????????? (piram-
maandamaana).  We name such equivalence 
classes fuzzy.  Some are well formed, but may not 
ultimately contribute to our mining, being an ab-
breviation, such as ICC (in Tamil, ? ? ?), even 
though they are used similar to any NE in Tamil. 
While most equivalence classes contained inflec-
tions of single stems, we also found morphological 
variations of several compound names in the same 
equivalence class such as, ????????? (akamath?a-
kar), ????????? (akamathaapaath), with ????? 
(akamath). 
4.2 Classifier for Transliteration Pair Identi-
fication 
We used SVM-light [Joachims, 1999], a Support-
vector Machine (SVM) from Cornell University, to 
identify near transliterations between English and 
Tamil.   We used a seed corpus consisting of 5000 
transliteration pair samples collected from a differ-
ent resource, unrelated to the experimental compa-
rable corpora. In addition to the 5000 positive   
examples from this seed corpus, 5000 negative   
examples were extracted randomly, but incorrectly, 
aligned names from this same seed corpus and 
used for the classifier. 
 
The features used for the classification are binary 
features based on the length of the pair of strings 
and all aligned unigram and bigram pairs, in each 
direction, between the two strings in the seed cor-
pus in English and Tamil.  The length features in-
clude the difference in lengths between them (up to 
3), and a separate binary feature if they differ by 
more than 3.  For unigram pairs, the ith character in  
a language string is matched to (i-1)st,  ith and  
(i+1)st characters of the other language string.  
Each string is padded with special characters at the 
beginning and the end, for appropriately forming 
the unigrams for the first and the last characters of 
the string.  In the same manner, for binary features, 
every bigram extracted with a sliding window of 
size 2 from a language string, is matched with 
those extracted from the other language string.  
After the classifier is trained on the seed corpus of 
hand crafted transliteration pairs, during the min-
ing phase, it compares every English NE extracted 
from the English corpus, to every signature from 
the Tamil corpus. 
 
While classifier provided ranked list of all the sig-
natures from Tamil side, we consider only the top-
30 signatures (and the words in the equivalence 
classes) for subsequent steps of our methodology.  
We hand-verified a random sample of about 100 
NEs from English side, and report in Table 5, the 
fraction of the English NEs for which we found at 
least one legitimate transliteration in the top-30 
candidates (for example, the  recall of the classifier 
is 0.56, in identifying a right signature in the top-
30 candidates, when the threshold FE is 10 & FT is 
1).   
 
It is interesting to note that as the two threshold 
factors are increased, the number of NEs extracted 
from the English side decreases (as expected), and 
the average number of positive classifications per 
English NE reduces (as shown in Table 2), consi-
dering all NEs.  This makes sense as the classifier 
for identifying potential transliterations is trained 
with sizable corpora and is hence accurate; but, as 
the thresholds increase, it has less data to work 
with, and possibly a fraction of legitimate translite-
rations also gets filtered with noise. 
 
Parameters Extracted 
English NEs 
Ave. Positive 
Classifications/ 
English NE 
FE: 10, FT: 1 274 79.34 
FE: 5, FT: 5 588 29.50 
FE: 10, FT: 10 274 17.49 
FE: 20, FT: 20 125 10.55 
Table 2: Threshold Parameters vs Mining Quantity 
 
Table 3 shows some sample results after the classi-
fication step with parameter values as (FE: 10, FT: 1). 
Right signature for Aishwarya (corresponding to 
all correct transliterations) has been ranked 10 and 
Gandhi (with only a subset of the equivalence class 
corresponding to the right transliterations) has been 
ranked at 8.  Three different variations of Argenti-
na can be found, ranked 2nd, 3rd and 13th.  While, in 
general no abbreviations are found (usually their 
Tamil equivalents are spelled out), a rare case of 
abbreviation (SAARC) and its right transliteration is 
ranked 1st.   
 
 
English 
Named Entity 
Tamil Equivalence Class  
Signature 
Precision Rank 
aishwarya ???????? (aiSvaryaa) 1 10 
argentina ?????????????    
(arjantinaavila) 
1 2 
argentina ?????????????    
(aarjantinaavi) 
1 3 
argentina ??????????????    
(aarjantinaavil) 
1 13 
gandhi ????? (kaa?tha) 0.2121 8 
saarc  ????? (saark) 1 1 
Table 3: Ranked List after Classification Step 
 
4.3 Enhancing the Quality of Transliteration-
Pairs 
For the frequency analysis, we use the frequency 
distribution of the words in English and Tamil side 
of the comparable corpora, counting the number of 
occurrences of NEs in English and the Tamil    
signatures in each temporal bin spanning the entire 
corpus. We consider one temporal bin to be equal 
to two successive days. Thus, each of the English 
NEs and the Tamil signatures is represented by a 
vector of dimension approximately 120. We com-
pute the distance between the two vectors, and  
hypothesize that they may represent the same (or, 
similar) name, if the difference between them is 
zero (or, small).  Note that, as mentioned earlier,  
the frequency vector of the Tamil signature will 
contain the sum of individual frequencies of the 
elements in the equivalence class corresponding to 
it.  Given that the classifier step outputs a list of 
English NEs, and associated with each entry, a 
ranked list of Tamil signatures that are identified as 
potential transliteration by the classifier, we com-
pute the distance between the frequency vector of 
every English NE, with each of the top-30 signa-
tures in the ranked list.  We re-rank the top-30 
candidate strings, using this distance measure.  The 
output is similar to that shown in Table 4, but with 
possibly a different rank order. 
 
 
English 
Named Entity 
Tamil Equivalence Class  
Signature 
Precision Rank 
aishwarya ???????? (aiSvaryaa) 1 1 
argentina ?????????????           
(arjantinaavila) 
1 1 
argentina ?????????????          
(aarjantinaavi) 
1 3 
argentina ??????????????          
(aarjantinaavil) 
1 14 
gandhi ????? (kaa?tha) 0.2121 16 
saarc  ????? (saark) 1 1 
Table 4: Ranked List after Frequency Analysis Step 
 
On comparing Table 3 and 4, we observe that some 
of the ranks have moved for the better, and some 
of them for the worse.  It is interesting to note that 
the ranking of different stems corresponding to 
Argentina has moved differently.  It is quite likely 
that merging these three equivalence classes cor-
responding to the English NE Argentina might re-
sult in a frequency profile that is more closely 
aligned to that of the English NE.   
4.4 Overall Performance of Transliteration 
Pairs Mining 
To find the effectiveness of each step of the mining 
process in identifying the right signatures (and 
hence, the equivalence classes) for a given English 
NE, we computed the Mean Reciprocal Rank 
(MRR) of the random sample of 100 transliteration 
pairs mined, in two different ways:  First, we com-
puted MRRpure, which corresponded to the first oc-
currence of a pure equivalence class, and MRRfuzzy, 
which corresponded to the first occurrence of a 
fuzzy equivalence class in the random samples.  
MRRfuzzy captures how successful the mining was 
in identifying one possible transliteration, MRRpure, 
captures how successful we were in identifying an 
equivalence class that contains only right translite-
rations2.  In addition, these metrics were computed, 
corresponding to different frequency thresholds for 
the occurrence of a English NE (FE) and a Tamil 
signature (FT).  The overall quality profile of the 
mining framework in mining the NE transliteration 
pairs in English and Tamil is shown in Table 5.  
Additionally, we also report the recall metric (the 
fraction of English NEs, for which at least one le-
                                                 
2 However, it should be noted that the current metrics 
neither capture how pure an equivalence class is (frac-
tion of the set that are correct transliterations), nor the 
size of the equivalence class.  We hope to specify these 
as part of quality of mining, in our subsequent work.  
gitimate Tamil signature was identified) computed 
on a randomly chosen 100 entity pairs. 
 
Parameters 
Classification 
Step 
Frequency 
Analysis Step Re-
call MRR 
fuzzy  
MRR 
pure 
MRR 
fuzzy  
MRR 
pure 
FE: 10, FT: 1 0.3579 0.2831 0.3990 0.3145 0.56 
FE: 5, FT: 5 0.4490 0.3305 0.5064 0.3529 0.61 
FE: 10, FT: 10 0.4081 0.2731 0.4930 0.3494 0.57 
FE: 20, FT: 20 0.3489 0.2381 0.4190 0.2779 0.47 
Table 5: Quality Profile of NE Pairs Extraction 
 
First, it should be noted that the recalls are the 
same for both the steps, since Frequency Analysis 
step merely re-arranges the output of the Classifi-
cation step.  Second, the recall figures drop, as 
more filtering is applied to the NEs on both sides.  
This trend makes sense, since the classifier gets 
less data to work with, as more legitimate words 
are filtered out with noise.  Third, as can be ex-
pected, MRRpure is less than the MRRfuzzy at every 
step of the mining process.  Fourth, we see that the 
MRRpure and the MRRfuzzy improve between the two 
mining steps, indicating that the time-series analy-
sis has, in general, made the output better.   
 
Finally, we find that the MRRpure and the MRRfuzzy 
keep dropping with increased filtering of English 
NEs and Tamil signatures based on their frequen-
cy, in both the classification and frequency analy-
sis steps. The fall of the MRRs after the classifica-
tion steps is due to the fact that the classifier has 
less and less data with the increasing threshold, 
and hence some legitimate transliterations may be 
filtered out as noise.  However, the frequency 
analysis step critically depends on availability of 
sufficient words from the Tamil side for similarity 
testing.  In frequency analysis step, the fall of 
MRRs from threshold 5 to 10 is 0.0134 on MRRfuzzy 
and 0.0035 on MRRpure. This fall is comparatively 
less to the fall of MRRs from threshold 10 to 20 
which is 0.074 on MRRfuzzy and 0.0715 on MRRpure. 
This may be due to the fact that the number of legi-
timate transliterations filtered out from threshold 5 
to 10 is less when compared to the number of legi-
timate transliterations filtered out from threshold 
10 to 20. These results show that with less number 
of words filtered, it can get reasonable recall and 
MRR values. More profiling experiments may be 
needed to  validate this claim.      
5 Open Issues in NE pair Mining 
In this paper, we outline our experience in mining 
parallel NEs between English and Tamil, in an  
approach similar to the one discussed in [Klemen-
tiev and Roth, 2006].  Over and above, we made 
parameter choices, and some procedural modifica-
tions to bridge the underspecified methodology 
given in the above work.  While the results are 
promising, we find several issues that need further 
research.  We outline some of them below: 
5.1 Indistinguishable Signatures 
Table 7 shows a signature that offers little help in 
distinguishing a set of words.  Both the words, 
? ???? (cennai) and morphological variations of 
? ?? (cen), share the same 5-character signature, 
namely, ? ???  (cenna), affecting the frequency 
distribution of the signature adversely. 
 
English 
Named 
Entity 
Tamil 
Named 
Entity 
Tamil  
Equivalent Class 
chennai 
? ???? 
(cennai) 
? ???? (cennai), ? ???????? (cennaiyil), 
? ?????????????? (cennaiyilirunthu), 
? ?????? (cennin), ? ????????           (cen-
nukku),? ?????? (cennaiyai) 
Table 7: Multiple-Entity Equivalence Class 
5.2 Abbreviations 
Table 8 shows a set of abbreviations, that are not 
identified well in our NE pair mining. Between the 
two languages, the abbreviations may be either 
expanded, as BJP expanded to (the equivalent 
translation for Bharatiya Janatha Party in Tamil), 
or spelled out, as in BSNL referred to as 
??????????? (pieSenel).  The last example is very 
interesting, as each W in English is written out as 
??????? (tapiLyuu).  All these are hard to capture 
by a simple classifier that is trained on well-formed 
transliteration pairs.  
 
English 
Named 
Entity 
Tamil  
Named Entity 
BJP 
???? (paajaka), ??.?.?. (paa. ja. ka.), ?????? 
???? ??? ? (paarathiiya janathaa katci) 
BSNL 
??????????? (pieSenel), ???????????????     
(pieSenellin), ????????????? (piesenellai) 
WWW 
?????????????????????  
(tapiLyuutapiLyuutapiLyuu) 
Table 8: Multiple-Entity Equivalence Class 
5.3 Multiword Expressions 
This methodology is currently designed for mining 
only single word expressions.  It may be an inter-
esting line of research to mine multiword expres-
sions automatically. 
6 Related Work 
Our work essentially follows a similar procedure 
as reported in [Klementiev and Roth, 2006] paper, 
but applied to English-Tamil language pair.  Earli-
er works, such as [Cucerzan and Yarowsky, 1999] 
and [Collins and Singer, 1999] addressed identifi-
cation of NEs from untagged corpora. They relied 
on significant contextual and morphological clues.  
[Hetland, 2004] outlined methodologies based on 
time distribution of terms in a corpus to identify 
NEs, but only in English.  While a large body of 
literature exists on transliteration, we merely point 
out that the focus of this work (based on [Klemen-
tiev and Roth, 2006]) is not on transliteration, but 
mining transliteration pairs, which may be used for 
developing a transliteration system.   
7 Conclusions 
In this paper, we focused on mining NE transliteration 
pairs in two different languages, namely English and an 
Indian language, Tamil.  While we adopted a methodol-
ogy similar to that in [Klementiev and Roth, 2006], our 
focus was on mining parallel NE transliteration pairs, 
leveraging the availability of comparable corpora and a 
well-trained linear classifier to identify transliteration 
pairs.  We profiled the performance of our mining 
framework on several parameters, and presented the 
results.  Our experiment results are inline with those 
reported by [Klementiev and Roth, 2006]. Given that 
the NE pairs are an important resource for several NLP 
tasks, we hope that such a methodology to mine the 
comparable corpora may be fruitful, as comparable   
corpora may be freely available in perpetuity in several 
of the world?s languages.  
8 Acknowledgements 
We would like to thank Raghavendra Udupa, 
Chris Quirk, Aasish Pappu, Baskaran Sankaran,  
Jagadeesh Jagarlamudi and Debapratim De for 
their help. 
References 
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical 
transliteration for English-Arabic cross language informa-
tion retrieval. In Proceedings of CIKM, pages 139?146, 
New York, NY, USA. 
Jenny Rose Finkel, Trond Grenager, and Christopher 
Manning. 2005. Incorporating Non-local Information into 
Information Extraction Systems by Gibbs Sampling. In 
Proceedings of the 43nd Annual Meeting of the Association 
for Computational Linguistics (ACL 2005), pp. 363-370. 
L Haizhou, Z Min and S Jian. 2004. A Joint Source-Channel 
Model for Machine Transliteration. In Proceedings of 42nd 
Meeting of Assoc. of Computational Linguistics. 
 
Magnus Lie Hetland. 2004. Data Mining in Time Series Data-
bases, a chapter in A Survey of Recent Methods for Effi-
cient Retrieval of Similar Time Sequences. World Scientif-
ic.  
T. Joachims. 1999. 11 in: Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf and C. Burges 
and A. Smola (ed.), MIT Press. 
Sung Young Jung, SungLim Hong, and Eunok Paek. 2000. An 
English to Korean transliteration model of extended mar-
kov window. In Proceedings of the International Confe-
rence on Computational Linguistics (COLING), pages 
383?389.  
Alexandre Klementiev and Dan Roth. 2006. Named Entity 
Transliteration and Discovery from Multilingual Compara-
ble Corpora. In Proceedings of the Human Language 
Technology Conference of the North American Chapter of 
the ACL, pages 82?88. 
Kevin Knight and Jonathan Graehl. 1997. Machine translite-
ration. In Proceedings of the Meeting of the European As-
sociation of Computational Linguistics, pages 128?135.  
Yusuke Shinyama and Satoshi Sekine. 2004. Named entity 
discovery using comparable news articles. In Proceedings 
the International Conference on Computational Linguistics 
(COLING), pages 848?853. 
Richard Sproat, Tao Tao, ChengXiang Zhai. 2006. Named 
Entity Transliteration with Comparable Corpora. In Pro-
ceedings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the ACL, 
pages 73?80, Sydney. 
Tao Tao and ChengXiang Zhai. 2005. Mining comparable 
bilingual text corpora for cross-language information inte-
gration. In KDD?05, pages 691?696. 
 
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat, and 
ChengXiang Zhai. 2006. Unsupervised named entity transli-
teration using temporal and phonetic correlation. In EMNLP 
2006, Sydney, July. 
Paula Virga and Sanjeev Khudanpur. 2003. Transliteration of 
Proper Names in Cross-Lingual Information Retrieval.  In 
Proceedings of Workshop on Multilingual and Mixed-
Language Named Entity Recognition. 
Designing a Common POS-Tagset Framework for Indian Languages 
Sankaran Baskaran, Microsoft Research India. Bangalore. baskaran@microsoft.com 
Kalika Bali, Microsoft Research India. Bangalore. kalikab@microsoft.com 
Tanmoy Bhattacharya, Delhi University, Delhi. tanmoy1@gmail.com 
Pushpak Bhattacharyya, IIT-Bombay, Mumbai. pb@cse.iitb.ac.in 
Girish Nath Jha, Jawaharlal Nehru University, Delhi. girishj@mail.jnu.ac.in 
Rajendran S, Tamil University, Thanjavur. raj_ushush@yahoo.com 
Saravanan K, Microsoft Research India, Bangalore. v-sarak@microsoft.com 
Sobha L, AU-KBC Research Centre, Chennai. sobha@au-kbc.org 
Subbarao K V. Delhi. kvs2811@yahoo.com
 
 
Abstract 
Research in Parts-of-Speech (POS) tagset 
design for European and East Asian lan-
guages started with a mere listing of impor-
tant morphosyntactic features in one lan-
guage and has matured in later years to-
wards hierarchical tagsets, decomposable 
tags, common framework for multiple lan-
guages (EAGLES) etc. Several tagsets 
have been developed in these languages 
along with large amount of annotated data 
for furthering research. Indian Languages 
(ILs) present a contrasting picture with 
very little research in tagset design issues. 
We present our work in designing a com-
mon POS-tagset framework for ILs, which 
is the result of in-depth analysis of eight 
languages from two major families, viz. 
Indo-Aryan and Dravidian. Our framework 
follows hierarchical tagset layout similar to 
the EAGLES guidelines, but with signifi-
cant changes as needed for the ILs. 
1 Introduction 
A POS tagset design should take into consideration 
all possible morphosyntactic categories that can 
occur in a particular language or group of languag-
es (Hardie, 2004). Some effort has been made in 
the past, including the EAGLES guidelines for 
morphosyntactic annotation (Leech and Wilson, 
1996) to define guidelines for a common tagset 
across multiple languages with an aim to capture 
more detailed morphosyntactic features of these 
languages.  
However, most of the tagsets for ILs are lan-
guage specific and cannot be used for tagging data 
in other language. This disparity in tagsets hinders 
interoperability and reusability of annotated corpo-
ra. This further affects NLP research in resource 
poor ILs where non-availability of data, especially 
tagged data, remains a critical issue for researchers. 
Moreover, these tagsets capture the morphosyntac-
tic features only at a shallow level and miss out the 
richer information that is characteristic of these 
languages. 
The work presented in this paper focuses on de-
signing a common tagset framework for Indian 
languages using the EAGLES guidelines as a mod-
el. Though Indian languages belong to (mainly) 
four distinct families, the two largest being Indo-
Aryan and Dravidian, as languages that have been 
in contact for a long period of time, they share sig-
nificant similarities in morphology and syntax. 
This makes it desirable to design a common tagset 
framework that can exploit this similarity to facili-
tate the mapping of different tagsets to each other. 
This would not only allow corpora tagged with 
different tagsets for the same language to be reused 
but also achieve cross-linguistic compatibility be-
tween different language corpora. Most important-
ly, it will ensure that common categories of differ-
ent languages are annotated in the same way. 
In the next section we will discuss the impor-
tance of a common standard vis-?-vis the currently 
available tagsets for Indian languages. Section 3 
will provide the details of the design principles 
The 6th Workshop on Asian Languae Resources, 2008
89
behind the framework presented in this paper. Ex-
amples of tag categories in the common framework 
will be presented in Section 4. Section 5 will dis-
cuss the current status of the paper and future steps 
envisaged.  
2 Common Standard for POS Tagsets 
Some of the earlier POS tagsets were designed 
for English (Greene and Rubin, 1981; Garside, 
1987; Santorini, 1990) in the broader context of 
automatic parsing of English text. These tagsets 
popular even today, though designed for the same 
language differ significantly from each other mak-
ing the corpora tagged by one incompatible with 
the other. Moreover, as these are highly language 
specific tagsets they cannot be reused for any other 
language without substantial changes this requires 
standardization of POS tagsets (Hardie 2004).  
Leech and Wilson (1999) put forth a strong argu-
ment for the need to standardize POS tagset for 
reusability of annotated corpora and interopera-
bility across corpora in different languages. 
EAGLES guidelines (Leech and Wilson 1996) 
were a result of such an initiative to create stan-
dards that are common across languages that share 
morphosyntactic features. 
Several POS tagsets have been designed by a 
number of research groups working on Indian 
Languages though very few are available publicly 
(IIIT-tagset, Tamil tagset). However, as each of 
these tagsets have been motivated by specific re-
search agenda, they differ considerably in terms of 
morphosyntactic categories and features, tag defi-
nitions, level of granularity, annotation guidelines 
etc. Moreover, some of the tagsets (Tamil tagset) 
are language specific and do not scale across other 
Indian languages. This has led to a situation where 
despite strong commonalities between the lan-
guages addressed resources cannot be shared due 
to incompatibility of tasgets. This is detrimental to 
the development of language technology for Indian 
languages which already suffer from a lack of ade-
quate resources in terms of data and tools. 
In this paper, we present a common framework 
for all Indian languages where an attempt is made 
to treat equivalent morphosyntactic phenomena 
consistently across all languages. The hierarchical 
design, discussed in detail in the next section, also 
allows for a systematic method to annotate lan-
guage particular categories without disregarding 
the shared traits of the Indian languages.  
3 Design Principles 
Whilst several large projects have been concerned 
with tagset development very few have touched 
upon the design principles behind them. Leech 
(1997), Cloeren (1999) and Hardie (2004) are 
some important examples presenting universal 
principles for tagset design. 
In this section we restrict the discussion to the 
principles behind our tagset framework. Important-
ly, we diverge from some of the universal prin-
ciples but broadly follow them in a consistent way.  
Tagset structure: Flat tagsets just list down the 
categories applicable for a particular language 
without any provision for modularity or feature 
reusability. Hierarchical tagsets on the other hand 
are structured relative to one another and offer a 
well-defined mechanism for creating a common 
tagset framework for multiple languages while 
providing flexibility for customization according to 
the language and/ or application. 
Decomposability in a tagset alows different fea-
tures to be encoded in a tag by separate sub-stings. 
Decomposable tags help in better corpus analysis 
(Leech 1997) by allowing to search with an un-
derspecified search string. 
In our present framework, we have adopted the 
hierarchical layout as well as decomposable tags 
for designing the tagset. The framework will have 
three levels in the hierarchy with categories, types 
(subcategories) and features occupying the top, 
medium and the bottom layers. 
What to encode? One thumb rule for the POS 
tagging is to consider only the aspects of morpho-
syntax for annotation and not that of syntax, se-
mantics or discourse. We follow this throughout 
and focus only on the morphosyntactic aspects of 
the ILs for encoding in the framework. 
Morphology and Granularity: Indian languag-
es have complex morphology with varying degree 
of richness. Some of the languages such as those of 
the Dravidian family also display agglutination as 
an important characteristic. This entails that mor-
phological analysis is a desirable pre-process for 
the POS tagging to achieve better results in auto-
matic tagging. We encode all possible morphosyn-
tactic features in our framework assuming the exis-
The 6th Workshop on Asian Languae Resources, 2008
90
tence of morphological analysers and leave the 
choice of granularity to users. 
As pointed out by Leech (1997) some of the 
linguistically desirable distinctions may not be 
feasible computationally. Therefore, we ignore 
certain features that may not be computationally 
feasible at POS tagging level. 
Multi-words: We treat the constituents of Mul-
ti-word expressions (MWEs) like Indian Space 
Research Organization as individual words and tag 
them separately rather than giving a single tag to 
the entire word sequence. This is done because: 
Firstly, this is in accordance with the standard 
practice followed in earlier tagsets. Secondly, 
grouping MWEs into a single unit should ideally 
be handled in chunking. 
Form vs. function: We try to adopt a balance 
between form and function in a systematic and 
consistent way through deep analysis. Based on 
our analysis we propose to consider the form in 
normal circumstances and the function for words 
that are derived from other words. More details on 
this will be provided in the framework document 
(Baskaran et al2007) 
Theoretical neutrality: As Leech (1997) points 
out the annotation scheme should be theoretically 
neutral to make it clearly understandable to a larger 
group and for wider applicability. 
Diverse Language families: As mentioned ear-
lier, we consider eight languages coming from two 
major language families of India, viz. Indo-Aryan 
and Dravidian. Despite the distinct characteristics 
of these two families, it is however striking to note 
the typological parallels between them, especially 
in syntax. For example, both families follow SOV 
pattern. Also, several Indo-Aryan languages such 
as Marathi, Bangla etc. exhibit some agglutination, 
though not to the same extent of Dravidian. Given 
the strong commonalities between the two families 
we decided to use a single framework for them 
4 POS Tagset Framework for Indian lan-
guages 
The tagset framework is laid out at the following 
four levels similar to EAGLES. 
I. Obligatory attributes or values are generally 
universal for all languages and hence must be 
included in any morphosyntactic tagset. The 
major POS categories are included here. 
II. Recommended attributes or values are recog-
nised to be important sub-categories and fea-
tures common to a majority of languages.  
III. Special extensions1 
a. Generic attributes or values 
b. Language-specific attributes or values are 
the attributes that are relevant only for few lan-
guages and do not apply to most languages. 
All the tags were discussed and debated in detail 
by a group of linguists and computer scien-
tists/NLP experts for eight Indian languages viz. 
Bengali, Hindi, Kannada, Malayalam, Marathi, 
Sanskrit, Tamil and Telugu.  
Now, because of space constraints we present 
only the partial tagset framework. This is just to 
illustrate the nature of the framework and the com-
plete version as well as the rationale for different 
categories/features in the framework can be found 
in Baskaran et al (2007).2 
In the top level the following 12 categories are 
identified as universal categories for all ILs and 
hence these are obligatory for any tagset. 
 
1. [N] Nouns 7.   [PP] Postpositions  
2. [V] Verbs  8.   [DM] Demonstratives 
3. [PR] Pronouns  9.   [QT] Quantifiers 
4. [JJ] Adjectives  10. [RP] Particles  
5. [RB] Adverbs  11. [PU] Punctuations  
6. [PL] Participles  12. [RD] Residual3 
 
The partial tagset illustrated in Figure 1 high-
lights entries in recommended and optional catego-
ries for verbs and participles marked for three le-
vels.4 The features take the form of attribute-value 
pairs with values in italics and in some cases (such 
as case-markers for participles) not all the values 
are fully listed in the figure. 
5 Current Status and Future Work 
In the preceding sections we presented a common 
framework being designed for POS tagsets for In-
dian Languages. This hierarchical framework has 
                                                 
1
 We do not have many features defined under the special 
extensions and this is mainly retained for any future needs. 
2 Currently this is just the draft version and the final version 
will be made available soon 
3 For words or segments in the text occurring outside the gam-
bit of grammatical categories like foreign words, symbols,etc.   
4  These are not finalised as yet and there might be some 
changes in the final version of the framework. 
The 6th Workshop on Asian Languae Resources, 2008
91
three levels to permit flexibility and interoperabili-
ty between languages. We are currently involved in 
a thorough review of the present framework by 
using it to design the tagset for specific Indian lan-
guages. The issues that come up during this 
process will help refine and consolidate the 
framework further.  In the future, annotation guide-
lines with some recommendations for handling 
ambiguous categories will also be defined.  With 
the common framework in place, it is hoped that 
researchers working with Indian Languages would 
be able to not only reuse data annotated by each 
other but also share tools across projects and lan-
guages. 
References 
Baskaran S. et al 2007. Framework for a Common 
     Parts-of-Speech Tagset for Indic Languages. (Draft) 
    http://research.microsoft.com/~baskaran/POSTagset/ 
  
Cloeren, J. 1999. Tagsets. In Syntactic Wordclass Tagging, 
ed. Hans van Halteren, Dordrecht.: Kluwer Academic. 
Hardie, A . 2004. The Computational Analysis of Morpho-
syntactic Categories in Urdu. PhD thesis submitted to 
Lancaster University. 
Greene, B.B. and Rubin, G.M. 1981. Automatic grammati-
cal tagging of English. Providence, R.I.: Department of 
Linguistics, Brown University 
Garside, R. 1987 The CLAWS word-tagging system. In 
The Computational Analysis of English, ed. Garside, 
Leech and Sampson, London: Longman. 
Leech, G and Wilson, A. 1996. Recommendations for the 
Morphosyntactic Annotation of Corpora. EAGLES Re-
port EAG-TCWG-MAC/R. 
Leech, G. 1997. Grammatical Tagging. In Corpus Annota-
tion: Linguistic Information from Computer Text Cor-
pora, ed: Garside, Leech and McEnery, London: Long-
man  
Leech, G and Wilson, A. 1999. Standards for Tag-sets. In 
Syntactic Wordclass Tagging, ed. Hans van Halteren, 
Dordrecht: Kluwer Academic. 
Santorini, B. 1990. Part-of-speech tagging guidelines for 
the Penn Treebank Project. Technical report MS-CIS-
90-47, Department of Computer and Information 
Science, University of Pennsylvania 
IIIT-tagset. A Parts-of-Speech tagset for Indian languages. 
http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.
pdf 
Tamil tagset. AU-KBC Parts-of-Speech tagset for Tamil. 
http://nrcfosshelpline.in/smedia/images/downloads/Tam
il_Tagset-opensource.odt 
Aspect 
 Perfect 
 Imperfect 
 Progressive 
Mood 
 Declarative 
 Subjunctative/   
        Hortative 
 Conditional 
 Imperative 
 Presumptive 
Level - 3 
Nouns 
Verbs 
Pronouns 
Adjectives 
Adverbs 
Postpositions 
Demonstratives 
Quantifiers 
Particles 
Punctuations 
Residual Participles 
Level - 1 
Type 
 Finite 
 Auxiliary 
 Infinitive 
 Non-finite 
 Nominal 
Gender 
 Masculine 
 Feminine 
 Neuter 
Number 
 Singular 
 Plural/Hon. 
 Dual 
 Honourific 
Person 
 First 
 Second 
 Third 
Tense 
 Past 
 Present 
 Future 
Negative 
Type 
 General 
 Adjectival 
 Verbal 
 Nominal 
Gender 
 As in verbs 
Number 
 Singular 
 Plural 
 Dual 
Case 
 Direct 
 Oblique 
Case-markers 
 Ergative 
 Accusative 
 etc. 
Tense 
 As in verbs 
Negative 
Level - 2 
Fig-1. Tagset framework - partial representation 
The 6th Workshop on Asian Languae Resources, 2008
92
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 29?32,
Suntec, Singapore, 3 August 2009. c?2009 ACL and AFNLP
WikiBABEL: A Wiki-style Platform for Creation of Parallel Data 
 A Kumaran?      K Saravanan?      Naren Datha*      B Ashok*      Vikram Dendi? 
 
?Multilingual Systems  
Research 
Microsoft Research India 
 
*Advanced Development & 
Prototyping 
Microsoft Research India 
 
?Machine Translation  
Incubation   
Microsoft Research 
 
Abstract 
In this demo, we present a wiki-style platform ? 
WikiBABEL ? that enables easy collaborative 
creation of multilingual content in many non-
English Wikipedias, by leveraging the relatively 
larger and more stable content in the English 
Wikipedia.  The platform provides an intuitive 
user interface that maintains the user focus on 
the multilingual Wikipedia content creation, by 
engaging search tools for easy discoverability of 
related English source material, and a set of lin-
guistic and collaborative tools to make the con-
tent translation simple.  We present two different 
usage scenarios and discuss our experience in 
testing them with real users.  Such integrated 
content creation platform in Wikipedia may yield 
as a by-product, parallel corpora that are critical 
for research in statistical machine translation sys-
tems in many languages of the world.   
1 Introduction 
Parallel corpora are critical for research in many 
natural language processing systems, especially, 
the Statistical Machine Translation (SMT) and 
Crosslingual Information Retrieval (CLIR) sys-
tems, as the state-of-the-art systems are based on 
statistical learning principles; a typical SMT sys-
tem in a pair of language requires large parallel 
corpora, in the order of a few million parallel 
sentences.  Parallel corpora are traditionally 
created by professionals (in most cases, for busi-
ness or governmental needs) and are available 
only in a few languages of the world.  The prohi-
bitive cost associated with creating new parallel 
data implied that the SMT research was re-
stricted to only a handful of languages of the 
world.  To make such research possible widely, it 
is important that innovative and inexpensive 
ways of creating parallel corpora are found.  Our 
research explores such an avenue: by involving 
the user community in creation of parallel data. 
In this demo, we present a community colla-
boration platform ? WikiBABEL ? which 
enables the creation of multilingual content in 
Wikipedia.  WikiBABEL leverages two signifi-
cant facts with respect to Wikipedia data: First, 
there is a large skew between the content of Eng-
lish and non-English Wikipedias.  Second, while 
the original content creation requires subject 
matter experts, subsequent translations may be 
effectively created by people who are fluent in 
English and the target language.  In general, we 
do expect the large English Wikipedia to provide 
source material for multilingual Wikipedias; 
however on specific topics specific multilingual 
Wikipedia may provide the source material 
(http://ja.wikipedia.org/wiki/?? may be better 
than http://en.wikipedia.org/wiki/haiku).   We 
leverage these facts in the WikiBABEL frame-
work, enabling a community of interested native 
speakers of a language, to create content in their 
respective language Wikipedias.  We make such 
content creation easy by integrating linguistic 
tools and resources for translation, and collabora-
tive mechanism for storing and sharing know-
ledge among the users.  Such methodology is 
expected to generate comparable data (similar, 
but not the same content), from which parallel 
data may be mined subsequently (Munteanu et 
al, 2005) (Quirk et al 2007).   
We present here the WikiBABEL platform, 
and trace its evolution through two distinct usage 
versions: First, as a standalone deployment pro-
viding a community of users a translation plat-
form on hosted Wikipedia data to generate paral-
lel corpora, and second, as a transparent edit 
layer on top of Wikipedias to generate compara-
ble corpora.  Both paradigms were used for user 
testing, to gauge the usability of the tool and the 
viability of the approach for content creation in 
multilingual Wikipedias.  We discuss the imple-
mentations and our experience with each of the 
above scenarios.  Such experience may be very 
valuable in fine-tuning methodologies for com-
munity creation of various types of linguistic 
data.  Community contributed efforts may per-
haps be the only way to collect sufficient corpora 
effectively and economically, to enable research 
in many resource-poor languages of the world. 
29
2 Architecture of WikiBABEL 
The architecture of WikiBABEL is as illustrated 
in Figure 1: Central to the architecture is the Wi-
kiBABEL component that coordinates the interac-
tion between its linguistic and collaboration 
components, and the users and the Wikipedia 
system.  WikiBABEL architecture is designed to 
support a host of linguistic tools and resources 
that may be helpful in the content creation 
process: Bilingual dictionaries for providing for 
word-level translations, allowing user customiza-
tion of domain-specific, or even, user-specific 
bilingual dictionaries.  Also available are ma-
chine translation and transliteration systems for 
rough initial translation [or transliteration] of a 
source language string at sentential/phrasal levels 
[or names] to the intended target language.  As 
the quality of automatic translations are rarely 
close to human quality translations, the user may 
need to correct any such automatically translated 
or transliterated content, and an intuitive edit 
framework provides tools for such corrections.  
A collaborative translation memory component 
stores all the user corrections (or, sometimes, 
their selection from a set of alternatives) of ma-
chine translations, and makes them available to 
the community as a translation help (?tribe know-
ledge?).  Voting mechanisms are available that 
may prioritize more frequently chosen alterna-
tives as preferred suggestions for subsequent us-
ers.  The user-management tracks the user de-
mographic information, and their contributions 
(its quality and quantity) for possible recogni-
tion.  The user interface features are imple-
mented as light-weight components, requiring 
minimal server-side interaction.  Finally, the ar-
chitecture is designed open, to integrate any user-
developed tools and resources easily.  
 
 
 
3 WikiBABEL on Wikipedia 
IN this section we discuss Wikipedia content and 
user characteristics and outline our experience 
with the two versions on Wikipedia.   
3.1 Wikipedia: User & Data Characteristics 
Wikipedia content is acknowledged to be on par 
with the best of the professionally created re-
sources (Giles, 2005) and is used regularly as 
academic reference (Rainie et al, 2007).  How-
ever, there is a large disparity in content between 
English and other language Wikipedias. English 
Wikipedia - the largest - has about 3.5 Million 
topics, but with an exception of a dozen or so 
Western European and East Asian languages, 
most of the 250-odd languages have less than 1% 
of English Wikipedia content (Wikipedia, 2009).  
Such skew, despite the size of the respective user 
population, indicates a large room for growth in 
many multilingual Wikipedias.  On the contribu-
tion side, Wikipedia has about 200,000 contribu-
tors (> 10 total contributions); but only about 4% 
of them are very active (> 100 contributions per 
month).  The general perception that a few very 
active users contributed to the bulk of Wikipedia 
was disputed in a study (Swartz, 2006) that 
claims that large fraction of the content were 
created by those who made very few or occa-
sional contributions that are primarily editorial in 
nature.  It is our strategy to provide a platform 
for easy multilingual Wikipedia content creation 
that may be harvested for parallel data.   
3.2 Version 1: A Hosted Portal 
In our first version, a set of English Wikipedia 
topics (stable non-controversial articles, typically 
from Medicine, Healthcare, Science & Technol-
ogy, Literature, etc.) were chosen and hosted in 
our WikiBABEL portal.  Such set of articles is 
already available as Featured Articles in most 
Wikipedias.  English Wikipedia has a set of 
~1500 articles that are voted by the community 
as stable and well written, spanning many do-
mains, such as, Literature, Philosophy, History, 
Science, Art, etc.  The user can choose any of 
these Wikipedia topics to translate to the target 
language and correct the machine translation er-
rors.  Once a topic is chosen, a two-pane window 
is presented to the user, as shown in Figure 2, in 
which the original English Wikipedia article is 
shown in the left panel and a rough translation of 
the same article in the user-chosen target lan-
guage is presented in the right panel.  The right 
panel has the same look and feel as the original 
30
English Wikipedia article, and is editable, while 
the left panel is primarily intended for providing 
source material for reference and context, for the 
translation correction.  On mouse-over the paral-
lel sentences are highlighted, linking visually the 
related text on both panels.  On a mouse-click, an 
edit-box is opened in-place in the right panel, 
and the current content may be edited.  As men-
tioned earlier, integrated linguistic tools and re-
sources may be invoked during edit process, to 
help the user.  Once the article reaches sufficient 
quality as judged by the users, the content may 
be transferred to target language Wikipedia, ef-
fectively creating a new topic in the target lan-
guage Wikipedia. 
User Feedback: We field tested our first ver-
sion with a set of Wikipedia users, and a host of 
amateur and professional translators.  The prima-
ry feedback we got was that such efforts to create 
content in multilingual Wikipedia was well ap-
preciated.  The testing provided much quantita-
tive (in terms of translation time, effort, etc.) and 
qualitative (user experience) measures and feed-
back.  The details are available in (Kumaran et 
al., 2008), and here we provide highlights only: 
? Integrated linguistic resources (e.g., bilingual 
dictionaries, transliteration systems, etc.) 
were appreciated by all users. 
? Amateur users used the automatic translations 
(in direct correlation with its quality), and 
improved their throughput up to 40%.  
? In contrast, those who were very fluent in 
both the languages were distracted by the 
quality of translations, and were slowed by 
30%.  In most cases, they preferred to redo 
the entire translations, rather than considering 
and correcting the rough translation.  
? One qualitative feedback from the Wikipedia 
community is that the sentence-by-sentence 
translation enforced by the portal is not in 
tune with their philosophy of user-decided 
content for the target topic.  
We used the feedback from the version 1, to re-
design WikiBABEL in version 2. 
3.3 Version 2: As a Transparent Edit Layer 
In our second version, we implemented the 
significant feedback from Wikipedians, pertain-
ing to source content selection and the user con-
tribution.  In this version, we delivered the Wi-
kiBABEL experience as an add-on to Wikipedia, 
as a semi-transparent overlay that augments the 
basic Wikipedia edit capabilities without taking 
the contributor away from the site. Capable of 
being launched with one click (via a bookmark-
let, or a browser plug-in, or as a potential server 
side integration with Wikipedia), the new version 
offered a more seamless workflow and integrated 
linguistic and collaborative components.  This 
add-on may be invoked on Wikipedia itself, pro-
viding all WikiBABEL functionalities.  In a typi-
cal WikiBABEL usage scenario, a Wikipedia 
31
content creator may be at an English Wikipedia 
article for which no corresponding article exists 
in the target language, or at target language Wi-
kipedia article which has much less content 
compared to the corresponding English article.  
The WikiBABEL user interface in this version 
is as shown in Figure 3.  The source English Wi-
kipedia article is shown in the left panel tabs, and 
may be toggled between English and the target 
language; also it may be viewed in HTML or in 
Wiki-markup.  The right panel shows the target 
language Wikipedia article (if it exists), or a 
newly created stub (otherwise); either case, the 
right panel presents a native target language Wi-
kipedia edit page, for the chosen topic.  The left 
panel content is used as a reference for content 
creation in target language Wikipedia in the right 
panel.  The user may compose the target lan-
guage Wikipedia article, either by dragging-and-
dropping translated content from the left to the 
right panel (into the target language Wikipedia 
editor), or add new content as a typical Wikipe-
dia user would.  To enable the user to stay within 
WikiBABEL for their content research, we have 
provided the capability to search through other 
Wikipedia articles in the left panel.  All linguistic 
and collaborative features are available to the 
users in the right panel, as in the previous ver-
sion.  The default target language Wikipedia pre-
view is at any time.  While the user testing of this 
implementation is still in the preliminary stages, 
we wish to make the following observations on 
the methodology: 
? There is a marked shift of focus from 
?translation from English Wikipedia article? 
to ?content creation in target Wikipedia?. 
? The user is never taken away from Wiki-
pedia site, requiring optionally only Wikipe-
dia credentials. The content is created direct-
ly in the target Wikipedia. 
 
The WikiBABEL Version 2 prototype will be 
made available externally in the future.   
References  
Kumaran, A, Saravanan, K and Maurice, S. WikiBA-
BEL: Community Creation of Multilingual Data.  
WikiSYM 2008 Conference, 2008.  
Munteanu, D. and Marcu, D. Improving the MT per-
formance by exploiting non-parallel corpora. 
Computational Linguistics. 2005. 
Giles, J. Internet encyclopaedias go head to head.  
Nature. 2005. doi:10.1038/438900a.  
Quirk, C., Udupa, R. U. and Menezes, A. Generative 
models of noisy translations with app. to parallel 
fragment extraction.  MT Summit XI, 2007.   
Rainie, L. and Tancer, B. Pew Internet and American 
Life.  http://www.pewinternet.org/.   
Swartz, A. Raw thought: Who writes Wikipedia? 
2006. http://www.aaronsw.com/.  
Wikipedia Statistics, 2009.http://stats.wikimedia.org/.  
32
