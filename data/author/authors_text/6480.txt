Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 430?433,
Prague, June 2007. c?2007 Association for Computational Linguistics
UPV-SI: Word Sense Induction using Self Term Expansion?
David Pinto(1,2) and Paolo Rosso1
1Polytechnic University of Valencia
DSIC, Valencia, Spain, 46022
2B. Autonomous University of Puebla
FCC, Puebla, Mexico, 72570
{dpinto, prosso}@dsic.upv.es
He?ctor Jime?nez-Salazar
Autonomous Metropolitan University
Department of Information Technologies
Cuajimalpa, DF, Mexico, 11850
hgimenezs@gmail.com
Abstract
In this paper we are reporting the re-
sults obtained participating in the ?Eval-
uating Word Sense Induction and Dis-
crimination Systems? task of Semeval
2007. Our totally unsupervised system
performed an automatic self-term expan-
sion process by mean of co-ocurrence
terms and, thereafter, it executed the
unsupervised KStar clustering method.
Two ranking tables with different eval-
uation measures were calculated by the
task organizers, every table with two
baselines and six runs submitted by dif-
ferent teams. We were ranked third
place in both ranking tables obtaining a
better performance than three different
baselines, and outperforming the average
score.
1 Introduction
Word Sense Disambiguation (WSD) is a partic-
ular problem of computational linguistics which
consists in determining the correct sense for a
given ambiguous word. It is well-known that su-
pervised algorithms have obtained the best re-
sults in public evaluations, but their accuracy
is close related with the amount of hand-tagged
data available. The construction of that kind
of training data is difficult for real applications.
The unsupervised WSD overcomes this draw-
back by using clustering algorithms which do
?This work has been partially supported by the MCyT
TIN2006-15265-C06-04 project, as well as by the BUAP-
701 PROMEP/103.5/05/1536 grant
not need training data in order to determine the
possible sense for a given ambiguous word.
This paper describes a simple technique for
unsupervised sense induction for ambiguous
words. The approach is based on a self term ex-
pansion technique which constructs a set of co-
ocurrence terms and, thereafter, it uses this set
to expand the target dataset. The implemented
system was performed in the task ?SemEval-
2007 Task 2: Evaluating Word Sense Induc-
tion and Discrimination Systems?(Agirre and
A., 2007). The aim of the task was to per-
mit a comparison across sense-induction and dis-
crimination systems. Moreover, the comparison
with other supervised and knowledge-based sys-
tems may be also done, since the test corpus was
borrowed from the well known ?English lexical-
sample? task in SemEval-2007, with the usual
training + test split.
The self term expansion method consists in
replacing terms of a document by a set of co-
related terms. The goal is to improve natu-
ral language processing tasks such as cluster-
ing narrow-domain short texts. This process
may be done by mean of different ways, of-
ten just by using a knowledge database. In
information retrieval, for instance, the expan-
sion of query terms is a very investigated topic
which has shown to improve results with respect
to when query expansion is not employed (Qiu
and Frei, 1993; Ruge, 1992; R.Baeza-Yates and
Ribeiro-Neto, 1999; Grefenstette, 1994; Rijsber-
gen, 1979).
The availability of Machine Readable Re-
sources (MRR) like ?Dictionaries?, ?Thesauri?
and ?Lexicons? has allowed to apply term ex-
430
pansion to other fields of natural language pro-
cessing like WSD. In (Banerjee and Pedersen,
2002) we may see the typical example of using
a external knowledge database for determining
the correct sense of a word given in some con-
text. In this approach, every word close to the
one we would like to determine its correct sense
is expanded with its different senses by using
the WordNet lexicon (Fellbaum, 1998). Then,
an overlapping factor is calculated in order to
determine the correct sense of the ambiguous
word. Different other approaches have made use
of a similar procedure. By using dictionaries,
the proposals presented in (Lesk, 1986; Wilks et
al., 1990; Nancy and Ve?ronis, 1990) are the most
sucessful in WSD. Yarowsky (Yarowsky, 1992)
used instead thesauri for their experiments. Fi-
nally, in (Sussna, 1993; Resnik, 1995; Baner-
jee and Pedersen, 2002) the use of lexicons in
WSD has been investigated. Although in some
cases the knowledge resource seems not to be
used strictly for term expansion, the aplication
of co-occurrence terms is included in their algo-
rithms. Like in information retrieval, the appli-
cation of term expansion in WSD by using co-
related terms has shown to improve the baseline
results if we carefully select the external resource
to use, with a priori knowledge of the domain
and the broadness of the corpus (wide or nar-
row domain). Evenmore, we have to be sure that
the Lexical Data Base (LDB) has been suitable
constructed. Due to the last facts, we consider
that the use of a self automatically constructed
LDB (using the same test corpora), may be of
high benefit. This assumption is based on the
intrinsic properties extracted from the corpus it-
self. Our proposal is related somehow with the
investigations presented in (Schu?tze, 1998) and
(Purandare and Pedersen, 2004), where words
are also expanded with co-ocurrence terms for
word sense discrimination. The main difference
consists in the use of the same corpora for con-
structing the co-ocurrence list.
Following we describe the self term expan-
sion method used and, thereafter, the results
obtained in the task #2 of Semeval 2007 com-
petition.
2 The Self Term Expansion Method
In literature, co-ocurrence terms is the most
common technique used for automatic construc-
tion of LDBs (Grefenstette, 1994; Frakes and
Baeza-Yates, 1992). A simple approach may use
n-grams, which allows to predict a word from
previous words in a sample of text. The fre-
quency of each n-gram is calculated and then
filtered according to some threshold. The re-
sulting n-grams constitutes a LDB which may
be used as an ?expansion dictionary? for each
term.
On the other hand, an information theory-
based co-ocurrence measure is discussed in
(Manning and Schu?tze, 2003). This measure
is named pointwise Mutual Information (MI),
and its applications for finding collocations are
analysed by determining the co-ocurrence de-
gree among two terms. This may be done by cal-
culating the ratio between the number of times
that both terms appear together (in the same
context and not necessarily in the same order)
and the product of the number of times that
each term ocurrs alone. Given two terms X1
and X2, the pointwise mutual information be-
tween X1 and X2 can be calculated as follows:
MI(X1,X2) = log2
P (X1X2)
P (X1)? P (X2)
The numerator could be modified in order to
take into account only bigrams, as presented
in (Pinto et al, 2006), where an improvement
of clustering short texts in narrow domains has
been obtained.
We have used the pointwise MI for obtaining
a co-ocurrence list from the same target dataset.
This list is then used to expand every term of the
original data. Since the co-ocurrence formula
captures relations between related terms, it is
possible to see that the self term expansion mag-
nifies less the noisy than the meaninful informa-
tion. Therefore, the execution of the clustering
algorithm in the expanded corpus should out-
perform the one executed over the non-expanded
data.
In order to fully appreciate the self term ex-
pansion method, in Table 1 we show the co-
431
ocurrence list for some words related with the
verb ?kill? of the test corpus. Since the MI
is calculated after preprocessing the corpus, we
present the stemmed version of the terms.
Word Co-ocurrence terms
soldier kill
rape women think shoot peopl old man
kill death beat
grenad todai live guerrilla fight explod
death shoot run rape person peopl outsid
murder life lebanon kill convict...
temblor tuesdai peopl least kill earthquak
Table 1: An example of co-ocurrence terms
For the task #2 of Semeval 2007, a set of 100
ambiguous words (35 nouns and 65 verbs) were
provided. We preprocessed this original dataset
by eliminating stopwords and then applying the
Porter stemmer (Porter, 1980). Thereafter,
when we used the pointwise MI, we determined
that the single ocurrence of each term should
be at least three (see (Manning and Schu?tze,
2003)), whereas the maximum separation among
the two terms was five. Finally, we selected
the unsupervised KStar clustering method (Shin
and Han, 2003) for our experiments, defining the
average of similarities among all the sentences
for a given ambiguous word as the stop criterion
for this clustering method. The input similarity
matrix for the clustering method was calculated
by using the Jaccard coefficient.
3 Evaluation
The task organizers decided to use two differ-
ent measures for evaluating the runs submitted
to the task. The first measure is called unsuper-
vised one, and it is based on the Fscore measure.
Whereas the second measure is called supervised
recall. For further information on how these
measures are calculated refer to (Agirre et al,
2006a; Agirre et al, 2006b). Since these mea-
sures give conflicting information, two different
evaluation results are reported in this paper.
In Table 2 we may see our ranking and the Fs-
core measure obtained (UPV-SI). We also show
the best and worst team Fscores; as well as the
total average and two baselines proposed by the
task organizers. The first baseline (Baseline1)
assumes that each ambiguous word has only one
sense, whereas the second baseline (Baseline2) is
a random assignation of senses. We are ranked
as third place and our results are better scored
than the other teams except for the best team
score. However, given the similar values with
the ?Baseline1?, we may assume that that team
presented one cluster per ambiguous word as its
result as the Baseline1 did; whereas we obtained
9.03 senses per ambiguous word in average.
Name Rank All Nouns Verbs
Baseline1 1 78.9 80.7 76.8
Best Team 2 78.7 80.8 76.3
UPV-SI 3 66.3 69.9 62.2
Average - 63.6 66.5 60.3
Worst Team 7 56.1 65.8 45.1
Baseline2 8 37.8 38.0 37.6
Table 2: Unsupervised evaluation (Fscore per-
formance).
In Table 3 we show our ranking and the super-
vised recall obtained (UPV-SI). We again show
the best and worst team recalls. The total av-
erage and one baseline is also presented (the
other baseline obtained the same Fscore). In
this case, the baseline tags each test instance
with the most frequent sense obtained in a train
split. We are ranked again in third place and
our score is slightly above the baseline.
Name Rank All Nouns Verbs
Best Team 1 81.6 86.8 76.2
UPV-SI 3 79.1 82.5 75.3
Average - 79.1 82.8 75.0
Baseline 4 78.7 80.9 76.2
Worst Team 6a 78.5 81.8 74.9
Worst Team 6b 78.5 81.4 75.2
Table 3: Supervised evaluation (Recall).
The results show that the technique employed
have learned, since our simple approach ob-
tained a better performance than the baselines,
especially the one that have chosen the most fre-
quent sense as baseline.
432
4 Conclusions
We have reported the performance of a single
approach based on self term expansion. The
technique uses the pointwise mutual information
for calculating a set of co-ocurrence terms which
then are used to expand the original dataset.
Once the expansion has been done, the unsu-
pervised KStar clustering method was used to
induce the sense for the different ocurrences of
each ambiguous word. We obtained the third
place in the two measures proposed in the task.
We will further investigate whether an improve-
ment may be obtained by applying term selec-
tion methods to the expanded corpus.
References
E. Agirre and Soroa A. 2007. SemEval-2007 Task 2:
Evaluating Word Sense Induction and Discrimina-
tion Systems. In SemEval-2007. Association for
Computational Linguistics.
E. Agirre, O. Lopez de Lacalle Lekuona, D. Mar-
tinez, and A. Soroa. 2006a. Evaluating and opti-
mizing the parameters of an unsupervised graph-
based WSD algorithm. In Textgraphs 2006 work-
shop, NAACL06, pages 89?96.
E. Agirre, O. Lopez de Lacalle Lekuona, D. Mar-
tinez, and A. Soroa. 2006b. Two graph-based
algorithms for state-of-the-art WSD. In EMNLP,
pages 585?593. ACL.
S. Banerjee and T. Pedersen. 2002. An Adapted
Lesk Algorithm for Word Sense Disambiguation
Using WordNet. In CICLing 2002 Conference,
volume 3878 of LNCS, pages 136?145. Springer-
Verlang.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
W. B. Frakes and R. A. Baeza-Yates. 1992. Infor-
mation Retrieval: Data Structures & Algorithms.
Prentice-Hall.
G. Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic.
M. Lesk. 1986. Automatic sense disambiguation:
How to tell a pine cone from an ice cream cone.
In ACM SIGDOC Conference, pages 24?26. ACM
Press.
D. C. Manning and H. Schu?tze. 2003. Foundations
of Statistical Natural Language Processing. MIT
Press. Revised version May 1999.
I. Nancy and J. Ve?ronis. 1990. Mapping dictionar-
ies: A spreading activation approach. In 6th An-
nual Conference of the Centre for the New Oxford
English Dictionary, pages 52?64.
D. Pinto, H. Jime?nez-Salazar, and P. Rosso. 2006.
Clustering abstracts of scientific texts using the
transition point technique. In CICLing, volume
3878 of LNCS, pages 536?546. Springer-Verlang.
M. F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3).
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing, pages 41?48, Boston, MA.
Y. Qiu and H. P. Frei. 1993. Concept based Query
Expansion. In ACM SIGIR on R&D in informa-
tion retrieval, pages 160?169. ACM Press.
R.Baeza-Yates and B. Ribeiro-Neto. 1999. Mod-
ern information retrieval. New York: ACM Press;
Addison-Wesley.
P. Resnik. 1995. Disambiguating Noun Groupings
with Respect to WordNet Senses. In 3rd Work-
shop on Very Large Corpora, pages 54?68. ACL.
C. J. Van Rijsbergen. 1979. Information Retrieval,
2nd edition. Dept. of Computer Science, Univer-
sity of Glasgow.
G. Ruge. 1992. Experiments on linguistically-
based term associations. Information Processing
& Management, 28(3):317?332.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
K. Shin and S. Y. Han. 2003. Fast clustering algo-
rithm for information organization. In CICLing,
volume 2588 of LNCS, pages 619?622. Springer-
Verlang.
M. Sussna. 1993. Word sense disambiguation for
free-test indexing using a massive semantic net-
work. In 2nd International Conference on Infor-
mation and Knowledge Management, pages 67?74.
Y. Wilks, D. Fass, C. Guo, J. McDonald, T. Plate,
and B. Slator. 1990. Providing machine tractable
dictionary tools. Machine Translation, 5(2):99?
154.
D. Yarowsky. 1992. Word-sense disambiguation us-
ing statistical models of Rogets categories trained
on large corpora. In 14th Conference on Compu-
tational Linguistics, pages 454?460. ACL.
433
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 112?116,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
FCC: Modeling Probabilities with GIZA++ for Task #2 and #3 of
SemEval-2
Darnes Vilarin?o, Carlos Balderas, David Pinto, Miguel Rodr??guez, Saul Leo?n
Faculty of Computer Science, BUAP
Puebla, Mexico
{darnes,mrodriguez,dpinto}@cs.buap.mx
Abstract
In this paper we present a na??ve approach
to tackle the problem of cross-lingual
WSD and cross-lingual lexical substitu-
tion which correspond to the Task #2 and
#3 of the SemEval-2 competition. We used
a bilingual statistical dictionary, which is
calculated with Giza++ by using the EU-
ROPARL parallel corpus, in order to cal-
culate the probability of a source word to
be translated to a target word (which is as-
sumed to be the correct sense of the source
word but in a different language). Two ver-
sions of the probabilistic model are tested:
unweighted and weighted. The obtained
values show that the unweighted version
performs better thant the weighted one.
1 Introduction
Word Sense Disambiguation (WSD) is con-
sidered one of the most important prob-
lems in Natural Language Processing
(Agirre and Edmonds, 2006). It is claimed
that WSD is essential for those applications that
require of language comprehension modules
such as search engines, machine translation
systems, automatic answer machines, second life
agents, etc. Moreover, with the huge amounts
of information in Internet and the fact that this
information is continuosly growing in different
languages, we are encourage to deal with cross-
lingual scenarios where WSD systems are also
needed. Despite the WSD task has been studied
for a long time, the expected feeling is that WSD
should be integrated into real applications such as
mono and multi-lingual search engines, machine
translation systems, automatic answer machines,
etc (Agirre and Edmonds, 2006). Different stud-
ies on this issue have demonstrated that those
applications benefit from WSD, such as in the
case of machine translation (Chan et al, 2007;
Carpuat and Wu., 2007). On the other hand,
Lexical Substitution (LS) refers to the process
of finding a substitute word for a source word
in a given sentence. The LS task needs to be
approached by firstly disambiguating the source
word, therefore, these two tasks (WSD and LS)
are somehow related.
Since we are describing the modules of our
system, we did not provide information of the
datasets used. For details about the corpora,
see the task description paper for both tasks (#2
and #3) in this volume (Mihalcea et al, 2010;
Lefever and Hoste, 2010). Description about the
other teams are also described in the same papers.
2 A Na??ve Approach to WSD and LS
In this section it is presented an overview of the
presented system, but also we further discuss the
particularities of the general approach for each
task evaluated. We will start this section by
explaining the manner we deal with the Cross-
Lingual Word Sense Disambiguation (C-WSD)
problem.
2.1 Cross-Lingual Word Sense
Disambiguation
We have approached the cross-lingual word sense
disambiguation task by means of a probabilistic
system which considers the probability of a word
sense (in a target language), given a sentence (in a
source language) containing the ambiguous word.
In particular, we used the Naive Bayes classifier
in two different ways. First, we calculated the
probability of each word in the source language
of being associated/translated to the corresponding
word (in the target language). The probabilities
were estimated by means of a bilingual statistical
dictionary which is calculated using the Giza++
system over the EUROPARL parallel corpus. We
filtered this corpus by selecting only those sen-
112
tences which included some senses of the ambigu-
ous word which were obtained by translating this
ambiguous word on the Google search engine.
In Figure 1 we may see the complete process for
approaching the problem of cross-lingual WSD.
The second approach considered a weighted
probability for each word in the source sentence.
The closer a word of the sentence to the ambigu-
ous word, the higher the weight given to it.
In other words, given an English sentence S =
{w
1
, w
2
, ? ? ? , w
k
, ? ? ? , w
k+1
, ? ? ? } with the am-
biguous word w
k
in position k. Let us consider
N candidate translations of w
k
, {t
k
1
, t
k
2
, ? ? ? , t
k
N
}
obtained somehow (we will further discuss about
this issue in this section). We are insterested on
finding the most probable candidate translations
for the polysemous word w
k
. Therefore, we may
use a Na??ve Bayes classifier which considers the
probability of tk
i
given w
k
. A formal description
of the classifier is given as follows.
p(t
k
i
|S) = p(t
k
i
|w
1
, w
2
, ? ? ? , w
k
, ? ? ? ) (1)
p(t
k
i
|S) =
p(t
k
i
)p(w
1
, w
2
, ? ? ? , w
k
, ? ? ? |t
k
i
)
p(w
1
, w
2
, ? ? ? , w
k
, ? ? ? )
(2)
We are interested on finding the argument that
maximizes p(tk
i
|S), therefore, we may to calculate
the denominator. Moreover, if we assume that all
the different translations are equally distributed,
then Eq. (2) may be approximated by Eq. (3).
p(t
k
i
|w
1
, w
2
, ? ? ? , w
k
, ? ? ? ) ? p(w
1
, w
2
, ? ? ? , w
k
, ? ? ? |t
k
i
)
(3)
The complete calculation of Eq. (3) requires to
apply the chain rule. However, if we assumed that
the words of the sentence are independent, then we
may rewrite Eq. (3) as Eq. (4).
p(t
k
i
|w
1
, w
2
, ? ? ? , w
k
, ? ? ? ) ?
|S|
?
j=1
p(w
j
|t
k
i
) (4)
The best translation is obtained as shown in Eq.
(5). Nevertheless the position of the ambiguous
word, we are only considering a product of the
probabilites of translation. Thus, we named this
approach, the unweighted version. Algorithm 1
provides details about the implementation.
BestSense
u
(w
k
) = arg max
t
k
i
|S|
?
j=1
p(w
j
|t
k
i
) (5)
with i = 1, ? ? ? , N .
Algorithm 1: An unweighted na??ve Bayes ap-
proach to cross-lingual WSD
Input: A set Q of sentences:
Q = {S
1
, S
2
, ? ? ? };
Dictionary = p(w|t): A bilingual statistical
dictionary;
Output: The best word/sense for each
ambiguous word w
j
? S
l
for l = 1 to |Q| do1
for i = 1 to N do2
P
l,i
= 1;3
for j = 1 to |S
l
| do4
foreach w
j
? S
l
do5
if w
j
? Dictionary then6
P
l,i
= P
l,i
? p(w
j
|t
k
i
);7
else8
P
l,i
= P
l,i
? ?;9
end10
end11
end12
end13
end14
return arg max
t
k
i
?
|S|
j=1
p(w
j
|t
k
i
)15
A second approach (weighted version) is also
proposed as shown in Eq. (6). Algorithm 2 pro-
vides details about its implementation.
BestSense
w
(w
k
) =
arg max
t
k
i
|S|
?
j=1
p(w
j
|t
k
i
) ?
1
k ? j + 1
(6)
With respect to the N candidate translations
of the polysemous word w
k
, {t
k
1
, t
k
2
, ? ? ? , t
k
N
}, we
have used of the Google translator1 . Google pro-
vides all the possible translations for w
k
with
the corresponding grammatical category. There-
fore, we are able to use those translations that
match with the same grammatical category of the
1http://translate.google.com.mx/
113
Figure 1: An overview of the presented approach for cross-lingual word sense disambiguation
Algorithm 2: A weighted na??ve Bayes ap-
proach to cross-lingual WSD
Input: A set Q of sentences:
Q = {S
1
, S
2
, ? ? ? };
Dictionary = p(w|t): A bilingual statistical
dictionary;
Output: The best word/sense for each
ambiguous word w
j
? S
l
for l = 1 to |Q| do1
for i = 1 to N do2
P
l,i
= 1;3
for j = 1 to |S
l
| do4
foreach w
j
? S
l
do5
if w
j
? Dictionary then6
P
l,i
=7
P
l,i
? p(w
j
|t
k
i
) ?
1
k?j+1
;
else8
P
l,i
= P
l,i
? ?;9
end10
end11
end12
end13
end14
return arg max
t
k
i
?
|S|
j=1
p(w
j
|t
k
i
) ?
1
k?j+1
15
ambiguous word. Even if we attempted other
approaches such as selecting the most probable
translations from the statistical dictionary, we con-
firmed that by using the Google online transla-
tor we obtain the best results. We consider that
this result is derived from the fact that Google has
a better language model than we have, because
our bilingual statistical dictionary was trained only
with the EUROPARL parallel corpus.
The experimental results of both, the un-
weighted and the weighted versions of the pre-
sented approach for cross-lingual word sense dis-
ambiguation are given in Section 3.
2.2 Cross-Lingual Lexical Substitution
This module is based on the cross-lingual word
sense disambiguation system. Once we knew
the best word/sense (Spanish) for the ambigu-
ous word(English), we lemmatized the Spanish
word. Thereafter, we searched, at WordNet, the
synonyms of this word (sense) that agree with
the grammatical category (noun, verb, etc) of the
query (source polysemous word), and we return
those synonyms as possible lexical substitutes.
Notice again that this task is complemented by the
WSD solver.
In Figure 2 we may see the complete process of
approaching the problem of cross-lingual lexical
substitution.
114
Figure 2: An overview of the presented approach for cross-lingual lexical substitution
3 Experimental Results
In this section we present the obtained results for
both, the cross-lingual word sense disambiguation
task and the cross-lingual lexical substitution task.
3.1 Cross-Lingual Word Sense
Disambiguation
In Table 2 we may see the results we have ob-
tained with the different versions of the presented
approach. In the same Table we can find a com-
parison of our runs with others presented at the
SemEval-2 competition. In particular, we have
tested four different runs which correspond to two
evaluations for each different version of the prob-
abilistic classifier. The description of each run is
given in Table 1.
We obtained a better performance with those
runs that were evaluated with the five best trans-
lations (oof) than with those that were evaluated
with only the best ones. This fact lead us to con-
sider in further work to improve the ranking of the
translations found by our system. On other hand,
the unweighted version of the proposed classifier
improved the weighted one. This behavior was un-
expected, because in the development dataset, the
results were opposite. We consider that the prob-
lem comes from taking into account the entire sen-
tence instead of a neighborhood (windows) around
the ambiguous word. We will further investigate
about this issue. We got a better performance than
other systems, and those runs that outperformed
our system runs did it by around 3% of precision
and recall in the case of the oof evaluation.
3.2 Cross-Lingual Lexical Substitution
In Table 3 we may see the obtained results for
the cross-lingual lexical substitution task. The ob-
tained results are low in comparison with the best
one. Since this task relies on the C-WSD task, then
a lower performance on the C-WSD task will con-
duct to a even lower performance in C-LS. Firstly,
we need to improve the C-WSD solver. In partic-
ular, we need to improve the ranking procedure in
order to obtain a better translation of the source
ambiguous word. Moreover, we consider that the
use of language modeling would be of high ben-
efit, since we could test whether or not a given
translation together with the terms in its context
would have high probability in the target language.
115
Run name Description
FCC-WSD1 : Best translation (one target word) / unweighted version
FCC-WSD2 : Five best translations (five target words - oof) / unweighted version
FCC-WSD3 : Best translation (one target word) / weighted version
FCC-WSD4 : Five best translations (five target words - oof) / weighted version
Table 1: Description of runs
System name Precision (%) Recall (%)
UvT-v 23.42 23.42
UvT-g 19.92 19.92
FCC-WSD1 15.09 15.09
FCC-WSD3 14.43 14.43
UHD-1 20.48 16.33
UHD-2 20.2 16.09
T3-COLEUR 19.78 19.59
System name Precision (%) Recall (%)
UvT-v 42.17 42.17
UvT-g 43.12 43.12
FCC-WSD2 40.76 40.76
FCC-WSD4 38.46 38.46
UHD-1 38.78 31.81
UHD-2 37.74 31.3
T3-COLEUR 35.84 35.46
a) Best translation b) Five best translations (oof)
Table 2: Evaluation of the cross-lingual word sense disambiguation task
System name Precision (%) Recall (%)
SWAT-E 174.59 174.59
SWAT-S 97.98 97.98
UvT-v 58.91 58.91
UvT-g 55.29 55.29
UBA-W 52.75 52.75
WLVUSP 48.48 48.48
UBA-T 47.99 47.99
USPWLV 47.6 47.6
ColSlm 43.91 46.61
ColEur 41.72 44.77
TYO 34.54 35.46
IRST-1 31.48 33.14
FCC-LS 23.9 23.9
IRSTbs 8.33 29.74
DICT 44.04 44.04
DICTCORP 42.65 42.65
Table 3: Evaluation of the cross-lingual lexical
substitution task (the ten best results - oot)
4 Conclusions and Further Work
In this paper we have presented a system for cross-
lingual word sense disambiguation and cross-
lingual lexical substitution. The approach uses a
Na??ve Bayes classifier which is fed with the prob-
abilities obtained from a bilingual statistical dic-
tionary. Two different versions of the classifier,
unweighted and weighted were tested. The results
were compared with those of an international com-
petition, obtaining a good performance. As fur-
ther work, we need to improve the ranking mod-
ule of the cross-lingual WSD classifier. Moreover,
we consider that the use of a language model for
Spanish would highly improve the results on the
cross-lingual lexical substitution task.
Acknowledgments
This work has been partially supported by CONA-
CYT (Project #106625) and PROMEP (Grant
#103.5/09/4213).
References
[Agirre and Edmonds2006] E. Agirre and P. Edmonds.
2006. Word Sense Disambiguation, Text, Speech
and Language Technology. Springer.
[Carpuat and Wu.2007] M. Carpuat and D. Wu. 2007.
Improving statistical machine translation using word
sense disambiguation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLPCoNLL), pages 61?72.
[Chan et al2007] Y.S. Chan, H.T. Ng, and D. Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 33?40.
[Lefever and Hoste2010] E. Lefever and V. Hoste.
2010. Semeval-2010 task3:cross-lingual word
sense disambiguation. In Proceedings of the
Fifth International Workshop on Semantic Evalu-
ations (SemEval-2010). Association for Computa-
tional Linguistics.
[Mihalcea et al2010] R. Mihalcea, R. Sinha, and
D. McCarthy. 2010. Semeval-2010 task2:cross-
lingual lexical substitution. In Proceedings of the
Fifth International Workshop on Semantic Evalu-
ations (SemEval-2010). Association for Computa-
tional Linguistics.
116
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 174?177,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BUAP: An Unsupervised Approach to Automatic Keyphrase Extraction
from Scientific Articles
Roberto Ortiz, David Pinto, Mireya Tovar
Faculty of Computer Science, BUAP
Puebla, Mexico
korn resorte2003@hotmail.com,
{dpinto, mtovar}@cs.buap.mx
He?ctor Jime?nez-Salazar
Information Technologies Dept., UAM
DF, Mexico
hgimenezs@gmail.com
Abstract
In this paper, it is presented an unsuper-
vised approach to automatically discover
the latent keyphrases contained in scien-
tific articles. The proposed technique is
constructed on the basis of the combi-
nation of two techniques: maximal fre-
quent sequences and pageranking. We
evaluated the obtained results by using
micro-averaged precision, recall and F-
scores with respect to two different gold
standards: 1) reader?s keyphrases, and 2)
a combined set of author?s and reader?s
keyphrases. The obtained results were
also compared against three different base-
lines: one unsupervised (TF-IDF based)
and two supervised (Na??ve Bayes and
Maximum Entropy).
1 Introduction
The task of automatic keyphrase extraction has
been studied for several years. Firstly, as semantic
metadata useful for tasks such as summarization
(Barzilay and Elhadad, 1997; Lawrie et al, 2001;
DAvanzo and Magnini, 2005), but later rec-
ognizing the impact that good keyphrases
would have on the quality of various Nat-
ural Language Processing (NLP) applica-
tions (Frank et al, 1999; Witten et al, 1999;
Turney, 1999; Barker and Corrnacchia, 2000;
Medelyan and Witten, 2008). Thus, the selection
of important, topical phrases from within the
body of a document may be used in order to
improve the performance of systems dealing
with different NLP problems such as, clustering,
question-answering, named entity recognition,
information retrieval, etc.
In general, a keyphrase may be considered as
a sequence of one or more words that capture the
main topic of the document, as that keyphrase is
expected to represent one of the key ideas ex-
pressed by the document author. Following the
previously mentioned hypothesis, we may take ad-
vantage of two different techniques of text analy-
sis: maximal frequent sequences to extract a se-
quence of one or more words from a given text,
and pageranking, expecting to extract those word
sequences that represent the key ideas of the au-
thor.
The interest on extracting high quality
keyphrases from raw text has motivated forums,
such as SemEval, where different systems may
evaluate their performances. The purpose of
SemEval is to evaluate semantic analysis systems.
In particular, in this paper we are reporting the
results obtained in Task #5 of SemEval-2 2010,
which has been named: ?Automatic Keyphrase
Extraction from Scientific Articles?. We focused
this paper on the description of our approach and,
therefore, we do not describe into detail the task
nor the dataset used. For more information about
this information read the ?Task #5 Description
paper?, also published in this proceedings volume
(Nam Kim et al, 2010).
The rest of this paper is structured as follows.
Section 2 describes into detail the components of
the proposed approach. In Section 3 it is shown
the performance of the presented system. Finally,
in Section 4 a discussion of findings and further
work is given.
2 Description of the approach
The approach presented in this paper relies on the
combination of two different techniques for select-
ing the most prominent terms of a given text: max-
imal frequent sequences and pageranking. In Fig-
ure 1 we may see this two step approach, where
we are considering a sequence to be equivalent to
an n-gram. The complete description of the pro-
cedure is given as follows.
We select maximal frequent sequences which
174
we consider to be candidate keyphrases and, there-
after, we ranking them in order to determine which
ones are the most importants (according to the
pageranking algorithm). In the following subsec-
tions we give a brief description of these two tech-
niques. Afterwards, we provide an algorithm of
the presented approach.
Figure 1: Two step approach of BUAP Team at the
Task #5 of SemEval-2
2.1 Maximal Frequent Sequences
Definition: If a sequence p is a subsequence of q
and the number of elements in p is equal to n, then
the p is called an n-gram in q.
Definition: A sequence p = a
1
? ? ? a
k
is a sub-
sequence of a sequence q if all the items a
i
occur
in q and they occur in the same order as in p. If
a sequence p is a subsequence of a sequence q we
say that p occurs in q.
Definition: A sequence p is frequent in S if p is
a subsequence of at least ? documents in S where
? is a given frequency threshold. Only one oc-
currence of sequence in the document is counted.
Several occurrences within one document do not
make the sequence more frequent.
Definition: A sequence p is a maximal frequent
sequence in S if there does not exists any sequence
q in S such that p is a subsequence of q and p is
frequent in S.
2.2 PageRanking
The algorithm of PageRanking was defined by
Brin and Page in (Brin and Page, 1998). It is a
graph-based algorithm used for ranking webpages.
The algorithm considers input and output links of
each page in order to construct a graph, where
each vertex is a webpage and each edge may be
the input or output links for this webpage. They
denote as In(V
i
) the set of input links of webpage
V
i
, and Out(V
i
) their output links. The algorithm
proposed to rank each webpage based on the vot-
ing or recommendation of other webpages. The
higher the number of votes that are cast for a ver-
tex, the higher the importance of the vertex. More-
over, the importance of the vertex casting the vote
determines how important the vote itself is, and
this information is also taken into account by the
ranking model.
Although this algoritm has been initially pro-
posed for webpages ranking, it has been also used
for other NLP applications which may model their
corresponding problem in a graph structure. Eq.
(1) is the formula proposed by Brin and Page.
S(V
i
) = (1 ? d) + d ?
?
j?In(V
i
)
1
|Out(V
j
)|
S(V
j
)
(1)
where d is a damping factor that can be set be-
tween 0 and 1, which has the role of integrat-
ing into the model the probability of jumping
from a given vertex to another random vertex
in the graph. This factor is usually set to 0.85
(Brin and Page, 1998).
There are some other propossals, like the one
presented in (Mihalcea and Tarau, 2004), where a
textranking algorithm is presented. The authors
consider a weighted version of PageRank and
present some applications to NLP using unigrams.
They also construct multi-word terms by exploring
the conections among ranked words in the graph.
Our algorithm differs from textranking in that we
use MFS for feeding the PageRanking algorithm.
2.3 Algorithm
The complete algoritmic description of the pre-
sented approach is given in Algorithm 1. Read-
ers and writers keyphrases may be quite dif-
ferent. In particular, writers usually introduce
acronyms in their text, but they use the complete
or expanded representation of these acronyms
for their keyphrases. Therefore, we have in-
cluded a module (Extract Acronyms) for ex-
tracting both, acronyms with their corresponding
expanded version, which are used afterwards as
output of our system. We have preprocessed the
dataset removing stopwords and punctuation sym-
bols. Lemmatization (TreeTagger1) and stemming
(Porter Stemmer (Porter, 1980)) were also applied
in some stages of preprocessing.
The Maximal Freq Sequences module ex-
tracts maximal frequent sequences of words and
we feed the PageRaking module (PageRanking)
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
175
with all these sequences for determining the most
important ones. We use the structure of the sci-
entific articles in order to determine in and out
links of the sequences found. In fact, we use a
neighborhood criterion (a pair of MFS in the same
sentence) for determining the links between those
MFS?s. Once the ranking is calculated, we may se-
lect those sequences of a given length (unigrams,
bigrams and trigrams) as output of our system. We
also return a maximum of three acronyms, and
their associated multiterm phrases (MultiTerm),
as candidate keyphrases. Determining the length
and quantity of the sequences (n-grams) was ex-
perimentally deduced from the training corpus.
Algorithm 1: Algorithm of the Two Step ap-
proach for the Task #5 at SemEval-2
Input: A document set: D = {d
1
, d
2
, ? ? ? }
Output: A set K = {K
1
,K
2
, ? ? ? } of
keyphrases for each document d
i
:
K
i
= {k
i,1
, k
i,2
, ? ? ? }
foreach d
i
? D do1
AcronymSet = Extract Acronyms(d
i
);2
d
1
i
= Pre Processing(d
i
);3
MFS = Maximal Freq Sequences(d1
i
);4
CK = PageRanking(d1
i
, MFS);5
CU = Top Nine Unigrams(CK);6
CT = Top Three Trigrams(CK);7
K
i
= CT ;8
NU = 0;9
Acronyms = 0;10
foreach unigram ? CU do11
if unigram ? AcronymSet then12
if Acronyms < 3 then13
K
i
= K
i
?
{unigram};14
EA = MultiTerm(unigram);15
K
i
= K
i
?
{EA};16
Acronyms++;17
end18
else19
K
i
= K
i
?
{unigram};20
NU++;21
end22
end23
N = (15?(2?Acronyms+|CT |+NU));24
CB = Top N Bigrams(CK, N );25
K
i
= K
i
?
CB;26
end27
return K = {K
1
,K
2
, ? ? ? }28
In this edition of the Task #5 of SemEval-2
2010, we tested three different runs, which were
named: BUAP ? 1, BUAP ? 2 and BUAP ? 3.
Definition and differences among the three runs
are given in Table 3.
The results obtained with each run, together
with three different baselines are given in the fol-
lowing section.
3 Experimental results
In all tables, P , R, F mean micro-averaged pre-
cision, recall and F -scores. For baselines, there
were provided 1,2,-3 grams as candidates and
TFIDF as features. In Table 2, TFIDF is an
unsupervised method to rank the candidates based
on TFIDF scores. NB and ME are super-
vised methods using Na??ve Bayes and maximum
entropy in WEKA. In second column, R means
to use the reader-assigned keyword set as gold-
standard data and C means to use both author-
assigned and reader-assigned keyword sets as an-
swers.
Notice from Tables 2 and 3 that we outper-
formed all the baselines for the Top 15 candidates.
However, the Top 10 candidates were only outper-
formed by the Reader-Assigned keyphrases found.
This implies that the Writer keyphrases we ob-
tained were not of as good as the Reader ones. As
we mentioned, readers and writers assign different
keywords. The former write keyphrases based on
the lecture done, by the latter has a wider context
and their keyphrases used to be more complex. We
plan to investigate this issue in the future.
4 Conclusions
We have presented an approach based on the ex-
traction of maximal frequent sequences which are
then ranked by using the pageranking algorithm.
Three different runs were tested, modifying the
preprocessing stage and the number of bigrams
given as output. We did not see an improve-
ment when we used lemmatization of the docu-
ments. The run which obtained the best results
was ranking by the organizer according to the top
15 best keyphrases, however, we may see that our
runs need to be analysed more into detail in order
to provide a re-ranking procedure for the best 15
keyphrases found. This procedure may improve
the top 5 candidates precision.
176
Run name Description
BUAP ? 1 : This run is exactly the one described in Algorithm 1.
BUAP ? 2 : Same as BUAP ? 1 but lemmatization was applied a priori and stemming at the end.
BUAP ? 3 : Same as BUAP ? 2 but output twice the number of bigrams.
Table 1: Description of the three runs submitted to the Task #5 of SemEval-2 2010
Method by top 5 candidates top 10 candidates top 15 candidates
P R F P R F P R F
TF ? IDF R 17.80% 7.39% 10.44% 13.90% 11.54% 12.61% 11.60% 14.45% 12.87%
C 22.00% 7.50% 11.19% 17.70% 12.07% 14.35% 14.93% 15.28% 15.10%
NB R 16.80% 6.98% 9.86% 13.30% 11.05% 12.07% 11.40% 14.20% 12.65%
C 21.40% 7.30% 10.89% 17.30% 11.80% 14.03% 14.53% 14.87% 14.70%
ME R 16.80% 6.98% 9.86% 13.30% 11.05% 12.07% 11.40% 14.20% 12.65%
C 21.40% 7.30% 10.89% 17.30% 11.80% 14.03% 14.53% 14.87% 14.70%
Table 2: Baselines
Method by top 5 candidates top 10 candidates top 15 candidates
P R F P R F P R F
BUAP ? 1 R 10.40% 4.32% 6.10% 13.90% 11.54% 12.61% 14.93% 18.60% 16.56%
C 13.60% 4.64% 6.92% 17.60% 12.01% 14.28% 19.00% 19.44% 19.22%
BUAP ? 2 R 10.40% 4.32% 6.10% 13.80% 11.46% 12.52% 14.67% 18.27% 16.27%
C 14.40% 4.91% 7.32% 17.80% 12.14% 14.44% 18.73% 19.17% 18.95%
BUAP ? 3 R 10.40% 4.32% 6.10% 12.10% 10.05% 10.98% 12.33% 15.37% 13.68%
C 14.40% 4.91% 7.32% 15.60% 10.64% 12.65% 15.67% 16.03% 15.85%
Table 3: The three different runs submitted to the competition
Acknowledgments
This work has been partially supported by CONA-
CYT (Project #106625) and PROMEP (Grant
#103.5/09/4213).
References
[Barker and Corrnacchia2000] K. Barker and N. Cor-
rnacchia. 2000. Using noun phrase heads to extract
document keyphrases. In 13th Biennial Conference
of the Canadian Society on Computational Studies
of Intelligence: Advances in Artificial Intelligence.
[Barzilay and Elhadad1997] R. Barzilay and M. El-
hadad. 1997. Using lexical chains for text sum-
marization. In ACL/EACL 1997 Workshop on Intel-
ligent Scalable Text Summarization, pages 10?17.
[Brin and Page1998] S. Brin and L. Page. 1998. The
anatomy of a large-scale hypertextual web search
engine. In COMPUTER NETWORKS AND ISDN
SYSTEMS, pages 107?117. Elsevier Science Pub-
lishers B. V.
[DAvanzo and Magnini2005] E. DAvanzo and
B. Magnini. 2005. A keyphrase-based approach
to summarization:the lake system. In Document
Understanding Conferences (DUC-2005).
[Frank et al1999] E. Frank, G.W. Paynter, I. Witten,
C. Gutwin, and C.G. Nevill-Manning. 1999. Do-
main specific keyphrase extraction. In 16th Interna-
tional Joint Conference on AI, pages 668?673.
[Lawrie et al2001] D. Lawrie, W. B. Croft, and
A. Rosenberg. 2001. Finding topic words for hi-
erarchical summarization. In SIGIR 2001.
[Medelyan and Witten2008] O. Medelyan and I. H.
Witten. 2008. Domain independent automatic
keyphrase indexing with small training sets. Jour-
nal of American Society for Information Science and
Technology, 59(7):1026?1040.
[Mihalcea and Tarau2004] R. Mihalcea and P. Tarau.
2004. Textrank: Bringing order into texts. In
EMNLP 2004, ACL, pages 404?411.
[Nam Kim et al2010] S. Nam Kim, O. Medelyan, and
M.Y. Kan. 2010. Semeval-2010 task5: Auto-
matic keyphrase extraction from scientific articles.
In Proceedings of the Fifth International Workshop
on Semantic Evaluations (SemEval-2010). Associa-
tion for Computational Linguistics.
[Porter1980] M. F. Porter. 1980. An algorithm for suf-
fix stripping. Program, 14(3).
[Turney1999] P. Turney. 1999. Learning to extract
keyphrases from text. Technical Report ERB-1057.
(NRC #41622), National Research Council, Institute
for Information Technology.
[Witten et al1999] I. Witten, G. Paynter, E. Frank,
C. Gutwin, and G. Nevill-Manning. 1999.
Kea:practical automatic key phrase extraction. In
fourth ACM conference on Digital libraries, pages
254?256.
177
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 502?505,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: A First Approximation to Relational Similarity Measuring
Mireya Tovar, J. Alejandro Reyes,
Azucena Montes
CENIDET, Department of
Computer Science
Int. Internado Palmira S/N, Col. Palmira
Cuernavaca, Morelos, Me?xico
{mtovar, alexreyes06c, amr}
@cenidet.edu.mx
Darnes Vilarin?o, David Pinto,
Saul Leo?n
B. Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur y Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes, dpinto}@cs.buap.mx
saul.ls@live.com
Abstract
We describe a system proposed for measuring
the degree of relational similarity beetwen a
pair of words at the Task #2 of Semeval 2012.
The approach presented is based on a vec-
torial representation using the following fea-
tures: i) the context surrounding the words
with a windows size = 3, ii) knowledge ex-
tracted from WordNet to discover several se-
mantic relationships, such as meronymy, hy-
ponymy, hypernymy, and part-whole between
pair of words, iii) the description of the pairs
with their POS tag, morphological informa-
tion (gender, person), and iv) the average num-
ber of words separating the two words in text.
1 Introduction
The Task # 2 of Semeval 2012 focuses on measuring
the degree of relational similarity between the ref-
erence words pairs (training) and the test pairs for a
given class (Jurgens et al, 2012).
The training data set consists of 10 classes and
the testing data set consists of the 69 classes. These
datasets as well as the particularities of the task are
better described at overview paper (Jurgens et al,
2012). In this paper we report the approach submit-
ted to the competition, which is based on a vector
space model representation for each pair (Salton et
al., 1975). With respect to the type of features used,
we have observed that Fabio Celli (Celli, 2010) con-
siders that contextual information is useful, as well
the lexical and semantic information are in the ex-
traction of semantic relationships task. Additionally,
in (Chen et al, 2010) and (Negri and Kouylekov,
2010) are proposed WordNet based features with the
same purpose.
In the experiments carried out in this paper, we
use a set of lexical, semantic, WordNet-based and
contextual features which allows to construct the
vectors. Actually, we have tested a subset of the 20
contextual features proposed by Celli (Celli, 2010)
and some of those proposed by Chen (Chen et al,
2010) and Negri (Negri and Kouylekov, 2010).
The cosine similarity measure is used for deter-
mining the degree of relational similarity (Frakes
and Baeza-Yates, 1992) among the vectors.
The rest of this paper is structured as follows.
Section 2 describes the system employed. Section
3 show the obtained results. Finally, in Section 4 the
final conclusions are given.
2 System description
The approach reported in this paper measures the
relational similarity of a set of word pairs that be-
long to the same semantic relationship. Those word
pairs are represented by means of the vector space
model (Salton et al, 1975). Each value of the vec-
tor represents the average value of the correspond-
ing feature. This average is calculated using 100
samples obtained from Internet by employing the
Google search engine. The search process is car-
ried out assuming that those words co-occurring in
the same context contain some kind of semantic re-
lationship.
Let (w1, w2) be a word pair, then the vectorial
representation of this pair (~x) using semantic, con-
textual, lexical, and WordNet-based features may be
expressed as it can be seen in Eq. (1).
502
~x = (avg(f1), avg(f2), ..., avg(fn)) (1)
where avg(fk) is the average value of the feature fk.
The cardinality of the vector is 42, because we
extracted 4 lexical features, 6 semantic features, 7
WordNet-based features and 25 contextual features
(n = 42). Each word pair is then represented by
a unique vector with values associated to each fea-
ture. In Figure 1, we show the vectorial represen-
tation of the word pair (transportation, bus) using
a unique text sample (s). In this example, the num-
ber and type of features described below is followed,
i.e., the first 4 values are lexical, the following 6 are
semantic and so on.
s =?The Toyama Chih Railway is a transporta-
tion company that operates railway, tram, and
bus lines in the eastern part of the prefecture.?
~x = (6, 1, 0, 0, 27, 4, 4, 4, 4, 5, 2, 4, 5, 25, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4,
4, 0, 4, 4, 4, 4)
Figure 1: Example of a feature vector for a word pair and
its corresponding sentence s.
The previous example is only illustrative, since
we have gathered 100 sentence per word pair. In
total, we collected a corpus containing 2,054,687 to-
kens, with a average class terms of 26,684 and with
an average class vocabulary of 4,006.
The features extracted are described as follows:
2.1 Lexical features
The lexical features describe morphologically and
syntactically the word pair (w1, w2). The lexical
features extracted are the following:
? Average number of words separating the two
words (w1, w2) in the text.
? The position of w1 with respect to w2 in the
text. If w1 appears before w2 then the feature
value is 1, otherwise, the value is 2.
? The Part of Speech Tag for each word in the
pair (two features). We use the FreeLing PoS-
tagger (Padro? et al, 2010) for obtaining the
grammatical category. The possible values are
the following: adjective=1; adverb=2; arti-
cle=3; noun=4; verb=5; pronoun=6; conjunc-
tion=7; preposition=8
2.2 Semantic features
The following four semantic features are boolean
values (true or false) indicating:
? If w1 and w2 are named entities (two features)1.
? If w1 and w2 are entities defined (two fea-
tures)2.
The following two semantic features indicate:
? The type of prepositional phrase in case of
existing for w1 and w2. The feature val-
ues are nominal: about=1; after=2; at=3; be-
hind=4; between=5; by=6; except=7; from=8;
into=9; near=10; of=11; over=12; through=13;
until=14; under=15; upon=16; without=17;
above=18; among=19; before=20; below=21;
beside=22; but=23; down=24; for=25; in=26;
on=27; since=28; to=29; with=30.
2.3 WordNet-based features
The semantic features are boolean values (true or
false) indicating whether or not w2 is contained in:
? the synonym set of w1
? the antonym set of w1
? the meronymy set of w1
? the hyponymy set of w1
? the hypernymy set of w1
? the part-whole set of w1
? the gloss set of w1
We used WordNet (Fellbaum, 1998) in order to de-
termine the relationship set for word w1.
1A named entity is defined by a Proper Noun Phrase, which
was detected using the module NER-Named Entity Recognition
of the FreeLing 2.1 tool.
2A defined sentence is one that begins with a definite article.
503
2.4 Contextual features
Contextual features considers values for the words
that occur in the context of w1 and w2 (in a window
size of 3). The description of those features follows.
? Nominal values indicating the Part of Speech
Tag (adjective=1; adverb=2; article=3; noun=4;
verb=5; pronoun=6; conjunction=7; preposi-
tion=8) for the three words at:
? the left context of w1 (three features).
? the right context of w1 (three features).
? the left context of w2 (three features).
? the right context of w2 (three features).
? A Nominal value indicating number of the fol-
lowing grammatical categories between w1 and
w2: verbs, adjectives and nouns (three fea-
tures).
? Nominal values indicating the frequencies of
the verbs: be, do, have, locate, know, make, use,
become, include, take between w1 and w2 (ten
features).
2.5 Feature selection
We carried out a feature selection process with the
aim of discarding irrelevant features. In this step,
we apply the attribute selection filter reported in
(Hall, 1999), that evaluates the worth of a subset
of attributes by considering the individual predic-
tive ability of each feature along with the degree of
redundancy between them and an exhaustive search
method.
The following features were obtained as relevant:
the average number of words between w1 and w2;
Named Entity of w1 and w2; phrase defined of w1
and w2; prepositional phrase type w1 and w2; part
of speech tag w1 and w2; part of speech tag of
right context of w1 with a windows size of 3; oc-
currences of verbs between w1 and w2; frequency of
verbs be, do, make, locate, take; synonym, antonym,
meronymy, hyponymy, hypernymy, part-whole and
gloss relationships between w1 and w2.
After applying the aforementioned feature selec-
tion method, we removed 17 features, and the vec-
torial representation of each word pair will be done
with only 25 values (features).
2.6 Determining the degree of similarity
We have used the features mentioned before for con-
structing a prototype vector representing a given se-
mantic class. In order to do so, we have employed
the training corpus for gathering samples from Inter-
net and, thereafter, we average the feature values in
order to construct such prototype vector.
For each word pair in the test dataset, we ob-
tained a vector using the same process explained
before. We determined the similarity for each test
feature vector with respect to the prototype of the
given class by using the cosine similarity coefficient
(Frakes and Baeza-Yates, 1992), i.e., measuring the
cosine of the angle between the two vectors.
In this way, we obtain a similarity measure of each
test word pair with respect to its corresponding class.
Finally, we may output a ranking of all the word
pairs at the test dataset by sorting these similarity
values obtained.
3 Experimental results
The approach submitted to the Task #2 of SemEval
2012 obtained very poor results. The Spearman cor-
relation coefficient, which measured the correlation
of the approach with respect to the gold standard, it
is quite low (see Table 1).
Team-Algorithm Spearman MaxDiff
UTD-NB 0.23 39.4
UTD-SVM 0.12 34.7
DULUTH-V0 0.05 32.4
DULUTH-V1 0.04 31.5
DULUTH-V2 0.04 31.1
BUAP 0.01 31.7
Random 0.02 31.2
Table 1: Spearman and MaxDiff scores obtained at the
Task #2 of Semeval 2012
Actually, it shows that the run submitted does not
correlate with the gold standard. We consider that
this behavior is derived from the nature of the sup-
port corpus used for obtaining the features set. The
number of sentences (100) used for representing the
word pairs was not enough for constructing a real
prototype of both, the semantic class and the word
pairs. A further analysis will confirm this issue.
504
Despite this limitation we note that the MaxDiff
score was 31.7% slightly above the baseline (31.2%)
and not far from the best score of the task (39.4%).
That is, we achieved an average of 31.7% of ques-
tions answered correctly.
4 Discussion and conclusion
In this paper we report the set of features used in
the approach submitted for measuring the degrees of
relational similarity between a given reference word
pair and a variety of other pairs. The results obtained
are not encouraging with a Spearman correlation co-
efficient close to zero, which mean that there are
not correlation between the run submitted and the
gold standard. A deeper analysis of the approach is
needed in order to determine if the limitation of the
system falls in the features used, the similarity mea-
sure, or the support corpus used for extracting the
features.
Acknowledgments
This project has been partially supported by projects
CONACYT #106625, VIEP #PIAD-ING11-II and
#VIAD-ING11-II.
References
Fabio Celli. 2010. Unitn: Part-of-speech counting in
relation extraction. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
?10, pages 198?201, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yuan Chen, Man Lan, Jian Su, Zhi Min Zhou, and
Yu Xu. 2010. Ecnu: Effective semantic relations
classification without complicated features or multi-
ple external corpora. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
?10, pages 226?229, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press.
William B. Frakes and Ricardo A. Baeza-Yates, editors.
1992. Information Retrieval: Data Structures & Algo-
rithms. Prentice-Hall.
Mark A. Hall. 1999. Correlation-based Feature Sub-
set Selection for Machine Learning. Ph.D. thesis, De-
partment of Computer Science, University of Waikato,
Hamilton, New Zealand.
David A. Jurgens, Saif M. Mohammad, Peter D. Turney,
and Keith J. Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012), Montreal, Canada.
Matteo Negri and Milen Kouylekov. 2010. Fbk nk: A
wordnet-based system for multi-way classification of
semantic relations. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
?10, pages 202?205, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Llu??s Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling
2.1: Five years of open-source language processing
tools. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta. European Language Re-
sources Association (ELRA).
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Commun. ACM,
18(11):613?620, November.
505
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 631?634,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: Three Approaches for Semantic Textual Similarity
Maya Carrillo, Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n, Esteban Castillo
Beneme?rita Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur & Av. San Claudio, CU
Puebla, Puebla, Me?xico
{cmaya, darnes, dpinto, mtovar}@cs.buap.mx
saul.ls@live.com, ecjbuap@gmail.com
Abstract
In this paper we describe the three approaches
we submitted to the Semantic Textual Similar-
ity task of SemEval 2012. The first approach
considers to calculate the semantic similar-
ity by using the Jaccard coefficient with term
expansion using synonyms. The second ap-
proach uses the semantic similarity reported
by Mihalcea in (Mihalcea et al, 2006). The
third approach employs Random Indexing and
Bag of Concepts based on context vectors. We
consider that the first and third approaches ob-
tained a comparable performance, meanwhile
the second approach got a very poor behav-
ior. The best ALL result was obtained with
the third approach, with a Pearson correlation
equal to 0.663.
1 Introduction
Finding the semantic similarity between two sen-
tences is very important in applications of natural
language processing such as information retrieval
and related areas. The problem is complex due to the
small number of terms involved in sentences which
are tipically less than 10 or 15. Additionally, it is re-
quired to ?understand? the meaning of the sentences
in order to determine the ?semantic? similarity of
texts, which is quite different of finding the lexical
similarity.
There exist different works at literature dealing
with semantic similarity, but the problem is far to
be solved because of the aforementioned issues.
In (Mihalcea et al, 2006), for instance, it is pre-
sented a method for measuring the semantic simi-
larity of texts, using corpus-based and knowledge-
based measures of similarity. The approaches pre-
sented in (Shrestha, 2011) are based on the Vector
Space Model, with the aim to capture the contex-
tual behavior, senses and correlation, of terms. The
performance of the method is better than the base-
line method that uses vector based cosine similarity
measure.
In this paper, we present three different ap-
proaches for the Textual Semantic Similarity task of
Semeval 2012 (Agirre et al, 2012). The task is de-
scribed as follows: Given two sentences s1 and s2,
the aim is to compute how similar s1 and s2 are,
returning a similarity score, and an optional confi-
dence score. The approaches should provide values
between 0 and 5 for each pair of sentences. These
values roughly correspond to the following consid-
erations, even when the system should output real
values:
5: The two sentences are completely equivalent,
as they mean the same thing.
4: The two sentences are mostly equivalent, but
some unimportant details differ.
3: The two sentences are roughly equivalent, but
some important information differs/missing.
2: The two sentences are not equivalent, but share
some details.
1: The two sentences are not equivalent, but are
on the same topic.
0: The two sentences are on different topics.
631
The description of the runs submitted to the com-
petition follows.
2 Experimentation setup
The three runs submitted to the competition use
completely different mechanisms to find the degree
of semantic similarity between two sentences. The
approaches are described as follows:
2.1 Approach BUAP-RUN-1: Term expansion
with synonyms
Let s1 = w1,1w1,2...w1,|s1| and s2 =
w2,1w2,2...w2,|s2| be two sentences. The synonyms
of a given word wi,k, expressed as synonyms(wi,k),
are obtained from online dictionaries by extracting
the synonyms of wi,k. A better matching between
the terms contained in the text fragments and the
terms at the dictionary are obtained by stemming all
the terms (using the Porter stemmer).
In order to determine the semantic similarity be-
tween any pair of terms of the two sentences (w1,i
and w2,j) we use Eq. (1).
sim(w1,i, w2,j) =
?
?
?
?
?
?
?
?
?
1 if (w1,i == w2,j) ||
w1,i ? synonyms(w2,j) ||
w2,j ? synonyms(w1,i)
0 otherwise
(1)
The similarity between sentences s1 and s2 is cal-
culated as shown in Eq. (2).
similarity(s1, s2) =
5 ? ?ni=1
?n
j=1 sim(w1i, w2j)
|s1 ? s2|
(2)
2.2 Approach BUAP-RUN-2
In this approach, the similarity of s1 and s2 is calcu-
lated as shown in Eq. (3) (Mihalcea et al, 2006).
similarity(s1, s2) = 12 (
?
w?{s1}
(maxSim(w,s2)?idf(w))
?
w?{s1}
idf(w)
+
?
w?{s2}
(maxSim(w,s1)?idf(w))
?
w?{s2}
idf(w) )
(3)
where idf(w) is the inverse document frequency of
the word w, and maxSim(w, s2) is the maximum
lexical similarity between the word w in sentence s2
and all the words in sentence s2 calculated by means
of the Eq. (4) reported by (Wu and Palmer, 1994).
The sentence terms are assumed to be concepts, LCS
is the depth of the least common subsumer, and the
equation is calculated using the NLTK libraries1.
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(4)
2.3 Approach BUAP-RUN-3: Random
Indexing and Bag of Concepts
The vector space model (VSM) for document rep-
resentation supporting search is probably the most
well-known IR model. The VSM assumes that term
vectors are pair-wise orthogonal. This assumption
is very restrictive because words are not indepen-
dent. There have been various attempts to build
representations for documents that are semantically
richer than only vectors based on the frequency of
terms occurrence. One example is Latent Seman-
tic Indexing (LSI), a method of word co-occurrence
analysis to compute semantic vectors (context vec-
tors) for words. LSI applies singular-value decom-
position (SVD) to the term-document matrix in or-
der to construct context vectors. As a result the di-
mension of the produced vector space will be signif-
icantly smaller; consequently the vectors that repre-
sent terms cannot be orthogonal. However, dimen-
sion reduction techniques such as SVD are expen-
sive in terms of memory and processing time. Per-
forming the SVD takes time O (nmz), where n is
the vocabulary size, m is the number of documents,
and z is the number of nonzero elements per column
in the words-by-documents matrix. As an alterna-
tive, there is a vector space methodology called Ran-
dom Indexing (RI) (Sahlgren, 2005), which presents
an efficient, scalable, and incremental method for
building context vectors. Its computational com-
plexity is O (nr) where n is as previously described
and r is the vector dimension. Particularly, we apply
RI to capture the inherent semantic structure using
Bag of Concepts representation (BoC) as proposed
by Sahlgren and Co?ster (Sahlgren and Co?ster, 2004),
where the meaning of a term is considered as the
sum of contexts in which it occurs.
1http://www.nltk.org/
632
2.3.1 Random Indexing
Random Indexing (RI) is a vector space method-
ology that accumulates context vectors for words
based on co-occurrence data. The technique can be
described as:
? First a unique random representation known as
index vector is assigned to each context (docu-
ment). Index vectors are binary vectors with a
small number of non-zero elements, which are
either +1 or -1, with equal amounts of both.
For example, if the index vectors have twenty
non-zero elements in a 1024-dimensional vec-
tor space, they have ten +1s and ten -1s. Index
vectors serve as indices or labels for documents
? Index vectors are used to produce context vec-
tors by scanning through the text and every
time a target word occurs in a context, the in-
dex vector of the context is added to the con-
text vector of the target word. Thus, at each
encounters of the target word t with a context c
the context vector of t is updated as follows: ct
+ = ic where ct is the context vector of t and ic
is the index vector of c. In this way, the context
vector of a word keeps track of the contexts in
which it occurred.
RI methodology is similar to latent semantic in-
dexing (LSI) (Deerwester et al, 1990). However,
to reduce the co-occurrence matrix no dimension re-
duction technique such as SVD is needed, since the
dimensionality d of the random index vectors is pre-
established as a parameter (implicit dimension re-
duction). Consequently d does not change once it
has been set; as a result, the dimensionality of con-
text vectors will never change with the addition of
new data.
2.3.2 Bag of Concepts
Bag of Concepts (BoC) is a recent representa-
tion scheme proposed by Sahlgren and Co?ster in
(Sahlgren and Co?ster, 2004), which is based on the
perception that the meaning of a document can be
considered as the union of the meanings of its terms.
This is accomplished by generating term context
vectors from each term within the document, and
generating a document vector as the weighted sum
of the term context vectors contained within that
document. Therefore, we use RI to represent the
meaning of a word as the sum of contexts (entire
documents) in which it occurs. Illustrating this tech-
nique, suppose you have two documents: D1: A man
with a hard hat is dancing, and D2: A man wearing
a hard hat is dancing. Let us suppose that they have
index vectors ID1 and ID2, respectively: the context
vector for hat will be the ID1 + ID2, because this
word appears in both documents. Once the context
vectors have been built by RI, they are used to repre-
sent the document as BoC. For instance, supposing
CV1, CV2, CV3, . . . and CV8, are the context vec-
tors of each word in D1, then document D1 will be
represented as the weighted sum of these eight con-
text vectors.
2.3.3 Implementation
The sentences of each file were processed to gen-
erate the BoC representations of them. BoC rep-
resentations were generated by first stemming all
words in the sentences. We then used random index-
ing to produce context vectors for each word in the
files (i.e. STS.input.MSRpar, STS.input.MSRvid,
etc.), each file was considered a different corpus and
documents were the sentences in them. The dimen-
sion of the context vectors was fixed at 2048, de-
termined by experimentation using the training set.
These context vectors were then tf ? idf -weighted,
according to the corpus, and added up for each sen-
tence, to produce BoC representations. Therefore
the similarity values were calculated by the cosine
function. Finally cosine values were multiplied by 5
to produce values between 0 and 5.
3 Experimental results
In Table 1 we show the results obtained by the
three approaches submitted to the competition. The
columns of Table 1 stand for:
? ALL: Pearson correlation with the gold stan-
dard for the five datasets, and corresponding
rank.
? ALLnrm: Pearson correlation after the system
outputs for each dataset are fitted to the gold
standard using least squares, and corresponding
rank.
633
Run ALL Rank ALL
nrm
Rank
Nrm
Mean Rank
Mean
MSR
par
MSR
vid
SMT
eur
On -
WN
SMT-
news
BUAP-
RUN-1
0.4997 63 0.7568 62 0.4892 57 0.4037 0.6532 0.4521 0.605 0.4537
BUAP-
RUN-2
-0.026 89 0.5933 89 0.0669 89 0.1109 0.0057 0.0348 0.1788 0.1964
BUAP-
RUN-3
0.663 25 0.7474 64 0.488 59 0.4018 0.6378 0.4758 0.5691 0.4057
Table 1: Results of approaches of BUAP in Task 6.
? Mean: Weighted mean across the 5 datasets,
where the weight depends on the number of
pairs in the dataset.
Followed by Pearson for individual datasets.
At this moment, we are not aware of the reasons
because the second approach obtained a very poor
performance. The way in which the idf(w) is calcu-
lated could be one of the reasons, because the corpus
used is relatively small and also from a different do-
main. With respect to the other two approaches, we
consider that they (first and third) obtained a com-
parable performance, even when the third approach
obtained the best ALL result with a Pearson correla-
tion equal to 0.663.
4 Discussion and conclusion
We have presented three different approaches for
tackling the problem of Semantic Textual Similarity.
The use of term expansion by synonyms performed
well in general and obtained a comparable behavior
than the third approach which used random index-
ing and bag of concepts. It is interesting to observe
that these two approaches performed similar when
the two term expansion mechanism are totally dif-
ferent. As further, it is important to analyze the poor
behavior of the second approach. We would like also
to introduce semantic relationships other than syn-
onyms in the process of term expansion.
Acknowledgments
This project has been partially supported by
projects CONACYT #106625, #VIAD-ING11-II,
PROMEP/103.5/11/4481 and VIEP #PIAD-ING11-
II.
References
E. Agirre, D. Cer, M. Diab, and B. Dolan. 2012.
SemEval-2012 Task 6: Semantic Textual Similarity.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In proceedings of AAAI?06,
pages 775?780.
Magnus Sahlgren and Rickard Co?ster. 2004. Using bag-
of-concepts to improve the performance of support
vector machines in text categorization. In Proceedings
of the 20th international conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
M. Sahlgren. 2005. An Introduction to Random Index-
ing. Methods and Applications of Semantic Indexing
Workshop at the 7th International Conference on Ter-
minology and Knowledge Engineering, TKE 2005.
Prajol Shrestha. 2011. Corpus-based methods for short
text similarity. In TALN 2011, Montpellier, France.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In 32nd. Annual Meeting of the
Association for Computational Linguistics, pages 133
?138, New Mexico State University, Las Cruces, New
Mexico.
634
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 706?709,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: Lexical and Semantic Similarity for Cross-lingual Textual
Entailment
Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n, Esteban Castillo
Beneme?rita Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur & Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes, dpinto, mtovar}@cs.buap.mx
saul.ls@live.com, ecjbuap@gmail.com
Abstract
In this paper we present a report of the two di-
fferent runs submitted to the task 8 of Semeval
2012 for the evaluation of Cross-lingual Tex-
tual Entailment in the framework of Content
Synchronization. Both approaches are based
on textual similarity, and the entailment judg-
ment (bidirectional, forward, backward or no
entailment) is given based on a set of decision
rules. The first approach uses textual simi-
larity on the translated and original versions
of the texts, whereas the second approach ex-
pands the terms by means of synonyms. The
evaluation of both approaches show a similar
behavior which is still close to the average and
median.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
recently proposed by (Mehdad et al, 2010; Mehdad
et al, 2011) as an extension of the Textual Entail-
ment task (Dagan and Glickman, 2004). Given a text
(T ) and an hypothesis (H) in different languages,
the CLTE task consists of determining if the mea-
ning of H can be inferred from the meaning of T .
In this paper we present a report of the obtained
results after submitting two different runs for the
Task 8 of Semeval 2012, named ?Cross-lingual Tex-
tual Entailment for Content Synchronization? (Negri
et al, 2012). In this task, the Cross-Lingual Tex-
tual Entailment addresses textual entailment recog-
nition under a new dimension (cross-linguality), and
within a new challenging application scenario (con-
tent synchronization). The task 8 of Semeval 2012
may be formally defined as follows:
Given a pair of topically related text fragments
(T1 and T2) in different languages, the task consists
of automatically annotating it with one of the follo-
wing entailment judgments:
? Bidirectional (T1 ? T2 & T1 ? T2): the two
fragments entail each other (semantic equiva-
lence)
? Forward (T1 ? T2 & T1 ! ? T2): unidirec-
tional entailment from T1 to T2
? Backward (T1 ! ? T2 & T1 ? T2): unidirec-
tional entailment from T2 to T1
? No Entailment (T1 ! ? T2 & T1 ! ? T2): there
is no entailment between T1 and T2
In this task, both T1 and T2 are assumed to be
TRUE statements; hence in the dataset there are no
contradictory pairs. Cross-lingual datasets are avai-
lable for the following language combinations:
? Spanish/English (SPA-ENG)
? German/English (DEU-ENG)
? Italian/English (ITA-ENG)
? French/English (FRA-ENG)
The remaining of this paper is structured as fo-
llows: Section 2 describes the two different approa-
ches presented in the competition. The obtained re-
sults are shown and dicussed in Section 3. Finally,
the findings of this work are given in Section 4.
706
2 Experimental setup
For this experiment we have considered to tackle the
CLTE task by means of textual similarity and textual
length. In particular, the textual similarity is used to
determine whether some kind of entailment exists or
not. We have established the threshold of 0.5 for the
similarity function as evidence of textual entailment.
Since the two sentences to be evaluated are written
in two different languages, we have translated each
sentence to the other language, so that, we have two
sentences in English, and two sentences in the origi-
nal language (Spanish, German, Italian and French).
We have used the Google translate for this purpose
1
.
The corpora used in the experiments comes from
a cross-lingual Textual Entailment dataset presented
in (Negri et al, 2011), and provided by the task orga-
nizers. We have employed the training dataset only
for adjust some parameters of the system, but the
approach is knowledge-based and, therefore, it does
not need a training corpus. Both, the training and
test corpus contain 500 sentences for each language.
The textual length is used to determine the entail-
ment judgment (bidirectional, forward, backward,
no entailment). We have basically, assumed that the
length of a text may give some evidence of the type
of entailment. The decision rules used for determi-
ning the entailment judgment are described in Sec-
tion 2.3.
In this competition we have submitted two diffe-
rent runs which differ with respect to the type of tex-
tual similarity used (lexical vs semantic). The first
one, calculates the similarity using only the trans-
lated version of the original sentences, whereas the
second approach uses text expansion by means of
synonyms and, thereafter, it calculates the similarity
between the pair of sentences.
Let T1 be the sentence in the original language,
T2 the T1 topically related text fragment (written in
English). Let T3 be the English translation of T1,
and T4 the translation of T2 to the original language
(Spanish, German, Italian and French). The formal
description of these two approaches are given as fo-
llows.
1http://translate.google.com.mx/
2.1 Approach 1: Lexical similarity
The evidence of textual entailment between T1 and
T2 is calculated using two formulae of lexical si-
milarity. Firstly, we determine the similarity bet-
ween the two texts written in the source language
(SimS). Additionally, we calculate the lexical simi-
larity between the two sentences written in the target
language (SimT ), in this case English.
Given the limited text length of the text fragments,
we have used the Jaccard coefficient as similarity
measure. Eq. (1) shows the lexical similarity for the
two texts written in the original language, whereas,
Eq. (2) presents the Jaccard coefficient for the texts
written in English.
simS = simJaccard(T1, T4) =
|T1 ? T4|
|T1 ? T4|
(1)
simT = simJaccard(T2, T3) =
|T2 ? T3|
|T2 ? T3|
(2)
2.2 Approach 2: Semantic similarity
In this case we calculate the semantic similarity bet-
ween the two texts written in the original language
(simS), and the semantic similarity between the two
text fragments written in English (simT ). The se-
mantic level of similarity is given by considering
the synonyms of each term for each sentence (in
the original and target language). For this purpose,
we have employed five dictionaries containing syno-
nyms for the five different languages considered in
the competition (English, Spanish, German, Italian,
and French)2. In Table 1 we show the number of
terms, so as the number of synonyms in average by
term considered for each language.
Let T1 = w1,1w1,2...w1,|T1|, T2 =
w2,1w2,2...w2,|T2| be the source and target
sentences, and let T3 = w3,1w3,2...w3,|T3|,
T4 = w4,1w4,2...w4,|T4| be translated version of the
original source and target sentences, respectively.
The synonyms of a given word wi,k, expressed as
synset(wi,k), are obtained from the aforementioned
dictionaries by extracting the synonyms of wi,k. In
order to obtain a better matching between the terms
contained in the text fragments and the terms in the
2http://extensions.services.openoffice.org/en/dictionaries
707
Table 1: Dictionaries of synonyms used for term expan-
sion
Language Terms synonyms per term
(average)
English 2,764 60
Spanish 9,887 45
German 21,958 115
Italian 25,724 56
French 36,207 93
dictionary, we have stemmed all the terms using the
Porter stemmer.
In order to determine the semantic similarity bet-
ween two terms of sentences written in the source
language (w1,i and w4,j) we use Eq. (3). The se-
mantic similariy between two terms of the English
sentences are calculated as shown in Eq. (4).
sim(w1,i, w4,j) =
?
?
?
?
?
?
?
?
?
1 if (w1,i == w4,j) ||
w1,i ? synset(w4,j) ||
w4,j ? synset(w1,i)
0 otherwise
(3)
sim(w2,i, w3,j) =
?
?
?
?
?
?
?
?
?
1 if (w2,i == w3,j) ||
w2,i ? synset(w3,j) ||
w3,j ? synset(w2,i)
0 otherwise
(4)
Both equations consider the existence of semantic
similarity when the two words are identical, or when
the some of the two words appear in the synonym set
of the other word.
The semantic similarity of the complete text frag-
ments T1 and T4 (simS) is calculated as shown in
Eq. (5). Whereas, the semantic similarity of the
complete text fragments T2 and T3 (simT ) is cal-
culated as shown in Eq. (6).
simS(T1, T4) =
?|T1|
i=1
?|T4|
j=1 sim(w1,i,w4,j)
|T1?T4|
(5)
simT (T2, T3) =
?|T2|
i=1
?|T3|
j=1 sim(w2,i,w3,j)
|T2?T3|
(6)
2.3 Decision rules
Both approches used the same decision rules in or-
der to determine the entailment judgment for a given
pair of text fragments (T1 and T2). The following al-
gorithm shows the decision rules used.
Algorithm 1.
If |T2| < |T3| then
If (simT > 0.5 and simS > 0.5)
then forward
ElseIf |T2| > |T3| then
If (simT > 0.5 and simS > 0.5)
then backward
ElseIf (|T1| == |T4| and |T2| == |T3|) then
If (simT > 0.5 and simS > 0.5)
then bidirectional
Else no entailment
As mentioned above, the rules employed the le-
xical or semantic textual similarity, and the textual
length for determining the textual entailment.
3 Results
In Table 2 we show the overall results obtained by
the two approaches submitted to the competition.
We also show the highest, lowest, average and me-
dian overall results obtained in the competition.
SPA-
ENG
ITA-
ENG
FRA-
ENG
DEU-
ENG
Highest 0.632 0.566 0.57 0.558
Average 0.407 0.362 0.366 0.357
Median 0.346 0.336 0.336 0.336
Lowest 0.266 0.278 0.278 0.262
BUAP run1 0.35 0.336 0.334 0.33
BUAP run2 0.366 0.344 0.342 0.268
Table 2: Overall statistics obtained in the Task 8 of Se-
meval 2012
The runs submitted perform similar, but the se-
mantic approach obtained a slightly better perfor-
mance. The two results are above the median but
below the average. We consider that better results
may be obtained if the two features used (textual si-
milarity and textual length) were introduced into a
supervised classifier, so that, the decision rules were
approximated on the basis of a training dataset, ins-
tead of the empirical setting done in this work. Fu-
ture experiments will be carried out in this direction.
708
4 Discussion and conclusion
Two different approaches for the Cross-lingual Tex-
tual Entailment for Content Synchronization task of
Semeval 2012 are reported in this paper. We used
two features for determining the textual entailment
judgment between two texts T1 and T2 (written in
two different languages). The first approach pro-
posed used lexical similarity, meanwhile the second
used semantic similarity by means of term expan-
sion with synonyms.
Even if the performance of both approaches is
above the median and slighly below the average,
we consider that we may easily improve this perfor-
mance by using syntactic features of the text frag-
ments. Additionally, we are planning to integrate
some supervised techniques based on decision rules
which may be trained in a supervised dataset. Future
experiments will be executed in this direction.
Acknowledgments
This project has been partially supported by projects
CONACYT #106625, #VIAD-ING11-II and VIEP
#PIAD-ING11-II.
References
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Learning Methods for Text Un-
derstanding and Mining, January.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 321?
324, Los Angeles, California, June. Association for
Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 1336?1345, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 670?679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2012. Semeval-2012 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012).
709
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 124?127, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
BUAP: N -gram based Feature Evaluation for the Cross-Lingual Textual
Entailment Task
Darnes Vilarin?o, David Pinto, Sau?l Leo?n, Yuridiana Alema?n, Helena Go?mez-Adorno
Beneme?rita Universidad Auto?noma de Puebla
Faculty of Computer Science
14 Sur y Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes, dpinto, saul.leon, candy.aleman, helena.gomez}@cs.buap.mx
Abstract
This paper describes the evaluation of differ-
ent kinds of textual features for the Cross-
Lingual Textual Entailment Task of SemEval
2013. We have counted the number of N -
grams for three types of textual entities (char-
acter, word and PoS tags) that exist in the
pair of sentences from which we are inter-
ested in determining the judgment of textual
entailment. Difference, intersection and dis-
tance (Euclidian, Manhattan and Jaccard) of
N -grams were considered for constructing a
feature vector which is further introduced in
a support vector machine classifier which al-
lows to construct a classification model. Five
different runs were submitted, one of them
considering voting system of the previous four
approaches. The results obtained show a per-
formance below the median of six teams that
have participated in the competition.
1 Introduction
The cross-lingual textual entailment (CLTE), re-
cently proposed by (Mehdad et al, 2012) and
(Mehdad et al, 2011), is an extension of the tex-
tual entailment task (Dagan and Glickman, 2004).
Formally speaking, given a pair of topically related
text fragments (T1 and T2 which are assumed to
be TRUE statements) written in different languages,
the CLTE task consists of automatically annotating
it with one of the following entailment judgments:
? bidirectional (T1 ? T2 & T1 ? T2): the two
fragments entail each other (semantic equiva-
lence);
? forward (T1 ? T2 & T1 8 T2): unidirec-
tional entailment from T1 to T2;
? backward (T1 9 T2 & T1 ? T2): unidirec-
tional entailment from T2 to T1;
? no entailment (T1 9 T2 & T1 8 T2): there
is no entailment between T1 and T2 in both
directions;
The Cross-lingual datasets evaluated were avail-
able for the following language combinations (T1-
T2):
? Spanish-English (SPA-ENG)
? German-English (DEU-ENG)
? Italian-English (ITA-ENG)
? French-English (FRA-ENG)
In this paper we describe the evaluation of differ-
ent features extracted from each pair of topically re-
lated sentences. N -grams of characters, words and
PoS tags were counted with the aim of constructing
a representative vector for each judgment entailment
(FORWARD, BACKWARD, BI-DIRECTIONAL or
NO-ENTAILMENT). The resulting vectors were
fed into a supervised classifier based on Support
Vector Machines (SVM)1 which attempted to con-
struct a classification model. The description of the
features and the vectorial representation is given in
Section 2. The obtained results are shown and di-
cussed in Section 3. Finally, the findings of this
work are given in Section 4.
1We have employed the implementation of the Weka tool
(Hall et al, 2009).
124
2 Experimental Setup
We have considered the task as a classification prob-
lem using the pivot approach. Thus, we have trans-
lated2 each pair to their corresponding language in
order to have two pairs of sentences written in the
same language. Let Pair(T1, T2) be the origi-
nal pair of topically related sentences. Then, we
have obtained the English translation of T1, de-
noted by T3, which will be aligned with T2. On
the other hand, we have translated T2 to the other
language (Spanish, German, Italian or French), de-
noted by T4, which will be aligned with T1. The
two pairs of sentences, Pair(T2, T3) (English) and
Pair(T1, T4) (other language), are now written in
the same language, and we can proceed to calculate
the textual features we are interested in.
The features used to represent both sentences are
described below:
? N -grams of characters, with N = 2, ? ? ? , 5.
? N -grams of words, with N = 2, ? ? ? , 4.
? N -grams of PoS tags, with N = 2, ? ? ? , 4.
? Euclidean measure between each pair of sen-
tences (Pair(T1, T4) and Pair(T2, T3)).
? Manhattan measure between each pair of sen-
tences (Pair(T1, T4) and Pair(T2, T3)).
? Jaccard coefficient, expanding English terms in
both sentences, T2 and T3, with their corre-
sponding synonyms (none disambiguation pro-
cess was considered).
The manner we have used the above mentioned
features is described in detail in the following sub-
sections.
2.1 Approach 1: Difference operator
For each pair of sentences written in the same lan-
guage, this approach counts the number of N -grams
that occur in the first sentence (for instance T1), and
do not occur in the second sentence (for instance
T4) and viceversa. Formally speaking, the values
obtained are ????Pair(T1, T2) = {D1, D2, ? ? ? , Dk},
with D1 = |T1 ? T4|, D2 = |T4 ? T1|, D3 =
2For this purpose we have used Google Translate
Table 1: Classes considered in the composition of binary
classifiers
Class 1 Class 2
BACKWARD OTHER
BI-DIRECTIONAL OTHER
FORWARD OTHER
NO-ENTAILMENT OTHER
BACKWARD & BI-DIRECTIONAL OTHER
BACKWARD & FORWARD OTHER
BACKWARD & NO-ENTAILMENT OTHER
BI-DIRECTIONAL & NO-ENTAILMENT OTHER
FORWARD & BI-DIRECTIONAL OTHER
FORWARD & NO-ENTAILMENT OTHER
|T2 ? T3|, D4 = |T3 ? T2|, ? ? ?. This vector is
calculated for all the possible values of N for each
type of N -gram, i.e., character, word and PoS tag.
The cardinality of ????Pair(T1, T2) will be 34, that is,
16 values when the N -grams of characters are con-
sidered, 12 values with word N -grams, and 6 values
when the PoS tag N -grams are used.
The vectors obtained are labeled with the corre-
sponding tag in order to construct a training dataset
which will be further used to feed a multiclass clas-
sifier which constructs the final classification model.
In this case, the system will directly return one of the
four valid entailment judgments (i.e. forward, back-
ward, bidirectional, no entailment).
2.2 Approach 2: Difference and Intersection
operators
This approach enriches the previous one, by adding
the intersection between the two sentences of each
pair. In a sense, we have considered all the features
appearing in the pair of sentences. In this case, the
total number of features extracted, i.e., the cardinal-
ity of the ????Pair(T1, T2) vector is 51.
2.3 Approach 3: Metaclassifier
In this approach, we have constructed a system
which is a composition of different binary classifica-
tion models. The binary judgments were constructed
considering the classes shown in Table 1.
The approach 2 was also considered in this com-
position generating a total of 11 models. 10 of them
are based on the features used by Approach 1, and
the last one is based on the features used by Ap-
proach 2. The result obtained is a vector which tells
whether or not a pair is judged to have some kind of
textual entailment or not (the OTHER class). This
125
vector is then labeled with the correct class obtained
from the gold standard (training corpus) for auto-
matically obtaining a decision tree which allows us
to determine the correct class. Thus, the different
outputs of multiple classifiers are then introduced to
another supervised classifier which constructs the fi-
nal classification model.
2.4 Approach 4: Distances measures
This approach is constructed by adding five distance
values to the Approach 2. These values are calcu-
lated as follows :
? The Euclidean distance between T2 and T3,
and between T1 and T4. We have used the fre-
quency of each word for constructing a repre-
sentative vector of each sentence.
? The Manhattan distance between T2 and T3,
and between T1 and T4. We have used the fre-
quency of each word for constructing a repre-
sentative vector of each sentence.
? A variant of the Jaccard?s Coefficient that con-
sider synonyms (Carrillo et al, 2012). Since we
have only obtained synonyms for the English
language, this measure was only calculated be-
tween T2 and T3.
Therefore, the total number of features of the????
Pair(T1, T2) vector is 56.
2.5 Approach 5: Voting system
With the results of the previous four models, we pre-
pared a voting system which uses the majority crite-
rion (3 of 4).
3 Experimental results
The results obtained in the competition are presented
and discussed in this section. First, we describe the
training and test corpus, and thereafter, the results
obtained with the different approaches submitted.
3.1 Dataset
In order to train the different approaches already dis-
cussed, we have constructed a training corpus made
up of two datasets: the training data provided by the
task organizers the task 8 of SemEval 2013 (Negri
et al, 2013), and the test dataset together with the
gold standard of CLTE task of SemEval 2012 (Ne-
gri et al, 2011). Thus, the training corpus contains
4000 sentence pairs. The test set provided in the
competition contains 2000 sentence pairs. The cor-
pus is balanced, with 1000 pairs for each language
in the training dataset, whereas, 500 pairs are given
in the test set for each language (see Table 2).
Table 2: Description of the dataset
Languages Training Test
SPA-ENG 1000 500
DEU-ENG 1000 500
ITA-ENG 1000 500
FRA-ENG 1000 500
Total 4000 2000
3.2 Results
In Table 3 we can see the results obtained by each
one of the five approaches we submitted to the com-
petition. Each approach has been labeled with the
prefix ?BUAP-R? for indicating the approach used
by each submitted run. For instance, the BUAP-R1
run corresponds to the approach 1 described in the
previous section. As can be seen, the behavior of the
five approaches is quite similar, which we consider
it is expected because the underlying methodology
employed is almost the same for all the approaches.
With exception of the pair of sentences written in
SPA-ENG in which the best approach was obtained
by the BUAP-R5 run, the approach 4 outperformed
the other appproaches. We believe that this has been
a result of introducing measures of similarity be-
tween the two sentences and their translations. In
this table it is also reported the Highest, Average,
Median and Lowest values of the competition. The
results we obtained are under the Median but outper-
formed the results of two teams in the competition.
With the purpose of analyzing the behavior of the
approach 4 in each one of the entailment judgments,
we have provided the results obtained in Table 4.
There we can see that the BACKWARD class is the
easiest one for being predicted, independently of the
language. The second easiest class is FORWARD,
followed by NO-ENTAILMENT. Also we can see
that the BI-DIRECTIONAL class is the one that pro-
duce more confusion, thus leading to obtain a lower
performance than the other ones.
126
Table 3: Overall statistics obtained in the Task-8 of Se-
mEval 2013
SPA- ITA- FRA- DEU-
RUN ENG ENG ENG ENG
Highest 0.434 0.454 0.458 0.452
Average 0.393 0.393 0.401 0.375
Median 0.392 0.402 0.416 0.369
Lowest 0.340 0.324 0.334 0.316
BUAP-R1 0.364 0.358 0.368 0.322
BUAP-R2 0.374 0.358 0.364 0.318
BUAP-R3 0.380 0.358 0.362 0.316
BUAP-R4 0.364 0.388 0.392 0.350
BUAP-R5 0.386 0.360 0.372 0.318
Table 4: Statistics of the approach 4, detailed by entail-
ment judgment
ENTAILMENT SPA- ITA- FRA- DEU-
JUDGEMENT ENG ENG ENG ENG
BACKWARD 0.495 0.462 0.431 0.389
FORWARD 0.374 0.418 0.407 0.364
NO-ENTAILMENT 0.359 0.379 0.379 0.352
BI-DIRECTIONAL 0.277 0.327 0.352 0.317
4 Conclusions
Five different approaches for the Cross-lingual Tex-
tual Entailment for the Content Synchronization task
of Semeval 2013 are reported in this paper. We used
several features for determining the textual entail-
ment judgment between two texts T1 and T2 (writ-
ten in two different languages). The approach 4
proposed, which employed lexical similarity and se-
mantic similarity in English language only was the
one that performed better. As future work, we would
like to include more distance metrics which allow to
extract additional features of the pair of sentences
topically related.
References
Maya Carrillo, Darnes Vilarin?o, David Pinto,
Mireya Tovar, Saul Leo?n, and Esteban Castillo.
Fcc: Three approaches for semantic textual sim-
ilarity. In *SEM 2012: The First Joint Confer-
ence on Lexical and Computational Semantics ?
Volume 1 and 2 (SemEval 2012), pages 631?634,
Montre?al, Canada, 7-8 June 2012. Association for
Computational Linguistics.
Ido Dagan and Oren Glickman. Probabilistic tex-
tual entailment: Generic applied modeling of
language variability. In PASCAL Workshop on
Learning Methods for Text Understanding and
Mining, 2004.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Wit-
ten. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November
2009. ISSN 1931-0145.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proc. of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 1336?1345,
Stroudsburg, PA, USA, 2011. Association for
Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. Detecting semantic equivalence and infor-
mation disparity in cross-lingual documents. In
Proc. of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: Short Papers
- Volume 2, ACL ?12, pages 120?124, Strouds-
burg, PA, USA, 2012. Association for Computa-
tional Linguistics.
M. Negri, A. Marchetti, Y. Mehdad, L. Ben-
tivogli, and D. Giampiccolo. Semeval-2013 Task
8: Cross-lingual Textual Entailment for Content
Synchronization. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (Se-
mEval 2013), 2013.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad,
Danilo Giampiccolo, and Alessandro Marchetti.
Divide and conquer: crowdsourcing the creation
of cross-lingual textual entailment corpora. In
Proc. of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?11,
pages 670?679, Stroudsburg, PA, USA, 2011. As-
sociation for Computational Linguistics. ISBN
978-1-937284-11-4.
127
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 145?148,
Dublin, Ireland, August 23-24, 2014.
BUAP: Evaluating Compositional Distributional Semantic Models on Full
Sentences through Semantic Relatedness and Textual Entailment
Sau?l Leo?n, Darnes Vilarin?o, David Pinto, Mireya Tovar, Beatriz Beltra?n
Beneme?rita Universidad Auto?noma de Puebla
Faculty of Computer Science
14 Sur y Av. San Claudio, CU
Puebla, Puebla, Me?xico
{saul.leon,darnes,dpinto,mtovar,bbeltran}@cs.buap.mx
Abstract
The results obtained by the BUAP team at
Task 1 of SemEval 2014 are presented in this
paper. The run submitted is a supervised ver-
sion based on two classification models: 1)
We used logistic regression for determining
the semantic relatedness between a pair of
sentences, and 2) We employed support vec-
tor machines for identifying textual entailment
degree between the two sentences. The be-
haviour for the second subtask (textual entail-
ment) obtained much better performance than
the one evaluated at the first subtask (related-
ness), ranking our approach in the 7th position
of 18 teams that participated at the competi-
tion.
1 Introduction
The Compositional Distributional Semantic Models
(CDSM) applied to sentences aim to approximate
the meaning of those sentences with vectors summa-
rizing their patterns of co-occurrence in corpora. In
the Task 1 of SemEval 2014, the organizers aimed
to evaluate the performance of this kind of models
through the following two tasks: semantic related-
ness and textual entailment. Semantic relatedness
captures the degree of semantic similarity, in this
case, between a pair of sentences, whereas textual
entailment allows to determine the entailment rela-
tion holding between two sentences.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
This document is a description paper, therefore,
we focus the rest of it on the features and models we
used for carrying out the experiments. A complete
description of the task and the dataset used are given
in Marelli et al. (2014a) and in Marelli et al. (2014b),
respectively.
The remaining of this paper is structured as fol-
lows. In Section 2 we describe the general model
we used for comparing two sentences and the set of
the features used for constructing the vectorial rep-
resentation for each sentence. Section 3 shows how
we integrate the features calculated in a single vector
which fed a supervised classifier aiming to construct
a classication model that solves the two aforemen-
tioned problems: semantic relatedness and textual
entailment. In the same section we show the ob-
tained results. Finally, in Section 4 we present our
findings.
2 Description of the Distributional
Semantic Model Used
Given a sentence S = w
1
w
2
? ? ?w
|S|
, with w
i
a sen-
tence word, we have calculated different correlated
terms (t
i,j
) or a numeric vector (V
i
) for each word
w
i
as follows:
1. {t
i,j
|relation(t
i,j
, w
i
)} such as ?relation? is
one the following dependency relations: ?ob-
ject?, ?subject? or ?property?.
2. {t
i,j
|t
i,j
= c
k
? ? ? c
k+n
} with n = 2, ? ? ? , 5, and
c
k
? w
i
; these tokens are also known as n-
grams of length n.
3. {t
i,j
|t
i,j
= c
k
? ? ? c
k+((n?1)?r)
} with n =
145
2, ? ? ? , 5, r = 2, ? ? ? , 5, and c
k
? w
i
; these to-
kens are also known as skip-grams of length
n.
4. V
i
is obtained by applying the Latent Semantic
Analysis (LSA) algorithm implemented in the
R software environment for statistical comput-
ing and graphics. V
i
is basically a vector of val-
ues that represent relation of the word w
i
with
it context, calculated by using a corpus con-
structed by us, by integrating information from
Europarl, Project-Gutenberg and Open Office
Thesaurus.
3 A Classification Model for Semantic
Relatedness and Textual Entailment
based on DSM
Once each sentence has been represented by means
of a vectorial representation of patterns, we con-
structed a single vector, ??u , for each pair of sen-
tences with the aim of capturing the semantic relat-
edness on the basis of a training corpus.
The entries of this representation vector are calcu-
lated by obtaining the semantic similarity between
each pair of sentences, using each of the DSM
shown in the previous section. In order to calcu-
late each entry, we have found the maximum similar-
ity between each word of the first sentence with re-
spect to the second sentence and, thereafter, we have
added all these values, thus, ??u = {f
1
, ? ? ? , f
9
}.
Given a pair of sentences S
1
=
w
1,1
w
2,1
? ? ?w
|S
1
|,1
and S
2
= w
1,2
w
2,2
? ? ?w
|S
2
|,2
,
such as each w
i,k
is represented according to the
correlated terms or numeric vectors established
at Section 2, the entry f
i
of ??u is calculated
as: f
l
=
?
|S
1
|
i=1
max{sim(w
i,1
, w
j,2
)}, with
j = 1, ? ? ? , |S
2
|.
The specific similarity measure (sim()) and the
correlated term or numeric vector used for each f
l
is
described as follows:
1. f
1
: w
i,k
is the ?object? of w
i
(as defined
in 2), and sim() is the maximum similar-
ity obtained by using the following six Word-
Net similarity metrics offered by NLTK: Lea-
cock & Chodorow (Leacock and Chodorow,
1998), Lesk (Lesk, 1986), Wu & Palmer (Wu
and Palmer, 1994), Resnik (Resnik, 1995), Lin
(Lin, 1998), and Jiang & Conrath1 (Jiang and
Conrath, 1997).
2. f
2
: w
i,k
is the ?subject? of w
i
, and sim() is
the maximum similarity obtained by using the
same six WordNet similarity metrics.
3. f
3
: w
i,k
is the ?property? of w
i
, and sim() is
the maximum similarity obtained by using the
same six WordNet similarity metrics.
4. f
4
: w
i,k
is an n-gram containing w
i
, and sim()
is the cosine similarity measure.
5. f
5
: w
i,k
is an skip-gram containing w
i
, and
sim() is the cosine similarity measure.
6. f
6
: w
i,k
is numeric vector obtained with LSA,
and sim() is the Rada Mihalcea semantic sim-
ilarity measure (Mihalcea et al., 2006).
7. f
7
: w
i,k
is numeric vector obtained with LSA,
and sim() is the cosine similarity measure.
8. f
8
: w
i,k
is numeric vector obtained with LSA,
and sim() is the euclidean distance.
9. f
9
: w
i,k
is numeric vector obtained with LSA,
and sim() is the Chebyshev distance.
All these 9 features were introduced to a logistic
regression classifier in order to obtain a classifica-
tion model which allows us to determine the value of
relatedness between a new pair of sentences2. Here,
we use as supervised class, the value of relatedness
given to each pair of sentences on the training cor-
pus.
The obtained results for the relatedness subtask
are given in Table 1. In columns 2, 3 and 5, a large
value signals a more efficient system, but a large
MSE (column 4) means a less efficient system. As
can be seen, our run obtained the rank 12 of 17, with
values slightly below the overall average.
3.1 Textual Entailment
In order to calculate the textual entailment judgment,
we have enriched the vectorial representation previ-
ously mentioned with synonyms, antonyms and cue-
1Natural Language Toolkit of Python; http://www.nltk.org/
2We have employed the Weka tool with the default settings
for this purpose
146
Table 1: Results obtained at the substask ?Relatedness? of the Semeval 2014 Task 1
TEAM ID PEARSON SPEARMAN MSE Rank
ECNU run1 0.82795 0.76892 0.32504 1
StanfordNLP run5 0.82723 0.75594 0.32300 2
The Meaning Factory run1 0.82680 0.77219 0.32237 3
UNAL-NLP run1 0.80432 0.74582 0.35933 4
Illinois-LH run1 0.79925 0.75378 0.36915 5
CECL ALL run1 0.78044 0.73166 0.39819 6
SemantiKLUE run1 0.78019 0.73598 0.40347 7
CNGL run1 0.76391 0.68769 0.42906 8
UTexas run1 0.71455 0.67444 0.49900 9
UoW run1 0.71116 0.67870 0.51137 10
FBK-TR run3 0.70892 0.64430 0.59135 11
BUAP run1 0.69698 0.64524 0.52774 12
UANLPCourse run2 0.69327 0.60269 0.54225 13
UQeResearch run1 0.64185 0.62565 0.82252 14
ASAP run1 0.62780 0.59709 0.66208 15
Yamraj run1 0.53471 0.53561 2.66520 16
asjai run5 0.47952 0.46128 1.10372 17
overall average 0.71876 0.67159 0.63852 8-9
Our difference against the overall average -2% -3% 11% -
words (?no?, ?not?, ?nobody? and ?none?) for de-
tecting negation at the sentences3 . Thus, if some of
these new features exist on the training pair of sen-
tences, we add a boolean value of 1, otherwise we
set the feature to zero.
This new set of vectors is introduced to a support
vector machine classifier4, using as class the textual
entailment judgment given on the training corpus.
The obtained results for the textual entailment
subtask are given in Table 2. Our run obtained the
rank 7 of 18, with values above the overall average.
We consider that this improvement over the related-
ness task was a result of using other features that
are quite important for semantic relatedness, such
as lexical relations (synonyms and antonyms), and
the consideration of the negation phenomenon in the
sentences.
4 Conclusions
This paper describes the use of compositional distri-
butional semantic models for solving the problems
3Synonyms were extracted from WordNet, whereas the
antonyms were collected from Wikipedia.
4Again, we have employed the weka tool with the default
settings for this purpose.
of semantic relatedness and textual entailment. We
proposed different features and measures for that
purpose. The obtained results show a competitive
approach that may be further improved by consider-
ing more lexical relations or other type of semantic
similarity measures.
In general, we obtained the 7th place in the official
ranking list from a total of 18 teams that participated
at the textual entailment subtask. The result at the
semantic relatedness subtask could be improved if
we were considered to add the new features taken
into consideration at the textual entailment subtask,
an idea that we will implement in the future.
References
Jay J. Jiang and David W. Conrath. Semantic simi-
larity based on corpus statistics and lexical taxon-
omy. In Proc of 10th International Conference
on Research in Computational Linguistics, RO-
CLING?97, pages 19?33, 1997.
Claudia Leacock and Martin Chodorow. Combin-
ing local context and wordnet similarity for word
sense identification. In Christiane Fellfaum, edi-
tor, MIT Press, pages 265?283, 1998.
147
Table 2: Results obtained at the substask ?Textual Entailment? of the Semeval 2014 Task 1
TEAM ID ACCURACY Rank
Illinois-LH run1 84.575 1
ECNU run1 83.641 2
UNAL-NLP run1 83.053 3
SemantiKLUE run1 82.322 4
The Meaning Factory run1 81.591 5
CECL ALL run1 79.988 6
BUAP run1 79.663 7
UoW run1 78.526 8
CDT run1 77.106 9
UIO-Lien run1 77.004 10
FBK-TR run3 75.401 11
StanfordNLP run5 74.488 12
UTexas run1 73.229 13
Yamraj run1 70.753 14
asjai run5 69.758 15
haLF run2 69.413 16
CNGL run1 67.201 17
UANLPCourse run2 48.731 18
Overall average 75.358 11-12
Our difference against the overall average 4.31% -
Michael Lesk. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceed-
ings of the 5th Annual International Conference
on Systems Documentation, pages 24?26. ACM,
1986.
Dekang Lin. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Inter-
national Conference on Machine Learning, ICML
?98, pages 296?304, San Francisco, CA, USA,
1998. Morgan Kaufmann Publishers Inc.
Marco Marelli, Luisa Bentivogli, Marco Baroni,
Raffaella Bernardi, Stefano Menini, and Roberto
Zamparelli. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on
full sentences through semantic relatedness and
textual entailment. In Proceedings of the 8th
International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, 2014a.
Marco Marelli, Stefano Menini, Marco Baroni,
Luisa Bentivogli, Raffaella Bernardi, and Roberto
Zamparelli. A sick cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC 2014, Reykjavik, Iceland,
2014b.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings
of the 21st National Conference on Artificial In-
telligence, pages 775?780, 2006.
Philip Resnik. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceed-
ings of the 14th International Joint Conference on
Artificial Intelligence, IJCAI?95, pages 448?453,
San Francisco, CA, USA, 1995.
Zhibiao Wu and Martha Stone Palmer. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, 1994.
148
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 149?153,
Dublin, Ireland, August 23-24, 2014.
BUAP: Evaluating Features for Multilingual and Cross-Level Semantic
Textual Similarity
Darnes Vilarin?o, David Pinto, Sau?l Leo?n, Mireya Tovar,Beatriz Beltra?n
Beneme?rita Universidad Auto?noma de Puebla
Faculty of Computer Science
14 Sur y Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes,dpinto,saul.leon,mtovar,bbeltran}@cs.buap.mx
Abstract
In this paper we present the evaluation of
different features for multiligual and cross-
level semantic textual similarity. Three dif-
ferent types of features were used: lexical,
knowledge-based and corpus-based. The re-
sults obtained at the Semeval competition rank
our approaches above the average of the rest
of the teams highlighting the usefulness of the
features presented in this paper.
1 Introduction
Semantic textual similarity aims to capture whether
the meaning of two texts are similar. This concept
is somehow different from the textual similarity def-
inition itself, because in the latter we are only in-
terested in measuring the number of lexical com-
ponents that the two texts share. Therefore, tex-
tual similarity can range from exact semantic equiv-
alence to a complete unrelatedness pair of texts.
Finding the semantic similarity between a pair
of texts has become a big challenge for specialists
in Natural Language Processing (NLP), because it
has applications in some NLP task such as machine
translation, automatic construction of summaries,
authorship attribution, machine reading comprehen-
sion, information retrieval, among others, which
usually need a manner to calculate degrees of simi-
larity between two given texts.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
Semantic textual similarity can be calculated us-
ing texts of different sizes, for example between, a
paragraph and a sentence, or a sentence and a phrase,
or a phrase and a word, or even a word and a sense.
When we consider this difference, we say the task is
called ?Cross-Level Semantic Similarity?, but when
this distinction is not considered, then we call the
task just as ?Semantic Textual Similarity?.
In this paper, we evaluate different features for de-
termining those that obtain the best performances for
calculating both, cross-level semantic similarity and
multilingual semantic textual similarity.
The remaining of this paper is structured as fol-
lows. Section 2 presents the features used in both
experiments. Section 3 shows the manner we used
the features for determining the degree of seman-
tic textual similarity. Section 4, on the other hand,
shows the experiments we have carried out for de-
termining cross-level semantic similarity. Finally, in
Section 5 the conclusions and findings are given.
2 Description of Features
In this section we describe the different features used
for evaluation semantic textual similarity. Basically,
we have used three different types of features: lex-
ical, knowledge-based and corpus-based. The first
one, counts the frequency of ocurrence of lexical
features which include n-grams of characters, skip-
grams1, words and some lexical relationships such
as synonymy or hypernymy. Additionally, we have
used two other features: the Jaccard coefficient be-
tween the two text, expanding each term with a set of
1They are also known as disperse n-grams because they con-
sider to ?skip? a certain number of characters.
149
synonyms taken from WordReference Carrillo et al.
(2012), and the cosine between the two texts repre-
sented each by a bag of character n-grams and skip-
grams. In this case, we did not applied any word
sense disambiguation system before expanding with
synonyms, a procedure that may be performed in a
further work.
The second set of features considers the following
six word similarity metrics offered by NLTK: Lea-
cock & Chodorow (Leacock and Chodorow, 1998),
Lesk (Lesk, 1986), Wu & Palmer (Wu and Palmer,
1994), Resnik (Resnik, 1995), Lin (Lin, 1998), and
Jiang & Conrath2 (Jiang and Conrath, 1997). In
this case, we determine the similarity between two
texts as the maximum possible pair of words similar-
ity. The third set of features considers two corpus-
based measures, both based on Rada Mihalcea?s tex-
tual semantic similarity (Mihalcea et al., 2006). The
first one uses Pointwise Mutual Information (PMI)
(Turney, 2001) for calculating the similarity between
pairs of words, whereas the second one uses Latent
Semantic Analysis (LSA) (Landauer et al., 1998)
(implemented in the R software environment for sta-
tistical computing) for that purpose. In particular,
the PMI and LSA values were obtained using a cor-
pus built on the basis of Europarl, Project-Gutenberg
and Open Office Thesaurus. A summary of these
features can be seen in Table 1.
3 Multilingual Semantic Textual Similarity
This task aims to find the semantic textual similar-
ity between two texts written in the same language.
Two different languages were considered: English
and Spanish. The degree of semantic similarity
ranges from 0 to 5; the bigger this value, the best se-
mantic match between the two texts. For the experi-
ments we have used the training datasets provided at
2012, 2013 and 2014 Semeval competitions. These
datasets are completely described at the task descrip-
tion papers of these Semeval editions Agirre et al.
(2013, 2014).
In order to calculate the semantic textual simi-
larity for the English language, we have used all
the features mentioned at Section 2. We have con-
structed a single vector for each pair of texts of the
training corpus, thus resulting 6,627 vectors in total.
2Natural Language Toolkit of Python; http://www.nltk.org/
The resulting set of vectors fed a supervised classi-
fier, in particular, a logistic regression model3. This
approach has been named as BUAP-EN-run1. The
most representative results obtained at the competi-
tion for the English language can be seen in Table 2.
As can be seen, we outperformed the average result
in all the cases, except on the case that the OnWN
corpus was used.
In order to calculate the semantic textual similar-
ity for the Spanish language, we have submitted two
runs, the first one is a supervised approach which
constructs a regression model, similar that the one
constructed for the English language, but consider-
ing only the following features: character n-grams,
character skip-grams, and the cosine similarity of
bag of character n-grams and skip-grams. This ap-
proach was named BUAP-run1. Given that the num-
ber of Spanish samples was so small, we decided
to investigate the behaviour of training with English
and testing with Spanish language. It is quite inter-
esting that this approach obtained a relevant ranking
(17 from 22 runs), even if the type of features used
were na??ve.
The second approach submitted for determining
the semantic textual similarity for the Spanish lan-
guage is an unsupervised one. It uses the same fea-
tures of the supervised approach for Spanish, but
these features were used to create a representation
vector for each text (independently), so that we may
be able to calculate the similarity by means of the
cosine measure between the two vectors. The ap-
proach was named BUAP-run2.
The most representative results obtained at the
competition for the Spanish language can be seen
in Table 3. There we can see that our unsupervised
approach slightly outperformed the overall average,
but the supervised approach was below the overall
average, a fact that is expected since we have trained
using the English corpus and testing with the Span-
ish language. Despite this, it is quite interesting that
the result obtained with this supervised approach is
not so bad.
Due to space constraints, we did not reported the
complete set of results of the competition, however,
these results can be seen at the task 10 description
3We used the version of the logistic classifier implemented
in the the Weka toolkit
150
Table 1: Features used for calculating semantic textual similarity
Feature Type
n-grams of characters (n = 2, ? ? ? , 5) Lexical
skip-grams of characters (skip = 2, ? ? ? , 5) Lexical
Number of words shared Lexical
Number of synonyms shared Lexical
Number of hypernyms shared Lexical
Jaccard coefficient with synonyms expansion Lexical
Cosine of bag of character n-grams and skip-grams Lexical
Leacock & Chodorow?s word similarity Knowledge-based
Lesk?s word similarity Knowledge-based
Wu & Palmer?s word similarity Knowledge-based
Resnik?s word similarity Knowledge-based
Lin?s word similarity Knowledge-based
Jiang & Conrath?s word similarity Knowledge-based
Rada Mihalcea?s metric using PMI Corpus-based
Rada Mihalcea?s metric using LSA Corpus-based
Table 2: Results obtained at the Task 10 of the Semeval competition for the English language
Team Name deft-forum deft-news headlines images OnWN tweet-news Weighted mean Rank
DLS@CU-run2 0.4828 0.7657 0.7646 0.8214 0.8589 0.7639 0.7610 1
Meerkat Mafia-pairingWords 0.4711 0.7628 0.7597 0.8013 0.8745 0.7793 0.7605 2
NTNU-run3 0.5305 0.7813 0.7837 0.8343 0.8502 0.6755 0.7549 3
BUAP-EN-run1 0.4557 0.6855 0.6888 0.6966 0.6539 0.7706 0.6715 19
Overall average 0.3607 0.6198 0.5885 0.6760 0.6786 0.6001 0.6015 27-28
Bielefeld SC-run2 0.2108 0.4307 0.3112 0.3558 0.3607 0.4087 0.3470 36
UNED-run22 p np 0.1043 0.3148 0.0374 0.3243 0.5086 0.4898 0.3097 37
LIPN-run2 0.0843 - - - - - 0.0101 38
Our difference against the average 9% 7% 10% 2% -2% 17% 7% -
Table 3: Results obtained at the Task 10 of the Semeval competition for the Spanish language (NOTE: The * symbol
denotes a system that used Wikipedia to build its model for the Wikipedia test dataset)
Team Name System type Wikipedia News Weighted correlation Rank
UMCC DLSI-run2 supervised 0.7802 0.8254 0.8072 1
Meerkat Mafia-run2 unsupervised 0.7431 0.8454 0.8042 2
UNAL-NLP-run1 weakly supervised 0.7804 0.8154 0.8013 3
BUAP-run2 unsupervised 0.6396 0.7637 0.7137 14
Overall average - 0.6193 0.7504 0.6976 14-15
BUAP-run1 supervised 0.5504 0.6785 0.6269 17
RTM-DCU-run2 supervised 0.3689 0.6253 0.5219 20
Bielefeld SC-run2 unsupervised* 0.2646 0.5546 0.4377 21
Bielefeld SC-run1 unsupervised* 0.2632 0.5545 0.4371 22
Difference between our run1 and the overall average - -7% -7% -7% -
Difference between our run2 and the overall average - 2% 1% 2% -
paper (Agirre et al., 2014) of Semeval 2014.
4 Cross-Level Semantic Similarity
This task aims to find semantic similarity between
a pair of texts of different length written in En-
glish language, actually each text belong to a dif-
ferent level of representation of language (para-
graph, sentence, phrase, word, and sense). Thus,
the pair of levels that were required to be compared
in order to determine their semantic similarity were:
paragraph-to-sentence, sentence-to-phrase, phrase-
to-word, and word-to-sense.
The task cross level similarity judgments are
based on five rating levels which goes from 0 to
151
4. The first (0) implies that the two items do not
mean the same thing and are not on the same topic,
whereas the last one (4) implies that the two items
have very similar meanings and the most important
ideas, concepts, or actions in the larger text are rep-
resented in the smaller text. The remaining rating
levels imply something in the middle.
For word-to-sense comparison, a sense is paired
with a word and the perceived meaning of the word
is modulated by virtue of the comparison with the
paired sense?s definition. For the experiments pre-
sented at the competition, a corpus of 2,000 pairs
of texts were provided for training and other 2,000
pairs for testing. This dataset considered 500 pairs
for each type of level of semantic similarity. The
complete description of this task together with the
dataset employed is given in the task description pa-
per Jurgens et al. (2014).
We submitted two supervised approaches, to this
task employing all the features presented at Section
2. The first approach simply constructs a single vec-
tor for each pair of training texts using the afore-
mentioned features. These vectors are introduced in
Weka for constructing a classification model based
on logistic regression. This approach was named
BUAP-run1.
We have observed that when comparing texts of
different length, there may be a high discrepancy
between those texts because a very small length in
the texts may difficult the process of determining the
semantic similarity. Therefore, we have proposed
to expand small text with the aim of having more
term useful in the process of calculating the degree
of semantic similarity. In particular, we have ex-
panded words for the phrase-to-word and word-to-
sense cases. The expansion has been done as fol-
lows. When we calculated the similarity between
phrases and words, we expanded the word compo-
nent with those related terms obtained by means of
the Related-Tags Service of Flickr. When we cal-
culated the semantic similarity between words and
senses, we expanded the word component with their
WordNet Synsets (none word sense disambiguation
method was employed). This second approach was
named BUAP-run2.
The most representative results for the cross-level
semantic similarity task (which include our results)
are shown in Table 4. There we can see that the fea-
tures obtained a good performance when we com-
puted the semantic similarity between paragraphs
and sentences, and when we calculated the simili-
raty between sentences to phrases. Actually, both
runs obtained exactly the same result, because the
main difference between these two runs is that the
second one expands the word/sense using the Re-
lated Tags of Flickr. However, the set of expansion
words did not work properly, in particular when cal-
culating the semantic similarity between phrases and
words. We consider that this behaviour is due to
the domain of the expansion set do not match with
the domain of the dataset to be evaluated. In the
case of expanding words for calculating the similar-
ity between words and senses, we obtained a slightly
better performance, but again, this values are not
sufficient to highly outperform the overall average.
As future work we consider to implement a self-
expansion technique for obtaining a set of related
terms by means of the same training corpus. This
technique has proved to be useful when the expan-
sion process is needed in restricted domains Pinto
et al. (2011).
5 Conclusions
This paper presents the results obtained by the
BUAP team at the Task 3 and 10 of SemEval 2014.
In both task we have used a set of similar features,
due to the aim of these two task are quite similar:
determining semantic similarity. Some special mod-
ifications has been done according to each task in
order to tackle some issues like the language or the
text length.
In general, the features evaluated performed well
over the two approaches, however, some issues arise
that let us know that we need to tune the approaches
presented here. For example, a better expansion set
is required in the case of the Task 3, and a great num-
ber of samples for the spanish samples of Task 10
will be required.
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor
Gonzalez-Agirre, and Weiwei Guo. *sem 2013
shared task: Semantic textual similarity. In 2nd
Joint Conference on Lexical and Computational
152
Table 4: Results obtained at Task 3 of Semeval 2014
Team System Paragraph-to-Sentence Sentence-to-Phrase Phrase-to-Word Word-to-Sense Rank
SimCompass run1 0.811 0.742 0.415 0.356 1
ECNU run1 0.834 0.771 0.315 0.269 2
UNAL-NLP run2 0.837 0.738 0.274 0.256 3
BUAP run1 0.805 0.714 0.162 0.201 9
BUAP run2 0.805 0.714 0.142 0.194 10
Overall average - 0.728 0.651 0.198 0.192 11-12
Our run1 - Overall average 8% 6% -4% 1% -
Our run2 - Overall average 8% 6% -6% 0% -
Semantics (*SEM), pages 32?43, Atlanta, Geor-
gia, USA, 2013.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. Semeval-2014 task 10: Multilingual se-
mantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evalua-
tion (SemEval-2014), Dublin, Ireland, 2014.
Maya Carrillo, Darnes Vilarin?o, David Pinto,
Mireya Tovar, Saul Leo?n, and Esteban Castillo.
Fcc: Three approaches for semantic textual sim-
ilarity. In Proceedings of the 1st Joint Con-
ference on Lexical and Computational Seman-
tics (SemEval 2012), pages 631?634, Montre?al,
Canada, 2012.
Jay J. Jiang and David W. Conrath. Semantic simi-
larity based on corpus statistics and lexical taxon-
omy. In Proc of 10th International Conference
on Research in Computational Linguistics, RO-
CLING?97, pages 19?33, 1997.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. Semeval-2014 task 3: Cross-
level semantic similarity. In Proceedings of the
8th International Workshop on Semantic Evalua-
tion (SemEval-2014), Dublin, Ireland, 2014.
Thomas K. Landauer, Peter W. Foltz, and Darrell
Laham. An Introduction to Latent Semantic Anal-
ysis. Discourse Processes, (25):259?284, 1998.
Claudia Leacock and Martin Chodorow. Combin-
ing local context and wordnet similarity for word
sense identification. In Christiane Fellbaum, edi-
tor, MIT Press, pages 265?283, 1998.
Michael Lesk. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceed-
ings of the 5th Annual International Conference
on Systems Documentation, pages 24?26. ACM,
1986.
Dekang Lin. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Inter-
national Conference on Machine Learning, ICML
?98, pages 296?304, San Francisco, CA, USA,
1998. Morgan Kaufmann Publishers Inc.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings
of the 21st National Conference on Artificial In-
telligence, pages 775?780, 2006.
David Pinto, Paolo Rosso, and He?ctor Jime?nez-
Salazar. A self-enriching methodology for clus-
tering narrow domain short texts. Computer Jour-
nal, 54(7):1148?1165, 2011.
Philip Resnik. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceed-
ings of the 14th International Joint Conference on
Artificial Intelligence, IJCAI?95, pages 448?453,
San Francisco, CA, USA, 1995.
Peter D. Turney. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of the
12th European Conference on Machine Learning,
pages 491?502. Springer-Verlag, 2001.
Zhibiao Wu and Martha Stone Palmer. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, 1994.
153
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 154?159,
Dublin, Ireland, August 23-24, 2014.
BUAP: Polarity Classification of Short Texts
David Pinto1, Darnes Vilarin?o1, Saul Leo?n1, Miguel Jasso1,2, Cupertino Lucero2
1 Beneme?rita Universidad Auto?noma de Puebla
14 Sur y Av. San Claudio, CU, 72570
Puebla, Puebla, Me?xico
{dpinto,darnes,saul.leon}@cs.buap.mx
2 Universidad Tecnolo?gica de Izu?car de Matamoros
Prolongacio?n Reforma 168, Santiago Mihuacan, 74420
Izu?car de Matamoros, Puebla, Me?xico
migueljhdz18@yahoo.com.mx, cuper lucero@hotmail.com
Abstract
We report the results we obtained at the sub-
task B (Message Polarity Classification) of Se-
mEval 2014 Task 9. The features used for
representing the messages were basically tri-
grams of characters, trigrams of PoS and a
number of words selected by means of a graph
mining tool. Our approach performed slightly
below the overall average, except when a cor-
pus of tweets with sarcasm was evaluated,
in which we performed quite well obtaining
around 6% above the overall average.
1 Introduction
Analyzing polarity in texts is an important task that
may have various applications in real life. There ex-
ist plenty of tasks that may be benefited of computa-
tional procedures that automatically allow to detect
if the author intention has been to express himself as
a positive, negative, neutral or objective manner. Let
us consider, for instance, when a public figure (such
as a politician, celebrity, or business leader) would
like to investigate its reputation in public media. An-
other example would be to calculate the reputation
of a public or private institution. In any case, the
construction of methods for determining the polar-
ity of messages at Internet would help to investigate
their reputation.
In this paper, we present the results we obtained
when we carried out experiments for the subtask B
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
of Semeval 2014 Task 9, which was named ?Mes-
sage Polarity Classification?, and was defined as fol-
low: ?Given a message, decide whether the mes-
sage is of positive, negative, or neutral sentiment.
For messages conveying both a positive and nega-
tive sentiment, whichever is the stronger sentiment
should be chosen?.
The remaining of this paper is structured as fol-
lows. In Section 2 we present some related work
found at the literature with respect to the identifica-
tion of emotions in short texts such as twitter. Sec-
tion 3 presents the description of the features and
classification model used in our experiments. The
results obtained together with a discussion of these
results are given in Section 4. Finally, the conclu-
sions are given in Section 5.
2 Related Work
There exist a number of works in literature associ-
ated to the automatic identification of emotions in
Twitter, mainly due to the massification of this so-
cial network around the world and the easy manner
we can access to the Tweets from API?s provided by
Twitter itself. Some of these works have focused on
the contribution of some particular features, such as
Part of Speech (PoS) tags, emoticons, etc. on the
aforementioned task. In Agarwal et al. (2011), for
example, the a priori likelihood of each PoS is cal-
culated. They use up to 100 additional features that
include emoticons and a dictionary of positive and
negative words. They have reported a 60% of ac-
curacy in the task. On the other hand, in Mukher-
jee and Bhattacharyya (2012), a strategy based on
discursive relations, such as conectiveness and con-
154
ditionals, with low number of lexical resources is
proposed. These relations are integrated in classi-
cal models of representation like bag of words with
the aim of improving the accuracy values obtained
in the process of classification. The influence of se-
mantic operators such as modals and negations are
analyzed, in particular, the degree in which they af-
fect the emotion present in a given paragraph or sen-
tence.
One of the major advances obtained in the task
of sentiment analysis has been done in the frame-
work of the SemEval competition. In 2013, several
teams have participated with different approaches
Becker et al. (2013); Han et al. (2013); Chawla et al.
(2013); Balahur and Turchi (2013); Balage Filho
and Pardo (2013); Moreira et al. (2013); Reckman
et al. (2013); Tiantian et al. (2013); Marchand et al.
(2013); Clark and Wicentwoski (2013); Hamdan
et al. (2013); Mart??nez-Ca?mara et al. (2013); Lev-
allois (2013). Most of these works have contributed
in the mentioned task by proposing methods, tech-
niques for representing and classifying documents
towards the automatic classification of sentiment in
Tweets.
3 Description of the Presented Approach
We have employed a supervised approach based on
machine learning in which we construct a classifica-
tion model using the following general features ob-
tained from the training corpus.
1. Character trigrams
2. PoS tags trigrams
3. Significant Tweet words obtained by using a
graph mining tool known as SubDue
The description of how we calculated each feature
in order to construct a representation vector for each
message is given as follows.
The probability of each character trigram given
the polarity class, P (trigram|class), was cal-
culated in the training corpus. Thereafter, we
assigned a normalized probability to each sen-
tence polarity by combining the probability of
each character trigram of the sentence, i.e.,
?
|message|
i=1
log [P (trigram
i
|class)]. Since we
have four classes (?positive?,?negative?,?neutral?
and ?objective?), we have obtained four features for
the final vectorial representation of the message.
We then calculated other four features by per-
forming a similar calculation than the previous one,
but in this case, using the PoS tags of the message.
For this purpose, we used the Twitter NLP and Part-
of-Speech Tagging tool provided by the Carnegie
Mellon University (Owoputi et al., 2013). Since the
PoS tag given by this tool is basically a character,
then the same procedure can be applied.
We performed preliminary experiments by using
these eight features on a trial corpus, and we ob-
served that the results may be improved by select-
ing significant words that may not be discovered
by the statistical techniques used until now. So,
we decided to make use of techniques based on
graph mining for attempting to find those signifi-
cant words. In order to find them, we constructed a
graph representation for each message class (?pos-
itive?,?negative?,?neutral? and ?objective?), using
the training corpus. The manner we constructed
those graphs is shown as follows.
Formally, given a graph G = (V,E,L, f) with V
being the non-empty set of vertices, E ? V ?V the
edges, L the tag set, and f : E ? L, a function
that assigns a tag to a pair of associated vertices.
This graph-based representation attempt to capture
the sequence among the sentence words, so as the
sequence among their PoS tags with the aim of feed-
ing a graph mining tool which may extract relevant
features that may be further used for representing the
texts. Thus, the set V is constructed from the differ-
ent words and PoS of the target document.
In order to demonstrate the way we construct the
graph for each short text, consider the following
message: ?ooh i love you for posting this :-)?. The
associated graph representation to this message is
shown in Figure 1.
Once each paragraph is represented by means of
a graph, we apply a data mining algorithm in or-
der to find subgraphs from which we will be able
to find the significant words which will be, in our
case, basically, the nodes of these subgraphs. Sub-
due is a data mining tool widely used in structured
domains. This tool has been used for discovering
structured patterns in texts represented by means of
graphs Olmos et al. (2005). Subdue uses an eval-
uation model named ?Minimum encoding?, a tech-
155
Figure 1: Graph based message representation with words and their corresponding PoS tags
nique derived from the minimum description length
principle Rissanen (1989), in which t he best graph
sub-structures are chosen. The best subgraphs are
those that minimize the number of bits that repre-
sent the graph. In this case, the number of bits is
calculated consi dering the size of the graph adjan-
cency matrix. Thus, the best substructure is the one
that minimizes I(S) + I(G|S), where I(S) is the
number of bits required to describe the sub structure
S, and I(G|S) is the number of bits required to de-
scribe graph G after it has been compacted by the
substructure S.
By applying this procedure we obtained 597 sig-
nicant negative words, 445 positive words, 616 ob-
jective words and 925 positive words. For the final
representation vector we compiled the union of these
words, obtaining 1915 significant words. Therefore,
the total number of features for each message was
1,923.
We have used the training corpus provided at the
competition (Rosenthal et al., 2014), however, we
removed all those messsages tagged as the class
?objective-OR-neutral?, because all these messages
introduced noise to the classification process. In to-
tal, we constructed 5,217 vectors of message repre-
sentation which fed a support vector machine classi-
fier. We have used the SVM implementation of the
WEKA tool with default parameters for our exper-
iments (Hall et al., 2009). The obtained results are
shown in the next section.
4 Experimental Results
The test corpus was made up short texts (mes-
sages) categorized as: ?LiveJournal2014?,
?SMS2013?, ?Twitter2013?, ?Twitter2014? and
?Twitter2014Sarcasm?. A complete description of
the training and test datasets can be found at the
task description paper (Rosenthal et al., 2014).
In Table 1 we can see the results obtained at the
competition. Our approach performed in almost all
the cases slightly below to the overall average, ex-
cept when we processed the corpus of Twitter with
Sarcasm characteristics. We consider that two main
problems were the cause of this result: 1) The corpus
was very unbalanced and our approaches for allevi-
ating this problem were not sufficient, and 2) From
our particular point of view, there were a high differ-
ence between the vocabulary of the training and the
test corpus, thus, leading the classification model to
fail.
156
Table 1: Results obtained at the substask B of the Semeval 2014 Task 9
System LiveJournal2014 SMS2013 Twitter2013 Twitter2014 Twitter2014Sarcasm Average
NRC-Canada-B 74.84 70.28 70.75 69.85 58.16 68.78
CISUC KIS-B-late 74.46 65.90 67.56 67.95 55.49 66.27
coooolll-B 72.90 67.68 70.40 70.14 46.66 65.56
TeamX-B 69.44 57.36 72.12 70.96 56.50 65.28
RTRGO-B 72.20 67.51 69.10 69.95 47.09 65.17
AUEB-B 70.75 64.32 63.92 66.38 56.16 64.31
SWISS-CHOCOLATE-B 73.25 66.43 64.81 67.54 49.46 64.30
SentiKLUE-B 73.99 67.40 69.06 67.02 43.36 64.17
TUGAS-B 69.79 62.77 65.64 69.00 52.87 64.01
SAIL-B 69.34 56.98 66.80 67.77 57.26 63.63
senti.ue-B 71.39 59.34 67.34 63.81 55.31 63.44
Synalp-Empathic-B 71.75 62.54 63.65 67.43 51.06 63.29
Lt 3-B 68.56 64.78 65.56 65.47 47.76 62.43
UKPDIPF-B 71.92 60.56 60.65 63.77 54.59 62.30
AMI ERIC-B 65.32 60.29 70.09 66.55 48.19 62.09
ECNU-B 69.44 59.75 62.31 63.17 51.43 61.22
LyS-B 69.79 60.45 66.92 64.92 42.40 60.90
SU-FMI-B-late 68.24 61.67 60.96 63.62 48.34 60.57
NILC USP-B-twitter 69.02 61.35 65.39 63.94 42.06 60.35
CMU-Qatar-B-late 65.63 62.95 65.11 65.53 40.52 59.95
columbia nlp-B 68.79 59.84 64.60 65.42 40.02 59.73
CMUQ-Hybrid-B-late 65.14 61.75 63.22 62.71 40.95 58.75
Citius-B 62.40 57.69 62.53 61.92 41.00 57.11
KUNLPLab-B 63.77 55.89 58.12 61.72 44.60 56.82
USP Biocom-B 67.80 53.57 58.05 59.21 43.56 56.44
UPV-ELiRF-B 64.11 55.36 63.97 59.33 37.46 56.05
Rapanakis-B 59.71 54.02 58.52 63.01 44.69 55.99
DejaVu-B 64.69 55.57 57.43 57.02 42.46 55.43
GPLSI-B 57.32 46.63 57.49 56.06 53.90 54.28
Indian Inst of Tech-Patna-B 60.39 51.96 52.58 57.25 41.33 52.70
BUAP-B 53.94 44.27 56.85 55.76 51.52 52.47
SAP-RI-B 57.86 49.00 50.18 55.47 48.64 52.23
UMCC DLSI Sem 53.12 50.01 51.96 55.40 42.76 50.65
Alberta-B 52.38 49.05 53.85 52.06 40.40 49.55
SINAI-B 58.33 57.34 50.59 49.50 31.15 49.38
IBM EG-B 59.24 46.62 54.51 52.26 34.14 49.35
SU-sentilab-B-tweet 55.11 49.60 50.17 49.52 31.49 47.18
lsis lif-B 61.09 38.56 46.38 52.02 34.64 46.54
IITPatna-B 54.68 40.56 50.32 48.22 36.73 46.10
UMCC DLSI Graph-B 47.81 36.66 43.24 45.49 53.15 45.27
University-of-Warwick-B 39.60 29.50 39.17 45.56 39.77 38.72
DAEDALUS-B 40.83 40.86 36.57 33.03 28.96 36.05
Overall average 63.81 55.82 59.72 60.30 45.43 57.02
5 Conclusions
We have presented an approach for detecting mes-
sage polarity using basically three kind of features:
character trigrams, PoS tags trigrams and significant
words obtained by means of a graph mining tool.
The obtained results show that these features were
not sufficient for detecting the correct polarity of a
given message with high precision. We consider that
the unbalanced characteristic and the fact the vocab-
ulary changed significantly from the training to the
test corpus influenced the results we obtained at the
competition. However, a deep analysis we plan to
do to the datasets evaluated will allow us in the fu-
ture to find more accurate features for the message
polarity detection task.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen
Rambow, and Rebecca Passonneau. Sentiment
analysis of twitter data. In Proceedings of the
Workshop on Language in Social Media (LSM
2011), pages 30?38, Portland, Oregon, June 2011.
Pedro Balage Filho and Thiago Pardo. Nilc usp:
A hybrid system for sentiment analysis in twitter
messages. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
157
Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013),
pages 568?572, Atlanta, Georgia, USA, June
2013.
Alexandra Balahur and Marco Turchi. Improving
sentiment analysis in twitter using multilingual
machine translated data. In Proceedings of the In-
ternational Conference Recent Advances in Natu-
ral Language Processing RANLP 2013, pages 49?
55, Hissar, Bulgaria, September 2013. INCOMA
Ltd. Shoumen, BULGARIA.
Lee Becker, George Erhart, David Skiba, and Valen-
tine Matula. Avaya: Sentiment analysis on twitter
with self-training and polarity lexicon expansion.
In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages
333?340, Atlanta, Georgia, USA, June 2013.
Karan Chawla, Ankit Ramteke, and Pushpak Bhat-
tacharyya. Iitb-sentiment-analysts: Participation
in sentiment analysis in twitter semeval 2013 task.
In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages
495?500, Atlanta, Georgia, USA, June 2013.
Sam Clark and Rich Wicentwoski. Swatcs: Combin-
ing simple classifiers with estimated accuracy. In
Second Joint Conference on Lexical and Compu-
tational Semantics (*SEM), Volume 2: Proceed-
ings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 425?
429, Atlanta, Georgia, USA, June 2013.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Wit-
ten. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November
2009. ISSN 1931-0145.
Hussam Hamdan, Frederic Be?chet, and Patrice Bel-
lot. Experiments with dbpedia, wordnet and sen-
tiwordnet as resources for sentiment analysis in
micro-blogging. In Second Joint Conference on
Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh Inter-
national Workshop on Semantic Evaluation (Se-
mEval 2013), pages 455?459, Atlanta, Georgia,
USA, June 2013.
Qi Han, Junfei Guo, and Hinrich Schuetze. Codex:
Combining an svm classifier and character n-
gram language models for sentiment analysis on
twitter text. In Second Joint Conference on
Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh Inter-
national Workshop on Semantic Evaluation (Se-
mEval 2013), pages 520?524, Atlanta, Georgia,
USA, June 2013.
Clement Levallois. Umigon: sentiment analysis for
tweets based on terms lists and heuristics. In Sec-
ond Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Seman-
tic Evaluation (SemEval 2013), pages 414?417,
Atlanta, Georgia, USA, June 2013.
Morgane Marchand, Alexandru Ginsca, Romaric
Besanc?on, and Olivier Mesnard. [lvic-limsi]: Us-
ing syntactic features and multi-polarity words for
sentiment analysis in twitter. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation
(SemEval 2013), pages 418?424, Atlanta, Geor-
gia, USA, June 2013.
Eugenio Mart??nez-Ca?mara, Arturo Montejo-Ra?ez,
M. Teresa Mart??n-Valdivia, and L. Alfonso Uren?a
Lo?pez. Sinai: Machine learning and emotion of
the crowd for sentiment analysis in microblogs. In
Second Joint Conference on Lexical and Compu-
tational Semantics (*SEM), Volume 2: Proceed-
ings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 402?
407, Atlanta, Georgia, USA, June 2013.
Silvio Moreira, Joa?o Filgueiras, Bruno Martins,
Francisco Couto, and Ma?rio J. Silva. Reac-
tion: A naive machine learning approach for sen-
timent classification. In Second Joint Confer-
ence on Lexical and Computational Semantics
(*SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation
(SemEval 2013), pages 490?494, Atlanta, Geor-
gia, USA, June 2013.
Subhabrata Mukherjee and Pushpak Bhattacharyya.
158
Sentiment analysis in Twitter with lightweight
discourse analysis. In Proceedings of COLING
2012, pages 1847?1864, Mumbai, India, Decem-
ber 2012. The COLING 2012 Organizing Com-
mittee.
Ivan Olmos, Jesus A. Gonzalez, and Mauricio Os-
orio. Subgraph isomorphism detection using a
code based representation. In FLAIRS Confer-
ence, pages 474?479, 2005.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. Improved part-of-speech tagging for
online conversational text with word clusters.
In Proceedings of NAACL-HLT, pages 380?390,
2013.
Hilke Reckman, Cheyanne Baird, Jean Crawford,
Richard Crowell, Linnea Micciulla, Saratendu
Sethi, and Fruzsina Veress. teragram: Rule-based
detection of sentiment phrases using sas senti-
ment analysis. In Second Joint Conference on
Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh Inter-
national Workshop on Semantic Evaluation (Se-
mEval 2013), pages 513?519, Atlanta, Georgia,
USA, June 2013.
Jorma Rissanen. Stochastic Complexity in Statis-
tical Inquiry Theory. World Scientific Publish-
ing Co., Inc., River Edge, NJ, USA, 1989. ISBN
981020311X.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. Semeval-2014 task 9: Senti-
ment analysis in twitter. In Proceedings of the 8th
International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, 2014.
Zhu Tiantian, Zhang Fangxi, and Man Lan. Ec-
nucs: A surface information based system de-
scription of sentiment analysis in twitter in the
semeval-2013 (task 2). In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation
(SemEval 2013), pages 408?413, Atlanta, Geor-
gia, USA, June 2013.
159
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 146?152,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
On the Difficulty of Clustering Microblog Texts for
Online Reputation Management
Fernando Perez-Tellez
SMRG, Institute of Technology
Tallaght Dublin, Ireland
fernandopt@gmail.com
John Cardiff
SMRG, Institute of Technology
Tallaght Dublin, Ireland
John.Cardiff@ittdublin.ie
David Pinto
FCC, Beneme?rita Universidad
Auto?noma de Puebla, Mexico
dpinto@cs.buap.mx
Paolo Rosso
NLE Lab. -ELiRF, Universidad
Polite?cnica de Valencia, Spain
prosso@dsic.upv.es
Abstract
In recent years microblogs have taken on an
important role in the marketing sphere, in
which they have been used for sharing opin-
ions and/or experiences about a product or ser-
vice. Companies and researchers have become
interested in analysing the content generated
over the most popular of these, the Twitter
platform, to harvest information critical for
their online reputation management (ORM).
Critical to this task is the efficient and accurate
identification of tweets which refer to a com-
pany distinguishing them from those which do
not. The aim of this work is to present and
compare two different approaches to achieve
this. The obtained results are promising while
at the same time highlighting the difficulty of
this task.
1 Introduction
Twitter1 - a microblog of the Web 2.0 genre that al-
lows users to publish brief message updates - has
become an important channel through which users
can share their experiences or opinions about a prod-
uct, service or company. In general, companies have
taken advantage of this medium for developing mar-
keting strategies.
Online reputation management - the monitoring
of media and the detection and analysis of opinions
about an entity - is becoming an important area of
research as companies need up to the minute infor-
mation on what is being send on the WWW about
them and their products. Being unaware of negative
1http://twitter.com
comments regarding a company may affect its repu-
tation and misguide consumers into not buying par-
ticular products. On the other hand companies may
identify user feedback and use it in order to provide
better products and services which could make them
more competitive.
A first step in this process is the automatic col-
lection of tweets relating to a company. In this pa-
per we present an approach to the categorisation of
tweets which contain a company name, into two
clusters corresponding to those which refer to the
company and those which do not. Clearly this is not
as straightforward as matching keywords due to the
potential for ambiguity. Providing a solution to this
problem will allow companies to access to the im-
mediate user reaction to their products or services,
and thereby manage their reputations more effec-
tively (Milstein et al, 2008).
The rest of this paper is organised as follows. Sec-
tion 2 describes the problem and the related work.
Section 3 presents the data set used in the experi-
ments. Section 4 explains the approaches used in
this research work. Section 5 shows the experi-
ments, the obtained results and a discussion of them.
Finally, Section 6 presents the conclusions.
2 Problem Description and Related Work
We are interested in discriminating between Twit-
ter entries that correspond to a company from those
that do not, in particular where the company name
also has a separate meaning in the English language
(e.g. delta, palm, ford, borders). In this research
work, we regard a company name as ambiguous if
the word/s that comprise its name can be used in
146
different contexts. An example can be seen in Ta-
ble 1 where the word borders is used in the con-
text of a company (row 1 & 3) and as the bound-
ary of a country (row 2). We adapt a clustering ap-
proach to solving this problem although the size of
tweets presents a considerable challenge. Moreover
the small vocabulary size in conjunction with the
writing style makes the task more difficult. Tweets
are written in an informal style, and may also con-
tain misspellings or be grammatically incorrect. In
order to improve the representation of the tweets we
have proposed two approaches based on an expan-
sion procedure (enriching semantic similarity hid-
den behind the lexical structure). In this research
Table 1: Examples of ?True? and ?False? tweets that con-
tains the Borders word
TRUE excessively tracking the book i
ordered from borders.com. kfjgjdfkgjfd.
FALSE With a severe shortage of manpower, existing threat
to our borders, does it make any sense to send troops
to Afghanistan? @centerofright
TRUE 33% Off Borders Coupon : http://wp.me/pKHuj-qj
work we demonstrate that a term expansion method-
ology, as presented in this paper, can improve the
representation of the microblogs from a clustering
perspective, and as a consequence the performance
of the clustering task. In addition, we test the hy-
pothesis that specific company names - names that
can not be found in a dictionary - such as Lennar or
Warner may be more easily identified than generic
company names such as Borders, Palm or Delta,
because of the ambiguity of the latter.
We describe briefly here the work related to the
problem of clustering short texts related to compa-
nies. In particular those works in the field of cate-
gorisation of tweets and clustering of short texts.
In (Sankaranarayanan et al, 2009) an approach is
presented for binary classification of tweets (class
?breaking news? or other). The class ?breaking
news? is then clustered in order to find the most
similar news tweets, and finally a location of the
news for each cluster is provided. Tweets are con-
sidered short texts as mentioned in (Sriram et al,
2010) where a proposal for classifying tweets is pre-
sented. This work addressed the problem by using a
small set of domain-specific features extracted from
the author?s profile and the tweet text itself. They
claim to effectively classify the tweet to a predefined
set of generic classes such as News, Events, Opin-
ions, Deals, and Private Messages. Therefore, it is
important to analyse some techniques for categori-
sation of short texts.
The main body of relevant related research em-
anates from the WePS-3 evaluation campaign in
the task 2 called Online Reputation Management
(Amigo? et al, 2010). In (Garc??a-Cumbreras et al,
2010) the authors based their approach on recog-
nising named entities, extracting external informa-
tion and predefining rules. They use the well-known
Name Entity Recogniser (NER) included in GATE2
for recognising all the entities in their Tweets. They
also use the web page of the organisation, Wikipedia
and DBpedia3. Predefined rules are then applied to
determine if a Twitter entry belongs to an organisa-
tion or not.
The work presented in (Kalmar, 2010) uses data
from the company website. This data is used to cre-
ate a initial model from which to bootstrap a model
from the Tweets, the keywords and description are
weighted. The features used are the co-occurring
words in each tweet and the relevance of them was
calculated according to the Pointwise Mutual Infor-
mation value. Although it seems to be an interesting
approach the results shown are disappointing.
In (Yerva et al, 2010) a support vector machine
(SVM) classifier is used with the profiles built a
priori. Profiles are constructed for each company
which are sets of keywords that are related to the
company or sets of keywords unrelated to the com-
pany. This system uses external resources such as
Wordnet4, meta-data from the company web page,
GoogleSet5 and user feedback. The research pre-
sented in (Yoshida et al, 2010) propose that organi-
sation names can be classified as ?organization-line
names? or ?general-word-like names?. The authors
have observed that the fact that ratio of positive or
negative (if the tweet is related to the organisation or
not) has a strong correlation with the types of organ-
isation names i.e., ?organization-like names? have
high percentages of tweets related to the company
2http://gate.ac.uk/
3http://dbpedia.org/
4http://wordnet.princeton.edu/
5http://labs.google.com/sets
147
and when compared to ?general-word-like names?
Another approach is described in (Tsagkias and Ba-
log, 2010), in which the authors trained the well-
known J48 decision tree classifier using as features
the company name, content value such as the pres-
ence of URLs, hashtags or is-part-of-a-conversation,
content quality such as ratio of punctuation and cap-
ital characters and organisational context. This ap-
proach is quite interesting but they require a training
set.
3 Dataset Description
We base our experiments on the corpus provided for
task two of the WePS-3 evaluation campaign6, re-
lated to Online Reputation Management for organi-
sations, or specifically on the problem of organisa-
tion (company) name ambiguity.
Table 2: Statistics of company tweets used in the experi-
ments.
Company T/F 3 4 ? 5
Bestbuy 24/74 704 14.70 6 22
Borders 25/69 665 12.29 2 20
Delta 39/57 584 12.27 5 20
Ford 62/35 700 12.79 2 22
Leapfrog 70/26 1262 13.14 3 20
Opera 25/73 671 12.32 1 25
Overstock 70/24 613 13.84 3 22
Palm 28/71 762 14.20 4 22
Southwest 39/60 665 13.61 4 21
Sprint 56/38 624 12.10 3 22
Armani 312/103 2325 13.64 2 23
Barclays 286/133 2217 14.10 2 24
Bayer 228/143 2105 13.63 3 22
Blockbuster 306/131 5595 11.75 3 21
Cadillac 271/156 2449 12.19 2 24
Harpers 142/295 2356 12.20 2 23
Lennar 74/25 438 13.37 5 21
Mandalay 322/113 2085 12.42 2 22
Mgm 177/254 1977 13.63 2 24
Warner 23/76 596 13.15 4 20
T/F - No. of true/false Tweets,
3 - Vocabulary size,
4 - Average words in Tweets,
? - Minimum number of words in Tweets,
5 - Maximum number of words in Tweets.
The corpus was obtained from the trial and train-
ing data sets of this evaluation campaign. The trial
corpus of task 2 contains entries for 17 (English)
6WePS3: searching information about entities in the Web,
http://nlp.uned.es/weps/, February 2010
and 6 (Spanish) organisations; whereas the train-
ing data set contains 52 (English) organisations. The
corpus was labelled by five annotators: the true la-
bel means that the tweet is associated to a company,
whereas the false one means that the tweet is not
related to any company, and the unknown label is
used where the annotators were unable to make a
decision.
In order to gauge the problem and to estab-
lish a baseline for the potential of a clustering ap-
proach. We decided to cluster the data sets (trial
and training) using the K-means algorithm (Mac-
Queen, 1967) with k equal to three in order to have
a clear reference and detect possible drawbacks that
the collections may contain. The results were eval-
uated using the F-measure (van Rijsbergen, 1979)
and gave values of 0.52 and 0.53 for the trial and
training data sets respectively. This was expected, as
clustering approaches typically work best with long
documents and balanced groups (Perez-Tellez et al,
2009). Using this baseline, we then considered how
a clustering approach could be improved by apply-
ing text enrichment methods. In order to compare
only the effect of the enrichment however, we have
modified the data set by including only those tweets
written in English and for which a true or false
label has been established, i.e., in the experiments
carried out we do not consider the unknown label.
Furthermore, the subset used in the experiments
includes only those 20 companies with a sufficient
number of positive and negative samples (true/false),
i.e., at least 20 items must be in each category. Fi-
nally, each selected company must contain at least
90 labeled tweets, which was the minimum num-
ber of tweets associated with a company found in
the collection. In Table 2 we present a detailed de-
scription of the corpus features such as the number
of true and false tweets, the average length of the
tweets (average number of words),the minimum and
maximum number of words contained in tweets. In
the following section we present and compare the
different approaches we propose for dealing with
this problem.
4 Clustering Company Tweets
The purpose of this research work is to cluster tweets
that contain a possible company entity into two
148
groups, those that refer to the company and those
that refer to a different topic. We approach this
problem by introducing and, thereafter, evaluating
two different methodologies that use term expan-
sion. The term expansion of a set of documents is a
process for enriching the semantic similarity hidden
behind the lexical structure. Although the idea has
been previously studied in literature (Qiu and Frei,
1993; Grefenstette, 1994; Banerjee and Pedersen,
2002; Pinto et al, 2010) we are not aware of any
work in which has applied it to microblog texts. In
this paper, we evaluate the performance of two dif-
ferent approaches for term enriching in the task of
clustering company tweets.
In order to establish the difficulty of clustering
company tweets, we split the 20 companies group
into two groups that we hypothetically considered
easier and harder to be clustered. The first group
is composed of 10 companies with generic names,
i.e., names that can be ambiguous (i.e., they have an-
other common meaning and appear in a dictionary).
The second group contains specific names which are
considered to be less ambiguous (words that can be
used in limited number of contexts or words that do
not appear in a dictionary). We expect the latter
group will be easier to be categorised than the for-
mer one. In Table 3 we see the distribution of the
two groups. We have selected the K-means cluster-
Table 3: Types of Company names
Generic Company Names
BestBuy Borders Delta Ford
Leapfrog Opera Overstock Palm
Southwest Sprint
Specific Company Names
Armani Barclays Bayer Blockbuster
Cadillac Harpers Mandalay Mgm
Lennar Warner
ing method (MacQueen, 1967) for the experiments
carried out in this paper. The reason is that it is a
well-known method, it produces acceptable results
and our approaches may be compared with future
implementations. The clustering algorithm (includ-
ing the representation and matrix calculation) is ap-
plied after we have improved the representation of
tweets in order to show the improvement gained by
applying the enriching process.
Figure 1: Full Term Expansion Methodology
4.1 Full Term Expansion Methodology
(TEM-Full)
In this methodology we expand only the ambiguous
word (the company name) with all the words that co-
occur alongside it, without restrictions for the level
of co-occurrence. Our hypothesis states that the
ambiguous words may bring important information
from the identification of co-occurrence-relations to
the next step of filtering relevant terms. It is impor-
tant to mention that we have used the Term Selection
technique in order to select the most discriminative
terms for the categories. The process is shown in
Figure 1. Note that this expansion process does not
use an external resource. We believe that due to the
low term frequency and the shortness of the data, it
is better to include all the information that co-occurs
in the corpus of a company and provide more infor-
mation to the enriching process.
The Term Selection Technique helps us to identify
the best features for the clustering process. How-
ever, it is also useful to reduce the computing time
of the clustering algorithms.
4.2 Full Tem Expansion Methodology with a
Text Formaliser (TEM-Full+F)
In this approach, we test the hypothesis that we can
improve the cluster quality by increasing the level
of formality in the document text. Due to the length
restriction of 140 characters users tend to write com-
ments using abbreviations. We have used an ab-
breviation dictionary7 that contains 5,173 abbrevi-
ations commonly used in microblogs, tweets and
short messages. After the formalisation step, the ex-
pansion is performed but it is only applied to the
ambiguous word (the company name) and words
7http://noslang.com/dictionary
149
Figure 2: Full Term Expansion Methodology with a Text
Formaliser (TEM-Full+F)
which highly co-occur with it. These words were
selected as they appear in frequently with the am-
biguous word in positive tweets (i.e., those related
to the companies). We consider that this kind of
word may help us take the correct decision during
the clustering process because they are highly re-
lated with the company tweets. The words selected
to be expanded were closely related to the company
such as crew, jet, flight, airlines, airplane for Delta
company name. In the case of the Opera company
name the words expanded were software, technol-
ogy, developers, interface, web, browser. The num-
ber of words per company name were between five
and ten, showing that even a small number of words
that co-occur highly may help in the enriching pro-
cess. We have used the Term Selection Technique
as described in 4.1 and no external resource. The
process is shown in Figure 2.
5 Experimental Results
In this section we present the results obtain by the
related approaches and also the results obtained by
our methodologies proposed.
5.1 Related Approaches
Although the results are not directly comparable
with our approaches due to the slightly different
dataset used in the experiments (see Section 3), we
would like to provide a clear description of the dif-
ferent approaches with the objective of highlight the
strengths of the related approaches developed for
this purpose.
In Table 4, the best results (F-measure related
classes) reported by the approaches presented to
the task two of the WePS-3 evaluation campaign
Table 4: Related approaches (F-measure related)
Approaches
L S I U K
0.74 0.36 0.51 0.36 0.47
L = LSIR-EPFL, S = SINAI, I = ITC-UT,
U = UVA, K = KALMAR
(Amigo? et al, 2010). It is important to mention that
all these systems used the whole collection even if
the companies subsets where very imbalanced. In
our case, we are interested in proposing approaches
that can deal with two different kind of company
names such as ?generic? and ?specific? rather than
one methodology for both.
In Table 4 the LSIR-EPFL system (Yerva et al,
2010) showed very good performance even when
the subsets are very imbalanced. The SINAI sys-
tem (Garc??a-Cumbreras et al, 2010) took advan-
tage of the entity recognition process and they re-
port that named entities contained in the microblog
documents seem to be appropriate for certain com-
pany names. ITC-UT (Yoshida et al, 2010) incor-
porated a classifier and made use of Named Entity
Recognition and Part-of-Speech tagger is also good
in their performance but as the authors in (Amigo?
et al, 2010) have mentioned ?it is difficult to know
what aspect lead the system to get ahead other sys-
tems? as each takes advantage of different aspects
available such as external resources or tools. UVA
(Tsagkias and Balog, 2010) is an interesting contri-
bution but the only problem is training data will not
always be available for some domains. Finally, the
KALMAR system (Kalmar, 2010) seems to achieve
good performance when applied to well-balanced
collections. In contrast to these approaches, we
would like to emphasize that our approaches are pre-
dominantly based on the information to be clustered.
5.2 Results of Our Experiments
In order to present the performance of the different
proposed approaches, we have calculated a baseline
based on clustering, with K-means, and with no en-
riching procedure. The obtained results using the
two methodologies are compared in Table 5. We
have shown in bold text the cases in which the result
equalled or improved upon the baseline. We have
compared the methodologies presented with the two
150
subsets (generic and specific company names sub-
sets) described previously.
Table 5: A comparison of each methodology with respect
to one baseline using the F -measure.
Company Methodologies
TEM-Full TEM-Full+F B
Generic Company Names Subset
Bestbuy 0.74 0.75 0.62
Borders 0.73 0.72 0.60
Delta 0.71 0.70 0.61
Ford 0.67 0.65 0.64
Leapfrog 0.71 0.63 0.63
Opera 0.73 0.74 0.70
Overstock 0.66 0.72 0.58
Palm 0.72 0.70 0.62
Southwest 0.67 0.72 0.64
Sprint 0.67 0.65 0.64
Average 0.70 0.69 0.62
Specific Company Names Subset
Armani 0.73 0.70 0.62
Barclays 0.72 0.72 0.55
Bayer 0.71 0.70 0.63
Blockbuster 0.71 0.71 0.66
Cadillac 0.69 0.69 0.61
Harpers 0.68 0.68 0.63
Mandalay 0.74 0.84 0.64
Mgm 0.54 0.75 0.69
Lennar 0.72 0.97 0.96
Warner 0.54 0.67 0.67
Average 0.67 0.74 0.66
OA 0.68 0.72 0.64
B - Baseline, OA - Overall Average
We consider that there still some limitations on
obtaining improved results due to the particular writ-
ing style of tweets. The corpus exhibits a poor
grammatical structure and many out-of-vocabulary
words, a fact that makes the task of clustering tweets
very difficult. There is, however, a clear improve-
ment in most cases in comparison with the baseline.
This indicates that the enriching procedure yields
benefits for the clustering process.
The TEM-Full methodology has demonstrated
good performance with the corpus of generic com-
pany names with 0.70 average (F-measure value) 8
points over the average baseline. In this case, we
have expanded only the ambiguous word (the name
of the company), whereas the TEM-Full+F method-
ologies performed well (0.74 F-measure) with the
corpus of specific company names. We have ob-
served that, regardless of whether or not we are
using an external resource in TEM-Full and TEM-
Full+F approaches, we may improve the representa-
tion of company tweets for the clustering task. It
is important to mention that the good results pre-
sented in companies such as Bestbuy or Lennar
were obtained because the low overlapping vocabu-
lary between the two categories (positive and neg-
ative) and, therefore, the clustering process could
find well-delimited groups. We also would like to
note that sometimes the methodologies have pro-
duced only minor performance improvement. This
we believe is largely due to the length of the tweets,
as it has been demonstrated in other experiments that
better results can be achieved with longer documents
(Perez-Tellez et al, 2009; Pinto et al, 2010).
The best result has been achieved with the TEM-
Full+F methodology which achieved an overall av-
erage F-measure value 0.72, it is 8 points more than
the overall average of the baseline. This methodol-
ogy has not disimproved on the baseline in any in-
stance and it produces good results in most cases.
Although the term expansion procedure has been
shown to be effective for improving the task of clus-
tering company tweets, we believe that there is still
room for improving the obtained F -Measure values
by detecting and filtering stronger relations that may
help in the identification of the positive company
tweets. This fact may lead us to consider that re-
gardless of the resource used (internal or external),
the clustering company tweets is a very difficult task.
6 Conclusions
Clustering short text corpora is a difficult task. Since
tweets are by definition short texts (having a maxi-
mum of 140 characters), the clustering of tweets is
also a challenging problem as stronger results typ-
ically achieved with longer text documents. Fur-
thermore, due to the nature of writing style of these
kinds of texts - typically they exhibits an informal
writing style, with poor grammatical structure and
many out of vocabulary words - this kind of data
typically causes most clustering methods to obtain
poor performance.
The main contribution of this paper has been to
propose and compare two different approaches for
representing tweets on the basis term expansion and
their impact on the problem of clustering company
151
tweets. In particular, we introduced two methodolo-
gies for enriching term representation of tweets. We
expected that these different representations would
lead classical clustering methods, such as K-means,
to obtain a better performance than when clustering
the same data set and the enriching methodology is
not applied.
We consider that TEM-Full performed well on
the former data set and, another methodology ob-
tained the best results on the latter data set TEM-
Full+F. However, the TEM-Full+F methodology
appears suitable for both kinds of corpora, and
does not require any external resource. TEM-Full
and TEM-Full+F are completely unsupervised ap-
proaches which construct a thesaurus from the same
data set to be clustered and, thereafter, uses this re-
source for enriching the terms. On the basis of the
results presented, we can say that using this par-
ticular data, the unsupervised methodology TEM-
Full+F has shown improved results.
This paper has reported on our efforts to ap-
ply clustering and term enrichment to the important
problem of company identification in microblogs.
We expect to do further work in proposing highly
scalable methods that may be able to deal with the
huge amounts of information published every day in
Twitter.
Acknowledgments
This work was carried out in the framework
of the MICINN Text-Enterprise TIN2009-13391-
C04-03 research project and the Microcluster
VLC/Campus (International Campus of Excel-
lence) on Multimodal Intelligent Systems, PROMEP
#103.5/09/4213 and CONACYT #106625, as well
as a grant provided by the Mexican Council of Sci-
ence and Technology (CONACYT).
References
E. Amigo?, J. Artiles, J. Gonzalo, D. Spina, B. Liu, and
A. Corujo. 2010. WePS-3 evaluation campaign:
Overview of the online reputation management task.
In CLEF 2010 (Notebook Papers/LABs/Workshops).
S. Banerjee and T. Pedersen. 2002. An adapted lesk al-
gorithm for word sense disambiguation using wordnet.
In Proc. of the CICLing 2002 Conf., pages 136?145.
LNCS Springer-Verlag.
M. A. Garc??a-Cumbreras, M. Garc??a Vega, F. Mart??nez
Santiago, and J. M. Perea-Ortega. 2010. Sinai at
weps-3: Online reputation management. In CLEF
2010 (Notebook Papers/LABs/Workshops).
G. Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Ac.
P. Kalmar. 2010. Bootstrapping websites for classifica-
tion of organization names on twitter. In CLEF 2010
(Notebook Papers/LABs/Workshops).
J.B. MacQueen. 1967. Some methods for classification
and analysis of multivariate observations. In Proc. of
the 5th Berkeley Symposium on Mathematical Statis-
tics and Probability, pages 281?297. University of
California Press.
S. Milstein, A. Chowdhury, G. Hochmuth, B. Lorica,
and R. Magoulas. 2008. Twitter and the micro-
messaging revolution: Communication, connections,
and immediacy-140 characters at a time. O?Really
Report.
F. Perez-Tellez, D. Pinto, Cardiff J., and P. Rosso. 2009.
Improving the clustering of blogosphere with a self-
term enriching technique. In Proc. of the 12th Int.
Conf. on Text, Speech and Dialogue, pages 40?49.
LNAI.
D. Pinto, P. Rosso, and H. Jimenez. 2010.
A self-enriching methodology for clustering nar-
row domain short texts. The Computer Journal,
doi:10.1093/comjnl/bxq069.
Y. Qiu and H.P. Frei. 1993. Concept based query ex-
pansion. In Proc. of the 16th Annual Int. ACM SIGIR
Conf. on Research and Development in Information
Retrieval, pages 160?169. ACM.
J. Sankaranarayanan, H. Samet, B.E. Teitler, M.D.
Lieberman, and J. Sperling. 2009. Twitterstand: news
in tweets. In Proc. of the 17th ACM SIGSPATIAL Int.
Conf. on Advances in Geographic Information Sys-
tems, pages 42?51. ACM.
B. Sriram, D. Fuhry, E. Demir, and H. Ferhatosmanoglu.
2010. Short text classification in twitter to improve in-
formation filtering. In The 33rd ACM SIGIR?10 Conf.,
pages 42?51. ACM.
M. Tsagkias and K. Balog. 2010. The university of
amsterdam at weps3. In CLEF 2010 (Notebook Pa-
pers/LABs/Workshops).
C.J. van Rijsbergen. 1979. Information Retrieval. But-
terworths, London.
S. R. Yerva, Z. Miklo?s, and K. Aberer. 2010. It was
easy, when apples and blackberries were only fruits.
In CLEF 2010 (Notebook Papers/LABs/Workshops).
M. Yoshida, S. Matsushima, S. Ono, I. Sato, and H. Nak-
agawa. 2010. Itc-ut: Tweet categorization by query
categorization for on-line reputation management. In
CLEF (Notebook Papers/LABs/Workshops).
152
