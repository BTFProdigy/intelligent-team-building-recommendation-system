Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 678?687,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
The infinite HMM for unsupervised PoS tagging
Jurgen Van Gael
Department of Engineering
University of Cambridge
jv249@cam.ac.uk
Andreas Vlachos
Computer Laboratory
University of Cambridge
av308@cl.cam.ac.uk
Zoubin Ghahramani
Department of Engineering
University of Cambridge
zoubin@eng.cam.ac.uk
Abstract
We extend previous work on fully unsu-
pervised part-of-speech tagging. Using
a non-parametric version of the HMM,
called the infinite HMM (iHMM), we ad-
dress the problem of choosing the number
of hidden states in unsupervised Markov
models for PoS tagging. We experi-
ment with two non-parametric priors, the
Dirichlet and Pitman-Yor processes, on the
Wall Street Journal dataset using a paral-
lelized implementation of an iHMM in-
ference algorithm. We evaluate the re-
sults with a variety of clustering evalua-
tion metrics and achieve equivalent or bet-
ter performances than previously reported.
Building on this promising result we eval-
uate the output of the unsupervised PoS
tagger as a direct replacement for the out-
put of a fully supervised PoS tagger for the
task of shallow parsing and compare the
two evaluations.
1 Introduction
Many Natural Language Processing (NLP) tasks
are commonly tackled using supervised learning
approaches. These learning methods rely on the
availability of labeled datasets which are usually
produced by expensive manual annotation. For
some tasks, we have the choice to use unsuper-
vised learning approaches. While they do not nec-
essarily achieve the same level of performance,
they are appealing as unlabeled data is usually
abundant. In particular, for the purpose of ex-
ploring new domains and languages, obtainining
labeled material can be prohibitively expensive
and unsupervised learning methods are a very at-
tractive choice. Recent work (Johnson, 2007;
Goldwater and Griffiths, 2007; Gao and Johnson,
2008) explored the task of part-of-speech tagging
(PoS) using unsupervised Hidden Markov Models
(HMMs) with encouraging results. PoS tagging is
a standard component in many linguistic process-
ing pipelines, so any improvement on its perfor-
mance is likely to impact a wide range of tasks.
It is important to point out that a completely
unsupervised learning method will discover the
statistics of a dataset according to a particular
model choice but these statistics might not cor-
respond exactly to our intuition about PoS tags.
Johnson (2007) and Gao & Johnson (2008) as-
sume that words are generated by a hidden Markov
model and find that the resulting states strongly
correlate with POS tags. Nonetheless, identifying
the HMM states with appropriate POS tags is hard.
Because many evaluation methods often require
POS tags (rather than HMM states) this identifica-
tion problem makes unsupervised systems difficult
to evaluate.
One potential solution is to add a small amount
of supervision as in Goldwater & Griffiths (2007)
who assume a dictionary of frequent words asso-
ciated with possible PoS tags extracted from a la-
beled corpus. Although this technique improves
performance, in this paper we explore the com-
pletely unsupervised approach. The reason for this
is that better unsupervised approaches provide us
with better starting points from which to explore
how and where to incorporate supervision.
In previous work on unsupervised PoS tagging
a main question was how to set the number of hid-
den states appropriately. Johnson (2007) reports
results for different numbers of hidden states but it
is unclear how to make this choice a priori, while
Goldwater & Griffiths (2007) leave this question
as future work.
It is not uncommon in statistical machine learn-
ing to distinguish between parameters of a model
and the capacity of a model. E.g. in a clustering
context, the choice for the number of clusters (ca-
pacity) and the parameters of each cluster are often
678
treated differently: the latter are estimated using
algorithms like EM, MCMC or Variational Bayes
while the former is chosen using common sense,
heuristics or in a Bayesian framework maybe us-
ing evidence maximization.
Non-parametric Bayesian methods are a class of
probability distributions which explicitly treat the
capacity of a model as ?just another parameter?.
Potential advantages are
? the model capacity can automatically adjust
to the amount of data: e.g. when clustering
a very small dataset, it is unlikely that many
fine grained clusters can be distinguished,
? inference can be more efficient: e.g. instead
of running full inference for different model
capacities and then choosing the best ca-
pacity (according to some choice of ?best?),
inference in non-parametric Bayesian meth-
ods integrates the capacity search in one al-
gorithm. This is particularly advantageous
when parameters other than capacity need to
be explored, since it reduces signifcantly the
number of experiments needed.
None of these potential advantages are guaranteed
and in this paper we investigate these two aspects
for the task of unsupervised PoS tagging.
The contributions in this paper extend previous
work on unsupervised PoS tagging in five ways.
First, we introduce the use of a non-parametric
version of the HMM, namely the infinite HMM
(iHMM) (Beal et al, 2002) for unsupervised PoS
tagging. This answers an open problem from
Goldwater & Griffiths (2007). Second, we care-
fully implemented a parallelized version of the
inference algorithms for the iHMM so we could
use it on the Wall Street Journal Penn Treebank
dataset. Third, we introduce a new variant of
the iHMM that builds on the Pitman-Yor process.
Fourth, we evaluate the results with a variety of
clustering evaluation methods and achieve equiv-
alent or better performances than previously re-
ported. Finally, building on this promising result
we use the output of the unsupervised PoS tagger
as a direct replacement for the output of a fully su-
pervised PoS tagger for the task of shallow pars-
ing. This evaluation enables us to assess the appli-
cability of an unsupervised PoS tagging method
and provides us with means of comparing its per-
formance against a supervised PoS tagger.
The rest of the paper is structured as follows:
in section 2 we introduce the iHMM as a non-
parametric version of the Bayesian HMM used
in previous work on unsupervised PoS tagging.
Then, in section 3 we describe some details of
our implementation of the iHMM. In section 4 we
present a variety of evaluation metrics to compare
our results with previous work. Finally, in sec-
tion 5 we report our experimental results. We con-
clude this paper with a discussion of ongoing work
and experiments.
2 The Infinite HMM
In this section, we describe a non-parametric hid-
den Markov model known as the infinite HMM
(iHMM) (Beal et al, 2002; Teh et al, 2006). As
we show below, this model is flexible in the num-
ber of hidden states which it can accomodate. In
other words, the capacity is an uncertain quantity
with an a priori infinite range that is a posteriori
inferred by the data. It is instructive to first re-
view the finite HMM and its Bayesian treatment:
for one, it is the model that has been used in previ-
ous work on unsupervised PoS tagging, secondly
it allows us to better understand the iHMM.
The Bayesian HMM A finite first-order HMM
consists of a hidden state sequence s =
(s
1
, s
2
, . . . , s
T
) and a corresponding observation
sequence y = (y
1
, y
2
, . . . , y
T
). Each state vari-
able s
t
can take on a finite number of states, say
1 . . .K. Transitions between states are governed
by Markov dynamics parameterized by the tran-
sition matrix pi, where pi
ij
= p(s
t
= j|s
t?1
=
i), while the initial state probabilities are pi
0i
=
p(s
1
= i). For each state s
t
? {1 . . .K} there
is a parameter ?
s
t
which parameterizes the obser-
vation likelihood for that state: y
t
|s
t
? F (?
s
t
).
Given the parameters {pi
0
,pi,?,K} of the HMM,
the joint distribution over hidden states s and ob-
servations y can be written (with s
0
= 0):
p(s,y|pi
0
,pi,?,K) =
T
?
t=1
p(s
t
|s
t?1
)p(y
t
|s
t
)
As Johnson (2007) clearly explained, training the
HMM with EM leads to poor results in PoS tag-
ging. However, we can easily treat the HMM in a
fully Bayesian way (MacKay, 1997) by introduc-
ing priors on the parameters of the HMM. With
no further prior knowledge, a typical prior for the
transition (and initial) probabilities are symmet-
ric Dirichlet distributions. This corresponds to our
679
belief that, a priori, each state is equally likely to
transition to every other state. Also, it is com-
monly known that the parameter of a Dirichlet
distribution controls how sparse its samples are.
In other words, by making the hyperprior on the
Dirichlet distribution for the rows of the transi-
tion matrix small, we can encode our belief that
any state (corresponding to a PoS tag in this ap-
plication context) will only be followed by a small
number of other states. As we explain below, we
will be able to include this desirable property in
the non-parametric model as well. Secondly, we
need to introduce a prior on the observation pa-
rameters ?
k
. Without any further prior knowl-
edge, a convenient choice here is another sym-
metric Dirichlet distribution with sparsity induc-
ing hyperprior. This encodes our belief that only
a subset of the words correspond to a particular
state.
The Infinite HMM A first na??ve way to obtain
a non-parametric HMM with an infinite number
of states might be to use symmetric Dirichlet pri-
ors over the transition probabilities with parameter
?/K and take K ? ?. This approach unfortu-
nately does not work: ?/K ? 0 when K ? ?
and hence the rows of the matrix will become ?in-
finitely sparse?. Since the sum of the entries must
sum to one, the rows of the transition matrix will
be zero everywhere and all its mass in a random
location. Unfortunately, this random location is
out of an infinite number of possible locations and
hence with probability 1 will be different for all
the rows. As a consequence, at each timestep the
HMM moves to a new state and will never revisit
old states. As we shall see shortly, we can fix this
by using a hierarchical Bayesian formalism where
the Dirichlet priors on the rows have a shared pa-
rameter.
Before moving on to the iHMM, let us look at
the finite HMM from a different perspective. The
finite HMM of length T with K hidden states can
be seen as a sequence of T finite mixture models.
The following equation illustrates this idea: con-
ditioned on the previous state s
t?1
, the marginal
probability of observation y
t
can be written as:
p(y
t
|s
t?1
= k) =
K
?
s
t
=1
p(s
t
|s
t?1
= k)p(y
t
|s
t
),
=
K
?
s
t
=1
pi
k,s
t
p(y
t
|?
s
t
). (1)
The variable s
t?1
= k specifies the mixing
weights pi
k,?
for the mixture distribution, while s
t
indexes the mixture component generating the ob-
servation y
t
. In other words, equation (1) says that
each row of the transition matrix pi specifies a dif-
ferent mixture distribution over the same set of K
mixture components ?.
Our second attempt to define a non-parametric
version of the hidden Markov model is to replace
the finite mixture by an infinite mixture. The
theory of Dirichlet process mixtures (Antoniak,
1974) tells us exactly how to do this. A draw
G ? DP (?,H) from a Dirichlet process (DP)
with base measure H and concentration parame-
ter ? ? 0 is a discrete distribution which can be
written as an infinite mixture of atoms
G(?) =
?
?
i=1
pi
i
?
?
i
(?)
where the ?
i
are i.i.d. draws from the base mea-
sure H , ?
?
i
(?) represents a point distribution at
?
i
and pi
i
= v
i
?
i?1
l=1
(1 ? v
l
) where each v
l
?
Beta(1, ?). The distribution over pi
i
is called a
stick breaking construction and is essentially an
infinite dimensional version of the Dirichlet dis-
tribution. We refer to Teh et al (2006) for more
details.
Switching back to the iHMM our next step is to
introduce a DP G
j
for each state j ? {1 ? ? ??};
we write G
j
(?) =
?
?
i=1
pi
j
i
?
?
j
i
(?). There is now
a parameter for each state j and each index i ?
{1, 2, ? ? ? ,?}. Next, we draw the datapoint at
timestep t given that the previous datapoint was in
state s
t?1
by drawing from DP G
s
t?1
. We first se-
lect a mixture component s
t
from the vector pi
s
t?1
,?
and then sample a datapoint y
t
? F (?
s
t?1
,s
t
) so
we get the following distribution for y
t
p(y
t
|?, s
t?1
) =
?
?
s
t
=1
pi
s
t?1
,s
t
p(y
t
|?
s
t?1
,s
t
).
This is almost the non-parametric equivalent of
equation (1) but there is a subtle difference: each
G
j
selects their own set of parameters ?
j
?
. This
is unfortunate as it means that the output distribu-
tion would not be the same for each state, it would
depend on which state we were moving to! Luck-
ily, we can easily fix this: by introducing an in-
termediate distribution G
0
? DP (?,H) and let
G
j
? DP (?,G
0
) we enforce that the i.i.d. draws
?
j
?
are draws from a discrete distribution (sinceG
0
680
is a draw from a Dirichlet process) and hence all
G
j
will share the same infinite set of atoms as cho-
sen byG
0
. Figure 1 illustrates the graphical model
for the iHMM.
The iHMM with Pitman-Yor Prior The
Dirichlet process described above defines a very
specific distribution over the number of states
in the iHMM. One particular generalization of
the Dirichlet process that has been studied in the
NLP literature before is the Pitman-Yor process.
Goldwater et al (2006) have shown that the
Pitman-Yor distribution can more accurately cap-
ture power-law like distributions that frequently
occur in natural language.
More specifically, a draw G ? PY (d, ?,H)
from a Pitman-Yor process (PY) with base mea-
sure H , discount parameter 0 ? d < 1 and con-
centration parameter ? > ?d is a discrete distri-
bution which can be written as an infinite mixture
of atoms
G(?) =
?
?
i=1
pi
i
?
?
i
(?)
where the ?
i
are i.i.d. draws from the base mea-
sure H , ?
?
i
(?) represents a point distribution at
?
i
and pi
i
= v
i
?
i?1
l=1
(1 ? v
l
) where each v
l
?
Beta(1?d, ?+ ld). Note the similarity to the DP:
in fact, the DP is a special case of PY with d = 0.
In our experiments, we constructed an iHMM
where the DP (?,H) base measure G
0
is re-
placed with its two parameter generalization
PY (d, ?,H). Because the Dirichlet and Pitman-
Yor processes only differ in the way pi is con-
structed, without loss of generality we will de-
scribe hyper-parameter choice and inference in the
context of the iHMM with Dirichlet process base
measure.
Hyperparameter Choice The description
above shows that there are 4 parameters which
we must specify: the base measure H , the
output distribution p(y
t
|?
s
t
), the discount
1
and
concentration
2
parameters d, ? for G
0
and the
concentration parameter ? for the DP?sG
j
. Just as
in the finite case, the base measure H is the prior
distribution on the parameter ? of p(y
t
|?
s
t
). We
chose to use a symmetric Dirichlet distribution
with parameter ? over the word types in our
corpus. Since we do not know the sparsity level ?
of the output distributions we decided to learn this
1
for Pitman-Yor base measure
2
for both Dirichlet and Pitman-Yor base measures
parameter from the data. We initially set a vague
Gamma prior over ? but soon realized that as we
expect hidden states in the iHMM to correspond
to PoS tags, it is unrealistic to expect each state
to have the same sparsity level. Hence we chose
a Dirichlet process as the prior for ?; this way
we end up with a small discrete set of sparsity
levels: e.g. we can learn that states corresponding
to verbs and nouns share one sparsity level
while states correpsonding to determiners have
their own (much sparser) sparsity level. For the
output distribution p(y
t
|?
s
t
) we chose a simple
multinomial distribution.
The hyperparameters d and ? mostly control the
number of states in the iHMM while - as we dis-
cussed above - ? controls the sparsity of the tran-
sition matrix. In the experiments below we report
both fixing the two parameters and learning them
by sampling (using vague Gamma hyperpriors).
Because of computational constraints, we chose to
use vague Bayesian priors for all hyperparameters
rather than run the whole experiment over a grid of
?reasonable? parameter settings and use the best
ones according to cross validation.
3 Inference
The Wall Street Journal part of the Penn Tree-
bank that was used for our experiments contains
about one million words. In the non-parametric
Bayesian literature not many algorithms have been
described that scale into this regime. In this sec-
tion we describe our parallel implementation of
the iHMM which can easily handle a dataset of
this scale.
There is a wealth of evidence (Scott, 2002; Gao
and Johnson, 2008) in the machine learning litera-
ture that Gibbs sampling for Markov models leads
to slow mixing times. Hence we decided our start-
ing point for inference needs to be based on dy-
namic programming. Because we didn?t have a
good idea for the number of states that we were go-
ing to end up with, we prefered the beam sampler
of Van Gael et al (2008) over a finite truncation
of the iHMM. Moreover, the beam sampler also
introduces a certain amount of sparsity in the dy-
namic program which can speed up computations
(potentially at the cost of slower mixing).
The beam sampler is a blocked Gibbs sampler
where we alternate between sampling the param-
eters (transition matrix, output parameters), the
state sequence and the hyperparameters. Sam-
681
k = 1 ? ? ?1
s0s1s2y1y2?k?k???H
Figure 1: The graphical model for the iHMM. The variable ? represents the mixture for the DP G
0
.
pling the transition matrix and output distribu-
tion parameters requires computing their sufficient
statistics and sampling from a Dirichlet distribu-
tion; we refer to the beam sampling paper for de-
tails. For the hyperparameters we use standard
Gibbs sampling. We briefly sketch the resam-
pling step for the state sequence for a single se-
quence of data (sentence of words). Running stan-
dard dynamic programming is prohibitive because
the state space of the iHMM is infinitely large.
The central idea of the beam sampler is to adap-
tively truncate the state space of the iHMM and
run dynamic programming. In order to truncate
the state space, we sample an auxilary variable u
t
for each word in the sequence from the distribu-
tion u
t
? Uniform(0, pi
s
t?1
s
t
) where pi represents
the transition matrix.
Intuitively, when we sample u
1:T
|s
1:T
accord-
ing to the distribution above, the only valid sam-
ples are those for which the u
t
are smaller than
the transition probabilities of the state sequence
s
1:T
. This means that when we sample s
1:T
|u
1:T
at a later point, it must be the case that the u
t
?s
are still smaller than the new transition probabil-
ities. This significantly reduces the set of valid
state sequences that we need to consider. More
specifically, Van Gael et al (2008) show that we
can compute p(s
t
|y
1:t
, u
1:t
) using the following
dynamic programming recursion p(s
t
|y
1:t
, u
1:t
) =
p(y
t
|s
t
)
?
s
t?1
:u
t
<pi
s
t?1
,s
t
p(s
t?1
|y
1:t?1
, u
1:t?1
).
The summation
?
s
t?1
:u
t
<pi
s
t?1
,s
t
ensures that this
computation remains finite. When we compute
p(s
t
|y
1:t
, u
1:t
) for t ? {1 ? ? ?T}, we can easily
sample s
T
and using Bayes rule backtrack sample
every other s
t
. It can be shown that this procedure
produces samples from the exact posterior.
Notice that the dynamic program only needs to
perform computation when u
t
< pi
s
t?1
,s
t
. A care-
ful implementation of the beam sampler consists
of preprocessing the transition matrix pi and sort-
ing its elements in descending order. We can then
iterate over the elements of the transition matrix
starting from the largest element and stop once
we reach the first element of the transition matrix
smaller than u
t
. In our experiments we found that
this optimization reduces the amount of computa-
tion per sentence by an order of magnitutde.
A second optimization which we introduced
is to use the map-reduce paradigm (Dean and
Ghemawat, 2004) to parallelize our computations.
More specifically, after we preprocess the transi-
tion matrix, the dynamic program computations
are independent for each sentence in the dataset.
This means we can perform each dynamic pro-
gram in parallel; in other words our ?map? con-
sists of running the dynamic program on one sen-
tence in the dataset. Next, we need to resample
the transition matrix and output distribution pa-
rameters. In order to do so we need to compute
their sufficient statistics: the number of transitions
from state to state and the number of emissions of
each word out of each state. Our ?reduce? func-
tion consists of computing the sufficient statistics
for each sentence and then aggregating the statis-
tics for the whole dataset. Our implementation
runs on a quad-core shared memory architecture
and we find an almost linear speedup going from
one to four cores.
4 Evaluation
Evaluating unsupervised PoS tagging is rather dif-
ficult mainly due to the fact that the output of such
682
systems are not actual PoS tags but state identi-
fiers. Therefore it is impossible to evaluate per-
formance against a manually annotated gold stan-
dard using accuracy. Recent work (Goldwater and
Griffiths, 2007; Johnson, 2007; Gao and Johnson,
2008) on this task explored a variety of method-
ologies to address this issue.
The most common approach followed in pre-
vious work is to evaluate unsupervised PoS tag-
ging as clustering against a gold standard using
the Variation of Information (VI) (Meil?a, 2007).
VI assesses homogeneity and completeness us-
ing the quantities H(C|K) (the conditional en-
tropy of the class distribution in the gold stan-
dard given the clustering) and H(K|C) (the con-
ditional entropy of clustering given the class dis-
tribution in the gold standard). However, as Gao
& Johnson (2008) point out, VI is biased to-
wards clusterings with a small number of clus-
ters. A different evaluation measure that uses
the same quantities but weighs them differently is
the V-measure (Rosenberg and Hirschberg, 2007),
which is defined in Equation 2 by setting the pa-
rameter ? to 1.
h = 1?
H(C|K)
H(C)
c = 1?
H(K|C)
H(K)
V
?
=
(1 + ?)hc
(?h) + c
(2)
Vlachos et al (2009) noted that V-measure favors
clusterings with a large number of clusters. Both
of these biases become crucial in our experiments,
since the number of clusters (states of the iHMM)
is not fixed in advance. Vlachos et al proposed a
variation of the V-measure, V-beta, that adjusts the
balance between homogeneity and completeness
using the parameter ? in Eq. 2.
It is worth mentioning that, unlike V-measure
and V-beta, VI scores are not normalized
and therefore they are difficult to interpret.
Meil?a (2007) presented two normalizations,
acknowledging the potential disadvantages
they have. The first one normalizes VI by
2 log(max(|K|, |C|)), which is inappropriate
when the number of clusters discovered |K|
changes between experiments. The second
normalization involves the quantity logN which
is appropriate when comparing different algo-
rithms on the same dataset (N is the number
of instances). However, this quantity depends
exclusively on the size of the dataset and hence if
the dataset is very large it can result in normalized
VI scores misleadingly close to 100%. This does
not affect rankings, i.e. a better VI score will also
be translated into a better normalized VI score. In
our experiments, we report results only with the
un-normalized VI scores, V-measure and V-beta.
All the evaluation measures mentioned so far
evaluate PoS tagging as a clustering task against
a manually annotated gold standard. While this
is reasonable, it still does not provide means of
assessing the performance in a way that would
allow comparisons with supervised methods that
output actual PoS tags. Even for the normalized
measures V-measure and V-beta, it is unclear how
their values relate to accuracy levels. Gao & John-
son (2008) partially addressed this issue by map-
ping states to PoS tags following two different
strategies, cross-validation accuracy, and greedy
1-to-1 mapping, which both have shortcomings.
We argue that since an unsupervised PoS tagger is
trained without taking any gold standard into ac-
count, it is not appropriate to evaluate against a
particular gold standard, or at least this should not
be the sole criterion. The fact that different authors
use different versions of the same gold standard to
evaluate similar experiments (e.g. Goldwater &
Griffiths (2007) versus Johnson (2007)) supports
this claim. Furthermore, PoS tagging is seldomly
a goal in itself, but it is a component in a linguistic
pipeline.
In order to address these issues, we perform an
extrinsic evaluation using a well-explored task that
involves PoS tags. While PoS tagging is consid-
ered a pre-processing step in many natural lan-
guage processing pipelines, the choice of task is
restricted by the lack of real PoS tags in the out-
put of our system. For our purposes we need a
task that relies on discriminating between PoS tags
rather than the PoS tag semantics themselves, in
other words, a task in which knowing whether a
word is tagged as noun instead of a verb is equiv-
alent to knowing it is tagged as state 1 instead of
state 2. Taking these considerations into account,
in Section 5 we experiment with shallow pars-
ing in the context of the CoNLL-2000 shared task
(Tjong Kim Sang and Buchholz, 2000) in which
very good performances were achieved using only
the words with their PoS tags. Our intuition is that
if the iHMM (or any unsupervised PoS tagging
683
method) has a reasonable level of performance, it
should improve on the performance of a system
that does not use PoS tags. Moreover, if the per-
formance is very good indeed, it should get close
to the performance of a system that uses real PoS
tags, provided either by human annotation or by a
good supervised system. Similar extrinsic evalu-
ation was performed by Biemann et al (2007). It
is of interest to compare the results between the
clustering evaluation and the extrinsic one.
A different approach in evaluating non-
parametric Bayesian models for NLP is state-
splitting (Finkel et al, 2007; Liang et al, 2007).
In this setting, the model is used in order to re-
fine existing annotation of the dataset. While this
approach can provide us with some insights and
interpretable results, the use of existing annotation
influences the output of the model. In this work,
we want to verify whether the output of the iHMM
(without any supervision) can be used instead of
that of a supervised system.
5 Experiments
In all our experiments, the Wall Street Journal
(WSJ) part of the Penn Treebank was used. As ex-
plained in Section 4, we evaluate the output of the
iHMM in two ways, as clustering with respect to a
gold standard and as direct replacement of the PoS
tags in the task of shallow parsing. In each experi-
ment, we obtain a sample from the iHMM over all
the sections of WSJ. The states for sections 15-18
and 20 of the WSJ (training and testing sets re-
spectvely in the CoNLL shared task) are used for
the evaluation based on shallow parsing, while the
remaining sections are used for evaluation against
the WSJ gold standard PoS tags using clustering
evaluation measures.
As described in Section 2 we performed three
runs with the iHMM: one run with DP prior and
fixed ?, ?, one with PY prior and fixed d, ?, ? and
one with DP prior but where we learn the hyper-
parameters ?, ? from the data. Our inference algo-
rithm uses 1000 burn-in iterations after which we
collect a sample every 1000 iterations. Our infer-
ence procedure is annealed during the first 1000
burnin and 2400 iterations by powering the likeli-
hood of the output distribution with a number that
smoothly increases from 0.4 to 1.0 over the 3400
first iterations. The numbers of iterations reported
in the remainder of the section refer to the itera-
tions after burn-in. We initialized the sampler by:
a) sampling the hyperparameters from the prior
where applicable, b) uniformly assign each word
one out of 20 iHMM states. For the DP run with
fixed parameters, we chose ? = 0.8 to encourage
some sparsity in the transition matrix and ? = 5.0
to allow for enough hidden states. For the PY run
with fixed parameters, we chose ? = 0.8 for simi-
lar reasons and d = 0.1 and ? = 1.0. We point out
that one weakness of MCMC methods is that they
are hard to test for convergence. We chose to run
the simulations until they became prohibitively ex-
pensive to obtain a new sample.
First, we present results using clustering eval-
uation measures which appear in the figures of
Table 1. The three runs exhibit different behav-
ior. The number of states reached by the iHMM
with fixed parameters using the DP prior stabilizes
close to 50 states, while for the experiment with
learnt hyperparameters the number of states grows
more rapidly, reaching 194 states after 8,000 iter-
ations. With the PY prior, the number of states
reached grows less rapidly reaching 90 states. All
runs achieve better performances with respect to
all the measures used as the number of iterations
grows. An exception is that VI scores tend to in-
crease (lower VI scores are better) when the num-
ber of states grows larger than the gold standard.
It is interesting to notice how the measures exhibit
different biases, in particular that VI penalizes the
larger numbers of states discovered in the DP run
with learnt parameters as well as the run with the
PY prior, compared to the more lenient scores pro-
vided by V-measure and V-beta. The latter though
assigns lower scores to the DP run with learnt pa-
rameters because it takes into account that the high
homogeneity is achieved using even more states.
Finally, the interpretability of these scores presents
some interest. For example, in the run with fixed
parameters using the DP prior, after burn-in VI
was 4.6, which corresponds to 76.65% normalized
VI score, while V-measure and V-beta were 12.7%
and 9% respectively. In 8,000 iterations after burn-
in, VI was 3.94 (80.3% when normalized), while
V-measure and V-beta were 53.3%, since the num-
ber of states was almost the same as the number of
unique PoS tags in the gold standard.
The closest experiment to ours is the one by
Gao & Johnson (2008) who run their Bayesian
HMM over the whole WSJ and evaluated against
the full gold standard, the only difference being
is that we exclude the CoNLL shared task sec-
684
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 200
 0  1  2  3  4  5  6  7  8
s
ta
te
s
DP-learnt
DP-fixed
PY-fixed
 0
 10
 20
 30
 40
 50
 60
 70
 0  1  2  3  4  5  6  7  8
h
o
m
o
ge
ne
ity
DP-learnt
DP-fixed
PY-fixed
 25
 30
 35
 40
 45
 50
 55
 60
 0  1  2  3  4  5  6  7  8
c
o
m
pl
et
en
es
s
DP-learnt
DP-fixed
PY-fixed
 3.6
 3.8
 4
 4.2
 4.4
 4.6
 4.8
 5
 5.2
 0  1  2  3  4  5  6  7  8
V
I
DP-learnt
DP-fixed
PY-fixed
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 0  1  2  3  4  5  6  7  8
V
-m
e
a
su
re
DP-learnt
DP-fixed
PY-fixed
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 0  1  2  3  4  5  6  7  8
V
-b
e
ta
DP-learnt
DP-fixed
PY-fixed
Table 1: Performance of the three iHMM runs according to clustering evaluation measures against num-
ber of iteretions (in thousands).
 93.2
 93.4
 93.6
 93.8
 94
 94.2
 94.4
 94.6
 0  1  2  3  4  5  6  7  8
a
c
c
u
ra
c
y
DP-learnt
DP-fixed
PY-fixed
 88.5
 89
 89.5
 90
 90.5
 91
 0  1  2  3  4  5  6  7  8
F
-s
co
re
DP-learnt
DP-fixed
PY-fixed
Table 2: Performance of the output of the three iHMM runs when used in shallow parsing against number
of iteretions (in thousands).
tions from our evaluation, which leaves us with 19
sections instead of 24. Their best VI score was
4.03886 which they achieved using the collapsed,
sentence-blocked Gibbs sampler with the number
of states fixed to 50. The VI score achieved by the
iHMM with fixed parameters using the PY prior
reaches 3.73, while using the DP prior VI reaches
4.32 with learnt parameters and 3.93 with fixed
685
parameters. These results, even if they are not
directly comparable, are on par with the state-of-
the-art, which encouraged us to proceed with the
extrinsic evaluation.
For the experiments with shallow parsing we
used the CRF++ toolkit
3
which has an efficient
implementation of the model introduced by Sha &
Pereira (2003) for this task. First we ran an experi-
ment using the words and the PoS tags provided in
the shared task data and the performances obtained
were 96.07% accuracy and 93.81% F-measure.
The PoS tags were produced using the Brill tag-
ger (Brill, 1994) which employs tranformation-
based learning and was trained using the WSJ cor-
pus. Then we ran an experiment removing the
PoS tags altogether, and the performances were
93.25% accuracy and 88.58% F-measure respec-
tively. This gave us some indication as to what the
contribution of the PoS tags is in the context of the
shallow parsing task at hand.
The experiments using the output of the iHMM
as PoS tags for shallow parsing are presented in
Table 2. The best performance achieved was
94.48% and 90.98% in accuracy and F-measure,
which is 1.23% and 2.4% better respectively than
just using words, but worse by 1.57% and 2.83%
compared to using the supervised PoS tagger out-
put. Given that the latter is trained on WSJ we be-
lieve that this is a good result. Interestingly, this
was obtained by using the last sample from the
iHMM run using the DP prior with learnt param-
eters which has worse overall clustering evalua-
tion scores, especially in terms of VI. This sample
though has the best homogeneity score (69.39%).
We believe that homogeneity is more important
than the overall clustering score due to the fact
that, in the application considered, it is probably
worse to assign tokens that belong to different PoS
tags to the same state, e.g. verb and adverbs, rather
than generate more than one state for the same
PoS. This is likely to be the case in tasks where
we are interested in distinguishing between PoS
tags rather than the actual tag itself. Also, clus-
tering evaluation measures tend to score leniently
consistent mixing of members of different classes
in the same cluster. However, such mixing results
in consistent noise when the clustering output be-
comes input to a machine learning method, which
is harder to deal with.
3
http://crfpp.sourceforge.net/
6 Conclusions - Future Work
In the context of shallow parsing we saw that the
performance of the iHMM does not match the
performance of a supervised PoS tagger but does
lead to a performance increase over a model us-
ing only words as features. Given that it was con-
structed without any need for human annotation,
we believe this is a good result. At the same time
though, it suggests that it is still some way from
being a direct drop-in replacement for a supervised
method. We argue that the extrinsic evaluation of
unsupervised PoS tagging performed in this paper
is quite informative as it allowed us to assess our
results in a more realistic context. In this work we
used shallow parsing for this, but we are consider-
ing other tasks in which we hope that PoS tagging
performance will be more crucial.
Our experiments also suggest that the number of
states in a Bayesian non-parametric model can be
rather unpredictable. On one hand, this is a strong
warning towards inference algorithms which per-
form finite truncation of non-parametric models.
On the other hand, the remarkable difference in
behavior between the DP with fixed and learned
priors suggests that more research is needed to-
wards understanding the influence of hyperparam-
eters in Bayesian non-parametric models.
We are currently experimenting with a semi-
supervised PoS tagger where we let the transi-
tion matrix of the iHMM depend on annotated
PoS tags. This model allows us to: a) use an-
notations whenever they are available and do un-
supervised learning otherwise; b) use the power
of non-parametric methods to possibly learn more
fine grained statistical structure than tag sets cre-
ated manually.
On the implementation side, it would be in-
teresting to see how our methods scale in a dis-
tributed map-reduce architecture where network
communication overhead becomes an issue.
Finally, the ultimate goal of our investigation is
to do unsupervised PoS tagging using web-scale
datasets. Although the WSJ corpus is reasonably
sized, our computational methods do not currently
scale to problems with one or two order of magni-
tude more data. We will need new breakthroughs
to unleash the full potential of unsupervised learn-
ing for NLP.
686
References
Charles E. Antoniak. 1974. Mixtures of dirichlet pro-
cesses with applications to bayesian nonparametric
problems. The Annals of Statistics, 2(6):1152?1174.
M. J. Beal, Z. Ghahramani, and C. E. Rasmussen.
2002. The infinite hidden markov model. Advances
in Neural Information Processing Systems, 14:577 ?
584.
Chris Biemann, Claudio Giuliano, and Alfio Gliozzo.
2007. Unsupervised part-of-speech tagging support-
ing supervised methods. In Proceedings of RANLP.
Eric Brill. 1994. Some advances in transformation-
based part of speech tagging. In National Confer-
ence on Artificial Intelligence, pages 722?727.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapre-
duce: Simplified data processing on large clusters.
In Sixth Symposium on Operating System Design
and Implementation.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2007. The infinite tree. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 272?279,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
J. Van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahra-
mani. 2008. Beam sampling for the infinite hidden
markov model. In Proceedings of the 25th interna-
tional conference on Machine learning, volume 25,
Helsinki.
Jianfeng Gao and Mark Johnson. 2008. A compari-
son of bayesian estimators for unsupervised hidden
markov model pos taggers. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 344?352.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 744?751, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
S. Goldwater, T. Griffiths, and M. Johnson. 2006. In-
terpolating between types and tokens by estimating
power-law generators. Advances in Neural Informa-
tion Processing Systems, 18.
Mark Johnson. 2007. Why Doesn?t EM Find Good
HMM POS-Taggers? In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 296?305.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007.
The infinite PCFG using hierarchical Dirichlet pro-
cesses. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
D. J. C. MacKay. 1997. Ensemble learning for hidden
Markov models. Technical report, Cavendish Labo-
ratory, University of Cambridge, 1997.
Marina Meil?a. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 410?420, Prague, Czech
Republic, June.
Steven L. Scott. 2002. Bayesian methods for hidden
markov models: Recursive computing in the 21st
century. Journal of the American Statistical Asso-
ciation, 97(457):337?351, March.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Human Lan-
guage Technology Conference and the 4th Meeting
of the North American Association for Computa-
tional Linguistics.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and D. M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Claire Cardie, Walter Daelemans,
Claire Nedellec, and Erik Tjong Kim Sang, editors,
Proceedings of the Fourth Conference on Computa-
tional Natural Language Learning, pages 127?132.
Lisbon, Portugal, September.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and Constrained
Dirichlet Process Mixture Models for Verb Cluster-
ing. In Proceedings of the EACL workshop on GEo-
metrical Models of Natural Language Semantics.
687
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 74?82,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Unsupervised and Constrained Dirichlet Process Mixture Models for Verb
Clustering
Andreas Vlachos
Computer Laboratory
University of Cambridge
Cambridge CB3 0FD, UK
av308l@cl.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
Cambridge CB3 0FD, UK
alk23@cl.cam.ac.uk
Zoubin Ghahramani
Department of Engineering
University of Cambridge
Cambridge CB2 1PZ, UK
zoubin@eng.cam.ac.uk
Abstract
In this work, we apply Dirichlet Process
Mixture Models (DPMMs) to a learning
task in natural language processing (NLP):
lexical-semantic verb clustering. We thor-
oughly evaluate a method of guiding DP-
MMs towards a particular clustering so-
lution using pairwise constraints. The
quantitative and qualitative evaluation per-
formed highlights the benefits of both
standard and constrained DPMMs com-
pared to previously used approaches. In
addition, it sheds light on the use of evalu-
ation measures and their practical applica-
tion.
1 Introduction
Bayesian non-parametric models have received a
lot of attention in the machine learning commu-
nity. These models have the attractive property
that the number of components used to model
the data is not fixed in advance but is actually
determined by the model and the data. This
property is particularly interesting for NLP where
many tasks are aimed at discovering novel, pre-
viously unknown information in corpora. Recent
work has applied Bayesian non-parametric mod-
els to anaphora resolution (Haghighi and Klein,
2007), lexical acquisition (Goldwater, 2007) and
language modeling (Teh, 2006) with good results.
Recently, Vlachos et al (2008) applied the ba-
sic models of this class, Dirichlet Process Mix-
ture Models (DPMMs) (Neal, 2000), to a typical
learning task in NLP: lexical-semantic verb clus-
tering. The task involves discovering classes of
verbs similar in terms of their syntactic-semantic
properties (e.g. MOTION class for travel, walk,
run, etc.). Such classes can provide important
support for other NLP tasks, such as word sense
disambiguation, parsing and semantic role label-
ing (Dang, 2004; Swier and Stevenson, 2004).
Although some fixed classifications are available
(e.g. VerbNet (Kipper-Schuler, 2005)) these are
not comprehensive and are inadequate for specific
domains (Korhonen et al, 2006b).
Unlike the clustering algorithms applied to this
task before, DPMMs do not require the number of
clusters as input. This is important because even
if the number of classes in a particular task was
known (e.g. in the context of a carefully controlled
experiment), a particular dataset may not contain
instances for all the classes. Moreover, each class
is not necessarily contained in one cluster exclu-
sively, since the target classes are defined manu-
ally without taking into account the feature rep-
resentation used. The fact that DPMMs do not
require the number of target clusters in advance,
renders them promising for the many NLP tasks
where clustering is used for learning purposes.
While the results of Vlachos et al (2008) are
promising, the use of a clustering approach which
discovers the number of clusters in data presents
a new challenge to existing evaluation measures.
In this work, we investigate optimal evaluation
for such approaches, using the dataset and the ba-
sic method of Vlachos et al as a starting point.
We review the applicability of existing evalua-
tion measures and propose a modified version of
the newly introduced V-measure (Rosenberg and
Hirschberg, 2007). We complement the quanti-
tative evaluation with thorough qualitative assess-
ment, for which we introduce a method to summa-
rize samples obtained from a clustering algorithm.
In preliminary work by Vlachos et al (2008),
a constrained version of DPMMs which takes ad-
vantage of must-link and cannot-link pairwise con-
straints was introduced. It was demonstrated how
such constraines can guide the clustering solution
towards some prior intuition or considerations rel-
evant to the specific NLP application in mind. We
explain the inference algorithm for the constrained
DPMM in greater detail and evaluate quantita-
74
tively the contribution of each constraint type of
independently, complementing it with qualitative
analysis. The latter demonstrates how the pairwise
constraints added affects instances beyond those
involved directly. Finally, we discuss how the un-
supervised and the constrained version of DPMMs
can be used in a real-world setup.
The results from our comprehensive evaluation
show that both versions of DPMMs are capable
of learning novel information not in the gold stan-
dard, and that the constrained version is more ac-
curate than a previous verb clustering approach
which requires setting the number of clusters in
advance and is therefore less realistic.
2 Unsupervised clustering with DPMMs
With DPMMs, as with other Bayesian non-
parametric models, the number of mixture compo-
nents is not fixed in advance, but is determined by
the model and the data. The parameters of each
component are generated by a Dirichlet Process
(DP) which can be seen as a distribution over the
parameters of other distributions. In turn, each in-
stance is generated by the chosen component given
the parameters defined in the previous step:
G|?,G0 ? DP (?,G0)
?i|G ? G (1)
xi|?i ? F (?i)
In Eq. 1, G0 and G are probability distributions
over the component parameters (?), and ? > 0 is
the concentration parameter which determines the
variance of the Dirichlet process. We can think
of G as a randomly drawn probability distribution
with meanG0. Intuitively, the larger ? is, the more
similar G will be to G0. Instance xi is generated
by distribution F , parameterized by ?i. The graph-
ical model is depicted in Figure 1.
The prior probability of assigning an instance
to a particular component is proportionate to the
number of instances already assigned to it (n?i,z).
In other words, DPMMs exhibit the ?rich get
richer? property. In addition, the probability that
a new cluster is created is dependent on the con-
centration parameter ?. A popular metaphor to de-
scribe DPMMs which exhibits an equivalent clus-
tering property is the Chinese Restaurant Process
(CRP). Customers (instances) arrive at a Chinese
restaurant which has an infinite number of tables
(components). Each customer sits at one of the ta-
bles that is either occupied or vacant with popular
tables attracting more customers.
Figure 1: Graphical representation of DPMMs.
In this work, the distribution used to model the
components is the multinomial and the prior used
is the Dirichlet distribution (F and G0 in Eq. 1).
The conjugacy between them allows for the ana-
lytic integration over the component parameters.
Following Neal (2000), the component assign-
ments zi are sampled using the following scheme:
P (zi = z|z?i, xi) ?
p(zi = z|z?i)DirM(xi|zi = z, x?i,z, ?) (2)
In Eq. 2DirM is the Dirichlet-Multinomial distri-
bution, ? are the parameters of the Dirichlet prior
G0 and x?i,z are the instances assigned already to
component z (none if we are sampling the prob-
ability of assignment to a new component). This
sampling scheme is possible due to the fact that the
instances in the model are exchangeable, i.e. the
order in which they are generated is not relevant.
In terms of the CRP metaphor, we consider each
instance xi as the last customer to arrive and he
chooses to sit together with other customers at an
existing table or to sit at a new table. Following
Navarro et al (2006) who used the same model to
analyze individual differences, we sample the con-
centration parameter ? using the inverse Gamma
distribution as a prior.
3 Evaluation measures
The evaluation of unsupervised clustering against
a gold standard is not straightforward because the
clusters found are not explicitly labelled. Formally
defined, an unsupervised clustering algorithm par-
titions a set of instances X = {xi|i = 1, ..., N}
into a set of clusters K = {kj |j = 1, ..., |K|}.
The standard approach to evaluate the quality of
the clusters is to use an external gold standard in
which the instances are partitioned into a set of
75
classes C = {cl|l = 1, ..., |C|}. Given this, the
goal is to find a partitioning of the instances K
that is as close as possible to the gold standard C.
Most work on verb clustering has used the F-
measure or the Rand Index (RI) (Rand, 1971)
for evaluation, which rely on counting pairwise
links between instances. However, Rosenberg and
Hirschberg (2007) pointed out that F-measure as-
sumes (the missing) mapping between cl and kj .
In practice, RI values concentrate in a small inter-
val near 100% (Meila?, 2007).
Rosenberg & Hirschberg (2007) proposed an
information-theoretic metric: V-measure. V-
measure is the harmonic mean of homogeneity
and completeness which evaluate the quality of the
clustering in a complementary way. Homogeneity
assesses the degree to which each cluster contains
instances from a single class of C. This is com-
puted as the conditional entropy of the class dis-
tribution of the gold standard given the clustering
discovered by the algorithm, H(C|K), normal-
ized by the entropy of the class distribution in the
gold standard, H(C). Completeness assesses the
degree to which each class is contained in a single
cluster. This is computed as the conditional en-
tropy of the cluster distribution discovered by the
algorithm given the class, H(K|C), normalized
by the entropy of the cluster distribution, H(K).
In both cases, we subtract the resulting ratios from
1 to associate higher scores with better solutions:
h = 1?
H(C|K)
H(C)
c = 1?
H(K|C)
H(K)
V? =
(1 + ?) ? h ? c
(? ? h) + c
(3)
The parameter ? in Eq. 3 regulates the balance
between homogeneity and completeness. Rosen-
berg & Hirschberg set it to 1 in order to obtain the
harmonic mean of these qualities. They also note
that V-measure favors clustering solutions with a
large number of clusters (large |K|), since such so-
lutions can achieve very high homogeneity while
maintaining reasonable completeness. This ef-
fect is more prominent when a dataset includes a
small number of instaces for gold standard classes.
While increasing |K| does not guarantee an in-
crease in V-measure (splitting homogeneous clus-
ters would reduce completeness without improv-
ing homogeneity), it is easier to achieve higher
scores when more clusters are produced.
Another relevant measure is the Variation of In-
formation (VI) (Meila?, 2007). Like V-measure,
it assesses homogeneity and completeness using
the quantitiesH(C|K) andH(K|C) respectively,
however it simply adds them up to obtain a final
result (higher scores are worse). It is also a metric,
i.e. VI scores can be added, subtracted, etc, since
the quantities involved are measured in bits. How-
ever, it can be observed that if |C| and |K| are very
different then the terms H(C|K) and H(K|C)
will not necessarily be in the same range. In par-
ticular, if |K|  |C| then H(K|C) (and V I) will
be low. In addition, VI scores are not normalized
and therefore their interpretation is difficult.
Both V-measure and VI have important advan-
tages over RI and F-measure: they do not assume
a mapping between classes and clusters and their
scores depend only on the relative sizes of the clus-
ters. However, V-measure and VI can be mislead-
ing if the number of clusters found (|K|) is sub-
stantially different than the number of gold stan-
dard classes (|C|). In order to ameliorate this, we
suggest to take advantage of the ? parameter in
Eq. 3 in order to balance homogeneity and com-
pleteness. More specifically, setting ? = |K|/|C|
assigns more weight to completeness than to ho-
mogeneity in case |K| > |C| since the former is
harder to achieve and the latter is easier when the
clustering solution has more clusters than the gold
standard has classes. The opposite occurs when
|K| < |C|. In case |K| = |C| the score is the
same as the original V-measure. Achieving 100%
score according to any of these measures requires
correct prediction of the number of clusters.
In this work, we evaluate our results using the
three measures described above (V-measure, VI,
V-beta). We complement this evaluation with
qualitative evaluation which assesses the poten-
tial of DPMMs to discover novel information that
might not be included in the gold standard.
4 Experiments
To perform lexical-semantic verb clustering we
used the dataset of Sun et al (2008). It contains
204 verbs belonging to 17 fine-grained classes in
Levin?s (1993) taxonomy so that each class con-
tains 12 verbs. The classes and their verbs were
selected randomly. The features for each verb are
its subcategorization frames (SCFs) and associ-
ated frequencies in corpus data, which capture the
76
DPMM Sun et al
no. of clusters 37.79 17
homogeneity 60.23% 57.57%
completeness 55.82% 60.19%
V-measure 57.94% 58.85%
V-beta 57.11% 58.85%
VI (bits) 3.5746 3.3598
Table 1: Clustering performances.
syntactic context in which the verb occurs. SCFs
were extracted from the publicly available VALEX
lexicon (Korhonen et al, 2006a). VALEX was ac-
quired automatically using a domain-independent
statistical parsing toolkit, RASP (Briscoe and Car-
roll, 2002), and a classifier which identifies verbal
SCFs. As a consequence, it includes some noise
due to standard text processing and parsing errors
and due to the subtlety of argument-adjunct dis-
tinction. In our experiments, we used the SCFs
obtained from VALEX1, parameterized for the
prepositional frame, which had the best perfor-
mance in the experiments of Sun et al (2008).
The feature sets based on verbal SCFs are very
sparse and the counts vary over a large range of
values. This can be problematic for generative
models like DPMMs, since a few dominant fea-
tures can mislead the model. To reduce the spar-
sity, we applied non-negative matrix factorization
(NMF) (Lin, 2007) which decomposes the dataset
in two dense matrices with non-negative values. It
has proven useful in a variety of tasks, e.g. infor-
mation retrieval (Xu et al, 2003) and image pro-
cessing (Lee and Seung, 1999).
We use a symmetric Dirichlet prior with param-
eters of 1 (? in Equation 2). The number of di-
mensions obtained using NMF was 35. We run
the Gibbs sampler 5 times, using 100 iterations for
burn-in and draw 20 samples from each run with
5 iterations lag between samples. Table 1 shows
the average performances. The DPMM discov-
ers 37.79 verb clusters on average with its perfor-
mance ranging between 53% and 58% depending
on the evaluation measure used. Homogeneity is
4.5% higher than completeness, which is expected
since the number of classes in the gold standard is
17. The fact that the DPMM discovers more than
twice the number of classes is reflected in the dif-
ference between the V-measure and V-beta, the lat-
ter being lower. In the same table, we show the re-
sults of Sun et al (2008), who used pairwise clus-
tering (PC) (Puzicha et al, 2000) which involves
determining the number of clusters in advance.
The performance of the DPMM is 1%-3% lower
than that of Sun et al As expected, the differ-
ence in V-measure is smaller since the DPMM
discovers a larger number of clusters, while for
VI it is larger. The slightly better performance
of PC can be attributed to two factors. First,
the (correct) number of clusters is given as in-
put to the PC algorithm and not discovered like
by the DPMM. Secondly, PC uses the similarities
between the instances to perform the clustering,
while the DPMM attempts to find the parameters
of the process that generated the data, which is a
different and typically a harder task. In addition,
the DPMM has two clear advantages which we il-
lustrate in the following sections: it can be used to
discover novel information and it can be modified
to incorporate intuitive human supervision.
5 Qualitative evaluation
The gold standard employed in this work (Sun et
al., 2008) is not fully accurate or comprehensive.
It classifies verbs according to their predominant
senses in the fairly small SemCor data. Individ-
ual classes are relatively coarse-grained in terms
of syntactic-semantic analysis1 and they capture
some of the meaning components only. In addi-
tion, the gold standard does not capture the se-
mantic relatedness of distinct classes. In fact, the
main goal of clustering is to improve such exist-
ing classifications with novel information and to
create classifications for new domains. We per-
formed qualitative analysis to investigate the ex-
tent to which the DPMM meets this goal.
We prepared the data for qualitative analysis as
follows: We represented each clustering sample
as a linking matrix between the instances of the
dataset and measured the frequency of each pair
of instances occurring in the same cluster. We
constructed a partial clustering of the instances
using only those links that occur with frequency
higher than a threshold prob link. Singleton clus-
ters were formed by considering instances that
are not linked with any other instances more fre-
quently than a threshold prob single. The lower
the prob link threshold, the larger the clusters will
be, since more instances get linked. Note that in-
cluding more links in the solution can either in-
1Many original Levin classes have been manually refined
in VerbNet.
77
crease the number of clusters when instances in-
volved were not linked otherwise, or decrease it
when linking instances that already belong to other
clusters. The higher the prob single threshold,
the more instances will end up as singletons. By
adjusting these two thresholds we can affect the
coverage of the analysis. This approach was cho-
sen because it enables to conduct qualitative analy-
sis of data relevant to most clustering samples and
irrespective of individual samples. It can also be
useful in order to use the output of the clustering
algorithm as a component in a pipeline which re-
quires a single result rather than multiple samples.
Using this method, we generated data sets for
qualitative analysis using 4 sets of values for
prob link and prob single, respectively: (99%,
1%), (95%, 5%), (90%, 10%) and (85%, 15%).
Table 1 shows the number of a) verbs, b) clusters
(2 or more instances) and c) singletons in each
resulting data set, along with the percentage and
size of the clusters which represent 1, 2, or mul-
tiple gold standard classes. As expected, higher
threshold values produce high precision clusters
for a smaller set of verbs (e.g. (99%,1%) pro-
duces 5 singletons and assigns 70 verbs to 20 clus-
ters, 55% of which represent a single gold stan-
dard class), while less extreme threshold values
yield higher recall clusters for a larger set of verbs
(e.g. (85%,15%) produces 10 singletons and as-
signs 140 verbs to 25 clusters, 20% of which con-
tain verbs from several gold standard classes).
We conducted the qualitative analysis by com-
paring the four data sets against the gold standard,
SCF distributions, and WordNet (Fellbaum, 1998)
senses for each test verb. We first analysed the
5-10 singletons in data sets and discovered that
while 3 of the verbs resist classification because
of syntactic idiosyncrasy (e.g. unite takes intransi-
tive SCFs with frequency higher than other mem-
bers of class 22.2), the majority of them (7) end
up in singletons for valid semantic reasons: taking
several frequent WordNet senses they are ?too pol-
ysemous? to be realistically clustered according to
their predominant sense (e.g. get and look).
We then examined the clusters, and discovered
that even in the data set created with the lowest
prob link threshold of 85%, almost half of the
?errors? are in fact novel semantic patterns discov-
ered by clustering. Many of these could be new
sub-classes of existing gold standard classes. For
example, looking at the 13 high accuracy clusters
which correspond to a single gold standard class
each, they only represent 9 gold standard classes
because as many as 4 classes been divided into
two clusters, suggesting that the gold standard is
too coarse-grained. Interestingly, each such sub-
division seems semantically justified (e.g. the 11.1
PUT verbs bury and immerse appear in a differ-
ent cluster than the semantically slightly different
place and situate).
In addition, the DPMM discovers semantically
similar gold standard classes. For example, in the
data set created with the prob link threshold of
99%, 6 of the clusters include members from 2
different gold standard classes. 2 occur due to
syntactic idiosyncrasy, but the majority (4) oc-
cur because of true semantic relatedness (e.g. the
clustering relates 22.2 AMALGAMATE and 36.1
CORRESPOND classes which share similar mean-
ing components). Similarly, in the data set pro-
duced by the prob link threshold of 85%, one
of the largest clusters includes 26 verbs from 5
gold standard classes. The majority of them be-
long to 3 classes which are related by the meaning
component of ?motion?: 43.1 LIGHT EMISSION,
47.3 MODES OF BEING INVOLVING MOTION, and
51.3.2 RUN verbs:
? class 22.2 AMALGAMATE: overlap
? class 36.1 CORRESPOND: banter, concur, dissent, hag-
gle
? class 43.1 LIGHT EMISSION: flare, flicker, gleam, glis-
ten, glow, shine, sparkle
? class 47.3 MODES OF BEING INVOLVING MOTION:
falter, flutter, quiver, swirl, wobble
? class 51.3.2 RUN: fly, gallop, glide, jog, march, stroll,
swim, travel, trot
Thus many of the singletons and the clusters
in the different outputs capture finer or coarser-
grained lexical-semantic differences than those
captured in the gold standard. It is encouraging
that this happens despite us focussing on a rela-
tively small set of 204 verbs and 17 classes only.
6 Constrained DPMMs
While the ability to discover novel information is
attractive in NLP, in many cases it is also desir-
able to influence the solution with respect to some
prior intuition or consideration relevant to the ap-
plication in mind. For example, while discover-
ing finer-grained classes than those included in the
gold standard is useful for some applications, oth-
ers may benefit from a coarser clustering or a clus-
tering that reveals a specific aspect of the dataset.
78
% and size of clusters containing
THR verbs clusters singletons 1 class 2 classes multiple classes
99%,1% 70 20 5 55% (3.0) 30% (2.8) 15% (4.5)
95%,5% 104 25 9 40% (3.7) 44% (2.8) 16% (6.8)
90%,10% 128 28 9 46% (3.4) 39% (2.5) 14% (11.0)
85%,15% 140 25 10 44% (3.7) 28% (3.3) 20% (13.0)
Table 2: An overview of the data sets generated for qualitative analysis
Preliminary work by Vlachos et al (2008) intro-
duced a constrained version of DPMMs that en-
ables human supervision to guide the clustering
solution when needed. We model the human su-
pervision as pairwise constraints over instances,
following Wagstaff & Cardie (2000): given a pair
of instances, they are either linked together (must-
link) or not (cannot-link). For example, charge
and run should form a must-link if the aim is
to cluster 51.3 MOTION verbs together, but they
should form a cannot-link if we are interested in
54.5 BILL verbs. In the discussion and the experi-
ments that follow, we assume that all links are con-
sistent with each other. This information can be
obtained by asking human experts to label links,
or by extracting it from extant lexical resources.
Specifying the relations between the instances re-
sults in a partial labeling of the instances. Such
labeling is likely to be re-usable, since relations
between the instances are likely to be useful for a
wider range of tasks which might not have identi-
cal labels but could still have similar relations.
In order to incorporate the constraints in the
DPMM, we modify the underlying generative pro-
cess to take them into account. In particular must-
linked instances are generated by the same com-
ponent and cannot-linked instances always by dif-
ferent ones. In terms of the CRP metaphor, cus-
tomers connected with must-links arrive at the
restaurant together and choose a table jointly, re-
specting their cannot-links with other customers.
They get seated at the same table successively one
after the other. Customers without must-links with
others choose tables avoiding their cannot-links.
In order to sample the component assignments
according to this model, we restrict the Gibbs sam-
pler to take them into account using the sampling
scheme of Fig. 2. First we identify linked-groups
of instances, taking into account transitivity2. We
then sample the component assignments only from
distributions that respect the links provided. More
2If A is linked to B and B to C, then A is linked to C.
specifically, for each instance that does not belong
to a linked-group, we restrict the sampler to choose
components that do not contain instances cannot-
linked with it. For instances in a linked-group, we
sample their assignment jointly, again taking into
account their cannot-links. This is performed by
adding each instance of the linked-group succes-
sively to the same component. In Fig. 2, Ci are the
cannot-links for instance(s) i, ` are the indices of
the instances in a linked-group, and z<i and x<i
are the assignments and the instances of a linked-
group that have been assigned to a component be-
fore instance i.
Input: data X , must-linksM, cannot-links C
linked groups = find linked groups(X ,M)
Initialize Z according toM, C
for i not in linked groups
for z = 1 to |Z|+ 1
if x?i,z ? Ci = ?
P (zi = z|z?i, xi) (Eq. 2)
else
P (zi = z|z?i, xi) = 0
Sample from P (zi)
for ` in linked groups
for z = 1 to |Z|+ 1
if x?`,z ? C` = ?
Set P (z` = z|z?`, x`) = 1
for i in `
P (z`= z|z?`, x`)? =
P (zi = z|z?`, x?`,z, z<i, x<i)
else
P (z` = z|z?`, x`) = 0
Sample from P (z`)
Figure 2: Gibbs sampler incorporating must-links
and cannot-links.
7 Experiments using constraints
To investigate the impact of pairwise constraints
on clustering by the DPMM, we conduct exper-
79
iments in which the links are sampled randomly
from the gold standard. The number of links var-
ied from 10 to 50 and the random choice was re-
peated 5 times without checking for redundancy
due to transitivity. All the other experimental set-
tings are identical to those in Section 4. Follow-
ing Wagstaff & Cardie (2000), in Table 3 we show
the impact of each link type independently (la-
beled ?must? and ?cannot? accordingly), as well
as when mixed in equal proportions (?mix?).
Adding randomly selected pairwise links is ben-
eficial. In particular, must-links improve the clus-
tering rapidly. Incorporating 50 must-links im-
proves the performance by 7-8% according to the
evaluation measures. In addition, it reduces the
average number of clusters by approximately 4.
The cannot-links are rather ineffective, which is
expected as the clustering discovered by the un-
supervised DPMM is more fine-grained than the
gold standard. For the same reason, it is more
likely that the randomly selected cannot-links are
already discovered by the DPMM and are thus re-
dundant. Wagstaff & Cardie also noted that the
impact of the two types of links tends to vary
across data sets. Nevertheless, a minor improve-
ment is observed in terms of homogeneity. The
balanced mix improves the performance, but less
rapidly than the must-links.
In order to assess how the links added help the
DPMM learn other links we use the Constrained
Rand Index (CRI), which is a modification of the
Rand Index that takes into account only the pair-
wise decisions that are not dictated by the con-
straints added (Wagstaff and Cardie, 2000; Klein
et al, 2002). We evaluate the constrained DPMM
with CRI (Table 3, bottom right graph) and our re-
sults show that the improvements obtained using
pairwise constraints are due to learning links be-
yond the ones enforced.
In a real-world setting, obtaining the mixed set
of links is equivalent to asking a human expert to
give examples of verbs that should be clustered to-
gether or not. Such information could be extracted
from a lexical resource (e.g. ontology). Alterna-
tively, the DPMM could be run without any con-
straints first and if a human expert judges the clus-
tering too coarse (or fine) then cannot-links (or
must-links) could help, since they can adapt the
clustering rapidly. When 20 randomly selected
must-links are integrated, the DPMM reaches or
exceeds the performance of PC used by Sun et
al. (2008) according to all the evaluation mea-
sures. We also argue that it is more realistic to
guide the clustering algorithm using pairwise con-
straints than by defining the number of clusters in
advance. Instead of using pairwise constraints to
affect the clustering solution, one could alter the
parameters for the Dirichlet prior G0 (Eq. 1) or
experiment with varying concentration parameter
values. However, it is difficult to predict in ad-
vance the exact effect such changes would have in
the solution discovered.
Finally, we conducted qualitative analysis of the
samples obtained constraining the DPMM with 10
randomly selected must-links. We first prepared
the data according to the method described in Sec-
tion 5, using prob link and prob single thresh-
olds of 99% and 1% respectively. This resulted in
26 clusters and one singleton for 79 verbs. Recall
that without constraining the DPMM these thresh-
olds produced 20 clusters and 5 singletons for 70
verbs. 49 verbs are shared in both outputs, while
the average cluster size is similar.
The resulting clusters are highly accurate. As
many as 16 (i.e. 62%) of them represent a sin-
gle gold standard class, 7 of which contain (only)
the pairs of must-linked verbs. Interestingly, only
11 out of 17 gold standard classes are exempli-
fied among the 16 clusters, with 5 classes sub-
divided into finer-grained classes. Each of these
sub-divisions seems semantically fully motivated
(e.g. 30.3 PEER verbs were subdivided so that
peep and peek were assigned to a different cluster
than the semantically different gaze, glance and
stare) and 4 of them can be directly attributed to
the use of must-links.
From the 6 clusters that contained members
from two different gold standard classes, the ma-
jority (5) make sense as well. 3 of these contain
members of must-link pairs together with verbs
from semantically related classes (e.g. 37.7 SAY
and 40.2 NONVERBAL EXPRESSION classes). 3 of
the clusters that contain members of several gold
standard classes include must-link pairs as well.
In two cases must-links have helped to bring to-
gether verbs which belong to the same class (e.g.
the members of the must-link pair broaden-freeze
which represent 45.4 CHANGE OF STATE class ap-
pear now in the same cluster with other class mem-
bers dampen, soften and sharpen). Thus, DP-
MMs prove useful in learning novel information
taking into account pairwise constraints. Only 4
80
 60
 61
 62
 63
 64
 65
 66
 67
 68
 0  10  20  30  40  50
Ho
m
og
en
eit
y
mix
must
cannot
 55
 56
 57
 58
 59
 60
 61
 62
 63
 0  10  20  30  40  50
Co
m
ple
te
ne
ss
mix
must
cannot
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 0  10  20  30  40  50
V-
m
ea
su
re
mix
must
cannot
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 0  10  20  30  40  50
V-
be
ta
mix
must
cannot
 2.9
 3
 3.1
 3.2
 3.3
 3.4
 3.5
 3.6
 3.7
 0  10  20  30  40  50
VI
mix
must
cannot
 90
 90.2
 90.4
 90.6
 90.8
 91
 91.2
 91.4
 91.6
 91.8
 0  10  20  30  40  50
CR
I
mix
must
cannot
Table 3: Performance of constrained DPMMs incorporating pairwise links.
(i.e. 15%) of the clusters in the output examined
are not meaningful (mostly due to the mismatch
between the syntax and semantics of verbs).
8 Related work
Previous work on unsupervised verb clustering
used algorithms that require the number of clus-
ters as input e.g. PC, Information Bottleneck (Ko-
rhonen et al, 2006b) and spectral clustering (Brew
and Schulte im Walde, 2002). In terms of apply-
ing non-parametric Bayesian approaches to NLP,
Haghighi and Klein (2007) evaluated the cluster-
ing properties of DPMMs by performing anaphora
resolution with good results.
There is a large body of work on semi-
supervised learning (SSL), but relatively little
work has been done on incorporating some form
of supervision in clustering. It is important to note
that the pairwise links used in this work consti-
tute a weak form of supervision since they cannot
be used to infer class labels which are required for
SSL. However, the opposite can be done. Wagstaff
& Cardie (2000) employed must-links and cannot-
links to constrain the COBWEB algorithm, while
Klein et al (2002) applied them to complete-link
hierarchical agglomerative clustering. The latter
also studied how the added links affect instances
not directly involved in them.
It can be argued that one could use clustering
algorithms that require the number of clusters to
be known in advance to discover interesting sub-
classes such as those discovered by the DPMMs.
However, this would normally require multiple
runs and manual inspection of the results, while
DPMMs discover them automatically. Apart from
the fact that fixing the number of clusters in ad-
vance restricts the discovery of novel information
in the data, such algorithms cannot take full ad-
vantage of the pairwise constraints, since the latter
are likely to change the number of clusters.
9 Conclusions - Future Work
In this work, following Vlachos et al (2008) we
explored the application of DPMMs to the task of
verb clustering. We modified V-measure (Rosen-
berg and Hirschberg, 2007) to deal more appro-
priately with the varying number of clusters dis-
covered by DPMMs and presented a method of
agregating the generated samples which allows for
qualitative evaluation. The quantitative and qual-
itative evaluation demonstrated that they achieve
performance comparable with that of previous
work and in addition discover novel information in
the data. Furthermore, we evaluated the incorpo-
ration of constraints to guide the DPMM obtaining
promising results and we discussed their applica-
tion in a real-world setup.
The results obtained encourage the application
of DPMMs and non-parametric Bayesian methods
to other NLP tasks. We plan to extend our ex-
periments to larger datasets and further domains.
While the improvements achieved using randomly
selected pairwise constraints were promising, an
active constraint selection scheme as in Klein et
al. (2002) could increase their impact. Finally,
an extrinsic evaluation of the clustering provided
by DPMMs in the context of an NLP application
would be informative on their practical potential.
81
Acknowledgments
We are grateful to Diarmuid O? Se?aghdha and Jur-
gen Van Gael for helpful discussions.
References
Chris Brew and Sabine Schulte im Walde. 2002. Spec-
tral Clustering for German Verbs. In Proceedings of
the 2002 Conference on Empirical Methods in Nat-
ural Language Processing, pages 117?124.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pages 1499?1504.
Hoa Trang Dang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Sharon J. Goldwater. 2007. Nonparametric bayesian
models of lexical acquisition. Ph.D. thesis, Brown
University, Providence, RI, USA.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 848?855, Prague, Czech Republic.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Dan Klein, Sepandar Kamvar, and Chris Manning.
2002. From instance-level constraints to space-level
constraints: Making the most of prior knowledge in
data clustering. In Proceedings of the Nineteenth In-
ternational Conference on Machine Learning.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006a. A large subcategorization lexicon for natural
language processing applications. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2006b. Automatic classification of verbs in
biomedical texts. In Proceedings of the COLING-
ACL, pages 345?352.
Daniel D. Lee and Sebastian H. Seung. 1999. Learning
the parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788?791, October.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago.
Chih-Jen Lin. 2007. Projected gradient methods for
nonnegative matrix factorization. Neural Compua-
tion, 19(10):2756?2779.
Marina Meila?. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
Daniel J. Navarro, Thomas L. Griffiths, Mark Steyvers,
and Michael D. Lee. 2006. Modeling individual dif-
ferences using dirichlet processes. Journal of Math-
ematical Psychology, 50(2):101?122, April.
Radford M. Neal. 2000. Markov Chain Sam-
pling Methods for Dirichlet Process Mixture Mod-
els. Journal of Computational and Graphical Statis-
tics, 9(2):249?265, June.
Jan Puzicha, Thomas Hofmann, and Joachim Buh-
mann. 2000. A theory of proximity based clus-
tering: Structure detection by optimization. Pattern
Recognition, 33(4):617?634.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical Association, 66(336):846?850.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of EMNLP-
CoNLL, pages 410?420, Prague, Czech Republic.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the 9th International Conference
on Intelligent Text Processing and Computational
Linguistics.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 95?102.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of COLING-ACL, pages 985?992, Syd-
ney, Australia.
Andreas Vlachos, Zoubin Ghahramani, and Anna Ko-
rhonen. 2008. Dirichlet process mixture models for
verb clustering. In Proceedings of the ICML work-
shop on Prior Knowledge for Text and Language.
Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning, pages 1103?1110, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Docu-
ment clustering based on non-negative matrix factor-
ization. In Proceedings of the 26th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in informaion retrieval, pages 267?273,
New York, NY, USA. ACM Press.
82
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 57?61,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Active Learning for Constrained Dirichlet Process Mixture Models
Andreas Vlachos
Computer Laboratory
University of Cambridge
av308@cl.cam.ac.uk
Zoubin Ghahramani
Department of Engineering
University of Cambridge
zoubin@eng.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
ejb@cl.cam.ac.uk
Abstract
Recent work applied Dirichlet Process
Mixture Models to the task of verb cluster-
ing, incorporating supervision in the form
of must-links and cannot-links constraints
between instances. In this work, we intro-
duce an active learning approach for con-
straint selection employing uncertainty-
based sampling. We achieve substantial
improvements over random selection on
two datasets.
1 Introduction
Bayesian non-parametric mixture models have the
attractive property that the number of components
used to model the data is not fixed in advance but
is determined by the model and the data. This
property is particularly interesting for NLP where
many tasks are aimed at discovering novel in-
formation. Recent work has applied such mod-
els to various tasks with promising results, e.g.
Teh (2006) and Cohn et al (2009).
Vlachos et al (2009) applied the basic model
of this class, the Dirichlet Process Mixture Model
(DPMM), to lexical-semantic verb clustering with
encouraging results. The task involves discov-
ering classes of verbs similar in terms of their
syntactic-semantic properties (e.g. MOTION class
for travel, walk, run, etc.). Such classes can pro-
vide important support for other tasks, such as
word sense disambiguation, parsing and seman-
tic role labeling. (Dang, 2004; Swier and Steven-
son, 2004) Although some fixed classifications are
available these are not comprehensive and are in-
adequate for specific domains.
Furthermore, Vlachos et al (2009) used a con-
strained version of the DPMM in order to guide
clustering towards some prior intuition or consid-
erations relevant to the specific task at hand. This
supervision was modelled as pairwise constraints
between instances and it informs the model of re-
lations between them that cannot be recovered by
the model on the basis of the feature representa-
tion used. Like other forms of supervision, these
constraints require manual annotation and it is im-
portant to maximize the benefits obtained from it.
Therefore it is natural to consider active learning
(Settles, 2009) in order to focus the supervision on
clusterings on which the model is uncertain.
In this work, we propose a simple yet effec-
tive active learning method employing uncertainty
based sampling. The effectiveness of the AL
method is demonstrated on two datasets, one of
which has multiple gold standards.
2 Constrained DPMMs for clustering
In DPMMs, the parameters of each component are
generated by a Dirichlet Process (DP) which can
be seen as a distribution over distributions. Each
instance, represented by its features, is generated
by the component it is assigned to. The compo-
nents discovered correspond to the clusters. The
prior probability of assigning an instance to a par-
ticular component is proportionate to the number
of instances already assigned to it, in other words,
the DPMM exhibits the ?rich get richer? prop-
erty. A popular metaphor to describe the DPMM
which exhibits an equivalent clustering property
is the Chinese Restaurant Process (CRP). Cus-
tomers (instances) arrive at a Chinese restaurant
which has an infinite number of tables (compo-
nents). Each customer sits at one of the tables that
is either occupied or vacant with popular tables at-
tracting more customers.
Following Navarro et al (2006), parameter es-
timation is performed using Gibbs sampling by
sampling the assignment zi of each instance xi
given all the others z?i and the data X:
P (zi = z|z?i, X) ?
p(zi = z|z?i)P (xi|zi = z,X?i) (1)
57
In Eq. 1 p(zi = z|z?i) is the CRP prior and
P (xi|zi = z,X?i) is the distribution that gener-
ates instance xi given it has been assigned to com-
ponent z. This sampling scheme is possible be-
cause the assignments in the model are exchange-
able, i.e. their order is not relevant.
The constrained version of the DPMM uses
pairwise constraints over instances in order to
adapt the clustering discovered. Following
Wagstaff & Cardie (2000), a pair of instances is
either linked together (must-link) or not (cannot-
link). For example, charge and run should form a
must-link if the aim is to cluster MOTION verbs
together, but they should form a cannot-link if we
are interested in BILL verbs. All links are as-
sumed to be consistent with each other. In order
to incorporate the constraints in the DPMM, the
Gibbs sampling scheme is modified so that must-
linked instances are generated by the same compo-
nent and cannot-linked instances always by differ-
ent ones. Following Vlachos et al (2009), for each
instance that does not belong to a linked-group, the
sampler is restricted to choose components that do
not contain instances cannot-linked with it. For
instances in a linked-group, their assignment is
sampled jointly, again taking into account their
cannot-links. This is performed by adding each
instance of the linked-group successively to the
same component. In terms of the CRP metaphor,
customers connected with must-links arrive at the
restaurant and choose a table jointly, respecting
their cannot-links with other customers.
3 Active Constraint Selection
In active learning, the model selects the supervi-
sion to be provided by a human expert. In the con-
text of the DPMMs, the model chooses a pair of
instances for which a must-link or a cannot-link
must be provided. To select the pair, we employ
the simple but effective idea of uncertainty based
sampling. We consider the most informative link
as that on which the model is most uncertain, more
formally the link between instances l?ij that maxi-
mizes the following entropy:
l?ij = argmaxi,j
H(zi = zj) (2)
If we consider clustering as binary classification of
links into must-links and cannot-links, it is equiv-
alent to selecting the pair with the highest label
entropy. During the sampling process used for
parameter inference, component assignments vary
between samples and the components themselves
are not identifiable, i.e. one cannot match the com-
ponents of one sample with those of another. Fur-
thermore, the conditional assignments estimated
during Gibbs sampling (Eq. 1) they do not capture
the uncertainty of the assignments z?i on which
they condition. Therefore, we resort to generating
a set of samples from the (possibly constrained)
DPMM and pick the link on which these sam-
ples maximally disagree, i.e. we approximate the
distribution in Eq. 2 with the probability that in-
stances i, j are in the same cluster or not. Thus,
in a given set of samples the most uncertain link
would be the one between two instances which are
in the same cluster in exactly half of these sam-
ples. Using multiple samples allows us to take into
account the uncertainty in the assignments of the
other instances, as well as the varying number of
components.
Compared to standard pool-based AL, when
clustering with constraints the possible links be-
tween two instances (ignoring transitivity) are
C(N, 2) = N(N ? 1)/2 (N is the size of the
dataset) and there is an equal number of candi-
date queries to be considered, as opposed to N
queries in a supervised classification task. Another
interesting difference is that the the AL process
can be initiated without any supervision, since the
DPMM is unsupervised. On the other hand, in
the standard AL scenario a (usually small) labelled
seed set is used. Therefore, we rely exclusively on
the model and the features to guide the constraint
selection process. If the model combined with the
features is not appropriate for the task then the
constraints chosen are unlikely to be useful.
4 Datasets and Evaluation
In our experiments we used two verb clustering
datasets, one from general English (Sun et al,
2008) and one from the biomedical domain (Ko-
rhonen et al, 2006). In both datasets the fea-
tures for each verb are its subcategorization frames
(SCFs) which capture the syntactic context in
which it occurs. They were acquired automati-
cally using a domain-independent statistical pars-
ing toolkit, RASP (Briscoe and Carroll, 2002), and
a classifier which identifies verbal SCFs. As a
consequence, they include some noise due to stan-
dard text processing and parsing errors and due to
the subtlety of the argument-adjunct distinction.
The general English dataset contains 204 verbs
58
belonging to 17 fine-grained classes in Levin?s
(Levin, 1993) taxonomy so that each class con-
tains 12 verbs. The biomedical dataset consists of
193 medium to high frequency verbs from a cor-
pus of 2230 full-text articles from 3 biomedical
journals. A team of linguists and biologists cre-
ated a three-level gold standard with 16, 34 and
50 classes. Both datasets were pre-processed us-
ing non-negative matrix factorization (Lin, 2007)
which decomposes a large sparse matrix into two
dense matrices (of lower dimensionality) with
non-negative values. In all experiments 35 dimen-
sions were kept. Preliminary experiments with
different number of dimensions kept did not affect
the performance substantially.
We evaluate our results using three informa-
tion theoretic measures: Variation of Informa-
tion (Meila?, 2007), V-measure (Rosenberg and
Hirschberg, 2007) and V-beta (Vlachos et al,
2009). All three assess the two desirable proper-
ties that a clustering should have with respect to
a gold standard, homogeneity and completeness.
Homogeneity reflects the degree to which each
cluster contains instances from a single class and
is defined as the conditional entropy of the class
distribution of the gold standard given the clus-
tering. Completeness reflects the degree to which
each class is contained in a single cluster and is de-
fined as the conditional entropy of clustering given
the class distribution in the gold standard. V-beta
balances these properties explicitly by taking into
account the ratio of the number of cluster discov-
ered over the number of classes in the gold stan-
dard. While an ideal clustering should have both
properties, naively improving one of them can be
harmful for the other. Compared to the more com-
monly used F-measure (Fung et al, 2003), these
measures have the advantage that they do not as-
sume a mapping between clusters and classes.
5 Experiments
We performed experiments in order to assess the
effectiveness of the AL algorithm for the con-
strained DPMM comparing it to random selection.
In each AL round, we run the Gibbs sampler for
the (constrained) DPMM five times, using 100 it-
erations for burn-in, draw 20 samples from each
run with 5 iterations lag between samples and se-
lect the most uncertain link to be labeled. Fol-
lowing Navarro et al (2006), the concentration
parameter is inferred from the data using Gibbs
sampling. The performances were averaged across
the collected samples. Random selection was re-
peated three times. The three levels of the biomed-
ical gold standard were used independently and to-
gether with the general English dataset result in
four experimental setups.
The comparison between AL and random se-
lection for each dataset is shown in graphs 1(a)-
1(d) using V-beta, noting that the observations
made hold with all evaluation metrics used. Con-
straints selected via AL improve the performance
rapidly. Indicatively, the performance reached us-
ing 1000 randomly chosen constraints is obtained
using only 110 actively selected ones in the bio-50
dataset. AL performance levels out in later stages
with performance superior to the one achieved us-
ing random selection with the same number of
constraints. The poor performance of random se-
lection is expected, since the unsupervised DPMM
predicts more than 90% of the binary links cor-
rectly. Another interesting observation is that, dur-
ing AL, homogeneity increased faster than com-
pleteness (graphs 1(g) and 1(h)). This suggests
that the features used lead the model towards finer-
grained clusters, which is further confirmed by
the fact that the highest scores on the biomedical
dataset are achieved when comparing against the
finest-grained version of the gold standard. While
it is possible to choose constraints to the model
that would increase completeness with respect to
the gold standard, we argue that this would not al-
low us to obtain obtain insights on the model and
the features used.
We also noticed that the choice of batch size
has a significant effect on the learning rate of the
model. This phenomenon occurs in varying de-
grees in many applications of AL. Manual inspec-
tion of the links chosen at each round revealed that
batches often contained links involving the same
instances. This is expected due to transitivity: if
the link between instances A and B is uncertain
but the link between instances B and C is certain,
then the link between A and C will be uncertain
too. While reducing the batch size leads to bet-
ter learning rates, it requires estimating the model
more often. In order to ameliorate this issue, af-
ter obtaining the label of the most uncertain link,
we remove the samples that disagreed with it and
re-calculate the uncertainty of the remaining links
given the remaining samples. This is repeated un-
til the intended batch size is reached. Thus, we
59
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active
random
(a) bio-16
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active
random
(b) bio-34
 0.72
 0.76
 0.8
 0.84
 0.88
 0  50  100  150  200  250
V-
be
ta
links
active
random
(c) bio-50
 0.55
 0.6
 0.65
 0.7
 0.75
 0  50  100  150  200  250
V-
be
ta
links
active
random
(d) gen. English
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active10
batch
(e) bio-16
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
active10
batch
(f) bio-34
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250
V-
be
ta
links
hom
comp
(g) bio-50
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  50  100  150  200  250
V-
be
ta
links
hom
comp
(h) gen. English
Figure 1: (a)-(d): Constrained DPMM learning curves comparing random selection and AL. (e),(f):
Batch selection comparison. (g),(h): Homogeneity and completeness curves during AL.
avoid selecting links involving the same instance,
unless their uncertainty was not reduced by the
constraints added. A consideration that arises is
that by reducing the number of samples used for
uncertainty estimation, progressively we are left
with fewer samples to rank the remaining links.
Each labeled link reduces the number of samples
approximately by half since the most uncertain
link is likely to be a must-link in half the sam-
ples and a cannot-lnk in the remaining half. As
a result, for a batch with size |B| the uncertainty
of the last link will be estimated using |S|/2|B|?1
samples. A crude solution would be to generate
enough samples for the desired batch size. How-
ever, obtaining a very large number of samples can
be computationally expensive. Therefore, we set a
threshold for the minimum number of samples to
be used to estimate the link uncertainty and when
it is reached, more samples are generated using the
constraints selected. In graphs 1(e) and 1(f) we
demonstrate the effectiveness of the batch selec-
tion method proposed (labeled ?batch?) compared
to naive batch selection (labeled ?active10?).
6 Discussion and Future Work
We presented an AL method for constrained DP-
MMs employing uncertainty based sampling. We
applied it to two different verb clustering datasets
with 4 gold standards in total and obtained very
good results compared to random selection. The
idea, while explored in the context of verb cluster-
ing with the constrained DPMM, is likely to be ap-
plicable to other models that can incorporate must-
links and cannot-links in MCMC sampling.
Most literature on AL for NLP considers super-
vised methods for classification or sequential tag-
ging. However, AL for clustering is a relatively
under-explored area. Klein et al (2002) incorpo-
rated actively selected constraints in hierarchical
agglomerative clustering. Basu et al (2006) have
applied AL to obtain must-links and cannot-links
however, the clustering framework used requires
the number of clusters to be known in advance
which restricts counter-intuitively the clustering
solutions that are discovered. Moreover, semi-
supervised clustering is a form of semi-supervised
learning and in this light, our approach is related
to the work of Zhu et al (2003).
With respect to the practical application of the
AL method suggested, it is worth noting that in all
our experiments the constraints were obtained for
the respective gold standard of the dataset at ques-
tion and consequently they are all consistent with
each other. However, this assumption might not
hold in case human experts are employed for the
same purpose. In order to use such feedback in the
framework suggested, it is necessary to filter the
constraints provided in order to obtain a consistent
subset. To this end, it would be interesting to in-
vestigate the potential of using ?soft? constraints,
i.e. constraints that are provided with relative con-
fidence.
60
References
Sugato Basu, Mikhail Bilenko, Arindam Banerjee,
and Raymond J. Mooney. 2006. Probabilis-
tic semi-supervised clustering with constraints. In
O. Chapelle, B. Schoelkopf, and A. Zien, edi-
tors, Semi-Supervised Learning, pages 73?102. MIT
Press.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pages 1499?1504.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548?556.
Hoa Trang Dang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA.
Benjamin C. M. Fung, Ke Wang, and Martin Ester.
2003. Hierarchical document clustering using fre-
quent itemsets. In Proceedings of SIAM Interna-
tional Conference on Data Mining, pages 59?70.
Dan Klein, Sepandar D. Kamvar, and Christopher D.
Manning. 2002. From instance-level constraints to
space-level constraints: Making the most of prior
knowledge in data clustering. In Proceedings of the
Nineteenth International Conference on Machine
Learning, pages 307?314.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2006. Automatic classification of verbs in
biomedical texts. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 345?352.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago.
Chih-Jen Lin. 2007. Projected gradient methods for
nonnegative matrix factorization. Neural Compua-
tion, 19(10):2756?2779.
Marina Meila?. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
Daniel J. Navarro, Thomas L. Griffiths, Mark Steyvers,
and Michael D. Lee. 2006. Modeling individual dif-
ferences using Dirichlet processes. Journal of Math-
ematical Psychology, 50(2):101?122, April.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 410?420.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, Univer-
sity of Wisconsin?Madison.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the 9th International Conference
on Intelligent Text Processing and Computational
Linguistics.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 95?102.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 985?992, Sydney, Australia, July.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and Constrained
Dirichlet Process Mixture Models for Verb Cluster-
ing. In Proceedings of the EACL workshop on GEo-
metrical Models of Natural Language Semantics.
Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning, pages 1103?1110.
Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani.
2003. Combining Active Learning and Semi-
Supervised Learning Using Gaussian Fields and
Harmonic Functions. In ICML workshop on The
Continuum from Labeled to Unlabeled Data in Ma-
chine Learning and Data Mining, pages 58?65.
61
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, page 56,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Bayesian Hidden Markov Models and Extensions
Invited Talk
Zoubin Ghahramani
Engineering Department, University of Cambridge, Cambridge, UK
zoubin@eng.cam.ac.uk
Hidden Markov models (HMMs) are one of the cornerstones of time-series modelling. I will review
HMMs, motivations for Bayesian approaches to inference in them, and our work on variational Bayesian
learning. I will then focus on recent nonparametric extensions to HMMs. Traditionally, HMMs have
a known structure with a fixed number of states and are trained using maximum likelihood techniques.
The infinite HMM (iHMM) allows a potentially unbounded number of hidden states, letting the model
use as many states as it needs for the data. The recent development of ?Beam Sampling? ? an efficient
inference algorithm for iHMMs based on dynamic programming ? makes it possible to apply iHMMs to
large problems. I will show some applications of iHMMs to unsupervised POS tagging and experiments
with parallel and distributed implementations. I will also describe a factorial generalisation of the iHMM
which makes it possible to have an unbounded number of binary state variables, and can be thought of
as a time-series generalisation of the Indian buffet process. I will conclude with thoughts on future
directions in Bayesian modelling of sequential data.
56
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 91?99,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
A Generative Model of Vector Space Semantics
Jacob Andreas
Computer Laboratory
University of Cambridge
jda33@cam.ac.uk
Zoubin Ghahramani
Department of Engineering
University of Cambridge
zoubin@eng.cam.ac.uk
Abstract
We present a novel compositional, gener-
ative model for vector space representa-
tions of meaning. This model reformulates
earlier tensor-based approaches to vector
space semantics as a top-down process,
and provides efficient algorithms for trans-
formation from natural language to vectors
and from vectors to natural language. We
describe procedures for estimating the pa-
rameters of the model from positive exam-
ples of similar phrases, and from distribu-
tional representations, then use these pro-
cedures to obtain similarity judgments for
a set of adjective-noun pairs. The model?s
estimation of the similarity of these pairs
correlates well with human annotations,
demonstrating a substantial improvement
over several existing compositional ap-
proaches in both settings.
1 Introduction
Vector-based word representations have gained
enormous popularity in recent years as a basic tool
for natural language processing. Various models
of linguistic phenomena benefit from the ability to
represent words as vectors, and vector space word
representations allow many problems in NLP to be
reformulated as standard machine learning tasks
(Blei et al, 2003; Deerwester et al, 1990).
Most research to date has focused on only one
means of obtaining vectorial representations of
words: namely, by representing them distribution-
ally. The meaning of a word is assumed to be
fully specified by ?the company it keeps? (Firth,
1957), and word co-occurrence (or occasionally
term-document) matrices are taken to encode this
context adequately. Distributional representations
have been shown to work well for a variety of dif-
ferent tasks (Schu?tze and Pedersen, 1993; Baker
and McCallum, 1998).
The problem becomes more complicated when
we attempt represent larger linguistic structures?
multiword constituents or entire sentences?
within the same vector space model. The most ba-
sic issue is one of sparsity: the larger a phrase, the
less frequently we expect it to occur in a corpus,
and the less data we will have from which to es-
timate a distributional representation. To resolve
this problem, recent work has focused on compo-
sitional vector space models of semantics. Based
on the Fregean observation that the meaning of a
sentence is composed from the individual mean-
ings of its parts (Frege, 1892), research in com-
positional distributional semantics focuses on de-
scribing procedures for combining vectors for in-
dividual words in order to obtain an appropriate
representation of larger syntactic constituents.
But various aspects of this account remain un-
satisfying. We have a continuous semantic space
in which finitely many vectors are associated with
words, but no way (other than crude approxima-
tions like nearest-neighbor) to interpret the ?mean-
ing? of all the other points in the space. More gen-
erally, it?s not clear that it even makes sense to talk
about the meaning of sentences or large phrases in
distributional terms, when there is no natural con-
text to represent.
We can begin to address these concerns by turn-
ing the conventional account of composition in
vector space semantics on its head, and describ-
ing a model for generating language from vectors
in semantic space. Our approach is still composi-
tional, in the sense that a sentence?s meaning can
be inferred from the meanings of its parts, but we
relax the requirement that lexical items correspond
to single vectors by allowing any vector. In the
process, we acquire algorithms for both meaning
inference and natural language generation.
Our contributions in this paper are as follows:
? A new generative, compositional model of
91
phrase meaning in vector space.
? A convex optimization procedure for map-
ping words onto their vector representations.
? A training algorithm which requires only
positive examples of phrases with the same
meaning.
? Another training algorithm which requires
only distributional representations of phrases.
? A set of preliminary experimental results in-
dicating that the model performs well on real-
world data in both training settings.
2 Model overview
2.1 Motivations
The most basic requirement for a vector space
model of meaning is that whatever distance metric
it is equipped with accurately model human judg-
ments of semantic similarity. That is: sequences
of words which are judged to ?mean the same
thing? should cluster close together in the seman-
tic space, and totally unrelated sequences of words
should be spread far apart.
Beyond this, of course, inference of vector
space representations should be tractable: we re-
quire efficient algorithms for analyzing natural
language strings into their corresponding vectors,
and for estimating the parameters of the model that
does the mapping. For some tasks, it is also useful
to have an algorithm for the opposite problem?
given a vector in the semantic space, it should be
possible to produce a natural-language string en-
coding the meaning of that vector; and, in keep-
ing with our earlier requirements, if we choose
a vector close to the vector corresponding to a
known string, the resulting interpretation should
be judged by a human to mean the same thing,
and perhaps with some probability be exactly the
same.
It is these three requirements?the use of human
similarity judgments as the measure of the seman-
tic space?s quality, and the existence of efficient al-
gorithms for both generation and inference?that
motivate the remainder of this work.
We take as our starting point the general pro-
gram of Coecke et al (2010) which suggests that
the task of analyzing into a vector space should
be driven by syntax. In this framework, the com-
positional process consists of repeatedly combin-
ing vector space word representations according to
linguistic rules, in a bottom-up process for trans-
lating a natural language string to a vector in
space.
But our requirement that all vectors be trans-
latable into meanings?that we have both analy-
sis and generation algorithms?suggests that we
should take the opposite approach, working with a
top down model of vector space semantics.
For simplicity, our initial presentation of this
model, and the accompanying experiments, will
be restricted to the case of adjective-noun pairs.
Section 5 will then describe how this framework
can be extended to full sentences.
2.2 Preliminaries
We want to specify a procedure for mapping a
natural language noun-adjective pair (a, n) into
a vector space which we will take to be Rp.
We assume that our input sentence has already
been assigned a single CCG parse (Steedman and
Baldridge, 2011), which for noun-adjective pairs
has the form
blue orangutans
N/N N >
N
(1)
Here, the parser has assigned each token a cate-
gory of the form N, N/N, etc. Categories are ei-
ther simple, drawn from a set of base types (here
just N for ?noun?), or complex, formed by com-
bining simple categories. A category of the form
X/Y ?looks right? for a category of the form Y,
and can combine with other constituents by appli-
cation (we write X/Y Y ? X) or composition
(X/Y Y/Z ? X/Z) to form higher-level con-
stituents.
To this model we add a vector space seman-
tics. We begin with a brief review the work of
Coecke et al (2010). Having assigned simple cat-
egories to vector spaces (in this case, N to Rp),
complex categories correspond to spaces of ten-
sors. A category of the form X/Y is recursively
associated with SX ? SY, where SX and SY are
the tensor spaces associated with the categories X
and Y respectively. So the space of adjectives (of
type N/N) is just Rq?Rq, understood as the set of
q? q matrices. To find the meaning of a adjective-
noun pair, we simply multiply the adjective matrix
and noun vector as specified by the CCG deriva-
tion. The result is another vector in the same se-
mantic space as the noun, as desired.
92
To turn this into a top-down process, we need
to describe a procedure for splitting meanings and
their associated categories.
2.3 Generation
Our goal in this subsection is to describe a proba-
bilistic generative process by which a vector in a
semantic space is realized in natural language.
Given a constituent of category X, and a corre-
sponding vector x residing in some SX , we can ei-
ther generate a lexical item of the appropriate type
or probabilistically draw a CCG derivation rooted
in X, then independently generate the leaves. For
noun-adjective pairs, this can only be done in one
way, namely as in (1) (for a detailed account of
generative models for CCG see Hockenmaier and
Steedman (2002)). We will assume that this CCG
derivation tree is observed, and concern ourselves
with filling in the appropriate vectors and lexical
items. This is a strong independence assumption!
It effectively says ?the grammatical realization of
a concept is independent of its meaning?. We will
return to it in Section 6.
The adjective-noun model has four groups of
parameters: (1) a collection ?N/N of weight vec-
tors ?a for adjectives a, (2) a collection ?N of
weight vectors ?n for nouns n, (3) a collection
EN/N of adjective matrices Ea for adjectives a, and
finally (4) a noise parameter ?2. For compactness
of notation we will denote this complete set of pa-
rameters ?.
Now we can describe how to generate an
adjective-noun pair from a vector x. The CCG
derivation tells us to produce a noun and an ad-
jective, and the type information further informs
us that the adjective acts as a functor (here a ma-
trix) and the noun as an argument. We begin by
choosing an adjective a conditional on x. Having
made our lexical choice, we deterministically se-
lect the corresponding matrix Ea from EN/N. Next
we noisily generate a new vector y = Eax + ?,
a vector in the same space as x, corresponding
to the meaning of x without the semantic content
of a. Finally, we select a noun n conditional on
y, and output the noun-adjective pair (a, n). To
use the previous example, suppose x means blue
orangutans. First we choose an adjective a =
?blue? (or with some probability ?azure? or ?ultra-
marine?), and select a corresponding adjectiveEa.
Then the vector y = Eax should mean orangutan,
and when we generate a noun conditional on y we
should have n = ?orangutan? (or perhaps ?mon-
key?, ?primate?, etc.).
This process can be summarized with the graph-
ical model in Figure 1. In particular, we draw a
?N/N
a
x
Ea y n
EN/N ?2 ?N
Figure 1: Graphical model of the generative pro-
cess.
from a log-linear distribution over all words of the
appropriate category, and use the corresponding
Ea with Gaussian noise to map x onto y:
p(a|x; ?N/N) =
exp(?>a x)?
????N/N
exp(??>x)
(2)
p(y|x,Ea;?
2) = N (Eax, ?
2)(y) (3)
Last we choose n as
p(n|y; ?N) =
exp(?>n y)?
????N
exp(??>z)
(4)
Some high-level intuition about this model: in
the bottom-up account, operators (drawn from ten-
sor spaces associated with complex categories)
can be thought of as acting on simple objects and
?adding? information to them. (Suppose, for ex-
ample, that the dimensions of the vector space cor-
respond to actual perceptual dimensions; in the
bottom-up account the matrix corresponding to the
adjective ?red? should increase the component of
an input vector that lies in dimension correspond-
ing to redness.) In our account, by contrast, ma-
trices remove information, and the ?red? matrix
should act to reduce the vector component corre-
sponding to redness.
2.4 Analysis
Now we must solve the opposite problem: given
an input pair (a, n), we wish to map it to an ap-
propriate vector in Rp. We assume, as before, that
we already have a CCG parse of the input. Then,
analysis corresponds to solving the following op-
timization problem:
arg min
x
? log p(x|a, n;?)
93
By Bayes? rule,
p(x|a, n;?) ? p(a, n|x;?)p(x)
so it suffices to minimize
? log p(x)? log p(a, n|x;?)
To find the single best complete derivation of an
input pair (equivalent to the Viterbi parse tree in
syntactic parsing), we can rewrite this as
arg min
x,y
? log p(x)? log p(a, b, y|x;?) (5)
where, as before, y corresponds to the vector space
semantics representation of the noun alone. We
take our prior log p(x) to be a standard normal.
We have:
? log p(a, n, y|x)
= ? log p(a|x;?)? log p(y|a, x;?)
? log p(n|y;?)
? ??>a x+ log
?
????N/N
exp ??>x
+
1
?2
||Eax? y||
2
? ?>n y + log
?
????N
exp ??>y
Observe that this probability is convex: it con-
sists of a sum of linear terms, Euclidean norms,
and log-normalizers, all convex functions. Conse-
quently, Equation 5 can be solved exactly and ef-
ficiently using standard convex optimization tools
(Boyd and Vandenberghe, 2004).
3 Relation to existing work
The approach perhaps most closely related to the
present work is the bottom-up account given by
Coecke et al (2010), which has already been dis-
cussed in some detail in the preceding section. A
regression-based training procedure for a similar
model is given by Grefenstette et al (2013). Other
work which takes as its starting point the decision
to endow some (or all) lexical items with matrix-
like operator semantics include that of Socher et
al. (2012) and Baroni and Zamparelli (2010). In-
deed, it is possible to think of the model in Ba-
roni and Zamparelli?s paper as corresponding to
a training procedure for a special case of this
model, in which the positions of both nouns and
noun-adjective vectors are fixed in advance, and in
which no lexical generation step takes place. The
adjective matrices learned in that paper correspond
to the inverses of the E matrices used above.
Also relevant here is the work of Mitchell and
Lapata (2008) and Zanzotto et al (2010), which
provide several alternative procedures for compos-
ing distributional representations of words, and
Wu et al (2011), which describes a compositional
vector space semantics with an integrated syntac-
tic model. Our work differs from these approaches
in requiring only positive examples for training,
and in providing a mechanism for generation as
well as parsing. Other generative work on vec-
tor space semantics includes that of Hermann et
al. (2012), which models the distribution of noun-
noun compounds. This work differs from the
model that paper in attempting to generate com-
plete natural language strings, rather than simply
recover distributional representations.
In training settings where we allow all posi-
tional vectors to be free parameters, it?s possible
to view this work as a kind of linear relational em-
bedding (Paccanaro and Hinton, 2002). It differs
from that work, obviously, in that we are interested
in modeling natural language syntax and seman-
tics rather than arbitrary hierarchical models, and
provide a mechanism for realization of the embed-
ded structures as natural language sentences.
4 Experiments
Since our goal is to ensure that the distance be-
tween natural language expressions in the vector
space correlates with human judgments of their
relatedness, it makes sense to validate this model
by measuring precisely that correlation. In the re-
mainder of this section, we provide evidence of the
usefulness of our approach by focusing on mea-
surements of the similarity of adjective-noun pairs
(ANs). We describe two different parameter esti-
mation procedures for different kinds of training
data.
4.1 Learning from matching pairs
We begin by training the model on matching
pairs. In this setting, we start with a collec-
tion N sets of up to M adjective-noun pairs
(ai1, ni1), (ai2, ni2), . . . which mean the same
thing. We fix the vector space representation yi of
each noun ni distributionally, as described below,
and find optimal settings for the lexical choice pa-
rameters ?N/N and ?N, matrices (here all q ? q)
94
EN/N, and, for each group of adjective-noun pairs
in the training set, a latent representation xi. The
fact that the vectors yi are tied to their distribu-
tional vectors does not mean we have committed
to the distributional representation of the corre-
sponding nouns! The final model represents lexi-
cal choice only with the weight vectors ??fixing
the vectors just reduces the dimensionality of the
parameter estimation problem and helps steer the
training algorithm toward a good solution. The
noise parameter then acts as a kind of slack vari-
able, modeling the fact that there may be no pa-
rameter setting which reproduces these fixed dis-
tributional representations through exact linear op-
erations alone.
We find a maximum-likelihood estimate for
these parameters by minimizing
L(?, x) = ?
N?
i=1
M?
i=1
log p(aij , nij |xi;?) (6)
The latent vectors xi are initialized to one of their
corresponding nouns, adjective matricesE are ini-
tialized to the identity. The components of ?N are
initialized identically to the nouns they select, and
the components of ?N/N initialized randomly. We
additionally place an L2 regularization penalty on
the scoring vectors in both ? (to prevent weights
from going to infinity) and E (to encourage adjec-
tives to behave roughly like the identity). These
penalties, as well as the noise parameter, are ini-
tially set to 0.1.
Note that the training objective, unlike the
analysis objective, is non-convex. We use L-
BFGS (Liu and Nocedal, 1989) on the likeli-
hood function described above with ten such ran-
dom restarts, and choose the parameter setting
which assigns the best score to a held-out cross-
validation set. Computation of the objective and
its gradient at each step is linear in the number of
training examples and quadratic in the dimension-
ality of the vector space.
Final evaluation is performed by taking a set of
pairs of ANs which have been assigned a similar-
ity score from 1?6 by human annotators. For each
pair, we map it into the vector space as described
in Section 2.4 above. and finally compute the co-
sine similarity of the two pair vectors. Perfor-
mance is measured in the correlation (Spearman?s
?) between these cosine similarity scores and the
human similarity judgments.
4.1.1 Setup details
Noun vectors yi are estimated distributionally
from a corpus of approximately 10 million tokens
of English-language Wikipedia data (Wikimedia
Foundation, 2013). A training set of adjective-
noun pairs are collected automatically from a col-
lection of reference translations originally pre-
pared for a machine translation task. For each
foreign sentence we have four reference transla-
tions produced by different translators. We as-
sign POS tags to each reference (Loper and Bird,
2002) then add to the training data any adjec-
tive that appears exactly once in multiple refer-
ence translations, with all the nouns that follow it
(e.g. ?great success?, ?great victory?, ?great ac-
complishment?). We then do the same for repeated
nouns and the adjectives that precede them (e.g.
?great success?, ?huge success?, ?tremendous suc-
cess?). This approach is crude, and the data col-
lected are noisy, featuring such ?synonym pairs?
as (?incomplete myths?, ?incomplete autumns?)
and (?similar training?, ?province-level training?),
as well as occasional pairs which are not adjective-
noun pairs at all (e.g. ?first parliamentary?). Nev-
ertheless, as results below suggest, they appear to
be good enough for purposes of learning an appro-
priate representation.
For the experiments described in this section,
we use 500 sets of such adjective-noun pairs,
corresponding to 1104 total training examples.
Testing data consists of the subset of entries in
the dataset from (Mitchell and Lapata, 2010) for
which both the adjective and noun appear at least
once (not necessarily together) in the training set,
a total of 396 pairs. None of the pairs in this test
set appears in training. We additionally withhold
from this set the ten pairs assigned a score of 6
(indicating exact similarity), setting these aside for
cross-validation.
In addition to the model discussed in the first
section of this paper (referred to here as ?GEN?),
we consider a model in which there is only one
adjective matrix E used regardless of the lexical
item (referred to as ?GEN-1?).
The NP space is taken to be R20, and we re-
duce distributional vectors to 20 dimensions using
a singular value decomposition.
95
4.2 Learning from distributional
representations
While the model does not require distributional
representations of latent vectors, it?s useful to con-
sider whether it can also provide a generative ana-
log to recent models aimed explicitly at produc-
ing vectorial representations of phrases given only
distributional representations of their constituent
words. To do this, we take as our training data a
set of N single ANs, paired with a distributional
representation of each AN. In the new model, the
meaning vectors x are no longer free parameters,
but fully determined by these distributional repre-
sentations. We must still obtain estimates for each
? and EN/N, which we do by minimizing
L(?) = ?
N?
i=1
log p(ai,j , ni,j |xi;?) (7)
4.2.1 Experimental setup
Experimental setup is similar to the previous sec-
tion; however, instead of same-meaning pairs col-
lected from a reference corpus, our training data
is a set of distributional vectors. We use the same
noun vectors, and obtain these new latent pair vec-
tors by estimating them in the same fashion from
the same corpus.
In order to facilitate comparison with the other
experiment, we collect all pairs (ai, ni) such that
both ai and ni appear in the training set used in
Section 4.1 (although, once again, not necessar-
ily together). Initialization of ? and E , regular-
ization and noise parameters, as well as the cross-
validation procedure, all proceed as in the previ-
ous section. We also use the same restricted eval-
uation set, again to allow the results of the two
experiments to be compared. We evaluate by mea-
suring the correlation of cosine similarities in the
learned model with human similarity judgments,
and as before consider a variant of the model in
which a single adjective matrix is shared.
4.3 Results
Experimental results are displayed in Table 1. For
comparison, we also provide results for a base-
line which uses a distributional representation of
the noun only, the Adjective-Specific Linear Map
(ALM) model of Baroni and Zamparelli (2010) and
two vector-based compositional models discussed
in (Mitchell and Lapata, 2008): , which takes
the Hadamard (elementwise) product of the distri-
butional representations of the adjective and noun,
and +, which adds the distributions. As before,
we use SVD to project these distributional repre-
sentations onto a 20-dimensional subspace.
We observe that in both matrix-based learn-
ing settings, the GEN model or its parameter-
tied variant achieves the highest score (though
the distributionally-trained GEN-1 doesn?t per-
form as well as the summing approach). The pair-
trained model performs best overall. All corre-
lations except  and the distributionally-trained
GEN are statistically significant (p < 0.05), as
are the differences in correlation between the
matching-pairs-trained GEN and all other mod-
els, and between the distributionally-trained GEN-
1 and ALM. Readers familiar with other papers
employing the similarity-judgment evaluation will
note that scores here are uniformly lower than re-
ported elsewhere; we attribute this to the compar-
atively small training set (with hundreds, instead
of thousands or tens of thousands of examples).
This is particularly notable in the case of the ALM
model, which Baroni and Zamparelli report out-
performs the noun baseline when given a training
set of sufficient size.
Training data Model ?
Word distributions Noun .185
+ .239
 .000
Matching pairs GEN-1 .130
GEN .365
Word and phrase ALM .136
distributions GEN-1 .201
GEN .097
Table 1: Results for the similarity judgment exper-
iment.
We also give a brief demonstration of the gen-
eration capability of this model as shown in Fig-
ure 2. We demonstrate generation from three dif-
ferent vectors: one inferred as the latent represen-
tation of ?basic principles? during training, one
obtained by computing a vectorial representation
of ?economic development? as described in Sec-
tion 2.4 and one selected randomly from within
vector space. We observe that the model cor-
rectly identifies the adjectives ?fundamental? and
?main? as synonymous with ?basic? (at least when
applied to ?principles?). It is also able to cor-
rectly map the vector associated with ?economic
96
Input Realization
Training vector tyrannical principles
(?basic principles?) fundamental principles
main principles
Test vector economic development
(?economic development?) economic development
economic development
Random vector vital turning
further obligations
bad negotiations
Figure 2: Generation examples using the GEN
model trained with matching pairs.
development? back onto the correct lexical real-
ization. Words generated from the random vector
appear completely unrelated; this suggests that we
are sampling a portion of the space which does not
correspond to any well-defined concept.
4.4 Discussion
These experimental results demonstrate, first and
foremost, the usefulness of a model that is not tied
to distributional representations of meaning vec-
tors: as the comparatively poor performance of
the distribution-trained models shows, with only
a small number of training examples it is better to
let the model invent its own latent representations
of the adjective-noun pairs.
It is somewhat surprising, in the experi-
ments with distributional training data, that the
single-adjective model outperforms the multiple-
adjective model by so much. We hypothesize that
this is due to a search error?the significantly ex-
panded parameter space of the multiple-adjective
model makes it considerably harder to estimate
parameters; in the case of the distribution-only
model it is evidently so hard the model is unable
to identify an adequate solution even over multiple
training runs.
5 Extending the model
Having described and demonstrated the usefulness
of this model for capturing noun-adjective similar-
ity, we now describe how to extend it to capture
arbitrary syntax. While appropriate experimental
evaluation is reserved for future work, we outline
the formal properties of the model here. We?ll take
as our example the following CCG derivation:
sister Cecilia has blue orangutans
N/N N (S\N)/N N/N N
> >
N N
>
S\N
<
S
Observe that ?blue orangutans? is generated ac-
cording to the noun-adjective model already de-
scribed.
5.1 Generation
To handle general syntax, we must first extend the
set EN/N of adjective matrices to sets EX for all
functor categories X, and create an additional set
of weight vectors ?X for every category X.
When describing how to generate one split in
the CCG derivation (e.g. a constituent of type S
into constituents of type NP and S\NP), we can
identify three cases. The first, ?fully-lexicalized?
case is the one already described, and is the gen-
erative process by which the a vector meaning
blue orangutans is transformed into ?blue? and
?orangutans?, or sister Cecilia into ?sister? and
?Cecilia?. But how do we get from the top-level
sentence meaning to a pair of vectors meaning
sister Cecilia and has blue orangutans (an ?un-
lexicalized? split), and from has blue orangutans
to the word ?has? and a vector meaning blue
orangutans (a ?half-lexicalized? split)?
Unlexicalized split We have a vector xwith cat-
egory X, from which we wish to obtain a vector y
with category Y, and z with category Z. For this we
further augment the sets E with matrices indexed
by category rather than lexical item. Then we pro-
duce y = EYx + ?, z = EZ + ? where, as in the
previous case, ? is Gaussian noise with variance
?2. We then recursively generate subtrees from y
and z.
Half-lexicalized split This proceeds much as in
the fully lexicalized case. We have a vector x from
which we wish to obtain a vector y with category
Y, and a lexical item w with category Z.
We choose w according to Equation 2, select
a matrix Ew and produce y = Ewx + ? as be-
fore, and then recursively generate a subtree from
y without immediately generating another lexical
item for y.
5.2 Analysis
As before, it suffices to minimize
? log p(x) ? log p(W,P |x) for a sentence
97
W = (w1, w2, ? ? ? , wn) and a set of internal
vectors P . We select our prior p(x) exactly as
before, and can define p(W,P |x) recursively. The
fully-lexicalized case is exactly as above. For the
remaining cases, we have:
Unlexicalized split Given a subsequence
Wi:j = (wi, ? ? ? , wj), if the CCG parse splits Wi:j
into constituents Wi:k and Wk:j , with categories
Y and Z, we have:
? log p(Wi:j |x) =? log p(Wi:k, P |EY x)
? log p(Wk:j , P |EZx)
Half-lexicalized split If the parse splits Wi:j
into wi and Wi+1:j with categories Y and Z, and
y ? P is the intermediate vector used at this step
of the derivation, we have:
? log p(Wi:j , y|x)
= ? log p(wi|x)? log p(y|x,wi)
? log p(Wi+1:j |y)
? ??Twix+ log
?
w??LY
exp ?Tw?x
+
1
?2
||Ewix? y||
2
? log p(Wi+1:j , P |y)
Finally, observe that the complete expression
of the log probability of any derivation is, as be-
fore, a sum of linear and convex terms, so the
optimization problem remains convex for general
parse trees.
6 Future work
Various extensions to the model proposed in this
paper are possible. The fact that relaxing the
distributional requirement for phrases led to per-
formance gains suggests that something similar
might be gained from nouns. If a reliable train-
ing procedure could be devised with noun vectors
as free parameters, it might learn an even better
model of phrase similarity?and, in the process,
simultaneously perform unsupervised word sense
disambiguation on the training corpus.
Unlike the work of Coecke et al (2010), the
structure of the types appearing in the CCG deriva-
tions used here are neither necessary nor sufficient
to specify the form of the matrices used in this
paper. Instead, the function of the CCG deriva-
tion is simply to determine which words should
be assigned matrices, and which nouns. While
CCG provides a very natural way to do this, it
is by no means the only way, and future work
might focus on providing an analog using a differ-
ent grammar?all we need is a binary-branching
grammar with a natural functor-argument distinc-
tion.
Finally, as mentioned in Section 2.3, we have
made a significant independence assumption in re-
quiring that the entire CCG derivation be gener-
ated in advance. This assumption was necessary
to ensure that the probability of a vector in mean-
ing space given its natural language representation
would be a convex program. We suspect, however,
that it is possible to express a similar probabil-
ity for an entire packed forest of derivations, and
optimize it globally by means of a CKY-like dy-
namic programming approach. This would make
it possible to optimize simultaneously over all pos-
sible derivations of a sentence, and allow positions
in meaning space to influence the form of those
derivations.
7 Conclusion
We have introduced a new model for vector
space representations of word and phrase mean-
ing, by providing an explicit probabilistic process
by which natural language expressions are gener-
ated from vectors in a continuous space of mean-
ings. We?ve given efficient algorithms for both
analysis into and generation out of this meaning
space, and described two different training proce-
dures for estimating the parameters of the model.
Experimental results demonstrate that these al-
gorithms are capable of modeling graded human
judgments of phrase similarity given only positive
examples of matching pairs, or distributional rep-
resentations of pairs as training data; when trained
in this fashion, the model outperforms several
other compositional approaches to vector space
semantics. We have concluded by suggesting how
syntactic information might be more closely inte-
grated into this model. While the results presented
here are preliminary, we believe they present com-
pelling evidence of representational power, and
motivate further study of related models for this
problem.
Acknowledgments
We would like to thank Stephen Clark and An-
dreas Vlachos for feedback on a draft of this pa-
per.
98
References
L Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In Proceedings of the 21st annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 96?103.
ACM.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of Ma-
chine Learning Research, 3:993?1022.
Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex optimization. Cambridge university press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. arXiv
preprint arXiv:1003.4394.
Scott Deerwester, Susan T. Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391?407.
John Rupert Firth. 1957. A synopsis of linguistic the-
ory, 1930-1955.
Gottlob Frege. 1892. Uber Sinn und Bedeutung.
Zeitschrift fur Philosophie und philosophische Kri-
tik, pages 25?50. English Translation: em On Sense
and Meaning, in Brian McGuinness (ed), em Frege:
collected works, pp. 157?177, Basil Blackwell, Ox-
ford.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for compo-
sitional distributional semantics. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
Karl Moritz Hermann, Phil Blunsom, and Stephen Pul-
man. 2012. An unsupervised ranking model for
noun-noun compositionality. In Proceedings of the
First Joint Conference on Lexical and Computa-
tional Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 132?141. Association for
Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with combina-
tory categorial grammar. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 335?342. Association for Com-
putational Linguistics.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Edward Loper and Steven Bird. 2002. Nltk: the nat-
ural language toolkit. In Proceedings of the ACL-
02 Workshop on Effective tools and methodologies
for teaching natural language processing and com-
putational linguistics - Volume 1, ETMTNLP ?02,
pages 63?70, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of
ACL-08: HLT, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Alberto Paccanaro and Jefferey Hinton. 2002. Learn-
ing hierarchical structures with linear relational em-
bedding. In Advances in Neural Information Pro-
cessing Systems 14: Proceedings of the 2001 Neural
Information Processing Systems (NIPS) Conference,
volume 14, page 857. MIT Press.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector
model for syntagmatic and paradigmatic relatedness.
Making sense of words, pages 104?113.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Mark Steedman and Jason Baldridge. 2011. Combi-
natory categorial grammar. Non-Transformational
Syntax Oxford: Blackwell, pages 181?224.
Wikimedia Foundation. 2013. Wikipedia.
http://dumps.wikimedia.org/enwiki/. Accessed:
2013-04-20.
Stephen Wu, William Schuler, et al 2011. Struc-
tured composition of semantic vectors. In Proceed-
ings of the Ninth International Conference on Com-
putational Semantics (IWCS 2011), pages 295?304.
Citeseer.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distri-
butional semantics. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1263?1271. Association for Computa-
tional Linguistics.
99
